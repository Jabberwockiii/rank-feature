[
  {
    "title": "Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text",
    "link": "https://arxiv.org/pdf/2401.12070.pdf",
    "upvote": "40",
    "text": "Preprint\nSPOTTING LLMS WITH BINOCULARS:\nZERO-SHOT\nDETECTION OF MACHINE-GENERATED TEXT\nAbhimanyu Hans\u2217\nUniversity of Maryland\nAvi Schwarzschild\u2217\nCarnegie Mellon University\nValeriia Cherepanova\nUniversity of Maryland\nHamid Kazemi\nUniversity of Maryland\nAniruddha Saha\nUniversity of Maryland\nMicah Goldblum\nNew York University\nJonas Geiping\nELLIS Institute & MPI for Intelligent Systems,\nT\u00a8ubingen AI Center\nTom Goldstein\nUniversity of Maryland\nABSTRACT\nDetecting text generated by modern large language models is thought to be hard,\nas both LLMs and humans can exhibit a wide range of complex behaviors. How-\never, we find that a score based on contrasting two closely related language mod-\nels is highly accurate at separating human-generated and machine-generated text.\nBased on this mechanism, we propose a novel LLM detector that only requires\nsimple calculations using a pair of pre-trained LLMs. The method, called Binoc-\nulars, achieves state-of-the-art accuracy without any training data. It is capable of\nspotting machine text from a range of modern LLMs without any model-specific\nmodifications.\nWe comprehensively evaluate Binoculars on a number of text\nsources and in varied situations. Over a wide range of document types, Binoc-\nulars detects over 90% of generated samples from ChatGPT (and other LLMs) at\na false positive rate of 0.01%, despite not being trained on any ChatGPT data.\nNews\nCreative\nWriting\nStudent\nEssay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTPR @ 0.01% FPR\n0.949\n0.935\n0.976\n0.911\n0.900\n0.869\n0.991\n0.505\n0.647\n0.010\n0.045\n0.010\nBinoculars (Ours)\nGhostbuster (OOD)\nGPTZero\nDetectGPT (LLaMA-2-13B)\nFigure 1: Detection of Machine-Generated Text from ChatGPT. Our detection approach using\nBinoculars is highly accurate at separating machine-generated and human-written samples from\nNews, Creative Writing and Student Essay datasets with a false positive rate of 0.01%. Binoculars,\nbased on open-source Falcon models with no finetuning, outperforms commercial detection systems,\nsuch as GPTZero, as well as strong open-source detectors \u2013 even though both of these baselines are\nspecifically tuned to detect ChatGPT (Verma et al., 2023; Tian, 2023a). Our approach operates\nentirely in a zero-shot setting and has not been tuned nor trained to detect ChatGPT in particular.\n\u2217Equal Contribution. Correspondence to ahans1@umd.edu.\nCode available at https://github.com/ahans30/Binoculars.\n1\narXiv:2401.12070v1  [cs.CL]  22 Jan 2024\nPreprint\n1\nINTRODUCTION\nWe present a method for detecting LLM-generated text that works in the zero-shot setting in which\nno training examples are used from the LLM source. Even with this strict limitation, our scheme\nstill out-performs all open-source methods for ChatGPT detection and is competitive with or better\nthan commercial APIs, despite these competitors using training samples from ChatGPT (Mitchell\net al., 2023; Verma et al., 2023). At the same time, because of the zero-shot nature of our detector,\nthe very same detector can spot multiple different LLMs with high accuracy \u2013 something that all\nexisting solutions fail to do.\nThe ability to detect LLMs in the zero-shot setting addresses issues of growing importance. Prior\nresearch on combating academic plagiarism (TurnitIn.com) has fixated strongly on ChatGPT be-\ncause of its simple and accessible interface. But more sophisticated actors use LLM APIs to operate\nbots, create fake product reviews, and spread misinformation on social media platforms at a large\nscale. These actors have a wide range of LLMs available to them beyond just ChatGPT, making\nzero-shot, model-agnostic detection critical for social media moderation and platform integrity as-\nsurance (Crothers et al., 2022; Bail et al., 2023). Our zero-shot capability is a departure from existing\ndetectors that rely on model-specific training data and often fail to transfer to new models.\nOur proposed detector, called Binoculars, works by viewing text through two lenses. First, we\ncompute the log perplexity of the text in question using an \u201cobserver\u201d LLM. Then, we compute all\nthe next-token predictions that a \u201cperformer\u201d LLM would make at each position in the string, and\ncompute their perplexity according to the observer. If the string is written by a machine, we should\nexpect these two perplexities to be similar. If it is written by a human they should be different. We\nfirst motivate this simple observation, and then show that it is sufficient to build a strong zero-shot\ndetector, which we extensively stress-test in a number of text domains.\n2\nTHE LLM DETECTION LANDSCAPE\nA common first step in harm reduction for generative AI is detection. Specifically, applications that\nrange from documenting and tracing text origins (Biderman & Raff, 2022) to investigating spam and\nfake news campaigns (Zellers et al., 2019) and to analyzing training data corpora, all benefit from\nsignals that quantify whether text is human- or machine-generated (Bender et al., 2021; Crothers\net al., 2022; Mirsky et al., 2023).\nSuccessful efforts to spot machine-generated text show promise on early models whose generation\noutput is not convincingly human. However, with the rise of transformer models for language model-\ning (Radford et al., 2019; Brown et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023), primitive\nmechanisms to detect machine-generated text are rendered useless. While one approach is to record\n(Krishna et al., 2023) or watermark all generated text (Kirchenbauer et al., 2023), these preemptive\ndetection approaches can only be implemented with full control over the generative model.\nInstead, the recent spread of machine-generated text, especially via ChatGPT, has lead to a flurry of\nwork on post-hoc detection approaches that can be used to detect machine text without cooperation\nfrom model owners. These detectors can be separated into two main groups. The first is trained\ndetection models, where a pretrained language model backbone is finetuned for the binary classi-\nfication task of detection (Solaiman et al., 2019; Zellers et al., 2019; Yu et al., 2023; Zhan et al.,\n2023), including techniques like adversarial training (Hu et al., 2023) and abstention (Tian et al.,\n2023). Alternatively, instead of finetuning the whole backbone, a linear classifier can be fit on top\nof frozen learned features, which allows for the inclusion of commercial API outputs (Verma et al.,\n2023).\nThe second category of approaches comprises statistical signatures that are characteristic of\nmachine-generated text. These approaches have the advantage of requiring none or little training\ndata and they can easily be adapted to newer model families (Pu et al., 2022). Examples include\ndetectors based on perplexity (Tian, 2023b; Vasilatos et al., 2023; Wang et al., 2023), perplexity\ncurvature (Mitchell et al., 2023), log rank (Su et al., 2023), intrinsic dimensionality of generated text\n(Tulchinskii et al., 2023), and n-gram analysis (Yang et al., 2023). Our coverage of the landscape is\nnon-exhaustive, and we refer to recent surveys Ghosal et al. (2023); Tang et al. (2023); Dhaini et al.\n(2023); Guo et al. (2023) for additional details.\n2\nPreprint\nFrom a theoretical perspective, Varshney et al. (2020), Helm et al. (2023), and Sadasivan et al. (2023)\nall discuss the limits of detection. These works generally agree that fully general-purpose models\nof language would be, by definition, impossible to detect. However, Chakraborty et al. (2023) note\nthat even models that are arbitrarily close to this optimum are technically detectable given a suffi-\ncient number of samples. In practice, the relative success of detection approaches, such as the one\nwe propose and analyze in this work, provides constructive evidence that current language models\nare imperfect representations of human writing \u2013 and thereby detectable. Finally, the robustness of\ndetectors to attacks attempting to circumvent detection can provide stronger practical limits on reli-\nability in the worst case (Bhat & Parthasarathy, 2020; Wolff & Wolff, 2022; Liyanage & Buscaldi,\n2023).\nWith an understanding of how much work exists on LLM detection, a crucial question arises: How\ndo we appropriately and thoroughly evaluate detectors? Many works focus on accuracy on balanced\ntest sets and/or AUC of their proposed classifiers, but these metrics are not well-suited for the high-\nstakes question of detection. Ultimately, only detectors with low false positive rates across a wide\ndistribution of human-written text, truly reduce harm. Further, Liang et al. (2023) note that detectors\nare often only evaluated on relatively easy datasets that are reflective of their training data. Their\nperformance on out-of-domain samples is often abysmal. For example, TOEFL essays written by\nnon-native English speakers were wrongly marked as machine-generated 48-76% of the time by\ncommercial detectors (Liang et al., 2023).\nIn Section 3, we motivate our approach and discuss why detecting language model text, especially\nin the ChatGPT world, is difficult. In this work, our emphasis is directed toward baselines that\nfunction within post-hoc, out-of-domain (zero-shot), and black-box detection scenarios. We use the\nstate-of-the-art open source detector Ghostbuster (Verma et al., 2023), the commercially deployed\nGPTZero,1 and DetectGPT (Mitchell et al., 2023) to compare detection performance across various\ndatasets in Section 4. In Section 5, we evaluate the reliability of Binoculars in various settings that\nconstitute edge cases and crucial deployment behaviors that a detector based on Binoculars has to\ntake into account.\n3\nBinoculars: HOW IT WORKS\nOur approach, Binoculars, is so named as we look at inputs through the lenses of two different\nlanguage models. It is well known that perplexity \u2013 a common baseline for machine/human classi-\nfication \u2013 is insufficient on its own, leading prior work to disfavor approaches based on statistical\nsignatures. However we propose using a ratio of two scores, where one is a perplexity measurement\nand the other is cross-perplexity, a notion of how surprising the next token predictions of one model\nare to another model. This two-model mechanism is the basis for our general and accurate detector,\nand we show that this mechanism is able to detect a number of large language models, even when\nthey are unrelated to the two models used by Binoculars.\n3.1\nBACKGROUND & NOTATION\nA string of characters s can be parsed into tokens and represented as a list of token indices \u20d7x by\na tokenizer T. Let xi denote the token ID of the i-th token, which refers to an entry in the LLMs\nvocabulary V = {1, 2..., n}. Given a token sequence as input, a language model M predicts the\nnext token by outputting a probability distribution over the vocabulary:\nM(T(s)) = M(\u20d7x) = Y\nYij = P(vj|x0:i\u22121) for all j \u2208 V.\n(1)\nWe will abuse notation and abbreviate M(T(s)) as M(s) where the tokenizer is implicitly the one\nused in training M. For our purposes, we define log PPL, the log-perplexity, as the average negative\nlog-likelihood of all tokens in the given sequence. Formally, let\nlog PPLM(s) = \u2212 1\nL\nL\nX\ni=1\nlog(Yixi),\nwhere \u20d7x = T(s), Y = M(\u20d7x) and L = number of tokens in s.\n(2)\n1https://gptzero.me/\n3\nPreprint\nIntuitively, log-perplexity measures how \u201csurprising\u201d a string is to a language model. As mentioned\nabove, perplexity has been used to detect LLMs, as humans produce more surprising text than LLMs.\nThis is reasonable, as log PPL is also the loss function used to train generative LLMs, and models\nare likely to score their own outputs as unsurprising.\nOur method also measures how surprising the output of one model is to another. We define the\ncross-perplexity, which takes two models and a string as its arguments. Let log X-PPLM1,M2(s)\nmeasure the average per-token cross-entropy between the outputs of two models, M1 and M2 ,\nwhen operating on the tokenization of s.2\nlog X-PPLM1,M2(s) = \u2212 1\nL\nL\nX\ni=1\nM1(s)i \u00b7 log (M2(s)i) .\n(3)\nNote that \u00b7 denotes the dot product between two vector-valued quantities.\n3.2\nWHAT MAKES DETECTION HARD? A PRIMER ON THE CAPYBARA PROBLEM.\nWhy do we require measurements of both perplexity and cross-perplexity? Unsurprisingly, LLMs\ntend to generate text that is unsurprising to an LLM. Meanwhile, because humans differ from ma-\nchines, human text has higher perplexity according to an LLM observer. For this reason, it is tempt-\ning to use raw perplexity for LLM detection, as high perplexity is a strong sign of a human author.\nUnfortunately, this intuition breaks when hand-crafted prompts are involved. Prompts have a strong\ninfluence over downstream text, and prompts are typically unknown to the detector. On the one\nhand, the prompt \u201c1, 2, 3,\u201d might result in the very low perplexity completion \u201c4, 5, 6.\u201d On the\nother hand, the prompt \u201cCan you write a few sentences about a capybara that is an astrophysicist?\u201d\nwill yield a response that seems more strange. In the presence of the prompt, the response may be\nunsurprising (low perplexity). But in the absence of the prompt, a response containing the curious\nwords \u201ccapybara\u201d and \u201castrophysicist\u201d in the same sentence will have high perplexity, resulting in\nthe false determination that the text was written by a human, see the example in Figure 2. Clearly,\ncertain contexts will result in high perplexity and others low perplexity, regardless of whether the\nauthor is human or machine. We refer to this dilemma as \u201cthe capybara problem\u201d \u2013 in the absence\nof the prompt, LLM detection seems difficult and naive perplexity-based detection fails.\n\u201cDr. Capy Cosmos, a capybara unlike any other, astounded the scientific community\nwith his groundbreaking research in astrophysics. With his keen sense of observation\nand unparalleled ability to interpret cosmic data, he uncovered new insights into the\nmysteries of black holes and the origins of the universe. As he peered through tele-\nscopes with his large, round eyes, fellow researchers often remarked that it seemed as\nif the stars themselves whispered their secrets directly to him. Dr. Cosmos not only\nbecame a beacon of inspiration to aspiring scientists but also proved that intellect and\ninnovation can be found in the most unexpected of creatures.\u201d - ChatGPT\nFigure 2: This quote is LLM output from ChatGPT (GPT-4) when prompted with \u201cCan you\nwrite a few sentences about a capybara that is an astrophysicist?\u201d The Falcon LLM assigns this\nsample a high perplexity (2.20), well above the mean for both human and machine data. Despite\nthis problem, our detector correctly assigns a Binoculars score of 0.73, which is well below the\nglobal threshold of 0.901, resulting in a correct classification with high confidence. For reference,\nDetectGPT wrongly assigns a score of 0.14, which is below it\u2019s optimal threshold of 0.17, and\nclassifies the text as human. GPTZero assigns a 49.71% score that this text is generated by AI.\n3.3\nOUR DETECTION SCORE\nBinoculars solves the capybara problem by providing a mechanism for estimating the baseline per-\nplexity induced by the prompt. By comparing the perplexity of the observed text to this expected\nbaseline, we get fiercely improved LLM detection.\n2This requires that M1 and M2 share a tokenizer.\n4\nPreprint\nMotivation. Language models are known for producing low-perplexity text relative to humans and\nthus a perplexity threshold classifier makes for an obvious detecting scheme. However, in the LLM\nera, the generated text may exhibit a high perplexity score depending on the prompt specified (see\nthe \u201cCapybara Problem\u201d in Table 2). To calibrate for prompts that yield high-perplexity generation,\nwe use cross-perplexity introduced Equation (3) as a normalizing factor that intuitively encodes the\nperplexity level of next-token predictions from two models.\nRather than examining raw perplexity scores, we instead propose measuring whether the tokens that\nappear in a string are surprising relative to the baseline perplexity of an LLM acting on the same\nstring. A string might have properties that result in high perplexity when completed by any agent,\nmachine or human. Yet, we expect the next-token choices of humans to be even higher perplexity\nthan those of a machine. By normalizing the observed perplexity by the expected perplexity of a\nmachine acting on the same text, we can arrive at a detection metric that is fairly invariant to the\nprompt; see Table 2.\nWe propose the Binoculars score B as a sort of normalization or reorientation of perplexity. In\nparticular we look at the ratio of perplexity to cross-perplexity.\nBM1,M2(s) =\nlog PPLM1(s)\nlog X-PPLM1,M2(s)\n(4)\nHere, the numerator is simply the perplexity, which measures how surprising a string is to M1.\nThe denominator measures how surprising the token predictions of M2 are when observed by M1.\nIntuitively, we expect a human to diverge from M1 more than M2 diverges from M1, provided the\nLLMs M1 and M2 are more similar to each other than they are to a human.\nThe Binoculars score is a general mechanism that captures a statistical signature of machine text. In\nthe sections below, we show that for most obvious choices of M1 and M2, Binoculars does separate\nmachine and human text much better than perplexity alone. Importantly, it is capable of detecting\ngeneric machine-text generated by a third model altogether.\nInterestingly, we can draw some connection to other approaches that contrast two strong language\nmodels, such as contrastive decoding (Li et al., 2023), which aims to generate high-quality text\ncompletions by generating text that roughly maximizes the difference between a weak and a strong\nmodel. Speculative decoding is similar (Chen et al., 2023; Leviathan et al., 2023), it uses a weaker\nmodel to plan completions. Both approaches function best when pairing a strong model with a very\nweak secondary model. However, as we show below, our approach works best for two models that\nare very close to each other in performance. In the remainder of this work, we use the open-source\nmodels Falcon-7b model (M1) and the Falcon-7b-instruct (M2) (Almazrouei et al., 2023). The full\nset of combinations of scoring models used can be found in Table 2 in the appendix.\n4\nACCURATE ZERO-SHOT DETECTION\nIn this section we evaluate our proposed score, and build a zero-shot LLM detector with it. With\nBinoculars, we are able to spot machine-generated text in a number of domains. In our experimental\nevaluation, we focus on the problem setting of detecting machine-generated text from a modern\nLLM, as generated in common use cases without consideration for the detection mechanism.\n4.1\nDATASETS\nWe start our experiments with several datasets described in the LLM detection literature. The most\nrecent baseline to which we compare is Ghostbuster. Verma et al. (2023), who propose this method,\nalso introduce three datasets that we include in our study: Writing Prompts, News, and Student\nEssay. These are balanced datasets with equal numbers of human samples and machine samples.\nThe machine samples are written by ChatGPT.3\nWe also generate several datasets of our own to evaluate our capability in detecting other language\nmodels aside from ChatGPT. Drawing samples of human-written text from CCNews (Hamborg\n3In the following we will always use ChatGPT as short-hand for the chat versions of GPT-3.5-(turbo), not\nfor the chat versions of GPT-4.\n5\nPreprint\net al., 2017), PubMed (Sen et al., 2008), and CNN (Hermann et al., 2015), we generate alternative,\nmachine-generated completions using LLaMA-2-7B and Falcon-7B. To do so, we peel off the first\n50 tokens of each human sample and use it as a prompt to generate up to 512 tokens of machine\noutput. We then remove the human prompt from the generation and only use the purely machine-\ngenerated text in our machine-text datasets. Further, we use the Orca dataset (Lian et al., 2023),\nwhich provides several million instruction prompts with their machine-generated completions from\nchat versions of GPT-3 and GPT-4. This dataset allows us to check the reliability of the proposed\nmethod when detecting instruction-tuned models, and allows us to quantify detection differences\nbetween GPT-3 and GPT-4.\n4.2\nMETRICS\nSince detectors are binary classifiers, the standard suite of binary classification metrics are relevant.\nIn particular, it is often considered comprehensive to look at ROC curves and use the area under\nthe curve (AUC) as a performance metric. In fact, Verma et al. (2023) and Mitchell et al. (2023)\nonly report performance as measured by AUC and F1 scores. We argue that these metrics alone are\ninadequate when measuring LLM detection accuracy.\nIn high-stakes detection settings, the most concerning harms often arise from false positives, i.e.,\ninstances when human text is wrongly labeled as machine-generated. For this reason, we focus on\ntrue-positive rates (TPR) at low false-positive rates (FPR), and adopt a standard FPR threshold of\n0.01%.4 We will present F1 scores and AUC values only for comparison to prior publications, but\nwe prefer to focus on TPR values at low FPR as a key metric. The reader may observe that AUC\nscores are often uncorrelated with TRP@FPR when the FPR is below 1%.\n4.3\nBENCHMARK PERFORMANCE\nUsing a handful of datasets, we compare the AUC and TPR of Binoculars to Ghostbuster (Verma\net al., 2023), GPTZero (Tian, 2023a), and DetectGPT (using LLaMA-2-13B to score curvature)\n(Mitchell et al., 2023). We highlight that these comparisons on machine samples from ChatGPT\nare in favor of GPTZero and Ghostbuster, as these detectors have been tuned to detect ChatGPT\noutput, and comparisons using samples from LLaMA models are in favor of DetectGPT for the\nsame reason.\nOur score-based detector has just one single tunable parameter; a threshold to separate machine\nand human text, which we preset using reference data. We set the threshold using the combination\nof training splits from all of our reference datasets: News, Creative Writing, and Student Essay\ndatasets from Verma et al. (2023), which are generated using ChatGPT. We also compare detectors\non LLaMA-2-13B and Falcon-7B generated text with prompts from CC News, CNN, and PubMed\ndatasets. All of these datasets have an equal number of human and machine-generated text samples.\nWe optimize and fix our threshold globally using these datasets. As one exception, to be sure\nthat we meet the Ghostbuster definition of \u201cout-of-domain,\u201d when comparing our performance with\nGhostbuster we do not include the ChatGPT datasets (News, Creative Writing, and Student Essay)\nin the threshold determination, and only use samples from CC News, CNN, and PubMed (generated\nvia LLaMA and Falcon) to choose our threshold.\nGhostbuster Datasets. The Ghostbuster detector is a recent detector tuned to detect output from\nChatGPT. Using the same three datasets introduced and examined in the original work by Verma\net al. (2023), we compare TPR at 0.01% FPR in Figure 1 (and F1-Score in Figure 11 in Appendix)\nto show that Binoculars outperforms Ghostbuster in the \u201cout-of-domain\u201d setting. This setting is\nthe most realistic, and includes evaluation on datasets other than Ghostbuster\u2019s training data. A\ndesirable property for detectors is that with more information they get stronger. Figure 3 shows that\nboth Binoculars and Ghostbuster have this property, and that the advantages of Binoculars are even\nclearer in the few-token regime.\nOpen-Source Language Models. We show that our detector is capable of detecting the output\nof several LLMs, such as LLaMA as shown in Figure 4 and Falcon as shown in Figure 14 in the\nappendix. Here we also observe that Ghostbuster is indeed only capable of detecting ChatGPT,\n4The smallest threshold we can comprehensively evaluate to sufficient statistical significance with our com-\npute resources.\n6\nPreprint\n128\n256\n512\n0.00\n0.25\n0.50\n0.75\n1.00\nTPR at 0.01% FPR\nNews\n128\n256\n512\nCreative Writing\n128\n256\n512\nStudent Essay\nDocument Size (# of Tokens)\nBinoculars (Ours)\nGhostbuster (OOD)\nGPTZero\nDetectGPT (LLaMA-2-13B)\nFigure 3: Impact of Document Size on Detection Performance. The plot displays the TPR at\n0.01% FPR across varying document sizes. The x-axis represents the number of tokens of the ob-\nserved document, while the y-axis indicates the corresponding detection performance, highlighting\nthe Binoculars ability to detect with a low number of tokens.\nand it fails to reliably detect LLaMA generated text. The detailed ROC plots in Figure 4 compare\nperformance across thresholds for all methods.\n5\nRELIABILITY IN THE WILD\nHow well does Binoculars work when faced with scenarios encountered in the wild? The key take-\naway that we want to highlight throughout this section is that the score underlying Binoculars, i.e.\nEquation (4) is a machine-text detector. Intuitively, this means that is predicts how likely it is that\nthe given piece of text could have been generated by a similar language model. This has a num-\nber of crucial implications regarding memorized samples, text from non-native speakers, modified\nprompting strategies, and edge cases, all of which we comprehensively evaluate in this section.\n5.1\nVARIED TEXT SOURCES\nWe start our in-the-wild investigation by exploring detector performance in additional settings out-\nside of natural English language. To this end we investigate the Multi-generator, Multi-domain, and\nMulti-lingual (M4) detection datasets (Wang et al., 2023). These samples come from Arxiv, Reddit,\nWikihow, and Wikipedia sources, and include examples in varied languages, such as Urdu, Russian,\nBulgarian, and Arabic. Machine text samples in this dataset are generated via ChatGPT. In Figure 5,\n10\n3\n10\n2\n10\n1\n100\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrue Positive Rate\nPubMed\n10\n3\n10\n2\n10\n1\n100\nCC News\n10\n3\n10\n2\n10\n1\n100\nCNN\nFalse Positive Rate\nFalse Positive Rate\nFalse Positive Rate\nBinoculars (Ours)\nGhostbuster (OOD)\nGPTZero\nDetectGPT (LLaMA-2-13B)\nFigure 4: Detecting LLaMA-2-13B generations. Binoculars achieves higher TPRs for low FPRs\nthan competing baselines.\n7\nPreprint\n80%\n90%\n100%\n60%\n70%\n80%\n90%\n100%\nPrecision\nArxiv\n60%\n80%\n100%\nReddit\n25%\n50%\n75%\nWikihow\n60%\n80%\n100%\nWikipedia\nRecall\nDetector\nBinoculars (Ours)\nRoberta\nLR GLTR\nStylistic\nNela\nFigure 5: Detection of ChatGPT-generated text in various domains from M4 Dataset. Binoc-\nulars maintain high precision over 4 domains using the global threshold (tuned out-of-domain) for\ndetection. We use the mean of out-of-domain performance metrics reported by Wang et al. (2023)\nwe show the precision and recall of Binoculars and four other baselines, showing that our method\ngeneralizes across domains and languages. These baselines, released with the M4 Datasets, include\nRoBERTa (Liu et al., 2019) fine-tuned to detect LLM text (Zellers et al., 2019; Solaiman et al.,\n2019), Logistic Regression over Giant Language Model Test Room (LR GLTR) (Gehrmann et al.,\n2019) which generates features assuming predictions are sampled from a specific token distribution,\nStylistic (Li et al., 2014) which employs syntactic features at character, word, and sentence level,\nNews Landscape classifiers (NELA) (Horne et al., 2019) which generates and leverages semantic\nand structural features for veracity classification. We reprint this result from the benchmark for\nreference. Results with more source models appear in Figure 6.\n5.2\nOTHER LANGUAGES\nWhen evaluating Binoculars on samples from languages that are not well represented in Common\nCrawl data (standard LLM pretraining data), we find that false-positives rates remain low, which is\nhighly desirable from a harm reduction perspective. However, machine text in these low-resource\nlanguages is often classified as human. Figure 7 shows that we indeed have reasonable precision\nbut poor recall in these settings. While this ordering of scores is a win for harmlessness, why is\nmultilingual text detection limited?\nAs we outline above, Binoculars is a machine-text detector, detecting whether text may have been\ngenerated from a similar language model. As the Falcon models we use in our experiments are\nhighly limited in their capability of generating text in these low-resource languages, the ChatGPT-\ngenerated text is unlikely to be machine-generated, according to our score. We hypothesize that a\nstronger multilingual pair of models would lead to a version of Binoculars that could spot ChatGPT-\ngenerated text in these languages more effectively.\nFalse-Positive Rates on text written by non-native speakers. A significant concern about LLM\ndetection algorithms, as raised by Liang et al. (2023), is that LLM detectors are inadvertently biased\nagainst non-native English speakers (ESL) classifying their writing as machine-generated exceed-\nHuman\nFlant\nCohere\nDolly\nBloomz ChatGPT daVinci\n0%\n20%\n40%\n60%\n80%\n100%\nM4 Dataset\nRecall\nPrecision\nText Generator\nFigure 6: Performance of Binoculars on samples\nfrom various generative models.\n1%\n2%\nRecall\n60%\n70%\n80%\n90%\nPrecision\nArabic\nBulgarian\nRussian\nUrdu\nFigure 7: Binoculars operates at high pre-\ncision in Bulgarian and Urdu, but with low\nrecall in all four languages.\n8\nPreprint\n0.85\n0.90\n0.95\n1.00\n1.05\n1.10\nBinoculars Score\n0\n50\n100\n150\n200\n250\n300\n350\nCount\nEssay Forum\nOriginal Essay\nCorrected Essay\nBinoculars Optimal Threshold\nFigure 8: The distribution of Binoculars scores\nremains unchanged when the English grammar\nis corrected in essays composed by non-native\nspeakers. Both original and corrected essays are\nunambiguously classified as human-written.\n20%\n40%\n60%\n80%\nOpen Orca\nDefault\nSystem\nPrompt\nCarl Sagan No Robotic\nWords\nPirate\nSystem Prompt for LLaMA-2-13B-Chat\n0%\n2%\n4%\n6%\nFalse Negative Rate\nBinoculars (Ours)\nGhostbuster (OOD)\nGPTZero\nDetectGPT\n(LLaMA-2-13B)\nFigure 9: Detection of LLaMA-2-13B-chat out-\nput with modified system prompts.\ningly often. To test this, we analyze essays from EssayForum, a web page for ESL students to\nimprove their academic writing (EssayForum, 2022). This dataset contains both the original essays,\nas well as grammar-corrected versions. We compare the distribution of Binoculars scores across the\noriginal and the grammar-corrected samples. Interestingly, and in stark comparison to commercial\ndetectors examined by Liang et al. (2023) on a similar dataset, Binoculars attains equal accuracy\nat 99.67% for both corrected and uncorrected essay datasets (see Figure 8). We also point out that\nthe Binoculars score distribution on non-native English speaker\u2019s text highly overlaps with that of\ngrammar-corrected versions of the same essays, showing that detection through Binoculars is insen-\nsitive to this type of shift.\n5.3\nMEMORIZATION\nOne common feature (or bug?) of perplexity-based detection is that highly memorized examples are\nclassified as machine-generated. For example, famous quotes that appear many times in the training\ndata likely have low perplexity according to an observer model that has overfit to these strings. By\nlooking at several examples, we examine how Binoculars performs on this type of data.\nSee Table 4 in Appendix A.3 for all famous text evaluated in this study. First, we ask about the\nUS Constitution \u2013 a document that is largely memorized by modern LLMs. This example has a\nBinoculars score of 0.76, well into the machine range. Of the 11 famous texts we study, this was the\nlowest score (most machine-y). Three of the 11 fall on the machine-side of our threshold.\nIt is important to note that while this behavior may be surprising, and does require careful\nconsideration in deployment, it is fully consistent with a machine-text detector. Memorized text is\nboth text written by human writers, and text that is likely to be generated by an LLM. Classification\nof memorized text as machine generated may be acceptable or even desirable in some applications\n(e.g., plagiarism detection), or undesirable in others (e.g., removal of LLM-generated text from a\ntraining corpus).\n5.4\nMODIFIED PROMPTING STRATEGIES\nThe Open Orca dataset contains machine generations from both GPT-3 and GPT-4 for a wide range\nof tasks (Lian et al., 2023). This serves as a diverse test bed for measuring Binoculars on both of\nthese modern high-performing LLMs. Impressively, Binoculars detects 92% of GPT-3 samples and\n89.57% of GPT-4 samples when using the global threshold (from reference datasets). Note, we only\nreport accuracy since this is over a set of machine-generated text only. This dataset also provides\nprompts that allow us to explore how sensitive Binoculars is to modifying prompts.\n9\nPreprint\nTable 1: Excerpts from LLaMA-2-13B-chat generations using an Open-Orca sample prompt with varying\nsystem prompts. For the full modifications to the prompts to generate these stylized outputs, see Appendix A.5.\nPrompt\nWrite an article based on this \u201cUse an adjustable jump. Create your jump with cinder blocks and plywood. Use\nnatural outdoor obstacles as jumps. Set down a treat on the far side of the jump. Practice at the lowest level until\nyour dog is comfortable. Raise the bar gradually. Use the same method to teach your dog to jump through hoops.\u201d\nDefault\nA well-trained dog is not only a delight to be around, but it also provides a sense of pride and accomplishment to\nthe owner. One captivating and useful skill that you may want to teach your dog is jumping. Whether you\u2019re doing\nit for fun, agility competitions, or simply to keep your furry companion physically engaged, teaching your dog how\nto jump can be a bonding and rewarding experience. In this article, we will discuss various steps and techniques\nthat will guide you through the process of training your dog to confidently jump over obstacles and through hoops.\nCarl Sagan\nAh, my dear friend, we stand at the precipice of a most wondrous adventure, an odyssey of the canine kind. Our\nquest, to teach our loyal companions, those furry wonders of nature, to soar through the air with grace and agility.\nThe tool at our disposal, the adjustable jump. This marvel of ingenuity allows us to create a series of obstacles, each\none more challenging than the last, and to guide our canine companions through them with precision and patience.\nNon-Robotic\nHey there, fellow dog lovers! Are you looking for a fun and challenging way to keep your furry friend active and\nengaged? Look no further than an adjustable jump! Not only is it a great way to provide your dog with physical\nexercise, but it\u2019s also a great way to bond with your pup. Plus, it\u2019s super easy to set up and can be customized to fit\nyour dog\u2019s skill level.\nPirate\nArrrr, me hearties! Ye landlubbers be wantin\u2019 yer pooch ta learn ta jump, eh? Well, fear not! I be here ta help ye\nteach yer furry first mate ta jump over obstacles like a proper salty sea dog! First things first, ye need ta create the\njump. Now, I know what ye be thinkin\u2019: \u201dCinder blocks and plywood? That be a bit...rugged, don\u2019t ye think?\u201d But\nnever fear, me hearties! These materials be sturdy and easy ta work with. Ye can build a jump that be adjustable, so\nye can raise the bar ta suit yer pooch\u2019s skill level.\nSimple detection schemes are sometimes fooled by simple changes to prompting strategies, which\ncan produce stylized text that deviates from the standard output distribution. With this in mind,\nwe use LLaMA-2-13B-chat and prompts designed to tweak the style of the output. Specifically, we\nprompt LLaMA2-13B-chat with three different system prompts by appending to the standard system\nprompt a request to write in Carl Sagan\u2019s voice or without any mechanical or robotic sounding words\nor like a pirate.\nIn general, we find that these stylistic changes do not significantly impact the accuracy of Binocu-\nlars. The biggest impact we observe arises when asking for pirate-sounding output, and this only\ndecreases the sensitivity (increases the false negative rate) by 1%; see Figure 9. Table 1 records\ngenerations based on a sample prompt employing the specified prompting strategies.\n5.5\nRANDOMIZED DATA\n0.0\n0.9\n0\n50\n100\n150\n200\nCount\n1.30\n1.32\n1.34\n1.36\n1.38\n1.40\n1.42\nBinoculars Score\nRandom Tokens\nBinoculars Optimal Threshold\nFigure 10: Random token sequences fall strictly on\nthe human side of the Binoculars threshold.\nNext, we also want to test whether arbitrary\nmistakes, hashcodes, or other kinds of ran-\ndom (and random seeming) strings bias our\nmodel towards false positives. To test the im-\npact of randomness, we generate random se-\nquences of tokens from the Falcon tokenizer,\nand score them with Binoculars as usual. We\nplot a histogram of this distribution in Fig-\nure 10. We find that Binoculars confidently\nscores this as human, with a mean score\naround 1.35 for Falcon (humans have a mean\nof around 1).\nThis is expected, as trained\nLLMs are strong models of language and\nexceedingly unlikely to ever generate these\ncompletely random sequences of tokens in any situation. In particular, the generation of these ran-\ndom sequences is even less likely than the generation of perfect human-written text by chance.\n6\nDISCUSSION AND LIMITATIONS\nWe present Binoculars, a method for detecting LLMs in the zero-shot case in which no data is\navailable from the model being detected. Our transferable detector works in the zero-shot setting,\nwithout access to the particular model used for generation or example data from it. We speculate\n10\nPreprint\nthat this transferability arises from the similarity between modern LLMs, as they all use nearly\nidentical transformer components and are likely trained on datasets comprising mostly Common\nCrawl (commoncrawl.org) data from similar time periods. As the number of open source LLMs\nrapidly increases, the ability to detect multiple LLMs with a single detector is a major advantage of\nBinoculars when used in practice, for example for platform moderation.\nOur study has a number of limitations. Due to limited GPU memory, we do not perform broader\nstudies with larger (30B+) open-source models in our detector. Further, we focus on the problem\nsetting of detecting machine-generated text in normal use, and we do not consider explicit efforts\nto bypass detection. Finally, there are other non-conversational text domains, such as source code,\nwhich we have not investigated in this study.\nREPRODUCIBILITY STATEMENT\nWe provide details on all datasets used, and on the exact method that we employ in the main body\nof this work. Additionally, we provide code to exactly replicate our detection score implementation\nwith the supplementary material of this work. We note that comparisons to commercial detection\nAPIs, such as GPTZero are based on API evaluations from September 2023, and may not be repro-\nducible in the future, underscoring the importance of transparent, open-source solutions.\nETHICS STATEMENT\nLanguage model detection may be a key technology to reduce harm, whether to monitor machine-\ngenerated text on internet platforms and social media, filter training data, or identify responses in\nchat applications. Nevertheless, care has to be taken so that detection mechanisms actually reduce\nharm, instead of proliferating or increasing it. We provide an extensive reliability investigation of\nthe proposed Binoculars mechanisms in Section 5, and believe that this is a significant step forward\nin terms of reliability, for example when considering domains such as text written by non-native\nspeakers. Yet, we note that this analysis is only a first step in the process of deploying LLM detection\nstrategies and does not absolve developers of such applications from carefully verifying the impact\non their systems. We especially caution that the existence of LLM detectors does not imply that\nusing them is worthwhile in all scenarios.\nAlso, we explicitly highlight that we consider the task of detecting \u201cnaturally\u201d occurring machine-\ngenerated text, as generated by LLMs in common use cases. We understand that no detector is\nperfect and we do not guarantee any performance in settings where a motivated adversary tries to\nfool our system. We present a thorough evaluation across a wide variety of test sources, but we\nmaintain that directed attempts to bypass our classifier might be possible, as is often the case for\nsystems that rely on neural networks.\nACKNOWLEDGMENTS\nThis work was made possible by the ONR MURI program and the AFOSR MURI program.\nCommercial support was provided by Capital One Bank, the Amazon Research Award program,\nand Open Philanthropy. Further support was provided by the National Science Foundation (IIS-\n2212182), and by the NSF TRAILS Institute (2229885).\nREFERENCES\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Co-\njocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic,\nBadreddine Noune, Baptiste Pannier, and Guilherme Penedo. Falcon-40B: An open large lan-\nguage model with state-of-the-art performance, 2023.\nURL https://falconllm.tii.\nae/.\nChristopher Bail, Lisa Pinheiro, and Jimmy Royer. Difficulty Of Detecting AI Content Poses Legal\nChallenges. Law360, April 2023.\n11\nPreprint\nEmily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the\nDangers of Stochastic Parrots: Can Language Models Be Too Big?\nIn Proceedings of the\n2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT \u201921, pp. 610\u2013623,\nNew York, NY, USA, March 2021. Association for Computing Machinery. ISBN 978-1-4503-\n8309-7. doi: 10.1145/3442188.3445922. URL https://doi.org/10.1145/3442188.\n3445922.\nMeghana Moorthy Bhat and Srinivasan Parthasarathy.\nHow Effectively Can Machines Defend\nAgainst Machine-Generated Fake News?\nAn Empirical Study.\nIn Proceedings of the First\nWorkshop on Insights from Negative Results in NLP, pp. 48\u201353, Online, November 2020. As-\nsociation for Computational Linguistics. doi: 10.18653/v1/2020.insights-1.7. URL https:\n//aclanthology.org/2020.insights-1.7.\nStella Biderman and Edward Raff.\nFooling MOSS Detection with Pretrained Language Mod-\nels. In Proceedings of the 31st ACM International Conference on Information & Knowledge\nManagement, CIKM \u201922, pp. 2933\u20132943, New York, NY, USA, October 2022. Association\nfor Computing Machinery. ISBN 978-1-4503-9236-5. doi: 10.1145/3511808.3557079. URL\nhttps://dl.acm.org/doi/10.1145/3511808.3557079.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-\nwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,\nDaniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner,\nSam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.\nLanguage Models\nare Few-Shot Learners.\nIn 34th Conference on Neural Information Processing Systems\n(NeurIPS 2020), December 2020.\nURL https://papers.nips.cc/paper/2020/\nhash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\nSouradip Chakraborty, Amrit Singh Bedi, Sicheng Zhu, Bang An, Dinesh Manocha, and Furong\nHuang. On the Possibilities of AI-Generated Text Detection. arxiv:2304.04736[cs], April 2023.\ndoi: 10.48550/arXiv.2304.04736. URL http://arxiv.org/abs/2304.04736.\nCharlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and\nJohn Jumper.\nAccelerating Large Language Model Decoding with Speculative Sampling.\narxiv:2302.01318[cs], February 2023.\ndoi:\n10.48550/arXiv.2302.01318.\nURL http://\narxiv.org/abs/2302.01318.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,\nKensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam\nShazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James\nBradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Lev-\nskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin\nRobinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret\nZoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick,\nAndrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica\nMoreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-\nnan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas\nEck, Jeff Dean, Slav Petrov, and Noah Fiedel. PaLM: Scaling Language Modeling with Pathways.\narXiv:2204.02311 [cs], April 2022. URL http://arxiv.org/abs/2204.02311.\nEvan Crothers, Nathalie Japkowicz, and Herna Viktor. Machine Generated Text: A Comprehensive\nSurvey of Threat Models and Detection Methods. arxiv:2210.07321[cs], November 2022. doi:\n10.48550/arXiv.2210.07321. URL http://arxiv.org/abs/2210.07321.\nMahdi Dhaini, Wessel Poelman, and Ege Erdogan. Detecting ChatGPT: A Survey of the State of\nDetecting ChatGPT-Generated Text. arxiv:2309.07689[cs], September 2023. doi: 10.48550/\narXiv.2309.07689. URL http://arxiv.org/abs/2309.07689.\nEssayForum.\nNid989/EssayFroum-Dataset \u00b7 Datasets at Hugging Face, September 2022.\nURL\nhttps://huggingface.co/datasets/nid989/EssayFroum-Dataset.\n12\nPreprint\nSebastian Gehrmann, Hendrik Strobelt, and Alexander Rush. GLTR: Statistical detection and visu-\nalization of generated text. In Marta R. Costa-juss`a and Enrique Alfonseca (eds.), Proceedings of\nthe 57th Annual Meeting of the Association for Computational Linguistics: System Demonstra-\ntions, pp. 111\u2013116, Florence, Italy, July 2019. Association for Computational Linguistics. doi:\n10.18653/v1/P19-3019. URL https://aclanthology.org/P19-3019.\nSoumya Suvra Ghosal, Souradip Chakraborty, Jonas Geiping, Furong Huang, Dinesh Manocha,\nand Amrit Singh Bedi. Towards possibilities & impossibilities of ai-generated text detection: A\nsurvey. arXiv preprint arXiv:2310.15264, 2023.\nBiyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and\nYupeng Wu. How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and\nDetection. arxiv:2301.07597[cs], January 2023. doi: 10.48550/arXiv.2301.07597. URL http:\n//arxiv.org/abs/2301.07597.\nFelix Hamborg, Norman Meuschke, Corinna Breitinger, and Bela Gipp. news-please: A generic\nnews crawler and extractor. In Proceedings of the 15th International Symposium of Information\nScience, pp. 218\u2013223, March 2017. doi: 10.5281/zenodo.4120316.\nHayden Helm, Carey E. Priebe, and Weiwei Yang.\nA Statistical Turing Test for Generative\nModels.\narxiv:2309.08913[cs], September 2023.\ndoi: 10.48550/arXiv.2309.08913.\nURL\nhttp://arxiv.org/abs/2309.08913.\nKarl Moritz Hermann, Tom\u00b4a\u02c7s Ko\u02c7cisk\u00b4y, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa\nSuleyman, and Phil Blunsom. Teaching machines to read and comprehend. In Proceedings of the\n28th International Conference on Neural Information Processing Systems - Volume 1, NIPS\u201915,\npp. 1693\u20131701, Cambridge, MA, USA, December 2015. MIT Press.\nBenjamin D. Horne, Jeppe N\u00f8rregaard, and Sibel Adali. Robust fake news detection over time and\nattack. ACM Trans. Intell. Syst. Technol., 11(1), dec 2019. ISSN 2157-6904. doi: 10.1145/\n3363818. URL https://doi.org/10.1145/3363818.\nXiaomeng Hu, Pin-Yu Chen, and Tsung-Yi Ho. RADAR: Robust AI-Text Detection via Adversarial\nLearning. arxiv:2307.03838[cs], July 2023. doi: 10.48550/arXiv.2307.03838. URL http:\n//arxiv.org/abs/2307.03838.\nJohn Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. A\nWatermark for Large Language Models. In Proceedings of the 40th International Conference\non Machine Learning, pp. 17061\u201317084. PMLR, July 2023. URL https://proceedings.\nmlr.press/v202/kirchenbauer23a.html.\nKalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, and Mohit Iyyer. Paraphrasing\nevades detectors of AI-generated text, but retrieval is an effective defense. arxiv:2303.13408[cs],\nMarch 2023.\ndoi: 10.48550/arXiv.2303.13408.\nURL http://arxiv.org/abs/2303.\n13408.\nYaniv Leviathan, Matan Kalman, and Yossi Matias. Fast Inference from Transformers via Spec-\nulative Decoding. In Proceedings of the 40th International Conference on Machine Learning,\npp. 19274\u201319286. PMLR, July 2023. URL https://proceedings.mlr.press/v202/\nleviathan23a.html.\nJenny S. Li, John V. Monaco, Li-Chiou Chen, and Charles C. Tappert. Authorship authentication\nusing short messages from social networking sites. In 2014 IEEE 11th International Conference\non e-Business Engineering, pp. 314\u2013319, 2014. doi: 10.1109/ICEBE.2014.61.\nXiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke\nZettlemoyer, and Mike Lewis.\nContrastive Decoding: Open-ended Text Generation as Opti-\nmization.\nIn Proceedings of the 61st Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pp. 12286\u201312312, Toronto, Canada, July 2023. Asso-\nciation for Computational Linguistics.\ndoi: 10.18653/v1/2023.acl-long.687.\nURL https:\n//aclanthology.org/2023.acl-long.687.\n13\nPreprint\nWing Lian, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and \u201dTeknium\u201d.\nOpenorca: An open dataset of gpt augmented flan reasoning traces.\nhttps://https://\nhuggingface.co/Open-Orca/OpenOrca, 2023.\nWeixin Liang, Mert Yuksekgonul, Yining Mao, Eric Wu, and James Zou. GPT detectors are biased\nagainst non-native English writers. arxiv:2304.02819[cs], April 2023. doi: 10.48550/arXiv.2304.\n02819. URL http://arxiv.org/abs/2304.02819.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach, 2019.\nVijini Liyanage and Davide Buscaldi. Detecting Artificially Generated Academic Text: The Im-\nportance of Mimicking Human Utilization of Large Language Models.\nIn Elisabeth M\u00b4etais,\nFarid Meziane, Vijayan Sugumaran, Warren Manning, and Stephan Reiff-Marganiec (eds.),\nNatural Language Processing and Information Systems, Lecture Notes in Computer Science,\npp. 558\u2013565, Cham, 2023. Springer Nature Switzerland.\nISBN 978-3-031-35320-8.\ndoi:\n10.1007/978-3-031-35320-8 42.\nYisroel Mirsky, Ambra Demontis, Jaidip Kotak, Ram Shankar, Deng Gelei, Liu Yang, Xiangyu\nZhang, Maura Pintor, Wenke Lee, Yuval Elovici, and Battista Biggio.\nThe Threat of Offen-\nsive AI to Organizations. Computers & Security, 124:103006, January 2023. ISSN 0167-4048.\ndoi: 10.1016/j.cose.2022.103006. URL https://www.sciencedirect.com/science/\narticle/pii/S0167404822003984.\nEric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D. Manning, and Chelsea Finn. De-\ntectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature. In Proceed-\nings of the 40th International Conference on Machine Learning, pp. 24950\u201324962. PMLR, July\n2023. URL https://proceedings.mlr.press/v202/mitchell23a.html.\nJiashu Pu, Ziyi Huang, Yadong Xi, Guandan Chen, Weijie Chen, and Rongsheng Zhang. Unraveling\nthe Mystery of Artifacts in Machine Generated Text. In Proceedings of the Thirteenth Language\nResources and Evaluation Conference, pp. 6889\u20136898, Marseille, France, June 2022. European\nLanguage Resources Association. URL https://aclanthology.org/2022.lrec-1.\n744.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nModels are Unsupervised Multitask Learners. OpenAI, pp. 24, 2019.\nVinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and Soheil Feizi.\nCan AI-Generated Text be Reliably Detected?\narxiv:2303.11156[cs], March 2023. doi: 10.\n48550/arXiv.2303.11156. URL http://arxiv.org/abs/2303.11156.\nPrithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-\nRad.\nCollective Classification in Network Data.\nAI Magazine, 29(3):93\u201393, September\n2008. ISSN 2371-9621. doi: 10.1609/aimag.v29i3.2157. URL https://ojs.aaai.org/\naimagazine/index.php/aimagazine/article/view/2157.\nIrene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec\nRadford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, Miles McCain, Alex Newhouse, Ja-\nson Blazakis, Kris McGuffie, and Jasmine Wang. Release Strategies and the Social Impacts of\nLanguage Models. arxiv:1908.09203[cs], November 2019. doi: 10.48550/arXiv.1908.09203.\nURL http://arxiv.org/abs/1908.09203.\nJinyan Su, Terry Yue Zhuo, Di Wang, and Preslav Nakov. DetectLLM: Leveraging Log Rank Infor-\nmation for Zero-Shot Detection of Machine-Generated Text. arxiv:2306.05540[cs], May 2023.\ndoi: 10.48550/arXiv.2306.05540. URL http://arxiv.org/abs/2306.05540.\nRuixiang Tang, Yu-Neng Chuang, and Xia Hu. The Science of Detecting LLM-Generated Texts.\narxiv:2303.07205[cs], March 2023. doi: 10.48550/arXiv.2303.07205. URL http://arxiv.\norg/abs/2303.07205.\n14\nPreprint\nEdward Tian. Gptzero update v1, January 2023a. URL https://gptzero.substack.com/\np/gptzero-update-v1.\nEdward Tian. New year, new features, new model, January 2023b. URL https://gptzero.\nsubstack.com/p/new-year-new-features-new-model.\nYuchuan Tian, Hanting Chen, Xutao Wang, Zheyuan Bai, Qinghua Zhang, Ruifeng Li, Chao\nXu, and Yunhe Wang.\nMultiscale Positive-Unlabeled Detection of AI-Generated Texts.\narxiv:2305.18149[cs], June 2023. doi: 10.48550/arXiv.2305.18149. URL http://arxiv.\norg/abs/2305.18149.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher,\nCristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy\nFu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,\nSaghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel\nKloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya\nLee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar\nMishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan\nSaladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen\nTan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan\nZarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez,\nRobert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open Foundation and Fine-\nTuned Chat Models. arxiv:2307.09288[cs], July 2023. doi: 10.48550/arXiv.2307.09288. URL\nhttp://arxiv.org/abs/2307.09288.\nEduard Tulchinskii, Kristian Kuznetsov, Laida Kushnareva, Daniil Cherniavskii, Serguei Baran-\nnikov, Irina Piontkovskaya, Sergey Nikolenko, and Evgeny Burnaev. Intrinsic Dimension Es-\ntimation for Robust Detection of AI-Generated Texts. arxiv:2306.04723[cs], June 2023. doi:\n10.48550/arXiv.2306.04723. URL http://arxiv.org/abs/2306.04723.\nTurnitIn.com. URL https://www.turnitin.com/.\nLav R. Varshney, Nitish Shirish Keskar, and Richard Socher. Limits of Detecting Text Generated by\nLarge-Scale Language Models. In 2020 Information Theory and Applications Workshop (ITA),\npp. 1\u20135, February 2020. doi: 10.1109/ITA50056.2020.9245012.\nChristoforos Vasilatos, Manaar Alam, Talal Rahwan, Yasir Zaki, and Michail Maniatakos.\nHowkGPT: Investigating the Detection of ChatGPT-generated University Student Homework\nthrough Context-Aware Perplexity Analysis. arxiv:2305.18226[cs], June 2023. doi: 10.48550/\narXiv.2305.18226. URL http://arxiv.org/abs/2305.18226.\nVivek Verma, Eve Fleisig, Nicholas Tomlin, and Dan Klein. Ghostbuster: Detecting Text Ghost-\nwritten by Large Language Models. arxiv:2305.15047[cs], May 2023. doi: 10.48550/arXiv.2305.\n15047. URL http://arxiv.org/abs/2305.15047.\nYuxia Wang, Jonibek Mansurov, Petar Ivanov, Jinyan Su, Artem Shelmanov, Akim Tsvigun,\nChenxi Whitehouse, Osama Mohammed Afzal, Tarek Mahmoud, Alham Fikri Aji, and Preslav\nNakov. M4: Multi-generator, Multi-domain, and Multi-lingual Black-Box Machine-Generated\nText Detection.\narxiv:2305.14902[cs], May 2023.\ndoi: 10.48550/arXiv.2305.14902.\nURL\nhttp://arxiv.org/abs/2305.14902.\nMax Wolff and Stuart Wolff. Attacking Neural Text Detectors. arxiv:2002.11768[cs], January 2022.\ndoi: 10.48550/arXiv.2002.11768. URL http://arxiv.org/abs/2002.11768.\nXianjun Yang, Wei Cheng, Linda Petzold, William Yang Wang, and Haifeng Chen.\nDNA-\nGPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text.\narxiv:2305.17359[cs], May 2023. doi: 10.48550/arXiv.2305.17359. URL http://arxiv.\norg/abs/2305.17359.\nXiao Yu, Yuang Qi, Kejiang Chen, Guoqiang Chen, Xi Yang, Pengyuan Zhu, Weiming Zhang, and\nNenghai Yu. GPT Paternity Test: GPT Generated Text Detection with GPT Genetic Inheritance.\narxiv:2305.12519[cs], May 2023. doi: 10.48550/arXiv.2305.12519. URL http://arxiv.\norg/abs/2305.12519.\n15\nPreprint\nRowan Zellers,\nAri Holtzman,\nHannah Rashkin,\nYonatan Bisk,\nAli Farhadi,\nFranziska\nRoesner,\nand Yejin Choi.\nDefending Against Neural Fake News.\nIn Advances\nin\nNeural\nInformation\nProcessing\nSystems,\nvolume\n32.\nCurran\nAssociates,\nInc.,\n2019.\nURL\nhttps://proceedings.neurips.cc/paper/2019/hash/\n3e9f0fc9b2f89e043bc6233994dfcf76-Abstract.html.\nHaolan Zhan, Xuanli He, Qiongkai Xu, Yuxiang Wu, and Pontus Stenetorp. G3Detector: General\nGPT-Generated Text Detector. arxiv:2305.12680[cs], May 2023. doi: 10.48550/arXiv.2305.\n12680. URL http://arxiv.org/abs/2305.12680.\nA\nAPPENDIX\nA.1\nBENCHMARK PERFORMANCE\nChatGPT Text Detection. F1-scores on ChatGPT dataset released by (Verma et al., 2023). The\nnumbers for Zero-Shot baseline method are taken from the same work.\nNews\nCreative\nWriting\nStudent\nEssay\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nF1-Score\n0.994\n0.985\n0.993\n0.984\n0.978\n0.977\n0.999\n0.975\n0.974\n0.714\n0.720\n0.702\n0.441\n0.288\n0.534\nBinoculars (Ours)\nGhostbuster (OOD)\nGPTZero\nDetectGPT (LLaMA-2-13B)\nZero-Shot\nFigure 11: F1 scores for detection of ChatGPT-generated text indicate that several detectors perform\nsimilarly. We discuss below how this metric can be a poor indicator of performance at low FPR.\nA.2\nABLATION STUDIES\nComparison to Other Scoring Model Pairs.\nString Length. Is there a correlation between Binoculars score and sequence length? Such correla-\ntions may create a bias towards incorrect results for certain lengths. In Figure 12, we show the joint\ndistribution of token sequence length and Binocular score. Sequence length offers little information\nabout class membership.\nScore Components. Perplexity is used by many detecting formulations in isolation. We show in\nFigure 13 that both perplexity and cross-perplexity are not effective detectors in isolation. Table 3\nshow the results where we compute PPL and X-PPL with different model families viz. LLaMA-2\nand Falcon.\nA.3\nOTHER FAMOUS TEXTS\nTwo songs by Bob Dylan further demonstrate this behavior. Blowin\u2019 In The Wind, a famous Dylan\ntrack has a much lower Falcon perplexity than his unreleased song To Fall In Love With You (logPPL\nvalues are 1.11 and 3.30, respectively.) It might be reasonable for famous songs to get classified as\nmachine text and they are more likely output than less famous songs. Binoculars, however, labels\nboth of these samples confidently as human samples (with scores of 0.92, and 1.01, respectively).\nA.4\nIDENTICAL SCORING MODEL\nWe inspect Binocular\u2019s performance when we choose to use identical M1 and M1 models in equa-\ntion (4). We use Falcon-7B and Falcon-7B-Instruct models and compare the two performances with\n16\nPreprint\nFirefox\n\ufb01le:///Users/avi/Downloads/LLM%20Detection/\ufb01gures/do\nFigure 12: A closer look at the actual distribution of scores in terms of sequence length for the\nGhostbuster news dataset.\n17\nPreprint\nTable 2: Other combinations of scoring models, evaluated on our reference datasets as described in\nthe main body.\nPPL Scorer (M1)\nX-Cross PPL Scorers (M\u2032\n1, M2)\nTPR\nat\n0.01% FPR\nTPR\nat\n0.1% FPR\nF1-Score\nAUC\nFalcon-7B-Instruct\nFalcon-7B, Falcon-7B-Instruct\n100.0000\n100.0000\n1.0000\n1.0000\nLlama-2-13B\nLlama-13B, Llama-2-13B\n99.6539\n99.6539\n0.9982\n0.9999\nLlama-2-7B\nLlama-7B, Llama-2-7B\n99.3079\n99.3079\n0.9965\n0.9998\nLlama-2-13B\nLlama-13B, Llama-2-13B\n98.3549\n98.3549\n0.9913\n0.9997\nFalcon-7B-Instruct\nFalcon-7B, Falcon-7B-Instruct\n98.7200\n99.1600\n0.9953\n0.9996\nFalcon-7B-Instruct\nFalcon-7B, Falcon-7B-Instruct\n94.9200\n99.4000\n0.9963\n0.9996\nLlama-2-7B\nLlama-7B, Llama-2-7B\n95.8441\n97.5757\n0.9922\n0.9996\nLlama-2-13B\nLlama-13B, Llama-2-13B\n98.6400\n99.0400\n0.9953\n0.9995\nLlama-2-7B\nLlama-7B, Llama-2-7B\n98.8000\n99.2800\n0.9959\n0.9995\nLlama-2-7B\nLlama-7B, Llama-2-7B\n98.1600\n98.6000\n0.9937\n0.9992\nLlama-2-13B\nLlama-13B, Llama-2-13B\n98.4000\n98.7200\n0.9943\n0.9992\nFalcon-7B-Instruct\nFalcon-7B, Falcon-7B-Instruct\n94.1125\n97.9220\n0.9926\n0.9992\nFalcon-7B-Instruct\nFalcon-7B, Falcon-7B-Instruct\n93.5000\n93.5000\n0.9875\n0.9990\nFalcon-7B-Instruct\nFalcon-7B, Falcon-7B-Instruct\n92.0000\n92.0000\n0.9918\n0.9990\nLlama-2-7B\nLlama-7B, Llama-2-7B\n94.0000\n94.0000\n0.9850\n0.9989\nLlama-2-7B\nLlama-7B, Llama-2-7B\n98.0000\n98.0000\n0.9956\n0.9988\nFalcon-7B-Instruct\nFalcon-7B, Falcon-7B-Instruct\n72.6957\n72.7857\n0.9908\n0.9988\nLlama-2-13B\nLlama-13B, Llama-2-13B\n97.8750\n97.8750\n0.9931\n0.9987\nLlama-2-13B-Chat\nLlama-2-13B, Llama-2-13B-Chat\n71.3199\n82.6799\n0.9846\n0.9986\nLlama-2-13B\nLlama-13B, Llama-2-13B\n97.5000\n97.5000\n0.9875\n0.9985\nFalcon-7B-Instruct\nFalcon-7B, Falcon-7B-Instruct\n97.5778\n97.5778\n0.9930\n0.9983\nFalcon-7B-Instruct\nFalcon-7B, Falcon-7B-Instruct\n23.3076\n48.3732\n0.9842\n0.9975\nLlama-2-13B\nLlama-13B, Llama-2-13B\n0.3200\n32.0800\n0.9840\n0.9968\nLlama-2-13B-Chat\nLlama-2-13B, Llama-2-13B-Chat\n20.9172\n60.0671\n0.9763\n0.9968\nLlama-2-13B\nLlama-13B, Llama-2-13B\n47.1476\n69.2953\n0.9747\n0.9964\nFigure 13: Perplexity and Cross-perplexity are not strong detectors on their own.\nBinoculars Score over dataset by (Verma et al., 2023) in Figure 15. We observe although the vanilla\nBinoculars score is best over 3 domains, using Falcon-7B as input models is competitive.\nA.5\nMODIFIED SYSTEM PROMPTS\nWe test Binoculars\u2019 and comparable baselines\u2019 performances in Section 5.4 on multiple prompting\nstrategies. We prompt LLaMA-2-13B-chat with samples from the Open-Orca dataset. In addition to\nthe default sample-specific prompt, we use 3 different versions in which we append instructions into\nthe system prompt. These include instruction to write in the style of Carl Sagan, in a non-robotic\n18\nPreprint\nTable 3: Over various datasets, we show that perplexity alone or cross-perplexity alone are poor\npredictors of human versus machine, whereas Binoculars perform well even at low false-positive\nrates (FPR).\nTrue Positive Rate\nDataset\nDetector\nAUC\n@ 0.01% FPR\n@ 0.1% FPR\n@ 1% FPR\n@ 5% FPR\nFalcon PPL\n1.00\n0.86\n0.86\n0.94\n0.98\nFalcon X-PPL\n0.94\n0.56\n0.56\n0.59\n0.79\nWriting\nLLaMA PPL\n0.99\n0.86\n0.86\n0.92\n0.98\nPrompts\nLLaMA X-PPL\n0.86\n0.04\n0.04\n0.10\n0.43\nBinoculars-Falcon\n1.00\n0.93\n0.93\n0.96\n1.00\nBinoculars-LLaMA\n1.00\n0.95\n0.95\n0.98\n1.00\nFalcon PPL\n0.99\n0.65\n0.77\n0.90\n0.95\nFalcon X-PPL\n0.85\n0.04\n0.12\n0.29\n0.53\nNews\nLLaMA PPL\n0.98\n0.67\n0.71\n0.89\n0.95\nLLaMA X-PPL\n0.26\n0.00\n0.00\n0.00\n0.01\nBinoculars-Falcon\n1.00\n0.95\n0.99\n1.00\n1.00\nBinoculars-LLaMA\n1.00\n0.99\n0.99\n1.00\n1.00\nFalcon PPL\n1.00\n0.78\n0.78\n0.88\n0.99\nFalcon X-PPL\n0.93\n0.25\n0.25\n0.38\n0.70\nEssay\nLLaMA PPL\n0.99\n0.42\n0.42\n0.90\n0.98\nLLaMA X-PPL\n0.80\n0.01\n0.01\n0.04\n0.16\nBinoculars-Falcon\n1.00\n0.98\n0.98\n0.99\n1.00\nBinoculars-LLaMA\n1.00\n0.99\n0.99\n1.00\n1.00\nFigure 14: Comparison of Ghostbuster and Binoculars AUC on PubMed, CCNews and CNN\ndatasets.\ntone, and like a pirate. In Table 5 we mention the exact instruction appended to the default system\nprompts.\n19\nPreprint\nTable 4: Case Studies of Text Samples likely to be memorized by LLMs.\nHuman Sample\nPPL (Falcon 7B In-\nstruct)\nCross PPL (Falcon\n7B, Falcon 7B In-\nstruct)\nBinoculars Score\nPredicted\nas\nHuman-\nWritten\nUS Constitution\n0.6680\n0.8789\n0.7600\np\n\u201cI have a dream speech\u201d\n1.0000\n1.2344\n0.8101\np\nSnippet from Cosmos series\n2.3906\n2.8281\n0.8453\np\nBlowin\u2019 In the Wind (song)\n1.1172\n1.2188\n0.9167\n\u2713\nOscar Wilde\u2019s quote\n2.9219\n3.0781\n0.9492\n\u2713\nSnippet from White Night\n2.6875\n2.8125\n0.9556\n\u2713\nWish You Were Here\n2.5000\n2.5938\n0.9639\n\u2713\nSnippet from Harry Potter book\n2.5938\n2.6875\n0.9651\n\u2713\nFirst chapter of A Tale of Two Cities\n2.7188\n2.7500\n0.9886\n\u2713\nSnippet from Crime and Punishment\n2.8750\n2.9063\n0.9892\n\u2713\nTo Fall In Love With You (song)\n3.2969\n3.2656\n1.0096\n\u2713\n10\n3\n10\n2\n10\n1\n100\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrue Positive Rate\nNews\nFalcon-7B (AUC= 0.9988)\nFalcon-Instruct-7B (AUC= 0.9937)\nBinoculars (AUC= 0.9996)\n10\n2\n10\n1\n100\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCreative Writing\nFalcon-7B (AUC= 0.9860)\nFalcon-Instruct-7B (AUC= 0.8921)\nBinoculars (AUC= 0.9990)\n10\n2\n10\n1\n100\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nStudent Essay\nFalcon-7B (AUC= 0.9884)\nFalcon-Instruct-7B (AUC= 0.7953)\nBinoculars (AUC= 0.9983)\nFalse Positive Rate\nFigure 15: AUC Curve Binoculars score using identical M1 and M2 models using Falcon-7B and\nFalcon-7B-Instruct.\nTable 5: Instructions appended in system prompts for 3 different strategies.\nPrompting Strategy\nInstruction appended to the default system prompt\nCarl Sagan\nWrite in the voice of Carl Sagan.\nNon-Robotic\nWrite your response in a way that doesn\u2019t sound pretentious or overly formal.\nDon\u2019t use robotic-sounding words like \u2018logical\u2019 and \u2018execute.\u2019 Write in the casual\nstyle of a normal person.\nPirate\nWrite in the voice of a pirate.\n20\n"
  },
  {
    "title": "Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs",
    "link": "https://arxiv.org/pdf/2401.11708.pdf",
    "upvote": "26",
    "text": "Mastering Text-to-Image Diffusion:\nRecaptioning, Planning, and Generating with Multimodal LLMs\nLing Yang * 1 Zhaochen Yu * 1 Chenlin Meng 2 3 Minkai Xu 2 Stefano Ermon 2 Bin Cui 1\nhttps://github.com/YangLing0818/RPG-DiffusionMaster\nAbstract\nDiffusion models have exhibit exceptional per-\nformance in text-to-image generation and edit-\ning. However, existing methods often face chal-\nlenges when handling complex text prompts that\ninvolve multiple objects with multiple attributes\nand relationships.\nIn this paper, we propose\na brand new training-free text-to-image genera-\ntion/editing framework, namely Recaption, Plan\nand Generate (RPG), harnessing the powerful\nchain-of-thought reasoning ability of multimodal\nLLMs to enhance the compositionality of text-to-\nimage diffusion models. Our approach employs\nthe MLLM as a global planner to decompose the\nprocess of generating complex images into mul-\ntiple simpler generation tasks within subregions.\nWe propose complementary regional diffusion to\nenable region-wise compositional generation. Fur-\nthermore, we integrate text-guided image gener-\nation and editing within the proposed RPG in\na closed-loop fashion, thereby enhancing gener-\nalization ability. Extensive experiments demon-\nstrate our RPG outperforms state-of-the-art text-\nto-image diffusion models, including DALL-E\n3 and SDXL, particularly in multi-category ob-\nject composition and text-image semantic align-\nment.\nNotably, our RPG framework exhibits\nwide compatibility with various MLLM archi-\ntectures (e.g., MiniGPT-4) and diffusion back-\nbones (e.g., ControlNet).\nOur code is avail-\nable at https://github.com/YangLing0818/RPG-\nDiffusionMaster\n1. Introduction\nRecent advancements in diffusion models (Sohl-Dickstein\net al., 2015; Dhariwal & Nichol, 2021; Song et al., 2020;\n*Equal contribution 1Peking University, China 2Stanford Uni-\nversity, USA 3Pika Labs, USA. Correspondence to: Ling Yang\n<yangling0818@163.com>.\nPreprint.\nText \nPrompt\nText \nPrompt\nLLM \nText \nPrompt\nMLLM \nRecaptioning\nMLLM \nCoT Planning\nSet of Subprompts\nAssgin Subprompts\nto Subregions\nConditioning\nLayout\nText \nPrompt\nLayout/Attention Mask\nComplementary Regional Diffusion\nOptional Self-Refinement\nGenerated \nImage\nDiffusion Models\nDiffusion Models\nDiffusion Models\nRPG (Ours)\na.\nb.\nc.\nd.\nFigure 1. Architecture comparison between (a) text-conditional\ndiffusion models (Ramesh et al., 2022), (b) layout/attention-based\ndiffusion models (Feng et al., 2022; Cao et al., 2023), (c) LLM-\ngrounded diffusion models (Lian et al., 2023) and (d) our RPG.\nYang et al., 2023c) have significantly improve the synthesis\nresults of text-to-image models, such as Imagen (Saharia\net al., 2022), DALL-E 2/3 (Ramesh et al., 2022; Betker et al.,\n2023) and SDXL (Podell et al., 2023). However, despite\ntheir remarkable capabilities in synthesizing realistic images\nconsistent with text prompts, most diffusion models usually\nstruggle to accurately follow some complex prompts (Feng\net al., 2022; Lian et al., 2023; Liu et al., 2022; Bar-Tal et al.,\n2023), which require the model to compose objects with\ndifferent attributes and relationships into a single image\n(Huang et al., 2023a).\nSome works begin to solve this problem by introducing\nadditional layouts/boxes (Li et al., 2023b; Xie et al., 2023;\nYang et al., 2023e; Qu et al., 2023; Chen et al., 2024; Wu\net al., 2023b; Lian et al., 2023) as conditions or leveraging\nprompt-aware attention guidance (Feng et al., 2022; Chefer\net al., 2023; Wang et al., 2023) to improve compositional\ntext-to-image synthesis. For example, StructureDiffusion\n(Feng et al., 2022) incorporates linguistic structures into the\nguided generation process by manipulating cross-attention\nmaps in diffusion models. GLIGEN (Li et al., 2023b) de-\nsigns trainable gated self-attention layers to incorporate\nspatial inputs, such as bounding boxes, while freezing the\nweights of original diffusion model.\n1\narXiv:2401.11708v2  [cs.CV]  6 Feb 2024\nMastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs\nPrompt: A beautiful landscape with a river in the middle, the left of the river is in the evening and in the winter with a big iceberg and a\nsmall village while some people are skiing on the river and some people are skating, the right of the river is in the summer with a volcano\nin the morning and a small village while some people are playing.\nSDXL\nDALL-E 3\nRPG (Ours)\nLeft Prompt: A Chinese general wearing a crown, with whiskers and golden Chinese style armor, standing with a majestic dragon head on\nhis chest, symbolizing his strength, wearing black and gold boots. His appearance exudes a sense of authority, wisdom, and an unyielding\nspirit , embodying the ideal ancient Chinese hero.\nRight Prompt: This painting is a quintessential example of ancient Chinese ink art , At the top of the painting , towering mountains shrouded\nin mist rise majestically. The mountains\u2018 craggy peaks are sketched with fine , precise lines , typical of traditional Chinese ink art. A slender\nswirling mists, meandering waterfall begins its descent here , its water appearing almost ethereal amidst the soft. In the middle section, the\nwaterfall cascades energetically , creating a dynamic contrast with the serene mountains above. Lush pine trees , rendered with graceful ,\nflowing brush strokes , flank the waterfall. These trees appear to dance with the rhythm of the water , adding a vibrant life to the scene. At\nthe bottom , the waterfall concludes its journey in a tranquil pool. The water's surface is calm , reflecting the surrounding nature and the sky\nabove. Here , delicate flowers and small shrubs are depicted along the water's edge , symbolizing peace and harmony with nature.\nSDXL\nDALL-E 3\nRPG (Ours)\nSDXL\nDALL-E 3\nRPG (Ours)\nPrompt: A green twintail girl in orange dress is sitting on the sofa while a messy desk in under a big window on the left, while a\nlively aquarium is on the top right of the sofa, realistic style.\nSDXL\nDALL-E 3\nRPG (Ours)\nFigure 2. Compared to SDXL (Podell et al., 2023) and DALL-E 3 (Betker et al., 2023), our proposed RPG exhibits a superior ability to\nconvey intricate and compositional text prompts within generated images (colored text denotes critical part).\n2\nMastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs\nBase prompt: A beautiful girl standing in her bright room \nRegion 0: A delicate blue vases with pink roses \nRegion 1: A beautiful black hair girl with her eyes closed in champagne \nlong sleeved formal dress \nRegion 2: Some white roses, filled with upgraded growth all around\nUser input: A beautiful black hair girl with her eyes closed in champagne \nlong sleeved formal dress standing in her bright room with delicate blue \nvases with pink roses on the left and some white roses, filled with \nupgraded growth all around on the right\nBase prompt: Under the clear starry sky, clear river water flows in the \nmountains, and the lavender flower sea in front of me dances with the \nwind, A peaceful, beautiful, and harmonious atmosphere \nRegion 0: Clear starry sky, many stars twinkling in the night sky \nRegion 1: The blue stream flows through the valleys covered with \ncolorful flowers ,reflecting the starry sky \nRegion 2: The sea of lavender dancing in the wind,\nUser input: Under the clear starry sky, clear river water flows in the \nmountains, and the lavender flower sea in front of me dances with the \nwind, a peaceful, beautiful, and harmonious atmosphere \nRegion 0: The city in spring where every where is lively \nand colorful with pink flowers mountains\nRegion 1: The city with buildings with green rooves in \nsummer, where a lake is beautiful \nRegion 2: The city in autumn with buildings with orange \nrooves, where every where is light orange and golden \nRegion 3:  The city in winter with trees covered with \nsnow and blue rooves covered with snow, where every \nis white and covered with snow\nUser input: From left to right, an ancient Chinese city \nin spring, summer, autumn and winter in four different \nregions\nPose\nControlNet\nRPG + ControlNet\nDepth\nControlNet\nRPG + ControlNet\nControlNet\nCanny Edge\nRPG + ControlNet\nFigure 3. Our RPG framework can extend text-to-image generation with more conditions (e.g., pose, depth and canny edge) by utilizing\nControlNet (Zhang et al., 2023a). Compared to original ControlNet, RPG significantly improves its prompt understanding by decomposing\n\u201duser input\u201d into the combination of base prompt and subprompts, and further enhance its compositional semantic alignment of generated\nimages by performing region-wise diffusion generation (in Section 2.2).\n3\nMastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs\nAnother potential solution is to leverage image understand-\ning feedback (Huang et al., 2023a; Xu et al., 2023; Sun\net al., 2023a; Fang et al., 2023) for refining diffusion gener-\nation. For instance, GORS (Huang et al., 2023a) finetunes a\npretrained text-to-image model with generated images that\nhighly align with the compositional prompts, where the fine-\ntuning loss is weighted by the text-image alignment reward.\nInspired by the reinforcement learning from human feed-\nback (RLHF) (Ouyang et al., 2022; Stiennon et al., 2020)\nin natural language processing, ImageReward (Xu et al.,\n2023) builds a general-purpose reward model to improve\ntext-to-image models in aligning with human preference.\nDespite some improvements achieved by these methods,\nthere are still two main limitations in the context of com-\npositional/complex image generation: (i) existing layout-\nbased or attention-based methods can only provide rough\nand suboptimal spatial guidance, and struggle to deal with\noverlapped objects (Cao et al., 2023; Hertz et al., 2022; Lian\net al., 2023) ; (ii) feedback-based methods require to collect\nhigh-quality feedback and incur additional training costs.\nTo address these limitations, we introduce a new training-\nfree text-to-image generation framework, namely Recap-\ntion, Plan and Generate (RPG), unleashing the impressive\nreasoning ability of multimodal LLMs to enhance the com-\npositionality and controllability of diffusion models. We\npropose three core strategies in RPG:\nMultimodal Recaptioning. We specialize in transforming\ntext prompts into highly descriptive ones, offering informa-\ntive augmented prompt comprehension and semantic align-\nment in diffusion models. We use LLMs to decompose the\ntext prompt into distinct subprompts, and recaption them\nwith more detailed descriptions. We use MLLMs to auto-\nmatically recaption input image for identifying the semantic\ndiscrepancies between generated images and target prompt.\nChain-of-Thought Planning. In a pioneering approach,\nwe partition the image space into complementary subre-\ngions and assign different subprompts to each subregion,\nbreaking down compositional generation tasks into multiple\nsimpler subtasks. Thoughtfully crafting task instructions\nand in-context examples, we harness the powerful chain-\nof-thought reasoning capabilities of MLLMs (Zhang et al.,\n2023d) for efficient region division. By analyzing the recap-\ntioned intermediate results, we generate detailed rationales\nand precise instructions for subsequent image compositions.\nComplementary Regional Diffusion.\nBased on the\nplanned non-overlapping subregions and their respective\nprompts, we propose complementary regional diffusion to\nenhance the flexibility and precision of compositional text-\nto-image generation. Specifically, we independently gener-\nate image content guided by subprompts within designated\nrectangle subregion, and subsequently merge them spatially\nin a resize-and-concatenate approach. This region-specific\ndiffusion effectively addresses the challenge of conflicting\noverlapped image contents. Furthermore, we extend this\nframework to accommodate editing tasks by employing\ncontour-based regional diffusion, enabling precise manipu-\nlation of inconsistent regions targeted for modification.\nThis new RPG framework can unify both text-guided image\ngeneration and editing tasks in a closed-loop fashion. We\ncompare our RPG framework with previous work in Figure 1\nand summarize our main contributions as follows:\n\u2022 We propose a new training-free text-to-image genera-\ntion framework, namely Recaption, Plan and Generate\n(RPG), to improve the composibility and controllability\nof diffusion models to the fullest extent.\n\u2022 RPG is the first to utilize MLLMs as both multimodal\nrecaptioner and CoT planner to reason out more infor-\nmative instructions for steering diffusion models.\n\u2022 We propose complementary regional diffusion to en-\nable extreme collaboration with MLLMs for composi-\ntional image generation and precise image editing.\n\u2022 Our RPG framework is user-friendly, and can be gener-\nalized to different MLLM architectures (e.g., MiniGPT-\n4) and diffusion backbones (e.g., ControlNet).\n\u2022 Extensive qualitative and quantitative comparisons\nwith previous SOTA methods, such as SDXL, DALL-E\n3 and InstructPix2Pix, demonstrate our superior text-\nguided image generation/editing ability.\n2. Method\n2.1. Overview of Proposed RPG\nIn this section, we introduce our novel training-free frame-\nwork - Recaption, Plan and Generate (RPG). We delineate\nthree fundamental strategies of our RPG in text-to-image\ngeneration (Section 2.2), as depicted in Figure 4. Specifi-\ncally, given a complex text prompt that includes multiple en-\ntities and relationships, we leverage (multimodal) LLMs to\nrecaption the prompt by decomposing it into a base prompt\nand highly descriptive subprompts. Subsequently, we utilize\nmultimodal CoT planning to allocate the split (sub)prompts\nto complementary regions along the spatial axes. Building\nupon these assignments, we introduce complementary re-\ngional diffusion to independently generate image latents and\naggregate them in each sampling step.\nOur RPG framework exhibits versatility by extending its\napplication to text-guided image editing with minimal ad-\njustments, as exemplified in Section 2.3. For instance, in the\nrecaptioning phase, we utilize MLLMs to analyze the paired\ntarget prompt and source image, which results in informative\n4\nMastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs\nUser Text (Base) Prompt:\nA nobleman is standing \nunder a bright moon while \ntwo owls are on a big oak.\nIdentify key phrases:\n1\u3001A noble man\n2\u3001A bright moon\n3\u3001Two owls\n4\u3001A big oak\nText-to-Text Recaptioning\n1. A nobleman, regal and distinguished, with a \nsharp gaze, dressed in a velvet doublet, \nstanding proudly in his ancestral castle\n2. A bright moon, full and radiant, casting a silver \nglow over a tranquil lake, serene and majestic \nin the night sky.\n3. Two owls, perched side by side, wise and \nmysterious, with piercing eyes, on an ancient, \ngnarled branch under the starlit sky\n4. A big oak, towering and robust, its sprawling \nbranches a testament to centuries, leaves \nwhispering stories in the gentle breeze.\nRationale Generation\n## Task objectives: Generate complementary \nsubregions based on the recaptioned subprompts\nand assign them to subregions.\n[Instructions + In-context examples]\n[Question]:\nSubprompts: 1. A nobleman, regal and \u2026..\n2. A bright moon, full and radiant \u2026\u2026\n3. Two owls, perched side by side \u2026\u2026\n4. A big oak, towering and robust \u2026\u2026\nLet\u2019s think step by step (trigger CoT reasoning)\nRationale:  Here we have 4 subprompts, thus total \n4 regions. First, we take the nobleman and his \ncastle as foreground and background, we place \nthem in lower left of the image. Next, bright moon \nshould in the sky so we assign the moon to the \nhigher left corner\u2026\u2026 (Coarse grained area division)\nA moon\nA nobleman\nA \nnobleman\n(0.8)\nA moon\n(0.8)\nTwo owls\n(0.8)\nA big oak\n(0.8)\nRecaption-Plan-Generate (RPG) for\nText-to-Image Generation\nStage 1: Recaption\nStage 2: Plan\nStage 3: Generate\nBase Prompt\n(0.2)\nComplementary Regional Diffusion\nMLLMs\nMLLMs\nWeighted sum for each sampling step\nRegion-wise\nGeneration\nChain-of-Thought\nPlanning\nSubregion Planning\nTherefore, the region split ratio should be:\n1,1,1;2,2,3 \uff08Column Mode\uff09\n[Assign subprompts to subregions]\nRegion 0\uff1a A bright moon, full and  radiant\u2026\u2026\nRegion 1\uff1aTwo owls, perched side by side \u2026\u2026\nRegion 2\uff1aA nobleman, regal and \u2026..\nRegion 3\uff1a A big oak, towering and robust  \u2026\u2026\nCoT Reasoning\nTwo owls\nBig oak\nStep=10\nStep=20\nStep=30\nStep=40\nStep=5\nFigure 4. Overview of our RPG framework for text-to-image generation.\nmultimodal feedback that captures their cross-modal seman-\ntic discrepancies. In multimodal CoT planning, we generate\na step-by-step edit plan and produce precise contours for our\nregional diffusion. Furthermore, we demonstrate the ability\nto execute our RPG workflow in a closed-loop manner for\nprogressive self-refinement, as showcased in Section 2.3.\nThis approach combines precise contour-based editing with\ncomplementary regional diffusion generation.\n2.2. Text-to-image Generation\nPrompt Recaptioning\nLet yc be a complex user prompt\nwhich includes multiple entities with different attributes and\nrelationships. We use MLLMs to identify the key phrases\nin yc to obtain subpormpts denoted as:\n{yi}n\ni=0 = {y0, y1, ..., yn} \u2286 yc,\n(1)\nwhere n denotes the number of key phrases. Inspired by\nDALL-E 3 (Betker et al., 2023), which uses pre-trained\nimage-to-text (I2T) caption models to generate descriptive\nprompts for images, and construct new datasets with high-\nquality image-text pairs. In contrast, we leverage the im-\npressive language understanding and reasoning abilities of\nLLMs and use the LLM as the text-to-text (T2T) captioner\nto further recaption each subprompt with more informative\ndetailed descriptions:\n{\u02c6y0, \u02c6y1, ..., \u02c6yn} = Recaption({yi}n\ni=0).\n(2)\nIn this way, we can produce denser fine-grained details for\neach subprompt in order to effectively improve the fidelity\nof generated image, and reduce the semantic discrepancy\nbetween prompt and image.\nExample\nSplit ratio: 1,1,1; 2,1,1\n1. First, we split the image into two rows where horizontal \nsplit ratio is 1:2\n2. Second, we split the first row into two columns, where\nvertical split ratio is 1:1, and we get region 0 and region 1\n3. Finally, we split the second row into two columns, where\nvertical split ratio is also 1:1, and we get region 2 and region 3\nRules: We start splitting from the top left corner of the image, and \nthe region is numbered from the top left region to right, row by row.\n1:2\n3\n2\n0\n1\n1:1\n1:1\n1\n2\n3\nFigure 5. An illustrative example for region division.\nCoT Planning for Region Division\nBased on the recap-\ntioned subprompts, we leverage the powerful multimodal\nchain-of-thought (CoT) reasoning ability of LLMs (Zhang\net al., 2023d) to plan the compositions of final image content\nfor diffusion models. Concretely, we divide image space\nH \u00d7 W into several complementary regions, and assign\neach augmented subprompt \u02c6yi to specific region Ri:\n{Ri}n\ni=0 = {R0, R1, ..., Rn} \u2286 H \u00d7 W,\n(3)\nIn order to produce meaningful and accurate subregions,\nwe need to carefully specify two components for planning\nregion divisions: (i) region parameters: we define that rows\nare separated by \u201d;\u201d and each column is denoted by a series\nof numbers separated by commas (e.g., \u201d1,1,1\u201d). To be\nspecific , we first use \u201d;\u201d to split an image into different\nrows, then within each row, we use commas to split a row\ninto different regions, see Figure 5 for better comprehension;\n(ii) region-wise task specifications to instruct MLLMs: we\nutilize the CoT reasoning of MLLMs with some designed\nin-context examples to reason out the plan of region division.\nWe here provide a simplified template of our instructions\nand in-context examples:\n5\nMastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs\nDecompose complex text prompt \ninto simpler subprompts\nIntermediate \nlatent\nQuery\nlatent\nToken Keys (from text prompt)\n\u00d7\nM1\nM2\nM3\nM4\n\u2026\u2026\nAttention maps\n\u00d7\nToken Values (From text prompt)\nDenoised latent\n\u2026\u2026\nParallel Subprompt-Latent Cross Attention\nResize\n\u00d7\nBase \nratio\n\u00d7\n1-\nBase \nratio\nLatent with base prompt\nConcatenated latent\nCompositional \nlatent\nSubregion Latent Concatenation\nText prompt:  An old man is telling \na story to a little girl while a cat is \nlistening as well on the tree.\nDecomposed Prompt:\nRegion 0: a cat is on the tree \u2026\u2026\nRegion 1: An old man is telling a story .\u2026..\nRegion 2: Some leaves \u2026\u2026..\nRegion 3: A little girl \u2026\u2026\nAn old man\nA cat\nSome leaves\nA little girl\nConcatenate\nFigure 6. The demonstration of each sampling step in our Complementary Regional Diffusion.\n1.Task instruction\nYou are an smart region planner for image. You\nshould use split ratio to specify the split method\nof the image, and then recaption each subregion\nprompts with more descriptive prompts while main-\ntaining the original meaning.\n2.Multi-modal split tutorial\n......\n3. In-context examples\nUser Prompt: A girl with white ponytail and black\ndress are chatting with a blonde curly hair girl in a\nwhite dress in a cafe.\n# Key pharses extraction and Recaption\n# Split ratio Planning\n# Composition Logic\n# Aesthetic Considerations:\n# Final output\n4.Trigger CoT reasoning ability of MLLMs\nUser Prompt: An old man with his dog is looking\nat a parrot on the tree.\nReasoning: Let\u2019s think step by step......\nTo facilitating inferring the region for each subprompt, we\nadhere to three key principles in designing in-context ex-\nample and generating informative rationales: (i) the objects\nwith same class name (e.g., five apples) will be separately as-\nsign to different regions to ensure the numeric accuracy; (ii)\nfor complex interactions between two entities, we take these\ntwo entities as a whole to avoid contradictory overlapped\ngeneration results mentioned in (Lian et al., 2023); (iii) If\nthe prompt focuses more on the appearance of a specific\nentity, we treat the different parts of this entity as different\nentities (e.g., A green hair twintail in red blouse , wearing\nblue skirt. =\u21d2 green hair twintail, red blouse, blue skirt).\nComplementary Regional Diffusion\nRecent works (Liu\net al., 2022; Wang et al., 2023; Chefer et al., 2023; Feng\net al., 2022) have adjusted cross-attention masks or lay-\nouts to facilitate compositional generation. However, these\napproaches predominantly rely on simply stacking latents,\nleading to conflicts and ambiguous results in overlapped\nregions. To address this issue, as depicted in Figure 6, we\nintroduce a novel approach called complementary regional\ndiffusion for region-wise generation and image composition.\nWe extract non-overlapping complementary rectangular re-\ngions and apply a resize-and-concatenate post-processing\nstep to achieve high-quality compositional generation. Ad-\nditionally, we enhance coherence by combining the base\nprompt with recaptioned subprompts to reinforce the con-\njunction of each generated region and maintain overall im-\nage coherence (detailed ablation study in Section 4). This\ncan be represented as:\nxt\u22121 = CRD(xt, ybase, {\u02c6yi}n\ni=0, {Ri}n\ni=0, t, s),\n(4)\nwhere s is a fixed random seed, CRD is the abbreviation for\ncomplementary regional diffusion.\nMore concretely, we construct a prompt batch with base\n6\nMastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs\nUser Text Prompt:\nSix patterned mugs, arranged in \ntwo columns, on a marble surface, \nand a rose in the vase on the left.\nTask Instruction +\nIn-context Examples +\nUser Text Prompt\nEdited Image\nMulti-Round Editing with RPG\nImage-to-Text Recaptioning\n## Task objectives: Analyze user text prompt to identify key \nphrases and attributes,  utilizing visual reasoning on the \nimage to detect inconsistency\n[Task instruction+ In-context examples ]\n[Question]\nUser text prompt: Six patterned mugs, arranged in two columns, \non a marble surface, and a rose in the vase on the left.\nTask description: Analyze the prompt and  recaption the image, \nfind inconsistency between the prompt and the recaption prompt, \nmake an edit plan, Let\u2019s think step by step (trigger CoT reasoning):\nRecaption the image: Eight patterned mugs, six mugs arranged in \ntwo columns on a marble surface, and two roses in the two mugs \non the left lower corner. \nStage 1: Recaption\nEditing Planning:\nRationale Generation\uff1a\n1. More mugs staggered \uff08delete\uff09\n2. More roses exist (delete)\n3. There is no vase visible in the image (replace or add)\nEdit plan: \nAccording to the inconsistency,\n(i) Firstly, we delete the mug on the right down corner along with its reflection image.\n(ii) Next, we delete the rose in the first column from left to right along with its reflection image.\n(iii) Finally, here is only one inconsistency, we should replace the mug in the left lower corner \nwith a vase along with its reflection image.\nStage 2:  Plan\nStage 3: Generate\nAnother Loop or Exit\nDetect \nPrecise Contours\nRegion-wise Editing\nChain-of-Thought\nPlanning\nMask and \nInpainting\nSource/Generated Image\nFigure 7. RPG unifies text-guided image generation and editing in a closed-loop approach.\nprompt ybase = yc and the recaptioned subprompts:\nPrompt Batch:\n{ybase, {\u02c6yi}n\ni=0}.\n(5)\nIn each timestep, we deliver the prompt batch into the de-\nnoising network and manipulate the cross-attention layers\nto generate different latents {zi\nt\u22121}n\ni=0 and zbase\nt\u22121 in parallel,\nas demonstrated in Figure 6. We formulate this process as:\nzi\nt\u22121 = Softmax((WQ \u00b7 \u03d5(zt))(WK \u00b7 \u03c8(\u02c6yi))T\n\u221a\nd\n)WV \u00b7\u03c8(\u02c6yi),\n(6)\nwhere image latent zt is the query and each subprompt \u02c6yi\nworks as a key and value. WQ, WK, WV are linear projec-\ntions and d is the latent projection dimension of the keys\nand queries. Then, we shall proceed with resizing and con-\ncatenating the generated latents {zi\nt\u22121}n\ni=0, according to\ntheir assigned region numbers (from 0 to n) and respective\nproportions. Here we denote each resized latent as:\nzi\nt\u22121(h, w) = Resize(zi\nt\u22121, Ri),\n(7)\nwhere h, w are the height and the width of its assigned\nregion Ri. We directly concatenate them along the spatial\naxes:\nzcat\nt\u22121 = Concatenate({zi\nt\u22121(h, w)}n\ni=0).\n(8)\nTo ensure a coherent transition in the boundaries of different\nregions and a harmonious fusion between the background\nand the entities within each region, we use the weighted\nsum of the base latents zbase\nt\u22121 and the concatenated latent\nzcat\nt\u22121 to produce the final denoising output:\nzt\u22121 = \u03b2 \u2217 zbase\nt\u22121 + (1 \u2212 \u03b2) \u2217 zcat\nt\u22121.\n(9)\nHere \u03b2 is used to achieve a suitable balance between human\naesthetic perception and alignment with the complex text\nprompt of the generated image. It is worth noting that\ncomplementary regional diffusion can generalize to arbitrary\ndiffusion backbones including SDXL (Podell et al., 2023),\nConPreDiff (Yang et al., 2023b) and ControlNet (Zhang\net al., 2023a), which will be evaluated in Section 3.1.\n2.3. Text-Guided Image Editing\nImage Recaptioning\nOur RPG can also generalize to text-\nguided image editing tasks as illustrated in Figure 7. In\nrecaptioning stage, RPG adopts MLLMs as a captioner to\nrecaption the source image, and leverage its powerful rea-\nsoning ability to identify the fine-grained semantic discrep-\nancies between the image and target prompt. We directly\nanalyze how the input image x aligns with the target prompt\nytar. Specifically, we identify the key entities in x and ytar:\n{yi}n\ni=0 = {y0, y1, ..., yn} \u2286 ytar,\n{ei}m\ni=0 = {e0, e1, ..., em} \u2286 Recaption(x),\n(10)\nThen we utilize MLLMs (e.g., GPT4 (OpenAI, 2023), Gem-\nini Pro (Team et al., 2023)) to check the differences between\n7\nMastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs\nAttribute Binding\nRPG\n(Ours)\nDALL-E 3\nTwo hardcover \nbooks, one red \nopen and one \ngreen along \nwith a red pen \nand a pair of \nred glasses\nlying on the \ncarpet\nA green \ntwintail hair \ngirl wearing a \nwhite shirt \nand skirt \nprinted with \ngreen apple\nTwo women are \nnegotiating a \ndeal in a \nmeeting room, \nthe woman on the \nleft with blonde \nblunt bangs is \nin white suit, \nthe woman on the \nright with white \ncurly hair is in \nblack suit.\nSDXL\nLMD+\nNumeric Accuracy\nFour orange \ncats and four\nblue balloons \nfloating above \nthem\nThree\nsunflowers in \ntwo vases \nwhile three\nbutterflies \nare flying\nTwo teddy bears \nsitting on the \nwhite carpet \nsurrounded by \nthree toy cars\nComplex Relationship\nA white \ntwintail girl \nin orange dress \nis sitting on \nthe sofa, a \nmessy desk is \non the left \nwhile a wide \nwindow is above \nthe desk and \nthe aquarium on \nthe top of the \nsofa is lively, \nanime style\nTwo apples on \nleft and an \nipad on the \nright on the \nbed, where the \nsheet printed \nwith apple\nA girl with \ngreen twintail\nis standing in \nher room, her \ndesk on the \nleft is messy \nand the \naquarium on \nthe top of the \norange sofa on \nthe right is \nlively, Disney \nstyle.\nFigure 8. Qualitative comparison between our RPG and SOTA text-to-image models (SDXL (Podell et al., 2023) and DALL-E 3 (Betker\net al., 2023)), and LLM-grounded diffusion model LMD+ (Lian et al., 2023).\n{yi}n\ni=0 and {ei}m\ni=0 regarding numeric accuracy, attribute\nbinding and object relationships. The resulting multimodal\nunderstanding feedback would be delivered to MLLMs for\nreason out editing plans.\nCoT Planning for Editing\nBased on the captured se-\nmantic discrepancies between prompt and image, RPG\ntriggers the CoT reasoning ability of MLLMs with high-\nquality filtered in-context examples, which involves man-\nually designed step-by-step editing cases such as entity\nmissing/redundancy, attribute mismatch, ambiguous rela-\ntionships. Here, in our RPG, we introduce three main edit\noperations for dealing with these issues: addition Add(),\ndeletion Del(), modification Mod(). Take the multimodal\nfeedback as the grounding context, RPG plans out a series\nof editing instructions. An example Plan(ytar, x) can be\ndenoted as a composed operation list:\n{Del(yi, x), \u00b7 \u00b7 \u00b7 , Add(yj, x), \u00b7 \u00b7 \u00b7 , Mod(yk, x)},\n(11)\nwhere i, j, k <= n, length(Plan(ytar, x0)) = L. In this way,\nwe are able to decompose original complex editing task into\nsimpler editing tasks for more accurate results.\nContour-based Regional Diffusion\nTo collaborate more\neffectively with CoT-planned editing instructions, we gener-\nalize our complementary regional diffusion to text-guided\nediting. We locate and mask the target contour associated\nwith the editing instruction (Kirillov et al., 2023), and apply\ndiffusion-based inpainting (Rombach et al., 2022) to edit the\ntarget contour region according to the planned operation list\nPlan(ytar, x). Compared to traditional methods that utilize\ncross-attention map swap or replacement (Hertz et al., 2022;\nCao et al., 2023) for editing, our mask-and-inpainting\nmethod powered by CoT planning enables more accurate\nand complex editing operations (i.e., addition, deletion and\nmodification).\nMulti-Round Editing for Closed-Loop Refinement\nOur\ntext-guided image editing workflow is adaptable for a closed-\nloop self-refined text-to-image generation, which combines\nthe contour-based editing with complementary regional dif-\nfusion generation. We could conduct multi-round closed-\nloop RPG workflow controlled by MLLMs to progressively\nrefine the generated image for aligning closely with the tar-\nget text prompt. Considering the time efficiency, we set a\nmaximum number of rounds to avoid being trapped in the\nclosed-loop procedure. Based on this closed-loop paradigm,\nwe can unify text-guided generation and editing in our RPG,\nproviding more practical framework for the community.\n8\nMastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs\nTable 1. Evaluation results on T2I-CompBench. RPG consistently demonstrates best performance regarding attribute binding, object\nrelationships, and complex compositions. We denote the best score in blue , and the second-best score in green . The baseline data is\nquoted from Chen et al. (2023a).\nModel\nAttribute Binding\nObject Relationship\nComplex\u2191\nColor \u2191\nShape\u2191\nTexture\u2191\nSpatial\u2191\nNon-Spatial\u2191\nStable Diffusion v1.4 (Rombach et al., 2022)\n0.3765\n0.3576\n0.4156\n0.1246\n0.3079\n0.3080\nStable Diffusion v2 (Rombach et al., 2022)\n0.5065\n0.4221\n0.4922\n0.1342\n0.3096\n0.3386\nComposable Diffusion (Liu et al., 2022)\n0.4063\n0.3299\n0.3645\n0.0800\n0.2980\n0.2898\nStructured Diffusion (Feng et al., 2022)\n0.4990\n0.4218\n0.4900\n0.1386\n0.3111\n0.3355\nAttn-Exct v2 (Chefer et al., 2023)\n0.6400\n0.4517\n0.5963\n0.1455\n0.3109\n0.3401\nGORS (Huang et al., 2023a)\n0.6603\n0.4785\n0.6287\n0.1815\n0.3193\n0.3328\nDALL-E 2 (Ramesh et al., 2022)\n0.5750\n0.5464\n0.6374\n0.1283\n0.3043\n0.3696\nSDXL (Betker et al., 2023)\n0.6369\n0.5408\n0.5637\n0.2032\n0.3110\n0.4091\nPixArt-\u03b1 (Chen et al., 2023a)\n0.6886\n0.5582\n0.7044\n0.2082\n0.3179\n0.4117\nConPreDiff (Yang et al., 2023b)\n0.7019\n0.5637\n0.7021\n0.2362\n0.3195\n0.4184\nRPG (Ours)\n0.8335\n0.6801\n0.8129\n0.4547\n0.3462\n0.5408\n3. Experiments\n3.1. Text-to-Image Generation\nImplementation Details\nOur RPG is general and exten-\nsible, we can incorporate arbitrary MLLM architectures\nand diffusion backbones into the framework. In our experi-\nment, we choose GPT-4 (OpenAI, 2023) as the recaptioner\nand CoT planner, and use SDXL (Podell et al., 2023) as\nthe base diffusion backbone to build our RPG framework.\nConcretely, in order to trigger the CoT planning ability of\nMLLMs, we carefully design task-aware template and high-\nquality in-context examples to conduct few-shot prompting.\nBase prompt and its weighted hyperparameter base ratio\nare critical in our regional diffusion, we have provide fur-\nther analysis in Figure 16. When the user prompt includes\nthe entities with same class (e.g., two women, four boys),\nwe need to set higher base ratio to highlight these distinct\nidentities. On the contrary, when user prompt includes the\nthe entities with different class name (e.g., ceramic vase and\nglass vase), we need lower base ratio to avoid the confusion\nbetween the base prompt and subprompts.\nMain Results\nWe compare with previous SOTA text-to-\nimage models DALL-E 3 (Betker et al., 2023), SDXL and\nLMD+ (Lian et al., 2023) in three main compositional sce-\nnarios: (i) Attribute Binding. Each text prompt in this\nscenario has multiple attributes that bind to different entities.\n(ii) Numeric Accuracy. Each text prompt in this scenario\nhas multiple entities sharing the same class name, the num-\nber of each entity should be greater than or equal to two. (iii)\nComplex Relationship. Each text prompt in this scenario\nhas multiple entities with different attributes and relation-\nships (e.g., spatial and non-spational). As demonstrated\nin Table 1, our RPG is significantly superior to previous\nmodels in all three scenarios, and achieves remarkable level\nof both fidelity and precision in aligning with text prompt.\nHierarchical Regional Diffusion\n0\n0\n1\n0\n2\n1\n3\nTwo girls are chatting in a \nChinese restaurant , the one on \nthe left is a blonde long hair girl \nwith blue and white sailor shirt  , \nblack, the one on the right is a \nblack double bun hair girl , green \ncheongsam printed with flowers\nBase prompt:\nTwo girls are chatting in a Chinese \nrestaurant , \nRegion 0:a blonde long hair girl \nwith blue and white sailor shirt,\nRegion 1:a black double bun hair \ngirl , green cheongsam printed with \nflowers\nBase prompt: \nTwo girls are chatting in a Chinese \nrestaurant \nRegion 0: a blonde long hair girl\nRegion 1: A black double bun hair girl, \ngreen cheongsam printed with flowers \nRegion 2:  a blue and white sailor shirt, \nRegion 3: Green cheongsam printed \nwith flowers \n0\nA large bookshelf with five floors \nand six compartments with lively \naquarium on the top left,and \nplant in terrarium on the top \nright,the Books with ancient kraft \npaper covers in the second and \nthird floors,newly printed books \nincluding red and blue books in \nthe fourth and fifth floors.\n0\n1\n2\n0\n1\n2\n3\n4\n5\nBase prompt : A large bookshelf with \nthree floors and six compartments \nwith books, lively aquarium, and \nplant in terrarium \nRegion 0: small lively aquarium with \ngoldfish and sea weed in the \ncompartment of the bookshelf, \ndelicate flowers in the terrarium in \nthe compartment of the bookshelf \nRegion 1: Books with ancient kraft \npaper covers in the compartment of \nthe bookshelf \nRegion 2: Some new books including \nred covers and blue covers in the \ncompartment of the bookshelf \nBase prompt : A large bookshelf with three floors \nand six compartments with books, lively aquarium, \nand plant in terrarium \nRegion 0: small lively aquarium with goldfish and \nsea weed in the compartment of the bookshelf, \nRegion 1: delicate flowers in the terrarium in the \ncompartment of the bookshelf\nRegion 2: Books with ancient kraft paper covers in \nthe compartment of the bookshelf \nRegion 3: Books with ancient kraft paper covers in \nthe compartment of the bookshelf \nRegion 4: Some new books including red covers and \nblue covers in the compartment of the bookshelf \nRegion 4: Some new books including red covers and \nblue covers in the compartment of the bookshelf \nFigure 9. Demonstration of our hierarchical regional diffusion. Dif-\nfusion with more hierarchies can produce more satisfying results.\n9\nMastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs\nText prompt: A girl with \nwhite ponytail and black \ndress are chatting with a \nblonde curly hair girl in a \nwhite dress in a caf\u00e9.\nGPT-4\nLlama 2-13B\nLlama 2-70B\nText prompt: A cute \nragdoll cat is standing \non the table in the \nmiddle, and beautiful \nteapot on the right and \na tea cup on the left.\nMiniGPT-4\nVicuna-33B\nFigure 10. Generalizing RPG to different (multimodal) LLM architectures, including Llama 2 (Touvron et al., 2023b), Vicuna (Chiang\net al., 2023) and MiniGPT-4 (Zhu et al., 2023).\nWe observe that SDXL and DALL-E 3 have poor genera-\ntion performance regarding numeric accuracy and complex\nrelationship. In contrast, our RPG can effectively plan out\nprecise number of subregions, and utilize proposed com-\nplementary regional diffusion to accomplish compositional\ngeneration. Compared to LMD+ (Lian et al., 2023), a LLM-\ngrounded layout-based text-to-image diffusion model, our\nRPG demonstrates both enhanced semantic expression ca-\npabilities and image fidelity. We attribute this to our CoT\nplanning and complementary regional diffusion. For quan-\ntitative results, we assess the text-image alignment of our\nmethod in a comprehensive benchmark, T2I-Compbench\n(Huang et al., 2023a), which is utilized to evaluate the com-\npositional text-to-image generation capability. In Table 1,\nwe consistently achieve best performance among all meth-\nods proposed for both general text-to-image generation and\ncompositional generation, including SOTA model ConPreD-\niff (Yang et al., 2023b).\nHierarchical Regional Diffusion\nWe can extend our re-\ngional diffusion to a hierarchical format by splitting cer-\ntain subregion to smaller subregions. As illustrated in Fig-\nure 9, when we increase the hierarchies of our region split,\nRPG can achieve a significant improvement in text-to-image\ngeneration. This promising result reveals that our comple-\nmentary regional diffusion provides a new perspective for\nhandling complex generation tasks and has the potential to\ngenerate arbitrarily compositional images.\nGeneralizing to Various LLMs and Diffusion Backbones\nOur RPG framework is of great generalization ability, and\ncan be easily generalized to various (M)LLM architectures\n(in Figure 10) and diffusion backbones (in Figure 11). We\nobserve that both LLM and diffusion architectures can in-\nfluence the generation results. We also generalize RPG to\nControlNet (Zhang et al., 2023a) for incorporating more\nconditional modalities. As demonstrated in Figure 3, our\nRPG can significantly improve the composibility of origi-\nnal ControlNet in both image fidelity and textual semantic\nalignment.\nRPG with ConPreDiff\nRPG with SD v2.1\nText prompt: In a fantasy world, two beautiful girls are \nwandering in the city mall chat and eating where the stall is \non both side and the big castle is behind\nText prompt: The scene is framed to focus on the interaction \nbetween Snow White and the dwarfs,with the warm interior \nand the cozy fire suggesting a safe haven from the dark forest \noutside. The cottage feels lived-in and welcoming,a perfect \nsetting for an evening of storytelling and camaraderie.\nFigure 11. Generalizing RPG to different diffusion backbones, Sta-\nble Diffusion v2.1 (Rombach et al., 2022) and recent SOTA diffu-\nsion model ConPreDiff (Yang et al., 2023b).\n10\nMastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs\nSource \nImage\nPrompt2Pro\nmpt\nInstruct\nPix2Pix\nMasaCtrl\nRPG\n(Ours)\nEdit task:\nBlue compute -> green \ncomputer\nGreen chair -> wooden \nchair\nWrist -> wrist with Rolex\nEdit task:\nApple on the book -> \ndelete\nBlue book -> blue book \nprinted with apple\nEdit task:\nBlack cat -> delete\nBlue shirt -> Cyan \nshirt\nFigure 12. Qualitative comparison in text-guided image edit-\ning.\nWe outperform previous powerful methods including\nPrompt2Prompt (Hertz et al., 2022), InstructPix2Pix (Brooks et al.,\n2023) and MasaCtrl (Cao et al., 2023).\n3.2. Text-Guided Image Editing\nQualitative Results\nIn the qualitative comparison of text-\nguided image editing, we choose some strong baseline\nmethods, including Prompt2Prompt (Hertz et al., 2022),\nInstructPix2Pix (Brooks et al., 2023) and MasaCtrl (Cao\net al., 2023). Prompt2Prompt and MasaCtrl conduct edit-\ning mainly through text-grounded cross-attention swap or\nreplacement, InstructPix2Pix aims to learn a model that can\nfollow human instructions. As presented in Figure 12, RPG\nproduces more precise editing results than previous meth-\nods, and our mask-and-inpainting editing strategy can also\nperfectly preserve the semantic structure of source image.\nMulti-Round Editing\nWe conduct multi-round editing\nto evaluate the self-refinement with our RPG framework in\nFigure 13. We conclude that the self-refinement based on\nRPG can significantly improve precision, demonstrating the\neffectiveness of our recaptioning-based multimodal feed-\nMulti-Round Text-to-Image Editing\nEditing prompt: A green twintail girl with white shirt and pink dress\nEditing prompt: In a fantasy world, two beautiful girls are wandering in the city mall chat and eating \nwhere the stall is on both side and the big castle is behind\nSource/Generated Image\nRound 1\nRound 2\nEditing prompt: A brave knight is coming to crusade a giant big dragon in lightning and thunder\nFigure 13. Multi-round text-guided image editing with our RPG\nframework.\nback and CoT planning. We also find that RPG is able to\nachieve satisfying editing results within 3 rounds.\n4. Model Analysis\nEffect of Recaptioning\nWe conduct ablation study about\nthe recaptioning, and show the result in Figure 14. From\nthe result, we observe that without recaptioning, the model\ntends to ignore some key words in the generated images.\nOur recaptioning can describe these key words with high-\ninformative and denser details, thus generating more delicate\nand precise images.\nEffect of CoT Planning\nIn the ablation study about CoT\nplanning, as demonstrated in Figure 15, we observe that\nthe model without CoT planning fail to parse and convey\ncomplex relationships from text prompt. In contrast, our\nCoT planning can help the model better identify fine-grained\nattributes and relationships from text prompt, and express\nthem through a more realistic planned composition.\nEffect of Base Prompt\nIn RPG, we leverage the generated\nlatent from base prompt in diffusion models to improve the\ncoherence of image compositions. Here we conduct more\nanalysis on it in Figure 16. From the results, we find that\nthe proper ratio of base prompt can benefit the conjunction\nof different subregions, enabling more natural composition.\nAnother finding is that excessive base ratio may result in\n11\nMastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs\nWithout \nRecaptioning\nBase prompt: A man and his dog is looking at a \nparrot on the tree\nRegion 0: A contemplative man stands beside \nhis loyal dog, both gazing upward with a sense \nof wonder, the bond between them palpable \nand heartwarming. \nRegion 1: A robust tree stands tall, its branches \na cradle for wildlife, leaves whispering stories of \nthe forest, a symbol of life and growth. \nRegion 2: A vibrant parrot perches alertly , its \nfeather is smooth like blue silk, its sharp eyes \nscanning the world from its lofty vantage point.\nBase prompt: A man and his dog is \nlooking at a parrot on the tree\nRegion 0: An old man with his dog\nRegion 1: A big tree\nRegion 2: A parrot\nBase prompt: A nobleman standing in front of \nhis castle while two owls on a big oak at night \nRegion 0: A bright moon. \nRegion 1: two owls \nRegion 2: A nobleman \nRegion 3: A big oak\nBase prompt: A nobleman standing in front of his \ncastle while two owls on a big oak at night \nRegion 0: A bright moon, full and radiant, casting a \nsilver glow over a tranquil lake, serene and majestic \nin the night sky. \nRegion 1: Two owls, perched side by side, wise and \nmysterious, with piercing eyes, on an ancient, \ngnarled branch under the starlit sky \nRegion 2: A nobleman, regal and distinguished, with \na sharp gaze, dressed in a velvet doublet, standing \nproudly in his ancestral castle \nRegion 3: A big oak, towering and robust, its \nsprawling branches a testament to centuries, leaves \nwhispering stories in the gentle breeze. \nWith \nRecaptioning\nWithout \nRecaptioning\nWith \nRecaptioning\nFigure 14. Ablation study of recaptioning in RPG.\nundesirable results because of the confusion between the\nbase prompt and regional prompt.\n5. Related Work\nText-Guided Diffusion Models\nDiffusion models (Sohl-\nDickstein et al., 2015; Song & Ermon, 2019; Ho et al.,\n2020; Song & Ermon, 2020; Song et al., 2020) are a promis-\ning class of generative models, and Dhariwal & Nichol\n(2021) have demonstrated the superior image synthesis qual-\nity of diffusion model over generative adversarial networks\n(GANs) (Reed et al., 2016; Creswell et al., 2018). GLIDE\n(Nichol et al., 2021) and Imagen (Saharia et al., 2022) focus\non the text-guided image synthesis, leveraging pre-trained\nCLIP model (Radford et al., 2021; Raffel et al., 2020) in the\nimage sampling process to improve the semantic alignment\nbetween text prompt and generated image. Latent Diffusion\nModels (LDMs) (Rombach et al., 2022) move the diffu-\nPrompt: Two beautiful Chinese girls wearing cheongsams are\ndrinking tea in the tea room, and a Chinese Landscape\nPainting is hanging on the wall, the girl on the left is black\nponytail in red cheongsam, the girl on the right is white\nponytail in orange cheongsam\nPrompt: A close up view of a child admiring the fireworks on \nthe sky at night by the lake while crowds of people are going \nto the temple fair in Chinese lunar new year.\nWithout \nCoT Planning\nWith \nCoT Planning\nFigure 15. Ablation study of CoT planning in RPG.\nsion process from pixel space to latent space for balancing\nalgorithm efficiency and image quality. Recent advance-\nments in text-to-image diffusion models , such as SDXL\n(Podell et al., 2023) Dreambooth (Ruiz et al., 2023) and\nDALL-E 3 (Betker et al., 2023), further improve both qual-\nity and alignment from different perspectives. Despite their\ntremendous success, generating high-fidelity images with\ncomplex prompt is still challenging (Ramesh et al., 2022;\nBetker et al., 2023; Huang et al., 2023a). This problem\nis exacerbated when dealing with compositional descrip-\ntions involving spatial relationships, attribute binding and\nnumeric awareness. In this paper, we aim to address this\nissue by incorporating the powerful CoT reasoning ability\nof MLLMs into text-to-image diffusion models.\nCompositional Diffusion Generation\nRecent researches\naim to improve compositional ability of text-to-image diffu-\nsion models. Some approaches mainly introduce additional\nmodules into diffusion models in training (Li et al., 2023b;\nAvrahami et al., 2023; Zhang et al., 2023a; Mou et al., 2023;\nYang et al., 2023e; Huang et al., 2023b;a). For example,\nGLIGEN (Li et al., 2023b) and ReCo (Yang et al., 2023e)\ndesign position-aware adapters on top of the diffusion mod-\nels for spatially-conditioned image generation. T2I-Adapter\nand ControlNet (Zhang et al., 2023a; Mou et al., 2023)\nspecify some high-level features of images for controlling\nsemantic structures (Zhang et al., 2023b). These methods,\nhowever, result in additional training and inference costs.\nTraining-free methods aim to steer diffusion models through\n12\nMastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs\nA white ceramic vase contains a sunflower and a butterfly is flying on \nthe left , and a glass vase contains a pink rose on the right\nNo Base Prompt\nBase Ratio=0.5\nBase Ratio=0.2\nBase Ratio=0.4\nTwo women are negotiating a deal in a meeting room, the woman on the \nleft with blonde blunt bangs is in white suit, the woman on the right\nwith white curly hair is in black suit.\nNo Base Prompt\nBase Ratio=0.5\nBase Ratio=0.2\nBase Ratio=0.4\nFigure 16. Ablation study of base prompt in complementary re-\ngional diffusion.\nmanipulating latent or cross-attention maps according to spa-\ntial or semantic constraints during inference stages (Feng\net al., 2022; Liu et al., 2022; Hertz et al., 2022; Cao et al.,\n2023; Chen et al., 2024; Chefer et al., 2023). Composable\nDiffusion (Liu et al., 2022) decomposes a compositional\nprompt into smaller sub-prompts to generate distinct latents\nand combines them with a score function. Chen et al. (2024)\nand Lian et al. (2023) utilize the bounding boxes (layouts)\nto propagate gradients back to the latent and enable the\nmodel to manipulate the cross-attention maps towards spe-\ncific regions. Other methods apply Gaussian kernels (Chefer\net al., 2023) or incorporate linguistic features (Feng et al.,\n2022; Rassin et al., 2023) to manipulate the cross-attention\nmaps. Nevertheless, such manipulation-based methods can\nonly make rough controls, and often lead to unsatisfied\ncompositional generation results, especially when dealing\nwith overlapped objects (Lian et al., 2023; Cao et al., 2023).\nHence, we introduce an effective training-free complemen-\ntary regional diffusion model, grounded by MLLMs, to\nprogressively refine image compositions with more precise\ncontrol in the sampling process.\nMultimodal LLMs for Image Generation\nLarge Lan-\nguage Models (LLMs) (ChatGPT, 2022; Chung et al., 2022;\nZhang et al., 2022; Iyer et al., 2022; Workshop et al., 2022;\nMuennighoff et al., 2022; Zeng et al., 2022; Taylor et al.,\n2022; Chowdhery et al., 2023; Chen et al., 2023b; Zhu\net al., 2023; Touvron et al., 2023a; Yang et al., 2023a; Li\net al., 2023a) have profoundly impacted the AI commu-\nnity. Leading examples like ChatGPT (ChatGPT, 2022)\nhave showcased the advanced language comprehension and\nreasoning skills through techniques such as instruction tun-\ning (Ouyang et al., 2022; Li et al., 2023c; Zhang et al.,\n2023c; Liu et al., 2023). Further, Multimodal Large lan-\nguage Models (MLLMs), (Koh et al., 2023; Yu et al., 2023;\nSun et al., 2023b; Dong et al., 2023; Fu et al., 2023; Pan\net al., 2023; Wu et al., 2023a; Zou et al., 2023; Yang et al.,\n2023d; Gupta & Kembhavi, 2023; Sur\u00b4\u0131s et al., 2023) inte-\ngrate LLMs with vision models to extend their impressive\nabilities from language tasks to vision tasks, including im-\nage understanding, reasoning and synthesis. The collabora-\ntion between LLMs (ChatGPT, 2022; OpenAI, 2023) and\ndiffusion models (Ramesh et al., 2022; Betker et al., 2023)\ncan significantly improve the text-image alignment as well\nas the quality of generated images (Yu et al., 2023; Chen\net al., 2023b; Dong et al., 2023; Wu et al., 2023b; Feng\net al., 2023; Pan et al., 2023). For instance, GILL (Koh\net al., 2023) can condition on arbitrarily interleaved image\nand text inputs to synthesize coherent image outputs, and\nEmu (Sun et al., 2023b) stands out as a generalist multi-\nmodal interface for both image-to-text and text-to-image\ntasks. Recently, LMD (Lian et al., 2023) utilizes LLMs to\nenhance the compositional generation of diffusion models\nby generating images grounded on bounding box layouts\nfrom the LLM (Li et al., 2023b). However, existing works\nmainly incorporate the LLM as a simple plug-in component\ninto diffusion models, or simply take the LLM as a layout\ngenerator to control image compositions. In contrast, we\nutilize MLLMs to plan out image compositions for diffusion\nmodels where MLLMs serves as a global task planner in\nboth region-based generation and editing process.\n6. Conclusion\nIn this paper, aiming to address the challenges of complex or\ncompositional text-to-image generation, we propose a SOTA\ntraining-free framework RPG, harnessing MLLMs to mas-\nter diffusion models. In RPG, we propose complementary\nregional diffusion models to collaborate with our designed\nMLLM-based recaptioner and planner. Furthermore, our\nRPG can unify text-guided imgae generation and editing in a\nclosed-loop approach, and is capable of generalizing to any\nMLLM architectures and diffusion backbones. For future\nwork, we will continue to improve this new framework for\nincorporating more complex modalities as input condition,\nand extend it to more realistic applications.\nReferences\nAvrahami, O., Hayes, T., Gafni, O., Gupta, S., Taigman, Y.,\nParikh, D., Lischinski, D., Fried, O., and Yin, X. Spa-\ntext: Spatio-textual representation for controllable image\ngeneration. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pp.\n18370\u201318380, 2023.\nBar-Tal, O., Yariv, L., Lipman, Y., and Dekel, T. Multi-\ndiffusion: Fusing diffusion paths for controlled image\ngeneration. arXiv preprint arXiv:2302.08113, 2023.\n13\nMastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs\nBetker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L.,\nOuyang, L., Zhuang, J., Lee, J., Guo, Y., et al. Improving\nimage generation with better captions. Computer Science.\nhttps://cdn. openai. com/papers/dall-e-3. pdf, 2023.\nBrooks, T., Holynski, A., and Efros, A. A. Instructpix2pix:\nLearning to follow image editing instructions. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp. 18392\u201318402, 2023.\nCao, M., Wang, X., Qi, Z., Shan, Y., Qie, X., and Zheng, Y.\nMasactrl: Tuning-free mutual self-attention control for\nconsistent image synthesis and editing. arXiv preprint\narXiv:2304.08465, 2023.\nChatGPT, I. Introducing chatgpt, 2022.\nChefer, H., Alaluf, Y., Vinker, Y., Wolf, L., and Cohen-Or, D.\nAttend-and-excite: Attention-based semantic guidance\nfor text-to-image diffusion models. ACM Transactions\non Graphics (TOG), 42(4):1\u201310, 2023.\nChen, J., Yu, J., Ge, C., Yao, L., Xie, E., Wu, Y., Wang, Z.,\nKwok, J., Luo, P., Lu, H., et al. Pixart-alpha: Fast training\nof diffusion transformer for photorealistic text-to-image\nsynthesis. arXiv preprint arXiv:2310.00426, 2023a.\nChen, M., Laina, I., and Vedaldi, A. Training-free layout\ncontrol with cross-attention guidance. In Proceedings\nof the IEEE/CVF Winter Conference on Applications of\nComputer Vision, pp. 5343\u20135353, 2024.\nChen, W.-G., Spiridonova, I., Yang, J., Gao, J., and Li, C.\nLlava-interactive: An all-in-one demo for image chat,\nsegmentation, generation and editing. arXiv preprint\narXiv:2311.00571, 2023b.\nChiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang,\nH., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E.,\net al. Vicuna: An open-source chatbot impressing gpt-4\nwith 90%* chatgpt quality. See https://vicuna. lmsys. org\n(accessed 14 April 2023), 2023.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\nG., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,\nGehrmann, S., et al. Palm: Scaling language modeling\nwith pathways. Journal of Machine Learning Research,\n24(240):1\u2013113, 2023.\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y.,\nFedus, W., Li, Y., Wang, X., Dehghani, M., Brahma,\nS., et al. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416, 2022.\nCreswell, A., White, T., Dumoulin, V., Arulkumaran, K.,\nSengupta, B., and Bharath, A. A. Generative adversarial\nnetworks: An overview. IEEE signal processing maga-\nzine, 35(1):53\u201365, 2018.\nDhariwal, P. and Nichol, A. Diffusion models beat gans\non image synthesis.\nAdvances in neural information\nprocessing systems, 34:8780\u20138794, 2021.\nDong, R., Han, C., Peng, Y., Qi, Z., Ge, Z., Yang, J., Zhao,\nL., Sun, J., Zhou, H., Wei, H., et al. Dreamllm: Syn-\nergistic multimodal comprehension and creation. arXiv\npreprint arXiv:2309.11499, 2023.\nFang, G., Jiang, Z., Han, J., Lu, G., Xu, H., and\nLiang, X.\nBoosting text-to-image diffusion models\nwith fine-grained semantic rewards.\narXiv preprint\narXiv:2305.19599, 2023.\nFeng, W., He, X., Fu, T.-J., Jampani, V., Akula, A. R.,\nNarayana, P., Basu, S., Wang, X. E., and Wang, W. Y.\nTraining-free structured diffusion guidance for composi-\ntional text-to-image synthesis. In The Eleventh Interna-\ntional Conference on Learning Representations, 2022.\nFeng, W., Zhu, W., Fu, T.-j., Jampani, V., Akula, A., He,\nX., Basu, S., Wang, X. E., and Wang, W. Y. Layout-\ngpt: Compositional visual planning and generation with\nlarge language models. arXiv preprint arXiv:2305.15393,\n2023.\nFu, T.-J., Hu, W., Du, X., Wang, W. Y., Yang, Y., and Gan, Z.\nGuiding instruction-based image editing via multimodal\nlarge language models. arXiv preprint arXiv:2309.17102,\n2023.\nGupta, T. and Kembhavi, A. Visual programming: Compo-\nsitional visual reasoning without training. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 14953\u201314962, 2023.\nHertz, A., Mokady, R., Tenenbaum, J., Aberman, K.,\nPritch, Y., and Cohen-Or, D.\nPrompt-to-prompt im-\nage editing with cross attention control. arXiv preprint\narXiv:2208.01626, 2022.\nHo, J., Jain, A., and Abbeel, P. Denoising diffusion proba-\nbilistic models. Advances in neural information process-\ning systems, 33:6840\u20136851, 2020.\nHuang, K., Sun, K., Xie, E., Li, Z., and Liu, X.\nT2i-\ncompbench: A comprehensive benchmark for open-world\ncompositional text-to-image generation. arXiv preprint\narXiv:2307.06350, 2023a.\nHuang, L., Chen, D., Liu, Y., Shen, Y., Zhao, D., and\nZhou, J. Composer: Creative and controllable image\nsynthesis with composable conditions. arXiv preprint\narXiv:2302.09778, 2023b.\nIyer, S., Lin, X. V., Pasunuru, R., Mihaylov, T., Simig,\nD., Yu, P., Shuster, K., Wang, T., Liu, Q., Koura, P. S.,\net al. Opt-iml: Scaling language model instruction meta\n14\nMastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs\nlearning through the lens of generalization. arXiv preprint\narXiv:2212.12017, 2022.\nKirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C.,\nGustafson, L., Xiao, T., Whitehead, S., Berg, A. C.,\nLo, W.-Y., et al.\nSegment anything.\narXiv preprint\narXiv:2304.02643, 2023.\nKoh, J. Y., Fried, D., and Salakhutdinov, R. Generating\nimages with multimodal language models. arXiv preprint\narXiv:2305.17216, 2023.\nLi, J., Li, D., Savarese, S., and Hoi, S.\nBlip-2: Boot-\nstrapping language-image pre-training with frozen im-\nage encoders and large language models. arXiv preprint\narXiv:2301.12597, 2023a.\nLi, Y., Liu, H., Wu, Q., Mu, F., Yang, J., Gao, J., Li, C.,\nand Lee, Y. J. Gligen: Open-set grounded text-to-image\ngeneration. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 22511\u2013\n22521, 2023b.\nLi, Y., Zhang, C., Yu, G., Wang, Z., Fu, B., Lin, G., Shen,\nC., Chen, L., and Wei, Y. Stablellava: Enhanced visual\ninstruction tuning with synthesized image-dialogue data.\narXiv preprint arXiv:2308.10253, 2023c.\nLian, L., Li, B., Yala, A., and Darrell, T. Llm-grounded dif-\nfusion: Enhancing prompt understanding of text-to-image\ndiffusion models with large language models.\narXiv\npreprint arXiv:2305.13655, 2023.\nLiu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction\ntuning. arXiv preprint arXiv:2304.08485, 2023.\nLiu, N., Li, S., Du, Y., Torralba, A., and Tenenbaum, J. B.\nCompositional visual generation with composable dif-\nfusion models. In European Conference on Computer\nVision, pp. 423\u2013439. Springer, 2022.\nMou, C., Wang, X., Xie, L., Zhang, J., Qi, Z., Shan, Y., and\nQie, X. T2i-adapter: Learning adapters to dig out more\ncontrollable ability for text-to-image diffusion models.\narXiv preprint arXiv:2302.08453, 2023.\nMuennighoff, N., Wang, T., Sutawika, L., Roberts, A., Bi-\nderman, S., Scao, T. L., Bari, M. S., Shen, S., Yong, Z.-X.,\nSchoelkopf, H., et al. Crosslingual generalization through\nmultitask finetuning. arXiv preprint arXiv:2211.01786,\n2022.\nNichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin,\nP., McGrew, B., Sutskever, I., and Chen, M.\nGlide:\nTowards photorealistic image generation and editing\nwith text-guided diffusion models.\narXiv preprint\narXiv:2112.10741, 2021.\nOpenAI, R. Gpt-4 technical report. arxiv 2303.08774. View\nin Article, 2:3, 2023.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\net al. Training language models to follow instructions\nwith human feedback. Advances in Neural Information\nProcessing Systems, 35:27730\u201327744, 2022.\nPan, X., Dong, L., Huang, S., Peng, Z., Chen, W., and\nWei, F. Kosmos-g: Generating images in context with\nmultimodal large language models.\narXiv preprint\narXiv:2310.02992, 2023.\nPodell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn,\nT., M\u00a8uller, J., Penna, J., and Rombach, R. Sdxl: Im-\nproving latent diffusion models for high-resolution image\nsynthesis. arXiv preprint arXiv:2307.01952, 2023.\nQu, L., Wu, S., Fei, H., Nie, L., and Chua, T.-S. Layoutllm-\nt2i: Eliciting layout guidance from llm for text-to-image\ngeneration. In Proceedings of the 31st ACM International\nConference on Multimedia, pp. 643\u2013654, 2023.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,\net al. Learning transferable visual models from natural\nlanguage supervision. In International conference on\nmachine learning, pp. 8748\u20138763. PMLR, 2021.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research,\n21(1):5485\u20135551, 2020.\nRamesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M.\nHierarchical text-conditional image generation with clip\nlatents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.\nRassin, R., Hirsch, E., Glickman, D., Ravfogel, S., Gold-\nberg, Y., and Chechik, G. Linguistic binding in diffusion\nmodels: Enhancing attribute correspondence through at-\ntention map alignment. arXiv preprint arXiv:2306.08877,\n2023.\nReed, S., Akata, Z., Yan, X., Logeswaran, L., Schiele, B.,\nand Lee, H. Generative adversarial text to image synthe-\nsis. In International conference on machine learning, pp.\n1060\u20131069. PMLR, 2016.\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and\nOmmer, B. High-resolution image synthesis with latent\ndiffusion models. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pp.\n10684\u201310695, 2022.\n15\nMastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs\nRuiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., and\nAberman, K. Dreambooth: Fine tuning text-to-image dif-\nfusion models for subject-driven generation. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp. 22500\u201322510, 2023.\nSaharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton,\nE. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan,\nB., Salimans, T., et al. Photorealistic text-to-image dif-\nfusion models with deep language understanding. Ad-\nvances in Neural Information Processing Systems, 35:\n36479\u201336494, 2022.\nSohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and\nGanguli, S. Deep unsupervised learning using nonequi-\nlibrium thermodynamics. In International conference on\nmachine learning, pp. 2256\u20132265. PMLR, 2015.\nSong, Y. and Ermon, S. Generative modeling by estimating\ngradients of the data distribution. Advances in neural\ninformation processing systems, 32, 2019.\nSong, Y. and Ermon, S. Improved techniques for train-\ning score-based generative models. Advances in neural\ninformation processing systems, 33:12438\u201312448, 2020.\nSong, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Er-\nmon, S., and Poole, B. Score-based generative modeling\nthrough stochastic differential equations. arXiv preprint\narXiv:2011.13456, 2020.\nStiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R.,\nVoss, C., Radford, A., Amodei, D., and Christiano,\nP. F. Learning to summarize with human feedback. Ad-\nvances in Neural Information Processing Systems, 33:\n3008\u20133021, 2020.\nSun, J., Fu, D., Hu, Y., Wang, S., Rassin, R., Juan, D.-\nC., Alon, D., Herrmann, C., van Steenkiste, S., Krishna,\nR., et al. Dreamsync: Aligning text-to-image genera-\ntion with image understanding feedback. arXiv preprint\narXiv:2311.17946, 2023a.\nSun, Q., Yu, Q., Cui, Y., Zhang, F., Zhang, X., Wang,\nY., Gao, H., Liu, J., Huang, T., and Wang, X.\nGen-\nerative pretraining in multimodality.\narXiv preprint\narXiv:2307.05222, 2023b.\nSur\u00b4\u0131s, D., Menon, S., and Vondrick, C. Vipergpt: Visual\ninference via python execution for reasoning.\narXiv\npreprint arXiv:2303.08128, 2023.\nTaylor, R., Kardas, M., Cucurull, G., Scialom, T., Hartshorn,\nA., Saravia, E., Poulton, A., Kerkez, V., and Stojnic, R.\nGalactica: A large language model for science. arXiv\npreprint arXiv:2211.09085, 2022.\nTeam, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu,\nJ., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al.\nGemini: a family of highly capable multimodal models.\narXiv preprint arXiv:2312.11805, 2023.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\nM.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\nAzhar, F., et al. Llama: Open and efficient foundation lan-\nguage models. arXiv preprint arXiv:2302.13971, 2023a.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., et al. Llama 2: Open foundation and fine-\ntuned chat models. arXiv preprint arXiv:2307.09288,\n2023b.\nWang, R., Chen, Z., Chen, C., Ma, J., Lu, H., and Lin,\nX. Compositional text-to-image synthesis with atten-\ntion map control of diffusion models. arXiv preprint\narXiv:2305.13921, 2023.\nWorkshop, B., Scao, T. L., Fan, A., Akiki, C., Pavlick, E.,\nIli\u00b4c, S., Hesslow, D., Castagn\u00b4e, R., Luccioni, A. S., Yvon,\nF., et al. Bloom: A 176b-parameter open-access multilin-\ngual language model. arXiv preprint arXiv:2211.05100,\n2022.\nWu, C., Yin, S., Qi, W., Wang, X., Tang, Z., and Duan, N.\nVisual chatgpt: Talking, drawing and editing with visual\nfoundation models. arXiv preprint arXiv:2303.04671,\n2023a.\nWu, T.-H., Lian, L., Gonzalez, J. E., Li, B., and Darrell, T.\nSelf-correcting llm-controlled diffusion models. arXiv\npreprint arXiv:2311.16090, 2023b.\nXie, J., Li, Y., Huang, Y., Liu, H., Zhang, W., Zheng, Y.,\nand Shou, M. Z. Boxdiff: Text-to-image synthesis with\ntraining-free box-constrained diffusion. In Proceedings\nof the IEEE/CVF International Conference on Computer\nVision, pp. 7452\u20137461, 2023.\nXu, J., Liu, X., Wu, Y., Tong, Y., Li, Q., Ding, M., Tang,\nJ., and Dong, Y. Imagereward: Learning and evaluating\nhuman preferences for text-to-image generation. arXiv\npreprint arXiv:2304.05977, 2023.\nYang, A., Xiao, B., Wang, B., Zhang, B., Bian, C., Yin,\nC., Lv, C., Pan, D., Wang, D., Yan, D., et al. Baichuan\n2: Open large-scale language models. arXiv preprint\narXiv:2309.10305, 2023a.\nYang, L., Liu, J., Hong, S., Zhang, Z., Huang, Z., Cai,\nZ., Zhang, W., and Bin, C. Improving diffusion-based\nimage synthesis with context prediction. In Thirty-seventh\nConference on Neural Information Processing Systems,\n2023b.\n16\nMastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs\nYang, L., Zhang, Z., Song, Y., Hong, S., Xu, R., Zhao, Y.,\nZhang, W., Cui, B., and Yang, M.-H. Diffusion models:\nA comprehensive survey of methods and applications.\nACM Computing Surveys, 56(4):1\u201339, 2023c.\nYang, Z., Li, L., Wang, J., Lin, K., Azarnasab, E., Ahmed,\nF., Liu, Z., Liu, C., Zeng, M., and Wang, L. Mm-react:\nPrompting chatgpt for multimodal reasoning and action.\narXiv preprint arXiv:2303.11381, 2023d.\nYang, Z., Wang, J., Gan, Z., Li, L., Lin, K., Wu, C., Duan,\nN., Liu, Z., Liu, C., Zeng, M., et al. Reco: Region-\ncontrolled text-to-image generation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 14246\u201314255, 2023e.\nYu, L., Shi, B., Pasunuru, R., Muller, B., Golovneva, O.,\nWang, T., Babu, A., Tang, B., Karrer, B., Sheynin, S., et al.\nScaling autoregressive multi-modal models: Pretraining\nand instruction tuning. arXiv preprint arXiv:2309.02591,\n2023.\nZeng, A., Liu, X., Du, Z., Wang, Z., Lai, H., Ding, M.,\nYang, Z., Xu, Y., Zheng, W., Xia, X., et al. Glm-130b:\nAn open bilingual pre-trained model.\narXiv preprint\narXiv:2210.02414, 2022.\nZhang, L., Rao, A., and Agrawala, M. Adding conditional\ncontrol to text-to-image diffusion models. In Proceedings\nof the IEEE/CVF International Conference on Computer\nVision, pp. 3836\u20133847, 2023a.\nZhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,\nChen, S., Dewan, C., Diab, M., Li, X., Lin, X. V.,\net al. Opt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068, 2022.\nZhang, T., Zhang, Y., Vineet, V., Joshi, N., and Wang, X.\nControllable text-to-image generation with gpt-4. arXiv\npreprint arXiv:2305.18583, 2023b.\nZhang, Y., Zhang, R., Gu, J., Zhou, Y., Lipka, N., Yang, D.,\nand Sun, T. Enhanced visual instruction tuning for text-\nrich image understanding. In NeurIPS 2023 Workshop on\nInstruction Tuning and Instruction Following, 2023c.\nZhang, Z., Zhang, A., Li, M., Zhao, H., Karypis, G., and\nSmola, A. Multimodal chain-of-thought reasoning in lan-\nguage models. arXiv preprint arXiv:2302.00923, 2023d.\nZhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M.\nMinigpt-4: Enhancing vision-language understanding\nwith advanced large language models. arXiv preprint\narXiv:2304.10592, 2023.\nZou, X., Dou, Z.-Y., Yang, J., Gan, Z., Li, L., Li, C., Dai, X.,\nBehl, H., Wang, J., Yuan, L., et al. Generalized decoding\nfor pixel, image, and language. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 15116\u201315127, 2023.\n17\n"
  },
  {
    "title": "CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark",
    "link": "https://arxiv.org/pdf/2401.11944.pdf",
    "upvote": "24",
    "text": "PREPRINT\nCMMMU:\nA Chinese Massive Multi-discipline Multimodal Understand-\ning Benchmark\nGe Zhang ,1,2\u2217Xinrun Du \u2217Bei Chen9\u2217\nYiming Liang3,4 Tongxu Luo1 Tianyu Zheng ,9 Kang Zhu Yuyang Cheng1,5 Chunpu Xu6\nShuyue Guo9 Haoran Zhang1 Xingwei Qu Junjie Wang1,7 Ruibin Yuan ,1 Yizhi Li ,8\nZekun Wang ,9 Yudong Liu9 Yu-Hsuan Tsai9 Fengji Zhang9\nChenghua Lin ,8\nWenhao Huang ,9\u2020 Wenhu Chen ,2\u2020 Jie Fu ,1\u2020\nMultimodal Art Projection Research Community\n1Hong Kong University of Science and Technology\n2University of Waterloo\n3Institute of Automation, Chinese Academy of Sciences\n4School of Artificial Intelligence, University of Chinese Academy of Sciences\n5Peking University\n6The Hong Kong Polytechnic University\n7Waseda University\n8University of Manchester\n901.AI\n{zhangge,huangwenhao,duxinrun,chenbei}@01.ai\nwenhu.chen@uwaterloo.ca\njiefu@ust.hk\nhttps://cmmmu-benchmark.github.io/\nAbstract\nAs the capabilities of large multimodal models (LMMs) continue to ad-\nvance, evaluating the performance of LMMs emerges as an increasing need.\nAdditionally, there is an even larger gap in evaluating the advanced knowl-\nedge and reasoning abilities of LMMs in non-English contexts such as\nChinese. We introduce CMMMU, a new Chinese Massive Multi-discipline\nMultimodal Understanding benchmark designed to evaluate LMMs on\ntasks demanding college-level subject knowledge and deliberate reason-\ning in a Chinese context. CMMMU is inspired by and strictly follows the\nannotation and analysis pattern of MMMU (Yue et al., 2023). CMMMU\nincludes 12k manually collected multimodal questions from college exams,\nquizzes, and textbooks, covering six core disciplines. These questions span\n30 subjects and comprise 39 highly heterogeneous image types, such as\ncharts, diagrams, maps, tables, music sheets, and chemical structures. CM-\nMMU focuses on complex perception and reasoning with domain-specific\nknowledge in the Chinese context. We evaluate 11 open-source LLMs and\none proprietary GPT-4V(ision). Even GPT-4V only achieves accuracy of\n42%, indicating a large space for improvement. CMMMU aims to enhance\nthe development of next-generation LMMs for expert AI and support LMM\ndemocratization through offering varied language contexts.\n1\nIntroduction\nLarge Multimodal Models (LMMs) have exhibited impressive problem-solving skills in\nmany tasks, e.g., zero-shot image/video classification, zero-shot image/video-text retrieval,\nand multimodal question answering. But Yue et al. (2023); Lu et al. (2023); Deng et al. (2023)\nreveals a significant gap between advanced LMMs and multimodal expert AI, notably\nin complex perception and reasoning within specialized knowledge areas. To close this\ngap, college-level exams for different disciplines are a natural starting point for evaluating\nwhether a Large Language Model (LLM) or an LMM can perform like an expert adult Yue\net al. (2023); Hendrycks et al. (2021); Zhong et al. (2023); Zhang et al. (2023).\nAdditionally, with benchmarks a void, the development of bilingual LMMs has no sense of\ndirection. We fill the gap by proposing CMMMU, a new comprehensive Chinese benchmark\n\u2217These authors contribute equally to the work.\n\u2020Corresponding Authors.\n1\narXiv:2401.11944v2  [cs.CL]  18 Mar 2024\nPREPRINT\ndesigned to evaluate LMMs on massive multi-discipline tasks, guiding the development of\nbilingual LMMs towards a path toward expert-level artificial intelligence.\nAs in Fig. 1, CMMMU, including 12k manually collected Chinese multimodal questions\nfrom college exams, quizzes, and textbooks, covering six core disciplines: Art & Design,\nBusiness, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering,\nis one of the most comprehensive benchmarks for evaluating LMMs\u2019 complex reasoning and\nperception abilities. Each question in CMMMU is further annotated with detailed subfields\nand image types to investigate which types of questions are difficult for LMMs.\nWe provide a comprehensive error analysis of 150 samples, which GPT-4V(ision) an-\nswers incorrectly, evenly distributed among 30 subjects, and covering most cases leading\nthe most advanced LMMs to astray. By evaluating top-performing LMMs, e.g., Qwen-\nVL-Plus and GPT-4V, on CMMMU, we argue that there is still a long way to go to-\nwards an expert-level bilingual LMM. Even the most advanced closed-source LMMs,\nGPT-4V and Qwen-VL-Plus, only achieve accuracies of 43% and 36%, respectively, in-\ndicating significant room for improvement.\nWe further reveal that the gap between\nLMMs released by the open-source community and the most powerful closed-source\nLMMs in a Chinese context is much smaller than in English, as demonstrated in MMMU.\nTechnology & \nEngineering(27%)\nScience(23%)\nBusiness(14%)\nHealth & Medicine(17%)\nArts & Design(10%)\nHumanities & Social Sci.(9%)\nFigure 1: Disciplines of CMMMU.\nFor example, the most powerful open-\nsource LMM, i.e., Yi-VL-34B, achieves an\naccuracy of 36%, with a 7% gap compared\nto GPT-4V, while the gap in English is 11%.\nIn light of the insights obtained while de-\nveloping CMMMU and benchmarking ex-\nisting open-source LMMs, we observe that\nonly Yi-VL-6B1, Yi-VL-34B2, and Qwen-VL-\nChat perform notably better compared to a\nrandom choice setting and are close to GPT-\n4V, while other open-source LMMs perform\nsimilarly to the random choice setting. Sur-\nprisingly, Yi-VL-34B even narrows the gap\nbetween open-source LMMs and GPT-4V\non CMMMU to 7%.\nWe believe CMMMU can benefit the ongoing LMM research and development efforts, and\npromote the democratization of LMMs. Our contributions are summarized as follows:\n\u2022 We introduce CMMMU, the first Chinese Massive Multi-discipline Multimodal\nUnderstanding benchmark.\n\u2022 We reveal that existing LMMs, even including GPT-4V, perform poorly on complex\nreasoning and understanding in a Chinese context.\n\u2022 We examine the gap between open-source bilingual LMMs and closed-source LMMs\nin Chinese, finding it notably narrower than in English contexts.\n2\nRelated Work\n2.1\nMultimodal Benchmark\nTraditionally, multimodal benchmarks are task-oriented, thus not designed to evaluate\nLMMs.and benchmarking relies on tasks that align and utilize representations from various\nmodalities, such as visual question answering (VQA) (Antol et al., 2015b), image caption-\ning (Vinyals et al., 2014), and information retrieval (Wei et al., 2023; Wu et al., 2024). The\nsuccess of building such multimodal tasks and benchmarks heavily relies on large-scale\nannotated datasets like MSCOCO (Lin et al., 2014) and Flickr30k (Plummer et al., 2015).\n1https://huggingface.co/01-ai/Yi-VL-6B\n2https://huggingface.co/01-ai/Yi-VL-34B\n2\nPREPRINT\nSubcategory: \u97f3\u4e50\nSubfield: \u4e50\u7406\u77e5\u8bc6\uff0c\u65cb\u5f8b\nDistribution: \u672c\u79d1\nImage Type: \u4e50\u8c31\nDifficulty Level: Middle\nQuestion:  \u5173\u4e8e\u4e0b\u56fe<\u56fe\u7247 1>\u8fd9\u6bb5\u4e3b\u9898\u65cb\u5f8b\u51fa\u81ea\u4e8e()\nOption:\n(A) \u300a\u6881\u5c71\u4f2f\u4e0e\u795d\u82f1\u53f0\u300b\n(B) \u300a\u65b0\u7586\u4e4b\u6625\u300b\n(C) \u300a\u4e8c\u6cc9\u6620\u6708\u300b\n(D) \u300a\u5361\u95e8\u5e8f\u66f2\u300b\n<\u56fe\u7247 1>\n\u827a\u672f\u8bbe\u8ba1 Arts & Design\nQuestion: \u5224\u65ad\u4e0b\u9762\u9648\u8ff0\u5bf9\u9519\uff1a\u6839\u636e<\u56fe\u7247 1>\u4e0b\u9762\u4e24\u4e2a\u5316\n\u5408\u7269\u7684pKa\u503c\uff0c\u573a\u6548\u5e94\u8d77\u4e3b\u8981\u5f71\u54cd\u3002\nAnswer: \u5bf9\n\u79d1\u5b66 Science\nSubcategory: \u5316\u5b66\nSubfield: \u6709\u673a\u5316\u5b66\uff0c\u5316\u5b66\n\u6027\u8d28\uff0c\u89e3\u79bb\nDistribution: \u8003\u7814\u9898\nImage Type: \u5316\u5b66\u7ed3\u6784\nDifficulty Level: Middle\n<\u56fe\u7247 1>\nQuestion: \u6839\u636e\u4e45\u671f\u53d8\u5316\u56fe<\u56fe\u7247 1>\u53ef\u4ee5\u770b\u51fa\u6536\u76ca\u7387\u540c\u7b49\n\u53d8\u5316\u5e45\u5ea6\u4e0b\uff0c\u503a\u5238\u4ef7\u683c\u589e\u52a0\u7684\u5e45\u5ea6\u8981\u8d85\u8fc7\u503a\u6743\u51cf\u5c11\u7684\u5e45\u5ea6\uff0c\n\u8be5\u503a\u5238\u4ef7\u683c\u6ce2\u52a8\u7b26\u5408__\u7279\u5f81\nOption: \n(A) \u53cd\u51f8\u6027\n(B) \u6b63\u51f8\u6027 \n(C) \u6781\u5927\u503c \n(D) \u6781\u5c0f\u503c\n\u5546\u4e1a Business\nSubcategory: \u91d1\u878d\nSubfield: \u91d1\u878d\u7ba1\u7406\uff0c\u98ce\u9669\n\u7ba1\u7406\nDistribution: \u672c\u79d1\nImage Type: \u56fe\u8868\nDifficulty Level: Middle\n<\u56fe\u7247 1>\nQuestion: \u4e0b\u56fe\u6240\u793a\u7535\u8def\u4e2d\uff0cY\u6052\u4e3a0\u7684\u56fe\u662f\uff08 \uff09\nOption:\n(A) <\u56fe\u7247 1>\n(B) <\u56fe\u7247 2>\n(C) <\u56fe\u7247 3>\n(D) <\u56fe\u7247 4>\n\u6280\u672f\u4e0e\u5de5\u7a0b Technology & Engineering\nSubcategory: \u7535\u5b50\u5b66\nSubfield: \u7535\u5b50\u6280\u672f\uff0c\u7535\u8def\n\u5206\u6790\nDistribution: \u672c\u79d1\nImage Type: \u7535\u8def\u56fe\nDifficulty Level: Easy\n<\u56fe\u7247 1>\n<\u56fe\u7247 3>\n<\u56fe\u7247 2>\n<\u56fe\u7247 4>\nQuestion:\u9488\u523a\u4e0d\u540c\u7a74\u4f4d\u7684\u9547\u75db\u6548\u679c\u5982\u56fe<\u56fe\u7247 1>\uff0c\u5047\u8bbe\nH0\uff1a\u4e09\u7a74\u4f4d\u9547\u75db\u6548\u679c\u7684\u5206\u5e03\u76f8\u540c\uff0c H1 \uff1a\u4e09\u5b66\u4f4d\u9547\u75db\u6548\u679c\n\u7684\u4e0d\u540c\u6216\u4e0d\u5168\u76f8\u540c\uff0ca=0.05\uff0c\u8ba1\u7b97H=___\u3002\nAnswer: 2.212\n\u5065\u5eb7\u4e0e\u533b\u5b66 Health & Medicine\nSubcategory: \u516c\u5171\u536b\u751f\nSubfield: \u536b\u751f\u7edf\u8ba1\u5b66\uff0c\u533b\n\u7597\u4fe1\u606f\u5b66\uff0c\u75c5\u7406\u751f\u7406\u5b66\nDistribution: \u672c\u79d1\nImage Type: \u8868\u683c\nDifficulty Level: Hard\n<\u56fe\u7247 1>\nQuestion: \u8bf7\u6839\u636e\u4e0b\u9762\u6c49\u5b57\u7684\u6f14\u53d8\u8fc7\u7a0b\uff0c\u5199\u51fa\u8be5\u5b57\u7684\u6977\u4e66\n\u5f62\u5f0f\uff08 \uff09\u7532\u9aa8\u6587<\u56fe\u7247 1>\u91d1\u6587<\u56fe\u7247 2>\u7bc6\u4e66<\u56fe\u7247 3>\nAnswer: \u59a5\n\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 Humanities & Social Sci.\nSubcategory: \u6587\u732e\u5b66\nSubfield: \u53e4\u4ee3\u6c49\u8bed\uff0c\u53e4\u6587\n\u5b57\u5b66\nDistribution: \u672c\u79d1\nImage Type: \u4e66\u6cd5\nDifficulty Level: Middle\n<\u56fe\u7247 1>\n<\u56fe\u7247 2>\n<\u56fe\u7247 3>\nFigure 2: CMMMU examples sampled from each discipline. The pictures include music\nscores, tables, chemical structures, curves, circuit diagrams and other types of pictures, and\nthe difficulty of the questions requires expert-level knowledge to understand and reason.\nSome work also evaluates the cross-modal alignment ability with VQA data derived from\ngeneral knowledge bases (Marino et al., 2019; Schwenk et al., 2022).\nA recent line of research attempts to design benchmarks tailored to evaluating LMMs.\nFor example, we can examine the models by requiring them to perceive and learn the\ncomplicated knowledge from the given data distribution, e.g., in the scientific domain (Lu\net al., 2022; Wu et al., 2024). To construct benchmarks compatible with generative LMMs,\nMME (Fu et al., 2023) uses yes-no problems, and MMBench (Liu et al., 2023) is based on\nthe multi-choice format. Some recent studies propose examining whether models can\nperceive and interpret information produced in more challenging scenarios like math\nreasoning (Lu et al., 2023), website interaction Deng et al. (2023), or comprehensive college-\nlevel knowledge reasoning (Yue et al., 2023). Though promising progress in this field of\nmultimodal benchmarking has been made, a dominant ratio of the dataset is in English,\nwhich makes it an urgent gap to build a comprehensive and challenging benchmark in other\nfrequently used languages like Chinese.\n2.2\nBilingual Large Multimodal Models\nDifferent from the development trace of the benchmarks, many of the existing multimodal\nmodels support both English and Chinese due to the integrated bilingual large language\nmodels(LLMs). Although such a statement is established in different models, the cases may\nvary on nuanced features. While multimodal models aim to go beyond the textual data\nby adapting the language models with cross-modality alignment methods, some language\nmodels pre-trained with Chinese-English bilingual corpus are selected as the component for\ntext modeling (Hu et al., 2023; Bai et al., 2023b; Ding et al., 2021; Du et al., 2022; LinkSoul-\nAI, 2023). Although some interesting insights are explored, only a few of the models are\nevaluated on Chinese multimodal tasks. For instance, it is revealed by Hu et al. (2023) that\nmultimodal models trained only with English instruction tuning data work well in Chinese\neven in the zero-shot setting. Another set of models selects the language models adapted\nto Chinese with efficient tuning (Cui et al., 2023). Given the proper alignment architecture\ndesigns and training data selection, these models still show strong performances on bilingual\nmultimodal tasks (Ye et al., 2023; Sun et al., 2023; Chen et al., 2023; Wang et al., 2023; Hong\net al., 2023; LinkSoul-AI, 2023). Moreover, even though the closed-source GPT-4 (Achiam\net al., 2023) does not provide architecture-relevant details, it is a worth mentioning baseline\n3\nPREPRINT\nfor Chinese multimodal benchmarks, given it achieves visual understanding tasks in English\nclose to human-level.\nRegardless of the choice of language models and the training data, many multimodal models\nshow the capability for Chinese tasks at a certain level in practical use. In this work, we\naim to quantitatively measure the ability boundaries of the models with comprehensive\nand challenging Chinese multimodal tasks, as most of them have been only assessed with\nEnglish tasks.\n3\nThe CMMMU Benchmark\nWe introduce the Chinese Massive Multi-discipline Multimodal Understanding (CMMMU)\nbenchmark, a manually curated benchmark covering college-level knowledge to evaluate\nLMMs\u2019 expert-level multimodal understanding capability across a broad scope of tasks. CM-\nMMU is the first multimodal question-answering benchmark in a Chinese context and one\nof the few existing multimodal benchmarks investigating LMMs\u2019 complex understanding\nand reasoning capacities.\nImage Type\n#Num\nImage Type\n#Num\nImage Type\n#Num\n\u5e7f\u544a\nAdvertisement\n4\n\u5386\u53f2\u65f6\u95f4\u7ebf\nHistorical Timeline\n6\n\u4eba\u4f53\u626b\u63cf\nBody Scan\n9\n\u7535\u529b\u5b66\u7b26\u53f7\nElectrical Symbols\n10\nDNA\u5e8f\u5217\nDNA Sequence\n13\n\u6570\u5b66\u7b26\u53f7\nMathematical Symbols\n21\n\u6807\u5fd7\u548c\u54c1\u724c\u5f62\u8c61\nLogos and Brand Identity\n22\n\u98ce\u666f\u753b\nLandscape Painting\n23\n3D\u6e32\u67d3\u56fe\n3D Rendering\n24\n\u5929\u6587\u56fe\u50cf\nAstronomical Images\n31\n\u56fe\u6807\u548c\u7b26\u53f7\nIcons and Symbols\n31\n\u5176\u4ed6\nOther\n39\n\u6d77\u62a5\nPoster\n47\n\u6811\u5f62\u56fe\nTree Diagram\n54\n\u96d5\u5851\nSculpture\n67\n\u4e66\u6cd5\nCalligraphy\n72\n\u6709\u5411\u56fe\nDirected Graph\n82\n\u5730\u56fe\nMap\n85\n\u5efa\u7b51\u8bbe\u8ba1\u56fe\nArchitectural Design Drawing\n94\n\u75c5\u7406\u56fe\u50cf\nPathology Images\n99\n\u673a\u68b0\u5de5\u7a0b\u56fe\nMechanical Engineering Drawings\n107\n\u6d41\u7a0b\u56fe\nFlowchart\n128\n\u4e50\u8c31\nSheet Music\n137\n\u7cfb\u7edf\u6846\u56fe\nSystem Diagram\n174\n\u6f2b\u753b\u548c\u5361\u901a\nCartoons and Comics\n209\n\u8096\u50cf\nPortrait\n235\n\u7ed8\u753b\u4f5c\u54c1\nArtwork\n286\n\u5c4f\u5e55\u622a\u56fe\nScreenshot\n301\n\u673a\u68b0\u7ed3\u6784\u56fe\nMechanical Structure Diagram\n339\n\u51e0\u4f55\u5f62\u72b6\nGeometric Shapes\n346\n\u663e\u5fae\u955c\u56fe\u50cf\nMicroscope Image\n416\n\u533b\u5b66\u56fe\u50cf\nMedical Images\n491\n\u5de5\u7a0b\u7ed3\u6784\u56fe\nEngineering Structural Diagram\n517\n\u7535\u8def\u56fe\nCircuit Diagram\n557\n\u5316\u5b66\u7ed3\u6784\nChemical Structures\n676\n\u56fe\u8868\nCharts\n851\n\u7167\u7247\nPhotographs\n1680\n\u8868\u683c\nTable\n2480\n\u8349\u56fe\nSketches\n3180\nTable 1: Image type and corresponding number.\n3.1\nData Curation Process\nData Collection: We carefully design a three-stage data collection procedure. In Stage 1,\nannotator organizers (mainly the authors) collect sources satisfying license requirements in\nthe format of website links or book titles. The annotator organizers are well instructed to\nadhere to copyright and license regulations, avoiding data from sites prohibiting copying\nand redistribution. We collect at least 20 annotation sources, i.e., websites or books, for each\nsubject in each discipline. In Stage 2, annotator organizers forward the annotation sources\nto the crowdsourcing annotators for further annotation. All annotators are undergraduate\nstudents or have higher degrees to ensure they can verify the annotated questions and\nrelated explanations. During the annotation process, we ask the annotators to strictly\nfollow several key principles to filter out unqualified questions with images: (1) Questions\n4\nPREPRINT\nDataset\nSize\nImages\nFormat\nSource\nAnswer\nVQA (Antol et al., 2015a)\n> 1M\nV\nI+T\nAnnotated\nOpen\nGQA (Hudson & Manning, 2019)\n> 1M\nV\nI+T\nSynthesized\nOpen\nVizWiz (Gurari et al., 2018)\n32K\nV\nI+T\nAnnotated\nOpen\nTextVQA (Ganz et al., 2023)\n45K\nOC\nI+T\nAnnotated\nMC\nOKVQA (Marino et al., 2019)\n14K\nV+OC\nI+T\nAnnotated\nOpen\nSEED (Li et al., 2023)\n19K\nV+OC\nI+T\nAnnotated\nMC\nMMBench (Liu et al., 2023)\n3K\nV+OC\nI+T\nRepurposed\nMC\nMM-Vet (Yu et al., 2023)\n0.2K\nV+OC\nI+T\nAnnotated\nOpen\nScienceQA (Lu et al., 2022)\n6K\n5 Types\nI+T\nTextbooks\nMC\nMathVista (Lu et al., 2023)\n6K\nV+OC\nI+T\nSynthesized\nMC/Open\nMMMU (Yue et al., 2023)\n11.5K\n30 Types\nInterleaved\nTextbooks\nInternet\nAnnotated\nOpen\nMC\nCMMMU\n12K\n39 Types\nInterleaved\nTextbooks\nInternet\nAnnotated\nOpen\nMC\nT/F\nTable 2: Comparison with other benchmarks. V: visual input, OC: optical characters, I+T:\nimages and text, Open: open questions, MC: multiple choice questions, FIB: fill in the blank\nquestions, T/F: true or false questions.\nthat can be answered without the images should be filtered out. (2) Questions that use\nthe same image should be filtered out as much as possible. (3) Questions not requiring\nexpert knowledge to answer should be filtered out as much as possible. (4) The number of\nquestions that are about the same specific knowledge point and have similar question angles\nshould not exceed 10. We also ask annotators to follow the data annotation protocol in the\nAppendix.G of Yue et al. (2023). In Stage 3, annotator organizers additionally supplement\nquestions to subjects that lack questions, e.g., Arts, Diagnostics, and Economics, to balance\nthe datasets.\nData Quality Control: To further improve the data quality of CMMMU, we follow a strict\ndata quality control protocol. First, each question is manually verified by at least one of the\npaper\u2019s authors. We carefully filter out questions with answers that are too hard to extract\nfrom the responses generated by LMMs. During the process, we also carefully filter out all\nthe questions that are not up to college-level examinations. Second, given the concern of\ndata contamination, we filter out all the questions that can be correctly solved by GPT-4,\nQwen-7B, Deepseek-7B, and Yi-7B simultaneously without the assistance of OCR. Some\nexample questions are shown in Fig. 2.\n3.2\nComparison with Existing Benchmarks\nWe compare CMMMU with existing multimodal benchmarks in Tab. 2.From the input\nimage type, the common image formats in the benchmark can be roughly divided into\nthree simple categories: visual input (V), optical characters (OC) and V+OC. In addition,\nthere are 5 types of image formats in the ScienceQA benchmark. CMMMU benchmark\nhas 39 types as in Tab. 1, involving charts, tables, diagrams, chemical structures, photos,\npaintings, geometric shapes, musical scores, and medical images. Concerning the input\nformat, existing benchmarks generally exhibit a relatively independent relationship between\nthe input images and text(I+T). In the CMMMU benchmark, images and text are interleaved,\nestablishing a markedly tighter connection. In terms of question types, most of the common\nbenchmarks are open questions (Open) or multiple choice questions (MC). CMMMU not\nonly contains open-ended questions and multiple choice questions, but also adds judgment\nquestions to enrich the question types. In terms of knowledge depth, previous benchmarks\ntypically require common sense or simple physical or temporal reasoning. In contrast, our\nproposed CMMMU benchmark requires thoughtful reasoning with university-level subject\nknowledge.\n5\nPREPRINT\nStatistics\nNumber\nTotal Questions\n12012\nDisciplines/Subjects/Subfields\n6/30/4165\nImage Types\n39\nDev:Validation:Test\n112:900:11000\nEasy: Medium: Hard\n30%:58%:12%\nAverage question length\n51.12\nAverage option length\n8.76\nAverage explanation length\n78.29\nStatistics\nNumber\nMultiple-choice Questions\n7738 (64.41%)\nFill in the blank Questions\n2998 (24.95%)\nTrue or false Questions\n1276 (10.62%)\nQuestions with an Explanation\n247 (2.05%)\nImage in the Question\n11760 (84.42%)\nImage in Options\n2169 (15.57%)\nExample with Multiple Images\n597 (4.97%)\nTable 3: Statistics of CMMMU\n3.3\nStatistics of CMMMU\nCMMMU covers 6 disciplines, including Art & Design, Business, Science, Health & Medicine,\nHumanities & Social Science, and Tech & Engineering, spanning over 30 subjects. As Fig.\n3, CMMMU consists of 12K questions, divided into few-shot development set, validation\nset, and test set. The few-shot development set comprises 5 questions for each topic, the\nvalidation set aids in hyperparameter selection with 900 questions, and the test set includes\n11K questions.\nThe pictures include 39 types such as pathological diagrams, musical scores, circuit diagrams,\nand chemical structure diagrams. We categorized the data as Easy (30%), Medium (58%),\nand Hard (12%) by logical difficulty rather than intellectual difficulty. According to the\nquestion type, there are 7738 multiple choice questions, 2998 fill-in-the-blank questions, and\n1276 judgment questions. Of these examples, 11,760 are images in the question, 2169 are\nimages in the option, and 597 are images with multiple images. The average question length\nis approximately 51 words, the average option length is about 9 words, and the average\nexplanation length is around 78 words.\nValidation\nOverall\nTest\nOverall\nArt &\nDesign\nBusiness\nScience\nHealth &\nMedicine\nHuman. &\nSocial Sci.\nTech &\nEng.\n(900)\n(11,000)\n(1,091)\n(1,538)\n(2,494)\n(1,865)\n(1,038)\n(2,974)\nRandom Choice\n21.6\n21.6\n32.9\n9.1\n18.8\n23.8\n23.8\n23.9\nFrequent Choice\n24.1\n26.0\n36.2\n11.8\n23.9\n30.2\n28.5\n27.7\nLMMs: Text + Image as Input\nmPLUG-Owl2\n20.8\n22.2\n30.4\n13.3\n19.6\n25.2\n24.7\n23.4\nVisCPM\n25.2\n22.7\n37.7\n11.3\n19.1\n26.1\n24.0\n23.7\nChinese-LLaVA\n25.5\n23.4\n34.4\n11.7\n21.6\n25.5\n26.3\n24.7\nEmu2-Chat\n23.8\n24.5\n35.3\n11.7\n22.1\n25.5\n28.0\n27.1\nCogAgent-Chat\n24.6\n23.6\n33.8\n14.1\n20.6\n26.3\n24.8\n25.3\nQwen-VL-Chat\n30.7\n31.3\n52.6\n18.5\n26.9\n33.4\n34.1\n31.4\nInternVL-Chat-V1.1\n34.7\n34.0\n56.7\n19.7\n28.6\n39.2\n39.6\n32.3\nYi-VL-6B\n35.8\n35.0\n58.0\n19.9\n32.3\n39.3\n40.6\n32.1\nYi-VL-34B\n36.2\n36.5\n62.9\n19.1\n31.5\n42.1\n42.5\n34.5\nQwen-VL-Plus\n39.5\n36.8\n61.5\n23.2\n32.8\n40.5\n43.4\n33.3\nGPT-4V\n42.5\n43.7\n61.0\n36.3\n40.9\n46.8\n44.2\n41.5\nLLMs: Only Text as Input\nDeepSeek-7B\n22.3\n21.9\n41.3\n11.2\n18.3\n23.5\n24.7\n21.3\nBaichuan-7B\n26.0\n24.3\n42.7\n12.6\n19.6\n28.0\n27.8\n23.9\nQwen-7B\n24.7\n25.1\n43.8\n12.6\n20.7\n30.5\n26.9\n24.5\nYi-6B\n25.6\n24.2\n26.3\n15.0\n23.4\n29.1\n27.0\n24.7\nDeepSeek-7B + OCR\n25.2\n23.2\n41.2\n13.2\n19.4\n26.1\n26.5\n21.8\nBaichuan-7B + OCR\n25.3\n24.7\n40.2\n15.2\n21.0\n27.9\n30.7\n22.8\nQwen-7B + OCR\n27.0\n26.1\n44.6\n14.3\n22.1\n29.3\n29.8\n25.4\nYi-6B + OCR\n28.4\n26.8\n33.4\n16.9\n24.8\n32.3\n33.2\n25.5\nTable 4: Overall results of open-source and closed-source models on the CMMMU validation\nand test set. bold results in LMMs indicate the best results for all models, and the blue\nresults indicate the best results among the open-source models.\n6\nPREPRINT\n4\nExperiments\nWe perform a comprehensive evaluation of various models, including LLMs and LMMs,\nwith considering both closed-source and open-source implementations. The evaluation\nprocess employs zero-shot settings, rather than fine-tuning or few-shot settings, to examine\nthe raw ability of the model to generate accurate answers on multimodal tasks. For models\nwith corresponding task prompts, we use the default prompts for either multiple-choice or\nopen-ended question-answering tasks. As for models without corresponding task prompts,\nwe use the same task prompts, which are hand-picked on the validation set. In addition,\nwe also test the results of some models on few-shot settings, which are documented in the\nAppendix. All these experiments are performed on NVIDIA A100 GPUs.\n4.1\nBaselines\nLMMs. We consider the current mainstream Chinese-English bilingual large multimodal\nmodels. We use each model\u2019s official API (closed-source) or official checkpoint (open-\nsource) published on the huggingface website. Baselines includes: (1) mPLUG-Owl2 (Ye\net al., 2023) employs a modular network design with a language decoder as a common\ninterface for managing different modalities, effectively exploiting modal collaboration to\nimprove performance in textual and multimodal tasks. (2) VisCPM (Hu et al., 2023) is\ntrained based on the large language model CPM-Bee with 10B parameters, fusing visual\nencoder (Q-Former) and visual decoder (Diffusion-UNet) to support visual inputs and\noutputs. (3) Chinese-LLaVA (LinkSoul-AI, 2023) uses Chinese Llama2 as the language\nmodel base, plus image understanding capabilities. The work follows the structure of LLaVA\nwith a two-stage training using Chinese data. (4) Emu2 (Sun et al., 2023) is a generative\nmultimodal model with 37 billion parameters that performs well in few-shot Settings. (5)\nCogAgent (Hong et al., 2023) is a 180 billion-parameter Vision-Language Model designed\nfor GUI comprehension and navigation. (6) Qwen-VL (Bai et al., 2023b) uses Qwen-7B\nas the initialization of the LLM, and Openclip ViT-bigG as the initialization of the visual\nencoder. And connects them with a randomly initialized cross-attention layer. We choose\nQWen-VL-Chat and QWen-VL-plus. (7) InternVL (Chen et al., 2023) scales up the Vision\nTransformer (ViT) to 6B parameters and aligns it with LLM. There are multimodal models\nwith varying sizes of language models within the InternVL series, including InternVL-Chat-\nvit-6B-Vicuna-7B, InternVL-Chat-vit-6B-Vicuna-13B, InternVL-Chat-vit-6B-Llama2-13B, and\nInternVL-Chat-V1.1. (8) GPT-4V 3 is a closed-source large multimodal model from OpenAI\nthat accepts image and text inputs and emits text outputs, demonstrating human-level\nperformance on a variety of professional and academic benchmarks. (9) Yi-VL-6B and\nYi-VL-34B are our multimodal models, providing image understanding capabilities to large\nlanguage models. In these models, Vit is the Openclip 224, and the language model is either\nYi-6B-Chat or Yi-34B-Chat.\nModels\nQuestion Type\nQuestion Difficulty\nOverall\nMC\nFIB\nT/F\nEasy\nMedium\nHard\nmPLUG-Owl2\n22.9\n7.0\n53.8\n25.5\n20.8\n20.7\n22.2\nVisCPM\n24.5\n5.4\n52.8\n26.8\n21.1\n20.1\n22.7\nChinese-LLaVA\n25.6\n5.4\n52.7\n25.5\n26.3\n24.7\n23.4\nEmu2\n28.4\n2.9\n51.4\n28.0\n22.4\n25.1\n24.5\nCogAgent\n25.9\n5.9\n51.9\n27.7\n21.7\n22.7\n23.6\nInternVL-Chat-V1.1\n36.7\n14.4\n63.5\n41.8\n30.8\n29.4\n34.0\nYi-VL-6B\n40.8\n11.7\n54.9\n43.3\n31.6\n30.3\n35.0\nYi-VL-34B\n42.5\n10.4\n61.6\n45.6\n32.6\n31.9\n36.5\nQwen-VL-Plus\n42.9\n15.7\n49.4\n46.7\n32.9\n29.9\n36.8\nGPT-4V\n46.4\n27.4\n66.0\n51.5\n40.7\n38.3\n43.7\nTable 5: Combined result decomposition across question type and difficulty level. MC:\nmultiple choice questions, FIB: fill in the blank questions, T/F: true or false questions.\n3https://openai.com/research/gpt-4v-system-card\n7\nPREPRINT\nText-only LLMs. We evaluate the performance of LLMs (e.g., GPT44, Qwen-7B (Bai et al.,\n2023a), Deepseek-7B (DeepSeek-AI et al., 2024), Yi-6B5) when dealing with plain text, and\nBaichuan-7B on multimodal data. In addition, to verify whether external image tools can\nenhance the performance of LLMs on multimodal data, we deploy OCR by Mathpix 6\nprocessing images to convert certain image information into textual forms.\nEvaluation. We build a systematic and rule-based evaluation pipeline. Robust regular\nexpressions are built to extract answers from the model responses. Specifically, for multiple-\nchoice questions, we directly use options as keywords to extract model responses, and take\nthe one with the highest number of options in the model response as the answer. If there\nis no valid answer in the model\u2019s response, random selection is performed for multiple-\nchoice questions. For the judgment and open-ended question answering questions, we\nutilize specific rules to extract some segments where the answer may occur, and then detect\nwhether the answer occurs in them. We add random selection and frequent selection as\nbaselines: the former randomly selects an option, while the latter selects the most frequent\noption for each specific topic in the validation set based on its frequency of occurrence in\nthat topic. Finally, we adopt micro-average accuracy as the evaluation metric. The prompts\nwe use are in Appendix A.\n4.2\nResults of CMMMU\nIn this section, we present the main result and detailed ablation studies of different LMMs\u2019\nand their performances on the CMMMU benchmark. Results are shown in Tab. 4, 5 and A1.\nWe emphasize our key observations as follows:\n\u2022 CMMMU is much more challenging than MMMU, while MMMU is already\nvery challenging. GPT-4V only achieves an accuracy of 41.7% while it achieves\nan accuracy of 55.7% in an English context. It reveals that existing cross-linguistic\ngeneralization is not good enough even for the most advanced closed-source LMMs.\n\u2022 The disparity between representative open-source models and GPT-4V is rela-\ntively smaller in a Chinese context compared to MMMU. The disparity between\nQwen-VL-Chat and GPT-4V on CMMMU is 13.3% while the disparity between\nBLIP2-FLAN-T5- XXL and GPT-4V on MMMU is 21.9%. Surprisingly, Yi-VL-34B\neven shortens the disparity between open-source bilingual LMMs and GPT-4V on\nCMMMU to 7.5%, meaning that open-source bilingual LMMs hold a candle to\nGPT-4V in a Chinese context, which is a promising progress in the open-source\ncommunity.\n\u2022 The key disparity between open-source LMMs and GPT-4V is the capacity to\ncalculate and reason given complex conditions. Notably, the performance disparity\nbetween Open-source LMMs and GPT-4V of Business, Science, and Tech & Eng is\nlarger compared to other disciplines. More questions require complex reasoning in\nthe three disciplines, which reveals that open-source LMMs cannot calculate and\nreason given complex conditions.\n\u2022 The game of pursuing expert Chinese Multimodal Artificial General Intelligence\n(AGI) has just begun in the open-source community. We point out that all the\nbilingual LMMs from the open-source community only achieve comparable accura-\ncies with the frequent choice setting referring to MMMU, except recently released\nQwen-VL-Chat, Yi-VL-6B, and Yi-VL-34B. These three LMMs trigger the first shot\nfor the race of Chinese Multimodal AGI.\nWe conduct result decomposition across question difficulties, as shown in Tab. ??. Notably,\nthere is a larger gap between the best open-source LMM, i.e. Yi-VL-34B, and GPT-4V when\nfacing the medium and hard questions. This is further strong evidence of the observation\nthat the key disparity between open-source LMMs and GPT-4V is the capacity to calculate\nand reason given complex conditions.\n4https://openai.com/research/gpt-4\n5https://huggingface.co/01-ai/Yi-6B-Chat\n6https://mathpix.com/\n8\nPREPRINT\nWe conduct result decomposition across question types, as shown in Tab. ??. We notice\nthat Qwen-VL-Plus does not well on True or False questions, indicating that Qwen-VL-Plus\nmay not understand the prompt for answering True or False questions. It might be a free\nlunch for Qwen-VL-Plus to improve its performance on CMMMU. We further point out that\nthe disparity between Yi-VL Series, Qwen-VL-Plus, and GPT-4V is mainly because of their\ncapacity difference for answering Multiple-choice questions.\n4.3\nError Analysis\nThis section carefully analyzes over 150 examples of GPT-4V\u2019s incorrect answers. As shown\nin the error distribution Fig. 3, several main types of errors are found, such as perceptual\nerrors, lack of knowledge, reasoning errors, rejection to answer, and annotation errors.\nAnalyzing these error types is key to understanding the capabilities and limitations of\ncurrent LMMs, and can also guide future improvements in designing and training models.\nThe 75 examples of correct responses and 150 examples of incorrect responses are detailed\nin Appendix B, and the characteristics of each error type are described next.\nPerceptual Errors (26%): Perceptual errors are one of the primary reasons for the generation\nof erroneous examples by GPT-4V. On one hand, when the model fails to comprehend\narrows and symbols in the image, misinterprets the sequence from top to bottom and left\nto right, it introduces deviations in the basic perception of the image, leading to incorrect\nresponses. On the other hand, when the model encounters ambiguity in domain-specific\nknowledge, hidden meanings, or unclear formulas, it tends to exhibit perceptual errors\nspecific to that domain. In such cases, GPT-4V tends to rely more on answering based on\ntextual information (i.e., the question and options), prioritizing textual information over\nvisual input, causing a bias in understanding multimodal data.\nResoning Errors (26%): Reasoning Error is another major factor contributing to the genera-\ntion of erroneous examples by GPT-4V. On the one hand, reasoning errors arise when the\nmodel receives incorrect information, often stemming from the perceptual errors mentioned\nearlier, such as in the illustration of Fig. B149, where the model fails to perceive the hidden\nmeaning of symbols, leading to erroneous inferences and outputs. On the other hand, even\nif the model correctly perceives the meaning conveyed by the image and text, errors in\nthe reasoning process can occur when solving problems that require complex logical and\nmathematical reasoning. Typically, such errors result from the model\u2019s weaker logical and\nmathematical reasoning capabilities.\nLack of Knowledge (22%): The lack of expertise is also one of the reasons why GPT-\n4V generates erroneous responses. The example in Fig. B150 shows GPT-4V producing\nincorrect answers due to the lack of corresponding physics knowledge. Since CMMMU is\nfor evaluating expert AGI of LMMs, expert-level knowledge in different disciplines and\nsubfields is required. So, injecting expert-level knowledge into LMMs is also one of the\ndirections that can be worked towards AGI.\nFigure 3: GPT-4V error response distribution.\nRejection (12%): The phenomenon of the\nmodel refusing to answer, resulting in in-\ncorrect responses, is also a common occur-\nrence. Through analysis, we have identified\nseveral reasons for the model\u2019s refusal to\nanswer: (i) The model fails to perceive in-\nformation from the image, and the textual\ninformation in the question is insufficient,\ncausing the model to wait for more informa-\ntion. (ii) Questions involving religious mat-\nters or personal real-life information lead\nthe model to refrain from answering, adher-\ning to human values. (iii) When questions\ninvolve gender and subjective matters, the\nmodel avoids providing accurate responses.\n9\nPREPRINT\nOther Errors: The remaining errors are text comprehension errors (7%), annotation errors\n(2%), and answer extraction errors (5%). These errors are attributed to various factors\nsuch as complex instruction following ability, complex text logic understanding, limitations\nin response generation, errors in data annotation, and problems encountered in answer\nmatching extraction.\n5\nConclusion\nCMMMU represents a significant stride in developing AGI. The CMMMU\u2019s design is tailored\nto rigorously evaluating the latest LMMs, and testing elementary perceptual skills, intricate\nlogical reasoning, and profound expertise in specific domains. We reveal the disparity\nbetween the reasoning capacity of the most advanced bilingual LMMs in a Chinese context\nand an English context by comparing LMMs\u2019 performance on CMMMU and MMMU. Such\nan exhaustive assessment is pivotal for delineating the trajectory towards achieving AGI\nthat parallels the proficiency of seasoned professionals in various fields.\nEthics Policy\nIn developing the CMMMU benchmark, we strictly adhere to ethical and legal standards,\nensuring that our data collection and usage comply fully with pertinent ethical guidelines\nand legal regulations. Our dedication to promoting fairness, inclusivity, and diversity in\nour dataset is critical, aiming to reduce biases that might exacerbate societal disparities.\nWe emphasize the importance of protecting privacy and intellectual property rights, high-\nlighting our commitment to responsible and lawful data management. This methodology\nreflects our steadfast commitment to ethical integrity and legal compliance in the pursuit of\nadvancing research in multimodal understanding and reasoning.\nLimitations\nWe recognize the inherent limitations of our study. Although the CMMMU benchmark is\ncomprehensive, it does not encompass the entire range of human knowledge and cognitive\nskills. It is primarily focused on college-level content, which, despite its complexity, covers\nonly a portion of human expertise. Additionally, our evaluation metrics, despite their robust-\nness, might not completely grasp the sophisticated understanding and reasoning abilities\nof advanced AI systems. These limitations highlight the iterative process of our research,\nmotivating ongoing refinement and expansion of our benchmarks to more accurately reflect\nthe complexities of human cognition and learning. Moreover, the challenges of conducting\nmanual experiments with contracted experts for question-answering tasks have precluded\nus from offering a comprehensive score for human experiments to date. Nonetheless, we\nare contemplating the inclusion of such scores in future updates.\nReferences\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni\nAleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.\nGpt-4 technical report. ArXiv preprint, abs/2303.08774, 2023. URL https://arxiv.org/\nabs/2303.08774.\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,\nC. Lawrence Zitnick, and Devi Parikh. VQA: visual question answering. In 2015 IEEE\nInternational Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13,\n2015, pp. 2425\u20132433. IEEE Computer Society, 2015a. doi: 10.1109/ICCV.2015.279. URL\nhttps://doi.org/10.1109/ICCV.2015.279.\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,\nC. Lawrence Zitnick, and Devi Parikh. VQA: visual question answering. In 2015 IEEE\nInternational Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13,\n10\nPREPRINT\n2015, pp. 2425\u20132433. IEEE Computer Society, 2015b. doi: 10.1109/ICCV.2015.279. URL\nhttps://doi.org/10.1109/ICCV.2015.279.\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin\nGe, Yu Han, Fei Huang, et al. Qwen technical report. ArXiv preprint, abs/2309.16609,\n2023a. URL https://arxiv.org/abs/2309.16609.\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang\nZhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding,\nlocalization, text reading, and beyond. ArXiv preprint, abs/2308.12966, 2023b. URL\nhttps://arxiv.org/abs/2308.12966.\nZhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong,\nQinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng\nDai. Internvl: Scaling up vision foundation models and aligning for generic visual-\nlinguistic tasks. ArXiv preprint, abs/2312.14238, 2023. URL https://arxiv.org/abs/\n2312.14238.\nYiming Cui, Ziqing Yang, and Xin Yao. Efficient and effective text encoding for chinese\nllama and alpaca. ArXiv preprint, abs/2304.08177, 2023. URL https://arxiv.org/abs/\n2304.08177.\nDeepSeek-AI, :, Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi\nDeng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun\nGao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao,\nYing He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y. K. Li,\nWenfeng Liang, Fangyun Lin, A. X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan\nLiu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao,\nJunjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao,\nJunxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang,\nPeiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda\nXie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R. X. Xu, Yanhong Xu, Dejian Yang, Yuxiang\nYou, Shuiping Yu, Xingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang,\nMingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao,\nYao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, and Yuheng Zou. Deepseek llm:\nScaling open-source language models with longtermism. arXiv preprint arXiv: 2401.02954,\n2024.\nXiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan\nSun, and Yu Su. Mind2web: Towards a generalist agent for the web. ArXiv preprint,\nabs/2306.06070, 2023. URL https://arxiv.org/abs/2306.06070.\nMing Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin,\nXu Zou, Zhou Shao, Hongxia Yang, and Jie Tang. Cogview: Mastering text-to-image gen-\neration via transformers. In Marc\u2019Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin,\nPercy Liang, and Jennifer Wortman Vaughan (eds.), Advances in Neural Information Process-\ning Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS\n2021, December 6-14, 2021, virtual, pp. 19822\u201319835, 2021. URL https://proceedings.\nneurips.cc/paper/2021/hash/a4d92e2cd541fca87e4620aba658316d-Abstract.html.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang.\nGLM: General language model pretraining with autoregressive blank infilling. In Proceed-\nings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pp. 320\u2013335, Dublin, Ireland, 2022. Association for Computational Linguistics. doi:\n10.18653/v1/2022.acl-long.26. URL https://aclanthology.org/2022.acl-long.26.\nChaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang,\nXiawu Zheng, Ke Li, Xing Sun, et al. Mme: A comprehensive evaluation benchmark\nfor multimodal large language models. ArXiv preprint, abs/2306.13394, 2023. URL\nhttps://arxiv.org/abs/2306.13394.\n11\nPREPRINT\nRoy Ganz, Oren Nuriel, Aviad Aberdam, Yair Kittenplon, Shai Mazor, and Ron Litman.\nTowards models that can see and read. IEEE International Conference on Computer Vision,\n2023. doi: 10.1109/ICCV51070.2023.01985.\nDanna Gurari, Qing Li, Abigale J. Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo\nLuo, and Jeffrey P. Bigham. Vizwiz grand challenge: Answering visual questions from\nblind people. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR\n2018, Salt Lake City, UT, USA, June 18-22, 2018, pp. 3608\u20133617. IEEE Computer Society,\n2018. doi: 10.1109/CVPR.2018.00380. URL http://openaccess.thecvf.com/content_\ncvpr_2018/html/Gurari_VizWiz_Grand_Challenge_CVPR_2018_paper.html.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song,\nand Jacob Steinhardt. Measuring massive multitask language understanding. In 9th\nInternational Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-\n7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=d7KBjmI3GmQ.\nWenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang,\nZihan Wang, Yuxiao Dong, Ming Ding, and Jie Tang. Cogagent: A visual language model\nfor gui agents, 2023.\nJinyi Hu, Yuan Yao, Chongyi Wang, Shan Wang, Yinxu Pan, Qianyu Chen, Tianyu Yu,\nHanghao Wu, Yue Zhao, Haoye Zhang, Xu Han, Yankai Lin, Jiao Xue, Dahai Li, Zhiyuan\nLiu, and Maosong Sun. Large multilingual models pivot zero-shot multimodal learning\nacross languages. 2023.\nDrew A. Hudson and Christopher D. Manning. GQA: A new dataset for real-world visual\nreasoning and compositional question answering. In IEEE Conference on Computer Vision\nand Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pp. 6700\u20136709.\nComputer Vision Foundation / IEEE, 2019. doi: 10.1109/CVPR.2019.00686. URL http:\n//openaccess.thecvf.com/content_CVPR_2019/html/Hudson_GQA_A_New_Dataset_\nfor_Real-World_Visual_Reasoning_and_Compositional_CVPR_2019_paper.html.\nBohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench:\nBenchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:\n2307.16125, 2023.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan,\nPiotr Doll\u00b4ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In\nComputer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12,\n2014, Proceedings, Part V 13, pp. 740\u2013755. Springer, 2014.\nLinkSoul-AI. Chinese llava. https://github.com/LinkSoul-AI/Chinese-LLaVA, 2023.\nYuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike\nYuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an\nall-around player? ArXiv preprint, abs/2307.06281, 2023. URL https://arxiv.org/abs/\n2307.06281.\nPan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind\nTafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via\nthought chains for science question answering. Advances in Neural Information Processing\nSystems, 35:2507\u20132521, 2022.\nPan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao\nCheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathe-\nmatical reasoning of foundation models in visual contexts. ArXiv preprint, abs/2310.02255,\n2023. URL https://arxiv.org/abs/2310.02255.\nKenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. OK-VQA:\nA visual question answering benchmark requiring external knowledge.\nIn IEEE\nConference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA,\nUSA, June 16-20, 2019, pp. 3195\u20133204. Computer Vision Foundation / IEEE, 2019.\n12\nPREPRINT\ndoi: 10.1109/CVPR.2019.00331. URL http://openaccess.thecvf.com/content_CVPR_\n2019/html/Marino_OK-VQA_A_Visual_Question_Answering_Benchmark_Requiring_\nExternal_Knowledge_CVPR_2019_paper.html.\nBryan A. Plummer, Liwei Wang, Chris M. Cervantes, Juan C. Caicedo, Julia Hockenmaier,\nand Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for\nricher image-to-sentence models. In 2015 IEEE International Conference on Computer Vision,\nICCV 2015, Santiago, Chile, December 7-13, 2015, pp. 2641\u20132649. IEEE Computer Society,\n2015. doi: 10.1109/ICCV.2015.303. URL https://doi.org/10.1109/ICCV.2015.303.\nDustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh\nMottaghi. A-okvqa: A benchmark for visual question answering using world knowledge.\nIn European Conference on Computer Vision, pp. 146\u2013162. Springer, 2022.\nQuan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze\nWang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative mul-\ntimodal models are in-context learners. ArXiv preprint, abs/2312.13286, 2023. URL\nhttps://arxiv.org/abs/2312.13286.\nOriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural\nimage caption generator. corr abs/1411.4555 (2014). arXiv preprint arXiv:1411.4555, 2014.\nWeihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi\nYang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding,\nand Jie Tang. Cogvlm: Visual expert for pretrained language models, 2023.\nCong Wei, Yang Chen, Haonan Chen, Hexiang Hu, Ge Zhang, Jie Fu, Alan Ritter, and Wenhu\nChen. Uniir: Training and benchmarking universal multimodal information retrievers.\nArXiv preprint, abs/2311.17136, 2023. URL https://arxiv.org/abs/2311.17136.\nSiwei Wu, Yizhi Li, Kang Zhu, Ge Zhang, Yiming Liang, Kaijing Ma, Chenghao Xiao, Haoran\nZhang, Bohao Yang, Wenhu Chen, Wenhao Huang, Noura Al Moubayed, Jie Fu, and\nChenghua Lin. Scimmir: Benchmarking scientific multi-modal information retrieval,\n2024.\nQinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang,\nand Jingren Zhou. mplug-owl2: Revolutionizing multi-modal large language model with\nmodality collaboration. ArXiv preprint, abs/2311.04257, 2023. URL https://arxiv.org/\nabs/2311.04257.\nWeihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao\nWang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated\ncapabilities. arXiv preprint arXiv: 2308.02490, 2023.\nXiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens,\nDongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline\nmultimodal understanding and reasoning benchmark for expert agi. ArXiv preprint,\nabs/2311.16502, 2023. URL https://arxiv.org/abs/2311.16502.\nXiaotian Zhang, Chunyang Li, Yi Zong, Zhengyu Ying, Liang He, and Xipeng Qiu. Evalu-\nating the performance of large language models on gaokao benchmark. ArXiv preprint,\nabs/2305.12474, 2023. URL https://arxiv.org/abs/2305.12474.\nWanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin\nSaied, Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating\nfoundation models. ArXiv preprint, abs/2304.06364, 2023. URL https://arxiv.org/abs/\n2304.06364.\n13\nPREPRINT\nA\nAppendix\nIn experiments, the prompts we use and their corresponding question types are as follows:\nMultiple-choice questions: \u8bf7\u56de\u7b54\u4ee5\u4e0b\u591a\u9879\u9009\u62e9\u9898\uff0c\u5e76\u9009\u51fa\u6b63\u786e\u9009\u9879\u3002\u8fd9\u4e9b\u9898\u76ee\u53ef\u80fd\u5305\n\u62ec\u5355\u9009\u548c\u591a\u9009\u9898\u578b\u3002\u5982\u679c\u6240\u63d0\u4f9b\u7684\u4fe1\u606f\u4e0d\u8db3\u4ee5\u786e\u5b9a\u4e00\u4e2a\u660e\u786e\u7684\u7b54\u6848\uff0c\u90a3\u4e48\u8bf7\u6839\u636e\u53ef\u7528\u7684\n\u6570\u636e\u548c\u4f60\u7684\u5224\u65ad\u6765\u9009\u62e9\u6700\u53ef\u80fd\u6b63\u786e\u7684\u9009\u9879\u3002 (Please answer the following multiple-choice\nquestions and select the correct options. These questions may include both single-choice\nand multiple-choice formats. If the provided information is not sufficient to determine a\ndefinite answer, please choose the option that is most likely correct based on the available\ndata and your judgment.)7\nTrue/False questions: \u8bf7\u56de\u7b54\u4ee5\u4e0b\u5224\u65ad\u9898\uff0c\u5e76\u6839\u636e\u9898\u76ee\u63cf\u8ff0\u548c\u6240\u7ed9\u7684\u4fe1\u606f\u6765\u5224\u65ad\u95ee\u9898\u4e2d\n\u9648\u8ff0\u7684\u5bf9\u9519\u3002\u5982\u679c\u4fe1\u606f\u4e0d\u5b8c\u6574\u6216\u4e0d\u8db3\u4ee5\u4f5c\u51fa\u7edd\u5bf9\u5224\u65ad\uff0c\u8bf7\u8fd0\u7528\u4f60\u7684\u903b\u8f91\u63a8\u7406\u548c\u73b0\u6709\u4fe1\u606f\n\u6765\u505a\u51fa\u6700\u53ef\u80fd\u7684\u5224\u65ad\u3002 (Please answer the following true/false questions and determine\nthe correctness of the statements based on the question descriptions and the provided\ninformation. If the information is incomplete or insufficient for an absolute judgment, please\nuse your logical reasoning and available information to make the most likely judgment.)\nFill-in-the-blank questions: \u8bf7\u56de\u7b54\u4ee5\u4e0b\u586b\u7a7a\u9898\uff0c\u5e76\u6839\u636e\u9898\u76ee\u7684\u8981\u6c42\u548c\u6240\u63d0\u4f9b\u7684\u4fe1\u606f\u6765\u7ed9\u51fa\n\u6700\u6070\u5f53\u7684\u7b54\u6848\u3002\u5982\u679c\u4fe1\u606f\u4e0d\u8db3\u4ee5\u786e\u5207\u56de\u7b54\uff0c\u90a3\u4e48\u8bf7\u4f9d\u636e\u73b0\u6709\u7684\u6570\u636e\u548c\u4f60\u7684\u63a8\u7406\u80fd\u529b\u6765\u586b\u5199\n\u6700\u5408\u7406\u7684\u7b54\u6848\u3002 (Please answer the following fill-in-the-blank questions and provide the\nmost appropriate answer based on the question requirements and the provided information.\nIf the information is insufficient for an exact answer, please fill in the most reasonable\nresponse based on the available data and your reasoning abilities.)\nFig. A1 shows the proportion of 6 disciplines and 30 subjects in CMMMU, and Tab. A1\nshows the result decomposition across image types.\n\u519c\u4e1a Agriculture (283, 2.35%)\n\u836f\u7528\u690d\u7269\u683d\u57f9\uff0c\u98df\u54c1\u79d1\u5b66\uff0c\u75c5\u866b\u5bb3\u9632\n\u6cbb\u2026\u2026\n\u5efa\u7b51\u5b66 Architecture (650, 5.41%)\n\u571f\u6728\u5de5\u7a0b\uff0c\u6c34\u5229\u7ed3\u6784\uff0c\u7ed3\u6784\u529b\u5b66\u2026\u2026\n\u8ba1\u7b97\u673a\u79d1\u5b66 Computer Sci. (463, \n3.85%)\n\u8ba1\u7b97\u673a\u7f51\u7edc\uff0c\u64cd\u4f5c\u7cfb\u7edf,\u8ba1\u7b97\u673a\u56fe\u5f62\n\u5b66\u2026\u2026\n\u7535\u5b50\u5b66 Electronics (387, 3.22%)\n\u7535\u8def\u5206\u6790\uff0c\u5f71\u50cf\u7535\u5b50\u5b66\uff0c\u7535\u5b50\u6280\n\u672f\u2026\u2026\n\u80fd\u6e90\u548c\u7535\u529b Energy & Power (422, \n3.51%)\n\u70ed\u529b\u7cfb\u7edf\uff0c\u6838\u7535\u77e5\u8bc6\uff0c\u7535\u6c14\u5de5\u7a0b\u2026\u2026\n\u6750\u6599 Materials (504, 4.19%)\n\u7ed3\u6784\u5de5\u7a0b\uff0c\u6750\u6599\u529b\u5b66\uff0c\u6750\u6599\u8868\u5f81\u6280\n\u672f\u2026\u2026\n\u673a\u68b0\u5de5\u7a0b Mechanical Eng. (540, \n4.49%)\n\u673a\u68b0\u539f\u7406\uff0c\u5de5\u7a0b\u52a8\u529b\u5b66\uff0c\u673a\u68b0\u8bbe\u8ba1\u7406\n\u8bba\u2026\u2026\n\u827a\u672f Arts (219, 1.82%)\n\u7ed8\u753b\u4f5c\u54c1\uff0c\u7535\u5f71\u827a\u672f\uff0c\u620f\n\u5267\u827a\u672f\uff0c\u8bd7\u6b4c\u9274\u8d4f\u2026\u2026\n\u827a\u672f\u7406\u8bba Art Theory \n(442, 3.67%)\n\u6784\u56fe\u624b\u6cd5\uff0c\u897f\u65b9\u7f8e\u5b66\uff0c\u4e2d\n\u56fd\u7f8e\u672f\uff0c\u5fc3\u7406\u5b66\u827a\u672f\u2026\u2026\n\u8bbe\u8ba1 Design (246, 2.04%)\n\u5e73\u9762\u8bbe\u8ba1\uff0c\u5efa\u7b51\u8bbe\u8ba1\uff0c\u5f71\n\u50cf\u5904\u7406\uff0c\u7ed8\u56fe\u6548\u679c\u2026\u2026\n\u97f3\u4e50 Music (283, 2.35%)\n\u4f20\u7edf\u4e50\u5668\uff0c\u58f0\u4e50\u6f14\u5531\uff0c\u4e50\n\u7406\u77e5\u8bc6\uff0c\u5408\u5531\u827a\u672f\u2026\u2026\n\u751f\u7269 Biology (473, 3.93%)\n\u690d\u7269\u751f\u7269\u5b66\uff0c\u751f\u7269\u5316\u5b66\uff0c\n\u5fae\u751f\u7269\u7ed3\u6784\u2026\u2026\n\u5316\u5b66 Chemistry (561, \n4.67%)\n\u5206\u6790\u5316\u5b66\uff0c\u6709\u673a\u5316\u5b66\uff0c\u805a\n\u5408\u7269\u5316\u5b66\u2026\u2026\n\u5730\u7406 Geography (647, \n5.38%)\n\u533a\u57df\u5730\u7406\uff0c\u6c14\u5019\u7279\u5f81\uff0c\u5730\n\u8d28\u5730\u8c8c\u5b66\u539f\u7406\u2026\u2026\n\u6570\u5b66 Mathematics (574, \n4.77%)\n\u5fae\u79ef\u5206\uff0c\u7ecf\u6d4e\u6570\u5b66\uff0c\u6982\u7387\n\u8bba\u4e0e\u6570\u7406\u7edf\u8ba1\u2026\u2026\n\u7269\u7406 Physics (468, 3.89%)\n\u7535\u78c1\u57fa\u7840\uff0c\u521a\u4f53\u529b\u5b66\u57fa\u7840\uff0c\n\u7ecf\u5178\u529b\u5b66\u2026\u2026\n\u4f1a\u8ba1 Accounting (524, \n4.36%)\n\u57fa\u7840\u4f1a\u8ba1\uff0c\u8d22\u52a1\u5206\u6790,\u503a\n\u52a1\u8d44\u672c\u2026\u2026\n\u7ecf\u6d4e Economics (267, \n2.22%)\n\u5b8f\u89c2\u7ecf\u6d4e\u5b66\uff0c\u7ba1\u7406\u7ecf\u6d4e\n\u5b66\uff0c\u5fae\u89c2\u7ecf\u6d4e\u2026\u2026\n\u91d1\u878d Finance (386, \n3.21%)\n\u516c\u53f8\u91d1\u878d\uff0c\u80a1\u7968\u7ba1\u7406\uff0c\n\u6295\u8d44\u7ba1\u7406\u2026\u2026\n\u7ba1\u7406 Management (294, \n2.44%)\n\u8fd0\u7b79\u5b66\uff0c\u4f01\u4e1a\u7ecf\u8425\uff0c\u6218\n\u7565\u7ba1\u7406\u2026\u2026\n\u8425\u9500 Marketing (209, \n1.73%)\n\u5e02\u573a\u8425\u9500\uff0c\u5e02\u573a\u9884\u6d4b\uff0c\n\u6570\u636e\u5206\u6790\u2026\u2026\n\u5386\u53f2 History (331,  \n2.75%)\n\u4eba\u6587\u5386\u53f2\uff0c\u4e2d\u56fd\u53e4\u4ee3\u53f2\uff0c\n\u5730\u7406\u5386\u53f2,\u519b\u4e8b\u53f2\u2026\u2026\n\u6587\u732e\u5b66 Literature \n(98, 0.81%)\n\u6587\u5316\u53f2\uff0c\u8003\u53e4\u5b66\uff0c\u53e4\u4ee3\n\u6587\u5b66,\u79d1\u5b66\u53f2\u2026\u2026\n\u5fc3\u7406\u5b66 \nPsychology (382, \n3.18%)\n\u5b9e\u9a8c\u5fc3\u7406\u5b66\uff0c\u4fe1\u53f7\u68c0\u6d4b\n\u8bba\uff0c\u8ba4\u77e5\u5fc3\u7406\u5b66\u2026\u2026\n\u793e\u4f1a\u5b66 Sociology \n(323, 2.68%)\n\u793e\u4f1a\u5b66\u53f2\uff0c\u793e\u4f1a\u7ecf\u6d4e\u5b66\uff0c\n\u6280\u672f\u4e0e\u793e\u4f1a\u2026\u2026\n\u57fa\u7840\u533b\u5b66 Basic Med. (422, \n3.51%)\n\u751f\u7269\u5b9e\u9a8c\uff0c\u7ec6\u80de\u5f62\u6001\uff0c\u7ec6\u80de\n\u5f62\u6001\uff0c\u4eba\u4f53\u7ed3\u6784\u2026\u2026\n\u8bca\u65ad\u5b66 Diagnostic (159, \n1.32%)\n\u751f\u7269\u533b\u5b66\uff0c\u65ad\u5c42\u89e3\u5256\u5b66\uff0c\u75c5\n\u7406\u5207\u7247\uff0c\u533b\u5b66\u68c0\u67e5\u2026\u2026\n\u4e34\u5e8a\u533b\u5b66 Clinical Med. \n(374, 3.11%)\n\u4e34\u5e8a\u75c5\u7406\u5b66\uff0c\u9aa8\u9abc\u7ed3\u6784\uff0c\u4e34\n\u5e8a\u795e\u7ecf\u5b66\uff0c\u514d\u75ab\u5b66\u2026\u2026\n\u5236\u836f Pharmacy (462, \n3.84%)\n\u751f\u7269\u5236\u836f\uff0c\u836f\u7269\u5316\u5b66\uff0c\u9178\u78b1\n\u7406\u8bba,\u751f\u7269\u836f\u5242\u5b66\u2026\u2026\n\u516c\u5171\u536b\u751f Public Health \n(619, 5.15%)\n \u6d41\u884c\u75c5\u5b66\uff0c\u4f20\u67d3\u75c5\u63a7\u5236\uff0c\n\u5065\u5eb7\u98ce\u9669\u8bc4\u4f30\u2026\u2026\n\u827a\u672f\u8bbe\u8ba1\nArts & Design\n\u5065\u5eb7\u4e0e\u533b\u5b66\nHealth & Medicine\n\u5546\u4e1a \nBusiness\n\u79d1\u5b66\nScience\n\u4eba\u6587\u793e\u4f1a\u79d1\u5b66 \nHumanities & \nSocial Sci.\n\u6280\u672f\u4e0e\u5de5\u7a0b\nTechnology & Engineering\nFigure A1: The proportion of 6 disciplines and 30 subjects in the CMMMU. The multimodal\nsamples in 30 subjects uniformly cover the relevant expert-level domain knowledge.\n7The English version is not part of the input to the models.\n14\nPREPRINT\nModels\nSketches\nTable\nPhotos\nCharts\nChemical\nStructures\nCircuit\nDiagram\nEngineering\nDiagram\nMedical\nImages\nMicrosc.\nImages\nOverall\nmPLUG-Owl2\n21.8\n15.7\n29.0\n22.5\n24.6\n14.3\n20.2\n26.7\n21.4\n22.2\nVisCPM\n22.4\n14.0\n31.2\n23.8\n20.6\n16.2\n21.7\n28.9\n21.2\n22.7\nChinese-LLaVA\n24.2\n15.5\n29.8\n21.8\n24.1\n15.0\n24.0\n26.9\n20.9\n23.4\nEmu2\n25.6\n15.8\n30.3\n22.9\n27.3\n16.6\n26.1\n26.9\n21.2\n24.5\nCogAgent\n23.7\n16.1\n30.5\n24.5\n22.4\n20.3\n22.9\n28.0\n23.5\n23.6\nInternVL-Chat-V1.1\n29.9\n21.3\n51.0\n33.1\n31.1\n20.3\n23.9\n44.0\n35.7\n34.0\nYi-VL-6B\n30.6\n21.3\n52.6\n35.1\n34.8\n19.9\n26.5\n42.3\n36.0\n35.0\nYi-VL-34B\n31.2\n20.8\n55.5\n35.0\n34.9\n26.2\n22.3\n47.4\n36.8\n36.5\nQwen-VL-Plus\n30.2\n24.2\n53.1\n41.1\n35.1\n23.0\n27.8\n43.4\n37.6\n36.8\nGPT-4V\n35.7\n38.8\n56.8\n44.9\n44.0\n37.7\n25.1\n44.3\n35.2\n43.7\nTable A1: Result decomposition across images type. bold results in LMMs indicate the best\nresults for all models, and the Blue results indicate the best results among the open-source\nmodels.\n15\nB\nCase Study\nThe appendix is our sample analysis of GPT-4V, including an analysis of 150 error examples\nand 75 correct examples.\nList of Case Study Figures\n1\nArt 1: Correct Case\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n2\nArt 2: Correct Case\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\n3\nArt 3: Perceptual Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n4\nArt 4: Perceptual Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n5\nArt 5: Perceptual Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n6\nArt 6: Perceptual Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n7\nArt 7: Lack of Knowledge\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n8\nArt 8: Lack of Knowledge\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\n9\nArt Theory 1: Lack of Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n10 Design 1: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\n11 Design 2: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n12 Design 3: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n13 Design 4: Reject to Answer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\n14 Design 5: Reject to Answer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\n15 Design 6: Perceptual Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n16 Design 7: Reasoning Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\n17 Design 8: Reasoning Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n36\n18 Design 9: Lack of Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\n19 Music 1: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\n20 Music 2: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\n21 Music 3: Reject to Answer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n22 Music 4: Perceptual Error\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\n23 Music 5: Perceptual Error\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\n24 Music 6: Lack of Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\n25 Music 7: Lack of Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n44\n26 Basic Medicine 1: Correct Case\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n45\n27 Basic Medicine 2: Correct Case\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n46\n28 Basic Medicine 3: Correct Case\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n47\n29 Basic Medicine 4: Reject to Answer . . . . . . . . . . . . . . . . . . . . . . . . . . .\n48\n30 Basic Medicine 5: Perceptual Error . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\n31 Basic Medicine 6: Perceptual Error . . . . . . . . . . . . . . . . . . . . . . . . . . .\n50\n32 Basic Medicine 7: Reasoning Error\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n51\n33 Basic Medicine 8: Reasoning Error\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n52\n34 Basic Medicine 9: Answer Extraction Error\n. . . . . . . . . . . . . . . . . . . . . .\n53\n35 Basic Medicine 10: Lack of Knowledge . . . . . . . . . . . . . . . . . . . . . . . . .\n54\n36 Clinical Medicine 1: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n55\n37 Clinical Medicine 2: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n56\n38 Clinical Medicine 3: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n57\n39 Clinical Medicine 4: Reject to Answer\n. . . . . . . . . . . . . . . . . . . . . . . . .\n58\n40 Clinical Medicine 5: Perceptual Error . . . . . . . . . . . . . . . . . . . . . . . . . .\n59\n41 Clinical Medicine 6: Annotation Error . . . . . . . . . . . . . . . . . . . . . . . . .\n60\n42 Clinical Medicine 7: Reasoning Error . . . . . . . . . . . . . . . . . . . . . . . . . .\n61\n43 Clinical Medicine 8: Textual Understanding . . . . . . . . . . . . . . . . . . . . . .\n62\n44 Clinical Medicine 9: Answer Extraction Error . . . . . . . . . . . . . . . . . . . . .\n63\n45 Clinical Medicine 10: Lack of Knowledge\n. . . . . . . . . . . . . . . . . . . . . . .\n64\n46 Diagnostics and Lab Medicine 1: Correct Case\n. . . . . . . . . . . . . . . . . . . .\n65\n47 Diagnostics and Lab Medicine 2: Correct Case\n. . . . . . . . . . . . . . . . . . . .\n66\n48 Diagnostics and Lab Medicine 3: Correct Case\n. . . . . . . . . . . . . . . . . . . .\n67\n49 Diagnostics and Lab Medicine 4: Reject to Answer . . . . . . . . . . . . . . . . . .\n68\n50 Diagnostics and Lab Medicine 5: Reject to Answer . . . . . . . . . . . . . . . . . .\n69\n51 Diagnostics and Lab Medicine 6: Reasoning Error\n. . . . . . . . . . . . . . . . . .\n70\n16\nPREPRINT\n52 Diagnostics and Lab Medicine 7: Lack of Knowledge\n. . . . . . . . . . . . . . . .\n71\n53 Diagnostics and Lab Medicine 8: Lack of Knowledge\n. . . . . . . . . . . . . . . .\n72\n54 Pharmacy 1: Correct Case\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n73\n55 Pharmacy 2: Correct Case\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n74\n56 Pharmacy 3: Correct Case\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n75\n57 Pharmacy 4: Perceptual Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n76\n58 Pharmacy 5: Perceptual Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n77\n59 Pharmacy 6: Perceptual Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n78\n60 Pharmacy 7: Perceptual Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n79\n61 Pharmacy 8: Perceptual Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n80\n62 Pharmacy 9: Lack of Knowledge\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n81\n63 Public Health 1: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n82\n64 Public Health 2: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n83\n65 Public Health 3: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n84\n66 Public Health 4: Reject to Answer . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n85\n67 Public Health 5: Perceptual Error . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n86\n68 Public Health 6: Perceptual Error . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n87\n69 Public Health 7: Reasoning Error . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n88\n70 Public Health 8: Textual Understanding . . . . . . . . . . . . . . . . . . . . . . . .\n89\n71 Public Health 9: Answer Extraction Error . . . . . . . . . . . . . . . . . . . . . . .\n90\n72 Public Health 10: Lack of Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . .\n91\n73 Accounting 1: Correct Case\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n92\n74 Accounting 2: Perceptual Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n93\n75 Accounting 3: Reasoning Error\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n94\n76 Economics 1: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n95\n77 Economics 2: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n96\n78 Economics 3: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n97\n79 Economics 4: Reject to Answer\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n98\n80 Economics 5: Reasoning Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n99\n81 Economics 6: Reasoning Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n100\n82 Economics 7: Textual Understanding . . . . . . . . . . . . . . . . . . . . . . . . . .\n101\n83 Economics 8: Lack of Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n102\n84 Economics 9: Lack of Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n103\n85 Finance 1: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n104\n86 Finance 2: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n105\n87 Finance 3: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n106\n88 Finance 4: Reject to Answer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n107\n89 Finance 5: Perceptual Error\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n108\n90 Finance 6: Annotation Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n109\n91 Finance 7: Reasoning Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n110\n92 Finance 8: Reasoning Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n111\n93 Finance 9: Textual Understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n112\n94 Finance 10: Lack of Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n113\n95 Management 1: Correct Case\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n114\n96 Management 2: Correct Case\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n115\n97 Management 3: Correct Case\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n116\n98 Management 4: Perceptual Error\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n117\n99 Management 5: Reasoning Error\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n118\n100Management 6: Reasoning Error\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n119\n101Management 7: Textual Understanding\n. . . . . . . . . . . . . . . . . . . . . . . .\n120\n102Management 8: Lack of Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . .\n121\n103Management 9: Lack of Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . .\n122\n104Management 10: Lack of Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . .\n123\n105Marketing 1: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n124\n106Marketing 2: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n125\n107Marketing 3: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n126\n108Marketing 4: Perceptual Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n127\n109Marketing 5: Textual Understanding . . . . . . . . . . . . . . . . . . . . . . . . . .\n128\n110Marketing 6: Lack of Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n129\n17\nPREPRINT\n111Marketing 7: Lack of Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n130\n112Marketing 8: Lack of Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n131\n113Marketing 9: Lack of Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n132\n114Marketing 10: Lack of Knowledge\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n133\n115Biology 1: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n134\n116Biology 2: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n135\n117Biology 3: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n136\n118Biology 4: Perceptual Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n137\n119Biology 5: Reasoning Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n138\n120Biology 6: Reasoning Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n139\n121Biology 7: Answer Extraction Error . . . . . . . . . . . . . . . . . . . . . . . . . . .\n140\n122Biology 8: Answer Extraction Error . . . . . . . . . . . . . . . . . . . . . . . . . . .\n141\n123Biology 9: Lack of Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n142\n124Biology 10: Lack of Knowledge, Reject to Answer\n. . . . . . . . . . . . . . . . . .\n143\n125Chemistry 1: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n144\n126Chemistry 2: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n145\n127Chemistry 3: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n146\n128Chemistry 4: Reject to Answer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n147\n129Chemistry 5: Perceptual Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n148\n130Chemistry 6: Perceptual Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n149\n131Chemistry 7: Annotation Error\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n150\n132Chemistry 8: Reasoning Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n151\n133Geography 1: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n152\n134Geography 2: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n153\n135Geography 3: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n154\n136Geography 4: Reject to Answer . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n155\n137Geography 5: Perceptual Error, Lack of Knowledge\n. . . . . . . . . . . . . . . . .\n156\n138Geography 6: Reasoning Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n157\n139Geography 7: Reasoning Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n158\n140Geography 8: Textual Understanding . . . . . . . . . . . . . . . . . . . . . . . . . .\n159\n141Geography 9: Answer Extraction Error . . . . . . . . . . . . . . . . . . . . . . . . .\n160\n142Geography 10: Lack of Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . .\n161\n143Mathematics 1: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n162\n144Mathematics 2: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n163\n145Mathematics 3: Perceptual Error\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n164\n146Physics 1: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n165\n147Physics 2: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n166\n148Physics 3: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n167\n149Physics 4: Perceptual Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n168\n150Physics 5: Lack of Knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n169\n151Physics 6: Lack of Knowledge, Answer Extraction Error . . . . . . . . . . . . . . .\n170\n152History 1: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n171\n153History 2: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n172\n154History 3: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n173\n155History 4: Perceptual Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n174\n156History 5: Reasoning Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n175\n157Literature 1: Correct Case\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n176\n158Literature 2: Correct Case\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n177\n159Literature 3: Correct Case\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n178\n160Literature 4: Perceptual Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n179\n161Literature 5: Perceptual Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n180\n162Psychology 1: Correct Case\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n181\n163Psychology 2: Correct Case\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n182\n164Psychology 3: Correct Case\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n183\n165Psychology 4: Reject to Answer . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n184\n166Psychology 5: Reject to Answer . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n185\n167Psychology 6: Perceptual Error\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n186\n168Psychology 7: Reasoning Error\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n187\n169Sociology 1: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n188\n18\nPREPRINT\n170Sociology 2: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n189\n171Sociology 3: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n190\n172Sociology 4: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n191\n173Sociology 5: Reject to Answer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n192\n174Sociology 6: Reject to Answer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n193\n175Sociology 7: Reasoning Error\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n194\n176Agriculture 1: Correct Case\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n195\n177Agriculture 2: Correct Case\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n196\n178Agriculture 3: Correct Case\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n197\n179Agriculture 4: Perceptual Error\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n198\n180Agriculture 5: Reasoning Error\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n199\n181Agriculture 6: Lack of Knowledge\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n200\n182Agriculture 7: Lack of Knowledge\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n201\n183Architecture 1: Reject to Answer\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n202\n184Architecture 2: Reject to Answer\n. . . . . . . . . . . . . . . . . . . . . . . . . . . .\n203\n185Architecture 3: Reject to Answer, Perceptual Error . . . . . . . . . . . . . . . . . .\n204\n186Architecture 4: Perceptual Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n205\n187Architecture 5: Perceptual Error, Reject to Answer . . . . . . . . . . . . . . . . . .\n206\n188Architecture 6: Reasoning Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n207\n189Architecture 7: Reasoning Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n208\n190Architecture 8: Reasoning Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n209\n191Architecture 9: Reasoning Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n210\n192Computer Science 1: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n211\n193Computer Science 2: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n212\n194Computer Science 3: Textual Understanding\n. . . . . . . . . . . . . . . . . . . . .\n213\n195Computer Science 4: Lack of Knowledge . . . . . . . . . . . . . . . . . . . . . . . .\n214\n196Electronics 1: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n215\n197Electronics 2: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n216\n198Electronics 3: Correct Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n217\n199Electronics 4: Reasoning Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n218\n200Electronics 5: Reasoning Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n219\n201Electronics 6: Reasoning Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n220\n202Energy and Power 1: Correct Case\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n221\n203Energy and Power 2: Correct Case\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n222\n204Energy and Power 3: Perceptual Error . . . . . . . . . . . . . . . . . . . . . . . . .\n223\n205Energy and Power 4: Perceptual Error . . . . . . . . . . . . . . . . . . . . . . . . .\n224\n206Energy and Power 5: Reasoning Error . . . . . . . . . . . . . . . . . . . . . . . . .\n225\n207Energy and Power 6: Reasoning Error . . . . . . . . . . . . . . . . . . . . . . . . .\n226\n208Energy and Power 7: Lack of Knowledge\n. . . . . . . . . . . . . . . . . . . . . . .\n227\n209Energy and Power 8: Lack of Knowledge\n. . . . . . . . . . . . . . . . . . . . . . .\n228\n210Energy and Power 9: Lack of Knowledge\n. . . . . . . . . . . . . . . . . . . . . . .\n229\n211Materials 1: Reject to Answer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n230\n212Materials 2: Perceptual Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n231\n213Materials 3: Perceptual Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n232\n214Materials 4: Perceptual Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n233\n215Materials 5: Reasoning Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n234\n216Materials 6: Reasoning Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n235\n217Mechanical Engineering 1: Correct Case . . . . . . . . . . . . . . . . . . . . . . . .\n236\n218Mechanical Engineering 2: Correct Case . . . . . . . . . . . . . . . . . . . . . . . .\n237\n219Mechanical Engineering 3: Correct Case . . . . . . . . . . . . . . . . . . . . . . . .\n238\n220Mechanical Engineering 4: Perceptual Error, Lack of Knowledge . . . . . . . . . .\n239\n221Mechanical Engineering 5: Reasoning Error . . . . . . . . . . . . . . . . . . . . . .\n240\n222Mechanical Engineering 6: Textual Understanding . . . . . . . . . . . . . . . . . .\n241\n223Mechanical Engineering 7: Answer Extraction Error . . . . . . . . . . . . . . . . .\n242\n224Mechanical Engineering 8: Answer Extraction Error . . . . . . . . . . . . . . . . .\n243\n225Mechanical Engineering 9: Lack of Knowledge . . . . . . . . . . . . . . . . . . . .\n244\n19\nPREPRINT\n\u827a\u672f\u4e0e\u8bbe\u8ba1:\u827a\u672f\nQuestion: \u5982<\u56fe\u7247 1>\uff0c\u5317\u5b8b\u5c71\u6c34\u753b\u300a\u6eaa\u5c71\u884c\u65c5\u56fe\u300b\u8fd0\u7528\u7684\u7a7a\u95f4\u8868\u73b0\u65b9\u6cd5\u662f\u6cd5\nOption:\n(A) \u5e73\u8fdc\u6cd5\n(B) \u9614\u8fdc\u6cd5\n(C) \u6df1\u8fdc\u6cd5\n(D) \u9ad8\u8fdc\u6cd5\nGPT-4V: \u8fd9\u5e45\u753b\u5c55\u73b0\u4e86\u4e2d\u56fd\u53e4\u5178\u5c71\u6c34\u753b\u7684\u7279\u70b9\uff0c\u5176\u4e2d\u5305\u62ec\u4e86\u5c71\u5cf0\u3001\u4e91\u96fe\u548c\u6811\u6728\u3002\u6839\u636e\u8fd9\u4e9b\u5143\u7d20\u7684\u6392\u5217\n\u548c\u8868\u73b0\u624b\u6cd5\uff0c\u53ef\u4ee5\u63a8\u65ad\u753b\u5bb6\u53ef\u80fd\u4f7f\u7528\u4e86\u4e00\u79cd\u7279\u5b9a\u7684\u7a7a\u95f4\u8868\u73b0\u65b9\u6cd5\u6765\u8425\u9020\u6df1\u5ea6\u548c\u8fdc\u8fd1\u611f\u3002\n\u9009\u9879\u4e2d\u7684\uff1a\n(A) \u5e73\u8fdc\u6cd5 - \u901a\u5e38\u6307\u7684\u662f\u901a\u8fc7\u5e73\u884c\u6392\u5217\u7684\u5c71\u5cf0\u3001\u6c34\u9762\u7b49\u5143\u7d20\u6765\u8868\u73b0\u6df1\u8fdc\u7684\u7a7a\u95f4\u611f\u3002\n(B) \u9614\u8fdc\u6cd5 - \u901a\u8fc7\u5bbd\u9614\u7684\u6c34\u9762\u6216\u5e73\u539f\u6765\u8868\u73b0\u7a7a\u95f4\u7684\u6df1\u8fdc\u3002\n(C) \u6df1\u8fdc\u6cd5 - \u901a\u8fc7\u5c42\u5c42\u53e0\u52a0\u7684\u5c71\u5cf0\u3001\u4e91\u96fe\u7b49\u5143\u7d20\u6765\u8868\u73b0\u7a7a\u95f4\u7684\u6df1\u5ea6\u548c\u8fdc\u8fd1\u5173\u7cfb\u3002\n(D) \u9ad8\u8fdc\u6cd5 - \u901a\u8fc7\u9ad8\u8038\u7684\u5c71\u5cf0\u548c\u89c6\u89d2\u7684\u62ac\u5347\u6765\u8868\u73b0\u7a7a\u95f4\u7684\u6df1\u8fdc\u3002\n\u4ece\u56fe\u7247\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0c\u753b\u4e2d\u7684\u5c71\u5cf0\u9ad8\u8038\uff0c\u4e91\u96fe\u73af\u7ed5\uff0c\u7ed9\u4eba\u4e00\u79cd\u9ad8\u8fdc\u548c\u6df1\u9083\u7684\u611f\u89c9\u3002\n\u56e0\u6b64\uff0c\u6700\u53ef\u80fd\u6b63\u786e\u7684\u9009\u9879\u662f\uff1a\n\u6b63\u786e\u7b54\u6848\uff1a(D) \u9ad8\u8fdc\u6cd5\nGround Truth: (D) \u9ad8\u8fdc\u6cd5\n<\u56fe\u7247 1>\nFigure B1: A sample correct case of Art and Design (subcategory: Art).\nBack to List of figures\n20\nPREPRINT\n\u827a\u672f\u4e0e\u8bbe\u8ba1:\u827a\u672f\nQuestion: \u5982<\u56fe\u7247 1>,\u8be5\u4f5c\u54c1(\u5c40\u90e8)\u7684\u540d\u79f0\u662f( )\u3002\nOption:\n(A) \u300a\u8e0f\u6b4c\u56fe\u300b\n(B) \u300a\u6eaa\u5c71\u884c\u65c5\u56fe\u300b\n(C) \u300a\u8662\u56fd\u592b\u4eba\u6e38\u6625\u56fe\u300b\n(D) \u300a\u671d\u5143\u4ed9\u4ed7\u56fe\u300b\nGPT-4V: \u6b63\u786e\u7b54\u6848\u662f\uff1a(C) \u300a\u8662\u56fd\u592b\u4eba\u6e38\u6625\u56fe\u300b\n\u8fd9\u5e45\u753b\u662f\u4e2d\u56fd\u5317\u5b8b\u65f6\u671f\u753b\u5bb6\u5f20\u62e9\u7aef\u521b\u4f5c\u7684\u300a\u8662\u56fd\u592b\u4eba\u6e38\u6625\u56fe\u300b\uff0c\u63cf\u7ed8\u4e86\u4e00\u4f4d\u9ad8\u7ea7\u5b98\u5458\u7684\u592b\u4eba\u51fa\u6e38\u7684\u573a\n\u666f\u3002\nGround Truth: (C) \u300a\u8662\u56fd\u592b\u4eba\u6e38\u6625\u56fe\u300b\n<\u56fe\u7247 1>\nFigure B2: A sample correct case of Art and Design (subcategory: Art).\nBack to List of figures\n21\nPREPRINT\n\u827a\u672f\u4e0e\u8bbe\u8ba1:\u827a\u672f\nQuestion:  <\u56fe\u7247 1>\u8be5\u7ebf\u63cf\u7c7b\u578b\u5c5e\u4e8e\uff08\n\uff09\u3002\nOption:\n(A) \u94c1\u7ebf\u63cf\n(B) \u9489\u5934\u9f20\u5c3e\u63cf\n(C) \u9ad8\u53e4\u6e38\u4e1d\u63cf\n(D) \u67f3\u53f6\u63cf\nGPT-4V: \u8fd9\u5f20\u56fe\u7247\u5c55\u793a\u7684\u662f\u4e00\u79cd\u7ebf\u63cf\u827a\u672f\uff0c\u4f46\u7531\u4e8e\u6211\u65e0\u6cd5\u770b\u5230\u5b9e\u9645\u7684\u7ebf\u6761\u5bbd\u7a84\u53d8\u5316\u548c\u7ebf\u6761\u7684\u5177\u4f53\u98ce\u683c\uff0c\n\u56e0\u6b64\u65e0\u6cd5\u51c6\u786e\u5224\u65ad\u5b83\u5c5e\u4e8e\u54ea\u4e00\u79cd\u7ebf\u63cf\u7c7b\u578b\u3002\u4e0d\u8fc7\uff0c\u6839\u636e\u7ebf\u6761\u7684\u6d41\u7545\u6027\u548c\u7b80\u6d01\u6027\uff0c\u6211\u53ef\u4ee5\u5c1d\u8bd5\u731c\u6d4b\uff1a\u9009\n\u9879 (A) \u94c1\u7ebf\u63cf \u901a\u5e38\u6307\u7684\u662f\u7ebf\u6761\u575a\u786c\u5982\u94c1\uff0c\u5f3a\u8c03\u529b\u5ea6\u548c\u7ed3\u6784\u611f\u3002\n\u9009\u9879 (B) \u9489\u5934\u9f20\u5c3e\u63cf \u63cf\u8ff0\u7684\u662f\u7ebf\u6761\u5f00\u59cb\u65f6\u7c97\u800c\u672b\u7aef\u7ec6\uff0c\u7c7b\u4f3c\u9489\u5934\u548c\u9f20\u5c3e\u3002\n\u9009\u9879 (C) \u9ad8\u53e4\u6e38\u4e1d\u63cf \u662f\u6307\u975e\u5e38\u7ec6\u817b\u548c\u590d\u6742\u7684\u7ebf\u6761\uff0c\u5982\u6e38\u4e1d\u822c\u8f7b\u67d4\u3002\n\u9009\u9879 (D) \u67f3\u53f6\u63cf \u5219\u662f\u6307\u7ebf\u6761\u7ec6\u957f\u5982\u67f3\u53f6\uff0c\u67d4\u7f8e\u6d41\u7545\u3002\n\u6839\u636e\u8fd9\u5f20\u56fe\u7247\uff0c\u7ebf\u6761\u770b\u8d77\u6765\u6bd4\u8f83\u7b80\u6d01\u6d41\u7545\uff0c\u4f46\u6ca1\u6709\u8db3\u591f\u7684\u4fe1\u606f\u6765\u786e\u5b9a\u5b83\u662f\u5426\u7b26\u5408\u4e0a\u8ff0\u4efb\u4f55\u4e00\u79cd\u7279\u5b9a\u7684\n\u7ebf\u63cf\u98ce\u683c\u3002\u5982\u679c\u5fc5\u987b\u9009\u62e9\uff0c\u6211\u53ef\u80fd\u4f1a\u503e\u5411\u4e8e\u9009\u62e9 (D) \u67f3\u53f6\u63cf\uff0c\u56e0\u4e3a\u7ebf\u6761\u770b\u8d77\u6765\u8f83\u4e3a\u6d41\u7545\uff0c\u6709\u70b9\u50cf\u67f3\n\u53f6\u7684\u5f62\u6001\u3002\u7136\u800c\uff0c\u8fd9\u53ea\u662f\u4e00\u4e2a\u57fa\u4e8e\u53ef\u89c1\u4fe1\u606f\u7684\u731c\u6d4b\uff0c\u800c\u4e0d\u662f\u4e00\u4e2a\u786e\u5b9a\u7684\u7b54\u6848\u3002\n\u6b63\u786e\u7b54\u6848\uff1a(D) \u67f3\u53f6\u63cf\uff08\u731c\u6d4b\uff09\nGround Truth: (B) \u9489\u5934\u9f20\u5c3e\u63cf\n<\u56fe\u7247 1>\nError Category: Perceptual Error\nError Reason: GPT-4 attempts to guess possible answers by characterizing the type of line drawing \nprovided by each option. However, this method is very limited in its accuracy and validity in the \nabsence of direct observation and analysis of the picture. Without access to the specifics of the \npictures, the GPT-4 is unable to effectively analyze or provide accurate answers. In addition, GPT-4 \nmade it clear in its response that it could not \u201csee\u201d the actual lines.\nFigure B3: A sample error case of Art and Design (subcategory: Art).\nBack to List of figures\n22\nPREPRINT\n\u827a\u672f\u4e0e\u8bbe\u8ba1:\u827a\u672f\nQuestion:  <\u56fe\u7247 1>\u4e0b\u56fe\u4e2d\u7684\u4e66\u6cd5\u7528\u7b14\u662f\uff08\uff09\u8fd0\u7b14\u3002\nOption:\n(A) \u6b63\u950b\n(B) \u4e2d\u950b\n(C) \u4fa7\u950b\n(D) \u60ac\u950b\nGPT-4V: \u6b63\u786e\u7b54\u6848\u662f\uff1a(C) \u4fa7\u950b\n\u5728\u4e66\u6cd5\u4e2d\uff0c\u4fa7\u950b\u8fd0\u7b14\u662f\u6307\u7b14\u5c16\u504f\u79bb\u4e2d\u5fc3\u7ebf\uff0c\u7528\u7b14\u7684\u4e00\u4fa7\u8fdb\u884c\u4e66\u5199\u3002\u4ece\u63d0\u4f9b\u7684\u56fe\u7247\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0c\u7b14\u5c16\u5e76\n\u6ca1\u6709\u5782\u76f4\u7eb8\u9762\uff0c\u800c\u662f\u6709\u4e00\u5b9a\u7684\u503e\u659c\u89d2\u5ea6\uff0c\u8fd9\u8868\u660e\u662f\u5728\u4f7f\u7528\u4fa7\u950b\u8fd0\u7b14\u7684\u65b9\u5f0f\u3002\nGround Truth: (B) \u4e2d\u950b\n<\u56fe\u7247 1>\nError Category: Perceptual Error\nError Reason: GPT-4V incorrectly interpreted the angle of the calligraphy brush when processing \nthe image. In this case, the correct answer needs to be based on the angle of the tip of the \ncalligraphy brush relative to the surface of the paper. In the correct case, the tip of the brush should \nbe perpendicular to the surface of the paper, i.e., a \"center-front\" stroke, but the model incorrectly \nrecognizes that the tip of the brush is not perpendicular, and thus incorrectly chooses a \"side-front\" \nstroke as the answer. GPT-4V\u5728\u5904\u7406\u56fe\u50cf\u65f6\u9519\u8bef\u5730\u89e3\u8bfb\u4e86\u4e66\u6cd5\u7528\u7b14\u7684\u89d2\u5ea6\u3002\u5728\u8fd9\u4e2a\u95ee\u9898\u4e2d\uff0c\u6b63\u786e\u7684\u7b54\n\u6848\u9700\u8981\u4f9d\u636e\u4e66\u6cd5\u7b14\u5c16\u4e0e\u7eb8\u9762\u7684\u76f8\u5bf9\u89d2\u5ea6\u6765\u786e\u5b9a\u3002\u6b63\u786e\u7684\u60c5\u51b5\u5e94\u8be5\u662f\u7b14\u5c16\u5782\u76f4\u4e8e\u7eb8\u9762\uff0c\u4e5f\u5c31\u662f\u201c\u4e2d\u950b\u201d\n\u8fd0\u7b14\uff0c\u4f46\u6a21\u578b\u9519\u8bef\u5730\u8bc6\u522b\u4e3a\u7b14\u5c16\u5e76\u975e\u5782\u76f4\uff0c\u4ece\u800c\u9519\u8bef\u5730\u9009\u62e9\u4e86\u201c\u4fa7\u950b\u201d\u8fd0\u7b14\u4f5c\u4e3a\u7b54\u6848\u3002\nFigure B4: A sample error case of Art and Design (subcategory: Art).\nBack to List of figures\n23\nPREPRINT\n\u827a\u672f\u4e0e\u8bbe\u8ba1:\u827a\u672f\nQuestion:  \u5982<\u56fe\u7247 1>\uff0c\u8be5\u4f5c\u54c1\u7684\u4f5c\u8005\nOption:\n(A) \u5fb7\u62c9\u514b\u6d1b\u74e6\n(B) \u5e2d\u91cc\u79d1\n(C) \u7c73\u52d2\n(D) \u7f57\u4e39\nGPT-4V: \u8fd9\u5e45\u753b\u662f\u7531\u6cd5\u56fd\u6d6a\u6f2b\u4e3b\u4e49\u753b\u5bb6\u5fb7\u62c9\u514b\u6d1b\u74e6\u521b\u4f5c\u7684\uff0c\u540d\u4e3a\u300a\u9a6c\u8d5b\u7684\u5973\u6027\u300b\u3002\n\u56e0\u6b64\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a(A) \u5fb7\u62c9\u514b\u6d1b\u74e6\nGround Truth: (B) \u5e2d\u91cc\u79d1\n<\u56fe\u7247 1>\nError Category: Perceptual Error\nError Reason: Since GPT-4V cannot directly process and recognize image content, it cannot \naccurately identify the artwork and its author in the image. As a result, the model is unable to \nvalidate its assumptions or assertions about the artwork, leading to the misattribution of the author \nof The Raft of Medusa. \nFigure B5: A sample error case of Art and Design (subcategory: Art).\nBack to List of figures\n24\nPREPRINT\n\u827a\u672f\u4e0e\u8bbe\u8ba1:\u827a\u672f\nQuestion:  \u4e0b\u56fe\u4e2d\u7684\u7ed8\u753b\u4f5c\u54c1\u662f\uff08\uff09<\u56fe\u7247 1>\nOption:\n(A) \u300a\u6d1b\u795e\u8d4b\u56fe\u300b\n(B) \u300a\u5973\u53f2\u7bb4\u56fe\u300b\n(C) \u300a\u6b65\u8f87\u56fe\u300b\n(D) \u300a\u5386\u4ee3\u5e1d\u738b\u56fe\u300b\nGPT-4V: \u5728\u5176\u6b63\u786e\u7b54\u6848\u662f\uff1a(B) \u300a\u5973\u53f2\u7bb4\u56fe\u300b\n\u8fd9\u5e45\u56fe\u662f\u5f20\u8431\u7684\u300a\u5973\u53f2\u7bb4\u56fe\u300b\uff0c\u662f\u4e2d\u56fd\u5510\u4ee3\u8457\u540d\u7684\u7ed8\u753b\u4f5c\u54c1\u3002\nGround Truth: (A) \u300a\u6d1b\u795e\u8d4b\u56fe\u300b\n<\u56fe\u7247 1>\nError Category: Perceptual Error\nError Reason: GPT-4V needs to recognize the name of a painting based on the picture provided. \nThe answer given by the model is \"(B) The Picture of the Lady of History\". However, the correct \nanswer should be \"(A) The Picture of Luoshen Fu\". The model's error may be due to a \nmisunderstanding of the content of the image or a lack of knowledge in ancient Chinese paintings. \nBoth \"The Picture of a Woman in the Palace of History\" and \"Luoshen Fu\" are famous paintings in \nancient China, and there may be similarities in style and subject of these two paintings, which may \nhave led to the modeling confusion. Although both \"The Picture of a Woman in the Palace of \nHistory\" and \"Luoshen Fu\" belong to ancient Chinese figure paintings, they differ in terms of figure \nmodeling, costumes, and backgrounds. \nFigure B6: A sample error case of Art and Design (subcategory: Art).\nBack to List of figures\n25\nPREPRINT\n\u827a\u672f\u4e0e\u8bbe\u8ba1:\u827a\u672f\nQuestion:  <\u56fe\u7247 1>\u9ec4\u7b4c\u300a\u5199\u751f\u73cd\u79bd\u56fe\u300b\u7684\u521b\u4f5c\u65f6\u4ee3\u662f\uff08 \uff09\u3002\nOption:\n(A) \u5510\u4ee3\n(B) \u5317\u5b8b\n(C) \u5357\u5b8b\n(D) \u4e94\u4ee3\nGPT-4V: \u62b1\u6b49\uff0c\u6211\u65e0\u6cd5\u76f4\u63a5\u8bc6\u522b\u56fe\u7247\u4e2d\u7684\u827a\u672f\u4f5c\u54c1\u53ca\u5176\u521b\u4f5c\u65f6\u4ee3\u3002\u4f46\u6839\u636e\u60a8\u63d0\u4f9b\u7684\u4fe1\u606f\uff0c\u9ec4\u7b4c\u662f\u5357\u5b8b\n\u65f6\u671f\u7684\u753b\u5bb6\uff0c\u56e0\u6b64\u300a\u5199\u751f\u73cd\u79bd\u56fe\u300b\u7684\u521b\u4f5c\u65f6\u4ee3\u5e94\u8be5\u662f\u5357\u5b8b\u3002\n\u6240\u4ee5\u6b63\u786e\u7b54\u6848\u662f\uff1a(C) \u5357\u5b8b\nGround Truth: (D) \u4e94\u4ee3\n<\u56fe\u7247 1>\nError Category: Lack of Knowledge\nError Reason: GPT-4V is unable to recognize the content of the image directly, and is unable to \ndetermine what kind of artwork it is. Moreover, in the answer, the model mentions that \"Huang \nChyuan was a painter in the Southern Song Dynasty\", which indicates that it relies on the historical \ninformation in its training data. However, the model's training data may not contain accurate \ninformation about the specific era in which Huang Chyuan's Rare Bird Drawings were created, or \nthis information may not be correctly learned and recalled by the model. This causes the model to \nexhibit knowledge deficiencies in its responses. \nFigure B7: A sample error case of Art and Design (subcategory: Art).\nBack to List of figures\n26\nPREPRINT\n\u827a\u672f\u4e0e\u8bbe\u8ba1:\u827a\u672f\nQuestion:  \u5982<\u56fe\u7247 1>\uff0c\u9ec4\u516c\u671b\u5728\u4f5c\u54c1\u300a\u5bcc\u6625\u5c71\u5c45\u56fe\u300b(\u5c40\u90e8)\u4e2d\u4e3b\u8981\u4f7f\u7528\u7684\u662f\u54ea\u79cd\u88c5\u6cd5?\nOption:\n(A) \u6298\u5e26\u76b4\n(B) \u65a7\u5288\n(C) \u62ab\u9ebb\u76b4\n(D) \u8377\u53f6\nGPT-4V: \u9ec4\u516c\u671b\u5728\u5176\u4f5c\u54c1\u300a\u5bcc\u6625\u5c71\u5c45\u56fe\u300b\u4e2d\u4f7f\u7528\u4e86\u591a\u79cd\u7ed8\u753b\u6280\u5de7\uff0c\u4f46\u6839\u636e\u8fd9\u5e45\u753b\u7684\u5c40\u90e8\u56fe\u50cf\uff0c\u6211\u4eec\u53ef\n\u4ee5\u770b\u5230\u7684\u662f\u4e00\u79cd\u7ec6\u817b\u800c\u6709\u5c42\u6b21\u611f\u7684\u7ebf\u6761\u6280\u6cd5\uff0c\u8fd9\u79cd\u6280\u6cd5\u5728\u4e2d\u56fd\u753b\u4e2d\u88ab\u79f0\u4e3a\u76b4\u6cd5\u3002\u5728\u63d0\u4f9b\u7684\u9009\u9879\u4e2d\uff0c\u6700\n\u7b26\u5408\u8fd9\u79cd\u7ebf\u6761\u7279\u5f81\u7684\u662f(A) \u6298\u5e26\u76b4\u3002\n\u56e0\u6b64\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a(A) \u6298\u5e26\u76b4\nGround Truth: (C) \u62ab\u9ebb\u76b4\n<\u56fe\u7247 1>\nError Category: Lack of Knowledge\nError Reason: GPT-4V did not accurately identify or understand these specialized painting terms. \nThe model's choice of (A) Folded Band Chafing suggests that it has a limited understanding of \nChinese painting terminology and techniques and is unable to accurately differentiate between \ndifferent chafing techniques. Additionally, the model's responses appear to be based on general \ndescriptions of painting styles rather than a precise understanding of specific terms.\nFigure B8: A sample error case of Art and Design (subcategory: Art).\nBack to List of figures\n27\nPREPRINT\n\u827a\u672f\u4e0e\u8bbe\u8ba1:\u827a\u672f\u7406\u8bba\nQuestion:  \u5982\u56fe<\u56fe\u7247 1>\u9676\u5668\u4ea7\u751f\u4e0e\u4ec0\u4e48\u65f6\u4ee3\uff08\uff09\nOption:\n(A) \u539f\u59cb\u65f6\u671f\n(B) \u65b0\u77f3\u5668\u65f6\u4ee3\n(C) \u65e7\u77f3\u5668\u65f6\u4ee3\n(D) \u4f5c\u4e3a\u65e7\u77f3\u5668\u65f6\u4ee3\u548c\u65b0\u77f3\u5668\u65f6\u4ee3\u7684\u5212\u5206\u6807\u8bc6\nGPT-4V: \u9676\u5668\u7684\u4ea7\u751f\u901a\u5e38\u4e0e\u65b0\u77f3\u5668\u65f6\u4ee3\u76f8\u5173\u8054\u3002\u56e0\u6b64\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a(B) \u65b0\u77f3\u5668\u65f6\u4ee3\nGround Truth: (D) \u4f5c\u4e3a\u65e7\u77f3\u5668\u65f6\u4ee3\u548c\u65b0\u77f3\u5668\u65f6\u4ee3\u7684\u5212\u5206\u6807\u8bc6\n<\u56fe\u7247 1>\nError Category: Lack of Knowledge\nError Reason: Although it is true that pottery was widely used starting in the Neolithic Age, the \nGPT-4V model's recognition of this fact is not incorrect in itself. However, the model failed to \naccurately grasp the focus of the question, which is that the emergence of pottery is an important \ndistinguishing marker between the Paleolithic and Neolithic periods. This suggests that the model \nlacks comprehensive knowledge in considering the characteristics of historical periods and \narchaeological divisions.\nFigure B9: A sample error case of Art and Design (subcategory: Art Theory).\nBack to List of figures\n28\nPREPRINT\n\u827a\u672f\u4e0e\u8bbe\u8ba1:\u8bbe\u8ba1\nQuestion: \u5982\u56fe\u6240\u793a\u7684\u5730\u9762\u56fe\u6848\u5f62\u6210\u4e86\u8fd1\u5927\u8fdc\u5c0f\u7684\u6548\u679c\uff0c\u8fd9\u662f\u901a\u8fc7\uff08 \uff09\u53d8\u5f62\u64cd\u4f5c\u751f\u6210\u7684\u3002<\u56fe\u7247 1>\nOption:\n(A) \u3010\u7f16\u8f91\u3011/\u3010\u53d8\u6362\u3011/\u3010\u653e\u7f29\u3011\n(B) \u3010\u7f16\u8f91\u3011/\u3010\u53d8\u6362\u3011/\u3010\u659c\u5207\u3011\n(C) \u3010\u7f16\u8f91\u3011/\u3010\u53d8\u6362\u3011/\u3010\u900f\u89c6\u3011\n(D) \u3010\u7f16\u8f91\u3011/\u3010\u53d8\u6362\u3011/\u3010\u53d8\u5f62\u3011\nGPT-4V: \u6b63\u786e\u7b54\u6848\u662f\uff1a(C) \u3010\u7f16\u8f91\u3011/\u3010\u53d8\u6362\u3011/\u3010\u900f\u89c6\u3011\n\u56fe\u4e2d\u7684\u5730\u9762\u56fe\u6848\u663e\u793a\u4e86\u4e00\u79cd\u900f\u89c6\u6548\u679c\uff0c\u5176\u4e2d\u56fe\u6848\u7684\u5927\u5c0f\u968f\u7740\u8ddd\u79bb\u7684\u589e\u52a0\u800c\u51cf\u5c0f\uff0c\u6a21\u62df\u4e86\u5728\u4e09\u7ef4\u7a7a\u95f4\u4e2d\n\u7684\u89c6\u89c9\u73b0\u8c61\u3002\u8fd9\u79cd\u6548\u679c\u901a\u5e38\u662f\u901a\u8fc7\u900f\u89c6\u53d8\u5f62\u64cd\u4f5c\u6765\u5b9e\u73b0\u7684\u3002\nGround Truth: (C) \u3010\u7f16\u8f91\u3011/\u3010\u53d8\u6362\u3011/\u3010\u900f\u89c6\u3011\n<\u56fe\u7247 1>\nFigure B10: A sample correct case of Art and Design (subcategory: Design).\nBack to List of figures\n29\nPREPRINT\n\u827a\u672f\u4e0e\u8bbe\u8ba1:\u8bbe\u8ba1\nQuestion: \u5c06\u56feB\u5b8c\u6574\u5730\u62f7\u8d1d\u5230\u56feA\u4e2d\u5f62\u6210\u201c\u56fe\u5c421\u201d\uff0c\u5728\u3010\u56fe\u5c42\u3011\u9762\u677f\u4e2d\u5c06\u201c\u56fe\u5c421\u201d\u7684\u3010\u6df7\u5408\u6a21\u5f0f\u3011\n\u66f4\u6539\u4e3a\uff08 \uff09\u53ef\u4ee5\u5f97\u5230\u56feC\u6240\u793a\u7684\u6548\u679c\uff0c\u5373\u767d\u8272\u80cc\u666f\u81ea\u52a8\u900f\u660e\u3002 <\u56fe\u7247 1>\nOption:\n(A) \u3010\u6ee4\u8272\u3011\n(B) \u3010\u4eae\u5149\u3011\n(C) \u3010\u53e0\u52a0\u3011\n(D) \u3010\u6b63\u7247\u53e0\u5e95\u3011\nGPT-4V: \u6839\u636e\u63d0\u4f9b\u7684\u56fe\u7247\u548c\u63cf\u8ff0\uff0c\u8981\u5b9e\u73b0\u56feC\u6240\u793a\u7684\u6548\u679c\uff0c\u5373\u5c06\u56feB\u7684\u767d\u8272\u80cc\u666f\u53d8\u4e3a\u900f\u660e\uff0c\u540c\u65f6\u4fdd\u7559\u56fe\nB\u4e2d\u7684\u7ea2\u8272\u7ebf\u6761\uff0c\u5e76\u53e0\u52a0\u5230\u56feA\u4e0a\uff0c\u6700\u5408\u9002\u7684\u6df7\u5408\u6a21\u5f0f\u662f\u3010\u6b63\u7247\u53e0\u5e95\u3011\u3002\n\u6b63\u786e\u7b54\u6848\u662f\uff1a(D) \u3010\u6b63\u7247\u53e0\u5e95\u3011\nGround Truth: (D) \u3010\u6b63\u7247\u53e0\u5e95\u3011\n<\u56fe\u7247 1>\nFigure B11: A sample correct case of Art and Design (subcategory: Design).\nBack to List of figures\n30\nPREPRINT\n\u827a\u672f\u4e0e\u8bbe\u8ba1:\u8bbe\u8ba1\nQuestion: \n\u5982\u679c\u8981\u7ed8\u5236\u51fa<\u56fe\u7247 1>\u4e2d\u6240\u793a\u7684\u9010\u6e10\u6de1\u51fa\u7684\u7b14\u89e6,\u9700\u8981\u5728\u3010\u753b\u7b14\u3011\u63a7\u5236\u9762\u677f\u4e2d\u8bbe\u7f6e\u7684\u4e00\u9879\n\u91cd\u8981\u53c2\u6570\u662f( \uff09\u3002\nOption:\n(A) \u3010\u6296\u52a8\u3011\n(B) \u3010\u6e7f\u8fb9\u3011\n(C) \u3010\u989c\u8272\u52a8\u6001\u3011\n(D) \u3010\u6e10\u9690\u3011\nGPT-4V: \u8fd9\u5f20\u56fe\u7247\u663e\u793a\u7684\u662f\u4e00\u7cfb\u5217\u7b14\u89e6\uff0c\u5b83\u4eec\u4f3c\u4e4e\u4ece\u6e05\u6670\u9010\u6e10\u53d8\u5f97\u6a21\u7cca\u548c\u6de1\u51fa\u3002\u5728\u6570\u5b57\u7ed8\u753b\u8f6f\u4ef6\u4e2d\uff0c\n\u8981\u5b9e\u73b0\u8fd9\u79cd\u6548\u679c\uff0c\u901a\u5e38\u9700\u8981\u8c03\u6574\u7684\u662f\u753b\u7b14\u7684\u6e10\u9690\u6548\u679c\uff0c\u8fd9\u6837\u7b14\u89e6\u5c31\u4f1a\u968f\u7740\u65f6\u95f4\u6216\u7b14\u89e6\u7684\u957f\u5ea6\u800c\u9010\u6e10\u51cf\n\u5c11\u5176\u4e0d\u900f\u660e\u5ea6\uff0c\u4ece\u800c\u5b9e\u73b0\u6de1\u51fa\u7684\u6548\u679c\u3002\n\u6b63\u786e\u7b54\u6848\u662f\uff1a(D) \u3010\u6e10\u9690\u3011\nGround Truth: (D) \u3010\u6e10\u9690\u3011\n<\u56fe\u7247 1>\nFigure B12: A sample correct case of Art and Design (subcategory: Design).\nBack to List of figures\n31\nPREPRINT\n\u827a\u672f\u4e0e\u8bbe\u8ba1:\u8bbe\u8ba1\nQuestion:  \u5224\u65ad\u4e0b\u9762\u9648\u8ff0\u5bf9\u9519\uff1a<\u56fe\u7247 1>\u7684\u65e5\u5386\u90e8\u5206\u662f\u7531\u4e8e\u4f7f\u7528\u4e86\u201c\u4eff\u5236\u56fe\u7ae0\u5de5\u5177\u201d\u800c\u53d8\u82b1\u3002\nOption:\n(A) \u6b63\u786e\n(B) \u9519\u8bef\nGPT-4V: \u5bf9\u4e0d\u8d77\uff0c\u6211\u65e0\u6cd5\u63d0\u4f9b\u5173\u4e8e\u56fe\u7247\u5185\u5bb9\u7684\u5e2e\u52a9\u3002\nGround Truth: (B) \u9519\u8bef\n<\u56fe\u7247 1>\nError Category: Reject to Answer\nError Reason: In this response, the GPT-4V rejects to answet the question. This choice may be due to \na misunderstanding of its image processing capabilities. In fact, GPT-4V has some image analysis \ncapabilities, and is able to perform basic visual content analysis on supplied images, such as \nidentifying objects, colors, layouts, etc. In this question, the user asked about the image processing \ncapabilities of GPT-4V. In this question, the user is asking about the impact of image editing \ntechniques (\"faux stamp tools\"), which is something that GPT-4V could theoretically answer by \nanalyzing the visual characteristics of the image. In addition, the model may not have properly \nunderstood the context of the question and mistakenly assumed that it was a request involving the \ndirect modification or generation of image content, a feature that GPT-4V does not currently have. \nIn reality, however, the question is only asking to analyze the image content, not to modify the \nimage. \nFigure B13: A sample error case of Art and Design (subcategory: Design).\nBack to List of figures\n32\nPREPRINT\n\u827a\u672f\u4e0e\u8bbe\u8ba1:\u8bbe\u8ba1\nQuestion:  \u4e0b\u9762\u6240\u5199\u51fa\u4e0b\u56fe<\u56fe\u7247 1>\u6240\u793a\u5bf9\u8c61\u7684\u540d\u79f0\u3001\u671d\u4ee3\u3001\u5730\u70b9_\nOption:\n(A) \u4e34\u6f7c\u59dc\u5be8\u539f\u59cb\u793e\u4f1a\u6c0f\u65cf\u6751\u843d\u9057\u5740\u5e73\u9762\n(B) \u9655\u897f\u957f\u5b89\u53bf\u5ba2\u7701\u5e84\u539f\u59cb\u793e\u4f1a\u534a\u5730\u7a74\u4f4f\u5b85\u9057\u5740\u5e73\u9762\n(C) \u5043\u5e08\u4e8c\u91cc\u5934\u5546\u4ee3\u5bab\u6bbf\u9057\u5740\u5e73\u9762\n(D) \u5c90\u5c71\u51e4\u96cf\u897f\u5468\u5efa\u7b51\u9057\u5740\u5e73\u9762\nGPT-4V: \u5f88\u62b1\u6b49\uff0c\u6211\u65e0\u6cd5\u76f4\u63a5\u8bc6\u522b\u56fe\u7247\u4e2d\u7684\u9057\u5740\u5e73\u9762\u56fe\u5e76\u5c06\u5176\u4e0e\u7279\u5b9a\u7684\u5386\u53f2\u9057\u5740\u76f8\u5339\u914d\u3002\u8fd9\u9700\u8981\u4e13\u4e1a\n\u7684\u8003\u53e4\u6216\u5386\u53f2\u77e5\u8bc6\uff0c\u4ee5\u53ca\u5bf9\u7279\u5b9a\u9057\u5740\u7684\u8be6\u7ec6\u4e86\u89e3\u3002\u5982\u679c\u6ca1\u6709\u66f4\u591a\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u6216\u6570\u636e\uff0c\u6211\u65e0\u6cd5\u786e\u5b9a\u6b63\n\u786e\u7b54\u6848\u3002\u5982\u679c\u60a8\u80fd\u63d0\u4f9b\u66f4\u591a\u7684\u80cc\u666f\u4fe1\u606f\u6216\u7ec6\u8282\uff0c\u6211\u4f1a\u5c3d\u529b\u5e2e\u52a9\u60a8\u3002\nGround Truth: (D) \u5c90\u5c71\u51e4\u96cf\u897f\u5468\u5efa\u7b51\u9057\u5740\u5e73\u9762\n<\u56fe\u7247 1>\nError Category: Reject to Answer\nError Reason: GPT-4V chooses to refuse to answer directly rather than attempting to reason \nlogically or provide historical knowledge based on the options given in the question, reflecting a \nrejecting attitude. In this case, even though it is not possible to analyze the pictures directly, the \nmodel can still use its extensive knowledge base to provide contextual information about the \noptions, thus assisting the user in reasoning about the possible correct answers. For example, \ndescribing the historical sites or eras mentioned in each option, or providing some of the common \nfeatures and historical context of these sites. \nFigure B14: A sample error case of Art and Design (subcategory: Design).\nBack to List of figures\n33\nPREPRINT\n\u827a\u672f\u4e0e\u8bbe\u8ba1:\u8bbe\u8ba1\nQuestion:  \u5bf9\u4e8e\u4e0b\u9762\u6240\u793a\u7684\u6548\u679c\u662f\u901a\u8fc7\u4ec0\u4e48\u6765\u5b9e\u73b0\u7684\uff1f<\u56fe\u7247 1>\uff08\uff09\nOption:\n(A) \u8499\u677f\u529f\u80fd\u7684\u4f7f\u7528\n(B) \u83dc\u5e27\u5e76\u5e27\u52a8\u753b\u7684\u4f7f\u7528\n(C) \u8865\u95f4\u52a8\u753b\u7684\u4f7f\u7528\n(D) \u4ee5\u4e0aA B C\u90fd\u53ef\u4ee5\u6765\u5b9e\u73b0\nGPT-4V: \u62b1\u6b49\uff0c\u7531\u4e8e\u6280\u672f\u9650\u5236\uff0c\u6211\u65e0\u6cd5\u67e5\u770b\u56fe\u7247\u3002\u4f46\u662f\uff0c\u6839\u636e\u4f60\u63d0\u4f9b\u7684\u9009\u9879\uff0c\u6211\u53ef\u4ee5\u63d0\u4f9b\u4e00\u4e9b\u5173\u4e8e\u8fd9\n\u4e9b\u9009\u9879\u7684\u4fe1\u606f\uff0c\u8fd9\u53ef\u80fd\u4f1a\u5e2e\u52a9\u4f60\u786e\u5b9a\u6b63\u786e\u7b54\u6848\u3002\n(A) \u8499\u677f\u529f\u80fd\u7684\u4f7f\u7528 - \u8499\u677f\u529f\u80fd\u53ef\u4ee5\u7528\u6765\u5728\u52a8\u753b\u4e2d\u521b\u5efa\u7279\u5b9a\u5f62\u72b6\u7684\u53ef\u89c6\u533a\u57df\uff0c\u4f7f\u5f97\u53ea\u6709\u5728\u8499\u677f\u5f62\u72b6\n\u5185\u7684\u90e8\u5206\u53ef\u89c1\u3002\n(B) \u5e27\u5e76\u5e27\u52a8\u753b\u7684\u4f7f\u7528 - \u5e27\u5e76\u5e27\u52a8\u753b\u662f\u4e00\u79cd\u4f20\u7edf\u52a8\u753b\u6280\u672f\uff0c\u5176\u4e2d\u6bcf\u4e00\u5e27\u90fd\u662f\u5355\u72ec\u7ed8\u5236\u7684\uff0c\u4ee5\u521b\u5efa\u52a8\n\u753b\u6548\u679c\u3002\n(C) \u8865\u95f4\u52a8\u753b\u7684\u4f7f\u7528 - \u8865\u95f4\u52a8\u753b\u662f\u4e00\u79cd\u52a8\u753b\u6280\u672f\uff0c\u5176\u4e2d\u52a8\u753b\u7684\u5f00\u59cb\u548c\u7ed3\u675f\u70b9\u662f\u5b9a\u4e49\u7684\uff0c\u800c\u4e2d\u95f4\u7684\u5e27\n\u662f\u7531\u8f6f\u4ef6\u81ea\u52a8\u751f\u6210\u7684\u3002\n(D) \u4ee5\u4e0aA B C\u90fd\u53ef\u4ee5\u6765\u5b9e\u73b0 - \u8fd9\u4e2a\u9009\u9879\u8868\u660e\u6240\u6709\u4e0a\u8ff0\u6280\u672f\u90fd\u53ef\u4ee5\u7528\u6765\u5b9e\u73b0\u6240\u63cf\u8ff0\u7684\u6548\u679c\u3002\n\u7531\u4e8e\u6211\u65e0\u6cd5\u770b\u5230\u56fe\u7247\uff0c\u6211\u65e0\u6cd5\u786e\u5b9a\u54ea\u79cd\u6280\u672f\u88ab\u7528\u6765\u521b\u5efa\u7279\u5b9a\u7684\u6548\u679c\u3002\u4f46\u662f\uff0c\u5982\u679c\u6548\u679c\u6d89\u53ca\u5f62\u72b6\u7684\u53ef\u89c6\n\u533a\u57df\u53d8\u5316\uff0c\u53ef\u80fd\u4f7f\u7528\u4e86\u8499\u677f\u529f\u80fd\uff1b\u5982\u679c\u6548\u679c\u770b\u8d77\u6765\u975e\u5e38\u624b\u5de5\u548c\u8be6\u7ec6\uff0c\u53ef\u80fd\u4f7f\u7528\u4e86\u5e27\u5e76\u5e27\u52a8\u753b\uff1b\u5982\u679c\u6548\n\u679c\u6d89\u53ca\u5e73\u6ed1\u7684\u8fc7\u6e21\uff0c\u53ef\u80fd\u4f7f\u7528\u4e86\u8865\u95f4\u52a8\u753b\u3002\u5982\u679c\u6548\u679c\u770b\u8d77\u6765\u53ef\u80fd\u7531\u591a\u79cd\u6280\u672f\u5b9e\u73b0\uff0c\u90a3\u4e48\u9009\u9879(D)\u53ef\u80fd\n\u662f\u6b63\u786e\u7684\u3002\nGround Truth: (A) \u8499\u677f\u529f\u80fd\u7684\u4f7f\u7528\n<\u56fe\u7247 1>\nError Category: Perceptual Error\nError Reason: GPT-4V explicitly shows that it is unable to view images and therefore cannot \ndirectly analyze their content in order to answer questions. This demonstrates the model's \nlimitations in processing image-related information, even if it possesses some degree of image \nprocessing capability. The model attempts to compensate for the lack of perception of image \ncontent by providing a general explanation of each option, but this approach cannot accurately \nanswer questions specific to image content. While the model's explanations are technically correct, \nthey are not directly applicable to a specific image, as this requires direct analysis of the image \ncontent. \nFigure B15: A sample error case of Art and Design (subcategory: Design).\nBack to List of figures\n34\nPREPRINT\n\u827a\u672f\u4e0e\u8bbe\u8ba1:\u8bbe\u8ba1\nQuestion:  \u5728\u3010\u6e10\u53d8\u7f16\u8f91\u5668\u3011\u5bf9\u8bdd\u6846\u4e2d\u6709\u5982\u56fe\u6240\u793a\u7684\u989c\u8272\u6761\uff0c\u4f4d\u4e8e\u8272\u6761\u4e0a\u90e8\u548c\u4e0b\u90e8\u7684\u5c0f\u65b9\u5757\u5206\u522b\u7528\u6765\n\u8c03\u8282\u6e10\u53d8\u8272\u7684\uff08 \uff09\u5c5e\u6027\u3002<\u56fe\u7247 1>\nOption:\n(A) \u3010\u989c\u8272\u3011\u548c\u3010\u4f4d\u7f6e\u3011\n(B) \u3010\u989c\u8272\u3011\u548c\u3010\u4e0d\u900f\u660e\u5ea6\u3011\n(C) \u3010\u4e0d\u900f\u660e\u5ea6\u3011\u548c\u3010\u989c\u8272\u3011\n(D) \u3010\u4f4d\u7f6e\u3011\u548c\u3010\u5e73\u6ed1\u5ea6\u3011\nGPT-4V: \u5728\u6e10\u53d8\u7f16\u8f91\u5668\u4e2d\uff0c\u4f4d\u4e8e\u989c\u8272\u6761\u4e0a\u90e8\u7684\u5c0f\u65b9\u5757\u901a\u5e38\u7528\u6765\u8c03\u8282\u6e10\u53d8\u8272\u7684\u3010\u989c\u8272\u3011\u5c5e\u6027\uff0c\u800c\u4f4d\u4e8e\u989c\n\u8272\u6761\u4e0b\u90e8\u7684\u5c0f\u65b9\u5757\u5219\u7528\u6765\u8c03\u8282\u3010\u4e0d\u900f\u660e\u5ea6\u3011\u3002\n\u56e0\u6b64\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a(B) \u3010\u989c\u8272\u3011\u548c\u3010\u4e0d\u900f\u660e\u5ea6\u3011\nGround Truth: (C) \u3010\u4e0d\u900f\u660e\u5ea6\u3011\u548c\u3010\u989c\u8272\u3011\n<\u56fe\u7247 1>\nError Category: Reasoning Error\nError Reason: The error made by GPT-4V may stem from a misunderstanding of the function of the \nsmall square of the color bar in the Gradient Editor. In graphic editing software, the design and \nfunctionality of the gradient editor may vary from software to software. The model may infer \nanswers based on general knowledge from its training data or the functionality of a particular \nsoftware without accurately grasping the details of a particular situation in the question.\nFigure B16: A sample error case of Art and Design (subcategory: Design).\nBack to List of figures\n35\nPREPRINT\n\u827a\u672f\u4e0e\u8bbe\u8ba1:\u8bbe\u8ba1\nQuestion:  \u8bf7\u9009\u62e9\u56fe\u4e2d\u622a\u9762\u9644\u5408\u6784\u9020\u8981\u6c42\u7684\u6df7\u51dd\u571f\u57fa\u7840\u3002\uff08\uff09\nOption:\n(A) <\u56fe\u7247 1>\n(B) <\u56fe\u7247 2>\n(C) <\u56fe\u7247 3>\n(D) <\u56fe\u7247 4>\nGPT-4V: \u6839\u636e\u63d0\u4f9b\u7684\u56fe\u7247\uff0c\u6211\u4eec\u9700\u8981\u9009\u62e9\u4e00\u4e2a\u7b26\u5408\u6784\u9020\u8981\u6c42\u7684\u6df7\u51dd\u571f\u57fa\u7840\u622a\u9762\u3002\u5728\u571f\u6728\u5de5\u7a0b\u4e2d\uff0c\u6df7\u51dd\n\u571f\u57fa\u7840\u7684\u8bbe\u8ba1\u9700\u8981\u8003\u8651\u627f\u8f7d\u529b\u3001\u7a33\u5b9a\u6027\u548c\u7ed3\u6784\u7684\u8010\u4e45\u6027\u3002\u57fa\u7840\u901a\u5e38\u9700\u8981\u6709\u8db3\u591f\u7684\u5e95\u9762\u79ef\u6765\u5206\u6563\u4e0a\u90e8\u7ed3\n\u6784\u7684\u8377\u8f7d\uff0c\u5e76\u4e14\u6709\u9002\u5f53\u7684\u5f62\u72b6\u4ee5\u9632\u6b62\u4e0d\u5747\u5300\u6c89\u964d\u3002\u4ece\u63d0\u4f9b\u7684\u56db\u4e2a\u9009\u9879\u4e2d\uff1a\n- \u56fe\u7247 1 \u663e\u793a\u4e86\u4e00\u4e2a\u5e26\u6709\u6269\u5927\u5e95\u90e8\u7684\u57fa\u7840\uff0c\u8fd9\u6709\u52a9\u4e8e\u5206\u6563\u8377\u8f7d\u3002\n- \u56fe\u7247 2 \u663e\u793a\u4e86\u4e00\u4e2a\u7c7b\u4f3c\u7684\u57fa\u7840\uff0c\u4f46\u5e95\u90e8\u6ca1\u6709\u6269\u5927\u3002\n- \u56fe\u7247 3 \u663e\u793a\u4e86\u4e00\u4e2a\u57fa\u7840\uff0c\u5e95\u90e8\u6269\u5927\u5f97\u975e\u5e38\u5e73\u5766\uff0c\u53ef\u80fd\u4e0d\u5229\u4e8e\u8377\u8f7d\u7684\u4f20\u9012\u3002\n- \u56fe\u7247 4 \u663e\u793a\u4e86\u4e00\u4e2a\u57fa\u7840\uff0c\u5e95\u90e8\u6269\u5927\u4e14\u5f62\u72b6\u4ecb\u4e8e\u56fe\u7247 1 \u548c\u56fe\u7247 3 \u4e4b\u95f4\u3002\n\u901a\u5e38\uff0c\u57fa\u7840\u7684\u5e95\u90e8\u6269\u5927\u662f\u4e3a\u4e86\u63d0\u4f9b\u66f4\u5927\u7684\u627f\u8f7d\u9762\u79ef\uff0c\u51cf\u5c11\u57fa\u7840\u4e0b\u7684\u571f\u538b\u529b\uff0c\u4ece\u800c\u63d0\u9ad8\u7ed3\u6784\u7684\u7a33\u5b9a\u6027\u3002\n\u56e0\u6b64\uff0c\u6211\u4eec\u901a\u5e38\u4f1a\u9009\u62e9\u90a3\u4e9b\u5e95\u90e8\u6709\u9002\u5f53\u6269\u5927\u7684\u57fa\u7840\u8bbe\u8ba1\u3002\n\u5728\u8fd9\u56db\u4e2a\u9009\u9879\u4e2d\uff0c\u56fe\u7247 1 \u7684\u57fa\u7840\u5e95\u90e8\u6269\u5927\u770b\u8d77\u6765\u6700\u4e3a\u5408\u7406\uff0c\u56e0\u4e3a\u5b83\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5747\u5300\u7684\u6269\u5c55\uff0c\u6709\u52a9\u4e8e\n\u7a33\u5b9a\u548c\u5206\u6563\u8377\u8f7d\u3002\n\u56e0\u6b64\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a(A) <\u56fe\u7247 1>\nGround Truth: (B) <\u56fe\u7247 2>\n<\u56fe\u7247 1>\nError Category: Reasoning Error\nError Reason: GPT-4V incorrectly applied its understanding of civil engineering principles to image \nanalysis when analyzing image content. The model attempted to infer the characteristics of each \nimage based on its general knowledge of concrete foundation design, but did not properly apply \nthis knowledge to the specific image analysis. This may be due to the model's lack of sufficient \nexpertise to accurately identify and evaluate the construction requirements of concrete foundations.\n<\u56fe\u7247 2>\n<\u56fe\u7247 4>\n<\u56fe\u7247 3>\nFigure B17: A sample error case of Art and Design (subcategory: Design).\nBack to List of figures\n36\nPREPRINT\n\u827a\u672f\u4e0e\u8bbe\u8ba1:\u8bbe\u8ba1\nQuestion:  \u5982<\u56fe\u7247 1>\u6240\u793a,\u56fe\u4e2d\u6587\u5b57\u90e8\u5206\u7684\u6587\u5b57\u900f\u89c6\u6548\u679c\u662f\u901a\u8fc7\u201c\u53d8\u5f62\u6587\u672c\u201d\u4e2d\u7684\u54ea\u79cd\u6837\u5f0f\u4ea7\u751f\u7684\uff1f\nOption:\n(A) \u8d1d\u58f3\n(B) \u65d7\u5e1c\n(C) \u4e0b\u5f27\n(D) \u6247\u5f62\nGPT-4V: \u56fe\u4e2d\u6587\u5b57\u90e8\u5206\u7684\u6587\u5b57\u900f\u89c6\u6548\u679c\u662f\u901a\u8fc7\u201c\u53d8\u5f62\u6587\u672c\u201d\u4e2d\u7684\u201c\u4e0b\u5f27\u201d\u6837\u5f0f\u4ea7\u751f\u7684\u3002\n\u6b63\u786e\u7b54\u6848\uff1a(C) \u4e0b\u5f27\nGround Truth: (D) \u6247\u5f62\n<\u56fe\u7247 1>\nError Category: Lack of Knowledge\nError Reason: The source of the error is the lack of expertise in GPT-4V, i.e., the lack of proper \nrecognition and understanding of the specific text style in the image. Although the text in the image \nshows an arc-like visual effect, the correct answer should be \"(D) Fan\". This suggests a knowledge \nlimitation in the model's ability to understand and differentiate between different text morphing \nstyles. The model relies on the information in its training data when answering such questions. If \nthere is insufficient information about specific text styles (e.g., fan, downward arc, etc.) in the \ntraining data, the model may not be able to accurately recognize and understand the characteristics \nof these styles and application scenarios. \nFigure B18: A sample error case of Art and Design (subcategory: Design).\nBack to List of figures\n37\nPREPRINT\n\u827a\u672f\u4e0e\u8bbe\u8ba1:\u97f3\u4e50\nQuestion:  <\u56fe\u7247 1>\u662f\u6c11\u65cf\u7ba1\u5f26\u4e50\u961f\u7684\u4f4d\u7f6e\u56fe\uff0c\u2460\u6240\u5728\u7684\u533a\u57df\u901a\u5e38\u662f\u54ea\u79cd\u4e50\u5668\uff1f\uff08\uff09\nOption:\n(A) \u62c9\u5f26\u4e50\u5668\n(B) \u6253\u51fb\u4e50\u5668\n(C) \u5f39\u62e8\u4e50\u5668\n(D) \u5439\u7ba1\u4e50\u5668\nGPT-4V: \u6b63\u786e\u7b54\u6848\u662f\uff1a(A) \u62c9\u5f26\u4e50\u5668\n\u5728\u4f20\u7edf\u7684\u7ba1\u5f26\u4e50\u961f\u5e03\u5c40\u4e2d\uff0c\u2460\u6240\u5728\u7684\u533a\u57df\u901a\u5e38\u662f\u4e3a\u7b2c\u4e00\u5c0f\u63d0\u7434\u548c\u7b2c\u4e8c\u5c0f\u63d0\u7434\u9884\u7559\u7684\u4f4d\u7f6e\uff0c\u5b83\u4eec\u5c5e\u4e8e\u62c9\n\u5f26\u4e50\u5668\u3002\u8fd9\u4e2a\u533a\u57df\u901a\u5e38\u4f4d\u4e8e\u6307\u6325\u7684\u5de6\u624b\u8fb9\uff0c\u9760\u8fd1\u89c2\u4f17\u3002\nGround Truth: (A) \u62c9\u5f26\u4e50\u5668\n<\u56fe\u7247 1>\nFigure B19: A sample correct case of Art and Design (subcategory: Music).\nBack to List of figures\n38\nPREPRINT\n\u827a\u672f\u4e0e\u8bbe\u8ba1:\u97f3\u4e50\nQuestion:  \u5173\u4e8e\u4e0b\u56fe<\u56fe\u7247 1>\u4eac\u5267\u300a\u6c99\u5bb6\u6d5c\u300b\u4e2d\u963f\u5e86\u5ac2\u7684\u89d2\u8272\u5c5e\u4e8e\u4ec0\u4e48\u89d2\u8272\uff08\uff09\nOption:\n(A) \u65e6\u884c\n(B) \u751f\u884c\n(C) \u4e11\u884c\n(D) \u51c0\u884c\nGPT-4V: \u963f\u5e86\u5ac2\u662f\u4eac\u5267\u300a\u6c99\u5bb6\u6d5c\u300b\u4e2d\u7684\u4e00\u4e2a\u89d2\u8272\uff0c\u6839\u636e\u5979\u7684\u670d\u88c5\u548c\u89d2\u8272\u6027\u522b\uff0c\u5979\u5c5e\u4e8e\u65e6\u884c\u3002\u65e6\u884c\u662f\u4eac\n\u5267\u4e2d\u4e13\u6307\u626e\u6f14\u5973\u6027\u89d2\u8272\u7684\u884c\u5f53\u3002\n\u6b63\u786e\u7b54\u6848\u662f\uff1a(A) \u65e6\u884c\nGround Truth: (A) \u65e6\u884c\n<\u56fe\u7247 1>\nFigure B20: A sample correct case of Art and Design (subcategory: Music).\nBack to List of figures\n39\nPREPRINT\n\u827a\u672f\u4e0e\u8bbe\u8ba1:\u97f3\u4e50\nQuestion: \u4e0b\u5217\u54ea\u4e9b\u8c03\u5f0f\u97f3\u9636\u4e0e\u97f3\u5217<\u56fe\u7247 1>\u7684\u7ed3\u6784\u76f8\u4f3c()\nOption: \n(A) D\u96c5\u4e50\u8f83\u5927\u8c03\n(B) D\u548c\u58f0\u5927\u8c03\u5f0f\n(C) D\u6e05\u4e50\u8c03\u5f0f\n(D) D\u81ea\u7136\u5c0f\u8c03\u5f0f\nGPT-4V: \u5f88\u62b1\u6b49\uff0c\u7531\u4e8e\u56fe\u7247\u4e2d\u7684\u97f3\u7b26\u548c\u8c31\u53f7\u65e0\u6cd5\u88ab\u6211\u76f4\u63a5\u89e3\u8bfb\uff0c\u6211\u65e0\u6cd5\u63d0\u4f9b\u4e00\u4e2a\u786e\u5207\u7684\u7b54\u6848\u3002\u4f46\u662f\uff0c\n\u6211\u53ef\u4ee5\u6839\u636e\u60a8\u63d0\u4f9b\u7684\u9009\u9879\u6765\u89e3\u91ca\u6bcf\u4e2a\u8c03\u5f0f\u7684\u7279\u70b9\uff0c\u8fd9\u6837\u60a8\u53ef\u4ee5\u6839\u636e\u56fe\u7247\u4e2d\u7684\u97f3\u5217\u6765\u5224\u65ad\u54ea\u4e2a\u8c03\u5f0f\u4e0e\u4e4b\n\u76f8\u4f3c\u3002\n(A) D\u96c5\u4e50\u8f83\u5927\u8c03 - \u8fd9\u4e2a\u9009\u9879\u53ef\u80fd\u662f\u6307D\u5927\u8c03\u7684\u4e00\u4e2a\u53d8\u79cd\uff0c\u4f46\u201c\u96c5\u4e50\u8f83\u5927\u8c03\u201d\u4e0d\u662f\u4e00\u4e2a\u6807\u51c6\u7684\u97f3\u4e50\u672f\n\u8bed\u3002\n(B) D\u548c\u58f0\u5927\u8c03\u5f0f - D\u548c\u58f0\u5927\u8c03\u5f0f\u4ee5D\u4e3a\u4e3b\u97f3\uff0c\u5176\u7279\u70b9\u662f\u5728\u7b2c\u4e03\u97f3\u4e0a\u5347\u534a\u97f3\u5f62\u6210\u5347\u9ad8\u7684\u4e03\u5ea6\u97f3\uff0c\u4ee5\u5f62\u6210\n\u4e00\u4e2a\u5b8c\u5168\u7684\u4e3b\u548c\u5f26\u3002\n(C) D\u6e05\u4e50\u8c03\u5f0f - \u8fd9\u4e2a\u9009\u9879\u53ef\u80fd\u662f\u6307D\u65cb\u5f8b\u5c0f\u8c03\u5f0f\u6216D\u81ea\u7136\u5c0f\u8c03\u5f0f\u7684\u4e00\u4e2a\u53d8\u79cd\uff0c\u4f46\u201c\u6e05\u4e50\u8c03\u5f0f\u201d\u4e0d\u662f\u4e00\n\u4e2a\u6807\u51c6\u7684\u97f3\u4e50\u672f\u8bed\u3002\n(D) D\u81ea\u7136\u5c0f\u8c03\u5f0f - D\u81ea\u7136\u5c0f\u8c03\u5f0f\u4ee5D\u4e3a\u4e3b\u97f3\uff0c\u5176\u97f3\u9636\u7ed3\u6784\u4e3a\u5168\u534a\u5168\u5168\u534a\u5168\u5168\uff08\u5168\u8868\u793a\u5168\u97f3\uff0c\u534a\u8868\u793a\u534a\n\u97f3\uff09\u3002\n\u5982\u679c\u60a8\u80fd\u591f\u6839\u636e\u97f3\u7b26\u7684\u5347\u964d\u53f7\u548c\u5b83\u4eec\u7684\u95f4\u9694\u6765\u5224\u65ad\u97f3\u9636\u7684\u7ed3\u6784\uff0c\u60a8\u5c31\u53ef\u4ee5\u9009\u62e9\u4e0e\u56fe\u7247\u4e2d\u97f3\u5217\u7ed3\u6784\u6700\u76f8\n\u4f3c\u7684\u8c03\u5f0f\u3002\n\u9884\u6d4b\u7b54\u6848\u4e3a\uff1aABCD\nGround Truth: (A) D\u96c5\u4e50\u8f83\u5927\u8c03, (C) D\u6e05\u4e50\u8c03\u5f0f, (D) D\u81ea\u7136\u5c0f\u8c03\u5f0f1V\n<\u56fe\u7247 1>\nError Category: Reject to Answer\nError Reason: GPT-4V failed to accurately recognize and analyze the provided music sheet image, \nthus it rejected to answer the question. However, the model gave the appropriate tonal analyses for \nthe four options given in texts, indicating that the model actually possesses a certain amount of \nknowledge in the fundamentals of music theory, but lacks the appropriate perceptual and \ninterpretive analyzing ability for the current image.\nFigure B21: A sample error case of Art and Design (subcategory: Music).\nBack to List of figures\n40\nPREPRINT\n\u827a\u672f\u4e0e\u8bbe\u8ba1:\u97f3\u4e50\nQuestion:  \u5173\u4e8e\u4e0b\u56fe<\u56fe\u7247 1>\u620f\u66f2\u300a\u5218\u5927\u54e5\u8bb2\u8bdd\u7406\u592a\u504f\u300b\u5c5e\u4e8e____\u3002\nOption:\n(A) \u8d8a\u5267\n(B) \u4eac\u5267\n(C) \u8c6b\u5267\n(D) \u9ec4\u6885\u620f\nGPT-4V: \u8fd9\u5f20\u56fe\u7247\u5c55\u793a\u4e86\u4e00\u7fa4\u7a7f\u7740\u534e\u4e3d\u620f\u670d\u7684\u6f14\u5458\uff0c\u4ed6\u4eec\u7684\u670d\u9970\u548c\u5316\u5986\u98ce\u683c\u53ef\u4ee5\u5e2e\u52a9\u6211\u4eec\u5224\u65ad\u4ed6\u4eec\u53ef\n\u80fd\u6b63\u5728\u8868\u6f14\u7684\u620f\u66f2\u7c7b\u578b\u3002\u7136\u800c\uff0c\u7531\u4e8e\u4e2d\u56fd\u7684\u620f\u66f2\u79cd\u7c7b\u7e41\u591a\uff0c\u4e14\u4e0d\u540c\u620f\u66f2\u4e4b\u95f4\u7684\u670d\u9970\u548c\u5316\u5986\u6709\u65f6\u5019\u53ef\u80fd\n\u76f8\u4f3c\uff0c\u56e0\u6b64\u4ec5\u51ed\u56fe\u7247\u6765\u786e\u5b9a\u786e\u5207\u7684\u620f\u66f2\u7c7b\u578b\u53ef\u80fd\u4f1a\u6709\u4e00\u5b9a\u7684\u6311\u6218\u6027\u3002\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d\uff0c\u6f14\u5458\u4eec\u7684\u670d\u9970\u8272\n\u5f69\u9c9c\u8273\uff0c\u5934\u9970\u4e5f\u975e\u5e38\u7cbe\u81f4\uff0c\u8fd9\u4e9b\u7279\u70b9\u5728\u591a\u79cd\u5730\u65b9\u620f\u4e2d\u90fd\u6709\u53ef\u80fd\u51fa\u73b0\u3002\u6ca1\u6709\u66f4\u591a\u7684\u6587\u5316\u6216\u5730\u57df\u7ebf\u7d22\uff0c\u5f88\n\u96be\u786e\u5b9a\u662f\u54ea\u4e00\u79cd\u620f\u66f2\u3002\u56e0\u6b64\uff0c\u6211\u65e0\u6cd5\u4ec5\u51ed\u8fd9\u5f20\u56fe\u7247\u63d0\u4f9b\u4e00\u4e2a\u786e\u5207\u7684\u7b54\u6848\u3002\u5982\u679c\u4f60\u6709\u66f4\u591a\u5173\u4e8e\u8fd9\u90e8\u620f\u66f2\n\u7684\u4fe1\u606f\uff0c\u6bd4\u5982\u5b83\u7684\u8d77\u6e90\u3001\u6d41\u884c\u5730\u533a\u6216\u8005\u7279\u5b9a\u7684\u8868\u6f14\u98ce\u683c\uff0c\u90a3\u4e48\u6211\u53ef\u80fd\u80fd\u591f\u5e2e\u52a9\u4f60\u66f4\u51c6\u786e\u5730\u5224\u65ad\u3002\u5728\u6ca1\n\u6709\u989d\u5916\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u6211\u5efa\u8bae\u9009\u62e9\u6700\u53ef\u80fd\u6b63\u786e\u7684\u9009\u9879\u3002\n\u6b63\u786e\u7b54\u6848\uff1a\u65e0\u6cd5\u786e\u5b9a\u3002\nGround Truth: (C) \u8c6b\u5267\n<\u56fe\u7247 1>\nError Category: Perceptual Error\nError Reason: GPT-4V was unable to correctly recognize and interpret the information in the \nimages to answer the questions. This suggests a perceptual limitation in the model's ability to \nprocess image input. Although the model was able to recognize some basic elements in the image \n(e.g., actors' costumes and make-up styles), it failed to accurately associate these elements with \nspecific types of theatre. This may be due to the fact that the model lacks a sufficiently in-depth \nunderstanding of the visual features and stylistic details of various types of Chinese theater. \nFigure B22: A sample error case of Art and Design (subcategory: Music).\nBack to List of figures\n41\nPREPRINT\n\u827a\u672f\u4e0e\u8bbe\u8ba1:\u97f3\u4e50\nQuestion:  \u5173\u4e8e\u4e0b\u56fe<\u56fe\u7247 1>\u8fd9\u6bb5\u4e3b\u9898\u65cb\u5f8b\u51fa\u81ea\u4e8e\u90a3\u90e8\u6b4c\u66f2\uff08\uff09\nOption:\n(A) \u300a\u6ce2\u83b1\u7f57\u300b\n(B) \u300a\u7b2c\u4e94\uff08\u547d\u8fd0\uff09\u4ea4\u54cd\u66f2\u300b\n(C) \u300a\u5361\u95e8\u5e8f\u66f2\u300b\n(D) \u300a\u6447\u7bee\u66f2\u300b\nGPT-4V: \u62b1\u6b49\uff0c\u6211\u65e0\u6cd5\u76f4\u63a5\u8bc6\u522b\u4e50\u8c31\u4e2d\u7684\u65cb\u5f8b\u5bf9\u5e94\u7684\u6b4c\u66f2\u3002\u4f46\u662f\uff0c\u5982\u679c\u4f60\u80fd\u591f\u6f14\u594f\u6216\u54fc\u5531\u8fd9\u6bb5\u65cb\u5f8b\uff0c\n\u6216\u8005\u63d0\u4f9b\u66f4\u591a\u7684\u4fe1\u606f\uff0c\u6211\u53ef\u80fd\u80fd\u591f\u5e2e\u52a9\u4f60\u786e\u5b9a\u8fd9\u6bb5\u65cb\u5f8b\u51fa\u81ea\u54ea\u90e8\u6b4c\u66f2\u3002\nGround Truth: (B) \u300a\u7b2c\u4e94\uff08\u547d\u8fd0\uff09\u4ea4\u54cd\u66f2\u300b\n<\u56fe\u7247 1>\nError Category: Perceptual Error\nError Reason: GPT-4V model has the ability to recognize images, but in this particular case it did \nnot correctly utilize this ability to recognize the sheet music in the image and thus the \ncorresponding song. The model's response, \"I cannot directly recognize the song that corresponds to \nthe melody in the sheet music,\" suggests that the model is not aware of its ability to process image \ninput, especially in interpreting sheet music. This may be due to the model's strategies or \nlimitations in processing image input that are not adapted to this particular query. \nFigure B23: A sample error case of Art and Design (subcategory: Music).\nBack to List of figures\n42\nPREPRINT\n\u827a\u672f\u4e0e\u8bbe\u8ba1:\u97f3\u4e50\nQuestion:  \u5728\u56fe\u4e2d\u952e\u76d8<\u56fe\u7247 1>\u4e2d\uff0c\u4ece\u53f3\u9762\u5f00\u59cb\u7684\u7b2c\u56db\u4e2a\u9ed1\u952e\u7684\u97f3\u540d\u662f\u4ec0\u4e48\nOption:\n(A) \u964dD\n(B) E\n(C) \u964dE\n(D) \u964dF\nGPT-4V: \u5728\u94a2\u7434\u952e\u76d8\u4e0a\uff0c\u9ed1\u952e\u4f4d\u4e8e\u4e24\u4e2a\u767d\u952e\u4e4b\u95f4\uff0c\u53ef\u4ee5\u8868\u793a\u4e3a\u8fd9\u4e24\u4e2a\u767d\u952e\u7684\u5347\u6216\u964d\u97f3\u3002\u4ece\u53f3\u4fa7\u5f00\u59cb\u6570\uff0c\n\u7b2c\u56db\u4e2a\u9ed1\u952e\u4f4d\u4e8eE\u548cF\u4e4b\u95f4\u3002\u56e0\u6b64\uff0c\u8fd9\u4e2a\u9ed1\u952e\u53ef\u4ee5\u88ab\u79f0\u4e3aE\u7684\u5347\u97f3\uff08\u5347E\uff09\u6216F\u7684\u964d\u97f3\uff08\u964dF\uff09\u3002\u5728\u97f3\u4e50\u7406\n\u8bba\u4e2d\uff0c\u8fd9\u4e24\u4e2a\u540d\u79f0\u6307\u7684\u662f\u540c\u4e00\u4e2a\u97f3\u3002\n\u6b63\u786e\u7b54\u6848\u662f\uff1a(C) \u964dE (D) \u964dF\nGround Truth: (A) \u964dD\n<\u56fe\u7247 1>\nError Category: Lack of Knowledge\nError Reason: GPT-4V demonstrates a lack of knowledge in certain music theories. On a piano \nkeyboard, each black key is indeed an ascending or descending tone of two white keys, but they \nhave specific tone names. In this case, the model failed to accurately provide the correct tone name \nfor the black key, but instead gave general descriptions, which demonstrates a lack of knowledge in \nmusic theory. The model should have enough knowledge to accurately identify and name each key \non the piano keyboard.\nFigure B24: A sample error case of Art and Design (subcategory: Music).\nBack to List of figures\n43\nPREPRINT\n\u827a\u672f\u4e0e\u8bbe\u8ba1:\u97f3\u4e50\nQuestion:  \u5173\u4e8e\u4e0b\u56fe<\u56fe\u7247 1>\u620f\u6b4c\u300a\u5531\u8138\u8c31\u300b\u7684\u65cb\u5f8b\u6c72\u53d6\u4e86\u4ec0\u4e48\u5143\u7d20\uff08\uff09\nOption:\n(A) \u4eac\u5267\u5143\u7d20\n(B) \u8c6b\u5267\u5143\u7d20\n(C) \u9ec4\u6885\u620f\u5143\u7d20\n(D) \u6606\u66f2\u5143\u7d20\nGPT-4V: \u8fd9\u5f20\u56fe\u7247\u5c55\u793a\u4e86\u4e94\u4e2a\u6234\u7740\u4e2d\u56fd\u4f20\u7edf\u620f\u5267\u8138\u8c31\u7684\u4eba\u7269\u3002\u8138\u8c31\u901a\u5e38\u4e0e\u4e2d\u56fd\u4e0d\u540c\u5730\u533a\u7684\u620f\u5267\u5f62\u5f0f\u76f8\n\u5173\u8054\uff0c\u5982\u4eac\u5267\u3001\u8c6b\u5267\u3001\u9ec4\u6885\u620f\u548c\u6606\u66f2\u7b49\u3002\u7136\u800c\uff0c\u4ec5\u51ed\u8fd9\u5f20\u56fe\u7247\uff0c\u6211\u65e0\u6cd5\u786e\u5b9a\u300a\u5531\u8138\u8c31\u300b\u8fd9\u9996\u6b4c\u7684\u65cb\u5f8b\n\u5177\u4f53\u6c72\u53d6\u4e86\u54ea\u79cd\u620f\u5267\u5143\u7d20\uff0c\u56e0\u4e3a\u56fe\u7247\u4e2d\u6ca1\u6709\u63d0\u4f9b\u97f3\u4e50\u6216\u65cb\u5f8b\u7684\u4fe1\u606f\u3002\n\u5982\u679c\u6ca1\u6709\u5176\u4ed6\u4fe1\u606f\uff0c\u6211\u65e0\u6cd5\u51c6\u786e\u56de\u7b54\u8fd9\u4e2a\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u57fa\u4e8e\u56fe\u7247\u63d0\u4f9b\u7684\u4fe1\u606f\uff0c\u6211\u65e0\u6cd5\u9009\u62e9\u4e00\u4e2a\u6b63\u786e\u7684\n\u7b54\u6848\u3002\u5982\u679c\u6709\u5173\u4e8e\u8fd9\u9996\u6b4c\u7684\u66f4\u591a\u80cc\u666f\u4fe1\u606f\u6216\u8005\u65cb\u5f8b\u7684\u63cf\u8ff0\uff0c\u6211\u53ef\u80fd\u4f1a\u505a\u51fa\u66f4\u51c6\u786e\u7684\u5224\u65ad\u3002\nGround Truth: (A) \u4eac\u5267\u5143\u7d20\n<\u56fe\u7247 1>\nError Category: Lack of Knowledge\nError Reason: In this response, the image content does provide enough information to determine \nthe source of the melodic elements of the song Singing Faces, and the model's failure to recognize \nand utilize this information demonstrates its lack of domain-specific knowledge. \nFigure B25: A sample error case of Art and Design (subcategory: Music).\nBack to List of figures\n44\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66:\u57fa\u7840\u533b\u5b66\nQuestion: \u56fe<\u56fe\u7247 1>\u4e2d\u7684\u7ec4\u7ec7\u7ec6\u80de\u591a\u5c42\uff0c\u7531\u8868\u5c42\u5230\u57fa\u5e95\u5c42\u7ec6\u80de\u5f62\u72b6\u4f9d\u6b21\u4e3a\u6241\u5e73\u5f62\u3001\u591a\u8fb9\u5f62\u3001\u77ee\u67f1\u72b6\uff0c\n\u662f\uff08\uff09\nOption: \n(A) \u590d\u5c42\u6241\u5e73\u4e0a\u76ae\n(B) \u5047\u590d\u5c42\u7ea4\u6bdb\u67f1\u72b6\u4e0a\u76ae\n(C) \u5355\u5c42\u67f1\u72b6\u4e0a\u76ae\n(D) \u5176\u4f59\u9009\u9879\u5747\u4e0d\u5bf9\nGPT-4V: \u8fd9\u5f20\u56fe\u7247\u663e\u793a\u7684\u662f\u4e00\u79cd\u7ec4\u7ec7\u5207\u7247\uff0c\u4f46\u7531\u4e8e\u6211\u65e0\u6cd5\u8fdb\u884c\u5b9e\u9645\u7684\u663e\u5fae\u955c\u68c0\u67e5\uff0c\u6211\u53ea\u80fd\u6839\u636e\u60a8\u63d0\u4f9b\n\u7684\u63cf\u8ff0\u548c\u56fe\u7247\u7684\u89c6\u89c9\u4fe1\u606f\u6765\u63a8\u6d4b\u3002\u6839\u636e\u60a8\u7684\u63cf\u8ff0\uff0c\u8fd9\u79cd\u7ec4\u7ec7\u7ec6\u80de\u662f\u591a\u5c42\u7684\uff0c\u8868\u5c42\u7ec6\u80de\u4e3a\u6241\u5e73\u5f62\uff0c\u4e2d\u95f4\n\u5c42\u7ec6\u80de\u4e3a\u591a\u8fb9\u5f62\uff0c\u800c\u57fa\u5e95\u5c42\u7ec6\u80de\u4e3a\u77ee\u67f1\u72b6\u3002\u8fd9\u79cd\u7ec6\u80de\u6392\u5217\u548c\u5f62\u72b6\u7684\u63cf\u8ff0\u7b26\u5408\u590d\u5c42\u6241\u5e73\u4e0a\u76ae\u7684\u7279\u5f81\u3002\n\u56e0\u6b64\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a (A) \u590d\u5c42\u6241\u5e73\u4e0a\u76ae\nGround Truth: (A)\u590d\u5c42\u6241\u5e73\u4e0a\u76ae\n<\u56fe\u7247 1>\nFigure B26: A sample correct case of Health and Medicine (subcategory: Basic Medicine).\nBack to List of figures\n45\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66:\u57fa\u7840\u533b\u5b66\nQuestion: <\u56fe\u7247 1>\u4e2d\u7ec6\u80de\u6241\u5e73\uff0c\u6392\u5217\u7d27\u5bc6\u7684\u808c\u7ec4\u7ec7\u662f\uff08\uff09\nGPT-4V: \u8fd9\u5f20\u56fe\u7247\u663e\u793a\u7684\u662f\u4e00\u79cd\u808c\u7ec4\u7ec7\u7684\u663e\u5fae\u955c\u56fe\u50cf\u3002\u6839\u636e\u7ec6\u80de\u6241\u5e73\u3001\u6392\u5217\u7d27\u5bc6\u7684\u63cf\u8ff0\uff0c\u8fd9\u5f88\u53ef\u80fd\u662f\n\u5e73\u6ed1\u808c\u7ec4\u7ec7\u3002\u5e73\u6ed1\u808c\u7ec6\u80de\u5728\u663e\u5fae\u955c\u4e0b\u901a\u5e38\u5448\u73b0\u4e3a\u957f\u6761\u5f62\uff0c\u4e24\u7aef\u5c16\u7ec6\uff0c\u4e2d\u95f4\u8f83\u5bbd\uff0c\u4e14\u6838\u5448\u692d\u5706\u5f62\u6216\u8c46\u5f62\uff0c\n\u6392\u5217\u7d27\u5bc6\u4f46\u6ca1\u6709\u6a2a\u7eb9\u3002\u56e0\u6b64\uff0c<\u56fe\u7247 1>\u4e2d\u7ec6\u80de\u6241\u5e73\uff0c\u6392\u5217\u7d27\u5bc6\u7684\u808c\u7ec4\u7ec7\u662f\u5e73\u6ed1\u808c\u3002\nGround Truth: \u5e73\u6ed1\u808c\n<\u56fe\u7247 1>\nFigure B27: A sample correct case of Health and Medicine (subcategory: Basic Medicine).\nBack to List of figures\n46\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66:\u57fa\u7840\u533b\u5b66\nQuestion: <\u56fe\u7247 1>\u4e2d\u662f\u8840\u6d82\u7247\uff0c5\u662f\uff08\uff09\nGPT-4V: \u56fe\u7247\u4e2d\u6807\u8bb0\u4e3a5\u7684\u7ec6\u80de\u662f\u4e00\u79cd\u767d\u7ec6\u80de\uff0c\u6839\u636e\u5176\u5f62\u6001\u7279\u5f81\uff0c\u5b83\u53ef\u80fd\u662f\u4e00\u79cd\u6dcb\u5df4\u7ec6\u80de\u3002\u5728\u8840\u6d82\u7247\n\u4e2d\uff0c\u6dcb\u5df4\u7ec6\u80de\u7684\u7279\u70b9\u662f\u8f83\u5927\u7684\u5706\u5f62\u6216\u7565\u5448\u692d\u5706\u5f62\u7684\u6838\uff0c\u4ee5\u53ca\u8f83\u5c11\u7684\u7ec6\u80de\u8d28\u3002\u56e0\u6b64\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a5\u662f\n\u6dcb\u5df4\u7ec6\u80de\u3002\nGround Truth: \u6dcb\u5df4\u7ec6\u80de\n<\u56fe\u7247 1>\nFigure B28: A sample correct case of Health and Medicine (subcategory: Basic Medicine).\nBack to List of figures\n47\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66:\u57fa\u7840\u533b\u5b66\nQuestion: <\u56fe\u7247 1>\u4e2d\u662f\u900f\u660e\u8f6f\u9aa8\u7ec4\u7ec7\uff0c\u6846\u4e2d\u5708\u51fa\u7684\u662f\uff08\uff09\nOption: \n(A) \u80be\u4e0a\u817a\u76ae\u8d28\n(B) \u6dcb\u5df4\u5c0f\u7ed3\u751f\u53d1\u4e2d\u5fc3\n(C) \u7532\u72b6\u817a\u6ee4\u6ce1\n(D) \u540c\u6e90\u7ec6\u80de\u7fa4\nGPT-4V: \u5f88\u62b1\u6b49\uff0c\u6211\u65e0\u6cd5\u63d0\u4f9b\u6709\u5173\u6b64\u56fe\u50cf\u7684\u533b\u5b66\u6216\u751f\u7269\u5b66\u5206\u6790\u3002\u5982\u679c\u60a8\u6709\u5176\u4ed6\u7c7b\u578b\u7684\u95ee\u9898\u6216\u9700\u8981\u5176\u4ed6\n\u5e2e\u52a9\uff0c\u8bf7\u544a\u8bc9\u6211\uff01\nGround Truth: (D) \u540c\u6e90\u7ec6\u80de\u7fa4\n<\u56fe\u7247 1>\nError Category: Reject to Answer\nError Reason: Due to ethical considerations and other factors, GPT-4V refuses to provide medical or \nbiological analyses.\nFigure B29: A sample error case of Health and Medicine (subcategory: Basic Medicine).\nBack to List of figures\n48\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66:\u57fa\u7840\u533b\u5b66\nQuestion: <\u56fe\u7247 1>\u56fe\u793a\u7bad\u5934\u6240\u6307\u7684\u7ec6\u80de\u540d\u79f0\uff08\uff09\nOption: \n(A) \u8840\u5c0f\u677f\n(B) \u8840\u5c0f\u677f\u805a\u96c6\n(C) \u5de8\u5927\u8840\u5c0f\u677f\n(D) \u541e\u566c\u7ec6\u80de\nGPT-4V: \u8fd9\u5f20\u56fe\u7247\u663e\u793a\u7684\u662f\u67d3\u8272\u540e\u7684\u8840\u6db2\u6d82\u7247\uff0c\u7bad\u5934\u6307\u5411\u7684\u7ec6\u80de\u7531\u4e8e\u56fe\u7247\u5206\u8fa8\u7387\u548c\u7f3a\u4e4f\u7279\u5b9a\u7684\u7ec6\u80de\n\u5b66\u7279\u5f81\uff0c\u96be\u4ee5\u51c6\u786e\u5224\u65ad\u3002\u4f46\u662f\uff0c\u6211\u4eec\u53ef\u4ee5\u6839\u636e\u4e00\u4e9b\u57fa\u672c\u7684\u7ec6\u80de\u5b66\u77e5\u8bc6\u6765\u5c1d\u8bd5\u63a8\u65ad\uff1a (A) \u8840\u5c0f\u677f - \u901a\n\u5e38\u6bd4\u8f83\u5c0f\uff0c\u4e0d\u542b\u6838\uff0c\u8fd9\u4e9b\u7ec6\u80de\u770b\u8d77\u6765\u8f83\u5927\u4e14\u53ef\u80fd\u542b\u6709\u6838\uff0c\u56e0\u6b64\u4e0d\u592a\u53ef\u80fd\u662f\u8840\u5c0f\u677f\u3002 (B) \u8840\u5c0f\u677f\u805a\u96c6\n- \u8840\u5c0f\u677f\u805a\u96c6\u901a\u5e38\u662f\u6307\u591a\u4e2a\u8840\u5c0f\u677f\u7c98\u5728\u4e00\u8d77\uff0c\u8fd9\u4e9b\u7ec6\u80de\u770b\u8d77\u6765\u662f\u5355\u4e2a\u7684\uff0c\u4e0d\u662f\u805a\u96c6\u5728\u4e00\u8d77\u7684\u3002 (C) \n\u5de8\u5927\u8840\u5c0f\u677f - \u5de8\u5927\u8840\u5c0f\u677f\u662f\u5f02\u5e38\u5927\u7684\u8840\u5c0f\u677f\uff0c\u4f46\u662f\u8fd9\u4e9b\u7ec6\u80de\u7684\u5927\u5c0f\u548c\u5f62\u6001\u4e0e\u5178\u578b\u7684\u5de8\u5927\u8840\u5c0f\u677f\u4e0d\u592a\n\u76f8\u7b26\u3002 (D) \u541e\u566c\u7ec6\u80de - \u541e\u566c\u7ec6\u80de\uff0c\u5982\u5de8\u566c\u7ec6\u80de\uff0c\u901a\u5e38\u8f83\u5927\uff0c\u542b\u6709\u8f83\u5927\u7684\u6838\u548c\u4e30\u5bcc\u7684\u7ec6\u80de\u8d28\u3002\u7bad\u5934\u6307\n\u5411\u7684\u7ec6\u80de\u53ef\u80fd\u662f\u541e\u566c\u7ec6\u80de\uff0c\u4f46\u6ca1\u6709\u66f4\u591a\u7684\u4fe1\u606f\u5f88\u96be\u786e\u5b9a\u3002 \u7531\u4e8e\u7f3a\u4e4f\u8db3\u591f\u7684\u4fe1\u606f\uff0c\u65e0\u6cd5\u786e\u5b9a\u4e00\u4e2a\u660e\u786e\n\u7684\u7b54\u6848\u3002\u4f46\u662f\uff0c\u6839\u636e\u7ec6\u80de\u7684\u5927\u5c0f\u548c\u5f62\u6001\uff0c\u6700\u53ef\u80fd\u7684\u9009\u9879\u662f(D) \u541e\u566c\u7ec6\u80de\u3002\nGround Truth: (B)\u8840\u5c0f\u677f\u805a\u96c6\n<\u56fe\u7247 1>\nError Category: Perceptual Error\nError Reason: GPT-4V correctly understood the question, but it made an error in image perception. \nThe arrow points to a group of cells clustered together, whereas GPT-4V identified it as a single cell, \nleading to an error.\nFigure B30: A sample error case of Health and Medicine (subcategory: Basic Medicine).\nBack to List of figures\n49\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66:\u57fa\u7840\u533b\u5b66\nQuestion: <\u56fe\u7247 1>\u56fe\u793a\u7bad\u5934\u6240\u6307\u7684\u7ec6\u80de\u540d\u79f0\uff08\uff09\nOption: \n(A) \u8840\u5c0f\u677f\n(B) \u8840\u5c0f\u677f\u805a\u96c6\n(C) \u5de8\u5927\u8840\u5c0f\u677f\n(D) \u8840\u5c0f\u677f\u536b\u661f\u73b0\u8c61\nGPT-4V: \u8fd9\u5f20\u56fe\u7247\u663e\u793a\u7684\u662f\u4e00\u4e2a\u8840\u6db2\u6d82\u7247\uff0c\u7bad\u5934\u6307\u5411\u7684\u7ec6\u80de\u53ef\u80fd\u662f\u8840\u5c0f\u677f\u805a\u96c6\uff0c\u56e0\u4e3a\u5b83\u770b\u8d77\u6765\u662f\u7531\u591a\n\u4e2a\u8f83\u5c0f\u7684\u7ec6\u80de\u7ec4\u6210\u7684\u4e00\u56e2\uff0c\u8fd9\u4e0e\u8840\u5c0f\u677f\u805a\u96c6\u7684\u7279\u5f81\u76f8\u7b26\u3002\u8840\u5c0f\u677f\uff08\u9009\u9879A\uff09\u901a\u5e38\u662f\u5355\u4e2a\u7684\u5c0f\u7ec6\u80de\uff0c\u800c\n\u5de8\u5927\u8840\u5c0f\u677f\uff08\u9009\u9879C\uff09\u662f\u5355\u4e2a\u4f46\u5f02\u5e38\u5927\u7684\u8840\u5c0f\u677f\u3002\u8840\u5c0f\u677f\u536b\u661f\u73b0\u8c61\uff08\u9009\u9879D\uff09\u901a\u5e38\u662f\u6307\u8840\u5c0f\u677f\u56f4\u7ed5\u7740\u767d\n\u7ec6\u80de\u3002\u6839\u636e\u8fd9\u5f20\u56fe\u7247\uff0c\u6700\u53ef\u80fd\u6b63\u786e\u7684\u9009\u9879\u662f\uff1a (B) \u8840\u5c0f\u677f\u805a\u96c6\nGround Truth: (C)\u5de8\u5927\u8840\u5c0f\u677f\n<\u56fe\u7247 1>\nError Category: Perceptual Error\nError Reason: GPT-4V correctly understood the question, but it made an error in image perception. \nThe arrow should be pointing to a large cell (compared to normal platelets), whereas GPT-4V \nidentified it as a group of smaller cells clustered together, leading to an error.\nFigure B31: A sample error case of Health and Medicine (subcategory: Basic Medicine).\nBack to List of figures\n50\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66:\u57fa\u7840\u533b\u5b66\nQuestion: <\u56fe\u7247 1>\u4e3a\u79bb\u5fc3\u6c89\u6dc0\u540e\u7684\u8840\u6db2\uff0c\u8fd9\u4e09\u5c42\u4ece\u4e0a\u81f3\u4e0b\u4e3a\uff08\uff09\nOption: \n(A) \u8840\u6e05,\u767d\u7ec6\u80de\u548c\u8840\u5c0f\u677f,\u7ea2\u7ec6\u80de\n(B) \u8840\u6e05,\u7ea2\u7ec6\u80de,\u767d\u7ec6\u80de\u548c\u8840\u5c0f\u677f\n(C) \u8840\u6d46,\u767d\u7ec6\u80de\u548c\u8840\u5c0f\u677f,\u7ea2\u7ec6\u80de\n(D) \u8840\u6d46,\u7ea2\u7ec6\u80de,\u767d\u7ec6\u80de\u548c\u8840\u5c0f\u677f\nGPT-4V: \u8fd9\u5f20\u56fe\u7247\u663e\u793a\u7684\u662f\u79bb\u5fc3\u6c89\u6dc0\u540e\u7684\u8840\u6db2\u6837\u672c\u3002\u4ece\u4e0a\u81f3\u4e0b\uff0c\u8840\u6db2\u901a\u5e38\u5206\u4e3a\u4e09\u5c42\uff1a\u6700\u4e0a\u9762\u662f\u8840\u6d46\u6216\n\u8840\u6e05\uff0c\u4e2d\u95f4\u662f\u767d\u7ec6\u80de\u548c\u8840\u5c0f\u677f\u5f62\u6210\u7684\u767d\u8272\u6216\u6de1\u9ec4\u8272\u5c42\uff0c\u79f0\u4e3a\u767d\u7ec6\u80de\u5c42\u6216\u767d\u819c\uff0c\u6700\u4e0b\u9762\u662f\u7ea2\u7ec6\u80de\u3002 \u6839\n\u636e\u8fd9\u4e2a\u63cf\u8ff0\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a (A) \u8840\u6e05,\u767d\u7ec6\u80de\u548c\u8840\u5c0f\u677f,\u7ea2\u7ec6\u80de\nGround Truth: (C) \u8840\u6d46,\u767d\u7ec6\u80de\u548c\u8840\u5c0f\u677f,\u7ea2\u7ec6\u80de\n<\u56fe\u7247 1>\nError Category: Reasoning Error\nError Reason: GPT-4V correctly understood the question and the image, determining that the blood \nsample after centrifugation separates into three layers. However, in the subsequent reasoning, GPT-\n4V confused plasma with serum.\nFigure B32: A sample error case of Health and Medicine (subcategory: Basic Medicine).\nBack to List of figures\n51\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66:\u57fa\u7840\u533b\u5b66\nQuestion: <\u56fe\u7247 1>\u56fe\u793a\u7bad\u5934\u6240\u6307\u7684\u7ec6\u80de\u540d\u79f0____\nOption: \n(A) \u84dd\u7ec6\u80de\uff08\u6d82\u62b9\u7ec6\u80de\uff09\n(B) \u8102\u80aa\u7ec6\u80de\n(C) \u6dcb\u5df4\u7ec6\u80de\n(D) \u5176\u4f59\u9009\u9879\u5747\u4e0d\u5bf9\nGPT-4V: \u8fd9\u5f20\u56fe\u7247\u663e\u793a\u7684\u662f\u4e00\u4e2a\u663e\u5fae\u955c\u4e0b\u7684\u8840\u6db2\u6d82\u7247\u3002\u7bad\u5934\u6240\u6307\u7684\u7ec6\u80de\u7531\u4e8e\u56fe\u7247\u7684\u5206\u8fa8\u7387\u548c\u7f3a\u4e4f\u7279\u5b9a\n\u7684\u67d3\u8272\u7ec6\u8282\uff0c\u96be\u4ee5\u786e\u5b9a\u5176\u786e\u5207\u7c7b\u578b\u3002\u7136\u800c\uff0c\u6839\u636e\u5176\u76f8\u5bf9\u8f83\u5927\u7684\u5c3a\u5bf8\u548c\u4e0d\u89c4\u5219\u7684\u5f62\u72b6\uff0c\u8fd9\u4e2a\u7ec6\u80de\u53ef\u80fd\u662f\n\u4e00\u79cd\u767d\u7ec6\u80de\u3002\u5728\u7ed9\u51fa\u7684\u9009\u9879\u4e2d\uff0c\u84dd\u7ec6\u80de\uff08\u6d82\u62b9\u7ec6\u80de\uff09\u901a\u5e38\u4e0d\u662f\u8840\u6db2\u4e2d\u7684\u4e00\u4e2a\u7ec4\u6210\u90e8\u5206\uff0c\u8102\u80aa\u7ec6\u80de\u5728\u8840\n\u6db2\u6d82\u7247\u4e2d\u4e5f\u4e0d\u5e38\u89c1\u3002\u6dcb\u5df4\u7ec6\u80de\u662f\u8840\u6db2\u4e2d\u7684\u4e00\u79cd\u5e38\u89c1\u767d\u7ec6\u80de\uff0c\u4f46\u662f\u7531\u4e8e\u7f3a\u4e4f\u8be6\u7ec6\u4fe1\u606f\uff0c\u65e0\u6cd5\u786e\u5b9a\u8fd9\u662f\u5426\n\u662f\u6dcb\u5df4\u7ec6\u80de\u3002 \u56e0\u6b64\uff0c\u6839\u636e\u53ef\u7528\u4fe1\u606f\uff0c\u6700\u53ef\u80fd\u6b63\u786e\u7684\u9009\u9879\u662f\uff1a (C) \u6dcb\u5df4\u7ec6\u80de \u4f46\u8bf7\u6ce8\u610f\uff0c\u6ca1\u6709\u66f4\u591a\u7684\n\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u7279\u5b9a\u7684\u67d3\u8272\u7ec6\u8282\uff0c\u8fd9\u4e2a\u7b54\u6848\u5e76\u4e0d\u662f\u786e\u5b9a\u65e0\u7591\u7684\u3002\nGround Truth: (A)\u84dd\u7ec6\u80de\uff08\u6d82\u62b9\u7ec6\u80de\uff09\n<\u56fe\u7247 1>\nError Category: Reasoning Error\nError Reason: GPT-4V correctly understood the question and the image, deducing that the image is \na blood smear under the microscope. However, it excluded the option based on blue cells (smear \ncells), typically not being a component of blood. In usual circumstances, smear cells refer to \ndamaged white blood cells during the process of making a blood smear.\nFigure B33: A sample error case of Health and Medicine (subcategory: Basic Medicine).\nBack to List of figures\n52\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66:\u57fa\u7840\u533b\u5b66\nQuestion: \u56fe<\u56fe\u7247 1>\u4e2d\u4f4d\u4e8e\u809d\u5c0f\u53f6\u76f8\u8fde\u51fa\uff0c\u6709\u4e09\u4e2a\u7ba1\uff0c\u5355\u5c42\u7acb\u65b9\u4e0a\u76ae\u7684\u5c0f\u53f6\u95f4\u80c6\u7ba1\u3001\u5c0f\u53f6\u95f4\u52a8\u3001\u9759\n\u8109\u7684\u662f\u4ec0\u4e48\uff1f_____\nGPT-4V: \u8fd9\u4e2a\u7ed3\u6784\u662f\u809d\u4e09\u89d2\u3002\u5728\u809d\u810f\u7684\u7ec4\u7ec7\u5b66\u5207\u7247\u4e2d\uff0c\u809d\u4e09\u89d2\uff08\u4e5f\u79f0\u4e3a\u95e8\u533a\u6216\u95e8\u7ba1\u533a\uff09\u662f\u4f4d\u4e8e\u809d\u5c0f\u53f6\n\u4e4b\u95f4\u7684\u533a\u57df\uff0c\u5176\u4e2d\u5305\u542b\u4e86\u4e09\u4e2a\u4e3b\u8981\u7ed3\u6784\uff1a\u5c0f\u53f6\u95f4\u80c6\u7ba1\u3001\u5c0f\u53f6\u95f4\u52a8\u8109\u548c\u5c0f\u53f6\u95f4\u9759\u8109\u3002\u8fd9\u4e9b\u7ed3\u6784\u5171\u540c\u6784\u6210\n\u4e86\u809d\u810f\u7684\u8840\u7ba1\u548c\u80c6\u6c41\u6392\u6cc4\u7cfb\u7edf\u7684\u57fa\u672c\u5355\u5143\u3002\u5c0f\u53f6\u95f4\u80c6\u7ba1\u8d1f\u8d23\u6536\u96c6\u809d\u7ec6\u80de\u5206\u6ccc\u7684\u80c6\u6c41\uff0c\u800c\u5c0f\u53f6\u95f4\u52a8\u8109\u548c\n\u9759\u8109\u5219\u5206\u522b\u8d1f\u8d23\u5c06\u8840\u6db2\u8f93\u9001\u5230\u809d\u5c0f\u53f6\u548c\u4ece\u809d\u5c0f\u53f6\u4e2d\u5e26\u8d70\u8840\u6db2\u3002\nGround Truth: \u95e8\u7ba1\u533a\n<\u56fe\u7247 1>\nError Category: Answer Extraction Error\nError Reason: GPT-4V correctly understood the question and the image, inferring from the image \nfeatures that it is the liver hilum (also known as the porta hepatis). However, during the Extract & \nMatch process, it did not match with GPT-4V's actual answer.\nFigure B34: A sample error case of Health and Medicine (subcategory: Basic Medicine).\nBack to List of figures\n53\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66:\u57fa\u7840\u533b\u5b66\nQuestion: \u56fe<\u56fe\u7247 1>\u4e2d\u6838\u8f66\u8f6e\u72b6\u504f\u5411\u4e00\u4fa7\uff0c\u6838\u65c1\u6709\u6de1\u67d3\u533a\u7684\u662f\uff08\uff09\nGPT-4V: \u8fd9\u5f20\u56fe\u7247\u663e\u793a\u7684\u662f\u67d0\u79cd\u7ec6\u80de\u7684\u663e\u5fae\u955c\u56fe\u50cf\u3002\u6839\u636e\u60a8\u7684\u63cf\u8ff0\uff0c\u56fe\u4e2d\u7684\u7ec6\u80de\u6838\u8f66\u8f6e\u72b6\u504f\u5411\u4e00\u4fa7\uff0c\n\u6838\u65c1\u6709\u6de1\u67d3\u533a\uff0c\u8fd9\u53ef\u80fd\u662f\u67d0\u79cd\u7279\u5b9a\u7c7b\u578b\u7684\u767d\u7ec6\u80de\u3002\u5728\u8840\u6db2\u5b66\u4e2d\uff0c\u8fd9\u6837\u7684\u63cf\u8ff0\u53ef\u80fd\u6307\u5411\u67d0\u79cd\u7279\u6b8a\u7684\u767d\u7ec6\n\u80de\u5f62\u6001\uff0c\u5982\u55dc\u4e2d\u6027\u7c92\u7ec6\u80de\u7684\u5e26\u72b6\u6838\u6216\u5176\u4ed6\u7c7b\u578b\u7684\u767d\u7ec6\u80de\u3002\u7136\u800c\uff0c\u7531\u4e8e\u6211\u65e0\u6cd5\u63d0\u4f9b\u4e13\u4e1a\u7684\u533b\u5b66\u8bca\u65ad\uff0c\u6240\n\u4ee5\u4e0d\u80fd\u786e\u5207\u5730\u544a\u8bc9\u60a8\u8fd9\u662f\u54ea\u79cd\u7ec6\u80de\u3002\u901a\u5e38\uff0c\u8fd9\u79cd\u7c7b\u578b\u7684\u5206\u6790\u9700\u8981\u7531\u5177\u6709\u76f8\u5e94\u4e13\u4e1a\u77e5\u8bc6\u7684\u533b\u7597\u4e13\u4e1a\u4eba\u5458\n\u6765\u5b8c\u6210\u3002\nGround Truth: \u6d46\u7ec6\u80de\n<\u56fe\u7247 1>\nError Category: Lack of Knowledge\nError Reason: GPT-4V inferred from the question and the image that the cells in the picture might \nbe a type of white blood cell. However, lacking specific knowledge, it couldn't deduce the cells to be \nplasma cells based on their features.\nFigure B35: A sample error case of Health and Medicine (subcategory: Basic Medicine).\nBack to List of figures\n54\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66:\u4e34\u5e8a\u533b\u5b66\nQuestion: \u5173\u4e8e\u9aa8\u6298\u7684\u665a\u671f\u7efc\u5408\u5f81\u4e2d\uff0c\u5bb9\u6613\u9020\u6210<\u56fe\u7247 1>\u56fe\u4e2d\u75c5\u7406\u5f81\u8c61\u7684\u662f\uff08\uff09 \nOption: \n(A) \u7f3a\u8840\u6027\u9aa8\u574f\u6b7b\n(B) \u7f3a\u8840\u6027\u808c\u631b\u7f29\n(C) \u5173\u8282\u50f5\u786c\n(D) \u635f\u4f24\u6027\u9aa8\u5316\nGPT-4V: \u56fe\u7247\u663e\u793a\u7684\u662f\u4e00\u53ea\u624b\uff0c\u624b\u6307\u5448\u73b0\u5f02\u5e38\u5f2f\u66f2\u7684\u59ff\u6001\u3002\u8fd9\u79cd\u75c5\u7406\u5f81\u8c61\u53ef\u80fd\u4e0e\u591a\u79cd\u75c5\u7406\u72b6\u6001\u6709\u5173\uff0c\n\u4f46\u6839\u636e\u63d0\u4f9b\u7684\u9009\u9879\uff0c\u6211\u4eec\u53ef\u4ee5\u5c1d\u8bd5\u786e\u5b9a\u6700\u53ef\u80fd\u7684\u539f\u56e0\u3002 (A) \u7f3a\u8840\u6027\u9aa8\u574f\u6b7b - \u8fd9\u901a\u5e38\u6307\u7684\u662f\u7531\u4e8e\u8840\u6db2\n\u4f9b\u5e94\u4e0d\u8db3\u5bfc\u81f4\u7684\u9aa8\u7ec4\u7ec7\u6b7b\u4ea1\uff0c\u53ef\u80fd\u4f1a\u5f71\u54cd\u5173\u8282\u7684\u529f\u80fd\uff0c\u4f46\u4e0d\u4e00\u5b9a\u76f4\u63a5\u5bfc\u81f4\u624b\u6307\u7684\u5f02\u5e38\u5f2f\u66f2\u3002 (B) \u7f3a\n\u8840\u6027\u808c\u631b\u7f29 - \u8fd9\u662f\u7531\u4e8e\u808c\u8089\u8840\u6db2\u4f9b\u5e94\u4e0d\u8db3\u5bfc\u81f4\u7684\u808c\u8089\u6536\u7f29\uff0c\u53ef\u80fd\u4f1a\u5bfc\u81f4\u624b\u6307\u6216\u5176\u4ed6\u80a2\u4f53\u7684\u5f02\u5e38\u59ff\u6001\u3002\n(C) \u5173\u8282\u50f5\u786c - \u8fd9\u662f\u6307\u5173\u8282\u5931\u53bb\u6b63\u5e38\u6d3b\u52a8\u8303\u56f4\uff0c\u53ef\u80fd\u662f\u7531\u4e8e\u957f\u65f6\u95f4\u4e0d\u52a8\u3001\u708e\u75c7\u6216\u5176\u4ed6\u75c5\u7406\u539f\u56e0\u3002\u5173\n\u8282\u50f5\u786c\u53ef\u80fd\u5bfc\u81f4\u624b\u6307\u65e0\u6cd5\u6b63\u5e38\u4f38\u5c55\u3002 (D) \u635f\u4f24\u6027\u9aa8\u5316 - \u8fd9\u662f\u6307\u5728\u8f6f\u7ec4\u7ec7\u4e2d\u5f62\u6210\u9aa8\u7ec4\u7ec7\uff0c\u901a\u5e38\u662f\u7531\u4e8e\n\u635f\u4f24\u6216\u708e\u75c7\u53cd\u5e94\u3002\u8fd9\u53ef\u80fd\u5bfc\u81f4\u5173\u8282\u5468\u56f4\u7684\u5f02\u5e38\u786c\u5316\uff0c\u4f46\u4e0d\u4e00\u5b9a\u76f4\u63a5\u5bfc\u81f4\u624b\u6307\u7684\u5f02\u5e38\u5f2f\u66f2\u3002 \u6839\u636e\u56fe\u7247\n\u4e2d\u624b\u6307\u7684\u5f02\u5e38\u5f2f\u66f2\u59ff\u6001\uff0c\u6700\u53ef\u80fd\u7684\u9009\u9879\u662f(B) \u7f3a\u8840\u6027\u808c\u631b\u7f29\uff0c\u56e0\u4e3a\u8fd9\u79cd\u60c5\u51b5\u4e0b\u808c\u8089\u6536\u7f29\u53ef\u80fd\u5bfc\u81f4\u624b\u6307\n\u4fdd\u6301\u5728\u4e0d\u6b63\u5e38\u7684\u4f4d\u7f6e\u3002\u56e0\u6b64\uff0c\u6b63\u786e\u7b54\u6848\u662f(B) \u7f3a\u8840\u6027\u808c\u631b\u7f29\u3002\nGround Truth: (B)\u7f3a\u8840\u6027\u808c\u631b\u7f29\n<\u56fe\u7247 1>\nFigure B36: A sample correct case of Health and Medicine (subcategory: Clinical Medicine).\nBack to List of figures\n55\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66:\u4e34\u5e8a\u533b\u5b66\nQuestion:\u98df\u7ba1\u764c\u6700\u597d\u53d1\u7684\u90e8\u4f4d\u662f\u56fe<\u56fe\u7247 1>\u4e2d\u54ea\u4e2a\u90e8\u4f4d\uff1f\nOption: \n(A) A\n(B) B\n(C) C\n(D) D\nGPT-4V:\u98df\u7ba1\u764c\u6700\u5e38\u89c1\u7684\u53d1\u751f\u90e8\u4f4d\u662f\u98df\u7ba1\u4e2d\u4e0b\u6bb5\uff0c\u56e0\u6b64\u5728\u8fd9\u5e45\u56fe\u4e2d\uff0c\u6700\u53ef\u80fd\u6b63\u786e\u7684\u9009\u9879\u662f\uff1a (C) C \n\u8fd9\u4e2a\u90e8\u4f4d\u5927\u81f4\u5bf9\u5e94\u4e8e\u98df\u7ba1\u7684\u4e2d\u4e0b\u6bb5\uff0c\u662f\u98df\u7ba1\u764c\u6700\u597d\u53d1\u7684\u533a\u57df\u3002\nGround Truth: (C) C\n<\u56fe\u7247 1>\nFigure B37: A sample correct case of Health and Medicine (subcategory: Clinical Medicine).\nBack to List of figures\n56\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66:\u4e34\u5e8a\u533b\u5b66\nQuestion:\u60a3\u8005\uff0c\u5973\uff0c\u8fd1\u671f\u7ecf\u5e38\u70e6\u8e81\u4e0d\u5b89\uff0c\u5fc3\u60b8\uff0c\u4e14\u51fa\u73b0\u6015\u5149\u3001\u89c6\u529b\u4ee5\u53ca\u8bb0\u5fc6\u529b\u51cf\u9000\u7b49\u75c7\u72b6\uff0c\u56e0\u6b64\u6765\n\u533b\u9662\u5c31\u8bca\uff0c\u4f53\u67e5\uff1a\u60a3\u8005\u773c\u88c2\u589e\u5bbd\uff0c\u773c\u7403\u5916\u7a81\uff0c\u5982\u56fe<\u56fe\u7247 1>\u6240\u793a\u3002\u60a3\u8005\u6700\u53ef\u80fd\u60a3\u6709\uff08\uff09\nOption: \n(A)\u5355\u7eaf\u6027\u7532\u72b6\u817a\u80bf\n(B) \u7532\u72b6\u817a\u529f\u80fd\u4ea2\u8fdb\n(C) \u7532\u72b6\u817a\u529f\u80fd\u51cf\u9000\n(D) \u7532\u72b6\u817a\u764c\nGPT-4V: \u6839\u636e\u9898\u76ee\u63cf\u8ff0\uff0c\u60a3\u8005\u51fa\u73b0\u4e86\u70e6\u8e81\u4e0d\u5b89\u3001\u5fc3\u60b8\u3001\u6015\u5149\u3001\u89c6\u529b\u51cf\u9000\u548c\u8bb0\u5fc6\u529b\u51cf\u9000\u7684\u75c7\u72b6\uff0c\u4f53\u67e5\u53d1\n\u73b0\u773c\u88c2\u589e\u5bbd\u548c\u773c\u7403\u5916\u7a81\u3002\u8fd9\u4e9b\u75c7\u72b6\u548c\u4f53\u5f81\u4e0e\u7532\u72b6\u817a\u529f\u80fd\u4ea2\u8fdb\u76f8\u5173\u7684\u683c\u96f7\u592b\u65af\u75c5\uff08Graves' disease\uff09\n\u76f8\u7b26\uff0c\u8fd9\u662f\u4e00\u79cd\u81ea\u8eab\u514d\u75ab\u6027\u75be\u75c5\uff0c\u5176\u7279\u5f81\u4e4b\u4e00\u5c31\u662f\u7a81\u773c\uff08exophthalmos\uff09\u3002\u56e0\u6b64\uff0c\u6700\u53ef\u80fd\u7684\u7b54\u6848\u662f\uff1a\n(B) \u7532\u72b6\u817a\u529f\u80fd\u4ea2\u8fdb\nGround Truth: (B)\u7532\u72b6\u817a\u529f\u80fd\u4ea2\u8fdb\n<\u56fe\u7247 1>\nFigure B38: A sample correct case of Health and Medicine (subcategory: Clinical Medicine).\nBack to List of figures\n57\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66:\u4e34\u5e8a\u533b\u5b66\nQuestion:\u60a3\u8005\uff0c\u5973\uff0c57\u5c81\uff0c\u56e0\u5b50\u5bab\u8131\u5782\u6765\u533b\u9662\u5c31\u8bca\uff0c\u5177\u4f53\u60c5\u51b5\u5982\u56fe<\u56fe\u7247 1>\u6240\u793a\uff0c\u60a3\u8005\u5b50\u5bab\u8131\u5782\u7684\n\u7a0b\u5ea6\u4e3a\uff08\uff09\nOption: \n(A) I\u5ea6\u8f7b\u578b\n(B) I\u5ea6\u91cd\u578b\n(C) II\u5ea6\u8f7b\u578b\n(D) II\u5ea6\u91cd\u578b\nGPT-4V:\u5f88\u62b1\u6b49\uff0c\u6211\u65e0\u6cd5\u63d0\u4f9b\u533b\u5b66\u8bca\u65ad\u6216\u8bc4\u4f30\u3002\u5b50\u5bab\u8131\u5782\u7684\u7a0b\u5ea6\u901a\u5e38\u9700\u8981\u901a\u8fc7\u4e34\u5e8a\u68c0\u67e5\u6765\u786e\u5b9a\uff0c\u800c\u4e0d\n\u662f\u4ec5\u4ec5\u901a\u8fc7\u56fe\u7247\u3002\u5728\u5b9e\u9645\u533b\u7597\u60c5\u51b5\u4e2d\uff0c\u533b\u751f\u4f1a\u6839\u636e\u5b50\u5bab\u4e0b\u964d\u7684\u4f4d\u7f6e\u3001\u662f\u5426\u8d85\u8fc7\u9634\u9053\u53e3\u4ee5\u53ca\u76f8\u5173\u75c7\u72b6\u6765\n\u8bc4\u4f30\u8131\u5782\u7684\u7a0b\u5ea6\u3002\u901a\u5e38\uff0c\u5b50\u5bab\u8131\u5782\u5206\u4e3aI\u5ea6\uff08\u8f7b\u5ea6\uff09\u3001II\u5ea6\uff08\u4e2d\u5ea6\uff09\u548cIII\u5ea6\uff08\u91cd\u5ea6\uff09\u3002\u7531\u4e8e\u6211\u65e0\u6cd5\u8fdb\n\u884c\u5b9e\u9645\u7684\u533b\u5b66\u8bc4\u4f30\uff0c\u5efa\u8bae\u54a8\u8be2\u4e13\u4e1a\u7684\u533b\u7597\u4eba\u5458\u4ee5\u83b7\u5f97\u51c6\u786e\u7684\u8bca\u65ad\u3002\nGround Truth: (D) II\u5ea6\u91cd\u578b\n<\u56fe\u7247 1>\nError Category: Reject to Answer\nError Reason: Out of ethical considerations, GPT-4V refuses to provide medical-related diagnostic \nassessments.\nFigure B39: A sample error case of Health and Medicine (subcategory: Clinical Medicine).\nBack to List of figures\n58\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66:\u4e34\u5e8a\u533b\u5b66\nQuestion:\u60a3\u8005\uff0c\u7537\uff0c40 \u5c81\uff0c\u9ad8\u7a7a\u5760\u843d\u6454\u4f24\u534a\u5c0f\u65f6\uff0c\u56fe\u4e2d\u90e8\u4f4d\u53d1\u751f\u75bc\u75db\uff0c\u4f34\u8f7b\u5ea6\u6076\u5fc3\u3001\u5fc3\u614c\u3001\u51fa\u51b7\u6c57\u3002\n\u67e5\u4f53\uff1a\u8840\u538b 75/50mmHg\uff0c\u5fc3\u7387 130 \u6b21/\u5206\uff0c<\u56fe\u7247 1>\u56fe\u4e2d\u90e8\u4f4d\u80f8\u58c1\u76ae\u80a4\u632b\u88c2\u4f24\u4e14\u8f7b\u5ea6\u538b\u75db\uff0c\u53cd\u8df3\u75db\u4e0d\n\u660e\u663e\uff0c\u5c40\u9650\u6027\u8179\u808c\u7d27\u5f20\uff0c\u6709\u79fb\u52a8\u6027\u6d4a\u97f3\uff0c\u8179\u90e8\u62bd\u51fa\u4e0d\u51dd\u8840\u3002\u8be5\u60a3\u8005\u6700\u6709\u53ef\u80fd\u7684\u8bca\u65ad\u662f\uff08\uff09\nOption: \n(A)\u813e\u7834\u88c2\n(B)\u809d\u7834\u88c2\n(C)\u80a0\u7a7f\u5b54\n(D)\u80f0\u7834\u88c2\nGPT-4V:\u6839\u636e\u9898\u76ee\u63cf\u8ff0\uff0c\u60a3\u8005\u662f\u4e00\u540d40\u5c81\u7537\u6027\uff0c\u7ecf\u5386\u4e86\u9ad8\u7a7a\u5760\u843d\u6454\u4f24\uff0c\u51fa\u73b0\u4e86\u75bc\u75db\u3001\u8f7b\u5ea6\u6076\u5fc3\u3001\u5fc3\u614c\n\u548c\u51fa\u51b7\u6c57\u7684\u75c7\u72b6\u3002\u67e5\u4f53\u53d1\u73b0\u8840\u538b\u504f\u4f4e\uff0875/50mmHg\uff09\uff0c\u5fc3\u7387\u52a0\u5feb\uff08130\u6b21/\u5206\uff09\uff0c\u80f8\u58c1\u76ae\u80a4\u632b\u88c2\u4f24\u4e14\u8f7b\n\u5ea6\u538b\u75db\uff0c\u53cd\u8df3\u75db\u4e0d\u660e\u663e\uff0c\u5c40\u9650\u6027\u8179\u808c\u7d27\u5f20\uff0c\u6709\u79fb\u52a8\u6027\u6d4a\u97f3\uff0c\u8179\u90e8\u62bd\u51fa\u4e0d\u51dd\u8840\u3002\u8fd9\u4e9b\u75c7\u72b6\u548c\u4f53\u5f81\u8868\u660e\u60a3\n\u8005\u53ef\u80fd\u6709\u5185\u810f\u5668\u5b98\u635f\u4f24\u548c\u5185\u51fa\u8840\u3002 \u5728\u63d0\u4f9b\u7684\u9009\u9879\u4e2d\uff0c\u6211\u4eec\u9700\u8981\u6839\u636e\u56fe\u4e2d\u6807\u6ce8\u7684\u90e8\u4f4d\u548c\u4e34\u5e8a\u8868\u73b0\u6765\u5224\n\u65ad\u6700\u53ef\u80fd\u7684\u8bca\u65ad\uff1a (A) \u813e\u7834\u88c2 - \u813e\u810f\u4f4d\u4e8e\u5de6\u4e0a\u8179\u90e8\uff0c\u813e\u7834\u88c2\u53ef\u80fd\u5bfc\u81f4\u5de6\u4e0a\u8179\u75db\u3001\u4f11\u514b\u75c7\u72b6\uff08\u5982\u4f4e\u8840\n\u538b\u548c\u5fc3\u7387\u52a0\u5feb\uff09\uff0c\u4ee5\u53ca\u8179\u90e8\u62bd\u51fa\u4e0d\u51dd\u8840\u3002 (B) \u809d\u7834\u88c2 - \u809d\u810f\u4f4d\u4e8e\u53f3\u4e0a\u8179\u90e8\uff0c\u809d\u7834\u88c2\u53ef\u80fd\u5bfc\u81f4\u53f3\u4e0a\u8179\n\u75db\u3001\u4f11\u514b\u75c7\u72b6\uff0c\u4ee5\u53ca\u8179\u90e8\u62bd\u51fa\u4e0d\u51dd\u8840\u3002 (C) \u80a0\u7a7f\u5b54 - \u80a0\u7a7f\u5b54\u901a\u5e38\u4f1a\u5bfc\u81f4\u6025\u6027\u8179\u75db\u3001\u8179\u819c\u708e\u75c7\u72b6\uff08\u5982\n\u53cd\u8df3\u75db\u548c\u8179\u808c\u7d27\u5f20\uff09\uff0c\u4f46\u4e0d\u4e00\u5b9a\u4f34\u6709\u79fb\u52a8\u6027\u6d4a\u97f3\u3002 (D) \u80f0\u7834\u88c2 - \u80f0\u817a\u4f4d\u4e8e\u4e0a\u8179\u90e8\u4e2d\u592e\uff0c\u80f0\u7834\u88c2\u53ef\u80fd\n\u5bfc\u81f4\u4e0a\u8179\u75db\u3001\u80cc\u75db\uff0c\u4ee5\u53ca\u6d88\u5316\u9176\u76f8\u5173\u7684\u75c7\u72b6\u3002 \u8003\u8651\u5230\u60a3\u8005\u7684\u75c7\u72b6\u548c\u4f53\u5f81\uff0c\u4ee5\u53ca\u56fe\u4e2d\u6807\u6ce8\u7684\u90e8\u4f4d\uff0c\u6700\n\u6709\u53ef\u80fd\u7684\u8bca\u65ad\u662f\u813e\u7834\u88c2\u6216\u809d\u7834\u88c2\u3002\u4f46\u7531\u4e8e\u56fe\u4e2d\u6807\u6ce8\u7684\u90e8\u4f4d\u4f4d\u4e8e\u5de6\u4fa7\uff0c\u8fd9\u4f7f\u5f97\u813e\u7834\u88c2\u7684\u53ef\u80fd\u6027\u66f4\u9ad8\u3002\u56e0\n\u6b64\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a (A) \u813e\u7834\u88c2 \u8bf7\u6ce8\u610f\uff0c\u8fd9\u662f\u57fa\u4e8e\u9898\u76ee\u63cf\u8ff0\u548c\u56fe\u793a\u4fe1\u606f\u7684\u63a8\u65ad\uff0c\u5b9e\u9645\u8bca\u65ad\u9700\u8981\u901a\u8fc7\u8fdb\n\u4e00\u6b65\u7684\u533b\u5b66\u68c0\u67e5\uff0c\u5982\u8d85\u58f0\u6ce2\u68c0\u67e5\u3001CT\u626b\u63cf\u7b49\u6765\u786e\u5b9a\u3002\nGround Truth: (B)\u809d\u7834\u88c2\n<\u56fe\u7247 1>\nError Category: Perceptual Error\nError Reason: GPT-4V lacks an understanding of the perspective in images. While from an external \nviewpoint, the wound appears to be on the upper left abdomen, from the patient's perspective, the \nwound is actually on the upper right abdomen. GPT-4V's failure to comprehend the image's \nperspective resulted in an error.\nFigure B40: A sample error case of Health and Medicine (subcategory: Clinical Medicine).\nBack to List of figures\n59\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66:\u4e34\u5e8a\u533b\u5b66\nQuestion:\u60a3\u8005\uff0c\u5973\uff0c60 \u5c81\uff0c\u4eca\u65e5\u6668\u72ec\u81ea\u4e0a\u536b\u751f\u95f4\u6454\u5012\uff0c\u4f24\u540e\u611f\u4e0b\u80a2\u5267\u70c8\u75bc\u75db\uff0c\u4e0d\u80fd\u7ad9\u7acb\u884c\u8d70\uff0c\u901a\u8fc7\n<\u56fe\u7247 1>\u56fe\u4e2d\u5224\u65ad\u8be5\u60a3\u8005\u6700\u6709\u53ef\u80fd\u53d1\u751f\u4e86\uff08\uff09\nOption: \n(A)\u80a1\u9aa8\u5e72\u9aa8\u6298\n(B)\u9acb\u5173\u8282\u8131\u4f4d\n(C)\u80eb\u8153\u9aa8\u5e72\u9aa8\u6298\n(D)\u80a1\u9aa8\u9888\u9aa8\u6298\nGPT-4V:\u6839\u636e\u56fe\u7247\u4e2d\u7684\u63cf\u7ed8\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u60a3\u8005\u8eba\u5728\u5e8a\u4e0a\uff0c\u5176\u4e2d\u4e00\u6761\u817f\u5916\u5c55\u548c\u5916\u65cb\u7684\u59ff\u52bf\u3002\u8fd9\u79cd\u59ff\u52bf\n\u901a\u5e38\u4e0e\u9acb\u90e8\u7684\u95ee\u9898\u76f8\u5173\uff0c\u5c24\u5176\u662f\u5728\u8001\u5e74\u4eba\u4e2d\uff0c\u80a1\u9aa8\u9888\u9aa8\u6298\u662f\u4e00\u4e2a\u5e38\u89c1\u7684\u8bca\u65ad\uff0c\u5c24\u5176\u662f\u5728\u8dcc\u5012\u540e\u65e0\u6cd5\u627f\n\u91cd\u7684\u60c5\u51b5\u4e0b\u3002\u6b63\u786e\u7b54\u6848\u662f\uff1a (D) \u80a1\u9aa8\u9888\u9aa8\u6298 \u8fd9\u4e2a\u7ed3\u8bba\u662f\u57fa\u4e8e\u56fe\u4e2d\u60a3\u8005\u7684\u817f\u90e8\u59ff\u52bf\u548c\u5e74\u9f84\uff0c\u4ee5\u53ca\u63cf\u8ff0\n\u7684\u75c7\u72b6\u3002\u7136\u800c\uff0c\u6700\u7ec8\u7684\u8bca\u65ad\u5e94\u8be5\u7531\u533b\u7597\u4e13\u4e1a\u4eba\u5458\u901a\u8fc7\u9002\u5f53\u7684\u4e34\u5e8a\u8bc4\u4f30\u548c\u5f71\u50cf\u5b66\u68c0\u67e5\u6765\u786e\u5b9a\u3002\nGround Truth: (C)\u80eb\u8153\u9aa8\u5e72\u9aa8\u6298\n<\u56fe\u7247 1>\nError Category: Annotation Error\nError Reason: GPT-4V accurately interpreted the image where the patient is lying in bed with one \nleg in an abducted and externally rotated position. The model made the correct inference, but an \nerror occurred in our annotations. The Ground Truth should be a (D)\u80a1\u9aa8\u9888\u9aa8\u6298\nFigure B41: A sample error case of Health and Medicine (subcategory: Clinical Medicine).\nBack to List of figures\n60\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66:\u4e34\u5e8a\u533b\u5b66\nQuestion:\u60a3\u8005\uff0c\u7537\uff0c32 \u5c81\uff0c\u8170\u690e\u95f4\u76d8\u7a81\u51fa\u5165\u9662\uff0c\u884c\u9ad3\u6838\u6458\u9664\u672f\uff0c\u672f\u540e\u62a4\u58eb\u6307\u5bfc\u60a3\u8005\u8fdb\u884c<\u56fe\u7247 1>\u56fe\n\u793a\u953b\u70bc\uff0c\u5176\u76ee\u7684\u662f\u4e3a\u4e86\u9884\u9632\uff1a\nOption: \n(A)\u808c\u8089\u840e\u7f29\n(B)\u5207\u53e3\u611f\u67d3\n(C)\u5f62\u6210\u8840\u80bf\n(D)\u795e\u7ecf\u6839\u7c98\u8fde\nGPT-4V:\u8fd9\u5f20\u56fe\u7247\u5c55\u793a\u4e86\u4e00\u4e2a\u4eba\u8eba\u5728\u80cc\u4e0a\u8fdb\u884c\u817f\u90e8\u953b\u70bc\u7684\u793a\u610f\u56fe\u3002\u6839\u636e\u8fd9\u4e2a\u953b\u70bc\u7684\u59ff\u52bf\uff0c\u5b83\u4f3c\u4e4e\u662f\u4e3a\n\u4e86\u52a0\u5f3a\u817f\u90e8\u548c\u8170\u90e8\u7684\u808c\u8089\u3002\u5728\u8170\u690e\u95f4\u76d8\u7a81\u51fa\u624b\u672f\u540e\uff0c\u8fdb\u884c\u9002\u5f53\u7684\u953b\u70bc\u53ef\u4ee5\u5e2e\u52a9\u6062\u590d\u808c\u8089\u529b\u91cf\uff0c\u9884\u9632\u808c\n\u8089\u840e\u7f29\uff0c\u540c\u65f6\u4e5f\u6709\u52a9\u4e8e\u4fc3\u8fdb\u8170\u90e8\u7684\u7a33\u5b9a\u6027\u548c\u7075\u6d3b\u6027\uff0c\u51cf\u5c11\u795e\u7ecf\u6839\u7c98\u8fde\u7684\u98ce\u9669\u3002\u56e0\u6b64\uff0c\u6839\u636e\u63d0\u4f9b\u7684\u9009\u9879\uff0c\n\u6b63\u786e\u7b54\u6848\u53ef\u80fd\u662f\uff1a (A) \u808c\u8089\u840e\u7f29 (D) \u795e\u7ecf\u6839\u7c98\u8fde \u8fd9\u4e24\u4e2a\u9009\u9879\u90fd\u4e0e\u672f\u540e\u6062\u590d\u953b\u70bc\u7684\u76ee\u7684\u76f8\u5173\u3002\u7136\u800c\uff0c\n\u5982\u679c\u5fc5\u987b\u9009\u62e9\u4e00\u4e2a\u6700\u53ef\u80fd\u7684\u7b54\u6848\uff0c\u6211\u4f1a\u9009\u62e9\uff1a (A) \u808c\u8089\u840e\u7f29 \u56e0\u4e3a\u8fd9\u79cd\u7c7b\u578b\u7684\u953b\u70bc\u9996\u5148\u548c\u76f4\u63a5\u5730\u5173\u8054\n\u5230\u808c\u8089\u529b\u91cf\u7684\u7ef4\u6301\u548c\u589e\u5f3a\u3002\nGround Truth: (D)\u795e\u7ecf\u6839\u7c98\u8fde\n<\u56fe\u7247 1>\nError Category: Reasoning Error\nError Reason: GPT-4V accurately analyzed the image: a person lying on their back for leg exercises \nand inferred the possible correct answers in both (A) and (D). However, it overlooked that the \npatient was admitted due to lumbar disc herniation, and the exercise was intended to prevent (D) \nnerve root adhesion after lumbar disc herniation surgery, leading to an reasoning error.\nFigure B42: A sample error case of Health and Medicine (subcategory: Clinical Medicine).\nBack to List of figures\n61\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66:\u4e34\u5e8a\u533b\u5b66\nQuestion:\u67d0\u75c5\u4eba\uff0c\u73b0\u5728\u51fa\u73b0\u547c\u5438\u56f0\u96be\uff0c\u901a\u8fc7<\u56fe\u7247 1>\u56fe\u793a\uff0c\u53ef\u4ee5\u5224\u5b9a\u75c5\u4eba\u51fa\u73b0\u4e86\u54ea\u79cd\u75be\u75c5\uff1f\uff08\uff09\nOption: \n(A)\u8113\u80f8\n(B) \u5f20\u529b\u6027\u6c14\u80f8\n(C) \u5f00\u653e\u6027\u6c14\u80f8\n(D) \u95ed\u5408\u6027\u6c14\u80f8\nGPT-4V:\u8fd9\u5f20\u56fe\u7247\u5c55\u793a\u4e86\u4e24\u79cd\u4e0d\u540c\u7684\u80f8\u90e8\u60c5\u51b5\u3002\u5de6\u8fb9\uff081\uff09\u663e\u793a\u7684\u662f\u6b63\u5e38\u7684\u80f8\u8154\uff0c\u6ca1\u6709\u660e\u663e\u7684\u5f02\u5e38\u3002\n\u53f3\u8fb9\uff082\uff09\u663e\u793a\u7684\u80f8\u8154\u4e2d\uff0c\u80ba\u90e8\u6709\u660e\u663e\u7684\u584c\u9677\uff0c\u8fd9\u662f\u6c14\u80f8\u7684\u7279\u5f81\u3002\u6c14\u80f8\u662f\u6307\u7a7a\u6c14\u8fdb\u5165\u80f8\u819c\u8154\uff0c\u5bfc\u81f4\u80ba\n\u90e8\u4e0d\u80fd\u5145\u5206\u6269\u5f20\u7684\u60c5\u51b5\u3002 \u5728\u8fd9\u56db\u4e2a\u9009\u9879\u4e2d\uff0c\u8113\u80f8\uff08A\uff09\u901a\u5e38\u662f\u7531\u611f\u67d3\u5f15\u8d77\u7684\u8113\u6db2\u79ef\u805a\u5728\u80f8\u819c\u8154\u5185\uff0c\u800c\n\u4e0d\u662f\u7a7a\u6c14\u3002\u5f00\u653e\u6027\u6c14\u80f8\uff08C\uff09\u662f\u6307\u80f8\u58c1\u6709\u5f00\u653e\u6027\u4f24\u53e3\uff0c\u4f7f\u5f97\u5916\u754c\u7a7a\u6c14\u53ef\u4ee5\u8fdb\u5165\u80f8\u819c\u8154\u3002\u95ed\u5408\u6027\u6c14\u80f8\uff08D\uff09\n\u662f\u6307\u80f8\u58c1\u6ca1\u6709\u660e\u663e\u7684\u5f00\u653e\u6027\u4f24\u53e3\uff0c\u4f46\u80f8\u819c\u8154\u5185\u4ecd\u7136\u6709\u7a7a\u6c14\u79ef\u805a\u3002\u5f20\u529b\u6027\u6c14\u80f8\uff08B\uff09\u662f\u4e00\u79cd\u4e25\u91cd\u7684\u6c14\u80f8\uff0c\n\u5176\u4e2d\u7a7a\u6c14\u6301\u7eed\u8fdb\u5165\u80f8\u819c\u8154\u5e76\u4e0d\u80fd\u9038\u51fa\uff0c\u5bfc\u81f4\u538b\u529b\u589e\u52a0\uff0c\u5f71\u54cd\u5fc3\u810f\u548c\u80ba\u7684\u529f\u80fd\u3002 \u7531\u4e8e\u56fe\u7247\u4e2d\u6ca1\u6709\u663e\u793a\n\u80f8\u58c1\u7684\u5f00\u653e\u6027\u4f24\u53e3\uff0c\u4e5f\u6ca1\u6709\u8db3\u591f\u7684\u4fe1\u606f\u6765\u5224\u65ad\u662f\u5426\u4e3a\u5f20\u529b\u6027\u6c14\u80f8\uff0c\u56e0\u6b64\u6700\u53ef\u80fd\u7684\u9009\u9879\u662f\u95ed\u5408\u6027\u6c14\u80f8\n\uff08D\uff09\u3002\u7136\u800c\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u4ec5\u51ed\u4e00\u5f20\u56fe\u793a\u5e76\u4e0d\u80fd\u786e\u8bca\uff0c\u901a\u5e38\u9700\u8981\u7ed3\u5408\u60a3\u8005\u7684\u4e34\u5e8a\u8868\u73b0\u548c\u5176\u4ed6\u8bca\u65ad\n\u624b\u6bb5\uff0c\u5982X\u5149\u7247\u7b49\uff0c\u6765\u786e\u5b9a\u786e\u5207\u7684\u75be\u75c5\u7c7b\u578b\u3002 \u6b63\u786e\u7b54\u6848\uff1a(D) \u95ed\u5408\u6027\u6c14\u80f8\nGround Truth: (C)\u5f00\u653e\u6027\u6c14\u80f8\n<\u56fe\u7247 1>\nError Category: Textual Understanding\nError Reason: GPT-4V misunderstood the question, where the left side (1) represents inhalation and \nthe right side (2) represents exhalation, both depicting pathological images. GPT-4V incorrectly \ninterpreted the left side (1) as a normal thoracic cavity.\nFigure B43: A sample error case of Health and Medicine (subcategory: Clinical Medicine).\nBack to List of figures\n62\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66:\u4e34\u5e8a\u533b\u5b66\nQuestion:\u60a3\u8005\u7537\uff0c50 \u5c81\u3002\u786e\u8bca\u4e3a COPD\uff0c\u73b0\u8981\u6c42\u62a4\u58eb\u6307\u5bfc\u60a3\u8005\u505a\u547c\u5438\u529f\u80fd\u953b\u70bc\uff0c\u5982\u56fe\u7247<\u56fe\u7247 1>\u6240\n\u793a\uff0c\u4e0b\u5217\u63cf\u8ff0\u6b63\u786e\u7684\u662f\uff1a()\nOption: \n(A)\u6307\u5bfc\u60a3\u8005\u7f29\u5507\u5438\u6c14\uff0c\u7528\u9f3b\u7f13\u6162\u547c\u6c14\n(B) \u6307\u5bfc\u60a3\u8005\u7528\u9f3b\u5438\u6c14\uff0c\u7f29\u5507\u5feb\u901f\u547c\u6c14\n(C) \u5438\u6c14\u548c\u547c\u6c14\u4e4b\u6bd4\u4e3a 1\uff1a2 \n(D) \u5438\u6c14\u548c\u547c\u6c14\u4e4b\u6bd4\u662f 2\uff1a1\nGPT-4V:\u6839\u636e\u63d0\u4f9b\u7684\u56fe\u7247\u548c\u63cf\u8ff0\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a (A) \u6307\u5bfc\u60a3\u8005\u7f29\u5507\u5438\u6c14\uff0c\u7528\u9f3b\u7f13\u6162\u547c\u6c14 \u56fe\u7247\u5c55\u793a\u4e86\n\u4e00\u4e2a\u4eba\u5728\u8fdb\u884c\u7f29\u5507\u547c\u5438\u7ec3\u4e60\uff0c\u8fd9\u662f\u4e00\u79cd\u5e38\u7528\u4e8eCOPD\uff08\u6162\u6027\u963b\u585e\u6027\u80ba\u75be\u75c5\uff09\u60a3\u8005\u7684\u547c\u5438\u6280\u5de7\u3002\u8be5\u6280\u5de7\u6d89\n\u53ca\u901a\u8fc7\u9f3b\u5b50\u5438\u6c14\u548c\u901a\u8fc7\u7f29\u5c0f\u7684\u5634\u5507\u7f13\u6162\u547c\u6c14\uff0c\u4ee5\u589e\u52a0\u6c14\u9053\u538b\u529b\uff0c\u51cf\u5c11\u6c14\u9053\u584c\u9677\uff0c\u5e2e\u52a9\u6539\u5584\u547c\u5438\u3002\u9009\u9879\n(B)\u4e2d\u63d0\u5230\u7684\u201c\u5feb\u901f\u547c\u6c14\u201d\u662f\u4e0d\u6b63\u786e\u7684\uff0c\u56e0\u4e3a\u7f29\u5507\u547c\u5438\u7684\u76ee\u7684\u662f\u8981\u7f13\u6162\u547c\u6c14\u3002\u9009\u9879(C)\u548c(D)\u5173\u4e8e\u5438\u6c14\n\u548c\u547c\u6c14\u6bd4\u4f8b\u7684\u63cf\u8ff0\u6ca1\u6709\u76f4\u63a5\u4f53\u73b0\u5728\u56fe\u7247\u4e2d\uff0c\u4f46\u901a\u5e38\u5728\u7f29\u5507\u547c\u5438\u4e2d\uff0c\u547c\u6c14\u65f6\u95f4\u4f1a\u6bd4\u5438\u6c14\u65f6\u95f4\u957f\uff0c\u6240\u4ee5(C)\n\u53ef\u80fd\u66f4\u63a5\u8fd1\u6b63\u786e\u7684\u547c\u5438\u6bd4\u4f8b\u3002\u7136\u800c\uff0c\u6ca1\u6709\u8db3\u591f\u7684\u4fe1\u606f\u6765\u786e\u5b9a\u786e\u5207\u7684\u6bd4\u4f8b\uff0c\u56e0\u6b64\u6700\u51c6\u786e\u7684\u7b54\u6848\u662f(A)\u3002\nGround Truth: (A)\u6307\u5bfc\u60a3\u8005\u7f29\u5507\u5438\u6c14\uff0c\u7528\u9f3b\u7f13\u6162\u547c\u6c14\n<\u56fe\u7247 1>\nError Category: Answer Extraction Error\nError Reason: GPT-4V correctly interpreted the image and made an accurate judgment. However, \nin its subsequent analysis, the inclusion of keywords such as \u201c\u6b63\u786e\" led to errors in the Extract & \nMatch process, resulting in the extraction of answers (A) and (C).\nFigure B44: A sample error case of Health and Medicine (subcategory: Clinical Medicine).\nBack to List of figures\n63\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66:\u4e34\u5e8a\u533b\u5b66\nQuestion:\u60a3\u513f\uff0c\u7537\uff0c13 \u5c81\uff0c\u60a3\u6709\u5316\u8113\u6027\u9aa8\u9ad3\u708e\uff0c\u7ecf\u5168\u8eab\u6297\u751f\u7d20\u6cbb\u7597\uff0c\u6548\u679c\u4e0d\u4f73\uff0c\u7ed9\u4e88\u624b\u672f\u6cbb\u7597\uff0c\u8fdb\n\u884c\u5f15\u6d41\u8113\u6db2\uff0c\u5982\u56fe<\u56fe\u7247 1>\u6240\u793a\uff0c\u8bf7\u95ee\u2460\u548c\u2461\u9ad8\u5ea6\u5206\u522b\u4e3a\uff1f\nOption: \n(A) 40cm\uff5e50cm\u548c30cm \n(B) 50cm\uff5e60cm\u548c40cm \n(C) 60cm\uff5e70cm\u548c50cm \n(D) 70cm\uff5e80cm\u548c60cm\nGPT-4V: \u5f88\u62b1\u6b49\uff0c\u4f46\u6211\u65e0\u6cd5\u63d0\u4f9b\u7cbe\u786e\u7684\u6d4b\u91cf\u7ed3\u679c\uff0c\u56e0\u4e3a\u56fe\u7247\u4e2d\u6ca1\u6709\u63d0\u4f9b\u5c3a\u5ea6\u53c2\u8003\u6216\u4efb\u4f55\u53ef\u4ee5\u7528\u6765\u786e\u5b9a\n\u5b9e\u9645\u9ad8\u5ea6\u7684\u6807\u8bb0\u3002\u56e0\u6b64\uff0c\u6211\u65e0\u6cd5\u786e\u5b9a\u2460\u548c\u2461\u7684\u786e\u5207\u9ad8\u5ea6\u3002\u5982\u679c\u8fd9\u662f\u4e00\u4e2a\u7406\u8bba\u6216\u5b9e\u8df5\u8003\u8bd5\u95ee\u9898\uff0c\u901a\u5e38\u4f1a\n\u6709\u4e00\u4e9b\u80cc\u666f\u77e5\u8bc6\u6216\u6807\u51c6\u5b9e\u8df5\u6765\u6307\u5bfc\u4f60\u9009\u62e9\u6700\u5408\u9002\u7684\u7b54\u6848\u3002\u5728\u6ca1\u6709\u8fd9\u4e9b\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u6211\u65e0\u6cd5\u63d0\u4f9b\u6b63\u786e\n\u7b54\u6848\u3002\nGround Truth: (C) 60cm\uff5e70cm\u548c50cm\n<\u56fe\u7247 1>\nError Category: Lack of Knowledge\nError Reason: GPT-4V correctly interpreted the image and identified both \u2460 and \u2461, but due to a \nlack of practical knowledge, it could not determine the height of \u2460 and \u2461.\nFigure B45: A sample error case of Health and Medicine (subcategory: Clinical Medicine).\nBack to List of figures\n64\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66: \u8bca\u65ad\u5b66\u4e0e\u5b9e\u9a8c\u5ba4\u533b\u5b66\nQuestion:\u5982<\u56fe\u7247 1>\u6240\u793a\u4e3a\u5e38\u89c1\u7684\u70ed\u578b\u4e2d\u7684_____\u3002 \nOption: \n(A) \u95f4\u6b47\u70ed\n(B) \u5f1b\u5f20\u70ed\n(C) \u56de\u5f52\u70ed\n(D) \u7a3d\u7559\u70ed\nGPT-4V:\u8fd9\u5f20\u56fe\u7247\u663e\u793a\u7684\u662f\u4e00\u5f20\u4f53\u6e29\u56fe\uff0c\u4e0a\u9762\u6709\u4f53\u6e29\u968f\u65f6\u95f4\u53d8\u5316\u7684\u8bb0\u5f55\u3002\u4ece\u56fe\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0c\u4f53\u6e29\u5728\u4e00\n\u6bb5\u65f6\u95f4\u5185\u4fdd\u6301\u6b63\u5e38\uff0c\u7136\u540e\u7a81\u7136\u5347\u9ad8\uff0c\u4e4b\u540e\u53c8\u8fc5\u901f\u4e0b\u964d\u5230\u6b63\u5e38\u6c34\u5e73\uff0c\u8fd9\u79cd\u6a21\u5f0f\u91cd\u590d\u51fa\u73b0\u3002\u8fd9\u79cd\u4f53\u6e29\u53d8\u5316\n\u6a21\u5f0f\u7b26\u5408\u95f4\u6b47\u70ed\u7684\u7279\u70b9\uff0c\u95f4\u6b47\u70ed\u662f\u6307\u4f53\u6e29\u5728\u53d1\u70ed\u671f\u95f4\u5347\u9ad8\uff0c\u7136\u540e\u5728\u975e\u53d1\u70ed\u671f\u95f4\u6062\u590d\u6b63\u5e38\u3002\n\u6b63\u786e\u7b54\u6848\u662f\uff1a\n(A) \u95f4\u6b47\u70ed\nGround Truth: (A)\u95f4\u6b47\u70ed \n<\u56fe\u7247 1>\nFigure B46: A sample correct case of Health and Medicine (subcategory: Diagnostics and\nLab Medicine).\nBack to List of figures\n65\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66: \u8bca\u65ad\u5b66\u4e0e\u5b9e\u9a8c\u5ba4\u533b\u5b66\nQuestion:\u8bc6\u522b<\u56fe\u7247 1>\u5207\u7247\u4e3a_ \nOption: \n(A) \u809d\u7ec6\u80de\u8102\u80aa\u53d8\u6027\n(B) \u4e0a\u76ae\u7ec6\u80de\u8102\u80aa\u53d8\u6027\n(C) \u6dcb\u5df4\u7ec6\u80de\u8102\u80aa\u53d8\u6027\n(D) \u5176\u4f59\u9009\u9879\u5747\u4e0d\u5bf9\nGPT-4V:\u8fd9\u5f20\u56fe\u7247\u663e\u793a\u7684\u662f\u4e00\u4e2a\u663e\u5fae\u955c\u4e0b\u7684\u7ec4\u7ec7\u5207\u7247\u3002\u5728\u8fd9\u4e2a\u5207\u7247\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u8bb8\u591a\u542b\u6709\u5927\u91cf\u7a7a\n\u6ce1\u7684\u7ec6\u80de\uff0c\u8fd9\u4e9b\u7a7a\u6ce1\u53ef\u80fd\u4ee3\u8868\u8102\u80aa\u6ef4\u3002\u8fd9\u79cd\u73b0\u8c61\u901a\u5e38\u4e0e\u8102\u80aa\u53d8\u6027\u6709\u5173\uff0c\u8fd9\u662f\u4e00\u79cd\u7ec6\u80de\u5185\u8102\u80aa\u79ef\u7d2f\u7684\u75c5\n\u7406\u8fc7\u7a0b\u3002\n\u5728\u7ed9\u51fa\u7684\u9009\u9879\u4e2d\uff0c\u6211\u4eec\u9700\u8981\u786e\u5b9a\u54ea\u79cd\u7ec6\u80de\u7c7b\u578b\u53d1\u751f\u4e86\u8102\u80aa\u53d8\u6027\u3002\u809d\u7ec6\u80de\u8102\u80aa\u53d8\u6027\uff08\u9009\u9879A\uff09\u662f\u4e00\u79cd\u5e38\n\u89c1\u7684\u75c5\u7406\u73b0\u8c61\uff0c\u901a\u5e38\u4e0e\u9152\u7cbe\u6027\u6216\u975e\u9152\u7cbe\u6027\u809d\u75c5\u6709\u5173\u3002\u4e0a\u76ae\u7ec6\u80de\u8102\u80aa\u53d8\u6027\uff08\u9009\u9879B\uff09\u548c\u6dcb\u5df4\u7ec6\u80de\u8102\u80aa\u53d8\n\u6027\uff08\u9009\u9879C\uff09\u5219\u4e0d\u662f\u5e38\u89c1\u7684\u672f\u8bed\u6216\u8bca\u65ad\u3002\n\u6839\u636e\u56fe\u50cf\u4e2d\u7684\u7ec4\u7ec7\u7ed3\u6784\u548c\u7ec6\u80de\u7c7b\u578b\uff0c\u8fd9\u4e2a\u5207\u7247\u6700\u6709\u53ef\u80fd\u662f\u6765\u81ea\u809d\u810f\u7684\u3002\u56e0\u6b64\uff0c\u6700\u53ef\u80fd\u6b63\u786e\u7684\u9009\u9879\u662f\uff1a\n(A) \u809d\u7ec6\u80de\u8102\u80aa\u53d8\u6027\n\u8bf7\u6ce8\u610f\uff0c\u6ca1\u6709\u4e34\u5e8a\u4fe1\u606f\u548c\u7279\u5b9a\u7684\u67d3\u8272\u7ec6\u8282\uff0c\u8fd9\u53ea\u662f\u4e00\u4e2a\u57fa\u4e8e\u56fe\u50cf\u7279\u5f81\u7684\u5047\u8bbe\u6027\u5224\u65ad\u3002\u5b9e\u9645\u7684\u8bca\u65ad\u5e94\u7531\n\u4e13\u4e1a\u7684\u533b\u7597\u4eba\u5458\u5728\u7efc\u5408\u6240\u6709\u76f8\u5173\u4fe1\u606f\u540e\u505a\u51fa\u3002\nGround Truth: (A)\u809d\u7ec6\u80de\u8102\u80aa\u53d8\u6027 \n<\u56fe\u7247 1>\nFigure B47: A sample correct case of Health and Medicine (subcategory: Diagnostics and\nLab Medicine).\nBack to List of figures\n66\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66: \u8bca\u65ad\u5b66\u4e0e\u5b9e\u9a8c\u5ba4\u533b\u5b66\nQuestion:\u8bf7\u6839\u636e\u4ee5\u4e0bPAgT\u7684\u5b9e\u9a8c\u7ed3\u679c<\u56fe\u7247 1>\u5224\u65ad\u53ef\u80fd\u7684\u75be\u75c5\u3002\u75c5\u4f8b1\uff1a____________\uff1b\u75c5\u4f8b2\uff1a\n___________\uff1b\u75c5\u4f8b3\uff1a___________\uff1b\u75c5\u4f8b4\uff1a___________\u3002 \nOption: \n(A) \u8840\u5c0f\u677f\u805a\u96c6\u529f\u80fd\u6b63\u5e38\uff1b\u8840\u7ba1\u6027\u8840\u53cb\u75c5\u6216\u5de8\u5927\u8840\u5c0f\u677f\u7efc\u5408\u5f81\uff1b\u8840\u5c0f\u677f\u91ca\u653e\u529f\u80fd\u969c\u788d\u6216\u8d2e\u5b58\u6c60\u75c5\uff1b\u8840\n\u5c0f\u677f\u65e0\u529b\u75c7\n(B) \u8840\u7ba1\u6027\u8840\u53cb\u75c5\u6216\u5de8\u5927\u8840\u5c0f\u677f\u7efc\u5408\u5f81\uff1b\u8840\u5c0f\u677f\u91ca\u653e\u529f\u80fd\u969c\u788d\u6216\u8d2e\u5b58\u6c60\u75c5\uff1b\u8840\u5c0f\u677f\u65e0\u529b\u75c7\uff1b\u8840\u5c0f\u677f\u805a\n\u96c6\u529f\u80fd\u6b63\u5e38\n(C) \u8840\u5c0f\u677f\u91ca\u653e\u529f\u80fd\u969c\u788d\u6216\u8d2e\u5b58\u6c60\u75c5\uff1b\u8840\u5c0f\u677f\u65e0\u529b\u75c7\uff1b\u8840\u5c0f\u677f\u805a\u96c6\u529f\u80fd\u6b63\u5e38\uff1b\u8840\u7ba1\u6027\u8840\u53cb\u75c5\u6216\u5de8\u5927\u8840\n\u5c0f\u677f\u7efc\u5408\u5f81\n(D) \u8840\u7ba1\u6027\u8840\u53cb\u75c5\u6216\u5de8\u5927\u8840\u5c0f\u677f\u7efc\u5408\u5f81\uff1b\u8840\u5c0f\u677f\u65e0\u529b\u75c7\uff1b\u8840\u5c0f\u677f\u91ca\u653e\u529f\u80fd\u969c\u788d\u6216\u8d2e\u5b58\u6c60\u75c5\uff1b\u8840\u5c0f\u677f\u805a\n\u96c6\u529f\u80fd\u6b63\u5e38\nGPT-4V:\u6839\u636e\u56fe\u7247\u4e2d\u63d0\u4f9b\u7684PAgT\uff08\u8840\u5c0f\u677f\u805a\u96c6\u8bd5\u9a8c\uff09\u7684\u5b9e\u9a8c\u7ed3\u679c\uff0c\u6211\u4eec\u53ef\u4ee5\u5c1d\u8bd5\u5bf9\u6bcf\u4e2a\u75c5\u4f8b\u8fdb\u884c\u5206\u6790\uff1a\n\u75c5\u4f8b1\uff1a\u6240\u6709\u7684\u523a\u6fc0\u5242\uff08ADP\u3001Epinephrine\u3001Collagen\u3001Arachidonate\u3001Ristocetin\uff09\u90fd\u663e\u793a\u6b63\u5e38\u805a\n\u96c6\uff08N\uff09\uff0c\u8fd9\u8868\u660e\u75c5\u4f8b1\u7684\u8840\u5c0f\u677f\u805a\u96c6\u529f\u80fd\u6b63\u5e38\u3002\n\u75c5\u4f8b2\uff1a\u9664\u4e86Ristocetin\u523a\u6fc0\u4e0b\u663e\u793a\u5f02\u5e38\u805a\u96c6\uff08A\uff09\u5916\uff0c\u5176\u4ed6\u523a\u6fc0\u5242\u4e0b\u7684\u805a\u96c6\u90fd\u662f\u6b63\u5e38\u7684\u3002Ristocetin\n\u5f02\u5e38\u901a\u5e38\u4e0e\u8840\u7ba1\u6027\u8840\u53cb\u75c5\u6216\u5de8\u5927\u8840\u5c0f\u677f\u7efc\u5408\u5f81\u76f8\u5173\u3002\n\u75c5\u4f8b3\uff1aADP\u3001Epinephrine\u3001Collagen\u523a\u6fc0\u4e0b\u663e\u793a\u5f02\u5e38\u805a\u96c6\uff08A\uff09\uff0c\u800cRistocetin\u523a\u6fc0\u4e0b\u663e\u793a\u6b63\u5e38\u805a\u96c6\n\uff08N\uff09\u3002\u8fd9\u79cd\u6a21\u5f0f\u53ef\u80fd\u8868\u660e\u8840\u5c0f\u677f\u91ca\u653e\u529f\u80fd\u969c\u788d\u6216\u8d2e\u5b58\u6c60\u75c5\u3002\n\u75c5\u4f8b4\uff1a\u6240\u6709\u523a\u6fc0\u5242\uff08\u9664\u4e86Ristocetin\uff09\u4e0b\u90fd\u663e\u793a\u5f02\u5e38\u805a\u96c6\uff08A\uff09\uff0c\u8fd9\u53ef\u80fd\u8868\u660e\u8840\u5c0f\u677f\u65e0\u529b\u75c7\u3002\n\u56e0\u6b64\uff0c\u6839\u636e\u8fd9\u4e9b\u4fe1\u606f\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a\n\u75c5\u4f8b1\uff1a\u8840\u5c0f\u677f\u805a\u96c6\u529f\u80fd\u6b63\u5e38\uff1b\n\u75c5\u4f8b2\uff1a\u8840\u7ba1\u6027\u8840\u53cb\u75c5\u6216\u5de8\u5927\u8840\u5c0f\u677f\u7efc\u5408\u5f81\uff1b\n\u75c5\u4f8b3\uff1a\u8840\u5c0f\u677f\u91ca\u653e\u529f\u80fd\u969c\u788d\u6216\u8d2e\u5b58\u6c60\u75c5\uff1b\n\u75c5\u4f8b4\uff1a\u8840\u5c0f\u677f\u65e0\u529b\u75c7\u3002\n\u6240\u4ee5\u6b63\u786e\u7b54\u6848\u662f (A) \u8840\u5c0f\u677f\u805a\u96c6\u529f\u80fd\u6b63\u5e38\uff1b\u8840\u7ba1\u6027\u8840\u53cb\u75c5\u6216\u5de8\u5927\u8840\u5c0f\u677f\u7efc\u5408\u5f81\uff1b\u8840\u5c0f\u677f\u91ca\u653e\u529f\u80fd\u969c\n\u788d\u6216\u8d2e\u5b58\u6c60\u75c5\uff1b\u8840\u5c0f\u677f\u65e0\u529b\u75c7\u3002\nGround Truth: (A) \n<\u56fe\u7247 1>\nFigure B48: A sample correct case of Health and Medicine (subcategory: Diagnostics and\nLab Medicine).\nBack to List of figures\n67\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66: \u8bca\u65ad\u5b66\u4e0e\u5b9e\u9a8c\u5ba4\u533b\u5b66\nQuestion:<\u56fe\u7247 1>\u56fe\u793a\u53ef\u89c1\u7684\u5f02\u5e38\u7ec6\u80de\u540d\u79f0\uff08\uff09\nOption: \n(A) \u5206\u5316\u597d\u817a\u764c\u7ec6\u80de\n(B) \u5206\u5316\u5dee\u817a\u764c\u7ec6\u80de\n(C) \u5206\u5316\u597d\u7ea4\u7ef4\u6027\u9cde\u764c\u7ec6\u80de\n(D) \u653e\u7597\u53cd\u5e94\u7ec6\u80de\nGPT-4V:\u5f88\u62b1\u6b49\uff0c\u4f46\u6211\u65e0\u6cd5\u63d0\u4f9b\u533b\u5b66\u8bca\u65ad\u6216\u8bc6\u522b\u7279\u5b9a\u7c7b\u578b\u7684\u7ec6\u80de\u3002\u8fd9\u5f20\u56fe\u7247\u663e\u793a\u7684\u662f\u67d3\u8272\u540e\u7684\u7ec6\u80de\uff0c\n\u4f46\u662f\u6ca1\u6709\u8db3\u591f\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u6216\u4e13\u4e1a\u77e5\u8bc6\u6765\u786e\u5b9a\u8fd9\u4e9b\u7ec6\u80de\u7684\u786e\u5207\u7c7b\u578b\u3002\u5728\u533b\u5b66\u9886\u57df\uff0c\u6b63\u786e\u8bc6\u522b\u548c\u5206\u7c7b\u7ec6\n\u80de\u901a\u5e38\u9700\u8981\u4e13\u4e1a\u7684\u8bad\u7ec3\u548c\u77e5\u8bc6\uff0c\u4ee5\u53ca\u8be6\u7ec6\u7684\u4e34\u5e8a\u4fe1\u606f\u3002\u56e0\u6b64\uff0c\u6211\u65e0\u6cd5\u9009\u62e9\u6b63\u786e\u7b54\u6848\u3002\u5982\u679c\u8fd9\u662f\u4e00\u4e2a\u533b\n\u5b66\u76f8\u5173\u7684\u95ee\u9898\uff0c\u5efa\u8bae\u54a8\u8be2\u5177\u6709\u76f8\u5173\u4e13\u4e1a\u77e5\u8bc6\u7684\u533b\u7597\u4e13\u4e1a\u4eba\u5458\u3002\nGround Truth: (D)\u653e\u7597\u53cd\u5e94\u7ec6\u80de \n<\u56fe\u7247 1>\nError Category:Reject to Answer \nError Reason:The security mechanisms of the GPT series make GPT-4V extremely conservative in \ncritical areas such as healthcare, hence it refuses to answer.\nFigure B49: A sample error case of Health and Medicine (subcategory: Diagnostics and Lab\nMedicine).\nBack to List of figures\n68\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66: \u8bca\u65ad\u5b66\u4e0e\u5b9e\u9a8c\u5ba4\u533b\u5b66\nQuestion:<\u56fe\u7247 1>\u4e2d\u4f55\u8005\u4e0d\u5c5e\u4e8e\u80fc\u80dd\u4f53\u7684\u5206\u90e8\uff08\uff09\nOption: \n(A) \u80fc\u80dd\u4f53\u5634\n(B) \u80fc\u80dd\u4f53\u819d\n(C) \u80fc\u80dd\u4f53\u5e72\n(D) \u80fc\u80dd\u4f53\u4f53\u90e8\nGPT-4V:\u62b1\u6b49\uff0c\u7531\u4e8e\u56fe\u7247\u4e2d\u7684\u6587\u5b57\u548c\u6807\u8bb0\u4e0d\u8db3\u4ee5\u63d0\u4f9b\u5173\u4e8e\u80fc\u80dd\u4f53\u5206\u90e8\u7684\u786e\u5207\u4fe1\u606f\uff0c\u6211\u65e0\u6cd5\u76f4\u63a5\u4ece\u56fe\u7247\n\u4e2d\u786e\u5b9a\u54ea\u4e2a\u9009\u9879\u662f\u4e0d\u5c5e\u4e8e\u80fc\u80dd\u4f53\u7684\u5206\u90e8\u3002\u7136\u800c\uff0c\u6839\u636e\u6211\u6240\u77e5\uff0c\u80fc\u80dd\u4f53\u7684\u4e3b\u8981\u5206\u90e8\u5305\u62ec\u80fc\u80dd\u4f53\u524d\u90e8\uff08\u5305\n\u62ec\u5634\u90e8\uff09\u3001\u819d\u90e8\u3001\u5e72\u90e8\u548c\u540e\u90e8\uff08\u6709\u65f6\u79f0\u4e3a\u4f53\u90e8\uff09\u3002\u56e0\u6b64\uff0c\u6240\u6709\u63d0\u4f9b\u7684\u9009\u9879\uff08A\uff09\u80fc\u80dd\u4f53\u5634\u3001\uff08B\uff09\u80fc\u80dd\n\u4f53\u819d\u3001\uff08C\uff09\u80fc\u80dd\u4f53\u5e72\u548c\uff08D\uff09\u80fc\u80dd\u4f53\u4f53\u90e8\u90fd\u662f\u80fc\u80dd\u4f53\u7684\u4e00\u90e8\u5206\u3002\u6ca1\u6709\u63d0\u4f9b\u4e0d\u5c5e\u4e8e\u80fc\u80dd\u4f53\u5206\u90e8\u7684\u9009\u9879\u3002\n\u5982\u679c\u5fc5\u987b\u9009\u62e9\u4e00\u4e2a\u7b54\u6848\uff0c\u6211\u5efa\u8bae\u91cd\u65b0\u68c0\u67e5\u9898\u76ee\u548c\u9009\u9879\uff0c\u56e0\u4e3a\u53ef\u80fd\u5b58\u5728\u8bef\u89e3\u6216\u4fe1\u606f\u4e0d\u8db3\u3002\nGround Truth: (D)\u80fc\u80dd\u4f53\u4f53\u90e8 \n<\u56fe\u7247 1>\nError Category:Reject to Answer\nError Reason:GPT-4V refused to answer due to a lack of relevant knowledge, but the question can \nlead to the conclusion D.\nFigure B50: A sample error case of Health and Medicine (subcategory: Diagnostics and Lab\nMedicine).\nBack to List of figures\n69\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66: \u8bca\u65ad\u5b66\u4e0e\u5b9e\u9a8c\u5ba4\u533b\u5b66\nQuestion:\u4e0b\u9762\u662f\u67d0\u57fa\u56e0\u4e2d\u7684\u4e00\u4e2a\u7247\u6bb5\uff1a<\u56fe\u7247 1>\u6307\u51fa\u8f6c\u5f55\u7684\u65b9\u5411_______\uff0c\u8f6c\u5f55\u6a21\u677f\u662f______\u3002\nOption: \n(A) \u4ece\u5de6\u5411\u53f3\uff1b\u8d1f\u94fe\n(B) \u4ece\u53f3\u5411\u5de6\uff1b\u6b63\u94fe\n(C) \u4ece\u53f3\u5411\u5de6\uff1b\u8d1f\u94fe\n(D) \u4ece\u5de6\u5411\u53f3\uff1b\u6b63\u94fe\nGPT-4V:\u5728DNA\u4e2d\uff0c\u8f6c\u5f55\u662f\u7531RNA\u805a\u5408\u9176\u50ac\u5316\u7684\u8fc7\u7a0b\uff0c\u5b83\u6cbf\u7740DNA\u6a21\u677f\u94fe\u5408\u6210RNA\u3002RNA\u805a\u5408\u9176\u5728DNA\u6a21\u677f\n\u94fe\u4e0a\u79fb\u52a8\u7684\u65b9\u5411\u662f\u4ece3'\u52305'\uff0c\u56e0\u4e3aRNA\u662f\u6309\u71675'\u52303'\u7684\u65b9\u5411\u5408\u6210\u7684\u3002\u5728\u63d0\u4f9b\u7684DNA\u7247\u6bb5\u4e2d\uff0c\u6211\u4eec\u9700\u8981\u627e\n\u5230\u54ea\u6761\u94fe\u4f5c\u4e3a\u6a21\u677f\uff0c\u5e76\u786e\u5b9a\u8f6c\u5f55\u7684\u65b9\u5411\u3002\n\u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\uff0c\u6211\u4eec\u6ca1\u6709\u76f4\u63a5\u7684\u4fe1\u606f\u6765\u786e\u5b9a\u54ea\u6761\u94fe\u662f\u6a21\u677f\u94fe\uff0c\u56e0\u4e3a\u8f6c\u5f55\u53ef\u4ee5\u5728\u4e24\u6761\u94fe\u4e0a\u7684\u4efb\u4f55\u4e00\u6761\u8fdb\n\u884c\uff0c\u53d6\u51b3\u4e8e\u57fa\u56e0\u7684\u5177\u4f53\u4f4d\u7f6e\u548c\u65b9\u5411\u3002\u7136\u800c\uff0c\u901a\u5e38\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u53ef\u4ee5\u5047\u8bbe\u56fe\u50cf\u4e2d\u63d0\u4f9b\u7684\u4fe1\u606f\u662f\u4e3a\u4e86\u56de\u7b54\n\u95ee\u9898\u800c\u8bbe\u8ba1\u7684\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u53ef\u4ee5\u5047\u8bbe\u8f6c\u5f55\u662f\u6cbf\u77403'\u52305'\u65b9\u5411\u7684\u94fe\u8fdb\u884c\u7684\uff0c\u8fd9\u6837RNA\u805a\u5408\u9176\u5c31\u53ef\u4ee5\u57285'\n\u52303'\u65b9\u5411\u4e0a\u5408\u6210RNA\u3002\n\u6839\u636e\u8fd9\u4e2a\u903b\u8f91\uff0c\u5982\u679c\u8f6c\u5f55\u662f\u6cbf\u7740\u4e0a\u9762\u7684\u94fe\uff08\u8d1f\u94fe\uff09\u4ece5'\u52303'\u65b9\u5411\u8fdb\u884c\u7684\uff0c\u90a3\u4e48RNA\u805a\u5408\u9176\u5c06\u4f1a\u6cbf\u7740\u4e0b\n\u9762\u7684\u94fe\uff08\u6b63\u94fe\uff09\u4ece3'\u52305'\u65b9\u5411\u79fb\u52a8\uff0c\u8fd9\u610f\u5473\u7740\u8f6c\u5f55\u7684\u65b9\u5411\u662f\u4ece\u53f3\u5411\u5de6\uff0c\u8f6c\u5f55\u6a21\u677f\u662f\u6b63\u94fe\u3002\n\u56e0\u6b64\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a\n(B) \u4ece\u53f3\u5411\u5de6\uff1b\u6b63\u94fe\",\nGround Truth: (C)\u8840\u5c0f\u677f\u805a\u96c6\u529f\u80fd\u6b63\u5e38\uff1b\u8840\u7ba1\u6027\u8840\u53cb\u75c5\u6216\u5de8\u5927\u8840\u5c0f\u677f\u7efc\u5408\u5f81\uff1b\u8840\u5c0f\u677f\u91ca\u653e\u529f\u80fd\u969c\u788d\n\u6216\u8d2e\u5b58\u6c60\u75c5\uff1b\u8840\u5c0f\u677f\u65e0\u529b\u75c7 \n<\u56fe\u7247 1>\nError Category:Reasoning Error \nError Reason:Transcription indeed proceeds from right to left, therefore it occurs along the 3' to 5' \ndirection of the template (negative) strand. The strand mentioned above (negative strand) is the \ntranscription template. GPT-4V overextended in its deduction, leading to an incorrect conclusion.\nFigure B51: A sample error case of Health and Medicine (subcategory: Diagnostics and Lab\nMedicine).\nBack to List of figures\n70\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66: \u8bca\u65ad\u5b66\u4e0e\u5b9e\u9a8c\u5ba4\u533b\u5b66\nQuestion:<\u56fe\u7247 1>\u56fe\u793a\u4e3a\u5916\u5468\u8840\u6d82\u7247\uff0c\u4e2d\u5fc3\u533a\u7ec6\u80de\u540d\u79f0\uff08\uff09\nOption: \n(A) \u6d46\u7ec6\u80de\n(B) \u6dcb\u5df4\u7ec6\u80de\n(C) \u5de8\u6838\u7ec6\u80de\n(D) \u53cd\u5e94\u6027\u6dcb\u5df4\u7ec6\u80de\nGPT-4V:\u8fd9\u5f20\u56fe\u7247\u663e\u793a\u7684\u662f\u4e00\u4e2a\u8840\u6d82\u7247\u4e2d\u7684\u7ec6\u80de\uff0c\u5177\u6709\u8f83\u5927\u7684\u7ec6\u80de\u4f53\u79ef\u3001\u5706\u5f62\u6216\u7565\u5448\u4e0d\u89c4\u5219\u5f62\u72b6\u7684\u7ec6\n\u80de\u6838\uff0c\u4ee5\u53ca\u5bbd\u5e7f\u7684\u84dd\u8272\u80de\u6d46\u3002\u6839\u636e\u8fd9\u4e9b\u7279\u5f81\uff0c\u8fd9\u4e2a\u7ec6\u80de\u6700\u6709\u53ef\u80fd\u662f\u4e00\u4e2a\u6dcb\u5df4\u7ec6\u80de\u3002\u6dcb\u5df4\u7ec6\u80de\u901a\u5e38\u5177\u6709\n\u5706\u5f62\u6216\u7565\u5448\u4e0d\u89c4\u5219\u5f62\u72b6\u7684\u7ec6\u80de\u6838\u548c\u8f83\u5bbd\u7684\u80de\u6d46\u3002\u56e0\u6b64\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a\n(B) \u6dcb\u5df4\u7ec6\u80de\",\nGround Truth: (D)\u53cd\u5e94\u6027\u6dcb\u5df4\u7ec6\u80de \n<\u56fe\u7247 1>\nError Category:Lack of Knowledge \nError Reason:GPT-4V overlooked or lacked some knowledge. In fact, lymphocytes do indeed \npossess the characteristics mentioned by GPT-4V. However, the cell in the image not only has blue \ncytoplasm but also has a very irregular shape, characteristic of reactive lymphocytes. Similarly, \nreactive lymphocytes also possess the aforementioned characteristics.\nFigure B52: A sample error case of Health and Medicine (subcategory: Diagnostics and Lab\nMedicine).\nBack to List of figures\n71\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66: \u8bca\u65ad\u5b66\u4e0e\u5b9e\u9a8c\u5ba4\u533b\u5b66\nQuestion:\u5224\u65ad\u4e0b\u9762\u9648\u8ff0\u5bf9\u9519\uff1a\u4e0b\u5217\u662fDNA\u7684\u4e00\u6bb5\u78b1\u57fa\u5e8f\u5217<\u56fe\u7247 1>\uff0cDNA\u805a\u5408\u9176\u4ee5\u4e0a\u9762\u7684DNA\u7247\u6bb5\u4e3a\u6a21\n\u677f\uff0c\u590d\u5236\u51fa\u7684DNA\u78b1\u57fa\u5e8f\u5217\u4e3a5'-CTAATGCAACGTTGCAAGCT-3'\u3002\nGPT-4V:\u5728DNA\u590d\u5236\u4e2d\uff0cDNA\u805a\u5408\u9176\u4f1a\u6839\u636e\u6a21\u677f\u94fe\u7684\u78b1\u57fa\u5e8f\u5217\u5408\u6210\u4e00\u4e2a\u4e92\u8865\u7684\u65b0\u94fe\u3002\u5728\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\uff0c\u817a\n\u560c\u5464\uff08A\uff09\u4e0e\u80f8\u817a\u5627\u5576\uff08T\uff09\u914d\u5bf9\uff0c\u800c\u9e1f\u560c\u5464\uff08G\uff09\u4e0e\u80de\u5627\u5576\uff08C\uff09\u914d\u5bf9\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u53ef\u4ee5\u6839\u636e\u6240\u7ed9\u7684\u6a21\n\u677f\u94fe\u5e8f\u5217\u6765\u786e\u5b9a\u590d\u5236\u51fa\u7684\u65b0\u94fe\u7684\u5e8f\u5217\u3002\n\u6240\u7ed9\u7684\u6a21\u677f\u94fe\u5e8f\u5217\u4e3a5'-AGCTTGCAACGTTGCATTAG-3'\u3002\u6839\u636e\u78b1\u57fa\u914d\u5bf9\u539f\u5219\uff0c\u590d\u5236\u51fa\u7684\u65b0\u94fe\u5e94\u8be5\u662f\u6a21\u677f\u94fe\n\u7684\u4e92\u8865\u94fe\uff0c\u65b9\u5411\u76f8\u53cd\uff0c\u5373\u4ece3'\u52305'\u3002\u6240\u4ee5\uff0c\u590d\u5236\u51fa\u7684\u65b0\u94fe\u5e94\u8be5\u662f3'-TCGAACGTTGCAATGTAATC-5'\u3002\u4f46\u662f\uff0c\n\u901a\u5e38\u6211\u4eec\u6309\u71675'\u52303'\u7684\u65b9\u5411\u6765\u5199\u51fa\u5e8f\u5217\uff0c\u56e0\u6b64\uff0c\u6b63\u786e\u7684\u65b0\u94fe\u5e8f\u5217\u5e94\u8be5\u662f5'-GATTAACGTGCAACGTTGAC-3'\u3002\n\u9898\u76ee\u4e2d\u7ed9\u51fa\u7684\u590d\u5236\u51fa\u7684DNA\u78b1\u57fa\u5e8f\u5217\u4e3a5'-CTAATGCAACGTTGCAAGCT-3'\u3002\u4e0e\u6211\u4eec\u6839\u636e\u6a21\u677f\u94fe\u63a8\u5bfc\u51fa\u7684\u5e8f\n\u5217\u76f8\u6bd4\u8f83\uff0c\u53ef\u4ee5\u770b\u51fa\u9898\u76ee\u4e2d\u7ed9\u51fa\u7684\u5e8f\u5217\u4e0e\u6b63\u786e\u7684\u4e92\u8865\u5e8f\u5217\u4e0d\u4e00\u81f4\u3002\n\u56e0\u6b64\uff0c\u6839\u636e\u9898\u76ee\u63cf\u8ff0\u548c\u6240\u7ed9\u7684\u4fe1\u606f\uff0c\u9648\u8ff0\u662f\u9519\u8bef\u7684\u3002\nGround Truth: (\u6b63\u786e) \n<\u56fe\u7247 1>\nError Category:Lack of Knowledge \nError Reason:The text given in the question is actually the reverse complementary sequence of the \nimage DNF, so it's correct that the former is generated by the latter through DNA polymerase. GPT-\n4V lacks knowledge about reverse complementary sequences, leading to an incorrect judgment.\nFigure B53: A sample error case of Health and Medicine (subcategory: Diagnostics and Lab\nMedicine).\nBack to List of figures\n72\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66: \u5236\u836f\nQuestion: \u5982\u56fe<\u56fe\u7247 1>\uff0c\u5c11\u90e8\u5206\u836f\u7269\u5c0f\u5242\u91cf\u65f6\u4ee5\u4e00\u7ea7\u901f\u7387\u8f6c\u8fd0\uff0c\u800c\u5728\u5927\u5242\u91cf\u65f6\u4ee5\u96f6\u7ea7\u901f\u7387\u8f6c\u8fd0\u3002\u56e0\n\u6b64\u63cf\u8ff0\u8fd9\u7c7b\u836f\u7269\u7684\u6d88\u9664\u901f\u7387\u9700\u8981\u5c06\u4e24\u79cd\u901f\u7387\u7c7b\u578b\u7ed3\u5408\u8d77\u6765\uff0c\u901a\u5e38\u4ee5_\u63cf\u8ff0\u3002\nOption: \n(A) \u7f57\ufe63\u96f7\u6c0f\u65b9\u7a0b\u5f0f\n(B) \u5510\ufe63\u66fc\u6c0f\u65b9\u7a0b\u5f0f\n(C) \u7c73\ufe63\u66fc\u6c0f\u65b9\u7a0b\u5f0f\u63cf\u8ff0\n(D) \u5176\u4f59\u9009\u9879\u5747\u4e0d\u5bf9\nGPT-4V:\u6b63\u786e\u7b54\u6848\u662f\uff1a\n(C) \u7c73\ufe63\u66fc\u6c0f\u65b9\u7a0b\u5f0f\u63cf\u8ff0\n\u8fd9\u4e2a\u56fe\u8868\u5c55\u793a\u4e86\u836f\u7269\u6d88\u9664\u901f\u7387\u968f\u836f\u7269\u6d53\u5ea6\u53d8\u5316\u7684\u4e24\u79cd\u4e0d\u540c\u6a21\u5f0f\uff1a\u5c0f\u5242\u91cf\u65f6\u7684\u4e00\u7ea7\u901f\u7387\u8f6c\u8fd0\u548c\u5927\u5242\u91cf\u65f6\n\u7684\u96f6\u7ea7\u901f\u7387\u8f6c\u8fd0\u3002\u7c73-\u66fc\u6c0f\u65b9\u7a0b\u5f0f\uff08Michaelis-Menten equation\uff09\u901a\u5e38\u7528\u4e8e\u63cf\u8ff0\u8fd9\u79cd\u60c5\u51b5\uff0c\u5b83\u7ed3\u5408\u4e86\n\u4e00\u7ea7\u52a8\u529b\u5b66\u548c\u96f6\u7ea7\u52a8\u529b\u5b66\uff0c\u7528\u4e8e\u63cf\u8ff0\u5728\u4e0d\u540c\u6d53\u5ea6\u4e0b\u9176\u4fc3\u53cd\u5e94\u7684\u901f\u7387\uff0c\u8fd9\u5728\u836f\u7269\u52a8\u529b\u5b66\u4e2d\u4e5f\u9002\u7528\u3002\nGround Truth: (C)\u7c73\ufe63\u66fc\u6c0f\u65b9\u7a0b\u5f0f\u63cf\u8ff0 \n<\u56fe\u7247 1>\nFigure B54: A sample correct case of Health and Medicine (subcategory: Pharmacy).\nBack to List of figures\n73\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66: \u5236\u836f\nQuestion: \u5982\u56fe<\u56fe\u7247 1>\uff0c\u4e0b\u5217\u5316\u5408\u7269\u7ecf\u7845\u80f6\u67f1\u5c42\u6790\u5206\u79bb\u65f6\uff0c\u7528\u6c2f\u4eff\u2014\u7532\u9187\u7cfb\u7edf\u4f5c\u6d17\u8131\u5242\uff0c\u5176\u6d17\u8131\u987a\n\u5e8f\uff08\uff09\nOption: \n(A) \u4e19>\u4e01>\u7532>\u4e59\n(B) \u4e59>\u7532>\u4e01>\u4e19\n(C) \u4e01>\u4e19>\u7532>\u4e59\n(D) \u4e59>\u7532>\u4e19>\u4e01\nGPT-4V:\u5728\u7845\u80f6\u67f1\u5c42\u6790\u4e2d\uff0c\u5316\u5408\u7269\u7684\u6d17\u8131\u987a\u5e8f\u901a\u5e38\u53d6\u51b3\u4e8e\u5b83\u4eec\u4e0e\u6d17\u8131\u5242\u7684\u76f8\u4e92\u4f5c\u7528\u5f3a\u5ea6\u3002\u6d17\u8131\u5242\u7684\u6781\n\u6027\u8d8a\u9ad8\uff0c\u4e0e\u5316\u5408\u7269\u7684\u76f8\u4e92\u4f5c\u7528\u4e5f\u8d8a\u5f3a\uff0c\u56e0\u6b64\u6781\u6027\u8f83\u4f4e\u7684\u5316\u5408\u7269\u4f1a\u5148\u6d17\u8131\u51fa\u6765\u3002\n\u5728\u8fd9\u4e2a\u95ee\u9898\u4e2d\uff0c\u6211\u4eec\u9700\u8981\u8003\u8651\u5316\u5408\u7269\u7684\u6781\u6027\u3002\u5316\u5408\u7269\u7684\u6781\u6027\u901a\u5e38\u4e0e\u5b83\u4eec\u7684\u5b98\u80fd\u56e2\u6709\u5173\uff0c\u5982\u7f9f\u57fa\uff08-OH\uff09\n\u548c\u7fa7\u57fa\uff08-COOH\uff09\u662f\u6781\u6027\u5b98\u80fd\u56e2\uff0c\u800c\u7532\u6c27\u57fa\uff08-OCH3\uff09\u548c\u7532\u57fa\uff08-CH3\uff09\u662f\u975e\u6781\u6027\u5b98\u80fd\u56e2\u3002\n\u6839\u636e\u56fe\u4e2d\u7684\u5316\u5408\u7269\u7ed3\u6784\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff1a\n- \u7532\u5316\u5408\u7269\u6709\u4e09\u4e2a\u7f9f\u57fa\u548c\u4e00\u4e2a\u7532\u6c27\u57fa\u3002\n- \u4e59\u5316\u5408\u7269\u6709\u4e00\u4e2a\u7f9f\u57fa\u3001\u4e00\u4e2a\u7fa7\u57fa\u548c\u4e24\u4e2a\u916e\u57fa\u3002\n- \u4e19\u5316\u5408\u7269\u6709\u4e09\u4e2a\u7f9f\u57fa\u548c\u4e00\u4e2a\u7532\u57fa\u3002\n- \u4e01\u5316\u5408\u7269\u6709\u4e00\u4e2a\u7f9f\u57fa\u548c\u4e24\u4e2a\u7532\u57fa\u3002\n\u7fa7\u57fa\uff08-COOH\uff09\u662f\u8fd9\u91cc\u6700\u6781\u6027\u7684\u5b98\u80fd\u56e2\uff0c\u56e0\u6b64\u4e59\u5316\u5408\u7269\u7684\u6781\u6027\u6700\u9ad8\uff0c\u5e94\u8be5\u6700\u540e\u6d17\u8131\u3002\u63a5\u4e0b\u6765\u662f\u7f9f\u57fa\uff08-\nOH\uff09\uff0c\u7532\u5316\u5408\u7269\u548c\u4e19\u5316\u5408\u7269\u90fd\u6709\u4e09\u4e2a\u7f9f\u57fa\uff0c\u4f46\u7532\u5316\u5408\u7269\u8fd8\u6709\u4e00\u4e2a\u7532\u6c27\u57fa\uff0c\u8fd9\u4f7f\u5f97\u7532\u5316\u5408\u7269\u7684\u6781\u6027\u7565\u4f4e\n\u4e8e\u4e19\u5316\u5408\u7269\u3002\u6700\u540e\uff0c\u4e01\u5316\u5408\u7269\u6709\u6700\u5c11\u7684\u7f9f\u57fa\u548c\u6700\u591a\u7684\u7532\u57fa\uff0c\u4f7f\u5176\u6781\u6027\u6700\u4f4e\uff0c\u5e94\u8be5\u6700\u5148\u6d17\u8131\u3002\n\u56e0\u6b64\uff0c\u6d17\u8131\u987a\u5e8f\u5e94\u8be5\u662f\uff1a\n\u4e01\uff08\u6700\u975e\u6781\u6027\uff0c\u5148\u6d17\u8131\uff09> \u7532 > \u4e19 > \u4e59\uff08\u6700\u6781\u6027\uff0c\u6700\u540e\u6d17\u8131\uff09\n\u6b63\u786e\u7b54\u6848\u662f (C) \u4e01>\u4e19>\u7532>\u4e59\u3002\nGround Truth: (C)\u4e01>\u4e19>\u7532>\u4e59 \n<\u56fe\u7247 1>\nFigure B55: A sample correct case of Health and Medicine (subcategory: Pharmacy).\nBack to List of figures\n74\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66: \u5236\u836f\nQuestion: \u7ed9\u67d0\u60a3\u8005\u9759\u8109\u6ce8\u5c04\u4e00\u5355\u5ba4\u6a21\u578b\u836f\u7269,\u5242\u91cf\u4e3a100.0mg,\u6d4b\u5f97\u4e0d\u540c\u65f6\u523b\u8840\u836f\u6d53\u5ea6\u7684\u6570\u636e\u5982\u4e0b\u8868<\n\u56fe\u7247 1>\u6240\u793a\u3002\u8be5\u836f\u7269\u7684\u6d88\u9664\u901f\u7387\u5e38\u6570(\u5355\u4f4d$h^{-1}$)\u662f__\nGPT-4V:\u4e3a\u4e86\u8ba1\u7b97\u6d88\u9664\u901f\u7387\u5e38\u6570\uff08k\uff09\uff0c\u6211\u4eec\u901a\u5e38\u4f7f\u7528\u4e00\u9636\u52a8\u529b\u5b66\u65b9\u7a0b\uff0c\u5176\u4e2d\u8840\u836f\u6d53\u5ea6\u968f\u65f6\u95f4\u7684\u4e0b\u964d\n\u53ef\u4ee5\u8868\u793a\u4e3a\uff1a\nC = \ud835\udc36\ud835\udc360* \ud835\udc52\ud835\udc52\u2212\ud835\udc58\ud835\udc58\ud835\udc58\ud835\udc58\n\u5176\u4e2d C \u662f\u5728\u65f6\u95f4 t \u65f6\u7684\u8840\u836f\u6d53\u5ea6\uff0c\ud835\udc36\ud835\udc360\u662f\u521d\u59cb\u8840\u836f\u6d53\u5ea6\uff0ck\u662f\u6d88\u9664\u901f\u7387\u5e38\u6570\uff0ct\u662f\u65f6\u95f4\u3002\n\u4ece\u6240\u63d0\u4f9b\u7684\u6570\u636e\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u53d6\u4e24\u4e2a\u65f6\u95f4\u70b9\u7684\u6d53\u5ea6\u503c\u6765\u4f30\u7b97\u6d88\u9664\u901f\u7387\u5e38\u6570k\uff0c\u6211\u4eec\u901a\u5e38\u4f1a\u53d6\u5bf9\u6570\n\u503c\u6765\u5c06\u6307\u6570\u65b9\u7a0b\u8f6c\u6362\u4e3a\u7ebf\u6027\u65b9\u7a0b\uff0c\u8fd9\u6837\u53ef\u4ee5\u901a\u8fc7\u7ebf\u6027\u56de\u5f52\u6765\u4f30\u7b97 k\u3002\nln(C) = ln(\ud835\udc36\ud835\udc360) - kt\n\u6211\u4eec\u53ef\u4ee5\u9009\u62e9\u4efb\u610f\u4e24\u4e2a\u65f6\u95f4\u70b9\u7684\u6570\u636e\u6765\u8ba1\u7b97 k\u3002\u4f8b\u5982\uff0c\u6211\u4eec\u53ef\u4ee5\u9009\u62e9 t=1h \u65f6\u7684\u6d53\u5ea6\u548c t=6h \u65f6\u7684\u6d53\n\u5ea6\u3002\u9996\u5148\uff0c\u6211\u4eec\u9700\u8981\u8ba1\u7b97\u6bcf\u4e2a\u65f6\u95f4\u70b9\u7684 ln(C)\u3002\n\u5bf9\u4e8e t=1h\uff0cC=8.40 \u03bcg/mL\uff0cln(8.40) = 2.128\n\u5bf9\u4e8e t=6h\uff0cC=1.48 \u03bcg/mL\uff0cln(1.48) = 0.392\n\u73b0\u5728\u6211\u4eec\u6709\u4e24\u4e2a\u70b9 (1, 2.128) \u548c (6, 0.392)\u3002\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u8fd9\u4e24\u4e2a\u70b9\u6765\u8ba1\u7b97\u659c\u7387\uff08\u5373\u6d88\u9664\u901f\u7387\u5e38\n\u6570 k\uff09\u3002\nk = (ln(\ud835\udc36\ud835\udc360) - ln(\ud835\udc36\ud835\udc36\ud835\udc58\ud835\udc58)) / t\nk = (2.128 - 0.392) / (6 - 1)\nk = 1.736 / 5\nk = 0.3472 \u210e\u22121\n\u56e0\u6b64\uff0c\u8be5\u836f\u7269\u7684\u6d88\u9664\u901f\u7387\u5e38\u6570\u5927\u7ea6\u662f 0.3472 \u210e\u22121\u3002\",\nGround Truth: (0.3465) \n<\u56fe\u7247 1>\nFigure B56: A sample correct case of Health and Medicine (subcategory: Pharmacy).\nBack to List of figures\n75\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66: \u5236\u836f\nQuestion:\u4e0b\u56fe<\u56fe\u7247 1>\u836f\u7269\u4e2d\uff0c\u5206\u5b50\u4e2d\u542b\u6709\u6c28\u7532\u9178\u4e59\u916f\u7ed3\u6784\u7684\u836f\u7269\u662f__\nOption: \n(A) \u73ed\u5e03\u7279\u7f57\n(B) \u5730\u5339\u798f\u6797\n(C) \u7279\u975e\u90a3\u5b9a\n(D) \u6c2f\u96f7\u4ed6\u5b9a\nGPT-4V:\u6c28\u7532\u9178\u4e59\u916f\u7ed3\u6784\u662f\u4e00\u4e2a\u542b\u6709\u4ee5\u4e0b\u5b98\u80fd\u56e2\u7684\u7ed3\u6784\uff1a-O-C(=O)-NHR\uff0c\u5176\u4e2dR\u53ef\u4ee5\u662f\u4efb\u4f55\u70f7\u57fa\u6216\u82b3\n\u9999\u57fa\u3002\n\u5728\u63d0\u4f9b\u7684\u9009\u9879\u4e2d\uff0c\u6211\u4eec\u9700\u8981\u627e\u5230\u5305\u542b\u8fd9\u4e2a\u5b98\u80fd\u56e2\u7684\u5206\u5b50\u3002\u901a\u8fc7\u89c2\u5bdf\u56fe\u4e2d\u7684\u5206\u5b50\u7ed3\u6784\uff1a\n- (A) \u73ed\u5e03\u7279\u7f57\uff1a\u6ca1\u6709\u6c28\u7532\u9178\u4e59\u916f\u7ed3\u6784\u3002\n- (B) \u5730\u5339\u798f\u6797\uff1a\u6ca1\u6709\u6c28\u7532\u9178\u4e59\u916f\u7ed3\u6784\u3002\n- (C) \u7279\u975e\u90a3\u5b9a\uff1a\u5206\u5b50\u7ed3\u6784\u4e2d\u542b\u6709\u6c28\u7532\u9178\u4e59\u916f\u7ed3\u6784\u3002\n- (D) \u6c2f\u96f7\u4ed6\u5b9a\uff1a\u6ca1\u6709\u6c28\u7532\u9178\u4e59\u916f\u7ed3\u6784\u3002\n\u56e0\u6b64\uff0c\u6b63\u786e\u7b54\u6848\u662f (C) \u7279\u975e\u90a3\u5b9a\u3002\nGround Truth: (D)\u6c2f\u96f7\u4ed6\u5b9a \n<\u56fe\u7247 1>\nError Category:Perceptual Error\nError Reason:The molecule in option C contains an amino (-NH-) group and an ether (-O-) \nstructure. These two functional groups might be confused with components of ethyl carbamate. \nWhile these two parts contain nitrogen and oxygen atoms, respectively, they do not form an ethyl \ncarbamate structure. Moreover, the compound in option C includes an ether oxygen (-O-) situated \nbetween a phenyl ring and an ethoxy group, which led to the incorrect selection by GPT-4\nFigure B57: A sample error case of Health and Medicine (subcategory: Pharmacy).\nBack to List of figures\n76\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66: \u5236\u836f\nQuestion:\u5236\u836f\u53cd\u5e94\u8bbe\u5907\u4e2d\u7684\u53cd\u5e94\u5668\u7c7b\u578b\u5f88\u591a\uff0c\u7279\u70b9\u4e0d\u4e00\uff0c\u53ef\u6309\u4e0d\u540c\u65b9\u5f0f\u8fdb\u884c\u5206\u7c7b\u3002\u82e5\u6309\u7ed3\u6784\u5206\u7c7b\uff0c\n\u5982<\u56fe\u7247 1>\u6240\u793a\u4e3a\uff08\uff09\u53cd\u5e94\u5668\u3002\nOption: \n(A) \u677f\u5f0f\u5854\n(B) \u586b\u6599\u5854\n(C) \u9f13\u6ce1\u5854\n(D) \u55b7\u96fe\u5854\nGPT-4V:\u8fd9\u5f20\u56fe\u7247\u5c55\u793a\u7684\u662f\u4e00\u4e2a\u5854\u5f0f\u53cd\u5e94\u5668\u7684\u7b80\u5316\u793a\u610f\u56fe\u3002\u6839\u636e\u56fe\u4e2d\u663e\u793a\u7684\u7279\u5f81\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u5854\u4f53\n\u5185\u90e8\u4f3c\u4e4e\u586b\u5145\u4e86\u5927\u91cf\u7684\u5c0f\u9897\u7c92\uff0c\u8fd9\u901a\u5e38\u662f\u586b\u6599\u5854\u7684\u7279\u70b9\u3002\u586b\u6599\u5854\u5185\u90e8\u586b\u5145\u4e86\u586b\u6599\uff0c\u4ee5\u589e\u52a0\u6c14\u6db2\u4e4b\u95f4\u7684\n\u63a5\u89e6\u9762\u79ef\uff0c\u4ece\u800c\u63d0\u9ad8\u4f20\u8d28\u6548\u7387\u3002\u56e0\u6b64\uff0c\u6700\u53ef\u80fd\u6b63\u786e\u7684\u9009\u9879\u662f\uff1a\n(B) \u586b\u6599\u5854\nGround Truth: (D)\u55b7\u96fe\u5854 \n<\u56fe\u7247 1>\nError Category:Perceptual Error\nError Reason:GPT-4V incorrectly interpreted the image, mistaking the edge outline of the water \nmist from the nozzle as the body of a tower filled with material. This led to the mistaken \nidentification of option (D), the spray tower, as option (B), the packed tower.\nFigure B58: A sample error case of Health and Medicine (subcategory: Pharmacy).\nBack to List of figures\n77\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66: \u5236\u836f\nQuestion:\u5939\u5957\u662f\u5236\u836f\u7684\u6405\u62cc\u91dc\u5f0f\u53cd\u5e94\u5668\u7684\u4f20\u70ed\u5143\u4ef6\uff0c\u5982<\u56fe\u7247 1>\u6240\u793a\u4e3a\uff08\uff09\u3002\nOption: \n(A) \u73af\u5f62\u5939\u5957\n(B) \u87ba\u65cb\u6321\u677f\u5939\u5957\n(C) \u534a\u7ba1\u87ba\u65cb\u5939\u5957\n(D) \u5e26\u6270\u6d41\u55b7\u5634\u7684\u73af\u5f62\u5939\u5957\nGPT-4V:\u8fd9\u5f20\u56fe\u7247\u663e\u793a\u7684\u662f\u4e00\u4e2a\u5939\u5957\u7684\u7b80\u5316\u56fe\u3002\u6839\u636e\u56fe\u4e2d\u6240\u793a\u7684\u7ed3\u6784\uff0c\u8fd9\u4e2a\u5939\u5957\u662f\u4e00\u4e2a\u73af\u7ed5\u5728\u53cd\u5e94\u5668\n\u5916\u90e8\u7684\u901a\u9053\uff0c\u7528\u4e8e\u6d41\u4f53\u4ecb\u8d28\u7684\u6d41\u52a8\uff0c\u4ee5\u5b9e\u73b0\u4f20\u70ed\u7684\u76ee\u7684\u3002\u56fe\u4e2d\u5e76\u6ca1\u6709\u663e\u793a\u51fa\u87ba\u65cb\u6321\u677f\u6216\u8005\u534a\u7ba1\u87ba\u65cb\u7684\n\u7279\u5f81\uff0c\u4e5f\u6ca1\u6709\u5e26\u6270\u6d41\u55b7\u5634\u7684\u7279\u5f81\u3002\u56e0\u6b64\uff0c\u6700\u53ef\u80fd\u6b63\u786e\u7684\u9009\u9879\u662f\uff1a\n(A) \u73af\u5f62\u5939\u5957\n\u6b63\u786e\u7b54\u6848\u662f (A) \u73af\u5f62\u5939\u5957\u3002\nGround Truth: (D)\u5e26\u6270\u6d41\u55b7\u5634\u7684\u73af\u5f62\u5939\u5957 \n<\u56fe\u7247 1>\nError Category:Perceptual Error\nError Reason:The image clearly shows a turbulence nozzle, but GPT-4V might not have recognized \nit. Due to insufficient ability to read images or weaker recognition capabilities, it failed to correctly \nobtain all the true information from the image, which further led to the wrong selection.\nFigure B59: A sample error case of Health and Medicine (subcategory: Pharmacy).\nBack to List of figures\n78\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66: \u5236\u836f\nQuestion:\u4e0b\u5217\u5316\u5408\u7269\uff0c\u4f55\u8005\u9178\u6027\u6700\u5f3a\u7684\u662f\uff08\uff09\nOption: \n(A) <\u56fe\u7247 1>\n(B) <\u56fe\u7247 2>\n(C) <\u56fe\u7247 3>\n(D) <\u56fe\u7247 4>\nGPT-4V:\u6b63\u786e\u7b54\u6848\u662f (A) \u73af\u5f62\u5939\u5957\u3002\u5728\u8fd9\u4e2a\u95ee\u9898\u4e2d\uff0c\u6211\u4eec\u9700\u8981\u6bd4\u8f83\u56db\u4e2a\u5316\u5408\u7269\u7684\u9178\u6027\u5f3a\u5ea6\u3002\u9178\u6027\u5f3a\n\u5ea6\u53ef\u4ee5\u901a\u8fc7\u7f9f\u57fa\uff08-OH\uff09\u7684\u80fd\u529b\u6765\u91ca\u653e\u8d28\u5b50\uff08H+\uff09\u6765\u8bc4\u4f30\u3002\u901a\u5e38\uff0c\u5f53\u7f9f\u57fa\u8fde\u63a5\u7684\u82b3\u9999\u73af\u4e0a\u7684\u53d6\u4ee3\u57fa\u80fd\n\u591f\u7a33\u5b9a\u8d1f\u7535\u8377\u65f6\uff0c\u9178\u6027\u4f1a\u589e\u5f3a\u3002\u8fd9\u662f\u56e0\u4e3a\u53d6\u4ee3\u57fa\u53ef\u4ee5\u901a\u8fc7\u5171\u8f6d\u6548\u5e94\u6216\u8bf1\u5bfc\u6548\u5e94\u6765\u7a33\u5b9a\u8131\u8d28\u5b50\u540e\u7684\u8d1f\u7535\n\u8377\u3002\n(A) \u5316\u5408\u7269\u5177\u6709\u4e00\u4e2a\u7532\u6c27\u57fa\uff08-OCH3\uff09\u548c\u4e00\u4e2a\u7f9f\u57fa\uff08-OH\uff09\u5728\u82b3\u9999\u73af\u4e0a\uff0c\u7532\u6c27\u57fa\u662f\u5438\u7535\u5b50\u57fa\u56e2\uff0c\u4f1a\u51cf\u5f31\n\u9178\u6027\u3002\n(B) \u5316\u5408\u7269\u5177\u6709\u4e24\u4e2a\u7532\u6c27\u57fa\u548c\u4e00\u4e2a\u7f9f\u57fa\uff0c\u7532\u6c27\u57fa\u7684\u6570\u91cf\u66f4\u591a\uff0c\u56e0\u6b64\u4f1a\u8fdb\u4e00\u6b65\u51cf\u5f31\u9178\u6027\u3002\n(C) \u5316\u5408\u7269\u5177\u6709\u4e00\u4e2a\u7f9f\u57fa\u548c\u4e00\u4e2a\u7532\u6c27\u57fa\uff0c\u4f46\u7f9f\u57fa\u4f4d\u4e8e\u4e0e\u7532\u6c27\u57fa\u4e0d\u540c\u7684\u73af\u4e0a\uff0c\u8fd9\u53ef\u80fd\u4f1a\u5f71\u54cd\u5176\u9178\u6027\u3002\n(D) \u5316\u5408\u7269\u5177\u6709\u4e24\u4e2a\u7f9f\u57fa\u548c\u4e00\u4e2a\u7532\u6c27\u57fa\uff0c\u7f9f\u57fa\u7684\u6570\u91cf\u66f4\u591a\uff0c\u8fd9\u53ef\u80fd\u4f1a\u589e\u5f3a\u9178\u6027\u3002\n\u5728\u8fd9\u4e9b\u9009\u9879\u4e2d\uff0c\u7f9f\u57fa\u7684\u6570\u91cf\u548c\u4f4d\u7f6e\u4ee5\u53ca\u7532\u6c27\u57fa\u7684\u6548\u5e94\u90fd\u4f1a\u5f71\u54cd\u9178\u6027\u3002\u7f9f\u57fa\u662f\u63d0\u4f9b\u9178\u6027\u7684\u57fa\u56e2\uff0c\u800c\u7532\u6c27\n\u57fa\u662f\u5438\u7535\u5b50\u57fa\u56e2\uff0c\u4f1a\u51cf\u5f31\u9178\u6027\u3002\u56e0\u6b64\uff0c\u5177\u6709\u66f4\u591a\u7f9f\u57fa\u548c\u8f83\u5c11\u7532\u6c27\u57fa\u7684\u5316\u5408\u7269\u901a\u5e38\u9178\u6027\u66f4\u5f3a\u3002\\n\\n\u6839\u636e\n\u8fd9\u4e9b\u4fe1\u606f\uff0c(D) \u9009\u9879\u4e2d\u7684\u5316\u5408\u7269\u5177\u6709\u4e24\u4e2a\u7f9f\u57fa\u548c\u4e00\u4e2a\u7532\u6c27\u57fa\uff0c\u8fd9\u4f7f\u5f97\u5b83\u6bd4\u5176\u4ed6\u9009\u9879\u4e2d\u7684\u5316\u5408\u7269\u5177\u6709\u66f4\n\u5f3a\u7684\u9178\u6027\u3002\u56e0\u6b64\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a\n\u6b63\u786e\u7b54\u6848\uff1a(D)\nGround Truth: (C)<\u56fe\u7247 3> \n<\u56fe\u7247 1>\nError Category:Perceptual Error\nError Reason:Obviously, GPT-4V incorrectly counted the number of hydroxyl groups in the figure. \nIn reality, option C has the same number of hydroxyl groups as D, and there are two methoxy \ngroups on the same ring as the hydroxyl groups, which would greatly enhance the acidity of the \ncompound. Therefore, this is a case of misreading the diagram.\n<\u56fe\u7247 2>\n<\u56fe\u7247 3>\n<\u56fe\u7247 4>\nFigure B60: A sample error case of Health and Medicine (subcategory: Pharmacy).\nBack to List of figures\n79\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66: \u5236\u836f\nQuestion:\u4e0b\u56fe<\u56fe\u7247 1>\u4e3a\u836f\u7269\u4e0e\u53d7\u4f53\u7684\u4eb2\u548c\u529b\u53ca\u5185\u5728\u6d3b\u6027\u5bf9\u91cf-\u6548\u66f2\u7ebf\u7684\u5f71\u54cd\u56fe,\u4e0b\u5217\u63cf\u8ff0\u6b63\u786e\u7684\u662f()\nOption: \n(A) a\u3001b\u3001c\u4e09\u836f\u548c\u53d7\u4f53\u7684\u4eb2\u548c\u529b\u76f8\u7b49\n(B) a\u3001b\u3001c\u4e09\u836f\u7684\u5185\u5728\u6d3b\u6027\u76f8\u7b49\n(C) a\u836f\u7684\u4eb2\u548c\u529b\u6700\u5927\n(D) c\u836f\u7684\u5185\u5728\u6d3b\u6027\u6700\u5927\nGPT-4V:\u6839\u636e\u6240\u63d0\u4f9b\u7684\u91cf-\u6548\u66f2\u7ebf\u56fe\uff0c\u6211\u4eec\u53ef\u4ee5\u5bf9\u836f\u7269\u4e0e\u53d7\u4f53\u7684\u4eb2\u548c\u529b\u53ca\u5185\u5728\u6d3b\u6027\u8fdb\u884c\u5206\u6790\uff1a\n(A) a\u3001b\u3001c\u4e09\u836f\u548c\u53d7\u4f53\u7684\u4eb2\u548c\u529b\u76f8\u7b49 - \u8fd9\u4e2a\u9009\u9879\u4e0d\u6b63\u786e\u3002\u5728\u91cf-\u6548\u66f2\u7ebf\u4e2d\uff0c\u836f\u7269\u4e0e\u53d7\u4f53\u7684\u4eb2\u548c\u529b\u901a\n\u5e38\u7531EC50\uff08\u6216\u6b64\u56fe\u4e2d\u7684pD2\uff09\u8868\u793a\uff0c\u5373\u4ea7\u751f50%\u6700\u5927\u6548\u5e94\u6240\u9700\u7684\u836f\u7269\u6d53\u5ea6\u3002\u66f2\u7ebf\u8d8a\u5411\u5de6\uff0c\u4eb2\u548c\u529b\u8d8a\u9ad8\u3002\n\u4ece\u56fe\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0c\u4e09\u6761\u66f2\u7ebf\u7684EC50\uff08pD2\uff09\u4e0d\u540c\uff0c\u56e0\u6b64\u4eb2\u548c\u529b\u4e0d\u76f8\u7b49\u3002\n(B) a\u3001b\u3001c\u4e09\u836f\u7684\u5185\u5728\u6d3b\u6027\u76f8\u7b49 - \u8fd9\u4e2a\u9009\u9879\u4e0d\u6b63\u786e\u3002\u5185\u5728\u6d3b\u6027\u662f\u6307\u836f\u7269\u8fbe\u5230\u7684\u6700\u5927\u6548\u5e94\uff0c\u5373E(%)\u7684\n\u6700\u5927\u503c\u3002\u4ece\u56fe\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0c\u836f\u7269a\u3001b\u3001c\u8fbe\u5230\u7684\u6700\u5927\u6548\u5e94\u4e0d\u540c\uff0c\u56e0\u6b64\u5185\u5728\u6d3b\u6027\u4e0d\u76f8\u7b49\u3002\n(C) a\u836f\u7684\u4eb2\u548c\u529b\u6700\u5927 - \u8fd9\u4e2a\u9009\u9879\u4e0d\u6b63\u786e\u3002\u4ece\u56fe\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0c\u836f\u7269c\u7684\u66f2\u7ebf\u6700\u9760\u5de6\uff0c\u8868\u660e\u5176EC50\uff08pD2\uff09\n\u6700\u5c0f\uff0c\u56e0\u6b64c\u836f\u7684\u4eb2\u548c\u529b\u6700\u5927\u3002\n(D) c\u836f\u7684\u5185\u5728\u6d3b\u6027\u6700\u5927 - \u8fd9\u4e2a\u9009\u9879\u6b63\u786e\u3002\u4ece\u56fe\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0c\u836f\u7269c\u7684\u66f2\u7ebf\u8fbe\u5230\u4e86\u6700\u9ad8\u7684E(%)\u503c\uff0c\u5373\n100%\uff0c\u8868\u660e\u5176\u5185\u5728\u6d3b\u6027\u6700\u5927\u3002\n\u56e0\u6b64\uff0c\u6b63\u786e\u7b54\u6848\u662f (D)c\u836f\u7684\u5185\u5728\u6d3b\u6027\u6700\u5927\u3002\nGround Truth: (A)a\u3001b\u3001c\u4e09\u836f\u548c\u53d7\u4f53\u7684\u4eb2\u548c\u529b\u76f8\u7b49 \n<\u56fe\u7247 1>\nError Category:Perceptual Error\nError Reason:GPT-4V needs to improve its understanding of the affinity and intrinsic activity of \ndrugs with receptors. KD represents the affinity of a drug for a receptor, and its value is equal to \nEC50. The negative logarithm of KD (-lgKD) is the affinity index (pD2), which is directly \nproportional to the affinity. The effect of a drug binding to a receptor depends not only on its \naffinity but also on its intrinsic activity. When two drugs have equal affinity, their effect depends on \ntheir intrinsic activity. When two drugs have equal intrinsic activity, the effect depends on their \naffinity. From the diagram, it can be seen that the EC50 of drugs a, b, and c are equal, meaning their \naffinity indices (pD2) are equal, but their maximum effects are a > b > c, indicating different \nintrinsic activities.\nFigure B61: A sample error case of Health and Medicine (subcategory: Pharmacy).\nBack to List of figures\n80\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66: \u5236\u836f\nQuestion:\u5982\u56fe<\u56fe\u7247 1>\uff0c\u6b64\u751f\u7269\u78b1\u7ed3\u6784\u5c5e\u4e8e\uff08\uff09\nOption: \n(A) \u5432\u54da\u7c7b\n(B) \u83a8\u83ea\u70f7\u7c7b\n(C) \u55b9\u5549\u7c7b\n(D) \u841c\u7c7b\nGPT-4V:\u8fd9\u4e2a\u7ed3\u6784\u662f\u4e00\u4e2a\u5432\u54da\u7c7b\u751f\u7269\u78b1\u7684\u7ed3\u6784\u3002\u5432\u54da\u7c7b\u751f\u7269\u78b1\u7684\u7279\u5f81\u662f\u542b\u6709\u4e00\u4e2a\u5432\u54da\u73af\uff0c\u5373\u4e00\u4e2a\u516d\u5143\n\u7684\u82ef\u73af\u4e0e\u4e00\u4e2a\u4e94\u5143\u542b\u6c2e\u6742\u73af\u76f8\u878d\u5408\u3002\u5728\u60a8\u63d0\u4f9b\u7684\u7ed3\u6784\u4e2d\uff0c\u5de6\u4fa7\u7684\u90e8\u5206\u662f\u4e00\u4e2a\u5178\u578b\u7684\u5432\u54da\u7ed3\u6784\u3002\u56e0\u6b64\uff0c\n\u6b63\u786e\u7b54\u6848\u662f\uff1a\n(A) \u5432\u54da\u7c7b\nGround Truth: (B)\u83a8\u83ea\u70f7\u7c7b \n<\u56fe\u7247 1>\nError Category:Lack of Knowledge\nError Reason:.GPT-4V believed that the left part of the image was a typical indole structure, but in \nreality, it is only structurally similar. The left part of the image has an additional nitrogen atom and \nis not a benzene ring, indicating that GPT-4V did not precisely define the indole structure. \nTherefore, it incorrectly chose option A. Option B, on the other hand, consists of a tropane structure \nformed by a seven-membered ring, a six-membered ring, and a five-membered ring sharing one \ncarbon atom and two nitrogen atoms. Hence, option B is the correct choice.\nFigure B62: A sample error case of Health and Medicine (subcategory: Pharmacy).\nBack to List of figures\n81\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66:\u516c\u5171\u536b\u751f\nQuestion:\u7532\u3001\u4e59\u4e24\u4eba\u7fa4\u4e2d\u51e0\u79cd\u7279\u6b8a\u90e8\u4f4d\u764c\u80bf\u62a5\u544a\u65b0\u75c5\u4f8b\u7684\u76f8\u5bf9\u6570<\u56fe\u7247 1>\uff1a\u636e\u6b64\u63a8\u8bba\u7532\u4eba\u7fa4\u8f83\u4e59\u4eba\n\u7fa4\u66f4\u6613\u60a3\u80ba\u764c\u3001\u4e73\u817a\u764c\u3001\u5b50\u5bab\u9888\u764c\uff0c\u8be5\u63a8\u8bba\uff08\uff09\nOption: \n(A) \u4e0d\u6b63\u786e\uff0c\u56e0\u4e3a\u672a\u533a\u5206\u53d1\u75c5\u7387\u6216\u6b7b\u4ea1\u7387\n(B) \u4e0d\u6b63\u786e\uff0c\u56e0\u4e3a\u672a\u7528\u7387\u6307\u6807\u6d4b\u91cf\n(C) \u4e0d\u6b63\u786e\uff0c\u56e0\u4e3a\u672a\u8bbe\u7f6e\u5bf9\u7167\u7ec4\n(D) \u4e0d\u6b63\u786e\uff0c\u56e0\u4e3a\u672a\u8fdb\u884c\u7387\u7684\u6807\u5316\nGPT-4V: \u5f88\u62b1\u6b49\uff0c\u7531\u4e8e\u56fe\u7247\u4e2d\u7684\u6570\u636e\u4e0d\u8db3\u4ee5\u63d0\u4f9b\u5b8c\u6574\u7684\u4fe1\u606f\u6765\u786e\u5b9a\u7532\u4eba\u7fa4\u662f\u5426\u6bd4\u4e59\u4eba\u7fa4\u66f4\u6613\u60a3\u80ba\u764c\u3001\n\u4e73\u817a\u764c\u3001\u5b50\u5bab\u9888\u764c\uff0c\u56e0\u6b64\u65e0\u6cd5\u76f4\u63a5\u5f97\u51fa\u7ed3\u8bba\u3002\u4f46\u662f\uff0c\u6839\u636e\u6240\u63d0\u4f9b\u7684\u9009\u9879\uff0c\u6211\u4eec\u53ef\u4ee5\u5206\u6790\u54ea\u4e2a\u9009\u9879\u6700\u53ef\n\u80fd\u662f\u6b63\u786e\u7684\u3002 \u9009\u9879(A)\u63d0\u5230\u4e86\u672a\u533a\u5206\u53d1\u75c5\u7387\u6216\u6b7b\u4ea1\u7387\uff0c\u8fd9\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u8003\u8651\u56e0\u7d20\uff0c\u56e0\u4e3a\u4ec5\u6709\u7684\u65b0\u75c5\u4f8b\n\u6570\u5e76\u4e0d\u80fd\u76f4\u63a5\u53cd\u6620\u4eba\u7fa4\u7684\u98ce\u9669\uff0c\u9700\u8981\u77e5\u9053\u8fd9\u4e9b\u65b0\u75c5\u4f8b\u662f\u5728\u591a\u5927\u7684\u4eba\u7fa4\u4e2d\u53d1\u751f\u7684\u3002 \u9009\u9879(B)\u6307\u51fa\u4e86\u672a\u7528\n\u7387\u6307\u6807\u6d4b\u91cf\uff0c\u8fd9\u540c\u6837\u662f\u4e00\u4e2a\u5173\u952e\u70b9\uff0c\u56e0\u4e3a\u6ca1\u6709\u8003\u8651\u5230\u4eba\u7fa4\u7684\u5927\u5c0f\uff0c\u5355\u7eaf\u7684\u65b0\u75c5\u4f8b\u6570\u4e0d\u80fd\u53cd\u6620\u771f\u5b9e\u7684\u53d1\n\u75c5\u98ce\u9669\u3002 \u9009\u9879(C)\u63d0\u5230\u4e86\u672a\u8bbe\u7f6e\u5bf9\u7167\u7ec4\uff0c\u8fd9\u5bf9\u4e8e\u6bd4\u8f83\u7814\u7a76\u6765\u8bf4\u662f\u91cd\u8981\u7684\uff0c\u4f46\u5728\u8fd9\u4e2a\u60c5\u51b5\u4e0b\uff0c\u7532\u548c\u4e59\u4e24\n\u4e2a\u4eba\u7fa4\u672c\u8eab\u5c31\u662f\u76f8\u4e92\u6bd4\u8f83\u7684\uff0c\u6240\u4ee5\u8fd9\u4e2a\u9009\u9879\u53ef\u80fd\u4e0d\u662f\u6700\u5408\u9002\u7684\u3002 \u9009\u9879(D)\u63d0\u5230\u4e86\u672a\u8fdb\u884c\u7387\u7684\u6807\u5316\uff0c\u8fd9\n\u662f\u4e00\u4e2a\u5173\u952e\u7684\u7edf\u8ba1\u8fc7\u7a0b\uff0c\u7528\u4e8e\u8c03\u6574\u4e0d\u540c\u4eba\u7fa4\u4e2d\u7684\u5e74\u9f84\u3001\u6027\u522b\u7b49\u7ed3\u6784\u5dee\u5f02\uff0c\u4ee5\u4fbf\u66f4\u51c6\u786e\u5730\u6bd4\u8f83\u53d1\u75c5\u7387\u3002\n\u6839\u636e\u8fd9\u4e9b\u4fe1\u606f\uff0c\u6700\u53ef\u80fd\u6b63\u786e\u7684\u9009\u9879\u662f(B)\u4e0d\u6b63\u786e\uff0c\u56e0\u4e3a\u672a\u7528\u7387\u6307\u6807\u6d4b\u91cf\u3002\u8fd9\u662f\u56e0\u4e3a\u6ca1\u6709\u63d0\u4f9b\u8db3\u591f\u7684\u4fe1\n\u606f\u6765\u8ba1\u7b97\u6807\u51c6\u5316\u7684\u53d1\u75c5\u7387\u6216\u6b7b\u4ea1\u7387\uff0c\u8fd9\u662f\u6bd4\u8f83\u4e0d\u540c\u4eba\u7fa4\u98ce\u9669\u7684\u5173\u952e\u3002\nGround Truth: (B)\u4e0d\u6b63\u786e\uff0c\u56e0\u4e3a\u672a\u7528\u7387\u6307\u6807\u6d4b\u91cf\n<\u56fe\u7247 1>\nFigure B63: A sample correct case of Health and Medicine (subcategory: Public Health).\nBack to List of figures\n82\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66:\u516c\u5171\u536b\u751f\nQuestion:\u8bc6\u522b<\u56fe\u7247 1>\u4e3a\uff08\uff09\u7684\u6210\u50cf\nOption: \n(A) \u94a9\u866b\u5375\n(B) \u5e26\u7ee6\u866b\u5375\n(C) \u8840\u5438\u866b\u5375\n(D) \u53d7\u7cbe\u86d4\u866b\u5375\nGPT-4V: \u8fd9\u5f20\u56fe\u7247\u663e\u793a\u7684\u662f\u5728\u663e\u5fae\u955c\u4e0b\u89c2\u5bdf\u5230\u7684\u4e00\u4e2a\u5375\u3002\u6839\u636e\u5375\u7684\u5f62\u72b6\u3001\u5927\u5c0f\u548c\u53ef\u80fd\u7684\u7279\u5f81\uff0c\u6211\u4eec\u53ef\n\u4ee5\u5c1d\u8bd5\u8bc6\u522b\u5b83\u3002\u7136\u800c\uff0c\u7531\u4e8e\u6211\u4e0d\u662f\u533b\u5b66\u4e13\u5bb6\uff0c\u6211\u4e0d\u80fd\u63d0\u4f9b\u4e00\u4e2a\u786e\u5207\u7684\u8bca\u65ad\u3002\u901a\u5e38\uff0c\u94a9\u866b\u5375\u8f83\u5c0f\u4e14\u5448\u692d\n\u5706\u5f62\uff0c\u5e26\u7ee6\u866b\u5375\u5177\u6709\u975e\u5e38\u7279\u6b8a\u7684\u5f62\u72b6\u548c\u5185\u90e8\u7ed3\u6784\uff0c\u8840\u5438\u866b\u5375\u901a\u5e38\u6709\u4e00\u4e2a\u7279\u5f81\u6027\u7684\u5c16\u7aef\u6216\u7a81\u8d77\uff0c\u800c\u53d7\u7cbe\n\u86d4\u866b\u5375\u5219\u5448\u5706\u5f62\u6216\u692d\u5706\u5f62\uff0c\u8868\u9762\u53ef\u80fd\u6709\u4e0d\u89c4\u5219\u7684\u5916\u58f3\u3002 \u6839\u636e\u8fd9\u5f20\u56fe\u7247\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u5375\u662f\u5706\u5f62\u7684\uff0c\n\u5e76\u4e14\u6709\u4e00\u4e2a\u660e\u663e\u7684\u5916\u58f3\u3002\u7136\u800c\uff0c\u6ca1\u6709\u66f4\u591a\u7684\u7279\u5b9a\u4fe1\u606f\uff0c\u5f88\u96be\u786e\u5b9a\u8fd9\u662f\u54ea\u79cd\u5bc4\u751f\u866b\u7684\u5375\u3002\u5982\u679c\u5fc5\u987b\u731c\u6d4b\uff0c\n\u6211\u4f1a\u8bf4\u8fd9\u4e2a\u5375\u6700\u53ef\u80fd\u662f(D) \u53d7\u7cbe\u86d4\u866b\u5375\uff0c\u56e0\u4e3a\u5b83\u770b\u8d77\u6765\u6bd4\u8f83\u63a5\u8fd1\u8fd9\u79cd\u7c7b\u578b\u7684\u5375\u7684\u63cf\u8ff0\u3002\u4f46\u8bf7\u6ce8\u610f\uff0c\u8fd9\n\u53ea\u662f\u4e00\u4e2a\u57fa\u4e8e\u53ef\u89c1\u7279\u5f81\u7684\u731c\u6d4b\uff0c\u800c\u4e0d\u662f\u4e00\u4e2a\u4e13\u4e1a\u7684\u533b\u5b66\u8bca\u65ad\u3002 \u6b63\u786e\u7b54\u6848\uff1a(D) \u53d7\u7cbe\u86d4\u866b\u5375\uff08\u8fd9\u662f\u4e00\n\u4e2a\u57fa\u4e8e\u56fe\u50cf\u548c\u4e00\u822c\u77e5\u8bc6\u7684\u731c\u6d4b\uff0c\u800c\u975e\u786e\u5207\u7b54\u6848\uff09\nGround Truth: (D)\u53d7\u7cbe\u86d4\u866b\u5375\n<\u56fe\u7247 1>\nFigure B64: A sample correct case of Health and Medicine (subcategory: Public Health).\nBack to List of figures\n83\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66:\u516c\u5171\u536b\u751f\nQuestion:\u4e3a\u4e86\u8bc4\u4ef7\u6c34\u75d8\u75ab\u82d7\u7684\u6d41\u884c\u75c5\u5b66\u6548\u679c\uff0c\u5176\u968f\u8bbf\u7ed3\u679c\u89c1<\u56fe\u7247 1>\uff0c\u8be5\u75ab\u82d7\u7684\u4fdd\u62a4\u7387\u662f\uff08\uff09\nOption: \n(A) 0.8\n(B) 0.833\n(C) 0.9\n(D) 0.925\nGPT-4V: \u4fdd\u62a4\u7387\uff08Efficacy\uff09\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u516c\u5f0f\u8ba1\u7b97\uff1a [\ud835\udc38\ud835\udc38 = (1 \u2212\n\u53d1\u75c5\u7387\u63a5\u79cd\u7ec4\n\u53d1\u75c5\u7387\u5bf9\u7167\u7ec4\n) \u00d7 100%]\n \u9996\u5148\uff0c\u6211\u4eec\u9700\u8981\u8ba1\u7b97\u63a5\u79cd\u7ec4\u548c\u5bf9\u7167\u7ec4\u7684\u53d1\u75c5\u7387\u3002 \u63a5\u79cd\u7ec4\u53d1\u75c5\u7387 = \u53d1\u75c5\u4eba\u6570 / \u63a5\u79cd\u4eba\u6570 = 10 / \n400 = 0.025 \u5bf9\u7167\u7ec4\u53d1\u75c5\u7387 = \u53d1\u75c5\u4eba\u6570 / \u5bf9\u7167\u4eba\u6570 = 90 / 600 = 0.15 \u7136\u540e\uff0c\u6211\u4eec\u5c06\u8fd9\u4e9b\u503c\u4ee3\u5165\n\u4fdd\u62a4\u7387\u516c\u5f0f: [\ud835\udc38\ud835\udc38 = (1 \u2212\n0.025\n0.15 ) \u00d7 100%][\ud835\udc38\ud835\udc38 = (1 \u2212\n1\n6) \u00d7 100%][\ud835\udc38\ud835\udc38 = (1 \u2212 0.1667) \u00d7 100%][\ud835\udc38\ud835\udc38 = 0.8333\n\u00d7 100%][\ud835\udc38\ud835\udc38 = 83.33%]\n\u6240\u4ee5\uff0c\u4fdd\u62a4\u7387\u6700\u63a5\u8fd1\u7684\u9009\u9879\u662f (B) 0.833\u3002\nGround Truth: (B)0.833\n<\u56fe\u7247 1>\nFigure B65: A sample correct case of Health and Medicine (subcategory: Public Health).\nBack to List of figures\n84\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66:\u516c\u5171\u536b\u751f\nQuestion:\u8bc6\u522b<\u56fe\u7247 1>\u5185\u5bb9\u4e3a\uff08\uff09\u866b\u5375\nOption: \n(A) \u809d\u5438\u866b\u5375\n(B) \u80ba\u5438\u866b\u5375\n(C) \u97ad\u866b\u5375\n(D) \u59dc\u7247\u866b\u5375\nGPT-4V: \u5f88\u62b1\u6b49\uff0c\u6211\u65e0\u6cd5\u63d0\u4f9b\u533b\u5b66\u8bca\u65ad\u6216\u8bc6\u522b\u5177\u4f53\u7684\u5bc4\u751f\u866b\u5375\u3002\u56fe\u7247\u663e\u793a\u7684\u662f\u5728\u663e\u5fae\u955c\u4e0b\u89c2\u5bdf\u5230\u7684\u4e00\n\u4e2a\u7269\u4f53\uff0c\u4f46\u662f\u6ca1\u6709\u4e13\u4e1a\u7684\u533b\u5b66\u80cc\u666f\u548c\u5177\u4f53\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u6211\u65e0\u6cd5\u786e\u5b9a\u8fd9\u662f\u54ea\u79cd\u5bc4\u751f\u866b\u7684\u5375\u3002\u6b63\u786e\u8bc6\u522b\n\u5bc4\u751f\u866b\u5375\u901a\u5e38\u9700\u8981\u4e13\u4e1a\u7684\u77e5\u8bc6\u548c\u5b9e\u9a8c\u5ba4\u6d4b\u8bd5\u3002\u5982\u679c\u9700\u8981\u6b64\u7c7b\u4fe1\u606f\uff0c\u8bf7\u54a8\u8be2\u533b\u5b66\u4e13\u4e1a\u4eba\u58eb\u3002\nGround Truth: (B)\u80ba\u5438\u866b\u5375\n<\u56fe\u7247 1>\nError Category: Reject to Answer\nError Reason: Due to ethical considerations and other factors, GPT-4V refuses to provide medical \ndiagnoses or identify specific parasitic eggs.\nFigure B66: A sample error case of Health and Medicine (subcategory: Public Health).\nBack to List of figures\n85\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66:\u516c\u5171\u536b\u751f\nQuestion: \u5982\u56fe<\u56fe\u7247 1>\uff0c\u96cc\u868a\u548c\u96c4\u868a\u7684\u8f6e\u6bdb\u6709\u4ec0\u4e48\u533a\u522b\uff1f\nOption: \n(A) \u96cc\u868a\u7684\u8f6e\u6bdb\u77ed\u800c\u5bc6\uff0c\u96c4\u868a\u7684\u8f6e\u6bdb\u957f\u800c\u758f\n(B) \u96cc\u868a\u7684\u8f6e\u6bdb\u957f\u800c\u758f\uff0c\u96c4\u868a\u7684\u8f6e\u6bdb\u77ed\u800c\u5bc6\n(C) \u65e0\u533a\u522b\n(D) \u96cc\u868a\u7684\u8f6e\u6bdb\u77ed\u800c\u758f\uff0c\u96c4\u868a\u7684\u8f6e\u6bdb\u957f\u800c\u5bc6\nGPT-4V: \u6b63\u786e\u7b54\u6848\u662f\uff1a(A) \u96cc\u868a\u7684\u8f6e\u6bdb\u77ed\u800c\u5bc6\uff0c\u96c4\u868a\u7684\u8f6e\u6bdb\u957f\u800c\u758f \u4ece\u63d0\u4f9b\u7684\u56fe\u7247\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0c\u96cc\u868a\n\uff08c\uff09\u7684\u8f6e\u6bdb\u76f8\u5bf9\u8f83\u77ed\u4e14\u5206\u5e03\u8f83\u5bc6\u96c6\uff0c\u800c\u96c4\u868a\uff08d\uff09\u7684\u8f6e\u6bdb\u5219\u8f83\u957f\u4e14\u5206\u5e03\u8f83\u758f\u3002\nGround Truth: (D)\u96cc\u868a\u7684\u8f6e\u6bdb\u77ed\u800c\u758f\uff0c\u96c4\u868a\u7684\u8f6e\u6bdb\u957f\u800c\u5bc6\n<\u56fe\u7247 1>\nError Category: Perceptual Error\nError Reason: GPT-4V made an error in image perception, incorrectly identifying the sparse setae \non the left (image c) of the female mosquito as dense, and the dense setae on the right (image d) of \nthe male mosquito as sparse.\nFigure B67: A sample error case of Health and Medicine (subcategory: Public Health).\nBack to List of figures\n86\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66:\u516c\u5171\u536b\u751f\nQuestion: \u4e3a\u8bc4\u4ef7\u4e00\u79cd\u5bb6\u7528\u201c\u7b80\u4fbf\u201d\u598a\u5a20\u8bd5\u9a8c\u836f\u76d2\u7684\u6548\u679c\uff0c\u5176\u8bd5\u9a8c\u7ed3\u679c\u4e0e\u533b\u9662\u5e38\u89c4\u7684\u598a\u5a20\u8bca\u65ad\u8bd5\u9a8c\u7ed3\n\u679c\u5982\u56fe<\u56fe\u7247 1>\uff0c\u8be5\u836f\u76d2\u7684\u654f\u611f\u5ea6\u662f\uff08\uff09\nOption: \n(A) 42.65%\n(B) 50.88%\n(C) 42.68%\n(D) 35%\nGPT-4V: \u654f\u611f\u5ea6\uff08Sensitivity\uff09\u662f\u6307\u5728\u6240\u6709\u771f\u5b9e\u4e3a\u9633\u6027\u7684\u6837\u672c\u4e2d\uff0c\u6d4b\u8bd5\u6b63\u786e\u8bc6\u522b\u4e3a\u9633\u6027\u7684\u6bd4\u4f8b\u3002\u6839\n\u636e\u6240\u63d0\u4f9b\u7684\u56fe\u8868\uff0c\u6211\u4eec\u53ef\u4ee5\u8ba1\u7b97\u654f\u611f\u5ea6\uff1a \u654f\u611f\u5ea6 = \u771f\u9633\u6027 / (\u771f\u9633\u6027 + \u5047\u9634\u6027) \u4ece\u56fe\u8868\u4e2d\uff0c\u6211\u4eec\n\u53ef\u4ee5\u770b\u5230\u771f\u9633\u6027\uff08\u5373\u5bb6\u7528\u8bd5\u9a8c\u836f\u76d2\u548c\u533b\u9662\u8bca\u65ad\u90fd\u4e3a\u9633\u6027\uff09\u7684\u6570\u91cf\u662f29\uff0c\u5047\u9634\u6027\uff08\u5373\u533b\u9662\u8bca\u65ad\u4e3a\u9633\u6027\u4f46\n\u5bb6\u7528\u8bd5\u9a8c\u836f\u76d2\u4e3a\u9634\u6027\uff09\u7684\u6570\u91cf\u662f39\u3002 \u654f\u611f\u5ea6 = 29 / (29 + 39) \u654f\u611f\u5ea6 = 29 / 68 \u654f\u611f\u5ea6 = \n0.4265 \u6216\u8005 42.65% \u6240\u4ee5\u6b63\u786e\u7b54\u6848\u662f (A) 42.65%\u3002\nGround Truth: (B)50.88%\n<\u56fe\u7247 1>\nError Category: Perceptual Error\nError Reason: GPT-4V made an error in image perception. While it correctly inferred the false-\nnegative scenario as \"diagnosed positive in the hospital but negative in the home test kit,\" it \nmisread the false-negative count as 39, when the correct count for false negatives is 28.\nFigure B68: A sample error case of Health and Medicine (subcategory: Public Health).\nBack to List of figures\n87\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66:\u516c\u5171\u536b\u751f\nQuestion: \u5982\u56fe<\u56fe\u7247 1>\uff0c\u8bf7\u9009\u62e9\u6b63\u786e\u7684\u540d\u79f0\u53ca\u5176\u4e0e\u5bf9\u5e94\u5bc4\u751f\u866b\u4e4b\u95f4\u7684\u5173\u7cfb\uff08\uff09\nOption: \n(A) \u8378\u8360\uff0c\u809d\u5438\u866b\u7684\u7b2c\u4e00\u4e2d\u95f4\u5bbf\u4e3b\n(B) \u832d\u767d\uff0c\u59dc\u7247\u866b\u7684\u7b2c\u4e00\u4e2d\u95f4\u5bbf\u4e3b\n(C) \u8378\u8360\uff0c\u59dc\u7247\u866b\u7684\u4f20\u64ad\u5a92\u4ecb\n(D) \u832d\u767d\uff0c\u59dc\u7247\u866b\u7684\u4f20\u64ad\u5a92\u4ecb\nGPT-4V: \u8fd9\u5f20\u56fe\u7247\u663e\u793a\u7684\u662f\u8378\u8360\u3002\u8378\u8360\u662f\u4e00\u79cd\u6c34\u751f\u690d\u7269\u7684\u5757\u830e\uff0c\u5b83\u5e76\u4e0d\u662f\u5bc4\u751f\u866b\u7684\u5bbf\u4e3b\u3002\u56e0\u6b64\uff0c\u9009\u9879\n(A)\u548c(C)\u90fd\u662f\u4e0d\u6b63\u786e\u7684\u3002\u832d\u767d\u662f\u53e6\u4e00\u79cd\u6c34\u751f\u690d\u7269\uff0c\u4e0e\u8378\u8360\u4e0d\u540c\uff0c\u6240\u4ee5\u9009\u9879(B)\u548c(D)\u4e2d\u63d0\u5230\u7684\u832d\u767d\u4e5f\u662f\n\u4e0d\u6b63\u786e\u7684\u3002\u7531\u4e8e\u9898\u76ee\u4e2d\u7684\u4fe1\u606f\u4e0d\u8db3\u4ee5\u786e\u5b9a\u8378\u8360\u4e0e\u4efb\u4f55\u5bc4\u751f\u866b\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u6b63\u786e\u7b54\u6848\u5e94\u8be5\u662f\u6ca1\u6709\u5217\u51fa\u7684\n\u9009\u9879\uff0c\u5373\u8fd9\u4e9b\u9009\u9879\u90fd\u4e0d\u6b63\u786e\u3002\nGround Truth: (C)\u8378\u8360\uff0c\u59dc\u7247\u866b\u7684\u4f20\u64ad\u5a92\u4ecb\n<\u56fe\u7247 1>\nError Category: Reasoning Error\nError Reason: GPT-4V correctly identified the plant in the image as water chestnut. However, it \nmade an incorrect inference by ruling out the correct option (C), as water chestnut is not a host for \nparasites.\nFigure B69: A sample error case of Health and Medicine (subcategory: Public Health).\nBack to List of figures\n88\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66:\u516c\u5171\u536b\u751f\nQuestion:\u4e3a\u8bc4\u4ef7\u6c34\u75d8\u75ab\u82d7\u7684\u6d41\u884c\u75c5\u5b66\u6548\u679c\uff0c\u5176\u968f\u8bbf\u7ed3\u679c\u5982\u56fe<\u56fe\u7247 1>\uff0c\u8be5\u75ab\u82d7\u7684\u6548\u679c\u6307\u6570\u662f\uff08\uff09\nOption: \n(A) 6\n(B) 7\n(C) 8\n(D) 9\nGPT-4V: \u4e3a\u4e86\u8ba1\u7b97\u75ab\u82d7\u7684\u6548\u679c\u6307\u6570\uff0c\u6211\u4eec\u9700\u8981\u4f7f\u7528\u4ee5\u4e0b\u516c\u5f0f\u6765\u8ba1\u7b97\u75ab\u82d7\u6548\u529b\uff08VE\uff0cVaccine \nEfficacy\uff09\uff1a VE = (1 - RR) \u00d7 100% \u5176\u4e2dRR\u662f\u76f8\u5bf9\u98ce\u9669\uff08Relative Risk\uff09\uff0c\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u516c\u5f0f\u8ba1\n\u7b97\uff1a RR = (\u63a5\u79cd\u75ab\u82d7\u540e\u53d1\u75c5\u7387) / (\u672a\u63a5\u79cd\u75ab\u82d7\u7684\u53d1\u75c5\u7387) \u6839\u636e\u56fe\u4e2d\u7684\u6570\u636e\uff0c\u6211\u4eec\u53ef\u4ee5\u8ba1\u7b97\u51fa\u63a5\u79cd\u75ab\n\u82d7\u540e\u548c\u672a\u63a5\u79cd\u75ab\u82d7\u7684\u53d1\u75c5\u7387\uff1a \u63a5\u79cd\u75ab\u82d7\u540e\u53d1\u75c5\u7387 = \u63a5\u79cd\u75ab\u82d7\u540e\u53d1\u75c5\u4eba\u6570 / \u63a5\u79cd\u75ab\u82d7\u4eba\u6570 = 10 / \n400 = 0.025 \u672a\u63a5\u79cd\u75ab\u82d7\u7684\u53d1\u75c5\u7387 = \u672a\u63a5\u79cd\u75ab\u82d7\u7684\u53d1\u75c5\u4eba\u6570 / \u672a\u63a5\u79cd\u75ab\u82d7\u4eba\u6570 = 90 / 600 = 0.15 \n\u7136\u540e\u8ba1\u7b97\u76f8\u5bf9\u98ce\u9669\uff08RR\uff09\uff1a RR = 0.025 / 0.15 = 1/6 \u2248 0.1667 \u6700\u540e\u8ba1\u7b97\u75ab\u82d7\u6548\u529b\uff08VE\uff09\uff1a VE = \n(1 - 0.1667) \u00d7 100% \u2248 83.33% \u5728\u6240\u7ed9\u7684\u9009\u9879\u4e2d\uff0c\u5e76\u6ca1\u6709\u4e00\u4e2a\u9009\u9879\u76f4\u63a5\u5bf9\u5e94\u523083.33%\uff0c\u4f46\u662f\u6211\u4eec\n\u53ef\u4ee5\u9009\u62e9\u6700\u63a5\u8fd1\u7684\u9009\u9879\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a (D) 9 \u56e0\u4e3a83.33%\u6700\u63a5\u8fd1\u4e8e\u9009\u9879(D) 9\uff0c\u5982\u679c\n9\u4ee3\u8868\u7684\u662f\u75ab\u82d7\u6548\u679c\u6307\u6570\u7684\u767e\u5206\u6bd4\u5f62\u5f0f\uff0c\u537390%\u3002\nGround Truth: (A)6\n<\u56fe\u7247 1>\nError Category: Textual Understanding\nError Reason: GPT-4V misinterpreted the question, confusing \"efficacy index\" with \"vaccine \nefficacy.\" The correct formula for the efficacy index is: Efficacy Index = Incidence Rate in Control \nGroup / Incidence Rate in Vaccinated Group = 0.15 / 0.025 = 6.\nFigure B70: A sample error case of Health and Medicine (subcategory: Public Health).\nBack to List of figures\n89\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66:\u516c\u5171\u536b\u751f\nQuestion: \u5982\u56fe<\u56fe\u7247 1>\uff0c\u662f1938\uff5e1952\u5e74\u67d0\u5730\u533a\u533b\u9662\u63d0\u4f9b\u4e24\u6027\u5a74\u5e7c\u513f\u67d0\u75c5\u7684\u53d1\u75c5\u7387\u8d44\u6599\uff0cX2=0.02\uff0c\nP\uff1e0.10\u3002 \u4e0a\u8ff0\u8d44\u6599\u7684\u7ed3\u679c\u7684\u542b\u610f\u662f\uff08\uff09\nOption: \n(A)\u5a74\u5e7c\u513f\u7684\u6027\u522b\u56e0\u7d20\u4e0e\u67d0\u75c5\u7684\u53d1\u75c5\u7387\u6709\u5dee\u522b\n(B) \u5a74\u5e7c\u513f\u7684\u6027\u522b\u56e0\u7d20\u4e0e\u67d0\u75c5\u53d1\u75c5\u7387\u65e0\u7edf\u8ba1\u5b66\u7684\u5dee\u522b\n(C) \u67d0\u75c5\u4e0e\u5a74\u5e7c\u513f\u6027\u522b\u6709\u76f8\u5173\n(D) \u6027\u522b\u662f\u8be5\u75c5\u7684\u75c5\u56e0\u56e0\u7d20\nGPT-4V: \u6839\u636e\u6240\u63d0\u4f9b\u7684\u4fe1\u606f\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u5361\u65b9\u68c0\u9a8c\u7684\u7ed3\u679c\u662f X2 =0.02\uff0c\u5e76\u4e14 P>0.10\u3002\u8fd9\u610f\u5473\u7740\u5728\n\u7edf\u8ba1\u5b66\u4e0a\uff0c\u5a74\u5e7c\u513f\u7684\u6027\u522b\u56e0\u7d20\u4e0e\u67d0\u75c5\u7684\u53d1\u75c5\u7387\u4e4b\u95f4\u6ca1\u6709\u663e\u8457\u5dee\u5f02\uff0c\u56e0\u4e3aP\u503c\u5927\u4e8e0.10\uff0c\u901a\u5e38\u8ba4\u4e3aP\u503c\u5c0f\n\u4e8e0.05\u65f6\u7ed3\u679c\u624d\u5177\u6709\u7edf\u8ba1\u5b66\u610f\u4e49\u3002 \u56e0\u6b64\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a (B)\u5a74\u5e7c\u513f\u7684\u6027\u522b\u56e0\u7d20\u4e0e\u67d0\u75c5\u53d1\u75c5\u7387\u65e0\u7edf\u8ba1\n\u5b66\u7684\u5dee\u522b \u5176\u4ed6\u9009\u9879\uff1a (A)\u9519\u8bef\uff0c\u56e0\u4e3a\u7edf\u8ba1\u7ed3\u679c\u8868\u660e\u6ca1\u6709\u663e\u8457\u5dee\u5f02\u3002 (C)\u9519\u8bef\uff0c\u56e0\u4e3a\u6ca1\u6709\u7edf\u8ba1\u5b66\u76f8\u5173\n\u6027\u3002(D)\u9519\u8bef\uff0c\u56e0\u4e3a\u6027\u522b\u4e0d\u662f\u8be5\u75c5\u7684\u75c5\u56e0\u56e0\u7d20\uff0c\u81f3\u5c11\u5728\u7edf\u8ba1\u5b66\u4e0a\u6ca1\u6709\u8bc1\u636e\u8868\u660e\u6709\u76f8\u5173\u6027\u3002\nGround Truth: (B)\u5a74\u5e7c\u513f\u7684\u6027\u522b\u56e0\u7d20\u4e0e\u67d0\u75c5\u53d1\u75c5\u7387\u65e0\u7edf\u8ba1\u5b66\u7684\u5dee\u522b\n<\u56fe\u7247 1>\nError Category: Answer Extraction Error\nError Reason: GPT-4V correctly understood the image and made an accurate judgment. However, \nin its subsequent analysis, it also examined each incorrect option, leading to errors in the Extract & \nMatch process, resulting in the extraction of answers (A), (B), (C), and (D).\nFigure B71: A sample error case of Health and Medicine (subcategory: Public Health).\nBack to List of figures\n90\nPREPRINT\n\u5065\u5eb7\u4e0e\u533b\u5b66:\u516c\u5171\u536b\u751f\nQuestion:\u8bc6\u522b<\u56fe\u7247 1>\u5185\u5bb9\u4e3a\uff08\uff09\u866b\u5375\nOption: \n(A) \u80ba\u5438\u866b\u5375\n(B) \u8840\u5438\u866b\u5375\n(C) \u809d\u5438\u866b\u5375\n(D) \u59dc\u7247\u866b\u5375\nGPT-4V: \u8fd9\u5f20\u56fe\u7247\u663e\u793a\u7684\u662f\u5728\u663e\u5fae\u955c\u4e0b\u89c2\u5bdf\u5230\u7684\u4e00\u4e2a\u5375\u3002\u7531\u4e8e\u6211\u65e0\u6cd5\u8fdb\u884c\u5b9e\u9645\u7684\u5b9e\u9a8c\u5ba4\u6d4b\u8bd5\u6216\u5206\u6790\uff0c\n\u6211\u4e0d\u80fd\u63d0\u4f9b\u4e00\u4e2a\u786e\u5207\u7684\u7b54\u6848\u3002\u7136\u800c\uff0c\u6839\u636e\u5f62\u72b6\u548c\u5916\u89c2\uff0c\u8fd9\u4e2a\u5375\u53ef\u80fd\u662f\u67d0\u79cd\u5bc4\u751f\u866b\u7684\u5375\u3002\u5728\u9009\u9879\u4e2d\uff0c\u80ba\n\u5438\u866b\u3001\u8840\u5438\u866b\u548c\u809d\u5438\u866b\u90fd\u662f\u5bc4\u751f\u866b\uff0c\u800c\u59dc\u7247\u866b\u5e76\u4e0d\u662f\u4e00\u4e2a\u5e38\u89c1\u7684\u5bc4\u751f\u866b\u540d\u79f0\uff0c\u8fd9\u53ef\u80fd\u662f\u4e00\u4e2a\u5e72\u6270\u9879\u3002\n\u901a\u5e38\uff0c\u80ba\u5438\u866b\u5375\u3001\u8840\u5438\u866b\u5375\u548c\u809d\u5438\u866b\u5375\u5728\u663e\u5fae\u955c\u4e0b\u6709\u7279\u5b9a\u7684\u5f62\u72b6\u548c\u5927\u5c0f\uff0c\u4f46\u662f\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u6765\u51c6\u786e\u8bc6\n\u522b\u3002 \u5982\u679c\u5fc5\u987b\u731c\u6d4b\uff0c\u6211\u4f1a\u5efa\u8bae\u9009\u62e9\u6700\u53ef\u80fd\u6b63\u786e\u7684\u9009\u9879\uff0c\u4f46\u8bf7\u6ce8\u610f\uff0c\u8fd9\u4e0d\u662f\u4e00\u4e2a\u57fa\u4e8e\u5b9e\u9a8c\u5ba4\u5206\u6790\u7684\u786e\n\u5207\u7b54\u6848\u3002\u56e0\u6b64\uff0c\u6b63\u786e\u7b54\u6848\u53ef\u80fd\u662f\uff1a (A) \u80ba\u5438\u866b\u5375 (B) \u8840\u5438\u866b\u5375 (C) \u809d\u5438\u866b\u5375 \u7531\u4e8e\u7f3a\u4e4f\u5177\u4f53\u4fe1\u606f\uff0c\n\u6211\u65e0\u6cd5\u786e\u5b9a\u5177\u4f53\u662f\u54ea\u4e00\u4e2a\u3002\u5982\u679c\u8fd9\u662f\u4e00\u4e2a\u5b9e\u9645\u7684\u533b\u5b66\u6216\u79d1\u5b66\u95ee\u9898\uff0c\u5efa\u8bae\u54a8\u8be2\u4e13\u4e1a\u4eba\u58eb\u8fdb\u884c\u51c6\u786e\u7684\u9274\u5b9a\u3002\nGround Truth: (A)\u80ba\u5438\u866b\u5375\n<\u56fe\u7247 1>\nError Category: Lack of Knowledge\nError Reason: GPT-4V correctly understood the image and inferred that the answer lies within the \napproximate range of (A), (B), and (C) based on the image. However, due to a lack of domain-\nspecific knowledge, it couldn't provide a specific answer.\nFigure B72: A sample error case of Health and Medicine (subcategory: Public Health).\nBack to List of figures\n91\nPREPRINT\n\u5546\u4e1a:\u4f1a\u8ba1\nQuestion: \n\u5982\u679c\u8981\u7ed8\u5236\u51fa<\u56fe\u7247 1>\u4e2d\u6240\u793a\u7684\u9010\u6e10\u6de1\u51fa\u7684\u7b14\u89e6,\u9700\u8981\u5728\u3010\u753b\u7b14\u3011\u63a7\u5236\u9762\u677f\u4e2d\u8bbe\u7f6e\u7684\u4e00\u9879\n\u91cd\u8981\u53c2\u6570\u662f( \uff09\u3002\nOption:\n(A) \u5e73\u8861\u6301\u6709\u91cf\n(B) \u6700\u4f73\u6301\u6709\u91cf\n(C) \u6700\u4f4e\u6301\u6709\u91cf\n(D) \u6700\u9ad8\u6301\u6709\u91cf\nGPT-4V: \u8fd9\u5f20\u56fe\u5c55\u793a\u4e86\u4e00\u4e2a\u5178\u578b\u7684\u5e93\u5b58\u6210\u672c\u6a21\u578b\uff0c\u5176\u4e2d\u5305\u62ec\u56fa\u5b9a\u6210\u672c\u3001\u53d8\u52a8\u6210\u672c\u548c\u603b\u6210\u672c\u3002\u56fe\u4e2d\u7684\u603b\n\u6210\u672c\u66f2\u7ebf\u5728\u67d0\u4e00\u70b9\u8fbe\u5230\u6700\u4f4e\u503c\uff0c\u8fd9\u4e2a\u70b9\u5bf9\u5e94\u7684\u73b0\u91d1\u6c34\u5e73\u662f\u4f01\u4e1a\u6301\u6709\u73b0\u91d1\u7684\u6700\u4f73\u6c34\u5e73\u3002\u56e0\u6b64\uff0c\u5f53\u4e24\u79cd\u6210\n\u672c\u4e4b\u548c\uff0c\u4e5f\u5373\u603b\u6210\u672c\u8fbe\u5230\u6700\u5c0f\u503c\u65f6\uff0c\u4f01\u4e1a\u6240\u6301\u6709\u7684\u73b0\u91d1\u6c34\u5e73\u4e3a\u6700\u4f73\u6301\u6709\u91cf\u3002\n\u6b63\u786e\u7b54\u6848\u662f\uff1a\n(B) \u6700\u4f73\u6301\u6709\u91cf\nGround Truth: (B) \u6700\u4f73\u6301\u6709\u91cf\n<\u56fe\u7247 1>\nFigure B73: A sample correct case of Business (subcategory: Accounting).\nBack to List of figures\n92\nPREPRINT\n\u5546\u4e1a:\u4f1a\u8ba1\nQuestion:  \u67d0\u4f01\u4e1a\u4e3a\u4e86\u8fdb\u884c\u4e3b\u8981\u8d44\u4ea7\u9879\u76ee\u7684\u6bd4\u91cd\u5206\u6790,\u7279\u6839\u636e\u8d44\u4ea7\u8d1f\u503a\u8868\u8868\u5236\u5b9a\u4e86\u4e3b\u8981\u8d44\u4ea7\u9879\u76ee\u6bd4\u91cd\n\u53ca\u5176\u5dee\u5f02\u8868\uff0c\u5982\u4e0b\uff1a<\u56fe\u7247 1>\uff0c\u7531\u6b64\u53ef\u89c1\u975e\u6d41\u52a8\u8d44\u4ea7\u4e2d\u56fa\u5b9a\u8d44\u4ea7\u6bd4\u91cd__\nOption:\n(A)  \u4e0a\u5347\n(B) \u964d\u4f4e\n(C) \u4fdd\u6301\u4e0d\u53d8\n(D) \u65e0\u6cd5\u5224\u65ad\nGPT-4V: \u6839\u636e\u63d0\u4f9b\u7684\u56fe\u7247\u4e2d\u7684\u6570\u636e\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u201c\u56fa\u5b9a\u8d44\u4ea7\u201d\u8fd9\u4e00\u884c\u7684\u6bd4\u91cd\u4ece\u4e0a\u4e00\u671f\u768414.7%\u4e0a\u5347\n\u5230\u4e86\u672c\u671f\u768414.4%\uff0c\u5dee\u5f02\u4e3a-0.3%\u3002\u8fd9\u8868\u660e\u56fa\u5b9a\u8d44\u4ea7\u7684\u6bd4\u91cd\u5b9e\u9645\u4e0a\u662f\u964d\u4f4e\u4e86\u3002\n\u56e0\u6b64\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a\n(B) \u964d\u4f4e\nGround Truth: (A) \u4e0a\u5347\n<\u56fe\u7247 1>\nError Category: Perceptual Error\nError Reason: GPT-4V incorrectly identified the line 'inventory' as fixed assets and recognized a -\n0.3% difference as a change in the proportion of fixed assets. However, in reality, the proportion of \nfixed assets changed by +8.2%. This error was due to a misperception by GPT-4V while reading the \nimage.\nFigure B74: A sample error case of Business (subcategory: Accounting).\nBack to List of figures\n93\nPREPRINT\n\u5546\u4e1a:\u4f1a\u8ba1\nQuestion:  \u5206\u6790\u8d44\u6599:\u67d0\u4f01\u4e1a\u751f\u4ea7\u4e19\u4ea7\u54c1,\u5176\u4ea7\u54c1\u5355\u4f4d\u6210\u672c\u7b80\u8868\u5982\u4e0b\u8868<\u56fe\u7247 1>\uff0c\u7528\u5dee\u989d\u8ba1\u7b97\u6cd5\u5206\u6790\u5355\n\u4f4d\u5de5\u65f6\u548c\u5c0f\u65f6\u5de5\u8d44\u7387\u53d8\u52a8\u5bf9\u5355\u4f4d\u76f4\u63a5\u4eba\u5de5\u7684\u5f71\u54cd\u4e3a__\nOption:\n(A) \u6750\u6599\u6210\u672c\u589e\u52a0\n(B) \u6750\u6599\u6210\u672c\u964d\u4f4e\n(C) \u4eba\u5de5\u6210\u672c\u964d\u4f4e\n(D) \u4eba\u5de5\u6210\u672c\u589e\u52a0\nGPT-4V: \u4e3a\u4e86\u5206\u6790\u5355\u4f4d\u5de5\u65f6\u548c\u5c0f\u65f6\u5de5\u8d44\u7387\u53d8\u52a8\u5bf9\u5355\u4f4d\u76f4\u63a5\u4eba\u5de5\u7684\u5f71\u54cd\uff0c\u6211\u4eec\u9700\u8981\u67e5\u770b\u8868\u683c\u4e2d\u7684\u76f8\u5173\u6570\n\u636e\u3002\n\u4ece\u8868\u683c\u4e2d\u53ef\u4ee5\u770b\u5230\uff0c\u76f4\u63a5\u4eba\u5de5\u7684\u5355\u4f4d\u6210\u672c\u5728\u6807\u51c6\u60c5\u51b5\u4e0b\u662f20\u5143\uff0c\u800c\u5728\u5b9e\u9645\u60c5\u51b5\u4e0b\u662f27\u5143\u3002\u8fd9\u610f\n\u5473\u7740\u5355\u4f4d\u76f4\u63a5\u4eba\u5de5\u6210\u672c\u5b9e\u9645\u4e0a\u589e\u52a0\u4e86\u3002\n\u540c\u65f6\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u6807\u51c6\u5de5\u65f6\u4e3a12\u5c0f\u65f6\uff0c\u5b9e\u9645\u5de5\u65f6\u4e3a11\u5c0f\u65f6\uff0c\u5355\u4f4d\u5de5\u65f6\u6210\u672c\u4ece\u6807\u51c6\u768410\u5143\u964d\u4f4e\u5230\u4e86\u5b9e\n\u9645\u768410\u5143\u3002\u5c3d\u7ba1\u5de5\u65f6\u6210\u672c\u6ca1\u6709\u53d8\u5316\uff0c\u4f46\u662f\u5de5\u65f6\u51cf\u5c11\u4e86\uff0c\u8fd9\u610f\u5473\u7740\u6bcf\u4e2a\u5355\u4f4d\u4ea7\u54c1\u7684\u5de5\u65f6\u6548\u7387\u63d0\u9ad8\u4e86\u3002\n\u7136\u800c\uff0c\u5c3d\u7ba1\u5de5\u65f6\u6548\u7387\u63d0\u9ad8\uff0c\u76f4\u63a5\u4eba\u5de5\u7684\u603b\u6210\u672c\u8fd8\u662f\u589e\u52a0\u4e86\uff0c\u8fd9\u8868\u660e\u5c0f\u65f6\u5de5\u8d44\u7387\u7684\u589e\u52a0\u5bf9\u6210\u672c\u7684\u5f71\u54cd\u8d85\n\u8fc7\u4e86\u5de5\u65f6\u6548\u7387\u7684\u63d0\u9ad8\u3002\n\u56e0\u6b64\uff0c\u6211\u4eec\u53ef\u4ee5\u5f97\u51fa\u7ed3\u8bba\uff0c\u5c0f\u65f6\u5de5\u8d44\u7387\u7684\u589e\u52a0\u5bfc\u81f4\u4e86\u5355\u4f4d\u76f4\u63a5\u4eba\u5de5\u6210\u672c\u7684\u589e\u52a0\u3002\n\u6b63\u786e\u7b54\u6848\u662f\uff1a\n(D) \u4eba\u5de5\u6210\u672c\u589e\u52a0\nGround Truth: (C) \u4eba\u5de5\u6210\u672c\u964d\u4f4e\n<\u56fe\u7247 1>\nError Category: Reasoning Error\nError Reason: GPT-4V, in answering the question, only qualitatively estimated the direct labor unit \ncost and used this as a basis to conclude that labor costs increased. However, in reality, the increase \nin labor expenses for a company is mainly due to the rise in hourly wage rates, and there has been \nno decline in the company's production efficiency. Improved efficiency has actually reduced the \nlabor cost per unit product by 2 yuan. Here, the model did not engage in quantitative reasoning, \nand therefore, it constitutes a Reasoning Error.\nFigure B75: A sample error case of Business (subcategory: Accounting).\nBack to List of figures\n94\nPREPRINT\n\u5546\u4e1a: \u7ecf\u6d4e\nQuestion:\u6839\u636e<\u56fe\u7247 1>\uff0c\u5e02\u573a\u7684\u5747\u8861\u4ef7\u683c\u548c\u6570\u91cf\u662f( )\nOption: \n(A) 18\u7f8e\u5143\u548c15\u5355\u4f4d\n(B)  5\u7f8e\u5143\u548c15\u5355\u4f4d\n(C) 10\u7f8e\u5143\u548c30\u5355\u4f4d\n(D) 5\u7f8e\u5143\u548c35\u5355\u4f4d\nGPT-4V:\u6839\u636e\u63d0\u4f9b\u7684\u56fe\u8868\uff0c\u5e02\u573a\u5747\u8861\u4ef7\u683c\u548c\u6570\u91cf\u662f\u4f9b\u7ed9\u548c\u9700\u6c42\u66f2\u7ebf\u7684\u4ea4\u70b9\u3002\u4ece\u56fe\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0c\u8fd9\u4e24\u6761\n\u66f2\u7ebf\u5728\u4ef7\u683c\u4e3a10\u7f8e\u5143\u548c\u6570\u91cf\u4e3a30\u5355\u4f4d\u7684\u70b9\u4e0a\u76f8\u4ea4\u3002\u56e0\u6b64\uff0c\u5e02\u573a\u7684\u5747\u8861\u4ef7\u683c\u662f10\u7f8e\u5143\uff0c\u5747\u8861\u6570\u91cf\u662f30\u5355\n\u4f4d\u3002\n\u6b63\u786e\u7b54\u6848\u662f\uff1a(C) 10\u7f8e\u5143\u548c30\u5355\u4f4d\u3002\nGround Truth: (C)10\u7f8e\u5143\u548c30\u5355\u4f4d \n<\u56fe\u7247 1>\nFigure B76: A sample correct case of Business (subcategory: Economics).\nBack to List of figures\n95\nPREPRINT\n\u5546\u4e1a: \u7ecf\u6d4e\nQuestion:\u5982\u56fe<\u56fe\u7247 1>\u6d1b\u4f26\u5179\u66f2\u7ebf\u53cd\u6620\u4e86\u6536\u5165\u5206\u914d\u7684\u4e0d\u5e73\u7b49\u7a0b\u5ea6\uff0c\u5f2f\u66f2\u7a0b\u5ea6\u4e0e\u6536\u5165\u5206\u914d\u4e0d\u5e73\u7b49\u6210\uff08\uff09\n\u6bd4\u3002 \nOption: \n(A) \u6b63\n(B) \u53cd\n(C) \u4e0d\u76f8\u5173\n(D) \u4ee5\u4e0a\u90fd\u5bf9\nGPT-4V:\u6d1b\u4f26\u5179\u66f2\u7ebf\u662f\u4e00\u4e2a\u7528\u6765\u8868\u793a\u6536\u5165\u6216\u8d22\u5bcc\u5206\u914d\u5e73\u7b49\u7a0b\u5ea6\u7684\u56fe\u5f62\u3002\u5728\u8fd9\u4e2a\u66f2\u7ebf\u4e0a\uff0c\u5b8c\u5168\u5e73\u7b49\u7684\u5206\n\u914d\u4f1a\u88ab\u8868\u793a\u4e3a\u4e00\u6761\u4ece\u539f\u70b9\uff080,0\uff09\u5f00\u59cb\u5230\uff08100%, 100%\uff09\u7684\u76f4\u7ebf\uff0c\u8fd9\u6761\u76f4\u7ebf\u88ab\u79f0\u4e3a\u201c\u5e73\u7b49\u7ebf\u201d\u3002\u6d1b\u4f26\n\u5179\u66f2\u7ebf\u8d8a\u662f\u5f2f\u66f2\uff0c\u8868\u793a\u6536\u5165\u5206\u914d\u7684\u4e0d\u5e73\u7b49\u7a0b\u5ea6\u8d8a\u9ad8\u3002\u56e0\u6b64\uff0c\u6d1b\u4f26\u5179\u66f2\u7ebf\u7684\u5f2f\u66f2\u7a0b\u5ea6\u4e0e\u6536\u5165\u5206\u914d\u4e0d\u5e73\u7b49\n\u6210\u6b63\u6bd4\u3002\n\u6b63\u786e\u7b54\u6848\u662f\uff1a\n(A) \u6b63\nGround Truth: (A)\u6b63 \n<\u56fe\u7247 1>\nFigure B77: A sample correct case of Business (subcategory: Economics).\nBack to List of figures\n96\nPREPRINT\n\u5546\u4e1a: \u7ecf\u6d4e\nQuestion:\u8bf7<\u56fe\u7247 1>\u662f\u4e2d\u56fd\u5b66\u8005\u6700\u65e9\u521b\u9020\u6027\u5021\u5bfc\u7684__\u7406\u8bba\u3002\nOption: \n(A) \u533a\u57df\u7ecf\u6d4e\u6210\u957f\u4e00\u9636\u6bb5\n(B) \u533a\u57df\u7ecf\u6d4e\u6210\u957f\u4e8c\u9636\u6bb5\n(C) \u533a\u57df\u7ecf\u6d4e\u6210\u957f\u4e09\u9636\u6bb5\n(D) \u533a\u57df\u7ecf\u6d4e\u6210\u957f\u56db\u9636\u6bb5\nGPT-4V:\u8fd9\u5f20\u56fe\u7247\u663e\u793a\u7684\u662f\u4e00\u4e2a\u5173\u4e8e\u533a\u57df\u7ecf\u6d4e\u6210\u957f\u7684\u7406\u8bba\u6a21\u578b\u56fe\uff0c\u56fe\u4e2d\u6807\u6ce8\u4e86\u56db\u4e2a\u9636\u6bb5\uff1aI - \u4ea7\u4e1a\u7ed3\n\u6784\u9ad8\u5ea6\u96c6\u4e2d\uff0cII - \u4ea7\u4e1a\u7ed3\u6784\u5206\u6563\uff0cIII - \u4ea7\u4e1a\u7ed3\u6784\u518d\u96c6\u4e2d\uff0cIV - \u4ea7\u4e1a\u7ed3\u6784\u9ad8\u5ea6\u5206\u6563\u3002\u6839\u636e\u8fd9\u4e2a\u4fe1\u606f\uff0c\n\u53ef\u4ee5\u63a8\u65ad\u8fd9\u662f\u4e00\u4e2a\u63cf\u8ff0\u533a\u57df\u7ecf\u6d4e\u6210\u957f\u56db\u4e2a\u9636\u6bb5\u7684\u7406\u8bba\u3002\u56e0\u6b64\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a\n(D) \u533a\u57df\u7ecf\u6d4e\u6210\u957f\u56db\u9636\u6bb5\nGround Truth: (D)\u533a\u57df\u7ecf\u6d4e\u6210\u957f\u56db\u9636\u6bb5 \n<\u56fe\u7247 1>\nFigure B78: A sample correct case of Business (subcategory: Economics).\nBack to List of figures\n97\nPREPRINT\n\u5546\u4e1a: \u7ecf\u6d4e\nQuestion:\u5982\u56fe<\u56fe\u7247 1>\u6240\u793a\uff0c\u5382\u5546\u7684\u7406\u6027\u51b3\u7b56\u5e94\u5728\uff08\uff09\nOption: \n(A) 3\u2264L\u22648\n(B) 3\u2264L\u22645\n(C) 0\u2264L\u22645\n(D) 5\u2264L\u22648\nGPT-4V:\u8fd9\u5f20\u56fe\u663e\u793a\u4e86\u5e73\u5747\u4ea7\u54c1\uff08AP\uff09\u548c\u8fb9\u9645\u4ea7\u54c1\uff08MP\uff09\u968f\u52b3\u52a8\u529b\uff08L\uff09\u7684\u53d8\u5316\u60c5\u51b5\u3002\u7406\u6027\u7684\u5382\u5546\u51b3\n\u7b56\u901a\u5e38\u662f\u5728\u8fb9\u9645\u4ea7\u54c1\uff08MP\uff09\u5f00\u59cb\u4e0b\u964d\u4e4b\u524d\u6216\u5f53\u5b83\u7b49\u4e8e\u5e73\u5747\u4ea7\u54c1\uff08AP\uff09\u65f6\uff0c\u56e0\u4e3a\u8fd9\u4ee3\u8868\u4e86\u751f\u4ea7\u6548\u7387\u7684\u6700\n\u5927\u5316\u3002\u5728\u56fe\u4e2d\uff0cMP\u66f2\u7ebf\u5728L=3\u65f6\u5f00\u59cb\u4e0b\u964d\uff0c\u5e76\u4e14\u5728L=5\u65f6\u4e0eAP\u66f2\u7ebf\u76f8\u4ea4\u3002\u56e0\u6b64\uff0c\u7406\u6027\u7684\u51b3\u7b56\u5e94\u8be5\u662f\u5728MP\n\u5f00\u59cb\u4e0b\u964d\u4e4b\u524d\uff0c\u5373\u5728L=3\u65f6\uff0c\u76f4\u5230MP\u7b49\u4e8eAP\uff0c\u5373L=5\u65f6\u3002\u6240\u4ee5\u6b63\u786e\u7684\u9009\u9879\u662f\uff1a\n(B) 3\u2264L\u22645\nGround Truth: (D)5\u2264L\u22648 \n<\u56fe\u7247 1>\nError Category:Reject to Answer\nError Reason:In economics, firms produce in the second stage, which in the diagram refers to L=5 \nto L=8. Therefore, the correct answer is D. The main reason GPT-4V got this wrong is due to a Lack \nof Knowledge.\nFigure B79: A sample error case of Business (subcategory: Economics).\nBack to List of figures\n98\nPREPRINT\n\u5546\u4e1a: \u7ecf\u6d4e\nQuestion:\u5728<\u56fe\u7247 1>\u6240\u793a\u7684\u535a\u5f08\u4e2d\uff0c\u7eb3\u4ec0\u5747\u8861\u662f ( \uff09\nOption: \n(A) \u4e0a\uff0c\u5de6\n(B) \u4e0a\uff0c\u53f3\n(C) \u4e0b\uff0c\u5de6\n(D) \u4e0b\uff0c\u53f3\nGPT-4V:\u5728\u8fd9\u4e2a\u535a\u5f08\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u68c0\u67e5\u6bcf\u4e2a\u73a9\u5bb6\u5728\u5bf9\u65b9\u7b56\u7565\u7ed9\u5b9a\u7684\u60c5\u51b5\u4e0b\u7684\u6700\u4f73\u54cd\u5e94\u6765\u627e\u5230\u7eb3\u4ec0\n\u5747\u8861\u3002\n\u9996\u5148\uff0c\u6211\u4eec\u770b\u73a9\u5bb6A\uff08\u884c\u73a9\u5bb6\uff09\u7684\u9009\u62e9\u3002\u5982\u679c\u73a9\u5bb6B\u9009\u62e9\u201c\u5de6\u201d\uff0c\u73a9\u5bb6A\u4f1a\u9009\u62e9\u201c\u4e0b\u201d\uff08\u56e0\u4e3a4\u5927\u4e8e1\uff09\u3002\n\u5982\u679c\u73a9\u5bb6B\u9009\u62e9\u201c\u53f3\u201d\uff0c\u73a9\u5bb6A\u540c\u6837\u4f1a\u9009\u62e9\u201c\u4e0b\u201d\uff08\u56e0\u4e3a7\u5927\u4e8e3\uff09\u3002\n\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u770b\u73a9\u5bb6B\uff08\u5217\u73a9\u5bb6\uff09\u7684\u9009\u62e9\u3002\u5982\u679c\u73a9\u5bb6A\u9009\u62e9\u201c\u4e0a\u201d\uff0c\u73a9\u5bb6B\u4f1a\u9009\u62e9\u201c\u53f3\u201d\uff08\u56e0\u4e3a2\u5927\u4e8e\n0\uff09\u3002\u5982\u679c\u73a9\u5bb6A\u9009\u62e9\u201c\u4e0b\u201d\uff0c\u73a9\u5bb6B\u4f1a\u9009\u62e9\u201c\u53f3\u201d\uff08\u56e0\u4e3a4\u5927\u4e8e5\uff09\u3002\n\u7ed3\u5408\u8fd9\u4e24\u4e2a\u73a9\u5bb6\u7684\u6700\u4f73\u54cd\u5e94\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u7eb3\u4ec0\u5747\u8861\u53d1\u751f\u5728\u73a9\u5bb6A\u9009\u62e9\u201c\u4e0b\u201d\uff0c\u73a9\u5bb6B\u9009\u62e9\u201c\u53f3\u201d\u7684\u60c5\n\u51b5\u4e0b\u3002\u56e0\u6b64\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a\n(D) (\u4e0b\uff0c\u53f3)\nGround Truth: (C)\u4e0b\uff0c\u5de6 \n<\u56fe\u7247 1>\nError Category:Reasoning Error \nError Reason:When player A chooses \"Down\" and player B chooses \"Right,\" GPT-4V considers this \na Nash equilibrium. However, in this situation, if player B changes their choice to \"Left,\" they will \ngain a higher payoff. Therefore, the combination of \"Down, Right\" is not a Nash equilibrium. GPT-\n4V made a Reasoning Error in this case.\nFigure B80: A sample error case of Business (subcategory: Economics).\nBack to List of figures\n99\nPREPRINT\n\u5546\u4e1a: \u7ecf\u6d4e\nQuestion:\u8003\u8651\u5728\u53cc\u5934\u5784\u65ad\u5e02\u573a\u4e0a\u5b58\u5728\u4e24\u4e2a\u4f01\u4e1a\u751f\u4ea7\u540c\u7c7b\u4ea7\u54c1\u3002\u4e3a\u4e86\u5728\u7ade\u4e89\u4e2d\u5360\u5f97\u5148\u673a\uff0c\u53cc\u65b9\u90fd\u8003\u8651\n\u662f\u5426\u91c7\u53d6\u964d\u4ef7\u7684\u7b56\u7565\u3002\u4ed6\u4eec\u51b3\u7b56\u7684\u4e0d\u540c\u7ec4\u5408\uff0c\u5bfc\u81f4\u6536\u76ca\u77e9\u9635\u5982<\u56fe\u7247 1>\u6240\u793a\u3002\u5047\u8bbe\u4e24\u5bb6\u4f01\u4e1a\u540c\u65f6\u51b3\u7b56\uff0c\n\u90a3\u4e48\u8be5\u535a\u5f08\u7684\u7eb3\u4ec0\u5747\u8861\u662f_____\u3002\nOption: \n(A) \u4f01\u4e1a1\u964d\u4ef7\uff0c\u4f01\u4e1a2\u4e0d\u964d\u4ef7\n(B) \u4f01\u4e1a1\u4e0d\u964d\u4ef7\uff0c\u4f01\u4e1a2\u964d\u4ef7\n(C) \u4f01\u4e1a1\u964d\u4ef7\uff0c\u4f01\u4e1a2\u964d\u4ef7\n(D) \u4f01\u4e1a1\u4e0d\u964d\u4ef7\uff0c\u4f01\u4e1a2\u4e0d\u964d\u4ef7\nGPT-4V:\u5f88\u5728\u8fd9\u4e2a\u6536\u76ca\u77e9\u9635\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u4f01\u4e1a 1 \u548c\u4f01\u4e1a 2 \u7684\u6536\u76ca\u53d6\u51b3\u4e8e\u4ed6\u4eec\u662f\u5426\u9009\u62e9\u964d\u4ef7\u3002\u4e3a\n\u4e86\u627e\u5230\u7eb3\u4ec0\u5747\u8861\uff0c\u6211\u4eec\u9700\u8981\u627e\u5230\u8fd9\u6837\u4e00\u79cd\u7b56\u7565\u7ec4\u5408\uff0c\u5373\u5728\u7ed9\u5b9a\u5bf9\u65b9\u7b56\u7565\u7684\u60c5\u51b5\u4e0b\uff0c\u4efb\u4f55\u4e00\u65b9\u6539\u53d8\u81ea\u5df1\n\u7684\u7b56\u7565\u90fd\u4e0d\u4f1a\u4f7f\u81ea\u5df1\u66f4\u597d\u3002\n\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u68c0\u67e5\u6bcf\u4e2a\u4f01\u4e1a\u5728\u5bf9\u65b9\u9009\u62e9\u4e0d\u540c\u7b56\u7565\u65f6\u7684\u6700\u4f73\u53cd\u5e94\u6765\u786e\u5b9a\u7eb3\u4ec0\u5747\u8861\uff1a\n- \u5f53\u4f01\u4e1a 2 \u9009\u62e9\u964d\u4ef7\u65f6\uff0c\u4f01\u4e1a 1 \u7684\u6700\u4f73\u53cd\u5e94\u662f\u964d\u4ef7\uff0818 > 4\uff09\u3002\n- \u5f53\u4f01\u4e1a 2 \u9009\u62e9\u4e0d\u964d\u4ef7\u65f6\uff0c\u4f01\u4e1a 1 \u7684\u6700\u4f73\u53cd\u5e94\u662f\u4e0d\u964d\u4ef7\uff0823 > 16\uff09\u3002\n- \u5f53\u4f01\u4e1a 1 \u9009\u62e9\u964d\u4ef7\u65f6\uff0c\u4f01\u4e1a 2 \u7684\u6700\u4f73\u53cd\u5e94\u662f\u964d\u4ef7\uff0818 > 4\uff09\u3002\n- \u5f53\u4f01\u4e1a 1 \u9009\u62e9\u4e0d\u964d\u4ef7\u65f6\uff0c\u4f01\u4e1a 2 \u7684\u6700\u4f73\u53cd\u5e94\u662f\u4e0d\u964d\u4ef7\uff0820 > 16\uff09\u3002\n\u56e0\u6b64\uff0c\u7eb3\u4ec0\u5747\u8861\u53d1\u751f\u5728\u4e24\u4e2a\u4f01\u4e1a\u90fd\u9009\u62e9\u964d\u4ef7\u7684\u60c5\u51b5\u4e0b\uff0c\u56e0\u4e3a\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u6ca1\u6709\u4e00\u65b9\u6709\u6fc0\u52b1\u5355\u65b9\u9762\u6539\n\u53d8\u7b56\u7565\u3002\n\u6b63\u786e\u7b54\u6848\u662f\uff1a\n(C) \u4f01\u4e1a 1 \u964d\u4ef7\uff0c\u4f01\u4e1a 2 \u964d\u4ef7\nGround Truth: (A)\u4f01\u4e1a1\u964d\u4ef7\uff0c\u4f01\u4e1a2\u4e0d\u964d\u4ef7 \n<\u56fe\u7247 1>\nError Category:Reasoning Error \nError Reason:GPT-4V has already listed the best responses for each firm when the other chooses \ndifferent strategies, and knows that a Nash equilibrium occurs when no party has the incentive to \nunilaterally change their strategy. Up to this point, there are no issues. However, the error occurred \nwhen reasoning which situation belongs to \"no party having the incentive to unilaterally change \ntheir strategy.\" When both firms 1 and 2 reduce prices, firm 2's change in strategy actually leads to a \ngreater profit. Therefore, the Nash equilibrium does not occur in option A but rather in option C.\nFigure B81: A sample error case of Business (subcategory: Economics).\nBack to List of figures\n100\nPREPRINT\n\u5546\u4e1a: \u7ecf\u6d4e\nQuestion:\u5982\u56fe<\u56fe\u7247 1>\u5fae\u7b11\u66f2\u7ebf\u4e2d\uff0c\u5904\u4e8e\u4e2d\u95f4\u73af\u8282\u7684\u5236\u9020\u9644\u52a0\u503c___\u3002\nOption: \n(A) \u6700\u9ad8\n(B) \u6700\u4f4e\n(C) \u8f83\u9ad8\n(D) \u8f83\u4f4e\nGPT-4V:\u6839\u636e\u6240\u63d0\u4f9b\u7684\u56fe\u7247\uff0c\u5fae\u7b11\u66f2\u7ebf\u5c55\u793a\u4e86\u4e0d\u540c\u73af\u8282\u7684\u9644\u52a0\u503c\u53d8\u5316\u3002\u5728\u8fd9\u4e2a\u66f2\u7ebf\u4e2d\uff0c\u5236\u9020\u73af\u8282\u4f4d\u4e8e\n\u4e2d\u95f4\u7684\u4f4e\u8c37\u4f4d\u7f6e\uff0c\u8fd9\u8868\u793a\u5236\u9020\u73af\u8282\u7684\u9644\u52a0\u503c\u76f8\u5bf9\u8f83\u4f4e\u3002\u56e0\u6b64\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a\n(D) \u8f83\u4f4e\nGround Truth: (B)\u6700\u4f4e \n<\u56fe\u7247 1>\nError Category:Textual Understanding \nError Reason:GPT-4V correctly understood the image and identified the key point that 'the \nmanufacturing link is located in the middle valley position', but it failed to correctly differentiate \nbetween 'lowest' and 'relatively low' in the question options. It mistook the globally lowest position \nfor being relatively low, which led to GPT-4V answering this question incorrectly.\nFigure B82: A sample error case of Business (subcategory: Economics).\nBack to List of figures\n101\nPREPRINT\n\u5546\u4e1a: \u7ecf\u6d4e\nQuestion:\u56fe<\u56fe\u7247 1>\u6240\u793a\u5f27\u5f39\u6027\u4e3a_\nOption: \n(A) \u65e0\u9650\u5f39\u6027\n(B) \u5bcc\u6709\u5f39\u6027\n(C) \u5355\u4f4d\u5f39\u6027\n(D) \u7f3a\u4e4f\u5f39\u6027\nGPT-4V:\u8fd9\u5f20\u56fe\u7247\u663e\u793a\u7684\u662f\u4e00\u4e2a\u5b8c\u5168\u65e0\u5f39\u6027\u7684\u9700\u6c42\u66f2\u7ebf\uff0c\u56e0\u4e3a\u65e0\u8bba\u6570\u91cf(Q)\u5982\u4f55\u53d8\u5316\uff0c\u4ef7\u683c(P)\u4fdd\u6301\u4e0d\n\u53d8\u3002\u56e0\u6b64\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a\n(D) \u7f3a\u4e4f\u5f39\u6027\nGround Truth: (A)\u65e0\u9650\u5f39\u6027 \n<\u56fe\u7247 1>\nError Category:Lack of Knowledge \nError Reason:Lack of elasticity refers to the sensitivity of the response to price changes. When the \nprice changes, the amount of change in demand is always zero, which is infinite elasticity. \nTherefore, the incorrect answer by GPT-4V is due to the lack of this knowledge point.\nFigure B83: A sample error case of Business (subcategory: Economics).\nBack to List of figures\n102\nPREPRINT\n\u5546\u4e1a: \u7ecf\u6d4e\nQuestion:\u5047\u8bbe\u67d0\u6d88\u8d39\u8005\u7684\u5747\u8861\u5982<\u56fe\u7247 1>\u6240\u793a\u3002\u5df2\u77e5\u5546\u54c11\u7684\u4ef7\u683cP1=2\u5143\u3002\u5219\u6d88\u8d39\u8005\u7684\u6536\u5165\u4e3a\uff08\uff09\u3002\nOption: \n(A) 50\n(B) 40\n(C) 70\n(D) 60\nGPT-4V:\u8fd9\u5f20\u56fe\u7247\u663e\u793a\u4e86\u4e00\u4e2a\u6d88\u8d39\u8005\u7684\u5747\u8861\u70b9\u5728\u4e24\u79cd\u5546\u54c1\uff08X1 \u548c X2\uff09\u7684\u6d88\u8d39\u7ec4\u5408\u4e0a\u3002\u5747\u8861\u70b9\u5728\u9884\u7b97\n\u7ebf\u4e0a\uff0c\u9884\u7b97\u7ebf\u7684\u65b9\u7a0b\u662f I=P1X1+P2X2\uff0c\u5176\u4e2d I \u662f\u6536\u5165\uff0cP1 \u662f\u5546\u54c11\u7684\u4ef7\u683c\uff0cP2 \u662f\u5546\u54c12\u7684\u4ef7\u683c\uff0cX1\n\u548c X2 \u5206\u522b\u662f\u5546\u54c11\u548c\u5546\u54c12\u7684\u6570\u91cf\u3002\n\u6839\u636e\u56fe\u4e2d\u7684\u4fe1\u606f\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u6d88\u8d39\u8005\u9009\u62e9\u4e8615\u5355\u4f4d\u7684\u5546\u54c11\uff08X1=15\uff09\u3002\u5df2\u77e5\u5546\u54c11\u7684\u4ef7\u683cP1\u662f2\u5143\uff0c\n\u6211\u4eec\u53ef\u4ee5\u8ba1\u7b97\u51fa\u6d88\u8d39\u8005\u5728\u5546\u54c11\u4e0a\u7684\u652f\u51fa\u662f 2\u5143/\u5355\u4f4d * 15\u5355\u4f4d = 30\u5143\u3002\n\u7136\u800c\uff0c\u6211\u4eec\u6ca1\u6709\u5546\u54c12\u7684\u4ef7\u683cP2\uff0c\u56e0\u6b64\u4e0d\u80fd\u76f4\u63a5\u8ba1\u7b97\u51fa\u6d88\u8d39\u8005\u7684\u603b\u6536\u5165\u3002\u4f46\u662f\uff0c\u6211\u4eec\u53ef\u4ee5\u89c2\u5bdf\u5230\u9884\u7b97\n\u7ebf\u5728X2\u8f74\u4e0a\u7684\u622a\u8ddd\u3002\u8fd9\u4e2a\u622a\u8ddd\u4ee3\u8868\u4e86\u5982\u679c\u6d88\u8d39\u8005\u5c06\u6240\u6709\u6536\u5165\u90fd\u7528\u4e8e\u8d2d\u4e70\u5546\u54c12\u65f6\u80fd\u591f\u8d2d\u4e70\u7684\u6700\u5927\u6570\u91cf\u3002\n\u4ece\u56fe\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0c\u8fd9\u4e2a\u622a\u8ddd\u662f20\u5355\u4f4d\u7684X2\u3002\n\u5982\u679c\u6211\u4eec\u5047\u8bbe\u6d88\u8d39\u8005\u5c06\u6240\u6709\u6536\u5165\u90fd\u7528\u4e8e\u8d2d\u4e70\u5546\u54c12\uff0c\u90a3\u4e48\u6211\u4eec\u53ef\u4ee5\u7528\u603b\u6536\u5165\u9664\u4ee5\u5546\u54c12\u7684\u6570\u91cf\u6765\u5f97\u5230\u5546\n\u54c12\u7684\u4ef7\u683cP2\u3002\u4f46\u662f\uff0c\u7531\u4e8e\u6211\u4eec\u6ca1\u6709\u5546\u54c12\u7684\u4ef7\u683c\uff0c\u6211\u4eec\u65e0\u6cd5\u4f7f\u7528\u8fd9\u79cd\u65b9\u6cd5\u6765\u786e\u5b9a\u786e\u5207\u7684\u6536\u5165\u3002\n\u4e0d\u8fc7\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u56fe\u4e2d\u7684\u4fe1\u606f\u6765\u4f30\u8ba1\u6536\u5165\u3002\u7531\u4e8e\u6d88\u8d39\u8005\u5728\u5546\u54c11\u4e0a\u82b1\u8d39\u4e8630\u5143\uff0c\u5982\u679c\u4ed6\u4eec\u5728\u5546\u54c12\u4e0a\n\u4e5f\u82b1\u8d39\u4e8630\u5143\uff0c\u90a3\u4e48\u603b\u6536\u5165\u5c06\u662f60\u5143\u3002\u4f46\u662f\uff0c\u5982\u679c\u5546\u54c12\u7684\u4ef7\u683c\u4f4e\u4e8e2\u5143\uff0c\u90a3\u4e48\u6d88\u8d39\u8005\u5728\u5546\u54c12\u4e0a\u7684\u82b1\n\u8d39\u5c06\u5c11\u4e8e30\u5143\uff0c\u603b\u6536\u5165\u5c06\u4f4e\u4e8e60\u5143\u3002\u540c\u6837\uff0c\u5982\u679c\u5546\u54c12\u7684\u4ef7\u683c\u9ad8\u4e8e2\u5143\uff0c\u603b\u6536\u5165\u5c06\u9ad8\u4e8e60\u5143\u3002\n\u7531\u4e8e\u6ca1\u6709\u8db3\u591f\u7684\u4fe1\u606f\u6765\u786e\u5b9a\u5546\u54c12\u7684\u4ef7\u683c\uff0c\u6211\u4eec\u65e0\u6cd5\u51c6\u786e\u8ba1\u7b97\u51fa\u603b\u6536\u5165\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u65e0\u6cd5\u4ece\u63d0\u4f9b\u7684\u9009\n\u9879\u4e2d\u786e\u5b9a\u4e00\u4e2a\u660e\u786e\u7684\u7b54\u6848\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u53ef\u4ee5\u9009\u62e9\u4e00\u4e2a\u53ef\u80fd\u7684\u7b54\u6848\uff0c\u4f46\u8bf7\u6ce8\u610f\u8fd9\u53ea\u662f\u4e00\u4e2a\u57fa\u4e8e\n\u53ef\u7528\u4fe1\u606f\u7684\u731c\u6d4b\u3002\n\u57fa\u4e8e\u4e0a\u8ff0\u5206\u6790\uff0c\u6b63\u786e\u7b54\u6848\u662f\u65e0\u6cd5\u786e\u5b9a\uff0c\u56e0\u4e3a\u6211\u4eec\u6ca1\u6709\u5546\u54c12\u7684\u4ef7\u683c\u4fe1\u606f\u3002\nGround Truth: (C)70 \n<\u56fe\u7247 1>\nError Category:Lack of Knowledge \nError Reason:GPT-4V made some deductions in the early stage, but the final conclusion failed to \nyield the correct answer. This is due to GPT-4V's lack of specific problem-solving knowledge. \nTherefore, the reason for GPT-4V's error here is 'Lack of Knowledge'.\nFigure B84: A sample error case of Business (subcategory: Economics).\nBack to List of figures\n103\nPREPRINT\n\u5546\u4e1a: \u91d1\u878d\nQuestion:\u67d0\u80a1\u7968\u6536\u76ca\u5206\u5e03\u7684\u53ef\u80fd\u60c5\u51b5\u5982\u4e0b\u8868<\u56fe\u7247 1>\uff0c\u8bd5\u8ba1\u7b97\u8be5\u80a1\u7968\u7684\u5e74\u9884\u671f\u6536\u76ca\u7387( )\nOption: \n(A) 0.20 \n(B) 0.24 \n(C) 0.21 \n(D) 0.18\nGPT-4V: \n\u4e3a\u4e86\u8ba1\u7b97\u80a1\u7968\u7684\u5e74\u9884\u671f\u6536\u76ca\u7387\uff0c\u6211\u4eec\u9700\u8981\u5c06\u6bcf\u79cd\u53ef\u80fd\u60c5\u51b5\u7684\u6536\u76ca\u7387\u4e0e\u5176\u76f8\u5e94\u7684\u6982\u7387\u76f8\u4e58\uff0c\n\u7136\u540e\u5c06\u8fd9\u4e9b\u4e58\u79ef\u76f8\u52a0\u3002\u6839\u636e\u63d0\u4f9b\u7684\u8868\u683c\uff0c\u6211\u4eec\u53ef\u4ee5\u8fdb\u884c\u4ee5\u4e0b\u8ba1\u7b97\uff1a \u9884\u671f\u6536\u76ca\u7387 = (A\u7684\u6536\u76ca\u7387 * A\u7684\n\u6982\u7387) + (B\u7684\u6536\u76ca\u7387 * B\u7684\u6982\u7387) + (C\u7684\u6536\u76ca\u7387 * C\u7684\u6982\u7387) \u9884\u671f\u6536\u76ca\u7387 = (0.4 * 0.25) + (0.4 * \n0.20) + (0.2 * 0.15) \u9884\u671f\u6536\u76ca\u7387 = (0.1) + (0.08) + (0.03) \u9884\u671f\u6536\u76ca\u7387 = 0.21\n\u6b63\u786e\u7b54\u6848\uff1a(C) 0.21\u3002\nGround Truth: (C) 0.21 \n<\u56fe\u7247 1>\nFigure B85: A sample correct case of Business (subcategory: Finance).\nBack to List of figures\n104\nPREPRINT\n\u5546\u4e1a: \u91d1\u878d\nQuestion:\u6839\u636e\u4e45\u671f\u53d8\u5316\u56fe<\u56fe\u7247 \n1>\u53ef\u4ee5\u770b\u51fa\u6536\u76ca\u7387\u540c\u7b49\u53d8\u5316\u5e45\u5ea6\u4e0b\uff0c\u503a\u5238\u4ef7\u683c\u589e\u52a0\u7684\u5e45\u5ea6\u8981\u8d85\u8fc7\u503a\n\u6743\u51cf\u5c11\u7684\u5e45\u5ea6\uff0c\u8be5\u503a\u5238\u4ef7\u683c\u6ce2\u52a8\u7b26\u5408__\u7279\u5f81\nOption: \n(A) \u53cd\u51f8\u6027\n(B) \u6b63\u51f8\u6027 \n(C) \u6781\u5927\u503c \n(D) \u6781\u5c0f\u503c\nGPT-4V: \n\u8fd9\u5f20\u56fe\u663e\u793a\u4e86\u503a\u5238\u4ef7\u683c\u4e0e\u6536\u76ca\u7387\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u4ece\u56fe\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0c\u5f53\u6536\u76ca\u7387\u4e0b\u964d\u65f6\uff08\u5373\u4ecey*\n\u5411\u5de6\u79fb\u52a8\u5230y1\uff09\uff0c\u503a\u5238\u4ef7\u683cP\u4e0a\u5347\u7684\u5e45\u5ea6\u6bd4\u6536\u76ca\u7387\u4e0a\u5347\u65f6\uff08\u5373\u4ecey*\u5411\u53f3\u79fb\u52a8\u5230y2\uff09\u503a\u5238\u4ef7\u683c\u4e0b\u964d\u7684\u5e45\n\u5ea6\u8981\u5927\u3002\u8fd9\u79cd\u4ef7\u683c\u53d8\u52a8\u7684\u975e\u5bf9\u79f0\u6027\u8868\u73b0\u51fa\u503a\u5238\u4ef7\u683c\u5bf9\u4e8e\u5229\u7387\u53d8\u52a8\u7684\u6b63\u51f8\u6027\uff08convexity\uff09\u7279\u5f81\u3002\n\u6b63\u786e\u7b54\u6848\uff1a (B) \u6b63\u51f8\u6027\nGround Truth: (B) \u6b63\u51f8\u6027 \n<\u56fe\u7247 1>\nFigure B86: A sample correct case of Business (subcategory: Finance).\nBack to List of figures\n105\nPREPRINT\n\u5546\u4e1a: \u91d1\u878d\nQuestion:<\u56fe\u7247 1>\u5728\u503a\u5238\u6536\u76ca\u7387\u7ee7\u7eed\u4e0a\u5347\uff0c\u503a\u5238\u4ef7\u683c\u4e0b\u964d\u65f6\uff0c\u9ec4\u8272\u865a\u7ebf\u90e8\u5206\u4ee3\u8868\u7684\u542b\u6743\u503a\u5238\u4ef7\u683c\u964d\n\u5e45\u5c0f\u4e8e\u4e0d\u542b\u6743\u503a\u5238\uff0c\u5e76\u4e14\u56de\u552e\u4ef7\u683c\u5c31\u662f\u4e0b\u964d\u7684\u6700\u5e95\u7ebf\uff0c\u5f62\u6210\u4e86\u4e00\u4e2a__\u4fdd\u62a4\nOption: \n(A) \u5730\u677f\u4ef7\u683c\n(B) \u5e73\u5747\u4ef7\u683c\n(C) \u6700\u9ad8\u4ef7\u683c\n(D) \u4ee5\u4e0a\u7686\u975e\nGPT-4V: \u6839\u636e\u6240\u63d0\u4f9b\u7684\u56fe\u7247\u548c\u63cf\u8ff0\uff0c\u9ec4\u8272\u865a\u7ebf\u4ee3\u8868\u7684\u542b\u6743\u503a\u5238\uff08Putable bond\uff09\u4ef7\u683c\u964d\u5e45\u5c0f\u4e8e\u4e0d\u542b\n\u6743\u503a\u5238\uff08Non putable bond\uff09\uff0c\u5e76\u4e14\u56de\u552e\u4ef7\u683c\uff08Put price\uff09\u5c31\u662f\u4e0b\u964d\u7684\u6700\u5e95\u7ebf\uff0c\u8fd9\u5f62\u6210\u4e86\u4e00\u4e2a\u4ef7\u683c\n\u4fdd\u62a4\u3002\u5728\u91d1\u878d\u672f\u8bed\u4e2d\uff0c\u8fd9\u79cd\u4ef7\u683c\u4fdd\u62a4\u901a\u5e38\u88ab\u79f0\u4e3a\u201c\u5730\u677f\u4ef7\u683c\u201d\uff08Floor price\uff09\uff0c\u56e0\u4e3a\u5b83\u4e3a\u503a\u5238\u4ef7\u683c\n\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6700\u4f4e\u9650\u5ea6\u3002\n\u6b63\u786e\u7b54\u6848\uff1a (A) \u5730\u677f\u4ef7\u683c\nGround Truth: (A) \u5730\u677f\u4ef7\u683c\n<\u56fe\u7247 1>\nFigure B87: A sample correct case of Business (subcategory: Finance).\nBack to List of figures\n106\nPREPRINT\n\u5546\u4e1a: \u91d1\u878d\nQuestion: \u5df2\u77e5\u67d0\u5e74\u5ea6\u975e\u91d1\u878d\u4f01\u4e1a\u90e8\u95e8\u7684\u5b9e\u7269\u6295\u8d44\u4e3a11849\uff0e8\u4ebf\u5143\uff0c\u5f53\u5e74\u8be5\u90e8\u95e8\u7684\u53ef\u652f\u914d\u6536\u5165\u4e3a\n6324\uff0e1\u4ebf\u5143\u3002\u8bf7\u6839\u636e\u4e0b\u8868<\u56fe\u7247 1>\u7ed9\u51fa\u7684\u975e\u91d1\u878d\u4f01\u4e1a\u90e8\u95e8\u7684\u8d44\u91d1\u6d41\u91cf\u6570\u636e\uff0c\u8bc1\u5238\u5e02\u573a\u5bf9\u4fc3\u8fdb\u4f01\u4e1a\u878d\n\u8d44\u7684\u4f5c\u7528\u975e\u5e38\u6709\u9650\uff0c\u53ea\u5360_%\u3002\nGPT-4V:\u4e3a\u4e86\u8ba1\u7b97\u8bc1\u5238\u5e02\u573a\u5728\u975e\u91d1\u878d\u4f01\u4e1a\u90e8\u95e8\u878d\u8d44\u4e2d\u7684\u6bd4\u4f8b\uff0c\u6211\u4eec\u9700\u8981\u67e5\u770b\u8868\u683c\u4e2d\u4e0e\u8bc1\u5238\u5e02\u573a\u76f8\u5173\u7684\n\u878d\u8d44\u6570\u636e\u3002\u4ece\u63d0\u4f9b\u7684\u56fe\u7247\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u201c\u4ece\u8bc1\u5238\u5e02\u573a\u878d\u8d44\u201d\u8fd9\u4e00\u9879\u7684\u6570\u636e\u3002\u7136\u800c\uff0c\u56fe\u7247\u4e2d\u5e76\u6ca1\u6709\n\u76f4\u63a5\u663e\u793a\u8fd9\u4e00\u6570\u636e\uff0c\u56e0\u6b64\u6211\u4eec\u65e0\u6cd5\u76f4\u63a5\u8ba1\u7b97\u51fa\u8bc1\u5238\u5e02\u573a\u5728\u975e\u91d1\u878d\u4f01\u4e1a\u90e8\u95e8\u878d\u8d44\u4e2d\u7684\u5177\u4f53\u6bd4\u4f8b\u3002 \u7531\u4e8e\n\u7f3a\u4e4f\u5177\u4f53\u7684\u8bc1\u5238\u5e02\u573a\u878d\u8d44\u6570\u636e\uff0c\u6211\u4eec\u65e0\u6cd5\u63d0\u4f9b\u4e00\u4e2a\u786e\u5207\u7684\u7b54\u6848\u3002\u5982\u679c\u6709\u66f4\u591a\u4fe1\u606f\u6216\u6570\u636e\u63d0\u4f9b\uff0c\u6211\u4eec\u53ef\n\u4ee5\u8fdb\u884c\u8ba1\u7b97\u3002\u5728\u76ee\u524d\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u53ea\u80fd\u8bf4\u8bc1\u5238\u5e02\u573a\u5bf9\u975e\u91d1\u878d\u4f01\u4e1a\u90e8\u95e8\u878d\u8d44\u7684\u4f5c\u7528\u7684\u786e\u5207\u6bd4\u4f8b\u672a\u77e5\u3002\nGround Truth: 0.48 \nError Category: Reject to Answer\nError Reason:GPT-4V believes that the image does not directly display the data on \"financing from \nthe securities market,\" making it impossible to directly calculate the specific proportion of financing \nfrom the securities market in the non-financial corporate sector. In reality, the \"net borrowing from \nthe securities market\" in the image is the key information needed for an answer. GPT-4V considers \nthat the lack of specific data prevents it from providing a response.\n<\u56fe\u7247 1>\nFigure B88: A sample error case of Business (subcategory: Finance).\nBack to List of figures\n107\nPREPRINT\n\u5546\u4e1a: \u91d1\u878d\nQuestion:\u67d0\u5382\u751f\u4ea7\u7684\u4e09\u79cd\u4ea7\u54c1\u7684\u6709\u5173\u8d44\u6599\u5982\u56fe<\u56fe\u7247 1>\uff0c\u4ea7\u91cf\u53d8\u5316\u4f7f\u603b\u6210\u672c\u51cf\u5c11___%\uff0c\u51cf\u5c11\u7684\u7edd\u5bf9\n\u91cf\u4e3a___\u5143\u3002\nOption: \n(A)  1.67\uff1b680 \n(B) 1.67\uff1b820 \n(C) 2.51\uff1b680 \n(D) 2.51\uff1b820\nGPT-4V:\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u9700\u8981\u8ba1\u7b97\u603b\u6210\u672c\u7684\u53d8\u5316\u3002\u9996\u5148\uff0c\u6211\u4eec\u6765\u8ba1\u7b97\u539f\u59cb\u7684\u603b\u6210\u672c\u548c\u53d8\u5316\u540e\n\u7684\u603b\u6210\u672c\u3002 \u539f\u59cb\u603b\u6210\u672c = \u5355\u4f4d\u6210\u672c * \u4ea7\u91cf \u53d8\u5316\u540e\u603b\u6210\u672c = (\u5355\u4f4d\u6210\u672c + \u5355\u4f4d\u6210\u672c\u53d8\u52a8) * (\u4ea7\u91cf\n+ \u4ea7\u91cf\u53d8\u52a8) \u4ea7\u54c1A\u7684\u539f\u59cb\u603b\u6210\u672c = 110\u5143/\u4ef6 * 600\u4ef6 = 66000\u5143 \u4ea7\u54c1A\u7684\u53d8\u5316\u540e\u603b\u6210\u672c = (110 + \n10)\u5143/\u4ef6 * (600 + 120)\u4ef6 = 120 * 720 = 86400\u5143 \u4ea7\u54c1B\u7684\u539f\u59cb\u603b\u6210\u672c = 120\u5143/\u4ef6 * 570\u4ef6 = \n68400\u5143 \u4ea7\u54c1B\u7684\u53d8\u5316\u540e\u603b\u6210\u672c = (120 + 60)\u5143/\u4ef6 * (570 + 180)\u4ef6 = 180 * 750 = 135000\u5143 \u4ea7\n\u54c1C\u7684\u539f\u59cb\u603b\u6210\u672c = 14\u5143/\u4ef6 * 50\u4ef6 = 700\u5143 \u4ea7\u54c1C\u7684\u53d8\u5316\u540e\u603b\u6210\u672c = (14 + 1)\u5143/\u4ef6 * (50 + 9)\n\u4ef6 = 15 * 59 = 885\u5143 \u539f\u59cb\u7684\u603b\u6210\u672c = 66000 + 68400 + 700 = 135100\u5143 \u53d8\u5316\u540e\u7684\u603b\u6210\u672c = \n86400 + 135000 + 885 = 224285\u5143 \u603b\u6210\u672c\u7684\u51cf\u5c11\u91cf = \u539f\u59cb\u603b\u6210\u672c - \u53d8\u5316\u540e\u603b\u6210\u672c \u603b\u6210\u672c\u7684\u51cf\u5c11\n\u91cf = 135100 - 224285 = -89185\u5143 \u56e0\u4e3a\u603b\u6210\u672c\u5b9e\u9645\u4e0a\u589e\u52a0\u4e86\uff0c\u6240\u4ee5\u6ca1\u6709\u51cf\u5c11\u7684\u7edd\u5bf9\u91cf\u3002\u8fd9\u610f\u5473\u7740\u6240\n\u6709\u9009\u9879\u90fd\u4e0d\u6b63\u786e\uff0c\u56e0\u4e3a\u5b83\u4eec\u90fd\u5047\u8bbe\u4e86\u6210\u672c\u7684\u51cf\u5c11\u3002\u7136\u800c\uff0c\u5982\u679c\u95ee\u9898\u662f\u5173\u4e8e\u6210\u672c\u7684\u589e\u52a0\uff0c\u6211\u4eec\u53ef\u4ee5\u8ba1\u7b97\n\u589e\u52a0\u7684\u767e\u5206\u6bd4\uff1a \u589e\u52a0\u7684\u767e\u5206\u6bd4 = (\u603b\u6210\u672c\u7684\u589e\u52a0\u91cf / \u539f\u59cb\u603b\u6210\u672c) * 100% \u589e\u52a0\u7684\u767e\u5206\u6bd4 = (89185 \n/ 135100) * 100% \u2248 66.01% \u4f46\u8fd9\u4e0d\u662f\u9898\u76ee\u8981\u6c42\u7684\uff0c\u6240\u4ee5\u6839\u636e\u9898\u76ee\u6240\u7ed9\u7684\u4fe1\u606f\u548c\u8ba1\u7b97\u7ed3\u679c\uff0c\u6ca1\u6709\u4e00\n\u4e2a\u9009\u9879\u662f\u6b63\u786e\u7684\u3002\nGround Truth: (D) 2.51\uff1b820\nError Category: Perceptual Error\nError Reason:GPT-4V provides the formula for calculating the total cost; however, it incorrectly \ninterprets the information in the chart. Taking Product A as an example: it mistakenly considers the \nbase period output as the unit cost and attributes the output of Product B as its own, resulting in a \nmisalignment of calculation data and consequently leading to errors in the computation.\n<\u56fe\u7247 1>\n<\u56fe\u7247 1>\nFigure B89: A sample error case of Business (subcategory: Finance).\nBack to List of figures\n108\nPREPRINT\n\u5546\u4e1a: \u91d1\u878d\nQuestion: \u67d0\u80a1\u7968\u7684\u6536\u76ca\u5206\u5e03\u7684\u53ef\u80fd\u60c5\u51b5\u5982\u4e0b\u8868<\u56fe\u7247 1>\uff0c\u8bd5\u8ba1\u7b97\u8be5\u80a1\u7968\u7684\u5e74\u9884\u671f\u6536\u76ca\u7387\u4e3a\uff08 \uff09\u3002\nOption: \n(A) 0.13 \n(B) 0.14 \n(C) 0.15 \n(D) 0.16\nGPT-4V:\u4e3a\u4e86\u8ba1\u7b97\u8be5\u80a1\u7968\u7684\u5e74\u9884\u671f\u6536\u76ca\u7387\uff0c\u6211\u4eec\u9700\u8981\u5c06\u6bcf\u79cd\u53ef\u80fd\u60c5\u51b5\u7684\u6536\u76ca\u7387\u4e58\u4ee5\u5176\u76f8\u5e94\u7684\u6982\u7387\uff0c\u7136\n\u540e\u5c06\u8fd9\u4e9b\u4e58\u79ef\u76f8\u52a0\u3002\u6839\u636e\u63d0\u4f9b\u7684\u8868\u683c\uff0c\u6211\u4eec\u53ef\u4ee5\u8fdb\u884c\u4ee5\u4e0b\u8ba1\u7b97\uff1a \u9884\u671f\u6536\u76ca\u7387 = (\u6982\u7387A * \u6536\u76ca\u7387A)\n+ (\u6982\u7387B * \u6536\u76ca\u7387B) + (\u6982\u7387C * \u6536\u76ca\u7387C) \u9884\u671f\u6536\u76ca\u7387 = (0.6 * 0.15) + (0.3 * 0.20) + (0.1\n* 0.10) \u9884\u671f\u6536\u76ca\u7387 = (0.09) + (0.06) + (0.01) \u9884\u671f\u6536\u76ca\u7387 = 0.16 \u56e0\u6b64\uff0c\u8be5\u80a1\u7968\u7684\u5e74\u9884\u671f\u6536\u76ca\n\u7387\u4e3a0.16\n\u6b63\u786e\u7b54\u6848\uff1a (D) 0.16\nGround Truth: (C) 0.15 \nError Category: Annotation Error\nError Reason:In this case, GPT-4V accurately grasped the meaning of the question and provided a \nrigorous formulation and calculation. The computed results were completely correct. However, due \nto an issue with annotating the standard answer, the correct response should be D 0.16, leading to \nan Annotation Error.\n<\u56fe\u7247 1>\nFigure B90: A sample error case of Business (subcategory: Finance).\nBack to List of figures\n109\nPREPRINT\n\u5546\u4e1a: \u91d1\u878d\nQuestion: \u67d0\u4eba\u6295\u8d44\u4e86\u4e09\u79cd\u80a1\u7968\uff0c\u8fd9\u4e09\u79cd\u80a1\u7968\u7684\u65b9\u5dee\u4e00\u534f\u65b9\u5dee\u77e9\u9635\u5982\u4e0b\u8868<\u56fe\u7247 1>\uff0c\u77e9\u9635\u7b2c(i\uff0cj)\u4f4d\n\u7f6e\u4e0a\u7684\u5143\u7d20\u4e3a\u80a1\u7968i\u4e0ej\u7684\u534f\u65b9\u5dee\uff0c\u5df2\u77e5\u6b64\u4eba\u6295\u8d44\u8fd9\u4e09\u79cd\u80a1\u7968\u7684\u6bd4\u4f8b\u5206\u522b\u4e3a0.3\uff0c0.3\uff0c0.4\uff0c\u5219\u8be5\u80a1\u7968\n\u6295\u8d44\u7ec4\u5408\u7684\u98ce\u9669\u662f( )\u3002\nOption: \n(A) 8.1\n(B) 5.1\n(C) 6.2\n(D) 9.2\nGPT-4V:\u4e3a\u4e86\u8ba1\u7b97\u80a1\u7968\u6295\u8d44\u7ec4\u5408\u7684\u98ce\u9669\uff0c\u6211\u4eec\u9700\u8981\u4f7f\u7528\u6295\u8d44\u7ec4\u5408\u7684\u65b9\u5dee\u516c\u5f0f\u3002\u5bf9\u4e8e\u4e09\u79cd\u80a1\u7968\u7684\u6295\u8d44\u7ec4\n\u5408\uff0c\u5176\u65b9\u5dee\u53ef\u4ee5\u8868\u793a\u4e3a\uff1a\ud835\udf0e\ud835\udf0e\ud835\udc5d\ud835\udc5d2 = \ud835\udc64\ud835\udc64\ud835\udc34\ud835\udc34\n2 \u22c5 \ud835\udf0e\ud835\udf0e\ud835\udc34\ud835\udc34\n2 + \ud835\udc64\ud835\udc64\ud835\udc35\ud835\udc35\n2 \u22c5 \ud835\udf0e\ud835\udf0e\ud835\udc35\ud835\udc35\n2 + \ud835\udc64\ud835\udc64\ud835\udc36\ud835\udc36\n2 \u22c5 \ud835\udf0e\ud835\udf0e\ud835\udc36\ud835\udc36\n2 +  2 \u22c5 \ud835\udc64\ud835\udc64\ud835\udc34\ud835\udc34 \u22c5 \ud835\udc64\ud835\udc64\ud835\udc35\ud835\udc35 \u22c5  \ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36 \ud835\udc34\ud835\udc34, \ud835\udc35\ud835\udc35 +  2 \u22c5 \ud835\udc64\ud835\udc64\ud835\udc34\ud835\udc34 \u22c5 \ud835\udc64\ud835\udc64\ud835\udc36\ud835\udc36\n\u22c5 \ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36 \ud835\udc34\ud835\udc34, \ud835\udc36\ud835\udc36 +  2 \u22c5 \ud835\udc64\ud835\udc64\ud835\udc35\ud835\udc35 \u22c5 \ud835\udc64\ud835\udc64\ud835\udc36\ud835\udc36 \u22c5 \ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36 \ud835\udc35\ud835\udc35, \ud835\udc36\ud835\udc36 \u5176\u4e2d\uff0c \ud835\udc64\ud835\udc64\ud835\udc34\ud835\udc34, \ud835\udc64\ud835\udc64\ud835\udc35\ud835\udc35 \u548c\n \ud835\udc64\ud835\udc64\ud835\udc36\ud835\udc36 \u662f\u5404\u80a1\u7968\u7684\u6295\u8d44\u6bd4\u4f8b\uff0c  \ud835\udf0e\ud835\udf0e\ud835\udc34\ud835\udc34\n2, \ud835\udf0e\ud835\udf0e\ud835\udc35\ud835\udc35\n2 \u548c\n \ud835\udf0e\ud835\udf0e\ud835\udc36\ud835\udc36\n2\n\u662f\u5404\u81ea\u7684\u65b9\u5dee\uff0c\u800c \ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36 \ud835\udc34\ud835\udc34, \ud835\udc35\ud835\udc35 , \ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36 \ud835\udc34\ud835\udc34, \ud835\udc36\ud835\udc36  \u548c \ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36 \ud835\udc35\ud835\udc35, \ud835\udc36\ud835\udc36 \u3002\u6839\u636e\u9898\u76ee\uff0c\u6211\u4eec\u6709\uff1a \ud835\udc64\ud835\udc64\ud835\udc34\ud835\udc34 =  0.3, \ud835\udc64\ud835\udc64\ud835\udc35\ud835\udc35 =  0.3, \ud835\udc64\ud835\udc64\ud835\udc36\ud835\udc36\n=  0.4 , \ud835\udf0e\ud835\udf0e\ud835\udc34\ud835\udc34\n2 =  25, \ud835\udf0e\ud835\udf0e\ud835\udc35\ud835\udc35\n2 =  36, \ud835\udf0e\ud835\udf0e\ud835\udc36\ud835\udc36\n2 =  49 , \ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36 \ud835\udc34\ud835\udc34, \ud835\udc35\ud835\udc35 =  30, \ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36 \ud835\udc34\ud835\udc34, \ud835\udc36\ud835\udc36 =  35, \ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36 \ud835\udc35\ud835\udc35, \ud835\udc36\ud835\udc36 =  42\u3002 \u5c06 \u8fd9 \u4e9b \u503c \u4ee3\n\u5165 \u4e0a \u8ff0 \u516c \u5f0f \u4e2d \uff0c \u6211 \u4eec \u5f97 \u5230 \uff1a \ud835\udf0e\ud835\udf0e\ud835\udc5d\ud835\udc5d2 =\n0.3 2 \u22c5 25 +\n0.3 2 \u22c5 36 +\n0.4 2 \u22c5 49 +  2 \u22c5 0.3 \u22c5 0.3 \u22c5 30 +  2 \n\u22c5 0.3 \u22c5 0.4 \u22c5 35 +  2 \u22c5 0.3 \u22c5 0.4 \u22c5 42 \u8ba1 \u7b97 \u8fd9 \u4e2a \u8868 \u8fbe \u5f0f \uff1a \ud835\udf0e\ud835\udf0e\ud835\udc5d\ud835\udc5d2 =  0.09 \u22c5 25 +  0.09 \u22c5 36 +  0.16 \u22c5 49 \n+  0.18 \u22c5 30 +  0.24 \u22c5 35 +  0.24 \u22c5 42 \uff0c\ud835\udf0e\ud835\udf0e\ud835\udc5d\ud835\udc5d2 =  2.25 +  3.24 +  7.84 +  5.4 +  8.4 +  10.08\uff0c\ud835\udf0e\ud835\udf0e\ud835\udc5d\ud835\udc5d2\n=  37.21\u6240\u4ee5\uff0c\u8be5\u80a1\u7968\u6295\u8d44\u7ec4\u5408\u7684\u98ce\u9669\uff08\u65b9\u5dee\uff09\u662f 37.21\u3002\u7136\u800c\uff0c\u8fd9\u4e2a\u7ed3\u679c\u5e76\u4e0d\u5728\u6240\u7ed9\u7684\u9009\u9879\u4e2d\u3002\u53ef\n\u80fd\u662f\u8ba1\u7b97\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u4e86\u9519\u8bef\uff0c\u6216\u8005\u9898\u76ee\u4e2d\u7684\u9009\u9879\u6709\u8bef\u3002\u6839\u636e\u6240\u7ed9\u7684\u9009\u9879\uff0c\u6ca1\u6709\u4e00\u4e2a\u4e0e\u8ba1\u7b97\u7ed3\u679c\u76f8\u7b26\uff0c\n\u56e0\u6b64\u65e0\u6cd5\u786e\u5b9a\u6b63\u786e\u7b54\u6848\u3002\u5982\u679c\u5fc5\u987b\u9009\u62e9\u4e00\u4e2a\u6700\u63a5\u8fd1\u7684\u9009\u9879\uff0c\u90a3\u4e48\u9009\u9879 (D) 9.2 \u662f\u6700\u63a5\u8fd1\u8ba1\u7b97\u7ed3\u679c\u7684\n\u5e73\u65b9\u6839\uff08\u6295\u8d44\u7ec4\u5408\u98ce\u9669\u7684\u6807\u51c6\u5dee\uff09\uff0c\u4f46\u8fd9\u5e76\u4e0d\u662f\u65b9\u5dee\u7684\u6b63\u786e\u7b54\u6848\u3002\"D\",\nGround Truth: (C) 6.2\nError Category: Reasoning Error\nError Reason:GPT-4V correctly presented the variance formula for an investment portfolio during \nthe response: \ud835\udf0e\ud835\udf0e\ud835\udc5d\ud835\udc5d2 = \ud835\udc64\ud835\udc64\ud835\udc34\ud835\udc34\n2 \u22c5 \ud835\udf0e\ud835\udf0e\ud835\udc34\ud835\udc34\n2 + \ud835\udc64\ud835\udc64\ud835\udc35\ud835\udc35\n2 \u22c5 \ud835\udf0e\ud835\udf0e\ud835\udc35\ud835\udc35\n2 + \ud835\udc64\ud835\udc64\ud835\udc36\ud835\udc36\n2 \u22c5 \ud835\udf0e\ud835\udf0e\ud835\udc36\ud835\udc36\n2 +  2 \u22c5 \ud835\udc64\ud835\udc64\ud835\udc34\ud835\udc34 \u22c5 \ud835\udc64\ud835\udc64\ud835\udc35\ud835\udc35 \u22c5 \ud835\udc36\ud835\udc36o\ud835\udc36\ud835\udc36 \ud835\udc34\ud835\udc34, \ud835\udc35\ud835\udc35 +  2 \u22c5 \ud835\udc64\ud835\udc64\ud835\udc34\ud835\udc34 \u22c5 \ud835\udc64\ud835\udc64\ud835\udc36\ud835\udc36 \u22c5 \ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36 \ud835\udc34\ud835\udc34, \ud835\udc36\ud835\udc36\n+  2 \u22c5 \ud835\udc64\ud835\udc64\ud835\udc35\ud835\udc35 \u22c5 \ud835\udc64\ud835\udc64\ud835\udc36\ud835\udc36 \u22c5 \ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36\ud835\udc36 \ud835\udc35\ud835\udc35, \ud835\udc36\ud835\udc36\n.Furthermore, it successfully utilized this formula to compute the portfolio's variance. \nHowever, during subsequent reasoning, GPT-4V did not accurately calculate the standard \ndeviation. Instead, it opted for an answer closest to the variance, resulting in a reasoning error.\n<\u56fe\u7247 1>\nFigure B91: A sample error case of Business (subcategory: Finance).\nBack to List of figures\n110\nPREPRINT\n\u5546\u4e1a: \u91d1\u878d\nQuestion: A\u3001B \u548c C \u4e09\u4e2a\u9879\u76ee\uff0c\u5df2\u77e5\u5404\u671f\u73b0\u91d1\u6d41\u60c5\u51b5\u5982<\u56fe\u7247 1>\u3002\u5982\u7528\u56de\u6536\u671f\u6cd5\u6765\u9009\u62e9\u9879\u76ee\u76ee\u6807\u56de\n\u6536\u671f\u4e3a3 \u5e74\uff0c\u9009\u62e9___\u9879\u76ee\nOption: \n(A) A\u548cB \n(B) A\u548cC \n(C) B\u548cC \n(D) A\u548cB\u548cC\nGPT-4V:\u6839\u636e\u56fe\u7247\u4e2d\u63d0\u4f9b\u7684\u73b0\u91d1\u6d41\u4fe1\u606f\uff0c\u6211\u4eec\u53ef\u4ee5\u8ba1\u7b97\u6bcf\u4e2a\u9879\u76ee\u7684\u56de\u6536\u671f\u3002 \u9879\u76eeA\u7684\u73b0\u91d1\u6d41\u4e3a\uff1a\u7b2c\u4e00\n\u5e74+1000\u5143\uff0c\u7b2c\u4e8c\u5e74+1000\u5143\uff0c\u7b2c\u4e09\u5e74+3000\u5143\u3002\u7d2f\u8ba1\u73b0\u91d1\u6d41\u4e3a\u7b2c\u4e00\u5e741000\u5143\uff0c\u7b2c\u4e8c\u5e742000\u5143\uff0c\u7b2c\u4e09\u5e74\n5000\u5143\u3002\u56e0\u6b64\uff0c\u9879\u76eeA\u5728\u7b2c\u4e09\u5e74\u56de\u6536\u4e86\u521d\u59cb\u6295\u8d44\uff0c\u6ee1\u8db33\u5e74\u7684\u76ee\u6807\u56de\u6536\u671f\u3002 \u9879\u76eeB\u7684\u73b0\u91d1\u6d41\u4e3a\uff1a\u7b2c\u4e00\u5e74\n0\u5143\uff0c\u7b2c\u4e8c\u5e74+1000\u5143\uff0c\u7b2c\u4e09\u5e74+2000\u5143\u3002\u7d2f\u8ba1\u73b0\u91d1\u6d41\u4e3a\u7b2c\u4e00\u5e740\u5143\uff0c\u7b2c\u4e8c\u5e741000\u5143\uff0c\u7b2c\u4e09\u5e743000\u5143\u3002\u56e0\n\u6b64\uff0c\u9879\u76eeB\u5728\u7b2c\u4e09\u5e74\u56de\u6536\u4e86\u521d\u59cb\u6295\u8d44\uff0c\u6ee1\u8db33\u5e74\u7684\u76ee\u6807\u56de\u6536\u671f\u3002 \u9879\u76eeC\u7684\u73b0\u91d1\u6d41\u4e3a\uff1a\u7b2c\u4e00\u5e74+1000\u5143\uff0c\n\u7b2c\u4e8c\u5e74+2000\u5143\uff0c\u7b2c\u4e09\u5e74+3000\u5143\u3002\u7d2f\u8ba1\u73b0\u91d1\u6d41\u4e3a\u7b2c\u4e00\u5e741000\u5143\uff0c\u7b2c\u4e8c\u5e743000\u5143\uff0c\u7b2c\u4e09\u5e746000\u5143\u3002\u56e0\u6b64\uff0c\n\u9879\u76eeC\u5728\u7b2c\u4e09\u5e74\u4e5f\u56de\u6536\u4e86\u521d\u59cb\u6295\u8d44\uff0c\u6ee1\u8db33\u5e74\u7684\u76ee\u6807\u56de\u6536\u671f\u3002 \u6839\u636e\u56de\u6536\u671f\u6cd5\uff0c\u6240\u6709\u4e09\u4e2a\u9879\u76eeA\u3001B\u548cC\u90fd\n\u57283\u5e74\u5185\u56de\u6536\u4e86\u521d\u59cb\u6295\u8d44\uff0c\n\u6b63\u786e\u7b54\u6848\uff1a(D) A\u548cB\u548cC\nGround Truth: (B) A\u548cC \nError Category: Reasoning Error\nError Reason:In this case, GPT-4V accurately calculated the situations for projects A and C. \nHowever, when calculating project B, it made an incorrect inference. With a cash flow of 3000 in the \nthird year, which is less than 5000, project B does not meet the requirement for recovering the initial \ninvestment. The correct answer should be A and C.\n<\u56fe\u7247 1>\nFigure B92: A sample error case of Business (subcategory: Finance).\nBack to List of figures\n111\nPREPRINT\n\u5546\u4e1a: \u91d1\u878d\nQuestion: \u4f60\u53ef\u4ee5\u6295\u8d44\u5982<\u56fe\u7247 1>\u4e24\u4e2a\u5b8c\u5168\u8d1f\u80a1\u7968\uff0c\u76ee\u6807\u6536\u76ca\u7387\u4e3a16%\uff0c\u90a3\u4e48\u8fd9\u4e2a\u6295\u8d44\u7ec4\u5408\u7684\u65b9\u5dee\u4e3a \n\uff08 \uff09\nOption: \n(A) 0.0225 \n(B) 0.0324 \n(C) 0.0357 \n(D) 0.0431\nGPT-4V:\u4e3a\u4e86\u8ba1\u7b97\u4e24\u4e2a\u80a1\u7968\u7ec4\u5408\u7684\u65b9\u5dee\uff0c\u6211\u4eec\u9700\u8981\u77e5\u9053\u6bcf\u4e2a\u80a1\u7968\u7684\u9884\u671f\u6536\u76ca\u7387\u3001\u5404\u81ea\u7684\u6807\u51c6\u5dee\u4ee5\u53ca\u5b83\n\u4eec\u4e4b\u95f4\u7684\u76f8\u5173\u7cfb\u6570\u3002\u7136\u800c\uff0c\u4ece\u63d0\u4f9b\u7684\u4fe1\u606f\u4e2d\uff0c\u6211\u4eec\u6ca1\u6709\u80a1\u7968\u4e4b\u95f4\u7684\u76f8\u5173\u7cfb\u6570\uff0c\u8fd9\u662f\u8ba1\u7b97\u7ec4\u5408\u65b9\u5dee\u7684\u5173\n\u952e\u8981\u7d20\u4e4b\u4e00\u3002 \u4e0d\u8fc7\uff0c\u6211\u4eec\u53ef\u4ee5\u5c1d\u8bd5\u4f7f\u7528\u7ed9\u5b9a\u7684\u4fe1\u606f\u6765\u4f30\u8ba1\u7ec4\u5408\u7684\u65b9\u5dee\u3002\u9996\u5148\uff0c\u6211\u4eec\u9700\u8981\u786e\u5b9a\u7ec4\u5408\u4e2d\n\u6bcf\u4e2a\u80a1\u7968\u7684\u6743\u91cd\uff0c\u8fd9\u6837\u624d\u80fd\u8fbe\u5230\u76ee\u6807\u6536\u76ca\u7387\u3002\u5047\u8bbe\u80a1\u7968A\u7684\u6743\u91cd\u4e3aw\uff0c\u80a1\u7968B\u7684\u6743\u91cd\u4e3a1-w\uff0c\u6211\u4eec\u53ef\u4ee5\u5efa\n\u7acb\u4ee5\u4e0b\u65b9\u7a0b\u6765\u89e3\u51b3w\uff1a w \u2217 12% + 1 \u2212 w \u2217 20% = 16% \u89e3\u8fd9\u4e2a\u65b9\u7a0b\uff0c\u6211\u4eec\u5f97\u5230\uff1a0.12w + 0.20 \u2212\n 0.20w = 0.16 \u22120.08w = \u22120.04 w = 0.5 \u8fd9\u610f\u5473\u7740\u80a1\u7968A\u548c\u80a1\u7968B\u5728\u7ec4\u5408\u4e2d\u7684\u6743\u91cd\u90fd\u662f50%\u3002\u7136\u800c\uff0c\u7531\u4e8e\n\u6211\u4eec\u6ca1\u6709\u80a1\u7968\u4e4b\u95f4\u7684\u76f8\u5173\u7cfb\u6570\uff0c\u6211\u4eec\u65e0\u6cd5\u51c6\u786e\u8ba1\u7b97\u7ec4\u5408\u7684\u65b9\u5dee\u3002\u5982\u679c\u6211\u4eec\u5047\u8bbe\u4e24\u4e2a\u80a1\u7968\u4e4b\u95f4\u7684\u76f8\u5173\u7cfb\n\u6570\u4e3a0\uff08\u5373\u5b83\u4eec\u662f\u5b8c\u5168\u4e0d\u76f8\u5173\u7684\uff09\uff0c\u90a3\u4e48\u7ec4\u5408\u7684\u65b9\u5dee\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u516c\u5f0f\u8ba1\u7b97\uff1a \u7ec4\u5408\u65b9\u5dee = \ud835\udc64\ud835\udc642 \u2217\n\u65b9\u5dee\ud835\udc34\ud835\udc34 +\n1 \u2212  \ud835\udc64\ud835\udc64 2 \u2217  \u65b9\u5dee\ud835\udc35\ud835\udc35 \u5176\u4e2d\u65b9\u5deeA\u662f\u80a1\u7968A\u7684\u6807\u51c6\u5dee\u7684\u5e73\u65b9\uff0c\u65b9\u5deeB\u662f\u80a1\u7968B\u7684\u6807\u51c6\u5dee\u7684\u5e73\u65b9\u3002\u5c06\n\u7ed9\u5b9a\u7684\u6807\u51c6\u5dee\u8f6c\u6362\u4e3a\u65b9\u5dee\uff1a \u65b9\u5dee\ud835\udc34\ud835\udc34 =\n15% 2 =  0.0225 \u65b9\u5dee\ud835\udc35\ud835\udc35 =\n45% 2 =  0.2025 \u73b0\u5728\u6211\u4eec\u53ef\u4ee5\n\u8ba1\u7b97\u7ec4\u5408\u7684\u65b9\u5dee\uff1a \u7ec4\u5408\u65b9\u5dee = 0.52 \u2217  0.0225 + 0.52 \u2217  0.2025 \u7ec4\u5408\u65b9\u5dee = 0.25 \u2217 0.0225 + 0.25 \u2217\n 0.2025 \u7ec4\u5408\u65b9\u5dee = 0.005625 + 0.050625 \u7ec4\u5408\u65b9\u5dee = 0.05625 \u8fd9\u4e2a\u7ed3\u679c\u5e76\u4e0d\u5728\u63d0\u4f9b\u7684\u9009\u9879\u4e2d\uff0c\u8fd9\u53ef\u80fd\n\u662f\u56e0\u4e3a\u6211\u4eec\u6ca1\u6709\u8003\u8651\u80a1\u7968\u4e4b\u95f4\u7684\u76f8\u5173\u7cfb\u6570\u3002\u5728\u5b9e\u9645\u60c5\u51b5\u4e2d\uff0c\u5982\u679c\u4e24\u4e2a\u80a1\u7968\u4e0d\u662f\u5b8c\u5168\u4e0d\u76f8\u5173\u7684\uff0c\u90a3\u4e48\u7ec4\n\u5408\u7684\u65b9\u5dee\u5c06\u4f1a\u53d7\u5230\u80a1\u7968\u4e4b\u95f4\u76f8\u5173\u7cfb\u6570\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u5bfc\u81f4\u4e0d\u540c\u7684\u7ed3\u679c\u3002 \u7531\u4e8e\u6211\u4eec\u65e0\u6cd5\u786e\u5b9a\u6b63\u786e\u7b54\u6848\uff0c\n\u6211\u4eec\u4e0d\u80fd\u9009\u62e9\u4efb\u4f55\u4e00\u4e2a\u9009\u9879\u4f5c\u4e3a\u6b63\u786e\u7b54\u6848\u3002\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u6295\u8d44\u51b3\u7b56\u4e2d\uff0c\u6295\u8d44\u8005\u9700\u8981\u8003\u8651\u6240\u6709\u76f8\u5173\u7684\u98ce\n\u9669\u56e0\u7d20\uff0c\u5305\u62ec\u8d44\u4ea7\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002\n\u6b63\u786e\u7b54\u6848\uff1aAB\nGround Truth: (A) 0.0225 \nError Category: Textual Understanding Error\nError Reason:GPT-4V incorrectly interpreted the question prompt. The prompt explicitly stated \nthat stocks A and B are two completely negatively correlated stocks, providing information about \ntheir correlation coefficient. However, GPT-4V overlooked this crucial information, leading to other \nerrors in subsequent inference calculations and resulting in multiple answers, including AB.\n<\u56fe\u7247 1>\nFigure B93: A sample error case of Business (subcategory: Finance).\nBack to List of figures\n112\nPREPRINT\n\u5546\u4e1a: \u91d1\u878d\nQuestion: <\u56fe\u7247 1>\u8d44\u4ea7i\u7684\u5fc5\u8981\u62a5\u916c\u7387Ri\u4e0e\u03b2\u7684\u51fd\u6570\u5f62\u6210\u7684\u76f4\u7ebf\u79f0\u4e3a__\uff08\u8bc1\u5238/\u8d44\u672c\uff09\u5e02\u573a\u7ebf\u3002\nGPT-4V:\u8d44\u4ea7i\u7684\u5fc5\u8981\u62a5\u916c\u7387Ri\u4e0e\u03b2\u7684\u51fd\u6570\u5f62\u6210\u7684\u76f4\u7ebf\u79f0\u4e3a\u8d44\u672c\u5e02\u573a\u7ebf\u3002\n\u6b63\u786e\u7b54\u6848\uff1a\u8d44\u672c\u5e02\u573a\u7ebf\u3002\nGround Truth: \u8bc1\u5238\nError Category: Lack of Knowledge\nError Reason: The response provided by GPT-4V is inaccurate. In the field of finance, the line \nformed by the relationship between the required rate of return (Ri) for asset i and its beta (\u03b2) is \ntypically referred to as the Security Market Line (SML), not the Capital Market Line. GPT-4V's error \nstems from a lack of knowledge in the relevant domain.\n<\u56fe\u7247 1>\n<\u56fe\u7247 1>\nFigure B94: A sample error case of Business (subcategory: Finance).\nBack to List of figures\n113\nPREPRINT\n\u5546\u4e1a: \u7ba1\u7406\nQuestion: \u5e02\u573a\u4e0aA\u3001B\u3001C\u3001D\u56db\u652f\u8bc1\u5238\u7684\u671f\u671b\u6536\u76ca\u7387\u548c\u6807\u51c6\u5dee\u5982<\u56fe\u7247 1>\uff0c\u9762\u5bf9\u56db\u652f\u8bc1\u5238\uff0c\u7406\u6027\u7684\u6295\n\u8d44\u8005\u8be5\u5982\u4f55\u9009\u62e9__\nOption: \n(A) A \n(B) B \n(C) C \n(D) D\nGPT-4V: \u5728\u9009\u62e9\u8bc1\u5238\u65f6\uff0c\u7406\u6027\u7684\u6295\u8d44\u8005\u901a\u5e38\u4f1a\u8003\u8651\u6536\u76ca\u7387\u548c\u98ce\u9669\uff08\u6807\u51c6\u5dee\uff09\u3002\u7406\u6027\u7684\u6295\u8d44\u8005\u4f1a\u5bfb\u627e\u98ce\n\u9669\u548c\u6536\u76ca\u7684\u6700\u4f73\u7ec4\u5408\uff0c\u5373\u5bfb\u627e\u98ce\u9669\u8c03\u6574\u540e\u6536\u76ca\u6700\u9ad8\u7684\u8bc1\u5238\u3002\n\u4ece\u63d0\u4f9b\u7684\u4fe1\u606f\u6765\u770b\uff1a\n- A\u8bc1\u5238\u7684\u671f\u671b\u6536\u76ca\u7387\u4e3a10%\uff0c\u6807\u51c6\u5dee\u4e3a12%\u3002\n- B\u8bc1\u5238\u7684\u671f\u671b\u6536\u76ca\u7387\u4e3a18%\uff0c\u6807\u51c6\u5dee\u4e3a20%\u3002\n- C\u8bc1\u5238\u7684\u671f\u671b\u6536\u76ca\u7387\u4e3a8%\uff0c\u6807\u51c6\u5dee\u4e3a24%\u3002\n- D\u8bc1\u5238\u7684\u671f\u671b\u6536\u76ca\u7387\u4e3a20%\uff0c\u6807\u51c6\u5dee\u4e3a10%\u3002 \u6bd4\u8f83\u8fd9\u4e9b\u8bc1\u5238\uff0cD\u8bc1\u5238\u6709\u6700\u9ad8\u7684\u671f\u671b\u6536\u76ca\u7387\uff0c\u540c\u65f6\u5b83\u7684\n\u6807\u51c6\u5dee\u662f\u6700\u4f4e\u7684\uff0c\u8fd9\u610f\u5473\u7740\u5b83\u63d0\u4f9b\u4e86\u6700\u9ad8\u7684\u6536\u76ca\u4e0e\u6700\u4f4e\u7684\u98ce\u9669\u3002\u56e0\u6b64\uff0c\u6839\u636e\u8fd9\u4e9b\u6570\u636e\uff0c\u7406\u6027\u7684\u6295\u8d44\u8005\n\u6700\u6709\u53ef\u80fd\u9009\u62e9D\u8bc1\u5238\u3002\n\u6b63\u786e\u7b54\u6848\uff1a (D) D\nGround Truth: (D) D\n<\u56fe\u7247 1>\nFigure B95: A sample correct case of Business (subcategory: Management).\nBack to List of figures\n114\nPREPRINT\n\u5546\u4e1a: \u7ba1\u7406\nQuestion: \u5728\u5982<\u56fe\u7247 1>\u6240\u793a\u884c\u4e1a\uff0c\u683c\u5170\u4ed5\u5360\u4e86\u4e00\u534a\u4ee5\u4e0a\u7684\u5e02\u573a\u4efd\u989d\uff0c\u8d22\u6e90\u6eda\u6eda\u800c\u5165\u3002 \u6839\u636e\u6ce2\u58eb\u987f\u54a8\n\u8be2\u96c6\u56e2\u5206\u6790\u6cd5\uff0c\u5982<\u56fe\u7247 1>\u6240\u793a\u662f\u683c\u5170\u4ed5\u7684\uff08\uff09\u3002\nOption: \n(A) \u95ee\u53f7\u7c7b\u6218\u7565\u4e1a\u52a1\u5355\u4f4d\n(B) \u660e\u661f\u7c7b\u6218\u7565\u4e1a\u52a1\u5355\u4f4d\n(C) \u73b0\u91d1\u725b\u7c7b\u6218\u7565\u4e1a\u52a1\u5355\u4f4d\n(D) \u72d7\u7c7b\u6218\u7565\u4e1a\u52a1\u5355\u4f4d\nGPT-4V: \u56fe\u7247\u663e\u793a\u4e86\u4e00\u4e2a\u683c\u5170\u4ed5\u54c1\u724c\u7684\u5fae\u6ce2\u7089\u3002\u6839\u636e\u9898\u76ee\u63cf\u8ff0\uff0c\u683c\u5170\u4ed5\u5728\u8fd9\u4e2a\u884c\u4e1a\u4e2d\u5360\u6709\u4e00\u534a\u4ee5\u4e0a\u7684\n\u5e02\u573a\u4efd\u989d\uff0c\u5e76\u4e14\u8d22\u6e90\u6eda\u6eda\u800c\u5165\u3002\u6839\u636e\u6ce2\u58eb\u987f\u54a8\u8be2\u96c6\u56e2\uff08BCG\uff09\u77e9\u9635\u7684\u5206\u6790\u6cd5\uff0c\u8fd9\u6837\u7684\u4e1a\u52a1\u5355\u4f4d\u5177\u6709\u9ad8\n\u5e02\u573a\u4efd\u989d\u5e76\u4e14\u4ea7\u751f\u4e86\u5927\u91cf\u73b0\u91d1\u6d41\uff0c\u901a\u5e38\u88ab\u5206\u7c7b\u4e3a\u201c\u73b0\u91d1\u725b\u201d\uff08Cash Cow\uff09\u3002\n\u6b63\u786e\u7b54\u6848\uff1a (C) \u73b0\u91d1\u725b\u7c7b\u6218\u7565\u4e1a\u52a1\u5355\u4f4d\nGround Truth: (C) \u73b0\u91d1\u725b\u7c7b\u6218\u7565\u4e1a\u52a1\u5355\u4f4d\n<\u56fe\u7247 1>\nFigure B96: A sample correct case of Business (subcategory: Management).\nBack to List of figures\n115\nPREPRINT\n\u5546\u4e1a: \u7ba1\u7406\nQuestion: \u67d0\u670d\u88c5\u751f\u4ea7\u4f01\u4e1a\u5b9e\u65bd\u5dee\u5f02\u5316\u6218\u7565\uff0c\u5411\u6d88\u8d39\u8005\u63d0\u4f9b\u4e0e\u4f17\u4e0d\u540c\u7684\u4ea7\u54c1\uff0c\u83b7\u5f97\u7ade\u4e89\u4f18\u52bf\u3002\u8be5\u4f01\u4e1a\n\u4e3a\u4e86\u964d\u4f4e\u539f\u6750\u6599\u91c7\u8d2d\u6210\u672c\uff0c\u8fdb\u5165\u7eba\u7ec7\u884c\u4e1a\uff0c\u81ea\u4e3b\u751f\u4ea7\u548c\u4f9b\u5e94\u670d\u88c5\u52a0\u5de5\u6240\u9700\u9762\u6599\u3002\u8be5\u4f01\u4e1a\u4ee5\u8bb8\u53ef\u7ecf\u8425\u7684\n\u5f62\u5f0f\u79ef\u6781\u62d3\u5c55\u6d77\u5916\u5e02\u573a\uff0c\u5141\u8bb8\u56fd\u5916\u4f01\u4e1a\u4f7f\u7528\u8be5\u4f01\u4e1a\u7684\u4e13\u5229\u3001\u5546\u6807\u3001\u8bbe\u8ba1\u6b3e\u5f0f\uff0c\u6269\u5927\u4f01\u4e1a\u7684\u56fd\u9645\u58f0\u8a89\u3002\n\u540c\u65f6\uff0c\u8be5\u4f01\u4e1a\u79ef\u6781\u8fdb\u884c\u65b0\u4ea7\u54c1\u5f00\u53d1\uff0c\u4e0d\u65ad\u63a8\u51fa\u65b0\u6b3e\u670d\u88c5\u3002\u62df\u63a8\u51fa\u7684\u65b0\u6b3e\u670d\u88c5\u5171\u6709\u7532\u4ea7\u54c1\u3001\u4e59\u4ea7\u54c1\u3001\u4e19\n\u4ea7\u54c1\u3001\u4e01\u4ea7\u54c1\u56db\u79cd\u5f00\u53d1\u65b9\u6848\u53ef\u4f9b\u9009\u62e9\uff1b\u6bcf\u79cd\u4ea7\u54c1\u65b9\u6848\u5747\u5b58\u5728\u7740\u5e02\u573a\u9700\u6c42\u9ad8\u3001\u5e02\u573a\u9700\u6c42\u4e00\u822c\u3001\u5e02\u573a\u9700\u6c42\n\u4f4e\u4e09\u79cd\u53ef\u80fd\u7684\u5e02\u573a\u72b6\u6001\uff0c\u4f46\u5404\u79cd\u72b6\u6001\u53d1\u751f\u7684\u6982\u7387\u96be\u4ee5\u6d4b\u7b97\u3002\u5728\u5e02\u573a\u8c03\u67e5\u7684\u57fa\u7840\u4e0a\uff0c\u8be5\u670d\u88c5\u751f\u4ea7\u4f01\u4e1a\u5bf9\n\u56db\u79cd\u5907\u9009\u65b9\u6848\u7684\u635f\u76ca\u503c\u8fdb\u884c\u4e86\u9884\u6d4b\uff0c\u5728\u4e0d\u540c\u5e02\u573a\u72b6\u6001\u4e0b\u635f\u76ca\u503c\u5982\u4e0b\u8868<\u56fe\u7247 1>\u6240\u793a\uff08\u5355\u4f4d\uff1a\u767e\u4e07\u5143\uff09\u3002\n\u82e5\u91c7\u7528\u540e\u6094\u503c\u539f\u5219\u8fdb\u884c\u65b0\u6b3e\u670d\u88c5\u7684\u51b3\u7b56\uff0c\u8be5\u4f01\u4e1a\u5e94\u9009\u62e9\u7684\u65b9\u6848\u4e3a\u5f00\u53d1\uff08 \uff09\nOption: \n(A) \u4e01\u4ea7\u54c1\n(B) \u4e59\u4ea7\u54c1\n(C) \u7532\u4ea7\u54c1\n(D) \u4e19\u4ea7\u54c1\nGPT-4V: \u540e\u6094\u503c\u539f\u5219\uff0c\u53c8\u79f0\u4e3a\u6700\u5c0f\u6700\u5927\u540e\u6094\u539f\u5219\uff0c\u662f\u4e00\u79cd\u51b3\u7b56\u51c6\u5219\uff0c\u7528\u4e8e\u5728\u4e0d\u786e\u5b9a\u6027\u6761\u4ef6\u4e0b\u505a\u51fa\u51b3\u7b56\u3002\n\u5b83\u8003\u8651\u7684\u662f\u5982\u679c\u4e0d\u9009\u62e9\u6700\u4f73\u65b9\u6848\u4f1a\u4ea7\u751f\u7684\u635f\u5931\uff08\u540e\u6094\u503c\uff09\uff0c\u5e76\u8bd5\u56fe\u6700\u5c0f\u5316\u8fd9\u79cd\u6f5c\u5728\u7684\u635f\u5931\u3002 \u4e3a\u4e86\u5e94\n\u7528\u8fd9\u4e2a\u539f\u5219\uff0c\u6211\u4eec\u9700\u8981\u8ba1\u7b97\u6bcf\u79cd\u5e02\u573a\u72b6\u6001\u4e0b\u4e0d\u9009\u62e9\u6700\u4f73\u65b9\u6848\u7684\u540e\u6094\u503c\u3002\u9996\u5148\uff0c\u6211\u4eec\u627e\u51fa\u6bcf\u79cd\u5e02\u573a\u72b6\u6001\n\u4e0b\u7684\u6700\u5927\u635f\u76ca\u503c\uff0c\u7136\u540e\u8ba1\u7b97\u5176\u4ed6\u65b9\u6848\u4e0e\u8fd9\u4e2a\u6700\u5927\u503c\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5373\u540e\u6094\u503c\u3002 \u6839\u636e\u63d0\u4f9b\u7684\u8868\u683c\uff0c\u6211\u4eec\n\u53ef\u4ee5\u8ba1\u7b97\u51fa\u6bcf\u79cd\u5e02\u573a\u72b6\u6001\u4e0b\u7684\u6700\u5927\u635f\u76ca\u503c\uff1a - \u5e02\u573a\u9700\u6c42\u9ad8\u65f6\uff0c\u6700\u5927\u635f\u76ca\u503c\u4e3a280\uff08\u4e19\u4ea7\u54c1\uff09\u3002 - \u5e02\n\u573a\u9700\u6c42\u4e00\u822c\u65f6\uff0c\u6700\u5927\u635f\u76ca\u503c\u4e3a150\uff08\u4e01\u4ea7\u54c1\uff09\u3002 - \u5e02\u573a\u9700\u6c42\u4f4e\u65f6\uff0c\u6700\u5927\u635f\u76ca\u503c\u4e3a30\uff08\u4e59\u4ea7\u54c1\uff09\u3002 \u73b0\n\u5728\uff0c\u6211\u4eec\u8ba1\u7b97\u6bcf\u4e2a\u65b9\u6848\u7684\u540e\u6094\u503c\uff1a - \u7532\u4ea7\u54c1\u7684\u540e\u6094\u503c\u4e3a\uff1a\u5e02\u573a\u9700\u6c42\u9ad8\u65f6\uff08280-270=10\uff09\uff0c\u5e02\u573a\u9700\u6c42\n\u4e00\u822c\u65f6\uff08150-110=40\uff09\uff0c\u5e02\u573a\u9700\u6c42\u4f4e\u65f6\uff0830-10=20\uff09\u3002 - \u4e59\u4ea7\u54c1\u7684\u540e\u6094\u503c\u4e3a\uff1a\u5e02\u573a\u9700\u6c42\u9ad8\u65f6\uff08280-\n265=15\uff09\uff0c\u5e02\u573a\u9700\u6c42\u4e00\u822c\u65f6\uff08150-100=50\uff09\uff0c\u5e02\u573a\u9700\u6c42\u4f4e\u65f6\uff0830-30=0\uff09\u3002 - \u4e19\u4ea7\u54c1\u7684\u540e\u6094\u503c\u4e3a\uff1a\u5e02\n\u573a\u9700\u6c42\u9ad8\u65f6\uff08280-280=0\uff09\uff0c\u5e02\u573a\u9700\u6c42\u4e00\u822c\u65f6\uff08150-140=10\uff09\uff0c\u5e02\u573a\u9700\u6c42\u4f4e\u65f6\uff0830-(-10)=40\uff09\u3002 -\n\u4e01\u4ea7\u54c1\u7684\u540e\u6094\u503c\u4e3a\uff1a\u5e02\u573a\u9700\u6c42\u9ad8\u65f6\uff08280-250=30\uff09\uff0c\u5e02\u573a\u9700\u6c42\u4e00\u822c\u65f6\uff08150-150=0\uff09\uff0c\u5e02\u573a\u9700\u6c42\u4f4e\u65f6\n\uff0830-20=10\uff09\u3002 \u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u627e\u51fa\u6bcf\u4e2a\u65b9\u6848\u7684\u6700\u5927\u540e\u6094\u503c\uff1a - \u7532\u4ea7\u54c1\u7684\u6700\u5927\u540e\u6094\u503c\u4e3a40\u3002 - \u4e59\u4ea7\n\u54c1\u7684\u6700\u5927\u540e\u6094\u503c\u4e3a50\u3002 - \u4e19\u4ea7\u54c1\u7684\u6700\u5927\u540e\u6094\u503c\u4e3a40\u3002 - \u4e01\u4ea7\u54c1\u7684\u6700\u5927\u540e\u6094\u503c\u4e3a30\u3002 \u6839\u636e\u540e\u6094\u503c\u539f\n\u5219\uff0c\u6211\u4eec\u5e94\u8be5\u9009\u62e9\u6700\u5927\u540e\u6094\u503c\u6700\u5c0f\u7684\u65b9\u6848\uff0c\u5373\u4e01\u4ea7\u54c1\u3002\n\u6b63\u786e\u7b54\u6848\uff1a (A) \u4e01\u4ea7\u54c1\u3002\nGround Truth: (A) \u4e01\u4ea7\u54c1\n<\u56fe\u7247 1>\nFigure B97: A sample correct case of Business (subcategory: Management).\nBack to List of figures\n116\nPREPRINT\n\u5546\u4e1a: \u7ba1\u7406\nQuestion: \u67d0\u4f01\u4e1a\u62df\u8d2d\u7f6e\u4e00\u5957\u5927\u578b\u8bbe\u5907\uff0c\u7532\u3001\u4e59\u3001\u4e19\u4e09\u4e2a\u4f9b\u5e94\u5546\u7684\u62a5\u4ef7\u76f8\u540c\uff0c\u8bbe\u5907\u6027\u80fd\u4e5f\u4e00\u6837\uff0c\u53ea\u662f\n\u4f7f\u7528\u8fc7\u7a0b\u4e2d\u9700\u8981\u7684\u7ef4\u4fee\u8d39\u4e0d\u540c\uff0c\u9884\u8ba1\u60c5\u51b5\u5982<\u56fe\u7247 1>\uff0c\u6839\u636e\u4ee5\u4e0a\u6570\u636e\uff0c\u4f01\u4e1a\u5e94\u8d2d\u4e70\u54ea\u4e2a(\u4e9b)\u4f9b\u5e94\u5546\u7684\n\u8bbe\u5907?\uff08\uff09\noption: \n(A) \u7532\u6216\u4e19\n(B) \u4e59\n(C) \u7532\n(D) \u4e19\nGPT-4V:\u6839\u636e\u63d0\u4f9b\u7684\u56fe\u7247\u4fe1\u606f\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u4e09\u4e2a\u4f9b\u5e94\u5546\u5728\u4e0d\u540c\u5e74\u4efd\u7684\u8bbe\u5907\u7ef4\u4fee\u8d39\u7528\u548c\u53d1\u751f\u6545\u969c\u7684\u6982\n\u7387\u3002\u4e3a\u4e86\u786e\u5b9a\u54ea\u4e2a\u4f9b\u5e94\u5546\u7684\u8bbe\u5907\u6700\u7ecf\u6d4e\uff0c\u6211\u4eec\u9700\u8981\u8ba1\u7b97\u6bcf\u4e2a\u4f9b\u5e94\u5546\u8bbe\u5907\u7684\u9884\u671f\u7ef4\u4fee\u6210\u672c\u3002 \u9884\u671f\u7ef4\u4fee\n\u6210\u672c\u53ef\u4ee5\u901a\u8fc7\u5c06\u6bcf\u5e74\u7684\u7ef4\u4fee\u8d39\u7528\u4e58\u4ee5\u76f8\u5e94\u5e74\u4efd\u53d1\u751f\u6545\u969c\u7684\u6982\u7387\u6765\u8ba1\u7b97\uff0c\u7136\u540e\u5c06\u6240\u6709\u5e74\u4efd\u7684\u7ed3\u679c\u76f8\u52a0\u5f97\n\u5230\u603b\u7684\u9884\u671f\u7ef4\u4fee\u6210\u672c\u3002 \u7532\u4f9b\u5e94\u5546\u7684\u9884\u671f\u7ef4\u4fee\u6210\u672c\u8ba1\u7b97\u5982\u4e0b\uff1a - \u7b2c\u4e00\u5e74\uff1a40\u4e07\u5143 \u00d7 40% = 16\u4e07\u5143 -\n\u7b2c\u4e8c\u5e74\uff1a10\u4e07\u5143 \u00d7 60% = 6\u4e07\u5143 - \u7b2c\u4e09\u5e74\uff1a30\u4e07\u5143 \u00d7 40% = 12\u4e07\u5143 - \u7b2c\u56db\u5e74\uff1a20\u4e07\u5143 \u00d7 60% =\n12\u4e07\u5143 - \u7b2c\u4e94\u5e74\uff1a50\u4e07\u5143 \u00d7 30% = 15\u4e07\u5143 - \u7b2c\u516d\u5e74\uff1a10\u4e07\u5143 \u00d7 70% = 7\u4e07\u5143 \u603b\u8ba1\uff1a16 + 6 +\n12 + 12 + 15 + 7 = 68\u4e07\u5143 \u4e59\u4f9b\u5e94\u5546\u7684\u9884\u671f\u7ef4\u4fee\u6210\u672c\u8ba1\u7b97\u5982\u4e0b\uff1a - \u7b2c\u4e00\u5e74\uff1a40\u4e07\u5143 \u00d7 10% = 4\u4e07\n\u5143 - \u7b2c\u4e8c\u5e74\uff1a10\u4e07\u5143 \u00d7 30% = 3\u4e07\u5143 - \u7b2c\u4e09\u5e74\uff1a30\u4e07\u5143 \u00d7 20% = 6\u4e07\u5143 - \u7b2c\u56db\u5e74\uff1a20\u4e07\u5143 \u00d7\n50% = 10\u4e07\u5143 - \u7b2c\u4e94\u5e74\uff1a50\u4e07\u5143 \u00d7 10% = 5\u4e07\u5143 - \u7b2c\u516d\u5e74\uff1a10\u4e07\u5143 \u00d7 60% = 6\u4e07\u5143 \u603b\u8ba1\uff1a4 + 3\n+ 6 + 10 + 5 + 6 = 34\u4e07\u5143 \u4e19\u4f9b\u5e94\u5546\u7684\u9884\u671f\u7ef4\u4fee\u6210\u672c\u8ba1\u7b97\u5982\u4e0b\uff1a - \u7b2c\u4e00\u5e74\uff1a40\u4e07\u5143 \u00d7 20% = 8\u4e07\n\u5143 - \u7b2c\u4e8c\u5e74\uff1a10\u4e07\u5143 \u00d7 50% = 5\u4e07\u5143 - \u7b2c\u4e09\u5e74\uff1a30\u4e07\u5143 \u00d7 10% = 3\u4e07\u5143 - \u7b2c\u56db\u5e74\uff1a20\u4e07\u5143 \u00d7\n40% = 8\u4e07\u5143 - \u7b2c\u4e94\u5e74\uff1a50\u4e07\u5143 \u00d7 20% = 10\u4e07\u5143 - \u7b2c\u516d\u5e74\uff1a10\u4e07\u5143 \u00d7 30% = 3\u4e07\u5143 \u603b\u8ba1\uff1a8 + 5\n+ 3 + 8 + 10 + 3 = 37\u4e07\u5143 \u6bd4\u8f83\u4e09\u4e2a\u4f9b\u5e94\u5546\u7684\u603b\u9884\u671f\u7ef4\u4fee\u6210\u672c\uff0c\u4e59\u4f9b\u5e94\u5546\u7684\u603b\u9884\u671f\u7ef4\u4fee\u6210\u672c\u6700\u4f4e\uff0c\n\u4e3a34\u4e07\u5143\u3002\u56e0\u6b64\uff0c\u4f01\u4e1a\u5e94\u8be5\u9009\u62e9\u4e59\u4f9b\u5e94\u5546\u7684\u8bbe\u5907\u3002\n\u6b63\u786e\u7b54\u6848\uff1a (B) \u4e59\nGround Truth: (A) \u7532\u6216\u4e19\nError Category: Perceptual Error\nError Reason:GPT-4V misunderstood the meaning of the data in the image. The data in the second \nrow should represent annual maintenance costs, and the data in the third row corresponds to the \nrespective probabilities. Each column represents a different supplier, and it does not involve \nchanges between different years. However, GPT mistakenly interpreted the data in different \ncolumns as changes over different years\n<\u56fe\u7247 1>\n<\u56fe\u7247 1>\nFigure B98: A sample error case of Business (subcategory: Management).\nBack to List of figures\n117\nPREPRINT\n\u5546\u4e1a: \u7ba1\u7406\nQuestion: \u9664\u51b0\u5242\u662f\u51ac\u5b63\u5317\u65b9\u57ce\u5e02\u7528\u4e8e\u53bb\u9664\u9053\u8def\u51b0\u96ea\u7684\u5e38\u89c1\u4ea7\u54c1\u3002\u4e0b\u8868<\u56fe\u7247 1>\u663e\u793a\u4e86\u4e94\u79cd\u9664\u51b0\u5242\u7684\n\u5404\u9879\u7279\u5f81\uff1a\u4ee5\u4e0b\u54ea\u9879\u5bf9\u4e0a\u8ff0\u4e94\u79cd\u9664\u51b0\u5242\u7684\u7279\u5f81\u6982\u62ec\u6700\u4e3a\u51c6\u786e?\uff08\uff09\nOption: \n(A) \u878d\u51b0\u901f\u5ea6\u8f83\u6162\u7684\u9664\u51b0\u5242\u5728\u6c61\u67d3\u571f\u58e4\u548c\u6c61\u67d3\u6c34\u4f53\u65b9\u9762\u7684\u98ce\u9669\u90fd\u4f4e\n(B) \u6ca1\u6709\u4e00\u79cd\u878d\u51b0\u901f\u5ea6\u5feb\u7684\u9664\u51b0\u5242\u4e09\u4e2a\u65b9\u9762\u7684\u98ce\u9669\u90fd\u9ad8\n(C) \u82e5\u67d0\u79cd\u9664\u51b0\u5242\u81f3\u5c11\u5728\u4e24\u4e2a\u65b9\u9762\u98ce\u9669\u4f4e\uff0c\u5219\u5176\u878d\u51b0\u901f\u5ea6\u4e00\u5b9a\u8f83\u6162\n(D) \u82e5\u67d0\u79cd\u9664\u51b0\u5242\u4e09\u65b9\u9762\u98ce\u9669\u90fd\u4e0d\u9ad8\uff0c\u5219\u5176\u878d\u51b0\u901f\u5ea6\u4e00\u5b9a\u4e5f\u4e0d\u5feb\nGPT-4V:\u6839\u636e\u63d0\u4f9b\u7684\u8868\u683c\u4fe1\u606f\uff0c\u6211\u4eec\u53ef\u4ee5\u5bf9\u6bcf\u4e2a\u9009\u9879\u8fdb\u884c\u5206\u6790\uff1a (A) \u878d\u51b0\u901f\u5ea6\u8f83\u6162\u7684\u9664\u51b0\u5242\u5728\u6c61\u67d3\n\u571f\u58e4\u548c\u6c61\u67d3\u6c34\u4f53\u65b9\u9762\u7684\u98ce\u9669\u90fd\u4f4e \u8fd9\u4e2a\u9009\u9879\u4e0d\u51c6\u786e\u3002\u4ece\u8868\u683c\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0c\u878d\u51b0\u901f\u5ea6\u8f83\u6162\u7684\u9664\u51b0\u5242\uff08III\n\u548cV\uff09\u5728\u6c61\u67d3\u571f\u58e4\u65b9\u9762\u7684\u98ce\u9669\u90fd\u662f\u4f4e\u7684\uff0c\u4f46\u5728\u6c61\u67d3\u6c34\u4f53\u65b9\u9762\uff0cIII\u7684\u98ce\u9669\u662f\u4f4e\u7684\uff0c\u800cV\u7684\u98ce\u9669\u662f\u4e2d\u7b49\u7684\u3002\n(B) \u6ca1\u6709\u4e00\u79cd\u878d\u51b0\u901f\u5ea6\u5feb\u7684\u9664\u51b0\u5242\u4e09\u4e2a\u65b9\u9762\u7684\u98ce\u9669\u90fd\u9ad8 \u8fd9\u4e2a\u9009\u9879\u662f\u51c6\u786e\u7684\u3002\u4ece\u8868\u683c\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0c\u878d\n\u51b0\u901f\u5ea6\u5feb\u7684\u9664\u51b0\u5242\uff08I\u548cIV\uff09\u6ca1\u6709\u4e00\u79cd\u5728\u4e09\u4e2a\u65b9\u9762\uff08\u5bf9\u690d\u7269\u7684\u5371\u5bb3\u3001\u6c61\u67d3\u571f\u58e4\u3001\u6c61\u67d3\u6c34\u4f53\uff09\u7684\u98ce\u9669\u90fd\n\u662f\u9ad8\u7684\u3002 (C) \u82e5\u67d0\u79cd\u9664\u51b0\u5242\u81f3\u5c11\u5728\u4e24\u4e2a\u65b9\u9762\u98ce\u9669\u4f4e\uff0c\u5219\u5176\u878d\u51b0\u901f\u5ea6\u4e00\u5b9a\u8f83\u6162 \u8fd9\u4e2a\u9009\u9879\u4e0d\u51c6\u786e\u3002\u4ece\u8868\n\u683c\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0c\u9664\u51b0\u5242II\u5728\u4e24\u4e2a\u65b9\u9762\u98ce\u9669\u4f4e\uff08\u5bf9\u690d\u7269\u7684\u5371\u5bb3\u548c\u6c61\u67d3\u6c34\u4f53\uff09\uff0c\u4f46\u5176\u878d\u51b0\u901f\u5ea6\u662f\u4e2d\u7b49\u7684\u3002\n(D) \u82e5\u67d0\u79cd\u9664\u51b0\u5242\u4e09\u65b9\u9762\u98ce\u9669\u90fd\u4e0d\u9ad8\uff0c\u5219\u5176\u878d\u51b0\u901f\u5ea6\u4e00\u5b9a\u4e5f\u4e0d\u5feb \u8fd9\u4e2a\u9009\u9879\u4e0d\u51c6\u786e\u3002\u4ece\u8868\u683c\u4e2d\u53ef\u4ee5\u770b\n\u51fa\uff0c\u9664\u51b0\u5242II\u5728\u4e09\u4e2a\u65b9\u9762\u7684\u98ce\u9669\u90fd\u4e0d\u662f\u9ad8\u7684\uff08\u5bf9\u690d\u7269\u7684\u5371\u5bb3\u662f\u4f4e\u7684\uff0c\u6c61\u67d3\u571f\u58e4\u548c\u6c61\u67d3\u6c34\u4f53\u662f\u4e2d\u7b49\u7684\uff09\uff0c\n\u4f46\u5176\u878d\u51b0\u901f\u5ea6\u662f\u4e2d\u7b49\u7684\uff0c\u4e0d\u662f\u4e0d\u5feb\u3002\n\u6b63\u786e\u7b54\u6848\uff1a (B) \u6ca1\u6709\u4e00\u79cd\u878d\u51b0\u901f\u5ea6\u5feb\u7684\u9664\u51b0\u5242\u4e09\u4e2a\u65b9\u9762\u7684\u98ce\u9669\u90fd\u9ad8\u3002\nGround Truth: (C) \u82e5\u67d0\u79cd\u9664\u51b0\u5242\u81f3\u5c11\u5728\u4e24\u4e2a\u65b9\u9762\u98ce\u9669\u4f4e\uff0c\u5219\u5176\u878d\u51b0\u901f\u5ea6\u4e00\u5b9a\u8f83\u6162\nError Category: Reasoning Error\nError Reason:In this case, there were some issues with the reasoning of GPT-4V. For instance, in \noption B, GPT-4V incorrectly concluded that de-icing agents with fast ice-melting rates (I and IV) do \nnot pose high risks in all three aspects. This contradicts the actual situation where option I has high \nrisks in all three aspects. Similarly, errors in reasoning were observed in the analysis of the \nremaining options, leading to incorrect judgments.\n<\u56fe\u7247 1>\nFigure B99: A sample error case of Business (subcategory: Management).\nBack to List of figures\n118\nPREPRINT\n\u5546\u4e1a: \u7ba1\u7406\nQuestion: \u67d0\u516c\u53f8\u8ba1\u5212\u5bf9\u67d0\u4e00\u9879\u76ee\u8fdb\u884c\u6295\u8d44\uff0c\u6295\u8d44\u989d\u4e3a300\u4e07\u5143\uff0c\u671f\u9650\u4e3a5\u5e74\uff0c\u6bcf\u5e74\u51c0\u73b0\u91d1\u6d41\u91cf\u5206\u522b\u4e3a\n150\u4e07\u5143\u3001200\u4e07\u5143\u3001200\u4e07\u5143\u3001100\u4e07\u5143\u3001200\u4e07\u5143\u3002\u5047\u8bbe\u8d44\u672c\u6210\u672c\u7387\u4e3a10\uff05\u3002\u8be5\u9879\u76ee\u7684\u51c0\u73b0\u91d1\u6d41\u91cf\u53ca\n\u590d\u5229\u73b0\u503c\u7cfb\u6570\u5982\u4e0b\u8868<\u56fe\u7247 1>\u6240\u793a\u3002\u5982\u679c\u4e0d\u8003\u8651\u8d44\u91d1\u65f6\u95f4\u4ef7\u503c\uff0c\u8be5\u9879\u76ee\u7684\u6295\u8d44\u56de\u6536\u671f\u4e3a\uff08\uff09\noption: \n(A) 1.75\u5e74\n(B) 2\u5e74\n(C) 2.75\u5e74\n(D) 2.83\u5e74\nGPT-4V:\u9898\u76ee\u8981\u6c42\u4e0d\u8003\u8651\u8d44\u91d1\u7684\u65f6\u95f4\u4ef7\u503c\uff0c\u56e0\u6b64\u6211\u4eec\u53ea\u9700\u8981\u8ba1\u7b97\u4f55\u65f6\u51c0\u73b0\u91d1\u6d41\u91cf\u7d2f\u8ba1\u8fbe\u5230\u6295\u8d44\u989d300\n\u4e07\u5143\u5373\u53ef\u3002 \u7b2c\u4e00\u5e74\u540e\uff0c\u51c0\u73b0\u91d1\u6d41\u91cf\u4e3a150\u4e07\u5143\uff1b \u7b2c\u4e8c\u5e74\u540e\uff0c\u7d2f\u8ba1\u51c0\u73b0\u91d1\u6d41\u91cf\u4e3a150 + 200 = 350\u4e07\u5143\u3002\n\u5728\u7b2c\u4e8c\u5e74\u7ed3\u675f\u65f6\uff0c\u7d2f\u8ba1\u51c0\u73b0\u91d1\u6d41\u91cf\u5df2\u7ecf\u8d85\u8fc7\u4e86\u6295\u8d44\u989d300\u4e07\u5143\u3002\u56e0\u6b64\uff0c\u6295\u8d44\u56de\u6536\u671f\u5c0f\u4e8e2\u5e74\u3002\u7531\u4e8e\u7b2c\u4e00\n\u5e74\u7ed3\u675f\u65f6\u8fd8\u6ca1\u6709\u56de\u6536\u5b8c\uff0c\n\u6b63\u786e\u7b54\u6848\uff1a (B) 2\u5e74\nGround Truth: (A) 1.75\u5e74\nError Category: Reasoning Error\nError Reason:during the reasoning process, GPT-4V failed to perform precise calculations for the \ntask and instead obtained a vague result of 'investment payback period less than 2 years' through \nestimation. This led to a reasoning error. The correct calculation should be: 1 + (300 - 150) / 200 = \n1.75 years.\n<\u56fe\u7247 1>\nFigure B100: A sample error case of Business (subcategory: Management).\nBack to List of figures\n119\nPREPRINT\n\u5546\u4e1a: \u7ba1\u7406\nQuestion: \u6709A \u3001B \u3001C\u3001D\u56db\u4e2a\u96f6\u4ef6\uff0c\u5747\u5148\u5728\u7b2c\u4e00\u53f0\u8bbe\u5907\u4e0a\u52a0\u5de5\uff0c\u518d\u5230\u7b2c\u4e8c\u53f0\u8bbe\u5907\u4e0a\u52a0\u5de5\uff0c\u5404\u4e2a\u96f6\u4ef6\n\u5728\u8fd9\u4e24\u53f0\u8bbe\u5907\u4e0a\u7684\u52a0\u5de5\u65f6\u95f4\u5982\u4e0b\u8868<\u56fe\u7247 1>\uff1a\u6309\u7ea6\u7ff0\u900a\u6cd5\u5219\uff0c\u56db\u4e2a\u96f6\u4ef6\u6700\u4f18\u7684\u52a0\u5de5\u987a\u5e8f\u5e94\u4e3a\uff08\uff09\noption: \n(A) B-A-C-D \n(B) C-D-A-B \n(C) B-C-D-A \n(D) D-A-C-B\nGPT-4V:\u7ea6\u7ff0\u900a\u6cd5\u5219\u662f\u4e00\u79cd\u7528\u4e8e\u786e\u5b9a\u5728\u4e24\u53f0\u8bbe\u5907\u4e0a\u8fdb\u884c\u4f5c\u4e1a\u7684\u96f6\u4ef6\u52a0\u5de5\u987a\u5e8f\u7684\u65b9\u6cd5\uff0c\u76ee\u7684\u662f\u6700\u5c0f\u5316\u603b\n\u7684\u52a0\u5de5\u65f6\u95f4\u3002\u6839\u636e\u7ea6\u7ff0\u900a\u6cd5\u5219\uff0c\u5e94\u8be5\u9996\u5148\u6bd4\u8f83\u5404\u4e2a\u96f6\u4ef6\u5728\u4e24\u53f0\u8bbe\u5907\u4e0a\u7684\u6700\u77ed\u52a0\u5de5\u65f6\u95f4\uff0c\u5e76\u6309\u7167\u8fd9\u4e2a\u65f6\n\u95f4\u4ece\u5c0f\u5230\u5927\u6392\u5e8f\u3002\u5982\u679c\u6700\u77ed\u65f6\u95f4\u5728\u7b2c\u4e00\u53f0\u8bbe\u5907\u4e0a\uff0c\u90a3\u4e48\u8be5\u96f6\u4ef6\u5e94\u8be5\u5c3d\u53ef\u80fd\u65e9\u5730\u8fdb\u884c\u52a0\u5de5\uff1b\u5982\u679c\u6700\u77ed\u65f6\n\u95f4\u5728\u7b2c\u4e8c\u53f0\u8bbe\u5907\u4e0a\uff0c\u90a3\u4e48\u8be5\u96f6\u4ef6\u5e94\u8be5\u5c3d\u53ef\u80fd\u665a\u5730\u8fdb\u884c\u52a0\u5de5\u3002 \u6839\u636e\u63d0\u4f9b\u7684\u56fe\u7247\u4fe1\u606f\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff1a\n- \u96f6\u4ef6A\u5728\u7b2c\u4e00\u53f0\u8bbe\u5907\u4e0a\u7684\u52a0\u5de5\u65f6\u95f4\u4e3a9\uff0c\u5728\u7b2c\u4e8c\u53f0\u8bbe\u5907\u4e0a\u7684\u52a0\u5de5\u65f6\u95f4\u4e3a10\u3002 - \u96f6\u4ef6B\u5728\u7b2c\u4e00\u53f0\u8bbe\u5907\u4e0a\n\u7684\u52a0\u5de5\u65f6\u95f4\u4e3a3\uff0c\u5728\u7b2c\u4e8c\u53f0\u8bbe\u5907\u4e0a\u7684\u52a0\u5de5\u65f6\u95f4\u4e3a2\u3002 - \u96f6\u4ef6C\u5728\u7b2c\u4e00\u53f0\u8bbe\u5907\u4e0a\u7684\u52a0\u5de5\u65f6\u95f4\u4e3a6\uff0c\u5728\u7b2c\u4e8c\n\u53f0\u8bbe\u5907\u4e0a\u7684\u52a0\u5de5\u65f6\u95f4\u4e3a4\u3002 - \u96f6\u4ef6D\u5728\u7b2c\u4e00\u53f0\u8bbe\u5907\u4e0a\u7684\u52a0\u5de5\u65f6\u95f4\u4e3a8\uff0c\u5728\u7b2c\u4e8c\u53f0\u8bbe\u5907\u4e0a\u7684\u52a0\u5de5\u65f6\u95f4\u4e3a9\u3002\n\u6309\u7167\u7ea6\u7ff0\u900a\u6cd5\u5219\uff0c\u6211\u4eec\u5e94\u8be5\u9996\u5148\u627e\u51fa\u6700\u77ed\u7684\u52a0\u5de5\u65f6\u95f4\uff0c\u7136\u540e\u6309\u7167\u4e0a\u8ff0\u89c4\u5219\u8fdb\u884c\u6392\u5e8f\u3002 \u6700\u77ed\u7684\u52a0\u5de5\u65f6\n\u95f4\u662f\u96f6\u4ef6B\u5728\u7b2c\u4e8c\u53f0\u8bbe\u5907\u4e0a\u76842\u5c0f\u65f6\uff0c\u6240\u4ee5B\u5e94\u8be5\u653e\u5728\u6700\u524d\u9762\u3002\u63a5\u4e0b\u6765\u662f\u96f6\u4ef6C\u5728\u7b2c\u4e8c\u53f0\u8bbe\u5907\u4e0a\u76844\u5c0f\u65f6\uff0c\nC\u5e94\u8be5\u653e\u5728B\u4e4b\u540e\u3002\u7136\u540e\u662f\u96f6\u4ef6A\u5728\u7b2c\u4e00\u53f0\u8bbe\u5907\u4e0a\u76849\u5c0f\u65f6\u548c\u96f6\u4ef6D\u5728\u7b2c\u4e00\u53f0\u8bbe\u5907\u4e0a\u76848\u5c0f\u65f6\uff0c\u56e0\u4e3aD\u7684\u65f6\n\u95f4\u66f4\u77ed\uff0c\u6240\u4ee5D\u5e94\u8be5\u653e\u5728C\u4e4b\u540e\u3002\u6700\u540e\u662f\u96f6\u4ef6A\u3002 \u56e0\u6b64\uff0c\u6700\u4f18\u7684\u52a0\u5de5\u987a\u5e8f\u5e94\u4e3a B-C-D-A\u3002\n\u6b63\u786e\u7b54\u6848\uff1a (C) B-C-D-A\nGround Truth: (D) D-A-C-B\nError Category: Textual Understanding\nError Reason:GPT-4V misunderstood the meaning of the task. During the reasoning process, it \noverlooked the prerequisite that the processing should first occur on the first device before moving \nto the second device. This oversight led to errors in the reasoning process\n<\u56fe\u7247 1>\nFigure B101: A sample error case of Business (subcategory: Management).\nBack to List of figures\n120\nPREPRINT\n\u5546\u4e1a: \u7ba1\u7406\nQuestion: \u5df2\u77e5\u6709\u4e24\u5bb6\u4e92\u4e0d\u76f8\u5173\u7684\u516c\u53f8\uff0c\u5176\u6709\u5173\u8d44\u6599\u5982\u4e0b<\u56fe\u7247 1>\uff0c\u8ba1\u7b97\u51faB\u516c\u53f8\u7684\u606f\u7a0e\u524d\u8d44\u4ea7\u5229\u6da6\u7387\n\u4e3a_%\nGPT-4V:\u606f\u7a0e\u524d\u8d44\u4ea7\u5229\u6da6\u7387\uff08EBIT Asset Return Rate\uff09\u662f\u4e00\u4e2a\u8d22\u52a1\u6307\u6807\uff0c\u7528\u6765\u8861\u91cf\u516c\u53f8\u5728\u6263\u9664\u5229\u606f\n\u548c\u7a0e\u9879\u524d\uff0c\u901a\u8fc7\u5176\u8d44\u4ea7\u8d5a\u53d6\u5229\u6da6\u7684\u80fd\u529b\u3002\u8ba1\u7b97\u516c\u5f0f\u4e3a\uff1a \u606f\u7a0e\u524d\u5229\u6da6\uff08EBIT\uff09/ \u603b\u8d44\u4ea7 = \u606f\u7a0e\u524d\u8d44\u4ea7\n\u5229\u6da6\u7387 \u6839\u636e\u56fe\u7247\u4e2d\u63d0\u4f9b\u7684B\u516c\u53f8\u7684\u6570\u636e\uff0c\u6211\u4eec\u53ef\u4ee5\u8ba1\u7b97\u51faB\u516c\u53f8\u7684\u606f\u7a0e\u524d\u8d44\u4ea7\u5229\u6da6\u7387\uff1a EBIT\uff08\u606f\u7a0e\u524d\n\u5229\u6da6\uff09= 36000 \u603b\u8d44\u4ea7 = 8000 \u606f\u7a0e\u524d\u8d44\u4ea7\u5229\u6da6\u7387 = (EBIT / \u603b\u8d44\u4ea7) * 100% \u606f\u7a0e\u524d\u8d44\u4ea7\u5229\u6da6\u7387 =\n(36000 / 8000) * 100% \u606f\u7a0e\u524d\u8d44\u4ea7\u5229\u6da6\u7387 = 4.5 * 100% \u606f\u7a0e\u524d\u8d44\u4ea7\u5229\u6da6\u7387 = 450% \u56e0\u6b64\uff0cB\u516c\u53f8\n\u7684\u606f\u7a0e\u524d\u8d44\u4ea7\u5229\u6da6\u7387\u4e3a450%\u3002\nGround Truth: 22.86\nError Category: Lack of Knowledge\nError Reason:GPT-4V mistakenly provided an incorrect formula for calculating the Earnings Before \nInterest and Taxes (EBIT) to Total Assets ratio. The correct formula should be Earnings Before \nInterest and Taxes (EBIT) divided by the average total assets employed. Additionally, GPT-4V \nexhibited deviation in processing image data, inaccurately reading the values of Earnings Before \nInterest and Taxes (EBIT) and total assets\n<\u56fe\u7247 1>\nFigure B102: A sample error case of Business (subcategory: Management).\nBack to List of figures\n121\nPREPRINT\n\u5546\u4e1a: \u7ba1\u7406\nQuestion: \u632f\u5b81\u516c\u53f8\u73b0\u6709A\u3001B\u3001C\u4e09\u4e2a\u6295\u8d44\u4e2d\u5fc3\uff0c2006\u5e74\u5404\u6295\u8d44\u4e2d\u5fc3\u7684\u6709\u5173\u8d44\u6599\u5982\u8868\u6240\u793a<\u56fe\u7247 1>\uff1a\u8ba1\n\u7b97A\u4e2d\u5fc3\u7684\u5269\u4f59\u6536\u76ca\u4e3a_\nGPT-4V:\u4e3a\u4e86\u8ba1\u7b97A\u4e2d\u5fc3\u7684\u5269\u4f59\u6536\u76ca\uff0c\u6211\u4eec\u9700\u8981\u4f7f\u7528\u4ee5\u4e0b\u516c\u5f0f\uff1a \u5269\u4f59\u6536\u76ca = \u8425\u4e1a\u6536\u5165 - (\u8d44\u672c\u6210\u672c\n\u7387 \u00d7 \u8d44\u672c\u6295\u8d44\u989d) \u6839\u636e\u56fe\u7247\u4e2d\u63d0\u4f9b\u7684\u6570\u636e\uff0cA\u4e2d\u5fc3\u7684\u8425\u4e1a\u6536\u5165\u4e3a270\u4e07\u5143\uff0c\u8d44\u672c\u6210\u672c\u7387\u4e3a18%\uff0c\u8d44\u672c\n\u6295\u8d44\u989d\u4e3a150\u4e07\u5143\u3002\u5c06\u8fd9\u4e9b\u6570\u503c\u4ee3\u5165\u516c\u5f0f\u4e2d\u8ba1\u7b97\uff1a \u5269\u4f59\u6536\u76ca = 270 - (18% \u00d7 150) \u5269\u4f59\u6536\u76ca = 270\n- 27 \u5269\u4f59\u6536\u76ca = 243\u4e07\u5143 \u56e0\u6b64\uff0cA\u4e2d\u5fc3\u7684\u5269\u4f59\u6536\u76ca\u4e3a243\u4e07\u5143\u3002\nGround Truth: 4.2\u4e07\u5143\nError Category: Lack of Knowledge\nError Reason:GPT-4V provided an incorrect calculation method for residual income, stating that it \nis equal to profit minus (capital cost rate \u00d7 capital investment). In the case of Center A, with a profit \nof 285,000, an average operating asset of 1,350,000, and a capital cost rate of 18%, the correct result \nshould be 285,000 - (1,350,000 \u00d7 18%) = 42,000.\n<\u56fe\u7247 1>\nFigure B103: A sample error case of Business (subcategory: Management).\nBack to List of figures\n122\nPREPRINT\n\u5546\u4e1a: \u7ba1\u7406\nQuestion: \u67d0\u4f01\u4e1a\u67d0\u90e8\u95e8\u8fd0\u7528\u4e00\u4e00\u5bf9\u6bd4\u6cd5\u5bf9\u6240\u5c5e\u76844\u540d\u5458\u5de5\u8fdb\u884c\u7ee9\u6548\u8003\u6838\uff0c\u8003\u6838\u60c5\u51b5\u5982\u4e0b\u8868\u6240<\u56fe\u7247 1>\n\u793a:\u7531\u6b64\u53ef\u77e5\uff0c\u7ee9\u6548\u6700\u5dee\u7684\u5458\u5de5\u662f\uff08\uff09\noption: \n(A) \u5f20XX \n(B) \u738bXX \n(C) \u674eXX \n(D) \u8d75XX\nGPT-4V:\u6839\u636e\u63d0\u4f9b\u7684\u8868\u683c\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u6bcf\u4e2a\u5458\u5de5\u7684\u7ee9\u6548\u8003\u6838\u7ed3\u679c\u662f\u901a\u8fc7\u4e0e\u5176\u4ed6\u5458\u5de5\u7684\u4e00\u5bf9\u4e00\u5bf9\u6bd4\u6765\n\u786e\u5b9a\u7684\u3002\u8868\u683c\u4e2d\u7684\u201c+\u201d\u8868\u793a\u80dc\u8fc7\u5bf9\u65b9\uff0c\u201c-\u201d\u8868\u793a\u4e0d\u5982\u5bf9\u65b9\uff0c\u201c0\u201d\u8868\u793a\u81ea\u5df1\uff0c\u4e0d\u8fdb\u884c\u6bd4\u8f83\u3002 \u4ece\u8868\u683c\n\u4e2d\u6211\u4eec\u53ef\u4ee5\u770b\u51fa\uff1a - \u5f20XX\u80dc\u8fc7\u674eXX\u548c\u8d75XX\uff0c\u4f46\u8f93\u7ed9\u4e86\u738bXX\u3002 - \u738bXX\u80dc\u8fc7\u6240\u6709\u5176\u4ed6\u4e09\u4eba\u3002 - \u674eXX\u8f93\n\u7ed9\u4e86\u6240\u6709\u5176\u4ed6\u4e09\u4eba\u3002 - \u8d75XX\u80dc\u8fc7\u674eXX\uff0c\u4f46\u8f93\u7ed9\u4e86\u5f20XX\u548c\u738bXX\u3002 \u56e0\u6b64\uff0c\u7ee9\u6548\u6700\u5dee\u7684\u5458\u5de5\u662f\u674eXX\uff0c\u56e0\u4e3a\n\u4ed6\u5728\u4e0e\u5176\u4ed6\u6240\u6709\u5458\u5de5\u7684\u5bf9\u6bd4\u4e2d\u90fd\u662f\u8d1f\u9762\u7684\u3002\n\u6b63\u786e\u7b54\u6848\uff1a(C) \u674eXX\u3002\nGround Truth: (D) \u8d75XX\nError Category: Lack of Knowledge\nError Reason:GPT-4V lacks relevant knowledge about the 'one-to-one comparison method,' leading \nto incorrect processing during comparative ranking. It mistakenly interprets higher comparisons as \nlower ones, resulting in erroneous inferences\n<\u56fe\u7247 1>\n<\u56fe\u7247 1>\nFigure B104: A sample error case of Business (subcategory: Management).\nBack to List of figures\n123\nPREPRINT\n\u5546\u4e1a: \u8425\u9500\nQuestion: \u8425\u9500\u73af\u5883\u4e0d\u4ee5\u67d0\u4e2a\u8425\u9500\u7ec4\u7ec7\u6216\u4e2a\u4eba\u7684\u610f\u5fd7\u4e3a\u8f6c\u79fb,\u5b83\u6709\u81ea\u5df1\u7684\u8fd0\u884c\u89c4\u5f8b\u548c\u7279\u70b9\u3002\u4f01\u4e1a\u7684\u8425\u9500\n\u6d3b\u52a8\u53ea\u80fd\u4e3b\u52a8\u5730\u9002\u5e94\u548c\u5229\u7528\u5ba2\u89c2\u73af\u5883,\u4e0d\u80fd\u6539\u53d8\u6216\u8fdd\u80cc\u3002\u5ba2\u89c2\u3001\u516c\u6b63\u5730\u5206\u6790\u8425\u9500\u73af\u5883\u56e0\u7d20\u624d\u80fd\u51cf\u5c11\u8425\n\u9500\u51b3\u7b56\u7684\u76f2\u76ee\u548c\u5931\u8bef,\u8d62\u5f97\u8425\u9500\u6d3b\u52a8\u7684\u6210\u529f\u3002\u7531\u6b64\u53ef\u77e5\u8be5\u56fe<\u56fe\u7247 1>\u4e3a__\nOption: \n(A) \u4ea4\u6613\u5e02\u573a\u8425\u9500\u73af\u5883\n(B) \u4ea4\u6613\u5e02\u573a\u5e7f\u544a\u73af\u5883\n(C) \u4f01\u4e1a\u5e02\u573a\u5e7f\u544a\u73af\u5883\n(D) \u4f01\u4e1a\u5e02\u573a\u8425\u9500\u73af\u5883\nGPT-4V: \u8fd9\u5f20\u56fe\u7247\u5c55\u793a\u7684\u662f\u4e00\u4e2a\u8425\u9500\u73af\u5883\u5206\u6790\u6a21\u578b\uff0c\u4e2d\u5fc3\u5199\u7740\u201c\u4f01\u4e1a\u201d\uff0c\u5468\u56f4\u5206\u4e3a\u4e0d\u540c\u7684\u90e8\u5206\uff0c\u5305\u62ec\n\u201c\u653f\u6cbb\u6cd5\u5f8b\u201d\u3001\u201c\u793e\u4f1a\u6587\u5316\u201d\u3001\u201c\u4e2d\u95f4\u5546\u201d\u3001\u201c\u4f9b\u5e94\u5546\u201d\u3001\u201c\u987e\u5ba2\u201d\u3001\u201c\u7ade\u4e89\u5bf9\u624b\u201d\u3001\u201c\u516c\u4f17\u201d\u548c\u201c\n\u6280\u672f\u73af\u5883\u201d\u3002\u8fd9\u4e2a\u6a21\u578b\u7528\u4e8e\u5e2e\u52a9\u5206\u6790\u548c\u7406\u89e3\u4e00\u4e2a\u4f01\u4e1a\u5728\u8fdb\u884c\u8425\u9500\u6d3b\u52a8\u65f6\u9700\u8981\u8003\u8651\u7684\u5916\u90e8\u73af\u5883\u56e0\u7d20\u3002\n\u6b63\u786e\u7b54\u6848\uff1a (D) \u4f01\u4e1a\u5e02\u573a\u8425\u9500\u73af\u5883\nGround Truth: (D) \u4f01\u4e1a\u5e02\u573a\u8425\u9500\u73af\u5883\n<\u56fe\u7247 1>\nFigure B105: A sample correct case of Business (subcategory: Marketing).\nBack to List of figures\n124\nPREPRINT\n\u5546\u4e1a: \u8425\u9500\nQuestion:\u67d0\u516c\u53f8\u9700\u8981\u5bf9\u67d0\u65b0\u4ea7\u54c1\u751f\u4ea7\u6279\u91cf\u505a\u51fa\u51b3\u7b56,\u5404\u79cd\u6279\u91cf\u5728\u4e0d\u4e0d\u540c\u7684\u81ea\u7136\u72b6\u6001\u4e0b\u7684\u6536\u76ca\u60c5\u51b5\u5982\n\u4e0b\u8868(\u6536\u76ca\u77e9\u9635)<\u56fe\u7247 1>,\u7528\u60b2\u89c2\u51b3\u7b56\u65b9\u6cd5,\u5bf9\u4e0a\u8ff0\u751f\u4ea7\u7ecf\u8425\u95ee\u9898\u505a\u51fa\u51b3\u7b56\u65b9\u6848\uff0c\u9009\u62e9________\u65b9\u6848\u3002\nOption: \n(A) S1 \n(B) S2 \n(C) S3\n(D) S2\u548cS3\nGPT-4V: \u60b2\u89c2\u51b3\u7b56\u65b9\u6cd5\uff0c\u53c8\u79f0\u4e3aMaximin\u51c6\u5219\uff0c\u662f\u4e00\u79cd\u5728\u4e0d\u786e\u5b9a\u6761\u4ef6\u4e0b\u7684\u51b3\u7b56\u65b9\u6cd5\u3002\u8fd9\u79cd\u65b9\u6cd5\u8003\u8651\u7684\n\u662f\u6bcf\u4e2a\u7b56\u7565\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u6700\u5927\u6536\u76ca\u3002\u5177\u4f53\u64cd\u4f5c\u662f\u5148\u627e\u51fa\u6bcf\u4e2a\u7b56\u7565\u4e0b\u6700\u574f\u60c5\u51b5\u7684\u6536\u76ca\uff0c\u7136\u540e\u4ece\u4e2d\u9009\u62e9\n\u6536\u76ca\u6700\u5927\u7684\u7b56\u7565\u3002\n\u6839\u636e\u63d0\u4f9b\u7684\u6536\u76ca\u77e9\u9635\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff1a\n-\u5bf9\u4e8e\u7b56\u7565S1\uff0c\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u6536\u76ca\u662f-6\u3002\n-\u5bf9\u4e8e\u7b56\u7565S2\uff0c\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u6536\u76ca\u662f-2\u3002\n-\u5bf9\u4e8e\u7b56\u7565S3\uff0c\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u6536\u76ca\u662f5\u3002\n\u5728\u8fd9\u4e09\u4e2a\u7b56\u7565\u4e2d\uff0c\nS3\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u6536\u76ca\u6700\u9ad8\uff0c\u4e3a5\u3002\u56e0\u6b64\uff0c\u6839\u636e\u60b2\u89c2\u51b3\u7b56\u65b9\u6cd5\uff0c\u6211\u4eec\u5e94\u8be5\u9009\u62e9S3\u3002\n\u6b63\u786e\u7b54\u6848\uff1a (C) S3\nGround Truth: (C) S3\n<\u56fe\u7247 1>\nFigure B106: A sample correct case of Business (subcategory: Marketing).\nBack to List of figures\n125\nPREPRINT\n\u5546\u4e1a: \u8425\u9500\nQuestion:\u73b0\u6709\u56db\u79cd\u8bc1\u5238\u8d44\u6599\u5982\u4e0b:<\u56fe\u7247\n1>\u65e0\u98ce\u9669\u62a5\u916c\u7387\u4e3a8%,\u5e02\u573a\u4e0a\u6240\u6709\u8bc1\u5238\u7684\u7ec4\u5408\u62a5\u916c\u7387\u4e3a14%\u3002\n\u8981\u6c42:\u8ba1\u7b97\u4e0a\u8ff0\u56db\u79cd\u8bc1\u5238\u5404\u81ea\u7684\u5fc5\u8981\u62a5\u916c\u7387\n\ud835\udc45\ud835\udc45 \ud835\udc34\ud835\udc34 = ______;\ud835\udc45\ud835\udc45 \ud835\udc35\ud835\udc35 = ______;\ud835\udc45\ud835\udc45 \ud835\udc36\ud835\udc36 =______; \ud835\udc45\ud835\udc45 \ud835\udc37\ud835\udc37 =______;\nOption: \n(A) 10.4%\uff1b14%\uff1b17%\uff1b23% \n(B) 17%\uff1b14%\uff1b10.4%\uff1b23% \n(C) 14%\uff1b17%\uff1b23%\uff1b10.4% \n(D) 17%\uff1b23%\uff1b14%\uff1b10.4%\nGPT-4V:\u4e3a\u4e86\u8ba1\u7b97\u6bcf\u79cd\u8bc1\u5238\u7684\u5fc5\u8981\u62a5\u916c\u7387\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u8d44\u672c\u8d44\u4ea7\u5b9a\u4ef7\u6a21\u578b\uff08Capital Asset \nPricing Model, CAPM\uff09\uff0c\u8be5\u6a21\u578b\u7684\u516c\u5f0f\u4e3a\uff1a \ud835\udc45\ud835\udc45\ud835\udc56\ud835\udc56 = \ud835\udc45\ud835\udc45\ud835\udc53\ud835\udc53 + \ud835\udefd\ud835\udefd\ud835\udc56\ud835\udc56 \ud835\udc45\ud835\udc45\ud835\udc5a\ud835\udc5a \u2212 \ud835\udc45\ud835\udc45\ud835\udc53\ud835\udc53 \u5176\u4e2d\uff1a \ud835\udc45\ud835\udc45\ud835\udc56\ud835\udc56 \u662f\u8bc1\u5238 i \u7684\u5fc5\u8981\u62a5\n\u916c\u7387 \ud835\udc45\ud835\udc45\ud835\udc53\ud835\udc53\u662f\u65e0\u98ce\u9669\u62a5\u916c\u7387\n\ud835\udefd\ud835\udefd\ud835\udc56\ud835\udc56 \u662f\u8bc1\u5238 i \u7684\u98ce\u9669\u7cfb\u6570\uff08Beta\uff09, \ud835\udc45\ud835\udc45\ud835\udc5a\ud835\udc5a \u662f\u5e02\u573a\u7ec4\u5408\u7684\u62a5\u916c\u7387 \u6839\u636e\u9898\u76ee\uff0c\n\u6211\u4eec\u77e5\u9053\n\ud835\udc45\ud835\udc45\ud835\udc53\ud835\udc53 = 8% \u548c \ud835\udc45\ud835\udc45\ud835\udc5a\ud835\udc5a = 14% \u3002\u73b0\u5728\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u6bcf\u79cd\u8bc1\u5238\u7684 Beta \u503c\u6765\u8ba1\u7b97\u5b83\u4eec\u7684\u5fc5\u8981\u62a5\u916c\n\u7387\u3002 \u5bf9\u4e8e\u8bc1\u5238 A\uff1a \ud835\udc45\ud835\udc45\ud835\udc34\ud835\udc34 = 8% + 1.5 \u00d7 14% \u2212 8% = 8% + 1.5 \u00d7 6% = 8% + 9% = 17% \u5bf9\n\u4e8e\u8bc1\u5238 B\uff1a\ud835\udc45\ud835\udc45\ud835\udc35\ud835\udc35 = 8% + 1.0 \u00d7 14% \u2212 8% = 8% + 6% = 14%.\u5bf9\u4e8e\u8bc1\u5238 C\uff1a\ud835\udc45\ud835\udc45\ud835\udc36\ud835\udc36 =  8% +  0.4 \n\u00d7 14% \u2212  8% =  8% +  0.4 \u00d7 6% =  8% +  2.4% =  10.4% \u5bf9\u4e8e\u8bc1\u5238 D\uff1a \ud835\udc45\ud835\udc45\ud835\udc37\ud835\udc37 =  8% +  2.5 \n\u00d7 14% \u2212  8% =  8% +  2.5 \u00d7 6% =  8% +  15% =  23%\u3002  \u56e0\u6b64\uff0c\u6bcf\u79cd\u8bc1\u5238\u7684\u5fc5\u8981\u62a5\u916c\u7387\u5206\u522b\u4e3a\uff1a\n\ud835\udc45\ud835\udc45\ud835\udc34\ud835\udc34 =  17%\uff0c\ud835\udc45\ud835\udc45\ud835\udc35\ud835\udc35 =  14% \uff0c\ud835\udc45\ud835\udc45\ud835\udc36\ud835\udc36 =  10.4% \uff0c\ud835\udc45\ud835\udc45\ud835\udc37\ud835\udc37 =  23% \u6b63\u786e\u7b54\u6848\u662f (B) 17%\uff1b14%\uff1b10.4%\uff1b23%\u3002\nGround Truth: (B) 17%\uff1b14%\uff1b10.4%\uff1b23% \n<\u56fe\u7247 1>\nFigure B107: A sample correct case of Business (subcategory: Marketing).\nBack to List of figures\n126\nPREPRINT\n\u5546\u4e1a: \u8425\u9500\nQuestion: \n\u67d0\u9ad8\u6821\u5728\u6821\u672c\u79d1\u751f40000\u4eba,\u5206\u4f18\u3001\u826f\u3001\u4e2d\u3001\u5dee4\u4e2a\u5c42\u6b21,\u5176\u4ed6\u8d44\u6599\u5982\u8868\u6240\u793a<\u56fe\u7247\n1>\u3002\u5f53\nn=400 \u4eba\u65f6,\u8bd5\u6309\u5206\u5c42\u6700\u4f73\u6bd4\u4f8b\u62bd\u6837\u6cd5\u786e\u5b9a\u4f18\u3001\u826f\u3001\u4e2d\u3001\u5dee\u5404\u5c42\u62bd\u6837\u6570_____________\u3002\nOption: \n(A) 45 \u3001116 \u3001176 \u300163 \n(B) 35 \u3001136 \u3001156 \u300173 \n(C) 55 \u3001116 \u3001166 \u300183 \n(D) 25 \u3001116 \u3001176 \u300183\nGPT-4V:\u4e3a\u4e86\u786e\u5b9a\u6bcf\u4e2a\u5c42\u6b21\u7684\u62bd\u6837\u6570\uff0c\u6211\u4eec\u9700\u8981\u4f7f\u7528\u5206\u5c42\u62bd\u6837\u7684\u516c\u5f0f\u3002\u5206\u5c42\u62bd\u6837\u7684\u76ee\u7684\u662f\u786e\u4fdd\u6bcf\u4e2a\u5c42\n\u6b21\u5728\u6837\u672c\u4e2d\u7684\u4ee3\u8868\u6027\u4e0e\u5176\u5728\u603b\u4f53\u4e2d\u7684\u6bd4\u4f8b\u76f8\u7b26\u3002\u5728\u5206\u5c42\u6700\u4f73\u6bd4\u4f8b\u62bd\u6837\u6cd5\u4e2d\uff0c\u6bcf\u4e2a\u5c42\u6b21\u7684\u62bd\u6837\u6570\u662f\u6839\u636e\n\u5c42\u5185\u603b\u6570(Ni)\u548c\u5c42\u5185\u6807\u51c6\u5dee(Si)\u7684\u4e58\u79ef\u7684\u6bd4\u4f8b\u6765\u786e\u5b9a\u7684\u3002 \u9996\u5148\uff0c\u6211\u4eec\u9700\u8981\u8ba1\u7b97\u6bcf\u4e2a\u5c42\u6b21\u7684NiSi\u503c\uff0c\n\u7136\u540e\u8ba1\u7b97\u6240\u6709\u5c42\u6b21NiSi\u503c\u7684\u603b\u548c\u3002\u4e4b\u540e\uff0c\u6bcf\u4e2a\u5c42\u6b21\u7684\u62bd\u6837\u6570\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u516c\u5f0f\u8ba1\u7b97\uff1a\u62bd\u6837\u6570 \ud835\udc5b\ud835\udc5b\ud835\udc5b\ud835\udc5b\n=\n\ud835\udc41\ud835\udc41\ud835\udc41\ud835\udc41\ud835\udc41\ud835\udc41\ud835\udc41\ud835\udc41\n\u603b\ud835\udc41\ud835\udc41\ud835\udc41\ud835\udc41\ud835\udc41\ud835\udc41\ud835\udc41\ud835\udc41 \u2217  \u603b\u62bd\u6837\u6570 \ud835\udc5b\ud835\udc5b\n\u6839\u636e\u56fe\u7247\u4e2d\u7684\u8868\u683c\uff0c\u6211\u4eec\u53ef\u4ee5\u8ba1\u7b97\u51fa\u6bcf\u4e2a\u5c42\u6b21\u7684NiSi\u503c\uff0c\u7136\u540e\u7528\u4e0a\u8ff0\u516c\u5f0f\n\u8ba1\u7b97\u51fa\u6bcf\u4e2a\u5c42\u6b21\u7684\u62bd\u6837\u6570\u3002 \u4f18\u5c42\u7684\ud835\udc41\ud835\udc41\ud835\udc5b\ud835\udc5b\ud835\udc46\ud835\udc46\ud835\udc5b\ud835\udc5b = 6000 \u2217 4 = 24000 \u826f\u5c42\u7684\ud835\udc41\ud835\udc41\ud835\udc5b\ud835\udc5b\ud835\udc46\ud835\udc46\ud835\udc5b\ud835\udc5b = 14000 \u2217 5\n= 70000 \u4e2d\u5c42\u7684\ud835\udc41\ud835\udc41\ud835\udc5b\ud835\udc5b\ud835\udc46\ud835\udc46\ud835\udc5b\ud835\udc5b = 16000 \u2217 5 = 80000 \u5dee\u5c42\u7684\ud835\udc41\ud835\udc41\ud835\udc5b\ud835\udc5b\ud835\udc46\ud835\udc46\ud835\udc5b\ud835\udc5b = 4000 \u2217 6 = 24000 \u603b\ud835\udc41\ud835\udc41\ud835\udc5b\ud835\udc5b\ud835\udc46\ud835\udc46\ud835\udc5b\ud835\udc5b = 24000\n+ 70000 + 80000 + 24000 = 198000 \u73b0\u5728\u6211\u4eec\u53ef\u4ee5\u8ba1\u7b97\u6bcf\u4e2a\u5c42\u6b21\u7684\u62bd\u6837\u6570\uff1a \u4f18\u5c42\u7684\u62bd\u6837\u6570\n= (24000 / 198000) \u2217  400 \u2248  48.48 \u2248  48\uff08\u56db\u820d\u4e94\u5165\uff09 \u826f\u5c42\u7684\u62bd\u6837\u6570 = (70000 / 198000) * 400 \n\u2248 141.41 \u2248 141\uff08\u56db\u820d\u4e94\u5165\uff09 \u4e2d\u5c42\u7684\u62bd\u6837\u6570 = (80000 / 198000) * 400 \u2248 161.62 \u2248 162\uff08\u56db\n\u820d\u4e94\u5165\uff09 \u5dee\u5c42\u7684\u62bd\u6837\u6570 = (24000 / 198000) * 400 \u2248 48.48 \u2248 48\uff08\u56db\u820d\u4e94\u5165\uff09 \u603b\u62bd\u6837\u6570 = 48 \n+ 141 + 162 + 48 = 399 \u7531\u4e8e\u603b\u62bd\u6837\u6570\u5e94\u8be5\u662f400\uff0c\u6211\u4eec\u9700\u8981\u5bf9\u67d0\u4e2a\u5c42\u6b21\u7684\u62bd\u6837\u6570\u8fdb\u884c\u8c03\u6574\uff0c\u4ee5\u786e\u4fdd\n\u603b\u6570\u4e3a400\u3002\u901a\u5e38\uff0c\u6211\u4eec\u4f1a\u589e\u52a0\u6700\u5927\u5c42\u6b21\u7684\u62bd\u6837\u6570\uff0c\u56e0\u6b64\u4e2d\u5c42\u7684\u62bd\u6837\u6570\u5e94\u8be5\u662f163\u3002 \u6700\u7ec8\u7684\u62bd\u6837\u6570\u4e3a\n\uff1a \u4f18\u5c42\uff1a48 \u826f\u5c42\uff1a141 \u4e2d\u5c42\uff1a163 \u5dee\u5c42\uff1a48 \u8fd9\u4e9b\u6570\u503c\u5e76\u4e0d\u5b8c\u5168\u5339\u914d\u6240\u7ed9\u7684\u9009\u9879\uff0c\u4f46\u662f\u6839\u636e\u8ba1\u7b97\u8fc7\n\u7a0b\uff0c\u6211\u4eec\u53ef\u4ee5\u9009\u62e9\u6700\u63a5\u8fd1\u7684\u9009\u9879\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u6ca1\u6709\u4e00\u4e2a\u9009\u9879\u5b8c\u5168\u5339\u914d\u6211\u4eec\u7684\u8ba1\u7b97\u7ed3\u679c\uff0c\u4f46\u662f\u9009\u9879\n(A) 45\u3001116\u3001176\u300163\u662f\u6700\u63a5\u8fd1\u7684\uff0c\u5c3d\u7ba1\u5b83\u4e0e\u6211\u4eec\u7684\u8ba1\u7b97\u7ed3\u679c\u6709\u6240\u5dee\u5f02\u3002\u5982\u679c\u5fc5\u987b\u9009\u62e9\u4e00\u4e2a\u9009\u9879\uff0c\u6211\n\u4eec\u5c06\u9009\u62e9(A)\u4f5c\u4e3a\u6700\u63a5\u8fd1\u7684\u7b54\u6848\u3002\u7136\u800c\uff0c\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u4e2a\u7b54\u6848\u5e76\u4e0d\u7cbe\u786e\uff0c\u53ef\u80fd\u662f\u7531\u4e8e\u56db\u820d\u4e94\u5165\u6216\n\u5176\u4ed6\u8ba1\u7b97\u5dee\u5f02\u5bfc\u81f4\u7684\u3002\nGround Truth: (D) 25 \u3001116 \u3001176 \u300183\nError Category: Perceptual Error\nError Reason:GPT-4V has no issues in understanding and inferring the textual content of the \nquestion. However, it encountered difficulties in interpreting images, mistakenly treating the\n\ud835\udc36\ud835\udc36\ud835\udc41\ud835\udc41 as \nSi. This resulted in problems throughout the subsequent calculation process, ultimately leading to \nan erroneous inference.\n<\u56fe\u7247 1>\nFigure B108: A sample error case of Business (subcategory: Marketing).\nBack to List of figures\n127\nPREPRINT\n\u5546\u4e1a: \u8425\u9500\nQuestion: \u67d0\u4f01\u4e1a\u6309\u4e94\u5e74\u8ba1\u5212\u89c4\u5b9a,\u67d0\u4ea7\u54c1\u6700\u540e\u4e00\u5e74\u4ea7\u91cf\u5e94\u8fbe\u5230200\u4e07\u5428,\u5404\u5e74\u5b9e\u9645\u751f\u4ea7\u60c5\u51b5\u5982\u8868\u6240\u793a:<\n\u56fe\u7247 1>\u8ba1\u7b97\u8be5\u4ea7\u54c1\u4e94\u5e74\u8ba1\u5212\u5b8c\u6210\u7a0b\u5ea6______\u548c\u63d0\u524d\u5b8c\u6210\u4e94\u5e74\u8ba1\u5212\u6307\u6807\u7684\u65f6\u95f4_______\u3002\nOption: \n(A) 92.3%\uff1b\u5ef6\u540e\u4e00\u4e2a\u5b63\u5ea6\n(B) 97.4%\uff1b\u5ef6\u540e\u4e00\u4e2a\u5b63\u5ea6\n(C) 107.5%\uff1b\u63d0\u524d\u4e00\u4e2a\u5b63\u5ea6\n(D) 112.6%\uff1b\u63d0\u524d\u4e00\u4e2a\u5b63\u5ea6\nGPT-4V:\u4e3a\u4e86\u56de\u7b54\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u9700\u8981\u5148\u8ba1\u7b97\u4e94\u5e74\u603b\u4ea7\u91cf\uff0c\u7136\u540e\u5c06\u5176\u4e0e\u8ba1\u5212\u4ea7\u91cf\u8fdb\u884c\u6bd4\u8f83\uff0c\u4ee5\u786e\u5b9a\u5b8c\n\u6210\u7a0b\u5ea6\u3002\u540c\u65f6\uff0c\u6211\u4eec\u9700\u8981\u67e5\u770b\u5b9e\u9645\u4ea7\u91cf\u8fbe\u5230\u8ba1\u5212\u4ea7\u91cf200\u4e07\u5428\u7684\u65f6\u95f4\u70b9\u3002 \u6839\u636e\u63d0\u4f9b\u7684\u56fe\u7247\uff0c\u6211\u4eec\u53ef\u4ee5\n\u770b\u5230\u5404\u5e74\u7684\u4ea7\u91cf\u5982\u4e0b\uff1a - \u7b2c\u4e00\u5e74\uff1a160\u4e07\u5428 - \u7b2c\u4e8c\u5e74\uff1a165\u4e07\u5428 - \u7b2c\u4e09\u5e74\uff1a165\u4e07\u5428 - \u7b2c\u56db\u5e74\uff1a40\n\u4e07\u5428 + 45\u4e07\u5428 = 85\u4e07\u5428 - \u7b2c\u4e94\u5e74\uff1a45\u4e07\u5428 + 50\u4e07\u5428 + 55\u4e07\u5428 + 60\u4e07\u5428 = 210\u4e07\u5428 \u4e94\u5e74\u603b\u4ea7\u91cf\n= 160 + 165 + 165 + 85 + 210 = 785\u4e07\u5428 \u8ba1\u5212\u603b\u4ea7\u91cf = 200\u4e07\u5428 * 5\u5e74 = 1000\u4e07\u5428 \u5b8c\u6210\u7a0b\u5ea6 =\n(\u5b9e\u9645\u4ea7\u91cf / \u8ba1\u5212\u4ea7\u91cf) * 100% = (785 / 1000) * 100% = 78.5% \u7136\u800c\uff0c\u8fd9\u4e2a\u8ba1\u7b97\u4e0e\u6240\u7ed9\u9009\u9879\u4e0d\u5339\n\u914d\uff0c\u8fd9\u53ef\u80fd\u662f\u56e0\u4e3a\u56fe\u7247\u4e2d\u7684\u6570\u636e\u6ca1\u6709\u88ab\u6b63\u786e\u89e3\u8bfb\u3002\u8bf7\u786e\u4fdd\u56fe\u7247\u4e2d\u7684\u6570\u636e\u662f\u51c6\u786e\u7684\uff0c\u5e76\u4e14\u6ca1\u6709\u9057\u6f0f\u4efb\u4f55\n\u4fe1\u606f\u3002\n\u73b0\u5728\uff0c\u6211\u4eec\u9700\u8981\u786e\u5b9a\u63d0\u524d\u5b8c\u6210\u8ba1\u5212\u7684\u65f6\u95f4\u3002\u6839\u636e\u56fe\u7247\u4e2d\u7684\u6570\u636e\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u5728\u7b2c\u4e94\u5e74\u7684\u6bcf\n\u4e2a\u5b63\u5ea6\u4ea7\u91cf\u90fd\u6709\u6240\u589e\u52a0\u3002\u6211\u4eec\u9700\u8981\u8ba1\u7b97\u5728\u54ea\u4e2a\u5b63\u5ea6\u65f6\uff0c\u7d2f\u8ba1\u4ea7\u91cf\u9996\u6b21\u8fbe\u5230\u6216\u8d85\u8fc7200\u4e07\u5428\u3002 \u7d2f\u8ba1\u4ea7\u91cf\n\u8fbe\u5230200\u4e07\u5428\u7684\u65f6\u95f4\u70b9\uff1a - \u7b2c\u4e00\u5e74\u672b\uff1a160\u4e07\u5428 - \u7b2c\u4e8c\u5e74\u672b\uff1a160 + 165 = 325\u4e07\u5428 - \u7b2c\u4e09\u5e74\u672b\uff1a\n325 + 165 = 490\u4e07\u5428 - \u7b2c\u56db\u5e74\u672b\uff1a490 + 85 = 575\u4e07\u5428 - \u7b2c\u4e94\u5e74\u7b2c\u4e00\u5b63\u5ea6\u672b\uff1a575 + 45 = 620\u4e07\n\u5428 - \u7b2c\u4e94\u5e74\u7b2c\u4e8c\u5b63\u5ea6\u672b\uff1a620 + 50 = 670\u4e07\u5428 - \u7b2c\u4e94\u5e74\u7b2c\u4e09\u5b63\u5ea6\u672b\uff1a670 + 55 = 725\u4e07\u5428 - \u7b2c\u4e94\n\u5e74\u7b2c\u56db\u5b63\u5ea6\u672b\uff1a725 + 60 = 785\u4e07\u5428 \u7531\u4e8e\u5728\u7b2c\u4e94\u5e74\u7684\u6bcf\u4e2a\u5b63\u5ea6\u7ed3\u675f\u65f6\uff0c\u7d2f\u8ba1\u4ea7\u91cf\u90fd\u6ca1\u6709\u8fbe\u5230\u8ba1\u5212\u7684\n200\u4e07\u5428\uff0c\u56e0\u6b64\u6211\u4eec\u65e0\u6cd5\u786e\u5b9a\u63d0\u524d\u5b8c\u6210\u8ba1\u5212\u7684\u5177\u4f53\u65f6\u95f4\u3002\u8fd9\u610f\u5473\u7740\u6211\u4eec\u65e0\u6cd5\u6839\u636e\u6240\u7ed9\u7684\u9009\u9879\u6765\u786e\u5b9a\u6b63\n\u786e\u7b54\u6848\u3002\n\u7531\u4e8e\u6240\u7ed9\u7684\u9009\u9879\u4e0e\u8ba1\u7b97\u7ed3\u679c\u4e0d\u7b26\uff0c\u6211\u4eec\u53ef\u80fd\u9700\u8981\u66f4\u591a\u4fe1\u606f\u6216\u8005\u91cd\u65b0\u68c0\u67e5\u9898\u76ee\u548c\u56fe\u7247\u4e2d\u7684\u6570\n\u636e\u3002\u5982\u679c\u56fe\u7247\u4e2d\u7684\u6570\u636e\u662f\u51c6\u786e\u7684\uff0c\u90a3\u4e48\u6839\u636e\u4e0a\u8ff0\u8ba1\u7b97\uff0c\u6ca1\u6709\u4e00\u4e2a\u9009\u9879\u662f\u6b63\u786e\u7684\u3002\nGround Truth: (C) 107.5%\uff1b\u63d0\u524d\u4e00\u4e2a\u5b63\u5ea6\nError Category: Textual Understanding\nError Reason:GPT-4V incorrectly interpreted the text information in the question. The content of \nthe five-year plan should be \"the final year's production of a certain product should reach 2 million \ntons.\" However, GPT-4V mistakenly assumed that the annual production for each of the five years \nshould be 2 million tons. Therefore, the correct approach is to consider only the production \nsituation in the last year when providing an answer.\n<\u56fe\u7247 1>\nFigure B109: A sample error case of Business (subcategory: Marketing).\nBack to List of figures\n128\nPREPRINT\n\u5546\u4e1a: \u8425\u9500\nQuestion: \u4ece\u4e00\u884c\u4e1a\u4e2d\u968f\u673a\u62bd\u53d65\u5bb6\u4f01\u4e1a\uff0c\u6240\u5f97\u4ea7\u54c1\u4ea7\u91cf\u4e0e\u751f\u4ea7\u8d39\u7528\u7684\u6570\u636e\u5982\u4e0b\uff1a<\u56fe\u7247 1>\u5229\u7528\u6700\u5c0f\u4e8c\n\u4e58\u6cd5\u6c42\u51fa\u4f30\u8ba1\u7684\u56de\u5f52\u65b9\u7a0b____________\u3002\nOption: \n(A) \u0ddc\ud835\udc66\ud835\udc66 = 124.248 + 0.567\ud835\udc65\ud835\udc65\n(B) \u0ddc\ud835\udc66\ud835\udc66 = 124.248 + 0.732\ud835\udc65\ud835\udc65\n(C) \u0ddc\ud835\udc66\ud835\udc66 = 111.314 + 0.732\ud835\udc65\ud835\udc65\n(D) \u0ddc\ud835\udc66\ud835\udc66 = 111.314 + 0.567\ud835\udc65\ud835\udc65\nGPT-4V:\u5f88\u62b1\u6b49\uff0c\u6211\u65e0\u6cd5\u76f4\u63a5\u8ba1\u7b97\u51fa\u6700\u5c0f\u4e8c\u4e58\u6cd5\u7684\u56de\u5f52\u65b9\u7a0b\uff0c\u56e0\u4e3a\u8fd9\u9700\u8981\u8fdb\u884c\u6570\u5b66\u8ba1\u7b97\uff0c\u800c\u6211\u76ee\u524d\u65e0\n\u6cd5\u6267\u884c\u8fd9\u6837\u7684\u64cd\u4f5c\u3002\u4e0d\u8fc7\uff0c\u6211\u53ef\u4ee5\u544a\u8bc9\u4f60\u5982\u4f55\u624b\u52a8\u8ba1\u7b97\u5b83\u3002 \u6700\u5c0f\u4e8c\u4e58\u6cd5\u7684\u56de\u5f52\u65b9\u7a0b\u901a\u5e38\u91c7\u7528\u4ee5\u4e0b\u5f62\n\u5f0f\uff1a \u0ddc\ud835\udc66\ud835\udc66 = \ud835\udc4e\ud835\udc4e + \ud835\udc4f\ud835\udc4f\ud835\udc65\ud835\udc65\u5176\u4e2d\uff0c\u0ddcy\u662f\u56e0\u53d8\u91cf\u7684\u4f30\u8ba1\u503c\uff0c\ud835\udc65\ud835\udc65 \u662f\u81ea\u53d8\u91cf\uff0c\ud835\udc4f\ud835\udc4f \u662f\u659c\u7387\uff0c\ud835\udc4e\ud835\udc4e\u662f\u622a\u8ddd\u3002\u659c\u7387 \ud835\udc4f\ud835\udc4f\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\n\u516c\u5f0f\u8ba1\u7b97\uff1a \ud835\udc4f\ud835\udc4f =\n\ud835\udc5b\ud835\udc5b(\u2211 \ud835\udc65\ud835\udc65\ud835\udc66\ud835\udc66)\u2212(\u2211 \ud835\udc65\ud835\udc65)(\u2211 \ud835\udc66\ud835\udc66)\n\ud835\udc5b\ud835\udc5b \u2211 \ud835\udc65\ud835\udc652 \u2212 \u2211 \ud835\udc65\ud835\udc65 2 ,\u622a\u8ddd\ud835\udc4e\ud835\udc4e\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u516c\u5f0f\u8ba1\u7b97\uff1a\ud835\udc4e\ud835\udc4e =\n\u2211 \ud835\udc66\ud835\udc66\u2212\ud835\udc4f\ud835\udc4f(\u2211 \ud835\udc65\ud835\udc65)\n\ud835\udc5b\ud835\udc5b\n\u5176\u4e2d\uff0c\ud835\udc5b\ud835\udc5b \u662f\u89c2\u6d4b\u503c\u7684\n\u6570\u91cf\u3002 \u4f60\u9700\u8981\u4f7f\u7528\u4e0a\u8ff0\u516c\u5f0f\u548c\u63d0\u4f9b\u7684\u6570\u636e\u6765\u8ba1\u7b97\ud835\udc4e\ud835\udc4e\u548c\ud835\udc4f\ud835\udc4f\u7684\u503c\uff0c\u7136\u540e\u5c06\u5b83\u4eec\u4ee3\u5165\u56de\u5f52\u65b9\u7a0b\u4e2d\u3002\u5982\u679c\u4f60\u80fd\n\u63d0\u4f9b\u8ba1\u7b97\u7ed3\u679c\uff0c\u6211\u53ef\u4ee5\u5e2e\u4f60\u9a8c\u8bc1\u54ea\u4e2a\u9009\u9879\u662f\u6b63\u786e\u7684\u3002\nGround Truth: (D) \u0ddd\ud835\udc9a\ud835\udc9a = 111.314 + \ud835\udfce\ud835\udfce. \ud835\udfd3\ud835\udfd3\ud835\udfd3\ud835\udfd3\ud835\udfd3\ud835\udfd3\ud835\udc99\ud835\udc99\nError Category: Lack of Knowledge\nError Reason:GPT-4V provided the formula for calculating the regression equation in response to \nthe request to use the least squares method for the regression equation. However, it seems that \nGPT-4V lacks the direct capability to compute the regression equation using the least squares \nmethod, and therefore, it refused to provide an answer.\n<\u56fe\u7247 1>\nFigure B110: A sample error case of Business (subcategory: Marketing).\nBack to List of figures\n129\nPREPRINT\n\u5546\u4e1a: \u8425\u9500\nQuestion:\u5df2\u77e5\u89c2\u5bdf\u671f\u6570\u636e\u8d44\u6599\u5982\u8868\u6240\u793a<\u56fe\u72471>,\u5efa\u7acb\u4e00\u5143\u7ebf\u6027\u56de\u5f52\u65b9\u7a0b\u6a21\u578b\uff0c\u8ba1\u7b97\u76f8\u5173\u7cfb\u6570\nr=______\uff0c\u6807\u51c6\u8bef\u5dee\ud835\udc46\ud835\udc46y\n=\n_______ \u3002\nOption: \n(A) 0.987543\uff1b6 \n(B) 0.994343\uff1b8 \n(C) 0.987543\uff1b8 \n(D) 0.994343\uff1b6\nGPT-4V:\u5f88\u62b1\u6b49\uff0c\u6211\u65e0\u6cd5\u76f4\u63a5\u8ba1\u7b97\u76f8\u5173\u7cfb\u6570r\u548c\u6807\u51c6\u8bef\u5dee\ud835\udc46\ud835\udc46y\u3002\u8fd9\u9700\u8981\u8fdb\u884c\u4e00\u4e9b\u7edf\u8ba1\u8ba1\u7b97\uff0c\u5305\u62ec\u6c42\u548c\u3001\n\u5e73\u65b9\u548c\u3001\u4e58\u79ef\u548c\u7b49\uff0c\u7136\u540e\u4f7f\u7528\u8fd9\u4e9b\u503c\u6765\u8ba1\u7b97\u76f8\u5173\u7cfb\u6570\u548c\u6807\u51c6\u8bef\u5dee\u3002\u5982\u679c\u4f60\u80fd\u63d0\u4f9b\u8fd9\u4e9b\u8ba1\u7b97\u7684\u7ed3\u679c\u6216\u8005\n\u66f4\u591a\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u6211\u53ef\u4ee5\u5e2e\u52a9\u4f60\u7406\u89e3\u8fd9\u4e9b\u6982\u5ff5\u548c\u5b83\u4eec\u662f\u5982\u4f55\u8ba1\u7b97\u7684\u3002\nGround Truth:(B) 0.994343\uff1b8\nError Category: Lack of Knowledge\nError Reason:GPT-4V cannot directly calculate the correlation coefficient \nand standard error  . This requires some statistical calculations, including summation, sum of \nsquares, product sum, and so on.\n<\u56fe\u7247 1>\nFigure B111: A sample error case of Business (subcategory: Marketing).\nBack to List of figures\n130\nPREPRINT\n\u5546\u4e1a: \u8425\u9500\nQuestion: \u67d0\u5bb6\u7528\u7535\u5668\u793e\u4f1a\u8d2d\u4e70\u529b(\u5341\u4e07\u5143)\u4e0e\u8be5\u5e02\u5bb6\u5ead\u4eba\u5747\u8d27\u5e01\u6536\u5165(\u5143)\u7684\u8d44\u6599\u5982\u8868\u6240\u793a<\u56fe\u7247 1>\uff0c\n\u5efa\u7acb\u4e00\u5143\u7ebf\u6027\u56de\u5f52\u65b9\u7a0b\u6a21\u578b\uff0c\u8bd5\u9884\u6d4b\u8be5\u5e021994\u5e74\u7684\u8d2d\u4e70\u529b\u533a\u95f4________\u5143\u3002\uff08\u03b1=0.05 \uff09\nOption: \n(A) 458.36~485.06\n(B) 466.54~485.06\n(C) 458.36~493.19\n(D) 466.54~493.19\nGPT-4V:\u5f88\u62b1\u6b49\uff0c\u6211\u65e0\u6cd5\u76f4\u63a5\u8ba1\u7b97\u6216\u5efa\u7acb\u4e00\u5143\u7ebf\u6027\u56de\u5f52\u65b9\u7a0b\u6a21\u578b\uff0c\u56e0\u4e3a\u8fd9\u9700\u8981\u4e13\u95e8\u7684\u7edf\u8ba1\u8f6f\u4ef6\u6216\u5de5\u5177\n\u6765\u8fdb\u884c\u8ba1\u7b97\u3002\u4e0d\u8fc7\uff0c\u6211\u53ef\u4ee5\u544a\u8bc9\u4f60\u4e00\u822c\u7684\u6b65\u9aa4\uff1a 1. \u9996\u5148\uff0c\u4f60\u9700\u8981\u4f7f\u7528\u5bb6\u5ead\u4eba\u5747\u8d27\u5e01\u6536\u5165\u4f5c\u4e3a\u81ea\u53d8\u91cf\n\uff08X\uff09\uff0c\u5bb6\u7528\u7535\u5668\u793e\u4f1a\u8d2d\u4e70\u529b\u4f5c\u4e3a\u56e0\u53d8\u91cf\uff08Y\uff09\uff0c\u5e76\u6536\u96c6\u8fd9\u4e9b\u6570\u636e\u70b9\u3002 2. \u7136\u540e\uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528\u6700\u5c0f\u4e8c\u4e58\n\u6cd5\u6765\u8ba1\u7b97\u7ebf\u6027\u56de\u5f52\u65b9\u7a0b\u7684\u53c2\u6570\uff0c\u5373\u659c\u7387\uff08b\uff09\u548c\u622a\u8ddd\uff08a\uff09\u3002 3. \u4e00\u65e6\u6709\u4e86\u7ebf\u6027\u56de\u5f52\u65b9\u7a0b\uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528\n\u5b83\u6765\u9884\u6d4b1994\u5e74\u7684\u5bb6\u7528\u7535\u5668\u793e\u4f1a\u8d2d\u4e70\u529b\u3002 4. \u6700\u540e\uff0c\u4f60\u9700\u8981\u8ba1\u7b97\u9884\u6d4b\u503c\u7684\u7f6e\u4fe1\u533a\u95f4\uff0c\u8fd9\u901a\u5e38\u6d89\u53ca\u5230\u67e5\n\u627et\u5206\u5e03\u8868\u548c\u8ba1\u7b97\u6807\u51c6\u8bef\u5dee\u3002 \u7531\u4e8e\u6211\u65e0\u6cd5\u6267\u884c\u8fd9\u4e9b\u8ba1\u7b97\uff0c\u6211\u5efa\u8bae\u4f60\u4f7f\u7528\u7edf\u8ba1\u8f6f\u4ef6\u6216\u6c42\u52a9\u4e8e\u6709\u7edf\u8ba1\u5b66\u80cc\n\u666f\u7684\u4e13\u4e1a\u4eba\u58eb\u6765\u5b8c\u6210\u8fd9\u4e2a\u4efb\u52a1\u3002\u5982\u679c\u4f60\u6709\u8fdb\u4e00\u6b65\u7684\u6570\u636e\u6216\u9700\u8981\u5e2e\u52a9\u7406\u89e3\u8fd9\u4e2a\u8fc7\u7a0b\uff0c\u8bf7\u544a\u8bc9\u6211\u3002\nGround Truth: (B) 466.54~485.06\nError Category: Lack of Knowledge\nError Reason:GPT-4V provides a detailed explanation of the specific steps to establish a simple \nlinear regression model, and there are no errors in the information. However, GPT-4V cannot \ndirectly compute or establish a simple linear regression model as it requires specialized statistical \nsoftware or tools for the calculations.\n<\u56fe\u7247 1>\nFigure B112: A sample error case of Business (subcategory: Marketing).\nBack to List of figures\n131\nPREPRINT\n\u5546\u4e1a: \u8425\u9500\nQuestion: \u4e03\u5947\u516c\u53f8\u5728\u6210\u957f\u8fc7\u7a0b\u4e2d\u62df\u8ffd\u52a0\u7b79\u8d444000\u4e07\u5143,\u73b0\u6709A\u3001B\u3001C\u4e09\u4e2a\u8ffd\u52a0\u7b79\u8d44\u65b9\u6848\u53ef\u4f9b\u9009\u62e9,\u6709\u5173\n\u8d44\u6599\u7ecf\u6574\u7406\u5217\u793a\u5982\u4e0b\u8868<\u56fe\u7247\n1>:\u6d4b\u7b97\u4e09\u4e2a\u516c\u53f8\u7684\u8fb9\u9645\u8d44\u672c\u6210\u672c\u7387\uff0cA\u65b9\u6848_______\uff1bB\u65b9\u6848_________\n\uff1bC\u65b9\u6848_______\u3002\nOption: \n(A) 10.5%\uff1b10.51%\uff1b10.75% \n(B) 10.75%\uff1b10.5%\uff1b10.51% \n(C) 10.5%\uff1b10.75%\uff1b10.51% \n(D) 10.51%\uff1b10.5%\uff1b10.75%\nGPT-4V:\u4e3a\u4e86\u8ba1\u7b97\u8fb9\u9645\u8d44\u672c\u6210\u672c\u7387\uff0c\u6211\u4eec\u9700\u8981\u6839\u636e\u63d0\u4f9b\u7684\u8d44\u6599\u8ba1\u7b97\u6bcf\u4e2a\u65b9\u6848\u7684\u52a0\u6743\u5e73\u5747\u8d44\u672c\u6210\u672c\uff08\nWACC\uff09\u3002\u8fb9\u9645\u8d44\u672c\u6210\u672c\u7387\u662f\u6307\u6700\u540e\u4e00\u7b14\u8d44\u91d1\u7684\u6210\u672c\uff0c\u901a\u5e38\u662f\u6307\u5728\u73b0\u6709\u8d44\u672c\u7ed3\u6784\u4e0b\uff0c\u516c\u53f8\u4e3a\u4e86\u7b79\u96c6\u989d\u5916\n\u8d44\u91d1\u800c\u5fc5\u987b\u652f\u4ed8\u7684\u8d44\u672c\u6210\u672c\u3002\u6839\u636e\u56fe\u7247\u4e2d\u7684\u8868\u683c\uff0c\u6211\u4eec\u53ef\u4ee5\u8ba1\u7b97\u51fa\u6bcf\u4e2a\u65b9\u6848\u7684\u8fb9\u9645\u8d44\u672c\u6210\u672c\u7387\u3002\u8ba1\u7b97\n\u65b9\u6cd5\u662f\u627e\u51fa\u6bcf\u4e2a\u65b9\u6848\u4e2d\u6700\u540e\u4e00\u7b14\u8d44\u91d1\u7684\u6210\u672c\uff0c\u5373\u6700\u9ad8\u7684\u8d44\u672c\u6210\u672c\u767e\u5206\u6bd4\u3002\u65b9\u6848A\u7684\u6700\u540e\u4e00\u7b14\u8d44\u91d1\u6210\u672c\n\u662f14%\uff0c\u65b9\u6848B\u7684\u6700\u540e\u4e00\u7b14\u8d44\u91d1\u6210\u672c\u4e5f\u662f14%\uff0c\u800c\u65b9\u6848C\u7684\u6700\u540e\u4e00\u7b14\u8d44\u91d1\u6210\u672c\u662f14%\u3002 \u56e0\u6b64\uff0c\u6b63\u786e\u7b54\u6848\u662f\n\uff1a (A) 10.5%\uff1b10.51%\uff1b10.75% \u4f46\u662f\uff0c\u6211\u4eec\u6ce8\u610f\u5230\u8fd9\u4e2a\u7b54\u6848\u4e0e\u6211\u4eec\u7684\u8ba1\u7b97\u7ed3\u679c\u4e0d\u7b26\u3002\u6240\u6709\u65b9\u6848\u7684\u6700\n\u540e\u4e00\u7b14\u8d44\u91d1\u6210\u672c\u90fd\u662f14%\uff0c\u6240\u4ee5\u6211\u4eec\u9700\u8981\u91cd\u65b0\u5ba1\u89c6\u9009\u9879\u3002\u7531\u4e8e\u6ca1\u6709\u4e00\u4e2a\u9009\u9879\u4e0e\u6211\u4eec\u7684\u8ba1\u7b97\u7ed3\u679c\u5b8c\u5168\u5339\n\u914d\uff0c\u6211\u4eec\u9700\u8981\u9009\u62e9\u6700\u63a5\u8fd1\u7684\u9009\u9879\u3002 \u6839\u636e\u6211\u4eec\u7684\u8ba1\u7b97\uff0c\u6240\u6709\u65b9\u6848\u7684\u8fb9\u9645\u8d44\u672c\u6210\u672c\u7387\u90fd\u5e94\u8be5\u662f14%\uff0c\u4f46\u662f\n\u8fd9\u4e2a\u9009\u9879\u5e76\u6ca1\u6709\u5728\u63d0\u4f9b\u7684\u9009\u9879\u4e2d\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u65e0\u6cd5\u6839\u636e\u63d0\u4f9b\u7684\u4fe1\u606f\u548c\u9009\u9879\u6765\u786e\u5b9a\u4e00\u4e2a\u660e\u786e\u7684\u7b54\u6848\u3002\u5728\n\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u53ef\u4ee5\u9009\u62e9\u4e00\u4e2a\u5305\u542b14%\u7684\u9009\u9879\uff0c\u5373\u4f7f\u5b83\u4e0d\u5b8c\u5168\u6b63\u786e\u3002\u6700\u63a5\u8fd1\u7684\u9009\u9879\u662f\uff1a(D)\n10.51%\n\uff1b10.5%\uff1b10.75% \u8bf7\u6ce8\u610f\uff0c\u8fd9\u4e2a\u7b54\u6848\u5e76\u4e0d\u51c6\u786e\uff0c\u4f46\u6839\u636e\u6211\u4eec\u7684\u8ba1\u7b97\u548c\u63d0\u4f9b\u7684\u9009\u9879\uff0c\u8fd9\u662f\u6700\u63a5\u8fd1\u7684\u9009\u62e9\n\u3002\nGround Truth: (C) 10.5%\uff1b10.75%\uff1b10.51% \nError Category: Lack of Knowledge\nError Reason:GPT-4V lacks relevant knowledge in calculating the marginal cost of capital and \nmistakenly interprets it as \"finding the cost of the last capital in each scenario, i.e., the highest \npercentage of capital cost.\" The correct approach should involve calculating the proportion of each \nfundraising amount relative to the total fundraising amount and using these proportions as weights \nto compute the marginal cost of capital\n<\u56fe\u7247 1>\n<\u56fe\u7247 1>\nFigure B113: A sample error case of Business (subcategory: Marketing).\nBack to List of figures\n132\nPREPRINT\n\u5546\u4e1a: \u8425\u9500\nQuestion: \u67d0\u884c\u4e1a\u7684\u5e02\u573a\u662f\u5b8c\u5168\u7ade\u4e89\u6027\u7684,\u73b0\u5728\u5e02\u573a\u4e0a\u6709100\u5bb6\u6210\u672c\u76f8\u540c\u7684\u4f01\u4e1a,\u6240\u6709\u4f01\u4e1a\u7684\u56fa\u5b9a\u6210\u672c\u548c\n\u53ef\u53d8\u6210\u672c\u5982\u4e0b,\u6210\u672c\u5355\u4f4d(\u5143):<\u56fe\u7247 1>\u73b0\u5728\u7684\u5e02\u573a\u5747\u8861\u4ef7\u683c\u662f9\u5143\uff0c\u6bcf\u4e2a\u4f01\u4e1a\u751f\u4ea7_____\uff0c\u5e02\u573a\u4f9b\u7ed9\u91cf\n\u662f______\u3002\nOption: \n(A) 3\uff1b300 \n(B) 4\uff1b400 \n(C) 5\uff1b500 \n(D) 6\uff1b600\nGPT-4V:\u5728\u5b8c\u5168\u7ade\u4e89\u5e02\u573a\u4e2d\uff0c\u4f01\u4e1a\u4f1a\u5728\u8fb9\u9645\u6210\u672c\u7b49\u4e8e\u5e02\u573a\u4ef7\u683c\u7684\u60c5\u51b5\u4e0b\u751f\u4ea7\uff0c\u56e0\u4e3a\u8fd9\u6837\u53ef\u4ee5\u6700\u5927\u5316\u5229\n\u6da6\u3002\u6839\u636e\u63d0\u4f9b\u7684\u6210\u672c\u6570\u636e\u8868\uff0c\u6211\u4eec\u9700\u8981\u627e\u5230\u8fb9\u9645\u6210\u672c\uff08MC\uff09\u7b49\u4e8e\u5e02\u573a\u5747\u8861\u4ef7\u683c9\u5143\u7684\u4ea7\u91cf\u6c34\u5e73\u3002 \u4ece\u56fe\n\u8868\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff1a - \u5f53\u4ea7\u91cf\u4e3a1\u65f6\uff0c\u8fb9\u9645\u6210\u672c\u662f5\u5143\u3002 - \u5f53\u4ea7\u91cf\u4e3a2\u65f6\uff0c\u8fb9\u9645\u6210\u672c\u662f7\u5143\u3002 - \u5f53\n\u4ea7\u91cf\u4e3a3\u65f6\uff0c\u8fb9\u9645\u6210\u672c\u662f10\u5143\u3002 - \u5f53\u4ea7\u91cf\u4e3a4\u65f6\uff0c\u8fb9\u9645\u6210\u672c\u662f19\u5143\u3002 \u7531\u4e8e\u5e02\u573a\u5747\u8861\u4ef7\u683c\u662f9\u5143\uff0c\u6211\u4eec\n\u9700\u8981\u627e\u5230\u8fb9\u9645\u6210\u672c\u521a\u597d\u7b49\u4e8e\u6216\u8005\u521a\u597d\u8d85\u8fc79\u5143\u7684\u4ea7\u91cf\u3002\u5728\u8fd9\u4e2a\u60c5\u51b5\u4e0b\uff0c\u8fb9\u9645\u6210\u672c\u5728\u4ea7\u91cf\u4e3a2\u65f6\u662f7\u5143\uff0c\n\u5728\u4ea7\u91cf\u4e3a3\u65f6\u662f10\u5143\u3002\u56e0\u6b64\uff0c\u6bcf\u4e2a\u4f01\u4e1a\u4f1a\u9009\u62e9\u4ea7\u91cf\u4e3a2\u7684\u60c5\u51b5\uff0c\u56e0\u4e3a\u8fd9\u662f\u8fb9\u9645\u6210\u672c\u6700\u63a5\u8fd1\u4f46\u4e0d\u8d85\u8fc7\u5e02\u573a\n\u4ef7\u683c\u7684\u70b9\u3002\n\u65e2\u7136\u6bcf\u4e2a\u4f01\u4e1a\u90fd\u4f1a\u751f\u4ea72\u5355\u4f4d\u7684\u4ea7\u54c1\uff0c\u800c\u5e02\u573a\u4e0a\u6709100\u5bb6\u4f01\u4e1a\uff0c\u5e02\u573a\u4f9b\u7ed9\u91cf\u5c06\u662f\u6bcf\u4e2a\u4f01\u4e1a\n\u7684\u4ea7\u91cf\u4e58\u4ee5\u4f01\u4e1a\u6570\u91cf\uff0c\u53732 * 100 = 200\u3002 \u8fd9\u4e2a\u9009\u62e9\u9898\u4e2d\u6ca1\u6709\u63d0\u4f9b\u201c200\u201d\u7684\u9009\u9879\uff0c\u6240\u4ee5\u53ef\u80fd\u662f\u9898\u76ee\n\u6709\u8bef\u6216\u8005\u9009\u9879\u6709\u8bef\u3002\u6839\u636e\u63d0\u4f9b\u7684\u4fe1\u606f\u548c\u6570\u636e\uff0c\u6700\u5408\u7406\u7684\u7b54\u6848\u662f\u6bcf\u4e2a\u4f01\u4e1a\u751f\u4ea72\u5355\u4f4d\uff0c\u5e02\u573a\u4f9b\u7ed9\u91cf\u662f200\n\u5355\u4f4d\uff0c\u4f46\u8fd9\u4e0d\u7b26\u5408\u4efb\u4f55\u7ed9\u5b9a\u7684\u9009\u9879\u3002\nGround Truth: (B) 4\uff1b400 \nError Category: Lack of Knowledge\nError Reason:GPT-4V lacks relevant knowledge in calculating marginal costs and mistakenly \ninterprets variable costs from the chart as marginal costs. However, the true meaning of marginal \ncost should be the change in total cost for a one-unit change in production. Therefore, when \nproducing 4 units, the marginal cost would be 19-10=9, at which point the marginal cost equals the \nmarket equilibrium price.\n<\u56fe\u7247 1>\nFigure B114: A sample error case of Business (subcategory: Marketing).\nBack to List of figures\n133\nPREPRINT\n\u79d1\u5b66:\u751f\u7269\nQuestion: <\u56fe\u7247 1>\u56fe\u793a\u4e3a\u88ab\u5b50\u690d\u7269\u7684\u751f\u6d3b\u53f2\uff0cA\u5904\u4e3a\uff08\uff09\nOption: \n(A) \u5408\u5b50\n(B) \u914d\u5b50\n(C) \u7cbe\u5b50\n(D) \u5b62\u5b50\u4f53\nGPT-4V: \u8fd9\u5f20\u56fe\u7247\u5c55\u793a\u7684\u662f\u88ab\u5b50\u690d\u7269\u7684\u751f\u6d3b\u53f2\uff0c\u5176\u4e2dA\u5904\u6807\u8bc6\u7684\u5e94\u8be5\u662f\u751f\u6d3b\u53f2\u4e2d\u7684\u4e00\u4e2a\u7279\u5b9a\u9636\u6bb5\u3002\u6839\n\u636e\u88ab\u5b50\u690d\u7269\u7684\u751f\u6d3b\u53f2\uff0cA\u5904\u4f4d\u4e8e\u53d7\u7cbe\u8fc7\u7a0b\u4e4b\u540e\uff0c\u56e0\u6b64A\u5904\u8868\u793a\u7684\u662f\u5408\u5b50\uff08zygote\uff09\uff0c\u8fd9\u662f\u7531\u7cbe\u5b50\u548c\u5375\u5b50\n\u7ed3\u5408\u540e\u5f62\u6210\u7684\u5355\u4e2a\u7ec6\u80de\u3002\u6240\u4ee5\u6b63\u786e\u7b54\u6848\u662f\uff1a(A) \u5408\u5b50\nGround Truth: (A) \u5408\u5b50\n<\u56fe\u7247 1>\nFigure B115: A sample correct case of Science (subcategory: Biology).\nBack to List of figures\n134\nPREPRINT\n\u79d1\u5b66:\u751f\u7269\nQuestion: \u5982\u56fe<\u56fe\u7247 1>\u53ef\u770b\u51fa\u76d0\u80c1\u8feb\u5bf9\u6839\u4f38\u957f\u7684\u5f71\u54cd\u3002\u76d0\u80c1\u8feb\u901a\u8fc7\u5f71\u54cd\u7ec6\u80de\u5468\u671f\u51cf\u5c11\u6839\u5206\u751f\u7ec4\u7ec7\u7684\n\u7ec6\u80de\u6570\u91cf\u6765\u6291\u5236\u6839\u7684\u4f38\u957f\u3002\u65e0\u76d0\u73af\u5883\u4e0b\u7684\u5e7c\u82d7\u8f6c\u79fb\u5230\u4e24\u79cd\u4ecb\u8d28\u4e2d,\u4e00\u79cd\u542b\u76d0,\u53e6\u4e00\u79cd\u4e0d\u542b\u76d0\u3002\u5728\u65e0\u76d0\u73af\n\u5883\u4e0b,\u6839\u5206\u751f\u7ec4\u7ec7\u7684\u7ec6\u80de\u6570\u91cf\u4e0d\u53d8,\u6839\u4f38\u957f\u7a0b\u5ea6\u7684\u8d8b\u52bf\u4e0d\u53d8\u3002\u800c\u5728\u542b\u76d0\u7684\u73af\u5883\u4e2d__\nOption: \n(A) \u5e7c\u82d7\u6839\u5206\u751f\u7ec4\u7ec7\u7684\u7ec6\u80de\u6570\u91cf\u589e\u52a0\uff0c\u6839\u4f38\u957f\u53d7\u5230\u4fc3\u8fdb\u3002\n(B) \u5e7c\u82d7\u6839\u5206\u751f\u7ec4\u7ec7\u7684\u7ec6\u80de\u6570\u91cf\u51cf\u5c11\uff0c\u6839\u4f38\u957f\u53d7\u5230\u4fc3\u8fdb\u3002\n(C) \u5e7c\u82d7\u6839\u5206\u751f\u7ec4\u7ec7\u7684\u7ec6\u80de\u6570\u91cf\u589e\u52a0\uff0c\u6839\u4f38\u957f\u53d7\u5230\u6291\u5236\u3002\n(D) \u5e7c\u82d7\u6839\u5206\u751f\u7ec4\u7ec7\u7684\u7ec6\u80de\u6570\u91cf\u51cf\u5c11\uff0c\u6839\u4f38\u957f\u53d7\u5230\u6291\u5236\u3002\nGPT-4V: \u6839\u636e\u9898\u76ee\u63cf\u8ff0\u548c\u6240\u63d0\u4f9b\u7684\u56fe\u7247\u4fe1\u606f\uff0c\u6211\u4eec\u53ef\u4ee5\u5f97\u51fa\u4ee5\u4e0b\u7ed3\u8bba\uff1a\u9898\u76ee\u63cf\u8ff0\u4e86\u76d0\u80c1\u8feb\u5bf9\u6839\u4f38\u957f\u7684\n\u5f71\u54cd\uff0c\u6307\u51fa\u76d0\u80c1\u8feb\u901a\u8fc7\u5f71\u54cd\u7ec6\u80de\u5468\u671f\u51cf\u5c11\u6839\u5206\u751f\u7ec4\u7ec7\u7684\u7ec6\u80de\u6570\u91cf\u6765\u6291\u5236\u6839\u7684\u4f38\u957f\u3002\u5728\u65e0\u76d0\u73af\u5883\u4e0b\uff0c\u6839\n\u5206\u751f\u7ec4\u7ec7\u7684\u7ec6\u80de\u6570\u91cf\u4e0d\u53d8\uff0c\u6839\u4f38\u957f\u7a0b\u5ea6\u7684\u8d8b\u52bf\u4e0d\u53d8\u3002\u800c\u5728\u542b\u76d0\u7684\u73af\u5883\u4e2d\uff0c\u53ef\u4ee5\u63a8\u65ad\u6839\u5206\u751f\u7ec4\u7ec7\u7684\u7ec6\u80de\n\u6570\u91cf\u4f1a\u51cf\u5c11\uff0c\u6839\u4f38\u957f\u53d7\u5230\u6291\u5236\u3002\u56e0\u6b64\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a(D) \u5e7c\u82d7\u6839\u5206\u751f\u7ec4\u7ec7\u7684\u7ec6\u80de\u6570\u91cf\u51cf\u5c11\uff0c\u6839\u4f38\u957f\u53d7\n\u5230\u6291\u5236\u3002\nGround Truth: (D) \u5e7c\u82d7\u6839\u5206\u751f\u7ec4\u7ec7\u7684\u7ec6\u80de\u6570\u91cf\u51cf\u5c11\uff0c\u6839\u4f38\u957f\u53d7\u5230\u6291\u5236\u3002\n<\u56fe\u7247 1>\nFigure B116: A sample correct case of Science (subcategory: Biology).\nBack to List of figures\n135\nPREPRINT\n\u79d1\u5b66:\u751f\u7269\nQuestion: <\u56fe\u7247 1>\u662f\u6839\u7684\u53d1\u80b2\u6a21\u5f0f\u4e2d\u7684\u54ea\u4e00\u9636\u6bb5\uff1f\uff08\uff09\nOption: \n(A) \u521d\u751f\u5206\u751f\u7ec4\u7ec7\n(B) \u521d\u751f\u7ed3\u6784\n(C) \u5f62\u6210\u5c42\u7684\u53d1\u751f\n(D) \u5f62\u6210\u5c42\u73af\u7684\u5f62\u6210\nGPT-4V: \u8fd9\u5f20\u56fe\u7247\u663e\u793a\u7684\u662f\u690d\u7269\u6839\u7684\u6a2a\u622a\u9762\u56fe\u3002\u6839\u636e\u56fe\u4e2d\u663e\u793a\u7684\u7ed3\u6784\uff0c\u53ef\u4ee5\u770b\u5230\u4e2d\u5fc3\u67f1\u3001\u521d\u751f\u6728\u8d28\u90e8\u3001\n\u521d\u751f\u97e7\u76ae\u90e8\u3001\u76ae\u5c42\u548c\u8868\u76ae\u7b49\u90e8\u5206\u3002\u8fd9\u4e9b\u90fd\u662f\u6839\u7684\u521d\u751f\u7ed3\u6784\u7684\u7ec4\u6210\u90e8\u5206\u3002\u56e0\u6b64\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a(B) \u521d\u751f\n\u7ed3\u6784\nGround Truth: (B) \u521d\u751f\u7ed3\u6784\n<\u56fe\u7247 1>\nFigure B117: A sample correct case of Science (subcategory: Biology).\nBack to List of figures\n136\nPREPRINT\n\u79d1\u5b66:\u751f\u7269\nQuestion: <\u56fe\u7247 1>\u662f\u566c\u83cc\u4f53\u4fb5\u67d3\u7ec6\u83cc\u7684\u793a\u610f\u56fe,\u4fb5\u67d3\u7ec6\u83cc\u7684\u6b63\u786e\u987a\u5e8f\u5e94\u662f\uff08\uff09\nOption:\n(A) BDAEC\n(B) ABCDE\n(C) BDEAC\n(D) DBAEC\nGPT-4V: \u8fd9\u5f20\u56fe\u7247\u5c55\u793a\u4e86\u566c\u83cc\u4f53\u4fb5\u67d3\u7ec6\u83cc\u7684\u4e0d\u540c\u9636\u6bb5\u3002\u6839\u636e\u566c\u83cc\u4f53\u7684\u751f\u547d\u5468\u671f\uff0c\u6b63\u786e\u7684\u987a\u5e8f\u5e94\u8be5\u662f\uff1a\n- \u566c\u83cc\u4f53\u9644\u7740\u5230\u7ec6\u83cc\u7ec6\u80de\u58c1\uff08A\uff09- \u6ce8\u5165\u9057\u4f20\u7269\u8d28\uff08B\uff09- \u7ec6\u80de\u5185\u5408\u6210\u566c\u83cc\u4f53\u7ec4\u5206\uff08C\uff09- \u7ec4\u88c5\u6210\u719f\u566c\n\u83cc\u4f53\uff08D\uff09- \u7ec6\u80de\u88c2\u89e3\u91ca\u653e\u566c\u83cc\u4f53\uff08E\uff09\u56e0\u6b64\uff0c\u6b63\u786e\u7684\u987a\u5e8f\u662f ABCDE\u3002\u6b63\u786e\u7b54\u6848\u662f\uff1a(B) ABCDE\nGround Truth: (A) BDAEC\nError Category: Perceptual Error \nError Reason: GPT-4V has knowledge about the process of bacteriophage infection in bacteria, \nunderstanding that the sequence of infection comprises the bacteriophage attaching itself to the \nbacterial cell wall, injecting genetic material, synthesizing bacteriophage components within the cell, \nassembling mature bacteriophages, and cell lysis to release the phages. However, it fails to \naccurately identify images depicting each stage, leading to incorrect matches between the images \nand their corresponding names.\n<\u56fe\u7247 1>\nFigure B118: A sample error case of Science (subcategory: Biology).\nBack to List of figures\n137\nPREPRINT\n\u79d1\u5b66:\u751f\u7269\nQuestion: \u5982\u56fe<\u56fe\u7247 1>\u4e3aCDK-cyclin\u590d\u5408\u4f53\u5728\u7ec6\u80de\u5468\u671f\u4e2d\u7684\u529f\u80fd\u3002__\u548c__\u90fd\u662f\u88ab\u7279\u5b9a\u7684CDK-cyclin\n\u590d\u5408\u4f53\u8c03\u63a7\u7684\u3002\u8fd9\u4e9b\u590d\u5408\u4f53\u5728\u7ec6\u80de\u5468\u540c\u671f\u7684\u9002\u5f53\u65f6\u95f4\u70b9\u5408\u6210\u3002\nOption:\n(A) G1\u671f\u7ec6\u80de\u751f\u957f\uff1bS\u671fDNA\u590d\u5236\n(B) M\u671f\u7eba\u9524\u4f53\u5f62\u6210\uff1bG1\u671f\u7ec6\u80de\u751f\u957f\n(C) S\u671fDNA\u590d\u5236\uff1bM\u671f\u7eba\u9524\u4f53\u5f62\u6210\n(D) G2\u671f\u51c6\u5907\u5206\u88c2\uff1bG1\u671f\u7ec6\u80de\u751f\u957f\nGPT-4V:\u6839\u636e\u63d0\u4f9b\u7684\u56fe\u7247\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230CDK-cyclin\u590d\u5408\u4f53\u5728\u7ec6\u80de\u5468\u671f\u7684\u4e0d\u540c\u9636\u6bb5\u8d77\u4f5c\u7528\u3002\u56fe\u7247\u4e2d\u6807\n\u6ce8\u4e86G1\u671f\u3001S\u671f\u3001G2\u671f\u548cM\u671f\uff0c\u5e76\u4e14\u5728G1\u671f\u548cS\u671f\u65c1\u8fb9\u6709CDK-cyclin\u590d\u5408\u4f53\u7684\u6807\u8bb0\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u53ef\u4ee5\u63a8\n\u65adCDK-cyclin\u590d\u5408\u4f53\u5728G1\u671f\u548cS\u671f\u6709\u7279\u5b9a\u7684\u529f\u80fd\u3002G1\u671f\u901a\u5e38\u4e0e\u7ec6\u80de\u751f\u957f\u6709\u5173\uff0c\u800cS\u671f\u4e0eDNA\u590d\u5236\u6709\u5173\u3002\n\u56e0\u6b64\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a(A) G1\u671f\u7ec6\u80de\u751f\u957f\uff1bS\u671fDNA\u590d\u5236\nGround Truth: (C) S\u671fDNA\u590d\u5236\uff1bM\u671f\u7eba\u9524\u4f53\u5f62\u6210\nError Category: Reasoning Error \nError Reason: GPT-4V accurately identified the stages in the cell cycle depicted in the image as G1, \nS, G2, and M phases. It also detected the CDK-cyclin marker adjacent to the S phase. It correctly \ninferred that CDK-cyclin was involved in DNA replication during the S phase, yet incorrectly \npresumed that CDK-cyclin also plays a role in cell growth during the G1 phase. Moreover, it \noverlooked the CDK-cyclin adjacent to the M phase. These misjudgments led to the incorrect \nanswer.\n<image 1>\n<\u56fe\u7247 1>\nFigure B119: A sample error case of Science (subcategory: Biology).\nBack to List of figures\n138\nPREPRINT\n\u79d1\u5b66:\u751f\u7269\nQuestion: <\u56fe\u7247 1>\u56fe\u4e3a\u8fdb\u884c\u6027\u808c\u8425\u517b\u4e0d\u826f\u9057\u4f20\u75c5\u7684\u7cfb\u8c31\u56fe\uff0c\u8be5\u75c5\u4e3a\u9690\u6027\u4f34\u6027\u9057\u4f20\u75c5\uff0c7\u53f7\u7684\u81f4\u75c5\u57fa\n\u56e0\u662f\u7531\uff08\uff09\u53f7\u4f20\u4e0b\u6765\u7684\nOption:\n(A) 1\n(B) 2\n(C) 3\n(D) 4\nGPT-4V: \u8fd9\u662f\u4e00\u4e2a\u5173\u4e8e\u9690\u6027\u4f34\u6027\u9057\u4f20\u75c5\u7684\u7cfb\u8c31\u56fe\u3002\u5728\u9690\u6027\u4f34\u6027\u9057\u4f20\u4e2d\uff0c\u5973\u6027\u9700\u8981\u4e24\u4e2a\u9690\u6027\u57fa\u56e0\u624d\u4f1a\u8868\n\u73b0\u51fa\u75be\u75c5\uff0c\u800c\u7537\u6027\u53ea\u9700\u8981\u4e00\u4e2a\u9690\u6027\u57fa\u56e0\u5c31\u4f1a\u8868\u73b0\u51fa\u75be\u75c5\uff0c\u56e0\u4e3a\u7537\u6027\u662fXY\u6027\u67d3\u8272\u4f53\uff0c\u5973\u6027\u662fXX\u6027\u67d3\u8272\u4f53\u3002\n\u5728\u8fd9\u4e2a\u7cfb\u8c31\u56fe\u4e2d\uff0c7\u53f7\u662f\u4e00\u4e2a\u8868\u73b0\u75be\u75c5\u7684\u7537\u6027\uff0c\u6240\u4ee5\u4ed6\u7684X\u67d3\u8272\u4f53\u4e0a\u5e26\u6709\u81f4\u75c5\u57fa\u56e0\u3002\u4ed6\u53ea\u80fd\u4ece\u4ed6\u7684\u6bcd\u4eb2\n\u90a3\u91cc\u7ee7\u627fX\u67d3\u8272\u4f53\uff0c\u56e0\u4e3a\u4ed6\u4ece\u7236\u4eb2\u90a3\u91cc\u7ee7\u627f\u7684\u662fY\u67d3\u8272\u4f53\u3002\u56e0\u6b64\uff0c7\u53f7\u7684\u81f4\u75c5\u57fa\u56e0\u662f\u7531\u4ed6\u7684\u6bcd\u4eb2\uff083\u53f7\uff09\n\u4f20\u4e0b\u6765\u7684\u3002 \u6b63\u786e\u7b54\u6848\u662f\uff1a (C) 3\nGround Truth: (B) 2\nError Category: Reasoning Error \nError Reason: GPT-4V accurately identified Individual 7 as a diseased male and correctly deduced \nthat his pathogenic gene was inherited from his mother. However, it misplaced the identification \nnumber of his mother. She should be labeled as Individual 5, not Individual 3.\n<\u56fe\u7247 1>\nFigure B120: A sample error case of Science (subcategory: Biology).\nBack to List of figures\n139\nPREPRINT\n\u79d1\u5b66:\u751f\u7269\nGround Truth: \u805a\u7c7b\u5206\u6790\nError Category: Answer Extraction Error\nError Reason: GPT-4V correctly answer the question, but the function to extract the answer has the\nlimitation, which led to the wrong answer being extracted.\nQuestion: <\u56fe\u7247 1>\u6240\u793a\u7684\u662f\u751f\u7269\u4fe1\u606f\u5b66\u4e2d\u5e38\u4f7f\u7528\u7684__\nGPT-4V: <\u56fe\u7247 1>\u6240\u793a\u7684\u662f\u751f\u7269\u4fe1\u606f\u5b66\u4e2d\u5e38\u4f7f\u7528\u7684\u805a\u7c7b\u5206\u6790\u7ed3\u679c\u56fe\u3002\n<\u56fe\u7247 1>\nFigure B121: A sample error case of Science (subcategory: Biology).\nBack to List of figures\n140\nPREPRINT\n\u79d1\u5b66:\u751f\u7269\nGround Truth: \u76f4\u94fe\u6dc0\u7c89\nError Category: Answer Extraction Error\nError Reason: GPT-4V correctly answer the question, but the function to extract the answer has the\nlimitation, which led to the wrong answer being extracted.\nQuestion: \u6dc0\u7c89\u662f\u690d\u7269\u7ec6\u80de\u4e2d\u4ee5\u8d2e\u85cf\u72b6\u6001\u5b58\u5728\u7684\u7cd6,\u662f\u8d2e\u5b58\u80fd\u91cf\u7684\u591a\u7cd6,<\u56fe\u7247 1>\u94fe\u4e0d\u5206\u652f\u7684\u6dc0\u7c89\u4e3a__\nOption: \n(A) \u521d\u751f\u5206\u751f\u7ec4\u7ec7\n(B) \u521d\u751f\u7ed3\u6784\n(C) \u5f62\u6210\u5c42\u7684\u53d1\u751f\n(D) \u5f62\u6210\u5c42\u73af\u7684\u5f62\u6210\nGPT-4V: \u94fe\u4e0d\u5206\u652f\u7684\u6dc0\u7c89\u4e3a\u76f4\u94fe\u6dc0\u7c89\uff0c\u4e5f\u79f0\u4e3a\u6dc0\u7c89\u76f4\u94fe\u90e8\u5206\u6216\u6dc0\u7c89\u7684\u76f4\u94fe\u5206\u5b50\uff0c\u5b66\u672f\u4e0a\u79f0\u4e3a\u6dc0\u7c89\u7d20\n\uff08Amylose\uff09\u3002\n<\u56fe\u7247 1>\nFigure B122: A sample error case of Science (subcategory: Biology).\nBack to List of figures\n141\nPREPRINT\n\u79d1\u5b66:\u751f\u7269\nQuestion:\u4e0b\u56fe<\u56fe\u7247 1>\u662f\u4e00\u53f0\u79bb\u5fc3\u8403\u53d6\u673a\uff0c\u6b63\u786e\u7684\u53d9\u8ff0\u662f\nOption:\n(A) A\u662f\u91cd\u6db2\u51fa\u53e3\uff0cB\u662f\u91cd\u6db2\u5165\u53e3\uff0cC\u662f\u8f7b\u6db2\u5165\u53e3\uff0cD\u662f\u8f7b\u6db2\u51fa\u53e3\n(B) A\u662f\u8f7b\u6db2\u51fa\u53e3\uff0cB\u662f\u8f7b\u6db2\u5165\u53e3\uff0cC\u662f\u91cd\u6db2\u5165\u53e3\uff0cD\u662f\u91cd\u6db2\u51fa\u53e3\n(C) A\u662f\u91cd\u6db2\u51fa\u53e3\uff0cB\u662f\u8f7b\u6db2\u5165\u53e3\uff0cC\u662f\u91cd\u6db2\u5165\u53e3\uff0cD\u662f\u8f7b\u6db2\u51fa\u53e3\n(D) A\u662f\u91cd\u6db2\u5165\u53e3\uff0cB\u662f\u8f7b\u6db2\u51fa\u53e3\uff0cC\u662f\u91cd\u6db2\u51fa\u53e3\uff0cD\u662f\u8f7b\u6db2\u5165\u53e3\nGPT-4V: \u5f88\u62b1\u6b49\uff0c\u6211\u65e0\u6cd5\u76f4\u63a5\u8bc6\u522b\u548c\u9a8c\u8bc1\u56fe\u7eb8\u4e0a\u7684\u6807\u8bb0\u4e0e\u5b9e\u9645\u79bb\u5fc3\u8403\u53d6\u673a\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u56e0\u4e3a\u8fd9\u9700\u8981\u5177\n\u4f53\u7684\u8bbe\u5907\u8bbe\u8ba1\u77e5\u8bc6\u548c\u8bf4\u660e\u3002\u901a\u5e38\uff0c\u79bb\u5fc3\u8403\u53d6\u673a\u7684\u5de5\u4f5c\u539f\u7406\u662f\u57fa\u4e8e\u5bc6\u5ea6\u5dee\u5f02\uff0c\u8f7b\u6db2\u548c\u91cd\u6db2\u4f1a\u5728\u79bb\u5fc3\u529b\u7684\n\u4f5c\u7528\u4e0b\u5206\u79bb\u3002\u8f7b\u6db2\u548c\u91cd\u6db2\u7684\u5165\u53e3\u548c\u51fa\u53e3\u4f4d\u7f6e\u53d6\u51b3\u4e8e\u5177\u4f53\u7684\u673a\u578b\u548c\u8bbe\u8ba1\u3002\u5982\u679c\u6ca1\u6709\u5177\u4f53\u7684\u8bbe\u5907\u624b\u518c\u6216\u8005\n\u8bbe\u8ba1\u8bf4\u660e\uff0c\u6211\u65e0\u6cd5\u786e\u5b9a\u54ea\u4e2a\u9009\u9879\u662f\u6b63\u786e\u7684\u3002\u5728\u5b9e\u9645\u60c5\u51b5\u4e2d\uff0c\u60a8\u5e94\u8be5\u53c2\u8003\u8bbe\u5907\u7684\u7528\u6237\u624b\u518c\u6216\u8005\u8054\u7cfb\u5236\u9020\n\u5546\u6765\u83b7\u53d6\u51c6\u786e\u7684\u4fe1\u606f\u3002\u5982\u679c\u8fd9\u662f\u4e00\u4e2a\u7406\u8bba\u6d4b\u8bd5\u9898\u76ee\uff0c\u901a\u5e38\u4f1a\u63d0\u4f9b\u8db3\u591f\u7684\u4fe1\u606f\u6765\u786e\u5b9a\u6b63\u786e\u7b54\u6848\uff0c\u6216\u8005\u5047\n\u8bbe\u4f60\u5df2\u7ecf\u6709\u4e86\u76f8\u5173\u80cc\u666f\u77e5\u8bc6\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5982\u679c\u6ca1\u6709\u989d\u5916\u7684\u4fe1\u606f\uff0c\u6211\u65e0\u6cd5\u63d0\u4f9b\u4e00\u4e2a\u786e\u5b9a\u7684\u7b54\u6848\u3002\nGround Truth: (C) A\u662f\u91cd\u6db2\u51fa\u53e3\uff0cB\u662f\u8f7b\u6db2\u5165\u53e3\uff0cC\u662f\u91cd\u6db2\u5165\u53e3\uff0cD\u662f\u8f7b\u6db2\u51fa\u53e3\nError Category: Lack of Knowledge \nError Reason: GPT-4V cannot determine the answer due to a lack of knowledge about the \ncentrifugal extractor.\n<\u56fe\u7247 1>\nFigure B123: A sample error case of Science (subcategory: Biology).\nBack to List of figures\n142\nPREPRINT\n\u79d1\u5b66:\u751f\u7269\nQuestion: \u5982\u56fe<\u56fe\u7247 1>\u6700\u65e9\u7684\u80fd\u5f52\u4eba\u73b0\u5b58\u751f\u7269\u5206\u7c7b\u5355\u4f4d\u7684\u771f\u6838\u751f\u7269\u662f\u4e00\u4e2a\u4fdd\u5b58\u5728\u52a0\u62ff\u5927\u5317\u90e812\u4ebf\u5e74\n\u524d\u7684\u5ca9\u77f3\u4e2d\u7684__,\u8be5\u5316\u77f3\u6e90\u81ea\u6f6e\u95f4\u6c89\u79ef\u7269\u3002\u8fd9\u4e2a\u79cd\u7c7b\u975e\u5e38\u50cf\u6697\u7d2b\u7ea2\u6bdb\u83dc,\u4e00\u7c7b\u73b0\u5b58\u7684\u5728\u6de1\u6c34\u548c\u6d77\u6c34\u4ea4\u754c\n\u5904\u751f\u957f\u7684\u85fb\u7c7b\nGPT-4V: \u5bf9\u4e0d\u8d77\uff0c\u6211\u65e0\u6cd5\u63d0\u4f9b\u5177\u4f53\u7684\u5316\u77f3\u540d\u79f0\uff0c\u56e0\u4e3a\u8fd9\u9700\u8981\u4e13\u4e1a\u7684\u53e4\u751f\u7269\u5b66\u77e5\u8bc6\u548c\u5bf9\u5177\u4f53\u5316\u77f3\u7814\u7a76\u7684\n\u4e86\u89e3\u3002\u4e0d\u8fc7\uff0c\u6839\u636e\u60a8\u63d0\u4f9b\u7684\u4fe1\u606f\uff0c\u8fd9\u79cd\u5316\u77f3\u975e\u5e38\u50cf\u73b0\u5b58\u7684\u6697\u7d2b\u7ea2\u6bdb\u83dc\uff0c\u8fd9\u662f\u4e00\u79cd\u85fb\u7c7b\u3002\u56e0\u6b64\uff0c\u6700\u5408\u7406\n\u7684\u7b54\u6848\u53ef\u80fd\u662f\u8fd9\u4e2a\u5316\u77f3\u5c5e\u4e8e\u67d0\u79cd\u53e4\u8001\u7684\u85fb\u7c7b\uff0c\u4f46\u662f\u6ca1\u6709\u66f4\u591a\u7684\u4fe1\u606f\uff0c\u6211\u65e0\u6cd5\u63d0\u4f9b\u4e00\u4e2a\u786e\u5207\u7684\u540d\u79f0\u3002\nGround Truth: \u6d77\u751f\u7ea2\u85fb\u5316\u77f3\n<\u56fe\u7247 1>\nError Category: Lack of Knowledge, Reject to Answer \nError Reason: GPT-4V refuses to answer the question due to a lack of professional paleontological \nknowledge and understanding of specific fossil studies.\nFigure B124: A sample error case of Science (subcategory: Biology).\nBack to List of figures\n143\nPREPRINT\n\u79d1\u5b66:\u5316\u5b66\nQuestion: \u5982\u56fe<\u56fe\u7247 1>\uff0c\u6bd4\u8f83\u4e0b\u5217\u5316\u5408\u7269\u9178\u6027\u5f3a\u5f31_\u3002\nOption: \n(A) B>A>C\n(B) C>B>A\n(C) A>C>B\n(D) A>B>C\nGPT-4V: \u6839\u636e\u6240\u63d0\u4f9b\u7684\u56fe\u7247\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u4e09\u4e2a\u5316\u5408\u7269\uff1aA: \u542b\u6709\u4e00\u4e2a\u6c28\u57fa\uff08-NH2\uff09\u7684\u5316\u5408\u7269\u3002B: \u542b\n\u6709\u4e00\u4e2a\u7f9f\u57fa\uff08-OH\uff09\u7684\u5316\u5408\u7269\u3002C: \u542b\u6709\u4e00\u4e2a\u7fa7\u57fa\uff08-COOH\uff09\u7684\u5316\u5408\u7269\u3002\u5728\u8fd9\u4e09\u4e2a\u5b98\u80fd\u56e2\u4e2d\uff0c\u7fa7\u57fa\u662f\u6700\n\u9178\u6027\u7684\uff0c\u56e0\u4e3a\u5b83\u53ef\u4ee5\u91ca\u653e\u8d28\u5b50\uff08H+\uff09\uff0c\u5f62\u6210\u7a33\u5b9a\u7684\u7fa7\u9178\u6839\u79bb\u5b50\u3002\u7f9f\u57fa\u6b21\u4e4b\uff0c\u800c\u6c28\u57fa\u901a\u5e38\u662f\u78b1\u6027\u7684\uff0c\u4e0d\n\u5bb9\u6613\u91ca\u653e\u8d28\u5b50\u3002\u56e0\u6b64\uff0c\u9178\u6027\u5f3a\u5f31\u987a\u5e8f\u5e94\u8be5\u662f C\uff08\u7fa7\u57fa\uff09> B\uff08\u7f9f\u57fa\uff09> A\uff08\u6c28\u57fa\uff09\u3002\u6b63\u786e\u7b54\u6848\u662f (D) \nA>B>C\u3002\nGround Truth: (D) A>B>C\n<\u56fe\u7247 1>\nFigure B125: A sample correct case of Science (subcategory: Chemistry).\nBack to List of figures\n144\nPREPRINT\n\u79d1\u5b66:\u5316\u5b66\nQuestion: <\u56fe\u7247 1>\u4e2d,\u54ea\u4e00\u70b9\u4ee3\u8868\u53cc\u94feDNA\u7684Tm\u503c?\uff08\uff09\nOption: \n(A) A\n(B) B\n(C) C\n(D) D\nGPT-4V: \u5728\u8fd9\u5f20\u56fe\u7247\u4e2d\uff0c\u6211\u4eec\u770b\u5230\u7684\u662f\u4e00\u4e2aDNA\u7194\u89e3\u66f2\u7ebf\uff0c\u5b83\u663e\u793a\u4e86\u968f\u7740\u6e29\u5ea6\uff08T\uff09\u7684\u5347\u9ad8\uff0cDNA\u5438\u6536\n\u5ea6\uff08OD260\uff09\u7684\u53d8\u5316\u3002\u53cc\u94feDNA\u7684\u7194\u89e3\u6e29\u5ea6\uff08Tm\uff09\u662f\u6307DNA\u4ece\u53cc\u94fe\u72b6\u6001\u8f6c\u53d8\u4e3a\u5355\u94fe\u72b6\u6001\u65f6\u7684\u6e29\u5ea6\uff0c\u8fd9\u901a\n\u5e38\u5bf9\u5e94\u4e8e\u5438\u6536\u5ea6\u66f2\u7ebf\u4e0a\u6700\u9661\u5ced\u7684\u90e8\u5206\uff0c\u5373\u66f2\u7ebf\u7684\u4e2d\u70b9\u3002\u5728\u8fd9\u4e2a\u56fe\u4e2d\uff0c\u70b9C\u4f4d\u4e8e\u66f2\u7ebf\u7684\u4e2d\u95f4\u90e8\u5206\uff0c\u8fd9\u4e2a\n\u4f4d\u7f6e\u4ee3\u8868\u4e86\u5438\u6536\u5ea6\u6025\u5267\u4e0a\u5347\u7684\u5730\u65b9\uff0c\u8fd9\u662f\u56e0\u4e3a\u53cc\u94feDNA\u89e3\u94fe\u6210\u5355\u94fe\uff0c\u5bfc\u81f4\u5438\u6536\u5ea6\u589e\u52a0\u3002\u56e0\u6b64\uff0c\u70b9C\u6700\u6709\n\u53ef\u80fd\u4ee3\u8868\u53cc\u94feDNA\u7684Tm\u503c\u3002\u6b63\u786e\u7b54\u6848\u662f\uff1a(C) C\nGround Truth: (C) C\n<\u56fe\u7247 1>\nFigure B126: A sample correct case of Science (subcategory: Chemistry).\nBack to List of figures\n145\nPREPRINT\n\u79d1\u5b66:\u5316\u5b66\nQuestion: \u7528y=ax+b\u7684\u5f62\u5f0f\u6765\u8868\u8fbe\u7c73\u6c0f\u65b9\u7a0b\u65f6,\u5176\u8868\u8fbe\u5f0f\u5982<\u56fe\u7247 1>\u6240\u793a\uff0c\u5f53\u5728\u53cd\u5e94\u4f53\u7cfb\u4e2d\u52a0\u5165\u975e\u7adf\u4e89\n\u6027\u6291\u5236\u5242\u65f6, a\u503c\u5c06_\nOption:\n(A) \u589e\u5927\n(B) \u51cf\u5c0f\n(C) \u4fdd\u6301\u4e0d\u53d8\n(D) \u4e0d\u786e\u5b9a\nGPT-4V: \u8fd9 \u5f20 \u56fe \u7247 \u663e \u793a \u7684 \u662f \u7c73 \u6c0f \u65b9 \u7a0b \uff08 Michaelis-Menten equation \uff09 \u7684 \u7ebf \u6027 \u5f62 \u5f0f \uff0c \u4e5f \u5c31 \u662f\nLineweaver-Burk plot\u7684\u65b9\u7a0b\u5f0f\u3002\u5728\u8fd9\u4e2a\u65b9\u7a0b\u4e2d\uff0c\n1\nV\u662fy\u8f74\u4e0a\u7684\u503c\uff0c\n1\n[S]\u662fx\u8f74\u4e0a\u7684\u503c\uff0c\u659c\u7387a\u7b49\u4e8e\nKm\nVmax \uff0c\n\u622a\u8dddb\u7b49\u4e8e\n1\nVmax \u3002\u5f53\u52a0\u5165\u975e\u7ade\u4e89\u6027\u6291\u5236\u5242\u65f6\uff0cVmax\u4f1a\u51cf\u5c0f\uff0c\u56e0\u4e3a\u6291\u5236\u5242\u4f1a\u964d\u4f4e\u9176\u7684\u6700\u5927\u53cd\u5e94\u901f\u7387\u3002\u7136\n\u800c\uff0cKm\uff08\u7c73\u6c0f\u5e38\u6570\uff09\u4fdd\u6301\u4e0d\u53d8\uff0c\u56e0\u4e3a\u975e\u7ade\u4e89\u6027\u6291\u5236\u5242\u4e0d\u5f71\u54cd\u9176\u4e0e\u5e95\u7269\u7ed3\u5408\u7684\u4eb2\u548c\u529b\u3002\u7531\u4e8ea\u503c\u7b49\u4e8e\nKm\nVmax \uff0cVmax\u7684\u51cf\u5c0f\u4f1a\u5bfc\u81f4a\u503c\u589e\u5927\u3002\u56e0\u6b64\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a (A) \u589e\u5927\nGround Truth: (A) \u589e\u5927\n<\u56fe\u7247 1>\nFigure B127: A sample correct case of Science (subcategory: Chemistry).\nBack to List of figures\n146\nPREPRINT\n\u79d1\u5b66:\u5316\u5b66\nQuestion: \u5982\u56fe<\u56fe\u7247 1>\uff0c\u5c06\u4e0b\u5217\u5316\u5408\u7269\u6309\u8131\u6c34\u53cd\u5e94\u901f\u7387\u7531\u5927\u5230\u5c0f\u6392\u5217_\u3002\nOptions:\n(A) A>B>C\n(B) B>C>A\n(C) C>A>B\n(D) A>C>B\nGPT-4V: \u62b1\u6b49\uff0c\u6211\u65e0\u6cd5\u76f4\u63a5\u63d0\u4f9b\u5316\u5b66\u95ee\u9898\u7684\u7b54\u6848\u3002\u4f46\u662f\uff0c\u6211\u53ef\u4ee5\u544a\u8bc9\u4f60\u5982\u4f55\u5206\u6790\u8fd9\u4e2a\u95ee\u9898\u3002\u5728\u6bd4\u8f83\u8131\n\u6c34\u53cd\u5e94\u901f\u7387\u65f6\uff0c\u6211\u4eec\u901a\u5e38\u8003\u8651\u4ee5\u4e0b\u56e0\u7d20\uff1a1. \u5206\u5b50\u4e2d\u7f9f\u57fa\uff08-OH\uff09\u7684\u4f4d\u7f6e\uff1a\u7f9f\u57fa\u6240\u5728\u7684\u78b3\u539f\u5b50\u7684\u6742\u5316\u7c7b\n\u578b\uff08sp3, sp2, sp\uff09\u4f1a\u5f71\u54cd\u8131\u6c34\u901f\u7387\u30022. \u7f9f\u57fa\u9644\u8fd1\u7684\u7a33\u5b9a\u5316\u56e0\u7d20\uff1a\u4f8b\u5982\uff0c\u82e5\u8131\u6c34\u540e\u80fd\u5f62\u6210\u7a33\u5b9a\u7684\u78b3\n\u6b63\u79bb\u5b50\uff0c\u5219\u8be5\u8131\u6c34\u53cd\u5e94\u66f4\u5bb9\u6613\u8fdb\u884c\u30023. \u5206\u5b50\u5185\u90e8\u53ef\u80fd\u7684\u7a33\u5b9a\u5316\u4f5c\u7528\uff1a\u4f8b\u5982\uff0c\u82e5\u8131\u6c34\u540e\u80fd\u901a\u8fc7\u5171\u8f6d\u6216\n\u8d85\u5171\u8f6d\u6548\u5e94\u7a33\u5b9a\u5316\uff0c\u5219\u8be5\u8131\u6c34\u53cd\u5e94\u901f\u7387\u4e5f\u4f1a\u66f4\u5feb\u3002\u6839\u636e\u8fd9\u4e9b\u539f\u5219\uff0c\u4f60\u53ef\u4ee5\u5206\u6790\u6bcf\u4e2a\u5316\u5408\u7269\u7684\u7ed3\u6784\uff0c\u786e\n\u5b9a\u8131\u6c34\u53cd\u5e94\u7684\u76f8\u5bf9\u901f\u7387\u3002\u901a\u5e38\uff0c\u82e5\u8131\u6c34\u80fd\u591f\u5bfc\u81f4\u7a33\u5b9a\u7684\u5171\u8f6d\u4f53\u7cfb\u7684\u5f62\u6210\uff0c\u90a3\u4e48\u8be5\u8131\u6c34\u53cd\u5e94\u7684\u901f\u7387\u4f1a\u66f4\n\u5feb\u3002\u5728\u8fd9\u4e2a\u7279\u5b9a\u7684\u95ee\u9898\u4e2d\uff0c\u4f60\u9700\u8981\u8003\u8651\u6bcf\u4e2a\u5316\u5408\u7269\u8131\u6c34\u540e\u53ef\u80fd\u5f62\u6210\u7684\u78b3\u6b63\u79bb\u5b50\u7684\u7a33\u5b9a\u6027\uff0c\u4ee5\u53ca\u662f\u5426\u6709\n\u5171\u8f6d\u4f53\u7cfb\u7684\u5f62\u6210\u3002\u8bf7\u6839\u636e\u8fd9\u4e9b\u4fe1\u606f\u548c\u4f60\u5bf9\u5316\u5b66\u53cd\u5e94\u673a\u7406\u7684\u4e86\u89e3\u6765\u9009\u62e9\u6700\u53ef\u80fd\u6b63\u786e\u7684\u7b54\u6848\u3002\nGround Truth: (B) B>C>A\nError Category: Reject to Answer \nError Reason: GPT-4V refuses to directly answer the question.\n<\u56fe\u7247 1>\nFigure B128: A sample error case of Science (subcategory: Chemistry).\nBack to List of figures\n147\nPREPRINT\n\u79d1\u5b66:\u5316\u5b66\nQuestion: \u4e0b\u5217\u88c5\u7f6e\u7684\u8bbe\u8ba1\u80fd\u8fbe\u5230\u5b9e\u9a8c\u76ee\u7684\u7684\u662f\uff08 \uff09\nOption:\n(A) <\u56fe\u7247 1>\n(B) <\u56fe\u7247 2>\n(C) <\u56fe\u7247 3>\n(D) <\u56fe\u7247 4>\nGPT-4V: \u7531\u4e8e\u9898\u76ee\u6ca1\u6709\u7ed9\u51fa\u5177\u4f53\u7684\u5b9e\u9a8c\u76ee\u7684\uff0c\u6211\u65e0\u6cd5\u76f4\u63a5\u7ed9\u51fa\u6b63\u786e\u7b54\u6848\u3002\u4e0d\u8fc7\uff0c\u6211\u53ef\u4ee5\u6839\u636e\u6bcf\u5f20\u56fe\u7247\n\u6240\u5c55\u793a\u7684\u5b9e\u9a8c\u88c5\u7f6e\u7684\u7279\u70b9\u6765\u5206\u6790\u5b83\u4eec\u53ef\u80fd\u7684\u7528\u9014\uff1a(A) \u8fd9\u4e2a\u88c5\u7f6e\u770b\u8d77\u6765\u50cf\u662f\u7528\u4e8e\u84b8\u998f\u7684\u5b9e\u9a8c\u88c5\u7f6e\uff0c\u5176\n\u4e2d\u5305\u62ec\u4e86\u84b8\u998f\u74f6\u3001\u51b7\u51dd\u7ba1\u548c\u63a5\u6536\u74f6\u3002(B) \u8fd9\u4e2a\u88c5\u7f6e\u662f\u4e00\u4e2a\u4f0f\u6253\u7535\u6c60\u7684\u793a\u610f\u56fe\uff0c\u5176\u4e2d\u5305\u542b\u4e86\u950c\u548c\u94dc\u4e24\u79cd\n\u91d1\u5c5e\u7535\u6781\uff0c\u4ee5\u53ca\u76d0\u6865\u3002(C) \u8fd9\u4e2a\u88c5\u7f6e\u53ef\u80fd\u7528\u4e8e\u5c55\u793a\u4e0d\u6eb6\u4e8e\u6c34\u7684\u6db2\u4f53\uff08\u5982\u56db\u6c2f\u5316\u78b3CCl4\uff09\u4e0e\u6c34\u5206\u79bb\u7684\u5b9e\n\u9a8c\u3002(D) \u8fd9\u4e2a\u88c5\u7f6e\u53ef\u80fd\u7528\u4e8e\u6536\u96c6\u6c14\u4f53\uff08\u5982\u6c22\u7898\u9178HI\uff09\u7684\u5b9e\u9a8c\uff0c\u5176\u4e2d\u5305\u62ec\u4e86\u53d1\u751f\u74f6\u548c\u6536\u96c6\u74f6\u3002\u5982\u679c\u4f60\u80fd\n\u63d0\u4f9b\u5b9e\u9a8c\u7684\u76ee\u7684\u6216\u8005\u66f4\u591a\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u6211\u53ef\u4ee5\u5e2e\u52a9\u4f60\u66f4\u51c6\u786e\u5730\u9009\u62e9\u6b63\u786e\u7684\u9009\u9879\u3002\nGround Truth: (C) <\u56fe\u7247 3>\nError Category: Perceptual Error \nError Reason: GPT-4V does not understand the text in each image and refuses to answer.\n<\u56fe\u7247 1>\n<\u56fe\u7247 2>\n<\u56fe\u7247 3>\n<\u56fe\u7247 4>\nFigure B129: A sample error case of Science (subcategory: Chemistry).\nBack to List of figures\n148\nPREPRINT\n\u79d1\u5b66:\u5316\u5b66\nQuestion: <\u56fe\u7247 1>\u4ee5\u4e0b\u5316\u5408\u7269\u6700\u5bb9\u6613\u4e0eH2SO4\u53cd\u5e94\u7684\u662f___\n(A) A\n(B) B\n(C) C\n(D) D\nGPT-4V: \u5f88\u62b1\u6b49\uff0c\u7531\u4e8e\u56fe\u7247\u4e2d\u7684\u5316\u5408\u7269\u7ed3\u6784\u4e0d\u5b8c\u6574\uff0c\u6211\u65e0\u6cd5\u770b\u5230\u5b8c\u6574\u7684\u5206\u5b50\u7ed3\u6784\uff0c\u56e0\u6b64\u65e0\u6cd5\u51c6\u786e\u5224\u65ad\n\u54ea\u4e2a\u5316\u5408\u7269\u6700\u5bb9\u6613\u4e0e\u786b\u9178\uff08 H2SO4 \uff09\u53cd\u5e94\u3002\u901a\u5e38\u60c5\u51b5\u4e0b\uff0c\u786b\u9178\u4f1a\u4e0e\u9187\u3001\u919a\u6216\u542b\u6709\u6d3b\u6cfc\u6c22\u7684\u5316\u5408\u7269\u53cd\u5e94\uff0c\n\u4f46\u662f\u9700\u8981\u77e5\u9053\u5316\u5408\u7269\u7684\u5b8c\u6574\u7ed3\u6784\u624d\u80fd\u505a\u51fa\u51c6\u786e\u7684\u5224\u65ad\u3002\u5982\u679c\u60a8\u80fd\u63d0\u4f9b\u66f4\u591a\u4fe1\u606f\u6216\u8005\u5316\u5408\u7269\u7684\u5b8c\u6574\u540d\u79f0\uff0c\n\u6211\u6216\u8bb8\u80fd\u591f\u5e2e\u52a9\u60a8\u627e\u5230\u6b63\u786e\u7b54\u6848\u3002\nGround Truth: (C) C\nError Category: Perceptual Error \nError Reason: GPT-4V does not understand the figure in each option.\n<\u56fe\u7247 1>\nFigure B130: A sample error case of Science (subcategory: Chemistry).\nBack to List of figures\n149\nPREPRINT\n\u79d1\u5b66:\u5316\u5b66\nQuestion: <\u56fe\u7247 1>\u8be5\u5316\u5408\u7269\u542b\u6709\u7684\u6742\u73af\u7ed3\u6784\u662f____\nGPT-4V: \u6b63\u786e\u7b54\u6848\uff1a\u5627\u5576\u548c\u54aa\u5511\nGround Truth: D\n<\u56fe\u7247 1>\nError Category: Annotation Error \nError Reason: The annotated answer is D, which is incorrect.\nFigure B131: A sample error case of Science (subcategory: Chemistry).\nBack to List of figures\n150\nPREPRINT\n\u79d1\u5b66:\u5316\u5b66\nQuestion:\u666e\u4f10\u4ed6\u6c40\u662f\u4e00\u79cd\u8c03\u8282\u8840\u8102\u7684\u836f\u7269\uff0c\u5176\u7ed3\u6784\u7b80\u5f0f\u5982\u56fe1\u6240\u793a(\u672a\u8868\u793a\u51fa\u5176\u7a7a\u95f4\u6784\u578b)\u3002\u4e0b\u5217\u5173\u4e8e\n\u666e\u4f10\u4ed6\u6c40\u7684\u5316\u5b66\u6027\u8d28\u63cf\u8ff0\u6b63\u786e\u7684\u662f\uff08\uff09\nOption: \n(A) \u80fd\u53d1\u751f\u52a0\u6210\u3001\u53d6\u4ee3\u3001\u6d88\u53bb\u53cd\u5e94\n(B) \u4e0d\u80fd\u4f7f\u9178\u6027KMnO4\u6eb6\u6db2\u892a\u8272\n(C) \u80fd\u4e0eFeCl3\u6eb6\u6db2\u53d1\u751f\u663e\u8272\u53cd\u5e94\n(D) 1mol\u8be5\u7269\u8d28\u6700\u591a\u53ef\u4e0e1mol NaOH\u53cd\u5e94\nGPT-4V: \u6839\u636e\u6240\u63d0\u4f9b\u7684\u666e\u4f10\u4ed6\u6c40\u7684\u7ed3\u6784\u7b80\u5f0f\uff0c\u6211\u4eec\u53ef\u4ee5\u5206\u6790\u5176\u5316\u5b66\u6027\u8d28\uff1a(A) \u666e\u4f10\u4ed6\u6c40\u5206\u5b50\u4e2d\u542b\u6709\u78b3\n\u78b3\u53cc\u952e\u3001\u7f9f\u57fa\u548c\u7fa7\u57fa\uff0c\u8fd9\u4e9b\u5b98\u80fd\u56e2\u53ef\u4ee5\u53c2\u4e0e\u52a0\u6210\u53cd\u5e94\uff08\u5982\u78b3\u78b3\u53cc\u952e\uff09\u3001\u53d6\u4ee3\u53cd\u5e94\uff08\u5982\u7f9f\u57fa\u7684\u53d6\u4ee3\uff09\u548c\n\u6d88\u53bb\u53cd\u5e94\uff08\u5982\u7fa7\u57fa\u7684\u8131\u6c34\u53cd\u5e94\uff09\u3002\u56e0\u6b64\uff0c\u9009\u9879(A)\u662f\u6b63\u786e\u7684\u3002(B) \u78b3\u78b3\u53cc\u952e\u53ef\u4ee5\u4e0e\u9178\u6027KMnO4\u6eb6\u6db2\u53d1\u751f\n\u53cd\u5e94\uff0c\u4f7f\u6eb6\u6db2\u892a\u8272\uff0c\u56e0\u4e3aKMnO4\u662f\u4e00\u79cd\u5f3a\u6c27\u5316\u5242\uff0c\u53ef\u4ee5\u6c27\u5316\u53cc\u952e\u3002\u56e0\u6b64\uff0c\u9009\u9879(B)\u662f\u9519\u8bef\u7684\u3002(C) \u666e\u4f10\n\u4ed6\u6c40\u5206\u5b50\u4e2d\u6ca1\u6709\u915a\u7f9f\u57fa\uff0c\u800c\u662f\u9187\u7f9f\u57fa\uff0c\u901a\u5e38FeCl3\u6eb6\u6db2\u4e0e\u915a\u7c7b\u5316\u5408\u7269\u53d1\u751f\u663e\u8272\u53cd\u5e94\u3002\u56e0\u6b64\uff0c\u9009\u9879(C)\u662f\n\u9519\u8bef\u7684\u3002(D) \u666e\u4f10\u4ed6\u6c40\u5206\u5b50\u4e2d\u53ea\u6709\u4e00\u4e2a\u7fa7\u57fa\uff0c\u8fd9\u4e2a\u7fa7\u57fa\u53ef\u4ee5\u4e0e1mol\u7684NaOH\u53cd\u5e94\u751f\u6210\u76d0\u548c\u6c34\u3002\u56e0\u6b64\uff0c\u9009\n\u9879(D)\u662f\u6b63\u786e\u7684\u3002\u7efc\u4e0a\u6240\u8ff0\uff0c\u6b63\u786e\u7b54\u6848\u662f(A)\u548c(D)\u3002\nGround Truth: (A) \u80fd\u53d1\u751f\u52a0\u6210\u3001\u53d6\u4ee3\u3001\u6d88\u53bb\u53cd\u5e94\n<\u56fe\u7247 1>\nError Category: Reasoning Error \nError Reason: For option D, GPT-4V accurately assessed that the carboxyl group can react with 1 \nmol of NaOH. However, it overlooked the presence of -COOC- in pravastatin, which can also react \nwith 1 mol of NaOH. Therefore, 1 mol of this substance can react with up to 2 mol of NaOH. \nFigure B132: A sample error case of Science (subcategory: Chemistry).\nBack to List of figures\n151\nPREPRINT\n\u79d1\u5b66:\u5730\u7406\nQuestion:\u5982\u4e0b\u4e3a\u798f\u5efa\u5e08\u8303\u5927\u5b66\u5730\u7406\u5b66\u9662\u5730\u7406\u4e13\u4e1a\u5b66\u751f\u5229\u7528\u6691\u671f\u65c5\u6e38\u8003\u5bdf\u6240\u753b\u7684\u5730\u8c8c\u666f\u89c2\u7d20\u63cf<\u56fe\u7247\n1>\uff0c\u5b66\u751f\u4e59\u8bf4\u4ed6\u662f\u5168\u73ed\u540c\u5b66\u4e2d\u5230\u8fbe\u7684\u6d77\u62d4\u6700\u9ad8\u7684\u4e00\u4e2a\uff0c\u4ed6\u7ed8\u5236\u7684\u6700\u6709\u53ef\u80fd\u662f_\uff08\u5e8f\u53f7\uff09\nGPT-4V: \u5b66\u751f\u4e59\u8bf4\u4ed6\u662f\u5168\u73ed\u540c\u5b66\u4e2d\u5230\u8fbe\u7684\u6d77\u62d4\u6700\u9ad8\u7684\u4e00\u4e2a\uff0c\u4ed6\u7ed8\u5236\u7684\u6700\u6709\u53ef\u80fd\u662f\u8868\u793a\u6d77\u62d4\u8f83\u9ad8\u5730\u533a\u7684\n\u5730\u8c8c\u3002\u5728\u63d0\u4f9b\u7684\u4e94\u5e45\u7d20\u63cf\u4e2d\uff0cC\u548cE\u770b\u8d77\u6765\u50cf\u662f\u5c71\u8109\uff0c\u901a\u5e38\u5c71\u8109\u7684\u6d77\u62d4\u4f1a\u6bd4\u8f83\u9ad8\u3002\u4f46\u662f\uff0cC\u56fe\u4e2d\u7684\u5c71\u8109\n\u66f4\u52a0\u9661\u5ced\u548c\u9ad8\u8038\uff0c\u66f4\u6709\u53ef\u80fd\u4ee3\u8868\u6d77\u62d4\u6700\u9ad8\u7684\u5730\u533a\u3002\u56e0\u6b64\uff0c\u5b66\u751f\u4e59\u6700\u6709\u53ef\u80fd\u7ed8\u5236\u7684\u662fC\u56fe\u3002\n\u6b63\u786e\u7b54\u6848\uff1aC\nGround Truth: C \n<\u56fe\u7247 1>\nFigure B133: A sample correct case of Science (subcategory: Geography).\nBack to List of figures\n152\nPREPRINT\n\u79d1\u5b66:\u5730\u7406\nQuestion:\u5982\u4e0b<\u56fe\u7247 1>C\u8239\u63a8\u8fdf\u8f6c\u5411\u7684\u51b3\u5b9a\u662f\u822a\u884c\u4e2d\u504f\u79bb\u822a\u7ebf\u7684\u8239\u8236\u901a\u8fc7_\u6765\u7ea0\u6b63\u504f\u8239\nOption: \n(A)\u589e\u52a0\u822a\u901f\n(B)\u51cf\u5c0f\u822a\u901f\n(C)\u8c03\u6574\u8f6c\u5411\u65f6\u673a\n(D)\u66f4\u6362\u5bfc\u822a\u8bbe\u5907\nGPT-4V: \u6839\u636e\u6240\u63d0\u4f9b\u7684\u56fe\u7247\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u4e09\u8258\u8239\u7684\u822a\u7ebf\u3002C\u8239\u7684\u822a\u7ebf\u663e\u793a\u4e86\u5b83\u5728\u8f6c\u5411\u65f6\u6709\u6240\u5ef6\u8fdf\uff0c\n\u5bfc\u81f4\u504f\u79bb\u4e86\u539f\u5b9a\u822a\u7ebf\u3002\u4e3a\u4e86\u7ea0\u6b63\u8fd9\u79cd\u504f\u79bb\uff0c\u8239\u8236\u901a\u5e38\u9700\u8981\u8c03\u6574\u5176\u8f6c\u5411\u65f6\u673a\uff0c\u800c\u4e0d\u662f\u4ec5\u4ec5\u6539\u53d8\u822a\u901f\u6216\u66f4\n\u6362\u5bfc\u822a\u8bbe\u5907\u3002\u56e0\u6b64\uff0c\u6700\u5408\u9002\u7684\u9009\u9879\u662f\uff1a(C) \u8c03\u6574\u8f6c\u5411\u65f6\u673a\n\u6240\u4ee5\u6b63\u786e\u7b54\u6848\u662f (C)\u3002\nGround Truth: C \n<\u56fe\u7247 1>\nFigure B134: A sample correct case of Science (subcategory: Geography).\nBack to List of figures\n153\nPREPRINT\n\u79d1\u5b66:\u5730\u7406\nQuestion: \u6b64\u4e3a\u6cb3\u6d41\u88ad\u593a\u793a\u610f<\u56fe\u7247 1>\uff0c\u7532\u4e3a\u88ad\u593a\u6cb3\uff0c\u4e59\u4e3a\u65ad\u5934\u6cb3\u6216\u4e0d\u9002\u79f0\u6cb3\uff0c\u4e59'\u4e3a\u88ab\u88ad\u593a\u6cb3\uff0c\u52192\n\u4e3a_\n \nOption: \n(A) \u5df2\u4e0a\u6eaf\u7684\u88ad\u593a\u88c2\u70b9\n(B) \u65ad\u5934\u6cb3\n(C) \u88ab\u88ad\u593a\u6cb3\n(D) \u6539\u5411\u6cb3\u6216\u53cd\u5411\u6cb3\nGPT-4V: \u6b63\u786e\u7b54\u6848\u662f\uff1a(A) \u5df2\u4e0a\u6eaf\u7684\u88ad\u593a\u88c2\u70b9\n\u5728\u6cb3\u6d41\u88ad\u593a\u7684\u8fc7\u7a0b\u4e2d\uff0c\u88ad\u593a\u6cb3\uff08\u7532\uff09\u901a\u8fc7\u4fb5\u8680\u4f5c\u7528\u9010\u6e10\u5411\u4e0a\u6e38\u5ef6\u4f38\uff0c\u6700\u7ec8\u4f1a\u5207\u65ad\u53e6\u4e00\u6761\u6cb3\u6d41\uff08\u4e59'\uff09\n\u7684\u90e8\u5206\u6d41\u7a0b\uff0c\u5f62\u6210\u4e00\u4e2a\u88ad\u593a\u70b9\u3002\u5728\u793a\u610f\u56fe\u4e2d\uff0c2\u53f7\u4f4d\u7f6e\u6807\u793a\u7684\u662f\u8fd9\u4e2a\u88ad\u593a\u70b9\uff0c\u5b83\u662f\u7532\u6cb3\u6d41\u88ad\u593a\u4e59'\u6cb3\u6d41\n\u7684\u7ed3\u679c\u3002\u56e0\u6b64\uff0c2\u53f7\u4f4d\u7f6e\u662f\u5df2\u4e0a\u6eaf\u7684\u88ad\u593a\u88c2\u70b9\u3002\nGround Truth: (A) \u5df2\u4e0a\u6eaf\u7684\u88ad\u593a\u88c2\u70b9 \n<\u56fe\u7247 1>\nFigure B135: A sample correct case of Science (subcategory: Geography).\nBack to List of figures\n154\nPREPRINT\n\u79d1\u5b66:\u5730\u7406\nQuestion: \u8bfb\u4e0b\u9762\u7684\u6211\u56fd\u6e24\u6d77\u897f\u5cb8\u67d0\u5730\u7684\u5730\u8868\u7269\u8d28\u5206\u5e03\u793a\u610f<\u56fe\u7247 1>\uff0c\u5c06\u6b63\u786e\u7b54\u6848\u586b\u5728\u62ec\u5f27\u5185\u3002\u8be5\u5730\n\u533a\u5728\u2460\u6c99\u4e18\u3001\u2461\u6d77\u8680\u5d16\u3001\u2462\u4e09\u89d2\u6d32\u5e73\u539f\u548c\u2463\u98ce\u8680\u8611\u83c7\u6c14\u7b49\u51e0\u79cd\u53ef\u80fd\u7684\u5730\u8c8c\u7c7b\u578b\u4e2d\uff0c\u4e3b\u8981\u662f\uff08\uff09\nOption: \n(A) \u2460\u2461\n(B) \u2461\u2462\n(C) \u2460\u2462\n(D) \u2461\u2463\nGPT-4V: \u7531\u4e8e\u56fe\u7247\u4e2d\u7684\u6587\u5b57\u548c\u56fe\u4f8b\u4e0d\u662f\u5f88\u6e05\u6670\uff0c\u6211\u65e0\u6cd5\u63d0\u4f9b\u4e00\u4e2a\u786e\u5b9a\u7684\u7b54\u6848\u3002\u4f46\u662f\uff0c\u6839\u636e\u5e38\u89c1\u7684\u5730\u8c8c\n\u7279\u5f81\uff0c\u6c99\u4e18\u901a\u5e38\u4e0e\u98ce\u6210\u6c89\u79ef\u6709\u5173\uff0c\u6d77\u8680\u5d16\u4e0e\u6d77\u6d0b\u4fb5\u8680\u6709\u5173\uff0c\u4e09\u89d2\u6d32\u5e73\u539f\u4e0e\u6cb3\u6d41\u6c89\u79ef\u6709\u5173\uff0c\u800c\u98ce\u8680\u8611\u83c7\n\u72b6\u5730\u8c8c\u662f\u7531\u98ce\u8680\u4f5c\u7528\u5f62\u6210\u7684\u3002\u5728\u8fd9\u4e2a\u60c5\u51b5\u4e0b\uff0c\u6211\u4f1a\u6839\u636e\u56fe\u4e2d\u7684\u56fe\u4f8b\u548c\u5206\u5e03\u7279\u5f81\u6765\u731c\u6d4b\u6700\u53ef\u80fd\u7684\u5730\u8c8c\u7c7b\n\u578b\u7ec4\u5408\u3002\u5982\u679c\u60a8\u80fd\u63d0\u4f9b\u66f4\u6e05\u6670\u7684\u56fe\u4f8b\u6216\u8005\u5bf9\u56fe\u4e2d\u7684\u6807\u8bb0\u8fdb\u884c\u89e3\u91ca\uff0c\u6211\u53ef\u4ee5\u5e2e\u52a9\u60a8\u66f4\u51c6\u786e\u5730\u786e\u5b9a\u7b54\u6848\u3002\nGround Truth: (C) \u2460\u2462\n<\u56fe\u7247 1>\nError Category: Reject to Answer\nError Reason: GPT-4V refuses to answer this question without further explanations for the figure.\nFigure B136: A sample error case of Science (subcategory: Geography).\nBack to List of figures\n155\nPREPRINT\n\u79d1\u5b66:\u5730\u7406\nQuestion: \u5982<\u56fe\u7247 1>\u6240\u793a\uff0c\u7532\u7684\u7edd\u5bf9\u9ad8\u5ea6\u4e3a_\u7c73\nGPT-4V: \u6839\u636e\u56fe\u7247\u4e2d\u7684\u4fe1\u606f\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u70b9\u7532\u4e0e\u6d77\u5e73\u9762\u7684\u76f8\u5bf9\u9ad8\u5ea6\u662f1500\u7c73\uff0c\u800c\u70b9\u4e59\u4e0e\u6d77\u5e73\u9762\u7684\u76f8\n\u5bf9\u9ad8\u5ea6\u662f500\u7c73\u3002\u70b9\u4e59\u5230\u70b9\u4e19\u7684\u76f8\u5bf9\u9ad8\u5ea6\u5dee\u662f800\u7c73\uff0c\u56e0\u6b64\u70b9\u4e19\u7684\u76f8\u5bf9\u9ad8\u5ea6\u662f500\u7c73\u52a0\u4e0a800\u7c73\uff0c\u7b49\u4e8e\n1300\u7c73\u3002\u70b9\u4e19\u5230\u70b9\u7532\u7684\u76f8\u5bf9\u9ad8\u5ea6\u5dee\u662f1000\u7c73\uff0c\u6240\u4ee5\u70b9\u7532\u7684\u7edd\u5bf9\u9ad8\u5ea6\u662f\u70b9\u4e19\u7684\u76f8\u5bf9\u9ad8\u5ea6\u52a0\u4e0a\u8fd9\u4e2a\u9ad8\u5ea6\u5dee\uff0c\n\u53731300\u7c73\u52a0\u4e0a1000\u7c73\uff0c\u7b49\u4e8e2300\u7c73\u3002\n\u6240\u4ee5\uff0c\u7532\u7684\u7edd\u5bf9\u9ad8\u5ea6\u4e3a2300\u7c73\u3002\nGround Truth: 2500\n<\u56fe\u7247 1>\nError Category: Perceptual Error, Lack of Knowledge \nError Reason: The GPT-4V lacks specific knowledge of the concept of \u201cabsolute height\u201d, failing to \ncomprehend that it refers to the height relative to sea level. Furthermore, GPT-4V misinterprets the \ndotted line in the image indicating sea level and the point labeled \u201c\u4e59\u201c and incorrectly calculates the \nheight difference between point \u201c\u7532\u201c and \u201c\u4e19\u201c .\nFigure B137: A sample error case of Science (subcategory: Geography).\nBack to List of figures\n156\nPREPRINT\n\u79d1\u5b66:\u5730\u7406\nQuestion: \u5982<\u56fe\u7247 1>\u6240\u793a,A\u7ebf\u4e3a\u7a33\u5b9a\u60c5\u51b5\u4e0b\u7684\u6c34\u4f4d\u6d41\u91cf\u5173\u7cfb\u66f2\u7ebf,\u5219\u6da8\u6d2a\u60c5\u51b5\u7684\u6c34\u4f4d\u6d41\u91cf\u5173\u7cfb\u66f2\u7ebf\u4e00\n\u822c\u4e3a_\uff08A/B/C\uff09\u7ebf\nGPT-4V: \u6b63\u786e\u7b54\u6848\uff1aB\u7ebf\n\u5728\u6da8\u6d2a\u60c5\u51b5\u4e0b\uff0c\u6c34\u4f4d\u6d41\u91cf\u5173\u7cfb\u66f2\u7ebf\u901a\u5e38\u4f1a\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6c34\u4f4d\u5bf9\u5e94\u66f4\u5927\u7684\u6d41\u91cf\uff0c\u8fd9\u662f\u56e0\u4e3a\u6d2a\u6c34\u589e\u52a0\u4e86\u6cb3\n\u6d41\u7684\u6d41\u91cf\uff0c\u540c\u65f6\u4e5f\u4f1a\u5bfc\u81f4\u6c34\u4f4d\u7684\u4e0a\u5347\u3002\u5728\u63d0\u4f9b\u7684\u56fe\u50cf\u4e2d\uff0cB\u7ebf\u76f8\u5bf9\u4e8eA\u7ebf\u5728\u6c34\u4f4d\uff08Z\u8f74\uff09\u548c\u6d41\u91cf\uff08Q\u8f74\uff09\n\u4e0a\u90fd\u6709\u6240\u589e\u52a0\uff0c\u8fd9\u7b26\u5408\u6da8\u6d2a\u60c5\u51b5\u4e0b\u6c34\u4f4d\u6d41\u91cf\u5173\u7cfb\u7684\u53d8\u5316\u8d8b\u52bf\u3002\u56e0\u6b64\uff0c\u6da8\u6d2a\u60c5\u51b5\u4e0b\u7684\u6c34\u4f4d\u6d41\u91cf\u5173\u7cfb\u66f2\u7ebf\n\u4e00\u822c\u4e3aB\u7ebf\u3002\nGround Truth: C\n<\u56fe\u7247 1>\nError Category: Reasoning Error \nError Reason: GPT-4V successfully understands the images and text and correctly recalls the \nknowledge that a higher water level corresponds to a larger flow rate in a flood situation. However,\nthe model directly select the answer without fixing the variables. Actually, the model should fix one\nvariable and compare the values of the other variable.\nFigure B138: A sample error case of Science (subcategory: Geography).\nBack to List of figures\n157\nPREPRINT\n\u79d1\u5b66:\u5730\u7406\nQuestion: <\u56fe\u7247 1>\u82e5\u56fe\u4e2d\u6240\u793a\u7684\u662f\u6c34\u5faa\u73af\u793a\u610f\u56fe\uff0c\u5219\u80fd\u4f53\u73b0\u4e1c\u4e9a\u590f\u5b63\u98ce\u7684\u73af\u8282\u662f( )\nOption:\n(A) \u2460\n(B) \u2461\n(C) \u2462\n(D) \u2463\nGPT-4V: \u8fd9\u5f20\u56fe\u7247\u663e\u793a\u7684\u662f\u4e00\u4e2a\u6c34\u5faa\u73af\u7684\u793a\u610f\u56fe\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u6709\u84b8\u53d1\u3001\u964d\u6c34\u7b49\u8fc7\u7a0b\u3002\u4e1c\u4e9a\u590f\u5b63\u98ce\u901a\n\u5e38\u4e0e\u6d77\u6d0b\u4e0a\u7684\u84b8\u53d1\u548c\u968f\u540e\u7684\u964d\u6c34\u8fc7\u7a0b\u6709\u5173\uff0c\u56e0\u4e3a\u590f\u5b63\u98ce\u5e26\u6765\u7684\u662f\u6765\u81ea\u6d77\u6d0b\u7684\u6e7f\u6da6\u7a7a\u6c14\uff0c\u8fd9\u4e9b\u6e7f\u6da6\u7a7a\u6c14\n\u4e0a\u5cb8\u540e\u4f1a\u4e0a\u5347\u5e76\u51b7\u5374\uff0c\u5bfc\u81f4\u6c34\u84b8\u6c14\u51dd\u7ed3\u6210\u4e91\uff0c\u6700\u7ec8\u5f62\u6210\u964d\u6c34\u3002 \u5728\u8fd9\u5f20\u56fe\u4e2d\uff0c\u2460\u53f7\u6807\u8bb0\u7684\u8fc7\u7a0b\u53ef\u80fd\u4ee3\n\u8868\u6c34\u84b8\u6c14\u7684\u6c34\u5e73\u8fd0\u52a8\uff0c\u2461\u53f7\u6807\u8bb0\u7684\u8fc7\u7a0b\u4ee3\u8868\u84b8\u53d1\uff0c\u2462\u53f7\u6807\u8bb0\u7684\u8fc7\u7a0b\u4ee3\u8868\u964d\u6c34\uff0c\u2463\u53f7\u6807\u8bb0\u7684\u8fc7\u7a0b\u4ee3\u8868\u6c34\n\u84b8\u6c14\u4e0a\u5347\u3002 \u6839\u636e\u8fd9\u4e9b\u4fe1\u606f\uff0c\u80fd\u4f53\u73b0\u4e1c\u4e9a\u590f\u5b63\u98ce\u73af\u8282\u7684\u662f\u84b8\u53d1\u548c\u964d\u6c34\u8fc7\u7a0b\uff0c\u5373\u9009\u9879 (B) \u2461 \u548c (C) \u2462\u3002\n\u7136\u800c\uff0c\u7531\u4e8e\u9898\u76ee\u8981\u6c42\u9009\u62e9\u5355\u4e2a\u9009\u9879\uff0c\u6211\u4eec\u9700\u8981\u786e\u5b9a\u54ea\u4e00\u4e2a\u73af\u8282\u66f4\u80fd\u4f53\u73b0\u4e1c\u4e9a\u590f\u5b63\u98ce\u7684\u7279\u70b9\u3002\u901a\u5e38\u60c5\u51b5\n\u4e0b\uff0c\u4e1c\u4e9a\u590f\u5b63\u98ce\u66f4\u4e3a\u4eba\u4eec\u6240\u719f\u77e5\u7684\u662f\u5176\u5e26\u6765\u7684\u964d\u6c34\uff0c\u56e0\u6b64\u6211\u4f1a\u9009\u62e9 (C) \u2462 \u4f5c\u4e3a\u6700\u53ef\u80fd\u6b63\u786e\u7684\u9009\u9879\u3002\nGround Truth: (D) \u2463\n<\u56fe\u7247 1>\nError Category: Reasoning Error \nError Reason: GPT-4V accurately identified the processes within the diagram of water cycle, \nincluding evaporation, precipitation, and recognized that summer winds bring moist air from the \nocean. However, it made mistakes while reasoning the various stages of the water cycle. The correct \nstages should be: \u2460 Precipitation, \u2461 Surface Runoff, \u2462 Evaporation, and \u2463 Vapor Transportation. \nAs the East Asian summer monsoon is a southeastern wind originating from the ocean, the vapor \ntransport in the \u2463 comes from the ocean. Therefore, the stage that reflects the characteristics of the \nEast Asian summer monsoon is \u2463.\nFigure B139: A sample error case of Science (subcategory: Geography).\nBack to List of figures\n158\nPREPRINT\n\u79d1\u5b66:\u5730\u7406\nQuestion: <\u56fe\u7247 1>\u4e2dM\u3001N\u4e24\u5730\u964d\u6c34\u7684\u5171\u540c\u7279\u70b9\u662f_\nOption: \n(A) \u5168\u5e74\u5c11\u96e8\n(B) \u5168\u5e74\u591a\u96e8\n(C) \u5b63\u8282\u6027\u964d\u6c34\n(D) \u5168\u5e74\u5747\u8861\u964d\u6c34\nGPT-4V:\u6839\u636e\u63d0\u4f9b\u7684\u56fe\u7247\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u4e00\u4e2a\u6c14\u5019\u56fe\uff0c\u5b83\u663e\u793a\u4e86\u964d\u6c34\u91cf\uff08\u4ee5\u6beb\u7c73\u4e3a\u5355\u4f4d\uff09\u548c\u6e29\u5ea6\uff08\u4ee5\n\u6444\u6c0f\u5ea6\u4e3a\u5355\u4f4d\uff09\u968f\u65f6\u95f4\uff08\u6708\u4efd\uff09\u7684\u53d8\u5316\u3002\u4ece\u56fe\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0c\u964d\u6c34\u91cf\u5728\u67d0\u4e9b\u6708\u4efd\u7279\u522b\u9ad8\uff0c\u800c\u5728\u5176\u4ed6\u6708\u4efd\n\u5219\u76f8\u5bf9\u8f83\u4f4e\u3002\u8fd9\u8868\u660e\u964d\u6c34\u5177\u6709\u5b63\u8282\u6027\u7684\u7279\u70b9\u3002\u56e0\u6b64\uff0c\u6839\u636e\u56fe\u8868\u6240\u793a\u7684\u6570\u636e\uff0cM\u548cN\u4e24\u5730\u7684\u5171\u540c\u7279\u70b9\u662f\uff1a\nC) \u5b63\u8282\u6027\u964d\u6c34\n\u6240\u4ee5\u6b63\u786e\u7b54\u6848\u662f (C) \u5b63\u8282\u6027\u964d\u6c34\u3002\nGround Truth: (A) \u5168\u5e74\u5c11\u96e8\n<\u56fe\u7247 1>\nError Category: Textual Understanding\nError Reason: The question inquired about the precipitation characteristics of areas M and N, \nhowever, GPT-4V's response pertained to the rainfall patterns of area P, which consequently \nresulted in an incorrect answer.\nFigure B140: A sample error case of Science (subcategory: Geography).\nBack to List of figures\n159\nPREPRINT\n\u79d1\u5b66:\u5730\u7406\nQuestion: \u5728\u4e24\u4e2a\u76f8\u540c\u7684\u957f\u65b9\u5f62\u5851\u6599\u76d2\u5b50\u7684\u4e00\u7aef\u7528\u6f6e\u6e7f\u7684\u571f\u58e4\u5206\u522b\u642d\u8d77\u4e00\u5ea7\u76f8\u540c\u5761\u5ea6\u7684\u201c\u5c0f\u5c71\u201d\uff0c\u4ece\n\u76d2\u5b50\u7684\u53e6\u4e00\u7aef\u5012\u5165\u7b49\u91cf\u7684\u6c34\uff0c\u5f62\u6210\u201c\u6e56\u6cca\u201d\uff1b\u4e0d\u540c\u7684\u662f\u5728\u5176\u4e2d\u4e00\u4e2a\u76d2\u5b50\u5185\uff0c\u5c06\u4e00\u5757\u5fae\u6e7f\u7684\u6d77\u7ef5\u538b\u5165\n\u201c\u5c0f\u5c71\u201d\u5e95\u7aef\u7684\u571f\u58e4\u91cc\uff08\u5982\u56fe<\u56fe\u7247 1>\uff09\u3002\u5206\u522b\u7528\u55b7\u6c34\u58f6\uff0c\u5f80\u4e24\u4e2a\u201c\u5c0f\u5c71\u201d\u4e0a\u6d12\u7b49\u91cf\u7684\u6c34\uff0c\u6a21\u62df\u964d\u6c34\u3002\n\u89c2\u5bdf\u3001\u6bd4\u8f83\u4e24\u4e2a\u76d2\u5b50\u5185\u6c34\u9762\u53d1\u751f\u7684\u53d8\u5316\u3002\u8be5\u5b9e\u9a8c\u6a21\u62df\u7684\u5730\u7406\u4e8b\u5b9e\u662f\uff08\uff09\nOption: \n(A) \u6e56\u6cca\u80fd\u591f\u8c03\u8282\u7a7a\u6c14\u6e7f\u5ea6\n(B) \u5761\u5ea6\u53d8\u5316\u4f1a\u6539\u53d8\u5730\u8868\u5f84\u6d41\u91cf\n(C) \u6e7f\u5730\u80fd\u591f\u5438\u7eb3\u5730\u8868\u5f84\u6d41\n(D) \u964d\u6c34\u53d8\u5316\u80fd\u591f\u5f71\u54cd\u81ea\u7136\u73af\u5883\nGPT-4V: \u6839\u636e\u5b9e\u9a8c\u7684\u63cf\u8ff0\u548c\u56fe\u793a\uff0c\u8fd9\u4e2a\u5b9e\u9a8c\u4e3b\u8981\u662f\u4e3a\u4e86\u89c2\u5bdf\u548c\u6bd4\u8f83\u6709\u65e0\u6d77\u7ef5\uff08\u4ee3\u8868\u6e7f\u5730\uff09\u5bf9\u5730\u8868\u5f84\u6d41\n\u7684\u5f71\u54cd\u3002\u5728\u8fd9\u4e2a\u5b9e\u9a8c\u4e2d\uff0c\u6d77\u7ef5\u7684\u4f5c\u7528\u662f\u5438\u6536\u548c\u51cf\u7f13\u6c34\u6d41\uff0c\u6a21\u62df\u6e7f\u5730\u5bf9\u5730\u8868\u5f84\u6d41\u7684\u5438\u7eb3\u4f5c\u7528\u3002\u56e0\u6b64\uff0c\u6b63\n\u786e\u7b54\u6848\u662f\uff1a(C) \u6e7f\u5730\u80fd\u591f\u5438\u7eb3\u5730\u8868\u5f84\u6d41\n\u5176\u4ed6\u9009\u9879\u4e0e\u5b9e\u9a8c\u7684\u76f4\u63a5\u76ee\u7684\u4e0d\u76f8\u5173\u3002\u9009\u9879 (A) \u6e56\u6cca\u80fd\u591f\u8c03\u8282\u7a7a\u6c14\u6e7f\u5ea6\uff0c(B) \u5761\u5ea6\u53d8\u5316\u4f1a\u6539\u53d8\u5730\u8868\u5f84\n\u6d41\u91cf\uff0c\u548c (D) \u964d\u6c34\u53d8\u5316\u80fd\u591f\u5f71\u54cd\u81ea\u7136\u73af\u5883\uff0c\u867d\u7136\u662f\u6b63\u786e\u7684\u5730\u7406\u4e8b\u5b9e\uff0c\u4f46\u5b83\u4eec\u5e76\u4e0d\u662f\u8fd9\u4e2a\u5b9e\u9a8c\u8bbe\u8ba1\u7684\n\u4e3b\u8981\u89c2\u5bdf\u70b9\u3002\nGround Truth: (C) \u6e7f\u5730\u80fd\u591f\u5438\u7eb3\u5730\u8868\u5f84\u6d41\n<\u56fe\u7247 1>\nError Category: Answer Extraction Error \nError Reason: GPT-4V answered the question correctly, but the function to extract the answer has \nthe limitation, which led to the wrong answer being extracted.\nFigure B141: A sample error case of Science (subcategory: Geography).\nBack to List of figures\n160\nPREPRINT\n\u79d1\u5b66:\u5730\u7406\nQuestion: \u5224\u65ad\u4e0b\u9762\u9648\u8ff0\u5bf9\u9519\uff1a\u5982<\u56fe\u7247 1>\u4e3a\u5e7c\u5e74\u671f\u6cb3\u8c37\nGPT-4V: \u8fd9\u4e2a\u9648\u8ff0\u662f\u6b63\u786e\u7684\u3002\u6240\u63d0\u4f9b\u7684\u56fe\u7247\u663e\u793a\u4e86\u4e00\u4e2a\u6cb3\u6d41\u5728\u5176\u6cb3\u8c37\u4e2d\u873f\u8712\u6d41\u52a8\uff0c\u6cb3\u8c37\u7684\u4fa7\u58c1\u8f83\u4e3a\u9661\n\u5ced\uff0c\u6cb3\u6d41\u7684\u5f2f\u66f2\u7a0b\u5ea6\u8f83\u5927\uff0c\u8fd9\u4e9b\u7279\u5f81\u7b26\u5408\u5e7c\u5e74\u671f\u6cb3\u8c37\u7684\u5178\u578b\u7279\u5f81\u3002\u5e7c\u5e74\u671f\u6cb3\u8c37\u901a\u5e38\u5177\u6709\u8f83\u4e3a\u9661\u5ced\u7684\u6cb3\n\u5cb8\u548c\u8f83\u591a\u7684\u6cb3\u6d41\u5f2f\u66f2\uff0c\u56e0\u4e3a\u6cb3\u6d41\u6b63\u5728\u4fb5\u8680\u5176\u5e8a\u5ca9\uff0c\u5f62\u6210\u6df1\u800c\u72ed\u7a84\u7684\u6cb3\u8c37\u3002\u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\uff0c\u6cb3\u6d41\u4f1a\u7ee7\n\u7eed\u4fb5\u8680\u548c\u62d3\u5bbd\u6cb3\u8c37\uff0c\u8fdb\u5165\u6210\u719f\u671f\u548c\u8001\u5e74\u671f\u3002\nGround Truth: \u9519\n<\u56fe\u7247 1>\nError Category: Lack of Knowledge\nError Reason: The model understands to assess the development stage of a valley based on the \nsteepness of the valley's sidewalls and the winding nature of the river. However, it has confused the \ncharacteristics of mature valleys with those of youthful ones. Rivers in youthful valleys are rather \nstraight, instead of winding.\nFigure B142: A sample error case of Science (subcategory: Geography).\nBack to List of figures\n161\nPREPRINT\n\u79d1\u5b66: \u6570\u5b66\nQuestion:  \u4e8c\u7ef4\u79bb\u6563\u578b\u968f\u673a\u53d8\u91cf(X,Y)\u7684\u5206\u5e03\u5f8b\u4e3a<\u56fe\u7247 \n1>\uff0cF(x,y)\u4e3a(X,Y)\u7684\u8054\u5408\u5206\u5e03\u51fd\u6570,\u5219\nF(1.5,1.5\uff09\u7b49\u4e8e\nOption: \n(A) 1/6\n(B) 1/2\n(C) 1/3\n(D) 1/4\nGPT-4V: \u8981\u627e\u5230F(1.5, 1.5)\uff0c\u6211\u4eec\u9700\u8981\u8ba1\u7b97\u6240\u6709X\u22641.5\u548cY\u22641.5\u7684\u6982\u7387\u503c\u4e4b\u548c\u3002\u6839\u636e\u7ed9\u5b9a\u7684\u5206\u5e03\u5f8b\n\u8868\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff1a\n- \u5f53X=1\u4e14Y=0\u65f6\uff0c\u6982\u7387\u662f1/6\u3002\n- \u5f53X=1\u4e14Y=1\u65f6\uff0c\u6982\u7387\u662f1/3\u3002\n\u7531\u4e8e1.5\u5927\u4e8e\u8868\u4e2d\u7684X\u548cY\u7684\u6700\u5927\u503c1\uff0c\u6211\u4eec\u4e0d\u9700\u8981\u8003\u8651X=2\u6216Y=2\u7684\u60c5\u51b5\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u503c\u8d85\u51fa\u4e86\u6211\u4eec\u8ba1\u7b97\nF(1.5, 1.5)\u65f6\u8003\u8651\u7684\u8303\u56f4\u3002\n\u56e0\u6b64\uff0cF(1.5, 1.5)\u662f\u8fd9\u4e24\u4e2a\u6982\u7387\u503c\u7684\u548c\uff1a\nF(1.5, 1.5) = 1/6 + 1/3 = 1/6 + 2/6 = 3/6 = 1/2\n\u6240\u4ee5\u6b63\u786e\u7b54\u6848\u662f (B) 1/2\u3002\nGround Truth: (B) 1/2\n<\u56fe\u7247 1>\nFigure B143: A sample correct case of Science (subcategory: Mathematics).\nBack to List of figures\n162\nPREPRINT\n\u79d1\u5b66: \u6570\u5b66\nQuestion: \u67d0\u7535\u5b50\u751f\u4ea7\u4f01\u4e1a2003\u5e74\u548c2002\u5e74\u4e09\u79cd\u4e3b\u8981\u4ea7\u54c1\u7684\u5355\u4f4d\u6210\u672c\u548c\u4ea7\u91cf\u8d44\u6599\u5982<\u56fe\u7247 1>\uff0c\u5219\u4e09\u79cd\n\u4ea7\u54c1\u7684\u4ea7\u503c\u603b\u6307\u6570\u53ca\u4ea7\u503c\u589e\u51cf\u603b\u989d\u4e3a_\u5143\u3002\nGPT-4V: \u4e3a\u4e86\u8ba1\u7b97\u4e09\u79cd\u4ea7\u54c1\u7684\u4ea7\u503c\u603b\u6307\u6570\u53ca\u4ea7\u503c\u589e\u51cf\u603b\u989d\uff0c\u6211\u4eec\u9700\u8981\u5148\u8ba1\u7b97\u6bcf\u79cd\u4ea7\u54c12002\u5e74\u548c2003\u5e74\n\u7684\u4ea7\u503c\uff0c\u7136\u540e\u8ba1\u7b97\u4ea7\u503c\u589e\u51cf\u989d\u548c\u4ea7\u503c\u603b\u6307\u6570\u3002\n\u9996\u5148\uff0c\u6211\u4eec\u8ba1\u7b97\u6bcf\u79cd\u4ea7\u54c1\u7684\u4ea7\u503c\uff08\u5355\u4f4d\u6210\u672c\u4e58\u4ee5\u4ea7\u91cf\uff09\uff1a\n\u7535\u80fd\u7535\u6c60\uff1a\n2002\u5e74\u4ea7\u503c = 900\u5143/\u5343 * 8.5\u5343 = 7650\u5143\n2003\u5e74\u4ea7\u503c = 1000\u5143/\u5343 * 9.0\u5343 = 9000\u5143\n\u7535\u8def\u677f\uff1a\n2002\u5e74\u4ea7\u503c = 500\u5143/\u5757 * 55.0\u5757 = 27500\u5143\n2003\u5e74\u4ea7\u503c = 500\u5143/\u5757 * 58.5\u5757 = 29250\u5143\n\u5bb6\u7535\u673a\u5177\uff1a\n2002\u5e74\u4ea7\u503c = 700\u5143/\u53f0 * 100.0\u53f0 = 70000\u5143\n2003\u5e74\u4ea7\u503c = 800\u5143/\u53f0 * 115.0\u53f0 = 92000\u5143\n\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u8ba1\u7b97\u6bcf\u79cd\u4ea7\u54c1\u7684\u4ea7\u503c\u589e\u51cf\u989d\uff1a\n\u7535\u80fd\u7535\u6c60\u589e\u51cf\u989d = 9000\u5143 - 7650\u5143 = 1350\u5143\n\u7535\u8def\u677f\u589e\u51cf\u989d = 29250\u5143 - 27500\u5143 = 1750\u5143\n\u5bb6\u7535\u673a\u5177\u589e\u51cf\u989d = 92000\u5143 - 70000\u5143 = 22000\u5143\n\u7136\u540e\uff0c\u6211\u4eec\u8ba1\u7b97\u4ea7\u503c\u589e\u51cf\u603b\u989d\uff1a\n\u4ea7\u503c\u589e\u51cf\u603b\u989d = 1350\u5143 + 1750\u5143 + 22000\u5143 = 25100\u5143\n\u6700\u540e\uff0c\u6211\u4eec\u8ba1\u7b97\u4ea7\u503c\u603b\u6307\u6570\u3002\u4ea7\u503c\u603b\u6307\u6570\u662f\u901a\u8fc7\u5c06\u5404\u4e2a\u4ea7\u54c1\u76842003\u5e74\u4ea7\u503c\u4e0e2002\u5e74\u4ea7\u503c\u8fdb\u884c\u6bd4\u8f83\u5f97\u51fa\n\u7684\u3002\u6211\u4eec\u4f7f\u7528\u7684\u662f\u7b80\u5355\u7684\u4ea7\u503c\u603b\u6307\u6570\u516c\u5f0f\uff0c\u5373\uff1a\n\u4ea7\u503c\u603b\u6307\u6570 = (2003\u5e74\u5404\u4ea7\u54c1\u4ea7\u503c\u4e4b\u548c / 2002\u5e74\u5404\u4ea7\u54c1\u4ea7\u503c\u4e4b\u548c) * 100%\n2002\u5e74\u4ea7\u503c\u603b\u548c = 7650\u5143 + 27500\u5143 + 70000\u5143 = 105150\u5143\n2003\u5e74\u4ea7\u503c\u603b\u548c = 9000\u5143 + 29250\u5143 + 92000\u5143 = 130250\u5143\n\u4ea7\u503c\u603b\u6307\u6570 = (130250\u5143 / 105150\u5143) * 100% \u2248 123.88%\n\u7efc\u4e0a\u6240\u8ff0\uff0c\u4e09\u79cd\u4ea7\u54c1\u7684\u4ea7\u503c\u603b\u6307\u6570\u4e3a123.88%\uff0c\u4ea7\u503c\u589e\u51cf\u603b\u989d\u4e3a25100\u5143\u3002\nGround Truth: 25100\n<\u56fe\u7247 1>\nFigure B144: A sample correct case of Science (subcategory: Mathematics).\nBack to List of figures\n163\nPREPRINT\n\u79d1\u5b66: \u6570\u5b66\nQuestion: \u968f\u673a\u62bd\u53d6\u4e8610\u4e2a\u5bb6\u5ead,\u8c03\u67e5\u4e86\u4ed6\u4eec\u7684\u5bb6\u5ead\u6708\u6536\u5165x(\u5355\u4f4d:\u767e\u5143)\u548c\u6708\u652f\u51fay(\u5355\u4f4d:\u767e\u5143),\u8bb0\u5f55\n\u4e8e<\u56fe\u7247 1>\uff0cy\u4e0ex_(\u586b\u201c\u662f\u201d\u6216\u201c\u5426\u201d)\u5b58\u5728\u7ebf\u6027\u5173\u7cfb_\u3002\nGPT-4V: \u6839\u636e\u6240\u63d0\u4f9b\u7684\u56fe\u7247\u4fe1\u606f\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u5bb6\u5ead\u6708\u6536\u5165x\u548c\u6708\u652f\u51fay\u7684\u6570\u636e\u3002\u4e3a\u4e86\u786e\u5b9a\u8fd9\u4e24\u4e2a\u53d8\u91cf\n\u4e4b\u95f4\u662f\u5426\u5b58\u5728\u7ebf\u6027\u5173\u7cfb\uff0c\u6211\u4eec\u9700\u8981\u5206\u6790\u6570\u636e\u70b9\u662f\u5426\u5927\u81f4\u6392\u5217\u5728\u4e00\u6761\u76f4\u7ebf\u4e0a\uff0c\u6216\u8005\u4f7f\u7528\u7edf\u8ba1\u65b9\u6cd5\u5982\u8ba1\u7b97\n\u76f8\u5173\u7cfb\u6570\u6765\u5224\u65ad\u3002\n\u4ece\u56fe\u7247\u4e2d\u63d0\u4f9b\u7684\u6570\u636e\u6765\u770b\uff0c\u6ca1\u6709\u660e\u663e\u7684\u6a21\u5f0f\u8868\u660ex\u548cy\u4e4b\u95f4\u5b58\u5728\u76f4\u63a5\u7684\u7ebf\u6027\u5173\u7cfb\u3002\u6570\u636e\u70b9\u6ca1\u6709\u663e\u793a\u51fa\u4e00\n\u81f4\u7684\u8d8b\u52bf\uff0c\u5373\u968f\u7740x\u7684\u589e\u52a0\uff0cy\u5e76\u6ca1\u6709\u663e\u793a\u51fa\u4e00\u81f4\u7684\u589e\u52a0\u6216\u51cf\u5c11\u3002\u56e0\u6b64\uff0c\u6839\u636e\u8fd9\u4e9b\u6570\u636e\u70b9\uff0c\u6211\u4eec\u4e0d\u80fd\u786e\n\u5b9a\u5b58\u5728\u7ebf\u6027\u5173\u7cfb\u3002\n\u6b63\u786e\u7b54\u6848\uff1a\u5426\nGround Truth: \u662f\n<\u56fe\u7247 1>\nError Category: Perceptual Error\nError Reason:  GPT-4V mistakenly believes that there is no linear correlation between x and y, but \nwe can calculate it based on the Pearson correlation coefficient. The answer is r=0.909, which means \nthere is a strong linear correlation.\nFigure B145: A sample error case of Science (subcategory: Mathematics).\nBack to List of figures\n164\nPREPRINT\n\u79d1\u5b66: \u7269\u7406\nQuestion: \u5982<\u56fe\u7247 1>\uff0c\u5355\u8272\u5e73\u884c\u5149\u5782\u76f4\u5165\u5c04\u5230\u53cc\u7f1d\u4e0a\u3002\u89c2\u5bdf\u5c4f\u4e0aP\u70b9\u5230\u4e24\u7f1d\u7684\u8ddd\u79bb\u5206\u522b\u4e3a\ud835\udc5f\ud835\udc5f1\u548c\ud835\udc5f\ud835\udc5f2\u3002\u8bbe\n\u53cc\u7f1d\u548c\u5c4f\u4e4b\u95f4\u5145\u6ee1\u6298\u5c04\u7387\u4e3a\ud835\udc5b\ud835\udc5b\u7684\u5a92\u8d28,\u5219P\u70b9\u5904\u4e8c\u76f8\u5e72\u5149\u7ebf\u7684\u5149\u7a0b\u5dee\u4e3a_\u3002\nOption: \n(A) \ud835\udc5f\ud835\udc5f2 \u2212 \ud835\udc5f\ud835\udc5f1\n(B) \ud835\udc5b\ud835\udc5b(\ud835\udc5f\ud835\udc5f2 \u2212 \ud835\udc5f\ud835\udc5f1)\n(C) 2\ud835\udc5b\ud835\udc5b(\ud835\udc5f\ud835\udc5f2 \u2212 \ud835\udc5f\ud835\udc5f1)\n(D) 2(\ud835\udc5f\ud835\udc5f2 \u2212 \ud835\udc5f\ud835\udc5f1)\nGPT-4V:\u6b63\u786e\u7b54\u6848\u662f (B) \ud835\udc5b\ud835\udc5b(\ud835\udc5f\ud835\udc5f2 \u2212 \ud835\udc5f\ud835\udc5f1)\u3002\n    \u5149\u7a0b\u5dee\u662f\u6307\u4e24\u675f\u5149\u5728\u76f8\u540c\u65f6\u95f4\u5185\u8d70\u8fc7\u7684\u8def\u5f84\u957f\u5ea6\u7684\u5dee\u3002\u5f53\u5149\u5728\u4e0d\u540c\u4ecb\u8d28\u4e2d\u4f20\u64ad\u65f6\uff0c\u5176\u901f\u5ea6\u4f1a\u6539\u53d8\uff0c\n\u56e0\u6b64\u5149\u7a0b\u5dee\u4e5f\u4f1a\u53d7\u5230\u4ecb\u8d28\u6298\u5c04\u7387\u7684\u5f71\u54cd\u3002\u5728\u8fd9\u4e2a\u95ee\u9898\u4e2d\uff0c\u4e24\u675f\u5149\u901a\u8fc7\u7684\u5a92\u8d28\u6298\u5c04\u7387\u4e3a\ud835\udc5b\ud835\udc5b\uff0c\u56e0\u6b64\u5149\u7a0b\u5dee\n\u4e0d\u4ec5\u53d6\u51b3\u4e8e\u4e24\u675f\u5149\u5728\u7a7a\u6c14\u4e2d\u7684\u8def\u5f84\u957f\u5ea6\u5dee\uff08\ud835\udc5f\ud835\udc5f2 \u2212 \ud835\udc5f\ud835\udc5f1\uff09\uff0c\u8fd8\u8981\u4e58\u4ee5\u5a92\u8d28\u7684\u6298\u5c04\u7387\ud835\udc5b\ud835\udc5b\u3002\u6240\u4ee5\uff0cP\u70b9\u5904\u4e8c\u76f8\n\u5e72\u5149\u7ebf\u7684\u5149\u7a0b\u5dee\u4e3a\ud835\udc5b\ud835\udc5b(\ud835\udc5f\ud835\udc5f2 \u2212 \ud835\udc5f\ud835\udc5f1)\u3002\nGround Truth: (B) \n<\u56fe\u7247 1>\nFigure B146: A sample correct case of Science (subcategory: Physics).\nBack to List of figures\n165\nPREPRINT\n\u79d1\u5b66: \u7269\u7406\nQuestion: <\u56fe\u7247 1>\u4e00\u8d28\u91cf\u4e3a\ud835\udc40\ud835\udc40\u7684\u5747\u5300\u7ec6\u6746\uff0c\u53ef\u7ed5\u5149\u6ed1\u6c34\u5e73\u8f74\u8f6c\u52a8\uff0c\u4e00\u8d28\u91cf\u4e3a\ud835\udc5a\ud835\udc5a\u7684\u5c0f\u7403\u4ee5\u901f\u5ea6\ud835\udc49\ud835\udc49\u6c34\u5e73\u98de\n\u6765\uff0c\u4e0e\u6746\u7aef\u505a\u5b8c\u5168\u975e\u5f39\u6027\u78b0\u649e\uff0c\u5219\u5c0f\u7403\u4e0e\u6746\u7ec4\u6210\u7684\u7cfb\u7edf(\u56fe1),\u6ee1\u8db3\uff08\uff09\nOption: \n(A)\u52a8\u91cf\u5b88\u6052\uff0c\u89d2\u52a8\u91cf\u5b88\u6052\uff1b\n(B) \u52a8\u91cf\u4e0d\u5b88\u6052\uff0c\u89d2\u52a8\u91cf\u5b88\u6052\uff1b\n(C) \u52a8\u91cf\u4e0d\u5b88\u6052\uff0c\u89d2\u52a8\u91cf\u4e0d\u5b88\u6052\uff1b\n(D) \u52a8\u91cf\u5b88\u6052\uff0c\u89d2\u52a8\u91cf\u4e0d\u5b88\u6052\u3002\nGPT-4V:\u5728\u8fd9\u4e2a\u95ee\u9898\u4e2d\uff0c\u6211\u4eec\u9700\u8981\u8003\u8651\u52a8\u91cf\u5b88\u6052\u548c\u89d2\u52a8\u91cf\u5b88\u6052\u7684\u6761\u4ef6\u3002\n    \u52a8\u91cf\u5b88\u6052\u5b9a\u5f8b\u6307\u51fa\uff0c\u5982\u679c\u6ca1\u6709\u5916\u529b\u4f5c\u7528\uff0c\u7cfb\u7edf\u7684\u603b\u52a8\u91cf\u4fdd\u6301\u4e0d\u53d8\u3002\u5728\u8fd9\u4e2a\u95ee\u9898\u4e2d\uff0c\u5c0f\u7403\u4e0e\u6746\u7684\u78b0\n\u649e\u662f\u4e00\u4e2a\u5185\u529b\u4f5c\u7528\uff0c\u4f46\u7531\u4e8e\u6746\u56fa\u5b9a\u5728\u4e00\u4e2a\u8f74\u4e0a\uff0c\u8fd9\u4e2a\u8f74\u5bf9\u6746\u7684\u4f5c\u7528\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5916\u529b\uff0c\u56e0\u6b64\u7cfb\u7edf\u7684\u7ebf\n\u52a8\u91cf\u4e0d\u5b88\u6052\u3002\u6240\u4ee5\u52a8\u91cf\u4e0d\u5b88\u6052\u3002\n    \u89d2\u52a8\u91cf\u5b88\u6052\u5b9a\u5f8b\u6307\u51fa\uff0c\u5982\u679c\u6ca1\u6709\u5916\u529b\u77e9\u4f5c\u7528\uff0c\u7cfb\u7edf\u7684\u603b\u89d2\u52a8\u91cf\u4fdd\u6301\u4e0d\u53d8\u3002\u5728\u8fd9\u4e2a\u95ee\u9898\u4e2d\uff0c\u7531\u4e8e\u6746\n\u7ed5\u4e00\u4e2a\u56fa\u5b9a\u8f74\u65cb\u8f6c\uff0c\u4e14\u8f74\u5bf9\u6746\u7684\u4f5c\u7528\u529b\u901a\u8fc7\u8f74\u5fc3\uff0c\u4e0d\u4ea7\u751f\u529b\u77e9\uff0c\u56e0\u6b64\u7cfb\u7edf\u7684\u89d2\u52a8\u91cf\u5b88\u6052\u3002\u56e0\u6b64\uff0c\u6b63\u786e\n\u7b54\u6848\u662f (B) \u52a8\u91cf\u4e0d\u5b88\u6052\uff0c\u89d2\u52a8\u91cf\u5b88\u6052\u3002\nGround Truth: (B) \n<\u56fe\u7247 1>\nFigure B147: A sample correct case of Science (subcategory: Physics).\nBack to List of figures\n166\nPREPRINT\n\u79d1\u5b66: \u7269\u7406\nQuestion:\u5224\u65ad\u4e0b\u9762\u9648\u8ff0\u5bf9\u9519\uff1a\u5982\u53f3\u56fe\u6240\u793a<\u56fe\u7247 1>\uff0c\u4e24\u4e2a\u534a\u5f84\u5206\u522b\u4e3aR1\u548c\ud835\udc45\ud835\udc452, \ud835\udc45\ud835\udc451 < \ud835\udc45\ud835\udc452 \u7684\u5747\u5e26\u7535\u7403\n\u9762\u540c\u5fc3\u653e\u7f6e\uff0c\u5176\u95f4\u5145\u6ee1\u4ecb\u7535\u5e38\u6570\u4e3a\ud835\udf00\ud835\udf00 \u7684\u5404\u5411\u540c\u6027\u5747\u7535\u4ecb\u8d28\uff0c\u5df2\u77e5\u4e24\u4e2a\u7403\u9762\u7684\u5e26\u7535\u91cf\u5747\u4e3a \ud835\udc44\ud835\udc44\u3002\u90a3\u4e48\u5185\u7403\n\u9762\u4ee5\u5185\u7684\u7a7a\u95f4\u4e2d\u7684\u7535\u573a\u80fd\u91cf\u4e3a0\u3002\nGPT-4V:\u6839\u636e\u7535\u78c1\u5b66\u7684\u57fa\u672c\u539f\u7406\uff0c\u5f53\u4e24\u4e2a\u5e26\u7535\u7403\u9762\u540c\u5fc3\u653e\u7f6e\u5e76\u4e14\u5e26\u6709\u76f8\u540c\u7684\u7535\u8377\u91cf \ud835\udc44\ud835\udc44 \u65f6\uff0c\u5185\u7403\u9762\u4ee5\n\u5185\u7684\u7a7a\u95f4\u5b9e\u9645\u4e0a\u662f\u4e00\u4e2a\u7535\u573a\u4e3a\u96f6\u7684\u533a\u57df\u3002\u8fd9\u662f\u56e0\u4e3a\u5185\u7403\u9762\u4e0a\u7684\u7535\u8377\u4f1a\u5728\u5176\u8868\u9762\u4ea7\u751f\u4e00\u4e2a\u5411\u5916\u7684\u7535\u573a\uff0c\n\u800c\u5916\u7403\u9762\u4e0a\u7684\u7535\u8377\u4f1a\u5728\u5185\u7403\u9762\u4ee5\u5185\u7684\u533a\u57df\u4ea7\u751f\u4e00\u4e2a\u7b49\u5927\u5c0f\u4f46\u65b9\u5411\u76f8\u53cd\u7684\u7535\u573a\uff0c\u4e24\u8005\u5728\u5185\u7403\u9762\u4ee5\u5185\u7684\u533a\n\u57df\u76f8\u4e92\u62b5\u6d88\uff0c\u5bfc\u81f4\u8be5\u533a\u57df\u7684\u7535\u573a\u5f3a\u5ea6\u4e3a\u96f6\u3002\n    \u7531\u4e8e\u7535\u573a\u80fd\u91cf\u5bc6\u5ea6\u4e0e\u7535\u573a\u5f3a\u5ea6\u7684\u5e73\u65b9\u6210\u6b63\u6bd4\uff08\u7535\u573a\u80fd\u91cf\u5bc6\u5ea6 \ud835\udc62\ud835\udc62 =  1/2 \ud835\udf00\ud835\udf00 \ud835\udc38\ud835\udc382\uff0c\u5176\u4e2d \ud835\udf00\ud835\udf00 \u662f\u4ecb\u7535\u5e38\u6570\uff0c\n\ud835\udc38\ud835\udc38 \u662f\u7535\u573a\u5f3a\u5ea6\uff09\uff0c\u5982\u679c\u7535\u573a\u5f3a\u5ea6\u4e3a\u96f6\uff0c\u90a3\u4e48\u8be5\u533a\u57df\u7684\u7535\u573a\u80fd\u91cf\u4e5f\u5c06\u4e3a\u96f6\u3002\n    \u56e0\u6b64\uff0c\u6839\u636e\u9898\u76ee\u63cf\u8ff0\u548c\u7535\u78c1\u5b66\u7684\u57fa\u672c\u539f\u7406\uff0c\u9648\u8ff0\u201c\u5185\u7403\u9762\u4ee5\u5185\u7684\u7a7a\u95f4\u4e2d\u7684\u7535\u573a\u80fd\u91cf\u4e3a0\u201d\u662f\u6b63\u786e\n\u7684\u3002\u6b63\u786e\u7b54\u6848\u662f\uff1a\u6b63\u786e\u3002\nGround Truth: \u6b63\u786e \n<\u56fe\u7247 1>\nFigure B148: A sample correct case of Science (subcategory: Physics).\nBack to List of figures\n167\nPREPRINT\n\u79d1\u5b66: \u7269\u7406\nQuestion:\u6ce2\u957f\u4e3a\ud835\udf06\ud835\udf06\u7684\u5355\u8272\u5149\u5782\u76f4\u7167\u5c04<\u56fe\u7247 1>\u6240\u793a\u7684\u900f\u660e\u8584\u819c\u3002\u819c\u539a\u5ea6\u4e3a\ud835\udc52\ud835\udc52,\u4e24\u675f\u53cd\u5c04\u5149\u7684\u5149\u7a0b\u5dee\ud835\udeff\ud835\udeff =_\nGPT-4V:\u5728\u8fd9\u4e2a\u95ee\u9898\u4e2d\uff0c\u6211\u4eec\u9700\u8981\u8ba1\u7b97\u4e24\u675f\u53cd\u5c04\u5149\u4e4b\u95f4\u7684\u5149\u7a0b\u5dee\u3002\u5149\u7a0b\u5dee\u662f\u7531\u4e8e\u5149\u5728\u4e0d\u540c\u4ecb\u8d28\u4e2d\u4f20\u64ad\n\u65f6\u901f\u5ea6\u4e0d\u540c\u800c\u4ea7\u751f\u7684\u3002\u5728\u8fd9\u4e2a\u60c5\u51b5\u4e0b\uff0c\u5149\u5728\u900f\u660e\u8584\u819c\u4e2d\u6765\u56de\u4f20\u64ad\u4e00\u6b21\uff0c\u56e0\u6b64\u5149\u7a0b\u5dee\ud835\udeff\ud835\udeff\u53ef\u4ee5\u7531\u4e0b\u9762\u7684\u516c\n\u5f0f\u8ba1\u7b97\u5f97\u51fa\uff1a\ud835\udeff\ud835\udeff =  2\ud835\udc5b\ud835\udc5b\ud835\udc52\ud835\udc52  \u2212  \ud835\udf06\ud835\udf06/2\n    \u8fd9\u91cc\u7684\ud835\udf06\ud835\udf06/2\u662f\u56e0\u4e3a\u4ece\ud835\udc5b\ud835\udc5b1\u5230\ud835\udc5b\ud835\udc5b2\u7684\u754c\u9762\u53cd\u5c04\u7684\u5149\u4f1a\u4ea7\u751f\u534a\u4e2a\u6ce2\u957f\u7684\u76f8\u4f4d\u53d8\u5316\uff08\u5373\ud835\udf0b\ud835\udf0b\u7684\u76f8\u4f4d\u5dee\uff09\uff0c\u800c\u4ece\n\ud835\udc5b\ud835\udc5b2\u5230\ud835\udc5b\ud835\udc5b3\u7684\u754c\u9762\u53cd\u5c04\u7684\u5149\u4e0d\u4f1a\u4ea7\u751f\u8fd9\u6837\u7684\u76f8\u4f4d\u53d8\u5316\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u4ece\u603b\u7684\u5149\u7a0b\u5dee\u4e2d\u51cf\u53bb\u534a\u4e2a\u6ce2\u957f\u6765\u8003\n\u8651\u8fd9\u4e2a\u76f8\u4f4d\u53d8\u5316\u3002\n    \u6240\u4ee5\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a\ud835\udeff\ud835\udeff =  2\ud835\udc5b\ud835\udc5b2\ud835\udc52\ud835\udc52 \u2212  \ud835\udf06\ud835\udf06/2\n    \u5176\u4e2d\uff0c\ud835\udc5b\ud835\udc5b2\u662f\u8584\u819c\u7684\u6298\u5c04\u7387\uff0c\ud835\udc52\ud835\udc52\u662f\u8584\u819c\u7684\u539a\u5ea6\u3002\u7531\u4e8e\u9898\u76ee\u4e2d\u6ca1\u6709\u7ed9\u51fa\u5177\u4f53\u7684\ud835\udf06\ud835\udf06\u548c\ud835\udc52\ud835\udc52\u7684\u6570\u503c\uff0c\u6240\u4ee5\u6211\u4eec\n\u4e0d\u80fd\u7ed9\u51fa\u4e00\u4e2a\u5177\u4f53\u7684\u6570\u503c\u7b54\u6848\uff0c\u53ea\u80fd\u63d0\u4f9b\u4e0a\u8ff0\u8868\u8fbe\u5f0f\u3002\nGround Truth: 2.60 \n<\u56fe\u7247 1>\nError Category: Perceptual Error\nError Reason: The model did not fully understand the image, especially failing to recognize the \nnumerical information in the image. Therefore, it produced an incorrect answer.\nFigure B149: A sample error case of Science (subcategory: Physics).\nBack to List of figures\n168\nPREPRINT\n\u79d1\u5b66: \u7269\u7406\nQuestion: \u5728\ud835\udc5d\ud835\udc5d \u2212 \ud835\udc49\ud835\udc49\u56fe<\u56fe\u7247 1>\u4e0a\u6709\u4e24\u6761\u66f2\u7ebf\ud835\udc4e\ud835\udc4e\ud835\udc4f\ud835\udc4f\ud835\udc4e\ud835\udc4e\u548c\ud835\udc4e\ud835\udc4e\ud835\udc4e\ud835\udc4e\ud835\udc4e\ud835\udc4e,\u7531\u6b64\u53ef\u4ee5\u5f97\u51fa\u4ee5\u4eba\u4e0b\u7ed3\u8bba\uff08\uff09\nOption: \n(A)\u5176\u4e2d\u4e00\u6761\u662f\u7edd\u70ed\u7ebf,\u53e6\u4e00\u6761\u662f\u7b49\u6e29\u7ebf       \n(B) \u4e24\u4e2a\u8fc7\u7a0b\u5438\u6536\u7684\u70ed\u91cf\u76f8\u540c\n(C) \u4e24\u4e2a\u8fc7\u7a0b\u4e2d\u7cfb\u7edf\u5bf9\u5916\u4f5c\u7684\u529f\u76f8\u7b49         \n(D) \u4e24\u4e2a\u8fc7\u7a0b\u4e2d\u7cfb\u7edf\u7684\u5185\u80fd\u53d8\u5316\u76f8\u540c\nGPT-4V:\u5728\ud835\udc5d\ud835\udc5d \u2212 \ud835\udc49\ud835\udc49\u56fe\u4e0a\uff0c\u4e24\u6761\u66f2\u7ebf\ud835\udc4e\ud835\udc4e\ud835\udc4f\ud835\udc4f\ud835\udc4e\ud835\udc4e\u548c\ud835\udc4e\ud835\udc4e\ud835\udc4e\ud835\udc4e\ud835\udc4e\ud835\udc4e\u8868\u793a\u4e86\u4e24\u4e2a\u4e0d\u540c\u7684\u70ed\u529b\u5b66\u8fc7\u7a0b\u3002\u6839\u636e\u70ed\u529b\u5b66\u7684\u7b2c\u4e00\u5b9a\u5f8b\uff0c\n\u7cfb\u7edf\u7684\u5185\u80fd\u53d8\u5316\u7b49\u4e8e\u7cfb\u7edf\u5438\u6536\u7684\u70ed\u91cf\u4e0e\u7cfb\u7edf\u5bf9\u5916\u505a\u7684\u529f\u4e4b\u5dee\u3002\u5728\u8fd9\u4e2a\u95ee\u9898\u4e2d\uff0c\u6211\u4eec\u6ca1\u6709\u8db3\u591f\u7684\u4fe1\u606f\u6765\n\u786e\u5b9a\u8fd9\u4e24\u4e2a\u8fc7\u7a0b\u7684\u5177\u4f53\u7c7b\u578b\uff08\u5982\u7edd\u70ed\u3001\u7b49\u6e29\u3001\u7b49\u538b\u7b49\uff09\uff0c\u56e0\u6b64\u6211\u4eec\u65e0\u6cd5\u76f4\u63a5\u786e\u5b9a\u9009\u9879(A)\u662f\u5426\u6b63\u786e\u3002    \n    \u5bf9\u4e8e\u9009\u9879(B)\uff0c\u7531\u4e8e\u6211\u4eec\u4e0d\u77e5\u9053\u8fd9\u4e24\u4e2a\u8fc7\u7a0b\u7684\u5177\u4f53\u7ec6\u8282\uff0c\u6211\u4eec\u65e0\u6cd5\u786e\u5b9a\u4e24\u4e2a\u8fc7\u7a0b\u5438\u6536\u7684\u70ed\u91cf\u662f\u5426\n\u76f8\u540c\u3002\n    \u5bf9\u4e8e\u9009\u9879(C)\uff0c\u5728\ud835\udc5d\ud835\udc5d \u2212 \ud835\udc49\ud835\udc49\u56fe\u4e0a\uff0c\u7cfb\u7edf\u5bf9\u5916\u505a\u7684\u529f\u53ef\u4ee5\u901a\u8fc7\u8ba1\u7b97\u66f2\u7ebf\u4e0b\u7684\u9762\u79ef\u6765\u786e\u5b9a\u3002\u7531\u4e8e\u4e24\u6761\u66f2\u7ebf\n\ud835\udc4e\ud835\udc4e\ud835\udc4f\ud835\udc4f\ud835\udc4e\ud835\udc4e\u548c\ud835\udc4e\ud835\udc4e\ud835\udc4e\ud835\udc4e\ud835\udc4e\ud835\udc4e\u5728\u56fe\u4e2d\u7684\u9762\u79ef\u770b\u8d77\u6765\u4e0d\u540c\uff0c\u6211\u4eec\u53ef\u4ee5\u63a8\u65ad\u4e24\u4e2a\u8fc7\u7a0b\u4e2d\u7cfb\u7edf\u5bf9\u5916\u505a\u7684\u529f\u4e0d\u76f8\u7b49\u3002\n    \u5bf9\u4e8e\u9009\u9879(D)\uff0c\u7cfb\u7edf\u7684\u5185\u80fd\u53d8\u5316\u53d6\u51b3\u4e8e\u5438\u6536\u7684\u70ed\u91cf\u548c\u5bf9\u5916\u505a\u7684\u529f\u3002\u7531\u4e8e\u6211\u4eec\u65e0\u6cd5\u786e\u5b9a\u5438\u6536\u7684\u70ed\u91cf\uff0c\n\u6211\u4eec\u4e5f\u65e0\u6cd5\u786e\u5b9a\u5185\u80fd\u53d8\u5316\u662f\u5426\u76f8\u540c\u3002\n    \u56e0\u6b64\uff0c\u6839\u636e\u56fe\u4e2d\u7684\u4fe1\u606f\u548c\u70ed\u529b\u5b66\u539f\u7406\uff0c\u6700\u53ef\u80fd\u6b63\u786e\u7684\u9009\u9879\u662f(C) \u4e24\u4e2a\u8fc7\u7a0b\u4e2d\u7cfb\u7edf\u5bf9\u5916\u4f5c\u7684\u529f\u4e0d\u76f8\n\u7b49\u3002\nGround Truth: (D)\u4e24\u4e2a\u8fc7\u7a0b\u4e2d\u7cfb\u7edf\u7684\u5185\u80fd\u53d8\u5316\u76f8\u540c \n<\u56fe\u7247 1>\nError Category: Lack of Knowledge\nError Reason:The conclusion in the options can be deduced and demonstrated from the physical \nprocesses expressed in the image, but the model did not answer using specific knowledge.\nFigure B150: A sample error case of Science (subcategory: Physics).\nBack to List of figures\n169\nPREPRINT\n\u79d1\u5b66:\u7269\u7406\nQuestion: <\u56fe\u7247 1>\u5728\ud835\udc65\ud835\udc65\ud835\udc36\ud835\udc36\ud835\udc66\ud835\udc66\u9762\u4e0a\u5012\u6263\u7740\u534a\u5f84\u4e3a\ud835\udc45\ud835\udc45\u7684\u534a\u7403\u9762\u4e0a\u7535\u8377\u5747\u5300\u5206\u5e03,\u9762\u7535\u8377\u5bc6\u5ea6\u4e3a\ud835\udf0e\ud835\udf0e\uff0cA\u70b9\u7684\u5750\u6807\n\u4e3a(0, \ud835\udc45\ud835\udc45/2),B \u70b9\u7684\u5750\u6807\u4e3a(3\ud835\udc45\ud835\udc45/2, 0),AB\u4e4b\u95f4\u7535\u52bf\u4e3a_____\nOption: \n(A)\n\ud835\udc45\ud835\udc45\u00d7\ud835\udf0e\ud835\udf0e\n2\u00d7\ud835\udf00\ud835\udf00 \n(B)\n\ud835\udc45\ud835\udc45\u00d7\ud835\udf0e\ud835\udf0e\n3\u00d7\ud835\udf00\ud835\udf00 \n(C)\n\ud835\udc45\ud835\udc45\u00d7\ud835\udf0e\ud835\udf0e\n4\u00d7\ud835\udf00\ud835\udf00 \n(D)\n\ud835\udc45\ud835\udc45\u00d7\ud835\udf0e\ud835\udf0e\n6\u00d7\ud835\udf00\ud835\udf00\nGPT-4V: \u5f88\u62b1\u6b49\uff0c\u4f46\u6211\u65e0\u6cd5\u76f4\u63a5\u63d0\u4f9b\u8fd9\u4e2a\u7269\u7406\u95ee\u9898\u7684\u7b54\u6848\uff0c\u56e0\u4e3a\u5b83\u9700\u8981\u7279\u5b9a\u7684\u7269\u7406\u77e5\u8bc6\u548c\u8ba1\u7b97\u6765\u89e3\u51b3\uff0c\n\u800c\u8fd9\u4e9b\u4fe1\u606f\u6ca1\u6709\u5728\u95ee\u9898\u4e2d\u7ed9\u51fa\u3002\u901a\u5e38\uff0c\u8ba1\u7b97\u4e24\u70b9\u95f4\u7684\u7535\u52bf\u5dee\u9700\u8981\u5e94\u7528\u7535\u573a\u548c\u7535\u52bf\u7684\u7269\u7406\u5b9a\u5f8b\uff0c\u5982\u9ad8\u65af\n\u5b9a\u5f8b\u6216\u5e93\u4ed1\u5b9a\u5f8b\uff0c\u5e76\u4e14\u53ef\u80fd\u9700\u8981\u79ef\u5206\u6765\u5904\u7406\u8fde\u7eed\u7684\u7535\u8377\u5206\u5e03\u3002\n    \u7136\u800c\uff0c\u5982\u679c\u4f60\u80fd\u63d0\u4f9b\u76f8\u5173\u7684\u7269\u7406\u516c\u5f0f\u6216\u8005\u66f4\u591a\u7684\u80cc\u666f\u4fe1\u606f\uff0c\u6211\u53ef\u4ee5\u5e2e\u52a9\u4f60\u7406\u89e3\u8fd9\u4e9b\u6982\u5ff5\uff0c\u6216\u8005\u6307\n\u5bfc\u4f60\u5982\u4f55\u4f7f\u7528\u8fd9\u4e9b\u516c\u5f0f\u6765\u89e3\u51b3\u95ee\u9898\u3002\nGround Truth: (D)\n\ud835\udc79\ud835\udc79\u00d7\ud835\udf48\ud835\udf48\n\ud835\udfd3\ud835\udfd3\u00d7\ud835\udf3a\ud835\udf3a \n<\u56fe\u7247 1>\nError Category: Lack of Knowledge, Answer Extraction Error\nError Reason: The model did not apply relevant knowledge in answering, it is obvious that GPT-4V \nlacked related physical knowledge in this round of answer.\nFigure B151: A sample error case of Science (subcategory: Physics).\nBack to List of figures\n170\nPREPRINT\nQuestion: \u5982\u56fe<\u56fe\u7247 1>\uff0c\u4e3a\u52a0\u5f3a\u4e2d\u592e\u5bf9\u5730\u65b9\u7684\u7ba1\u7406\uff0c\u5b83\u63a8\u884c_____\u5236\u5ea6\u3002\nGPT-4V: \u6839\u636e\u60a8\u7684\u95ee\u9898\u63cf\u8ff0\uff0c\u5982\u679c\u662f\u5173\u4e8e\u4e2d\u56fd\u5386\u53f2\u4e0a\u52a0\u5f3a\u4e2d\u592e\u5bf9\u5730\u65b9\u7684\u7ba1\u7406\u7684\u5236\u5ea6\uff0c\u4e00\u4e2a\u53ef\u80fd\u7684\u7b54\n\u6848\u662f\u201c\u90e1\u53bf\u5236\u201d\u5236\u5ea6\u3002\u8fd9\u662f\u4e2d\u56fd\u53e4\u4ee3\u4e00\u4e2a\u91cd\u8981\u7684\u884c\u653f\u7ba1\u7406\u5236\u5ea6\uff0c\u59cb\u4e8e\u79e6\u671d\uff0c\u7528\u4ee5\u52a0\u5f3a\u4e2d\u592e\u96c6\u6743\u3002\nGround Truth: \u90e1\u53bf\n<\u56fe\u7247 1>\n\u4eba\u6587\u793e\u4f1a\u79d1\u5b66: \u5386\u53f2\nFigure B152: A sample correct case of Humanities and Social Sciences (subcategory: History).\nBack to List of figures\n171\nPREPRINT\nQuestion: \u4e3a\u4e0b\u8868<\u56fe\u7247 1>\u9009\u53d6\u8868\u540d\uff0c\u6700\u6070\u5f53\u7684\u662f()\noption: \n(A)\u4eac\u6d25\u5180\u5730\u533a\u665a\u6e05\u6c11\u65cf\u4f01\u4e1a\u7b80\u8868\n(B)\u5317\u4eac\u8fd1\u4ee3\u6c11\u65cf\u4f01\u4e1a\u7b80\u8868\n(C)\u6d0b\u52a1\u8fd0\u52a8\u65f6\u671f\u5317\u65b9\u4f01\u4e1a\u7b80\u8868\n(D)\u8fd1\u4ee3\u6c11\u65cf\u8d44\u672c\u4e3b\u4e49\u4f01\u4e1a\u7b80\u8868\nGPT-4V:\u6839\u636e\u56fe\u7247\u4e2d\u63d0\u4f9b\u7684\u4fe1\u606f\uff0c\u8868\u683c\u5217\u51fa\u4e86\u4e0d\u540c\u5e74\u4efd\u6210\u7acb\u7684\u4f01\u4e1a\u4ee5\u53ca\u5b83\u4eec\u7684\u540d\u79f0\u548c\u5730\u70b9\u3002\u6240\u6709\u5217\u51fa\n\u7684\u4f01\u4e1a\u90fd\u662f\u5728\u665a\u6e05\u65f6\u671f\u6210\u7acb\u7684\uff0c\u5e76\u4e14\u5730\u70b9\u5305\u62ec\u4e86\u4e2d\u56fd\u5317\u65b9\u7684\u57ce\u5e02\uff0c\u5982\u5317\u4eac\u548c\u5929\u6d25\u3002\u56e0\u6b64\uff0c\u6700\u5408\u9002\u7684\u8868\n\u540d\u5e94\u8be5\u662f\u4e0e\u665a\u6e05\u65f6\u671f\u7684\u5317\u65b9\u5730\u533a\u548c\u6c11\u65cf\u4f01\u4e1a\u76f8\u5173\u7684\u3002\n\u9009\u9879(A) \"\u4eac\u6d25\u5180\u5730\u533a\u665a\u6e05\u6c11\u65cf\u4f01\u4e1a\u7b80\u8868\"\u63d0\u5230\u4e86\u4eac\u6d25\u5180\u5730\u533a\uff0c\u8fd9\u662f\u5305\u62ec\u5317\u4eac\u548c\u5929\u6d25\u5728\u5185\u7684\u5730\u533a\uff0c\u4e0e\u8868\n\u4e2d\u7684\u4fe1\u606f\u76f8\u7b26\u5408\u3002\n\u9009\u9879(B) \"\u5317\u4eac\u8fd1\u4ee3\u6c11\u65cf\u4f01\u4e1a\u7b80\u8868\"\u53ea\u63d0\u5230\u4e86\u5317\u4eac\uff0c\u800c\u8868\u4e2d\u5305\u542b\u4e86\u5929\u6d25\u7684\u4f01\u4e1a\uff0c\u56e0\u6b64\u4e0d\u591f\u5168\u9762\u3002\n\u9009\u9879(C) \"\u6d0b\u52a1\u8fd0\u52a8\u65f6\u671f\u5317\u65b9\u4f01\u4e1a\u7b80\u8868\"\u867d\u7136\u63d0\u5230\u4e86\u5317\u65b9\u4f01\u4e1a\uff0c\u4f46\u6ca1\u6709\u7279\u6307\u6c11\u65cf\u4f01\u4e1a\uff0c\u4e14\u6d0b\u52a1\u8fd0\u52a8\u7684\u65f6\n\u95f4\u8303\u56f4\u6bd4\u665a\u6e05\u66f4\u72ed\u7a84\u3002\n\u9009\u9879(D) \"\u8fd1\u4ee3\u6c11\u65cf\u8d44\u672c\u4e3b\u4e49\u4f01\u4e1a\u7b80\u8868\"\u867d\u7136\u63d0\u5230\u4e86\u8fd1\u4ee3\u6c11\u65cf\u8d44\u672c\u4e3b\u4e49\u4f01\u4e1a\uff0c\u4f46\u6ca1\u6709\u6307\u660e\u5730\u533a\uff0c\u4e14\u4e0d\u7279\n\u5b9a\u4e8e\u665a\u6e05\u65f6\u671f\u3002\n\u7efc\u4e0a\u6240\u8ff0\uff0c\u6700\u5408\u9002\u7684\u9009\u9879\u662f(A) \"\u4eac\u6d25\u5180\u5730\u533a\u665a\u6e05\u6c11\u65cf\u4f01\u4e1a\u7b80\u8868\"\u3002\nGround Truth: (A) \u4eac\u6d25\u5180\u5730\u533a\u665a\u6e05\u6c11\u65cf\u4f01\u4e1a\u7b80\u8868\n<\u56fe\u7247 1>\n<\u56fe\u7247 1>\n\u4eba\u6587\u793e\u4f1a\u79d1\u5b66: \u5386\u53f2\nFigure B153: A sample correct case of Humanities and Social Sciences (subcategory: History).\nBack to List of figures\n172\nPREPRINT\nQuestion: \u8bfb\u5982\u56fe<\u56fe\u7247 1>\uff0c\u4e0b\u5217\u8868\u8ff0\u6b63\u786e\u7684\u662f \u2460\u4e1c\u5357\u5730\u533a\u901a\u5546\u53e3\u5cb8\u8d38\u6613\u7f51\u7edc\u6700\u4e3a\u5bc6\u96c6\u2461\u201c\u4e94\u53e3\u901a\u5546\u201d\n\u4e2d\u7684\u9999\u6e2f\u6210\u4e3a\u8d38\u6613\u4e2d\u5fc3\u2462\u5929\u6d25\u7684\u53e3\u5cb8\u8d38\u6613\u5728\u5317\u65b9\u5c45\u4e8e\u6838\u5fc3\u5730\u4f4d\u2463\u6700\u65e9\u5f00\u57e0\u7684\u6c49\u53e3\u662f\u5357\u5317\u53e3\u5cb8\u8d38\u6613\u67a2\u7ebd\noption: \n(A)\u2460\u2461\n(B)\u2462\u2463\n(C)\u2460\u2462\n(D)\u2461\u2463\nGPT-4V:\u6839\u636e\u63d0\u4f9b\u7684\u56fe\u7247\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u4e2d\u56fd\u4e1c\u5357\u5730\u533a\u7684\u8d38\u6613\u7f51\u7edc\u56fe\u3002\u56fe\u4e2d\u663e\u793a\u4e86\u4e0d\u540c\u53e3\u5cb8\u4e4b\u95f4\u7684\u8d38\n\u6613\u8054\u7cfb\uff0c\u4ee5\u53ca\u5b83\u4eec\u7684\u76f8\u5bf9\u4f4d\u7f6e\u548c\u8fde\u63a5\u60c5\u51b5\u3002\n\u2460 \u56fe\u4e2d\u4e1c\u5357\u5730\u533a\u7684\u8d38\u6613\u7f51\u7edc\u786e\u5b9e\u770b\u8d77\u6765\u6bd4\u8f83\u5bc6\u96c6\uff0c\u6709\u591a\u4e2a\u8fde\u63a5\u70b9\u548c\u7ebf\u8def\u3002\n\u2461 \u56fe\u4e2d\u5e76\u6ca1\u6709\u660e\u786e\u6307\u51fa\u201c\u4e94\u53e3\u901a\u5546\u201d\u4e2d\u7684\u9999\u6e2f\uff0c\u56e0\u6b64\u65e0\u6cd5\u786e\u5b9a\u9999\u6e2f\u662f\u5426\u6210\u4e3a\u8d38\u6613\u4e2d\u5fc3\u3002\n\u2462 \u5929\u6d25\u4f4d\u4e8e\u56fe\u4e2d\u5317\u65b9\uff0c\u6709\u591a\u6761\u7ebf\u8def\u8fde\u63a5\uff0c\u770b\u8d77\u6765\u786e\u5b9e\u5728\u5317\u65b9\u53e3\u5cb8\u8d38\u6613\u4e2d\u5360\u636e\u6838\u5fc3\u5730\u4f4d\u3002\n\u2463 \u56fe\u4e2d\u5e76\u6ca1\u6709\u63d0\u4f9b\u8db3\u591f\u7684\u4fe1\u606f\u6765\u5224\u65ad\u6c49\u53e3\u662f\u5426\u662f\u6700\u65e9\u5f00\u57e0\u7684\uff0c\u4e5f\u65e0\u6cd5\u786e\u5b9a\u5b83\u662f\u5426\u662f\u5357\u5317\u53e3\u5cb8\u8d38\u6613\u7684\n\u67a2\u7ebd\u3002\n\u6839\u636e\u4ee5\u4e0a\u5206\u6790\uff0c\u6700\u53ef\u80fd\u6b63\u786e\u7684\u9009\u9879\u662f\uff1a\n(C) \u2460\u2462\nn\u6b63\u786e\u7b54\u6848\u662f (C) \u2460\u2462\u3002\nGround Truth: (C) \u2460\u2462\n<\u56fe\u7247 1>\n<\u56fe\u7247 1>\n\u4eba\u6587\u793e\u4f1a\u79d1\u5b66: \u5386\u53f2\nFigure B154: A sample correct case of Humanities and Social Sciences (subcategory: History).\nBack to List of figures\n173\nPREPRINT\nQuestion: \u7ed3\u5408\u6240\u5b66\uff0c\u5206\u6790<\u56fe\u7247 1>\uff0c\u5bf9\u6b64\u89e3\u8bfb\u6b63\u786e\u7684\u662f\uff08 \uff09\nOption: \n(A) \u91cd\u77ff\u4e1a\u548c\u4ea4\u901a\u4e1a\u7684\u53d1\u5c55\u59cb\u7ec8\u5904\u4e8e\u9886\u5148\u5730\u4f4d\n(B) \u4ee5\u8f7b\u5de5\u4e1a\u4e3a\u4e3b\u7684\u6c11\u65cf\u5de5\u4e1a\u53d1\u5c55\u76f8\u5bf9\u8f83\u5feb\n(C) \u4f20\u7edf\u7ecf\u6d4e\u5f62\u5f0f\u5df2\u7136\u6de1\u51fa\u5386\u53f2\u821e\u53f0\n(D) \u72ec\u7acb\u5b8c\u6574\u7684\u5de5\u4e1a\u4f53\u7cfb\u5df2\u7ecf\u5f62\u6210\nGPT-4V: \u6839\u636e\u63d0\u4f9b\u7684\u56fe\u7247\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u4e00\u4e2a\u8868\u683c\uff0c\u5176\u4e2d\u5217\u51fa\u4e861895-1911\u5e74\u95f4\u4e0d\u540c\u65f6\u95f4\u6bb5\u7684\u5de5\u4e1a\u4ea7\n\u503c\uff08\u4e07\u5143\uff09\u3002\u8868\u683c\u5206\u4e3a\u8f7b\u5de5\u4e1a\u3001\u91cd\u5de5\u4e1a\u548c\u4ea4\u901a\u4e1a\u4e09\u4e2a\u90e8\u5206\u3002\u901a\u8fc7\u5206\u6790\u8fd9\u4e9b\u6570\u636e\uff0c\u6211\u4eec\u53ef\u4ee5\u5c1d\u8bd5\u786e\u5b9a\u6b63\n\u786e\u7684\u9009\u9879\u3002\n(A)\u91cd\u77ff\u4e1a\u548c\u4ea4\u901a\u4e1a\u7684\u53d1\u5c55\u59cb\u7ec8\u5904\u4e8e\u9886\u5148\u5730\u4f4d - \u4ece\u8868\u683c\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0c\u91cd\u5de5\u4e1a\u548c\u4ea4\u901a\u4e1a\u7684\u4ea7\u503c\u5728\u5404\u4e2a\u65f6\n\u95f4\u6bb5\u90fd\u662f\u6700\u9ad8\u7684\uff0c\u56e0\u6b64\u8fd9\u4e2a\u9009\u9879\u662f\u6b63\u786e\u7684\u3002\n(B)\u4ee5\u8f7b\u5de5\u4e1a\u4e3a\u4e3b\u7684\u6c11\u65cf\u5de5\u4e1a\u53d1\u5c55\u76f8\u5bf9\u8f83\u5feb - \u8868\u683c\u4e2d\u7684\u6570\u636e\u663e\u793a\u8f7b\u5de5\u4e1a\u7684\u4ea7\u503c\u589e\u957f\u4e5f\u5f88\u5feb\uff0c\u4f46\u662f\u6ca1\u6709\n\u5177\u4f53\u7684\u6bd4\u8f83\u6570\u636e\u6765\u8bc1\u660e\u5b83\u7684\u589e\u957f\u901f\u5ea6\u76f8\u5bf9\u4e8e\u5176\u4ed6\u884c\u4e1a\u662f\u66f4\u5feb\u7684\u3002\u56e0\u6b64\uff0c\u8fd9\u4e2a\u9009\u9879\u6ca1\u6709\u8db3\u591f\u7684\u4fe1\u606f\n\u6765\u8bc1\u660e\u5176\u6b63\u786e\u6027\u3002\n(C)\u4f20\u7edf\u7ecf\u6d4e\u5f62\u5f0f\u5df2\u7136\u6de1\u51fa\u5386\u53f2\u821e\u53f0 - \u8868\u683c\u4e2d\u6ca1\u6709\u63d0\u4f9b\u5173\u4e8e\u4f20\u7edf\u7ecf\u6d4e\u5f62\u5f0f\u7684\u4fe1\u606f\uff0c\u56e0\u6b64\u65e0\u6cd5\u5224\u65ad\u8fd9\u4e2a\n\u9009\u9879\u7684\u6b63\u786e\u6027\u3002\n(D)\u72ec\u7acb\u5b8c\u6574\u7684\u5de5\u4e1a\u4f53\u7cfb\u5df2\u7ecf\u5f62\u6210 - \u8868\u683c\u4e2d\u7684\u6570\u636e\u663e\u793a\u5de5\u4e1a\u4ea7\u503c\u5728\u589e\u957f\uff0c\u4f46\u662f\u6ca1\u6709\u8db3\u591f\u7684\u4fe1\u606f\u6765\u8bc1\u660e\n\u662f\u5426\u5df2\u7ecf\u5f62\u6210\u4e86\u4e00\u4e2a\u72ec\u7acb\u5b8c\u6574\u7684\u5de5\u4e1a\u4f53\u7cfb\u3002\n\u7efc\u4e0a\u6240\u8ff0\uff0c\u6839\u636e\u8868\u683c\u4e2d\u7684\u6570\u636e\uff0c\u6700\u53ef\u80fd\u6b63\u786e\u7684\u9009\u9879\u662f (A) \u91cd\u77ff\u4e1a\u548c\u4ea4\u901a\u4e1a\u7684\u53d1\u5c55\u59cb\u7ec8\u5904\u4e8e\u9886\u5148\u5730\u4f4d\u3002\n\u5176\u4ed6\u9009\u9879\u8981\u4e48\u6ca1\u6709\u8db3\u591f\u7684\u4fe1\u606f\u6765\u8bc1\u660e\u5176\u6b63\u786e\u6027\uff0c\u8981\u4e48\u4e0e\u8868\u683c\u4e2d\u7684\u6570\u636e\u4e0d\u5b8c\u5168\u76f8\u5173\u3002\nGround Truth: (B) \u4ee5\u8f7b\u5de5\u4e1a\u4e3a\u4e3b\u7684\u6c11\u65cf\u5de5\u4e1a\u53d1\u5c55\u76f8\u5bf9\u8f83\u5feb\nError Category: Perceptual Error\nError Reason: GPT-4V model has a problem understanding of the picture, it does not understand \nthe speed of development of light industry.\n<\u56fe\u7247 1>\n\u4eba\u6587\u793e\u4f1a\u79d1\u5b66: \u5386\u53f2\nFigure B155: A sample error case of Humanities and Social Sciences (subcategory: History).\nBack to List of figures\n174\nPREPRINT\nQuestion: <\u56fe\u7247 1>\u56fe\u793a\u94dc\u955c\u6d41\u884c\u7684\u65f6\u671f\u4e3a_____\u65f6\u671f\nOption: \n(A) \u6b66\u662d\n(B) \u5ba3\u5e73\n(C) \u65b0\u83bd\n(D) \u9b4f\u664b\nGPT-4V: \u8fd9\u4e9b\u94dc\u955c\u7684\u56fe\u6848\u98ce\u683c\u548c\u5236\u4f5c\u5de5\u827a\u53ef\u80fd\u6697\u793a\u4e86\u5b83\u4eec\u6d41\u884c\u7684\u65f6\u671f\u3002\u7136\u800c\uff0c\u7531\u4e8e\u6211\u65e0\u6cd5\u76f4\u63a5\u8bc6\u522b\n\u8fd9\u4e9b\u94dc\u955c\u7684\u786e\u5207\u65f6\u671f\uff0c\u6211\u5c06\u6839\u636e\u4e2d\u56fd\u5386\u53f2\u4e0a\u94dc\u955c\u7684\u53d1\u5c55\u6765\u63a8\u6d4b\u3002\n(A) \u6b66\u662d - \u8fd9\u4e0d\u662f\u4e00\u4e2a\u5e7f\u4e3a\u4eba\u77e5\u7684\u5386\u53f2\u65f6\u671f\u540d\u79f0\uff0c\u53ef\u80fd\u662f\u4e00\u4e2a\u7279\u5b9a\u671d\u4ee3\u7684\u5e74\u53f7\u3002\n(B) \u5ba3\u5e73 - \u8fd9\u540c\u6837\u53ef\u80fd\u662f\u4e00\u4e2a\u5e74\u53f7\uff0c\u800c\u4e0d\u662f\u4e00\u4e2a\u5e7f\u4e3a\u4eba\u77e5\u7684\u5386\u53f2\u65f6\u671f\u3002\n(C) \u65b0\u83bd - \u65b0\u83bd\u662f\u6307\u738b\u83bd\u7be1\u593a\u897f\u6c49\u653f\u6743\u540e\u5efa\u7acb\u7684\u653f\u6743\uff0c\u65f6\u95f4\u5927\u7ea6\u662f\u516c\u51439\u5e74\u5230\u516c\u514323\u5e74\u3002\n(D) \u9b4f\u664b - \u8fd9\u6307\u7684\u662f\u9b4f\u664b\u5357\u5317\u671d\u65f6\u671f\uff0c\u5927\u7ea6\u662f\u516c\u5143220\u5e74\u5230\u516c\u5143589\u5e74\u3002\n\u6839\u636e\u4e2d\u56fd\u5386\u53f2\u4e0a\u94dc\u955c\u7684\u53d1\u5c55\uff0c\u9b4f\u664b\u5357\u5317\u671d\u65f6\u671f\u662f\u94dc\u955c\u975e\u5e38\u6d41\u884c\u7684\u65f6\u671f\uff0c\u7279\u522b\u662f\u5177\u6709\u7cbe\u7f8e\u56fe\u6848\u548c\u590d\u6742\u8bbe\n\u8ba1\u7684\u94dc\u955c\u3002\u56e0\u6b64\uff0c\u5982\u679c\u5fc5\u987b\u9009\u62e9\u6700\u53ef\u80fd\u7684\u9009\u9879\uff0c\u6211\u4f1a\u9009\u62e9\uff1a\n\u6b63\u786e\u7b54\u6848\uff1a(D) \u9b4f\u664b\nGround Truth: (B) \u5ba3\u5e73\nError Category: Reasoning Error\nError Reason: The knowledge and requirements of the problem were correctly understood, but \nerrors were made during reasoning. Only through the bronze mirror popular era, rather than \nthrough the style of the bronze mirror to infer the era.\n<\u56fe\u7247 1>\n\u4eba\u6587\u793e\u4f1a\u79d1\u5b66: \u5386\u53f2\nFigure B156: A sample error case of Humanities and Social Sciences (subcategory: History).\nBack to List of figures\n175\nPREPRINT\n\u4eba\u6587\u793e\u4f1a\u79d1\u5b66: \u6587\u732e\u5b66\nQuestion:  <\u56fe\u7247 1>\u8be5\u7687\u5e1d\u5728\u4f4d\u65f6\u547d\u4ee4\u4fee\u64b0\u7684\u201c\u4e16\u754c\u6709\u53f2\u4ee5\u6765\u6700\u5927\u7684\u767e\u79d1\u5168\u4e66\u201d\u540d\u4e3a\u300a____\u300b\nGPT-4V: \n\u8fd9\u4f4d\u7687\u5e1d\u662f\u660e\u671d\u7684\u6210\u7956\u6731\u68e3\uff0c\u4ed6\u5728\u4f4d\u65f6\u547d\u4ee4\u4fee\u64b0\u7684\u201c\u4e16\u754c\u6709\u53f2\u4ee5\u6765\u6700\u5927\u7684\u767e\u79d1\u5168\u4e66\u201d\u540d\u4e3a\n\u300a\u6c38\u4e50\u5927\u5178\u300b\u3002\nGround Truth: \u6c38\u4e50\u5927\u5178\n<\u56fe\u7247 1>\nFigure B157: A sample correct case of Humanities and Social Sciences (subcategory: Litera-\nture).\nBack to List of figures\n176\nPREPRINT\n\u4eba\u6587\u793e\u4f1a\u79d1\u5b66: \u6587\u732e\u5b66\nQuestion:  <\u56fe\u7247 1>\u56fe\u793a\u7684\u56fe\u4e66\u88c5\u5e27\u65b9\u5f0f\u662f____\nOption: \n(A) \u8774\u8776\u88c5\n(B) \u5377\u8f74\u88c5\n(C) \u7ecf\u6298\u88c5\n(D) \u5305\u80cc\u88c5\nGPT-4V: \u8fd9\u5f20\u56fe\u7247\u5c55\u793a\u7684\u662f\u4e00\u79cd\u53e4\u4ee3\u4e66\u7c4d\u7684\u88c5\u5e27\u65b9\u5f0f\u3002\u6839\u636e\u56fe\u793a\u7684\u7ed3\u6784\u7279\u70b9\uff0c\u8fd9\u79cd\u88c5\u5e27\u65b9\u5f0f\u6709\u5c01\u9762\u3001\n\u4e66\u810a\u548c\u6298\u9875\uff0c\u770b\u8d77\u6765\u50cf\u662f\u4e66\u9875\u88ab\u6298\u53e0\u8d77\u6765\u5e76\u4e14\u88c5\u8ba2\u5728\u4e00\u8d77\u3002\u8fd9\u79cd\u88c5\u5e27\u65b9\u5f0f\u4e0e\u4f20\u7edf\u7684\u8774\u8776\u88c5\u76f8\u4f3c\uff0c\u56e0\u4e3a\n\u8774\u8776\u88c5\u662f\u5c06\u5355\u5f20\u7eb8\u5bf9\u6298\uff0c\u4f7f\u5f97\u5370\u5237\u9762\u5728\u5185\uff0c\u6298\u75d5\u5728\u5916\uff0c\u7136\u540e\u5c06\u5bf9\u6298\u7684\u7eb8\u9875\u6346\u624e\u6210\u518c\uff0c\u5f62\u6210\u4e66\u7c4d\u3002\n\u6b63\u786e\u7b54\u6848\u662f\uff1a\n(A) \u8774\u8776\u88c5\nGround Truth: (A) \u8774\u8776\u88c5\n<\u56fe\u7247 1>\nFigure B158: A sample correct case of Humanities and Social Sciences (subcategory: Litera-\nture).\nBack to List of figures\n177\nPREPRINT\n\u4eba\u6587\u793e\u4f1a\u79d1\u5b66: \u6587\u732e\u5b66\nQuestion:  <\u56fe\u7247 1>\u6240\u793a\u7684\u6587\u732e\u57fa\u672c\u68c0\u7d22\u8f93\u5165\u65b9\u5f0f\u662f\u5e8f\u53f7\u68c0\u7d22\nOption: \n(A) \u520a\u540d\u68c0\u7d22\n(B) \u5206\u7c7b\u68c0\u7d22\n(C) \u5e8f\u53f7\u68c0\u7d22\n(D) \u9ad8\u7ea7\u68c0\u7d22\nGPT-4V: \n \n\u6839\u636e\u56fe\u7247\u4e2d\u663e\u793a\u7684\u754c\u9762\uff0c\u8fd9\u662f\u4e00\u4e2a\u9ad8\u7ea7\u68c0\u7d22\u7684\u754c\u9762\uff0c\u53ef\u4ee5\u770b\u5230\u6709\u591a\u4e2a\u68c0\u7d22\u6761\u4ef6\u7ec4\u5408\u7684\u9009\n\u9879\uff0c\u5982\u201c\u4e3b\u9898\u201d\u3001\u201c\u7bc7\u540d\u201d\u3001\u201c\u4f5c\u8005\u201d\u7b49\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7\u903b\u8f91\u8fd0\u7b97\u7b26\uff08\u5982AND\uff09\u6765\u7ec4\u5408\u8fd9\u4e9b\u6761\u4ef6\u3002\u56fe\n\u7247\u4e2d\u6ca1\u6709\u76f4\u63a5\u663e\u793a\u201c\u5e8f\u53f7\u68c0\u7d22\u201d\u7684\u9009\u9879\uff0c\u56e0\u6b64\u6839\u636e\u56fe\u7247\u63d0\u4f9b\u7684\u4fe1\u606f\uff0c\u6700\u53ef\u80fd\u6b63\u786e\u7684\u9009\u9879\u662f\uff1a\n(D) \u9ad8\u7ea7\u68c0\u7d22\"\nGround Truth: (D) \u9ad8\u7ea7\u68c0\u7d22\n<\u56fe\u7247 1>\nFigure B159: A sample correct case of Humanities and Social Sciences (subcategory: Litera-\nture).\nBack to List of figures\n178\nPREPRINT\n\u4eba\u6587\u793e\u4f1a\u79d1\u5b66: \u6587\u732e\u5b66\nQuestion:  <\u56fe\u7247 1>\u6240\u793a\u7684\u6587\u732e\u57fa\u672c\u68c0\u7d22\u8f93\u5165\u65b9\u5f0f\u662f__\nOption: \n(A) \u520a\u540d\u68c0\u7d22\n(B) \u5206\u7c7b\u68c0\u7d22\n(C) \u5355\u4f4d\u68c0\u7d22\n(D) \u9ad8\u7ea7\u68c0\u7d22\nGPT-4V: \u6839\u636e\u56fe\u7247\u4e2d\u7684\u4fe1\u606f\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u641c\u7d22\u6846\u4e2d\u8f93\u5165\u4e86\u201c\u4e2d\u56fd\u519c\u4e1a\u5927\u5b66TP309\u201d\uff0c\u5e76\u4e14\u641c\u7d22\u7ed3\u679c\n\u663e\u793a\u4e86\u4e0e\u8fd9\u4e2a\u5173\u952e\u8bcd\u76f8\u5173\u7684\u6587\u732e\u5217\u8868\u3002\u8fd9\u8868\u660e\u8fdb\u884c\u7684\u662f\u7279\u5b9a\u5173\u952e\u8bcd\u7684\u68c0\u7d22\uff0c\u4f46\u56fe\u7247\u4e2d\u6ca1\u6709\u76f4\u63a5\u663e\u793a\u8fd9\n\u4e9b\u5173\u952e\u8bcd\u662f\u7528\u4e8e\u520a\u540d\u3001\u5206\u7c7b\u3001\u5355\u4f4d\u8fd8\u662f\u5176\u4ed6\u9ad8\u7ea7\u68c0\u7d22\u6761\u4ef6\u3002\n\u7136\u800c\uff0c\u7531\u4e8e\u201c\u4e2d\u56fd\u519c\u4e1a\u5927\u5b66\u201d\u662f\u4e00\u4e2a\u5355\u4f4d\u540d\u79f0\uff0c\u800c\u201cTP309\u201d\u53ef\u80fd\u662f\u4e00\u4e2a\u5206\u7c7b\u53f7\uff0c\u8fd9\u6697\u793a\u4e86\u4f7f\u7528\u4e86\u5355\n\u4f4d\u540d\u79f0\u548c\u5206\u7c7b\u53f7\u8fdb\u884c\u68c0\u7d22\u3002\u5728\u6ca1\u6709\u66f4\u591a\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u53ef\u4ee5\u5047\u8bbe\u8fd9\u662f\u4e00\u4e2a\u7ed3\u5408\u4e86\u5355\u4f4d\u548c\u5206\u7c7b\u7684\u68c0\n\u7d22\u65b9\u5f0f\uff0c\u4f46\u8fd9\u79cd\u7ec4\u5408\u5728\u9009\u9879\u4e2d\u5e76\u672a\u76f4\u63a5\u7ed9\u51fa\u3002\n\u56e0\u6b64\uff0c\u6839\u636e\u56fe\u7247\u4e2d\u7684\u4fe1\u606f\uff0c\u6700\u53ef\u80fd\u6b63\u786e\u7684\u9009\u9879\u662f\uff1a\n(C) \u5355\u4f4d\u68c0\u7d22\n\u8fd9\u662f\u56e0\u4e3a\u201c\u4e2d\u56fd\u519c\u4e1a\u5927\u5b66\u201d\u662f\u4e00\u4e2a\u5355\u4f4d\u540d\u79f0\uff0c\u800c\u641c\u7d22\u7ed3\u679c\u4e0e\u8fd9\u4e2a\u5355\u4f4d\u76f8\u5173\u3002\u5c3d\u7ba1\u201cTP309\u201d\u770b\u8d77\u6765\u50cf\n\u4e00\u4e2a\u5206\u7c7b\u53f7\uff0c\u4f46\u5728\u63d0\u4f9b\u7684\u9009\u9879\u4e2d\u6ca1\u6709\u660e\u786e\u7684\u201c\u5206\u7c7b\u68c0\u7d22\u201d\u9009\u9879\uff0c\u6240\u4ee5\u6211\u4eec\u53ea\u80fd\u9009\u62e9\u6700\u7b26\u5408\u56fe\u7247\u4fe1\u606f\u7684\n\u9009\u9879\u3002\nGround Truth: (D) \u9ad8\u7ea7\u68c0\u7d22\n<\u56fe\u7247 1>\nError Category:  Perceptual Error\nError Reason: \n1. In the picture, it is \"\u4e2d\u56fe\u5206\u7c7b\u53f7TP309\" instead of \"\u4e2d\u56fd\u519c\u4e1a\u5927\u5b66TP309\";\n2. The search method in pictures allows users to accurately find documents by combining multiple \nsearch conditions, so it is an advanced search.\nFigure B160: A sample error case of Humanities and Social Sciences (subcategory: Litera-\nture).\nBack to List of figures\n179\nPREPRINT\n\u4eba\u6587\u793e\u4f1a\u79d1\u5b66: \u6587\u732e\u5b66\nQuestion:  <\u56fe\u7247 1>\u6240\u793a\u7684\u4fe1\u606f\u68c0\u7d22\u4e2d\u7684__\nOption: \n(A) \u6b63\u5411\u7d22\u5f15\n(B) \u5012\u6392\u7d22\u5f15\n(C) \u4f5c\u8005\u68c0\u7d22\n(D) \u520a\u540d\u68c0\u7d22\nGPT-4V:  \u6b63\u786e\u7b54\u6848\u662f\uff1a(B) \u5012\u6392\u7d22\u5f15\n\u8fd9\u5f20\u56fe\u7247\u5c55\u793a\u7684\u662f\u4e00\u4e2a\u5012\u6392\u7d22\u5f15\u7684\u7ed3\u6784\u3002\u5728\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u4e2d\uff0c\u5012\u6392\u7d22\u5f15\u662f\u4e00\u79cd\u7d22\u5f15\u65b9\u6cd5\uff0c\u5b83\u5c06\u6587\u6863\u7684\n\u5185\u5bb9\u6620\u5c04\u5230\u5305\u542b\u8fd9\u4e9b\u5185\u5bb9\u7684\u6587\u6863ID\u3002\u8fd9\u79cd\u7ed3\u6784\u4f7f\u5f97\u641c\u7d22\u5f15\u64ce\u80fd\u591f\u5feb\u901f\u627e\u5230\u5305\u542b\u7279\u5b9a\u8bcd\u6c47\u7684\u6240\u6709\u6587\u6863\u3002\n\u56fe\u4e2d\u663e\u793a\u4e86\u4e0d\u540c\u7684\u6587\u6863ID\u5bf9\u5e94\u5230\u5305\u542b\u7684\u8bcd\u6c47\uff0c\u8fd9\u662f\u5012\u6392\u7d22\u5f15\u7684\u5178\u578b\u8868\u793a\u65b9\u5f0f\u3002\nGround Truth: (A) \u6b63\u5411\u7d22\u5f15\n<\u56fe\u7247 1>\nError Category: Perceptual Error\nError Reason: \n1. Inverted Index: Maps keywords to a list of documents containing the keywords;\n2. Forward index: Maps documents to the keywords they contain.\nThe index method in the picture is the mapping of document ID to word, so it is a forward index.\nFigure B161: A sample error case of Humanities and Social Sciences (subcategory: Litera-\nture).\nBack to List of figures\n180\nPREPRINT\n\u4eba\u6587\u793e\u4f1a\u79d1\u5b66: \u5fc3\u7406\u5b66\nQuestion: <\u56fe\u7247 1>\u6240\u793a\u7684\u5fc3\u7406\u5b66\u6548\u5e94\u662f__\noption: \n(A)\u534a\u9014\u6548\u5e94\n(B)\u8fb9\u9645\u6548\u76ca\u9012\u51cf\u6548\u5e94\n(C)\u6bd4\u9a6c\u9f99\u6548\u5e94\n(D)\u5b89\u6cf0\u6548\u5e94\nGPT-4V: \u8fd9\u5f20\u56fe\u7247\u5c55\u793a\u4e86\u968f\u7740\u6d88\u8d39\u6570\u91cf\u7684\u589e\u52a0\uff0c\u5bf9\u6bcf\u4e00\u5355\u4f4d\u989d\u5916\u6d88\u8d39\u7684\u6ee1\u8db3\u5ea6\u6216\u5e78\u798f\u611f\u9010\u6e10\u964d\u4f4e\u7684\u73b0\n\u8c61\u3002\u8fd9\u79cd\u6548\u5e94\u5728\u5fc3\u7406\u5b66\u548c\u7ecf\u6d4e\u5b66\u4e2d\u88ab\u79f0\u4e3a\u8fb9\u9645\u6548\u7528\u9012\u51cf\u3002\u56e0\u6b64\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a\n(B) \u8fb9\u9645\u6548\u76ca\u9012\u51cf\u6548\u5e94\nGround Truth: (B) \u8fb9\u9645\u6548\u76ca\u9012\u51cf\u6548\u5e94\n<\u56fe\u7247 1>\nFigure B162: A sample correct case of Humanities and Social Sciences (subcategory: Psy-\nchology).\nBack to List of figures\n181\nPREPRINT\nQuestion: <\u56fe\u7247 1>\u6240\u793a\u7684\u5fc3\u7406\u5b66\u6548\u5e94\u662f\u662f__\noption: \n(A)\u5bcc\u5170\u514b\u6797\u6548\u5e94\n(B)\u9cb6\u9c7c\u6548\u5e94\n(C)\u5357\u98ce\u6548\u5e94\n(D)\u635f\u5931\u538c\u6076\u6548\u5e94\nGPT-4V: \u6b63\u786e\u7b54\u6848\u662f\uff1a(D) \u635f\u5931\u538c\u6076\u6548\u5e94\n\u8fd9\u5f20\u56fe\u7247\u63cf\u7ed8\u4e86\u4e00\u4e2a\u5929\u5e73\uff0c\u4e00\u8fb9\u6709\u6807\u8bb0\u4e3a\u201cLOSS\u201d\u7684\u8f83\u91cd\u7684\u90e8\u5206\uff0c\u53e6\u4e00\u8fb9\u6709\u6807\u8bb0\u4e3a\u201cGAIN\u201d\u7684\u8f83\u8f7b\u7684\n\u90e8\u5206\u3002\u8fd9\u4ee3\u8868\u4e86\u635f\u5931\u538c\u6076\u6548\u5e94\uff0c\u5373\u4eba\u4eec\u5bf9\u635f\u5931\u7684\u611f\u53d7\u6bd4\u5bf9\u540c\u7b49\u5927\u5c0f\u7684\u6536\u76ca\u7684\u611f\u53d7\u8981\u5f3a\u70c8\u5f97\u591a\u3002\nGround Truth: (D) \u635f\u5931\u538c\u6076\u6548\u5e94\n<\u56fe\u7247 1>\n\u4eba\u6587\u793e\u4f1a\u79d1\u5b66: \u5fc3\u7406\u5b66\nFigure B163: A sample correct case of Humanities and Social Sciences (subcategory: Psy-\nchology).\nBack to List of figures\n182\nPREPRINT\nQuestion: <\u56fe\u7247 1>\u6240\u793a\u7684\u6d88\u8d39\u8005\u5fc3\u7406\u662f__\noption:\n(A)\u4fa5\u5e78\u5fc3\u7406\n(B)\u5360\u4fbf\u5b9c\u5fc3\u7406\n(C)\u4ece\u4f17\u5fc3\u7406\n(D)\u6500\u6bd4\u5fc3\u7406\nGPT-4V: \u8fd9\u5f20\u56fe\u7247\u5c55\u793a\u4e86\u4e24\u4f4d\u5973\u58eb\uff0c\u4e00\u4f4d\u770b\u8d77\u6765\u5fc3\u60c5\u6cae\u4e27\uff0c\u5fc3\u5f62\u6c14\u6ce1\u4e2d\u5199\u7740\u201c\u865a\u8363\u5fc3\u201d\uff0c\u800c\u53e6\u4e00\u4f4d\u770b\n\u8d77\u6765\u5f88\u9ad8\u5174\uff0c\u624b\u91cc\u62ff\u7740\u8d2d\u7269\u888b\u3002\u6839\u636e\u8fd9\u4e9b\u4fe1\u606f\uff0c\u6211\u4eec\u53ef\u4ee5\u63a8\u65ad\u51fa\u8fd9\u5f20\u56fe\u7247\u53ef\u80fd\u5728\u8868\u8fbe\u5173\u4e8e\u6d88\u8d39\u8005\u5fc3\u7406\n\u7684\u67d0\u79cd\u5bf9\u6bd4\u3002\n\u9009\u9879\u4e2d\uff1a\n(A)\u4fa5\u5e78\u5fc3\u7406\u901a\u5e38\u6307\u7684\u662f\u4eba\u4eec\u5e0c\u671b\u80fd\u591f\u5e78\u8fd0\u5730\u83b7\u5f97\u597d\u5904\u6216\u907f\u514d\u574f\u4e8b\u7684\u5fc3\u7406\uff0c\u8fd9\u4e0e\u56fe\u7247\u5185\u5bb9\u4e0d\u7b26\u3002\n(B)\u5360\u4fbf\u5b9c\u5fc3\u7406\u662f\u6307\u4eba\u4eec\u559c\u6b22\u83b7\u53d6\u6bd4\u6b63\u5e38\u66f4\u6709\u5229\u7684\u4ea4\u6613\u6216\u4f18\u60e0\uff0c\u8fd9\u5728\u56fe\u7247\u4e2d\u6ca1\u6709\u660e\u663e\u7684\u8ff9\u8c61\u3002\n(C)\u4ece\u4f17\u5fc3\u7406\u662f\u6307\u4eba\u4eec\u503e\u5411\u4e8e\u8ddf\u968f\u5927\u4f17\u6216\u670b\u53cb\u7684\u9009\u62e9\u548c\u884c\u4e3a\uff0c\u56fe\u7247\u4e2d\u6ca1\u6709\u663e\u793a\u8fd9\u79cd\u5fc3\u7406\u3002\n(D)\u6500\u6bd4\u5fc3\u7406\u662f\u6307\u4eba\u4eec\u5728\u793e\u4f1a\u5730\u4f4d\u3001\u8d22\u5bcc\u3001\u6d88\u8d39\u7b49\u65b9\u9762\u4e0e\u4ed6\u4eba\u6bd4\u8f83\uff0c\u5e0c\u671b\u4e0d\u843d\u540e\u4e8e\u4eba\u751a\u81f3\u8d85\u8d8a\u4ed6\u4eba\u3002\n\u56fe\u7247\u4e2d\u7684\u201c\u865a\u8363\u5fc3\u201d\u6c14\u6ce1\u548c\u4e24\u4f4d\u5973\u58eb\u7684\u4e0d\u540c\u8868\u60c5\u53ef\u80fd\u6697\u793a\u4e86\u8fd9\u79cd\u5fc3\u7406\u3002\n\u56e0\u6b64\uff0c\u6700\u53ef\u80fd\u6b63\u786e\u7684\u9009\u9879\u662f\uff1a\n(D) \u6500\u6bd4\u5fc3\u7406\nGround Truth: (D) \u6500\u6bd4\u5fc3\u7406\n<\u56fe\u7247 1>\n<\u56fe\u7247 1>\n\u4eba\u6587\u793e\u4f1a\u79d1\u5b66: \u5fc3\u7406\u5b66\nFigure B164: A sample correct case of Humanities and Social Sciences (subcategory: Psy-\nchology).\nBack to List of figures\n183\nPREPRINT\nQuestion: \u5224\u65ad\u4e0b\u9762\u9648\u8ff0\u5bf9\u9519\uff1a<\u56fe\u7247 1>\u6240\u793a\u4eba\u7269\u662f\u5409\u5e03\u68ee\nGPT-4V: \u62b1\u6b49\uff0c\u6211\u65e0\u6cd5\u786e\u8ba4\u56fe\u7247\u4e2d\u7684\u4eba\u7269\u662f\u5426\u662f\u5409\u5e03\u68ee\uff0c\u56e0\u4e3a\u6211\u4e0d\u80fd\u63d0\u4f9b\u771f\u5b9e\u4eba\u7269\u7684\u8eab\u4efd\u4fe1\u606f\u3002\u5982\n\u679c\u60a8\u6709\u5176\u4ed6\u95ee\u9898\u6216\u9700\u8981\u5e2e\u52a9\uff0c\u8bf7\u544a\u8bc9\u6211\u3002\nGround Truth: \u9519\nError Category: Reject to Answer\nError Reason: The model declined to answer questions about personal information.\n<\u56fe\u7247 1>\n\u4eba\u6587\u793e\u4f1a\u79d1\u5b66: \u5fc3\u7406\u5b66\nFigure B165: A sample error case of Humanities and Social Sciences (subcategory: Psychol-\nogy).\nBack to List of figures\n184\nPREPRINT\nQuestion: <\u56fe\u7247 1>\u6240\u793a\u7684\u5fc3\u7406\u5b66\u6548\u5e94\u662f__\noption: \n(A)\u534a\u9014\u6548\u5e94\n(B)\u8d23\u4efb\u5206\u6563\u6548\u5e94\n(C)\u65c1\u89c2\u8005\u6548\u5e94\n(D)\u5b89\u6cf0\u6548\u5e94\nGPT-4V: \u8fd9\u5f20\u56fe\u7247\u63cf\u7ed8\u4e86\u4e24\u4e2a\u4eba\u5728\u6316\u96a7\u9053\uff0c\u4f46\u4ed6\u4eec\u4e4b\u95f4\u53ea\u6709\u4e00\u8584\u5c42\u571f\u58e4\uff0c\u800c\u5176\u4e2d\u4e00\u4e2a\u4eba\u5df2\u7ecf\u653e\u5f03\uff0c\u5982\n\u679c\u4ed6\u518d\u575a\u6301\u4e00\u4f1a\u513f\u5c31\u80fd\u5230\u8fbe\u53e6\u4e00\u8fb9\u3002\u8fd9\u4e2a\u56fe\u901a\u5e38\u7528\u6765\u8bf4\u660e\u575a\u6301\u548c\u52aa\u529b\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u53ca\u5728\u63a5\u8fd1\u6210\u529f\u65f6\u4e0d\n\u8981\u653e\u5f03\u7684\u9053\u7406\u3002\u7136\u800c\uff0c\u8fd9\u5e76\u4e0d\u76f4\u63a5\u5bf9\u5e94\u4e8e\u6240\u7ed9\u9009\u9879\u4e2d\u7684\u4efb\u4f55\u5fc3\u7406\u5b66\u6548\u5e94\u3002\u6839\u636e\u56fe\u7247\u5185\u5bb9\uff0c\u6ca1\u6709\u8db3\u591f\u7684\n\u4fe1\u606f\u6765\u786e\u5b9a\u8fd9\u4e0e\u6240\u7ed9\u9009\u9879\u4e2d\u7684\u54ea\u4e2a\u5fc3\u7406\u5b66\u6548\u5e94\u6700\u76f8\u5173\u3002\u56e0\u6b64\uff0c\u6211\u65e0\u6cd5\u57fa\u4e8e\u8fd9\u5f20\u56fe\u7247\u9009\u62e9\u4e00\u4e2a\u6b63\u786e\u7684\u7b54\n\u6848\u3002\u5982\u679c\u8fd9\u5f20\u56fe\u7247\u662f\u7528\u6765\u8c61\u5f81\u6027\u5730\u8868\u793a\u67d0\u79cd\u5fc3\u7406\u5b66\u6548\u5e94\uff0c\u90a3\u4e48\u8fd9\u79cd\u8054\u7cfb\u53ef\u80fd\u662f\u9690\u55bb\u6027\u7684\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\n\u7684\u3002\u5982\u679c\u4f60\u80fd\u63d0\u4f9b\u66f4\u591a\u7684\u4e0a\u4e0b\u6587\u6216\u89e3\u91ca\u56fe\u7247\u662f\u5982\u4f55\u4e0e\u8fd9\u4e9b\u6548\u5e94\u76f8\u5173\u8054\u7684\uff0c\u6211\u53ef\u80fd\u80fd\u591f\u63d0\u4f9b\u66f4\u591a\u5e2e\u52a9\u3002\nGround Truth: (A)\u534a\u9014\u6548\u5e94\nError Category: Reject to Answer\nError Reason: There is no obvious correlation between the image and the text, or the image does not \nhave an obvious opt-out option, the model refuses to answer.\n<\u56fe\u7247 1>\n\u4eba\u6587\u793e\u4f1a\u79d1\u5b66: \u5fc3\u7406\u5b66\nFigure B166: A sample error case of Humanities and Social Sciences (subcategory: Psychol-\nogy).\nBack to List of figures\n185\nPREPRINT\nQuestion: <\u56fe\u7247 1>\u8be5\u4eba\u8ba4\u4e3a\u8ba4\u4e3a\u6784\u6210\u610f\u8bc6\u7684\u5143\u7d20\u662f\uff08\uff09\",\n \noption: \n(A)\u611f\u89c9\u3001\u77e5\u89c9\u3001\u60c5\u611f\n(B)\u611f\u89c9\u3001\u77e5\u89c9\u3001\u8868\u8c61\n(C)\u611f\u89c9\u3001\u8868\u8c61\u3001\u60c5\u611f\n(D)\u611f\u89c9\u3001\u8bb0\u5fc6\u3001\u8868\u8c61\nGPT-4V:\u5f88\u62b1\u6b49\uff0c\u4f46\u6211\u65e0\u6cd5\u6839\u636e\u56fe\u7247\u63d0\u4f9b\u7684\u4fe1\u606f\u6765\u786e\u5b9a\u8fd9\u4e2a\u4eba\u7684\u89c2\u70b9\u662f\u4ec0\u4e48\u3002\u5982\u679c\u60a8\u80fd\u63d0\u4f9b\u66f4\u591a\u5173\u4e8e\n\u8fd9\u4e2a\u4eba\u7684\u80cc\u666f\u4fe1\u606f\u6216\u8005\u4ed6\u4eec\u7684\u7406\u8bba\uff0c\u6211\u53ef\u80fd\u80fd\u591f\u5e2e\u52a9\u60a8\u786e\u5b9a\u6b63\u786e\u7684\u7b54\u6848\u3002\nGround Truth: (C) \u611f\u89c9\u3001\u8868\u8c61\u3001\u60c5\u611f\nError Category: Perceptual Error\nError Reason: The model doesn't understand who the person in the picture is.\n<\u56fe\u7247 1>\n\u4eba\u6587\u793e\u4f1a\u79d1\u5b66: \u5fc3\u7406\u5b66\nFigure B167: A sample error case of Humanities and Social Sciences (subcategory: Psychol-\nogy).\nBack to List of figures\n186\nPREPRINT\nQuestion: <\u56fe\u7247 1>\u6210\u4eba\u4e0e\u513f\u7ae5\u5bf9\u4e00\u5e45\u56fe\u753b\u7684\u77e5\u89c9\u6709\u5f88\u5927\u7684\u5dee\u522b\uff0c\u5e74\u9f84\u8f83\u5c0f\u7684\u513f\u7ae5\u53ea\u80fd\u8bf4\u51fa\u56fe\u753b\u4e2d\u4e3b\n\u8981\u7684\u90e8\u5206\uff0c\u800c\u6210\u4eba\u80fd\u628a\u63e1\u6574\u4e2a\u56fe\u753b\u7684\u610f\u4e49\uff0c\u8fd9\u8bf4\u660e\u4e86\uff08\uff09\noption: \n(A)\u77e5\u89c9\u7684\u9009\u62e9\u6027\n(B)\u77e5\u89c9\u7684\u7406\u89e3\u6027\n(C)\u77e5\u89c9\u7684\u6052\u5e38\u6027\n(D)\u77e5\u89c9\u7684\u6574\u4f53\u6027\nGPT-4V: \u8fd9\u5f20\u56fe\u7247\u5c55\u793a\u4e86\u4e00\u4e2a\u6210\u4eba\u548c\u4e00\u4e2a\u513f\u7ae5\uff0c\u6210\u4eba\u4f3c\u4e4e\u5728\u5411\u513f\u7ae5\u5c55\u793a\u6216\u8bfb\u4e00\u672c\u4e66\u3002\u6839\u636e\u9898\u76ee\u63cf\u8ff0\uff0c\n\u6210\u4eba\u80fd\u591f\u7406\u89e3\u56fe\u753b\u7684\u6574\u4f53\u610f\u4e49\uff0c\u800c\u513f\u7ae5\u53ea\u80fd\u8bf4\u51fa\u56fe\u753b\u4e2d\u7684\u4e3b\u8981\u90e8\u5206\u3002\u8fd9\u8868\u660e\u6210\u4eba\u7684\u77e5\u89c9\u66f4\u52a0\u6574\u4f53\u548c\u5b8c\n\u6574\u3002\u56e0\u6b64\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a\n(D) \u77e5\u89c9\u7684\u6574\u4f53\u6027\nGround Truth: (B) \u77e5\u89c9\u7684\u7406\u89e3\u6027\nError Category: Reasoning Error\nError Reason: The model correctly understood the knowledge and requirements of the problem, \nbut it made mistakes in reasoning.\n<\u56fe\u7247 1>\n\u4eba\u6587\u793e\u4f1a\u79d1\u5b66: \u5fc3\u7406\u5b66\nFigure B168: A sample error case of Humanities and Social Sciences (subcategory: Psychol-\nogy).\nBack to List of figures\n187\nPREPRINT\n\u4eba\u6587\u793e\u4f1a\u79d1\u5b66: \u793e\u4f1a\u5b66\nQuestion: \u67d0\u80a1\u7968\u6536\u76ca\u5206\u5e03\u7684\u53ef\u80fd\u60c5\u51b5\u5982\u4e0b\u8868<\u56fe\u7247 1>\uff0c\u8bd5\u8ba1\u7b97\u8be5\u80a1\u7968\u7684\u5e74\u9884\u671f\u6536\u76ca\u7387( )\nOption: \n(A) 0.20 \n(B) 0.24 \n(C) 0.21 \n(D) 0.18\nGPT-4V: \n\u4e3a\u4e86\u8ba1\u7b97\u80a1\u7968\u7684\u5e74\u9884\u671f\u6536\u76ca\u7387\uff0c\u6211\u4eec\u9700\u8981\u5c06\u6bcf\u79cd\u53ef\u80fd\u60c5\u51b5\u7684\u6536\u76ca\u7387\u4e0e\u5176\u76f8\u5e94\u7684\u6982\u7387\u76f8\u4e58\uff0c\n\u7136\u540e\u5c06\u8fd9\u4e9b\u4e58\u79ef\u76f8\u52a0\u3002\u6839\u636e\u63d0\u4f9b\u7684\u8868\u683c\uff0c\u6211\u4eec\u53ef\u4ee5\u8fdb\u884c\u4ee5\u4e0b\u8ba1\u7b97\uff1a \u9884\u671f\u6536\u76ca\u7387 = (A\u7684\u6536\u76ca\u7387 * A\u7684\n\u6982\u7387) + (B\u7684\u6536\u76ca\u7387 * B\u7684\u6982\u7387) + (C\u7684\u6536\u76ca\u7387 * C\u7684\u6982\u7387) \u9884\u671f\u6536\u76ca\u7387 = (0.4 * 0.25) + (0.4 * \n0.20) + (0.2 * 0.15) \u9884\u671f\u6536\u76ca\u7387 = (0.1) + (0.08) + (0.03) \u9884\u671f\u6536\u76ca\u7387 = 0.21\n\u6b63\u786e\u7b54\u6848\uff1a(C) 0.21\u3002\nGround Truth: (C) 0.21 \n<\u56fe\u7247 1>\nFigure B169: A sample correct case of Humanities and Social Sciences (subcategory: Sociol-\nogy).\nBack to List of figures\n188\nPREPRINT\n\u4eba\u6587\u793e\u4f1a\u79d1\u5b66: \u793e\u4f1a\u5b66\nQuestion: <\u56fe\u7247 1>\u6240\u793a\u4eba\u7269\u7684\u7ecf\u5178\u6761\u4ef6\u53cd\u5c04\u5b66\u8bf4\u5185\u5bb9\uff1a\u4ed6\u5728\u7814\u7a76\u6d88\u5316\u73b0\u8c61\u65f6\uff0c\u89c2\u5bdf\u4e86\u72d7\u7684__\u5206\u6ccc\uff0c\n\u5373\u5bf9\u98df\u7269\u7684\u4e00\u79cd\u53cd\u5e94\u7279\u5f81\u3002\nGPT-4V: \u6b63\u786e\u7b54\u6848\uff1a\u553e\u6db2\nGround Truth: \u553e\u6db2\n<\u56fe\u7247 1>\nFigure B170: A sample correct case of Humanities and Social Sciences (subcategory: Sociol-\nogy).\nBack to List of figures\n189\nPREPRINT\n\u4eba\u6587\u793e\u4f1a\u79d1\u5b66: \u793e\u4f1a\u5b66\nQuestion: <\u56fe\u7247 1>\u56fe\u4e2d\u4eba\u7269\u9996\u6b21\u63d0\u51fa\u793e\u4f1a\u5b66\u8fd9\u4e00\u6982\u5ff5\u7684\u65f6\u95f4\u662f\uff08\uff09\noption: \n(A)1918\u5e74\n(B)1928\u5e74\n(C)1838\u5e74\n(D)1948\u5e74\nGPT-4V: \u6b63\u786e\u7b54\u6848\u662f\uff1a(C) 1838\u5e74\n\u793e\u4f1a\u5b66\u8fd9\u4e00\u6982\u5ff5\u6700\u65e9\u662f\u7531\u5965\u53e4\u65af\u7279\u00b7\u5b54\u5fb7\u57281838\u5e74\u63d0\u51fa\u7684\nGround Truth: (C)1838\u5e74\n<\u56fe\u7247 1>\nFigure B171: A sample correct case of Humanities and Social Sciences (subcategory: Sociol-\nogy).\nBack to List of figures\n190\nPREPRINT\n\u4eba\u6587\u793e\u4f1a\u79d1\u5b66: \u793e\u4f1a\u5b66\nQuestion\uff1a\u5982\u56fe<\u56fe\u7247 1>\u6240\u793a\u662f\u534e\u8054\u5546\u53a6\u67d0\u4e2a\u6708\u7532\u3001\u4e59\u3001\u4e19\u4e09\u79cd\u54c1\u724c\u5f69\u7535\u7684\u9500\u552e\u91cf\u7edf\u8ba1\u56fe\uff0c\u5219\u7532\u3001\u4e19\n\u4e24\u79cd\u54c1\u724c\u5f69\u7535\u8be5\u6708\u7684\u9500\u552e\u91cf\u4e4b\u548c\u4e3a\uff08\uff09\noption: \n(A)50\u53f0\n(B)65\u53f0\n(C)75\u53f0\n(D)95\u53f0\nGPT-4V: \u6839\u636e\u63d0\u4f9b\u7684\u7edf\u8ba1\u56fe\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u7532\u54c1\u724c\u5f69\u7535\u7684\u9500\u552e\u91cf\u5927\u7ea6\u662f40\u53f0\uff0c\u800c\u4e19\u54c1\u724c\u5f69\u7535\u7684\u9500\u552e\u91cf\n\u5927\u7ea6\u662f35\u53f0\u3002\u5c06\u8fd9\u4e24\u4e2a\u6570\u503c\u76f8\u52a0\uff0c\u6211\u4eec\u5f97\u5230\u7532\u3001\u4e19\u4e24\u79cd\u54c1\u724c\u5f69\u7535\u8be5\u6708\u7684\u9500\u552e\u91cf\u4e4b\u548c\u4e3a40\u53f0\u52a035\u53f0\uff0c\u7b49\n\u4e8e75\u53f0\u3002\n\u56e0\u6b64\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a\n(C) 75\u53f0\nGround Truth: (C) 75\u53f0\n<\u56fe\u7247 1>\nFigure B172: A sample correct case of Humanities and Social Sciences (subcategory: Sociol-\nogy).\nBack to List of figures\n191\nPREPRINT\nQuestion: \u5224\u65ad\u4e0b\u9762\u9648\u8ff0\u5bf9\u9519\uff1a<\u56fe\u7247 1>\u5b97\u6559\u662f\u4e00\u79cd\u5f88\u91cd\u8981\u7684\u793e\u4f1a\u63a7\u5236\u65b9\u5f0f\uff0c\u5e94\u5927\u529b\u63d0\u5021\u3002\nGPT-4V:\u8fd9\u4e2a\u95ee\u9898\u7684\u9648\u8ff0\u5305\u542b\u4e86\u4e00\u4e2a\u4e3b\u89c2\u7684\u89c2\u70b9\uff0c\u5373\u5b97\u6559\u662f\u4e00\u79cd\u5f88\u91cd\u8981\u7684\u793e\u4f1a\u63a7\u5236\u65b9\u5f0f\uff0c\u5e76\u4e14\u5e94\u8be5\u5927\n\u529b\u63d0\u5021\u3002\u5173\u4e8e\u5b97\u6559\u7684\u4f5c\u7528\u548c\u662f\u5426\u5e94\u8be5\u63d0\u5021\uff0c\u4e0d\u540c\u7684\u4eba\u548c\u793e\u4f1a\u6709\u4e0d\u540c\u7684\u770b\u6cd5\u3002\u6709\u4e9b\u4eba\u53ef\u80fd\u8ba4\u4e3a\u5b97\u6559\u5bf9\u4e8e\n\u7ef4\u6301\u793e\u4f1a\u79e9\u5e8f\u548c\u63d0\u4f9b\u9053\u5fb7\u6307\u5bfc\u662f\u91cd\u8981\u7684\uff0c\u800c\u53e6\u4e00\u4e9b\u4eba\u53ef\u80fd\u8ba4\u4e3a\u5b97\u6559\u4e0d\u5e94\u8be5\u7528\u4e8e\u793e\u4f1a\u63a7\u5236\u6216\u8005\u5e94\u8be5\u4fdd\u6301\n\u5b97\u6559\u4fe1\u4ef0\u7684\u4e2a\u4eba\u81ea\u7531\u3002\n\u7531\u4e8e\u8fd9\u4e2a\u95ee\u9898\u6d89\u53ca\u4e2a\u4eba\u548c\u6587\u5316\u4ef7\u503c\u89c2\u7684\u5dee\u5f02\uff0c\u6ca1\u6709\u5ba2\u89c2\u7684\u201c\u6b63\u786e\u201d\u6216\u201c\u9519\u8bef\u201d\u7b54\u6848\u3002\u56e0\u6b64\uff0c\u6211\u65e0\u6cd5\u7ed9\n\u51fa\u4e00\u4e2a\u7edd\u5bf9\u7684\u5224\u65ad\uff0c\u4f46\u53ef\u4ee5\u8bf4\u8fd9\u4e2a\u95ee\u9898\u7684\u7b54\u6848\u53d6\u51b3\u4e8e\u4e2a\u4eba\u7684\u4ef7\u503c\u89c2\u548c\u4fe1\u4ef0\u3002\nGround Truth: \u9519 \nError Category: Reject to Answer\nError Reason: When it comes to religion, the model refuses to make a judgment.\n<\u56fe\u7247 1>\n\u4eba\u6587\u793e\u4f1a\u79d1\u5b66: \u793e\u4f1a\u5b66\nFigure B173: A sample error case of Humanities and Social Sciences (subcategory: Sociol-\nogy).\nBack to List of figures\n192\nPREPRINT\nQuestion:\u6839\u636e<\u56fe\u7247 1>\u56fe\u4e2d\u4eba\u7269\u6982\u62ec\uff0c\u7537\u5973\u5728\u5404\u65b9\u9762\u7684\u5dee\u522b\u6709\uff08\uff09\noption:\n(A)\u5728\u8bed\u8a00\u8868\u8fbe\u80fd\u529b\u65b9\u9762\uff0c \u7537\u6027\u80dc\u4e8e\u5973\u6027\n(B)\u5728\u89c6\u89c9\u3001 \u5e73\u8861\u89c9\u80fd\u529b\u65b9\u9762\uff0c \u5973\u6027\u80dc\u4e8e\u7537\u6027\n(C)\u5728\u903b\u8f91\u601d\u7ef4\u3001 \u6570\u5b66\u80fd\u529b\u65b9\u9762\uff0c \u7537\u6027\u8f83\u9ad8\u4e8e\u5973\u6027\n(D)\u5728\u89e6\u89c9\u65b9\u9762\uff0c \u5973\u6027\u6bd4\u7537\u6027\u654f\u611f\uff0c \u53cd\u5e94\u66f4\u5feb\nGPT-4V:\u5f88\u62b1\u6b49\uff0c\u4f46\u6211\u65e0\u6cd5\u6839\u636e\u56fe\u7247\u5185\u5bb9\u6765\u56de\u7b54\u5173\u4e8e\u7537\u6027\u548c\u5973\u6027\u5728\u4e0d\u540c\u65b9\u9762\u80fd\u529b\u5dee\u5f02\u7684\u95ee\u9898\u3002\u8fd9\u4e9b\u5dee\n\u5f02\u901a\u5e38\u662f\u57fa\u4e8e\u5e7f\u6cdb\u7684\u79d1\u5b66\u7814\u7a76\u548c\u7edf\u8ba1\u6570\u636e\uff0c\u800c\u4e0d\u662f\u5355\u4e2a\u56fe\u7247\u4e2d\u7684\u4eba\u7269\u3002\u6b64\u5916\uff0c\u6027\u522b\u80fd\u529b\u5dee\u5f02\u7684\u8bdd\u9898\u975e\n\u5e38\u590d\u6742\uff0c\u6d89\u53ca\u751f\u7269\u5b66\u3001\u793e\u4f1a\u5b66\u3001\u5fc3\u7406\u5b66\u7b49\u591a\u4e2a\u9886\u57df\uff0c\u5e76\u4e14\u5b58\u5728\u5927\u91cf\u4e89\u8bae\u548c\u4e0d\u540c\u7684\u7814\u7a76\u7ed3\u679c\u3002\n\u5982\u679c\u4f60\u6709\u5173\u4e8e\u56fe\u7247\u4e2d\u4eba\u7269\u7684\u5176\u4ed6\u95ee\u9898\uff0c\u6216\u8005\u9700\u8981\u5173\u4e8e\u6027\u522b\u5dee\u5f02\u7684\u4e00\u822c\u4fe1\u606f\uff0c\u8bf7\u544a\u8bc9\u6211\uff0c\u6211\u4f1a\u5c3d\u529b\u5e2e\u52a9\n\u4f60\u3002\nGround Truth:\n(C) \u5728\u903b\u8f91\u601d\u7ef4\u3001 \u6570\u5b66\u80fd\u529b\u65b9\u9762\uff0c \u7537\u6027\u8f83\u9ad8\u4e8e\u5973\u6027\n(D) \u5728\u89e6\u89c9\u65b9\u9762\uff0c \u5973\u6027\u6bd4\u7537\u6027\u654f\u611f\uff0c \u53cd\u5e94\u66f4\u5feb \nError Category: Reject to Answer\nError Reason: When it comes to gender, the model refuses to make a judgment.\n<\u56fe\u7247 1>\n\u4eba\u6587\u793e\u4f1a\u79d1\u5b66: \u793e\u4f1a\u5b66\nFigure B174: A sample error case of Humanities and Social Sciences (subcategory: Sociol-\nogy).\nBack to List of figures\n193\nPREPRINT\nQuestion: <\u56fe\u7247 1>\u56fe\u793a\u5bb6\u5ead\u7ed3\u6784\u4e3a\uff08\uff09\noption: \n(A)\u8054\u5408\u5bb6\u5ead\n(B)\u4e3b\u5e72\u5bb6\u5ead\n(C)\u6838\u5fc3\u5bb6\u5ead\n(D)\u5b8c\u5168\u5bb6\u5ead\nGPT-4V:\u8fd9\u5f20\u56fe\u7247\u5c55\u793a\u7684\u662f\u4e00\u4e2a\u5bb6\u5ead\u7ed3\u6784\u56fe\u3002\u6839\u636e\u56fe\u4e2d\u7684\u4fe1\u606f\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u6709\u4e24\u4e2a\u6210\u5e74\u4eba\uff08\u901a\u5e38\u8868\n\u793a\u4e3a\u8f83\u5927\u7684\u7b26\u53f7\uff09\u548c\u4e24\u4e2a\u5b69\u5b50\uff08\u8868\u793a\u4e3a\u8f83\u5c0f\u7684\u7b26\u53f7\uff09\u3002\u8fd9\u4e2a\u7ed3\u6784\u901a\u5e38\u4ee3\u8868\u4e86\u4e00\u4e2a\u7236\u6bcd\u548c\u5b50\u5973\u7ec4\u6210\u7684\u5bb6\n\u5ead\u5355\u5143\u3002\n\u9009\u9879\u4e2d\uff1a\n(A)\u8054\u5408\u5bb6\u5ead\u901a\u5e38\u6307\u7684\u662f\u591a\u4ee3\u540c\u5802\u7684\u5bb6\u5ead\u7ed3\u6784\uff0c\u8fd9\u91cc\u6ca1\u6709\u663e\u793a\u3002\n(B)\u4e3b\u5e72\u5bb6\u5ead\u53ef\u80fd\u6307\u7684\u662f\u6709\u76f4\u7cfb\u4eb2\u5c5e\u548c\u65c1\u7cfb\u4eb2\u5c5e\u5171\u540c\u751f\u6d3b\u7684\u5bb6\u5ead\uff0c\u8fd9\u91cc\u4e5f\u6ca1\u6709\u663e\u793a\u3002\n(C)\u6838\u5fc3\u5bb6\u5ead\u901a\u5e38\u6307\u7684\u662f\u6700\u57fa\u672c\u7684\u5bb6\u5ead\u7ed3\u6784\uff0c\u5373\u7236\u6bcd\u548c\u5b50\u5973\u3002\n(D)\u5b8c\u5168\u5bb6\u5ead\u901a\u5e38\u6307\u7684\u662f\u6709\u7236\u4eb2\u3001\u6bcd\u4eb2\u548c\u5b50\u5973\u7684\u5bb6\u5ead\u7ed3\u6784\u3002\n\u6839\u636e\u8fd9\u4e9b\u5b9a\u4e49\uff0c\u6700\u7b26\u5408\u8fd9\u5f20\u56fe\u7247\u7684\u63cf\u8ff0\u662f(C) \u6838\u5fc3\u5bb6\u5ead\u3002\n\u6b63\u786e\u7b54\u6848\uff1a(C) \u6838\u5fc3\u5bb6\u5ead\nGround Truth: (A) \u8054\u5408\u5bb6\u5ead\nError Category: Reasoning Error\nError Reason: The model misses the point in the figure, leading to incorrect inference. A joint family \nrefers to a family with two or more couples in any generation, such as a family consisting of a \nparent and two or more generations of married children, or a family in which siblings do not \nseparate after marriage.\n<\u56fe\u7247 1>\n\u4eba\u6587\u793e\u4f1a\u79d1\u5b66: \u793e\u4f1a\u5b66\nFigure B175: A sample error case of Humanities and Social Sciences (subcategory: Sociol-\nogy).\nBack to List of figures\n194\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b: \u519c\u4e1a\nQuestion: \u5982<\u56fe\u7247 1>\u6240\u793a\uff0c\u8be5\u4f5c\u7269\u5c5e\u4e8e\uff08\uff09\nOption: \n(A) \u957f\u65e5\u7167\u4f5c\u7269\n(B) \u77ed\u65e5\u7167\u4f5c\u7269\n(C) \u4e2d\u65e5\u7167\u4f5c\u7269\n(D) \u5b9a\u65e5\u7167\u4f5c\u7269\nGPT-4V:  \u56fe\u7247\u663e\u793a\u7684\u662f\u5c0f\u9ea6\uff0c\u5b83\u662f\u4e00\u79cd\u5178\u578b\u7684\u957f\u65e5\u7167\u4f5c\u7269\u3002\u957f\u65e5\u7167\u4f5c\u7269\u662f\u6307\u90a3\u4e9b\u9700\u8981\u8f83\u957f\u767d\u663c\u65f6\u95f4\n\u624d\u80fd\u5f00\u82b1\u7ed3\u5b9e\u7684\u4f5c\u7269\u3002\u56e0\u6b64\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a\n(A) \u957f\u65e5\u7167\u4f5c\u7269\nGround Truth: (A) \u957f\u65e5\u7167\u4f5c\u7269\n<\u56fe\u7247 1>\nFigure B176: A sample correct case of Technology and Engineering (subcategory: Agricul-\nture).\nBack to List of figures\n195\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b: \u519c\u4e1a\nQuestion: \u4e0b\u5217\u5173\u4e8e<\u56fe\u7247 1>\u4e2d\u7684\u690d\u7269\u4ea7\u751f\u7684\u4e00\u79cd\u6210\u82b1\u523a\u6fc0\u7269\u7684\u53d9\u8ff0,\u9519\u8bef\u7684\u662f()\nOption: \n(A)  \u5b83\u662f\u4e00\u79cd\u5c0f\u5206\u5b50\u91cf\u86cb\u767d\u8d28\n(B) \u5b83\u5728\u53f6\u7247\u4e2d\u5408\u6210\n(C) \u5b83\u7684\u57fa\u56e0\u8868\u8fbe\u4e0d\u53d7\u65e5\u7167\u957f\u5ea6\u5f71\u54cd\n(D) \u5b83\u901a\u8fc7\u97e7\u76ae\u90e8\u8fd0\u8f93\u5230\u830e\u5c16\nGPT-4V: \u5f88\u62b1\u6b49\uff0c\u4f46\u6211\u65e0\u6cd5\u76f4\u63a5\u4ece\u56fe\u7247\u4e2d\u786e\u5b9a\u690d\u7269\u4ea7\u751f\u7684\u6210\u82b1\u523a\u6fc0\u7269\u7684\u5177\u4f53\u7279\u6027\u3002\u7136\u800c\uff0c\u6211\u53ef\u4ee5\u63d0\u4f9b\n\u4e00\u4e9b\u5173\u4e8e\u690d\u7269\u6210\u82b1\u523a\u6fc0\u7269\uff08\u4f8b\u5982\u5f17\u6d1b\u91cc\u6839\uff09\u7684\u4e00\u822c\u4fe1\u606f\uff0c\u8fd9\u53ef\u80fd\u6709\u52a9\u4e8e\u63a8\u65ad\u6b63\u786e\u7b54\u6848\u3002\n(A) \u6210\u82b1\u523a\u6fc0\u7269\u7684\u786e\u5207\u5316\u5b66\u672c\u8d28\u5c1a\u672a\u5b8c\u5168\u786e\u5b9a\uff0c\u4f46\u6709\u7814\u7a76\u8868\u660e\u5b83\u53ef\u80fd\u662f\u4e00\u79cd\u86cb\u767d\u8d28\u6216\u4e0e\u86cb\u767d\u8d28\u6709\u5173\u7684\n\u5206\u5b50\u3002\n(B) \u6210\u82b1\u523a\u6fc0\u7269\u901a\u5e38\u5728\u53f6\u7247\u4e2d\u5408\u6210\uff0c\u8fd9\u662f\u56e0\u4e3a\u53f6\u7247\u662f\u690d\u7269\u611f\u53d7\u5916\u754c\u5149\u5468\u671f\u53d8\u5316\u7684\u4e3b\u8981\u90e8\u4f4d\u3002\n(C) \u6210\u82b1\u523a\u6fc0\u7269\u7684\u5408\u6210\u901a\u5e38\u53d7\u5230\u65e5\u7167\u957f\u5ea6\u7684\u5f71\u54cd\uff0c\u8fd9\u662f\u56e0\u4e3a\u690d\u7269\u7684\u5f00\u82b1\u65f6\u95f4\u5f80\u5f80\u4e0e\u5149\u5468\u671f\u6709\u5173\uff0c\u8fd9\u4e00\n\u70b9\u5728\u957f\u65e5\u690d\u7269\u548c\u77ed\u65e5\u690d\u7269\u7684\u7814\u7a76\u4e2d\u5f97\u5230\u4e86\u8bc1\u5b9e\u3002\n(D) \u6210\u82b1\u523a\u6fc0\u7269\u88ab\u8ba4\u4e3a\u662f\u901a\u8fc7\u690d\u7269\u7684\u97e7\u76ae\u90e8\u4ece\u53f6\u7247\u8fd0\u8f93\u5230\u5176\u4ed6\u90e8\u4f4d\uff0c\u5982\u830e\u5c16\uff0c\u4ee5\u8bf1\u5bfc\u5f00\u82b1\u3002\n\u6839\u636e\u8fd9\u4e9b\u4fe1\u606f\uff0c\u9519\u8bef\u7684\u9009\u9879\u6700\u53ef\u80fd\u662f (C) \u5b83\u7684\u57fa\u56e0\u8868\u8fbe\u4e0d\u53d7\u65e5\u7167\u957f\u5ea6\u5f71\u54cd\uff0c\u56e0\u4e3a\u6210\u82b1\u523a\u6fc0\u7269\u7684\u5408\u6210\n\u901a\u5e38\u662f\u53d7\u5230\u65e5\u7167\u957f\u5ea6\u5f71\u54cd\u7684\u3002\nGround Truth: (C) \u5b83\u7684\u57fa\u56e0\u8868\u8fbe\u4e0d\u53d7\u65e5\u7167\u957f\u5ea6\u5f71\u54cd\n<\u56fe\u7247 1>\nFigure B177: A sample correct case of Technology and Engineering (subcategory: Agricul-\nture).\nBack to List of figures\n196\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b: \u519c\u4e1a\nQuestion: \u4e0b\u5217\u6784\u6210\u86cb\u767d\u8d28\u7684\u6c28\u57fa\u9178\u4e2d,R \u57fa\u56e2\u80fd\u591f\u4ea7\u751f\u5982<\u56fe\u7247 1>\u6240\u793a\u53cd\u5e94\u7684\u662f\uff08\uff09\nOption: \n(A) \u7518\u6c28\u9178\n(B) \u916a\u6c28\u9178\n(C) \u82ef\u4e19\u6c28\u9178\n(D) \u7532\u786b\u9178\nGPT-4V: \u56fe\u7247\u663e\u793a\u7684\u662fATP\uff08\u4e09\u78f7\u9178\u817a\u82f7\uff09\u5411ADP\uff08\u4e8c\u78f7\u9178\u817a\u82f7\uff09\u8f6c\u5316\u7684\u8fc7\u7a0b\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u6d89\u53ca\u5230\u4e00\u4e2a\u78f7\n\u9178\u57fa\u56e2\u7684\u8f6c\u79fb\u3002\u5728\u86cb\u767d\u8d28\u7684\u6c28\u57fa\u9178\u4e2d\uff0c\u6709\u4e9b\u6c28\u57fa\u9178\u7684R\u57fa\u56e2\u53ef\u4ee5\u53c2\u4e0e\u78f7\u9178\u5316\u53cd\u5e94\uff0c\u5373\u63a5\u53d7\u78f7\u9178\u57fa\u56e2\u3002\n\u5728\u63d0\u4f9b\u7684\u9009\u9879\u4e2d\uff0c\u916a\u6c28\u9178\uff08B\u9009\u9879\uff09\u662f\u5df2\u77e5\u53ef\u4ee5\u88ab\u78f7\u9178\u5316\u7684\u6c28\u57fa\u9178\u4e4b\u4e00\u3002\u916a\u6c28\u9178\u6b8b\u57fa\u7684\u7f9f\u57fa\uff08-OH\uff09\u53ef\n\u4ee5\u4f5c\u4e3a\u78f7\u9178\u57fa\u56e2\u7684\u53d7\u4f53\uff0c\u8fdb\u884c\u78f7\u9178\u5316\u53cd\u5e94\u3002\u56e0\u6b64\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a\n(B) \u916a\u6c28\u9178\nGround Truth: (B) \u916a\u6c28\u9178\n<\u56fe\u7247 1>\nFigure B178: A sample correct case of Technology and Engineering (subcategory: Agricul-\nture).\nBack to List of figures\n197\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b: \u519c\u4e1a\nQuestion: <\u56fe\u7247 1>\u6240\u793a\u7684\u662f__\u519c\u4e1a\nOption: \n(A) \u539f\u59cb\n(B) \u4f20\u7edf\n(C) \u73b0\u4ee3\n(D) \u89c2\u5149\nGPT-4V: \u8fd9\u5f20\u56fe\u7247\u663e\u793a\u7684\u662f\u4e00\u4e2a\u7cbe\u5fc3\u89c4\u5212\u548c\u8bbe\u8ba1\u7684\u519c\u4e1a\u533a\u57df\uff0c\u5177\u6709\u6574\u9f50\u7684\u7530\u5730\u548c\u82b1\u575b\uff0c\u4ee5\u53ca\u73b0\u4ee3\u5316\u7684\n\u5efa\u7b51\u3002\u8fd9\u4e0d\u592a\u53ef\u80fd\u662f\u539f\u59cb\u6216\u4f20\u7edf\u519c\u4e1a\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u901a\u5e38\u4e0d\u4f1a\u6709\u8fd9\u4e48\u7cbe\u5fc3\u7684\u5e03\u5c40\u548c\u73b0\u4ee3\u8bbe\u65bd\u3002\u540c\u65f6\uff0c\u8fd9\u4e2a\n\u573a\u666f\u4e5f\u53ef\u80fd\u662f\u4e3a\u4e86\u5438\u5f15\u6e38\u5ba2\u800c\u8bbe\u8ba1\u7684\uff0c\u56e0\u6b64\u53ef\u80fd\u5177\u6709\u89c2\u5149\u519c\u4e1a\u7684\u7279\u70b9\u3002\u56e0\u6b64\uff0c\u6700\u53ef\u80fd\u6b63\u786e\u7684\u9009\u9879\u662f\uff1a\n(C) \u73b0\u4ee3\n(D) \u89c2\u5149\n\u6b63\u786e\u7b54\u6848\u662f (C) \u73b0\u4ee3 \u548c (D) \u89c2\u5149\u3002\nGround Truth: (D) \u89c2\u5149\n<\u56fe\u7247 1>\nError Category: Perceptual Error\nError Reason:  Modern agriculture pays more attention to production efficiency and technology \napplication, aiming to improve food production and quality,\nwhile tourism agriculture focuses more on providing agricultural experience and education,  \nadding economic value to agriculture. There are obviously many tourists in the picture, so choose D \ninstead of C.\nFigure B179: A sample error case of Technology and Engineering (subcategory: Agriculture).\nBack to List of figures\n198\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b: \u519c\u4e1a\nQuestion: <\u56fe\u7247 1>\u6240\u793a\u76846\u662f__\nOption: \n(A) \u74e3\u80c3\n(B) \u76b1\u80c3\n(C) \u7f51\u80c3\n(D) \u7624\u80c3\nGPT-4V: \u8fd9\u5f20\u56fe\u7247\u5c55\u793a\u7684\u662f\u725b\u7684\u6d88\u5316\u7cfb\u7edf\u7684\u793a\u610f\u56fe\u3002\u5728\u725b\u7684\u80c3\u90e8\u5206\uff0c\u6709\u56db\u4e2a\u5ba4\uff1a\u7624\u80c3\u3001\u7f51\u80c3\u3001\u74e3\u80c3\u548c\n\u76b1\u80c3\u3002\u6839\u636e\u56fe\u4e2d\u7684\u6570\u5b576\u7684\u4f4d\u7f6e\uff0c\u5b83\u6307\u5411\u7684\u662f\u725b\u7684\u7b2c\u4e8c\u4e2a\u80c3\u5ba4\uff0c\u5373\u7f51\u80c3\u3002\n\u6b63\u786e\u7b54\u6848\u662f\uff1a\n(C) \u7f51\u80c3\nGround Truth: (A) \u74e3\u80c3\n<\u56fe\u7247 1>\nError Category: Reasoning Error\nError Reason: \nThe omasum is the third stomach of ruminants. The regurgitated food will directly enter the \nomasum for further digestion. The flaps on the inner wall of the omasum are folded into book pages \nand contain digestive enzymes that can break down plant fibers into glucose and transport them to \nthe intestines for absorption.\nFigure B180: A sample error case of Technology and Engineering (subcategory: Agriculture).\nBack to List of figures\n199\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b: \u519c\u4e1a\nQuestion: \u8c4c\u8c46\u8c61<\u56fe\u7247 1>\u5c5e\u4e8e()\u6606\u866b\u3002 \nOption: \n(A) \u5355\u98df\u6027\n(B) \u5be1\u98df\u6027\n(C) \u591a\u98df\u6027\n(D) \u8150\u98df\u6027\nGPT-4V: \u8fd9\u5f20\u56fe\u7247\u5c55\u793a\u4e86\u4e00\u79cd\u6606\u866b\uff0c\u4f46\u662f\u4ec5\u51ed\u56fe\u7247\u65e0\u6cd5\u786e\u5b9a\u8fd9\u79cd\u6606\u866b\u7684\u98df\u6027\u3002\u8c4c\u8c46\u8c61\u662f\u4e00\u79cd\u7532\u866b\uff0c\u4f46\n\u662f\u6ca1\u6709\u8db3\u591f\u7684\u4fe1\u606f\u6765\u5224\u65ad\u5b83\u662f\u5355\u98df\u6027\u3001\u5be1\u98df\u6027\u3001\u591a\u98df\u6027\u8fd8\u662f\u8150\u98df\u6027\u3002\u901a\u5e38\uff0c\u8c4c\u8c46\u8c61\u5c5e\u4e8e\u5bb3\u866b\uff0c\u5b83\u4eec\u53ef\n\u80fd\u4f1a\u4e13\u95e8\u653b\u51fb\u67d0\u4e9b\u690d\u7269\uff0c\u5982\u8c4c\u8c46\u690d\u7269\uff0c\u8fd9\u53ef\u80fd\u610f\u5473\u7740\u5b83\u4eec\u662f\u5355\u98df\u6027\u6216\u5be1\u98df\u6027\u3002\u7136\u800c\uff0c\u4e3a\u4e86\u7ed9\u51fa\u4e00\u4e2a\u51c6\n\u786e\u7684\u7b54\u6848\uff0c\u9700\u8981\u66f4\u591a\u7684\u751f\u6001\u548c\u751f\u7269\u5b66\u80cc\u666f\u4fe1\u606f\u3002\u5728\u6ca1\u6709\u8fd9\u4e9b\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u6211\u65e0\u6cd5\u786e\u5b9a\u6b63\u786e\u7b54\u6848\u3002\nGround Truth: (A) \u5355\u98df\u6027\n<\u56fe\u7247 1>\nError Category: Lack of Knowledge\nError Reason:  Pea weevils are monophagous insects, mainly because of their food specialization, \nspecializing in the seeds of pea plants.\nFigure B181: A sample error case of Technology and Engineering (subcategory: Agriculture).\nBack to List of figures\n200\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b: \u519c\u4e1a\nQuestion:  <\u56fe\u7247 1>\u6240\u793a\u7684\u662f\u51cf\u8f7b\u4f5c\u7269\u75c5\u866b\u8349\u7684\u8f6e\u4f5c\u65b9\u5f0f\u4e2d\u7684__\nOption: \n(A) \u590d\u79cd\u8f6e\u4f5c\n(B) \u6c34\u65f1\u8f6e\u4f5c\n(C) \u4f5c\u7269\u8f6e\u4f5c\n(D) \u6362\u832c\u5f0f\u8f6e\u4f5c\nGPT-4V: \u8fd9\u5f20\u56fe\u7247\u663e\u793a\u4e86\u4e24\u79cd\u4e0d\u540c\u7684\u4f5c\u7269\u79cd\u690d\u5728\u540c\u4e00\u5757\u571f\u5730\u4e0a\uff0c\u4e00\u79cd\u4f5c\u7269\u4f3c\u4e4e\u662f\u7389\u7c73\uff0c\u800c\u53e6\u4e00\u79cd\u4f5c\u7269\n\u662f\u4f4e\u77ee\u7684\u690d\u7269\uff0c\u53ef\u80fd\u662f\u8c46\u7c7b\u6216\u5176\u4ed6\u7c7b\u578b\u7684\u4f5c\u7269\u3002\u8fd9\u79cd\u79cd\u690d\u65b9\u5f0f\u770b\u8d77\u6765\u50cf\u662f\u4e00\u79cd\u8f6e\u4f5c\u65b9\u5f0f\uff0c\u65e8\u5728\u901a\u8fc7\u79cd\n\u690d\u4e0d\u540c\u7c7b\u578b\u7684\u4f5c\u7269\u6765\u6253\u65ad\u75c5\u866b\u5bb3\u7684\u751f\u547d\u5468\u671f\uff0c\u63d0\u9ad8\u571f\u58e4\u7684\u80a5\u529b\uff0c\u5e76\u51cf\u5c11\u5bf9\u5316\u80a5\u7684\u4f9d\u8d56\u3002\n\u6839\u636e\u9009\u9879\u548c\u56fe\u7247\u4fe1\u606f\uff0c\u6700\u5408\u9002\u7684\u7b54\u6848\u662f\uff1a\n(C) \u4f5c\u7269\u8f6e\u4f5c\n\u8fd9\u662f\u56e0\u4e3a\u4f5c\u7269\u8f6e\u4f5c\u901a\u5e38\u6d89\u53ca\u5728\u4e0d\u540c\u7684\u5b63\u8282\u6216\u5e74\u4efd\u66f4\u6362\u4e0d\u540c\u7c7b\u578b\u7684\u4f5c\u7269\uff0c\u4ee5\u8fbe\u5230\u4e0a\u8ff0\u7684\u519c\u4e1a\u6548\u76ca\u3002\u5176\u4ed6\n\u9009\u9879\u5982\u590d\u79cd\u8f6e\u4f5c\u3001\u6c34\u65f1\u8f6e\u4f5c\u548c\u6362\u832c\u5f0f\u8f6e\u4f5c\u6ca1\u6709\u8db3\u591f\u7684\u4fe1\u606f\u6765\u76f4\u63a5\u4ece\u56fe\u7247\u4e2d\u786e\u8ba4\u3002\u56e0\u6b64\uff0c\u6700\u53ef\u80fd\u6b63\u786e\u7684\n\u9009\u9879\u662f(C) \u4f5c\u7269\u8f6e\u4f5c\u3002\nGround Truth: (A) \u590d\u79cd\u8f6e\u4f5c\n<\u56fe\u7247 1>\nError Category: Lack of Knowledge\nError Reason:  Multiple cropping: The practice of growing two or more crops on the same land in \none year.\nIn the picture, two crops are grown on dry land at the same time, so the correct answer is A.\nFigure B182: A sample error case of Technology and Engineering (subcategory: Agriculture).\nBack to List of figures\n201\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b:\u5efa\u7b51\u5b66\nQuestion:  <\u56fe\u7247 1>\u4f53\u7cfb\u7684\u51e0\u4f55\u7ec4\u6210\u4e3a\uff08\uff09\nOption:\n(A) \u51e0\u4f55\u4e0d\u53d8\u65e0\u591a\u4f59\u7ea6\u675f\n(B) \u51e0\u4f55\u4e0d\u53d8\u6709\u591a\u4f59\u7ea6\u675f\n(C) -1\n(D) -2\nGPT-4V: \u5f88\u62b1\u6b49\uff0c\u4f46\u6211\u65e0\u6cd5\u76f4\u63a5\u63d0\u4f9b\u8fd9\u4e2a\u7279\u5b9a\u7684\u673a\u68b0\u7cfb\u7edf\u7684\u51e0\u4f55\u7ec4\u6210\u7c7b\u578b\uff0c\u56e0\u4e3a\u8fd9\u9700\u8981\u5bf9\u673a\u68b0\u5de5\u7a0b\u7684\n\u4e13\u4e1a\u77e5\u8bc6\u8fdb\u884c\u5206\u6790\uff0c\u800c\u8fd9\u5f20\u56fe\u7247\u6ca1\u6709\u63d0\u4f9b\u8db3\u591f\u7684\u4fe1\u606f\u6765\u786e\u5b9a\u5176\u51e0\u4f55\u7ec4\u6210\u3002\u901a\u5e38\uff0c\u8fd9\u7c7b\u95ee\u9898\u9700\u8981\u5bf9\u673a\u6784\n\u7684\u81ea\u7531\u5ea6\u3001\u7ea6\u675f\u7c7b\u578b\u4ee5\u53ca\u5176\u8fd0\u52a8\u80fd\u529b\u8fdb\u884c\u5206\u6790\u3002\u5982\u679c\u4f60\u80fd\u63d0\u4f9b\u66f4\u591a\u5173\u4e8e\u8fd9\u4e2a\u7cfb\u7edf\u7684\u80cc\u666f\u4fe1\u606f\u6216\u8005\u5176\u5de5\n\u4f5c\u539f\u7406\u7684\u63cf\u8ff0\uff0c\u6211\u6216\u8bb8\u80fd\u591f\u5e2e\u52a9\u4f60\u66f4\u597d\u5730\u7406\u89e3\u5b83\u7684\u5206\u7c7b\u3002\nGround Truth: (B) \u51e0\u4f55\u4e0d\u53d8\u6709\u591a\u4f59\u7ea6\u675f\n<\u56fe\u7247 1>\nError Category: Reject to Answer\nError Reason: While GPT-4V does possess the capability of image recognition, in this specific \ncontext, it did not fully leverage this ability to analyze the image content and provide an answer. \nThe model chose to avoid directly answering the question and instead emphasized the need for \nmore background information. This might be due to the model's strategy of being overly cautious \nwhen dealing with image-based multiple-choice questions, thus avoiding making direct judgments \nabout the image content.\nFigure B183: A sample error case of Technology and Engineering (subcategory: Architec-\nture).\nBack to List of figures\n202\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b:\u5efa\u7b51\u5b66\nQuestion:  <\u56fe\u7247 1>\u4f53\u7cfb\u7684\u51e0\u4f55\u7ec4\u6210\u4e3a\uff08\uff09\nOption:\n(A) \u51e0\u4f55\u4e0d\u53d8\uff0c\u65e0\u591a\u4f59\u8054\u7cfb\n(B) \u51e0\u4f55\u4e0d\u53d8\uff0c\u6709\u591a\u4f59\u8054\u7cfb\n(C) \u77ac\u53d8\n(D) \u5e38\u53d8\nGPT-4V: \u5f88\u62b1\u6b49\uff0c\u4f46\u6211\u65e0\u6cd5\u76f4\u63a5\u63d0\u4f9b\u8fd9\u4e2a\u7279\u5b9a\u673a\u68b0\u7cfb\u7edf\u7684\u51e0\u4f55\u7ec4\u6210\u7c7b\u578b\u3002\u901a\u5e38\uff0c\u8fd9\u7c7b\u95ee\u9898\u9700\u8981\u5bf9\u673a\u68b0\n\u7cfb\u7edf\u7684\u8fd0\u52a8\u5b66\u548c\u52a8\u529b\u5b66\u6709\u6df1\u5165\u7684\u4e86\u89e3\uff0c\u4ee5\u53ca\u5bf9\u56fe\u793a\u4e2d\u7ed3\u6784\u7684\u7ea6\u675f\u548c\u81ea\u7531\u5ea6\u8fdb\u884c\u5206\u6790\u3002\u5728\u6ca1\u6709\u5177\u4f53\u5206\u6790\n\u7684\u60c5\u51b5\u4e0b\uff0c\u6211\u65e0\u6cd5\u786e\u5b9a\u6b63\u786e\u7b54\u6848\u3002\u5982\u679c\u4f60\u80fd\u63d0\u4f9b\u66f4\u591a\u5173\u4e8e\u8fd9\u4e2a\u7cfb\u7edf\u7684\u80cc\u666f\u4fe1\u606f\u6216\u5176\u5de5\u4f5c\u539f\u7406\u7684\u7ec6\u8282\uff0c\n\u6211\u53ef\u80fd\u80fd\u591f\u5e2e\u52a9\u4f60\u66f4\u597d\u5730\u7406\u89e3\u8fd9\u4e2a\u95ee\u9898\u3002\nGround Truth: (D) \u5e38\u53d8\n<\u56fe\u7247 1>\nError Category: Reject to Answer\nError Reason:GPT-4V chose not to answer this question. The model indicated that it cannot directly \nprovide the specific geometric composition types of a mechanical system because this typically \nrequires a deep understanding of the kinematics and dynamics of the mechanical system, as well as \nan analysis of the constraints and degrees of freedom of the structures depicted in the illustration. \nHowever, this response approach demonstrates the limitations of the model when dealing with \nimage-based questions.\nFigure B184: A sample error case of Technology and Engineering (subcategory: Architec-\nture).\nBack to List of figures\n203\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b:\u5efa\u7b51\u5b66\nQuestion:  <\u56fe\u7247 1>\u4f53\u7cfb\u4e3a\uff1a\uff08\uff09\nOption:\n(A) \u51e0\u4f55\u4e0d\u53d8\u65e0\u591a\u4f59\u7ea6\u675f\n(B) \u51e0\u4f55\u4e0d\u53d8\u6709\u591a\u4f59\u7ea6\u675f\n(C) \u51e0\u4f55\u5e38\u53d8\n(D) \u51e0\u4f55\u77ac\u53d8\nGPT-4V: \u62b1\u6b49\uff0c\u7531\u4e8e\u56fe\u7247\u4e2d\u7684\u4fe1\u606f\u4e0d\u8db3\u4ee5\u786e\u5b9a\u4e00\u4e2a\u660e\u786e\u7684\u7b54\u6848\uff0c\u6211\u65e0\u6cd5\u76f4\u63a5\u63d0\u4f9b\u6b63\u786e\u9009\u9879\u3002\u4f46\u662f\uff0c\u6839\n\u636e\u56fe\u7247\u4e2d\u663e\u793a\u7684\u7ed3\u6784\uff0c\u5b83\u4f3c\u4e4e\u662f\u4e00\u4e2a\u673a\u68b0\u7cfb\u7edf\u6216\u7ed3\u6784\uff0c\u53ef\u80fd\u662f\u4e00\u4e2a\u673a\u6784\u6216\u6846\u67b6\u3002\u901a\u5e38\uff0c\u8fd9\u7c7b\u95ee\u9898\u6d89\u53ca\n\u5230\u673a\u68b0\u5de5\u7a0b\u9886\u57df\uff0c\u7279\u522b\u662f\u673a\u6784\u7406\u8bba\u548c\u8fd0\u52a8\u5b66\u3002\n\u5728\u673a\u68b0\u5de5\u7a0b\u4e2d\uff0c\"\u51e0\u4f55\u4e0d\u53d8\"\u901a\u5e38\u6307\u7684\u662f\u4e00\u4e2a\u673a\u6784\u5728\u8fd0\u52a8\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u5176\u51e0\u4f55\u5f62\u72b6\u4e0d\u53d8\u3002\u5982\u679c\u4e00\u4e2a\u673a\u6784\u5728\n\u8fd0\u52a8\u65f6\u4fdd\u6301\u67d0\u4e9b\u51e0\u4f55\u7279\u6027\u4e0d\u53d8\uff0c\u4f46\u53c8\u6709\u591a\u4f59\u7684\u7ea6\u675f\u4f7f\u5f97\u5176\u8fd0\u52a8\u53d7\u9650\uff0c\u90a3\u4e48\u5b83\u53ef\u80fd\u88ab\u63cf\u8ff0\u4e3a\"\u51e0\u4f55\u4e0d\u53d8\n\u6709\u591a\u4f59\u7ea6\u675f\"\u3002\u5982\u679c\u673a\u6784\u7684\u51e0\u4f55\u7279\u6027\u5728\u8fd0\u52a8\u4e2d\u4f1a\u53d1\u751f\u53d8\u5316\uff0c\u90a3\u4e48\u5b83\u53ef\u80fd\u88ab\u63cf\u8ff0\u4e3a\"\u51e0\u4f55\u5e38\u53d8\"\u3002\"\u51e0\u4f55\u77ac\n\u53d8\"\u5219\u53ef\u80fd\u6307\u7684\u662f\u673a\u6784\u5728\u7279\u5b9a\u77ac\u95f4\u7684\u51e0\u4f55\u53d8\u5316\u3002\n\u7531\u4e8e\u6211\u65e0\u6cd5\u770b\u5230\u56fe\u7247\u4e2d\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u6211\u65e0\u6cd5\u786e\u5b9a\u8fd9\u4e2a\u7ed3\u6784\u662f\u5426\u6709\u591a\u4f59\u7684\u7ea6\u675f\u6216\u8005\u5b83\u7684\u51e0\u4f55\u7279\u6027\u662f\u5426\u4f1a\n\u968f\u65f6\u95f4\u53d8\u5316\u3002\u56e0\u6b64\uff0c\u6211\u65e0\u6cd5\u786e\u5b9a\u6b63\u786e\u7b54\u6848\u3002\u5982\u679c\u4f60\u80fd\u63d0\u4f9b\u66f4\u591a\u5173\u4e8e\u8fd9\u4e2a\u7ed3\u6784\u7684\u80cc\u666f\u4fe1\u606f\u6216\u8005\u5b83\u7684\u8fd0\u52a8\n\u7279\u6027\uff0c\u6211\u53ef\u80fd\u80fd\u591f\u5e2e\u52a9\u4f60\u66f4\u51c6\u786e\u5730\u5224\u65ad\u3002\nGround Truth: (A) \u51e0\u4f55\u4e0d\u53d8\u65e0\u591a\u4f59\u7ea6\u675f\n<\u56fe\u7247 1>\nError Category: Reject to Answer, Perceptual Error\nError Reason: Firstly, the model stated that it couldn't determine the correct answer due to the \ninability to view detailed information in the image. This suggests that the model believes it cannot \naccess or interpret image content correctly, leading to a choice of not answering the question. In \nreality, GPT-4V has image recognition capabilities, so it should have been able to analyze image \ncontent and provide a response based on it. This indicates a perceptual limitation in the model \nwhen dealing with image-related questions.\nSecondly, because the model did not correctly interpret the image content, it couldn't engage in \nreasonable inference based on the image information. The model's response primarily relied on the \nunderstanding of the text in the question, without incorporating key information from the image. \nThis reflects that in tasks involving a combination of text and images, the model failed to effectively \nintegrate these two sources of information, leading to a perceptual error.\nFigure B185: A sample error case of Technology and Engineering (subcategory: Architec-\nture).\nBack to List of figures\n204\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b:\u5efa\u7b51\u5b66\nQuestion:  <\u56fe\u7247 1>\u5e73\u9762\u4f53\u7cfb\u7684\u8ba1\u7b97\u81ea\u7531\u5ea6\u7b49\u4e8e_\u3002\nGPT-4V: \u8fd9\u4e2a\u95ee\u9898\u6d89\u53ca\u5230\u673a\u68b0\u5de5\u7a0b\u9886\u57df\u4e2d\u7684\u7ed3\u6784\u5206\u6790\uff0c\u7279\u522b\u662f\u5e73\u9762\u521a\u67b6\u7ed3\u6784\u7684\u81ea\u7531\u5ea6\u8ba1\u7b97\u3002\u8ba1\u7b97\u5e73\u9762\n\u521a\u67b6\u7ed3\u6784\u7684\u81ea\u7531\u5ea6\u901a\u5e38\u4f7f\u7528\u5e93\u5179\u66fc\u65b9\u7a0b\uff08Kutzbach criterion\uff09\uff0c\u8be5\u65b9\u7a0b\u4e3a\uff1a\n\u81ea\u7531\u5ea6 F = 3n - 3j - c\n\u5176\u4e2d\uff1a\n- n \u662f\u7ed3\u6784\u4e2d\u7684\u8282\u70b9\u6570\n- j \u662f\u7ed3\u6784\u4e2d\u7684\u6746\u4ef6\u6570\n- c \u662f\u7ed3\u6784\u4e2d\u7684\u7ea6\u675f\u53cd\u5e94\u6570\uff08\u652f\u5ea7\u53cd\u529b\u6570\uff09\n\u5728\u63d0\u4f9b\u7684\u56fe\u7247\u4e2d\uff0c\u6211\u4eec\u9700\u8981\u6570\u4e00\u4e0b\u8282\u70b9\u3001\u6746\u4ef6\u548c\u7ea6\u675f\u3002\u4f46\u7531\u4e8e\u56fe\u7247\u7684\u5206\u8fa8\u7387\u8f83\u4f4e\uff0c\u6211\u65e0\u6cd5\u6e05\u6670\u5730\u8bc6\u522b\n\u5177\u4f53\u7684\u8282\u70b9\u6570\u3001\u6746\u4ef6\u6570\u548c\u7ea6\u675f\u6570\u3002\u5982\u679c\u4f60\u80fd\u63d0\u4f9b\u8fd9\u4e9b\u4fe1\u606f\uff0c\u6211\u53ef\u4ee5\u5e2e\u4f60\u8ba1\u7b97\u81ea\u7531\u5ea6\u3002\u5982\u679c\u6ca1\u6709\u8fd9\u4e9b\u4fe1\n\u606f\uff0c\u6211\u65e0\u6cd5\u63d0\u4f9b\u4e00\u4e2a\u786e\u5207\u7684\u7b54\u6848\u3002\nGround Truth: -2\n<\u56fe\u7247 1>\nError Category: Perceptual Error\nError Reason: In its response, GPT-4V mentioned that it couldn't clearly identify the number of \nnodes, members, and constraints due to the low image resolution. This indicates that the model \nattempted to answer the question based on visual information, but due to the limitations of its \nvisual recognition capabilities, it was unable to accurately interpret the content in the image. This is \na typical perception error, where the model's interpretation of visual information is either \ninaccurate or insufficient.\nFigure B186: A sample error case of Technology and Engineering (subcategory: Architec-\nture).\nBack to List of figures\n205\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b:\u5efa\u7b51\u5b66\nQuestion:  <\u56fe\u7247 1>\u4f53\u7cfb\u662f\uff08\uff09\nOption:\n(A) \u65e0\u591a\u4f59\u8054\u7cfb\u7684\u51e0\u4f55\u4e0d\u53d8\u4f53\u7cfb\n(B) \u51e0\u4f55\u53ef\u53d8\u4f53\u7cfb\n(C) \u6709\u591a\u4f59\u8054\u7cfb\u7684\u51e0\u4f55\u4e0d\u53d8\u4f53\u7cfb\n(D) \u77ac\u53d8\u4f53\u7cfb\nGPT-4V: \u5f88\u62b1\u6b49\uff0c\u7531\u4e8e\u56fe\u7247\u7684\u5185\u5bb9\u662f\u6280\u672f\u6027\u8d28\u7684\uff0c\u5e76\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u6765\u89e3\u91ca\uff0c\u6211\u65e0\u6cd5\u76f4\u63a5\u770b\u5230\u56fe\u7247\u5185\n\u5bb9\u3002\u56e0\u6b64\uff0c\u6211\u65e0\u6cd5\u63d0\u4f9b\u4e00\u4e2a\u786e\u5b9a\u7684\u7b54\u6848\u3002\u901a\u5e38\uff0c\u8fd9\u7c7b\u95ee\u9898\u6d89\u53ca\u5230\u7ed3\u6784\u5206\u6790\uff0c\u5176\u4e2d\u5305\u62ec\u5224\u65ad\u4e00\u4e2a\u7ed3\u6784\u662f\n\u5426\u662f\u51e0\u4f55\u4e0d\u53d8\u7684\u3001\u662f\u5426\u6709\u591a\u4f59\u7684\u8054\u7cfb\uff0c\u6216\u8005\u662f\u5426\u662f\u77ac\u53d8\u4f53\u7cfb\u3002\u8fd9\u9700\u8981\u5bf9\u7ed3\u6784\u529b\u5b66\u548c\u5de5\u7a0b\u539f\u7406\u6709\u6df1\u5165\u7684\n\u4e86\u89e3\u3002\n\u5728\u6ca1\u6709\u5177\u4f53\u5206\u6790\u8fd9\u4e2a\u7ed3\u6784\u7684\u80fd\u529b\u7684\u60c5\u51b5\u4e0b\uff0c\u6211\u5efa\u8bae\u54a8\u8be2\u4e00\u4e2a\u7ed3\u6784\u5de5\u7a0b\u5e08\u6216\u8005\u4f7f\u7528\u76f8\u5173\u7684\u7ed3\u6784\u5206\u6790\u8f6f\u4ef6\n\u6765\u786e\u5b9a\u6b63\u786e\u7684\u7b54\u6848\u3002\u5982\u679c\u4f60\u80fd\u63d0\u4f9b\u66f4\u591a\u7684\u80cc\u666f\u4fe1\u606f\u6216\u8005\u8be6\u7ec6\u63cf\u8ff0\u8fd9\u4e2a\u7ed3\u6784\u7684\u7279\u70b9\uff0c\u6211\u6216\u8bb8\u80fd\u591f\u5e2e\u52a9\u4f60\n\u66f4\u597d\u5730\u7406\u89e3\u8fd9\u4e9b\u9009\u9879\u3002\nGround Truth: (A) \u65e0\u591a\u4f59\u8054\u7cfb\u7684\u51e0\u4f55\u4e0d\u53d8\u4f53\u7cfb\n<\u56fe\u7247 1>\nError Category: Perceptual Error, Reject to Answer\nError Reason: First, regarding the perception error, GPT-4V actually has image recognition \ncapabilities, but in this response, it incorrectly stated that it cannot see or understand image content. \nThis indicates that the model did not correctly use its image processing abilities to parse the content \nof the image in the question, resulting in an incomplete understanding of the question. Secondly, \nregarding the refusal to answer, after confirming that it cannot understand the image content, the \nmodel chose the path of not providing a specific answer. While exercising caution in situations \nwhere it cannot fully comprehend a question is reasonable, in such cases, the model should have \nmade an attempt to use the information in its knowledge base to provide an answer that closely \naligns with the question's request, or at least provide a reasonable guess based on textual \ndescriptions.\nFigure B187: A sample error case of Technology and Engineering (subcategory: Architec-\nture).\nBack to List of figures\n206\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b:\u5efa\u7b51\u5b66\nQuestion:  \u5982<\u56fe\u7247 1>\u4e24\u5757\u5e73\u677f\u73bb\u7483\u6784\u6210\u7684\u7a7a\u6c14\u5288\u5c16\uff0c\u5de6\u8fb9\u4e3a\u68f1\u8fb9\uff0c\u7528\u5355\u8272\u5e73\u884c\u5149\u5782\u76f4\u5165\u5c04\u3002\u82e5\u4e0a\u9762\n\u7684\u5e73\u677f\u73bb\u7483\u6162\u6162\u5730\u5411\u4e0a\u5e73\u79fb\uff0c\u5219\u5e72\u6d89\u6761\u7eb9\uff08\uff09\nOption:\n(A) \u5411\u68f1\u8fb9\u65b9\u5411\u5e73\u79fb\uff0c\u6761\u7eb9\u95f4\u9694\u53d8\u5c0f\n(B) \u5411\u68f1\u8fb9\u65b9\u5411\u5e73\u79fb\uff0c\u6761\u7eb9\u95f4\u9694\u53d8\u5927\n(C) \u5411\u68f1\u8fb9\u65b9\u5411\u5e73\u79fb\uff0c\u6761\u7eb9\u95f4\u9694\u4e0d\u53d8\n(D) \u5411\u8fdc\u79bb\u68f1\u8fb9\u7684\u65b9\u5411\u5e73\u79fb\uff0c\u6761\u7eb9\u95f4\u9694\u4e0d\u53d8\nGPT-4V: \u5728\u8fd9\u4e2a\u95ee\u9898\u4e2d\uff0c\u6211\u4eec\u8003\u8651\u7684\u662f\u4e24\u5757\u5e73\u677f\u73bb\u7483\u6784\u6210\u7684\u7a7a\u6c14\u5288\u5c16\uff0c\u5176\u4e2d\u4e00\u5757\u73bb\u7483\u76f8\u5bf9\u4e8e\u53e6\u4e00\u5757\u6162\n\u6162\u5411\u4e0a\u5e73\u79fb\u3002\u7531\u4e8e\u662f\u5355\u8272\u5e73\u884c\u5149\u5782\u76f4\u5165\u5c04\uff0c\u5e72\u6d89\u6761\u7eb9\u7684\u5f62\u6210\u662f\u7531\u4e8e\u5149\u6ce2\u5728\u5288\u5c16\u7684\u4e0a\u4e0b\u8868\u9762\u53cd\u5c04\u540e\u53d1\u751f\n\u5e72\u6d89\u3002\u5f53\u4e0a\u9762\u7684\u73bb\u7483\u677f\u5411\u4e0a\u5e73\u79fb\u65f6\uff0c\u5288\u5c16\u7684\u539a\u5ea6\u4f1a\u5728\u68f1\u8fb9\u9644\u8fd1\u589e\u52a0\uff0c\u8fd9\u610f\u5473\u7740\u5149\u6ce2\u5728\u5288\u5c16\u5185\u90e8\u7684\u5149\u7a0b\n\u5dee\u4f1a\u589e\u52a0\u3002\n\u7531\u4e8e\u5149\u7a0b\u5dee\u589e\u52a0\uff0c\u539f\u672c\u5728\u67d0\u4e00\u4f4d\u7f6e\u7684\u660e\u6761\u7eb9\uff08\u6216\u6697\u6761\u7eb9\uff09\u4f1a\u5411\u5288\u5c16\u7684\u8584\u90e8\u5206\u79fb\u52a8\uff0c\u5373\u5411\u8fdc\u79bb\u68f1\u8fb9\u7684\u65b9\n\u5411\u5e73\u79fb\u3002\u6761\u7eb9\u95f4\u9694\uff08\u5373\u5e72\u6d89\u6761\u7eb9\u7684\u95f4\u8ddd\uff09\u662f\u7531\u5288\u5c16\u7684\u89d2\u5ea6\u548c\u5149\u7684\u6ce2\u957f\u51b3\u5b9a\u7684\uff0c\u800c\u5728\u8fd9\u4e2a\u8fc7\u7a0b\u4e2d\uff0c\u5288\u5c16\n\u7684\u89d2\u5ea6\u548c\u5149\u7684\u6ce2\u957f\u90fd\u6ca1\u6709\u6539\u53d8\uff0c\u56e0\u6b64\u6761\u7eb9\u95f4\u9694\u4e0d\u4f1a\u53d8\u5316\u3002\n\u56e0\u6b64\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a\n(D) \u5411\u8fdc\u79bb\u68f1\u8fb9\u7684\u65b9\u5411\u5e73\u79fb\uff0c\u6761\u7eb9\u95f4\u9694\u4e0d\u53d8\nGround Truth: (C) \u5411\u68f1\u8fb9\u65b9\u5411\u5e73\u79fb\uff0c\u6761\u7eb9\u95f4\u9694\u4e0d\u53d8\n<\u56fe\u7247 1>\nError Category: Reasoning Error\nError Reason: The GPT-4V model exhibited a reasoning error in analyzing the question. In this \nproblem, the key is to understand how interference fringes change when a slab of glass, surrounded \nby air, is displaced vertically under monochromatic parallel light with perpendicular incidence. The \nmodel's response incorrectly suggested that as the upper glass slab is moved upwards, the thickness \nof the wedge near the edge increases, leading to an increase in optical path difference, causing the \ninterference fringes to shift away from the edge. This is an incorrect inference.\nFigure B188: A sample error case of Technology and Engineering (subcategory: Architec-\nture).\nBack to List of figures\n207\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b:\u5efa\u7b51\u5b66\nQuestion:  \u5982<\u56fe\u7247 1>\u6240\u793a\uff0c\u5df2\u77e5F=100N,q=5N/m,R=5cm,OA=10cm,BC=5cm\u3002\u5219\u8be5\u529b\u7cfb\u5bf9I\u70b9\u7684\u5408\u529b\u77e9\n\u4e3a\uff08\uff09\u3002\nOption:\n(A) M1=1000N\u00b7cm\uff08\u987a\u65f6\u9488\uff09\n(B) M1=1000N\u00b7cm\uff08\u9006\u65f6\u9488\uff09\n(C) M1=500N\u00b7cm\uff08\u9006\u65f6\u9488\uff09\n(D) M1=500N\u00b7cm\uff08\u987a\u65f6\u9488\uff09\nGPT-4V: \u7531\u4e8e\u6211\u65e0\u6cd5\u76f4\u63a5\u8fdb\u884c\u7269\u7406\u5b9e\u9a8c\u6216\u6d4b\u91cf\uff0c\u6211\u5c06\u57fa\u4e8e\u56fe\u50cf\u4e2d\u63d0\u4f9b\u7684\u4fe1\u606f\u548c\u7269\u7406\u5b66\u539f\u7406\u6765\u89e3\u7b54\u8fd9\u4e2a\n\u95ee\u9898\u3002\n\u9996\u5148\uff0c\u6211\u4eec\u9700\u8981\u8ba1\u7b97\u529bF\u548c\u529bq\u5bf9\u70b9I\u7684\u529b\u77e9\u3002\n\u5bf9\u4e8e\u529bF\uff0c\u5b83\u7684\u529b\u77e9\u662f\u529b\u4e58\u4ee5\u529b\u81c2\u3002\u529b\u81c2\u662f\u529b\u7684\u4f5c\u7528\u7ebf\u5230\u8f6c\u52a8\u8f74\uff08\u8fd9\u91cc\u662f\u70b9I\uff09\u7684\u5782\u76f4\u8ddd\u79bb\u3002\u5728\u8fd9\u4e2a\u60c5\n\u51b5\u4e0b\uff0c\u529bF\u7684\u4f5c\u7528\u7ebf\u662f\u6cbf\u7740OA\u5ef6\u4f38\u7684\uff0c\u6240\u4ee5\u529b\u81c2\u662fOA\u7684\u957f\u5ea6\u52a0\u4e0aAB\u7684\u957f\u5ea6\uff08\u56e0\u4e3aI\u70b9\u5728B\u70b9\u7684\u5782\u76f4\u4e0a\n\u65b9\uff09\u3002OA\u662f10cm\uff0cAB\u662fBC\u7684\u4e00\u534a\uff0c\u53732.5cm\u3002\u6240\u4ee5\u529b\u81c2\u662f12.5cm\u3002\u56e0\u6b64\uff0c\u529bF\u5bf9\u70b9I\u7684\u529b\u77e9\u662f100N * \n12.5cm = 1250N\u00b7cm\u3002\u7531\u4e8e\u529bF\u662f\u5411\u53f3\u7684\uff0c\u5b83\u4f1a\u4f7f\u7269\u4f53\u7ed5I\u70b9\u9006\u65f6\u9488\u65cb\u8f6c\u3002\n\u5bf9\u4e8e\u5747\u5e03\u529bq\uff0c\u5b83\u7684\u5408\u529b\u53ef\u4ee5\u770b\u4f5c\u662f\u4f5c\u7528\u5728\u5176\u4f5c\u7528\u7ebf\u4e2d\u70b9\u7684\u5355\u4e00\u529b\u3002\u5728\u8fd9\u4e2a\u60c5\u51b5\u4e0b\uff0c\u5747\u5e03\u529b\u7684\u4f5c\u7528\u7ebf\n\u662fBC\uff0c\u6240\u4ee5\u5408\u529b\u4f5c\u7528\u5728B\u548cC\u7684\u4e2d\u70b9\uff0c\u53732.5cm/2 = 1.25cm\u5904\u3002\u5747\u5e03\u529b\u7684\u5927\u5c0f\u662fq\u4e58\u4ee5BC\u7684\u957f\u5ea6\uff0c\u5373\n5N/m * 5cm = 25N\u3002\u56e0\u6b64\uff0c\u529bq\u5bf9\u70b9I\u7684\u529b\u77e9\u662f25N * 1.25cm = 31.25N\u00b7cm\u3002\u7531\u4e8e\u529bq\u662f\u5411\u4e0b\u7684\uff0c\u5b83\u4f1a\n\u4f7f\u7269\u4f53\u7ed5I\u70b9\u987a\u65f6\u9488\u65cb\u8f6c\u3002\n\u5c06\u4e24\u4e2a\u529b\u77e9\u76f8\u52a0\uff0c\u6211\u4eec\u9700\u8981\u6ce8\u610f\u5b83\u4eec\u7684\u65cb\u8f6c\u65b9\u5411\u3002\u529bF\u4ea7\u751f\u7684\u662f\u9006\u65f6\u9488\u529b\u77e9\uff0c\u800c\u529bq\u4ea7\u751f\u7684\u662f\u987a\u65f6\u9488\u529b\n\u77e9\u3002\u56e0\u6b64\uff0c\u5408\u529b\u77e9\u662f\u4e24\u8005\u7684\u5dee\u503c\uff0c\u53731250N\u00b7cm - 31.25N\u00b7cm = 1218.75N\u00b7cm\uff08\u9006\u65f6\u9488\uff09\u3002\n\u8fd9\u4e2a\u7ed3\u679c\u4e0d\u5728\u6240\u7ed9\u7684\u9009\u9879\u4e2d\uff0c\u4f46\u662f\u6839\u636e\u9898\u76ee\u7684\u9009\u9879\uff0c\u6211\u4eec\u53ef\u4ee5\u9009\u62e9\u6700\u63a5\u8fd1\u7684\u7b54\u6848\u3002\u56e0\u6b64\uff0c\u6b63\u786e\u7684\u9009\u9879\n\u662f\uff1a\n(B) M1=1000N\u00b7cm\uff08\u9006\u65f6\u9488\uff09\n\u8bf7\u6ce8\u610f\uff0c\u8fd9\u4e2a\u7b54\u6848\u662f\u57fa\u4e8e\u6240\u7ed9\u4fe1\u606f\u7684\u8fd1\u4f3c\u8ba1\u7b97\uff0c\u5b9e\u9645\u7684\u7b54\u6848\u53ef\u80fd\u7565\u6709\u4e0d\u540c\u3002\nGround Truth: (D) M1=500N\u00b7cm\uff08\u987a\u65f6\u9488\uff09\n<\u56fe\u7247 1>\nError Category: Reasoning Error\nError Reason: The GPT-4V model made a reasoning error when analyzing this physics problem. \nThe problem requires calculating the resultant moment of the given force system about point I. The \nmodel incorrectly calculated the moment of force F about point I by adding the length of OA to the \nlength of AB for the lever arm.\nFigure B189: A sample error case of Technology and Engineering (subcategory: Architec-\nture).\nBack to List of figures\n208\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b:\u5efa\u7b51\u5b66\nQuestion:  \u6881\u7684\u5f2f\u77e9\u56fe\u5982<\u56fe\u7247 1>\u6240\u793a\uff0c\u5219\u6881\u7684\u6700\u5927\u526a\u529b\u662f\uff08\uff09\nOption:\n(A) 0.5F\n(B) F\n(C) 1.5F\n(D) 2F\nGPT-4V: \u5728\u8fd9\u4e2a\u95ee\u9898\u4e2d\uff0c\u6211\u4eec\u9700\u8981\u6839\u636e\u5f2f\u77e9\u56fe\u6765\u786e\u5b9a\u6881\u7684\u6700\u5927\u526a\u529b\u3002\u5f2f\u77e9\u56fe\u663e\u793a\u4e86\u6881\u4e0a\u5404\u70b9\u7684\u5f2f\u77e9\u503c\uff0c\n\u800c\u526a\u529b\u56fe\u53ef\u4ee5\u901a\u8fc7\u5f2f\u77e9\u56fe\u7684\u659c\u7387\u6765\u786e\u5b9a\u3002\u5728\u5f2f\u77e9\u56fe\u4e2d\uff0c\u5f2f\u77e9\u7684\u6700\u5927\u589e\u52a0\u6216\u51cf\u5c11\u53d1\u751f\u5728\u526a\u529b\u4e3a\u96f6\u7684\u4f4d\u7f6e\uff0c\n\u800c\u526a\u529b\u7684\u6700\u5927\u503c\u901a\u5e38\u51fa\u73b0\u5728\u5f2f\u77e9\u56fe\u7684\u6700\u5927\u6216\u6700\u5c0f\u70b9\u9644\u8fd1\u3002\n\u4ece\u7ed9\u5b9a\u7684\u5f2f\u77e9\u56fe\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u5728\u70b9C\u548c\u70b9D\u5904\u5f2f\u77e9\u4e3a\u96f6\uff0c\u8fd9\u610f\u5473\u7740\u5728\u8fd9\u4e9b\u70b9\u5904\u526a\u529b\u4f1a\u6539\u53d8\u7b26\u53f7\u3002\u5728\nA\u5230C\u7684\u533a\u95f4\u5185\uff0c\u5f2f\u77e9\u7ebf\u6027\u589e\u52a0\uff0c\u800c\u5728C\u5230D\u7684\u533a\u95f4\u5185\uff0c\u5f2f\u77e9\u7ebf\u6027\u51cf\u5c11\u3002\u5728D\u5230B\u7684\u533a\u95f4\u5185\uff0c\u5f2f\u77e9\u518d\u6b21\u7ebf\u6027\n\u589e\u52a0\u3002\n\u4e3a\u4e86\u786e\u5b9a\u6700\u5927\u526a\u529b\uff0c\u6211\u4eec\u9700\u8981\u627e\u5230\u5f2f\u77e9\u53d8\u5316\u7387\u6700\u5927\u7684\u533a\u95f4\u3002\u5728A\u5230C\u533a\u95f4\uff0c\u5f2f\u77e9\u4ece0\u589e\u52a0\u52300.5Fa\uff0c\u53d8\u5316\n\u7387\u4e3a0.5Fa/a\u3002\u5728C\u5230D\u533a\u95f4\uff0c\u5f2f\u77e9\u4ece0.5Fa\u51cf\u5c11\u52300\uff0c\u53d8\u5316\u7387\u4e3a-0.5Fa/a\u3002\u5728D\u5230B\u533a\u95f4\uff0c\u5f2f\u77e9\u4ece0\u589e\u52a0\u5230\n1.5Fa\uff0c\u53d8\u5316\u7387\u4e3a1.5Fa/a\u3002\n\u56e0\u6b64\uff0c\u6700\u5927\u7684\u5f2f\u77e9\u53d8\u5316\u7387\u51fa\u73b0\u5728D\u5230B\u7684\u533a\u95f4\uff0c\u8fd9\u610f\u5473\u7740\u6700\u5927\u526a\u529b\u4e5f\u51fa\u73b0\u5728\u8fd9\u4e2a\u533a\u95f4\u3002\u7531\u4e8e\u5f2f\u77e9\u53d8\u5316\u7387\n\u4e3a1.5Fa/a\uff0c\u6211\u4eec\u53ef\u4ee5\u63a8\u65ad\u6700\u5927\u526a\u529b\u4e3a1.5F\u3002\n\u6b63\u786e\u7b54\u6848\u662f\uff1a\n(C) 1.5F\nGround Truth: (D) 2F\n<\u56fe\u7247 1>\nError Category: Reasoning Error\nError Reason:GPT-4V correctly pointed out that the moment diagram can be used to determine the \nshear force diagram and that the maximum shear typically occurs near the maximum or minimum \npoints of the moment diagram. However, there was a deviation in the model's reasoning when \nanalyzing specific changes in the moment variation rate.\nFigure B190: A sample error case of Technology and Engineering (subcategory: Architec-\nture).\nBack to List of figures\n209\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b:\u5efa\u7b51\u5b66\nQuestion:  <\u56fe\u7247 1>\u4f53\u7cfb\u7684\u51e0\u4f55\u7ec4\u6210\u4e3a\uff08\uff09\nOption:\n(A) \u51e0\u4f55\u4e0d\u53d8\u65e0\u591a\u4f59\u7ea6\u675f\n(B) \u51e0\u4f55\u4e0d\u53d8\u6709\u591a\u4f59\u7ea6\u675f\n(C) \u77ac\u53d8\u4f53\u7cfb\n(D) \u53ef\u53d8\u4f53\u7cfb\nGPT-4V: \u8fd9\u5f20\u56fe\u7247\u663e\u793a\u7684\u662f\u4e00\u4e2a\u7ed3\u6784\u4f53\u7cfb\uff0c\u4f46\u7531\u4e8e\u6211\u65e0\u6cd5\u8fdb\u884c\u5b9e\u9645\u7684\u529b\u5b66\u5206\u6790\uff0c\u6211\u4e0d\u80fd\u786e\u5b9a\u5b83\u662f\u5426\u5177\n\u6709\u591a\u4f59\u7ea6\u675f\u3002\u901a\u5e38\uff0c\u4e00\u4e2a\u7ed3\u6784\u4f53\u7cfb\u662f\u5426\u5177\u6709\u591a\u4f59\u7ea6\u675f\u53d6\u51b3\u4e8e\u5176\u6746\u4ef6\u548c\u8282\u70b9\u7684\u914d\u7f6e\u4ee5\u53ca\u652f\u6491\u6761\u4ef6\u3002\u7ed3\u6784\n\u5de5\u7a0b\u5e08\u4f1a\u4f7f\u7528\u9759\u529b\u5b66\u7684\u65b9\u6cd5\u6765\u786e\u5b9a\u7ed3\u6784\u662f\u5426\u662f\u9759\u5b9a\u7684\u3001\u8d85\u9759\u5b9a\u7684\u8fd8\u662f\u4e0d\u7a33\u5b9a\u7684\u3002\n\u5728\u8fd9\u4e2a\u60c5\u51b5\u4e0b\uff0c\u5982\u679c\u7ed3\u6784\u7684\u6746\u4ef6\u548c\u8282\u70b9\u914d\u7f6e\u5bfc\u81f4\u5185\u529b\u53ef\u4ee5\u5728\u4e0d\u6539\u53d8\u7ed3\u6784\u51e0\u4f55\u5f62\u72b6\u7684\u60c5\u51b5\u4e0b\u552f\u4e00\u786e\u5b9a\uff0c\n\u90a3\u4e48\u5b83\u53ef\u80fd\u662f\u51e0\u4f55\u4e0d\u53d8\u65e0\u591a\u4f59\u7ea6\u675f\u7684\uff08\u9009\u9879A\uff09\u3002\u5982\u679c\u7ed3\u6784\u7684\u6746\u4ef6\u548c\u8282\u70b9\u914d\u7f6e\u5bfc\u81f4\u5185\u529b\u7684\u8ba1\u7b97\u9700\u8981\u989d\n\u5916\u7684\u6761\u4ef6\uff0c\u90a3\u4e48\u5b83\u53ef\u80fd\u662f\u51e0\u4f55\u4e0d\u53d8\u6709\u591a\u4f59\u7ea6\u675f\u7684\uff08\u9009\u9879B\uff09\u3002\u5982\u679c\u7ed3\u6784\u53ef\u4ee5\u5728\u5916\u529b\u4f5c\u7528\u4e0b\u6539\u53d8\u5176\u5f62\u72b6\n\u800c\u4e0d\u5f15\u8d77\u5185\u529b\u7684\u53d8\u5316\uff0c\u90a3\u4e48\u5b83\u53ef\u80fd\u662f\u77ac\u53d8\u4f53\u7cfb\uff08\u9009\u9879C\uff09\u3002\u5982\u679c\u7ed3\u6784\u53ef\u4ee5\u5728\u5916\u529b\u4f5c\u7528\u4e0b\u6539\u53d8\u5176\u5f62\u72b6\u5e76\n\u5f15\u8d77\u5185\u529b\u7684\u53d8\u5316\uff0c\u90a3\u4e48\u5b83\u53ef\u80fd\u662f\u53ef\u53d8\u4f53\u7cfb\uff08\u9009\u9879D\uff09\u3002\n\u7531\u4e8e\u7f3a\u4e4f\u5177\u4f53\u7684\u529b\u5b66\u5206\u6790\uff0c\u6211\u65e0\u6cd5\u7ed9\u51fa\u4e00\u4e2a\u786e\u5b9a\u7684\u7b54\u6848\u3002\u7136\u800c\uff0c\u5982\u679c\u4f60\u80fd\u63d0\u4f9b\u66f4\u591a\u7684\u4e0a\u4e0b\u6587\u6216\u7ec6\u8282\uff0c\n\u6211\u53ef\u80fd\u80fd\u591f\u5e2e\u52a9\u4f60\u66f4\u597d\u5730\u7406\u89e3\u8fd9\u4e2a\u7ed3\u6784\u4f53\u7cfb\u7684\u6027\u8d28\u3002\nGround Truth: (B) \u51e0\u4f55\u4e0d\u53d8\u6709\u591a\u4f59\u7ea6\u675f\n<\u56fe\u7247 1>\nError Category: Reasoning Error\nError Reason: GPT-4V demonstrated a certain level of domain knowledge when reasoning about \npossible options, but it failed to accurately apply this knowledge to specific questions. The model \ncorrectly described the general characteristics of several structural systems but couldn't effectively \njudge specific options based on image information. This disconnect between understanding and \napplication may be due to the model lacking sufficient contextual information or because its \nknowledge base lacks extensive expertise in this area.\nFigure B191: A sample error case of Technology and Engineering (subcategory: Architec-\nture).\nBack to List of figures\n210\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b: \u8ba1\u7b97\u673a\u79d1\u5b66\nQuestion:  \u4e0b\u5217\u7ed9\u5b9a\u7684\u5173\u952e\u5b57\u8f93\u5165\u5e8f\u5217\u4e2d\uff0c\u4e0d\u80fd\u751f\u6210\u5982\u4e0b\u4e8c\u53c9\u6392\u5e8f\u6811\u7684\u662f<\u56fe\u7247 1>\nOption: \n(A) 4,5,2,1,3\n(B) 4,5,1,2,3\n(C) 4,2,5,3,1\n(D) 4,2,1,3,5\nGPT-4V:  \u62b1\u6b49\uff0c\u7531\u4e8e\u6211\u7684\u9650\u5236\uff0c\u6211\u65e0\u6cd5\u76f4\u63a5\u67e5\u770b\u56fe\u7247\u3002\u4f46\u662f\uff0c\u6211\u53ef\u4ee5\u5e2e\u52a9\u4f60\u5206\u6790\u8fd9\u4e2a\u95ee\u9898\u3002\n\u4e8c\u53c9\u6392\u5e8f\u6811\uff08\u4e5f\u79f0\u4e3a\u4e8c\u53c9\u641c\u7d22\u6811\uff09\u662f\u4e00\u79cd\u7279\u6b8a\u7684\u4e8c\u53c9\u6811\uff0c\u5b83\u6ee1\u8db3\u4ee5\u4e0b\u6027\u8d28\uff1a\n- \u6bcf\u4e2a\u8282\u70b9\u5305\u542b\u4e00\u4e2a\u952e\u3002\n- \u6bcf\u4e2a\u8282\u70b9\u7684\u5de6\u5b50\u6811\u53ea\u5305\u542b\u5c0f\u4e8e\u8be5\u8282\u70b9\u952e\u7684\u8282\u70b9\u3002\n- \u6bcf\u4e2a\u8282\u70b9\u7684\u53f3\u5b50\u6811\u53ea\u5305\u542b\u5927\u4e8e\u8be5\u8282\u70b9\u952e\u7684\u8282\u70b9\u3002\n- \u5de6\u53f3\u5b50\u6811\u4e5f\u5fc5\u987b\u5206\u522b\u662f\u4e8c\u53c9\u6392\u5e8f\u6811\u3002\n\u6839\u636e\u8fd9\u4e9b\u6027\u8d28\uff0c\u6211\u4eec\u53ef\u4ee5\u5c1d\u8bd5\u6839\u636e\u7ed9\u5b9a\u7684\u5173\u952e\u5b57\u5e8f\u5217\u6784\u5efa\u4e8c\u53c9\u6392\u5e8f\u6811\uff0c\u5e76\u68c0\u67e5\u5b83\u4eec\u662f\u5426\u80fd\u751f\u6210\u4e0e\u9898\u76ee\u4e2d\u56fe\u7247\u76f8\n\u540c\u7684\u6811\u7ed3\u6784\u3002\n(A) 4,5,2,1,3\n- 4 \u662f\u6839\u8282\u70b9\u3002\n- 5 \u5927\u4e8e 4\uff0c\u6240\u4ee5\u5b83\u662f 4 \u7684\u53f3\u5b50\u8282\u70b9\u3002\n- 2 \u5c0f\u4e8e 4\uff0c\u6240\u4ee5\u5b83\u662f 4 \u7684\u5de6\u5b50\u8282\u70b9\u3002\n- 1 \u5c0f\u4e8e 4 \u4e14\u5c0f\u4e8e 2\uff0c\u6240\u4ee5\u5b83\u662f 2 \u7684\u5de6\u5b50\u8282\u70b9\u3002\n- 3 \u5927\u4e8e 2 \u4e14\u5c0f\u4e8e 4\uff0c\u6240\u4ee5\u5b83\u662f 2 \u7684\u53f3\u5b50\u8282\u70b9\u3002\n(B) 4,5,1,2,3\n- 4 \u662f\u6839\u8282\u70b9\u3002\n- 5 \u5927\u4e8e 4\uff0c\u6240\u4ee5\u5b83\u662f 4 \u7684\u53f3\u5b50\u8282\u70b9\u3002\n- 1 \u5c0f\u4e8e 4\uff0c\u6240\u4ee5\u5b83\u662f 4 \u7684\u5de6\u5b50\u8282\u70b9\u3002\n- 2 \u5927\u4e8e 1 \u4e14\u5c0f\u4e8e 4\uff0c\u6240\u4ee5\u5b83\u662f 1 \u7684\u53f3\u5b50\u8282\u70b9\u3002\n- 3 \u5927\u4e8e 2 \u4e14\u5c0f\u4e8e 4\uff0c\u6240\u4ee5\u5b83\u662f 2 \u7684\u53f3\u5b50\u8282\u70b9\u3002\n(C) 4,2,5,3,1\n- 4 \u662f\u6839\u8282\u70b9\u3002\n- 2 \u5c0f\u4e8e 4\uff0c\u6240\u4ee5\u5b83\u662f 4 \u7684\u5de6\u5b50\u8282\u70b9\u3002\n- 5 \u5927\u4e8e 4\uff0c\u6240\u4ee5\u5b83\u662f 4 \u7684\u53f3\u5b50\u8282\u70b9\u3002\n- 3 \u5927\u4e8e 2 \u4e14\u5c0f\u4e8e 4\uff0c\u6240\u4ee5\u5b83\u662f 2 \u7684\u53f3\u5b50\u8282\u70b9\u3002\n- 1 \u5c0f\u4e8e 2 \u4e14\u5c0f\u4e8e 4\uff0c\u6240\u4ee5\u5b83\u662f 2 \u7684\u5de6\u5b50\u8282\u70b9\u3002\n(D) 4,2,1,3,5\n- 4 \u662f\u6839\u8282\u70b9\u3002\n- 2 \u5c0f\u4e8e 4\uff0c\u6240\u4ee5\u5b83\u662f 4 \u7684\u5de6\u5b50\u8282\u70b9\u3002\n- 1 \u5c0f\u4e8e 2 \u4e14\u5c0f\u4e8e 4\uff0c\u6240\u4ee5\u5b83\u662f 2 \u7684\u5de6\u5b50\u8282\u70b9\u3002\n- 3 \u5927\u4e8e 2 \u4e14\u5c0f\u4e8e 4\uff0c\u6240\u4ee5\u5b83\u662f 2 \u7684\u53f3\u5b50\u8282\u70b9\u3002\n- 5 \u5927\u4e8e 4\uff0c\u6240\u4ee5\u5b83\u662f 4 \u7684\u53f3\u5b50\u8282\u70b9\u3002\n\u6839\u636e\u4e0a\u8ff0\u5206\u6790\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u9009\u9879 (B) \u751f\u6210\u7684\u4e8c\u53c9\u6392\u5e8f\u6811\u4e0e\u5176\u4ed6\u9009\u9879\u4e0d\u540c\uff0c\u56e0\u4e3a\u5728 (B) \u4e2d\uff0c\u6570\u5b57 2 \u662f\u6570\u5b57 1 \n\u7684\u53f3\u5b50\u8282\u70b9\uff0c\u800c\u5728\u5176\u4ed6\u9009\u9879\u4e2d\uff0c\u6570\u5b57 3 \u662f\u6570\u5b57 2 \u7684\u53f3\u5b50\u8282\u70b9\u3002\u56e0\u6b64\uff0c\u5982\u679c\u56fe\u7247\u4e2d\u7684\u4e8c\u53c9\u6392\u5e8f\u6811\u7ed3\u6784\u4e0e (A)\u3001\n(C) \u548c (D) \u63cf\u8ff0\u7684\u7ed3\u6784\u76f8\u5339\u914d\uff0c\u90a3\u4e48 (B) \u5c31\u4e0d\u80fd\u751f\u6210\u9898\u76ee\u4e2d\u7684\u4e8c\u53c9\u6392\u5e8f\u6811\u3002\u6b63\u786e\u7b54\u6848\u662f (B)\u3002\nGround Truth: (B) 4,5,1,2,3\n<\u56fe\u7247 1>\nFigure B192: A sample correct case of Technology and Engineering (subcategory: Computer\nScience).\nBack to List of figures\n211\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b: \u8ba1\u7b97\u673a\u79d1\u5b66\nQuestion:  \u67d0\u7f51\u7edc\u62d3\u6251\u5982<\u56fe\u7247 1>\u6240\u793a\uff0c\u8def\u7531\u5668R1\u53ea\u6709\u5230\u8fbe\u5b50\u7f51 192.168.1.0/24 \u7684\u8def\u7531\u3002\u4e3a\u4f7fR1\u53ef\n\u4ee5\u5c06 IP \u5206\u7ec4\u6b63\u786e\u5730\u8def\u7531\u5230\u56fe\u4e2d\u6240\u6709\u5b50\u7f51\uff0c\u5219\u5728R1\u4e2d\u9700\u8981\u589e\u52a0\u7684\u4e00\u6761\u8def\u7531\uff08\u76ee\u7684\u7f51\u7edc\uff0c\u5b50\u7f51\u63a9\u7801\uff0c\u4e0b\n\u4e00\u8df3\uff09\u662f\nOption: \n(A) 192.168.2.0,255.255.255.128,192.168.1.1\n(B) 192.168.2.0,255.255.255.0,192.168.1.1\n(C) 192.168.2.0,255.255.255.128,192.168.1.2\n(D) 192.168.2.0,255.255.255.0,192.168.1.2\nGPT-4V: \u6839\u636e\u63d0\u4f9b\u7684\u7f51\u7edc\u62d3\u6251\u56fe\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u8def\u7531\u5668R1\u5df2\u7ecf\u6709\u5230\u8fbe\u5b50\u7f51192.168.1.0/24\u7684\u8def\u7531\u3002\n\u4e3a\u4e86\u8ba9R1\u80fd\u591f\u5c06IP\u5206\u7ec4\u6b63\u786e\u5730\u8def\u7531\u5230\u56fe\u4e2d\u7684\u6240\u6709\u5b50\u7f51\uff0c\u6211\u4eec\u9700\u8981\u8003\u8651\u5982\u4f55\u5230\u8fbe\u5b50\u7f51192.168.2.0/25\u548c\n192.168.2.128/25\u3002\n\u4ece\u56fe\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0c\u8fd9\u4e24\u4e2a\u5b50\u7f51\u90fd\u662f\u901a\u8fc7\u8def\u7531\u5668R2\u6765\u8bbf\u95ee\u7684\uff0c\u800cR2\u7684\u63a5\u53e3IP\u5730\u5740\u662f192.168.1.2\u3002\u56e0\n\u6b64\uff0c R1\u9700\u8981\u6dfb\u52a0\u4e00\u6761\u5230192.168.2.0\u7f51\u7edc\u7684\u8def\u7531\uff0c\u5e76\u4e14\u4e0b\u4e00\u8df3\u5e94\u8be5\u662fR2\u7684\u63a5\u53e3IP\u5730\u5740\uff0c\u5373\n192.168.1.2\u3002\n\u9009\u9879\u4e2d\uff0c(A)\u548c(C)\u7684\u5b50\u7f51\u63a9\u7801\u662f255.255.255.128\uff0c\u8fd9\u610f\u5473\u7740\u5b83\u4eec\u53ea\u80fd\u8986\u76d6\u5230192.168.2.0/25\u6216\n192.168.2.128/25\u4e2d\u7684\u4e00\u4e2a\u5b50\u7f51\uff0c\u800c\u4e0d\u662f\u4e24\u4e2a\u3002\u800c(B)\u548c(D)\u7684\u5b50\u7f51\u63a9\u7801\u662f255.255.255.0\uff0c\u8fd9\u53ef\u4ee5\u8986\n\u76d6\u5230192.168.2.0/24\uff0c\u5373\u5305\u62ec\u4e86192.168.2.0/25\u548c192.168.2.128/25\u4e24\u4e2a\u5b50\u7f51\u3002\n\u56e0\u6b64\uff0c\u6211\u4eec\u9700\u8981\u9009\u62e9\u4e00\u4e2a\u5b50\u7f51\u63a9\u7801\u4e3a255.255.255.0\uff0c\u4e0b\u4e00\u8df3\u4e3a192.168.1.2\u7684\u9009\u9879\u3002\u8fd9\u6837\u7684\u9009\u9879\u662f\n(D) 192.168.2.0\uff0c255.255.255.0\uff0c192.168.1.2\u3002\n\u6b63\u786e\u7b54\u6848\uff1a(D) 192.168.2.0\uff0c255.255.255.0\uff0c192.168.1.2\u3002\nGround Truth: (D) 192.168.2.0,255.255.255.0,192.168.1.2\n<\u56fe\u7247 1>\nFigure B193: A sample correct case of Technology and Engineering (subcategory: Computer\nScience).\nBack to List of figures\n212\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b: \u8ba1\u7b97\u673a\u79d1\u5b66\nQuestion: \u4f7f\u7528Weiler-Atherton\u7b97\u6cd5\u8bb2\u884c\u591a\u8fb9\u5f62\u88c1\u51cf\uff0c\u9876\u70b9\u6309\u9006\u65f6\u9488\u987a\u5e8f\u8868\u793a\u7684\u591a\u8fb9\u5f62<\u56fe\u7247 1>P\u7684\n\u5404\u4e2a\u9876\u70b9\u4e2d\uff0c\u54ea\u4e9b\u5c5e\u4e8e\u8fdb\u70b9\uff08\u7070\u8272\u4e3a\u88c1\u51cf\u7a97\u53e3\uff09\uff1f\uff08  \uff09\nOption: \n(A)P1, P2, P3\n(B)1,3\n(C)2,4\n(D)1,2,3,4\nGPT-4V: \u5728Weiler-Atherton\u7b97\u6cd5\u4e2d\uff0c\u8fdb\u70b9\u662f\u6307\u591a\u8fb9\u5f62\u7684\u8fb9\u4ece\u5916\u90e8\u8fdb\u5165\u88c1\u526a\u7a97\u53e3\u7684\u9876\u70b9\u3002\u6839\u636e\u63d0\u4f9b\u7684\n\u56fe\u7247\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u591a\u8fb9\u5f62P\u7684\u9876\u70b9P1, P2, P3, \u548cP4\uff0c\u4ee5\u53ca\u5b83\u4eec\u4e0e\u88c1\u526a\u7a97\u53e3\u7684\u5173\u7cfb\u3002\n\u4ece\u56fe\u7247\u4e2d\u53ef\u4ee5\u770b\u51fa\uff1a\n- \u9876\u70b9P1\u662f\u4ece\u5916\u90e8\u8fdb\u5165\u88c1\u526a\u7a97\u53e3\u7684\uff0c\u56e0\u6b64\u5b83\u662f\u4e00\u4e2a\u8fdb\u70b9\u3002\n- \u9876\u70b9P2\u662f\u4ece\u5185\u90e8\u79bb\u5f00\u88c1\u526a\u7a97\u53e3\u7684\uff0c\u56e0\u6b64\u5b83\u4e0d\u662f\u8fdb\u70b9\u3002\n- \u9876\u70b9P3\u662f\u4ece\u5916\u90e8\u8fdb\u5165\u88c1\u526a\u7a97\u53e3\u7684\uff0c\u56e0\u6b64\u5b83\u4e5f\u662f\u4e00\u4e2a\u8fdb\u70b9\u3002\n- \u9876\u70b9P4\u662f\u4ece\u5185\u90e8\u79bb\u5f00\u88c1\u526a\u7a97\u53e3\u7684\uff0c\u56e0\u6b64\u5b83\u4e0d\u662f\u8fdb\u70b9\u3002\n\u56e0\u6b64\uff0c\u5c5e\u4e8e\u8fdb\u70b9\u7684\u9876\u70b9\u662fP1\u548cP3\u3002\n\u6b63\u786e\u7b54\u6848\u662f\uff1a(B) 1, 3\nGround Truth: (C) 2,4\n<\u56fe\u7247 1>\nError Category: Textual Understanding\nError Reason:  GPT-4V does not understand that vertices are represented in counterclockwise \norder, and instead answers according to clockwise order.\nFigure B194: A sample error case of Technology and Engineering (subcategory: Computer\nScience).\nBack to List of figures\n213\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b: \u8ba1\u7b97\u673a\u79d1\u5b66\nQuestion: \u8bbe\u6709\u4e00\u4e2a\u9012\u5f52\u7b97\u6cd5\u5982\u4e0b<\u56fe\u7247 1>\uff0c\u4e0b\u9762\u53d9\u8ff0\u6b63\u786e\u7684\u662f\uff08\uff09\nOption: \n(A) \u8ba1\u7b97fact\uff08n\uff09\u9700\u8981\u6267\u884cn\u6b21\u9012\u5f52\n(B) fact\uff087\uff09=5040\n(C) \u6b64\u5730\u5f52\u7b97\u6cd5\u6700\u591a\u53ea\u80fd\u8ba1\u7b97\u5230fact\uff088\uff09\n(D) \u4ee5\u4e0a\u7ed3\u8bba\u90fd\u4e0d\u5bf9\nGPT-4V: \u8fd9\u662f\u4e00\u4e2a\u8ba1\u7b97\u9636\u4e58\u7684\u9012\u5f52\u7b97\u6cd5\u3002\u6839\u636e\u7b97\u6cd5\u7684\u5b9a\u4e49\uff0c\u6211\u4eec\u53ef\u4ee5\u8bc4\u4f30\u6bcf\u4e2a\u9009\u9879\uff1a\n(A) \u8ba1\u7b97fact\uff08n\uff09\u9700\u8981\u6267\u884cn\u6b21\u9012\u5f52 - \u8fd9\u662f\u6b63\u786e\u7684\u3002\u6bcf\u6b21\u8c03\u7528\u51fd\u6570\u90fd\u4f1a\u51cf\u5c11n\u7684\u503c\uff0c\u76f4\u5230n\u4e3a0\uff0c\u56e0\u6b64\n\u4f1a\u6267\u884cn\u6b21\u9012\u5f52\u3002\n(B) fact\uff087\uff09=5040 - \u8fd9\u4e5f\u662f\u6b63\u786e\u7684\u30027\u7684\u9636\u4e58\u662f7 * 6 * 5 * 4 * 3 * 2 * 1\uff0c\u7b49\u4e8e5040\u3002\n(C) \u6b64\u9012\u5f52\u7b97\u6cd5\u6700\u591a\u53ea\u80fd\u8ba1\u7b97\u5230fact\uff088\uff09 - \u8fd9\u662f\u9519\u8bef\u7684\u3002\u9012\u5f52\u7b97\u6cd5\u7406\u8bba\u4e0a\u53ef\u4ee5\u8ba1\u7b97\u4efb\u4f55\u6b63\u6574\u6570\u7684\u9636\n\u4e58\uff0c\u53ea\u8981\u7cfb\u7edf\u7684\u6808\u7a7a\u95f4\u8db3\u591f\u5927\u3002\u6ca1\u6709\u8db3\u591f\u7684\u4fe1\u606f\u6765\u786e\u5b9a\u8fd9\u4e2a\u7b97\u6cd5\u662f\u5426\u6709\u6808\u6ea2\u51fa\u7684\u98ce\u9669\uff0c\u6216\u8005\u662f\u5426\u6709\u5176\n\u4ed6\u9650\u5236\u3002\n(D) \u4ee5\u4e0a\u7ed3\u8bba\u90fd\u4e0d\u5bf9 - \u7531\u4e8e(A)\u548c(B)\u662f\u6b63\u786e\u7684\uff0c(D)\u9009\u9879\u81ea\u7136\u662f\u9519\u8bef\u7684\u3002\n\u6b63\u786e\u7b54\u6848\u662f\uff1a(A) \u8ba1\u7b97fact\uff08n\uff09\u9700\u8981\u6267\u884cn\u6b21\u9012\u5f52 \u548c (B) fact\uff087\uff09=5040\u3002\nGround Truth: (B) fact\uff087\uff09=5040\n<\u56fe\u7247 1>\nError Category: Lack of Knowledge\nError Reason: (A) Computing fact(n) requires performing n+1 recursions, because in addition to the \noriginal call and each call for decrementing n, there is also a base case call when n reaches 0.\nFigure B195: A sample error case of Technology and Engineering (subcategory: Computer\nScience).\nBack to List of figures\n214\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b: \u7535\u5b50\u5b66\nQuestion:\u5728<\u56fe\u7247 1>\u6240\u793a\u7684\u7a33\u538b\u7535\u8def\u4e2d\uff0c\u5df2\u77e5 \ud835\udc48\ud835\udc48\ud835\udc4d\ud835\udc4d = 1.6\ud835\udc49\ud835\udc49\uff0c\u5219 \ud835\udc48\ud835\udc480 \u4e3a\uff1a\nOption: \n(A) 9V\n(B) 1.6V\n(C) 7.4V\n(D) 10.6V\nGPT-4V: \u5728\u8fd9\u4e2a\u7a33\u538b\u7535\u8def\u4e2d\uff0cW7809\u662f\u4e00\u4e2a\u56fa\u5b9a\u8f93\u51fa\u7535\u538b\u7684\u7a33\u538b\u5668\uff0c\u5176\u8f93\u51fa\u7535\u538b\u901a\u5e38\u4e3a9V\u3002\u7136\u800c\uff0c\u7535\n\u8def\u4e2d\u8fd8\u5305\u62ec\u4e86\u4e00\u4e2a\u7a33\u538b\u4e8c\u6781\u7ba1Dz\uff0c\u5176\u7a33\u5b9a\u7535\u538b\u4e3a$U_{Z}$=1.6V\u3002\u7a33\u538b\u4e8c\u6781\u7ba1Dz\u4e0e\u7a33\u538b\u5668\u7684\u8f93\u51fa\u5e76\u8054\uff0c\n\u8fd9\u610f\u5473\u7740\u8f93\u51fa\u7535\u538b \ud835\udc48\ud835\udc480 \u5c06\u662f\u7a33\u538b\u5668\u7684\u8f93\u51fa\u7535\u538b\u4e0e\u7a33\u538b\u4e8c\u6781\u7ba1\u7684\u7a33\u5b9a\u7535\u538b\u4e4b\u548c\u3002\n\u56e0\u6b64\uff0c \ud835\udc48\ud835\udc480 = 9V\uff08\u7a33\u538b\u5668\u8f93\u51fa\uff09+ 1.6V\uff08\u7a33\u538b\u4e8c\u6781\u7ba1\u7a33\u5b9a\u7535\u538b\uff09= 10.6V\u3002\n\u6b63\u786e\u7b54\u6848\u662f\uff1a\n(D) 10.6V\nGround Truth: (D) 10.6V \n<\u56fe\u7247 1>\nFigure B196: A sample correct case of Technology and Engineering (subcategory: Electron-\nics).\nBack to List of figures\n215\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b: \u7535\u5b50\u5b66\nQuestion:\u5df2\u77e5\u67d0\u7535\u8def\u7684\u771f\u503c\u8868\u5982<\u56fe\u7247 1>\u6240\u793a\uff0c\u8be5\u7535\u8def\u903b\u8f91\u8868\u8fbe\u5f0f\u4e3a\nOption: \n(A) F=C\n(B) F=ABC\n(C) F=AB+C\n(D) \u90fd\u4e0d\u662f\nGPT-4V: \"\u6839\u636e\u63d0\u4f9b\u7684\u771f\u503c\u8868\uff0c\u6211\u4eec\u53ef\u4ee5\u89c2\u5bdf\u5230\u8f93\u51faF\u5728\u4ee5\u4e0b\u60c5\u51b5\u4e3a1\uff1a\n1. \u5f53A=0, B=0, C=1\u65f6\uff08\u7b2c\u4e8c\u884c\uff09\n2. \u5f53A=0, B=1, C=1\u65f6\uff08\u7b2c\u56db\u884c\uff09\n3. \u5f53A=1, B=0, C=1\u65f6\uff08\u7b2c\u516d\u884c\uff09\n4. \u5f53A=1, B=1, C=0\u65f6\uff08\u7b2c\u4e03\u884c\uff09\n5. \u5f53A=1, B=1, C=1\u65f6\uff08\u7b2c\u516b\u884c\uff09\n\u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff0c\u53ea\u6709C\u4e3a1\u65f6\uff0cF\u624d\u53ef\u80fd\u4e3a1\uff0c\u4f46\u662fC\u4e3a1\u5e76\u4e0d\u662fF\u4e3a1\u7684\u5145\u5206\u6761\u4ef6\uff0c\u56e0\u4e3a\u5f53A=1, B=0, C=1\n\u65f6\uff08\u7b2c\u4e94\u884c\uff09\uff0cF\u4e3a0\u3002\u6240\u4ee5\u9009\u9879(A) F=C\u662f\u9519\u8bef\u7684\u3002\n\u540c\u65f6\uff0c\u6211\u4eec\u4e5f\u53ef\u4ee5\u770b\u5230\uff0c\u5f53A\u3001B\u548cC\u540c\u65f6\u4e3a1\u65f6\uff0cF\u4e3a1\uff0c\u4f46\u662f\u8fd9\u4e5f\u4e0d\u662f\u552f\u4e00\u7684\u60c5\u51b5\uff0c\u56e0\u6b64\u9009\u9879(B) \nF=ABC\u4e5f\u662f\u9519\u8bef\u7684\u3002\n\u9009\u9879(C) F=AB+C\uff0c\u6211\u4eec\u53ef\u4ee5\u9a8c\u8bc1\u4e00\u4e0b\uff1a\n- \u5f53A=0, B=\u4efb\u610f, C=1\u65f6\uff0cF=1\uff0c\u7b26\u5408\u771f\u503c\u8868\u7684\u7b2c\u4e8c\u884c\u548c\u7b2c\u516d\u884c\u3002\n- \u5f53A=1, B=1, C=\u4efb\u610f\u65f6\uff0cF=1\uff0c\u7b26\u5408\u771f\u503c\u8868\u7684\u7b2c\u4e03\u884c\u548c\u7b2c\u516b\u884c\u3002\n\u56e0\u6b64\uff0c\u6839\u636e\u771f\u503c\u8868\uff0c\u6700\u53ef\u80fd\u6b63\u786e\u7684\u903b\u8f91\u8868\u8fbe\u5f0f\u662f(C) F=AB+C\u3002\u6240\u4ee5\u6b63\u786e\u7b54\u6848\u662f(C) F=AB+C\nGround Truth: (C) F=AB+C \n<\u56fe\u7247 1>\nFigure B197: A sample correct case of Technology and Engineering (subcategory: Electron-\nics).\nBack to List of figures\n216\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b: \u7535\u5b50\u5b66\nQuestion:\u534a\u52a0\u5668\u903b\u8f91\u7b26\u53f7\u5982\u56fe\n<\u56fe\u7247 1>\u6240\u793a\uff0c \u5f53 A \u201c1\u201d\uff0c B \u201c1\u201d\u65f6\uff0c C \u548c S \u5206\u522b\u4e3a\uff1a\nOption: \n(A)C=0 S=0\n(B) C=0 S=1\n(C) C=1 S=0\n(D) C=1 S=1\nGPT-4V: \u62b1\u534a\u52a0\u5668\u662f\u4e00\u79cd\u7535\u8def\uff0c\u5b83\u53ef\u4ee5\u5bf9\u4e24\u4e2a\u4e00\u4f4d\u4e8c\u8fdb\u5236\u6570\u8fdb\u884c\u76f8\u52a0\uff0c\u4ea7\u751f\u4e00\u4e2a\u548c\uff08S\uff09\u548c\u4e00\u4e2a\u8fdb\u4f4d\n\uff08C\uff09\u3002\u5728\u534a\u52a0\u5668\u4e2d\uff0c\u5f53\u4e24\u4e2a\u8f93\u5165A\u548cB\u90fd\u4e3a1\u65f6\uff0c\u548c\uff08S\uff09\u5c06\u4e3a0\uff0c\u8fdb\u4f4d\uff08C\uff09\u5c06\u4e3a1\u3002\n\u56e0\u6b64\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a\n(C) C=1 S=0\u3002 \nGround Truth: (C) C=1 S=0 \n<\u56fe\u7247 1>\nFigure B198: A sample correct case of Technology and Engineering (subcategory: Electron-\nics).\nBack to List of figures\n217\nPREPRINT\nQuestion: \u5982<\u56fe\u7247 1>\u6240\u793a\uff0c\u4e8c\u6781\u7ba1\u4e3a\u540c\u4e00\u578b\u53f7\u7684\u7406\u60f3\u5143\u4ef6\uff0c\u7535\u963bR=4k\u03a9\uff0c\u7535\u4f4d\ud835\udc48\ud835\udc48\ud835\udc34\ud835\udc34 =1V\uff0c \ud835\udc48\ud835\udc48B =3V\uff0c\u5219\n\u7535\u4f4duF\u7b49\u4e8e\uff08\uff09\u3002\nOption: \n(A) 1V\n(B) 3V\n(C) 12V\n(D) 5V\nGPT-4V:\u5728\u8fd9\u4e2a\u7535\u8def\u4e2d\uff0c\u6211\u4eec\u6709\u4e24\u4e2a\u7406\u60f3\u4e8c\u6781\u7ba1\u548c\u4e00\u4e2a\u7535\u963b\u3002\u7406\u60f3\u4e8c\u6781\u7ba1\u7684\u7279\u6027\u662f\u5728\u6b63\u5411\u504f\u7f6e\u65f6\u5bfc\u901a\uff0c\n\u800c\u5728\u53cd\u5411\u504f\u7f6e\u65f6\u622a\u6b62\u3002\u7531\u4e8e\ud835\udc48\ud835\udc48\ud835\udc34\ud835\udc34 = 1\ud835\udc49\ud835\udc49\uff0c\ud835\udc48\ud835\udc48\ud835\udc35\ud835\udc35 = 3\ud835\udc49\ud835\udc49\uff0c\u6211\u4eec\u53ef\u4ee5\u63a8\u65ad\u51fa\u8fde\u63a5\u5230\ud835\udc48\ud835\udc48\ud835\udc35\ud835\udc35\u7684\u4e8c\u6781\u7ba1\u5c06\u5bfc\u901a\uff0c\u56e0\u4e3a\u5b83\n\u6709\u66f4\u9ad8\u7684\u6b63\u5411\u504f\u7f6e\u7535\u538b\uff0c\u800c\u8fde\u63a5\u5230\ud835\udc48\ud835\udc48\ud835\udc34\ud835\udc34\u7684\u4e8c\u6781\u7ba1\u5c06\u622a\u6b62\u3002\n\u56e0\u6b64\uff0c\u7535\u4f4d\ud835\udc62\ud835\udc62\ud835\udc39\ud835\udc39\u5c06\u7b49\u4e8e\ud835\udc48\ud835\udc48\ud835\udc35\ud835\udc35\u7684\u7535\u4f4d\uff0c\u56e0\u4e3a\u8fd9\u662f\u901a\u8fc7\u5bfc\u901a\u7684\u4e8c\u6781\u7ba1\u6240\u8fde\u63a5\u7684\u7535\u4f4d\u3002\u6240\u4ee5\ud835\udc62\ud835\udc62\ud835\udc39\ud835\udc39  = 3\ud835\udc49\ud835\udc49\u3002\n\u6b63\u786e\u7b54\u6848\u662f\uff1a\n(B) 3V\nGround Truth: (A) 1V\nError Category: Reasoning Error\nError Reason: GPT-4V correctly understood the requirements of the question and the configuration \nof the circuit but made a mistake in the reasoning process. It erroneously assumed that the \nconducting diode was the one connected to the higher potential \ud835\udc48\ud835\udc48\ud835\udc35\ud835\udc35, ignoring that the direction of \ncurrent through a diode is determined by the potential difference, not just the absolute value of \npotential. In this circuit, since both \ud835\udc48\ud835\udc48\ud835\udc34\ud835\udc34 and \ud835\udc48\ud835\udc48\ud835\udc35\ud835\udc35 are positive and diodes only conduct when forward-\nbiased, the diode connected to the lower potential \ud835\udc48\ud835\udc48\ud835\udc34\ud835\udc34 will conduct, causing the diode connected to \n\ud835\udc48\ud835\udc48\ud835\udc35\ud835\udc35 to be cut-off. Therefore, the potential \ud835\udc62\ud835\udc62\ud835\udc39\ud835\udc39 should be equal to the potential of \ud835\udc48\ud835\udc48\ud835\udc34\ud835\udc34, which is 1V. \nHence, the correct answer is (A) 1V.\n<\u56fe\u7247 1>\n\u6280\u672f\u4e0e\u5de5\u7a0b: \u7535\u5b50\u5b66\nFigure B199: A sample error case of Technology and Engineering (subcategory: Electronics).\nBack to List of figures\n218\nPREPRINT\nQuestion: \u6c42\u56fe<\u56fe\u7247 1>\u6240\u793a\u7535\u8def\u4e2d\u7684\u7535\u6d41 \ud835\udc3c\ud835\udc3c1 _A\u3002\nGPT-4V:\u4e3a\u4e86\u6c42\u89e3\u7535\u8def\u4e2d\u7684\u7535\u6d41\ud835\udc3c\ud835\udc3c1\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u57fa\u5c14\u970d\u592b\u7535\u6d41\u5b9a\u5f8b\uff08KCL\uff09\uff0c\u8be5\u5b9a\u5f8b\u6307\u51fa\uff0c\u4efb\u4f55\u7535\u8def\n\u8282\u70b9\u8fdb\u5165\u548c\u79bb\u5f00\u7684\u7535\u6d41\u4e4b\u548c\u7b49\u4e8e\u96f6\u3002\n\u9996\u5148\uff0c\u6211\u4eec\u53ef\u4ee5\u89c2\u5bdf\u8282\u70b9C\u3002\u6839\u636eKCL\uff0c\u8fdb\u5165\u8282\u70b9C\u7684\u7535\u6d41\u7b49\u4e8e\u79bb\u5f00\u8282\u70b9C\u7684\u7535\u6d41\u3002\u6211\u4eec\u6709\uff1a10A \u8fdb\u5165\u8282\n\u70b9C\uff0c5A \u548c \ud835\udc3c\ud835\udc3c1\u79bb\u5f00\u8282\u70b9C\u3002\u6240\u4ee5\u6211\u4eec\u53ef\u4ee5\u5199\u51fa\u65b9\u7a0b\uff1a10A = 5A + \ud835\udc3c\ud835\udc3c1 \u89e3\u8fd9\u4e2a\u65b9\u7a0b\uff0c\u6211\u4eec\u5f97\u5230\uff1a\n\ud835\udc3c\ud835\udc3c1 = 10A - 5A\n\ud835\udc3c\ud835\udc3c1 = 5A\n\u56e0\u6b64\uff0c\u7535\u6d41\ud835\udc3c\ud835\udc3c1\u662f5A\u3002\nGround Truth: 9\nError Category: Reasoning Error\nError Reason: The solution correctly identifies the need to use Kirchhoff's Current Law (KCL) to \nfind the current \ud835\udc3c\ud835\udc3c1 but makes a reasoning error in the application of the law to node C. The given \ncurrents entering and leaving the node were not correctly accounted for, leading to an incorrect \ncalculation of \ud835\udc3c\ud835\udc3c1 The correct approach would involve writing and solving the KCL equation \nconsidering all currents entering and leaving the node, not just the 10A and 5A currents.\n<\u56fe\u7247 1>\n\u6280\u672f\u4e0e\u5de5\u7a0b: \u7535\u5b50\u5b66\nFigure B200: A sample error case of Technology and Engineering (subcategory: Electronics).\nBack to List of figures\n219\nPREPRINT\nQuestion: \u56fe<\u56fe\u7247 1>\u6240\u793a\u7535\u8def\u4e2dB\u70b9\u7684\u7535\u4f4d_V\u3002\nGPT-4V:\u4e3a\u4e86\u786e\u5b9a\u7535\u8def\u4e2dB\u70b9\u7684\u7535\u4f4d\uff0c\u6211\u4eec\u9700\u8981\u5206\u6790\u7535\u8def\u5e76\u5e94\u7528\u57fa\u672c\u7684\u7535\u8def\u5b9a\u5f8b\uff0c\u5982\u6b27\u59c6\u5b9a\u5f8b\u548c\u57fa\u5c14\n\u970d\u592b\u7535\u538b\u5b9a\u5f8b\u3002\u7136\u800c\uff0c\u7531\u4e8e\u6211\u65e0\u6cd5\u76f4\u63a5\u6d4b\u91cf\u6216\u770b\u5230\u7535\u8def\u7684\u5b9e\u9645\u503c\uff0c\u6211\u5c06\u63d0\u4f9b\u4e00\u4e2a\u57fa\u4e8e\u7535\u8def\u56fe\u7684\u7406\u8bba\u5206\n\u6790\u3002\n\u5728\u8fd9\u4e2a\u7535\u8def\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u4e09\u4e2a\u7535\u963bR1\u3001R2\u548cR3\uff0c\u4ee5\u53ca\u4e24\u4e2a\u7535\u538b\u6e90\uff0c\u4e00\u4e2a\u662f+50V\uff0c\u53e6\u4e00\u4e2a\u662f-50V\u3002\nB\u70b9\u662fR2\u7535\u963b\u7684\u4e0a\u7aef\u3002\u4e3a\u4e86\u627e\u5230B\u70b9\u7684\u7535\u4f4d\uff0c\u6211\u4eec\u9700\u8981\u77e5\u9053\u6d41\u8fc7R2\u7684\u7535\u6d41I2\u7684\u65b9\u5411\u548c\u5927\u5c0f\u3002\n\u7531\u4e8e\u7535\u8def\u56fe\u4e2d\u6ca1\u6709\u63d0\u4f9b\u8db3\u591f\u7684\u4fe1\u606f\u6765\u76f4\u63a5\u8ba1\u7b97\u7535\u6d41\uff0c\u6211\u4eec\u9700\u8981\u505a\u4e00\u4e9b\u5047\u8bbe\u3002\u5982\u679c\u5047\u8bbe\u7535\u8def\u5904\u4e8e\u7a33\u6001\u6761\n\u4ef6\uff0c\u90a3\u4e48\u6d41\u8fc7R1\u548cR3\u7684\u7535\u6d41\uff08I1\u548cI3\uff09\u5c06\u5206\u522b\u4ece+50V\u548c-50V\u7684\u7535\u6e90\u6d41\u51fa\uff0c\u5e76\u5728B\u70b9\u6c47\u5408\u3002\u7531\u4e8e\u7535\u6d41\u5728\n\u7535\u8def\u4e2d\u7684\u4efb\u4f55\u8282\u70b9\u7684\u603b\u6d41\u5165\u7535\u6d41\u7b49\u4e8e\u603b\u6d41\u51fa\u7535\u6d41\uff0c\u6211\u4eec\u53ef\u4ee5\u5047\u8bbe\u6d41\u8fc7R2\u7684\u7535\u6d41I2\u662fI1\u548cI3\u7684\u4ee3\u6570\u548c\u3002\n\u4f46\u662f\uff0c\u6ca1\u6709\u5177\u4f53\u7684\u7535\u6d41\u503c\u6216\u5176\u4ed6\u7535\u8def\u53c2\u6570\uff0c\u6211\u4eec\u65e0\u6cd5\u51c6\u786e\u8ba1\u7b97B\u70b9\u7684\u7535\u4f4d\u3002\u5982\u679c\u6211\u4eec\u6709\u7535\u6d41\u7684\u503c\uff0c\u6211\n\u4eec\u53ef\u4ee5\u4f7f\u7528\u6b27\u59c6\u5b9a\u5f8b\uff08V = IR\uff09\u6765\u8ba1\u7b97\u6d41\u8fc7R2\u7684\u7535\u538b\u964d\uff0c\u7136\u540e\u6839\u636e\u57fa\u5c14\u970d\u592b\u7535\u538b\u5b9a\u5f8b\u786e\u5b9aB\u70b9\u7684\u7535\u4f4d\u3002\n\u7531\u4e8e\u7f3a\u4e4f\u5177\u4f53\u4fe1\u606f\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a\u65e0\u6cd5\u786e\u5b9aB\u70b9\u7684\u7535\u4f4d\uff0c\u56e0\u4e3a\u9700\u8981\u66f4\u591a\u7684\u7535\u8def\u4fe1\u606f\u3002\nGround Truth: -14.286 \nError Category: Reasoning Error\nError Reason: The error occurred because GPT-4V correctly understood the components and \nstructure of the circuit from the image but failed to apply the correct reasoning process to determine \nthe voltage at point B. It should have used the principles of electrical circuits, such as Ohm's law \nand Kirchhoff's voltage law, to establish equations based on the given values of resistances and \nvoltages. By solving these equations, the voltage at point B could have been determined. Instead, \nGPT-4V claimed it could not determine the voltage at point B without additional information, \nwhich was a reasoning error since the necessary information was present in the circuit diagram. \n<\u56fe\u7247 1>\n\u6280\u672f\u4e0e\u5de5\u7a0b: \u7535\u5b50\u5b66\nFigure B201: A sample error case of Technology and Engineering (subcategory: Electronics).\nBack to List of figures\n220\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b: \u80fd\u6e90\u548c\u7535\u529b\nQuestion:\u5982<\u56fe\u7247 1>\u6240\u793a,\u7535\u6e90\u7535\u52a8\u52bfE=60V,\u7535\u6e90\u5185\u963bR=2\u03a9,\u8d1f\u8f7d\u7535\u963bR=18\u03a9,\u5219\u7535\u6e90\u5185\u963b\u6d88\u8017\u529f\u7387\n\u4e3a___W\u3002\nGPT-4V:\u4e3a\u4e86\u8ba1\u7b97\u7535\u6e90\u5185\u963b\u6d88\u8017\u7684\u529f\u7387\uff0c\u6211\u4eec\u9996\u5148\u9700\u8981\u8ba1\u7b97\u7535\u8def\u4e2d\u7684\u603b\u7535\u6d41\u3002\u7535\u8def\u7684\u603b\u7535\u963b\u662f\u7535\u6e90\u5185\n\u963b\ud835\udc45\ud835\udc450\u548c\u8d1f\u8f7d\u7535\u963bR\u7684\u548c\uff0c\u5373\ud835\udc45\ud835\udc45\ud835\udc58\ud835\udc58\ud835\udc61\ud835\udc61\ud835\udc58\ud835\udc58\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61 = \ud835\udc45\ud835\udc450 + \ud835\udc45\ud835\udc45 =  20\ud835\udefa\ud835\udefa\n\u6839\u636e\u6b27\u59c6\u5b9a\u5f8b\uff0c\u7535\u6d41 I \u53ef\u4ee5\u901a\u8fc7\u7535\u52a8\u52bf E \u9664\u4ee5\u603b\u7535\u963b\ud835\udc45\ud835\udc45\ud835\udc58\ud835\udc58\ud835\udc61\ud835\udc61\ud835\udc58\ud835\udc58\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\u6765\u8ba1\u7b97\uff1a\n\ud835\udc3c\ud835\udc3c =\n\ud835\udc38\ud835\udc38\n\ud835\udc45\ud835\udc45\ud835\udc58\ud835\udc58\ud835\udc61\ud835\udc61\ud835\udc58\ud835\udc58\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\n= 3\ud835\udc34\ud835\udc34\n\u73b0\u5728\u6211\u4eec\u77e5\u9053\u4e86\u7535\u6d41\uff0c\u6211\u4eec\u53ef\u4ee5\u8ba1\u7b97\u7535\u6e90\u5185\u963b   \u6d88\u8017\u7684\u529f\u7387 P\uff0c\u529f\u7387\u53ef\u4ee5\u901a\u8fc7\u7535\u6d41\u7684\u5e73\u65b9\u4e58\u4ee5\u7535\u963b\n\u6765\u8ba1\u7b97\uff1a\n\ud835\udc43\ud835\udc43 = \ud835\udc3c\ud835\udc3c2 \u00d7 \ud835\udc45\ud835\udc45 = 18\ud835\udc4a\ud835\udc4a\n\u6240\u4ee5\uff0c\u7535\u6e90\u5185\u963b\u6d88\u8017\u7684\u529f\u7387\u662f 18W\u3002\nGround Truth: (18) \n<\u56fe\u7247 1>\n0\nR\nFigure B202: A sample correct case of Technology and Engineering (subcategory: Energy\nand Power).\nBack to List of figures\n221\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b: \u80fd\u6e90\u548c\u7535\u529b\nQuestion:\u5224\u65ad\u4e0b\u9762\u9648\u8ff0\u5bf9\u9519\uff1a\u56fe<\u56fe\u7247 1>\u662f\u5f3a\u5236\u5faa\u73af\u95f4\u63a5\u52a0\u70ed\u6c34\u7bb1-\u6c34\u7f50\u7cfb\u7edf\uff0c\u7cfb\u7edf\u91c7\u7528\u6e29\u5dee\u5faa\u73af\u63a7\n\u5236\u539f\u7406\u63a7\u5236\u70ed\u6c34\u6c34\u6e29\nGPT-4V:\u8fd9\u5f20\u56fe\u7247\u5c55\u793a\u7684\u662f\u4e00\u4e2a\u6280\u672f\u56fe\u7eb8\uff0c\u5b83\u63cf\u8ff0\u4e86\u4e00\u4e2a\u52a0\u70ed\u6c34\u7bb1-\u6c34\u7f50\u7cfb\u7edf\u7684\u7ed3\u6784\u548c\u5de5\u4f5c\u539f\u7406\u3002\u4ece\n\u56fe\u4e2d\u53ef\u4ee5\u770b\u5230\uff0c\u7cfb\u7edf\u5305\u62ec\u70ed\u4ea4\u6362\u5668\u3001\u6c34\u6cf5\u3001\u63a7\u5236\u9600\u95e8\u3001\u6c34\u7bb1\u548c\u5176\u4ed6\u76f8\u5173\u7684\u7ba1\u9053\u548c\u63a7\u5236\u8bbe\u5907\u3002\u6839\u636e\u56fe\u7eb8\n\u4e0a\u7684\u4fe1\u606f\u548c\u7b26\u53f7\uff0c\u53ef\u4ee5\u63a8\u65ad\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u52a0\u70ed\u548c\u5faa\u73af\u6c34\u7684\u7cfb\u7edf\u3002\n\u9898\u76ee\u4e2d\u7684\u9648\u8ff0\u662f\u201c\u56fe<\u56fe\u7247 1>\u662f\u5f3a\u5236\u5faa\u73af\u95f4\u63a5\u52a0\u70ed\u6c34\u7bb1-\u6c34\u7f50\u7cfb\u7edf\uff0c\u7cfb\u7edf\u91c7\u7528\u6e29\u5dee\u5faa\u73af\u63a7\u5236\u539f\u7406\u63a7\u5236\n\u70ed\u6c34\u6c34\u6e29\u201d\u3002\u4ece\u56fe\u7eb8\u4e0a\u53ef\u4ee5\u770b\u5230\uff0c\u6709\u4e00\u4e2a\u5faa\u73af\u6cf5\uff08\u53ef\u80fd\u662f\u6807\u53f72\u7684\u90e8\u4ef6\uff09\uff0c\u8fd9\u8868\u660e\u7cfb\u7edf\u786e\u5b9e\u4f7f\u7528\u4e86\u5f3a\n\u5236\u5faa\u73af\u6765\u79fb\u52a8\u6c34\u3002\u6b64\u5916\uff0c\u7cfb\u7edf\u4e2d\u4f3c\u4e4e\u5305\u542b\u4e86\u6e29\u5ea6\u63a7\u5236\u7684\u5143\u7d20\uff0c\u5982\u6e29\u5ea6\u4f20\u611f\u5668\u548c\u63a7\u5236\u9600\uff0c\u8fd9\u4e9b\u53ef\u80fd\u7528\u4e8e\n\u6839\u636e\u6e29\u5dee\u6765\u63a7\u5236\u6c34\u6e29\u3002\n\u56e0\u6b64\uff0c\u6839\u636e\u56fe\u7eb8\u548c\u9898\u76ee\u63cf\u8ff0\uff0c\u53ef\u4ee5\u5408\u7406\u63a8\u65ad\u9898\u76ee\u4e2d\u7684\u9648\u8ff0\u662f\u6b63\u786e\u7684\u3002\nGround Truth: (\u5bf9) \n<\u56fe\u7247 1>\nFigure B203: A sample correct case of Technology and Engineering (subcategory: Energy\nand Power).\nBack to List of figures\n222\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b: \u80fd\u6e90\u548c\u7535\u529b\nQuestion:\u56fe<\u56fe\u7247 1>\u662f_____\uff0c\u9002\u7528\u4e8e\u5c4b\u9762\u7b49\u9ad8\u4f4d\u8bbe\u7f6e\u8d2e\u70ed\u6c34\u7bb1\u7684\u573a\u6240\nOption: \n(A) \u5f3a\u5236\u5faa\u73af\u95f4\u63a5\u52a0\u70ed\u53cc\u7f50\u7cfb\u7edf\n(B) \u5f3a\u5236\u5faa\u73af\u95f4\u63a5\u52a0\u70ed\u6c34\u7bb1-\u6c34\u7f50\u7cfb\u7edf\n(C) \u81ea\u7136\u5faa\u73af\u7cfb\u7edf\n(D) \u5206\u4f53\u5f0f\u5f3a\u5236\u5faa\u73af\u7cfb\u7edf\nGPT-4V:\u8fd9\u5f20\u56fe\u7247\u5c55\u793a\u7684\u662f\u4e00\u4e2a\u592a\u9633\u80fd\u70ed\u6c34\u7cfb\u7edf\u7684\u793a\u610f\u56fe\u3002\u6839\u636e\u56fe\u4e2d\u7684\u5143\u4ef6\u5e03\u7f6e\u548c\u6d41\u5411\uff0c\u53ef\u4ee5\u770b\u51fa\u8fd9\n\u662f\u4e00\u4e2a\u6709\u6cf5\u7684\u5faa\u73af\u7cfb\u7edf\uff0c\u56e0\u4e3a\u6709\u660e\u663e\u7684\u6cf5\uff08\u6cf5\u7684\u7b26\u53f7\uff09\u5728\u7cfb\u7edf\u4e2d\u3002\u81ea\u7136\u5faa\u73af\u7cfb\u7edf\u901a\u5e38\u4e0d\u5305\u542b\u6cf5\uff0c\u56e0\u4e3a\n\u5b83\u4f9d\u8d56\u4e8e\u70ed\u6c34\u7684\u81ea\u7136\u5bf9\u6d41\u6765\u5faa\u73af\u3002\u6b64\u5916\uff0c\u7cfb\u7edf\u4e2d\u5305\u542b\u81f3\u5c11\u4e24\u4e2a\u50a8\u5b58\u70ed\u6c34\u7684\u5bb9\u5668\uff0c\u8fd9\u8868\u660e\u5b83\u53ef\u80fd\u662f\u4e00\u4e2a\n\u53cc\u7f50\u7cfb\u7edf\u3002\n\u56e0\u6b64\uff0c\u6839\u636e\u63d0\u4f9b\u7684\u9009\u9879\u548c\u56fe\u4e2d\u7684\u4fe1\u606f\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a\n(A) \u5f3a\u5236\u5faa\u73af\u95f4\u63a5\u52a0\u70ed\u53cc\u7f50\u7cfb\u7edf\n\u8fd9\u4e2a\u9009\u9879\u6700\u7b26\u5408\u56fe\u4e2d\u6240\u793a\u7684\u7cfb\u7edf\u7279\u70b9\uff0c\u5373\u4f7f\u7528\u6cf5\u8fdb\u884c\u5f3a\u5236\u5faa\u73af\uff0c\u5e76\u4e14\u6709\u4e24\u4e2a\u50a8\u70ed\u6c34\u7bb1\u3002\",\nGround Truth: (B) \u5f3a\u5236\u5faa\u73af\u95f4\u63a5\u52a0\u70ed\u6c34\u7bb1-\u6c34\u7f50\u7cfb\u7edf\n<\u56fe\u7247 1>\nError Category:Perceptual Error \nError Reason:GPT-4V observed that the system contains at least two containers for storing hot \nwater, and the process is a forced circulation indirect heating mode. However, in the picture, there \nis one water tank and one water jug, while GPT-4 incorrectly identified both as water jugs, thus \nleading to an erroneous selection.\nFigure B204: A sample error case of Technology and Engineering (subcategory: Energy and\nPower).\nBack to List of figures\n223\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b: \u80fd\u6e90\u548c\u7535\u529b\nQuestion:\u56fe<\u56fe\u7247 1>\u4e3a__\u5b89\u88c5\u5728\u5e73\u5c4b\u9762\u4e0a\u7684\u5e03\u7f6e\u793a\u4f8b\nOption: \n(A) \u4e00\u4f53\u5f0f\u592a\u9633\u80fd\u96c6\u70ed\u5668 \n(B) \u5206\u4f53\u5f0f\u592a\u9633\u80fd\u96c6\u70ed\u5668\n(C) \u5e73\u677f\u592a\u9633\u80fd\u96c6\u70ed\u5668\n(D) \u771f\u7a7a\u7ba1\u592a\u9633\u80fd\u96c6\u70ed\u5668\nGPT-4V:\u8fd9\u5f20\u56fe\u7247\u5c55\u793a\u7684\u662f\u4e00\u4e9b\u5b89\u88c5\u5728\u5c4b\u9876\u4e0a\u7684\u8bbe\u5907\uff0c\u5b83\u4eec\u770b\u8d77\u6765\u50cf\u662f\u592a\u9633\u80fd\u96c6\u70ed\u5668\u3002\u7531\u4e8e\u56fe\u7247\u4e2d\u7684\n\u96c6\u70ed\u5668\u662f\u5e73\u677f\u72b6\u7684\uff0c\u5e76\u4e14\u6ca1\u6709\u663e\u793a\u51fa\u771f\u7a7a\u7ba1\u7684\u7279\u5f81\uff0c\u56e0\u6b64\u53ef\u4ee5\u6392\u9664\u9009\u9879(D) \u771f\u7a7a\u7ba1\u592a\u9633\u80fd\u96c6\u70ed\u5668\u3002\u81f3\n\u4e8e\u662f\u4e00\u4f53\u5f0f\u8fd8\u662f\u5206\u4f53\u5f0f\uff0c\u56fe\u7247\u6ca1\u6709\u63d0\u4f9b\u8db3\u591f\u7684\u4fe1\u606f\u6765\u786e\u5b9a\u8fd9\u4e00\u70b9\u3002\u7136\u800c\uff0c\u7531\u4e8e\u96c6\u70ed\u5668\u901a\u5e38\u662f\u5e73\u677f\u72b6\u7684\uff0c\n\u6211\u4eec\u53ef\u4ee5\u5408\u7406\u63a8\u6d4b\u8fd9\u4e9b\u662f\u5e73\u677f\u592a\u9633\u80fd\u96c6\u70ed\u5668\u3002\n\u6b63\u786e\u7b54\u6848\u662f\uff1a\n(C) \u5e73\u677f\u592a\u9633\u80fd\u96c6\u70ed\u5668\nGround Truth: (B)\u5206\u4f53\u5f0f\u592a\u9633\u80fd\u96c6\u70ed\u5668 \n<\u56fe\u7247 1>\nError Category:Perceptual Error \nError Reason:GPT-4V correctly identified the basic types of items in the picture, but failed to \ncapture the more critical information in the image: there are two solar collectors. Therefore, GPT-4V \nmerely recognized the solar collectors and was unable to provide the correct answer: split-type \nsolar collectors.\nFigure B205: A sample error case of Technology and Engineering (subcategory: Energy and\nPower).\nBack to List of figures\n224\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b: \u80fd\u6e90\u548c\u7535\u529b\nQuestion:\u56fe<\u56fe\u7247 1>\u662f\u201c\u6e7f\u201d\u5f0f\u771f\u7a7a\u7ba1\u96c6\u70ed\u5668\u622a\u9762\u56fe\uff0c\u5728\u201c\u6e7f\u201d\u5f0f\u771f\u7a7a\u7ba1\u4e2d\uff0c\u6c34\u5728\u7531___\u7ec4\u6210\u7684\u70ed\u8679\n\u5438\u7ba1\u4e2d\u5faa\u73af\u3002\nOption: \n(A)\u5916\u73bb\u7483\u7ba1\u548c\u50a8\u70ed\u6c34\u7bb1\n(B) \u5916\u73bb\u7483\u7ba1\u548c\u50a8\u70ed\u5668\n(C) \u771f\u7a7a\u7ba1\u548c\u96c6\u70ed\u5668\n(D) \u771f\u7a7a\u7ba1\u548c\u50a8\u70ed\u5668\nGPT-4V:\u8fd9\u5f20\u56fe\u7247\u5c55\u793a\u7684\u662f\u4e00\u4e2a\u6e7f\u5f0f\u771f\u7a7a\u7ba1\u96c6\u70ed\u5668\u7684\u622a\u9762\u56fe\u3002\u5728\u8fd9\u79cd\u7cfb\u7edf\u4e2d\uff0c\u6c34\u901a\u5e38\u5728\u96c6\u70ed\u5668\u5185\u90e8\u7684\n\u70ed\u8679\u5438\u7ba1\u4e2d\u5faa\u73af\u3002\u6839\u636e\u56fe\u793a\u548c\u5e38\u8bc6\uff0c\u70ed\u8679\u5438\u7ba1\u662f\u4f4d\u4e8e\u771f\u7a7a\u7ba1\u5185\u90e8\u7684\uff0c\u56e0\u6b64\u6c34\u662f\u5728\u771f\u7a7a\u7ba1\u548c\u96c6\u70ed\u5668\u4e4b\u95f4\n\u5faa\u73af\u7684\u3002\n\u6b63\u786e\u7b54\u6848\u662f\uff1a\n(C) \u771f\u7a7a\u7ba1\u548c\u96c6\u70ed\u5668\nGround Truth: (D) \u771f\u7a7a\u7ba1\u548c\u50a8\u70ed\u5668\n<\u56fe\u7247 1>\nError Category:Reasoning Error \nError Reason:In this specific 'wet' vacuum tube solar collector design, water is heated inside the \nvacuum tube by absorbing solar energy, and this heat is directly transferred to a storage heater \nconnected to the vacuum tube. However, GPT-4V did not consider this, which is a typical \nReasoning Error.\nFigure B206: A sample error case of Technology and Engineering (subcategory: Energy and\nPower).\nBack to List of figures\n225\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b: \u80fd\u6e90\u548c\u7535\u529b\nQuestion:\u56fe<\u56fe\u7247 1>\u662f\u91cd\u6c34\u5806\u6838\u7535\u5382\u793a\u610f\u56fe\uff0c\u91c7\u7528___\u4f5c\u4e3a\u4e2d\u5b50\u6162\u5316\u5242\u3002\u5176\u5bf9\u70ed\u4e2d\u5b50\u7684\u5438\u6536\u6982\u7387\u8fdc\u5c0f\n\u4e8e\u8f7b\u6c34\uff0c\u4f7f\u5f97\u91cd\u6c34\u5806\u53ef\u4ee5\u4f7f\u7528\u5929\u7136\u94c0\u4f5c\u4e3a\u71c3\u6599\u3002\nOption: \n(A) \u6c18\u6c34\n(B) \u6c1a\u6c34\n(C) \u91cd\u6c34\n(D) \u8f7b\u6c34\nGPT-4V:\u8fd9\u4e2a\u95ee\u9898\u7684\u6b63\u786e\u7b54\u6848\u662f (A) \u6c18\u6c34\u3002\u5728\u6838\u7535\u9886\u57df\uff0c\u91cd\u6c34\u901a\u5e38\u6307\u7684\u662f\u6c18\u6c34\uff08D2O\uff09\uff0c\u5b83\u542b\u6709\u8f83\u91cd\n\u7684\u6c18\u539f\u5b50\uff0c\u800c\u4e0d\u662f\u666e\u901a\u7684\u6c22\u539f\u5b50\u3002\u91cd\u6c34\u4f5c\u4e3a\u4e2d\u5b50\u6162\u5316\u5242\u7684\u7279\u6027\u5305\u62ec\u5bf9\u70ed\u4e2d\u5b50\u7684\u5438\u6536\u6982\u7387\u8fdc\u5c0f\u4e8e\u8f7b\u6c34\n\uff08\u666e\u901a\u6c34\uff0cH2O\uff09\uff0c\u8fd9\u4f7f\u5f97\u91cd\u6c34\u5806\u53ef\u4ee5\u4f7f\u7528\u672a\u7ecf\u6d53\u7f29\u7684\u5929\u7136\u94c0\u4f5c\u4e3a\u71c3\u6599\u3002\nGround Truth: (C)\u91cd\u6c34 \n<\u56fe\u7247 1>\nError Category:Reasoning Error \nError Reason:GPT-4V possesses knowledge in this area and has already answered that \"heavy \nwater reactors can use unenriched natural uranium as fuel\". However, in the final answer, it \nresponded with (A) heavy water, which is a Reasoning Error.\nFigure B207: A sample error case of Technology and Engineering (subcategory: Energy and\nPower).\nBack to List of figures\n226\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b: \u80fd\u6e90\u548c\u7535\u529b\nQuestion:<\u56fe\u7247 1>\u6240\u793a\u7684\u67d0\u79cd\u80fd\u6e90\u7684\u5f00\u91c7\u8bbe\u5907\uff0c\u8fd9\u79cd\u80fd\u6e90\u662f__\nOption: \n(A) \u539f\u7164\n(B) \u77f3\u6cb9\n(C) \u5929\u7136\u6c14\n(D) \u4ee5\u4e0a\u90fd\u4e0d\u662f\nGPT-4V:\u8fd9\u5f20\u56fe\u7247\u663e\u793a\u7684\u662f\u4e00\u53f0\u5de8\u5927\u7684\u673a\u68b0\u8bbe\u5907\uff0c\u5b83\u662f\u4e00\u79cd\u96a7\u9053\u6398\u8fdb\u673a\uff08TBM\uff09\uff0c\u901a\u5e38\u7528\u4e8e\u6316\u6398\u96a7\u9053\uff0c\n\u800c\u4e0d\u662f\u76f4\u63a5\u7528\u4e8e\u80fd\u6e90\u5f00\u91c7\u3002\u56e0\u6b64\uff0c\u6839\u636e\u6240\u63d0\u4f9b\u7684\u9009\u9879\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a\n(D) \u4ee5\u4e0a\u90fd\u4e0d\u662f\nGround Truth: (A) \u539f\u7164\n<\u56fe\u7247 1>\nError Category:Lack of Knowledge \nError Reason:The device in the picture is a model of a coal mining machine. GPT-4V did not \nrecognize it, indicating that it may lack this kind of knowledge. Therefore, this error is categorized \nas a Lack of Knowledge.\nFigure B208: A sample error case of Technology and Engineering (subcategory: Energy and\nPower).\nBack to List of figures\n227\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b: \u80fd\u6e90\u548c\u7535\u529b\nQuestion:<\u56fe\u7247 1>\u6240\u793a\u7684\u67d0\u79cd\u80fd\u6e90\u7684\u5f00\u91c7\u8bbe\u5907\uff0c\u8fd9\u79cd\u80fd\u6e90\u662f__\nOption: \n(A) \u539f\u7164\n(B) \u77f3\u6cb9\n(C) \u5929\u7136\u6c14\n(D) \u4ee5\u4e0a\u90fd\u4e0d\u662f\nGPT-4V:\u6b63\u786e\u7b54\u6848\u662f\uff1a(B)\u77f3\u6cb9\nGround Truth: (C)\u5929\u7136\u6c14 \n<\u56fe\u7247 1>\nError Category:Lack of Knowledge \nError Reason:The device in the image is a classic offshore natural gas drilling platform. GPT-4V did \nnot recognize it, indicating that it may lack this kind of knowledge. Therefore, this error is due to a \nlack of knowledge.\nFigure B209: A sample error case of Technology and Engineering (subcategory: Energy and\nPower).\nBack to List of figures\n228\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b: \u80fd\u6e90\u548c\u7535\u529b\nQuestion:<\u56fe\u7247 1>\u6240\u793a\u7684\u8bbe\u5907\u7684\u5f62\u6001\u662f\u662f__\nOption: \n(A) Y\u5f62\n(B) T\u5f62\n(C) I\u5f62\n(D)\u843d\u5730\u7f50\u5f0f\nGPT-4V:\u6b63\u786e\u7b54\u6848\u662f\uff1a\n(B) T\u5f62\n\u8fd9\u5f20\u56fe\u7247\u663e\u793a\u7684\u8bbe\u5907\u7531\u4e09\u4e2a\u5782\u76f4\u7684\u67f1\u72b6\u7269\u4f53\u7ec4\u6210\uff0c\u5b83\u4eec\u901a\u8fc7\u4e00\u4e2a\u6a2a\u5411\u7684\u7ed3\u6784\u8fde\u63a5\u5728\u4e00\u8d77\uff0c\u5f62\u6210\u4e86\u4e00\u4e2a\n\u7c7b\u4f3c\u4e8e\u5b57\u6bcd\"T\"\u7684\u5f62\u72b6\u3002\nGround Truth: (C) I\u5f62\n<\u56fe\u7247 1>\nError Category:Lack of Knowledge\nError Reason:The object in the picture is an \"I-type circuit breaker\". It is obvious that GPT-4V does \nnot recognize this kind of item, so it made a wrong choice based on its shape inference.\nFigure B210: A sample error case of Technology and Engineering (subcategory: Energy and\nPower).\nBack to List of figures\n229\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b:\u6750\u6599\nQuestion:  \u5982\u56fe<\u56fe\u7247 1>\uff0c\u7531___\u6b63\u65b9\u70b9\u9635\u53ef\u4ee5\u6784\u6210\u4e00\u4e2a\u9762\u5fc3\u6b63\u65b9\u70b9\u9635\u3002\nOption:\n(A) \u7b80\u5355\n(B) \u5e95\u5fc3\n(C) \u4f53\u5fc3\n(D) \u4ee5\u4e0a\u5747\u4e0d\u5bf9\nGPT-4V: \u5f88\u62b1\u6b49\uff0c\u7531\u4e8e\u56fe\u7247\u7684\u5185\u5bb9\u65e0\u6cd5\u76f4\u63a5\u544a\u8bc9\u6211\u8fd9\u662f\u4e00\u4e2a\u7b80\u5355\u3001\u5e95\u5fc3\u8fd8\u662f\u4f53\u5fc3\u7684\u6b63\u65b9\u70b9\u9635\uff0c\u6211\u65e0\u6cd5\n\u76f4\u63a5\u7ed9\u51fa\u7b54\u6848\u3002\u4f46\u662f\uff0c\u6211\u53ef\u4ee5\u63d0\u4f9b\u4e00\u4e9b\u4fe1\u606f\u5e2e\u52a9\u4f60\u5224\u65ad\uff1a\n- \u7b80\u5355\u6b63\u65b9\u70b9\u9635\uff08simple cubic lattice\uff09\u662f\u6700\u7b80\u5355\u7684\u70b9\u9635\u7c7b\u578b\uff0c\u5b83\u7684\u539f\u5b50\u6216\u8005\u70b9\u4f4d\u4e8e\u6bcf\u4e2a\u7acb\u65b9\u4f53\u7684\n\u89d2\u4e0a\u3002\n- \u5e95\u5fc3\u6b63\u65b9\u70b9\u9635\uff08base-centered cubic lattice\uff09\u5728\u7b80\u5355\u6b63\u65b9\u70b9\u9635\u7684\u57fa\u7840\u4e0a\uff0c\u5728\u6bcf\u4e2a\u7acb\u65b9\u4f53\u7684\u5e95\u9762\n\u4e2d\u5fc3\u589e\u52a0\u4e86\u4e00\u4e2a\u539f\u5b50\u6216\u70b9\u3002\n- \u4f53\u5fc3\u6b63\u65b9\u70b9\u9635\uff08body-centered cubic lattice\uff09\u5728\u7b80\u5355\u6b63\u65b9\u70b9\u9635\u7684\u57fa\u7840\u4e0a\uff0c\u5728\u7acb\u65b9\u4f53\u7684\u4e2d\u5fc3\u589e\u52a0\n\u4e86\u4e00\u4e2a\u539f\u5b50\u6216\u70b9\u3002\n- \u9762\u5fc3\u6b63\u65b9\u70b9\u9635\uff08face-centered cubic lattice\uff09\u5728\u7b80\u5355\u6b63\u65b9\u70b9\u9635\u7684\u57fa\u7840\u4e0a\uff0c\u5728\u6bcf\u4e2a\u7acb\u65b9\u4f53\u7684\u6bcf\u4e2a\n\u9762\u7684\u4e2d\u5fc3\u589e\u52a0\u4e86\u4e00\u4e2a\u539f\u5b50\u6216\u70b9\u3002\n\u6839\u636e\u8fd9\u4e9b\u4fe1\u606f\uff0c\u4f60\u53ef\u4ee5\u89c2\u5bdf\u56fe\u7247\u4e2d\u7684\u70b9\u9635\u7ed3\u6784\uff0c\u770b\u770b\u5b83\u4eec\u662f\u5982\u4f55\u6392\u5217\u7684\uff0c\u7136\u540e\u9009\u62e9\u6700\u7b26\u5408\u7684\u9009\u9879\u3002\u5982\n\u679c\u56fe\u7247\u663e\u793a\u7684\u70b9\u9635\u7ed3\u6784\u4e0d\u8db3\u4ee5\u786e\u5b9a\u7b54\u6848\uff0c\u90a3\u4e48\u53ef\u80fd\u9700\u8981\u66f4\u591a\u7684\u4fe1\u606f\u624d\u80fd\u4f5c\u51fa\u9009\u62e9\u3002\nGround Truth: (C) \u4f53\u5fc3\n<\u56fe\u7247 1>\nError Category: Reject to Answer\nError Reason: The GPT-4V model chose to not answer the question directly (Reject to Answer) and \ninstead provided descriptive information about different types of square lattices. This is because the \nmodel lacks the ability to extract and understand crucial information from the image to determine \nwhich type of lattice is being displayed. The model adopted a cautious strategy of providing \nrelevant information rather than a direct answer to avoid giving an incorrect response.\nFigure B211: A sample error case of Technology and Engineering (subcategory: Materials).\nBack to List of figures\n230\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b:\u6750\u6599\nQuestion:  \u5982\u56fe<\u56fe\u7247 1>\uff0c\u4e3a\u70ed\u5206\u6790\u6cd5\u6d4b___\u76f8\u56fe\u3002\nOption:\n(A) \u5300\u6676\n(B) \u5171\u6676\n(C) \u504f\u6676\n(D) \u5171\u6790\nGPT-4V: \u5f88\u62b1\u6b49\uff0c\u7531\u4e8e\u56fe\u7247\u4e2d\u7684\u4fe1\u606f\u4e0d\u8db3\u4ee5\u786e\u5b9a\u4e00\u4e2a\u660e\u786e\u7684\u7b54\u6848\uff0c\u6211\u65e0\u6cd5\u76f4\u63a5\u7ed9\u51fa\u6b63\u786e\u9009\u9879\u3002\u4f46\u662f\uff0c\n\u6839\u636e\u56fe\u4e2d\u7684\u70ed\u5206\u6790\u66f2\u7ebf\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u6709\u4e00\u4e2a\u660e\u663e\u7684\u5e73\u53f0\u533a\u57df\uff0c\u8fd9\u901a\u5e38\u8868\u793a\u5728\u8be5\u6210\u5206\u6bd4\u4f8b\u4e0b\u6709\u4e00\u4e2a\u76f8\n\u53d8\u53d1\u751f\uff0c\u8fd9\u53ef\u80fd\u662f\u4e00\u4e2a\u5171\u6676\u6216\u5171\u6790\u53cd\u5e94\u3002\u56e0\u6b64\uff0c\u6700\u53ef\u80fd\u6b63\u786e\u7684\u9009\u9879\u662f\uff1a\n(B) \u5171\u6676\n(D) \u5171\u6790\n\u5982\u679c\u8fd9\u662f\u4e00\u4e2a\u5178\u578b\u7684\u5171\u6676\u6216\u5171\u6790\u76f8\u56fe\uff0c\u90a3\u4e48\u5728\u7279\u5b9a\u7684\u6210\u5206\u6bd4\u4f8b\u4e0b\uff0c\u6e29\u5ea6\u4f1a\u4fdd\u6301\u4e0d\u53d8\uff0c\u76f4\u5230\u76f8\u53d8\u5b8c\u6210\u3002\n\u7136\u800c\uff0c\u6ca1\u6709\u66f4\u591a\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5f88\u96be\u786e\u5b9a\u662f\u5171\u6676\u8fd8\u662f\u5171\u6790\u3002\u901a\u5e38\u9700\u8981\u66f4\u591a\u7684\u6750\u6599\u79d1\u5b66\u80cc\u666f\u77e5\u8bc6\u6216\u8005\u662f\n\u5bf9\u7279\u5b9a\u6750\u6599\u4f53\u7cfb\u7684\u4e86\u89e3\u6765\u51c6\u786e\u5224\u65ad\u3002\nGround Truth: (A) \u5300\u6676\n<\u56fe\u7247 1>\nError Category: Perceptual Error\nError Reason: According to the question description, the issue pertains to the interpretation of a \nphase diagram obtained through a thermal analysis method. However, GPT-4V's response \ndemonstrates its inability to accurately identify and interpret the crucial information in the image. \nThe model incorrectly interpreted the features in the image as possibly indicative of eutectic or \neutectoid reactions, whereas the correct answer is \"eutectic.\"\nFigure B212: A sample error case of Technology and Engineering (subcategory: Materials).\nBack to List of figures\n231\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b:\u6750\u6599\nQuestion:  \u5982\u56fe<\u56fe\u7247 1>\uff0c\u4e3aAl-Si\u7cfb\u7684___\u8303\u56f4\u3002\nOption:\n(A) \u5171\u6676\u533a\n(B) \u4f2a\u5171\u6676\u533a\n(C) \u56fa\u6eb6\u4f53\u533a\n(D) \u5171\u6790\u533a\nGPT-4V: \u8fd9\u5f20\u56fe\u662fAl-Si\uff08\u94dd\u7845\uff09\u5408\u91d1\u7684\u76f8\u56fe\u3002\u5728\u8fd9\u4e2a\u76f8\u56fe\u4e2d\uff0c\u53ef\u4ee5\u770b\u5230\u4e0d\u540c\u6210\u5206\u6bd4\u4f8b\u548c\u6e29\u5ea6\u4e0b\u6750\u6599\n\u7684\u76f8\u6001\u3002\u6839\u636e\u56fe\u4e2d\u7684\u6807\u8bb0\u548c\u76f8\u56fe\u7684\u5178\u578b\u7279\u5f81\uff0c\u56fe\u4e2d\u6807\u8bb0\u7684\u5782\u76f4\u7ebf\u6240\u5728\u7684\u4f4d\u7f6e\u662f\u94dd\u7845\u5408\u91d1\u7684\u5171\u6676\u7ec4\u6210\uff0c\n\u5373\u94dd\u542b\u91cf\u5927\u7ea6\u4e3a88.3%\uff0c\u7845\u542b\u91cf\u4e3a11.7%\u7684\u4f4d\u7f6e\u3002\u5728\u8fd9\u4e00\u7ec4\u6210\u6bd4\u4f8b\u4e0b\uff0c\u5408\u91d1\u5728\u51b7\u5374\u8fc7\u7a0b\u4e2d\u4f1a\u4ece\u6db2\u6001\uff08L\uff09\n\u76f4\u63a5\u53d8\u4e3a\u4e24\u4e2a\u56fa\u6001\u76f8\u7684\u6df7\u5408\u7269\uff08\u03b1+Si\uff09\uff0c\u800c\u4e0d\u7ecf\u5386\u56fa\u6db2\u5171\u5b58\u9636\u6bb5\u3002\u8fd9\u79cd\u8f6c\u53d8\u53d1\u751f\u5728\u5171\u6676\u6e29\u5ea6\uff0c\u5373\u76f8\n\u56fe\u4e2d\u7684\u6c34\u5e73\u7ebf\u6240\u8868\u793a\u7684\u6e29\u5ea6\u3002\n\u6b63\u786e\u7b54\u6848\u662f\uff1a\n(A) \u5171\u6676\u533a\nGround Truth: (B) \u4f2a\u5171\u6676\u533a\n<\u56fe\u7247 1>\nError Category: Perceptual Error\nError Reason: GPT-4V incorrectly interpreted the phase diagram in the image as representing a \n\"eutectic region,\" whereas the actual correct answer is the \"pseudo-eutectic region.\" Several factors \nmay have contributed to this error. Firstly, despite GPT-4V's capability for image recognition and \ninterpretation, it may face challenges when analyzing specific types of scientific images, especially \ncomplex phase diagrams. These images often contain rich scientific information and require \ndomain-specific knowledge for correct interpretation. In this case, the model may not have fully \nidentified all the crucial features in the image or may not have correctly understood the specific \nsignificance of these features in materials science.\nFigure B213: A sample error case of Technology and Engineering (subcategory: Materials).\nBack to List of figures\n232\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b:\u6750\u6599\nQuestion:  \u56fe\u793a<\u56fe\u7247 1>\u8f74\u5411\u53d7\u529b\u6746\u4ef6,\u6746\u5185\u6700\u5927\u62c9\u529b\u4e3a()\nOption:\n(A) 8kN\n(B) 4kN\n(C) 5kN\n(D) 3kN\nGPT-4V: \u6839\u636e\u6240\u63d0\u4f9b\u7684\u56fe\u7247\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u4e00\u4e2a\u8f74\u5411\u53d7\u529b\u7684\u6746\u4ef6\uff0c\u5176\u4e2d\u6807\u6ce8\u4e86\u4e09\u4e2a\u529b\uff1a3kN\u30018kN\u548c\n5kN\u3002\u4e3a\u4e86\u786e\u5b9a\u6746\u5185\u6700\u5927\u62c9\u529b\uff0c\u6211\u4eec\u9700\u8981\u5206\u6790\u8fd9\u4e9b\u529b\u7684\u4f5c\u7528\u65b9\u5f0f\u3002\n\u5728\u6746\u7684\u5de6\u4fa7\uff0c\u6709\u4e00\u4e2a\u5411\u5de6\u7684\u529b\uff0c\u5927\u5c0f\u4e3a3kN\u3002\u5728\u6746\u7684\u53f3\u4fa7\uff0c\u6709\u4e00\u4e2a\u5411\u53f3\u7684\u529b\uff0c\u5927\u5c0f\u4e3a5kN\u3002\u5728\u6746\u7684\u4e2d\u95f4\uff0c\n\u6709\u4e00\u4e2a\u5411\u53f3\u7684\u529b\uff0c\u5927\u5c0f\u4e3a8kN\u3002\n\u4e3a\u4e86\u4fdd\u6301\u5e73\u8861\uff0c\u6746\u5185\u7684\u529b\u5fc5\u987b\u76f8\u4e92\u62b5\u6d88\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u53ef\u4ee5\u63a8\u65ad\u51fa\uff1a\n- \u6746\u7684\u5de6\u4fa7\u90e8\u5206\u5fc5\u987b\u627f\u53d7\u4e00\u4e2a\u5411\u53f3\u7684\u529b\uff0c\u4ee5\u62b5\u6d883kN\u7684\u5411\u5de6\u529b\uff0c\u56e0\u6b64\u5de6\u4fa7\u90e8\u5206\u7684\u62c9\u529b\u4e3a3kN\u3002\n- \u6746\u7684\u53f3\u4fa7\u90e8\u5206\u5fc5\u987b\u627f\u53d7\u4e00\u4e2a\u5411\u5de6\u7684\u529b\uff0c\u4ee5\u62b5\u6d885kN\u7684\u5411\u53f3\u529b\uff0c\u56e0\u6b64\u53f3\u4fa7\u90e8\u5206\u7684\u62c9\u529b\u4e3a5kN\u3002\n- \u6746\u7684\u4e2d\u95f4\u90e8\u5206\u627f\u53d7\u7684\u662f8kN\u7684\u5411\u53f3\u529b\uff0c\u8fd9\u662f\u6746\u5185\u7684\u6700\u5927\u529b\u3002\n\u56e0\u6b64\uff0c\u6746\u5185\u6700\u5927\u7684\u62c9\u529b\u662f8kN\u3002\n\u6b63\u786e\u7b54\u6848\u662f\uff1a\n(A) 8kN\nGround Truth: (D) 3kN\n<\u56fe\u7247 1>\nError Category: Perceptual Error\nError Reason: Due to a misinterpretation of arrow directions, the GPT-4V model arrived at an \nincorrect conclusion. Properly understanding the direction of arrows is crucial for solving statics \nproblems as it directly impacts the analysis of force equilibrium. In this image, the 3kN force is to \nthe left, and the 5kN force is actually also to the left, not to the right as initially explained by the \nmodel, which is in contradiction.\nFigure B214: A sample error case of Technology and Engineering (subcategory: Materials).\nBack to List of figures\n233\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b:\u6750\u6599\nQuestion:  \u5982<\u56fe\u7247 1>\u6240\u793a\u4e3a\u6676\u6838\u7684\u534a\u5f84r\u4e0e\u2206\ud835\udc3a\ud835\udc3a\u95f4\u7684\u5173\u7cfb\uff0c\u73b0\u6709\u4e0d\u540c\u6e29\u5ea6\u7684\u4e09\u6761\u66f2\u7ebf\uff0c\u6e29\u5ea6\u6700\u4f4e\u7684\u662f\n\uff08\uff09\nOption:\n(A) T1\n(B) T2\n(C) T3\n(D) \u90fd\u4e00\u6837\nGPT-4V: \u8fd9\u5f20\u56fe\u7247\u663e\u793a\u4e86\u6676\u6838\u7684\u534a\u5f84 r \u4e0e\u81ea\u7531\u80fd\u53d8\u5316 \u2206\ud835\udc3a\ud835\udc3a \u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u6839\u636e\u6210\u6838\u7406\u8bba\uff0c\u81ea\u7531\u80fd\u53d8\u5316\n\u66f2\u7ebf\u7684\u5cf0\u503c\u4ee3\u8868\u4e86\u6210\u6838\u7684\u80fd\u5792\u3002\u6e29\u5ea6\u8d8a\u4f4e\uff0c\u6210\u6838\u80fd\u5792\u8d8a\u9ad8\uff0c\u56e0\u6b64\u66f2\u7ebf\u7684\u5cf0\u503c\u8d8a\u9ad8\u3002\n\u5728\u8fd9\u5f20\u56fe\u7247\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230 T1 \u66f2\u7ebf\u7684\u5cf0\u503c\u6700\u9ad8\uff0c\u8fd9\u610f\u5473\u7740\u5b83\u4ee3\u8868\u7684\u662f\u6700\u4f4e\u7684\u6e29\u5ea6\u3002\u56e0\u6b64\uff0c\u6b63\u786e\u7b54\n\u6848\u662f\uff1a\n(A) T1\nGround Truth: (C) T3\n<\u56fe\u7247 1>\nError Category: Reasoning Error\nError Reason: While explaining the image, GPT-4V correctly referenced general principles from \nnucleation theory but made an error in applying this principle for inference. In the question, the \nimage depicted the relationship between the nucleation radius 'r' and the change in free energy \n'\u0394G,' and it included three curves representing different temperatures. In theory, the height of the \nfree energy change curve is inversely proportional to temperature, meaning that a lower curve \nrepresents a higher temperature.\nHowever, when analyzing this image, GPT-4V incorrectly assumed that the curve with the highest \npeak represented the lowest temperature, neglecting a crucial piece of information mentioned in the \nquestion: \"G1>G2>G3, T1>T2>T3 when r is the same.\" This implies that, for the same nucleation \nradius, a larger change in free energy '\u2206\ud835\udc3a\ud835\udc3a' corresponds to a lower temperature.\nFigure B215: A sample error case of Technology and Engineering (subcategory: Materials).\nBack to List of figures\n234\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b:\u6750\u6599\nQuestion:  \u5982\u56fe<\u56fe\u7247 1>\uff0c\u662f\u6676\u4f53\u5c40\u90e8\u6ed1\u79fb\u4ea7\u751f___\u4f4d\u9519\u3002\nOption:\n(A) \u5203\u578b\n(B) \u87ba\u578b\n(C) \u6df7\u5408\n(D) \u65e0\u6cd5\u786e\u5b9a\nGPT-4V: \u8fd9\u4e24\u5e45\u56fe\u5c55\u793a\u4e86\u6676\u4f53\u4e2d\u7684\u4f4d\u9519\u79fb\u52a8\u3002\u5728\u5de6\u4fa7\u7684\u56fe\u4e2d\uff0c\u6676\u4f53\u7684\u4e00\u90e8\u5206\u4f3c\u4e4e\u5728\u4e00\u4e2a\u7279\u5b9a\u7684\u5e73\u9762\u4e0a\n\u53d1\u751f\u4e86\u6ed1\u79fb\uff0c\u800c\u53f3\u4fa7\u7684\u56fe\u663e\u793a\u4e86\u6ed1\u79fb\u540e\u7684\u7ed3\u679c\u3002\u6839\u636e\u8fd9\u4e9b\u56fe\u50cf\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u4e00\u4e2a\u989d\u5916\u7684\u534a\u5e73\u9762\u63d2\u5165\n\u5230\u6676\u4f53\u4e2d\uff0c\u8fd9\u662f\u5203\u578b\u4f4d\u9519\u7684\u7279\u5f81\u3002\u56e0\u6b64\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a\n(A) \u5203\u578b\nGround Truth: (B) \u87ba\u578b\n<\u56fe\u7247 1>\nError Category: Reasoning Error\nError Reason: The GPT-4V model mentioned \"additional half-planes inserted into the crystal,\" \nwhich is a typical characteristic of an edge dislocation. However, based on the question description \nand the correct answer, the image should be depicting a screw dislocation. This indicates that the \nmodel deviated in its logical reasoning about the content of the image, possibly due to incorrectly \nassociating the image features with an edge dislocation.\nFigure B216: A sample error case of Technology and Engineering (subcategory: Materials).\nBack to List of figures\n235\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b: \u673a\u68b0\u5de5\u7a0b\nQuestion: <\u56fe\u7247 1>\u673a\u68b0\u8bc6\u56fe\u7684\u4e09\u7ef4\u56fe\u5f62\u7814\u7a76\u4ece\u56fe\u6765\u7406\u89e3\u7269\uff0c\u901a\u8fc7\u770b\u5e73\u9762\u56fe\u5f62\uff0c\u60f3\u8c61\u7269\u4f53\u7684__\u3002\nOption: \n(A) \u7a7a\u95f4\u5927\u5c0f\n(B) \u989c\u8272\n(C) \u4f4d\u7f6e\n(D) \u7a7a\u95f4\u5f62\u72b6\nGPT-4V: \u6b63\u786e\u7b54\u6848\u662f\uff1a\n(D) \u7a7a\u95f4\u5f62\u72b6\n\u673a\u68b0\u8bc6\u56fe\u7684\u4e09\u7ef4\u56fe\u5f62\u7814\u7a76\u901a\u5e38\u6d89\u53ca\u4ece\u4e8c\u7ef4\u56fe\u50cf\u4e2d\u7406\u89e3\u548c\u60f3\u8c61\u7269\u4f53\u7684\u4e09\u7ef4\u5f62\u72b6\u3002\u56e0\u6b64\uff0c\u901a\u8fc7\u770b\u5e73\u9762\u56fe\u5f62\n\uff0c\u6211\u4eec\u53ef\u4ee5\u60f3\u8c61\u7269\u4f53\u7684\u7a7a\u95f4\u5f62\u72b6\u3002\nGround Truth: (D)\u7a7a\u95f4\u5f62\u72b6  \n<\u56fe\u7247 1>\nFigure B217: A sample correct case of Technology and Engineering (subcategory: Mechanical\nEngineering).\nBack to List of figures\n236\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b: \u673a\u68b0\u5de5\u7a0b\nQuestion: \u8bd5\u8ba1\u7b97<\u56fe\u7247 1>\u673a\u6784\u7684\u81ea\u7531\u5ea6F=__\u3002\nGPT-4V:\u8fd9\u5f20\u56fe\u7247\u663e\u793a\u7684\u662f\u4e00\u4e2a\u673a\u68b0\u8fde\u6746\u7cfb\u7edf\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u5e93\u5179\u5df4\u8d6b\u65b9\u7a0b\u6765\u8ba1\u7b97\u5176\u81ea\u7531\u5ea6\uff08F\uff09\u3002\n\u5e93\u5179\u5df4\u8d6b\u65b9\u7a0b\u662f\uff1a\nF = 3(n - 1) - 2j - h\n\u5176\u4e2d\uff1a\nn = \u8fde\u6746\u6570\uff08\u5305\u62ec\u5730\u9762\uff09\nj = \u94f0\u94fe\u6570\uff08\u5b8c\u5168\u7ea6\u675f\u7684\u8f6c\u52a8\u526f\uff09\nh = \u9ad8\u9636\u526f\u6570\uff08\u4f8b\u5982\uff0c\u6ed1\u5757\u526f\uff09\n\u4ece\u56fe\u7247\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff1a\n- \u67094\u4e2a\u8fde\u6746\uff08\u5305\u62ec\u5730\u9762\u4f5c\u4e3a\u4e00\u4e2a\u53c2\u8003\u8fde\u6746\uff09\n- \u67094\u4e2a\u94f0\u94fe\uff08A, B, C, D\uff09\n- \u6ca1\u6709\u9ad8\u9636\u526f\n\u56e0\u6b64\uff1a\nn = 4\nj = 4\nh = 0\n\u4ee3\u5165\u5e93\u5179\u5df4\u8d6b\u65b9\u7a0b\uff1a\nF = 3(4 - 1) - 2*4 - 0\nF = 3*3 - 8\nF = 9 - 8\nF = 1\n\u6240\u4ee5\uff0c\u8fd9\u4e2a\u673a\u6784\u7684\u81ea\u7531\u5ea6 F = 1\u3002\nGround Truth: 1  \n<\u56fe\u7247 1>\nFigure B218: A sample correct case of Technology and Engineering (subcategory: Mechanical\nEngineering).\nBack to List of figures\n237\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b: \u673a\u68b0\u5de5\u7a0b\nQuestion: \u8bbe\u6709\u4e00\u5f39\u7c27\u3001\u8d28\u91cf\u3001\u963b\u5c3c\u5668\u673a\u68b0\u7cfb\u7edf\uff0c\u5982<\u56fe\u7247 1>\u6240\u793a\uff0c\u4ee5\u5916\u529bf(t)\u4e3a\u8f93\u5165\u91cf\uff0c\u4f4d\u79fby(t)\n\u4e3a\u8f93\u51fa\u91cf\u7684\u8fd0\u52a8\u5fae\u5206\u65b9\u7a0b\u5f0f\u53ef\u4ee5\u5bf9\u56fe\u4e2d\u7cfb\u7edf\u8fdb\u884c\u63cf\u8ff0\uff0c\u90a3\u4e48\u8fd9\u4e2a\u5fae\u5206\u65b9\u7a0b\u7684\u9636\u6b21\u662f\uff08\uff09\nOption: \n(A) 1;\n(B) 2;\n(C) 3;\n(D) 4;\nGPT-4V:\u8fd9\u4e2a\u7cfb\u7edf\u662f\u4e00\u4e2a\u5178\u578b\u7684\u4e8c\u9636\u673a\u68b0\u7cfb\u7edf\uff0c\u56e0\u4e3a\u5b83\u5305\u542b\u6709\u8d28\u91cf\uff08m\uff09\u3001\u5f39\u7c27\uff08K\uff0c\u5f39\u6027\u7cfb\u6570\uff09\u548c\u963b\n\u5c3c\u5668\uff08B\uff0c\u963b\u5c3c\u7cfb\u6570\uff09\u3002\u5728\u8fd9\u6837\u7684\u7cfb\u7edf\u4e2d\uff0c\u4f4d\u79fb\uff08y(t)\uff09\u662f\u53d7\u529b\uff08f(t)\uff09\u7684\u51fd\u6570\uff0c\u800c\u8fd9\u4e2a\u5173\u7cfb\u53ef\u4ee5\u901a\n\u8fc7\u4e00\u4e2a\u4e8c\u9636\u5fae\u5206\u65b9\u7a0b\u6765\u63cf\u8ff0\uff0c\u56e0\u4e3a\u7cfb\u7edf\u7684\u52a8\u6001\u884c\u4e3a\u6d89\u53ca\u5230\u4f4d\u79fb\u7684\u52a0\u901f\u5ea6\uff08\u5373\u4f4d\u79fb\u7684\u4e8c\u9636\u5bfc\u6570\uff09\u3002\n\u56e0\u6b64\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a\n(B) 2\nGround Truth: (B)2  \n<\u56fe\u7247 1>\nFigure B219: A sample correct case of Technology and Engineering (subcategory: Mechanical\nEngineering).\nBack to List of figures\n238\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b: \u673a\u68b0\u5de5\u7a0b\nQuestion: \u5982<\u56fe\u7247 1>\u6240\u793a\uff0c\u7532\u3001\u4e59\u3001\u4e19\u662f\u8f6f\u94c1\u7247\uff0c\u7535\u952e\u95ed\u5408\u540e\uff0c\u5219\uff08\uff09\u3002\nOption: \n(A)\u7532\u7684\u5de6\u7aef\u51fa\u73b0N\u7ea7\n(B) \u4e19\u7684\u5de6\u7aef\u51fa\u73b0N\u7ea7\n(C) \u4e59\u7684\u5de6\u7aef\u51fa\u73b0N\u7ea7\n(D) \u4e59\u7684\u53f3\u7aef\u51fa\u73b0N\u7ea7\nGPT-4V:\u62b1\u6b49\uff0c\u7531\u4e8e\u56fe\u7247\u4e2d\u7684\u4fe1\u606f\u4e0d\u8db3\uff0c\u6211\u65e0\u6cd5\u63d0\u4f9b\u4e00\u4e2a\u786e\u5207\u7684\u7b54\u6848\u3002\u4f46\u662f\uff0c\u6839\u636e\u7535\u78c1\u5b66\u7684\u57fa\u672c\u539f\u7406\uff0c\n\u5f53\u7535\u6d41\u901a\u8fc7\u7ebf\u5708\u65f6\uff0c\u7ebf\u5708\u4f1a\u4ea7\u751f\u78c1\u573a\uff0c\u4f7f\u5f97\u9644\u8fd1\u7684\u8f6f\u94c1\u7247\u78c1\u5316\u3002\u78c1\u573a\u7684\u65b9\u5411\u9075\u5faa\u53f3\u624b\u5b9a\u5219\uff0c\u5373\u5982\u679c\u5c06\n\u53f3\u624b\u7684\u56db\u6307\u6307\u5411\u7535\u6d41\u7684\u65b9\u5411\uff0c\u62c7\u6307\u6307\u51fa\u7684\u65b9\u5411\u5c31\u662f\u78c1\u573a\u7684\u5317\u6781\u3002\n\u5728\u8fd9\u4e2a\u95ee\u9898\u4e2d\uff0c\u6211\u4eec\u9700\u8981\u77e5\u9053\u7535\u6d41\u7684\u65b9\u5411\u4ee5\u53ca\u7ebf\u5708\u7684\u7ed5\u6cd5\uff0c\u624d\u80fd\u786e\u5b9a\u8f6f\u94c1\u7247\u7684\u54ea\u4e00\u7aef\u4f1a\u51fa\u73b0N\u6781\uff08\u5317\n\u6781\uff09\u3002\u7531\u4e8e\u8fd9\u4e9b\u4fe1\u606f\u5728\u56fe\u7247\u4e2d\u6ca1\u6709\u63d0\u4f9b\uff0c\u6211\u65e0\u6cd5\u786e\u5b9a\u6b63\u786e\u7b54\u6848\u3002\u5982\u679c\u4f60\u80fd\u63d0\u4f9b\u66f4\u591a\u7684\u80cc\u666f\u4fe1\u606f\u6216\u8005\u7535\n\u6d41\u7684\u65b9\u5411\uff0c\u6211\u53ef\u80fd\u80fd\u591f\u5e2e\u52a9\u4f60\u786e\u5b9a\u6b63\u786e\u7684\u9009\u9879\u3002\nGround Truth: (A)\u7532\u7684\u5de6\u7aef\u51fa\u73b0N\u7ea7 (D) \u4e59\u7684\u53f3\u7aef\u51fa\u73b0N\u7ea7\nError Category: Perceptual Errorack of Knowledge\nError Reason: GPT-4V failed to accurately understand or analyze the information in the image, \nsuch as the direction of the electric current, the winding of the coil, and the related magnetic field \neffects. This error occurred because GPT-4V cannot directly parse details in images, such as circuit \ndiagrams or symbols, and therefore could not accurately determine the magnetic pole changes in \nthe soft iron pieces A, B, and C after the electrical key is closed. In this case, GPT-4V did not provide \na definitive answer but explained based on general principles of electromagnetism, indicating its \ninability to process specific visual information in the image. \n<\u56fe\u7247 1>\nFigure B220: A sample error case of Technology and Engineering (subcategory: Mechanical\nEngineering).\nBack to List of figures\n239\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b: \u673a\u68b0\u5de5\u7a0b\nQuestion: \u53d6\u4e00\u673a\u5668\u7684\u4e3b\u8f74\u4e3a\u7b49\u6548\u6784\u4ef6\uff0c\u5df2\u77e5\u4e3b\u8f74\u5e73\u5747\u8f6c\u901fn=1000r/min\uff0c\u5728\u4e00\u4e2a\u7a33\u5b9a\u8fd0\u52a8\u5faa\u73af\uff082 \uff09\n\u4e2d\u7684\u7b49\u6548\u963b\u529b\u77e9Mer\u5982<\u56fe\u7247 1>\u6240\u793a\uff0c\u7b49\u6548\u9a71\u52a8\u529b\u77e9Med\u4e3a\u5e38\u6570\u3002\u82e5\u4e0d\u8ba1\u673a\u5668\u4e2d\u5404\u6784\u4ef6\u7684\u8f6c\u52a8\u60ef\u91cf\uff0c\u8bd5\n\u6c42\uff1a\u5f53\u4e3b\u8f74\u8fd0\u8f6c\u4e0d\u5747\u5300\u7cfb\u6570 \uff1d0.05\u65f6\uff0c\u5e94\u5728\u4e3b\u8f74\u4e0a\u52a0\u88c5\u7684\u98de\u8f6e\u7684\u8f6c\u52a8\u60ef\u91cfJ=_\u3002\nGPT-4V:\u7531\u4e8e\u9898\u76ee\u4e2d\u6ca1\u6709\u63d0\u4f9b\u8db3\u591f\u7684\u4fe1\u606f\u6765\u76f4\u63a5\u8ba1\u7b97\u98de\u8f6e\u7684\u8f6c\u52a8\u60ef\u91cfJ\uff0c\u6211\u4eec\u9700\u8981\u4f9d\u636e\u73b0\u6709\u7684\u6570\u636e\u548c\n\u4e00\u4e9b\u57fa\u672c\u7684\u7269\u7406\u539f\u7406\u6765\u8fdb\u884c\u63a8\u7406\u3002\u9996\u5148\uff0c\u6211\u4eec\u77e5\u9053\u4e3b\u8f74\u7684\u4e0d\u5747\u5300\u7cfb\u6570\u662f0.05\uff0c\u8fd9\u4e2a\u7cfb\u6570\u5b9a\u4e49\u4e3a\u901f\u5ea6\u6ce2\n\u52a8\u7684\u6700\u5927\u503c\u4e0e\u5e73\u5747\u89d2\u901f\u5ea6\u7684\u6bd4\u503c\u3002\u4e0d\u5747\u5300\u7cfb\u6570\u53ef\u4ee5\u8868\u793a\u4e3a\uff1a\ud835\udeff\ud835\udeff =\n\u2206\ud835\udf14\ud835\udf14\n\ud835\udf14\ud835\udf14\ud835\udc4e\ud835\udc4e\ud835\udc4e\ud835\udc4e\ud835\udc4e\ud835\udc4e\u5176\u4e2d\uff0c\u2206\ud835\udf14\ud835\udf14 \u662f\u89d2\u901f\u5ea6\u7684\u6ce2\u52a8\u8303\u56f4\uff0c\n\ud835\udf14\ud835\udf14\ud835\udc4e\ud835\udc4e\ud835\udc4e\ud835\udc4e\ud835\udc4e\ud835\udc4e\u662f\u5e73\u5747\u89d2\u901f\u5ea6\u3002\u7531\u4e8e\u4e0d\u8ba1\u673a\u5668\u4e2d\u5404\u6784\u4ef6\u7684\u8f6c\u52a8\u60ef\u91cf\uff0c\u98de\u8f6e\u7684\u4f5c\u7528\u662f\u5e73\u8861\u7b49\u6548\u963b\u529b\u77e9Mer\u7684\u6ce2\u52a8\uff0c\n\u4f7f\u5f97\u4e3b\u8f74\u7684\u8f6c\u901f\u5c3d\u53ef\u80fd\u7a33\u5b9a\u3002\u98de\u8f6e\u7684\u8f6c\u52a8\u60ef\u91cfJ\u4e0e\u5176\u80fd\u591f\u50a8\u5b58\u7684\u52a8\u80fd\u91cf\u6210\u6b63\u6bd4\uff0c\u800c\u52a8\u80fd\u91cf\u4e0e\u89d2\u901f\u5ea6\u7684\n\u6ce2\u52a8\u8303\u56f4\u6709\u5173\u3002\u5728\u7a33\u5b9a\u8fd0\u52a8\u5faa\u73af\u4e2d\uff0c\u7b49\u6548\u9a71\u52a8\u529b\u77e9Med\u662f\u4e00\u4e2a\u5e38\u6570\uff0c\u56e0\u6b64\u6211\u4eec\u53ef\u4ee5\u5047\u8bbe\u5b83\u7b49\u4e8e\u7b49\u6548\u963b\n\u529b\u77e9Mer\u7684\u5e73\u5747\u503c\u3002\u4ece\u56fe\u4e2d\u53ef\u4ee5\u770b\u51fa\uff0cMer\u5728\u4e00\u4e2a\u5468\u671f\u5185\u662f\u4e00\u4e2a\u4e09\u89d2\u5f62\u6ce2\u52a8\uff0c\u6211\u4eec\u53ef\u4ee5\u8ba1\u7b97\u51fa\u8fd9\u4e2a\u4e09\u89d2\n\u5f62\u7684\u9762\u79ef\u6765\u4ee3\u8868\u4e00\u4e2a\u5468\u671f\u5185\u7684\u603b\u963b\u529b\u77e9\u5de5\u4f5c\u91cf\uff0c\u7136\u540e\u9664\u4ee5\u5468\u671f\u65f6\u95f4\u5f97\u5230\u5e73\u5747\u963b\u529b\u77e9\u3002\u7136\u800c\uff0c\u7531\u4e8e\u6ca1\u6709\n\u7ed9\u51faMed\u7684\u5177\u4f53\u6570\u503c\uff0c\u6211\u4eec\u65e0\u6cd5\u76f4\u63a5\u8ba1\u7b97\u51fa\u98de\u8f6e\u7684\u8f6c\u52a8\u60ef\u91cfJ\u3002\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0cJ\u7684\u8ba1\u7b97\u9700\u8981\u901a\u8fc7\u52a8\u80fd\n\u7684\u53d8\u5316\u91cf\u548c\u89d2\u901f\u5ea6\u7684\u6ce2\u52a8\u8303\u56f4\u6765\u786e\u5b9a\u3002\u8fd9\u901a\u5e38\u6d89\u53ca\u5230\u5bf9\u7cfb\u7edf\u52a8\u529b\u5b66\u7684\u8be6\u7ec6\u5206\u6790\uff0c\u5305\u62ec\u8ba1\u7b97\u7b49\u6548\u963b\u529b\u77e9\nMer\u548c\u7b49\u6548\u9a71\u52a8\u529b\u77e9Med\u4e4b\u95f4\u7684\u5dee\u503c\u5728\u4e00\u4e2a\u8fd0\u52a8\u5468\u671f\u5185\u7684\u65f6\u95f4\u79ef\u5206\uff0c\u4ee5\u53ca\u5e94\u7528\u80fd\u91cf\u5b88\u6052\u539f\u7406\u3002\u56e0\u6b64\uff0c\u6ca1\n\u6709\u5177\u4f53\u7684\u6570\u503c\u548c\u66f4\u591a\u7684\u7cfb\u7edf\u53c2\u6570\uff0c\u6211\u4eec\u65e0\u6cd5\u7ed9\u51fa\u4e00\u4e2a\u786e\u5207\u7684\u7b54\u6848\u3002\u5728\u5b9e\u9645\u5de5\u7a0b\u95ee\u9898\u4e2d\uff0c\u8fd9\u901a\u5e38\u9700\u8981\u8be6\n\u7ec6\u7684\u7cfb\u7edf\u5206\u6790\u548c\u8ba1\u7b97\u3002\nGround Truth: 0.51\nError Category: Reasoning Error\nError Reason: GPT-4V made a reasoning error in its response. The question asked for the \ncalculation of the moment of inertia of a flywheel that should be added to the main shaft, but GPT-\n4V failed to perform the necessary reasoning and calculations to arrive at the answer. While it \nprovided information about the non-uniformity coefficient and angular velocity fluctuation, it did \nnot apply this information to the calculation of the flywheel's moment of inertia. The correct \napproach involves calculating the maximum angular velocity fluctuation and then applying it to \nthe definition of the non-uniformity coefficient. Due to GPT-4V's failure to perform such reasoning \nand calculations, it resulted in an incorrect answer.\n<\u56fe\u7247 1>\nFigure B221: A sample error case of Technology and Engineering (subcategory: Mechanical\nEngineering).\nBack to List of figures\n240\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b: \u673a\u68b0\u5de5\u7a0b\nQuestion: \u6307\u51fa<\u56fe\u7247 1>\u4e0b\u5217\u5b9a\u4f4d\u5143\u5177\u4f53\u9650\u5236\u81ea\u7531\u5ea6\u65b9\u5411___\nOption: \n(A) \u0305\ud835\udc65\ud835\udc65\n(B) \u0d24\ud835\udc66\ud835\udc66\n(C) \u0305\ud835\udc65\ud835\udc65 \uff0c\u0d24\ud835\udc66\ud835\udc66\n(D) \u0305\ud835\udc65\ud835\udc65 \uff0c \u0305\ud835\udc67\ud835\udc67\nGPT-4V:\u8fd9\u5f20\u56fe\u7247\u663e\u793a\u4e86\u4e00\u4e2a\u673a\u68b0\u96f6\u4ef6\u7684\u5256\u9762\u56fe\uff0c\u5176\u4e2d\u5305\u542b\u4e86\u4e00\u4e2a\u5b9a\u4f4d\u5143\u3002\u6839\u636e\u56fe\u793a\uff0c\u5b9a\u4f4d\u5143\u4f3c\u4e4e\u9650\n\u5236\u4e86\u96f6\u4ef6\u5728\u5782\u76f4\u4e8e\u5b9a\u4f4d\u5143\u63a5\u89e6\u9762\u7684\u65b9\u5411\u4e0a\u7684\u79fb\u52a8\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5b9a\u4f4d\u5143\u9650\u5236\u4e86\u96f6\u4ef6\u6cbf\u7740z\u8f74\u7684\u79fb\u52a8\n\uff08\n\u0305\ud835\udc67\ud835\udc67 \uff09\u3002\u7136\u800c\uff0c\u7531\u4e8e\u56fe\u4e2d\u6ca1\u6709\u660e\u786e\u663e\u793a\u5b9a\u4f4d\u5143\u662f\u5426\u540c\u65f6\u9650\u5236\u4e86x\u8f74\u6216y\u8f74\u65b9\u5411\u4e0a\u7684\u79fb\u52a8\uff0c\u6211\u4eec\u65e0\u6cd5\u786e\u5b9a\n\u662f\u5426\u8fd8\u6709\u5176\u4ed6\u9650\u5236\u3002\n\u6839\u636e\u56fe\u793a\uff0c\u6211\u4eec\u53ef\u4ee5\u786e\u5b9a\u7684\u662f\u5b9a\u4f4d\u5143\u9650\u5236\u4e86\u6cbfz\u8f74\u7684\u79fb\u52a8\u3002\u56e0\u6b64\uff0c\u6b63\u786e\u7b54\u6848\u662f\uff1a\n(D)\n\u0305\ud835\udc65\ud835\udc65 \uff0c \u0305\ud835\udc67\ud835\udc67\n\u8bf7\u6ce8\u610f\uff0c\u8fd9\u4e2a\u7b54\u6848\u662f\u57fa\u4e8e\u56fe\u793a\u548c\u5e38\u89c1\u7684\u673a\u68b0\u8bbe\u8ba1\u539f\u5219\u505a\u51fa\u7684\u6700\u4f73\u5224\u65ad\u3002\u5982\u679c\u56fe\u4e2d\u63d0\u4f9b\u4e86\u66f4\u591a\u4fe1\u606f\u6216\u8005\n\u6709\u5177\u4f53\u7684\u8bbe\u8ba1\u80cc\u666f\uff0c\u53ef\u80fd\u4f1a\u6709\u4e0d\u540c\u7684\u7b54\u6848\nGround Truth: (C) \u0d25\ud835\udc99\ud835\udc99 , \u0d25\ud835\udc9a\ud835\udc9a\nError Category: Textual Understanding\nError Reason: GPT-4V had a problem with understanding the text of the question. The question \nasked to specify the specific directions in which the locating element restricts degrees of freedom, \nbut GPT-4V incorrectly interpreted the question, leading to an incorrect answer choice. It chose (D) \n\u0305\ud835\udc65\ud835\udc65\n\u0305\ud835\udc67\ud835\udc67 , but the actual correct answer should be(C)\n\u0305\ud835\udc65\ud835\udc65 \uff0c\u0d24\ud835\udc66\ud835\udc66 . This is because the question did not mention \nthe z-axis, and GPT-4V incorrectly included the z-axis in its answer. \n<\u56fe\u7247 1>\nFigure B222: A sample error case of Technology and Engineering (subcategory: Mechanical\nEngineering).\nBack to List of figures\n241\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b: \u673a\u68b0\u5de5\u7a0b\nQuestion: \u8bd5\u6c42<\u56fe\u7247 1>\u673a\u6784\u7684\u81ea\u7531\u5ea6F=__\u3002\nGPT-4V:\u8fd9\u5f20\u56fe\u7247\u663e\u793a\u7684\u662f\u4e00\u4e2a\u5e73\u9762\u673a\u6784\uff0c\u770b\u8d77\u6765\u50cf\u662f\u4e00\u4e2a\u56db\u8fde\u6746\u673a\u6784\u3002\u4e3a\u4e86\u8ba1\u7b97\u8fd9\u4e2a\u673a\u6784\u7684\u81ea\u7531\u5ea6\n\uff08F\uff09\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u5e93\u5179\u5df4\u8d6b\u516c\u5f0f\uff08Kutzbach criterion\uff09\u6765\u8fdb\u884c\u8ba1\u7b97\uff0c\u8be5\u516c\u5f0f\u9002\u7528\u4e8e\u5e73\u9762\u673a\u6784\uff1a\nF = 3(n - 1) - 2j\n\u5176\u4e2d\uff0cn \u662f\u673a\u6784\u4e2d\u7684\u6746\u4ef6\u6570\uff08\u5305\u62ec\u5730\u9762\uff09\uff0cj \u662f\u94f0\u63a5\u70b9\uff08\u5373\u8f6c\u52a8\u5173\u8282\uff09\u7684\u6570\u91cf\u3002\n\u4ece\u56fe\u7247\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\u6709\u56db\u4e2a\u6746\u4ef6\uff08\u5305\u62ec\u5730\u9762\u4f5c\u4e3a\u4e00\u4e2a\u6746\u4ef6\uff09\uff0c\u4ee5\u53ca\u56db\u4e2a\u94f0\u63a5\u70b9\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u53ef\u4ee5\n\u5c06\u8fd9\u4e9b\u503c\u4ee3\u5165\u516c\u5f0f\u4e2d\uff1a\nn = 4 \uff08\u4e09\u4e2a\u79fb\u52a8\u6746\u4ef6\u52a0\u4e0a\u5730\u9762\uff09\nj = 4 \uff08\u56db\u4e2a\u94f0\u63a5\u70b9\uff09\n\u4ee3\u5165\u516c\u5f0f\u5f97\uff1a\nF = 3(4 - 1) - 2*4\nF = 3*3 - 8\nF = 9 - 8\nF = 1\n\u6240\u4ee5\uff0c\u8fd9\u4e2a\u673a\u6784\u7684\u81ea\u7531\u5ea6 F = 1\u3002\u8fd9\u610f\u5473\u7740\u673a\u6784\u6709\u4e00\u4e2a\u72ec\u7acb\u7684\u8fd0\u52a8\u81ea\u7531\u5ea6\u3002\nGround Truth: 3\nError Category: Answer Extraction Error\nError Reason: Incorrect extraction and matching. In GPT-4V's response, it utilized the Kutzbach \ncriterion formula to calculate the degree of freedom of the mechanism, but it made a mistake during \nthe calculation process. Although it correctly determined the number of links and joints in the \nmechanism, it encountered an error in calculating the degree of freedom.\n<\u56fe\u7247 1>\nFigure B223: A sample error case of Technology and Engineering (subcategory: Mechanical\nEngineering).\nBack to List of figures\n242\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b: \u673a\u68b0\u5de5\u7a0b\nQuestion: \u56fe<\u56fe\u7247 1>\u4e2d\u662f\u54ea\u79cd\u8054\u63a5\u65b9\u5f0f\uff1f__\nOption: \n(A) \u5e73\u952e\u8054\u63a5\n(B) \u65e0\u952e\u8054\u63a5\n(C) \u6ed1\u952e\u8054\u63a5\n(D) \u82b1\u952e\u8054\u63a5\nGPT-4V:\u8fd9\u5f20\u56fe\u7247\u663e\u793a\u4e86\u4e00\u4e2a\u9f7f\u8f6e\u548c\u4e00\u4e2a\u8f74\uff0c\u8f74\u7684\u4e00\u7aef\u6709\u4e00\u7cfb\u5217\u7684\u51f8\u8d77\uff0c\u8fd9\u4e9b\u51f8\u8d77\u4e0e\u9f7f\u8f6e\u5185\u90e8\u7684\u69fd\u76f8\n\u5339\u914d\u3002\u8fd9\u79cd\u8bbe\u8ba1\u901a\u5e38\u7528\u4e8e\u82b1\u952e\u8054\u63a5\uff0c\u5b83\u5141\u8bb8\u8f74\u548c\u9f7f\u8f6e\u4e4b\u95f4\u65e2\u53ef\u4ee5\u4f20\u9012\u626d\u77e9\uff0c\u4e5f\u53ef\u4ee5\u76f8\u5bf9\u6ed1\u52a8\uff0c\u5982\u679c\u9700\n\u8981\u7684\u8bdd\u3002\n\u6b63\u786e\u7b54\u6848\u662f\uff1a\n(D) \u82b1\u952e\u8054\u63a5\nGround Truth: (B)\u65e0\u952e\u8054\u63a5\nError Category: Answer Extraction Error\nError Reason: The answer extracted by the code is not the correct answer for GPT-4V. The question \nasked to identify the type of connection shown in the image, and GPT-4V incorrectly chose (D) \nKeyed Connection, while the actual connection shown in the image is a Keyless Connection. This \nindicates that GPT-4V made an error in extracting the correct answer from the text and image, \nselecting an inappropriate answer.\n<\u56fe\u7247 1>\nFigure B224: A sample error case of Technology and Engineering (subcategory: Mechanical\nEngineering).\nBack to List of figures\n243\nPREPRINT\n\u6280\u672f\u4e0e\u5de5\u7a0b: \u673a\u68b0\u5de5\u7a0b\nQuestion: \u5982<\u56fe\u7247 1>\u6240\u793a\u4e2d\u7684\u87ba\u7eb9\u8054\u63a5\u4e3a\uff08\uff09\nOption: \n(A) \u87ba\u6813\u8054\u63a5\n(B) \u87ba\u67f1\u8054\u63a5\n(C) \u87ba\u9489\u8054\u63a5\n(D) \u5706\u67f1\u8054\u63a5\nGPT-4V:\u8fd9\u5f20\u56fe\u7247\u663e\u793a\u7684\u662f\u4e00\u4e2a\u87ba\u7eb9\u8054\u63a5\u7684\u793a\u610f\u56fe\u3002\u6839\u636e\u56fe\u4e2d\u6240\u793a\u7684\u7ec4\u4ef6\u7279\u5f81\uff0c\u53ef\u4ee5\u770b\u5230\u6709\u4e00\u4e2a\u5e26\u87ba\n\u7eb9\u7684\u6746\u72b6\u90e8\u4ef6\u7a7f\u8fc7\u4e00\u4e2a\u5b54\uff0c\u5e76\u4e14\u5728\u4e24\u7aef\u6709\u87ba\u6bcd\u56fa\u5b9a\u3002\u8fd9\u79cd\u7c7b\u578b\u7684\u8054\u63a5\u901a\u5e38\u662f\u87ba\u6813\u8054\u63a5\uff0c\u56e0\u4e3a\u5b83\u5305\u62ec\u4e00\n\u4e2a\u87ba\u6813\u548c\u4e24\u4e2a\u87ba\u6bcd\uff0c\u87ba\u6813\u7a7f\u8fc7\u88ab\u8054\u63a5\u7684\u90e8\u4ef6\uff0c\u5e76\u4e14\u5728\u4e24\u7aef\u7528\u87ba\u6bcd\u56fa\u5b9a\u3002\n\u6b63\u786e\u7b54\u6848\u662f\uff1a\n(A) \u87ba\u6813\u8054\u63a5\nGround Truth: (C) \u87ba\u9489\u8054\u63a5\nError Category: Lack of Knowledge\nError Reason: Lack of the knowledge required to answer this question or a hallucination regarding \nthe knowledge needed to answer this question. The question asks for the identification of the type \nof threaded connection, and GPT-4V incorrectly identified it as a bolted connection, while the \ncorrect answer is a screw connection. This error may be due to GPT-4V lacking sufficient \nknowledge to distinguish between different types of threaded connections, leading to an incorrect \nanswer choice. \n<\u56fe\u7247 1>\nFigure B225: A sample error case of Technology and Engineering (subcategory: Mechanical\nEngineering).\nBack to List of figures\n244\n"
  },
  {
    "title": "CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation",
    "link": "https://arxiv.org/pdf/2401.12208.pdf",
    "upvote": "20",
    "text": "CheXagent: Towards a Foundation Model\nfor Chest X-Ray Interpretation\nZhihong Chen1\u2217\nMaya Varma1\u2217\nJean-Benoit Delbrouck1\u2217\nMagdalini Paschali1\nLouis Blankemeier1\nDave Van Veen1\nJeya Maria Jose Valanarasu1\nAlaa Youssef1\nJoseph Paul Cohen1\u2020\nEduardo Pontes Reis1\nEmily B. Tsai1\nAndrew Johnston1\nCameron Olsen1\nTanishq Mathew Abraham2\nSergios Gatidis1\nAkshay S. Chaudhari1\nCurtis Langlotz1\n1Stanford University\n2Stability AI\n{zhihongc,mvarma2,jbdel,paschali,akshaysc,langlotz}@stanford.edu\nAbstract\nChest X-rays (CXRs) are the most frequently performed imaging test in clinical\npractice. Recent advances in the development of vision-language foundation mod-\nels (FMs) give rise to the possibility of performing automated CXR interpretation,\nwhich can assist physicians with clinical decision-making and improve patient out-\ncomes. However, developing FMs that can accurately interpret CXRs is challenging\ndue to the (1) limited availability of large-scale vision-language datasets in the\nmedical image domain, (2) lack of vision and language encoders that can capture\nthe complexities of medical data, and (3) absence of evaluation frameworks for\nbenchmarking the abilities of FMs on CXR interpretation. In this work, we address\nthese challenges by first introducing CheXinstruct - a large-scale instruction-tuning\ndataset curated from 28 publicly-available datasets. We then present CheXagent -\nan instruction-tuned FM capable of analyzing and summarizing CXRs. To build\nCheXagent, we design a clinical large language model (LLM) for parsing radiology\nreports, a vision encoder for representing CXR images, and a network to bridge the\nvision and language modalities. Finally, we introduce CheXbench - a novel bench-\nmark designed to systematically evaluate FMs across 8 clinically-relevant CXR\ninterpretation tasks. Extensive quantitative evaluations and qualitative reviews\nwith five expert radiologists demonstrate that CheXagent outperforms previously-\ndeveloped general- and medical-domain FMs on CheXbench tasks. Furthermore,\nin an effort to improve model transparency, we perform a fairness evaluation across\nfactors of sex, race and age to highlight potential performance disparities. Our\nproject is at https://stanford-aimi.github.io/chexagent.html.\nPreprint.\narXiv:2401.12208v1  [cs.CV]  22 Jan 2024\nQ: Given the image(s), describe \n\"Mediastinal\u201d.\nA: The mediastinal contours are \nnotable only for tortuosity of \nthe aorta.\nLocal Findings Generation\nQ: Where is the opacity located?\nA: Right of the midline, \nsuperior to the right hilum\nOpen-ended VQA\nCheXinstruct\n6 Million CXR - Text -\n QA Triplets\nCheXbench\nBenchmark over \n8 tasks and 7 datasets\nCheXagent\n8 Billion Parameter \nInstruction-tuned CXR FM\nFigure 1: Overview of the proposed pipeline: CheXinstruct is a curation of datasets for instruction-tuning across\nvarious CXR tasks, CheXagent is our clinical FM for CXR interpretation, and CheXbench is our comprehensive\nFM evaluation benchmark. Two example CXR interpretation tasks include local findings generation and open-\nended visual question answering (VQA).\n1\nIntroduction\nFoundation models (FMs) have recently emerged as a powerful class of models capable of performing\na diverse range of reasoning and comprehension tasks [9]. The rise of FMs presents a major\nopportunity to re-imagine complex healthcare workflows that commonly require posing multi-faceted\nquestions from inherently multi-modal data. One particular clinical workflow is the analysis of\nmedical imaging data. Take for example chest X-ray (CXR) interpretation - the most common medical\nimaging study, with 70+ million CXRs performed annually in the US [33]. Here, radiologists interpret\nhundreds of images daily, translate imaging insights into textual descriptions, and summarize image\nfindings succinctly to other clinicians, while maximizing accuracy and minimizing bias. Moreover,\npatient-facing tasks for CXRs might include answering clarifying questions about these generated\nfindings. A CXR FM capable of automating or increasing the efficiency of these tasks can substantially\nimprove clinical decision-making as well as patient satisfaction and outcomes [67, 75, 96].\nHigh-quality CXR FMs must bridge the gap between vision and language. Most prior approaches for\nbuilding vision-language FMs focus on natural image settings, where high-performing methods align\nimage encoders with pretrained large language models (LLMs). Such instruction-tuned multimodal\nLLMs demonstrate superior capabilities across a range of perception and generation tasks. However,\ndeveloping instruction-tuned multimodal LLMs for medical imaging is challenging for the following\nreasons:\n1. There is a lack of vision-language medical imaging datasets. Instruction-tuned multimodal\nLLMs require large-scale, diverse training datasets with data triplets consisting of instruc-\ntions, images, and answers. Whereas existing approaches in the natural image domain\nleverage millions of training samples from datasets like LAION-5B [66], the availability\nof such data is severely limited in the medical domain due to patient privacy concerns that\nprevent dissemination of large corpora of medical text. As a result, existing instruction-\ntuned multimodal LLMs developed for CXR interpretation are trained on small datasets\nwith limited instruction diversity [74, 89].\n2. Existing vision and language encoders fail to capture the complexities of medical data.\nMultimodal LLMs require powerful image and text encoders. However, CXRs represent a\nlarge domain shift from natural images and text, and thus, existing pretrained vision and\nlanguage encoders struggle with medical knowledge grounding.\n3. Performing rigorous evaluations of multimodal LLMs in medicine is challenging. Multi-\nmodal LLMs generate open-ended responses, and evaluating free-text responses for factual\ncorrectness and completeness is arduous, particularly for large-scale evaluations that require\ndomain expertise. Previous FMs developed for CXR interpretation are primarily evaluated\nusing visual-question-answer (VQA) or text generation tasks. To the best of our knowledge,\n*Equal contributions.\n\u2020Work not related to position at Amazon.\n2\nthere are no existing benchmarks for quantitative and qualitative evaluation of multimodal\nLLMs across diverse, clinically-relevant CXR interpretation tasks.\nIn this work, we address these challenges by (i) introducing CheXinstruct, a large-scale instruction-\ntuning dataset with instruction-image-answer triplets, (ii) developing CheXagent, an instruction-tuned\nmultimodal LLM for CXR interpretation, and (iii) curating CheXbench, a novel benchmark to enable\nsystematic comparisons of FMs across 8 clinically-relevant CXR interpretation tasks. Below we\noutline our key contributions, also summarized in Fig. 1, that can help create capable and robust CXR\nFMs:\n1. CheXinstruct is an instruction-tuning dataset with 6M instruction-image-answer triplets\ndesigned to improve the ability of FMs to interpret CXRs. We collect instructions from 34\ntasks and 65 unique datasets, spanning categories including coarse- and fine-grained image\nunderstanding, question answering, and text generation.\n2. CheXagent is an instruction-tuned foundation model with 8B parameters capable of analyz-\ning images, understanding text, and generating responses. Our methodology for developing\nCheXagent includes training (1) a clinical LLM capable of understanding radiology reports,\n(2) a vision encoder capable of reading CXRs, and (3) a network to bridge the vision and\nlanguage modalities. We then perform instruction-tuning using data from CheXinstruct.\n3. CheXbench is a novel benchmark designed to rigorously evaluate FMs across two evaluation\naxes: image perception and textual understanding. We introduce 8 tasks across 7 CXR\ndatasets, and we evaluate performance using close-ended multiple-choice predictions as\nwell as open-ended text generation.\nWe use CheXbench to compare CheXagent with six prior general-domain and medical-domain FMs.\nAcross six visual tasks, the performance of CheXagent surpasses general-domain FMs by 97.5% and\nmedical-domain FMs by 55.7%. Across two text generation tasks, CheXagent provides medical text\nevaluated via automated quantitative metrics and qualitative metrics from five expert radiologists. We\nfurther provide an evaluation of potential model bias and highlight performance disparities across\ndemographic factors of sex, race and age to improve model transparency.\n2\nRelated Work\n2.1\nFoundation Models\nThe surge in available data and computational resources has enabled the creation of FMs that are\nversatile in addressing a wide array of tasks with a single generalist model.\nLanguage: Significant strides in FMs were first seen in natural language processing (NLP) because\nlarge-scale text data was easily accessible online. LLMs like GPT-3 [10], ChatGPT, GPT-4 [54],\nFLAN-T5 [18], Llama-2 [76, 77], Mistral 7B [35], and PaLM-2 [17, 3] excel at multiple text-based\ntasks using only prompting, enabling new possibilities, such as outperforming human experts for\nclinical text summarization [81].\nVision: In vision, FMs like Stable Diffusion [63], DALL-E [61, 60], and Imagen [64] were proposed\nfor the task of text-to-image generation. Models Like Segment Anything Model (SAM) [38] and\nSegment Everything Everywhere Model (SEEM) [98] have been developed to perform in-the-wild\nsegmentation. To achieve LLM-like scaling in vision, ViT-22B [19], a scaled-up version of ViT [23]\nwas introduced as a large vision encoder. In the medical domain, there are several datasets designed\nfor training FMs [49].\nVision-Language: Given the inherent connection between different modalities such as language\nand vision, inter-modality supervision has been critical in the development of many vision-language\nfoundation models (VLMs), like CLIP [59]. Several other large-scale VLMs like Flamingo [1],\nCoca [94], Qwen-VL [5], BLIP [44], LLaVA [47], PaLI-X [14], and CogVLM [83] have been\nintroduced with a focus on developing models powerful enough to perceive and understand both\ntext and images. To evaluate the performance of these models, benchmarks like Multimodal Large\nLanguage Model Evaluation (MME) [25] and SEED-Bench [42] have been introduced. These\nbenchmarks are carefully curated using manually generated instruction-answer pairs to avoid any\ndata leakage.\n3\nStage 0: Train a clinical LLM\nStage 1: Train a vision encoder\nStage 2: Train a vision-language bridger\nStage 3: Instruction Tuning\nVision\nEncoder\nImage\nQformer\nText\nVision\nEncoder\nImage\nQformer\nImage\nCaptioning\nBridger\nLLM\nImage\nCaptioning\nImage-Text\nContrastive\nVision\nEncoder\nImage\nQformer\nBridger\nLLM\nNext Word\nPrediction\nLLM\nNext Word\nPrediction\nText\n\ud83d\udd25\n\ud83d\udd25\n\ud83d\udd25\n\ud83d\udd25\n\ud83d\udd25\n\ud83d\udd25\n\ud83d\udd25\nText Corpus:\n(a) Overview of CheXinstruct\n(b) Training of CheXagent\n: Frozen \ud83d\udd25: Trainable\nFigure 2: Collection of datasets and tasks comprising CheXinstruct (Left). The four-stage training process of\nCheXagent, starting from adapting a general LLM for clinical use, through training a CXR vision encoder and a\nvision-language bridger, to the final stage of instruction tuning on diverse CXR tasks (Right).\n2.2\nMedical Foundation Models\nBiomedical language models (LMs) like BioNLP [41], BioBERT [40], PubMedBERT [29], BioGPT\n[48], Med-PaLM [40] and Med-PaLM 2 [71] have shown that LMs can be fine-tuned on curated\nbiomedical corpora to perform competitively on medical question answering tasks. Specifically,\nMed-PaLM 2 [71] achieved high scores in questions in the style of the US Medical Licensing\nExamination (USMLE) highlighting rapid progress towards physician-level performance for medical\nquestion answering. Since medicine inherently involves multiple modalities and covers a broad\nrange of tasks, multi-modal FMs are particularly suitable for this field. Works like LLaVA-Med\n[43], Med-Flamingo [51], and Med-PaLM M [78] have proposed generalist models spanning across\nmodalities like imaging and clinical text for tasks in radiology, dermatology, pathology, etc. RadFM\n[88] introduced a foundation model for radiology for both 2D and 3D imaging tasks. XrayGPT [74]\naligned a MedCLIP [85] visual encoder with Vicuna [16], which was finetuned using clinical text.\nRoentGen, a fine-tuned version of Stable Diffusion, allows generating CXR images using radiology\nreport text prompts [13, 12].\nMed-PaLM M, Med-Flamingo, RadFM, and LLaVA-Med aim to establish generalist frameworks that\nempower models to process data from various image modalities. In contrast, CheXagent is designed\nto excel in handling multiple tasks related to CXRs within a single model.\n2.3\nMost Related Work\nOur approach is most similar to the following works. (i) BLIP-2 [44]: our model architecture is\nclosely aligned with that of BLIP-2, which uses a Querying Transformer (QFormer) to bridge the\nvision-language modality gap; (ii) FLAN [86] and MultiInstruct [90]: our development of instruction\ntuning datasets derived from existing annotated datasets is inspired by these works.\n3\nCheXinstruct: Instruction-Tuning Dataset\nMotivation CheXinstruct seeks to cover a broad range of tasks shown in Fig. 2a to support training\nCXR FMs. These tasks can either (i) improve the abilities of FMs to understand CXRs or (ii) improve\nclinical decision making.\nDesign Scheme CheXinstruct is organized into four levels:\n4\nTable 1: The statistics of CheXinstruct, where the total and unique numbers of questions and images are shown\nalong with the average number of times each unique image is reused in the dataset (reuse).\nSplit\nQuestions\nImages\nTotal\nUnique\nReuse\nTotal\nUnique\nReuse\nTrain\n6.1M\n1.9M\n3.2\n9.3M\n1.1M\n8.6\nVal\n203.3K\n82.9K\n2.5\n228.9K\n31.7K\n7.2\nTest\n188.3K\n72.8K\n2.6\n233.7K\n49.5K\n4.7\n1. Capability Level, where we specify the essential skills and competencies that a CXR FM\nshould possess.\n2. Task Level, where we outline a wide array of specific tasks that align with each identified\ncapability, providing a clear framework for what the FM should be able to accomplish in the\ncontext of CXRs.\n3. Dataset Level, where we identify and categorize various CXR datasets, each associated\nwith a particular task. We focus on ensuring that the datasets are relevant and appropriately\nmatched to the tasks they are intended to support.\n4. Instance Level, where we define individual instances within each dataset at the most granular\nlevel. Each instance comprises an input (such as a CXR image) and its corresponding labels,\nforming the basic units for training and evaluating the FM.\nTasks CheXinstruct consists of five task categories according to their capabilities:\n\u2022 Coarse-grained Image Understanding, which defines the overall understanding of CXRs,\ne.g., view classification [36], and disease classification [84, 32, 62, 55, 30, 6, 34, 11, 69].\n\u2022 Fine-grained Image Understanding, which defines the localized understanding of CXRs,\ne.g., abnormality detection [53, 58], abnormality grounding [8], and foreign object detec-\ntion [91].\n\u2022 Question Answering, which defines the ability to respond to a question related to CXRs, e.g.,\nclose-ended visual question answering (VQA) [97, 57], open-ended VQA [7, 4], difference\nVQA [31], and text QA.\n\u2022 Text Generation, which defines the ability to generate radiology report sections, including a\ndescription of the findings [21, 82, 56], impression generation [24], findings summariza-\ntion [15], and local findings generation [36].\n\u2022 Miscellaneous: This category defines the miscellaneous abilities that are critical for a CXR\nFM, e.g., report evaluation [93, 50], and natural language explanation [37].1\nDataset Sources With the aforementioned taxonomy of tasks, we created CheXinstruct by either (i)\ncollecting existing publicly available datasets or (ii) curating a dataset with new labels from existing\ndatasets.\nFor every dataset, we derive relevant information to formulate distinct tasks. All the datasets are split\nfollowing their corresponding official splits if applicable.2\nTask Instruction Creation Inspired by [86], we create instructions by manually writing ten in-\nstruction templates for each task.3 Each instruction template contains placeholders (e.g., <IMAGE>,\n<QUESTION>, and <OPTIONS>), which are replaced with specific values when creating instruction-\nfollowing instances.\nFinally, each CheXinstruct instance is a triplet consisting of an image (or none, in the case of\nnon-image-based tasks), a question, and an answer.\nData Analysis We describe the overall statistics of CheXinstruct, including the number of questions-\n1Additional details about the tasks are presented in Appendix A.\n2More details of the dataset sources are presented in Appendix B.\n3The annotators are researchers in radiology and computer vision (CV).\n5\nanswer pairs and CXRs in Table 1. There are 6.1M question-answer pairs in CheXinstruct. Fur-\nthermore, since images from one dataset may be used across multiple tasks, we present additional\ninformation to provide intuition about the potential \u201coverlap\u201d problem among different datasets.4\n4\nCheXagent: Instruction-Following CXR FM\n4.1\nProblem Setup\nThe aim of CheXagent is a model that can \u201csee\u201d images xI and/or \u201cread\u201d text xT and generate\n\u201cresponses\u201d y. To this end, we introduce three components of our model: a vision encoder Mv, a\nvision-language bridger Mb, and a language decoder Ml. Therefore, the ultimate formulation of the\ndesired model is:\ny = Ml(Mb(Mv(xI)), xT ).\n(1)\n4.2\nTraining CheXagent\nIn this subsection, we present the four training stages of CheXagent, which are illustrated in Figure 2b.\nStage 0: Train a clinical LLM Numerous open-source biomedical large language models (LLMs)\nexist, such as BioMegatron [70], GatorTron [92], BioGPT [48], BioMedLM5, and PMC-LLaMA [87].\nThese models are predominantly trained on PubMed Central (PMC) articles rather than clinical texts.\nTo address this gap, we focus on developing a clinical LLM by adapting a general-domain LLM.\nOur starting point is Mistral-7B-v0.1 [35], chosen for its demonstrated strong reasoning capabilities\nacross various benchmarks.\nTo infuse the model with comprehensive medical and clinical knowledge, we utilize five distinct text\nsources for training: (i) PMC article abstracts, (ii) radiology reports from MIMIC-IV, (iii) MIMIC-\nIV discharge summaries, (iv) medical terms from Wikipedia, and (v) CXRs from CheXinstruct.\nImportantly, for MIMIC-IV data, we meticulously exclude any studies that are part of the validation\nand test sets of MIMIC-CXR to prevent data leakage.\nStage 1: Train a vision encoder for CXR Drawing inspiration from the works of [45] and [94],\nwe train our vision encoder using a variety of visual pre-training objectives, namely image-text\ncontrastive (ITC) and image captioning (IC). Our model architecture mirrors that of [45]. For training\npurposes, we utilize datasets comprising image-text pairs, specifically from MIMIC-CXR, PadChest,\nand BIMCV-COVID-19. Preliminary studies indicate that employing a combination of ITC and IC,\nakin to the approach in [94], yields enhanced performance in our specific context.\nStage 2: Train a vision-language bridger Following the training of the clinical LLM and the CXR\nvision encoder, we focus on developing a bridger model, Mb. This model is designed to map visual\ndata to the corresponding language (semantic) space. During training, we keep both the clinical LLM,\nMl, and the CXR vision encoder, Mv frozen. This approach is crucial for preventing catastrophic\nforgetting of prior knowledge during the image-text alignment process. For training Mb, we employ\nthe same datasets as in Stage 1, applying an image captioning objective for learning.\nStage 3: Instruction tuning Upon completing Stage 2, we obtain a multi-modal LLM tailored for\nCXR interpretation. In this stage, our focus shifts to training the model on a variety of tasks within\nthe CheXinstruct framework. Prior to training, we consider two key aspects: (i) reserving certain\ntask-dataset pairs exclusively for evaluation purposes, and (ii) determining optimal dataset ratios to\nensure balanced training across different capabilities. For the first, we sequester datasets including\nOpenI, SLAKE, and SIIM, which facilitates a more streamlined evaluation process (as discussed in\n\u00a75). For the second, we heuristically establish dataset ratios by carefully assessing the quality and\ndiversity of each dataset. This method leaves room for future exploration into automated dataset\nselection and balancing.6 This training is conducted using a next-word prediction objective, with the\nloss computation being limited to answers.\nImplementation Details For model architecture, we use EVA-CLIP-g [73] for the vision encoder and\nBERT [22] for the Qformer, a linear layer for the bridger, and Mistral for the LLM. The optimization\n4More details about the data analysis are reported in Appendix C.\n5https://huggingface.co/stanford-crfm/BioMedLM\n6More details on dataset ratios can be found in Appendix D.\n6\nTable 2: Results of Evaluation Axis 1 of CheXbench for tasks associated with image perception comparing\nCheXagent with general domain and medical domain FMs on several CXR datasets. For each task, we report\naccuracy.\nGeneral-domain FMs\nMedical-domain FMs\nCheXagent\nTask\nDataset\nBLIP-2\nInstructBLIP\nXrayGPT\nMedFlamingo\nRadFM\nLLaVA-Med\n(Ours)\nMIMIC-CXR\n28.8\n25.3\n24.0\n25.0\n28.5\n23.8\n97.5\nView Classification\nCheXpert\n38.0\n34.0\n33.0\n39.0\n37.0\n30.0\n96.7\nSIIM\n53.0\n54.0\n50.0\n50.0\n50.0\n49.0\n64.0\nRSNA\n50.0\n60.0\n50.0\n50.0\n50.0\n44.0\n81.0\nBinary Disease Classification\nCheXpert\n51.5\n53.2\n51.5\n48.5\n55.8\n47.6\n76.0\nOpenI\n40.2\n40.2\n45.4\n39.0\n42.2\n43.8\n47.0\nMIMIC-CXR\n25.6\n22.6\n24.1\n25.6\n27.2\n26.7\n30.3\nSingle Disease Identification\nCheXpert\n21.3\n19.5\n23.7\n26.0\n26.6\n26.0\n29.6\nOpenI\n48.5\n54.4\n57.7\n46.1\n52.8\n53.9\n55.6\nMIMIC-CXR\n30.0\n25.3\n39.0\n14.7\n22.3\n28.7\n55.3\nMulti Disease Identification\nCheXpert\n4.3\n6.1\n3.9\n7.1\n23.6\n2.1\n52.1\nRad-Restruct\n41.2\n42.4\n38.6\n45.5\n48.5\n34.9\n57.1\nVisual Question Answering\nSLAKE\n74.3\n86.4\n52.4\n64.8\n85.0\n55.5\n78.1\nImage-Text Reasoning\nOpenI\n47.9\n52.6\n52.4\n54.7\n54.0\n45.8\n59.0\nof trainable parameters is structured across various stages: (a) the entire LLM is trained in Stage 0;\n(b) in Stage 1, we train both the LoRA parameters of the vision encoder and the entire BERT encoder;\n(c) Stage 2 involves training the vision-language bridger; and (d) in Stage 3, we focus on training\nboth the bridger and the entire LLM. For optimization across all these stages, we employ the AdamW\noptimizer, with each stage having its own hyper-parameters7.\n5\nCheXbench: A Benchmark for Evaluating FMs on CXR Interpretation\nIn this section, we introduce CheXbench, an evaluation benchmark for enabling systematic compar-\nisons of FMs across 8 clinically-relevant CXR interpretation tasks.\n5.1\nBenchmark Design\nCheXbench is structured with two evaluation axes, crafted to assess crucial aspects of CXR interpreta-\ntion: image perception and textual understanding. The evaluations within CheXbench are conducted\non a specific subset of the CheXinstruct test set.8 In the following sections, we describe the tasks\nassociated with each evaluation axis.\nEvaluation Axis 1 - Image Perception: We first aim to evaluate the ability of FMs to understand the\nvisual content of CXRs. Our goals are to (1) evaluate the ability of FMs to generalize to a variety of\ndata distributions and (2) include a range of challenging, clinically-relevant tasks. To this end, we\nintroduce 6 tasks across 7 datasets; in particular, we note that 3 datasets (SIIM, SLAKE, and OpenI)\nwere completely held-out from CheXagent training to avoid any potential data leakage. In line with\nprior benchmarks designed for general domain FMs [42, 26], we use a multiple-choice format, where\nan image and a question are posed to the FM and multiple options are considered. Since open-ended,\nfree-text outputs from FMs are challenging to evaluate, we instead compute log-likelihood scores\nassociated with each option; the option with the highest score is then selected as the response. For\neach task, we report accuracy.\n\u2022 View Classification (700 samples): Given a CXR, the FM is tasked with identifying the\nimaging view. We perform view classification on the CheXpert test set with three possible\noptions (AP, PA, and Lateral) as well as the MIMIC-CXR test set with four possible options\n(AP, PA, Lateral, and LL).\n\u2022 Binary Disease Classification (433 samples): Given a CXR and a disease label, the FM is\ntasked with determining if the disease is present in the image. We perform binary disease\nclassification with twelve disease labels in the CheXpert test set, one disease label in the\nRSNA dataset, and one disease label in the SIIM dataset. There are two possible options\n(yes and no).\n7More details on the implementation can be found in Appendix E.\n8This is comprehensively detailed in Appendix F.\n7\nTable 3: Results of evaluation axis 2 of CheXbench for the task of findings\ngeneration comparing CheXagent with baseline medical-domain FMs using\nvarious metrics.\nPrivate Dataset\nMIMIC-CXR\nModel\nSize\nBERT-S\nCheXbert-S\nRadGraph-S\nBERT-S\nCheXbert-S\nRadGraph-S\nMedFlamingo\n8B\n8.5\n2.7\n1.7\n10.4\n3.2\n2.2\nLLaVA-Med\n8B\n12.5\n17.0\n4.2\n6.2\n17.5\n4.0\nRadFM\n14B\n35.7\n12.7\n5.1\n45.7\n17.5\n10.9\nXrayGPT\n8B\n40.1\n23.4\n9.0\n44.0\n24.2\n11.2\nCheXagent\n8B\n46.6\n23.7\n14.6\n50.4\n24.9\n18.6\nTable 4: Results of eval.\naxis 2 on findings summa-\nrization.\nMIMIC-CXR\nModel\nSize\nRouge-L\nLlama-2\n7B\n20.3\nVicuna\n7B\n21.5\nFLAN-T5\n11B\n42.5\nFLAN-UL2\n20B\n42.1\nCheXagent\n8B\n40.3\n\u2022 Single Disease Identification (864 samples): Given a CXR, the FM is tasked with identify-\ning the disease present in the image. We implement single disease identification with the\nCheXpert test set (where disease labels are obtained from expert radiologist annotations) and\nOpenI (where disease labels are obtained from Medical Subject Headings (MeSH) codes).\nThere are four possible options associated with each question, and each option includes a\nsingle disease label (e.g. \u201cpneumonia\").\n\u2022 Multi-Disease Identification (1387 samples): Given a CXR, the FM is tasked with identify-\ning a set of multiple diseases present in the image. We implement multi-disease classification\nusing CheXpert and OpenI. There are four possible options associated with each question,\nand each option includes a set of multiple diseases (e.g. \u201cpneumonia, pleural effusion,\ncardiomegaly\").\n\u2022 Visual-Question-Answering (1319 samples): We evaluate FMs across two standard VQA\nbenchmarks: SLAKE and Rad-Restruct. SLAKE consists of questions with two options\n(yes and no), and Rad-Restruct consists of questions with between two and four options.\n\u2022 Image-Text Reasoning (380 samples): Given a CXR, the FM is tasked with identifying the\ndisease in the image. In contrast to the single-disease classification task, this task employs\nhard negatives; each question is associated with two challenging options, distinguished\nonly by a single word indicating location or severity (e.g. \u201cleft-sided pleural effusion\" vs.\n\u201cright-sided pleural effusion\"). We implement image-text reasoning with the OpenI dataset.\nEvaluation Axis 2 - Textual Understanding: We additionally evaluate the ability of CheXagent and\nbaseline FMs to generate and summarize text. To this end, we introduce the following 2 tasks: we\nevaluate open-ended responses using a combination of automated metrics (ROUGE-L [46], CheXbert-\nScore [72], BERT-Score [95], RadGraph-Score [20], and GPT-4) and human expert evaluations from\nfive radiologists.\n\u2022 Findings Section Generation: Given an image, the FM is tasked with generating the findings\nsection of the radiology report. This task involves identifying key features of the image,\nsuch as the presence of abnormalities. We implement the findings section generation task\nwith MIMIC-CXR. Since existing medical FMs are often trained on MIMIC-CXR, we also\nevaluate the models on a private dataset.\n\u2022 Findings Summarization: Given the findings section of a radiology report, the FM is tasked\nwith summarizing the key observations into a concise statement. This task does not include\nimages. We evaluate Findings Summarization on MIMIC-CXR.\n5.2\nEvaluation Results\nIn our study, we employ CheXbench to compare CheXagent against two general-domain instruction-\ntuned FMs, InstructBLIP and BLIP2, which achieve state-of-the-art performance in previous re-\nsearch [42]. Additionally, we compare CheXagent with four medical FMs: XrayGPT, MedFlamingo,\nRadFM, and LLaVA-Med [74, 51, 43, 88]. This comparison aims to provide a comprehensive\nunderstanding of CheXagent\u2019s performance in relation to both general and medical-specific models.\nTable 2 provides results on the six tasks associated with evaluation axis 1. CheXagent demonstrates\nsuperior performance across image perception tasks, achieving an average improvement of 97.5%\nover general-domain FMs and an average improvement of 55.7% over medical FMs. We provide a\ndetailed breakdown of CheXagent performance:\n8\n59%\n13%\n28%\n66%\n75%\n96%\n18%\n16%\n16%\n8%\n2%\n1%\nFigure 3: GPT-4 evaluations demonstrate that the reports generated by CheXagent outperform medical-domain\nFMs for the findings generation task on MIMIC-CXR.\n\u2022 On view classification, CheXagent achieves near perfect performance, demonstrating a\n68.7 point (238%) improvement over the closest baseline on MIMIC-CXR and a 57.7 point\n(148%) improvement over the closest baseline on CheXpert. The majority of general-domain\nand medical-domain FMs demonstrate performance near random.\n\u2022 On binary disease classification, single disease, and multi-disease identification, CheXagent\ndemonstrates an average improvement of 11.6 points over the closest baseline. In particular,\nwe note that CheXagent demonstrates superior performance on the SIIM and OpenI datasets,\nwhich were completely held-out from training CheXagent; this suggests the ability of our\nmodel to generalize to diverse CXRs. On multi-disease classification on OpenI, XrayGPT\noutperforms CheXagent by 1.1 points; this could be attributed to the fact that XrayGPT was\nfine-tuned on samples from OpenI, which would bias its classification accuracy on this task.\n\u2022 On visual question answering, CheXagent outperforms baselines on Rad-Restruct (achiev-\ning a 8.6 point improvement over the closest baseline) and is competitive with existing\napproaches on SLAKE. We note that the SLAKE and Rad-Restruct datasets were com-\npletely held-out from CheXagent training, whereas RadFM was trained using samples from\nSLAKE.\n\u2022 On image-text reasoning, CheXagent achieves a 4.3 point (7.8%) improvement over the\nclosest baseline, MedFlamingo. We observe that the image-text reasoning task, which was\nexplicitly designed to include hard negatives, is particularly challenging for all evaluated\nFMs.\nTables 3, 4 and Figure 3 provide results on the two tasks associated with evaluation axis 2, medical\ntext generation and summarization. We provide a detailed breakdown of CheXagent performance\nbelow:\n\u2022 On findings section generation, CheXagent outperforms all medical FMs across all metrics\non both the private dataset and MIMIC-CXR. In particular, CheXagent achieves an average\nimprovment of 6.5 points on RadGraph scores and 0.5 points on CheXbert scores; this is\nnotable since these metrics directly evaluate factual correctness. Figure 3 shows results\nfrom automated evaluations using GPT-4. For 152 randomly selected samples from our\nprivate dataset, we provided GPT-4 with a reference report, the findings section generated\nby CheXagent, and the findings section generated by each of the medical FM baselines;\nGPT-4 was then prompted to select the report with the highest accuracy. As shown in Figure\n3, our GPT-4 evaluations demonstrate that CheXagent generates high-quality reports when\ncompared to other medical FMs.\n\u2022 For findings summarization, as shown in Table 4, CheXagent outperforms LLMs of compa-\nrable size on the Rouge-L metric and achieves comparable performance to LLMs with more\nthan twice the number of parameters [80, 81].\n6\nHuman Evaluation\nTo complement the quantitative results presented in Section 5.2, we conduct a reader study in which\nfive radiologists compare text generated by CheXagent against text written by a physician. This\nstudy includes the two textual understanding tasks described in Section 5.1: findings generation and\n9\nWhich radiology report \ufb01ndings section...\n[Completeness] \n... more completely captures important information?\n[Correctness]\u00a0 \u00a0  \n... includes less false information?\n[Conciseness]\u00a0 \n... contains less non-important information?\nCheXagent\nsigni\ufb01cantly\n10\n5\n0\n-5\n-10\nCheXagent\nslightly\n \nneither\nPhysician\nslightly\nPhysician\nsigni\ufb01cantly\nFigure 4: Reader study with five radiologists. Top: Study design comparing the findings sections of CheXagent\nvs. that of human experts for two tasks (rows) across three attributes (columns). Bottom: Results. Compared\nto humans, CheXagent achieves parity in report summarization, while the results demonstrate a gap between\nCheXagent report generation and human-level expertise.\nCheXagent: the right-sided chest tube has been removed. there\nis no evidence of pneumothorax. there is a small right pleural\neffusion. bibasilar atelectasis is present. there is no\npulmonary edema. the heart size is normal. the mediastinal\ncontours are normal. the hilar contours are normal. there is no\npneumothorax. \nPhysician: right-sided chest tube remains in place, with slight\nincrease in size of a small right pleural effusion, but no\nvisible pneumothorax. bibasilar linear atelectasis has slightly\nworsened, and there is a persistent small left pleural\neffusion. \nColor key:\u00a0 \u00a0 \u00a0\u00a0Correct\u00a0 \u00a0 \u00a0\u00a0Error\u00a0 \u00a0 \u00a0\u00a0Refers to prior study\nFigure 5: Comparison of CheXagent (top) against physician (bottom) on report generation. Radiologist reports\noften refer to past studies (purple), a context not available to CheXagent. The presence of these references in the\ntraining set perhaps contributes to CheXagent\u2019s error (red) in an otherwise accurate report (green), motivating\nfuture work for vision-language models.\nfindings summarization on MIMIC-CXR. For both tasks, each radiologist views the same set of 20\nrandomly selected samples, which contain the task input as well as an A/B comparison (CheXagent\nvs. physician) of the task outputs. Radiologists then grade these outputs using a five-point Likert\nscale across: completeness, correctness, and conciseness.\nStudy results in Figure 4 show that CheXagent is comparable to physicians for findings summarization\nand that text from human experts achieves higher scores in terms of findings generation. Although\nCheXagent outperforms other vision-language models in quantitative metrics, these findings indicate\nthat further improvement could facilitate closing the gap between CheXagent and human radiologists.\nWe now discuss qualitative findings to better understand the opportunity for future improvement\nin vision-language models. As shown in Figure 5, physicians\u2019 reports typically reference past\npatient studies to track changes over time. Thus, the capabilities of CheXagent, which is trained on\ncross-sectional reports, could be further improved by incorporating longitudinal data during training.\nWe also observed that the model frequently produced a uniform distance measurement of 3.5 cm in\nits generated text, irrespective of context. This issue arises because CheXagent is not designed to\ndirectly estimate physical distances or other quantities from images. Thus, addressing this limitation\nrepresents yet another avenue for advancement in vision-language models.\n7\nFairness Evaluation\nRecent studies [28, 68] highlight the presence of biases in AI models used in radiology, raising\nconcerns about their equitable application across diverse populations. For fairness evaluation, we\ntest CheXagent on a subset of the CheXpert public test set, annotated by two expert radiologists\nto avoid label noise [65], using frontal view CXRs from individuals self-reporting as Asian, White,\nor Black. In total we use 159 unique subjects labeled as \u201cNo Finding\" and \u201cCardiomegaly\". To\ncreate a balanced test set, we resample with replacement [27] to ensure balanced disease prevalence\n10\nFigure 6: Evaluation of CheXagent subgroup performance on cardiomegaly classification investigating potential\nmodel biases. F1 Scores vary across sex, racial groups, and age categories.\nand subgroup representation. We generate 2000 bootstrap samples to calculate mean and standard\ndeviation of F1-scores.\nWe evaluate model performance for the detection of cardiomegaly with the prompt \u201cDoes this chest\nX-ray contain cardiomegaly?\" with possible answers \u201cYes\" and \u201cNo\". Detection of cardiomegaly is\ncrucial for early diagnosis, and treatment of heart conditions [2], with studies revealing significant\ndisparities in heart disease death rates based on demographic factors [79]. Our findings, shown in\nFig. 6 reveal disparities; F1-scores are higher for males compared to females and vary across racial\ngroups, with the model performing best for the Black subgroup and worst for the Asian subgroup.\nThis could reflect inherent differences in the presentation of cardiomegaly across races, and could\nbe influenced by the limited samples of 14 Black and 30 unique Asian subjects included in the\ntest set. Age-wise, the model performs better for the 65+ age group compared to the 0-65 group,\npotentially due to a higher prevalence of cardiomegaly in older patients, and age-related physiological\ndifferences. These results are consistent with existing literature [28] and underscore the need for\ncontinued efforts in mitigating biases in AI models used in healthcare. An effective approach to\naddress this issue is curating larger and more diverse datasets, which can help in developing models\nthat are more representative and equitable across different patient demographics.\n8\nConclusion\nIn conclusion, our work represents progress towards automated CXR interpretation. We introduce (i)\nCheXinstruct, an instruction-tuning dataset, (ii) CheXagent, an 8B-parameter vision-language FM and\ndemonstrate its abilities through (iii) CheXbench, our benchmarking framework including 8 tasks over\n7 datasets. CheXagent achieves improvement in visual perception and text generation tasks compared\nto general- and medical-domain LLMs and is validated by five expert radiologists. Furthermore,\nour fairness analysis across sex, race, and age contributes to the ongoing efforts to enhance model\ntransparency in healthcare AI. The release of CheXinstruct, CheXagent, and CheXbench to the public\ndomain not only underscores our commitment to advancing medical AI but also sets a new benchmark\nfor future developments in this critical area of research.\nAcknowledgements\nMV is supported by graduate fellowship awards from the Department of Defense (NDSEG) and\nthe Knight-Hennessy Scholars program at Stanford University. AC receives research support from\nthe National Institutes of Health (grants - R01 HL167974, R01 AR077604, R01 EB002524, R01\nAR079431, P41 EB027060, and contracts 75N92020C00008, 75N92020C00021); and from GE\nHealthcare and Philips. Research reported in this publication was made possible in part by the\nNational Institute of Biomedical Imaging and Bioengineering (NIBIB) of the National Institutes of\nHealth under contracts 75N92020C00008 and 75N92020C00021, and by grant #1R18HS028955 from\nthe Agency for Health Research and Quality. This work is supported in part by MIDRC (The Medical\nImaging and Data Resource Center), made possible by the National Institute of Biomedical Imaging\nand Bioengineering (NIBIB) of the National Institutes of Health under contract 75N92020D00021.\nWe acknowledge support by Stability AI in providing computational support for this work.\n11\nReferences\n[1] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican,\nM. Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in\nNeural Information Processing Systems, 35:23716\u201323736, 2022. 3\n[2] S. S. Alghamdi, I. Abdelaziz, M. Albadri, S. Alyanbaawi, R. Aljondi, and A. Tajaldeen. Study\nof cardiomegaly using chest x-ray. Journal of Radiation Research and Applied Sciences, 13(1):\n460\u2013467, 2020. 11\n[3] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa,\nP. Bailey, Z. Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023. 3\n[4] S. Bae, D. Kyung, J. Ryu, E. Cho, G. Lee, S. Kweon, J. Oh, L. Ji, E. I. Chang, T. Kim, et al.\nEhrxqa: A multi-modal question answering dataset for electronic health records with chest\nx-ray images. arXiv preprint arXiv:2310.18652, 2023. 5, 19\n[5] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou. Qwen-vl: A\nfrontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966,\n2023. 3\n[6] S. Bannur, S. Hyland, Q. Liu, F. P\u00e9rez-Garc\u00eda, M. Ilse, D. C. de Castro, B. Boecking, H. Sharma,\nK. Bouzid, A. Schwaighofer, et al. Ms-cxr-t: Learning to exploit temporal structure for\nbiomedical vision-language processing, 2023. 5, 19\n[7] A. Ben Abacha, S. A. Hasan, V. V. Datla, D. Demner-Fushman, and H. M\u00fcller. Vqa-med:\nOverview of the medical visual question answering task at imageclef 2019. In Proceedings of\nCLEF (Conference and Labs of the Evaluation Forum) 2019 Working Notes. 9-12 September\n2019, 2019. 5, 19\n[8] B. Boecking, N. Usuyama, S. Bannur, D. C. Castro, A. Schwaighofer, S. Hyland, M. Wetscherek,\nT. Naumann, A. Nori, J. Alvarez-Valle, et al. Making the most of text semantics to improve\nbiomedical vision\u2013language processing. In European conference on computer vision, pages\n1\u201321. Springer, 2022. 5, 19\n[9] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein,\nJ. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models.\narXiv preprint arXiv:2108.07258, 2021. 2\n[10] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural\ninformation processing systems, 33:1877\u20131901, 2020. 3\n[11] A. Bustos, A. Pertusa, J.-M. Salinas, and M. De La Iglesia-Vaya. Padchest: A large chest x-ray\nimage dataset with multi-label annotated reports. Medical image analysis, 66:101797, 2020. 5,\n19\n[12] P. Chambon, C. Bluethgen, J.-B. Delbrouck, R. Van der Sluijs, M. Po\u0142acin, J. M. Z. Chaves, T. M.\nAbraham, S. Purohit, C. P. Langlotz, and A. Chaudhari. Roentgen: Vision-language foundation\nmodel for chest x-ray generation, 2022. URL https://arxiv.org/abs/2211.12737. 4\n[13] P. Chambon, C. Bluethgen, C. P. Langlotz, and A. Chaudhari. Adapting pretrained vision-\nlanguage foundational models to medical imaging domains, 2022. 4\n[14] X. Chen, J. Djolonga, P. Padlewski, B. Mustafa, S. Changpinyo, J. Wu, C. R. Ruiz, S. Goodman,\nX. Wang, Y. Tay, et al. Pali-x: On scaling up a multilingual vision and language model. arXiv\npreprint arXiv:2305.18565, 2023. 3\n[15] Z. Chen, M. Varma, X. Wan, C. Langlotz, and J.-B. Delbrouck. Toward expanding the scope of\nradiology report summarization to multiple anatomies and modalities. In A. Rogers, J. Boyd-\nGraber, and N. Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers), pages 469\u2013484, Toronto, Canada, July\n2023. Association for Computational Linguistics. 5, 19\n12\n[16] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E.\nGonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.\nSee https://vicuna. lmsys. org (accessed 14 April 2023), 2023. 4\n[17] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W.\nChung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv\npreprint arXiv:2204.02311, 2022. 3\n[18] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, M. De-\nhghani, S. Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint\narXiv:2210.11416, 2022. 3\n[19] M. Dehghani, J. Djolonga, B. Mustafa, P. Padlewski, J. Heek, J. Gilmer, A. P. Steiner, M. Caron,\nR. Geirhos, I. Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. In\nInternational Conference on Machine Learning, pages 7480\u20137512. PMLR, 2023. 3\n[20] J.-B. Delbrouck, P. Chambon, C. Bluethgen, E. Tsai, O. Almusa, and C. Langlotz. Improving\nthe factual correctness of radiology report generation with semantic rewards. In Y. Goldberg,\nZ. Kozareva, and Y. Zhang, editors, Findings of the Association for Computational Linguistics:\nEMNLP 2022, pages 4348\u20134360, Abu Dhabi, United Arab Emirates, Dec. 2022. Association\nfor Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.319. URL https:\n//aclanthology.org/2022.findings-emnlp.319. 8\n[21] D. Demner-Fushman, S. Antani, M. Simpson, and G. R. Thoma. Design and development\nof a multimodal biomedical information retrieval system. Journal of Computing Science and\nEngineering, 6(2):168\u2013177, 2012. 5, 19\n[22] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding.\nIn Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, 2019. 6\n[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,\nM. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for\nimage recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 3\n[24] S. Feng, D. Azzollini, J. S. Kim, C.-K. Jin, S. P. Gordon, J. Yeoh, E. Kim, M. Han, A. Lee,\nA. Patel, et al. Curation of the candid-ptx dataset with free-text reports. Radiology: Artificial\nIntelligence, 3(6):e210136, 2021. 5, 19\n[25] C. Fu, P. Chen, Y. Shen, Y. Qin, M. Zhang, X. Lin, Z. Qiu, W. Lin, J. Yang, X. Zheng, et al.\nMme: A comprehensive evaluation benchmark for multimodal large language models. arXiv\npreprint arXiv:2306.13394, 2023. 3\n[26] L. Gao, J. Tow, S. Biderman, S. Black, A. DiPofi, C. Foster, L. Golding, J. Hsu, K. McDonell,\nN. Muennighoff, J. Phang, L. Reynolds, E. Tang, A. Thite, B. Wang, K. Wang, and A. Zou. A\nframework for few-shot language model evaluation, Sept. 2021. URL https://doi.org/10.\n5281/zenodo.5371628. 7\n[27] B. Glocker, C. Jones, M. Bernhardt, and S. Winzeck. Algorithmic encoding of protected\ncharacteristics in chest x-ray disease detection models. Ebiomedicine, 89, 2023. 10\n[28] B. Glocker, C. Jones, M. Roschewitz, and S. Winzeck. Risk of bias in chest radiography deep\nlearning foundation models. Radiology: Artificial Intelligence, 5(6):e230060, 2023. 10, 11\n[29] Y. Gu, R. Tinn, H. Cheng, M. Lucas, N. Usuyama, X. Liu, T. Naumann, J. Gao, and H. Poon.\nDomain-specific language model pretraining for biomedical natural language processing. ACM\nTransactions on Computing for Healthcare (HEALTH), 3(1):1\u201323, 2021. 4\n[30] G. Holste, S. Wang, A. Jaiswal, Y. Yang, M. Lin, Y. Peng, and A. Wang. Cxr-lt: Multi-label\nlong-tailed classification on chest x-rays. PhysioNet, 2023. 5, 19\n13\n[31] X. Hu, L. Gu, Q. An, M. Zhang, L. Liu, K. Kobayashi, T. Harada, R. M. Summers, and Y. Zhu.\nExpert knowledge-aware image difference graph representation learning for difference-aware\nmedical visual question answering. In Proceedings of the 29th ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining, pages 4156\u20134165, 2023. 5, 19\n[32] J. Irvin, P. Rajpurkar, M. Ko, Y. Yu, S. Ciurea-Ilcus, C. Chute, H. Marklund, B. Haghgoo,\nR. Ball, K. Shpanskaya, et al. Chexpert: A large chest radiograph dataset with uncertainty\nlabels and expert comparison. In Proceedings of the AAAI conference on artificial intelligence,\nvolume 33, pages 590\u2013597, 2019. 5, 19\n[33] L. Iyeke, R. Moss, R. Hall, J. Wang, L. Sandhu, B. Appold, E. Kalontar, D. Menoudakos,\nM. Ramnarine, S. P. LaVine, et al. Reducing unnecessary \u2018admission\u2019chest x-rays: An initiative\nto minimize low-value care. Cureus, 14(10), 2022. 2\n[34] S. Jaeger, S. Candemir, S. Antani, Y.-X. J. W\u00e1ng, P.-X. Lu, and G. Thoma. Two public chest\nx-ray datasets for computer-aided screening of pulmonary diseases. Quantitative imaging in\nmedicine and surgery, 4(6):475, 2014. 5, 19\n[35] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand,\nG. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.\n3, 6\n[36] A. E. Johnson, T. J. Pollard, N. R. Greenbaum, M. P. Lungren, C.-y. Deng, Y. Peng, Z. Lu, R. G.\nMark, S. J. Berkowitz, and S. Horng. Mimic-cxr-jpg, a large publicly available database of\nlabeled chest radiographs. arXiv preprint arXiv:1901.07042, 2019. 5\n[37] M. Kayser, C. Emde, O.-M. Camburu, G. Parsons, B. Papiez, and T. Lukasiewicz. Explaining\nchest x-ray pathologies in natural language. In International Conference on Medical Image\nComputing and Computer-Assisted Intervention, pages 701\u2013713. Springer, 2022. 5, 19\n[38] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C.\nBerg, W.-Y. Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023. 3\n[39] J. J. Lau, S. Gayen, A. Ben Abacha, and D. Demner-Fushman. A dataset of clinically generated\nvisual questions and answers about radiology images. Scientific data, 5(1):1\u201310, 2018. 19\n[40] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang. Biobert: a pre-trained\nbiomedical language representation model for biomedical text mining. Bioinformatics, 36(4):\n1234\u20131240, 2020. 4\n[41] P. Lewis, M. Ott, J. Du, and V. Stoyanov. Pretrained language models for biomedical and clinical\ntasks: understanding and extending the state-of-the-art. In Proceedings of the 3rd Clinical\nNatural Language Processing Workshop, pages 146\u2013157, 2020. 4\n[42] B. Li, R. Wang, G. Wang, Y. Ge, Y. Ge, and Y. Shan. Seed-bench: Benchmarking multimodal\nllms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. 3, 7, 8\n[43] C. Li, C. Wong, S. Zhang, N. Usuyama, H. Liu, J. Yang, T. Naumann, H. Poon, and J. Gao.\nLlava-med: Training a large language-and-vision assistant for biomedicine in one day. arXiv\npreprint arXiv:2306.00890, 2023. 4, 8\n[44] J. Li, D. Li, C. Xiong, and S. Hoi. Blip: Bootstrapping language-image pre-training for\nunified vision-language understanding and generation. In International Conference on Machine\nLearning, pages 12888\u201312900. PMLR, 2022. 3, 4\n[45] J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with\nfrozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. 6\n[46] C.-Y. Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization\nbranches out, pages 74\u201381, 2004. 8\n[47] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485,\n2023. 3\n14\n[48] R. Luo, L. Sun, Y. Xia, T. Qin, S. Zhang, H. Poon, and T.-Y. Liu. Biogpt: generative pre-trained\ntransformer for biomedical text generation and mining. Briefings in Bioinformatics, 23(6):\nbbac409, 2022. 4, 6\n[49] X. Mei, Z. Liu, P. M. Robson, B. Marinelli, M. Huang, A. Doshi, A. Jacobi, C. Cao, K. E. Link,\nT. Yang, et al. Radimagenet: an open radiologic deep learning research dataset for effective\ntransfer learning. Radiology: Artificial Intelligence, 4(5):e210315, 2022. 3\n[50] Y. Miura, Y. Zhang, E. Tsai, C. Langlotz, and D. Jurafsky. Improving factual completeness\nand consistency of image-to-text radiology report generation. In Proceedings of the 2021\nConference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 5288\u20135304, 2021. 5, 19\n[51] M. Moor, Q. Huang, S. Wu, M. Yasunaga, C. Zakka, Y. Dalmia, E. P. Reis, P. Rajpurkar,\nand J. Leskovec. Med-flamingo: a multimodal medical few-shot learner. arXiv preprint\narXiv:2307.15189, 2023. 4, 8\n[52] S. Mukherjee, A. Mitra, G. Jawahar, S. Agarwal, H. Palangi, and A. Awadallah. Orca: Progres-\nsive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707, 2023.\n19\n[53] H. Q. Nguyen, K. Lam, L. T. Le, H. H. Pham, D. Q. Tran, D. B. Nguyen, D. D. Le, C. M. Pham,\nH. T. Tong, D. H. Dinh, et al. Vindr-cxr: An open dataset of chest x-rays with radiologist\u2019s\nannotations. Scientific Data, 9(1):429, 2022. 5, 19\n[54] OpenAI. Gpt-4 technical report, 2023. 3\n[55] M. Pavlova, T. Tuinstra, H. Aboutalebi, A. Zhao, H. Gunraj, and A. Wong. Covidx cxr-3: a\nlarge-scale, open-source benchmark dataset of chest x-ray images for computer-aided covid-19\ndiagnostics. arXiv preprint arXiv:2206.03671, 2022. 5, 19\n[56] O. Pelka, S. Koitka, J. R\u00fcckert, F. Nensa, and C. M. Friedrich. Radiology objects in context\n(roco): a multimodal image dataset. In Intravascular Imaging and Computer Assisted Stenting\nand Large-Scale Annotation of Biomedical Data and Expert Label Synthesis, pages 180\u2013189.\nSpringer, 2018. 5, 19\n[57] C. Pellegrini, M. Keicher, E. \u00d6zsoy, and N. Navab. Rad-restruct: A novel vqa benchmark\nand method for structured radiology reporting. In International Conference on Medical Image\nComputing and Computer-Assisted Intervention, pages 409\u2013419. Springer, 2023. 5, 19\n[58] H. H. Pham, T. T. Tran, and H. Q. Nguyen. Vindr-pcxr: An open, large-scale pediatric chest\nx-ray dataset for interpretation of common thoracic diseases. PhysioNet, 2022. 5, 19\n[59] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,\nP. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision.\nIn International conference on machine learning, pages 8748\u20138763. PMLR, 2021. 3\n[60] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever.\nZero-shot text-to-image generation. In International Conference on Machine Learning, pages\n8821\u20138831. PMLR, 2021. 3\n[61] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image\ngeneration with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. 3\n[62] E. P. Reis, J. P. de Paiva, M. C. da Silva, G. A. Ribeiro, V. F. Paiva, L. Bulgarelli, H. M. Lee, P. V.\nSantos, V. M. Brito, L. T. Amaral, et al. Brax, brazilian labeled chest x-ray dataset. Scientific\nData, 9(1):487, 2022. 5, 19\n[63] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis\nwith latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 10684\u201310695, 2022. 3\n15\n[64] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gon-\ntijo Lopes, B. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion models\nwith deep language understanding. Advances in Neural Information Processing Systems, 35:\n36479\u201336494, 2022. 3\n[65] A. Saporta, X. Gui, A. Agrawal, A. Pareek, S. Q. Truong, C. D. Nguyen, V.-D. Ngo, J. Seekins,\nF. G. Blankenberg, A. Y. Ng, et al. Benchmarking saliency methods for chest x-ray interpretation.\nNature Machine Intelligence, 4(10):867\u2013878, 2022. 10\n[66] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes,\nA. Katta, C. Mullis, M. Wortsman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. Advances in Neural Information Processing Systems, 35:\n25278\u201325294, 2022. 2\n[67] J. C. Seah, C. H. Tang, Q. D. Buchlak, X. G. Holt, J. B. Wardman, A. Aimoldin, N. Esmaili,\nH. Ahmad, H. Pham, J. F. Lambert, et al. Effect of a comprehensive deep-learning model on the\naccuracy of chest x-ray interpretation by radiologists: a retrospective, multireader multicase\nstudy. The Lancet Digital Health, 3(8):e496\u2013e506, 2021. 2\n[68] L. Seyyed-Kalantari, H. Zhang, M. B. McDermott, I. Y. Chen, and M. Ghassemi. Underdiagnosis\nbias of artificial intelligence algorithms applied to chest radiographs in under-served patient\npopulations. Nature medicine, 27(12):2176\u20132182, 2021. 10\n[69] G. Shih, C. C. Wu, S. S. Halabi, M. D. Kohli, L. M. Prevedello, T. S. Cook, A. Sharma, J. K.\nAmorosa, V. Arteaga, M. Galperin-Aizenberg, et al. Augmenting the national institutes of health\nchest radiograph dataset with expert annotations of possible pneumonia. Radiology: Artificial\nIntelligence, 1(1):e180041, 2019. 5, 19\n[70] H.-C. Shin, Y. Zhang, E. Bakhturina, R. Puri, M. Patwary, M. Shoeybi, and R. Mani. Biomega-\ntron: Larger biomedical domain language model. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing (EMNLP), pages 4700\u20134706, 2020. 6\n[71] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, L. Hou, K. Clark, S. Pfohl, H. Cole-Lewis,\nD. Neal, et al. Towards expert-level medical question answering with large language models.\narXiv preprint arXiv:2305.09617, 2023. 4\n[72] A. Smit, S. Jain, P. Rajpurkar, A. Pareek, A. Y. Ng, and M. Lungren. Combining automatic\nlabelers and expert annotations for accurate radiology report labeling using bert. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),\npages 1500\u20131519, 2020. 8\n[73] Q. Sun, Y. Fang, L. Wu, X. Wang, and Y. Cao. Eva-clip: Improved training techniques for clip\nat scale. arXiv preprint arXiv:2303.15389, 2023. 6\n[74] O. Thawkar, A. Shaker, S. S. Mullappilly, H. Cholakkal, R. M. Anwer, S. Khan, J. Laaksonen,\nand F. S. Khan. Xraygpt: Chest radiographs summarization using medical vision-language\nmodels. arXiv preprint arXiv:2306.07971, 2023. 2, 4, 8\n[75] E. Tiu, E. Talius, P. Patel, C. P. Langlotz, A. Y. Ng, and P. Rajpurkar. Expert-level detection\nof pathologies from unannotated chest x-ray images via self-supervised learning. Nature\nBiomedical Engineering, 6(12):1399\u20131406, 2022. 2\n[76] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal,\nE. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv\npreprint arXiv:2302.13971, 2023. 3\n[77] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,\nP. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288, 2023. 3\n[78] T. Tu, S. Azizi, D. Driess, M. Schaekermann, M. Amin, P.-C. Chang, A. Carroll, C. Lau,\nR. Tanno, I. Ktena, et al. Towards generalist biomedical ai. arXiv preprint arXiv:2307.14334,\n2023. 4\n16\n[79] M. Van Dyke, S. Greer, E. Odom, L. Schieb, A. Vaughan, M. Kramer, and M. Casper. Heart\ndisease death rates among blacks and whites aged 35 years\u2014united states, 1968\u20132015. MMWR\nSurveillance Summaries, 67(5):1, 2018. 11\n[80] D. Van Veen, C. Van Uden, M. Attias, A. Pareek, C. Bluethgen, M. Polacin, W. Chiu, J.-\nB. Delbrouck, J. Zambrano Chaves, C. Langlotz, A. Chaudhari, and J. Pauly. RadAdapt:\nRadiology report summarization via lightweight domain adaptation of large language models.\nIn D. Demner-fushman, S. Ananiadou, and K. Cohen, editors, The 22nd Workshop on Biomedical\nNatural Language Processing and BioNLP Shared Tasks, pages 449\u2013460, Toronto, Canada, July\n2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.bionlp-1.42. URL\nhttps://aclanthology.org/2023.bionlp-1.42. 9\n[81] D. Van Veen, C. Van Uden, L. Blankemeier, J.-B. Delbrouck, A. Aali, C. Bluethgen, A. Pareek,\nM. Polacin, W. Collins, N. Ahuja, et al. Clinical text summarization: Adapting large language\nmodels can outperform human experts. arXiv preprint arXiv:2309.07430, 2023. 3, 9\n[82] M. D. L. I. Vay\u00e1, J. M. Saborit, J. A. Montell, A. Pertusa, A. Bustos, M. Cazorla, J. Galant,\nX. Barber, D. Orozco-Beltr\u00e1n, F. Garc\u00eda-Garc\u00eda, et al. Bimcv covid-19+: a large annotated\ndataset of rx and ct images from covid-19 patients. arXiv preprint arXiv:2006.01174, 2020. 5,\n19\n[83] W. Wang, Q. Lv, W. Yu, W. Hong, J. Qi, Y. Wang, J. Ji, Z. Yang, L. Zhao, X. Song, et al.\nCogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023.\n3\n[84] X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, and R. M. Summers. Chestx-ray8: Hospital-scale\nchest x-ray database and benchmarks on weakly-supervised classification and localization of\ncommon thorax diseases. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 2097\u20132106, 2017. 5, 19\n[85] Z. Wang, Z. Wu, D. Agarwal, and J. Sun. Medclip: Contrastive learning from unpaired medical\nimages and text. arXiv preprint arXiv:2210.10163, 2022. 4\n[86] J. Wei, M. Bosma, V. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le.\nFinetuned language models are zero-shot learners. In International Conference on Learning\nRepresentations, 2021. 4, 5\n[87] C. Wu, W. Lin, X. Zhang, Y. Zhang, Y. Wang, and W. Xie. Pmc-llama: Towards building\nopen-source language models for medicine. arXiv preprint arXiv:2305.10415, 2023. 6\n[88] C. Wu, X. Zhang, Y. Zhang, Y. Wang, and W. Xie. Towards generalist foundation model for\nradiology. arXiv preprint arXiv:2308.02463, 2023. 4, 8\n[89] S. Xu, L. Yang, C. Kelly, M. Sieniek, T. Kohlberger, M. Ma, W.-H. Weng, A. Kiraly,\nS. Kazemzadeh, Z. Melamed, et al. Elixr: Towards a general purpose x-ray artificial in-\ntelligence system through alignment of large language models and radiology vision encoders.\narXiv preprint arXiv:2308.01317, 2023. 2\n[90] Z. Xu, Y. Shen, and L. Huang. Multiinstruct: Improving multi-modal zero-shot learning via\ninstruction tuning. arXiv preprint arXiv:2212.10773, 2022. 4\n[91] Z. Xue, S. Candemir, S. Antani, L. R. Long, S. Jaeger, D. Demner-Fushman, and G. R.\nThoma. Foreign object detection in chest x-rays. In 2015 IEEE international conference on\nbioinformatics and biomedicine (BIBM), pages 956\u2013961. IEEE, 2015. 5, 19\n[92] X. Yang, A. Chen, N. PourNejatian, H. C. Shin, K. E. Smith, C. Parisien, C. Compas, C. Martin,\nA. B. Costa, M. G. Flores, et al. A large language model for electronic health records. NPJ\nDigital Medicine, 5(1):194, 2022. 6\n[93] F. Yu, M. Endo, R. Krishnan, I. Pan, A. Tsai, E. P. Reis, E. K. U. N. Fonseca, H. M. H. Lee,\nZ. S. H. Abad, A. Y. Ng, et al. Evaluating progress in automatic chest x-ray radiology report\ngeneration. Patterns, 4(9), 2023. 5, 19\n17\n[94] J. Yu, Z. Wang, V. Vasudevan, L. Yeung, M. Seyedhosseini, and Y. Wu. Coca: Contrastive\ncaptioners are image-text foundation models. arXiv preprint arXiv:2205.01917, 2022. 3, 6\n[95] T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi. Bertscore: Evaluating text\ngeneration with bert. In International Conference on Learning Representations, 2019. 8\n[96] X. Zhang, C. Wu, Y. Zhang, W. Xie, and Y. Wang. Knowledge-enhanced visual-language\npre-training on chest radiology images. Nature Communications, 14(1):4542, 2023. 2\n[97] X. Zhang, C. Wu, Z. Zhao, W. Lin, Y. Zhang, Y. Wang, and W. Xie. Pmc-vqa: Visual instruction\ntuning for medical visual question answering. arXiv preprint arXiv:2305.10415, 2023. 5, 19\n[98] X. Zou, J. Yang, H. Zhang, F. Li, L. Li, J. Gao, and Y. J. Lee. Segment everything everywhere\nall at once. arXiv preprint arXiv:2304.06718, 2023. 3\n18\nA. Additional details on CheXinstruct tasks\nWe report the detail of each task in Table 5, where three levels (i.e., capabilities, tasks, datasets) and\nthe corresponding task descriptions are shown.\nB. Additional details on CheXinstruct dataset sources\nWe created CheXinstruct by either (i) collecting existing publicly available datasets or (ii) curating a\ndataset with new labels from existing datasets.\nWe include the following publicly-available datasets: BIMCV-COVID-19 [82], BRAX [62],\nCANDID-PTX [24], ChestXray14 [84], CheXpert [32], COVIDX-CXR-3 [55], CXR-LT [30],\nMedVQA-2019 [7], MIMIC-CXR-VQA [4], MIMIC-Diff-VQA [31], MIMIC-RRS [15], MIMIC-\nNLE [37], MS-CXR [8], MS-CXR-T [6], NLM-TB [34], Object-CXR [91], OpenI [21], PadCh-\nest [11], PMC-VQA [97], Rad-Restruct [57], RadNLI [50], ReXVal [93], ROCO [56], RSNA [69],\nSIIM9, VinDr-CXR [53], VinDr-PCXR [58], and VQA-RAD [39].\nAdditionally, we use the\nOpenOrca10 [52] text instruction tuning dataset to improve the diversity of text instructions.\nIn order to curate datasets with new labels based on existing datasets, we perform translation on\nthe radiology reports of BIMCV-COVID-19 and PadChest from Spanish to English using GPT-4.\nAdditionally, we filter VQA-RAD, SLAKE, MedVQA-2019, and PMC-VQA to include only chest\nX-ray images and their corresponding question-answer pairs. For the MIMIC-CXR dataset, we use\nGPT-4 to format its Findings and Impression sections to structured ones (called MIMIC-CXR-Struct),\nwhere each sentence is annotated with its corresponding anatomical structure and paraphrased to\neliminate comparisons to the patient\u2019s prior images.\nC. Extended analysis of CheXinstruct\nWe report CheXinstruct statistics in Table 6, where the number of instances and number of images\nassociated with each dataset are provided.\nD. CheXagent dataset ratios\nIn Table 7, we report the sampling ratios of each dataset used for Stage-3 training of CheXagent. The\nvalue \u201c0\u201d means that the corresponding dataset is held out.\nE. CheXagent implementation details\nThe training of CheXagent involves four stages and we report the training hyperparameters of each\nstage in Table 8.\nF. CheXbench Design\nCheXbench evaluates two components of CXR interpretation: image perception and textual under-\nstanding.\nEvaluation Axis 1 - Image Perception: We assess FM performance on image perception with\nsix multiple-choice tasks across seven datasets. In combination, these tasks include 5,083 samples\nspanning view classification, binary disease classification, single disease identification, multi-disease\nidentification, visual-question-answering, and image-text reasoning. In Figure 7, we provide examples\nfrom each task. Table 9 provides a summary of all tasks included in Evaluation Axis 1.\nEvaluation Axis 2 - Textual Understanding: We assess FM performance on textual understanding\nwith two tasks: findings generation and findings summarization. We evaluate performance using\nseveral automated metrics: ROUGE-L, CheXbert-Score, BERT-Score, RadGraph-Score, and GPT-4.\nWe provide prompts used for GPT-4 evaluations in Figure 8.\n9https://www.kaggle.com/datasets/jesperdramsch/siim-acr-pneumothorax-segmentation-data\n10https://huggingface.co/datasets/Open-Orca/OpenOrca\n19\nWhat is the view of this chest X-ray? \nA.\nAP \nB.\nPA \nC.\nLATERAL\nView Classi\ufb01cation\nDoes this chest X-ray contain \npneumonia? \nA.\nno \nB.\nyes\nBinary Disease Classi\ufb01cation\nWhich \ufb01nding is in this chest X-ray? \nA.\nlung lesion \nB.\nno \ufb01nding \nC.\ncardiomegaly \nD.\npleural effusion\nSingle Disease identi\ufb01cation\nWhich \ufb01ndings are in this chest X-ray? \nA.\npneumonia, cardiomegaly \nB.\nconsolidation, cardiomegaly \nC.\nlung lesion, support devices \nD.\npleural effusion, support devices\nMulti-Disease Identi\ufb01cation\nAre there abnormalities in the \npatient\u2019s right lung? \nA.\nno \nB.\nyes\nVisual-Question-Answering\nWhich \ufb01nding is in this chest X-ray? \nA.\nelevated right diaphragm \nB.\nelevated left diaphragm\nImage-Text Reasoning\nFigure 7: Examples of CheXbench image perception tasks: CheXbench includes six multiple-choice tasks that\nevaluate the ability of FMs to interpret chest X-rays.\nThe prompt for GPT-4 Evaluation\nPrompt:\nWe would like to request your feedback on the radiology reports generated by two AI\nassistants by comparing them to the reference report written by radiologists.\n[Reference Report]\n[reference]\n[End of Reference Report]\n[Assistant 1]\n[report1]\n[Assistant 1]\n[Assistant 2]\n[report2]\n[End of Assistant 2]\n[Requirements]\n1. The length of the reports is not important.\n2. The style of the reports is not important.\n3. The clinical accuracy is important especially for positive findings (i.e., diseases).\nTherefore, please focus on clinical accuracy instead of the length and style.\n[End of Requirements]\nPlease compare the accuracy of their generated reports. You should tell me whether Assistant\n1 is \u201cbetter than\u201d, \u201cworse than\u201d, or \u201cequal to\u201d Assistant 2. Please first compare the generated\nreports with the reference report to analyze which one is more in line with the given\nrequirements. In the last line, please output a single line containing only a single label\nselecting from \u201cAssistant 1 is better than Assistant 2\u201d, \u201cAssistant 1 is worse than Assistant\n2\u201d, and \u201cAssistant 1 is equal to Assistant 2\u201d.\nFigure 8: The prompt for GPT-4 Evaluation.\n20\nTable 5: Detailed task information of CheXinstruct, where three levels (i.e., capabilities, tasks, datasets) and the\ncorresponding task descriptions are shown.\nCapability\nTask\nDataset\nDescription\nCoarse-grained Image Understanding\nDisease Classification\nChestX-ray14\nGiven an <IMAGE>, the model is required to diagnose if the <DISEASE> exists.\nCheXpert\nMIMIC-CXR\nPadChest\nRSNA\nCOVIDX-CXR-3\nCXR-LT\nBRAX\nNLM-TB\nTemporal image classification\nMS-CXR-T\nGiven the <PRIOR IMAGE> and <CURRENT IMAGE>, identify the progression <LABEL>.\nView Classification\nMIMIC-CXR\nGiven the <IMAGE>, identify its <VIEW>.\nView Matching\nMIMIC-CXR\nGiven the <IMAGE 1> and <IMAGE 2>, if they belong to the same study.\nFine-grained Image Understanding\nAbnormality Detection\nVinDr-CXR\nGiven the <IMAGE>, localize the <REGION> of abnormalities.\nVinDr-PCXR\nAbnormality Grounding\nVinDr-CXR\nGiven the <IMAGE>, localize the <REGION> of <abnormality>.\nVinDr-PCXR\nPneumothorax Segmentation\nCandid\nGiven the <IMAGE>, segment the <REGION> of pneumothorax.\nSIIM\nRib Fracture Segmentation\nCandid\nGiven the <IMAGE>, segment the <REGION> of rib fractures.\nChest Tube Segmentation\nCandid\nGiven the <IMAGE>, segment the <REGION> of Chest Tubes.\nForeign Object Detection\nObject-CXR\nGiven the <IMAGE>, detect the <REGION> of external objects.\nPhrase Grounding\nMS-CXR\nGiven the <IMAGE> and <PHRASE>, identify the <REGION> of <PHRASE>.\nGrounded Captioning\nMS-CXR\nGiven the <IMAGE> and <REGION>, generate a <CAPTION>.\nGrounded Diagnosis\nMS-CXR\nGiven the <IMAGE> and <REGION>, generate a <DIAGNOSIS>.\nGrounded Phrase Extraction\nMS-CXR\nGiven the <IMAGE>, its <REPORT>, and <REGION>s, extract a <PHRASE>.\nPhrase Extraction and Grounding\nMS-CXR\nGiven the <IMAGE> and its <REPORT>, extract a <PHRASE> and localize its <REGION>.\nText Generation\nReport Generation\nPadChest\nGiven the <IMAGE>, generate its <REPORT>.\nBIMCV-COVID19\nFindings Generation\nMIMIC-CXR\nGiven the <IMAGE>, generate its <FINDINGS>.\nMIMIC-CXR-Struct\nOpenI\nImpression Generetion\nMIMIC-CXR\nGiven the <IMAGE>, generate its <IMPRESSION>.\nMIMIC-CXR-Struct\nOpenI\nCandid\nProgression Findings Generation\nMIMIC-CXR\nGiven the <REFERENCE IMAGE> and <MAIN IMAGE>, generate its <FINDINGS>.\nProgression Impression Generation\nMIMIC-CXR\nGiven the <REFERENCE IMAGE> and <MAIN IMAGE>, generate its <IMPRESSION>.\nFindings Summarization\nMIMIC-CXR\nGiven the <FINDINGS>, generate its <IMPRESSION>.\nOpenI\nMIMIC-III\nCaption Generation\nROCO\nGiven the <IMAGE>, generate its <CAPTION>.\nLocal Findings Generation\nMIMIC-CXR-Struct\nGiven the <IMAGE> and a anatomy, generate its <FINDINGS>.\nLocal Impression Generation\nMIMIC-CXR-Struct\nGiven the <IMAGE> and a anatomy, generate its <IMPRESSION>.\nQuestion Answering\nOpen-ended VQA\nVQA-RAD\nGiven the content of the given <IMAGE>, answer the <QUESTION>.\nSLAKE\nMedVQA-2019\nPMC-VQA\nRad-Restruct\nMIMIC-CXR-VQA\nClose-ended VQA\nVQA-RAD\nGiven the content of the given <IMAGE>, choose one option from the <OPTIONS> to answer the <QUESTION>.\nSLAKE\nPMC-VQA\nRad-Restruct\nMIMIC-CXR-VQA\nDifference VQA\nMIMIC-Diff-VQA\nGiven a <REFERENCE IMAGE> and a <MAIN IMAGE>, answer the <QUESTION>.\nText QA\nRadQA\nGiven <PARAGRAPH>, answer the <QUSETION>.\nMiscellaneous\nImage-Text Matching\nMIMIC-CXR\nGiven the <IMAGE> and <REPORT>, decide if they match.\nROCO\nImage-Text Selection\nMIMIC-CXR\nGiven the <IMAGE>, select the text that best matches the image from <OPTIONS>.\nROCO\nReport Evaluation\nReXVal\nGiven a <REFENCE REPORT> and a <GENERATED REPORT>, identify the <ERROR>.\nNatural Language Explanation\nMIMIC-NLE\nGven an <IMAGE> and <DISEASE>, generate the natural language <EXPLANATION>.\nNatural Language Inference\nRadNLI\nGiven a <PREMISE REPORT>, determine whether a <HYPOTHESIS REPORT> is entailment, contradiction, or neutral.\nTemporal Sentence Similarity\nMS-CXR-T\nGiven <SENTENCE 1> and <SENTENCE 2>, identify their <SIMILARITY> in terms of disease progression.\n21\nTable 6: The basic statistics of CheXinstruct, where the numbers of instances and images of each dataset are\nshown. The datasets are split following their official or traditional ways.\nTask-Dataset Pair\nInstances (train)\nInstances (val)\nInstances (test)\nImages (train)\nImages (val)\nImages (test)\n(Open-Ended VQA) VQA-RAD\n713\n119\n119\n73\n50\n50\n(Open-Ended VQA) SLAKE\n1,093\n233\n220\n90\n19\n17\n(Open-Ended VQA) MedVQA-2019\n78\n11\n1\n78\n11\n1\n(Open-Ended VQA) PMC-VQA\n747\n0\n229\n646\n0\n193\n(Open-Ended VQA) Rad-Restruct\n142,340\n17,641\n17,641\n2,972\n374\n374\n(Open-Ended VQA) MIMIC-CXR-VQA\n259,484\n63,078\n11,347\n127,798\n8,647\n500\n(Close-Ended VQA) VQA-RAD\n417\n69\n69\n69\n37\n37\n(Close-Ended VQA) SLAKE\n297\n59\n64\n90\n19\n17\n(Close-Ended VQA) MedVQA-2019\n1\n0\n0\n1\n0\n0\n(Close-Ended VQA) PMC-VQA\n682\n0\n209\n596\n0\n180\n(Close-Ended VQA) Rad-Restruct\n142,340\n17,641\n17,641\n2,972\n374\n374\n(Close-Ended VQA) MIMIC-CXR-VQA\n162,577\n39,376\n6,952\n106,209\n8,586\n500\n(Difference VQA) MIMIC-Diff-VQA\n160,054\n1,325\n2,967\n160,377\n1,332\n2,969\n(Text QA) RadQA\n4,878\n614\n614\n0\n0\n0\n(Image Classification) ChestXray14\n78,484\n11,211\n22,425\n78,484\n11,211\n22,425\n(Image Classification) CheXpert\n223,414\n234\n668\n223,414\n234\n668\n(Image Classification) MIMIC-CXR\n212,098\n1,714\n3,131\n348,516\n2,813\n4,896\n(Image Classification) PadChest\n109,845\n0\n0\n160,742\n0\n0\n(Image Classification) RSNA\n18,678\n4,003\n4,003\n18,678\n4,003\n4,003\n(Image Classification) COVIDX-CXR-3\n29,986\n0\n400\n29,986\n0\n400\n(Image Classification) CXR-LT\n155,349\n0\n0\n255,445\n0\n0\n(Image Classification) Brax\n23,276\n0\n0\n40,967\n0\n0\n(Image Classification) NLM-TB\n800\n0\n0\n800\n0\n0\n(Temporal Image Classification) MS-CXR-T\n985\n10\n50\n1,903\n19\n96\n(View Classification) MIMIC-CXR\n353,640\n2,867\n4,834\n353,640\n2,867\n4,834\n(View Matching) MIMIC-CXR\n34,174\n290\n320\n33,797\n293\n349\n(Phrase Grounding) MS-CXR\n964\n7\n189\n878\n5\n164\n(Grounded Captioning) MS-CXR\n964\n7\n189\n878\n5\n164\n(Grounded Diagnosis) MS-CXR\n964\n7\n189\n878\n5\n164\n(Grounded Phrase Extraction) MS-CXR\n527\n7\n9\n490\n5\n8\n(Phrase Extraction and Grounding) MS-CXR\n527\n7\n9\n490\n5\n8\n(Abnormality Detection) VinDr-CXR\n15,000\n0\n3,000\n15,000\n0\n3,000\n(Abnormality Grounding) VinDr-CXR\n30,282\n0\n4,022\n11,685\n0\n1,999\n(Grounded Diagnosis) VinDr-CXR\n17,880\n0\n2,345\n4,510\n0\n937\n(Abnormality Detection) VinDr-PCXR\n7,728\n0\n1,397\n7,728\n0\n1,397\n(Abnormality Grounding) VinDr-PCXR\n6,648\n0\n1,218\n4,477\n0\n809\n(Grounded Diagnosis) VinDr-PCXR\n4,788\n0\n851\n2,496\n0\n469\n(Pneumothorax Segmentation) Candid\n8,195\n0\n0\n8,195\n0\n0\n(Pneumothorax Segmentation) SIIM\n7,621\n1,709\n1,704\n7,621\n1,709\n1,704\n(Rib Fracture Segmentation) Candid\n670\n0\n0\n668\n0\n0\n(Chest Tube Segmentation) Candid\n2,846\n0\n0\n2,775\n0\n0\n(Foreign Object Detection) Object-CXR\n8,000\n1,000\n0\n8,000\n1,000\n0\n(Report Generation) PadChest\n109,792\n0\n0\n160,670\n0\n0\n(Report Generation) BIMCV-COVID19\n46,941\n0\n0\n65,421\n0\n0\n(Findings Generation) MIMIC-CXR\n152,173\n1,196\n2,347\n270,790\n2,130\n3,858\n(Findings Generation) MIMIC-CXR-Struct\n148,501\n1,164\n2,309\n264,777\n2,079\n3,786\n(Findings Generation) OpenI\n0\n0\n3,337\n0\n0\n6,473\n(Impression Generation) MIMIC-CXR\n185,816\n1,521\n2,224\n316,684\n2,573\n3,724\n(Impression Generation) MIMIC-CXR-Struct\n175,152\n1,430\n2,141\n298,108\n2,429\n3,584\n(Impression Generation) OpenI\n0\n0\n3,820\n0\n0\n7,418\n(Impression Generation) Candid\n18,307\n0\n0\n19,206\n0\n0\n(Findings Summarization) MIMIC-CXR\n125,417\n991\n1,624\n0\n0\n0\n(Progression Findings Generation) MIMIC-CXR\n19,107\n138\n483\n34,354\n258\n851\n(Progression Impression Generation) MIMIC-CXR\n26,646\n216\n313\n46,227\n369\n560\n(Findings Summarization) OpenI\n0\n0\n3,419\n0\n0\n0\n(Findings Summarization) MIMIC-III\n59,320\n7,413\n13,057\n0\n0\n0\n(Caption Generation) ROCO\n2,554\n293\n324\n2,554\n293\n324\n(Local Findings Generation) MIMIC-CXR-Struct\n1,059,903\n8,299\n16,275\n264,688\n2,078\n3,785\n(Local Impression Generation) MIMIC-CXR-Struct\n674,284\n5,686\n8,196\n297,598\n2,426\n3,582\n(Image-Text Matching) MIMIC-CXR\n675,978\n5,434\n9,142\n354,619\n2,866\n4,710\n(Image-Text Matching) ROCO\n5,108\n586\n648\n2,554\n293\n324\n(Image-Text Selection) MIMIC-CXR\n337,989\n2,717\n4,571\n354,619\n2,866\n4,710\n(Image-Text Selection) ROCO\n2,554\n293\n324\n2,554\n293\n324\n(Report Evaluation) ReXVal\n0\n0\n200\n0\n0\n0\n(Natural Language Explanation) MIMIC-NLE\n37,016\n273\n714\n51,503\n373\n944\n(Natural Language Inference) RadNLI\n0\n480\n480\n0\n0\n0\n(Temporal Sentence Similarity) MS-CXR-T\n0\n0\n361\n0\n0\n0\n(Named Entity Recognition) RadGraph\n541\n9\n50\n0\n0\n0\n22\nTable 7: The sampling ratio of each dataset in the third-stage training of CheXagent.\nTask-Dataset Pair\nSampling Ratio\n(Named Entity Recognition) RadGraph\n10.00\n(Abnormality Grounding) VinDr-CXR\n1.00\n(Abnormality Grounding) VinDr-PCXR\n1.00\n(Caption Generation) ROCO\n1.00\n(Close-Ended VQA) PMC-VQA\n1.00\n(Close-Ended VQA) VQA-RAD\n1.00\n(Foreign Object Detection) Object-CXR\n1.00\n(Grounded Captioning) MS-CXR\n1.00\n(Grounded Diagnosis) MS-CXR\n1.00\n(Grounded Diagnosis) VinDr-CXR\n1.00\n(Grounded Diagnosis) VinDr-PCXR\n1.00\n(Grounded Phrase Extraction) MS-CXR\n1.00\n(Image Classification) COVIDX-CXR-3\n1.00\n(Image Classification) NLM-TB\n1.00\n(Image Classification) RSNA\n1.00\n(Impression Generation) Candid\n1.00\n(Open-Ended VQA) MedVQA-2019\n1.00\n(Open-Ended VQA) PMC-VQA\n1.00\n(Open-Ended VQA) VQA-RAD\n1.00\n(Phrase Grounding) MS-CXR\n1.00\n(Pneumothorax Segmentation) Candid\n1.00\n(Progression Findings Generation) MIMIC-CXR\n1.00\n(Progression Impression Generation) MIMIC-CXR\n1.00\n(Text QA) RadQA\n1.00\n(View Matching) MIMIC-CXR\n0.50\n(Findings Generation) MIMIC-CXR-Struct\n0.40\n(Impression Generation) MIMIC-CXR-Struct\n0.30\n(Natural Language Explanation) MIMIC-NLE\n0.30\n(Report Generation) BIMCV-COVID19\n0.25\n(Findings Summarization) MIMIC-III\n0.20\n(Image Classification) ChestXray14\n0.20\n(Close-Ended VQA) MIMIC-CXR-VQA\n0.10\n(Findings Generation) MIMIC-CXR\n0.10\n(Findings Summarization) MIMIC-CXR\n0.10\n(Image Classification) Brax\n0.10\n(Image Classification) CheXpert\n0.10\n(Image Classification) PadChest\n0.10\n(Image-Text Matching) ROCO\n0.10\n(Image-Text Selection) ROCO\n0.10\n(Impression Generation) MIMIC-CXR\n0.10\n(Report Generation) PadChest\n0.10\n(View Classification) MIMIC-CXR\n0.10\n(Image Classification) MIMIC-CXR\n0.05\n(Open-Ended VQA) MIMIC-CXR-VQA\n0.05\n(Local Findings Generation) MIMIC-CXR-Struct\n0.02\n(Local Impression Generation) MIMIC-CXR-Struct\n0.02\n(Text Instructions) OpenOrca\n0.02\n(Difference VQA) MIMIC-Diff-VQA\n0.01\n(Image Classification) CXR-LT\n0.01\n(Image-Text Matching) MIMIC-CXR\n0.01\n(Image-Text Selection) MIMIC-CXR\n0.01\n(Abnormality Detection) VinDr-CXR\n0.00\n(Abnormality Detection) VinDr-PCXR\n0.00\n(Chest Tube Segmentation) Candid\n0.00\n(Close-Ended VQA) Rad-Restruct\n0.00\n(Close-Ended VQA) SLAKE\n0.00\n(Open-Ended VQA) Rad-Restruct\n0.00\n(Open-Ended VQA) SLAKE\n0.00\n(Phrase Extraction and Grounding) MS-CXR\n0.00\n(Pneumothorax Segmentation) SIIM\n0.00\n(Rib Fracture Segmentation) Candid\n0.00\n23\nTable 8: Training hyperparameters of CheXagent.\nConfiguration\nStage 0\nStage 1\nStage 2\nStage 3\nViT init.\n-\nEVA01-CLIP-g-14-plus\nViT Stage 1\nViT Stage 2\nLLM init.\nMistral-7B-v0.1\n-\nLLM Stage 0\nLLM Stage 2\nQformer init.\n-\nBERT Base\nQformer Stage 1\nQformer Stage 2\nImage resolution\n-\n4482\n4482\n4482\nViT sequence length\n-\n1024\n1024\n1024\nLLM sequence length\n2048\n-\n512\n512\nLearnable query numbers\n-\n128\n128\n128\nGlobal batch size\n2048\n256\n2048\n512\nOptimizer\nAdamW\nOptimizer hyperparameter\n\u03b21 = 0.9, \u03b22 = 0.999, eps = 1e \u2212 8\nPeak learning rate\n5e \u2212 6\n1e \u2212 4\n1e \u2212 4\n1e \u2212 6\nMinimum learning rate\n5e \u2212 7\n1e \u2212 5\n1e \u2212 5\n1e \u2212 7\nLearning rate schedule\ncosine\nWeight decay\n0.05\nGradient clip\n1.0\nNumerical precision\nbf16\nDeepSpeed\nZeRO stage 2\n-\n-\nZeRO stage 2\nTable 9: Statistics for image perception tasks in CheXbench (Evaluation Axis 1).\nTask\nDataset\nNum. Samples\nNum. Options\nMIMIC-CXR\n400\n4\nView Classification\nCheXpert\n300\n3\nSIIM\n100\n2\nRSNA\n100\n2\nBinary Disease Classification\nCheXpert\n233\n2\nOpenI\n500\n4\nMIMIC-CXR\n195\n4\nSingle Disease Identification\nCheXpert\n169\n4\nOpenI\n807\n4\nMIMIC-CXR\n300\n4\nMulti Disease Identification\nCheXpert\n280\n4\nRad-Restruct\n899\n2-4\nVisual Question Answering\nSLAKE\n420\n2\nImage-Text Reasoning\nOpenI\n380\n2\n24\n"
  },
  {
    "title": "SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities",
    "link": "https://arxiv.org/pdf/2401.12168.pdf",
    "upvote": "19",
    "text": "2024-1-23\nSpatialVLM: Endowing Vision-Language\nModels with Spatial Reasoning Capabilities\nBoyuan Chen\u2217,\u2020,1, Zhuo Xu\u2217,1, Sean Kirmani1, Danny Driess1, Pete Florence1\nBrian Ichter1, Dorsa Sadigh1, Leonidas Guibas2, Fei Xia1\n1Google DeepMind, 2Google Research\nCorrespond to: boyuanc@mit.edu, zhuoxu@google.com, xiafei@google.com\nWebsite: https://spatial-vlm.github.io/\nYou are a cleaning robot that is 1 \nmeter wide, now you are \nstanding in a living room and see \nthe image, you want to move \nfrom here to the door that leads \nto the backyard, do you think I \ncan go through the path between \nthe sofa and the table and \nchairs.\nUser\nAs an AI, I'm unable to physically interact with environments, but I can provide some \ninsights based on the image provided. \u2026 From the image, we can make some visual \nestimations. Judging from the image, it appears that the sofa and the table are quite \nclose to each other, and there may not be enough space to accommodate a 1-meter \nwide robot\u2026.\n[VLM Reasoning] How wide is the path between the sofa and the table and chairs? \n[VLM Answer] 1.56m [Answer] Yes, the robot can go through the path between the \nsofa and the table and chairs since it is wider than the robot's width.\nGPT-4V*\nSpatial-VLM\nWhich can is the closest to the \nyellow \ufb01nger that is on the right \nmost side of the picture, the \nblue, orange or the silver one?\nUser\nAfter estimating the distances by measuring the Euclidean distances between the \nyellow \ufb01nger and each of the cans, it is clear that the silver can is the closest to the \nyellow \ufb01nger with an approximate distance of 110 units. In comparison, the blue can \nis about 262 units away, and the orange can is approximately 233 units away from the \nyellow \ufb01nger. \n[VLM Reasoning] How far is the yellow \ufb01nger from the blue can? [VLM Answer] \n0.5m [VLM Reasoning] How far is the yellow \ufb01nger from the orange can? [VLM \nAnser] 0.59m [VLM Reasoning] How far is the yellow \ufb01nger from the silver can? \n[VLM Answer] 0.21m [Answer] The silver can is the closest to the yellow \ufb01nger that \nis on the right most side of the picture.\nGPT-4V\nSpatial-VLM\nFigure 1 | We present SpatialVLM, a data synthesis and pre-training mechanism to enhance VLMs\u2019\nspatial reasoning capabilities. We demonstrate that VLMs trained on our synthetic data exhibit strong\nspatial reasoning capabilities, and can generate metric distance estimation from 2D input images,\naddressing blind spots of current state-of-the-art VLMs like GPT-4V. (\u2217 GPT-4V accessed Nov. 2023).\nUnderstanding and reasoning about spatial relationships is a fundamental capability for Visual Question\nAnswering (VQA) and robotics. While Vision Language Models (VLM) have demonstrated remarkable\nperformance in certain VQA benchmarks, they still lack capabilities in 3D spatial reasoning, such as\nrecognizing quantitative relationships of physical objects like distances or size difference. We hypothesize\nthat VLMs\u2019 limited spatial reasoning capability is due to the lack of 3D spatial knowledge in training data\nand aim to solve this problem by training VLMs with Internet-scale spatial reasoning data. To this end, we\npresent a system to facilitate this approach. We first develop an automatic 3D spatial VQA data generation\nframework that scales up to 2 billion VQA examples on 10 million real-world images. We then investigate\nvarious factors in training recipe including data quality, training pipeline and VLM architecture. Our\nwork features the first Internet-scale 3D spatial reasoning dataset in metric space. By training a VLM on\nsuch data, we significantly enhance its ability on both qualitative and quantitative spatial VQA. Finally, we\ndemonstrate that this VLM unlocks novel downstream applications in chain-of-thought spatial reasoning\nand robotics due to its quantitative estimation capability.\n\u00a9 2024 Google DeepMind. All rights reserved\n\u2217 Equal contribution and alphabetically listed. \u2020 Work done while being a student researcher at Google DeepMind.\narXiv:2401.12168v1  [cs.CV]  22 Jan 2024\nSpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities\n1. Introduction\nVision language models (VLMs) have made significant progress in recent years across a variety of tasks\nincluding image captioning, visual question answering (VQA), embodied planning, action recognition,\nand more [2, 18, 25, 33]. While VLMs are powerful general-purpose models for a wide range of tasks,\nmost state-of-the-art VLMs still struggle with spatial reasoning, i.e. tasks that require understanding the\nposition of objects in 3D space, or spatial relationships between them. Spatial reasoning capabilities are\nuseful in their own right, but also for downstream applications such as in robotics or AR. For example,\na spatial reasoning-imbued VLM can be used as a better general-purpose reward annotator [54] and\nsuccess detector [19].\nThe exploration of foundation models like VLMs is often inspired by human capabilities. Humans,\nthrough embodied experiences and evolutionary development, possess innate spatial reasoning skills.\nWe effortlessly determine spatial relationships, such as the positioning of objects relative to each other\nor estimating distances and sizes, without complex chain-of-thoughts or mental computations. This\nnatural proficiency in direct spatial reasoning tasks contrasts with the current limitations of VLMs and\nthus prevents them from accomplishing real-world tasks that requires multiple steps of spatial reasoning.\nThis gap leads us to a compelling research question: can we imbue VLMs with spatial reasoning abilities\nakin to those of humans?\nTherefore, we hypothesize that the limited the spatial reasoning abilities of current VLMs is not\ndue to a fundamental limitation of their architecture, but rather is a limitation in common datasets\navailable at scale on which such models are trained. For example, many VLMs [12, 18, 44] are trained\non internet-scale datasets characterized by image-caption pairs [13], which contain limited spatial\ninformation. This is partially due to the difficulties of obtaining spatial-information-rich embodied data\nor high-quality human annotations for 3D-aware queries.\nAutomatic data generation and augmentation techniques are one approach to deal with the data\nlimitation problem [38, 53, 56, 66]. However, most previous data generation efforts focus on rendering\nphotorealistic images with ground truth semantic annotation but overlook the richness of objects and\n3D relationships. In contrast, we focus on extracting spatial information directly from real world data\nin order to capture the diversity and complexity of the true 3D world.\nOur key insight is that recent advancement in off-the-shelf vision models can automatically generate\nrich 3D spatial annotations from 2D images. To this end, we propose a system called SpatialVLM that\nenables data generation and training of VLMs to enhance their spatial reasoning capabilities. Concretely,\nby combining 1) open-vocabulary detection, 2) metric depth estimation, 3) semantic segmentation\nand 4) object-centric captioning models, we can densely annotates real world data at scale. SpatialVLM\nconverts the data generated by vision models into a format can be used to train VLMs on a mixture\nof captioning, VQA and spatial reasoning data.\nThrough experiments, we find our trained VLM exhibit many desirable capabilities. First, its ability\nto answer qualitative spatial questions is greatly enhanced. Secondly, it can perform quantitative esti-\nmation reliably despite noisy training data. Such capability not only gives it common sense knowledge\nabout object sizes but also makes it useful as a open-vocabulary reward annotator for rearrangement\ntasks. Thirdly, we find this spatial Vision Language Model, benefiting from its natural language interface,\ncan perform spatial chain-of-thought to solve complex spatial reasoning tasks when combined with\na powerful Large Language Model.\nOur main contributions are:\n\u2022 We endow VLMs quantitative spatial reasoning capability, which is a fundamental capability of\nhumans.\n2\nSpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities\n\u2022 We design a framework to automatically label 3D spatial reasoning VQA data based on real world\nimages at the Internet scale.\n\u2022 We study various training recipes: data quality, training pipeline, freeze/unfreeze visual encoder,\netc, and investigate how they affect the learning quality.\n\u2022 We demonstrate new capabilities of SpatialVLM in complex reasoning and robotics unlocked by\nthe introduced task and method.\n2. Related Work\nLearning Spatial Reasoning.\nSpatial distance estimation has been traditionally addressed as a part\nof broader tasks, such as SLAM [8, 21] or depth estimation [24]. When applying these spatial concepts\nto reasoning, prior works often focus on explicit spatial scene memories [27, 28] or spatial scene graphs\n[31, 32, 62, 63]. Scene graphs allow interpretable, structured, statistical relation learning based on\nthe spatial structures they encode. To answer spatial problems in VQA formats, they must handle it\nexplicitly as a pathfinding problem on said scene graph. VLMs, on the other hand, are pretrained on\nlarge amounts of loosely structured information from vision-language datasets. Unlike scene graphs,\nthe spatial understanding is encoded implicitly. We can infuse the depth and 3D structure into the\nweights with an auxiliary task [36, 47], capturing the relational information. In our work, we address\nthe spatial relationship problem directly in the VLM, without an explicit underlying scene graph. In\naddition to understanding relative relationships in qualitative terms, we also explore estimating explicit\nmetric distance relationships between objects in a scene.\nGrounding Vision-Language Models.\nLarge language models (LLMs) are trained on internet-scale\ndata, making them effective commonsense reasoners. However, LLMs (and by extension VLMs) may\nlack the necessary grounding to perform well at social reasoning [42], physical reasoning [26], physics\nreasoning [46], embodied tasks [1, 34, 58], and spatial reasoning tasks [44, 55]. Though language\nmodel with interactive world experience show grounding improvements [67, 70], the introduction of\nlarge vision models, such as Flamingo [2], PaLI [12], or PaLM-E [18], has enabled a leap in performance.\nThese visually-grounded models have been used for several downstream tasks, such as in robotic success\ndetection [18, 20, 57, 68], action prediction [7, 59], and reward prediction [16, 23, 48, 50]. In this\nwork we approach the problem of spatial reasoning through finetuning a VLM on a generated VQA\ndataset. By directly finetuning a VLM on this task, we inherit the generality and reasoning capabilities\nof the underlying VLM as well as show how this approach is capable of tasks like reward generation.\nSpatial Information in Vision-Language Datasets.\nMany prior works have focused on benchmarking\nVLMs [61, 69], considering tasks like VQA (e.g. VQAv2 [29], OK-VQA [49], COCO [43], or Visual\nGenome [39]). Others have focused on fine-grained scene understanding, such as semantic segmen-\ntation [5, 37], object detection [11], or object identification [15, 60]. Others have focused specifically\non spatial reasoning as a task, answering questions about object spatial relations (e.g., above, below,\nleft, right) in real [44, 55] or simulated [35] scenes. Real data in this domain can be limited by the\namount generated by human labelers, while synthetic data has inherently bounded expressivity. In this\nwork we consider how to automatically generate real data, and focus on the problem of not just spatial\nrelations, but metric spatial distances, which can be directly applied to many downstream tasks.\n3\nSpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities\nRegion \nCaptioning\nMetric Depth \nEstimation\nCoordinate \nCanonicalization\nSegmentation\n-0.5\n0.5\n0\ncake shaped like a house\ncake on a plate\ngirl wearing a purple shirt\nblue stuffed animal\nClustering  \n(b) 2D Context Extraction\nQ:   How far is [A] from [B]?\nA:         It\u2019s [Distance].\n(e) Q&A Synthesis\nDistance / Predicate \nExtraction\nHuman Alignment\n(c) 2D Context to 3D Context\n(a) Semantic Filtering\nSemantic Filtering\n(d) Ambiguity Resolution\nFigure 2 | An overview of our data synthesis pipeline. (a) We use CLIP to filter noisy internet images\nand only keep scene-level photos. (b) We apply pre-trained expert models on internet-scale images\nso that we get object-centric segmentation, depth and caption. (c) We lift the 2D image into 3D point\nclouds, which can be parsed by shape analysis rules to extract useful properties like 3D bounding box.\n(d) We avoid asking ambiguous questions by clustering object captions using CLIP similarity score (e)\nWe synthesize millions of spatial question and answers from object captions and extracted properties.\n3. SpatialVLM\nTo equip VLMs with both qualitatively and quantitatively spatial reasoning capabilities, we propose\nto generate a large-scale spatial VQA dataset, which is used to train VLMs. Concretely, we design a\ncomprehensive data generation framework which first leverages off-the-shelf computer vision models\nincluding open-vocabulary detection, metric depth estimation, semantic segmentation and object-\ncentric captioning models to extract object-centric contexts, and then adopts template-based approach\nto generate massive spatial VQA data of reasonable quality. We train our SpatialVLM using the generated\ndataset to learn direct spatial reasoning capabilities, which we can then combine with the high-level\ncommonsense reasoning embedded in LLMs to unlock chain-of-thoughts spatial reasoning.\n3.1. Spatial Grounding from 2D Images\nWe hypothesize that the reason for the lack of spatial reasoning capabilities of today\u2019s VLMs is not their\narchitecture, but the lack of spatial reasoning training data. Following this insight, we design a pipeline\nthat generates VQA data containing spatial reasoning questions. The pipeline is summarized in in\nFigure 2 and described in detail as follows.\nSemantic Filtering\nWhile internet-scale image-captioning datasets have been widely used in VLM\ntraining [12], many images in these datasets are not suitable for synthesizing spatial reasoning QA,\ndue to the fact that they either consist of a single object or don\u2019t have a scene background (e.g. product\npictures on shopping websites or screenshots of computer screen). Therefore, as the first step in our\ndata synthesis pipeline, we adopt a CLIP-based open-vocabulary classification model to classify all\nimages and rule out those that are not suitable.\n4\nSpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities\nObject-centric Contexts Extraction from 2D Images\nIn order to extract object-centric spatial con-\ntexts from 2D images, we leverage a series of off-the-shelf expert models, including region proposal,\nregion captioning [4], and semantic segmentation [41] modules to extract object-centric information.\nWith this step, we obtain object-centric entities consisting of pixel clusters as well as open-vocabulary\ncaption descriptions.\nLifting 2D Contexts to 3D Contexts\nTraditional spatial VQA datasets generated using object detec-\ntion and bounding box positioning [40] are limited to the 2D image plane (lack of depth or altitude\ncontexts) and pixel-level reasoning (lack of metric-scale size and distance contexts). We perform depth\nestimation [6] to lift monocular 2D pixels to metric-scale 3D point clouds. We further canonicalize\nthe camera coordinate system of the point cloud into a geodetic coordinate system, which is done by\nhorizontal surface (e.g. \u201cfloor\u201d, \u201ctable top\u201d) segmentation [9] and frame transfer. To the best of our\nknowledge, we are the first to lift internet-scale images to object-centric 3D point clouds and use it to\nsynthesize VQA data embedded with 3D spatial reasoning supervision.\nAmbiguity Resolution\nSometimes there are multiple objects of similar categories in one image,\nleading to ambiguities of their caption labels. For example, one same caption label \u201ccake\u201d can refer\nto multiple different cakes in a same image. Therefore, before we can ask questions about these objects,\nwe need to make sure the reference expressions are not ambiguous. We made two key design choices\nthat have been validated empirically to be effective in tackling this challenge:\n\u2022 We deliberately choose to avoid common object detectors, which tend to produce fixed and coarse\ncategories such as \u201ccake\u201d, and adopt FlexCap [4], a user-configurable object-centric captioning\napproach. In practice, for each object we can sample a random caption of a variable length\nbetween 1\u22126 words. As a result, our object annotations are fine-grained, such as \u201ccake shaped\nlike a house\u201d and \u201ccup cake in plastic container\u201d\n\u2022 We design a semantic-oriented post-processing algorithm that further remove ambiguities by\naugmenting or rejecting object captions. Details of this algorithm are shown in Appendix A.2 .\n3.2. Large-Scale Spatial Reasoning VQA Dataset\nAs motivated in Section 3, we focus our study on infusing \u201cstraightforward\u201d spatial reasoning capabilities\ninto VLMs by pretraining with synthetic data. Therefore, we synthesize spatial-reasoning QA pairs that\ninvolve no more than two objects (denoted \u201cA\u201d and \u201cB\u201d) in the image and consider the two following\ncategories of questions.\nQualitative questions:\nthose that ask for judgement of some spatial relations. Examples are \u201cGiven\ntwo objects A and B, which is more towards the left?\u201d, \u201cIs object A more elevated than object B?\u201d and\n\u201cAmong A and B, which is bigger in width?\u201d.\nQuantitative questions:\nthose that ask for more fine-grained answers that include numbers and\nunits. Examples include \u201chow much to the left is object A compared to object B?\u201d, \u201cHow far is object\nA from the B?\u201d, \u201cFind out how far A is positioned behind B relative to the camera.\u201d. Similar to the\naforementioned examples, such questions can be synthesized using a main question template, and one\ncan fill the object name entries using the object captions after disambiguation. This property allows\nus to do template-based generation, an approach commonly adopted by instruction tuning works [64].\n5\nSpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities\nIt's \nwhi\nNo,\nwit\nthe\nYes\nof t\nQualitative Q & A\nQuantitative Q & A\nFind out if the girl \nwearing a purple shirt is \nsituated behind or in \nfront of the girl wearing a \nblue sweatshirt.\nIt's the woman \nwearing white shirt.\nMeasure the distance \nfrom the cake shaped like \na house to the girl \nwearing a purple shirt?\nAround half a meter\nDoes the stool lie in front \nof the microwave oven?\nYes, the stool is in front \nof the microwave oven.\nCan you provide the \ndistance measurement \nbetween the microwave \noven and the kitchen \ncounter?\nThey are 1 meter apart.\nIs the plate positioned \non the left or right side of \nthe green and yellow \nnapkin?\nThe plate is to the \nright.\nHow wide is the plate?\n20 centimeters\nFigure 3 | Example data entries from the synthetic dataset. Given the output of vision expert\nmodels, we follow a set of question generation template to generate both quantitative and qualitative\nquestion-answer pairs to highlight the diversity of the dataset. The spatial concepts are highlighted in\nblue. Such visual question-answer pairs can be easily mixed together with other captioning or question\nanswering datasets and use the same training objectives.\nThe answers to the questions are obtained through appropriate functions that we develop, which take\nas input the segmented point clouds and 3D bounding boxes of the relevant objects.\nWe designate 38 different types of qualitative and quantitative spatial reasoning questions, each\nfeaturing around 20 question templates and 10 answer templates (we show examples in Appendix. A.3).\nWe also add bias the sampling to encourage concise answers. Finally we introduce a human-aligned\nrounding mechanism in Appendix A.2 to make number roundings in a human-like way. Using such\nan approach, we are able to generate ample question answering data pairs for the monocular camera\nimages in webli and vqa datasets. Fig 3 shows several example synthetic question answering pairs\nwe obtained. In total, we create a massive dataset with 10 million images and 2 billion direct spatial\nreasoning QA pairs, featuring 50% qualitative questions and 50% quantitative questions. Thanks to\nthe diversity of object captions and distance units, our synthetic dataset features significant diversity\nin terms of object description, question type and phrasing.\n3.3. Learning Spatial Reasoning\nDirect Spatial Reasoning\nis defined as following, a Vision-Language Model takes as input an image\nI and a query Q of a spatial task, and output an answer A, in the format of a text string, without using\nexternal tools or interacting with other large models. We adopt the same architecture and training\nprocedure of PaLM-E [18] except replacing PaLM [14] backbone with PaLM 2-S [3], a smaller variant.\nWe then train our model using a mixture of the original PaLM-E dataset and our dataset, with 5% of\ntokens dedicated to spatial reasoning tasks. Similar to PaLM-E, our method has the ability to perform\nVQA as well as basic embodied planning when combined. The key difference is that it can answer spatial\nreasoning questions about both binary predicates and quantitative estimations.\nChain-of-Thought Spatial Reasoning\nMany real-world tasks require multiple steps of spatial rea-\nsoning. For example, to determine if object A can fit into object B, one would need to reason about\n6\nSpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities\nWhat is the distance between the blue coke can and the orange can? \n0.4m\nWhat is the distance between the orange can and the silver seven up can?\n0.48m\nWhat is the distance between the blue coke can and the silver seven up can? \n0.41m\nSince the difference between the longest and shortest triangle edges (0.48m - \n0.4m = 0.08m) is smaller than 0.1 m, the blue coke can, the orange can, and the \nsilver seven up can roughly form an isosceles triangle.\nLLM:\nVLM:\nLLM:\nVLM:\nLLM:\nVLM:\nLLM:\nDoes the blue coke can, the orange can, and the silver seven up can on the table roughly form an isosceles triangle? \nRoughly means the difference between the longest and shortest triangle edges is smaller than 0.1 m.\nUser:\nFigure 4 | Chain-of-thought spatial reasoning. We illustrate that we can perform Chain-of-Thought\nSpatial reasoning with SpatialVLM. In this example, with the help of an LLM orchestrating SpatialVLM,\nthe system is able to answer questions like \u201cDoes the blue coke can, the red coke can, and the green\nsponge on the table roughly form an equilateral triangle\".\nsizes and constraints. Sometimes one would need to reason over grounded spatial concept (e.g. the\ncounter in the image is 1 meter high) and common sense knowledge (so that a toddler cannot reach it).\nSpatialVLM provides a natural language interface to query with grounded concepts, when combined\nwith a powerful LLM, we can perform complex spatial reasoning.\nWe call this method \u201cChain-of-Thought Spatial Reasoning\". While our synthesized data only\ncontains direct spatial reasoning questions, it\u2019s easy for a VLM to compose them together to solve\ncomplex questions that require multi-hop chain-of-thought reasoning. Similar to the method in Socratic\nModels [71] and LLM as coordinator [10], we utilize an LLM (text-davinci-003) to coordinate and\ncommunicate with our SpatialVLM to solve complex problems with Chain-of-Thought prompting [65]\nas shown in Fig. 4. The LLM can break down complex questions into simple questions, query the VLM,\nand put the reasoning together to derive the result.\n4. Experiments\nWe conduct experiments to answer the following questions:\nQ1 Does our spatial VQA data generation and training pipeline improve VLM\u2019s general spatial reasoning\ncapabilities? And how well does it perform?\nQ2 How does the noisy synthetic spatial VQA data and different training strategies affect the learning\nperformance?\nQ3 Does the VLM equipped with \u201cdirect\u201d spatial reasoning capabilities unlock new capabilities such\nas chain-of-thought reasoning and embodied planning?\nWe train our model using a mixture of PaLM-E training set and our spatial VQA dataset. To verify\nwhether VLM\u2019s limitation in spatial reasoning is a data problem, we choose the following state-of-the-art\nVLMs as baselines, all trained on mixtures in which semantic-captioning tasks occupy a heavy weight,\nand without our spatial VQA dataset.\nGPT-4V1 GPT-4V is a version of GPT-4 [51] that supports multimodal input, it achieves state-of-the-art\nperformance in many vision-language tasks.\n1Accessed Nov 2023 via OpenAI API.\n7\nSpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities\nMethod\nGPT-4V\nLLaVA-1.5\nInstructBLIP\nPaLI\nPaLM-E\nPaLM 2-E\nOurs\nAccuracy\n68.0%\n71.3%\n60.4%\n60.7%\n50.2%\n50.4%\n75.2%\nTable 1 | Accuracy of different VLMs on binary predicate prediction tasks. Our proposed method\noutperform baselines on binary predicate prediction tasks by a large margin owing to the addition\nof synthetic data.\nPaLI [12]. An encoder-decoder VLM trained on multi-lingual corpora, it shows state-of-the-art per-\nformance on captioning and visual-question answering tasks. We used PaLI-X 55B variant in our\nexperiments.\nPaLM-E [18]. A VLM trained on internet-scale vision, language, and vision-language data, as well as\nrobotics data. It shows state-of-the-art performance in OKVQA benchmark, as well as being capable\nof robot planning tasks. We used PaLM-E 12B across our experiments.\nPaLM 2-E The vanilla PaLM 2-E is an updated version of PaLM-E[18] with exact same training procedure\nbut a more recent LLM backbone. Due to the shared network architecture and training procedure with\nSpatialVLM, vanilla PaLM 2-E naturally serves as the baseline to study the effect of generated data. In\nthe rest of the paper, unless specifically noted, PaLM 2-E corresponds to PaLM 2-S in terms of parameter\ncount following the naming convention in PaLM 2 technical report [3].\nFinally, we consider open source models like LLaVA-1.5 [45] and InstructBLIP [17].\n4.1. Spatial VQA performance\nTo stress-test the VLM\u2019s spatial reasoning capabilities, a spatial reasoning VQA benchmark with guar-\nanteed performance grounding is required. However, there is not such a proper benchmark available\nin the literature. Therefore, we created a benchmark by having human annotators label a diverse set\nof \u201cdirect\u201d qualitative and quantitative VQAs on a subset of WebLI images [12], which are unseen to\nall VLMs during the training phase. The benchmark questions and answers are diverse and freeform,\nfollowing the synthetic data generation pattern described in Section 3.2 (details in Appendix. A.1). We\nannotated 331 qualitative spatial reasoning VQA pairs and 215 quantitative spatial reasoning VQA pairs.\nQualitative Spatial VQA\nFor such questions, both the human annotated answers and VLM outputs\nare freeform natural language. Therefore, to evaluate the performance of the VLMs, we use human\nraters to determine if an answer is correct, and show the success rates of the VLMs in Table. 1. It is\nshown that SpatialVLM is able to achieve significantly higher accuracy compared to all baselines that\nare not trained using the synthetic spatial VQA data, surpassing other vision-language models including\nGPT-4V. Among the baselines, the second best model is LLaVA-1.5, which might be caused by their\nuse of bounding boxes and corresponding captions in visual instruction tuning. Anecdotally, we found\nLLaVA-1.5 performs well in 2D spatial relationship inference, but inferior to our models in 3D spatial\nreasoning. This experiment suggests that large and high-quality spatial reasoning data is key to spatial\nreasoning capabilities, which are not present in pretraining datasets of state-of-the-art VLMs.\nQuantitative Spatial VQA\nFor these questions, both human annotator answers and the VLM outputs\nare natural language descriptions of distance, height, elevation, etc, using their preferred units. We\ndesign two metrics for evaluating the performance of the VLM. First, we use the success rate of the VLM to\nproduce a number to reflect if the VLM is able to understand the quantitative spatial reasoning question.\nSecond, since the answer can range widely from centimeters to kilometers, we use percentages of the\n8\nSpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities\nGPT-4V\nLLaVA-1.5\nInstructBLIP\nPaLI\nPaLM-E\nPaLM 2-E\nOurs\nOutput numbers %\n1.0%\n20.9%\n26.0%\n52.0%\n83.2%\n88.8%\n99.0%\nIn range [50, 200]%\n0.0%\n13.0%\n7.9%\n5.3%\n23.7%\n33.9%\n37.2%\nTable 2 | Accuracy of different VLMs on quantitative questions about spatial relationship. As\ncan be seen from this table, first, our method outputs valid format more often (99.0% of the time)\nthan baseline methods. Second, our method outputs quantitative distance estimation that is closer\nto ground truth annotated by human more often than baseline methods.\n0\n1\n2\n3\n4\n5\n6\n7\nImage Index\n0.0\n0.2\n0.4\n0.6\nValues (m)\nDistance from grasp\nPredicted Gripper-Coke \nDistance (Mean \u00b1 Std)\nFigure 5 | Given a sequence of images where the robot gripper is approaching the coke can, we ask\nSpatialVLM \u201cWhat is the distance between the yellow gripper and the coke can\". We are able to get\naccurate and monotonically decreasing distance estimations.\nVLM answers that fall into half to twice of the ground truth value to represent how accurate the VLM\u2019s\nestimates are. The results are shown in Table. 2, and it is shown that our model performs better on both\nmetrics than baselines with large margins. We observed that baseline VLMs are reluctant to give answers\nconsisting of numbers. For example, replying \u201cNo.\" to questions like \u201cCan you tell me the distance between\n...\". This is likely due the the distribution of the training data. Additionally, we find that state-of-the-art\nVLM GPT-4V often refrain from generating answers about distance in SI units with a disclaimer text \u201cI\u2019m\nsorry, but I cannot provide an exact distance as the image does not offer precise references for measurement..\".\nOur approach SpatialVLM achieves significantly higher success rate than all baselines, achieving in-\nrange results on almost half of the questions. This performance is remarkable given that the human\nannotations are noisy, and agreement among annotators are not often guaranteed (Appendix. A.1). To\nbetter understand our model\u2019s performance and limitations, we visualized the relative error against the\nground truth value in Fig. 11 in the Appendix. We found that SpatialVLM does well on medium range\nscenes like those with objects 1\u221210 meters from the camera. This coincides with the range where our\nmonocular depth estimator [6] reliably outputs metric accurate depth estimations, which indicates that\nour method inherits the biases and limitations from expert vision models in the data synthesis pipeline.\n4.2. Effect of Spatial VQA Data to General VQA\nThe second question we want to answer is: since we co-train with a considerable amount of spatial VQA\ndata, whether the performance of VLM in other tasks will degrade as a result. We compared our model\nwith the vanilla PaLM 2-E trained without the spatial VQA dataset on general VQA benchmarks, and\nas summarized in Table. 3, our model achieves comparable performance as PaLM 2-E on the OKVQA\nbenchmark, in which limited spatial reasoning questions are included, and performs slightly better\non VQA-v2 test-dev benchmark, which includes spatial reasoning questions. This seem to suggest that\nVLMs are generally underfitting in the distribution of tasks close to spatial reasoning, and can benefit\nfrom spatial VQA supervisions without hurting their general VQA capabilities.\n9\nSpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities\nGeneral VQA benchmarks\nOKVQA\nVQA v2\nPaLM 2-E w/o co-training\n61.4%\n76.6%\nOurs\n61.0(-0.4)%\n79.0(+2.4)%\nTable 3 | VQA performance. Co-training on SpatialVLM training mix and finetuning on VQA datasets\n(VQA v2) improves VQA performance. A PaLM 2-E model trained with SpatialVLM data improves VQA\nv2 performance by 2.4% compared to a model with the same number of parameters, but without the\ndata. However, we don\u2019t find OKVQA task to benefit from SpatialVLM training.\n4.3. Effect of Visual Transformer (ViT) Encoder in Spatial Reasoning\nDoes a frozen ViT (trained on contrastive objective) encode enough information to perform spatial\nreasoning? To study this, we start at the 110k training step and branch into two training runs, one\nwith the ViT frozen, the other with ViT unfrozen. We train both models for 70k steps, and evaluate\npercentages of answers from both models that fall into various ranges of the ground truth value in Table 4.\n[50, 200]%\n[66.7, 150]%\n[90, 110]%\nFrozen ViT\n34.9%\n9.3%\n5.6%\nUnfrozen ViT\n37.2(+2.3)%\n10.7(+1.4)%\n8.4(+2.8)%\nTable 4 | Comparison on finetuning with frozen or unfrozen ViT. We find it is beneficial to unfreeze\nthe pretrained ViT for distance estimation tasks.\nIt is shown that for larger scale and less fine-grained distance estimation, such as making a rough\nestimation with in the half-to-twice range of the ground truth, training without freezing ViT performs\nslightlyworsebutcomparablewithunfrozenViT.However, formorefine-graineddistanceestimationlike\nestimating accurate quantitative values, the model with unfrozen ViT performed considerably better. We\nhypothesize that the pretrained ViT (with contrastive or classification loss) is lossy in its fine-grained spa-\ntial information. Our model achieves 8.4% accuracy for predicting a value 0.9\u00d7 to 1.1\u00d7 range of human\nannotation. This is remarkable since humans annotations are noisy. In fact, human sometimes tend to\ngive noisy estimations, as they prefer to round an estimation of 0.8 meter to 1 meter. It remains challeng-\ning to evaluate quantitative spatial reasoning capabilities of vision-language models in broad domains.\n4.4. Effect of Noisy Quantitative Spatial Answers\nSince the quantitative answers of the spatial VQA dataset are noisy, we study if VLMs can learn\ngeneralizable quantitative estimations from a large amount of noisy training data. To do so, we first\ncome up with a domain where we are able to generate high quality quantitative answers. As discussed in\nSection 4.1 the monocular depth estimation is one of the steps in the data generation pipeline that induce\nthe most noises. Therefore, we leverage our robotic manipulation dataset, which provides near-ground-\ntruth depth information captured using a depth camera. As a result, the generated quantitative answers\nare more accurate. We train VLM using this dataset, and find the model able to perform fine-grained\ndistance estimation in the manipulation domain (Fig. 5), which further demonstrates the data accuracy.\nTo study how noisy data affects VLM training, we add Gaussian noises upon the quantitative answers\nof the accurate manipulation spatial VQA dataset, and obtain a series of noisy datasets of different noise\nlevel. We train VLMs using the noisy datasets and evaluate them using a human annotated quantitative\nspatial VQA benchmark for manipulation. Table. 5 compares how different Gaussian noise standard\ndeviations affect the overall VLM performance on quantitative spatial VQA. Since the objects in the\nmanipulation VQA datasets are within 1 meter range, we added the mean squared error (MSE) as a\n10\nSpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities\nGaussian std\n0\n0.1\n0.2\n0.3\nMSE(m)\n0.046\n0.053\n0.039\n0.048\n[50, 200]%\n59.0%\n55.8%\n61.1%\n61.1%\nTable 5 | Comparison on different data noise levels, controlled using standard deviation (STD) of\nGaussian noise. We find that our model can learn despite moderate amount of random noise.\npick orange tea bottle\nput apple into the bowl\npick up the apple\nFigure 6 | SpatialVLM as reward generator for robotics tasks. SpatialVLM provides a \u201cnatural-language\nqueriable\" distance estimation tool, and can be used for robotics tasks. For example, for the task \u201cpick orange\ntea bottle\", the reward/cost function can be the a function of the response of \u201cWhat is the distance between the\nyellow gripper fingers and the orange tea bottle\". And for the task \u201cput the apple into the bowl\", the reward/cost\nfunction can be a function of the response of \u201cwhat is the distance between the apple and bowl\". We sample\ndifferent gripper positions and show the cost function in the above scatter plots.\nmetric to evaluate the VLM performance, as well as the half-to-twice percentage which is defined in\nSection 4.1. It is shown that VLMs trained on datasets of different noise levels achieve similar spatial\nreasoning accuracy. We speculate this is due to the noisy nature of the training data and the manually\nannotated evaluation benchmark, and that VLM can learn a spatial reasoning common-sense despite\nnoisy data. We observed this interesting phenomenon in robotics experiments as well. In Fig. 6, the\ndistance estimation is exhibit a bias towards the mean since the model is heavily regularized.\n4.5. Spatial Reasoning Unlocks Novel Applications\nVLM as a Dense Reward Annotator\nOne important application of VLM is robotics. Recently, works\nhave shown that VLMs and LLMs can serve as universal open-vocabulary reward annotators and success\ndetector [20] for robotics tasks, which can be used to derive useful control policies. However, the reward\nannotationabilityofVLMsareoftenlimitedbylackofspatialawareness. SinceSpatialVLMisabletoquan-\ntitativelyestimatedistancesorsizesfromimage, it\u2019suniquelysuitedasadenserewardannotator. Wecon-\nduct a real robot experiment where we specify a task in nature language and ask SpatialVLM to annotate\na reward for each frame in a trajectory. In Figure 6, each dot illustrates an object location and their color\nindicates the annotated reward. As the robot makes progress towards the specified goal, we can see the\nreward increase monotonically, indicating the ability of SpatialVLM to serve as a dense reward annotator.\nChain-of-Thought Spatial Reasoning\nIn this section, we investigate whether SpatialVLM can be\nused to do tasks requiring multi-step reasoning, given its enhanced ability to answer elemental spatial\nquestions. We demonstrate a few examples in Figure 1 and Figure 4. A large language model, in this\ncase GPT-4, when equipped with SpatialVLM as a spatial reasoning submodule, can perform complex\nspatial reasoning tasks, such as answering if 3 objects in the environment can form a \u201cisosceles triangle\".\n11\nSpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities\n5. Conclusion\nIn conclusion, our research addresses the challenge of infusing spatial reasoning to VLMs, and approach\nit by constructing a framework for automatic generation of 3D spatial reasoning VQA data based\non Internet-scale real-world images. We ablate different design choices in the recipes for training\nVLMs, such as training with large amount of noisy data and unfreezing ViT. While our direct spatial\nqueries are built on a finite set of templates, we show SpatialVLM can be extended to tackle more\ncomplicated chain-of-thought reasoning that requires spatial reasoning components. SpatialVLM is\nalso demonstrated to be useful for robotics tasks, where we show that a 3D spatial-aware VLM could be\nused as a reward annotator for robotics tasks. Additional study of more nuanced geometric primitives\ncan also help fully ground spatial reasoning in 3D geometry.\nReferences\n[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David,\nChelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as i can, not\nas i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.\n[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan\nCabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian\nBorgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo\nBarreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language\nmodel for few-shot learning, 2022.\n[3] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report.\narXiv preprint arXiv:2305.10403, 2023.\n[4] Anonymous.\nFlexcap: Generating rich, localized, and flexible captions in images.\nIn\nSubmitted to The Twelfth International Conference on Learning Representations, 2023. URL\nhttps://openreview.net/forum?id=7Phicg0WAg. under review.\n[5] Ivana Bala\u017eevi\u0107, David Steiner, Nikhil Parthasarathy, Relja Arandjelovi\u0107, and Olivier J H\u00e9naff.\nTowards in-context scene understanding. arXiv preprint arXiv:2306.01667, 2023.\n[6] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias M\u00fcller. Zoedepth:\nZero-shot transfer by combining relative and metric depth. arXiv preprint arXiv:2302.12288, 2023.\n[7] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choroman-\nski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action\nmodels transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023.\n[8] Cesar Cadena, Luca Carlone, Henry Carrillo, Yasir Latif, Davide Scaramuzza, Jos\u00e9 Neira, Ian\nReid, and John J Leonard. Past, present, and future of simultaneous localization and mapping:\nToward the robust-perception age. IEEE Transactions on robotics, 32(6):1309\u20131332, 2016.\n[9] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille.\nDeeplab: Semantic image segmentation with deep convolutional nets, atrous convolution,\nand fully connected crfs. IEEE transactions on pattern analysis and machine intelligence, 40(4):\n834\u2013848, 2017.\n12\nSpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities\n[10] Liangyu Chen, Bo Li, Sheng Shen, Jingkang Yang, Chunyuan Li, Kurt Keutzer, Trevor Darrell,\nand Ziwei Liu. Language models are visual reasoning coordinators. In ICLR 2023 Workshop on\nMathematical and Empirical Understanding of Foundation Models, 2023.\n[11] Ting Chen, Saurabh Saxena, Lala Li, David J Fleet, and Geoffrey Hinton. Pix2seq: A language\nmodeling framework for object detection. arXiv preprint arXiv:2109.10852, 2021.\n[12] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian\nGoodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-scaled multilingual\nlanguage-image model. arXiv preprint arXiv:2209.06794, 2022.\n[13] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and\nC Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint\narXiv:1504.00325, 2015.\n[14] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n[15] Vanya Cohen, Benjamin Burchfiel, Thao Nguyen, Nakul Gopalan, Stefanie Tellex, and George\nKonidaris. Grounding language attributes to objects using bayesian eigenobjects. In 2019\nIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 1187\u20131194.\nIEEE, 2019.\n[16] Yuchen Cui, Scott Niekum, Abhinav Gupta, Vikash Kumar, and Aravind Rajeswaran.\nCan\nfoundation models perform zero-shot task specification for robot manipulation? In Learning\nfor Dynamics and Control Conference, pages 893\u2013905. PMLR, 2022.\n[17] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language\nmodels with instruction tuning, 2023.\n[18] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter,\nAyzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar,\nPierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc\nToussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Palm-e: An embodied multi-\nmodal language model. In Proceedings of the International Conference on Machine Learning, 2023.\n[19] Yuqing Du, Ksenia Konyushkova, Misha Denil, Akhil Raju, Jessica Landon, Felix Hill, Nando\nde Freitas, and Serkan Cabi. Vision-language models as success detectors. arXiv preprint\narXiv:2303.07280, 2023.\n[20] Yuqing Du, Ksenia Konyushkova, Misha Denil, Akhil Raju, Jessica Landon, Felix Hill, Nando\nde Freitas, and Serkan Cabi. Vision-language models as success detectors, 2023.\n[21] Hugh Durrant-Whyte and Tim Bailey. Simultaneous localization and mapping: part i. IEEE\nrobotics & automation magazine, 13(2):99\u2013110, 2006.\n[22] Martin Ester, Hans-Peter Kriegel, J\u00f6rg Sander, Xiaowei Xu, et al. A density-based algorithm for\ndiscovering clusters in large spatial databases with noise. In kdd, volume 96, pages 226\u2013231, 1996.\n[23] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew\nTang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended\nembodied agents with internet-scale knowledge. Advances in Neural Information Processing\nSystems, 35:18343\u201318362, 2022.\n13\nSpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities\n[24] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao. Deep\nordinal regression network for monocular depth estimation. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 2002\u20132011, 2018.\n[25] Zhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng Liu, Jianfeng Gao, et al. Vision-language\npre-training: Basics, recent advances, and future trends. Foundations and Trends\u00ae in Computer\nGraphics and Vision, 14(3\u20134):163\u2013352, 2022.\n[26] Jensen Gao, Bidipta Sarkar, Fei Xia, Ted Xiao, Jiajun Wu, Brian Ichter, Anirudha Majumdar, and\nDorsa Sadigh. Physically grounded vision-language models for robotic manipulation, 2023.\n[27] Theophile Gervet, Soumith Chintala, Dhruv Batra, Jitendra Malik, and Devendra Singh Chaplot.\nNavigating to objects in the real world. Science Robotics, 8(79):eadf6991, 2023.\n[28] Daniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon, Dieter Fox, and\nAli Farhadi. Iqa: Visual question answering in interactive environments. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages 4089\u20134098, 2018.\n[29] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v\nin vqa matter: Elevating the role of image understanding in visual question answering, 2017.\n[30] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proceedings of\nthe IEEE international conference on computer vision, pages 2961\u20132969, 2017.\n[31] Sachithra Hemachandra, Matthew R Walter, Stefanie Tellex, and Seth Teller. Learning spatial-\nsemantic representations from natural language descriptions and scene classifications. In 2014\nIEEE International Conference on Robotics and Automation (ICRA), pages 2623\u20132630. IEEE, 2014.\n[32] Marcel Hildebrandt, Hang Li, Rajat Koner, Volker Tresp, and Stephan G\u00fcnnemann. Scene graph\nreasoning for visual question answering, 2020.\n[33] Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, and Lijuan\nWang. Scaling up vision-language pre-training for image captioning. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17980\u201317989, 2022.\n[34] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch.\nLanguage models as\nzero-shot planners: Extracting actionable knowledge for embodied agents. In International\nConference on Machine Learning, pages 9118\u20139147. PMLR, 2022.\n[35] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick,\nand Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual\nreasoning, 2016.\n[36] Mohi Khansari, Daniel Ho, Yuqing Du, Armando Fuentes, Matthew Bennice, Nicolas Sievers,\nSean Kirmani, Yunfei Bai, and Eric Jang. Practical imitation learning in the real world via task\nconsistency loss, 2022.\n[37] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete\nXiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint\narXiv:2304.02643, 2023.\n[38] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Matt\nDeitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, et al. Ai2-thor: An interactive 3d environment\nfor visual ai. arXiv preprint arXiv:1712.05474, 2017.\n14\nSpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities\n[39] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie\nChen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Fei-Fei Li. Visual\ngenome: Connecting language and vision using crowdsourced dense image annotations, 2016.\n[40] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie\nChen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language\nand vision using crowdsourced dense image annotations. International journal of computer vision,\n123:32\u201373, 2017.\n[41] Weicheng Kuo, Anelia Angelova, Jitendra Malik, and Tsung-Yi Lin. Shapemask: Learning to\nsegment novel objects by refining shape priors. In Proceedings of the IEEE/CVF international\nconference on computer vision, pages 9207\u20139216, 2019.\n[42] Minae Kwon, Hengyuan Hu, Vivek Myers, Siddharth Karamcheti, Anca Dragan, and Dorsa\nSadigh. Toward grounded social reasoning, 2023.\n[43] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro\nPerona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Doll\u00e1r. Microsoft coco: Common objects\nin context, 2015.\n[44] Fangyu Liu, Guy Emerson, and Nigel Collier.\nVisual spatial reasoning.\nTransactions of\nthe Association for Computational Linguistics, 11:635\u2013651, 2023.\nISSN 2307-387X.\ndoi:\n10.1162/tacl_a_00566. URL http://dx.doi.org/10.1162/tacl_a_00566.\n[45] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv\npreprint arXiv:2304.08485, 2023.\n[46] Ruibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu, Soroush Vosoughi, Claire Cui, Denny Zhou,\nand Andrew M Dai. Mind\u2019s eye: Grounded language model reasoning through simulation. arXiv\npreprint arXiv:2210.05359, 2022.\n[47] Shikun Liu, Linxi Fan, Edward Johns, Zhiding Yu, Chaowei Xiao, and Anima Anandkumar. Prismer:\nA vision-language model with an ensemble of experts. arXiv preprint arXiv:2303.02506, 2023.\n[48] Parsa Mahmoudieh, Deepak Pathak, and Trevor Darrell.\nZero-shot reward specification\nvia grounded natural language.\nIn International Conference on Machine Learning, pages\n14743\u201314752. PMLR, 2022.\n[49] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual\nquestion answering benchmark requiring external knowledge, 2019.\n[50] Suraj Nair, Eric Mitchell, Kevin Chen, Silvio Savarese, Chelsea Finn, et al. Learning language-\nconditioned robot behavior from offline data and crowd-sourced annotation. In Conference on\nRobot Learning, pages 1303\u20131315. PMLR, 2022.\n[51] OpenAI. Gpt-4 technical report, 2023.\n[52] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning,\npages 8748\u20138763. PMLR, 2021.\n[53] Stephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun. Playing for data: Ground truth\nfrom computer games. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam,\nThe Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 102\u2013118. Springer, 2016.\n15\nSpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities\n[54] Juan Rocamonde, Victoriano Montesinos, Elvis Nava, Ethan Perez, and David Lindner. Vision-\nlanguage models are zero-shot reward models for reinforcement learning.\narXiv preprint\narXiv:2310.12921, 2023.\n[55] Julia Rozanova, Deborah Ferreira, Krishna Dubba, Weiwei Cheng, Dell Zhang, and Andre\nFreitas. Grounding natural language instructions: Can large language models capture spatial\ninformation?, 2021.\n[56] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain,\nJulian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: A platform for embodied\nai research. In Proceedings of the IEEE/CVF international conference on computer vision, pages\n9339\u20139347, 2019.\n[57] Pierre Sermanet, Tianli Ding, Jeffrey Zhao, Fei Xia, Debidatta Dwibedi, Keerthana Gopalakr-\nishnan, Christine Chan, Gabriel Dulac-Arnold, Sharath Maddineni, Nikhil J Joshi, et al. Robovqa:\nMultimodal long-horizon reasoning for robotics. arXiv preprint arXiv:2311.00899, 2023.\n[58] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi,\nLuke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions\nfor everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10740\u201310749, 2020.\n[59] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: What and where pathways for robotic\nmanipulation. In Conference on Robot Learning, pages 894\u2013906. PMLR, 2022.\n[60] Jesse Thomason, Mohit Shridhar, Yonatan Bisk, Chris Paxton, and Luke Zettlemoyer. Language\ngrounding with 3d objects. In Conference on Robot Learning, pages 1691\u20131701. PMLR, 2022.\n[61] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela,\nand Candace Ross.\nWinoground: Probing vision and language models for visio-linguistic\ncompositionality. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5238\u20135248, 2022.\n[62] Johanna Wald, Helisa Dhamo, Nassir Navab, and Federico Tombari. Learning 3d semantic\nscene graphs from 3d indoor reconstructions. 2020 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), Jun 2020.\ndoi: 10.1109/cvpr42600.2020.00402.\nURL\nhttp://dx.doi.org/10.1109/cvpr42600.2020.00402.\n[63] Matthew R Walter, Sachithra Madhaw Hemachandra, Bianca S Homberg, Stefanie Tellex, and\nSeth Teller. Learning semantic maps from natural language descriptions. Robotics: Science and\nSystems, 2013.\n[64] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint\narXiv:2109.01652, 2021.\n[65] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances\nin Neural Information Processing Systems, 35:24824\u201324837, 2022.\n[66] Fei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra Malik, and Silvio Savarese. Gibson\nenv: Real-world perception for embodied agents. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 9068\u20139079, 2018.\n16\nSpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities\n[67] Jiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, and Zhiting Hu.\nLanguage models meet world models: Embodied experiences enhance language models. arXiv\npreprint arXiv:2305.10626, 2023.\n[68] Ted Xiao, Harris Chan, Pierre Sermanet, Ayzaan Wahid, Anthony Brohan, Karol Hausman, Sergey\nLevine, and Jonathan Tompson. Robotic skill acquisition via instruction augmentation with\nvision-language models. arXiv preprint arXiv:2211.11736, 2022.\n[69] Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan\nHuang, Yu Qiao, and Ping Luo. Lvlm-ehub: A comprehensive evaluation benchmark for large\nvision-language models. arXiv preprint arXiv:2306.09265, 2023.\n[70] Rowan Zellers, Ari Holtzman, Matthew Peters, Roozbeh Mottaghi, Aniruddha Kembhavi, Ali\nFarhadi, and Yejin Choi. Piglet: Language grounding through neuro-symbolic interaction in\na 3d world. arXiv preprint arXiv:2106.00188, 2021.\n[71] Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Fed-\nerico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, et al. Socratic models: Composing\nzero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598, 2022.\n17\nSpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities\nA. Appendix\nA.1. Additional Experiments and Details\nSpatial VQA Human Annotated Benchmark\nWe manually labelled 546 qualitative and quantitative\nquestion pairs for WebLi and robotic manipulation VQA. In the human annotation pipeline, we use\nthe spatial VQA data generation pipeline described in Section 3 to provide a sample question for each\nimage, the human annotator would look at the image and the sample question, decide if he or she\nwould like to use the question or type a more proper question, or to skip the annotation for the image.\nThen based on the sample question or the human input question, the human annotator would type\nthe answer he or she thinks as proper in the form of natural language. Fig. 7, Fig. 8, and Fig. 9 shows\nexamples of the human annotated spatial VQA pairs.\nQ: Is the fireplace screen with red doors \nsmaller than the dog standing on the floor \nin width?\nA: no\nQ: Compared to the little boy in a red \nshirt, which side is the man wearing a \nblue shirt on?\nA: left\nQ: Is the palm tree in distance taller than \nthe parked white car?\nA: yes\nQ: Are the windows positioned to the left of \nthe black television?\nA: yes\nFigure 7 | Example question-answer pairs of the Spatial VQA qualitative benchmark\nQ: Determine the distance of the fence \nfrom the giraffes in a zoo relative to the \ncamera.\nA: about 5 meters\nQ: How far is the striped tie towards the \nleft from the black cell phone?\nA: about 0.2 meter\nQ: Could you provide the distance between \nthe sign and the motorcyclist?\nA: about 0.5 meter\nQ: What is the distance between the sand and the \npeople that are standing on the beach?\nA: 0, as the people are directly standing on the \nsand\nFigure 8 | Example question-answer pairs of the Spatial VQA quantitative benchmark\nQ: How far is the yellow robot gripper finger \nfrom the white bottle that is laying on the \nleft side of the table?\nA: 10 cm.\nQ: What is the elevation of the bag of \nchips on table with respect to the table \ntop surface?\nA: The height of the bag of chips on table \nfrom the ground is 6 cm.\nQ: Could you provide the distance between \nthe yellow finger and the black snacks bag?\nA: 5 inches\nQ: How much distance is the red apple \nfrom the can on the table?\nA: It's approximately 20 centimeters.\nFigure 9 | Example question-answer pairs of the robotic manipulation VQA quantitative benchmark\nChain-of-thoughts\nHere we provide more details to our implementation of chain-of-thought spatial\nreasoning. As we mentioned in main paper, we prompt a LLM to perform chain-of-thought reasoning\n18\nSpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities\nis\nof\non\nto\nin\nwhich\nwhite\nside\nright\nwoman\nleft\npositioned\nblack\nyou\nmore\nwearing\ndetermine\ndoes\nbehind\nblue\nfront\nman\nthat's\ncloser\nred\ntable\nbrown\ngreen\nobject\nwooden\ncan\ntowards\nlie\nleft-hand\nstanding\ncompare\nright-hand\nme\nshirt\nsitting\nat\nfurther\nwall\ndo\nchair\nknow\nabove\nfrom\nimage\nyellow\nsmaller\nfloor\ntree\ntell\nperson\ngirl\nterms\nbelow\nother?\npink\nhanging\nposition\nedge\ncome\none\nceiling\nare\nhigher\nbench\ncould\nlight\nless\ndress\nobject,\nwider\nshirt?\ntable?\nflowers\nwidth?\npositions\nground\nplant\nidentify\nrug\nbackground\nbigger\nfind\nwater\nsituated\npeople\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\nFrequency\n1e8\nTop Word Frequencies in Questions\nFigure 10 | Top word frequency. Top words appeared in the training dataset, the red color indicate\nthe word is involved in discussing a spatial concept. It shows that our training data is rich and diverse\nin spatial questions.\nFigure 11 | Error vs Scene Depth ablation. The errors that SpatialVLM make eventually attributes\nto the noise in the data, we plot the distance estimation relative error (capped at 1.0) w.r.t. ground\ntruth distance, and found that there are generally larger errors for bigger distance. We hypothesize\nthis might be due to dataset bias of ZoeDepth [6].\nwith ability to query our SpatialVLM for visual information. Since the LLM isn\u2019t aware of visual infor-\nmation itself, we prompt it to make decision as if it\u2019s playing a game, by asking its friend who can see\nan image that it cannot see itself. We provide the full prompt below:\nListing 1 | SpatialVLM CoT Prompts\nYou are participating in a visual question answering game with your\nfriend. In this game, you are presented with a question which requires visual information from an\nimage to answer. You can see the question but not the image, while your friend can see the image but\nnot the original question. Luckily, you are allowed to decompose the question and ask your friend\nabout the image. Your friend gives you answers which can be used to answer the original question.\nHere is a sample conversation:\n[Question] How can I clean up the table? Give detailed instruction about how should I move my hand.\n[You] What objects are there in the image?\n[Friend] There is an empty coke can, a trash bin and a coffee machine.\n[You] Is the trash bin to the left or to the right of the coke can?\n[Friend] It\u2019s to the left.\n[You] Is the trash bin or the coke can further from you?\n[Friend] They are similar in depth.\n19\nSpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities\n[You] How much to the left is the trash bin compared to the coke can?\n[Friend] Around 20 centimeters.\n[Answer] One\nshould grab the coke can, move it 20 centimeters left and release it so it falls in the trash bin.\nHere is another example:\n[Question] Tell me if the distance\nbetween the blue bottle and the yellow book is longer than that between the plant and the coke can?\n[You] What is the distance between the blue bottle and the yellow book?\n[Tool] 0.3m\n[You] What is the distance between the plant and the coke can?\n[Friend] 0.7m\n[Robot] Since the distance between the blue bottle and the\nyellow book is 0.3m and distance between the plant while the coke can is 0.7m, the distance between\nthe blue bottle and the yellow book is not longer than that between the plant and the coke can.\n[Answer] No.\nHere is another example:\n[Question] Which object can\nbe reached by kids more easily, the white and yellow rabbit toy can or the dark green can of beer?\n[You] What is the elevation of the white and yellow rabbit toy can?\n[Friend] 0.9 m.\n[You] What is the elevation of the dark green can of beer?\n[Friend] 0.2 m.\n[Answer] Since the kids are generally shorter, it is easier for them to\nreach something that are lower in altitude, so it would be easier for them to reach the can of beer.\nNow, given\na new question, try to answer the questions by asking your friend for related visual information.\n[Question]\nBy doing so, we find LLM and SpatialVLM can effectively work together to derive the correct .\nA.2. Implementation Details\nSemantic Filtering\nIn the data filtering phase, we have 2 important objectives: First, we shall filter\nout images that humans can hardly ask any spatial questions, such as photo of a single object before\na white background. Second, since our process requires lifting a 2D image to 3D point cloud, we desire\nthe field of view to be close to a value our monocular depth estimation model is optimized for.\nTo achieve the first objective, we use a pretrained CLIP model to label the candidate images, and\nfilter out those that represent a product or an artwork. Positive CLIP labels include \u201can iphone photo\nof an indoor scene\", and \u201can iphone photo of an outdoor scene\", while negative labels are \u201ca close up\nshot of a single object\", \u201ca product displayed in front of a white background\", \u201can artwork\", \u201ca painting\",\n\u201ca screenshot of graphics user interface\", \u201ca piece of text\", \u201ca sketch\".\nWe choose \u201can iphone photo\u201d as a prefix for positive cases to satisfy the second objective. We observe\nthat this prefix effectively filters out data that has a wider field of view, as well as certain images with\nuncommon perspective ratio.\nSuch design choices in data filtering ensure the images left are within the effective distribution of\nour expert models and qa generation.\n2D Contexts Extraction\nAs we mentioned in method section, we use a variety of off-the-shelf models\nto extract relavent information to synthesize our question answer data. Here we provide additional\ndetails to context extraction. After data filtering, we run a region proposal network (RPN) followed by a\nnon-max suppression (NMS) [30]. For each object bounding box, we run a class agnostic segmentation\nmodel [41] to segment out the object. For each bounding box, we use FlexCap [4] to sample an\nobject-centric caption with random length between 1\u22126 words. In particular, we deliberately choose\n20\nSpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities\nto avoid traditional object detectors, as they are fixed to very coarse categories such as \u201ccake\u201d, while\nour approach can annotate objects with fine-grained descriptions like \u201ccake shaped like a house\u201d and\n\u201ccup cake in plastic container\u201d for the image in Figure 2.\n2DContextto3DContextLifting\nWethenrunthestate-of-the-artmetricdepthdetector, ZoeDepth[6]\non the image. ZoeDepth outputs metric depth (in real-world \u201cmeters\u201d). Combined with an fov esti-\nmation, we are able to to lift 2D images into 3D point clouds as illustrated with real-world scale as\nillustrated in Figure 2.\nIn this point cloud processing step, outliers, or points that significantly deviate from the main group,\nare removed to enhance data accuracy. A clustering algorithm DBSCAN [22] groups the points based\non proximity, focusing on densely populated regions and eliminating sparse, less significant points.\nThis results in a cleaner, more structured point cloud, ideal for subsequent shape and geometry analysis\nwhere dimensions of the shapes are measured. Since we already obtained the semantic segmentation\nfor the point cloud, we may use this information to process outliers at adaptive scales. For smaller\nobjects, we use a smaller threshold, proportional to the along each axis. We observe that such choice\neffectively removes point cloud outliers while also keeping important points for smaller objects. We\nprovide algorithm details below in Algorithm 2.\nListing 2 | Outlier removal with DBScan\nInput:\npoints_obj: Pointcloud of object of interest\npcd: Full Pointcloud\nscale = norm(points_obj.std(axis=0)) * 3.0 + 1e-6\npcd = pcd.remove_stat_outlier(neighbors=50, std=1.2)\npcd = pcd.down_sample(voxel_size=max(0.01, scale / 20))\nlabels = array(pcd.cluster_dbscan(\neps=scale / 3.6, min_points=len(pcd) // 10))\nclear plastic container\nclear plastic container\nappend spatial attribute \n=2\ndelete due to ambiguity\n>2\nno changes\n=1\ngirl wearing a blue sweatshirt\nkeyboard of the laptop\ngirl wearing a purple shirt\ncake shaped like a house\ncake in container\ncake on the plate\nclear plastic container to the left\nclear plastic container to the right\ngirl wearing a blue sweatshirt\nkeyboard of the laptop\ngirl wearing a purple shirt\ncake shaped like a house\ncake in container\ncake on the plate\nEmbedding\nCluster and Modify\nObjects without Ambiguity\nno changes\n=1\nno changes\n=1\nFigure 12 | This is a figure illustrating ambiguity removal.\nCoordinate Canonicalization\nNow we have a 3D point cloud under metric scale. However, the point\ncloud is still in camera frame, which limits the information we can extract. For example, an object closer\nto the upper side of the image isn\u2019t necessarily further from the ground, because the camera might be\npointing at the ground instead of the front. In order to solve this problem, we canonicalize the coordinate\nsystem of the pointcloud by detecting horizontal surfaces. We use a light weight segmentation model [9]\nto segment out pixels correspond to categories like \u201cfloor\u201d, \u201ctable top\u201d, before using RANSAC to fit the\nbiggest plane among these 3D points.\nWhen we detect a surface defined by enough points, we canonicalize the coordinate of the point\ncloud by creating a new origin by projecting camera origin to the detected plane. We use the normal\naxis of the detected plane as z-axis and project the original z-axis of camera on the the plane as the\n21\nSpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities\nnew x-axis. By doing so, when we can detect horizontal surfaces like ground, we effectively transform\nthe point cloud into world coordinate instead of camera coordinate. A more detailed algorithm can be\nfound in our algorithm box 3. On the other hand, when not enough points corresponding to horizontal\nsurfaces are detected, we flag canonicalization as failed and avoid synthesizing questions that depends\non canonicalization, such as questions about elevation.\nListing 3 | Canonicalization Algorithm\nInput:\ndepth: predicted depth for each point\nground_mask: detected ground or not for each point\npoints_cam = unproject_to_pointcloud(depth, fov)\npoints = points_cam\nground_mask = ground_mask.flatten()\ncanonicalized = False\nif ground_mask.mean() > canonicalize_threshold:\ncanonicalized = True\nground_pcd = subset(points_cam, ground_mask)\nplane, _ = ground_pcd.segment_plane(\ndistance_threshold=0.05, ransac_n=3, num_iterations=1000)\nif array([0, -1, 0]) @ plane[:3] < 0:\nplane = -plane\na, b, c, d = plane\nnormal = array([a, b, c])\nez = array([0, 0, 1])\nnew_y = ez - normal @ ez * normal\nnew_y = new_y / norm(new_y)\nrot = array([cross_prod(new_y, normal), new_y, normal])\nrot = array([[0, -1, 0], [1, 0, 0], [0, 0, 1]]) @ rot\ntrans = array([0, 0, d])\npoints_world = points_cam @ rot.T + trans[None]\npoints = points_world\nreturn points, canonicalized\nAmbiguity Removal\nAs shown in Figure 12, we first embed all captions with CLIP encoder [52].\nThis allows us to calculate a cosine distance between each caption pair. This forms a similarity matrix\nbetween all objects. If we threshold the similarity score, we can identify whether an object caption is too\nclose to others. In many cases, we have groups of exactly two similar captions, so we can easily augment\neach caption by appending an differentiating clause such as \u201cthat\u2019s more to the top of the image\u201d. Other\ncases involve more than two similar captions, which we choose to remove all together to avoid ambiguity.\nWe also remove common background objects based on CLIP similarity to categories like \u201csun\u201d or \u201csky\u201d.\nHuman Alignment\nHumans rarely say a distance measure with many decimal places like 0.95 meters.\nRather, they round such distance into some thing they prefer, such as 1 meter or half a meter. We would\nlike our model to align with such human preferences as well. In fact, since depth estimation and fov\nestimation contain irreducible errors, the model should be allowed to phrase its answers with uncertainty\nby rounding just like humans, unless when prompted to be accurate. To this end, we post process any\nquantitative distance unit to align with human preferences. As illustrated in Figure 13, we coded a\ndecision tree to make such alignment. For example, when the estimated distance is 0.86 meters, with\n75% probability we just round it to 1 meters, while we answer 3 feet, 90 cm with some lower probabilities.\nFor a distance like 23 meters, we round it to 20 meters with high probability as well. We also sample\nimperial units instead of metric units by a 20% chance, with similar human-like rounding rules.\nWhile this may align the model better with humans, at sampling time, one may want to get more\naccurate distance estimation. To do so, one simply need to sample multiple distance estimations and\n22\nSpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities\ntake the average. We used this in our robotic experiments to get more fine-grained values. Another\nway is to prompt VLM itself to keep a certain amount of digits. This can be added to data synthesis\nbut we leave this to future work.\n-0.5\n0.5\n0\n0.86\n1m\n0.8m\n90 cm\n3 feet\nHuman-like \nUnits\nHuman-like \nRounding\nHuman Alignment\nHuman-like \nDistribution\nFigure 13 | In human alignment, we define a set of rules that rounds with a probability that mimics\nthe decision rules of humans.\nVLM Training\nWetrainedourmulti-modallargelanguagemodelwithabatchsizeof512andanADAM\noptimizerwithlearningrateof2e-4. WetrainedthePaLM2-E-SmodelusingamixtureoftheoriginalVQA\ndatasets in PaLM-E and our generated spatial VQA dataset, with a sampling ratio of 174:2.5. We initially\ntrain the model with a frozen vision encoder for 110k steps, which doesn\u2019t use all the data we exhausted.\nTherefore the data we generated is more than enough. We then, like described in the experiment\nsection, finetune with the vision encoder either frozen or unfrozen for 70k steps till convergence.\nA.3. Question and Answer Template\nAs we mentioned in our method section, we synthesis question and answering pairs via templates.\nGiven a description of a pair of objects, such as \u201cthe yellow banana\u201d and \u201cthe cake in the shape of a\nhouse\u201d, our data synthesis pipeline can effectively extract an answer based on question types.\nHere we provide a distribution of the question and answer types in Figure 14 and Figure 15, followed\nby a description about each category.\n1. left predicate A question asking whether object A is to the left of object B from the viewer\u2019s\nperspective. The solution is a binary predicate true or false expressed in natural language, or\na phrase expressing uncertainty.\n2. right predicate A question asking whether object A is to the right of object B from the viewer\u2019s\nperspective. The solution is a binary predicate true or false expressed in natural language, or\na phrase expressing uncertainty.\n3. above predicate A question asking whether object A is above object B. Requires coordinate\ncanonicalization. The solution is a binary predicate true or false expressed in natural language,\nor a phrase expressing uncertainty.\n4. below predicate A question asking whether object A is below object B. Requires coordinate\ncanonicalization. The solution is a binary predicate true or false expressed in natural language,\nor a phrase expressing uncertainty.\n5. behind predicate A question asking whether object A behind object B from the viewer\u2019s perspec-\ntive. The solution is a binary predicate true or false expressed in natural language, or a phrase\nexpressing uncertainty.\n6. front predicate A question asking whether object A is in front of object B. The solution is a binary\npredicate true or false expressed in natural language, or a phrase expressing uncertainty.\n23\nSpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities\n7. tall predicate A question asking whether object A is taller than object B. Requires coordinate\ncanonicalization. The solution is a binary predicate true or false expressed in natural language,\nor a phrase expressing uncertainty.\n8. short predicate A question asking whether object A is shorter than object B. Requires coordinate\ncanonicalization. The solution is a binary predicate true or false expressed in natural language,\nor a phrase expressing uncertainty.\n9. wide predicate A question asking whether object A is wider than object B. The solution is a binary\npredicate true or false expressed in natural language, or a phrase expressing uncertainty.\n10. thin predicate A question asking whether object A is thiner than object B. The solution is a binary\npredicate true or false expressed in natural language, or a phrase expressing uncertainty.\n11. big predicate A question asking whether object A is bigger than object B. Requires coordinate\ncanonicalization. The solution is a binary predicate true or false expressed in natural language,\nor a phrase expressing uncertainty.\n12. small predicate A question asking whether object A is smaller than object B. Requires coordinate\ncanonicalization. The solution is a binary predicate true or false expressed in natural language,\nor a phrase expressing uncertainty.\n13. left choice A question asking which of object A and object B is more to the left from the viewer\u2019s\nperspective. The solution is an object name expressed in natural language, or a phrase expressing\nuncertainty.\n14. right choice A question asking which of object A and object B is more to the right from the viewer\u2019s\nperspective. The solution is an object name expressed in natural language, or a phrase expressing\nuncertainty.\n15. above choice A question asking which of object A and object B is more above. Requires coordinate\ncanonicalization. The solution is an object name expressed in natural language, or a phrase\nexpressing uncertainty.\n16. below choice A question asking which of object A and object B is more below. Requires coordinate\ncanonicalization. The solution is an object name expressed in natural language, or a phrase\nexpressing uncertainty.\nleft_predicate\nright_predicate\ndistance\nfront_predicate\nleft_right_classify\nbehind_predicate\nwide_predicate\nthin_predicate\nright_choice\nbehind_choice\nfront_choice\nwide_thin_classify\nbehind_front_classify\nleft_choice\nbig_predicate\nsmall_predicate\nbig_choice\nwide_choice\nleft_diff\nright_diff\nsmall_choice\nbig_small_classify\nfront_diff\nbehind_diff\nthin_choice\ntall_choice\ngap\ntall_short_classify\nabove_predicate\nbelow_predicate\nshort_choice\nbelow_diff\ntall_predicate\nshort_predicate\nabove_diff\nhorizontal_dist\nvertical_dist\nabove_below_classify\nwidth\nheight\nabove_choice\nbelow_choice\nelevation\nQuestion Type\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\nPercentage\nQA distribution when canonicalized = False\nFigure 14 | Distribution of generated question-answer categories when canonicalization fails.\n17. behind choice A question asking which of object A and object B is more behind. The solution\nis an object name expressed in natural language, or a phrase expressing uncertainty.\n18. front choice A question asking which of object A and object B is more to the front from the\nviewer\u2019s perspective. The solution is an object name expressed in natural language, or a phrase\nexpressing uncertainty.\n19. tall choice A question asking which of object A and object B is taller. Requires canonicalization.\nThe solution is an object name expressed in natural language, or a phrase expressing uncertainty.\n24\nSpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities\nleft_predicate\nright_predicate\nabove_predicate\nbelow_predicate\ndistance\nbehind_predicate\nfront_predicate\nleft_right_classify\nabove_below_classify\ntall_short_classify\nbelow_choice\nabove_choice\nright_choice\nleft_choice\nbehind_front_classify\nthin_predicate\nwide_thin_classify\nbehind_choice\nwide_predicate\nshort_predicate\ntall_predicate\nvertical_dist\nhorizontal_dist\nfront_choice\nbig_predicate\nsmall_predicate\nright_diff\ntall_choice\nshort_choice\nwide_choice\nleft_diff\nbig_choice\nsmall_choice\nbig_small_classify\ngap\nheight\nwidth\nelevation\nabove_diff\nbelow_diff\nbehind_diff\nfront_diff\nthin_choice\nQuestion Type\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\nPercentage\nQA distribution when canonicalized = True\nFigure 15 | Distribution of generated question-answer categories when canonicalization is successful.\n20. short choice A question asking which of object A and object B is shorter. Requires canonicalization.\nThe solution is an object name expressed in natural language, or a phrase expressing uncertainty.\n21. wide choice A question asking which of object A and object B is wider. The solution is an object\nname expressed in natural language, or a phrase expressing uncertainty.\n22. thin choice A question asking which of object A and object B is thinner. The solution is an object\nname expressed in natural language, or a phrase expressing uncertainty.\n23. big choice A question asking which of object A and object B is bigger. The solution is an object\nname expressed in natural language, or a phrase expressing uncertainty.\n24. small choice A question asking which of object A and object B is smaller. The solution is an object\nname expressed in natural language, or a phrase expressing uncertainty.\n25. left-right classify A question asking about the left-right comparative relationship between two\nobjects. The solution is left-right expressed in natural language, or a phrase expressing uncertainty.\n26. above-below classify A question asking about the above-below comparative relationship be-\ntween two objects. Requires canonicalization. The solution is above-below expressed in natural\nlanguage, or a phrase expressing uncertainty.\n27. behind-front classify A question asking about the behind-front comparative relationship between\ntwo objects. The solution is behind-front expressed in natural language, or a phrase expressing\nuncertainty.\n28. tall-short classify A question asking about the tall-short comparative relationship between two\nobjects. Requires canonicalization. The solution is tall-short expressed in natural language, or\na phrase expressing uncertainty.\n29. wide-thin classify A question asking about the wide-thin comparative relationship between\ntwo objects. The solution is wide-thin expressed in natural language, or a phrase expressing\nuncertainty.\n30. big-small classify A question asking about the big-small comparative relationship between two\nobjects. Requires canonicalization. The solution is big-small expressed in natural language, or\na phrase expressing uncertainty.\n31. distance estimation A question asking about the distance between the center of two objects. The\nsolution is a distance expressed in natural language, with a human-like distribution for rounding.\n32. gap estimation A question asking about the gap between two objects. The solution is a distance\nexpressed in natural language, with a human-like distribution for rounding.\n33. height estimation A question asking about the height of an object. The solution is a distance\nexpressed in natural language, with a human-like distribution for rounding. Requires canoni-\ncalization.\n34. width estimation A question asking about the width of an object. The solution is a distance\n25\nSpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities\nexpressed in natural language, with a human-like distribution for rounding.\n35. elevation estimation A question asking about the elevation of an object. The solution is a\ndistance expressed in natural language, with a human-like distribution for rounding. Requires\ncanonicalization.\n36. vertical distance estimation A question asking about the vertical distance between the center\nof two objects. The solution is a distance expressed in natural language, with a human-like\ndistribution for rounding. Requires canonicalization.\n37. horizontal distance estimation A question asking about the horizontal distance between the\ncenter of two objects. The solution is a distance expressed in natural language, with a human-like\ndistribution for rounding. Requires canonicalization.\n38. above difference estimationAquestionaskingaboutthethedistancebetweenthebottomofmore\nelevated object and the bottom of the less elevated object. The solution is a distance expressed\nin natural language, with a human-like distribution for rounding. Requires canonicalization.\n39. below difference estimation A question asking about the distance between the bottom of less\nelevated object and the bottom of the more elevated object. The solution is a distance expressed\nin natural language, with a human-like distribution for rounding. Requires canonicalization.\n40. behind difference estimation A question asking about how much an object is in behind another\na distance alone the camera ray. The solution is a distance expressed in natural language, with\na human-like distribution for rounding.\n41. front difference estimation A question asking about how much an object is in in front of another\na distance alone the camera ray. The solution is a distance expressed in natural language, with\na human-like distribution for rounding.\n42. left difference estimation A question asking about how much an object is to the left of another,\nfrom the viewer\u2019s perspective. The solution is a distance expressed in natural language, with a\nhuman-like distribution for rounding.\n43. right difference estimation A question asking about how much an object is to the right of another,\nfrom the viewer\u2019s perspective. The solution is a distance expressed in natural language, with a\nhuman-like distribution for rounding.\nWe provide a small set of question and answer pairs for generating QA data. For the full list please\nrefer to our website.\nListing 4 | SpatialVLM Question and Answer Template\nOBJ_A = \"[A]\"\nOBJ_B = \"[B]\"\nDIST = \"[X]\"\ndistance_questions = [\n\"What is the distance between [A] and [B]?\",\n\"How far apart are [A] and [B]?\",\n\"How distant is [A] from [B]?\",\n\"How far is [A] from [B]?\",\n\"How close is [A] from [B]?\",\n\"Could you measure the distance between [A] and [B]?\",\n\"Can you tell me the distance of [A] from [B]?\",\n\"How far away is [A] from [B]?\",\n\"Can you provide the distance measurement between [A] and [B]?\",\n\"Can you give me an estimation of the distance between [A] and [B]?\",\n\"Could you provide the distance between [A] and [B]?\",\n\"How much distance is there between [A] and [B]?\",\n\"Tell me the distance between [A] and [B].\",\n\"Give me the distance from [A] to [B].\",\n\"Measure the distance from [A] to [B].\",\n\"Measure the distance between [A] and [B].\",\n]\n26\nSpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities\ndistance_answers = [\n\"[X]\",\n\"[A] and [B] are [X] apart.\",\n\"[A] is [X] away from [B].\",\n\"A distance of [X] exists between [A] and [B].\",\n\"[A] is [X] from [B].\",\n\"[A] and [B] are [X] apart from each other.\",\n\"They are [X] apart.\",\n\"The distance of [A] from [B] is [X].\",\n]\nvertical_distance_questions = [\n\"What is the vertical distance between [A] and [B]?\",\n\"How far apart are [A] and [B] vertically?\",\n\"How distant is [A] from [B] vertically?\",\n\"How far is [A] from [B] vertically?\",\n\"Could you measure the vertical distance between [A] and [B]?\",\n\"Can you tell me the vertical distance between [A] and [B]?\",\n\"How far away is [A] from [B] vertically?\",\n(\n\"Can you provide the measurement of the vertical distance between [A]\"\n\" and [B]?\"\n),\n\"Estimate the vertical distance between [A] and [B].\",\n\"Could you provide the vertical distance between [A] and [B]?\",\n\"How much distance is there between [A] and [B] vertically?\",\n\"Tell me the distance between [A] and [B] vertically.\",\n\"Give me the vertical distance from [A] to [B].\",\n\"Measure the vertical distance from [A] to [B].\",\n\"Measure the distance between [A] and [B] vertically.\",\n]\nvertical_distance_answers = [\n\"[X]\",\n\"[A] and [B] are [X] apart vertically.\",\n\"[A] is [X] away from [B] vertically.\",\n\"A vertical distance of [X] exists between [A] and [B].\",\n\"[A] is [X] from [B] vertically.\",\n\"[A] and [B] are [X] apart vertically from each other.\",\n\"Vertically, They are [X] apart.\",\n\"The vertical distance of [A] from [B] is [X].\",\n\"They are [X] apart.\",\n\"It\u2019s approximately [X].\"\n]\nhorizontal_distance_questions = [\n\"What is the horizontal distance between [A] and [B]?\",\n\"How far apart are [A] and [B] horizontally?\",\n\"How distant is [A] from [B] horizontally?\",\n\"How far is [A] from [B] horizontally?\",\n\"Could you measure the horizontal distance between [A] and [B]?\",\n\"Can you tell me the horizontal distance of [A] from [B]?\",\n\"How far away is [A] from [B] horizontally?\",\n(\n\"Can you provide the measurement of the horizontal distance between [A]\"\n\" and [B]?\"\n),\n(\n\"Can you give me an estimation of the horizontal distance between [A]\"\n\" and [B]?\"\n),\n\"Could you provide the horizontal distance between [A] and [B]?\",\n\"How much distance is there between [A] and [B] horizontally?\",\n\"Tell me the distance between [A] and [B] horizontally.\",\n\"Give me the horizontal distance from [A] to [B].\",\n27\nSpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities\n\"Vertial gap between [A] and [B].\",\n\"Measure the horizontal distance from [A] to [B].\",\n\"Measure the distance between [A] and [B] horizontally.\",\n]\nhorizontal_distance_answers = [\n\"[X]\",\n\"[A] and [B] are [X] apart horizontally.\",\n\"[A] is [X] away from [B] horizontally.\",\n\"A horizontal distance of [X] exists between [A] and [B].\",\n\"[A] is [X] from [B] horizontally.\",\n\"[A] and [B] are [X] apart horizontally from each other.\",\n\"Horizontally, They are [X] apart.\",\n\"The horizontal distance of [A] from [B] is [X].\",\n\"They are [X] apart.\",\n\"It\u2019s approximately [X].\"\n]\nwidth_questions = [\n\"Measure the width of [A].\",\n\"Determine the horizontal dimensions of [A].\",\n\"Find out how wide [A] is.\",\n\"What is the width of [A]?\",\n\"How wide is [A]?\",\n\"What are the dimensions of [A] in terms of width?\",\n\"Could you tell me the horizontal size of [A]?\",\n\"What is the approximate width of [A]?\",\n\"How wide is [A]?\",\n\"How much space does [A] occupy horizontally?\",\n\"How big is [A]?\",\n\"How big is [A] in terms of width?\",\n\"What\u2019s the radius of [A]?\"\n]\nwidth_answers = [\n\"[X]\",\n\"The width of [A] is [X].\",\n\"[A] is [X] wide.\",\n\"[A] is [X] in width.\",\n\"It\u2019s [X].\"\n]\nbehind_predicate_questions = [\n\"Is [A] behind [B]?\",\n\"Is the position of [A] more distant than that of [B]?\",\n\"Does [A] lie behind [B]?\",\n\"Is [A] positioned behind [B]?\",\n\"Is [A] further to camera compared to [B]?\",\n\"Does [A] come behind [B]?\",\n\"Is [A] positioned at the back of [B]?\",\n\"Is [A] further to the viewer compared to [B]?\",\n]\nbehind_true = [\n\"Yes.\",\n\"Yes, it is.\",\n\"Yes, it\u2019s behind [B].\",\n\"That\u2019s True.\",\n\"Yes, [A] is further from the viewer.\",\n\"Yes, [A] is behind [B].\"\n]\nbehind_false = [\n\"No.\",\n\"No, it is not.\",\n\"No, it\u2019s in front of [B].\",\n\"That\u2019s False.\",\n28\nSpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities\n\"No, [A] is closer to the viewer.\",\n\"No, [B] is in front of [A].\"\n]\nfront_predicate_questions = [\n\"Is [A] in front of [B]?\",\n\"Is the position of [A] less distant than that of [B]?\",\n\"Does [A] lie in front of [B]?\",\n\"Is [A] positioned in front of [B]?\",\n\"Is [A] closer to camera compared to [B]?\",\n\"Does [A] come in front of [B]?\",\n\"Is [A] positioned before [B]?\",\n\"Is [A] closer to the viewer compared to [B]?\",\n]\nfront_true = [\n\"Yes.\",\n\"Yes, it is.\",\n\"Yes, it\u2019s in front of [B].\",\n\"That\u2019s True.\",\n\"Yes, [A] is closer to the viewer.\",\n\"Yes, [A] is in front of [B].\"\n]\nfront_false = [\n\"No.\",\n\"No, it is not.\",\n\"No, it\u2019s behind [B].\",\n\"That\u2019s False.\",\n\"No, [A] is further to the viewer.\",\n\"No, [B] is behind [A].\"\n]\n29\n"
  },
  {
    "title": "Scalable High-Resolution Pixel-Space Image Synthesis with Hourglass Diffusion Transformers",
    "link": "https://arxiv.org/pdf/2401.11605.pdf",
    "upvote": "18",
    "text": "Scalable High-Resolution Pixel-Space Image Synthesis with\nHourglass Diffusion Transformers\nKatherine Crowson * 1 Stefan Andreas Baumann * 2 Alex Birch * 3 Tanishq Mathew Abraham 1\nDaniel Z. Kaplan 4 Enrico Shippole 5\nFigure 1: Samples generated directly in RGB pixel space using our\nHDiT\nmodels trained on FFHQ-10242 and ImageNet-2562.\n1282\n2562\n5122\n1,0242\n10\n100\n1,000\n10,000\n1 \u00b7 105\nResolution (px)\nComputational Cost (GFLOP)\nPixel-space DiT-B/4\nHDiT-B/4 (Ours)\n\u221270%\n\u221290%\n\u221297%\n\u221299%\nFLOPs\nFigure 2: Scaling of computational cost\nw.r.t. target resolution of our\nHDiT-\nB/4 model vs.\nDiT-B/4 (Peebles &\nXie, 2023a), both in pixel space. At\nmegapixel resolutions, our model incurs\nless than 1% of the computational cost\ncompared to the standard diffusion trans-\nformer DiT at a comparable size.\nAbstract\nWe present the Hourglass Diffusion Transformer\n(HDiT), an image generative model that exhibits\nlinear scaling with pixel count, supporting train-\ning at high-resolution (e.g. 1024 \u00d7 1024) directly\nin pixel-space. Building on the Transformer ar-\nchitecture, which is known to scale to billions of\nparameters, it bridges the gap between the effi-\nciency of convolutional U-Nets and the scalability\nof Transformers.\n*Equal contribution\n1Stability AI 2LMU Munich 3Birchlabs\n4Independent Researcher\n5Independent Researcher.\nCorre-\nspondence to: Katherine Crowson <crowsonkb@gmail.com>,\nStefan Baumann\n<stefan.baumann@lmu.de>,\nAlex Birch\n<alex@birchlabs.co.uk>.\nHDiT trains successfully without typical high-\nresolution training techniques such as multi-\nscale architectures, latent autoencoders or self-\nconditioning. We demonstrate that HDiT per-\nforms competitively with existing models on Im-\nageNet 2562, and sets a new state-of-the-art for\ndiffusion models on FFHQ-10242.\nCode and additional results are available on\nthe project page: crowsonkb.github.io/\nhourglass-diffusion-transformers.\n1\narXiv:2401.11605v1  [cs.CV]  21 Jan 2024\nPreprint. Work in progress.\n1. Introduction\nDiffusion models have emerged as the pre-eminent method\nfor image generation, as evidenced by state-of-the-art ap-\nproaches like Stable Diffusion (Rombach et al., 2022), Im-\nagen (Saharia et al., 2022), eDiff-I (Balaji et al., 2023), or\nDall-E 2 (Ramesh et al., 2022). Their success extends be-\nyond static images to various modalities like video and audio\n(Blattmann et al., 2023; Kong et al., 2021), showcasing the\nversatility of diffusion architectures. This recent success\ncan be attributed to their scalability, stability in training, and\nthe diversity of generated samples.\nWithin the space of diffusion models, there is a large amount\nof variation in the backbone architectures used, spanning\nCNN-based (Ho et al., 2020), transformer-based (Peebles\n& Xie, 2023a; Bao et al., 2023a), CNN-transformer-hybrid\n(Hoogeboom et al., 2023), or even state-space models (Yan\net al., 2023). There is likewise variation in the approaches\nused to scale these models to support high-resolution image\nsynthesis. Current approaches add complexity to training,\nnecessitate additional models, or sacrifice quality.\nLatent diffusion (Rombach et al., 2022) reigns as the dom-\ninant method for achieving high-resolution image synthe-\nsis. In practice, it fails to represent fine detail (Dai et al.,\n2023), impacting sample quality and limiting its utility in\napplications such as image editing. Other approaches to\nhigh-resolution synthesis include cascaded super-resolution\n(Saharia et al., 2022), multi-scale losses (Hoogeboom\net al., 2023), the addition of inputs and outputs at multi-\nple resolutions (Gu et al., 2023), or the utilization of self-\nconditioning and the adaptation of fundamentally new ar-\nchitecture schemes (Jabri et al., 2023).\nOur work tackles high-resolution synthesis via backbone im-\nprovements. We introduce a pure transformer architecture\ninspired by the hierarchical structure introduced in (Nawrot\net al., 2022), which we call the Hourglass Diffusion Trans-\nformer (\nHDiT). By introducing a range of architectural\nimprovements, we obtain a backbone that is capable of\nhigh-quality image generation at megapixel scale in stan-\ndard diffusion setups. This architecture, even at low spatial\nresolutions such as 128 \u00d7 128 is substantially more effi-\ncient than common diffusion transformer backbones such\nas DiT (Peebles & Xie, 2023a) (see Figure 2) while being\ncompetitive in generation quality. Using our method for\nadapting the model architecture to different target resolu-\ntions, we obtain O(n) computational complexity scaling\nwith the target number of image tokens n in place of the\nO(n2) scaling of normal diffusion transformer architectures,\nmaking this the first transformer-based diffusion backbone\narchitecture that is competitive in computational complexity\nwith convolutional U-Nets for pixel-space high-resolution\nimage synthesis.\nOur main contributions are as follows:\n\u2022 We investigate how to adapt transformer-based diffu-\nsion backbones for efficient, high-quality pixel-space\nimage generation\n\u2022 We introduce the Hourglass Diffusion Transformer\n(\nHDiT) architecture for high-resolution pixel-space\nimage generation with subquadratic scaling of compute\ncost with resolution\n\u2022 We demonstrate that this architecture scales to high-\nquality direct pixel-space generation at resolutions of\n1024\u00d71024 without requiring high-resolution-specific\ntraining tricks such as progressive growing or multi-\nscale losses while still being competitive with previous\ntransformer-based architectures at lower resolutions\n2. Related Work\n2.1. Transformers\nTransformers (Vaswani et al., 2017) reign as the state-of-the-\nart architectures in various domains (OpenAI, 2023; Zong\net al., 2022; Zhang et al., 2022b; Yu et al., 2022; Piergio-\nvanni et al., 2023). Notably, they offer great scalability, up to\ntens of billions of parameters in the vision space, (Dehghani\net al., 2023) and beyond that in other domains such as nat-\nural language processing (Chowdhery et al., 2023; Fedus\net al., 2022). Transformers consider interactions between all\nelements in the sequence via the attention mechanism. This\nenables them to learn long-range interactions efficiently but\nhas the downside of causing their computational complexity\nto scale quadratically with the length of the input sequence.\nTransformer-based Diffusion Models Recent works ap-\nplied transformers to diffusion models, both for generating\nlow-dimensional embeddings as part of a diffusion prior\n(Ramesh et al., 2022) and for generating compressed image\nlatents (Peebles & Xie, 2023a; Bao et al., 2023a; Zheng\net al., 2023; Gao et al., 2023; Bao et al., 2023b; Chen\net al., 2023a;b) in a latent diffusion setup (Rombach et al.,\n2022), leading to state-of-the-art performance. Other works\n(Hoogeboom et al., 2023; Jing et al., 2023) also applied\ntransformer-based architectures at the lowest level of a U-\nNet (Ronneberger et al., 2015), or hybridized the two ar-\nchitectures (Cao et al., 2022), going beyond the common\npractice of putting self-attention blocks into the lower lev-\nels of diffusion U-Nets (Ho et al., 2020). However, most\ntransformer architectures for diffusion models are applied\nwith latent diffusion and not directly in pixel space as the\nquadratic computational complexity of the attention mecha-\nnism makes it difficult to apply diffusion transformers for\nhigh-resolution pixel-space image synthesis, as found in\n(Yang et al., 2022).\n2\nPreprint. Work in progress.\nNeighborhood\nHDiT Blocks\nNeighborhood\nHDiT Blocks\nGlobal\nHDiT Blocks\nNeighborhood\nHDiT Blocks\nNeighborhood\nHDiT Blocks\nImage Input\nPatching (p \u00d7 p)\n+ Embedding\n2\u00d72 Pixelunshuffle\nMerge + Proj.\n2\u00d72 Pixelunshuffle\nMerge + Proj.\nProj.+ 2\u00d72\nPixelshuffle Split\nlerp\nProj.+ 2\u00d72\nPixelshuffle Split\nlerp\nRMSNorm\nProj. + p \u00d7 p\nPixelshuffle\nImage Output\nFigure 3: High-level overview of our\nHDiT architecture, specifically the version for ImageNet at input resolutions of 2562\nat patch size p = 4, which has three levels. For any doubling in target resolution, another neighborhood attention block is\nadded. \u201clerp\u201d denotes a linear interpolation with learnable interpolation weight. All\nHDiT blocks have the noise level and\nthe conditioning (embedded jointly using a mapping network) as additional inputs.\nBased on the Diffusion Transformers (DiT) architecture\n(Peebles & Xie, 2023a), two works (Gao et al., 2023; Zheng\net al., 2023) also explored changing the diffusion training\nprocess, adding a masking operation to it to incentivize the\nmodel to learn better relations between object parts. We\nconsider these additional changes to be orthogonal to the\ngoals pursued in this work.\nTransformer Improvements As self-attention\u2019s computa-\ntional complexity scales quadratically with the sequence\nlength, many works (Liu et al., 2021; 2022a; Hassani et al.,\n2023) explored only applying attention to a local set of to-\nkens in vision transformers, leading to linear computational\ncomplexity regarding the number of tokens in these local\nattention mechanisms, at the cost of reducing the receptive\nfield.\nRecently, the typical absolute additive, frequency-based\npositional embedding has also come under scrutiny, with\nimprovements being proposed that effectively encode rela-\ntive position instead of absolute position. Rotary position\nembeddings(Su et al., 2022) is one such example, allowing\ntransformers to flexibly adapt to varying sequence lengths\nand improving performance.\nDespite these developments in improving the transformer\narchitecture, especially ViTs, these modifications have been\nminimally explored for diffusion transformers.\nHourglass Transformers The Hourglass architecture\n(Nawrot et al., 2022) is a hierarchical implementation of\ntransformers that has been demonstrated to be significantly\nmore efficient for language modeling than standard Trans-\nformer models both for training and inference. This is done\nby, over the course of applying the Transformer\u2019s layers,\niteratively shortening and then iteratively re-expanding the\nsequence. Additionally, some skip connections reintroduce\nhigher-resolution information near the expansion steps. Gen-\nerally, this architecture resembles a U-Net (Ronneberger\net al., 2015) but does not use any convolutional layers. Re-\nlatedly, (Wang et al., 2022) also showed great performance\nof a similar structure on image restoration tasks, which\ncan be considered closely related to the denoising diffusion\nobjective.\n2.2. High-Resolution Image Synthesis with Diffusion\nModels\nThere have been extensive investigations into enabling high-\nresolution image synthesis with diffusion models, a task\nthey typically struggle with out of the box. The most popu-\nlar approaches have been separating the generation process\ninto multiple steps by either learning multi-stage diffusion\nmodels, where a diffusion model generates an initial low-\nresolution representation \u2013 either a downsampled image\n(Ho et al., 2021) or a learned spatially downsampled \u201cla-\ntent\u201d representation (Rombach et al., 2022) \u2013 from which a\nhigh-resolution image is then generated by a convolutional\ndecoder (Rombach et al., 2022), another diffusion model\n(Ho et al., 2021; Li et al., 2022), or other generative models\n(Betker et al., 2023; Fischer et al., 2023). This approach\nis also used by the vast majority of transformer-based dif-\nfusion models (see Section 2.1). Recent works have also\nexplored high-resolution image synthesis in pixel space to\nsimplify the overall architecture, exploring fundamentally\nnew backbone architectures (Jabri et al., 2023), transforming\nthe image data using a discrete wavelet transform to reduce\nits spatial dimensions (Hoogeboom et al., 2023), and various\nmodifications to the diffusion (training) process, including\nself-conditioning across sampling steps (Jabri et al., 2023),\nmultiresolution training (Gu et al., 2023), and multiresolu-\ntion losses (Hoogeboom et al., 2023). Simpler approaches\nthat use neither multi-stage approaches nor the aforemen-\ntioned adaptations of the diffusion setup (Song et al., 2021)\ntypically struggle with producing samples that fully utilize\nthe available resolution and are globally coherent.\n3\nPreprint. Work in progress.\n3. Preliminaries\n3.1. Diffusion Models\nDiffusion Models generate data by learning to reverse a\ndiffusion process. This diffusion process is most commonly\ndefined to be a Gaussian noising process. Given a data\ndistribution pdata(x), we define a forward noising process\nwith the family of distributions p(x\u03c3t; \u03c3t) that is obtained\nby adding i.i.d. Gaussian noise of standard deviation \u03c3t\nwhich is provided by a predefined monotonically increasing\nnoise level schedule. Therefore, x\u03c3t = x0 + \u03c3t\u03f5 where\n\u03f5 \u223c N(0, I). A denoising neural network D\u03b8(x\u03c3t, \u03c3t) is\ntrained to predict x0 given x\u03c3t. Sampling is done by starting\nat xT \u223c N\n\u00000, \u03c32\nmaxI\n\u0001\nand sequentially denoising at each\nof the noise levels before resulting in the sample x. The\ndenoiser neural network is trained with a mean-squared\nerror loss:\nEx\u223cpdata(x)E\u03f5,\u03c3t\u223cp(\u03f5,\u03c3t)\n\u0002\n\u03bb\u03c3t\u2225D\u03b8(x\u03c3t, \u03c3t) \u2212 x\u22252\n2\n\u0003\n,\n(1)\nwhere \u03bb\u03c3t is a weighting function. Often the denoiser is\nparameterized as a noise predictor:\n\u03f5\u03b8(x\u03c3t, \u03c3t) = x\u03c3t \u2212 D\u03b8(x\u03c3t, \u03c3t)\n\u03c3t\n.\n(2)\nThis enables the formulation of a loss which predicts \u03f5:\nEx\u223cpdata(x)E\u03f5,\u03c3t\u223cp(\u03f5,\u03c3t)\n\u0002\n\u03bb\u03c3t\u2225\u03f5\u03b8(x\u03c3t, \u03c3t) \u2212 \u03f5\u22252\n2\n\u0003\n.\n(3)\nPrevious work has connected the diffusion model formula-\ntion with score-based generative models by observing that\nthe noise prediction objective is closely related to learning\nthe score via denoising score matching.\nDiffusion Improvements We describe here notable recent\nimprovements to diffusion practices adopted by our model.\nIn EDM (Karras et al., 2022), several modifications to the\ndiffusion framework were shown to improve performance.\nMost notably, preconditioning is applied to the input and\noutput of the denoiser neural network such that the input\nand output magnitudes remain constant over noise levels.\nSpecifically, we rewrite the denoiser neural network as:\nD\u03b8(x\u03c3t, \u03c3t) = cout(\u03c3t)F\u03b8(cin(\u03c3t)x\u03c3t, cnoise(\u03c3t))\n+ cskip(\u03c3t)x\u03c3t.\n(4)\nThe modulation functions are given in (Karras et al., 2022).\nAnother recent approach demonstrated in (Hang et al., 2023)\nadapts the loss weighting at different noise levels based\non clamped signal-to-noise ratios (SNR) in order to im-\nprove model convergence. In the EDM formulation, the loss\nweighting used is:\nw(\u03c3) = min{SNR(\u03c3), \u03b3}\nc2\nout(\u03c3)\n= min{SNR(\u03c3), \u03b3} \u00b7 (\u03c32 \u00b7 \u03c32\ndata)\n\u03c32\ndata + \u03c32\n(5)\nSince the Min-SNR loss weighting is applied for x0-\nparameterization, the c\u22122\nout (\u03c3) factor is incorporated to ac-\ncount for the EDM preconditioner parameterization.\nAnother improvement has been the adaption of noise sched-\nules for high resolutions. It was previously observed (Hooge-\nboom et al., 2023) that the commonly used noise schedules\nthat were originally designed for low resolutions (32x32 or\n64x64) fail to add enough noise at high resolutions. There-\nfore, the noise schedules can be shifted and interpolated\nfrom a reference low-resolution noise schedule in order to\nadd appropriate noise at higher resolutions.\n4. Hourglass Diffusion Transformers\nDiffusion Transformers (Peebles & Xie, 2023a) and other\nsimilar works (see Section 2.1) have demonstrated impres-\nsive performance as denoising diffusion autoencoders in\nlatent diffusion (Rombach et al., 2022) setups, surpassing\nprior works in terms of generative quality (Gao et al., 2023;\nZheng et al., 2023). However, their scalability to high res-\nolutions is limited by the fact that the computational com-\nplexity increases quadratically (O(n2) for images of shape\nh\u00d7w\u00d7channels, with n = w\u00b7h), making them prohibitively\nexpensive to both train and run on high-resolution inputs,\neffectively limiting transformers to spatially compressed\nlatents at sufficiently small dimensions, unless very large\npatch sizes are used (Cao et al., 2022), which have been\nfound to be detrimental to the quality of generated samples\n(Peebles & Xie, 2023a).\nWe propose a new, improved hierarchical architecture based\non Diffusion Transformers (Peebles & Xie, 2023a), and\nHourglass Transformers (Nawrot et al., 2022) \u2013 Hour-\nglass Diffusion Transformers (\nHDiT) \u2013 that enables high-\nquality pixel-space image generation and can be efficiently\nadapted to higher resolutions with a computational com-\nplexity scaling of O(n) instead of O(n2). This means that\neven scaling up these models to direct pixel-space gener-\nation at megapixel resolutions becomes viable, which we\ndemonstrate for models at resolutions of up to 1024 \u00d7 1024\nin Section 5.\n4.1. Leveraging the Hierarchical Nature of Images\nNatural images exhibit hierarchies (Saremi & Sejnowski,\n2013). This makes mapping the image generation process\ninto a hierarchical model an intuitive choice, which has\npreviously been successfully applied in the U-Net architec-\nture (Ronneberger et al., 2015) commonly used in diffusion\nmodels but is not commonly used by diffusion transform-\ners (Peebles & Xie, 2023a; Bao et al., 2023a). To leverage\nthis hierarchical nature of images for our transformer back-\nbone, we apply the hourglass structure (Nawrot et al., 2022),\nwhich has been shown to be effective for a range of different\n4\nPreprint. Work in progress.\nmodalities, including images, for the high-level structure of\nour transformer backbone. Based on the model\u2019s primary\nresolution, we choose the number of levels in the hierar-\nchy, such that the innermost level has 16 \u00d7 16 tokens. As\nlower-resolution levels have to process both low-resolution\ninformation and information that is relevant for following\nhigher-resolution levels, we choose a larger hidden dimen-\nsion for them. For every level on the encoder side, we merge\n2 \u00d7 2 tokens into one spatially using PixelUnShuffle (Shi\net al., 2016) and do the inverse on the decoder side.\nSkip Merging Mechanism One important consideration\nin such architectures is the merging mechanisms of skip\nconnections, as it can influence the final performance sig-\nnificantly (Bao et al., 2023a). While the previous non-\nhierarchical U-ViT (Bao et al., 2023a) uses a concatenation-\nbased skip implementation, similar to the standard U-Net\n(Ronneberger et al., 2015), and found this to be significantly\nbetter than other options, we find additive skips to perform\nbetter for this hierarchical architecture. As the usefulness\nof the information provided by the skips can differ signifi-\ncantly, especially in very deep hierarchies, we additionally\nenable the model to learn the relative importance of the skip\nand the upsampled branch by learning a linear interpola-\ntion (lerp) coefficient f between the two for each skip and\nimplement them as\nx(l. lerp)\nmerged = f \u00b7 xskip + (1 \u2212 f) \u00b7 xupsampled.\n(6)\n4.2. Hourglass Diffusion Transformer Block Design\nInput Tokens\nConditioning\nAdaRMSNorm\nMulti-Head RoPE\nCosine Similarity\nSelf-Attention\n+\nAdaRMSNorm\nHDiT Pointwise\nFeedforward\n+\nMLP\n\u03b31\n\u03b32\n(a)\nHDiT Block Architecture.\nInput Tokens\nConditioning\nAdaLN\nMulti-Head\nSelf-Attention\nScale\n+\nAdaLN\nDiT Pointwise\nFeedforward\nScale\n+\nMLP\n\u03b31, \u03b21\n\u03b11\n\u03b32, \u03b22\n\u03b12\n(b) DiT Block Architecture.\nFigure 4: A comparison of our transformer block architec-\nture and that used by DiT (Peebles & Xie, 2023a).\nOur basic transformer block design (shown in comparison\nwith that of DiT in Figure 4) is generally inspired by the\nblocks used by LLaMA (Touvron et al., 2023), a transformer\narchitecture that has recently been shown to be very capable\nof high-quality generation of language. To enable condi-\ntioning, we make the output scale used by the RMSNorm\noperations adaptive and have the mapping network, which\nis conditioned on the class and diffusion time step, pre-\ndict them. Unlike DiT, we do not employ an (adaptive)\noutput gate, but initialize the output projections of both\nself-attention and FFN blocks to zeros. To make positional\ninformation accessible to the transformer model, common\ndiffusion transformer architectures like DiT and U-ViT use\na learnable additive positional encoding. (Peebles & Xie,\n2023a; Bao et al., 2023a) As it is known to improve mod-\nels\u2019 generalization and their capability of extrapolating to\nnew sequence lengths, we replace this with an adaptation\nof rotary positional embeddings (RoPE) (Su et al., 2022)\nfor 2D image data: we follow an approach similar to (Ho\net al., 2019) and split the encoding to operate on each axis\nseparately, applying RoPE for each spatial axis to distinct\nparts of query and key respectively. We also found that\napplying this encoding scheme to only half of the query and\nkey vectors and not modifying the rest to be beneficial for\nperformance. Overall, we find empirically, that replacing\nthe normal additive positional embedding with our adapted\nRoPE improves convergence and helps remove patch ar-\ntifacts. Additionally to applying RoPE, we use a cosine\nsimilarity-based attention mechanism that has previously\nbeen used in (Liu et al., 2022a)1. We note that a similar\napproach has been proven at the multi-billion parameter\nscale for vision transformers (Dehghani et al., 2023).\nFor the feedforward block (see Figure 5 for a comparison\nwith DiT), instead of having an output gate like DiT, we\nuse GEGLU (Shazeer, 2020), where the modulation signal\ncomes from the data itself instead of the conditioning and is\napplied on the first instead of the second layer of the FFN.\nGEGLU (Shazeer, 2020)\nInput\nLinear\nLinear\nGELU\n\u2299\nDropout\nLinear\n+\n(a)\nHDiT FFN Block.\nInput\nLinear\nGELU\nLinear\n+\n(b) DiT FFN Block.\nFigure 5: A comparison of our pointwise feedforward block\narchitecture and that used by DiT (Peebles & Xie, 2023a).\n1We implement a slight adaptation of their parametrization:\ninstead of parametrizing the per-head scale in logarithmic space,\nwe learn it in linear space, which we find improves stability. See\nAppendix C for details.\n5\nPreprint. Work in progress.\n4.3. Efficient Scaling to High Resolutions\nThe hourglass structure enables us to process an image\nat a variety of resolutions. We use global self-attention\nat low resolutions to achieve coherence, and local self-\nattention (Liu et al., 2021; 2022a; Hassani et al., 2023) at\nall higher resolutions to enhance detail. This limits the need\nfor quadratic-complexity global attention to a manageable\namount, and enjoys linear-complexity scaling for any fur-\nther increase in resolution. Asymptotically, the complexity\nis O(n) (see Appendix A) w.r.t pixel count n.\nA typical choice for localized self-attention would be Shifted\nWindow attention (Liu et al., 2021; 2022a) as used by previ-\nous diffusion models (Cao et al., 2022; Li et al., 2022). We\nfind, however, that Neighborhood attention (Hassani et al.,\n2023) performs significantly better in practice.\nThe maximum resolution at which to apply global self-\nattention2 is a choice determined by dataset (the size at\nwhich small features requiring long-distance coherence be-\ncome large enough for attention to reason about) and by task\n(the smallest feature whose long-distance relationships need\nto be preserved in order to be acceptable). At particularly\nlow resolutions (e.g. 2562), some datasets permit coherent\ngeneration with fewer levels of global attention.\n5. Experiments\nWe evaluate the proposed\nHDiT architecture on condi-\ntional and unconditional image generation, ablating over\narchitectural choices (Section 5.2), and evaluating both\nmegapixel pixel-space image generation (Section 5.3) and\nlarge-scale pixel-space image generation (Section 5.4).\n5.1. Experimental Setup\nTraining Unless mentioned otherwise, we train class-\nconditional models on ImageNet (Deng et al., 2009) at\na resolution of 128 \u00d7 128 directly on RGB pixels with-\nout any kind of latent representation. We train all models\nwith AdamW (Loshchilov & Hutter, 2019) using a con-\nstant learning rate of 5 \u00d7 10\u22124\nand a weight decay of\n\u03bb = 0.01. We generally train at a batch size of 256 for\n400k steps (following (Peebles & Xie, 2023a)) with strat-\nified diffusion timestep sampling and do not use Dropout\nunless noted otherwise.\nFor small-scale ImageNet train-\nings at 128 \u00d7 128, we do not apply any augmentation. For\nruns on small datasets, we apply a non-leaking augmen-\ntation scheme akin to (Karras et al., 2020a). Following\n2For our FFHQ-10242 experiment, we apply two levels of\nglobal attention \u2013 one at 162 and one at 322.\nWhereas for\nImageNet-1282 and 2562, we found like prior works (Ho et al.,\n2020; Hoogeboom et al., 2023; Nichol & Dhariwal, 2021) that a\nsingle level of 162 global attention suffices, due to the low resolu-\ntions at which images were generated.\ncommon diffusion model training practice and (Peebles &\nXie, 2023a), we also compute the exponential moving av-\nerage (EMA) of the model weights with a decay of 0.9999.\nWe use this EMA version of the model for all evaluations\nand generated samples, and perform our sampling using 50\nsteps of DPM++(3M) (Lu et al., 2023; Crowson, 2023) SDE\nsampling. For further details, see Table 6.\nDiffusion We adapt our general training setup from (Kar-\nras et al., 2022), including their preconditioner, and use a\ncontinuous-time diffusion formulation. To enable classifier-\nfree guidance (Ho & Salimans, 2021) during inference, we\ndrop out the class conditioning information 10% of the time\nduring training on class-conditional datasets.\nEvaluation Following common practice for generative im-\nage models, we report the Fr\u00b4echet Inception Distance (FID)\n(Heusel et al., 2017) computed on 50k samples. To com-\npute FID, we use the commonly used implementation from\n(Dhariwal & Nichol, 2021). We also report both the abso-\nlute and asymptotic computational complexity for our main\nablation study, also including FLOPs for higher-resolution\nversions of the architecture.\n5.2. Effect of the Architecture\nTo evaluate the effect of our architectural choices, we per-\nform an ablation study where we start with a basic imple-\nmentation of the hourglass architecture for diffusion and\niteratively add the changes that enable our final architecture\nto efficiently perform high-quality megapixel image syn-\nthesis. We denote the ablation steps as A, B1, ..., E, and\nshow their feature composition and experimental results in\nTable 1. We also provide a set of baselines R1-R4, where\nwe trained DiT (Peebles & Xie, 2023a) models in various\nsettings to enable a fair comparison.\nWe generally use DiT-B-scale models for this comparison\n(approx. 130M parameters for DiT, approx 105M to 120M\nfor\nHDiT depending on the ablation step), due to their\nrelatively low training cost, and train them on pixel-space\nImageNet (Deng et al., 2009) at a resolution of 1282 and\npatch size of 4.\nBaselines We train 4 versions of DiT in different setups\nto provide fair comparisons with it as baselines in Table 1.\nR1 directly uses the official DiT implementation (Peebles\n& Xie, 2023b), but omits the VAE latent computation step\nand adjusts the scaling and variance to fit the data. No other\nchanges were made, as DiT can be directly applied to pixel\nspace (Peebles & Xie, 2023a). To evaluate the influence of\nour trainer and our loss weighting scheme, we implement\na wrapper that directly wraps the original DiT model and\n6\nPreprint. Work in progress.\ntrain it with our trainer3. The results of this experiment are\nshown as R2. R3 replaces the wrapped DiT model with\na hyperparameter-matched single-level version of ablation\nstep A, and matches the performance of the original DiT\ntrained with the original codebase. On top of this setup, we\nalso add soft-min-snr loss weighting to R4 as in ablation\nstep E to enable a fair comparison with our final model. The\ncomputational cost for the same architecture at resolutions\nof 256 \u00d7 256 and 512 \u00d7 512 is also reported. In the case of\nour models, every doubling in resolution involves adding\none local attention block (except for ablation step A, where\nit is global) as per Section 4.1.\nBase Hourglass Structure Configuration A is a simple\nhourglass structure with lower-resolution levels and our lin-\near skip interpolations, and the basic implementation of\nour blocks with RMSNorm, but without GEGLU, and with\nfull global self-attention at every level. A simple additive\npositional encoding is used here. Even this simple archi-\ntecture, without any of our additional changes, is already\nsubstantially cheaper (30% of the FLOPs per forward pass)\nthan similarly-sized DiT (Peebles & Xie, 2023a) models\noperating in pixel space due to the hourglass structure. This\ncomes at the cost of increased FID compared to the DiT\nbaselines at this step in the ablation.\nLocal Attention Mechanism Next, we add local attention\nto all levels except for the lowest-resolution one. We evalu-\nate two options \u2013 Shifted-Window (SWin) (Liu et al., 2021;\n2022a) attention (B1, a common choice in vision transform-\ners and previously also used in diffusion models (Cao et al.,\n2022; Li et al., 2022)) and Neighborhood (Hassani et al.,\n2023) attention (B2). Both result in a small reduction in\nFLOPs even at the low-resolution scale of 128 \u00d7 128 but,\nmost importantly, reduce the computational complexity w.r.t.\nthe base resolution from O(n2) to O(n), enabling practical\nscaling to significantly higher resolutions. Both variants\nsuffer from increased FID due to this reduced expressive-\nness of local attention. Still, this change is significantly less\npronounced for Neighborhood attention, making it a clearly\nsuperior choice in this case compared to the common choice\nof SWin attention.\nFeedforward Activation As the third step, we ablate over\nusing GEGLU (Shazeer, 2020), where the data itself affects\nthe modulation of the outputs of the feedforward block,\ncompared to the standard GeLU for the feedforward net-\nwork. Similar to previous work (Touvron et al., 2023), to\naccount for the effective change of the hidden size due to\nthe GEGLU operation, we decrease the hidden dimension\n3The pixel-space DiT R2 was trained with an identical setup to\nthe rest of our ablations except for the optimizer parameters: we\ninitially tried training this model with our optimizer parameters\nbut found it to both be unstable and worse than with the original\nparameters, so we used the original parameters from (Peebles &\nXie, 2023a) for the comparison.\nfrom 4 \u00b7 dmodel to 3 \u00b7 dmodel. We find that this change sig-\nnificantly improves FID at the cost of a slight increase in\ncomputational cost, as the width of the linear projections in\nthe feedforward block has to be increased to account for the\nhalving in output width.\nPositional Encoding Next, we replace the standard addi-\ntive positional embedding with our 2d axial adaptation of\nRoPE (Su et al., 2022) in D, completing our Hourglass DiT\nbackbone architecture. This further improves FID. As an\nadditional benefit, RoPE should enable significantly better\nextrapolation to other resolutions than additive positional\nembeddings, although our ablation study does not test for\nthat.\nLoss Weighting Finally, we also ablate over replacing\nthe standard\n1\n\u03c32 loss weighting (Ho et al., 2020; Song\net al., 2021) with our adapted min-snr (Hang et al., 2023)\nloss weighting method that we call soft-min-snr (see Ap-\npendix B), which reduces the loss weight compared to SNR\nweighting for low noise levels. This substantially improves\nFID further, demonstrating the effectiveness of\nHDiT\nwhen coupled with an appropriate training setup for pixel-\nspace diffusion.\nSkip Implementation Additionally to the main ablation\nstudy, we also ablate over different skip implementations\nbased on ablation step E. We compare our learnable linear\ninterpolation (lerp), which we empirically found to be espe-\ncially helpful when training deep hierarchies, with both a\nstandard additive skip, where the upsampled and skip data\nare directly added, and a concatenation version, where the\ndata is first concatenated and then projected to the original\nchannel count using a pointwise convolution. The results of\nthis ablation are shown in Table 2. We find that, even for\nshallow hierarchies as used for ImageNet-1282 generation\nin our ablations, the learnable linear interpolation outper-\nforms the addition slightly, with both the learnable lerp and\naddition substantially outperforming the commonly used\nconcatenation.\nTable 2: Skip Information Merging Mechanism Ablation\nSkip Implementation\nFID\u2193\nConcatenation (U-Net (Ronneberger et al., 2015))\n33.75\nAddition (Original Hourglass (Nawrot et al., 2022))\n28.37\nLearnable Linear Interpolation (Ours)\n27.74\n5.3. High-Resolution Pixel-Space Image Synthesis\nIn this section, we train our model for high-resolution pixel-\nspace image synthesis. Following previous works, we train\non FFHQ-10242 (Karras et al., 2021), the standard bench-\nmark dataset for image generation at such high resolutions.\nPrevious works require tricks such as self-conditioning\n7\nPreprint. Work in progress.\nTable 1: Ablation of our architectural choices, starting from a stripped-down implementation of our hourglass diffusion\ntransformer that is similar to DiT-B/4 (Peebles & Xie, 2023a). We also ablate over our additional choice of using soft-min-snr\nloss weighting, which we use to train our full models but do not consider part of our architecture. We also present results for\nvarious DiT-B/4-based models to act as baselines. In addition to training results, we report computational cost per forward\npass at multiple resolutions, including standard resolution-dependent model adaptations.\nConfiguration\nFID\u2193\nGFLOP@1282\u2193\nComplexity\u2193\nGFLOP@2562\nGFLOP@5122\nBaselines (R1 uses 250 DDPM sampling steps with learned \u03c3(t) as in the original publication instead of 50-step DPM++ sampling)\nR1\nDiT-B/4 (Peebles & Xie, 2023a)\n42.03\n106\nO(n2)\n657\n6,341\nR2\nR1 + our trainer (no soft-min-snr)\n69.86\n106\nO(n2)\n657\n6,341\nR3\nR2 + our basic blocks & mapping network\n42.49\n106\nO(n2)\n657\n6,341\nR4\nR3 + soft-min-snr\n30.71\n106\nO(n2)\n657\n6,341\nAblation Steps\nA\nGlobal Attention Diffusion Hourglass (Section 4.1)\n50.76\n032\nO(n2)\n114\n1,060\nB1\nA + Swin Attn. (Liu et al., 2021)\n55.93\n029\nO(n)\n060\n0,185\nB2\nA + Neighborhood Attn. (Hassani et al., 2023)\n51.07\n029\nO(n)\n060\n0,184\nC\nB2 + GeGLU (Shazeer, 2020)\n44.36\n031\nO(n)\n065\n0,198\nD\nC + Axial RoPE (Section 4.2)\n41.41\n031\nO(n)\n065\n0,198\nE\nD + soft-min-snr (Appendix B)\n27.74\n031\nO(n)\n065\n0,198\n(Jabri et al., 2023), multi-scale model architectures (Gu\net al., 2023), or multi-scale losses (Hoogeboom et al., 2023)\nto enable high-quality generation at such high resolutions.\nWe find that our model does not require such tricks to enable\nhigh-quality generation (although we expect them to further\nincrease the quality of generated samples) and, therefore,\ntrain our model without them, with the exception of adapt-\ning the SNR at each step according to the increase in the\nimages\u2019 redundancy (Hoogeboom et al., 2023). As seen in\nsamples from our model in Figure 6, our model can generate\nhigh-quality, globally coherent samples that properly utilize\nthe available resolution to produce sharp pictures with fine\ndetails, even without classifier-free guidance.\nFigure 6: Samples from our 85M-parameter FFHQ-10242\nmodel. Best viewed zoomed in.\nWe benchmark our models against state-of-the-at counter-\nparts in Table 3 for a quantitative comparison. Notably, as\nprecomputed metrics for the NCSN++ (Song et al., 2021)\nbaseline are unavailable, we independently compute them\nusing the provided checkpoint4. We find that our model\nsubstantially outperforms this baseline both quantitatively\nand qualitatively (see Figure 10 and Figure 11 for uncurated\nsamples from both our model and the NCSN++ baseline).\nNotably, our model excels in generating faces with symmet-\nric features, while NCSN++ exhibits noticeable asymmetry.\nMoreover,\nHDiT effectively leverages the available reso-\nlution, producing sharp and finely detailed images, a notable\nimprovement over the NCSN++ model, which often yields\nblurry samples. We find that our model is competitive re-\ngarding FID with high-resolution transformer GANs such as\nHiT (Zhao et al., 2021) or StyleSwin (Zhang et al., 2022a),\nbut does not reach the same FID as state-of-the-art GANs\nsuch as StyleGAN-XL (Sauer et al., 2022). It is worth not-\ning that the FID metric, known for its bias towards samples\ngenerated by GANs over those from diffusion models as\nhighlighted in (Stein et al., 2023), underscores the impres-\nsive performance of our model, suggesting that the achieved\ncloseness might be approaching the lower limit for this spe-\ncific metric for diffusion models.\n4Given resource constraints and the prohibitive sampling cost\nassociated with NCSN++ \u2013 drawing 50k samples would demand\nresources equivalent to training our model \u2013 we report quantitative\nmetrics for NCSN++ based on 5k samples, and also provide 5k\nsample-based metrics for\nHDiT.\n8\nPreprint. Work in progress.\nTable 3: Comparison of our results on FFHQ 1024 \u00d7 1024\nto other models in the literature. 50k samples are used for\nFID computation unless specified otherwise.\nMethod\nFID\u2193\nDiffusion Models\nNCSN++ (Song et al., 2021) (5k samples)\n53.52\nHDiT-85M (Ours, 5k samples)\n08.48\nHDiT-85M (Ours)\n05.23\nGenerative Adversarial Networks\nHiT-B (Zhao et al., 2021)\n06.37\nStyleSwin (Zhang et al., 2022a)\n05.07\nStyleGAN2 (Karras et al., 2020b)\n02.70\nStyleGAN-XL (Sauer et al., 2022)\n02.02\n5.4. Large-Scale ImageNet Image Synthesis\nAs seen in earlier experiments (see Section 5.3),\nHDiT\nshows good performance in generating high-fidelity high-\nresolution samples. To also evaluate its large-scale gener-\nation capabilities, we also train a class-conditional pixel-\nspace ImageNet-2562 model. We note that we have not\nperformed any hyperparameter tuning for this task and that\nthis model, at 557M parameters, is significantly smaller\nthan many state-of-the-art models. In alignment with our\nmethodology from high-resolution experiments, we refrain\nfrom applying non-standard training tricks or diffusion mod-\nifications, and, consistent with (Hoogeboom et al., 2023),\nwe compare results without the application of classifier-free\nguidance, emphasizing an out-of-the-box comparison.\nWe show samples in Figure 7 and compare quantitatively\nwith state-of-the-art diffusion models in Table 4. We find\nthat, qualitatively, our model is readily capable of generating\nhigh-fidelity samples on this task. Compared to the baseline\nmodel DiT, our model achieves a substantially lower FID\nand higher IS despite operating on pixel-space instead of\nlower-resolution latents. Compared to other single-stage\npixel-space diffusion models, our model outperforms simple\nU-Net-based models such as ADM but is outperformed by\nmodels that use self-conditioning during sampling (RIN) or\nare substantially larger (simple diffusion, VDM++).\n6. Conclusion\nThis work presents\nHDiT, a hierarchical pure transformer\nbackbone for image generation with diffusion models that\nscales to high resolutions more efficiently than previous\ntransformer-based backbones. Instead of treating images the\nsame regardless of resolution, this architecture adapts to the\ntarget resolution, processing local phenomena locally at high\nresolutions and separately processing global phenomena\nin low-resolution parts of the hierarchy. This yields an\nFigure 7:\nSamples from our class-conditional 557M-\nparameter ImageNet-2562 model without classifier-free\nguidance.\nTable 4: Comparison of our results on ImageNet-2562 to\nother models in the literature. Following (Hoogeboom et al.,\n2023), we report results without classifier-free guidance.\nBesides FID@50k and IS@50k, we also report trainable\nparameter count, samples seen (training iterations times\nbatch size), and sampling steps.\nMethod\nParams It.\u00d7BS Steps FID\u2193\nIS\u2191\nLatent Diffusion Models\nLDM-4 (Rombach et al., 2022)\n400M\n214M\n250\n10.56 209.5\nDiT-XL/2 (Peebles & Xie, 2023a)\n675M\n1.8B\n250\n09.62 121.5\nU-ViT-H/2 (Bao et al., 2023a)\n501M\n512M\n50\u00b72\n06.58\n-\nMDT-XL/2 (Gao et al., 2023)\n676M\n1.7B\n250\n06.23 143.0\nMaskDiT/2 (Zheng et al., 2023)\n736M\n2B\n40\u00b72\n05.69 178.0\nSingle-Stage Pixel-Space Diffusion Models\niDDPM (Nichol & Dhariwal, 2021)\n-\n-\n250\n32.50\n-\nADM (Dhariwal & Nichol, 2021)\n554M\n507M\n1000 10.94 101.0\nRIN (Jabri et al., 2023)\n410M\n614M\n1000 04.51 161.0\nsimple diffusion (Hoogeboom et al., 2023)\n2B\n1B\n512\n02.77 211.8\nVDM++ (Kingma & Gao, 2023)\n2B\n-\n256\u00b72 02.40 225.3\nHDiT (Ours)\n557M\n742M\n50\u00b72\n06.92 135.2\narchitecture whose computational complexity scales with\nO(n) when used at higher resolutions instead of O(n2),\nbridging the gap between the excellent scaling properties\nof transformer models and the efficiency of U-Nets. We\ndemonstrate that this architecture enables megapixel-scale\npixel-space diffusion models without requiring tricks such\nas self-conditioning or multiresolution architectures and that\nit is competitive with other transformer diffusion backbones\neven at small resolutions, both in fairly matched pixel-space\nsettings, where it is substantially more efficient, and when\ncompared to transformers in latent diffusion setups.\nGiven the promising results in this paper, we believe that\nHDiT can provide a basis for further research into ef-\nficient high-resolution image synthesis. While we only\nfocus on unconditional and class-conditional image syn-\nthesis,\nHDiT is likely well-suited to provide efficiency\nand performance gains in other generative tasks like super-\nresolution, text-to-image generation and synthesis of other\nmodalities such as audio and video, especially with archi-\ntecture scaling.\n9\nPreprint. Work in progress.\n7. Future Work\nHDiT was studied in the context of pixel-space diffusion\nmodels but future works could investigate applying\nHDiT\nin a latent diffusion setup to increase efficiency further and\nachieve multi-megapixel image resolutions, or apply orthog-\nonal tricks such as self-conditioning (Jabri et al., 2023) or\nprogressive training (Sauer et al., 2022) to improve the qual-\nity of generated samples further.\nWhile the results for our large-scale ImageNet training pre-\nsented in Section 5.4 are promising and perform competi-\ntively to many state-of-the-art architectures, we expect that\nsubstantial further improvements are possible with hyperpa-\nrameter tuning and architecture scaling. Future work could\nexplore how to fully realize the potential of this architecture.\nOur architecture with local attention blocks could also be\nuseful for efficient diffusion superresolution and diffusion\nVAE feature decoding models: if all levels are set to per-\nform local attention only (global attention blocks should\nnot be necessary as the global structure is already present\nin the samples for these applications), one can train effi-\ncient transformer-based models that can scale to arbitrary\nresolutions.\nAcknowledgements\nWe thank uptightmoose and Tao Hu for their extensive input\nduring the paper writing process. AB gratefully acknowl-\nedges LAION e.V. for providing access to compute budgets\ngranted by Gauss Centre for Supercomputing e.V. and by\nthe John von Neumann Institute for Computing (NIC) on the\nsupercomputers JUWELS Booster and JURECA at J\u00a8ulich\nSupercomputing Centre (JSC). ES gratefully acknowledges\nStability AI for resources to conduct experiments.\nReferences\nBalaji, Y., Nah, S., Huang, X., Vahdat, A., Song, J., Zhang,\nQ., Kreis, K., Aittala, M., Aila, T., Laine, S., Catanzaro,\nB., Karras, T., and Liu, M.-Y. eDiff-I: Text-to-Image\nDiffusion Models with an Ensemble of Expert Denoisers,\n2023.\nBao, F., Nie, S., Xue, K., Cao, Y., Li, C., Su, H., and Zhu,\nJ. All are Worth Words: A ViT Backbone for Diffusion\nModels. In IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), 2023a.\nBao, F., Nie, S., Xue, K., Li, C., Pu, S., Wang, Y., Yue,\nG., Cao, Y., Su, H., and Zhu, J. One Transformer Fits\nAll Distributions in Multi-Modal Diffusion at Scale. In\nInternational Conference on Machine Learning (ICML).\nJMLR.org, 2023b.\nBetker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L.,\nOuyang, L., Zhuang, J., Lee, J., Guo, Y., Manassra, W.,\nDhariwal, P., Chu, C., Jiao, Y., and Ramesh, A. Improving\nImage Generation with Better Captions. Technical report,\n2023.\nBlattmann, A., Rombach, R., Ling, H., Dockhorn, T., Kim,\nS. W., Fidler, S., and Kreis, K. Align your Latents: High-\nResolution Video Synthesis with Latent Diffusion Mod-\nels. In IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), 2023.\nCao, H., Wang, J., Ren, T., Qi, X., Chen, Y., Yao, Y., and\nZhang, L. Exploring Vision Transformers as Diffusion\nLearners, 2022.\nChen, J., Yu, J., Ge, C., Yao, L., Xie, E., Wu, Y., Wang,\nZ., Kwok, J., Luo, P., Lu, H., and Li, Z. PixArt-\u03b1: Fast\nTraining of Diffusion Transformer for Photorealistic Text-\nto-Image Synthesis, 2023a.\nChen, S., Xu, M., Ren, J., Cong, Y., He, S., Xie, Y., Sinha,\nA., Luo, P., Xiang, T., and Perez-Rua, J.-M. GenTron:\nDelving Deep into Diffusion Transformers for Image and\nVideo Generation, 2023b.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\nG., Roberts, A., Barham, P., Chung, H. W., Sutton,\nC., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko,\nS., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer,\nN., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B.,\nPope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G.,\nYin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S.,\nMichalewski, H., Garcia, X., Misra, V., Robinson, K., Fe-\ndus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph,\nB., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal,\nS., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M.,\nLewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee,\nK., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O.,\nCatasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean,\nJ., Petrov, S., and Fiedel, N. PaLM: Scaling Language\nModeling with Pathways. Journal of Machine Learning\nResearch (JMLR), 2023.\nCrowson,\nK.\nDPM-Solver++(3M)\nSDE,\n2023.\nURL\nhttps://github.com/\ncrowsonkb/k-diffusion/blob/\ncc49cf6182284e577e896943f8e2/k_\ndiffusion/sampling.py#L656.\nDai, X., Hou, J., Ma, C.-Y., Tsai, S., Wang, J., Wang, R.,\nZhang, P., Vandenhende, S., Wang, X., Dubey, A., Yu, M.,\nKadian, A., Radenovic, F., Mahajan, D., Li, K., Zhao, Y.,\nPetrovic, V., Singh, M. K., Motwani, S., Wen, Y., Song,\nY., Sumbaly, R., Ramanathan, V., He, Z., Vajda, P., and\nParikh, D. Emu: Enhancing Image Generation Models\nUsing Photogenic Needles in a Haystack, 2023.\n10\nPreprint. Work in progress.\nDehghani, M., Djolonga, J., Mustafa, B., Padlewski, P.,\nHeek, J., Gilmer, J., Steiner, A., Caron, M., Geirhos, R.,\nAlabdulmohsin, I., Jenatton, R., Beyer, L., Tschannen,\nM., Arnab, A., Wang, X., Riquelme, C., Minderer, M.,\nPuigcerver, J., Evci, U., Kumar, M., Van Steenkiste, S.,\nElsayed, G. F., Mahendran, A., Yu, F., Oliver, A., Huot,\nF., Bastings, J., Collier, M. P., Gritsenko, A. A., Birodkar,\nV., Vasconcelos, C., Tay, Y., Mensink, T., Kolesnikov,\nA., Paveti\u00b4c, F., Tran, D., Kipf, T., Lu\u02c7ci\u00b4c, M., Zhai, X.,\nKeysers, D., Harmsen, J., and Houlsby, N. Scaling Vision\nTransformers to 22 Billion Parameters. In International\nConference on Machine Learning (ICML). JMLR.org,\n2023.\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-\nFei, L.\nImageNet: A large-scale hierarchical image\ndatabase. In IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2009.\nDhariwal, P. and Nichol, A. Q. Diffusion Models Beat\nGANs on Image Synthesis. In Conference on Neural\nInformation Processing Systems (NeurIPS), 2021.\nFedus, W., Zoph, B., and Shazeer, N. Switch Transformers:\nScaling to Trillion Parameter Models with Simple and\nEfficient Sparsity. Journal of Machine Learning Research\n(JMLR), 2022.\nFischer, J. S., Gui, M., Ma, P., Stracke, N., Baumann, S. A.,\nand Ommer, B. Boosting Latent Diffusion with Flow\nMatching, 2023.\nGao, S., Zhou, P., Cheng, M.-M., and Yan, S. Masked\nDiffusion Transformer is a Strong Image Synthesizer.\nIn IEEE/CVF International Conference on Computer\nVision (ICCV), October 2023.\nGu, J., Zhai, S., Zhang, Y., Susskind, J., and Jaitly, N. Ma-\ntryoshka Diffusion Models, 2023.\nHang, T., Gu, S., Li, C., Bao, J., Chen, D., Hu, H., Geng,\nX., and Guo, B. Efficient Diffusion Training via Min-\nSNR Weighting Strategy. In IEEE/CVF International\nConference on Computer Vision (ICCV), October 2023.\nHassani, A., Walton, S., Li, J., Li, S., and Shi, H. Neighbor-\nhood Attention Transformer. In IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR),\nJune 2023.\nHenry, A., Dachapally, P. R., Pawar, S. S., and Chen, Y.\nQuery-key normalization for transformers. In Cohn, T.,\nHe, Y., and Liu, Y. (eds.), Findings of the Association\nfor Computational Linguistics: EMNLP 2020, Novem-\nber 2020.\nHeusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and\nHochreiter, S. GANs Trained by a Two Time-Scale Up-\ndate Rule Converge to a Local Nash Equilibrium. In\nGuyon, I., Luxburg, U. V., Bengio, S., Wallach, H.,\nFergus, R., Vishwanathan, S., and Garnett, R. (eds.),\nConference on Neural Information Processing Systems\n(NeurIPS), 2017.\nHo, J. and Salimans, T. Classifier-Free Diffusion Guidance.\nIn NeurIPS 2021 Workshop on Deep Generative Models\nand Downstream Applications, 2021.\nHo, J., Kalchbrenner, N., Weissenborn, D., and Salimans, T.\nAxial Attention in Multidimensional Transformers, 2019.\nHo, J., Jain, A., and Abbeel, P. Denoising Diffusion Prob-\nabilistic Models. In Conference on Neural Information\nProcessing Systems (NeurIPS), 2020.\nHo, J., Saharia, C., Chan, W., Fleet, D. J., Norouzi, M.,\nand Salimans, T. Cascaded Diffusion Models for High\nFidelity Image Generation, 2021.\nHoogeboom, E., Heek, J., and Salimans, T. Simple Dif-\nfusion: End-to-End Diffusion for High Resolution Im-\nages. In International Conference on Machine Learning\n(ICML). JMLR.org, 2023.\nJabri, A., Fleet, D., and Chen, T. Scalable Adaptive Compu-\ntation for Iterative Generation, 2023.\nJing, X., Chang, Y., Yang, Z., Xie, J., Triantafyllopoulos,\nA., and Schuller, B. W. U-DiT TTS: U-Diffusion Vision\nTransformer for Text-to-Speech, 2023.\nKarras, T., Aittala, M., Hellsten, J., Laine, S., Lehtinen, J.,\nand Aila, T. Training Generative Adversarial Networks\nwith Limited Data. In Conference on Neural Information\nProcessing Systems (NeurIPS), 2020a.\nKarras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J.,\nand Aila, T. Analyzing and Improving the Image Quality\nof StyleGAN. In IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2020b.\nKarras, T., Laine, S., and Aila, T. A Style-Based Genera-\ntor Architecture for Generative Adversarial Networks.\nIEEE Transactions on Pattern Analysis and Machine\nIntelligence (TPAMI), dec 2021.\nKarras, T., Aittala, M., Aila, T., and Laine, S. Elucidating\nthe Design Space of Diffusion-Based Generative Mod-\nels.\nIn Conference on Neural Information Processing\nSystems (NeurIPS), 2022.\nKingma, D. P. and Gao, R. Understanding Diffusion Ob-\njectives as the ELBO with Simple Data Augmentation,\n2023.\n11\nPreprint. Work in progress.\nKong, Z., Ping, W., Huang, J., Zhao, K., and Catanzaro,\nB.\nDiffWave: A Versatile Diffusion Model for Au-\ndio Synthesis. In International Conference on Learning\nRepresentations (ICLR), 2021.\nLi, R., Li, W., Yang, Y., Wei, H., Jiang, J., and Bai, Q.\nSwinv2-Imagen: Hierarchical Vision Transformer Diffu-\nsion Models for Text-to-Image Generation, 2022.\nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin,\nS., and Guo, B. Swin Transformer: Hierarchical Vi-\nsion Transformer using Shifted Windows. In IEEE/CVF\nInternational Conference on Computer Vision (ICCV),\n2021.\nLiu, Z., Hu, H., Lin, Y., Yao, Z., Xie, Z., Wei, Y., Ning, J.,\nCao, Y., Zhang, Z., Dong, L., Wei, F., and Guo, B. Swin\nTransformer V2: Scaling Up Capacity and Resolution. In\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2022a.\nLiu, Z., Hu, H., Lin, Y., Yao, Z., Xie, Z., Wei, Y., Ning, J.,\nCao, Y., Zhang, Z., Dong, L., Wei, F., and Guo, B. SWin\nTransformer v2, 2022b.\nURL https://github.\ncom/microsoft/Swin-Transformer/blob/\n2cb103f2de145ff43bb9f6fc2ae8800c24/\nmodels/swin_transformer_v2.py#L156.\nLoshchilov, I. and Hutter, F. Decoupled Weight Decay\nRegularization. In International Conference on Learning\nRepresentations (ICLR), 2019.\nLu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J. DPM-\nSolver++: Fast Solver for Guided Sampling of Diffusion\nProbabilistic Models, 2023.\nNawrot, P., Tworkowski, S., Tyrolski, M., Kaiser, L.,\nWu, Y., Szegedy, C., and Michalewski, H. Hierarchi-\ncal Transformers Are More Efficient Language Mod-\nels. In Findings of the Association for Computational\nLinguistics: NAACL 2022, July 2022.\nNichol, A. Q. and Dhariwal, P. Improved denoising diffu-\nsion probabilistic models. In International Conference\non Machine Learning (ICML). PMLR, 2021.\nOpenAI. GPT-4 Technical Report. Technical report, 2023.\nPeebles, W. and Xie, S. Scalable Diffusion Models with\nTransformers. In IEEE/CVF International Conference\non Computer Vision (ICCV), October 2023a.\nPeebles, W. and Xie, S. facebookresearch/dit, 2023b. URL\nhttps://github.com/facebookresearch/\nDiT/tree/ed81ce2229091fd4ecc9a22364.\nPiergiovanni, A., Kuo, W., and Angelova, A. Rethinking\nVideo ViTs: Sparse Video Tubes for Joint Image and\nVideo Learning. In IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2023.\nRamesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen,\nM. Hierarchical Text-Conditional Image Generation with\nCLIP Latents, 2022.\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and\nOmmer, B. High-Resolution Image Synthesis With La-\ntent Diffusion Models.\nIn IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), June\n2022.\nRonneberger, O., Fischer, P., and Brox, T. U-Net: Convo-\nlutional Networks for Biomedical Image Segmentation.\nIn Medical Image Computing and Computer-Assisted\nIntervention (MICCAI), 2015.\nSaharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton,\nE., Ghasemipour, S. K. S., Gontijo-Lopes, R., Ayan, B. K.,\nSalimans, T., Ho, J., Fleet, D. J., and Norouzi, M. Pho-\ntorealistic Text-to-Image Diffusion Models with Deep\nLanguage Understanding. In Oh, A. H., Agarwal, A.,\nBelgrave, D., and Cho, K. (eds.), Conference on Neural\nInformation Processing Systems (NeurIPS), 2022.\nSaremi, S. and Sejnowski, T. J.\nHierarchical model\nof natural images and the origin of scale invariance.\nProceedings of the National Academy of Sciences, 110\n(8):3071\u20133076, February 2013. ISSN 1091-6490. doi:\n10.1073/pnas.1222618110.\nURL http://dx.doi.\norg/10.1073/pnas.1222618110.\nSauer, A., Schwarz, K., and Geiger, A. StyleGAN-XL:\nScaling StyleGAN to Large Diverse Datasets. In ACM\nSIGGRAPH 2022 Conference Proceedings. Association\nfor Computing Machinery, 2022.\nShazeer, N. GLU Variants Improve Transformer, 2020.\nShi, W., Caballero, J., Huszar, F., Totz, J., Aitken, A. P.,\nBishop, R., Rueckert, D., and Wang, Z.\nReal-Time\nSingle Image and Video Super-Resolution Using an Ef-\nficient Sub-Pixel Convolutional Neural Network.\nIn\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), jun 2016.\nSong, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A.,\nErmon, S., and Poole, B.\nScore-Based Generative\nModeling through Stochastic Differential Equations. In\nInternational Conference on Learning Representations\n(ICLR), 2021.\nStein, G., Cresswell, J. C., Hosseinzadeh, R., Sui, Y., Ross,\nB. L., Villecroze, V., Liu, Z., Caterini, A. L., Taylor, J.\nE. T., and Loaiza-Ganem, G. Exposing flaws of genera-\ntive model evaluation metrics and their unfair treatment\nof diffusion models, 2023.\n12\nPreprint. Work in progress.\nSu, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., and Liu, Y.\nRoFormer: Enhanced Transformer with Rotary Position\nEmbedding, 2022.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\nM.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\nAzhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lam-\nple, G. LLaMA: Open and Efficient Foundation Language\nModels. Technical report, 2023.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L. u., and Polosukhin, I.\nAttention is All you Need.\nIn Conference on Neural\nInformation Processing Systems (NeurIPS), 2017.\nWang, P.\nFlash Cosine Similarity Attention, 2022.\nURL\nhttps://github.com/lucidrains/\nflash-cosine-sim-attention/tree/\n6f17f29a979a8bcab2479c65b7740523.\nWang, Z., Cun, X., Bao, J., Zhou, W., Liu, J., and Li,\nH. Uformer: A General U-Shaped Transformer for Im-\nage Restoration. In IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2022.\nYan, J. N., Gu, J., and Rush, A. M. Diffusion Models\nWithout Attention, 2023.\nYang, X., Shih, S.-M., Fu, Y., Zhao, X., and Ji, S. Your ViT\nis Secretly a Hybrid Discriminative-Generative Diffusion\nModel, 2022.\nYu, J., Wang, Z., Vasudevan, V., Yeung, L., Seyedhos-\nseini, M., and Wu, Y. CoCa: Contrastive Captioners\nare Image-Text Foundation Models.\nTransactions on\nMachine Learning Research (TMLR), 2022.\nZhang, B., Gu, S., Zhang, B., Bao, J., Chen, D., Wen,\nF., Wang, Y., and Guo, B.\nStyleSwin: Transformer-\nBased GAN for High-Resolution Image Generation. In\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pp. 11304\u2013\n11314, June 2022a.\nZhang, Y., Qin, J., Park, D. S., Han, W., Chiu, C.-C., Pang,\nR., Le, Q. V., and Wu, Y. Pushing the Limits of Semi-\nSupervised Learning for Automatic Speech Recognition,\n2022b.\nZhao, L., Zhang, Z., Chen, T., Metaxas, D., and Zhang, H.\nImproved Transformer for High-Resolution GANs. In\nConference on Neural Information Processing Systems\n(NeurIPS), 2021.\nZheng, H., Nie, W., Vahdat, A., and Anandkumar, A. Fast\nTraining of Diffusion Models with Masked Transformers,\n2023.\nZong, Z., Song, G., and Liu, Y.\nDETRs with Collab-\norative Hybrid Assignments Training.\nIn IEEE/CVF\nInternational Conference on Computer Vision (ICCV),\n2022.\n13\nPreprint. Work in progress.\nA. Computational Complexity of HDiT\nIn a traditional vision transformer, including those for diffu-\nsion models (Peebles & Xie, 2023a; Bao et al., 2023a), the\nasymptotic computational complexity with regard to image\nsize is dominated by the self-attention mechanism, which\nscales as O(n2d) with token/pixel count n and embedding\ndimension d. The feedforward blocks and the attention\nprojection heads, in turn, scale as O(nd2).\nFor our Hourglass Diffusion Transformer architecture, we\nadjust the architecture for different target resolutions, simi-\nlarly to previous approaches used with U-Nets (Ronneberger\net al., 2015). Our architecture is generally divided into mul-\ntiple hierarchical levels, where the outermost level operates\nat full patch resolution, and each additional level operates\nat half of the spatial resolution per axis. For simplicity, we\nwill first cover the cost at square resolutions of powers of\ntwo.\nWhen designing the architecture for a specific resolution,\nwe start with a dataset-dependent core architecture, which,\nfor natural images, typically includes one or two global-\nattention hierarchy levels that operate at 162 or 162 and 322,\nrespectively. Around that are a number of local attention\nlevels. As this core only operates on a fixed resolution, it\ndoes not influence the asymptotic computational complexity\nof the overall model.\nAsymptotic Complexity Scaling When this architecture is\nadapted to a higher resolution, additional local attention lev-\nels with shared parameters are added to keep the innermost\nlevel operating at 162. This means that the number of levels\nin our hierarchy scales with the number of image tokens\nas O(log(n)). While this might intuitively lead one to the\nconclusion of the overall complexity being O(n log(n)), as\nlocal attention layers\u2019 complexity is O(nd), the reduction\nin resolution at each level in the hierarchy has to be consid-\nered: due to the spatial downsampling, the number of tokens\ndecreases by a factor of four at every level in the hierarchy,\nmaking the cost of the self-attention \u2013 the only part of our\nmodel whose complexity does not scale linearly with token\ncount \u2013 of the additional levels\nlog4(n)\u2212log4(rescore)\nX\nl=1\nnd\n4l\u22121 .\nFactoring out n and defining m = l \u2212 1 yields\nn \u00b7\nlog4(n)\u2212log4(rescore)\u22121\nX\nm=0\nd \u00b7\n\u00121\n4\n\u0013m\n,\na (cut-off) geometric series with a common ratio of less than\none, which means that, as the geometric series converges,\nit does not affect the asymptotic complexity, making the\ncumulative complexity of the local self-attention of the addi-\ntional levels O(n). Thus, as no other parts of the scale worse\nthan O(n) either, the overall complexity of the Hourglass\nDiffusion Transformer architecture, as the target resolution\nis increased, is O(n).\nLocal Complexity Scaling at Arbitrary Resolutions\nWhen the target resolution is increased by a factor smaller\nthan a power of two per axis, the architecture is not adapted.\nThis means that, for these intermediate resolutions, a dif-\nferent scaling behavior prevails. Here, the cost of the local\nattention levels, whose number does not change in this case,\nscales with O(n) as before, but the global attention lev-\nels incur a quadratic increase in cost with the resolution.\nAs the resolution is increased further, however, new levels\nare added, which reduce the resolution the global attention\nblocks operate at to their original values, and retaining the\noverall asymptotic scaling behavior of O(n).\nB. Soft-Min-SNR Loss Weighting\nMin-SNR loss weighting (Hang et al., 2023) is a recently\nintroduced training loss weighting scheme that improves dif-\nfusion model training. It adapts the SNR weighting scheme\n(for image data scaled to x \u2208 [\u22121, 1]h\u00d7w\u00d7c)\nwSNR(\u03c3) = 1\n\u03c32\n(7)\nby clipping it at an SNR of \u03b3 = 5:\nwMin-SNR(\u03c3) = min\n\u001a 1\n\u03c32 , \u03b3\n\u001b\n.\n(8)\nWe utilize a slightly modified version that smoothes out\nthe transition between the normal SNR weighting and the\nclipped section:\nwSoft-Min-SNR(\u03c3) =\n1\n\u03c32 + \u03b3\u22121 .\n(9)\nFor \u03c3 \u226a \u03b3 and \u03c3 \u226b \u03b3, this matches Min-SNR, while\nproviding a smooth transition between both sections.\nIn practice, we also change the hyperparameter \u03b3 from\n\u03b3 = 5 to \u03b3 = 4.\nPlotting the resulting loss weight for both min-snr and our\nsoft-min-snr as shown in Figure 8 shows that our loss weight-\ning is identical to min-snr, except for the transition, where it\nis significantly smoother. An ablation of our soft-min-snr\ncompared to min-snr also shows that our loss weighting\nscheme leads to an improved FID score (see Table 5) for\nour model.\n14\nPreprint. Work in progress.\n10\u22122\n10\u22121\n100\n101\n\u03c3\n10\u22122\n100\nw(\u03c3)\nMin-SNR, \u03b3 = 5\nSoft-Min-SNR, \u03b3 = 5\nFigure 8: The resulting loss weighting over \u03c3 for our soft-\nmin-snr weighting (orange) and min-snr weighting (blue)\nwith \u03b3 = 5.\nTable 5: Soft-Min-SNR ablation on RGB ImageNet-1282.\nLoss Weighting\nFID\u2193\nSNR (Table 1 step D)\n41.41\nMin-SNR (Hang et al., 2023) (\u03b3 = 5)\n36.65\nMin-SNR (Hang et al., 2023) (\u03b3 = 4)\n35.62\nSoft-Min-SNR (Ours, \u03b3 = 4, Table 1 step E)\n27.74\nC. Scaled Cosine Similarity Attention\nFor the attention mechanism in\nHDiT, we use a slight\nvariation of the cosine similarity-based attention introduced\nin (Liu et al., 2022a) they dub Scaled Cosine Attention:\ninstead of computing the self-attention as\nSA(Q, K, V ) = softmax\n\u0012 QK\u22a4\n\u221adhead\n\u0013\nV,\n(10)\nthey compute it as\nSCA(Q, K, V ) = softmax\n\u0012simcos(Q, K)\n\u03c4\n+ Bij\n\u0013\nV,\n(11)\nwith \u03c4 being a per-head per-layer learnable scalar, and Bij\nbeing the relative positional bias between pixel i and j\n(which we do not use in our models). In practice, they\nparametrize \u03c4 based on a learnable parameter \u03b8 in the fol-\nlowing way (Liu et al., 2022b):\n1\n\u03c4 = exp\n\u0012\nmin\n\u001a\n\u03b8, log\n1\n0.01\n\u001b\u0013\n,\n(12)\nwith \u03b8 being initialized to \u03b8 = log 10.\nC.1. Improving Scale Learning Stability\nWe find that their parametrization of \u03c4 causes the learned\nscales to vary significantly during training, necessitating the\nclamping to a maximum value of 100 before exponentiation\nto prevent destabilization of the training. In this setting, we\nfind that a significant number of scale factors \u03c4 reach this\nmaximum value and values below 1 during our trainings.\nWe speculate that this instability might be the cause of the\nbehaviour observed in (Wang, 2022), where using scaled co-\nsine similarity attention was detrimental to the performance\nof generative models. To alleviate this problem, we find\nsimply learning \u03c4 directly, as done for normal attention in\n(Henry et al., 2020), prevents this large variance of its values\nin our models, with our converged models\u2019 scale typically\nreaching a range between 5 and 50.\nD. Additional Results for ImageNet-2562\nIn addition to the analyses in Section 5.4, which do not use\nclassifier-free guidance (Ho & Salimans, 2021), we also\nanalyze the FID-IS-tradeoff for difference guidance scales\nwcfg (we follow the guidance scale formulation used in\n(Saharia et al., 2022), where wcfg = 1 corresponds to no\nclassifier-free guidance being applied). The resulting curve\nis shown in Figure 9, with the lowest FID of 3.21 being\nachieved around wcfg = 1.3, with a corresponding IS of\n220.6.\n150\n200\n250\n300\n350\nIS\n3\n4\n5\n6\n7\n8\nFID\nwcfg = 1.0\n1.1\n1.3 1.4\n2.0\nFigure 9: Inception Score vs. Fr\u00b4echet Inception Distance\nat different classifier-free guidance weight scales (1 = no\nguidance) for our 557M ImageNet-2562 model.\n15\nPreprint. Work in progress.\nE. Experiment Details\nParameter\nImageNet-1282\nFFHQ-10242\nImageNet-2562\nExperiment\nAblation E5 (Section 5.2)\nHigh-Res Synthesis (Section 5.3)\nLarge-Scale (Section 5.4)\nParameters\n117M\n85M\n557M\nGFLOP/forward\n31\n206\n198\nTraining Steps\n400k\n1M\n2.2M6\nBatch Size\n256\n256\n256+6\nPrecision\nbfloat16\nbfloat16\nbfloat16\nTraining Hardware\n4 A100\n64 A100\n8 H100\nTraining Time\n15 hours\n5 days\n-\nPatch Size\n4\n4\n4\nLevels (Local + Global Attention)\n1 + 1\n3 + 2\n2 + 1\nDepth\n[2, 11]\n[2, 2, 2, 2, 2]\n[2, 2, 16]\nWidths\n[384, 768]\n[128, 256, 384, 768, 1024]\n[384, 768, 1536]\nAttention Heads (Width / Head Dim)\n[6, 12]\n[2, 4, 6, 12, 16]\n[6, 12, 24]\nAttention Head Dim\n64\n64\n64\nNeighborhood Kernel Size\n7\n7\n7\nMapping Depth\n1\n2\n2\nMapping Width\n768\n768\n768\nData Sigma\n0.5\n0.5\n0.5\nSigma Range\n[1e-3, 1e3]\n[1e-3, 1e3]\n[1e-3, 1e3]\nSigma Sampling Density\ninterpolated cosine\ninterpolated cosine\ninterpolated cosine\nAugmentation Probability\n0\n0.12\n0\nDropout Rate\n0\n[0, 0, 0, 0, 0.1]\n0\nConditioning Dropout Rate\n0.1\n0.1\n0.1\nOptimizer\nAdamW\nAdamW\nAdamW\nLearning Rate\n5e-4\n5e-4\n5e-4\nBetas\n[0.9, 0.95]\n[0.9, 0.95]\n[0.9, 0.95]\nEps\n1e-8\n1e-8\n1e-8\nWeight Decay\n1e-2\n1e-2\n1e-2\nEMA Decay\n0.9999\n0.9999\n0.9999\nSampler\nDPM++(3M) SDE\nDPM++(3M) SDE\nDPM++(3M) SDE\nSampling Steps\n50\n50\n50\nTable 6: Details of our training and inference setup.\n5The other ablation steps generally use the same parameters, except for the architectural changes indicated in the experiment\ndescription.\n6We initially trained for 2M steps. We then experimented with progressively increasing the batch size (waiting until the loss plateaued\nto a new, lower level each time), training at batch size 512 for an additional 50k steps, at batch size 1024 for 100k, and at batch size 2048\nfor 50k steps.\n16\nPreprint. Work in progress.\nF. Our FFHQ-10242 Samples\nFigure 10: Uncurated samples from our 85M\nHDiT FFHQ-10242 model.\n17\nPreprint. Work in progress.\nG. NCSN++ (Song et al., 2021) FFHQ-10242 Reference Samples\nFigure 11: Uncurated reference samples from the NCSN++ (Song et al., 2021) FFHQ-10242 baseline model.\n18\nPreprint. Work in progress.\nH. Our ImageNet-2562 Samples\nFigure 12: Uncurated random class-conditional samples from our 557M\nHDiT ImageNet-2562 model.\n19\nPreprint. Work in progress.\nFigure 13: More uncurated random class-conditional samples from our\nHDiT-557M ImageNet-2562 model.\n20\n"
  },
  {
    "title": "DITTO: Diffusion Inference-Time T-Optimization for Music Generation",
    "link": "https://arxiv.org/pdf/2401.12179.pdf",
    "upvote": "17",
    "text": "DITTO: Diffusion Inference-Time T -Optimization for Music Generation\nZachary Novack 1 2 * Julian McAuley 1 Taylor Berg-Kirkpatrick 1 Nicholas J. Bryan 2\nAbstract\nWe\npropose\nDiffusion\nInference-Time\nT -\nOptimization (DITTO), a general-purpose frame-\nwork for controlling pre-trained text-to-music\ndiffusion models at inference-time via optimiz-\ning initial noise latents. Our method can be used\nto optimize through any differentiable feature\nmatching loss to achieve a target (stylized) output\nand leverages gradient checkpointing for memory\nefficiency. We demonstrate a surprisingly wide-\nrange of applications for music generation includ-\ning inpainting, outpainting, and looping as well\nas intensity, melody, and musical structure con-\ntrol \u2013 all without ever fine-tuning the underlying\nmodel. When we compare our approach against\nrelated training, guidance, and optimization-\nbased methods, we find DITTO achieves state-of-\nthe-art performance on nearly all tasks, including\noutperforming comparable approaches on con-\ntrollability, audio quality, and computational\nefficiency, thus opening the door for high-quality,\nflexible, training-free control of diffusion mod-\nels. Sound examples can be found at https:\n//DITTO-Music.github.io/web/.\n1. Introduction\nLarge-scale diffusion models (Ho et al., 2020) have emerged\nas a leading paradigm for generative media, with strong re-\nsults in diverse modalities such as text-to-image (TTI) gen-\neration (Rombach et al., 2022; Karras et al., 2022; Chen,\n2023), video generation (Ho et al., 2022; Gupta et al., 2023),\nand 3D object generation (Watson et al., 2022; Poole et al.,\n2022). Recently, there has been growing work in applying\nimage-domain methods to audio by treating the frequency-\ndomain spectrograms of audio as images, producing promis-\ning results in general text-to-audio (TTA) generation (Liu\net al., 2023a;b; Huang et al., 2023b) and text-to-music\n\u2217Work done during an internship at Adobe Research.\n1University of California \u2013 San Diego 2Adobe Research. Corre-\nspondence to: Zachary Novack <znovack@ucsd.edu>, Nicholas\nJ. Bryan <njb@ieee.org>.\nPreprint. Under Review. Copyright 2024 by the author(s).\n(TTM) generation (Hawthorne et al., 2022; Forsgren & Mar-\ntiros, 2022; Chen et al., 2023; Huang et al., 2023a; Schneider\net al., 2023). These methods operate via pixel or latent diffu-\nsion (Rombach et al., 2022) over spectrograms with genre,\nmood, and/or keywords control articulated via text prompts.\nSuch approaches, however, typically only offer high-level\ncontrol, motivating further work. Current attempts to add\nmore precise control for TTM diffusion models are promis-\ning yet present their own tradeoffs. Finetuning-based meth-\nods like ControlNet (Wu et al., 2023a; Saharia et al., 2022a;\nZhang et al., 2023) require expensive large-scale training\nwith paired examples and fix the control signal at training\ntime, while inference-time methods that guide the diffusion\nsampling process struggle on fine-grained expressivity due\nto relying on approximations of the model outputs during\nsampling (Levy et al., 2023; Yu et al., 2023).\nWith the goal of a general-purpose and training-free control\nparadigm for TTM diffusion models, we propose DITTO:\nDiffusion Inference-Time T -Optimization. DITTO opti-\nmizes the initial noise latents xT with respect to an arbi-\ntrary feature matching loss across any differentiable diffu-\nsion sampling process to achieve a desired (stylized) output,\nand ensures efficient memory use via gradient checkpoint-\ning (Chen et al., 2016). Despite generally being considered\nas an afterthought and to encode little information (Song\net al., 2020; Preechakul et al., 2022), we show the power\nand precision initial noise latents have to control the diffu-\nsion process for a wide-variety of applications in music cre-\nation, enabling musically-salient feature control and high-\nquality audio editing. Compared to previous optimization-\nbased works from outside the audio domain (Wallace et al.,\n2023a), DITTO achieves SOTA control while also being 2x\nas time and memory efficient.\nOverall, our contributions are:\n\u2022 A general-purpose, training-free framework for con-\ntrolling pre-trained diffusion models to achieve a de-\nsired (stylized) output that leverages gradient check-\npointing for memory efficiency.\n\u2022 Application of our framework to a large number of\nfine-grained time-dependent tasks, including audio-\ndomain music inpainting, outpainting, melody control,\nintensity control, and the newly proposed looping and\nmusical structure control.\n1\narXiv:2401.12179v1  [cs.SD]  22 Jan 2024\nDITTO: Diffusion Inference-Time T -Optimization for Music Generation\nDiffusion \n(Checkpointed)\nMusic Spectrogram\nInput: \u201cUpbeat jazz music\u201d\n1\n2\n3\n4\nTarget Feature\n5\nText-to-Spectrogram\n<latexit sha1_base64=\"siUwrf\nfCM0uhN9NK6fgBas4sgs=\">ACAnicbVDLSsNAFL3xWeur6tJNsAhuL\nIn4WhbduKxgH9CEMplO2qEzkzAzEUvIzm9wq2t34tYfcemfOGmzsK0HL\nhzOuZdzOUHMqNKO820tLa+srq2XNsqbW9s7u5W9/ZaKEolJE0cskp0AK\ncKoIE1NSOdWBLEA0baweg29uPRCoaiQc9jonP0UDQkGKkjeR5AU+fsl\n6qT92sV6k6NWcCe5G4BalCgUav8uP1I5xwIjRmSKmu68TaT5HUFDOSlb\n1EkRjhERqQrqECcaL8dPJzZh8bpW+HkTQjtD1R/16kiCs15oHZ5EgP1b\nyXi/953USH135KRZxoIvA0KEyYrSM7L8DuU0mwZmNDEJbU/GrjIZIa1\nPTErA807c+QYWSeus5l7WLu7Pq/Wbop0SHMIRnIALV1CHO2hAEzDE8AK\nv8GY9W+/Wh/U5XV2yipsDmIH19QsE7JhW</latexit>xt\u22121\n<latexit sha1_base64=\"eZUvYUnT2l9Aq7R7OIiHW8LXEIE=\">ACAHic\nbVC7TgMxENwLrxBeAUoaiwiJKrpDvMoIGsogkYdITpHPcRIrtu9k+xDR6Rq+gRZqOkTLn1DyJ/iSK0hgpJVGM7va3QkizrRx3S+nsLS8srpWXC9tbG5t75R3\n95o6jBWhDRLyULUDrClnkjYM5y2I0WxCDhtBePrzG89UKVZKO/MJK+wEPJBoxgY6X7biCSx7SXuGmvXHGr7hToL/FyUoEc9V75u9sPSyoNIRjrTueGxk\n/wcowmla6saRpiM8ZB2LJVYUO0n04tTdGSVPhqEypY0aKr+nkiw0HoiAtspsBnpRS8T/M6sRlc+gmTUWyoJLNFg5gjE6LsfdRnihLDJ5Zgopi9FZERVp\ngYG9LclkBkmXiLCfwlzZOqd149uz2t1K7ydIpwAIdwDB5cQA1uoA4NICDhGV7g1Xly3px352PWnDymX2Yg/P5A7Mbl6A=</latexit>x0\n<latexit sha1_base64=\"0reAnD1LjL7wrd1p1zfwJGJ/oL4=\">ACA3icbVC7SgNBFL0bXzG+opY2g0GITdgVX2XQxjKCiYFkCbOT2WTIzOw\n6MyuGZUu/wVZrO7H1Qyz9EyePwiQeuHA4517O5QxZ9q47reTW1peWV3Lrxc2Nre2d4q7ew0dJYrQOol4pJoB1pQzSeuGU6bsaJYBJzeB4PrkX/SJVmkbwzw5j6AvckCxnBxkp+WG4HIn3KOqmbHXeKJbfijoEWiTclJZi1in+tLsRSQSVhnCsdctzY+OnWBlGOM0K7UTGJMB7tGWpRILqv1\n0/HSGjqzSRWGk7EiDxurfixQLrYcisJsCm76e90bif14rMeGlnzIZJ4ZKMgkKE45MhEYNoC5TlBg+tAQTxeyviPSxwsTYnmZSApHZTrz5BhZJ46TinVfObk9L1atpO3k4gEMogwcXUIUbqEdCDzAC7zCm/PsvDsfzudkNedMb/ZhBs7XL0FmHU=</latexit>f(x0)\n<latexit sha1_base64=\"TEW2/iHKR4FMDxTclWvyAJ8KINU=\">ACGnicbVC7TsMwFHV4lvIqMLJYrZBaCVUJ4jVWsDAwFIk+pDaKHNdprdpOZDuIK\nsrOR/ANrDCzIVYWRv4Ep+1AW45k6dxz7tW9Pn7EqNK2/W0tLa+srq3nNvKbW9s7u4W9/aYKY4lJA4cslG0fKcKoIA1NSPtSBLEfUZa/vA681sPRCoains9iojLUV/QgGKkjeQVil2O9Ajltym5aDc9XnymHqJnVaOYVaM0opXKNlVewy4SJwpKYEp6l7hp9sLcyJ0JghpTqOHWk3QVJTzEia78aKRAgPUZ90D\nBWIE+Um47+k8MgoPRiE0jyh4Vj9O5EgrtSI+6Yzu1zNe5n4n9eJdXDpJlREsSYCTxYFMYM6hFkwsEclwZqNDEFYUnMrxAMkEdYmvpktPk9NJs58AoukeVJ1zqtnd6el2tU0nRw4BEVQBg64ADVwA+qgATB4Ai/gFbxZz9a79WF9TlqXrOnMAZiB9fULqxOhKw=</latexit>L(f(x0), y)\n<latexit sha1_base64=\"yUeH/F4K6vrdx/RG5WpBhofh5k=\">AB/HicbVDLSgNBEO\nyNrxhfUY9eBoPgKeyKr2PQi8cI5gHJEmYnk2TMzOwyMysS/wGr3r2Jl79F4/+ibPJHkxiQUNR1U13VxBxpo3rfjuFldW19Y3iZmlre2d3r7x/0NRhrAhtkJCHqh1gTmTtGY4bQd\nKYpFwGkrGN9mfuJKs1C+WCSiPoCDyUbMIKNlZrdQKTJpFeuFV3CrRMvJxUIEe9V/7p9kMSCyoN4VjrjudGxk+xMoxwOil1Y0jTMZ4SDuWSiyo9tPptRN0YpU+GoTKljRoqv6dSL\nHQOhGB7RTYjPSil4n/eZ3YDK79lMkoNlS2aJBzJEJUfY6jNFieGJZgoZm9FZIQVJsYGNLclEFkm3mICy6R5VvUuqxf35XaTZ5OEY7gGE7BgyuowR3UoQEHuEFXuHNeXbenQ/n\nc9ZacPKZQ5iD8/ULrE6V8g=</latexit>y\nFeature Matching Loss\n<latexit sha1_base64=\"y\n7hjhdfpTSa3TJKrfale4Ai606w=\">ACF3icbVDLSsNAF\nJ3UV62vqEsXDhahgpREfC2LbnQjFfqCJoTJdNoOnUnCzEQ\nsIUs/wm9wq2t34talS/ESduFbT1w4XDOvdx7jx8xKpVlf\nRu5hcWl5ZX8amFtfWNzy9zeacgwFpjUchC0fKRJIwGpK6\noYqQVCYK4z0jTH1xnfvOBCEnDoKaGEXE56gW0SzFSWvLMf\ncfnyWPq1aAjKYcOR6qPEUvu0pJ1DG+PLNola0R4DyxJ6Q\nIJqh65o/TCXHMSaAwQ1K2bStSboKEopiRtODEkQID1CP\ntDUNECfSTUaPpPBQKx3YDYWuQMGR+nciQVzKIfd1Z3aonP\nUy8T+vHavupZvQIoVCfB4UTdmUIUwSwV2qCBYsaEmCAuq\nb4W4jwTCSmc3tcXnqc7Enk1gnjROyvZ5+ez+tFi5mqSTB3\nvgAJSADS5ABdyAKqgDJ7AC3gFb8az8W58GJ/j1pwxmdkF\nUzC+fgFKZp9G</latexit>xT \u21e0 N(0, I)\n<latexit sha1_base64=\"IK\nToTxQVXcE4WAF5ky3h7BH+V8=\">ACAXicdVDLSsNAFJ3\n4rPVdelmsAjiIiStRt0V3bis0BeksUymk3boTBJmJmIJW\nfkNbnXtTtz6JS79EydtBSt64MLhnHu59x4/ZlQqy/owFhaX\nldWC2vF9Y3Nre3Szm5LRonApIkjFomOjyRhNCRNRUjnVg\nQxH1G2v7oKvfbd0RIGoUNY6Jx9EgpAHFSGnJ7fo8vc9uj\n3sN2CuVLdO5sKtOFVqmNUFObKdiOdCeKWUwQ71X+uz2I5xw\nEirMkJSubcXKS5FQFDOSFbuJDHCIzQgrqYh4kR6eTkDB5\nqpQ+DSOgKFZyoPydSxKUc193cqSG8reXi395bqKCcy+lYZ\nwoEuLpoiBhUEUw/x/2qSBYsbEmCAuqb4V4iATCSqc0t8Xn\nmc7k+3H4P2lVTNsxT29OyrXLWToFsA8OwBGwRmogWtQB02\nAQewRN4Nh6MF+PVeJu2LhizmT0wB+P9C9dxl6k=</late\nxit>x\u21e4\nT\n<latexit sha1_base64=\"apcy1I8midzRcEzLbiD5y2DIYxo=\">AB/HicdVDLSsNAFJ3UV62vqks3g0VwFZJWo+6KblxWsA9oQ5lMJ+3YmUmYmQ\ngl1G9wq2t34tZ/cemfOGkjWNEDFw7n3Mu9wQxo0o7zodVWFpeWV0rpc2Nre2d8q7ey0VJRKTJo5YJDsBUoRQZqakY6sSIB4y0g/FV5rfviVQ0Erd6EhOfo6GgIcVIG6nVEyhgqF+uOLZ34da8GnRsZ4aMuF7V8aCbKxWQo9Evf/YGEU4ERozpFTXdWLtp0hqihmZlnqJIjHCYzQkXUMF4kT56ezaKT\nwygCGkTQlNJypPydSxJWa8MB0cqRH6reXiX953USH535KRZxoIvB8UZgwqCOYvQ4HVBKs2cQhCU1t0I8QhJhbQJa2BLwqcnk+3H4P2lVbdezT29OKvXLPJ0iOACH4Bi4AzUwTVogCbA4A48gifwbD1YL9ar9TZvLVj5zD5YgPX+BYVqlds=</latexit>r\n<latexit sha1_base64=\"apcy1I8midzRcEzLbiD5y2DIYxo=\">AB/HicdVDLSsNAFJ3UV62vqks3g0VwFZJWo+6KblxWsA9oQ5lMJ+3YmUmYmQ\ngl1G9wq2t34tZ/cemfOGkjWNEDFw7n3Mu9wQxo0o7zodVWFpeWV0rpc2Nre2d8q7ey0VJRKTJo5YJDsBUoRQZqakY6sSIB4y0g/FV5rfviVQ0Erd6EhOfo6GgIcVIG6nVEyhgqF+uOLZ34da8GnRsZ4aMuF7V8aCbKxWQo9Evf/YGEU4ERozpFTXdWLtp0hqihmZlnqJIjHCYzQkXUMF4kT56ezaKT\nwygCGkTQlNJypPydSxJWa8MB0cqRH6reXiX953USH535KRZxoIvB8UZgwqCOYvQ4HVBKs2cQhCU1t0I8QhJhbQJa2BLwqcnk+3H4P2lVbdezT29OKvXLPJ0iOACH4Bi4AzUwTVogCbA4A48gifwbD1YL9ar9TZvLVj5zD5YgPX+BYVqlds=</latexit>r\n<latexit sha1_base64=\"apcy1I8midzRcEzLbiD5y2DIYxo=\">AB/HicdVDLSsNAFJ3UV62vqks3g0VwFZJWo+6KblxWsA9oQ5lMJ+3YmUmYmQ\ngl1G9wq2t34tZ/cemfOGkjWNEDFw7n3Mu9wQxo0o7zodVWFpeWV0rpc2Nre2d8q7ey0VJRKTJo5YJDsBUoRQZqakY6sSIB4y0g/FV5rfviVQ0Erd6EhOfo6GgIcVIG6nVEyhgqF+uOLZ34da8GnRsZ4aMuF7V8aCbKxWQo9Evf/YGEU4ERozpFTXdWLtp0hqihmZlnqJIjHCYzQkXUMF4kT56ezaKT\nwygCGkTQlNJypPydSxJWa8MB0cqRH6reXiX953USH535KRZxoIvB8UZgwqCOYvQ4HVBKs2cQhCU1t0I8QhJhbQJa2BLwqcnk+3H4P2lVbdezT29OKvXLPJ0iOACH4Bi4AzUwTVogCbA4A48gifwbD1YL9ar9TZvLVj5zD5YgPX+BYVqlds=</latexit>r\n<latexit sha1_base64=\"apcy1I8midzRcEzLbiD5y2DIYxo=\">AB/HicdVDLSsNAFJ3UV62vqks3g0VwFZJWo+6KblxWsA9oQ5lMJ+3YmUmYmQ\ngl1G9wq2t34tZ/cemfOGkjWNEDFw7n3Mu9wQxo0o7zodVWFpeWV0rpc2Nre2d8q7ey0VJRKTJo5YJDsBUoRQZqakY6sSIB4y0g/FV5rfviVQ0Erd6EhOfo6GgIcVIG6nVEyhgqF+uOLZ34da8GnRsZ4aMuF7V8aCbKxWQo9Evf/YGEU4ERozpFTXdWLtp0hqihmZlnqJIjHCYzQkXUMF4kT56ezaKT\nwygCGkTQlNJypPydSxJWa8MB0cqRH6reXiX953USH535KRZxoIvB8UZgwqCOYvQ4HVBKs2cQhCU1t0I8QhJhbQJa2BLwqcnk+3H4P2lVbdezT29OKvXLPJ0iOACH4Bi4AzUwTVogCbA4A48gifwbD1YL9ar9TZvLVj5zD5YgPX+BYVqlds=</latexit>r\n<latexit sha1_base64=\"apcy1I8midzRcEzLbiD5y2DIYxo=\">AB/\nHicdVDLSsNAFJ3UV62vqks3g0VwFZJWo+6KblxWsA9oQ5lMJ+3YmUmYmQgl1G9wq2t34tZ/cemfOGkjWNEDFw7n3Mu9wQxo0o7zodVWFpeWV0rpc\n2Nre2d8q7ey0VJRKTJo5YJDsBUoRQZqakY6sSIB4y0g/FV5rfviVQ0Erd6EhOfo6GgIcVIG6nVEyhgqF+uOLZ34da8GnRsZ4aMuF7V8aCbKxWQo\n9Evf/YGEU4ERozpFTXdWLtp0hqihmZlnqJIjHCYzQkXUMF4kT56ezaKTwygCGkTQlNJypPydSxJWa8MB0cqRH6reXiX953USH535KRZxoIvB8UZg\nwqCOYvQ4HVBKs2cQhCU1t0I8QhJhbQJa2BLwqcnk+3H4P2lVbdezT29OKvXLPJ0iOACH4Bi4AzUwTVogCbA4A48gifwbD1YL9ar9TZvLVj5zD5Yg\nPX+BYVqlds=</latexit>r\n<latexit sha1_base64=\"apcy1I8midzRcEzLbiD5y2DIYxo=\">AB/\nHicdVDLSsNAFJ3UV62vqks3g0VwFZJWo+6KblxWsA9oQ5lMJ+3YmUmYmQgl1G9wq2t34tZ/cemfOGkjWNEDFw7n3Mu9wQxo0o7zodVWFpeWV0rpc\n2Nre2d8q7ey0VJRKTJo5YJDsBUoRQZqakY6sSIB4y0g/FV5rfviVQ0Erd6EhOfo6GgIcVIG6nVEyhgqF+uOLZ34da8GnRsZ4aMuF7V8aCbKxWQo\n9Evf/YGEU4ERozpFTXdWLtp0hqihmZlnqJIjHCYzQkXUMF4kT56ezaKTwygCGkTQlNJypPydSxJWa8MB0cqRH6reXiX953USH535KRZxoIvB8UZg\nwqCOYvQ4HVBKs2cQhCU1t0I8QhJhbQJa2BLwqcnk+3H4P2lVbdezT29OKvXLPJ0iOACH4Bi4AzUwTVogCbA4A48gifwbD1YL9ar9TZvLVj5zD5Yg\nPX+BYVqlds=</latexit>r\nFigure 1. We propose DITTO, or Diffusion Inference-Time T -Optimization, a general-purpose framework to control pre-trained diffusion\nmodels at inference-time. 1) We sample an initial noise latent xT ; 2) run diffusion sampling to generate a music spectrogram x0; 3) extract\nfeatures from the generated content; 4) input a target (stylized) output; and 5) optimize the initial noise latent to fit any differentiable loss.\n\u2022 Evaluation showing our approach outperforms Multi-\nDiffusion (Bar-Tal et al., 2023), FreeDoM (Yu et al.,\n2023), Guidance Gradients (Levy et al., 2023), Mu-\nsic ControlNet (Wu et al., 2023a), and the comparable\ninference-time optimization method DOODL (Wallace\net al., 2023a), while being over 2x faster than DOODL\nand using half the memory.\n2. Related Work\n2.1. Music Generation Overview\nGenerative music has been a longstanding goal of computer\nmusic researchers (Mathews et al., 1969). Early works\nfocused on symbolic generation (Dong et al., 2018; Chen\net al., 2020; Dai et al., 2021). Recently, audio-domain music\ngeneration has become popular due to advances in language\nmodels (LMs) like MusicLM (Agostinelli et al., 2023) and\ndiffusion models like AudioLDM (Liu et al., 2023a;b).\nLM-based approaches typically operate over discrete com-\npressed audio tokens (Zeghidour et al., 2021; D\u00b4efossez et al.,\n2022; Kumar et al., 2023), generating audio in an auto-\nregressive manner, either over time (Borsos et al., 2023a;\nDonahue et al., 2023; Agostinelli et al., 2023; Copet et al.,\n2023) or sample-iteration (Garcia et al., 2023; Borsos et al.,\n2023b), and convert generated tokens back to audio directly.\nDiffusion-based approaches, on the other hand, typically op-\nerate by generating 2D frequency-domain representations\nof audio or spectrograms that are decoded into audio via\nan image-to-audio converter or a vocoder (Forsgren & Mar-\ntiros, 2022; Liu et al., 2023a;b; Schneider et al., 2023).\n2.2. Diffusion Models with Text Control\nText is currently the most popular control medium for dif-\nfusion models. In this case, text captions are encoded into\nembeddings and injected into a generative model during\ntraining via cross attention, additive modulation, or similar\nas found in models like Stable Diffusion (Rombach et al.,\n2022) or Imagen (Saharia et al., 2022b). Despite its pop-\nularity, global caption-based text conditioning lacks fine-\ngrained control (Zhang et al., 2023), motivating alternatives.\n2.3. Alternative Train-time Control Methods\nWhen adding advanced control to diffusion models, it is\ncommon to fine-tune existing text-controllable models with\nadditional inputs. ControlNet (Zhang et al., 2023) and Uni-\nControlNet (Zhao et al., 2023) use large sets of paired data\nto fine-tune text-to-image diffusion models by adding addi-\ntional control adapters for specific predefined controls such\nas edge detection or pose estimation. To reduce training de-\nmands further, a number of works fine-tune pre-trained mod-\nels on a small number of examples (Ruiz et al., 2023; Choi\net al., 2023; Gal et al., 2022; Kawar et al., 2023). Others\nhave explored using external reward models for fine-tuning,\nthrough direct fine-tuning (Clark et al., 2023; Prabhudesai\net al., 2023) or reinforcement learning-like objectives (Black\net al., 2023). Such approaches, however, still require an ex-\npensive training process and the control mechanism cannot\nbe modified after training. For music, only a ControlNet-\nstyle approach has been taken (Wu et al., 2023a).\n2.4. Inference-time Guidance-based Control\nTo avoid having to perform large-scale model fine-tuning,\ninference-time control methods of diffusion models have\nbecome increasingly popular. Early approaches within this\ncategory include prompt-to-prompt image editing (Hertz\net al., 2022) and MultiDiffusion (Bar-Tal et al., 2023), which\nenable localized object replacement, inpainting, outpainting,\nand spatial-guidance control by fusing multiple masked\ndiffusion paths together. Such methods rely on control\ntargets that can be localized to specific pixel regions of an\nimage and are less applicable for audio spectrograms which\nhave indirect pixel correspondences across frequency and\nmultiple overlapping sources at once.\n2\nDITTO: Diffusion Inference-Time T -Optimization for Music Generation\nWe also note the class of guidance-based methods (Dhari-\nwal & Nichol, 2021; Chung et al., 2023; Levy et al., 2023;\nYu et al., 2023), which introduce updates at each sampling\nstep to steer the generation process via the gradient of a pre-\ntrained classifier \u2207xtL\u03d5(xt). These approaches require ei-\nther an approximation of model outputs during sampling,\nwhich limits fine-grained expressivity, or pre-trained classi-\nfiers per noise level, thus defeating the purpose of inference-\ntime efficiency. For music, guidance-based methods have\nonly been explored in Levy et al. (2023).\n2.5. Inference-time Optimization-based Control\nRecent work has shown optimization through diffusion sam-\npling is possible if GPU memory is managed appropriately.\nDirect optimization of diffusion latents (DOODL) (Wallace\net al., 2023a) leverages the recently proposed EDICT sam-\npling algorithm (Wallace et al., 2023b), which uses affine\ncoupling layers (ACLs) (Dinh et al., 2014; 2016) to form\na fully invertible sampling process, and backpropagates\nthrough EDICT to optimize initial diffusion noise latents for\nimproving CLIP guidance, vocabulary expansion, and aes-\nthetic improvement. DOODL, however, struggles on fine-\ngrained control signals (Wallace et al., 2023a) and has mul-\ntiple downsides due to its reliance on EDICT including 1) it\nis restricted to only invertible diffusion sampling algorithms;\n2) it requires double the model evaluations for both forward\nand reverse sampling that increase latency and memory use;\nand 3) it can suffer from stability issues and reward hacking\ndue to divergence between the ACL diffusion chains.\nThe diffusion noise optimization (DNO) (Karunratanakul\net al., 2023) method proposed backpropagating through the\nsampling process for human motion generation, operating\nover short sequences of limited joint positions. This work\nleverages numerous domain-specific modifications to re-\nduce memory usage, such as using a small (i.e. < 18M pa-\nrameters) transformer encoder-only architecture, very few\nsampling steps, long optimization time, and purely uncon-\nditional generation. Thus, this approach is not applicable\nto more standard generative tasks with higher memory de-\nmands like text-to-image, text-to-audio, and text-to-music.\n3. Diffusion Inference-Time T -Optimization\n3.1. Diffusion Background\nDenoising diffusion probabilistic models (DDPMs) (Sohl-\nDickstein et al., 2015; Ho et al., 2020) or diffusion models\nare defined by a forward and reverse random Markov pro-\ncess. The forward process takes clean data and iteratively\ncorrupts it with noise to train a neural network \u03f5\u03b8. The net-\nwork \u03f5\u03b8 typically inputs (noisy) data xt, the diffusion step\nt, and (text) conditioning information ctext. The reverse pro-\ncess takes random noise xT \u223c N(0, I) and iteratively re-\nfines it with the learned network to generate new data x0\nover T time steps (e.g., 1000) via the sampling process,\nxt\u22121 =\n1\n\u221a\u03b1t\n\u0010\nxt \u2212 1 \u2212 \u03b1t\n\u221a1 \u2212 \u00af\u03b1t\n\u03f5\u03b8(xt, t, ctext)\n\u0011\n+ \u03c3t\u03f5, (1)\nwhere \u03f5 \u223c N(0, I), \u03b10 := 1, \u03b1t and \u00af\u03b1t define the noise\nschedule, \u03c3t is the sampling standard deviation. To reduce\nsampling time, Denoising Diffusion Implicit Model (DDIM)\nsampling (Song et al., 2020) uses an alternative optimization\nobjective that yields a faster sampling process (e.g., 20 \u2212 50\nsteps) that can be deterministic.\nTo improve text conditioning, classifier-free guidance (CFG)\ncan be used to blend conditional and unconditional gener-\nation outputs (Ho & Salimans, 2021). When training with\nCFG, conditioning is randomly set to a null value a fraction\nof the time. During inference, the diffusion model output\n\u03f5\u03b8(xt, t, ctext) is linearly combined with \u03f5\u03b8(xt, t, c\u2205) using\nthe CFG scale w, where c\u2205 are null embeddings. Note, CFG\nduring inference doubles the forward passes of \u03f5\u03b8. For a dif-\nfusion model review, see Appendix A. Though xT \u2019s role in\nthe final output is normally only thought as a random seed,\nwe show in the next section how xT can in fact be leveraged\nfor fine-grained control over the generative process.\n3.2. Problem Formulation\nWe formulate the task of controlling pre-trained diffusion\nmodels as an optimization problem where we fit the initial\nstate, or latents, of the diffusion sampling process to gener-\nate a desired output given a control signal. Formally,\nx\u2217\nT = arg min\nxT L (f(x0), y)\n(2)\nxt\u22121 = Sampler(\u03f5\u03b8, xt, t, c), t = T, T \u2212 1, . . . , 1 (3)\nwhere \u03f5\u03b8 is a pre-trained diffusion model that inputs condi-\ntioning information c, Sampler is any differentiable diffu-\nsion sampling algorithm (e.g. DDIM), xT is a sample of a\nGaussian random vector \u223c N(0, I) otherwise known as ini-\ntial noise latents, x0 is the final generated output of the sam-\npler (e.g. an image or image representation of audio), f(\u00b7) as\nany differentiable feature extraction function, L is any differ-\nentiable loss function, and y are target features or the desired\noutputs. By framing the control task an as arbitrary feature-\nmatching optimization problem on the initial noise latents,\nwe are able to incorporate a diverse range of control tasks.\nSolving (2) using backpropagation, however, is typically in-\ntractable due to extreme memory requirements. Namely, the\ndiffusion sampling process is recursive by design and stan-\ndard automatic differentiation packages customarily require\nstoring all intermediate results for each of T recurrent calls\nto \u03f5\u03b8 within the sampler (2T sets of activations per step\nwhen CFG is used). Thus, even 2-3 sampling steps can cause\nmemory errors with standard U-Net diffusion architectures.\n3\nDITTO: Diffusion Inference-Time T -Optimization for Music Generation\nAlgorithm 1 Diffusion Inference-Time T -Optimization\n(DITTO)\ninput : \u03f5\u03b8, Sampler, sampling steps T, feature extractor\nf, loss L, target feature y, starting latent xT , text con-\nditioning c, optimization steps K, optimizer g.\n1: // Run optimization\n2: for i = 1 to K do\n3:\n// Initialize noise latents\n4:\nxt \u2190 xT\n5:\n// Diffusion sampling w/grad checkpointing per step\n6:\nfor t = T to 1 do\n7:\nxt\u22121 = Checkpoint(Sampler, \u03f5\u03b8, xt, t, c)\n8:\nend for\n9:\n// Extract features from generated output\n10:\n\u02c6y = f(x0)\n11:\n// Compute the loss and backprop\n12:\nxT \u2190 xT \u2212 g(\u2207xT L(\u02c6y, y))\n13: end for\noutput : x0\n3.3. Diffusion with Gradient Checkpointing\nTo circumvent large memory use during optimization, we\nuse gradient rematerialization or checkpointing (Chen et al.,\n2016). Gradient checkpointing was introduced to save mem-\nory when training very deep or recursive neural networks\nby trading memory cost for compute time. The core idea is\nto discard intermediate activation values stored during the\nforward pass of backpropagation that inflict high memory\nuse and/or are low cost to recompute and recalculate them\nduring the backward pass when needed from cached inputs.\nWe use gradient checkpointing on each diffusion model call\nduring sampling, as the memory required to store the in-\ntermediate noisy diffusion tensors and conditioning infor-\nmation is minute compared to the intermediate activations\nof a typical diffusion model (e.g., cross-attention activation\nmaps within a large UNet). Our memory cost to optimize (2)\nwith sampler-step checkpointing is 1) the memory needed to\nrun backpropagation on one diffusion model call \u03f5\u03b8 plus 2)\nthe cost to store the T intermediate noisy diffusion tensors\nxt\u2200t = 0, ..., T and conditioning c. Our memory reduction\nis paid for by an additional forward pass of the sampling\nprocess or T diffusion model calls as shown in Fig. 2.\nIn contrast to our approach, DOODL also leverages gradi-\nent checkpointing via the MemCNN library (Leemput et al.,\n2019). DOODL, however, requires the use of the EDICT\nsampling algorithm that splits the sampling process into two\nnon-parallelizable update equations per sampling step. As\na result, DOODL requires more than double the memory\nand runtime cost, and additionally suffers from overall in-\nstability during the sampling process (particularly at low\nsampling steps) due to EDICT\u2019s \u201cmixing\u201d layers to align\n<latexit sha1_base64=\"zb8BCm6dy9gesiVawqRqMtdjae8=\">AB/nicbVDLSgNBEJyNrxhfUY9eBoPgKeyKr2PQi8cI5gHJEmYns8mQmdlpl\ncMy4Lf4FXP3sSrv+LRP3GS7MEkFjQUVd10dwWx4AZc9sprKyurW8UN0tb2zu7e+X9g6aJEk1Zg0Yi0u2AGCa4Yg3gIFg71ozIQLBWMLqd+K1Hpg2P1AOMY+ZLMlA85JSAldrdQKZPWQ965YpbdafAy8TLSQXlqPfKP91+RBPJFBjOl4bgx+SjRwKlhW6iaGxYSOyIB1LFVEMuOn03szfGKVPg4jbUsB\nnqp/J1IijRnLwHZKAkOz6E3E/7xOAuG1n3IVJ8AUnS0KE4EhwpPncZ9rRkGMLSFUc3srpkOiCQUb0dyWQGY2E28xgWXSPKt6l9WL+/NK7SZPp4iO0DE6R6QjV0h+qogSgS6AW9ojfn2Xl3PpzPWvByWcO0Rycr19GkpbY</latexit>xt\nIntermediate\nActivations\nStored Values\nDiscarded Values\nRecalculated Values\nDITTO Backprop (checkpointing)\n<latexit sha1_base64=\"hEn2BRjdbNmEp6y6RvUNCOlufSM=\">AB6nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PUi8eIeUGyhNnJBkyO7vM9A\nphySd48aCIV7/Im3/jJNmDJhY0FXdHcFsRQGXfbya2srq1v5DcLW9s7u3vF/YOGiRLNeJ1FMtKtgBouheJ1FCh5K9achoHkzWB0N/WbT1wbEakajmPuh3SgRF8wilZ6vOnWusWSW3ZnIMvEy0gJMlS7xa9OL2JyBUySY1pe26Mfko1Cib5pNBJDI8pG9EBb1uqaMiNn85OnZATq/RIP9K2FJKZ+nsi\npaEx4zCwnSHFoVn0puJ/XjvB/rWfChUnyBWbL+onkmBEpn+TntCcoRxbQpkW9lbChlRThjadg3BW3x5mTOyt5l+eLhvFS5zeLIwxEcwyl4cAUVuIcq1IHBAJ7hFd4c6bw4787HvDXnZDOH8AfO5w/wI42W</latexit>AT\n<latexit sha1_base64=\"iMdZqhDp38bGN3XTktBm/qsP9Do=\">AB6nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2OIF48R84JkCbOT3mTI7OwyMy\nuEkE/w4kERr36RN/GSbIHTSxoKq6e4KEsG1cd1vJ7e2vrG5ld8u7Ozu7R8UD4+aOk4VwaLRazaAdUouMSG4UZgO1FIo0BgKxjdzfzWEyrNY1k34wT9iA4kDzmjxkqP1V69Vy5ZXcOskq8jJQgQ61X/Or2Y5ZGKA0TVOuO5ybGn1BlOBM4LXRTjQlIzrAjqWSRqj9yfzUKTmzSp+EsbIlDZmrvycm\nNJ6HAW2M6JmqJe9mfif10lNeOtPuExSg5ItFoWpICYms79JnytkRowtoUxeythQ6oMzadg3BW35lTQvyt51+erhslSpZnHk4QRO4Rw8uIEK3EMNGsBgAM/wCm+OcF6cd+dj0Zpzsplj+APn8wfxqY2X</latexit>BT\n<latexit sha1_base64=\"mIAYoI9nO5A7G5LG1UVGCTqBXU=\">ACAXicbVC7SgNBFL0bXzG+opY2i0GwCrviqwzaWFhEMA/YLGF2MkmGzGOZmRXCkspvsNXaTmz9Ekv/xNlkC5N4YOBwzr3cMyeKGdXG876dwsrq2vpGcbO0tb2zu1feP2hqmShMGlgyqdoR0oRQRqGkbasSKIR4y0otFt5reiNJUikczjknI0UDQPsXIWCnocGSGLH0ft\nItV7yqN4W7TPycVCBHvVv+6fQkTjgRBjOkdeB7sQlTpAzFjExKnUSTGOERGpDAUoE40WE6jTxT6zSc/tS2SeMO1X/bqSIaz3mkZ3MIupFLxP/84LE9K/DlIo4MUTg2aF+wlwj3ez/bo8qg0bW4Kwojari4dIWxsS3NXIp514i82sEyaZ1X/snrxcF6p3eTtFOEIjuEUfLiCGtxBHRqAQcILvMKb8+y8Ox/O52y04OQ7hzAH5+sXZgmYAg=</latexit>L\n<latexit sha1_base64=\"5Czkd7R7CmnEYhIs4hSFH8z18=\">AB/HicbVDLTgJBEOzF+IL9ehlIjHxRHaNryPRi0dM5JHAhvQOszAyO7uZmT\nUhBL/Bq569Ga/+i0f/xAH2IGAlnVSqutPdFSCa+O6305uZXVtfSO/Wdja3tndK+4f1HWcKspqNBaxagaomeCS1Qw3gjUTxTAKBGsEg9uJ3hiSvNYPphwvwIe5KHnKxUr0tMRDYKZbcsjsFWSZeRkqQodop/rS7MU0jJg0VqHXLcxPj1AZTgUbF9qpZgnSAfZYy1KJEdP+aHrtmJxYpUvCWNmShkzV\nvxMjLQeRoHtjND09aI3Ef/zWqkJr/0Rl0lqmKSzRWEqiInJ5HXS5YpRI4aWIFXc3kpoHxVSYwOa2xJEY5uJt5jAMqmflb3L8sX9ealyk6WThyM4hlPw4AoqcAdVqAGFR3iBV3hznp1358P5nLXmnGzmEObgfP0CRDWVsA=</latexit>r\n<latexit sha1_base64=\"mIAYoI9nO5A7G5LG1UVGCTqBXU=\">ACAXicbVC7SgNBFL0bXzG+opY2i0GwCrviqwzaWFhEMA/YLGF2MkmGzGOZmRXCkspvsNXaTmz9Ekv/xNlkC5N4YOBwzr3cMyeKGdXG876dwsrq2vpGcbO0tb2zu1feP2hqmShMGlgyqdoR0oRQRqGkbasSKIR4y0otFt5reiNJUikczjknI0UDQPsXIWCnocGSGLH0ft\nItV7yqN4W7TPycVCBHvVv+6fQkTjgRBjOkdeB7sQlTpAzFjExKnUSTGOERGpDAUoE40WE6jTxT6zSc/tS2SeMO1X/bqSIaz3mkZ3MIupFLxP/84LE9K/DlIo4MUTg2aF+wlwj3ez/bo8qg0bW4Kwojari4dIWxsS3NXIp514i82sEyaZ1X/snrxcF6p3eTtFOEIjuEUfLiCGtxBHRqAQcILvMKb8+y8Ox/O52y04OQ7hzAH5+sXZgmYAg=</latexit>L\n<latexit sha1_base64=\"p/zZUtXUnmaWgdOFWVx/x7XIJK4=\">AB+XicbVA9TwJBEJ3DL8Qv1NJmIzGxInfGr5JgY4nRAxK4kL1lDzbs7l1290zIhZ9gq7WdsfXWPpPXOAKAV8yct7M5mZFyacaeO6305hbX1jc6u4XdrZ3ds/KB8eNXWcKkJ9EvNYtUOsKWeS+oYZTtuJoliEnLbC0d3Ubz1TpVk\nsn8w4oYHA8kiRrCx0mO95/bKFbfqzoBWiZeTCuRo9Mo/3X5MUkGlIRxr3fHcxAQZVoYRTielbqpgskID2jHUokF1UE2O3WCzqzSR1GsbEmDZurfiQwLrcitJ0Cm6Fe9qbif14nNdFtkDGZpIZKMl8UpRyZGE3/Rn2mKDF8bAkmitlbERlihYmx6SxsCcXEZuItJ7BKmhdV7p69XBZqdXzdIpwAqdwDh7cQA3uoQE+EBjAC7zCm5M5786H8zlvLTj5zDEswPn6BWV+lAk=</latexit>B0\n<latexit sha1_base64=\"9Znxn6VDIRyBDLkDowM/rKGyYE=\">ACnicbVC7TsNAEDzDOFlQkljESFRTbiVUbQUAaJPKTYis6XTXLKnW3drR\nGRlT/gG2ihpkO0/AQlf8IlcUESRlpdmZXu5owEVyj635bK6tr6xubha3i9s7u3r59UGroOFUM6iwWsWqFVIPgEdSRo4BWoDKUEAzHN5O/OYjKM3j6AFHCQS9iPe4yikTp2yQ+lD4nmwnQ+DgBpxy67FXcKZ5l4OSmTHLWO/eN3Y5ZKiJAJqnXbcxMqQMwHjop9qSCgb0j60DY2oB1k09/HzolR\nuk4vVqYidKbq342MSq1HMjSTkuJAL3oT8T+vnWLvOsh4lKQIEZsd6qXCwdiZBOF0uQKGYmQIZYqbXx02oIoyNHNXQnl2GTiLSawTBpnFe+ycnF/Xq7e5OkUyBE5JqfEI1ekSu5IjdQJI0/khbySN+vZerc+rM/Z6IqV7xySOVhfv4VEm1E=</latexit>\u270f\u2713\n<latexit sha1_base64=\"8tPVoKSZYCmzIB4Wh5NsXh7F9Zo=\">AB/nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PQi8cI5gHJEmYns8mQmdlZlYMy4Lf4FXP3sSrv+LRP3GS7MEkFjQUV\nd10dwUxZ9q47rdTWFldW98obpa2tnd298r7B0dJYrQBol4pNoB1pQzSRuGU7bsaJYBJy2gtHtxG89UqVZJB/MOKa+wAPJQkawsVK7G4j0Ket5vXLFrbpToGXi5aQCOeq98k+3H5FEUGkIx1p3PDc2foqVYTrNRNI0xGeEB7VgqsaDaT6f3ZujEKn0URsqWNGiq/p1IsdB6LALbKbAZ6kVvIv7ndRITXvspk3FiqCSzRWHCkYnQ5HnUZ4oSw8eWYKYvRWRIVaYGBvR3JZAZDYTbzGBZd\nI8q3qX1Yv780rtJk+nCEdwDKfgwRXU4A7q0ACHF7gFd6cZ+fd+XA+Z60FJ585hDk4X7/dCpaV</latexit>x1\n<latexit sha1_base64=\"ROsC+1STl6wTckLfk8UwPJKESA=\">AB/nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PQi8cI5gHJEmYns8mQmdlZlYMy4Lf4FXP3sSrv+LRP3GS7MEkFjQUVd10dwUxZ9q47rdTWFldW98obpa2tnd298r7B0dJYrQBol4pNoB1pQzSRuGU7bsaJYBJy2gtHtxG89UqVZJB/MOKa+wAPJQkawsVK7G4j0Keu5vXLFrbpToGXi5aQCOeq98k+3H5FEUGkIx\n1p3PDc2foqVYTrNRNI0xGeEB7VgqsaDaT6f3ZujEKn0URsqWNGiq/p1IsdB6LALbKbAZ6kVvIv7ndRITXvspk3FiqCSzRWHCkYnQ5HnUZ4oSw8eWYKYvRWRIVaYGBvR3JZAZDYTbzGBZdI8q3qX1Yv780rtJk+nCEdwDKfgwRXU4A7q0ACHF7gFd6cZ+fd+XA+Z60FJ585hDk4X7/bd5aU</latexit>x0\n<latexit sha1_base64=\"N02aUbPDsVbROKfSFTfAZ+j1zOk=\">AB/nicbVDLSgNBEOz1GeMr6tHLYBA8hV3xdQx68RghL0iWMDuZTYbMzC4zs2JYFvwGr3r2Jl79FY/+iZNkDyaxoKGo6qa7\nK4g508Z1v52V1bX1jc3CVnF7Z3dv3Rw2NRoghtkIhHqh1gTmTtGY4bQdK4pFwGkrGN1N/NYjVZpFsm7GMfUFHkgWMoKNldrdQKRPWa/eK5XdijsFWiZeTsqQo9Yr/XT7EUkElYZwrHXHc2Pjp1gZRjNit1E0xiTER7QjqUSC6r9dHpvhk6t0kdhpGxJg6bq34kUC63HIrCdApuhXvQm4n9eJzHhjZ8yGSeGSjJbFCYcmQhNnkd9pigxfGwJorZWxEZYoWJsRHNbQlEZjPxFhNYJs3zindVuX\ny4KFdv83QKcAwncAYeXEMV7qEGDSDA4QVe4c15dt6dD+dz1ri5DNHMAfn6xcUMpa4</latexit>xT\n<latexit sha1_base64=\"N1FoC6dR3lAK9RKdY0Wkc2lBMH4=\">AB/nicbVDLSgNBEOz1GeMr6tHLYBA8hd3g6xj04jGCeUCyhNnJbDJkZnaZmRXDsuA3eNWzN/Hqr3j0T5wkezCJBQ1FVTfd\nXUHMmTau+2srK6tb2wWtorbO7t7+6WDw6aOEkVog0Q8Uu0Aa8qZpA3DKftWFEsAk5bweh24rceqdIskg9mHFNf4IFkISPYWKndDUT6lPWqvVLZrbhToGXi5aQMOeq90k+3H5FEUGkIx1p3PDc2foqVYTrNhNI0xGeEB7VgqsaDaT6f3ZujUKn0URsqWNGiq/p1IsdB6LALbKbAZ6kVvIv7ndRITXvspk3FiqCSzRWHCkYnQ5HnUZ4oSw8eWYKYvRWRIVaYGBvR3JZAZDYTbzGBZdKsVrzLys\nX9ebl2k6dTgGM4gTPw4ApqcAd1aABDi/wCm/Os/PufDifs9YVJ585gjk4X7/enZaW</latexit>x2\n<latexit sha1_base64=\"8tPVoKSZYCmzIB4Wh5NsXh7F9Zo=\">AB/nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PQi8cI5gHJEmYns8mQmdlZlYMy4Lf4FXP3sSrv+LRP3GS7MEkFjQUV\nd10dwUxZ9q47rdTWFldW98obpa2tnd298r7B0dJYrQBol4pNoB1pQzSRuGU7bsaJYBJy2gtHtxG89UqVZJB/MOKa+wAPJQkawsVK7G4j0Ket5vXLFrbpToGXi5aQCOeq98k+3H5FEUGkIx1p3PDc2foqVYTrNRNI0xGeEB7VgqsaDaT6f3ZujEKn0URsqWNGiq/p1IsdB6LALbKbAZ6kVvIv7ndRITXvspk3FiqCSzRWHCkYnQ5HnUZ4oSw8eWYKYvRWRIVaYGBvR3JZAZDYTbzGBZd\nI8q3qX1Yv780rtJk+nCEdwDKfgwRXU4A7q0ACHF7gFd6cZ+fd+XA+Z60FJ585hDk4X7/dCpaV</latexit>x1\n<latexit sha1_base64=\"ROsC+1STl6wTckLfk8UwPJKESA=\">AB/nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PQi8cI5gHJEmYns8mQmdlZlYMy4Lf4FXP3sSrv+LRP3GS7MEkFjQUVd10dwUxZ9q47rdTWFldW98obpa2tnd298r7B0dJYrQBol4pNoB1pQzSRuGU7bsaJYBJy2gtHtxG89UqVZJB/MOKa+wAPJQkawsVK7G4j0Keu5vXLFrbpToGXi5aQCOeq98k+3H5FEUGkIx\n1p3PDc2foqVYTrNRNI0xGeEB7VgqsaDaT6f3ZujEKn0URsqWNGiq/p1IsdB6LALbKbAZ6kVvIv7ndRITXvspk3FiqCSzRWHCkYnQ5HnUZ4oSw8eWYKYvRWRIVaYGBvR3JZAZDYTbzGBZdI8q3qX1Yv780rtJk+nCEdwDKfgwRXU4A7q0ACHF7gFd6cZ+fd+XA+Z60FJ585hDk4X7/bd5aU</latexit>x0\n<latexit sha1_base64=\"9Znxn6VDIRyBDLkDowM/rKGyYE=\">ACnicbVC7TsNAEDzDOFlQkljESFRTbiVUbQUAaJPKTYis6XTXLKnW3drR\nGRlT/gG2ihpkO0/AQlf8IlcUESRlpdmZXu5owEVyj635bK6tr6xubha3i9s7u3r59UGroOFUM6iwWsWqFVIPgEdSRo4BWoDKUEAzHN5O/OYjKM3j6AFHCQS9iPe4yikTp2yQ+lD4nmwnQ+DgBpxy67FXcKZ5l4OSmTHLWO/eN3Y5ZKiJAJqnXbcxMqQMwHjop9qSCgb0j60DY2oB1k09/HzolR\nuk4vVqYidKbq342MSq1HMjSTkuJAL3oT8T+vnWLvOsh4lKQIEZsd6qXCwdiZBOF0uQKGYmQIZYqbXx02oIoyNHNXQnl2GTiLSawTBpnFe+ycnF/Xq7e5OkUyBE5JqfEI1ekSu5IjdQJI0/khbySN+vZerc+rM/Z6IqV7xySOVhfv4VEm1E=</latexit>\u270f\u2713\n<latexit sha1_base64=\"9Znxn6VDIRyBDLkDowM/rKGyYE=\">ACnicbVC7TsNAEDzDOFlQkljESFRTbiVUbQUAaJPKTYis6XTXLKnW3drR\nGRlT/gG2ihpkO0/AQlf8IlcUESRlpdmZXu5owEVyj635bK6tr6xubha3i9s7u3r59UGroOFUM6iwWsWqFVIPgEdSRo4BWoDKUEAzHN5O/OYjKM3j6AFHCQS9iPe4yikTp2yQ+lD4nmwnQ+DgBpxy67FXcKZ5l4OSmTHLWO/eN3Y5ZKiJAJqnXbcxMqQMwHjop9qSCgb0j60DY2oB1k09/HzolR\nuk4vVqYidKbq342MSq1HMjSTkuJAL3oT8T+vnWLvOsh4lKQIEZsd6qXCwdiZBOF0uQKGYmQIZYqbXx02oIoyNHNXQnl2GTiLSawTBpnFe+ycnF/Xq7e5OkUyBE5JqfEI1ekSu5IjdQJI0/khbySN+vZerc+rM/Z6IqV7xySOVhfv4VEm1E=</latexit>\u270f\u2713\n<latexit sha1_base64=\"9Znxn6VDIRyBDLkDowM/rKGyYE=\">ACnicbVC7TsNAEDzDOFlQkljESFRTbiVUbQUAaJPKTYis6XTXLKnW3drR\nGRlT/gG2ihpkO0/AQlf8IlcUESRlpdmZXu5owEVyj635bK6tr6xubha3i9s7u3r59UGroOFUM6iwWsWqFVIPgEdSRo4BWoDKUEAzHN5O/OYjKM3j6AFHCQS9iPe4yikTp2yQ+lD4nmwnQ+DgBpxy67FXcKZ5l4OSmTHLWO/eN3Y5ZKiJAJqnXbcxMqQMwHjop9qSCgb0j60DY2oB1k09/HzolR\nuk4vVqYidKbq342MSq1HMjSTkuJAL3oT8T+vnWLvOsh4lKQIEZsd6qXCwdiZBOF0uQKGYmQIZYqbXx02oIoyNHNXQnl2GTiLSawTBpnFe+ycnF/Xq7e5OkUyBE5JqfEI1ekSu5IjdQJI0/khbySN+vZerc+rM/Z6IqV7xySOVhfv4VEm1E=</latexit>\u270f\u2713\n<latexit sha1_base64=\"5Czkd7R7CmnEYhIs4hSFH8z18=\">AB/HicbVDLTgJBEOzF+IL9ehlIjHxRHaNryPRi0dM5JHAhvQOszAyO7uZmT\nUhBL/Bq569Ga/+i0f/xAH2IGAlnVSqutPdFSCa+O6305uZXVtfSO/Wdja3tndK+4f1HWcKspqNBaxagaomeCS1Qw3gjUTxTAKBGsEg9uJ3hiSvNYPphwvwIe5KHnKxUr0tMRDYKZbcsjsFWSZeRkqQodop/rS7MU0jJg0VqHXLcxPj1AZTgUbF9qpZgnSAfZYy1KJEdP+aHrtmJxYpUvCWNmShkzV\nvxMjLQeRoHtjND09aI3Ef/zWqkJr/0Rl0lqmKSzRWEqiInJ5HXS5YpRI4aWIFXc3kpoHxVSYwOa2xJEY5uJt5jAMqmflb3L8sX9ealyk6WThyM4hlPw4AoqcAdVqAGFR3iBV3hznp1358P5nLXmnGzmEObgfP0CRDWVsA=</latexit>r\n<latexit sha1_base64=\"9Znxn6VDIRyBDLkDowM/rKGyYE=\">ACnicbVC7TsNAEDzDOFlQkljESFRTbiVUbQUAaJPKTYis6XTXLKnW3drR\nGRlT/gG2ihpkO0/AQlf8IlcUESRlpdmZXu5owEVyj635bK6tr6xubha3i9s7u3r59UGroOFUM6iwWsWqFVIPgEdSRo4BWoDKUEAzHN5O/OYjKM3j6AFHCQS9iPe4yikTp2yQ+lD4nmwnQ+DgBpxy67FXcKZ5l4OSmTHLWO/eN3Y5ZKiJAJqnXbcxMqQMwHjop9qSCgb0j60DY2oB1k09/HzolR\nuk4vVqYidKbq342MSq1HMjSTkuJAL3oT8T+vnWLvOsh4lKQIEZsd6qXCwdiZBOF0uQKGYmQIZYqbXx02oIoyNHNXQnl2GTiLSawTBpnFe+ycnF/Xq7e5OkUyBE5JqfEI1ekSu5IjdQJI0/khbySN+vZerc+rM/Z6IqV7xySOVhfv4VEm1E=</latexit>\u270f\u2713\n<latexit sha1_base64=\"5Czkd7R7CmnEYhIs4hSFH8z18=\">AB/HicbVDLTgJBEOzF+IL9ehlIjHxRHaNryPRi0dM5JHAhvQOszAyO7uZmT\nUhBL/Bq569Ga/+i0f/xAH2IGAlnVSqutPdFSCa+O6305uZXVtfSO/Wdja3tndK+4f1HWcKspqNBaxagaomeCS1Qw3gjUTxTAKBGsEg9uJ3hiSvNYPphwvwIe5KHnKxUr0tMRDYKZbcsjsFWSZeRkqQodop/rS7MU0jJg0VqHXLcxPj1AZTgUbF9qpZgnSAfZYy1KJEdP+aHrtmJxYpUvCWNmShkzV\nvxMjLQeRoHtjND09aI3Ef/zWqkJr/0Rl0lqmKSzRWEqiInJ5HXS5YpRI4aWIFXc3kpoHxVSYwOa2xJEY5uJt5jAMqmflb3L8sX9ealyk6WThyM4hlPw4AoqcAdVqAGFR3iBV3hznp1358P5nLXmnGzmEObgfP0CRDWVsA=</latexit>r\n<latexit sha1_base64=\"9Znxn6VDIRyBDLkDowM/rKGyYE=\">ACnicbVC7TsNAEDzDOFlQkljESFRTbiVUbQUAaJPKTYis6XTXLKnW3drR\nGRlT/gG2ihpkO0/AQlf8IlcUESRlpdmZXu5owEVyj635bK6tr6xubha3i9s7u3r59UGroOFUM6iwWsWqFVIPgEdSRo4BWoDKUEAzHN5O/OYjKM3j6AFHCQS9iPe4yikTp2yQ+lD4nmwnQ+DgBpxy67FXcKZ5l4OSmTHLWO/eN3Y5ZKiJAJqnXbcxMqQMwHjop9qSCgb0j60DY2oB1k09/HzolR\nuk4vVqYidKbq342MSq1HMjSTkuJAL3oT8T+vnWLvOsh4lKQIEZsd6qXCwdiZBOF0uQKGYmQIZYqbXx02oIoyNHNXQnl2GTiLSawTBpnFe+ycnF/Xq7e5OkUyBE5JqfEI1ekSu5IjdQJI0/khbySN+vZerc+rM/Z6IqV7xySOVhfv4VEm1E=</latexit>\u270f\u2713\n<latexit sha1_base64=\"8tPVoKSZYCmzIB4Wh5NsXh7F9Zo=\">AB/nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PQi8cI5gHJEmYns8mQmdlZlYMy4Lf4FXP3sSrv+LRP3GS7MEkFjQUV\nd10dwUxZ9q47rdTWFldW98obpa2tnd298r7B0dJYrQBol4pNoB1pQzSRuGU7bsaJYBJy2gtHtxG89UqVZJB/MOKa+wAPJQkawsVK7G4j0Ket5vXLFrbpToGXi5aQCOeq98k+3H5FEUGkIx1p3PDc2foqVYTrNRNI0xGeEB7VgqsaDaT6f3ZujEKn0URsqWNGiq/p1IsdB6LALbKbAZ6kVvIv7ndRITXvspk3FiqCSzRWHCkYnQ5HnUZ4oSw8eWYKYvRWRIVaYGBvR3JZAZDYTbzGBZd\nI8q3qX1Yv780rtJk+nCEdwDKfgwRXU4A7q0ACHF7gFd6cZ+fd+XA+Z60FJ585hDk4X7/dCpaV</latexit>x1\n<latexit sha1_base64=\"N1FoC6dR3lAK9RKdY0Wkc2lBMH4=\">AB/nicbVDLSgNBEOz1GeMr6tHLYBA8hd3g6xj04jGCeUCyhNnJbDJkZnaZmRXDsuA3eNWzN/Hqr3j0T5wkezCJBQ1FVTfd\nXUHMmTau+2srK6tb2wWtorbO7t7+6WDw6aOEkVog0Q8Uu0Aa8qZpA3DKftWFEsAk5bweh24rceqdIskg9mHFNf4IFkISPYWKndDUT6lPWqvVLZrbhToGXi5aQMOeq90k+3H5FEUGkIx1p3PDc2foqVYTrNhNI0xGeEB7VgqsaDaT6f3ZujUKn0URsqWNGiq/p1IsdB6LALbKbAZ6kVvIv7ndRITXvspk3FiqCSzRWHCkYnQ5HnUZ4oSw8eWYKYvRWRIVaYGBvR3JZAZDYTbzGBZdKsVrzLys\nX9ebl2k6dTgGM4gTPw4ApqcAd1aABDi/wCm/Os/PufDifs9YVJ585gjk4X7/enZaW</latexit>x2\n<latexit sha1_base64=\"N1FoC6dR3lAK9RKdY0Wkc2lBMH4=\">AB/nicbVDLSgNBEOz1GeMr6tHLYBA8hd3g6xj04jGCeUCyhNnJbDJkZnaZmRXDsuA3eNWzN/Hqr3j0T5wkezCJBQ1FVTfd\nXUHMmTau+2srK6tb2wWtorbO7t7+6WDw6aOEkVog0Q8Uu0Aa8qZpA3DKftWFEsAk5bweh24rceqdIskg9mHFNf4IFkISPYWKndDUT6lPWqvVLZrbhToGXi5aQMOeq90k+3H5FEUGkIx1p3PDc2foqVYTrNhNI0xGeEB7VgqsaDaT6f3ZujUKn0URsqWNGiq/p1IsdB6LALbKbAZ6kVvIv7ndRITXvspk3FiqCSzRWHCkYnQ5HnUZ4oSw8eWYKYvRWRIVaYGBvR3JZAZDYTbzGBZdKsVrzLys\nX9ebl2k6dTgGM4gTPw4ApqcAd1aABDi/wCm/Os/PufDifs9YVJ585gjk4X7/enZaW</latexit>x2\n<latexit sha1_base64=\"N02aUbPDsVbROKfSFTfAZ+j1zOk=\">AB/nicbVDLSgNBEOz1GeMr6tHLYBA8hV3xdQx68RghL0iWMDuZTYbMzC4zs2JYFvwGr3r2Jl79FY/+iZNkDyaxoKGo6qa7\nK4g508Z1v52V1bX1jc3CVnF7Z3dv3Rw2NRoghtkIhHqh1gTmTtGY4bQdK4pFwGkrGN1N/NYjVZpFsm7GMfUFHkgWMoKNldrdQKRPWa/eK5XdijsFWiZeTsqQo9Yr/XT7EUkElYZwrHXHc2Pjp1gZRjNit1E0xiTER7QjqUSC6r9dHpvhk6t0kdhpGxJg6bq34kUC63HIrCdApuhXvQm4n9eJzHhjZ8yGSeGSjJbFCYcmQhNnkd9pigxfGwJorZWxEZYoWJsRHNbQlEZjPxFhNYJs3zindVuX\ny4KFdv83QKcAwncAYeXEMV7qEGDSDA4QVe4c15dt6dD+dz1ri5DNHMAfn6xcUMpa4</latexit>xT\n<latexit sha1_base64=\"hEn2BRjdbNmEp6y6RvUNCOlufSM=\">AB6nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PUi8eIeUGyhNnJBkyO7vM9A\nphySd48aCIV7/Im3/jJNmDJhY0FXdHcFsRQGXfbya2srq1v5DcLW9s7u3vF/YOGiRLNeJ1FMtKtgBouheJ1FCh5K9achoHkzWB0N/WbT1wbEakajmPuh3SgRF8wilZ6vOnWusWSW3ZnIMvEy0gJMlS7xa9OL2JyBUySY1pe26Mfko1Cib5pNBJDI8pG9EBb1uqaMiNn85OnZATq/RIP9K2FJKZ+nsi\npaEx4zCwnSHFoVn0puJ/XjvB/rWfChUnyBWbL+onkmBEpn+TntCcoRxbQpkW9lbChlRThjadg3BW3x5mTOyt5l+eLhvFS5zeLIwxEcwyl4cAUVuIcq1IHBAJ7hFd4c6bw4787HvDXnZDOH8AfO5w/wI42W</latexit>AT\n<latexit sha1_base64=\"iMdZqhDp38bGN3XTktBm/qsP9Do=\">AB6nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2OIF48R84JkCbOT3mTI7OwyMy\nuEkE/w4kERr36RN/GSbIHTSxoKq6e4KEsG1cd1vJ7e2vrG5ld8u7Ozu7R8UD4+aOk4VwaLRazaAdUouMSG4UZgO1FIo0BgKxjdzfzWEyrNY1k34wT9iA4kDzmjxkqP1V69Vy5ZXcOskq8jJQgQ61X/Or2Y5ZGKA0TVOuO5ybGn1BlOBM4LXRTjQlIzrAjqWSRqj9yfzUKTmzSp+EsbIlDZmrvycm\nNJ6HAW2M6JmqJe9mfif10lNeOtPuExSg5ItFoWpICYms79JnytkRowtoUxeythQ6oMzadg3BW35lTQvyt51+erhslSpZnHk4QRO4Rw8uIEK3EMNGsBgAM/wCm+OcF6cd+dj0Zpzsplj+APn8wfxqY2X</latexit>BT\n<latexit sha1_base64=\"mIAYoI9nO5A7G5LG1UVGCTqBXU=\">ACAXicbVC7SgNBFL0bXzG+opY2i0GwCrviqwzaWFhEMA/YLGF2MkmGzGOZmRXCkspvsNXaTmz9E\nkv/xNlkC5N4YOBwzr3cMyeKGdXG876dwsrq2vpGcbO0tb2zu1feP2hqmShMGlgyqdoR0oRQRqGkbasSKIR4y0otFt5reiNJUikczjknI0UDQPsXIWCnocGSGLH0ftItV7yqN4W7TPycVCBHvVv+6fQkTjgRBjOkdeB7sQlTpAzFjExKnUSTGOERGpDAUoE40WE6jTxT6zSc/tS2SeMO1X/bqSIaz3mkZ3MIupFLxP/84LE9K/DlIo4MUTg2aF\n+wlwj3ez/bo8qg0bW4Kwojari4dIWxsS3NXIp514i82sEyaZ1X/snrxcF6p3eTtFOEIjuEUfLiCGtxBHRqAQcILvMKb8+y8Ox/O52y04OQ7hzAH5+sXZgmYAg=</latexit>L\n<latexit sha1_base64=\"5Czkd7R7CmnEYhIs4hSFH8z18=\">AB/HicbVDLTgJBEOzF+IL9ehlIjHxRHaNryPRi0dM5JHAhvQOszAyO7uZmT\nUhBL/Bq569Ga/+i0f/xAH2IGAlnVSqutPdFSCa+O6305uZXVtfSO/Wdja3tndK+4f1HWcKspqNBaxagaomeCS1Qw3gjUTxTAKBGsEg9uJ3hiSvNYPphwvwIe5KHnKxUr0tMRDYKZbcsjsFWSZeRkqQodop/rS7MU0jJg0VqHXLcxPj1AZTgUbF9qpZgnSAfZYy1KJEdP+aHrtmJxYpUvCWNmShkzV\nvxMjLQeRoHtjND09aI3Ef/zWqkJr/0Rl0lqmKSzRWEqiInJ5HXS5YpRI4aWIFXc3kpoHxVSYwOa2xJEY5uJt5jAMqmflb3L8sX9ealyk6WThyM4hlPw4AoqcAdVqAGFR3iBV3hznp1358P5nLXmnGzmEObgfP0CRDWVsA=</latexit>r\n<latexit sha1_base64=\"5Czkd7R7CmnEYhIs4hSFH8z18=\">AB/HicbVDLTgJBEOzF+IL9ehlIjHxRHaNryPRi0dM5JHAhvQOszAyO7uZmT\nUhBL/Bq569Ga/+i0f/xAH2IGAlnVSqutPdFSCa+O6305uZXVtfSO/Wdja3tndK+4f1HWcKspqNBaxagaomeCS1Qw3gjUTxTAKBGsEg9uJ3hiSvNYPphwvwIe5KHnKxUr0tMRDYKZbcsjsFWSZeRkqQodop/rS7MU0jJg0VqHXLcxPj1AZTgUbF9qpZgnSAfZYy1KJEdP+aHrtmJxYpUvCWNmShkzV\nvxMjLQeRoHtjND09aI3Ef/zWqkJr/0Rl0lqmKSzRWEqiInJ5HXS5YpRI4aWIFXc3kpoHxVSYwOa2xJEY5uJt5jAMqmflb3L8sX9ealyk6WThyM4hlPw4AoqcAdVqAGFR3iBV3hznp1358P5nLXmnGzmEObgfP0CRDWVsA=</latexit>r\n<latexit sha1_base64=\"5Czkd7R7CmnEYhIs4hSFH8z18=\">AB/HicbVDLTgJBEOzF+IL9ehlIjHxRHaNryPRi0dM5JHAhvQOszAyO7uZmT\nUhBL/Bq569Ga/+i0f/xAH2IGAlnVSqutPdFSCa+O6305uZXVtfSO/Wdja3tndK+4f1HWcKspqNBaxagaomeCS1Qw3gjUTxTAKBGsEg9uJ3hiSvNYPphwvwIe5KHnKxUr0tMRDYKZbcsjsFWSZeRkqQodop/rS7MU0jJg0VqHXLcxPj1AZTgUbF9qpZgnSAfZYy1KJEdP+aHrtmJxYpUvCWNmShkzV\nvxMjLQeRoHtjND09aI3Ef/zWqkJr/0Rl0lqmKSzRWEqiInJ5HXS5YpRI4aWIFXc3kpoHxVSYwOa2xJEY5uJt5jAMqmflb3L8sX9ealyk6WThyM4hlPw4AoqcAdVqAGFR3iBV3hznp1358P5nLXmnGzmEObgfP0CRDWVsA=</latexit>r\n<latexit sha1_base64=\"mIAYoI9nO5A7G5LG1UVGCTqBXU=\">ACAXicbVC7SgNBFL0bXzG+opY2i0GwCrviqwzaWFhEMA/YLGF2MkmGzGOZmRXCkspvsNXaTmz9E\nkv/xNlkC5N4YOBwzr3cMyeKGdXG876dwsrq2vpGcbO0tb2zu1feP2hqmShMGlgyqdoR0oRQRqGkbasSKIR4y0otFt5reiNJUikczjknI0UDQPsXIWCnocGSGLH0ftItV7yqN4W7TPycVCBHvVv+6fQkTjgRBjOkdeB7sQlTpAzFjExKnUSTGOERGpDAUoE40WE6jTxT6zSc/tS2SeMO1X/bqSIaz3mkZ3MIupFLxP/84LE9K/DlIo4MUTg2aF\n+wlwj3ez/bo8qg0bW4Kwojari4dIWxsS3NXIp514i82sEyaZ1X/snrxcF6p3eTtFOEIjuEUfLiCGtxBHRqAQcILvMKb8+y8Ox/O52y04OQ7hzAH5+sXZgmYAg=</latexit>L\n<latexit sha1_base64=\"p/zZUtXUnmaWgdOFWVx/x7XIJK4=\">AB+XicbVA9TwJBEJ3DL8Qv1NJmIzGxInfGr5JgY4nRAxK4kL1lDzbs7l1290\nzIhZ9gq7WdsfXWPpPXOAKAV8yct7M5mZFyacaeO6305hbX1jc6u4XdrZ3ds/KB8eNXWcKkJ9EvNYtUOsKWeS+oYZTtuJoliEnLbC0d3Ubz1TpVksn8w4oYHA8kiRrCx0mO95/bKFbfqzoBWiZeTCuRo9Mo/3X5MUkGlIRxr3fHcxAQZVoYRTielbqpgskID2jHUokF1UE2O3WCzqzSR1GsbEmDZurf\niQwLrcitJ0Cm6Fe9qbif14nNdFtkDGZpIZKMl8UpRyZGE3/Rn2mKDF8bAkmitlbERlihYmx6SxsCcXEZuItJ7BKmhdV7p69XBZqdXzdIpwAqdwDh7cQA3uoQE+EBjAC7zCm5M5786H8zlvLTj5zDEswPn6BWV+lAk=</latexit>B0\n<latexit sha1_base64=\"N02aUbPDsVbROKfSFTfAZ+j1zOk=\">AB/nicbVDLSgNBEOz1GeMr6tHLYBA8hV3xdQx68RghL0iWMDuZTYbMzC4zs2JYFvwGr3r2Jl79FY/+iZNkDyaxoKGo6qa7\nK4g508Z1v52V1bX1jc3CVnF7Z3dv3Rw2NRoghtkIhHqh1gTmTtGY4bQdK4pFwGkrGN1N/NYjVZpFsm7GMfUFHkgWMoKNldrdQKRPWa/eK5XdijsFWiZeTsqQo9Yr/XT7EUkElYZwrHXHc2Pjp1gZRjNit1E0xiTER7QjqUSC6r9dHpvhk6t0kdhpGxJg6bq34kUC63HIrCdApuhXvQm4n9eJzHhjZ8yGSeGSjJbFCYcmQhNnkd9pigxfGwJorZWxEZYoWJsRHNbQlEZjPxFhNYJs3zindVuX\ny4KFdv83QKcAwncAYeXEMV7qEGDSDA4QVe4c15dt6dD+dz1ri5DNHMAfn6xcUMpa4</latexit>xT\n<latexit sha1_base64=\"N1FoC6dR3lAK9RKdY0Wkc2lBMH4=\">AB/nicbVDLSgNBEOz1GeMr6tHLYBA8hd3g6xj04jGCeUCyhNnJbDJkZnaZmRXDsuA3eNWzN/Hqr3j0T5wkezCJBQ1FVTfd\nXUHMmTau+2srK6tb2wWtorbO7t7+6WDw6aOEkVog0Q8Uu0Aa8qZpA3DKftWFEsAk5bweh24rceqdIskg9mHFNf4IFkISPYWKndDUT6lPWqvVLZrbhToGXi5aQMOeq90k+3H5FEUGkIx1p3PDc2foqVYTrNhNI0xGeEB7VgqsaDaT6f3ZujUKn0URsqWNGiq/p1IsdB6LALbKbAZ6kVvIv7ndRITXvspk3FiqCSzRWHCkYnQ5HnUZ4oSw8eWYKYvRWRIVaYGBvR3JZAZDYTbzGBZdKsVrzLys\nX9ebl2k6dTgGM4gTPw4ApqcAd1aABDi/wCm/Os/PufDifs9YVJ585gjk4X7/enZaW</latexit>x2\n<latexit sha1_base64=\"8tPVoKSZYCmzIB4Wh5NsXh7F9Zo=\">AB/nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PQi8cI5gHJEmYns8mQmdlZlYMy4Lf4FXP3sSrv+LRP3GS7MEkFjQUV\nd10dwUxZ9q47rdTWFldW98obpa2tnd298r7B0dJYrQBol4pNoB1pQzSRuGU7bsaJYBJy2gtHtxG89UqVZJB/MOKa+wAPJQkawsVK7G4j0Ket5vXLFrbpToGXi5aQCOeq98k+3H5FEUGkIx1p3PDc2foqVYTrNRNI0xGeEB7VgqsaDaT6f3ZujEKn0URsqWNGiq/p1IsdB6LALbKbAZ6kVvIv7ndRITXvspk3FiqCSzRWHCkYnQ5HnUZ4oSw8eWYKYvRWRIVaYGBvR3JZAZDYTbzGBZd\nI8q3qX1Yv780rtJk+nCEdwDKfgwRXU4A7q0ACHF7gFd6cZ+fd+XA+Z60FJ585hDk4X7/dCpaV</latexit>x1\n<latexit sha1_base64=\"ROsC+1STl6wTckLfk8UwPJKESA=\">AB/nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PQi8cI5gHJEmYns8mQmdlZlYMy4Lf4FXP3sSrv+LRP3GS7MEkFjQUV\nd10dwUxZ9q47rdTWFldW98obpa2tnd298r7B0dJYrQBol4pNoB1pQzSRuGU7bsaJYBJy2gtHtxG89UqVZJB/MOKa+wAPJQkawsVK7G4j0Keu5vXLFrbpToGXi5aQCOeq98k+3H5FEUGkIx1p3PDc2foqVYTrNRNI0xGeEB7VgqsaDaT6f3ZujEKn0URsqWNGiq/p1IsdB6LALbKbAZ6kVvIv7ndRITXvspk3FiqCSzRWHCkYnQ5HnUZ4oSw8eWYKYvRWRIVaYGBvR3JZAZDYTbzGBZd\nI8q3qX1Yv780rtJk+nCEdwDKfgwRXU4A7q0ACHF7gFd6cZ+fd+XA+Z60FJ585hDk4X7/bd5aU</latexit>x0\n<latexit sha1_base64=\"ROsC+1STl6wTckLfk8UwPJKESA=\">AB/nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PQi8cI5gHJEmYns8mQmdlZlYMy4Lf4FXP3sSrv+LRP3GS7MEkFjQUV\nd10dwUxZ9q47rdTWFldW98obpa2tnd298r7B0dJYrQBol4pNoB1pQzSRuGU7bsaJYBJy2gtHtxG89UqVZJB/MOKa+wAPJQkawsVK7G4j0Keu5vXLFrbpToGXi5aQCOeq98k+3H5FEUGkIx1p3PDc2foqVYTrNRNI0xGeEB7VgqsaDaT6f3ZujEKn0URsqWNGiq/p1IsdB6LALbKbAZ6kVvIv7ndRITXvspk3FiqCSzRWHCkYnQ5HnUZ4oSw8eWYKYvRWRIVaYGBvR3JZAZDYTbzGBZd\nI8q3qX1Yv780rtJk+nCEdwDKfgwRXU4A7q0ACHF7gFd6cZ+fd+XA+Z60FJ585hDk4X7/bd5aU</latexit>x0\n<latexit sha1_base64=\"8tPVoKSZYCmzIB4Wh5NsXh7F9Zo=\">AB/nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PQi8cI5gHJEmYns8mQmdlZlYMy4Lf4FXP3sSrv+LRP3GS7MEkFjQUV\nd10dwUxZ9q47rdTWFldW98obpa2tnd298r7B0dJYrQBol4pNoB1pQzSRuGU7bsaJYBJy2gtHtxG89UqVZJB/MOKa+wAPJQkawsVK7G4j0Ket5vXLFrbpToGXi5aQCOeq98k+3H5FEUGkIx1p3PDc2foqVYTrNRNI0xGeEB7VgqsaDaT6f3ZujEKn0URsqWNGiq/p1IsdB6LALbKbAZ6kVvIv7ndRITXvspk3FiqCSzRWHCkYnQ5HnUZ4oSw8eWYKYvRWRIVaYGBvR3JZAZDYTbzGBZd\nI8q3qX1Yv780rtJk+nCEdwDKfgwRXU4A7q0ACHF7gFd6cZ+fd+XA+Z60FJ585hDk4X7/dCpaV</latexit>x1\n<latexit sha1_base64=\"N1FoC6dR3lAK9RKdY0Wkc2lBMH4=\">AB/nicbVDLSgNBEOz1GeMr6tHLYBA8hd3g6xj04jGCeUCyhNnJbDJkZnaZmRXDsuA3eNWzN/Hqr3j0T5wkezCJBQ1FVTfd\nXUHMmTau+2srK6tb2wWtorbO7t7+6WDw6aOEkVog0Q8Uu0Aa8qZpA3DKftWFEsAk5bweh24rceqdIskg9mHFNf4IFkISPYWKndDUT6lPWqvVLZrbhToGXi5aQMOeq90k+3H5FEUGkIx1p3PDc2foqVYTrNhNI0xGeEB7VgqsaDaT6f3ZujUKn0URsqWNGiq/p1IsdB6LALbKbAZ6kVvIv7ndRITXvspk3FiqCSzRWHCkYnQ5HnUZ4oSw8eWYKYvRWRIVaYGBvR3JZAZDYTbzGBZdKsVrzLys\nX9ebl2k6dTgGM4gTPw4ApqcAd1aABDi/wCm/Os/PufDifs9YVJ585gjk4X7/enZaW</latexit>x2\n<latexit sha1_base64=\"9Znxn6VDIRyBDLkDowM/rKGyYE=\">ACnicbVC7TsNAEDzDOFlQkljESFRTbiVUbQUAaJPKTYis6XTXLKnW3drR\nGRlT/gG2ihpkO0/AQlf8IlcUESRlpdmZXu5owEVyj635bK6tr6xubha3i9s7u3r59UGroOFUM6iwWsWqFVIPgEdSRo4BWoDKUEAzHN5O/OYjKM3j6AFHCQS9iPe4yikTp2yQ+lD4nmwnQ+DgBpxy67FXcKZ5l4OSmTHLWO/eN3Y5ZKiJAJqnXbcxMqQMwHjop9qSCgb0j60DY2oB1k09/HzolR\nuk4vVqYidKbq342MSq1HMjSTkuJAL3oT8T+vnWLvOsh4lKQIEZsd6qXCwdiZBOF0uQKGYmQIZYqbXx02oIoyNHNXQnl2GTiLSawTBpnFe+ycnF/Xq7e5OkUyBE5JqfEI1ekSu5IjdQJI0/khbySN+vZerc+rM/Z6IqV7xySOVhfv4VEm1E=</latexit>\u270f\u2713\n<latexit sha1_base64=\"9Znxn6VDIRyBDLkDowM/rKGyYE=\">ACnicbVC7TsNAEDzDOFlQkljESFRTbiVUbQUAaJPKTYis6XTXLKnW3drR\nGRlT/gG2ihpkO0/AQlf8IlcUESRlpdmZXu5owEVyj635bK6tr6xubha3i9s7u3r59UGroOFUM6iwWsWqFVIPgEdSRo4BWoDKUEAzHN5O/OYjKM3j6AFHCQS9iPe4yikTp2yQ+lD4nmwnQ+DgBpxy67FXcKZ5l4OSmTHLWO/eN3Y5ZKiJAJqnXbcxMqQMwHjop9qSCgb0j60DY2oB1k09/HzolR\nuk4vVqYidKbq342MSq1HMjSTkuJAL3oT8T+vnWLvOsh4lKQIEZsd6qXCwdiZBOF0uQKGYmQIZYqbXx02oIoyNHNXQnl2GTiLSawTBpnFe+ycnF/Xq7e5OkUyBE5JqfEI1ekSu5IjdQJI0/khbySN+vZerc+rM/Z6IqV7xySOVhfv4VEm1E=</latexit>\u270f\u2713\n<latexit sha1_base64=\"9Znxn6VDIRyBDLkDowM/rKGyYE=\">ACnicbVC7TsNAEDzDOFlQkljESFRTbiVUbQUAaJPKTYis6XTXLKnW3drR\nGRlT/gG2ihpkO0/AQlf8IlcUESRlpdmZXu5owEVyj635bK6tr6xubha3i9s7u3r59UGroOFUM6iwWsWqFVIPgEdSRo4BWoDKUEAzHN5O/OYjKM3j6AFHCQS9iPe4yikTp2yQ+lD4nmwnQ+DgBpxy67FXcKZ5l4OSmTHLWO/eN3Y5ZKiJAJqnXbcxMqQMwHjop9qSCgb0j60DY2oB1k09/HzolR\nuk4vVqYidKbq342MSq1HMjSTkuJAL3oT8T+vnWLvOsh4lKQIEZsd6qXCwdiZBOF0uQKGYmQIZYqbXx02oIoyNHNXQnl2GTiLSawTBpnFe+ycnF/Xq7e5OkUyBE5JqfEI1ekSu5IjdQJI0/khbySN+vZerc+rM/Z6IqV7xySOVhfv4VEm1E=</latexit>\u270f\u2713\nStandard Backprop through Sampling\nFigure 2. Different memory setups for backpropagation through\nsampling. Normally, all intermediate activations are stored in mem-\nory, which is intractable for modern diffusion models. In DITTO,\ngradient checkpointing allows us to achieve efficient memory usage\nwith only 2x the number of model calls to preserve fast runtime.\nthe correlated updates (see Section 6.3). We show memory\ndiagrams for DOODL, as well as an alternative memory pro-\ncedure for DOODL that is slightly more memory efficient\nbut is 6x as slow as standard backprop in Appendix B.\n3.4. Complete Algorithm\nPsuedo-code for our DITTO algorithm is shown in Algo-\nrithm 1. We define Checkpoint to be a gradient check-\npointing function that 1) inputs and stores a callable differ-\nentiable network (i.e., the sampler) and any input arguments\nto the network, 2) overrides the default activation caching\nbehavior of the network to turn off activation caching dur-\ning the forward pass of backpropagation and 3) recomputes\nactivations when needed in the backward pass. Note that in\npractice, we typically use a small subsequence of sampling\nsteps (e.g. 20) spanning from xT to x0.\n4. Applications\nWe apply our flexible framework to a range of applications1\nincluding outpainting, inpainting, looping, intensity con-\ntrol, melody control, and musical structure control, where\nboth musical structure and looping have been unexplored\nfor TTM diffusion models. These constitute both reference-\n1We leave the rhythm control from Wu et al. (2023a) for future\nwork, as their RNN beat detector would trigger an exceedingly\nlong backpropagation graph when using DITTO.\n4\nDITTO: Diffusion Inference-Time T -Optimization for Music Generation\nTime\nIntensity (dB)\nTarget Intensity Curve\nTime\nIntensity (dB)\nGenerated Intensity Curve\nTime\nC\nD\nE\nF\nG\nA\nB\nPitch class\nTarget Melody\nTime\nC\nD\nE\nF\nG\nA\nB\nPitch class\nGenerated Melody\nTime\nTime\nTarget Musical Structure\nTime\nTime\nGenerated Musical Structure\nFigure 3. Examples of DITTO\u2019s use for creative control, including intensity (left), melody (middle), and structure (right), with target\ncontrols and final features displayed below each spectrogram.\nbased (i.e. using existing audio) and reference-free (genera-\ntion from scratch) control operations as shown Fig. 3. Our\ngoal here is to display the expressive controllability that ini-\ntial noise latents have over the diffusion process.\nOutpainting \u2013 Outpainting is the task of extending the\nlength of real or previously generated content and is criti-\ncal for image and audio editing as well as generating long-\nduration music content using diffusion models. Past out-\npainting methods include MultiDiffusion (Bar-Tal et al.,\n2023) and Guidance Gradients (Levy et al., 2023) which\nstruggle to maintain long-form coherence and local smooth-\ning. We perform outpainting by 1) taking an existing refer-\nence audio signal xref; 2) defining an overlap region o in sec-\nonds at the end of the reference; 3) using DITTO to create\nnew content that matches the overlap region but then extends\nit; and 4) stitching the reference and newly generated con-\ntent together. More formally, we define Mref and Mgen as bi-\nnary masks that specify the location of the overlap region in\nthe reference and generated content respectively, f(x0) :=\nMgen \u2299 x0, y = Mref \u2299 xref, and L \u221d ||f(x0) \u2212 y||2\n2.\nInpainting \u2013 Inpainting is the task of replacing an interior\nregion of real or previously generated content and is essen-\ntial for audio editing and music remixing. Past work on in-\npainting has been explored in the image- and audio-domain\nto variable success (Chung et al., 2023; Levy et al., 2023).\nWe use DITTO to perform inpainting similar to outpainting,\nwith the only modification being Mref = Mgen denote two\noverlap regions (on each side of the spectrogram) to use as\ncontext for inpainting the gap in between.\nLooping \u2013 Looping is the task of generating content that\nrepeats in a circular pattern, creating repeatable music frag-\nments to form the basis of a larger composition. For looping,\nwe use DITTO similar to outpainting, but when we define\nMref and Mgen, we specify two overlapping edge regions of\nthe output (similar to inpainting) but corresponding to oppo-\nsite sides of the outputs (similar to outpainting), such that\nthe extended region seamlessly transitions back to the be-\nginning of the reference clip. To our knowledge, we are the\nfirst to imbue TTM diffusion models with looping control.\nIntensity Control \u2013 Musical intensity control is the task of\nadjusting the dynamic contrast of generated music across\ntime. We follow the intensity control protocol from Mu-\nsic ControlNet (see Wu et al. (2023a) for more details),\nwhich employs a training-time method to generate music\nthat follows a smoothed, decibel (dB) volume curve. In\nour case, we use DITTO in a similar fashion, albeit with-\nout the need for large-scale fine-tuning, by setting f(x0) :=\nw\u221720 log10(RMS(V(x0))), where w are the smoothing co-\nefficients used in Music ControlNet, \u2217 is a convolution op-\nerator, RMS is the Root Mean Squared energy of the audio,\ny is a given dB-scale target curve, L \u221d ||f(x0) \u2212 y||2\n2, and\nV is our vocoder (Lee et al., 2022) that translates spectro-\ngrams to the audio domain. Note here, we backpropagate\nthrough our vocoder as well.\nMelody Control \u2013 Musical melody control is the task\nof controlling prominent musical tones over time and al-\nlows creators to generate accompaniment music to exist-\ning melodies. Following recent work (Copet et al., 2023;\nWu et al., 2023a), the approx.\nmelody of a recording\ncan be extracted by computing the smoothed energy level\nof the 12-pitch classes over time via a highpass chroma-\ngram function C(\u00b7) (M\u00a8uller, 2015). Given this, we use\nDITTO with f(x0) = log(C(V(x0))), a target melody\ny \u2208 {1, . . . , 12}N\u00d71, the spectrogram length N, and L =\nNLLLoss(f(x0), y) or the negative log likelihood loss.\nSee Wu et al. (2023a) for further implementation details.\nMusical Structure Control \u2013 We define musical structure\ncontrol as the task of controlling the high-level musical\nform of generated music over time. To model musical form,\nwe follow musical structure analysis work (McFee & El-\nlis, 2014) that, in the simplest case, measures structure via\ncomputing a self-similarity (SS) matrix of local timbre fea-\ntures where timbre is \u201ceverything about a sound which is\nneither loudness nor pitch\u201d (Erickson, 1975). Thus, we use\nDITTO for musical structure control by setting y to be a\nknown, target SS matrix, f(x0) = T(x0)T(x0)\u22a4, T(\u00b7) to\n5\nDITTO: Diffusion Inference-Time T -Optimization for Music Generation\nbe a timbre extraction function, and L \u221d ||f(x0) \u2212 y||2\n2.\nSpecifically, we use the Mel-Frequency Cepstrum Coeffi-\ncients (MFCCs) (McFee et al., 2010), omitting the first co-\nefficient and normalized across the time axis, as the timbre\nextraction function, and then smooth the SS matrix via a 2D\nSavitzky-Golay filter in order to not penalize slight varia-\ntions in intra-phrase similarity. Such target SS matrices can\ntake the form of an \u201cABBA\u201d pattern (as shown in Fig. 3) for\ninstance. To our knowledge, we are the first to imbue TTM\ndiffusion models with structure control.\nOther Applications \u2013 Besides the applications described\nabove, DITTO can be used for numerous new extensions\npreviously unexplored in TTM generation which we de-\nscribe in the Appendix, such as correlation-based intensity\ncontrol (C), real-audio inversion (D), reference-free looping\n(E), musical structure transfer (F), other sampling methods\n(G), multi-feature optimization (H), and reusing optimized\nlatents for fast inference (I).\n5. Experimental Design\n5.1. DITTO Setup\nWe use Adam (Kingma & Ba, 2014) as our optimizer for\nDITTO, with a learning rate of 5 \u00d7 10\u22123 (as higher leads\nto stability issues). We use DDIM (Song et al., 2020) sam-\npling with 20 steps and dynamic thresholding (Saharia et al.,\n2022b) for all experiments. No optimizer hyperparameters\nwere changed across application besides the max number of\noptimization steps, which were doubled from 70 to 150 for\nthe melody and structure tasks.\n5.2. Datasets\nWe train our models on a dataset of \u22481800 hours of licensed\ninstrumental music with genre, mood, and tempo tags. Our\ndataset does not have free-form text description, so we\nuse class-conditional text control of global musical style,\nas done in JukeBox (Dhariwal et al., 2020). For melody\ncontrol references, we synthesize recordings from a 380-\nsample public-domain subset of the Wikifonia Lead-Sheet\nDataset (Simonetta et al., 2018). Like in Wu et al. (2023a),\nwe construct a small set of handcrafted intensity curves and\nmusical structure matrices (e.g. a smooth crescendo and\n\u201cABA\u201d form) for intensity and structure control (see Ap-\npendix H for more examples). For evaluation only, we also\nuse the MusicCaps Dataset (Agostinelli et al., 2023) with\naround 5K 10-second clips with text descriptions.\n5.3. Evaluation Metrics\nWe use Frechet Audio Distance (FAD) with the VGGish\n(Hershey et al., 2017) backbone, which measures the dis-\ntance between the distribution of embeddings from a set of\nbaseline recordings and that from generated recordings (Kil-\ngour et al., 2018). FAD metrics are calculated using Music-\nCaps as the reference distribution against 2.5K model gen-\nerations for all experiments. For reference-free targets, we\nalso use the CLAP score (Wu et al., 2023b), which mea-\nsures the overall alignment between the text caption and the\noutput audio; note that as our model is only tag-conditioned,\nwe convert each tag set into a caption using the template \u201cA\n[genre] [mood] song at [BPM] beats per minute\u201d. Addition-\nally, for the intensity and musical structure control, we re-\nport the average loss L across the generated outputs (i.e. the\nfinal feature matching distance), and report overall accuracy\nfor melody control, since it is framed as a classification task.\n5.4. Baselines\nWe benchmark against a wide-range of methods including:\n\u2022 Na\u00a8\u0131ve Masking: Here, after a DDIM-step we apply\nthe update xt\u22121 = Mref \u2299 N(\u221a\u00af\u03b1txref, (1 \u2212 \u00af\u03b1t)I) +\nMgen \u2299 xt\u22121 (i.e. setting the overlap region directly\nto the reference image at the appropriate noise level).\n\u2022 MultiDiffusion (Bar-Tal et al., 2023): This case is sim-\nilar to the na\u00a8\u0131ve approach, but instead averages the\nnoisy outputs in the overlapping region instead of us-\ning a hard mask. We can additionally stop this averag-\ning operation at certain points of the sampling process\n(such as half way through) and let the model sample\nwithout guiding the process; we denote the former ap-\nproach as MD and the latter as MD-50 for brevity.\n\u2022 FreeDoM (Yu et al., 2023): FreeDoM is a guidance-\nbased method, where we perform an additional update\nduring sampling xt = xt \u2212 \u03b7t\u2207xtL(f(\u02c6x0(xt)), y),\nwhere \u02c6x0(xt) denotes the first term in Eq. 12. \u03b7t is a\ntime-dependent learning rate that is a function of the\noverall gradient norm.\n\u2022 Guidance Gradients (GG) (Levy et al., 2023): GG takes\nthe update equation from FreeDoM and makes two\nsmall modifications. Namely, \u03b7t is fixed throughout\nsampling, and GG includes an additional data consis-\ntency step when the feature extractor f(\u00b7) is fully linear.\n\u2022 Music ControlNet (Wu et al., 2023a): Music Control-\nNet is a training-based approach that shares the same\nunderlying base model as our work but additionally\nfinetunes adaptor modules during large scale training\nto the control signal y as conditioning.\n\u2022 DOODL (Wallace et al., 2023a): DOODL2 is an\noptimization-based approach that uses the EDICT\n(Wallace et al., 2023b) sampler and multiple ad-hoc\n2https://github.com/salesforce/DOODL\n6\nDITTO: Diffusion Inference-Time T -Optimization for Music Generation\nRefence Audio:\nBaseline:\nDITTO:\nmaintains reference semantics with smooth transition\nsemantic mismatch \nbetween reference and \ngeneration with hard \n\u201cseam\u201d on transition\nFigure 4. Failure cases of baseline outpainting methods. Baseline\nmethods tend to create audible \u201cseams\u201d in the audio between\noverlap and non-overlap regions of the generated output, leading\nto unnatural jumps in semantic content. DITTO avoids this issue\nand provides seamless outpainting throughout the full generation.\nchanges to the optimization process such as injecting\nnoise and renormalizing xT . We use the same learning\nrate as DITTO due to similar stability issues.\nWe compare with Na\u00a8\u0131ve Masking, MultiDiffusion, and Guid-\nance Gradients for inpainting, outpainting, and looping ex-\nperiments since they all have linear feature matching objec-\ntive, Music ControlNet for the melody and intensity experi-\nments, and FreeDoM and DOODL for all experiments.\n6. Results\n6.1. Outpainting, Inpainting, and Looping Results\nWe show objective evaluation results for outpainting and\nlooping in Table 1 and inpainting results in Table 2. Here\nwe report FAD, as low loss over the overlap regions does\nnot necessitate that the overall audio is cohesive. We find\nDITTO achieves the lowest FAD against all baselines across\noverlap sizes of 1 to 3 seconds and inpainting gaps of 2 to\n4 seconds. DOODL performs next behind DITTO, and the\ninference-time guidance methods particularly struggle.\nQualitatively, we discover that all baselines (besides\nDOODL) tend to produce audible \u201cseams\u201d in the output mu-\nsic outside the overlap region as shown in Fig. 4, wherein\nthe final outputs tend to purely match the overlap region (i.e.\nover optimizing for the feature matching target) and ignore\nthe overall consistency between the overlap generation and\nthe rest of the generation. By optimizing xT for reconstruc-\nTable 1. Outpainting and looping results for DITTO against base-\nline pixel, guidance, and optimization-based methods.\nFAD (\u2193)\no = 1\no = 2\no = 3\nLooping\nDOODL\n9.4525\n9.0397\n9.0210\n8.9239\nNa\u00a8\u0131ve\n9.5281\n9.4074\n9.4053\n9.4954\nMD\n9.8201\n9.4784\n9.3108\n9.5815\nMD-50\n9.4387\n9.2521\n9.1156\n9.4053\nGG\n10.3076\n9.8281\n9.3881\n9.5524\nFreeDoM\n9.7073\n9.7708\n9.6027\n9.4535\nDITTO (ours)\n9.1859\n8.9178\n8.6897\n8.8431\nTable 2. Inpainting results for DITTO against baseline pixel, guid-\nance, and optimization-based methods.\nFAD (\u2193)\ngap = 2\ngap = 3\ngap = 4\nDOODL\n8.8018\n9.0050\n9.5049\nNa\u00a8\u0131ve\n9.4871\n9.5524\n9.6067\nMD\n9.3279\n9.7251\n10.0740\nMD-50\n9.0677\n9.4161\n9.8342\nGG\n9.7049\n10.1605\n10.9959\nFreeDoM\n9.4770\n9.5516\n10.3422\nDITTO (ours)\n8.3470\n8.3229\n8.5733\ntion over the overlap regions, DITTO effectively avoids such\nissues, as this process implicitly encourages the non-overlap\ngeneration sections to preserve semantic content seamlessly.\n6.2. Intensity, Melody, and Structure Results\nIn Table 3, we show objective metrics for intensity, melody,\nand structure control. We seek to understand 1) how dif-\nferent methods impose the target control on the generative\nmodel via MSE or Accuracy 2) overall audio quality via\nFAD and 3) how such control effects the baseline text condi-\ntioning via CLAP. We find DITTO achieves SOTA intensity\nand melody control, beating that of Music ControlNet with\nzero supervised training. We further explore Music Control-\nNet\u2019s poor intensity control more in-depth in Appendix C.\nAdditionally, we note FreeDoM slightly beats DITTO in\nstructure control, but exhibits poor performance for inten-\nsity and especially melody control, showing the limits of\nguidance-based methods for complicated feature extractors.\nA notable concern with optimization-based control is the\nchance of reward hacking (Skalse et al., 2022; Prabhudesai\net al., 2023), where the control target is over-optimized\nleading to degradation in model quality and base behavior.\nWe find that DOODL exhibits this reward hacking behavior\nconsistently in addition to generally being worse at control\nthan DITTO, sacrificing overall quality and significant text\nrelevance in favor of matching the control target. DITTO, on\nthe other hand, is able to balance the target control without\nover-optimizing and maintain quality and text relevance.\nIn Fig. 3, we show qualitative intensity, melody, and struc-\n7\nDITTO: Diffusion Inference-Time T -Optimization for Music Generation\nTable 3. Intensity, melody, and structure control results. DITTO achieves SOTA intensity and melody control. Music ControlNet struggles\non intensity control MSE. FreeDoM performs well on structure but struggles on more complex melody and intensity control.\nControl\nIntensity\nMelody\nStructure\nMetric\nMSE (\u2193)\nFAD (\u2193)\nCLAP (\u2191)\nAcc (\u2191)\nFAD (\u2193)\nCLAP (\u2191)\nMSE (\u2193)\nFAD (\u2193)\nCLAP (\u2191)\nDefault TTM\n40.8430\n8.6958\n0.3732\n0.10527\n8.6958\n0.3732\n0.3091\n8.6958\n0.3732\nControlNet\n38.4108\n11.1315\n0.3084\n0.8135\n7.8574\n0.4784\n\u2013\n\u2013\n\u2013\nFreeDoM\n23.2920\n9.5056\n0.4823\n0.3154\n9.2858\n0.4766\n0.0177\n6.6774\n0.4152\nDOODL\n4.7847\n12.3362\n0.3418\n0.8159\n9.5114\n0.3361\n0.0742\n9.3642\n0.3856\nDITTO (ours)\n4.7576\n10.5294\n0.4326\n0.8262\n8.2431\n0.4321\n0.0237\n7.0904\n0.4181\nture control results. On the left, we show a generated spectro-\ngram with a rising then falling intensity curve. In the middle,\nwe show a generated spectrogram with an input target and\ngenerated melody visualization (chromagram). On the right,\nwe show a generated spectrogram with target and generated\nself-similarity matrices with an ABBA structure pattern.\n6.3. Efficiency Comparison\nBesides comparing DITTO with DOODL in terms of their\ngeneration quality and control, we seek to understand how\nthey differ in terms of both practical efficiency and conver-\ngence speed, as slow per-iteration runtime could be offset\nby fast convergence, and how such behaviors change as the\nnumber of sampling steps increases. We focus on intensity\ncontrol since it represents a middle ground between the sim-\nple linear painting methods and the more complex melody\ncontrol. Besides MSE, FAD, and CLAP, we also report the\nmean steps to convergence (MS2C), i.e. the average number\nof optimization steps needed to reach an MSE below some\nthreshold \u03c4, the mean optimization speed (MOS), i.e. the\naverage number of seconds per optimization step, and the\nmean allocated memory (MAM), measuring the average\nGPU memory (in GB) used during optimization by the dif-\nfusion model. We run the test on a single 40GB A100 with\nK = 70 maximum optimization steps and \u03c4 = 2 dB. For\nDOODL, we use a mixing coefficient of p = 0.93 at 50\nsteps following Wallace et al. (2023a) and p = 0.83 at 20\nsteps due to severe divergence with higher p at 20 steps.\nIn Table 4, we empirically confirm that DOODL is \u2248 2x\nslower than DITTO and takes up \u2248 2x more GPU mem-\nory, as DOODL uses the EDICT sampler which doubles the\nnumber of model calls during both the forward and check-\npointed backwards pass and stores both chains of inputs in\nmemory. Most saliently, we discover that DOODL displays\npractically identical convergence speed to DITTO, show-\ning that DOODL\u2019s added complexity provides no benefit in\nspeeding up optimization. We note that increasing the num-\nber of sampling steps tends to degrade control adherence,\nlikely since the longer sampling chain makes backpropaga-\ntion more difficult. Interestingly, as sampling time increases\nthe overall FAD improves significantly for DOODL, giving\nevidence that EDICT particularly struggles with few sam-\nTable 4. Performance between DITTO and DOODL on intensity\ncontrol. DITTO and DOODL reach convergence in a similar\nnumber of steps yet DOODL is \u22482x less efficient than DITTO.\nMethod\nDITTO\nDOODL\nDITTO\nDOODL\nSampling Steps\n20\n20\n50\n50\nMSE (\u2193)\n4.7576\n4.7847\n7.6404\n8.8939\nFAD (\u2193)\n10.5294\n12.3362\n10.3652\n9.9090\nCLAP (\u2191)\n0.4326\n0.3418\n0.3977\n0.3114\nMS2C (\u2193)\n44.4661\n49.2031\n46.8550\n47.8341\nMOS(\u2193)\n1.8594\n4.1773\n4.4720\n10.0362\nMAM (\u2193)\n5.0020\n8.2740\n5.0941\n8.3112\npling steps, and thus DOODL cannot be sped up by using\nfewer steps without noticeable reward hacking.\n6.4. The Expressive Power of the Diffusion Latent Space\nTypically, the initial latent xT is ignored in diffusion models,\nas the diffusion latent space has previously been thought\nto encode little semantic meaning compared to GAN latent\nspaces (Song et al., 2020; Preechakul et al., 2022). DITTO\u2019s\nstrong performance, however, presents the surprising fact\nthat a wide-array of semantically meaningful fine-grained\nfeatures can be manipulated purely through exploring the\ndiffusion latent space without ever editing the pre-trained\ndiffusion base model. We explore this idea further, and how\nour findings are theoretically tied to the encoding of low-\nfrequency structure noted by Si et al. (2023) in Appendix J.\n7. Conclusion\nWe\npropose\nDITTO:\nDiffusion\nInference-Time\nT -\nOptimization, a unified training-free framework for control-\nling pre-trained diffusion models to enable a wide-range\nof creative editing and control tasks for music generation.\nDITTO achieves SOTA editing ability and matches the con-\ntrollability of fully training-based methods, outperforms the\nleading optimization-based approach while being 2x as time\nand memory efficient, and imposes no restrictions on the\nmodeling architecture or sampling process. In future work,\nwe hope to accelerate the optimization procedure to achieve\nreal-time interaction and more expressive control.\n8\nDITTO: Diffusion Inference-Time T -Optimization for Music Generation\nBroader Impacts\nWhile generative multimedia models may open up new av-\nenues for artistic creation, there is the concern of negatively\nimpacting current working musicians and creators and their\nown livelihoods. We find that it is exceedingly important\nto build TTM systems that protect artists and their data. To\nmitigate harm, we train on licensed music and place our fo-\ncus on improving controllability, allowing working artists\nto interface with TTM systems through more musically-\naligned controls, instead of only relying on high-level tex-\ntual prompts that may be too general for music profession-\nals.\nAcknowledgements\nThank you to Ge Zhu, Juan-Pablo Caceres, Zhiyao Duan,\nand Nicholas J. Bryan for sharing their high-fidelity vocoder\nused for our demo examples (citation coming soon).\nReferences\nAgostinelli, A., Denk, T. I., Borsos, Z., Engel, J., Verzetti,\nM., Caillon, A., Huang, Q., Jansen, A., Roberts, A.,\nTagliasacchi, M., et al. MusicLM: Generating music from\ntext. arXiv:2301.11325, 2023.\nBar-Tal, O., Yariv, L., Lipman, Y., and Dekel, T. MultiDif-\nfusion: Fusing diffusion paths for controlled image gener-\nation. In International Conference on Machine Learning\n(ICML), 2023.\nBlack, K., Janner, M., Du, Y., Kostrikov, I., and Levine, S.\nTraining diffusion models with reinforcement learning.\narXiv preprint arXiv:2305.13301, 2023.\nBorsos, Z., Marinier, R., Vincent, D., Kharitonov, E.,\nPietquin, O., Sharifi, M., Roblek, D., Teboul, O., et al.\nAudioLM: a language modeling approach to audio gen-\neration. IEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing (TASLP), 2023a.\nBorsos, Z., Sharifi, M., Vincent, D., Kharitonov, E., Zeghi-\ndour, N., and Tagliasacchi, M. Soundstorm: Efficient par-\nallel audio generation. ArXiv, abs/2305.09636, 2023b.\nChen, K., Wang, C.-i., Berg-Kirkpatrick, T., and Dubnov, S.\nMusic SketchNet: Controllable music generation via fac-\ntorized representations of pitch and rhythm. In Interna-\ntional Society for Music Information Retrieval (ISMIR),\n2020.\nChen, K., Wu, Y., Liu, H., Nezhurina, M., Berg-Kirkpatrick,\nT., and Dubnov, S. MusicLDM: Enhancing novelty in\ntext-to-music generation using beat-synchronous mixup\nstrategies. arXiv:2308.01546, 2023.\nChen, T. On the importance of noise scheduling for diffusion\nmodels. Technical report, Google Research, 2023.\nChen, T., Xu, B., Zhang, C., and Guestrin, C. Training\ndeep nets with sublinear memory cost. arXiv preprint\narXiv:1604.06174, 2016.\nChoi, J., Choi, Y., Kim, Y., Kim, J., and Yoon, S. Custom-\nEdit: Text-guided image editing with customized diffu-\nsion models. IEEE/CVF Conference on Computer Vision\nand Pattern Recognition - AI4CC Workshop, 2023.\nChung, H., Kim, J., McCann, M. T., Klasky, M. L., and Ye,\nJ. C. Diffusion posterior sampling for general noisy in-\nverse problems. In International Conference on Learning\nRepresentations (ICLR), 2023.\nClark, K., Vicol, P., Swersky, K., and Fleet, D. J. Directly\nfine-tuning diffusion models on differentiable rewards.\nArXiv, abs/2309.17400, 2023.\nCopet, J., Kreuk, F., Gat, I., Remez, T., Kant, D., Synnaeve,\nG., Adi, Y., and D\u00b4efossez, A. Simple and controllable\nmusic generation. In Neural Information Processing Sys-\ntems (NeurIPS), 2023.\nDai, S., Jin, Z., Gomes, C., and Dannenberg, R. Control-\nlable deep melody generation via hierarchical music struc-\nture representation. In International Society for Music\nInformation Retrieval (ISMIR), 2021.\nDefferrard, M., Benzi, K., Vandergheynst, P., and Bresson,\nX. FMA: A dataset for music analysis. In International\nSociety for Music Information Retrieval (ISMIR), 2017.\nURL https://arxiv.org/abs/1612.01840.\nDhariwal, P. and Nichol, A. Diffusion models beat GANs on\nimage synthesis. Neural Information Processing Systems\n(NeurIPS), 34, 2021.\nDhariwal, P., Jun, H., Payne, C., Kim, J. W., Radford, A.,\nand Sutskever, I. Jukebox: A generative model for music.\narXiv:2005.00341, 2020.\nDinh, L., Krueger, D., and Bengio, Y. NICE: Non-linear\nindependent components estimation. International Con-\nference on Learning Representations (ICLR) Workshop,\n2014.\nDinh, L., Sohl-Dickstein, J., and Bengio, S. Density estima-\ntion using real NVP. International Conference on Learn-\ning Representations (ICLR), 2016.\nDonahue, C., Caillon, A., Roberts, A., Manilow, E., Es-\nling, P., Agostinelli, A., Verzetti, M., et al. SingSong:\nGenerating musical accompaniments from singing.\narXiv:2301.12662, 2023.\n9\nDITTO: Diffusion Inference-Time T -Optimization for Music Generation\nDong, H.-W., Hsiao, W.-Y., Yang, L.-C., and Yang, Y.-H.\nMuseGAN: Multi-track sequential generative adversar-\nial networks for symbolic music generation and accom-\npaniment. In AAAI Conference on Artificial Intelligence,\nnumber 1, 2018.\nD\u00b4efossez, A., Copet, J., Synnaeve, G., and Adi, Y.\nHigh fidelity neural audio compression. arXiv preprint\narXiv:2210.13438, 2022.\nErickson, R. Sound structure in music. Univ of California\nPress, 1975.\nForsgren, S. and Martiros, H. Riffusion: Stable diffusion\nfor real-time music generation, 2022. URL https://\nriffusion.com/about.\nGal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano,\nA. H., Chechik, G., and Cohen-Or, D. An image is worth\none word: Personalizing text-to-image generation using\ntextual inversion, 2022.\nGarcia, H. F., Seetharaman, P., Kumar, R., and Pardo, B.\nVampNet: Music generation via masked acoustic token\nmodeling. In International Society for Music Information\nRetrieval (ISMIR), 2023.\nGupta, A., Yu, L., Sohn, K., Gu, X., Hahn, M., Li, F.-F.,\nEssa, I., Jiang, L., and Lezama, J. Photorealistic video\ngeneration with diffusion models. 2023.\nHawthorne, C., Simon, I., Roberts, A., Zeghidour, N., Gard-\nner, J., Manilow, E., and Engel, J. Multi-instrument mu-\nsic synthesis with spectrogram diffusion. In International\nSociety for Music Information Retrieval (ISMIR), 2022.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual\nlearning for image recognition. In IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2016.\nHershey, S., Chaudhuri, S., Ellis, D. P., Gemmeke, J. F.,\nJansen, A., Moore, R. C., Plakal, M., Platt, D., Saurous,\nR. A., Seybold, B., et al. CNN architectures for large-\nscale audio classification. In IEEE International Confer-\nence on Audio, Speech and Signal Processing (ICASSP),\n2017.\nHertz, A., Mokady, R., Tenenbaum, J., Aberman, K.,\nPritch, Y., and Cohen-Or, D.\nPrompt-to-prompt im-\nage editing with cross attention control. arXiv preprint\narXiv:2208.01626, 2022.\nHo, J. and Salimans, T. Classifier-free diffusion guidance.\nIn NeurIPS Workshop on Deep Gen. Models and Down-\nstream Applications, 2021.\nHo, J., Jain, A., and Abbeel, P. Denoising diffusion proba-\nbilistic models. Neural Information Processing Systems\n(NeurIPS), 33, 2020.\nHo, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi,\nM.,\nand Fleet,\nD. J.\nVideo diffusion models.\narXiv:2204.03458, 2022.\nHuang, Q., Park, D. S., Wang, T., Denk, T. I., Ly, A.,\nChen, N., Zhang, Z., Zhang, Z., Yu, J., Frank, C., et al.\nNoise2Music: Text-conditioned music generation with\ndiffusion models. arXiv:2302.03917, 2023a.\nHuang, R., Huang, J., Yang, D., Ren, Y., Liu, L., Li, M., Ye,\nZ., Liu, J., Yin, X., and Zhao, Z. Make-an-audio: Text-to-\naudio generation with prompt-enhanced diffusion models.\narXiv preprint arXiv:2301.12661, 2023b.\nKarras, T., Aittala, M., Aila, T., and Laine, S. Elucidating\nthe design space of diffusion-based generative models. In\nNeurIPS, 2022.\nKarunratanakul, K., Preechakul, K., Aksan, E., Beeler, T.,\nSuwajanakorn, S., and Tang, S. Optimizing diffusion\nnoise can serve as universal motion priors. arXiv preprint\narXiv:2312.11994, 2023.\nKawar, B., Zada, S., Lang, O., Tov, O., Chang, H., Dekel, T.,\nMosseri, I., and Irani, M. Imagic: Text-based real image\nediting with diffusion models. In IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR),\n2023.\nKilgour, K., Zuluaga, M., Roblek, D., and Sharifi, M.\nFrechet audio distance: A metric for evaluating music en-\nhancement algorithms. arXiv:1812.08466, 2018.\nKingma, D. P. and Ba, J. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980, 2014.\nKingma, D. P. and Welling, M. Auto-encoding variational\nbayes. In International Conference on Learning Repre-\nsentations (ICLR), 2013.\nKumar, R., Seetharaman, P., Luebs, A., Kumar, I., and Ku-\nmar, K. High-fidelity audio compression with improved\nRVQGAN. In Neural Information Processing Systems\n(NeurIPS), 2023.\nLee, S.-g., Ping, W., Ginsburg, B., Catanzaro, B., and Yoon,\nS. Bigvgan: A universal neural vocoder with large-scale\ntraining. arXiv preprint arXiv:2206.04658, 2022.\nLeemput, S. C. v., Teuwen, J., Ginneken, B. v., and Man-\nniesing, R. Memcnn: A python/pytorch package for cre-\nating memory-efficient invertible neural networks. Jour-\nnal of Open Source Software, 2019. ISSN 2475-9066.\ndoi: 10.21105/joss.01576.\nLevy, M., Giorgi, B. D., Weers, F., Katharopoulos, A., and\nNickson, T. Controllable music production with diffusion\nmodels and guidance gradients. ArXiv, abs/2311.00613,\n2023.\n10\nDITTO: Diffusion Inference-Time T -Optimization for Music Generation\nLiu, H., Chen, Z., Yuan, Y., Mei, X., Liu, X., Mandic, D.,\nWang, W., and Plumbley, M. D. AudioLDM: Text-to-\naudio generation with latent diffusion models. In Interna-\ntional Conference on Machine Learning (ICML), 2023a.\nLiu, H., Tian, Q., Yuan, Y., Liu, X., Mei, X., Kong, Q.,\nWang, Y., Wang, W., Wang, Y., and Plumbley, M. D. Au-\ndioLDM 2: Learning holistic audio generation with self-\nsupervised pretraining. arXiv preprint arXiv:2308.05734,\n2023b.\nLu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J. Dpm-\nsolver++: Fast solver for guided sampling of diffusion\nprobabilistic models. ArXiv, abs/2211.01095, 2022.\nMathews, M. V., Miller, J. E., Moore, F. R., Pierce, J. R., and\nRisset, J.-C. The technology of computer music. 1969.\nMcFee, B. and Ellis, D. Analyzing song structure with\nspectral clustering. In International Society for Music\nInformation Retrieval (ISMIR). Citeseer, 2014.\nMcFee, B., Barrington, L., and Lanckriet, G. R. Learning\nsimilarity from collaborative filters. In International So-\nciety for Music Information Retrieval (ISMIR), 2010.\nMokady, R., Hertz, A., Aberman, K., Pritch, Y., and Cohen-\nOr, D. Null-text inversion for editing real images using\nguided diffusion models. In IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 2023.\nM\u00a8uller, M. Fundamentals of music processing: Audio, anal-\nysis, algorithms, applications. Springer, 2015.\nPan, Z., Gherardi, R., Xie, X., and Huang, S. Effective\nreal image editing with accelerated iterative diffusion\ninversion. In IEEE/CVF International Conference on\nComputer Vision (CVPR), 2023.\nPoole, B., Jain, A., Barron, J. T., and Mildenhall, B. Dream-\nfusion: Text-to-3d using 2d diffusion. arXiv, 2022.\nPrabhudesai, M., Goyal, A., Pathak, D., and Fragkiadaki,\nK. Aligning text-to-image diffusion models with reward\nbackpropagation. ArXiv, abs/2310.03739, 2023.\nPreechakul, K., Chatthee, N., Wizadwongsa, S., and Suwa-\njanakorn, S. Diffusion autoencoders: Toward a mean-\ningful and decodable representation. In IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition\n(CVPR), 2022.\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and\nOmmer, B. High-resolution image synthesis with latent\ndiffusion models.\nIn IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2022.\nRonneberger, O., Fischer, P., and Brox, T. U-Net: Convolu-\ntional networks for biomedical image segmentation. In\nMedical Image Computing and Computer Assisted Inter-\nventions (MICCAI), 2015.\nRuiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M.,\nand Aberman, K.\nDreambooth: Fine tuning text-to-\nimage diffusion models for subject-driven generation. In\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2023.\nSaharia, C., Chan, W., Chang, H., Lee, C., Ho, J., Salimans,\nT., Fleet, D., and Norouzi, M. Palette: Image-to-image\ndiffusion models. In ACM SIGGRAPH Conference Pro-\nceedings, 2022a.\nSaharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton,\nE. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan,\nB., Salimans, T., et al. Photorealistic text-to-image diffu-\nsion models with deep language understanding. Neural\nInformation Processing Systems (NeurIPS), 35, 2022b.\nSchneider, F., Jin, Z., and Sch\u00a8olkopf, B. Mo\\\u02c6 usai: Text-\nto-music generation with long-context latent diffusion.\narXiv preprint arXiv:2301.11757, 2023.\nSi, C., Huang, Z., Jiang, Y., and Liu, Z. Freeu: Free lunch\nin diffusion u-net. ArXiv, abs/2309.11497, 2023.\nSimonetta, F., Carnovalini, F., Orio, N., and Rod`a, A. Sym-\nbolic music similarity through a graph-based representa-\ntion. In Audio Mostly 2018 on Sound in Immersion and\nEmotion. 2018.\nSkalse, J., Howe, N., Krasheninnikov, D., and Krueger,\nD. Defining and characterizing reward gaming. Neural\nInformation Processing Systems (NeuraIPS), 35, 2022.\nSohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and\nGanguli, S. Deep unsupervised learning using nonequi-\nlibrium thermodynamics. In International Conference on\nMachine Learning (ICML), 2015.\nSong, J., Meng, C., and Ermon, S. Denoising diffusion\nimplicit models. In International Conference on Learning\nRepresentations (ICLR), 2020.\nStevens, S. S., Volkmann, J., and Newman, E. B. A scale for\nthe measurement of the psychological magnitude pitch.\nJournal of the Acoustical Society of America (JASA),\n1937.\nWallace, B., Gokul, A., Ermon, S., and Naik, N. V. End-to-\nend diffusion latent optimization improves classifier guid-\nance. IEEE/CVF International Conference on Computer\nVision (ICCV), abs/2303.13703, 2023a.\n11\nDITTO: Diffusion Inference-Time T -Optimization for Music Generation\nWallace, B., Gokul, A., and Naik, N. EDICT: Exact diffu-\nsion inversion via coupled transformations. In IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2023b.\nWatson, D., Chan, W., Martin-Brualla, R., Ho, J., Tagliasac-\nchi, A., and Norouzi, M. Novel view synthesis with dif-\nfusion models. ArXiv, abs/2210.04628, 2022.\nWu, S.-L., Donahue, C., Watanabe, S., and Bryan, N. J. Mu-\nsic controlnet: Multiple time-varying controls for music\ngeneration. ArXiv, abs/2311.07069, 2023a.\nWu, Y., Chen, K., Zhang, T., Hui, Y., Berg-Kirkpatrick, T.,\nand Dubnov, S. Large-scale contrastive language-audio\npretraining with feature fusion and keyword-to-caption\naugmentation. In IEEE International Conference on Au-\ndio, Speech and Signal Processing (ICASSP), 2023b.\nXia, W., Zhang, Y., Yang, Y., Xue, J.-H., Zhou, B., and\nYang, M.-H. Gan inversion: A survey. IEEE Transactions\non Pattern Analysis and Machine Intelligence, 45, 2021.\nYu, J., Wang, Y., Zhao, C., Ghanem, B., and Zhang, J. Free-\ndom: Training-free energy-guided conditional diffusion\nmodel. IEEE/CVF International Conference on Com-\nputer Vision (ICCV), 2023.\nZeghidour, N., Luebs, A., Omran, A., Skoglund, J., and\nTagliasacchi, M. Soundstream: An end-to-end neural\naudio codec. IEEE/ACM Transactions on Audio, Speech,\nand Language Processing (TASLP), 30, 2021.\nZhang, L., Rao, A., and Agrawala, M. Adding conditional\ncontrol to text-to-image diffusion models. In IEEE/CVF\nInternational Conference on Computer Vision (ICCV),\n2023.\nZhao, S., Chen, D., Chen, Y.-C., Bao, J., Hao, S., Yuan, L.,\nand Wong, K.-Y. K. Uni-ControlNet: All-in-one control\nto text-to-image diffusion models. arXiv:2305.16322,\n2023.\n12\nDITTO: Diffusion Inference-Time T -Optimization for Music Generation\nA. Diffusion Review\nDenoising diffusion probabilistic models (DDPMs) (Sohl-Dickstein et al., 2015; Ho et al., 2020) or diffusion models are a\nclass of generative latent variable model. They are defined by a forward and reverse random Markov process. Intuitively, the\nforward process takes clean data and iteratively corrupts it with noise to train a (denoising) neural network and the reverse\nprocess takes random noise and iteratively refines it with the learned network to generate new data.\nThe forward process is defined as a Markov chain:\nq(x0, ..., xT ) := q(x0)\nT\nY\nt=1\nq(xt|xt\u22121)\n(4)\nq(xt|xt\u22121) := N(\np\n1 \u2212 \u03b2txt\u22121, \u03b2tI)\n(5)\nwhere q(x0) is the true data distribution, q(xT ) is a standard normal Gaussian distribution, 0 < \u03b21 < \u03b22 < \u00b7 \u00b7 \u00b7 < \u03b2T are\nnoise schedule parameters, and T is the total number of noise steps. To improve the efficiency of the fixed forward data\ncorruption process, (5) can be simplified to\nq(xt|x0) := N(\u221a\u00af\u03b1tx0, (1 \u2212 \u00af\u03b1t)I)\n(6)\nxt := \u221a\u00af\u03b1tx0 +\n\u221a\n1 \u2212 \u00af\u03b1t\u03f5 ,\n(7)\nwhere \u03b1t = 1 \u2212 \u03b2t, \u00af\u03b1t = Qt\ni=1 \u03b1t, and \u03f5 is standard normal Gaussian noise, enabling forward sampling for any step t given\nclean data x0.\nGiven the forward process, we can specify a model distribution p\u03b8(x0) that approximates q\u03b8(x0). To make p\u03b8(x0) easy to\nsample from, we specify the data generation process to be a\np\u03b8(x0) =\nZ\np\u03b8(x0, ..., xT )dx1,...,T\n(8)\np\u03b8(x0, ..., xT ) := p\u03b8(xT )\nT\nY\nt=1\np(t)\n\u03b8 (xt\u22121|x)\n(9)\nwhere x0, ..., xT are latent variables all in same data space.\nGiven the true data generation process (4) and model (9), we can train a neural network to recover the intermediate noisy\ndata xt\u22121 given xt. More specifically, Ho et al. (Ho et al., 2020) showed that if we optimize the variational lower bound\n(Kingma & Welling, 2013) of our data likelihood and we reparametrize our problem to predict the noise \u03f5, we can learn a\nsuitable neural network \u03f5\u03b8(xt, t) with parameters \u03b8 via minimizing the mean squared error via:\nEx0,\u03f5,t\nh\n\u2225\u03f5 \u2212 \u03f5\u03b8(xt, t)\u22252\n2\ni\n,\n(10)\nwhere t is the diffusion time-step.\nGiven a learned \u03f5\u03b8(xt, t), we can generate new data via the reverse diffusion process, a.k.a. sampling. To do so, we sample\nrandom Gaussian noise xT \u223c N(0, I) and then iteratively refine it via\nxt\u22121 =\n1\n\u221a\u03b1t\n\u0010\nxt \u2212 1 \u2212 \u03b1t\n\u221a1 \u2212 \u00af\u03b1t\n\u03f5\u03b8(xt, t)\n\u0011\n+ \u03c3t\u03f5,\n(11)\nuntil t = 0 to create our generated data x0 after T denoising iterations. To obtain high-quality generations, T is typically\nlarge (e.g., 1000), which results in a slow generation process.\nTo reduce the computational cost of sampling (inference), Song et al. (2020) proposed denoising diffusion implicit models\n(DDIM). DDIM uses an alternative variation optimization objective that itself yields an alternative sampling formulation\nxt\u22121 = \u221a\u03b1t\u22121\n \nxt \u2212 \u221a1 \u2212 \u03b1t\u03f5\u03b8(xt, t)\n\u221a\u03b1t\n!\n+\nq\n1 \u2212 \u03b1t\u22121 \u2212 \u03c32\nt \u03f5\u03b8(xt, t) + \u03c3t\u03f5,\n(12)\n13\nDITTO: Diffusion Inference-Time T -Optimization for Music Generation\nwhere \u03f5 \u223c N(0, I), \u03b10 := 1, and \u03c3t and different random noise scales. This formulation minimizes the number of sampling\nsteps needed during inference (e.g., 50 \u223c 100) with minimal impact on generation quality. Furthermore, special cases\nof DDIM are then two fold 1) when \u03c3t =\np\n(1 \u2212 \u03b1t\u22121)/(1 \u2212 \u03b1t)\np\n1 \u2212 \u03b1t/\u03b1t\u22121, DDIM sampling refers back to basic\nDDPM sampling and 2) when \u03c3t = 0 the sampling process becomes fully deterministic.\nTo improve text conditioning, classifier-free guidance (CFG) can be used to blend conditional and unconditional generation\noutputs and trade-off conditioning strength, mode coverage, and sample quality (Ho & Salimans, 2021). When training a\nmodel with CFG, conditioning is randomly set to a null value a fraction of the time. During inference, the diffusion model\noutput \u03f5\u03b8(xt, t, ctext) is replaced with\n\u02c6\u03f5CF G = w \u00b7 \u03f5\u03b8(xt, t, ctext) + (1 \u2212 w) \u00b7 \u03f5\u03b8(xt, t, c\u2205),\n(13)\nwhere ctext are text embeddings, w is the CFG scaling factor, and c\u2205 are null embeddings.\nB. EDICT and DOODL with invertible layers\nExact Diffusion Inversion via Coupled Transformations, or EDICT, is a sampling method introduced in Wallace et al.\n(2023b) to enable exact diffusion inversion. EDICT accomplishes this by denoising two correlated diffusion chains, x\u2032\nt and\nx\u2032\u2032\nt , at once, with the following updates:\nx\u2032inter\nt\n=\nr\u03b1t\u22121\n\u03b1t\nx\u2032\nt +\n\uf8eb\n\uf8edp\n1 \u2212 \u03b1t\u22121 \u2212\ns\n\u03b1t\u22121(1 \u2212 \u03b1t)\n\u03b1t\n\uf8f6\n\uf8f8 \u03f5\u03b8(x\u2032\u2032\nt , t)\nx\u2032\u2032inter\nt\n=\nr\u03b1t\u22121\n\u03b1t\nx\u2032\u2032\nt +\n\uf8eb\n\uf8edp\n1 \u2212 \u03b1t\u22121 \u2212\ns\n\u03b1t\u22121(1 \u2212 \u03b1t)\n\u03b1t\n\uf8f6\n\uf8f8 \u03f5\u03b8(x\u2032inter\nt\n, t)\nx\u2032\nt\u22121 = px\u2032inter\nt\n+ (1 \u2212 p)x\u2032\u2032inter\nt\nx\u2032\u2032\nt\u22121 = px\u2032\u2032inter\nt\n+ (1 \u2212 p)x\u2032\nt\u22121,\nwhere the first two lines denote affine coupling layers and the last two lines are mixing layers with a fixed mixing coefficient\np. This sampling procedure has the benefit of being exactly invertible:\nx\u2032\u2032inter\nt+1 = x\u2032\u2032\nt \u2212 (1 \u2212 p)x\u2032\nt\np\nx\u2032inter\nt+1 = x\u2032\nt \u2212 (1 \u2212 p)x\u2032\u2032inter\nt+1\np\nx\u2032\u2032\nt+1 =\nr\u03b1t+1\n\u03b1t\n \nx\u2032\u2032inter\nt+1 \u2212\n \n\u221a\n1 \u2212 \u03b1t \u2212\ns\n\u03b1t(1 \u2212 \u03b1t+1)\n\u03b1t+1\n!\n\u03f5\u03b8(x\u2032inter\nt+1 , t + 1)\n!\nx\u2032\nt+1 =\nr\u03b1t+1\n\u03b1t\n \nx\u2032inter\nt+1 \u2212\n \n\u221a\n1 \u2212 \u03b1t \u2212\ns\n\u03b1t(1 \u2212 \u03b1t+1)\n\u03b1t+1\n!\n\u03f5\u03b8(x\u2032\u2032\nt+1, t + 1)\n!\nOne consequence of the dual-chain sampling approach is the inherent tradeoff in setting the p mixing parameter, as p needs\nto be sufficiently low to prevent the two chains from diverging (especially at low sampling steps), and sufficiently high to\nprevent numerical precision errors when inverting the chains.\nIn the official implementation for DOODL, EDICT\u2019s invertibility is not used, and instead normal checkpointing is used on\nthe EDICT sampler, thus using 4x the number of model calls as standard backpropagation. However, given the invertible\nnature of EDICT, DOODL can alternatively be formulated to directly use the inverse operation rather than storing all\nfunction inputs in memory. In this setup, only the final x0 is stored in GPU memory, and then the inverse sampling operation\nis used to recalculate the function inputs, which are then passed back through the model to recalculate the intermediate\nactivations for gradient calculation. This procedure is more memory efficient than the official implementation of DOODL\nand DITTO, yet sextuples the number of model calls and runtime, thus being the slowest procedure for inference-time latent\noptimization. Figure 5 describes both setups more in detail.\n14\nDITTO: Diffusion Inference-Time T -Optimization for Music Generation\n<latexit sha1_base64=\"zb8BCm6dy\n9gesiVawqRqMtdjae8=\">AB/nicbVDLSgNBEJyNrxhfUY9eBoPgKeyKr2PQ\ni8cI5gHJEmYns8mQmdlplcMy4Lf4FXP3sSrv+LRP3GS7MEkFjQUVd10dwWx4\nAZc9sprKyurW8UN0tb2zu7e+X9g6aJEk1Zg0Yi0u2AGCa4Yg3gIFg71ozIQL\nBWMLqd+K1Hpg2P1AOMY+ZLMlA85JSAldrdQKZPWQ965YpbdafAy8TLSQXlqPf\nKP91+RBPJFBjOl4bgx+SjRwKlhW6iaGxYSOyIB1LFVEMuOn03szfGKVPg4j\nbUsBnqp/J1IijRnLwHZKAkOz6E3E/7xOAuG1n3IVJ8AUnS0KE4EhwpPncZ9rRk\nGMLSFUc3srpkOiCQUb0dyWQGY2E28xgWXSPKt6l9WL+/NK7SZPp4iO0DE6R6\n6QjV0h+qogSgS6AW9ojfn2Xl3PpzPWvByWcO0Rycr19GkpbY</latexit>xt\nIntermediate\nActivations\nStored Values\nDiscarded Values\nRecalculated \nValues\nDOODL Backprop w/Inverse\n<latexit sha1_base64=\"5Czkd7R7CmnEYhIs4hSFH8z18=\">AB/HicbVDLTgJBEOzF+IL9ehlIjHxRHaNryPRi0dM5JHAhvQOszAyO7uZmTUhBL/Bq56\n9Ga/+i0f/xAH2IGAlnVSqutPdFSCa+O6305uZXVtfSO/Wdja3tndK+4f1HWcKspqNBaxagaomeCS1Qw3gjUTxTAKBGsEg9uJ3hiSvNYPphwvwIe5KHnKxUr0tMRDYKZbcsjsFWSZeRkqQodop/rS7MU0jJg0VqHXLcxPj1AZTgUbF9qpZgnSAfZYy1KJEdP+aHrtmJxYpUvCWNmShkzVvxMjLQeRoHtjND09aI3Ef/zWqk\nJr/0Rl0lqmKSzRWEqiInJ5HXS5YpRI4aWIFXc3kpoHxVSYwOa2xJEY5uJt5jAMqmflb3L8sX9ealyk6WThyM4hlPw4AoqcAdVqAGFR3iBV3hznp1358P5nLXmnGzmEObgfP0CRDWVsA=</latexit>r\n<latexit sha1_base64=\"mIAYoI9nO5A7G5LG1UVGCTqBXU=\">ACAXicbVC7SgNBFL0bXzG+opY2i0GwCrviqwzaWFhEMA/YLGF2MkmGzGOZmRXCkspvsNXaTmz9Ekv/xNlkC5N4YOBwzr3cMyeKGdXG876dwsrq2vpGcbO0tb2zu1feP2hqmShMGlgyqdoR0oRQRqGkbasSKIR4y0otFt5reiNJUikczjknI0UDQPsXIWCnocGSGLH0ftItV7yqN4W7TPycVCBHvVv\n+6fQkTjgRBjOkdeB7sQlTpAzFjExKnUSTGOERGpDAUoE40WE6jTxT6zSc/tS2SeMO1X/bqSIaz3mkZ3MIupFLxP/84LE9K/DlIo4MUTg2aF+wlwj3ez/bo8qg0bW4Kwojari4dIWxsS3NXIp514i82sEyaZ1X/snrxcF6p3eTtFOEIjuEUfLiCGtxBHRqAQcILvMKb8+y8Ox/O52y04OQ7hzAH5+sXZgmYAg=</latexit>L\n<latexit sha1_base64=\"p/zZUtXUnmaWgdOFWVx/x7XIJK4=\">AB+XicbVA9TwJBEJ3DL8Qv1NJmIzGxInfGr5JgY4nRAxK4kL1lDzbs7l1290zIhZ9gq7W\ndsfXWPpPXOAKAV8yct7M5mZFyacaeO6305hbX1jc6u4XdrZ3ds/KB8eNXWcKkJ9EvNYtUOsKWeS+oYZTtuJoliEnLbC0d3Ubz1TpVksn8w4oYHA8kiRrCx0mO95/bKFbfqzoBWiZeTCuRo9Mo/3X5MUkGlIRxr3fHcxAQZVoYRTielbqpgskID2jHUokF1UE2O3WCzqzSR1GsbEmDZurfiQwLrcitJ0Cm6Fe9qbif14nNdF\ntkDGZpIZKMl8UpRyZGE3/Rn2mKDF8bAkmitlbERlihYmx6SxsCcXEZuItJ7BKmhdV7p69XBZqdXzdIpwAqdwDh7cQA3uoQE+EBjAC7zCm5M5786H8zlvLTj5zDEswPn6BWV+lAk=</latexit>B0\n<latexit sha1_base64=\"9Znxn6VDIRyBDLkDowM/rKGyYE=\">ACnicbVC7TsNAEDzDOFlQkljESFRTbiVUbQUAaJPKTYis6XTXLKnW3drRGRlT/gG2i\nhpkO0/AQlf8IlcUESRlpdmZXu5owEVyj635bK6tr6xubha3i9s7u3r59UGroOFUM6iwWsWqFVIPgEdSRo4BWoDKUEAzHN5O/OYjKM3j6AFHCQS9iPe4yikTp2yQ+lD4nmwnQ+DgBpxy67FXcKZ5l4OSmTHLWO/eN3Y5ZKiJAJqnXbcxMqQMwHjop9qSCgb0j60DY2oB1k09/HzolRuk4vVqYidKbq342MSq1HMjSTkuJ\nAL3oT8T+vnWLvOsh4lKQIEZsd6qXCwdiZBOF0uQKGYmQIZYqbXx02oIoyNHNXQnl2GTiLSawTBpnFe+ycnF/Xq7e5OkUyBE5JqfEI1ekSu5IjdQJI0/khbySN+vZerc+rM/Z6IqV7xySOVhfv4VEm1E=</latexit>\u270f\u2713\n<latexit sha1_base64=\"v0M8p7MtIqExe6ySO16lPjDBlG8=\">ACAHicbVC7TgMxENzjGcIrQEljEaFQRXeIVxlBQxk8hDJKfI5vsSK7TvZPkR0SsM30EJNh2j5E0r+BCe5giSMtNJoZle7O0HMmTau+0\nIeIFXeHOenXfnw/mcti452cwBzMD5+gWn4Jb3</latexit>sLa+srq3nNvKbW9s7u4W9/bqOEkVojUQ8Us0Aa8qZpDXDKfNWFEsAk4bweBm7DceqdIskvdmGFNf4J5kISPYWOmhHYj0aVQqdbxOoeiW3QnQIvEyUoQM1U7hp92NSCKoNIRjrVueGxs/xcowuko3040jTEZ4B5tWSqxoNpPJxeP0LFVuiMlC1p0ET9O5FiofVQBLZTYNPX895Y/M9rJSa8lMm48RQSaLwoQjE6Hx+6jLFCWGDy3BRDF7KyJ9rDAxNqSZLYEY2Uy8+QWSf207F2Uz+/OipXrLJ0cHMIRnIAHl1CBW6hCDQh\nx00\n1\n<latexit sha1_base64=\"dgtcBP7Ua97NSC73SABWXJ+WYpc=\">AB/3icbVC5TgMxEJ0NVwhXgJLGIkJQRbuIq4ygoQwSOVCyiryON7Fie1e2FxGtuAbaKGmQ7R8CiV/gnMUJOFJIz29N6OZeUHMmTau+3k\nc56d+fD+Zy05pzpzD7MwPn6BUJolsY=</latexit>lpZXVtfy64WNza3tneLuXl1HiSK0RiIeqWaANeVM0phtNmrCgWAaeNYHAz8huPVGkWyXszjKkvcE+ykBFsrPTQDkT6lB13vE6x5JbdMdAi8akBFNUO8WfdjciaDSEI61bnlubPwUK8MIp1mhnWgaYzLAPdqyVGJBtZ+OD87QkVW6KIyULWnQWP07kWKh9VAEtlNg09fz3kj8z2slJrzyUybjxFBJovChCMTodH3qMsUJYPLcFEMXsrIn2sMDE2o5ktgchsJt58Aoukflr2Lsrnd2elyvU0nTwcwCGcgAeXUIFbqEINCAh4gVd4\nx0\n1\n<latexit sha1_base64=\"9Znxn6VDIRyBDLkDowM/rKGyYE=\">ACnicbVC7TsNAEDzDOFlQkljESFRTbiVUbQUAaJPKTYis6XTXLKnW3drRGRlT/gG2i\nhpkO0/AQlf8IlcUESRlpdmZXu5owEVyj635bK6tr6xubha3i9s7u3r59UGroOFUM6iwWsWqFVIPgEdSRo4BWoDKUEAzHN5O/OYjKM3j6AFHCQS9iPe4yikTp2yQ+lD4nmwnQ+DgBpxy67FXcKZ5l4OSmTHLWO/eN3Y5ZKiJAJqnXbcxMqQMwHjop9qSCgb0j60DY2oB1k09/HzolRuk4vVqYidKbq342MSq1HMjSTkuJ\nAL3oT8T+vnWLvOsh4lKQIEZsd6qXCwdiZBOF0uQKGYmQIZYqbXx02oIoyNHNXQnl2GTiLSawTBpnFe+ycnF/Xq7e5OkUyBE5JqfEI1ekSu5IjdQJI0/khbySN+vZerc+rM/Z6IqV7xySOVhfv4VEm1E=</latexit>\u270f\u2713\n<latexit sha1_base64=\"6k1OAPZsTt7vM0y1gQ+vlx0FfY=\">ACAHicbVC7TgMxENzjGcIrQEljEaFQRXeIVxlBQxk8hDJKfI5vsSK7TvZPkR0SsM30EJNh2j5E0r+BCe5giSMtNJoZle7O0HMmTau+\n6hCDQhIeIFXeHOenXfnw/mcti452cwBzMD5+gWmTZb2</latexit>+0sLa+srq3nNvKbW9s7u4W9/bqOEkVojUQ8Us0Aa8qZpDXDKfNWFEsAk4bweBm7DceqdIskvdmGFNf4J5kISPYWOmhHYj0aVQqdxOoeiW3QnQIvEyUoQM1U7hp92NSCKoNIRjrVueGxs/xcowuko3040jTEZ4B5tWSqxoNpPJxeP0LFVuiMlC1p0ET9O5FiofVQBLZTYNPX895Y/M9rJSa8lMm48RQSaLwoQjE6Hx+6jLFCWGDy3BRDF7KyJ9rDAxNqSZLYEY2Uy8+QWSf207F2Uz+/OipXrLJ0cHMIRnIAHl1CBW\nx00\n0\n<latexit sha1_base64=\"xXnvqBfbteQYNecSf5rTsEKlqI=\">AB/3icbVC5TgMxEJ0NVwhXgJLGIkJQRbuIq4ygoQwSOVCyiryON7Fie1e2FxGtuAbaKGmQ7R8CiV/gnMUJOFJIz29N6OZeU\nU0nTwcwCGcgAeXUIFbqEINCAh4gVd4c56d+fD+Zy05pzpzD7MwPn6BUDVlsU=</latexit>HMmTau+3klpZXVtfy64WNza3tneLuXl1HiSK0RiIeqWaANeVM0phtNmrCgWAaeNYHAz8huPVGkWyXszjKkvcE+ykBFsrPTQDkT6lB13E6x5JbdMdAi8akBFNUO8WfdjciaDSEI61bnlubPwUK8MIp1mhnWgaYzLAPdqyVGJBtZ+OD87QkVW6KIyULWnQWP07kWKh9VAEtlNg09fz3kj8z2slJrzyUybjxFBJovChCMTodH3qMsUJYPLcFEMXsrIn2sMDE2o5ktgchsJt58Aoukflr2Lsrnd2elyv\nx0\n0\nInverse\n<latexit sha1_base64=\"9Znxn6VDIRyBDLkDowM/rKGyYE=\">ACnicbVC7TsNAEDzDOFlQkljESFRTbiVUbQUAaJPKTYis6XTXLKnW3drRGRlT/gG2i\nhpkO0/AQlf8IlcUESRlpdmZXu5owEVyj635bK6tr6xubha3i9s7u3r59UGroOFUM6iwWsWqFVIPgEdSRo4BWoDKUEAzHN5O/OYjKM3j6AFHCQS9iPe4yikTp2yQ+lD4nmwnQ+DgBpxy67FXcKZ5l4OSmTHLWO/eN3Y5ZKiJAJqnXbcxMqQMwHjop9qSCgb0j60DY2oB1k09/HzolRuk4vVqYidKbq342MSq1HMjSTkuJ\nAL3oT8T+vnWLvOsh4lKQIEZsd6qXCwdiZBOF0uQKGYmQIZYqbXx02oIoyNHNXQnl2GTiLSawTBpnFe+ycnF/Xq7e5OkUyBE5JqfEI1ekSu5IjdQJI0/khbySN+vZerc+rM/Z6IqV7xySOVhfv4VEm1E=</latexit>\u270f\u2713\nInverse\n<latexit sha1_base64=\"BJ8DbCKNBsr/M/FmJXiOm8D58PQ=\">ACAHicbVC7TsNAEFyHVwivACWNRYRCFdmIVxlBQxmkvERiRefLOTnl7mzdnRGR5YZvoIWaDtHyJ5T8CZfEBUkYaXRzK52\n5eL+vFS9ydLJwxEcwym4cAVuIMaNACDgBd4hTfr2Xq3PqzPWvOymYOYQ7W1y/e+Zca</latexit>d/yIUaUd59vKrayurW/kNwtb2zu7e8X9g6YKY4lJA4cslG0fKcKoIA1NSPtSBLEfUZa/uh24rceiVQ0FHU9jojH0UDQgGKkjfTQ9XnylJbLvXqvWHIqzhT2MnEzUoIMtV7xp9sPcyJ0JghpTquE2kvQVJTzEha6MaKRAiP0IB0DBWIE+Ul04tT+8QofTsIpSmh7an6dyJBXKkx90nR3qoFr2J+J/XiXVw7SVURLEmAs8WBTGzdWhP3rf7VBKs2dgQhCU1t9p4iCTC2oQ0t8XnqcnEXUxgmTPKu5l\nx00\nT\n<latexit sha1_base64=\"XIETZpkCMT+ls/qmQtLFEjpDWcI=\">AB/3icbVC7TgMxENzjGcIrQEljESGojvEq4ygoQxSXig5RT7Hl1ixfSfbh4hOKfgGWqjpEC2fQsmf4CRXkISRVhrN7Gp3\n9+fF8k2WTg4O4QhOwYMrKMdVKAGBAS8wCu8Oc/Ou/PhfE5bl5xs5gBm4Hz9AnmBluk=</latexit>J4g508Z1v52l5ZXVtfXcRn5za3tnt7C3X9dRogitkYhHqhlgTmTtGaY4bQZK4pFwGkjGNyO/cYjVZpFsmqGMfUF7kWMoKNlR7agUifRiedaqdQdEvuBGiReBkpQoZKp/DT7kYkEVQawrHWLc+NjZ9iZRjhdJRvJ5rGmAxwj7YslVhQ7aeTg0fo2CpdFEbKljRov6dSLHQeigC2ymw6et5byz+57USE17KZNxYqgk0VhwpGJ0Ph71GWKEsOHlmCimL0VkT5WmBib0cyWQIxsJt58AoukflbyLksX\nx0\nT\n<latexit sha1_base64=\"hEn2BRjdbNmEp6y6RvUNCOlufSM=\">AB6nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PUi8eIeUGyhNnJBkyO7vM9AphySd48aC\nIV7/Im3/jJNmDJhY0FXdHcFsRQGXfbya2srq1v5DcLW9s7u3vF/YOGiRLNeJ1FMtKtgBouheJ1FCh5K9achoHkzWB0N/WbT1wbEakajmPuh3SgRF8wilZ6vOnWusWSW3ZnIMvEy0gJMlS7xa9OL2JyBUySY1pe26Mfko1Cib5pNBJDI8pG9EBb1uqaMiNn85OnZATq/RIP9K2FJKZ+nsipaEx4zCwnSHFoVn0puJ/XjvB/rW\nfChUnyBWbL+onkmBEpn+TntCcoRxbQpkW9lbChlRThjadg3BW3x5mTOyt5l+eLhvFS5zeLIwxEcwyl4cAUVuIcq1IHBAJ7hFd4c6bw4787HvDXnZDOH8AfO5w/wI42W</latexit>AT\n<latexit sha1_base64=\"mIAYoI9nO5A7G5LG1UVGCTqBXU=\">ACAXicbVC7SgNBFL0bXzG+opY2i0GwCrviqwzaWFhEMA/YLGF2MkmGzGOZmRXCkspvsNXaTmz9Ekv/xNlkC5N4YOBwzr3cMyeKGdXG876dwsrq2vpGcbO0tb2zu1feP2hqmShMGlgyqdoR0oRQRqGkbasSKIR4y0otFt5reiNJUikczjknI0UDQPsXIWCnocGSGLH0ftItV7yqN4W7TPycVCBHvVv\n+6fQkTjgRBjOkdeB7sQlTpAzFjExKnUSTGOERGpDAUoE40WE6jTxT6zSc/tS2SeMO1X/bqSIaz3mkZ3MIupFLxP/84LE9K/DlIo4MUTg2aF+wlwj3ez/bo8qg0bW4Kwojari4dIWxsS3NXIp514i82sEyaZ1X/snrxcF6p3eTtFOEIjuEUfLiCGtxBHRqAQcILvMKb8+y8Ox/O52y04OQ7hzAH5+sXZgmYAg=</latexit>L\n<latexit sha1_base64=\"9Znxn6VDIRyBDLkDowM/rKGyYE=\">ACnicbVC7TsNAEDzDOFlQkljESFRTbiVUbQUAaJPKTYis6XTXLKnW3drRGRlT/gG2i\nhpkO0/AQlf8IlcUESRlpdmZXu5owEVyj635bK6tr6xubha3i9s7u3r59UGroOFUM6iwWsWqFVIPgEdSRo4BWoDKUEAzHN5O/OYjKM3j6AFHCQS9iPe4yikTp2yQ+lD4nmwnQ+DgBpxy67FXcKZ5l4OSmTHLWO/eN3Y5ZKiJAJqnXbcxMqQMwHjop9qSCgb0j60DY2oB1k09/HzolRuk4vVqYidKbq342MSq1HMjSTkuJ\nAL3oT8T+vnWLvOsh4lKQIEZsd6qXCwdiZBOF0uQKGYmQIZYqbXx02oIoyNHNXQnl2GTiLSawTBpnFe+ycnF/Xq7e5OkUyBE5JqfEI1ekSu5IjdQJI0/khbySN+vZerc+rM/Z6IqV7xySOVhfv4VEm1E=</latexit>\u270f\u2713\n<latexit sha1_base64=\"9Znxn6VDIRyBDLkDowM/rKGyYE=\">ACnicbVC7TsNAEDzDOFlQkljESFRTbiVUbQUAaJPKTYis6XTXLKnW3drRGRlT/gG2i\nhpkO0/AQlf8IlcUESRlpdmZXu5owEVyj635bK6tr6xubha3i9s7u3r59UGroOFUM6iwWsWqFVIPgEdSRo4BWoDKUEAzHN5O/OYjKM3j6AFHCQS9iPe4yikTp2yQ+lD4nmwnQ+DgBpxy67FXcKZ5l4OSmTHLWO/eN3Y5ZKiJAJqnXbcxMqQMwHjop9qSCgb0j60DY2oB1k09/HzolRuk4vVqYidKbq342MSq1HMjSTkuJ\nAL3oT8T+vnWLvOsh4lKQIEZsd6qXCwdiZBOF0uQKGYmQIZYqbXx02oIoyNHNXQnl2GTiLSawTBpnFe+ycnF/Xq7e5OkUyBE5JqfEI1ekSu5IjdQJI0/khbySN+vZerc+rM/Z6IqV7xySOVhfv4VEm1E=</latexit>\u270f\u2713\n<latexit sha1_base64=\"9Znxn6VDIRyBDLkDowM/rKGyYE=\">ACnicbVC7TsNAEDzDOFlQkljESFRTbiVUbQUAaJPKTYis6XTXLKnW3drRGRlT/gG2i\nhpkO0/AQlf8IlcUESRlpdmZXu5owEVyj635bK6tr6xubha3i9s7u3r59UGroOFUM6iwWsWqFVIPgEdSRo4BWoDKUEAzHN5O/OYjKM3j6AFHCQS9iPe4yikTp2yQ+lD4nmwnQ+DgBpxy67FXcKZ5l4OSmTHLWO/eN3Y5ZKiJAJqnXbcxMqQMwHjop9qSCgb0j60DY2oB1k09/HzolRuk4vVqYidKbq342MSq1HMjSTkuJ\nAL3oT8T+vnWLvOsh4lKQIEZsd6qXCwdiZBOF0uQKGYmQIZYqbXx02oIoyNHNXQnl2GTiLSawTBpnFe+ycnF/Xq7e5OkUyBE5JqfEI1ekSu5IjdQJI0/khbySN+vZerc+rM/Z6IqV7xySOVhfv4VEm1E=</latexit>\u270f\u2713\n<latexit sha1_base64=\"BJ8DbCKNBsr/M/FmJXiOm8D58PQ=\">ACAHicbVC7TsNAEFyHVwivACWNRYRCFdmIVxlBQxmkvERiRefLOTnl7mzdnRGR5YZvoIWaDtHyJ5T8CZfEBUkYaXRzK52\n5eL+vFS9ydLJwxEcwym4cAVuIMaNACDgBd4hTfr2Xq3PqzPWvOymYOYQ7W1y/e+Zca</latexit>d/yIUaUd59vKrayurW/kNwtb2zu7e8X9g6YKY4lJA4cslG0fKcKoIA1NSPtSBLEfUZa/uh24rceiVQ0FHU9jojH0UDQgGKkjfTQ9XnylJbLvXqvWHIqzhT2MnEzUoIMtV7xp9sPcyJ0JghpTquE2kvQVJTzEha6MaKRAiP0IB0DBWIE+Ul04tT+8QofTsIpSmh7an6dyJBXKkx90nR3qoFr2J+J/XiXVw7SVURLEmAs8WBTGzdWhP3rf7VBKs2dgQhCU1t9p4iCTC2oQ0t8XnqcnEXUxgmTPKu5l\nx00\nT\n<latexit sha1_base64=\"XIETZpkCMT+ls/qmQtLFEjpDWcI=\">AB/3icbVC7TgMxENzjGcIrQEljESGojvEq4ygoQxSXig5RT7Hl1ixfSfbh4hOKfgGWqjpEC2fQsmf4CRXkISRVhrN7Gp3\n9+fF8k2WTg4O4QhOwYMrKMdVKAGBAS8wCu8Oc/Ou/PhfE5bl5xs5gBm4Hz9AnmBluk=</latexit>J4g508Z1v52l5ZXVtfXcRn5za3tnt7C3X9dRogitkYhHqhlgTmTtGaY4bQZK4pFwGkjGNyO/cYjVZpFsmqGMfUF7kWMoKNlR7agUifRiedaqdQdEvuBGiReBkpQoZKp/DT7kYkEVQawrHWLc+NjZ9iZRjhdJRvJ5rGmAxwj7YslVhQ7aeTg0fo2CpdFEbKljRov6dSLHQeigC2ymw6et5byz+57USE17KZNxYqgk0VhwpGJ0Ph71GWKEsOHlmCimL0VkT5WmBib0cyWQIxsJt58AoukflbyLksX\nx0\nT\n<latexit sha1_base64=\"aNJv+jnZ6TankgM34GAN93ymJ6Q=\">AB/3icbVC7TgMxENzjGcIrQEljESGoruIVxlBQxk8kDJKfI5TmLF9p1sHyI6XcE30EJNh2j5FEr+BCe5giSMtNJoZle7\nE6vzsrVq6zdHJwCEdwCh5cQgVuoQo1ICDgBV7hzXl23p0P53PauRkMwcwA+frF0P7lsc=</latexit>O0HEmTau+0sLa+srq3nNvKbW9s7u4W9/boOY0VojYQ8VM0Aa8qZpDXDKfNSFEsAk4bwfBm7DceqdIslPdmFf4L5kPUawsdJDOxDJU3rSKXcKRbfkToAWiZeRImSodgo/7W5IYkGlIRxr3fLcyPgJVoYRTtN8O9Y0wmSI+7RlqcSCaj+ZHJyiY6t0US9UtqRBE/XvRIKF1iMR2E6BzUDPe2PxP68Vm96VnzAZxYZKMl3UizkyIRp/j7pMUWL4yBJMFLO3IjLAChNjM5rZEojUZuLNJ7BI6uWSd1\nx0\n2\n<latexit sha1_base64=\"sMvIxUPnF2+h4DnoxSxOQS57UJM=\">ACAHicbVC7TsNAEFyHVwivACWNRYRCFdkRrzKChjJI5CESKzpfzskpd2fr7oyILDd8Ay3UdIiWP6HkT7gkLkjCSCuNZna1\nEvKud3Z6XadZOHo7gGE7BhUuowS3UoQEYBLzAK7xZz9a79WF9zlpzVjZzCHOwvn4BqXOW+A=</latexit>u+NHjCrtON9WbmV1bX0jv1nY2t7Z3SvuHzRVGEtMGjhkoWz7SBFGBWloqhlpR5Ig7jPS8kc3E7/1SKSiobjX4h4HA0EDShG2kgPXZ8nT2m53Kv2iWn4kxhLxM3IyXIUO8Vf7r9EMecCI0ZUqrjOpH2EiQ1xYykhW6sSITwCA1Ix1CBOFeMr04tU+M0reDUJoS2p6qfycSxJUac90cqSHatGbiP95nVgHV15CRrIvBsURAzW4f25H27TyXBmo0NQVhSc6uNh0girE1Ic1t8npM3MUElkmzWn\nx00\n2\n<latexit sha1_base64=\"v0M8p7MtIqExe6ySO16lPjDBlG8=\">ACAHicbVC7TgMxENzjGcIrQEljEaFQRXeIVxlBQxk8hDJKfI5vsSK7TvZPkR0SsM30EJNh2j5E0r+BCe5giSMtNJoZle7O0HMmTau+0\nIeIFXeHOenXfnw/mcti452cwBzMD5+gWn4Jb3</latexit>sLa+srq3nNvKbW9s7u4W9/bqOEkVojUQ8Us0Aa8qZpDXDKfNWFEsAk4bweBm7DceqdIskvdmGFNf4J5kISPYWOmhHYj0aVQqdbxOoeiW3QnQIvEyUoQM1U7hp92NSCKoNIRjrVueGxs/xcowuko3040jTEZ4B5tWSqxoNpPJxeP0LFVuiMlC1p0ET9O5FiofVQBLZTYNPX895Y/M9rJSa8lMm48RQSaLwoQjE6Hx+6jLFCWGDy3BRDF7KyJ9rDAxNqSZLYEY2Uy8+QWSf207F2Uz+/OipXrLJ0cHMIRnIAHl1CBW6hCDQh\nx00\n1\n<latexit sha1_base64=\"dgtcBP7Ua97NSC73SABWXJ+WYpc=\">AB/3icbVC5TgMxEJ0NVwhXgJLGIkJQRbuIq4ygoQwSOVCyiryON7Fie1e2FxGtuAbaKGmQ7R8CiV/gnMUJOFJIz29N6OZeUHMmTau+3k\nc56d+fD+Zy05pzpzD7MwPn6BUJolsY=</latexit>lpZXVtfy64WNza3tneLuXl1HiSK0RiIeqWaANeVM0phtNmrCgWAaeNYHAz8huPVGkWyXszjKkvcE+ykBFsrPTQDkT6lB13vE6x5JbdMdAi8akBFNUO8WfdjciaDSEI61bnlubPwUK8MIp1mhnWgaYzLAPdqyVGJBtZ+OD87QkVW6KIyULWnQWP07kWKh9VAEtlNg09fz3kj8z2slJrzyUybjxFBJovChCMTodH3qMsUJYPLcFEMXsrIn2sMDE2o5ktgchsJt58Aoukflr2Lsrnd2elyvU0nTwcwCGcgAeXUIFbqEINCAh4gVd4\nx0\n1\n<latexit sha1_base64=\"xXnvqBfbteQYNecSf5rTsEKlqI=\">AB/3icbVC5TgMxEJ0NVwhXgJLGIkJQRbuIq4ygoQwSOVCyiryON7Fie1e2FxGtuAbaKGmQ7R8CiV/gnMUJOFJIz29N6OZeU\nU0nTwcwCGcgAeXUIFbqEINCAh4gVd4c56d+fD+Zy05pzpzD7MwPn6BUDVlsU=</latexit>HMmTau+3klpZXVtfy64WNza3tneLuXl1HiSK0RiIeqWaANeVM0phtNmrCgWAaeNYHAz8huPVGkWyXszjKkvcE+ykBFsrPTQDkT6lB13E6x5JbdMdAi8akBFNUO8WfdjciaDSEI61bnlubPwUK8MIp1mhnWgaYzLAPdqyVGJBtZ+OD87QkVW6KIyULWnQWP07kWKh9VAEtlNg09fz3kj8z2slJrzyUybjxFBJovChCMTodH3qMsUJYPLcFEMXsrIn2sMDE2o5ktgchsJt58Aoukflr2Lsrnd2elyv\nx0\n0\n<latexit sha1_base64=\"6k1OAPZsTt7vM0y1gQ+vlx0FfY=\">ACAHicbVC7TgMxENzjGcIrQEljEaFQRXeIVxlBQxk8hDJKfI5vsSK7TvZPkR0SsM30EJNh2j5E0r+BCe5giSMtNJoZle7O0HMmTau+\n6hCDQhIeIFXeHOenXfnw/mcti452cwBzMD5+gWmTZb2</latexit>+0sLa+srq3nNvKbW9s7u4W9/bqOEkVojUQ8Us0Aa8qZpDXDKfNWFEsAk4bweBm7DceqdIskvdmGFNf4J5kISPYWOmhHYj0aVQqdxOoeiW3QnQIvEyUoQM1U7hp92NSCKoNIRjrVueGxs/xcowuko3040jTEZ4B5tWSqxoNpPJxeP0LFVuiMlC1p0ET9O5FiofVQBLZTYNPX895Y/M9rJSa8lMm48RQSaLwoQjE6Hx+6jLFCWGDy3BRDF7KyJ9rDAxNqSZLYEY2Uy8+QWSf207F2Uz+/OipXrLJ0cHMIRnIAHl1CBW\nx00\n0\n<latexit sha1_base64=\"6k1OAPZsTt7vM0y1gQ+vlx0FfY=\">ACAHicbVC7TgMxENzjGcIrQEljEaFQRXeIVxlBQxk8hDJKfI5vsSK7TvZPkR0SsM30EJNh2j5E0r+BCe5giSMtNJoZle7O0HMmTau+\n6hCDQhIeIFXeHOenXfnw/mcti452cwBzMD5+gWmTZb2</latexit>+0sLa+srq3nNvKbW9s7u4W9/bqOEkVojUQ8Us0Aa8qZpDXDKfNWFEsAk4bweBm7DceqdIskvdmGFNf4J5kISPYWOmhHYj0aVQqdxOoeiW3QnQIvEyUoQM1U7hp92NSCKoNIRjrVueGxs/xcowuko3040jTEZ4B5tWSqxoNpPJxeP0LFVuiMlC1p0ET9O5FiofVQBLZTYNPX895Y/M9rJSa8lMm48RQSaLwoQjE6Hx+6jLFCWGDy3BRDF7KyJ9rDAxNqSZLYEY2Uy8+QWSf207F2Uz+/OipXrLJ0cHMIRnIAHl1CBW\nx00\n0\n<latexit sha1_base64=\"xXnvqBfbteQYNecSf5rTsEKlqI=\">AB/3icbVC5TgMxEJ0NVwhXgJLGIkJQRbuIq4ygoQwSOVCyiryON7Fie1e2FxGtuAbaKGmQ7R8CiV/gnMUJOFJIz29N6OZeU\nU0nTwcwCGcgAeXUIFbqEINCAh4gVd4c56d+fD+Zy05pzpzD7MwPn6BUDVlsU=</latexit>HMmTau+3klpZXVtfy64WNza3tneLuXl1HiSK0RiIeqWaANeVM0phtNmrCgWAaeNYHAz8huPVGkWyXszjKkvcE+ykBFsrPTQDkT6lB13E6x5JbdMdAi8akBFNUO8WfdjciaDSEI61bnlubPwUK8MIp1mhnWgaYzLAPdqyVGJBtZ+OD87QkVW6KIyULWnQWP07kWKh9VAEtlNg09fz3kj8z2slJrzyUybjxFBJovChCMTodH3qMsUJYPLcFEMXsrIn2sMDE2o5ktgchsJt58Aoukflr2Lsrnd2elyv\nx0\n0\n<latexit sha1_base64=\"5Czkd7R7CmnEYhIs4hSFH8z18=\">AB/HicbVDLTgJBEOzF+IL9ehlIjHxRHaNryPRi0dM5JHAhvQOszAyO7uZmTUhBL/Bq56\n9Ga/+i0f/xAH2IGAlnVSqutPdFSCa+O6305uZXVtfSO/Wdja3tndK+4f1HWcKspqNBaxagaomeCS1Qw3gjUTxTAKBGsEg9uJ3hiSvNYPphwvwIe5KHnKxUr0tMRDYKZbcsjsFWSZeRkqQodop/rS7MU0jJg0VqHXLcxPj1AZTgUbF9qpZgnSAfZYy1KJEdP+aHrtmJxYpUvCWNmShkzVvxMjLQeRoHtjND09aI3Ef/zWqk\nJr/0Rl0lqmKSzRWEqiInJ5HXS5YpRI4aWIFXc3kpoHxVSYwOa2xJEY5uJt5jAMqmflb3L8sX9ealyk6WThyM4hlPw4AoqcAdVqAGFR3iBV3hznp1358P5nLXmnGzmEObgfP0CRDWVsA=</latexit>r\n<latexit sha1_base64=\"9Znxn6VDIRyBDLkDowM/rKGyYE=\">ACnicbVC7TsNAEDzDOFlQkljESFRTbiVUbQUAaJPKTYis6XTXLKnW3drRGRlT/gG2i\nhpkO0/AQlf8IlcUESRlpdmZXu5owEVyj635bK6tr6xubha3i9s7u3r59UGroOFUM6iwWsWqFVIPgEdSRo4BWoDKUEAzHN5O/OYjKM3j6AFHCQS9iPe4yikTp2yQ+lD4nmwnQ+DgBpxy67FXcKZ5l4OSmTHLWO/eN3Y5ZKiJAJqnXbcxMqQMwHjop9qSCgb0j60DY2oB1k09/HzolRuk4vVqYidKbq342MSq1HMjSTkuJ\nAL3oT8T+vnWLvOsh4lKQIEZsd6qXCwdiZBOF0uQKGYmQIZYqbXx02oIoyNHNXQnl2GTiLSawTBpnFe+ycnF/Xq7e5OkUyBE5JqfEI1ekSu5IjdQJI0/khbySN+vZerc+rM/Z6IqV7xySOVhfv4VEm1E=</latexit>\u270f\u2713\n<latexit sha1_base64=\"9Znxn6VDIRyBDLkDowM/rKGyYE=\">ACnicbVC7TsNAEDzDOFlQkljESFRTbiVUbQUAaJPKTYis6XTXLKnW3drRGRlT/gG2i\nhpkO0/AQlf8IlcUESRlpdmZXu5owEVyj635bK6tr6xubha3i9s7u3r59UGroOFUM6iwWsWqFVIPgEdSRo4BWoDKUEAzHN5O/OYjKM3j6AFHCQS9iPe4yikTp2yQ+lD4nmwnQ+DgBpxy67FXcKZ5l4OSmTHLWO/eN3Y5ZKiJAJqnXbcxMqQMwHjop9qSCgb0j60DY2oB1k09/HzolRuk4vVqYidKbq342MSq1HMjSTkuJ\nAL3oT8T+vnWLvOsh4lKQIEZsd6qXCwdiZBOF0uQKGYmQIZYqbXx02oIoyNHNXQnl2GTiLSawTBpnFe+ycnF/Xq7e5OkUyBE5JqfEI1ekSu5IjdQJI0/khbySN+vZerc+rM/Z6IqV7xySOVhfv4VEm1E=</latexit>\u270f\u2713\nInverse\n<latexit sha1_base64=\"aNJv+jnZ6TankgM34GAN93ymJ6Q=\">AB/3icbVC7TgMxENzjGcIrQEljESGoruIVxlBQxk8kDJKfI5TmLF9p1sHyI6XcE30EJNh2j5FEr+BCe5giSMtNJoZle7\nE6vzsrVq6zdHJwCEdwCh5cQgVuoQo1ICDgBV7hzXl23p0P53PauRkMwcwA+frF0P7lsc=</latexit>O0HEmTau+0sLa+srq3nNvKbW9s7u4W9/boOY0VojYQ8VM0Aa8qZpDXDKfNSFEsAk4bwfBm7DceqdIslPdmFf4L5kPUawsdJDOxDJU3rSKXcKRbfkToAWiZeRImSodgo/7W5IYkGlIRxr3fLcyPgJVoYRTtN8O9Y0wmSI+7RlqcSCaj+ZHJyiY6t0US9UtqRBE/XvRIKF1iMR2E6BzUDPe2PxP68Vm96VnzAZxYZKMl3UizkyIRp/j7pMUWL4yBJMFLO3IjLAChNjM5rZEojUZuLNJ7BI6uWSd1\nx0\n2\n<latexit sha1_base64=\"sMvIxUPnF2+h4DnoxSxOQS57UJM=\">ACAHicbVC7TsNAEFyHVwivACWNRYRCFdkRrzKChjJI5CESKzpfzskpd2fr7oyILDd8Ay3UdIiWP6HkT7gkLkjCSCuNZna1\nEvKud3Z6XadZOHo7gGE7BhUuowS3UoQEYBLzAK7xZz9a79WF9zlpzVjZzCHOwvn4BqXOW+A=</latexit>u+NHjCrtON9WbmV1bX0jv1nY2t7Z3SvuHzRVGEtMGjhkoWz7SBFGBWloqhlpR5Ig7jPS8kc3E7/1SKSiobjX4h4HA0EDShG2kgPXZ8nT2m53Kv2iWn4kxhLxM3IyXIUO8Vf7r9EMecCI0ZUqrjOpH2EiQ1xYykhW6sSITwCA1Ix1CBOFeMr04tU+M0reDUJoS2p6qfycSxJUac90cqSHatGbiP95nVgHV15CRrIvBsURAzW4f25H27TyXBmo0NQVhSc6uNh0girE1Ic1t8npM3MUElkmzWn\nx00\n2\n<latexit sha1_base64=\"v0M8p7MtIqExe6ySO16lPjDBlG8=\">ACAHicbVC7TgMxENzjGcIrQEljEaFQRXeIVxlBQxk8hDJKfI5vsSK7TvZPkR0SsM30EJNh2j5E0r+BCe5giSMtNJoZle7O0HMmTau+0\nIeIFXeHOenXfnw/mcti452cwBzMD5+gWn4Jb3</latexit>sLa+srq3nNvKbW9s7u4W9/bqOEkVojUQ8Us0Aa8qZpDXDKfNWFEsAk4bweBm7DceqdIskvdmGFNf4J5kISPYWOmhHYj0aVQqdbxOoeiW3QnQIvEyUoQM1U7hp92NSCKoNIRjrVueGxs/xcowuko3040jTEZ4B5tWSqxoNpPJxeP0LFVuiMlC1p0ET9O5FiofVQBLZTYNPX895Y/M9rJSa8lMm48RQSaLwoQjE6Hx+6jLFCWGDy3BRDF7KyJ9rDAxNqSZLYEY2Uy8+QWSf207F2Uz+/OipXrLJ0cHMIRnIAHl1CBW6hCDQh\nx00\n1\n<latexit sha1_base64=\"dgtcBP7Ua97NSC73SABWXJ+WYpc=\">AB/3icbVC5TgMxEJ0NVwhXgJLGIkJQRbuIq4ygoQwSOVCyiryON7Fie1e2FxGtuAbaKGmQ7R8CiV/gnMUJOFJIz29N6OZeUHMmTau+3k\nc56d+fD+Zy05pzpzD7MwPn6BUJolsY=</latexit>lpZXVtfy64WNza3tneLuXl1HiSK0RiIeqWaANeVM0phtNmrCgWAaeNYHAz8huPVGkWyXszjKkvcE+ykBFsrPTQDkT6lB13vE6x5JbdMdAi8akBFNUO8WfdjciaDSEI61bnlubPwUK8MIp1mhnWgaYzLAPdqyVGJBtZ+OD87QkVW6KIyULWnQWP07kWKh9VAEtlNg09fz3kj8z2slJrzyUybjxFBJovChCMTodH3qMsUJYPLcFEMXsrIn2sMDE2o5ktgchsJt58Aoukflr2Lsrnd2elyvU0nTwcwCGcgAeXUIFbqEINCAh4gVd4\nx0\n1\n<latexit sha1_base64=\"5Czkd7R7CmnEYhIs4hSFH8z18=\">AB/HicbVDLTgJBEOzF+IL9ehlIjHxRHaNryPRi0dM5JHAhvQOszAyO7uZmTUhBL/Bq56\n9Ga/+i0f/xAH2IGAlnVSqutPdFSCa+O6305uZXVtfSO/Wdja3tndK+4f1HWcKspqNBaxagaomeCS1Qw3gjUTxTAKBGsEg9uJ3hiSvNYPphwvwIe5KHnKxUr0tMRDYKZbcsjsFWSZeRkqQodop/rS7MU0jJg0VqHXLcxPj1AZTgUbF9qpZgnSAfZYy1KJEdP+aHrtmJxYpUvCWNmShkzVvxMjLQeRoHtjND09aI3Ef/zWqk\nJr/0Rl0lqmKSzRWEqiInJ5HXS5YpRI4aWIFXc3kpoHxVSYwOa2xJEY5uJt5jAMqmflb3L8sX9ealyk6WThyM4hlPw4AoqcAdVqAGFR3iBV3hznp1358P5nLXmnGzmEObgfP0CRDWVsA=</latexit>r\n<latexit sha1_base64=\"9Znxn6VDIRyBDLkDowM/rKGyYE=\">ACnicbVC7TsNAEDzDOFlQkljESFRTbiVUbQUAaJPKTYis6XTXLKnW3drRGRlT/gG2i\nhpkO0/AQlf8IlcUESRlpdmZXu5owEVyj635bK6tr6xubha3i9s7u3r59UGroOFUM6iwWsWqFVIPgEdSRo4BWoDKUEAzHN5O/OYjKM3j6AFHCQS9iPe4yikTp2yQ+lD4nmwnQ+DgBpxy67FXcKZ5l4OSmTHLWO/eN3Y5ZKiJAJqnXbcxMqQMwHjop9qSCgb0j60DY2oB1k09/HzolRuk4vVqYidKbq342MSq1HMjSTkuJ\nAL3oT8T+vnWLvOsh4lKQIEZsd6qXCwdiZBOF0uQKGYmQIZYqbXx02oIoyNHNXQnl2GTiLSawTBpnFe+ycnF/Xq7e5OkUyBE5JqfEI1ekSu5IjdQJI0/khbySN+vZerc+rM/Z6IqV7xySOVhfv4VEm1E=</latexit>\u270f\u2713\n<latexit sha1_base64=\"v0M8p7MtIqExe6ySO16lPjDBlG8=\">ACAHicbVC7TgMxENzjGcIrQEljEaFQRXeIVxlBQxk8hDJKfI5vsSK7TvZPkR0SsM30EJNh2j5E0r+BCe5giSMtNJoZle7O0HMmTau+0\nIeIFXeHOenXfnw/mcti452cwBzMD5+gWn4Jb3</latexit>sLa+srq3nNvKbW9s7u4W9/bqOEkVojUQ8Us0Aa8qZpDXDKfNWFEsAk4bweBm7DceqdIskvdmGFNf4J5kISPYWOmhHYj0aVQqdbxOoeiW3QnQIvEyUoQM1U7hp92NSCKoNIRjrVueGxs/xcowuko3040jTEZ4B5tWSqxoNpPJxeP0LFVuiMlC1p0ET9O5FiofVQBLZTYNPX895Y/M9rJSa8lMm48RQSaLwoQjE6Hx+6jLFCWGDy3BRDF7KyJ9rDAxNqSZLYEY2Uy8+QWSf207F2Uz+/OipXrLJ0cHMIRnIAHl1CBW6hCDQh\nx00\n1\n<latexit sha1_base64=\"dgtcBP7Ua97NSC73SABWXJ+WYpc=\">AB/3icbVC5TgMxEJ0NVwhXgJLGIkJQRbuIq4ygoQwSOVCyiryON7Fie1e2FxGtuAbaKGmQ7R8CiV/gnMUJOFJIz29N6OZeUHMmTau+3k\nc56d+fD+Zy05pzpzD7MwPn6BUJolsY=</latexit>lpZXVtfy64WNza3tneLuXl1HiSK0RiIeqWaANeVM0phtNmrCgWAaeNYHAz8huPVGkWyXszjKkvcE+ykBFsrPTQDkT6lB13vE6x5JbdMdAi8akBFNUO8WfdjciaDSEI61bnlubPwUK8MIp1mhnWgaYzLAPdqyVGJBtZ+OD87QkVW6KIyULWnQWP07kWKh9VAEtlNg09fz3kj8z2slJrzyUybjxFBJovChCMTodH3qMsUJYPLcFEMXsrIn2sMDE2o5ktgchsJt58Aoukflr2Lsrnd2elyvU0nTwcwCGcgAeXUIFbqEINCAh4gVd4\nx0\n1\n<latexit sha1_base64=\"iMdZqhDp38bGN3XTktBm/qsP9Do=\">AB6nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2OIF48R84JkCbOT3mTI7OwyMyuEkE/w4kE\nRr36RN/GSbIHTSxoKq6e4KEsG1cd1vJ7e2vrG5ld8u7Ozu7R8UD4+aOk4VwaLRazaAdUouMSG4UZgO1FIo0BgKxjdzfzWEyrNY1k34wT9iA4kDzmjxkqP1V69Vy5ZXcOskq8jJQgQ61X/Or2Y5ZGKA0TVOuO5ybGn1BlOBM4LXRTjQlIzrAjqWSRqj9yfzUKTmzSp+EsbIlDZmrvycmNJ6HAW2M6JmqJe9mfif10lNeOt\nPuExSg5ItFoWpICYms79JnytkRowtoUxeythQ6oMzadg3BW35lTQvyt51+erhslSpZnHk4QRO4Rw8uIEK3EMNGsBgAM/wCm+OcF6cd+dj0Zpzsplj+APn8wfxqY2X</latexit>BT\nDOODL Backprop\n<latexit sha1_base64=\"iMdZqhDp3\n8bGN3XTktBm/qsP9Do=\">AB6nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2OI\nF48R84JkCbOT3mTI7OwyMyuEkE/w4kERr36RN/GSbIHTSxoKq6e4KEsG1c\nd1vJ7e2vrG5ld8u7Ozu7R8UD4+aOk4VwaLRazaAdUouMSG4UZgO1FIo0BgKx\njdzfzWEyrNY1k34wT9iA4kDzmjxkqP1V69Vy5ZXcOskq8jJQgQ61X/Or2Y5Z\nGKA0TVOuO5ybGn1BlOBM4LXRTjQlIzrAjqWSRqj9yfzUKTmzSp+EsbIlDZmr\nvycmNJ6HAW2M6JmqJe9mfif10lNeOtPuExSg5ItFoWpICYms79JnytkRowtoU\nxeythQ6oMzadg3BW35lTQvyt51+erhslSpZnHk4QRO4Rw8uIEK3EMNGsB\ngAM/wCm+OcF6cd+dj0Zpzsplj+APn8wfxqY2X</latexit>BT\n<latexit sha1_base64=\"5Czkd7R7CmnEYhIs4hSFH8z18=\">AB/HicbV\nDLTgJBEOzF+IL9ehlIjHxRHaNryPRi0dM5JHAhvQOszAyO7uZmTUhBL/Bq569Ga/+i0f/xAH2IGAlnVSqutPdFSCa+O6305uZXVtfSO/Wdja3tndK+4f1HWc\nKspqNBaxagaomeCS1Qw3gjUTxTAKBGsEg9uJ3hiSvNYPphwvwIe5KHnKxUr0tMRDYKZbcsjsFWSZeRkqQodop/rS7MU0jJg0VqHXLcxPj1AZTgUbF9qpZg\nnSAfZYy1KJEdP+aHrtmJxYpUvCWNmShkzVvxMjLQeRoHtjND09aI3Ef/zWqkJr/0Rl0lqmKSzRWEqiInJ5HXS5YpRI4aWIFXc3kpoHxVSYwOa2xJEY5uJt5jA\nMqmflb3L8sX9ealyk6WThyM4hlPw4AoqcAdVqAGFR3iBV3hznp1358P5nLXmnGzmEObgfP0CRDWVsA=</latexit>r\n<latexit sha1_base64=\"mIAYoI9nO5A7G5LG1UVGCTqBXU=\">ACAXicbVC7SgNBFL0bXzG+opY2i0GwCrviqwzaWFhEMA/YLGF2MkmGzGOZmRXCkspvsNXaTmz9Ekv/xNlkC5N4\nYOBwzr3cMyeKGdXG876dwsrq2vpGcbO0tb2zu1feP2hqmShMGlgyqdoR0oRQRqGkbasSKIR4y0otFt5reiNJUikczjknI0UDQPsXIWCnocGSGLH0ftItV7yqN4W7TPycVCBHvVv+6fQkTjgRBjOkdeB7sQlTpAzFjExKnUSTGOERGpDAUoE40WE6jTxT6zSc/tS2SeMO1X/bqSIaz3mkZ3MIupFLxP/84LE9K/DlIo4MUTg2aF+wlwj3ez/bo8qg0bW4Kwojari4dIWx\nsS3NXIp514i82sEyaZ1X/snrxcF6p3eTtFOEIjuEUfLiCGtxBHRqAQcILvMKb8+y8Ox/O52y04OQ7hzAH5+sXZgmYAg=</latexit>L\n<latexit sha1_base64=\"p/zZUtXUnmaWgdOFWVx/x7XIJK4=\">AB+XicbVA9TwJBEJ3DL8Qv1NJmIzGxInfGr5JgY4nRAxK4kL1lDzbs7l1290zIhZ9gq7W\ndsfXWPpPXOAKAV8yct7M5mZFyacaeO6305hbX1jc6u4XdrZ3ds/KB8eNXWcKkJ9EvNYtUOsKWeS+oYZTtuJoliEnLbC0d3Ubz1TpVksn8w4oYHA8kiRrCx0mO95/bKFbfqzoBWiZeTCuRo9Mo/3X5MUkGlIRxr3fHcxAQZVoYRTielbqpgskID2jHUokF1UE2O3WCzqzSR1GsbEmDZurfiQwLrcitJ0Cm6Fe9qbif14nNdF\ntkDGZpIZKMl8UpRyZGE3/Rn2mKDF8bAkmitlbERlihYmx6SxsCcXEZuItJ7BKmhdV7p69XBZqdXzdIpwAqdwDh7cQA3uoQE+EBjAC7zCm5M5786H8zlvLTj5zDEswPn6BWV+lAk=</latexit>B0\n<latexit sha1_base64=\"9Znxn6VDIRyBDLkDowM/rKGyYE=\">ACnicbV\nC7TsNAEDzDOFlQkljESFRTbiVUbQUAaJPKTYis6XTXLKnW3drRGRlT/gG2ihpkO0/AQlf8IlcUESRlpdmZXu5owEVyj635bK6tr6xubha3i9s7u3r59UGro\nOFUM6iwWsWqFVIPgEdSRo4BWoDKUEAzHN5O/OYjKM3j6AFHCQS9iPe4yikTp2yQ+lD4nmwnQ+DgBpxy67FXcKZ5l4OSmTHLWO/eN3Y5ZKiJAJqnXbcxMq\nqQMwHjop9qSCgb0j60DY2oB1k09/HzolRuk4vVqYidKbq342MSq1HMjSTkuJAL3oT8T+vnWLvOsh4lKQIEZsd6qXCwdiZBOF0uQKGYmQIZYqbXx02oIoyNHN\nXQnl2GTiLSawTBpnFe+ycnF/Xq7e5OkUyBE5JqfEI1ekSu5IjdQJI0/khbySN+vZerc+rM/Z6IqV7xySOVhfv4VEm1E=</latexit>\u270f\u2713\n<latexit sha1_base64=\"v0M8p7MtIqExe6ySO16lPjDBlG8=\">ACAHicbVC7TgMxENzjGcIrQElj\nEaFQRXeIVxlBQxk8hDJKfI5vsSK7TvZPkR0SsM30EJNh2j5E0r+BCe5giSMtNJoZle7O0HMmTau+0sLa+srq3nNvKbW9s7u4W9/bqOEkVojUQ8Us0Aa8qZpDXDKfNWFEsAk4bweBm7DceqdIskvdmGFNf4J\n+6jLFCWGDy3BRDF7KyJ9rDAxNqSZLYEY2Uy8+QWSf207F2Uz+/OipXrLJ0cHMIRnIAHl1CBW6hCDQhIeIFXeHOenXfnw/mcti452cwBzMD5+gWn4Jb3</latexit>5kISPYWOmhHYj0aVQqdbxOoeiW3QnQIvEyUoQM1U7hp92NSCKoNIRjrVueGxs/xcowuko3040jTEZ4B5tWSqxoNpPJxeP0LFVuiMlC1p0ET9O5FiofVQBLZTYNPX895Y/M9rJSa8lMm48RQSaLwoQjE6Hx\nx00\n1\n<latexit sha1_base64=\"dgtcBP7Ua97NSC73SABWXJ+WYpc=\">AB/3icbVC5TgMxEJ0NVwhXgJLG\nIkJQRbuIq4ygoQwSOVCyiryON7Fie1e2FxGtuAbaKGmQ7R8CiV/gnMUJOFJIz29N6OZeUHMmTau+3klpZXVtfy64WNza3tneLuXl1HiSK0RiIeqWaANeVM0phtNmrCgWAaeNYHAz8huPVGkWyXszjKkvcE+y\nJYPLcFEMXsrIn2sMDE2o5ktgchsJt58Aoukflr2Lsrnd2elyvU0nTwcwCGcgAeXUIFbqEINCAh4gVd4c56d+fD+Zy05pzpzD7MwPn6BUJolsY=</latexit>kBFsrPTQDkT6lB13vE6x5JbdMdAi8akBFNUO8WfdjciaDSEI61bnlubPwUK8MIp1mhnWgaYzLAPdqyVGJBtZ+OD87QkVW6KIyULWnQWP07kWKh9VAEtlNg09fz3kj8z2slJrzyUybjxFBJovChCMTodH3qMsU\nx0\n1\n<latexit sha1_base64=\"6k1OAPZsTt7vM0y1gQ+vlx0FfY=\">ACAHicbVC7TgMxENzjGcIrQEljEaFQRXeIVxlBQxk8hDJKfI5vsSK7TvZPkR0SsM30EJNh2j5E0r+BCe5giSMtNJoZle7O0HMmTau+\n6hCDQhIeIFXeHOenXfnw/mcti452cwBzMD5+gWmTZb2</latexit>+0sLa+srq3nNvKbW9s7u4W9/bqOEkVojUQ8Us0Aa8qZpDXDKfNWFEsAk4bweBm7DceqdIskvdmGFNf4J5kISPYWOmhHYj0aVQqdxOoeiW3QnQIvEyUoQM1U7hp92NSCKoNIRjrVueGxs/xcowuko3040jTEZ4B5tWSqxoNpPJxeP0LFVuiMlC1p0ET9O5FiofVQBLZTYNPX895Y/M9rJSa8lMm48RQSaLwoQjE6Hx+6jLFCWGDy3BRDF7KyJ9rDAxNqSZLYEY2Uy8+QWSf207F2Uz+/OipXrLJ0cHMIRnIAHl1CBW\nx00\n0\n<latexit sha1_base64=\"xXnvqBfbteQYNecSf5rTsEKlqI=\">AB/3icbVC5TgMxEJ0NVwhXgJLGIkJQRbuIq4ygoQwSOVCyiryON7Fie1e2FxGtuAbaKGmQ7R8CiV/gnMUJOFJIz29N6OZeU\nU0nTwcwCGcgAeXUIFbqEINCAh4gVd4c56d+fD+Zy05pzpzD7MwPn6BUDVlsU=</latexit>HMmTau+3klpZXVtfy64WNza3tneLuXl1HiSK0RiIeqWaANeVM0phtNmrCgWAaeNYHAz8huPVGkWyXszjKkvcE+ykBFsrPTQDkT6lB13E6x5JbdMdAi8akBFNUO8WfdjciaDSEI61bnlubPwUK8MIp1mhnWgaYzLAPdqyVGJBtZ+OD87QkVW6KIyULWnQWP07kWKh9VAEtlNg09fz3kj8z2slJrzyUybjxFBJovChCMTodH3qMsUJYPLcFEMXsrIn2sMDE2o5ktgchsJt58Aoukflr2Lsrnd2elyv\nx0\n0\n<latexit sha1_base64=\"hEn2BRjdb\nNmEp6y6RvUNCOlufSM=\">AB6nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PU\ni8eIeUGyhNnJBkyO7vM9AphySd48aCIV7/Im3/jJNmDJhY0FXdHcFsRQGX\nfbya2srq1v5DcLW9s7u3vF/YOGiRLNeJ1FMtKtgBouheJ1FCh5K9achoHkzW\nB0N/WbT1wbEakajmPuh3SgRF8wilZ6vOnWusWSW3ZnIMvEy0gJMlS7xa9OL2J\nJyBUySY1pe26Mfko1Cib5pNBJDI8pG9EBb1uqaMiNn85OnZATq/RIP9K2FJKZ\n+nsipaEx4zCwnSHFoVn0puJ/XjvB/rWfChUnyBWbL+onkmBEpn+TntCcoRxbQp\nkW9lbChlRThjadg3BW3x5mTOyt5l+eLhvFS5zeLIwxEcwyl4cAUVuIcq1IH\nBAJ7hFd4c6bw4787HvDXnZDOH8AfO5w/wI42W</latexit>AT\n<latexit sha1_base64=\"mIAYoI9nO5A7G5LG1UVGCTqBXU=\">ACAXicbVC7SgNBFL0bXzG+opY2i0GwCrviqwzaWFhEMA/YLGF2MkmGzGOZmRXCkspvsNXaTmz9Ekv/xNlkC5N4\nYOBwzr3cMyeKGdXG876dwsrq2vpGcbO0tb2zu1feP2hqmShMGlgyqdoR0oRQRqGkbasSKIR4y0otFt5reiNJUikczjknI0UDQPsXIWCnocGSGLH0ftItV7yqN4W7TPycVCBHvVv+6fQkTjgRBjOkdeB7sQlTpAzFjExKnUSTGOERGpDAUoE40WE6jTxT6zSc/tS2SeMO1X/bqSIaz3mkZ3MIupFLxP/84LE9K/DlIo4MUTg2aF+wlwj3ez/bo8qg0bW4Kwojari4dIWx\nsS3NXIp514i82sEyaZ1X/snrxcF6p3eTtFOEIjuEUfLiCGtxBHRqAQcILvMKb8+y8Ox/O52y04OQ7hzAH5+sXZgmYAg=</latexit>L\n<latexit sha1_base64=\"9Znxn6VDIRyBDLkDowM/rKGyYE=\">ACnicbV\nC7TsNAEDzDOFlQkljESFRTbiVUbQUAaJPKTYis6XTXLKnW3drRGRlT/gG2ihpkO0/AQlf8IlcUESRlpdmZXu5owEVyj635bK6tr6xubha3i9s7u3r59UGro\nOFUM6iwWsWqFVIPgEdSRo4BWoDKUEAzHN5O/OYjKM3j6AFHCQS9iPe4yikTp2yQ+lD4nmwnQ+DgBpxy67FXcKZ5l4OSmTHLWO/eN3Y5ZKiJAJqnXbcxMq\nqQMwHjop9qSCgb0j60DY2oB1k09/HzolRuk4vVqYidKbq342MSq1HMjSTkuJAL3oT8T+vnWLvOsh4lKQIEZsd6qXCwdiZBOF0uQKGYmQIZYqbXx02oIoyNHN\nXQnl2GTiLSawTBpnFe+ycnF/Xq7e5OkUyBE5JqfEI1ekSu5IjdQJI0/khbySN+vZerc+rM/Z6IqV7xySOVhfv4VEm1E=</latexit>\u270f\u2713\n<latexit sha1_base64=\"9Znxn6VDIRyBDLkDowM/rKGyYE=\">ACnicbV\nC7TsNAEDzDOFlQkljESFRTbiVUbQUAaJPKTYis6XTXLKnW3drRGRlT/gG2ihpkO0/AQlf8IlcUESRlpdmZXu5owEVyj635bK6tr6xubha3i9s7u3r59UGro\nOFUM6iwWsWqFVIPgEdSRo4BWoDKUEAzHN5O/OYjKM3j6AFHCQS9iPe4yikTp2yQ+lD4nmwnQ+DgBpxy67FXcKZ5l4OSmTHLWO/eN3Y5ZKiJAJqnXbcxMq\nqQMwHjop9qSCgb0j60DY2oB1k09/HzolRuk4vVqYidKbq342MSq1HMjSTkuJAL3oT8T+vnWLvOsh4lKQIEZsd6qXCwdiZBOF0uQKGYmQIZYqbXx02oIoyNHN\nXQnl2GTiLSawTBpnFe+ycnF/Xq7e5OkUyBE5JqfEI1ekSu5IjdQJI0/khbySN+vZerc+rM/Z6IqV7xySOVhfv4VEm1E=</latexit>\u270f\u2713\n<latexit sha1_base64=\"9Znxn6VDIRyBDLkDowM/rKGyYE=\">ACnicbV\nC7TsNAEDzDOFlQkljESFRTbiVUbQUAaJPKTYis6XTXLKnW3drRGRlT/gG2ihpkO0/AQlf8IlcUESRlpdmZXu5owEVyj635bK6tr6xubha3i9s7u3r59UGro\nOFUM6iwWsWqFVIPgEdSRo4BWoDKUEAzHN5O/OYjKM3j6AFHCQS9iPe4yikTp2yQ+lD4nmwnQ+DgBpxy67FXcKZ5l4OSmTHLWO/eN3Y5ZKiJAJqnXbcxMq\nqQMwHjop9qSCgb0j60DY2oB1k09/HzolRuk4vVqYidKbq342MSq1HMjSTkuJAL3oT8T+vnWLvOsh4lKQIEZsd6qXCwdiZBOF0uQKGYmQIZYqbXx02oIoyNHN\nXQnl2GTiLSawTBpnFe+ycnF/Xq7e5OkUyBE5JqfEI1ekSu5IjdQJI0/khbySN+vZerc+rM/Z6IqV7xySOVhfv4VEm1E=</latexit>\u270f\u2713\n<latexit sha1_base64=\"BJ8DbCKNBsr/M/F\nmJXiOm8D58PQ=\">ACAHicbVC7TsNAEFyHVwivACWNRYRCFdmIVxlBQxmkvERiRefLOTnl7mz\ndnRGR5YZvoIWaDtHyJ5T8CZfEBUkYaXRzK52d/yIUaUd59vKrayurW/kNwtb2zu7e8X9g6YKY\n4lJA4cslG0fKcKoIA1NSPtSBLEfUZa/uh24rceiVQ0FHU9jojH0UDQgGKkjfTQ9XnylJbLvXq\nvWHIqzhT2MnEzUoIMtV7xp9sPcyJ0JghpTquE2kvQVJTzEha6MaKRAiP0IB0DBWIE+Ul04tT+\n8QofTsIpSmh7an6dyJBXKkx90nR3qoFr2J+J/XiXVw7SVURLEmAs8WBTGzdWhP3rf7VBKs2dg\nTfr2Xq3PqzPWvOymYOYQ7W1y/e+Zca</latexit>QhCU1t9p4iCTC2oQ0t8XnqcnEXUxgmTPKu5l5eL+vFS9ydLJwxEcwym4cAVuIMaNACDgBd4h\nx00\nT\n<latexit sha1_base64=\"XIETZpkCMT+ls/q\nmQtLFEjpDWcI=\">AB/3icbVC7TgMxENzjGcIrQEljESGojvEq4ygoQxSXig5RT7Hl1ixfSf\nbh4hOKfgGWqjpEC2fQsmf4CRXkISRVhrN7Gp3J4g508Z1v52l5ZXVtfXcRn5za3tnt7C3X9dRo\ngitkYhHqhlgTmTtGaY4bQZK4pFwGkjGNyO/cYjVZpFsmqGMfUF7kWMoKNlR7agUifRiedaqd\nQdEvuBGiReBkpQoZKp/DT7kYkEVQawrHWLc+NjZ9iZRjhdJRvJ5rGmAxwj7YslVhQ7aeTg0fo2\nCpdFEbKljRov6dSLHQeigC2ymw6et5byz+57USE17KZNxYqgk0VhwpGJ0Ph71GWKEsOHlmC\nc/Ou/PhfE5bl5xs5gBm4Hz9AnmBluk=</latexit>imL0VkT5WmBib0cyWQIxsJt58AoukflbyLksX9+fF8k2WTg4O4QhOwYMrKMdVKAGBAS8wCu8O\nx0\nT\n<latexit sha1_base64=\"aNJv+jnZ6TankgM34GAN93ymJ6Q=\">AB/3icbVC7TgMxENzjGc\nIrQEljESGoruIVxlBQxk8kDJKfI5TmLF9p1sHyI6XcE30EJNh2j5FEr+BCe5giSMtNJoZle7O0HEmTau+0sLa+srq3nNvKbW9s7u4W9/boOY0VojYQ8VM0Aa8qZpDXDKfNSFEsAk4bwfBm7\nDceqdIslPdmFf4L5kPUawsdJDOxDJU3rSKXcKRbfkToAWiZeRImSodgo/7W5IYkGlIRxr3fLcyPgJVoYRTtN8O9Y0wmSI+7RlqcSCaj+ZHJyiY6t0US9UtqRBE/XvRIKF1iMR2E6BzUDPe2PxP\ntexit>68Vm96VnzAZxYZKMl3UizkyIRp/j7pMUWL4yBJMFLO3IjLAChNjM5rZEojUZuLNJ7BI6uWSd1E6vzsrVq6zdHJwCEdwCh5cQgVuoQo1ICDgBV7hzXl23p0P53PauRkMwcwA+frF0P7lsc=</la\nx0\n2\n<latexit sha1_base64=\"sMvIxUPnF2+h4DnoxSxOQS57UJM=\">ACAHicbVC7TsNAEFyHVw\nivACWNRYRCFdkRrzKChjJI5CESKzpfzskpd2fr7oyILDd8Ay3UdIiWP6HkT7gkLkjCSCuNZna1u+NHjCrtON9WbmV1bX0jv1nY2t7Z3SvuHzRVGEtMGjhkoWz7SBFGBWloqhlpR5Ig7jPS8kc3E\n7/1SKSiobjX4h4HA0EDShG2kgPXZ8nT2m53Kv2iWn4kxhLxM3IyXIUO8Vf7r9EMecCI0ZUqrjOpH2EiQ1xYykhW6sSITwCA1Ix1CBOFeMr04tU+M0reDUJoS2p6qfycSxJUac90cqSHatGbi\n</latexit>P95nVgHV15CRrIvBsURAzW4f25H27TyXBmo0NQVhSc6uNh0girE1Ic1t8npM3MUElkmzWnEvKud3Z6XadZOHo7gGE7BhUuowS3UoQEYBLzAK7xZz9a79WF9zlpzVjZzCHOwvn4BqXOW+A=\nx00\n2\n<latexit sha1_base64=\"v0M8p7MtIqExe6ySO16lPjDBlG8=\">ACAHicbVC7TgMxENzjGcIrQElj\nEaFQRXeIVxlBQxk8hDJKfI5vsSK7TvZPkR0SsM30EJNh2j5E0r+BCe5giSMtNJoZle7O0HMmTau+0sLa+srq3nNvKbW9s7u4W9/bqOEkVojUQ8Us0Aa8qZpDXDKfNWFEsAk4bweBm7DceqdIskvdmGFNf4J\n+6jLFCWGDy3BRDF7KyJ9rDAxNqSZLYEY2Uy8+QWSf207F2Uz+/OipXrLJ0cHMIRnIAHl1CBW6hCDQhIeIFXeHOenXfnw/mcti452cwBzMD5+gWn4Jb3</latexit>5kISPYWOmhHYj0aVQqdbxOoeiW3QnQIvEyUoQM1U7hp92NSCKoNIRjrVueGxs/xcowuko3040jTEZ4B5tWSqxoNpPJxeP0LFVuiMlC1p0ET9O5FiofVQBLZTYNPX895Y/M9rJSa8lMm48RQSaLwoQjE6Hx\nx00\n1\n<latexit sha1_base64=\"dgtcBP7Ua97NSC73SABWXJ+WYpc=\">AB/3icbVC5TgMxEJ0NVwhXgJLG\nIkJQRbuIq4ygoQwSOVCyiryON7Fie1e2FxGtuAbaKGmQ7R8CiV/gnMUJOFJIz29N6OZeUHMmTau+3klpZXVtfy64WNza3tneLuXl1HiSK0RiIeqWaANeVM0phtNmrCgWAaeNYHAz8huPVGkWyXszjKkvcE+y\nJYPLcFEMXsrIn2sMDE2o5ktgchsJt58Aoukflr2Lsrnd2elyvU0nTwcwCGcgAeXUIFbqEINCAh4gVd4c56d+fD+Zy05pzpzD7MwPn6BUJolsY=</latexit>kBFsrPTQDkT6lB13vE6x5JbdMdAi8akBFNUO8WfdjciaDSEI61bnlubPwUK8MIp1mhnWgaYzLAPdqyVGJBtZ+OD87QkVW6KIyULWnQWP07kWKh9VAEtlNg09fz3kj8z2slJrzyUybjxFBJovChCMTodH3qMsU\nx0\n1\n<latexit sha1_base64=\"xXnvqBfbteQYNecSf5rTsEKlqI=\">AB/3icbVC5TgMxEJ0NVwhXgJLGIkJQRbuIq4ygoQwSOVCyiryON7Fie1e2FxGtuAbaKGmQ7R8CiV/gnMUJOFJIz29N6OZeU\nU0nTwcwCGcgAeXUIFbqEINCAh4gVd4c56d+fD+Zy05pzpzD7MwPn6BUDVlsU=</latexit>HMmTau+3klpZXVtfy64WNza3tneLuXl1HiSK0RiIeqWaANeVM0phtNmrCgWAaeNYHAz8huPVGkWyXszjKkvcE+ykBFsrPTQDkT6lB13E6x5JbdMdAi8akBFNUO8WfdjciaDSEI61bnlubPwUK8MIp1mhnWgaYzLAPdqyVGJBtZ+OD87QkVW6KIyULWnQWP07kWKh9VAEtlNg09fz3kj8z2slJrzyUybjxFBJovChCMTodH3qMsUJYPLcFEMXsrIn2sMDE2o5ktgchsJt58Aoukflr2Lsrnd2elyv\nx0\n0\n<latexit sha1_base64=\"6k1OAPZsTt7vM0y1gQ+vlx0FfY=\">ACAHicbVC7TgMxENzjGcIrQEljEaFQRXeIVxlBQxk8hDJKfI5vsSK7TvZPkR0SsM30EJNh2j5E0r+BCe5giSMtNJoZle7O0HMmTau+\n6hCDQhIeIFXeHOenXfnw/mcti452cwBzMD5+gWmTZb2</latexit>+0sLa+srq3nNvKbW9s7u4W9/bqOEkVojUQ8Us0Aa8qZpDXDKfNWFEsAk4bweBm7DceqdIskvdmGFNf4J5kISPYWOmhHYj0aVQqdxOoeiW3QnQIvEyUoQM1U7hp92NSCKoNIRjrVueGxs/xcowuko3040jTEZ4B5tWSqxoNpPJxeP0LFVuiMlC1p0ET9O5FiofVQBLZTYNPX895Y/M9rJSa8lMm48RQSaLwoQjE6Hx+6jLFCWGDy3BRDF7KyJ9rDAxNqSZLYEY2Uy8+QWSf207F2Uz+/OipXrLJ0cHMIRnIAHl1CBW\nx00\n0\n<latexit sha1_base64=\"5Czkd7R7CmnEYhIs4hSFH8z18=\">AB/HicbV\nDLTgJBEOzF+IL9ehlIjHxRHaNryPRi0dM5JHAhvQOszAyO7uZmTUhBL/Bq569Ga/+i0f/xAH2IGAlnVSqutPdFSCa+O6305uZXVtfSO/Wdja3tndK+4f1HWc\nKspqNBaxagaomeCS1Qw3gjUTxTAKBGsEg9uJ3hiSvNYPphwvwIe5KHnKxUr0tMRDYKZbcsjsFWSZeRkqQodop/rS7MU0jJg0VqHXLcxPj1AZTgUbF9qpZg\nnSAfZYy1KJEdP+aHrtmJxYpUvCWNmShkzVvxMjLQeRoHtjND09aI3Ef/zWqkJr/0Rl0lqmKSzRWEqiInJ5HXS5YpRI4aWIFXc3kpoHxVSYwOa2xJEY5uJt5jA\nMqmflb3L8sX9ealyk6WThyM4hlPw4AoqcAdVqAGFR3iBV3hznp1358P5nLXmnGzmEObgfP0CRDWVsA=</latexit>r\n<latexit sha1_base64=\"9Znxn6VDIRyBDLkDowM/rKGyYE=\">ACnicbV\nC7TsNAEDzDOFlQkljESFRTbiVUbQUAaJPKTYis6XTXLKnW3drRGRlT/gG2ihpkO0/AQlf8IlcUESRlpdmZXu5owEVyj635bK6tr6xubha3i9s7u3r59UGro\nOFUM6iwWsWqFVIPgEdSRo4BWoDKUEAzHN5O/OYjKM3j6AFHCQS9iPe4yikTp2yQ+lD4nmwnQ+DgBpxy67FXcKZ5l4OSmTHLWO/eN3Y5ZKiJAJqnXbcxMq\nqQMwHjop9qSCgb0j60DY2oB1k09/HzolRuk4vVqYidKbq342MSq1HMjSTkuJAL3oT8T+vnWLvOsh4lKQIEZsd6qXCwdiZBOF0uQKGYmQIZYqbXx02oIoyNHN\nXQnl2GTiLSawTBpnFe+ycnF/Xq7e5OkUyBE5JqfEI1ekSu5IjdQJI0/khbySN+vZerc+rM/Z6IqV7xySOVhfv4VEm1E=</latexit>\u270f\u2713\n<latexit sha1_base64=\"v0M8p7MtIqExe6ySO16lPjDBlG8=\">ACAHicbVC7TgMxENzjGcIrQElj\nEaFQRXeIVxlBQxk8hDJKfI5vsSK7TvZPkR0SsM30EJNh2j5E0r+BCe5giSMtNJoZle7O0HMmTau+0sLa+srq3nNvKbW9s7u4W9/bqOEkVojUQ8Us0Aa8qZpDXDKfNWFEsAk4bweBm7DceqdIskvdmGFNf4J\n+6jLFCWGDy3BRDF7KyJ9rDAxNqSZLYEY2Uy8+QWSf207F2Uz+/OipXrLJ0cHMIRnIAHl1CBW6hCDQhIeIFXeHOenXfnw/mcti452cwBzMD5+gWn4Jb3</latexit>5kISPYWOmhHYj0aVQqdbxOoeiW3QnQIvEyUoQM1U7hp92NSCKoNIRjrVueGxs/xcowuko3040jTEZ4B5tWSqxoNpPJxeP0LFVuiMlC1p0ET9O5FiofVQBLZTYNPX895Y/M9rJSa8lMm48RQSaLwoQjE6Hx\nx00\n1\n<latexit sha1_base64=\"dgtcBP7Ua97NSC73SABWXJ+WYpc=\">AB/3icbVC5TgMxEJ0NVwhXgJLG\nIkJQRbuIq4ygoQwSOVCyiryON7Fie1e2FxGtuAbaKGmQ7R8CiV/gnMUJOFJIz29N6OZeUHMmTau+3klpZXVtfy64WNza3tneLuXl1HiSK0RiIeqWaANeVM0phtNmrCgWAaeNYHAz8huPVGkWyXszjKkvcE+y\nJYPLcFEMXsrIn2sMDE2o5ktgchsJt58Aoukflr2Lsrnd2elyvU0nTwcwCGcgAeXUIFbqEINCAh4gVd4c56d+fD+Zy05pzpzD7MwPn6BUJolsY=</latexit>kBFsrPTQDkT6lB13vE6x5JbdMdAi8akBFNUO8WfdjciaDSEI61bnlubPwUK8MIp1mhnWgaYzLAPdqyVGJBtZ+OD87QkVW6KIyULWnQWP07kWKh9VAEtlNg09fz3kj8z2slJrzyUybjxFBJovChCMTodH3qMsU\nx0\n1\n<latexit sha1_base64=\"aNJv+jnZ6TankgM34GAN93ymJ6Q=\">AB/3icbVC7TgMxENzjGc\nIrQEljESGoruIVxlBQxk8kDJKfI5TmLF9p1sHyI6XcE30EJNh2j5FEr+BCe5giSMtNJoZle7O0HEmTau+0sLa+srq3nNvKbW9s7u4W9/boOY0VojYQ8VM0Aa8qZpDXDKfNSFEsAk4bwfBm7\nDceqdIslPdmFf4L5kPUawsdJDOxDJU3rSKXcKRbfkToAWiZeRImSodgo/7W5IYkGlIRxr3fLcyPgJVoYRTtN8O9Y0wmSI+7RlqcSCaj+ZHJyiY6t0US9UtqRBE/XvRIKF1iMR2E6BzUDPe2PxP\ntexit>68Vm96VnzAZxYZKMl3UizkyIRp/j7pMUWL4yBJMFLO3IjLAChNjM5rZEojUZuLNJ7BI6uWSd1E6vzsrVq6zdHJwCEdwCh5cQgVuoQo1ICDgBV7hzXl23p0P53PauRkMwcwA+frF0P7lsc=</la\nx0\n2\n<latexit sha1_base64=\"sMvIxUPnF2+h4DnoxSxOQS57UJM=\">ACAHicbVC7TsNAEFyHVw\nivACWNRYRCFdkRrzKChjJI5CESKzpfzskpd2fr7oyILDd8Ay3UdIiWP6HkT7gkLkjCSCuNZna1u+NHjCrtON9WbmV1bX0jv1nY2t7Z3SvuHzRVGEtMGjhkoWz7SBFGBWloqhlpR5Ig7jPS8kc3E\n7/1SKSiobjX4h4HA0EDShG2kgPXZ8nT2m53Kv2iWn4kxhLxM3IyXIUO8Vf7r9EMecCI0ZUqrjOpH2EiQ1xYykhW6sSITwCA1Ix1CBOFeMr04tU+M0reDUJoS2p6qfycSxJUac90cqSHatGbi\n</latexit>P95nVgHV15CRrIvBsURAzW4f25H27TyXBmo0NQVhSc6uNh0girE1Ic1t8npM3MUElkmzWnEvKud3Z6XadZOHo7gGE7BhUuowS3UoQEYBLzAK7xZz9a79WF9zlpzVjZzCHOwvn4BqXOW+A=\nx00\n2\n<latexit sha1_base64=\"5Czkd7R7CmnEYhIs4hSFH8z18=\">AB/HicbV\nDLTgJBEOzF+IL9ehlIjHxRHaNryPRi0dM5JHAhvQOszAyO7uZmTUhBL/Bq569Ga/+i0f/xAH2IGAlnVSqutPdFSCa+O6305uZXVtfSO/Wdja3tndK+4f1HWc\nKspqNBaxagaomeCS1Qw3gjUTxTAKBGsEg9uJ3hiSvNYPphwvwIe5KHnKxUr0tMRDYKZbcsjsFWSZeRkqQodop/rS7MU0jJg0VqHXLcxPj1AZTgUbF9qpZg\nnSAfZYy1KJEdP+aHrtmJxYpUvCWNmShkzVvxMjLQeRoHtjND09aI3Ef/zWqkJr/0Rl0lqmKSzRWEqiInJ5HXS5YpRI4aWIFXc3kpoHxVSYwOa2xJEY5uJt5jA\nMqmflb3L8sX9ealyk6WThyM4hlPw4AoqcAdVqAGFR3iBV3hznp1358P5nLXmnGzmEObgfP0CRDWVsA=</latexit>r\n<latexit sha1_base64=\"9Znxn6VDIRyBDLkDowM/rKGyYE=\">ACnicbV\nC7TsNAEDzDOFlQkljESFRTbiVUbQUAaJPKTYis6XTXLKnW3drRGRlT/gG2ihpkO0/AQlf8IlcUESRlpdmZXu5owEVyj635bK6tr6xubha3i9s7u3r59UGro\nOFUM6iwWsWqFVIPgEdSRo4BWoDKUEAzHN5O/OYjKM3j6AFHCQS9iPe4yikTp2yQ+lD4nmwnQ+DgBpxy67FXcKZ5l4OSmTHLWO/eN3Y5ZKiJAJqnXbcxMq\nqQMwHjop9qSCgb0j60DY2oB1k09/HzolRuk4vVqYidKbq342MSq1HMjSTkuJAL3oT8T+vnWLvOsh4lKQIEZsd6qXCwdiZBOF0uQKGYmQIZYqbXx02oIoyNHN\nXQnl2GTiLSawTBpnFe+ycnF/Xq7e5OkUyBE5JqfEI1ekSu5IjdQJI0/khbySN+vZerc+rM/Z6IqV7xySOVhfv4VEm1E=</latexit>\u270f\u2713\n<latexit sha1_base64=\"aNJv+jnZ6TankgM34GAN93ymJ6Q=\">AB/3icbVC7TgMxENzjGc\nIrQEljESGoruIVxlBQxk8kDJKfI5TmLF9p1sHyI6XcE30EJNh2j5FEr+BCe5giSMtNJoZle7O0HEmTau+0sLa+srq3nNvKbW9s7u4W9/boOY0VojYQ8VM0Aa8qZpDXDKfNSFEsAk4bwfBm7\nDceqdIslPdmFf4L5kPUawsdJDOxDJU3rSKXcKRbfkToAWiZeRImSodgo/7W5IYkGlIRxr3fLcyPgJVoYRTtN8O9Y0wmSI+7RlqcSCaj+ZHJyiY6t0US9UtqRBE/XvRIKF1iMR2E6BzUDPe2PxP\ntexit>68Vm96VnzAZxYZKMl3UizkyIRp/j7pMUWL4yBJMFLO3IjLAChNjM5rZEojUZuLNJ7BI6uWSd1E6vzsrVq6zdHJwCEdwCh5cQgVuoQo1ICDgBV7hzXl23p0P53PauRkMwcwA+frF0P7lsc=</la\nx0\n2\n<latexit sha1_base64=\"sMvIxUPnF2+h4DnoxSxOQS57UJM=\">ACAHicbVC7TsNAEFyHVw\nivACWNRYRCFdkRrzKChjJI5CESKzpfzskpd2fr7oyILDd8Ay3UdIiWP6HkT7gkLkjCSCuNZna1u+NHjCrtON9WbmV1bX0jv1nY2t7Z3SvuHzRVGEtMGjhkoWz7SBFGBWloqhlpR5Ig7jPS8kc3E\n7/1SKSiobjX4h4HA0EDShG2kgPXZ8nT2m53Kv2iWn4kxhLxM3IyXIUO8Vf7r9EMecCI0ZUqrjOpH2EiQ1xYykhW6sSITwCA1Ix1CBOFeMr04tU+M0reDUJoS2p6qfycSxJUac90cqSHatGbi\n</latexit>P95nVgHV15CRrIvBsURAzW4f25H27TyXBmo0NQVhSc6uNh0girE1Ic1t8npM3MUElkmzWnEvKud3Z6XadZOHo7gGE7BhUuowS3UoQEYBLzAK7xZz9a79WF9zlpzVjZzCHOwvn4BqXOW+A=\nx00\n2\n<latexit sha1_base64=\"BJ8DbCKNBsr/M/F\nmJXiOm8D58PQ=\">ACAHicbVC7TsNAEFyHVwivACWNRYRCFdmIVxlBQxmkvERiRefLOTnl7mz\ndnRGR5YZvoIWaDtHyJ5T8CZfEBUkYaXRzK52d/yIUaUd59vKrayurW/kNwtb2zu7e8X9g6YKY\n4lJA4cslG0fKcKoIA1NSPtSBLEfUZa/uh24rceiVQ0FHU9jojH0UDQgGKkjfTQ9XnylJbLvXq\nvWHIqzhT2MnEzUoIMtV7xp9sPcyJ0JghpTquE2kvQVJTzEha6MaKRAiP0IB0DBWIE+Ul04tT+\n8QofTsIpSmh7an6dyJBXKkx90nR3qoFr2J+J/XiXVw7SVURLEmAs8WBTGzdWhP3rf7VBKs2dg\nTfr2Xq3PqzPWvOymYOYQ7W1y/e+Zca</latexit>QhCU1t9p4iCTC2oQ0t8XnqcnEXUxgmTPKu5l5eL+vFS9ydLJwxEcwym4cAVuIMaNACDgBd4h\nx00\nT\n<latexit sha1_base64=\"XIETZpkCMT+ls/q\nmQtLFEjpDWcI=\">AB/3icbVC7TgMxENzjGcIrQEljESGojvEq4ygoQxSXig5RT7Hl1ixfSf\nbh4hOKfgGWqjpEC2fQsmf4CRXkISRVhrN7Gp3J4g508Z1v52l5ZXVtfXcRn5za3tnt7C3X9dRo\ngitkYhHqhlgTmTtGaY4bQZK4pFwGkjGNyO/cYjVZpFsmqGMfUF7kWMoKNlR7agUifRiedaqd\nQdEvuBGiReBkpQoZKp/DT7kYkEVQawrHWLc+NjZ9iZRjhdJRvJ5rGmAxwj7YslVhQ7aeTg0fo2\nCpdFEbKljRov6dSLHQeigC2ymw6et5byz+57USE17KZNxYqgk0VhwpGJ0Ph71GWKEsOHlmC\nc/Ou/PhfE5bl5xs5gBm4Hz9AnmBluk=</latexit>imL0VkT5WmBib0cyWQIxsJt58AoukflbyLksX9+fF8k2WTg4O4QhOwYMrKMdVKAGBAS8wCu8O\nx0\nT\nFigure 5. Forward and Backward pass for DOODL, both in its official implementation and alternatively by using the EDICT invertible\nlayers. The standard DOODL backprop doubles the number of model calls due to the EDICT sampling, yet uses checkpointing to store\nfunction inputs for each timestep. When utilizing EDICT\u2019s invertibility, only the final outputs are stored in memory, yet the inversion\nprocess requires two more model passes per timestep during the backwards pass.\nC. Correlation-Based Intensity Control\nGiven the surprising poor control performance of Music ControlNet (Wu et al., 2023a) on the intensity control task despite\nbeing fully trained on such inputs, we investigated alternative metrics for understanding control adherence. Notably, we find\nthat Music ControlNet implicitly models the intensity correlation, paying more attention to the overall shape of the intensity\ncurve across time than the absolute dB values of the curve itself. We believe this makes sense, given the UNet backbone\nconvolution (correlation) layers are both scale and location invariant. Given this result, we can alternatively parameterize\nintensity control to directly optimize for correlation by setting L \u221d \u2212\u03c1(f(x0), y), or by maximizing the correlation between\nthe target and output intensity curves.\nTable 5. Intensity correlation results for Music ControlNet and DITTO with both the standard and correlation-based loss function. By\noptimizing for correlation instead of absolute intensity, we can match the correlation of Music ControlNet while improving audio quality\nand text relevance.\nMethod\nMSE (\u2193)\n\u03c1 (\u2191)\nFAD (\u2193)\nCLAP (\u2191)\nMusic ControlNet\n38.4108\n0.9413\n11.1315\n0.3084\nDITTO (L \u221d ||f(x0) \u2212 y||2\n2)\n4.7576\n0.6166\n10.5294\n0.4326\nDITTO (L \u221d \u2212\u03c1(f(x0), y))\n60.8952\n0.9040\n11.0858\n0.3503\nIn Table 5, we show both the absolute MSE and correlation \u03c1 values for Music ControlNet, DITTO, and DITTO with the\ncorrelation based loss function. Music ControlNet has exceptional performance for intensity correlation, while baseline\nDITTO unsurprisingly prioritizes absolute intensity over correlation given its optimization objective. By switching to the\ncorrelation objective, DITTO can nearly match the correlation performance of Music ControlNet, all the while maintaining\nsome of the absolute intensity DITTO\u2019s performance in audio quality and text relevance. This experiment shows how a\nsingle target feature can be parameterized in DITTO\u2019s flexible setup in multiple ways to change the intended behavior for\nrapid experimentation.\nD. DITTO for Real-Audio Inversion\nInversion, or the task of encoding real reference media xref into a generative model\u2019s latent space, is crucial for image\nand audio editing tasks (Song et al., 2020; Dhariwal & Nichol, 2021; Xia et al., 2021; Mokady et al., 2023). Past audio-\ndomain inversion work is very limited while past image-domain methods include naively adding noise to inputs (Song\net al., 2020), reversing the DDIM sampling process (Dhariwal & Nichol, 2021), and learning additional null-text parameters\nto improve inversion accuracy (Mokady et al., 2023). We use DITTO for the task of inversion by setting f(x0) = x0,\ny = xref, and the loss to be the MSE or L \u221d ||f(x0) \u2212 y||2\n2. Then, we can solve (2) to find an xT such that (3) will produce\nx0 that reconstructs the target reference media xref. While high-quality reconstruction is trivially possible with the fully\n15\nDITTO: Diffusion Inference-Time T -Optimization for Music Generation\ninvertible EDICT sampler (Wallace et al., 2023b), further editing with inverted content is complicated by its dual chain\nfully-deterministic sampling (Pan et al., 2023).\nFor generative text-conditioned models, a key factor of the inversion equation is the scale of the classifier-free guidance\nparameter, which helps improve controllability through text (Ho & Salimans, 2021), but noticeably makes the inversion\nprocess more difficult, as using classifier-free guidance results in diverging from the simple DDIM-based inversion (Mokady\net al., 2023). Against DITTO, we compare with the Na\u00a8\u0131ve inversion method of simply adding Gaussian noise to the reference\nspectrogram, the DDIM-based inversion which runs the DDIM sampling process in reverse through the model, and the\nrecent Null-Text Inversion (Mokady et al., 2023) method, which starts with the DDIM inversion and then learns a time-\ndependent unconditional text embedding c\u2205,t to improve inversion results in the presence of high guidance scales. Like in\nnull-text, we use the DDIM inversion as an initial guess for DITTO.\nAs the goal is direct recreation of the reference audio, we report MSE reconstruction across the entire 5K-sample MusicCaps\ndataset. We run this evaluation across four different guidance scales (ranging from 0, which is purely unconditional, to 7.5),\nand additionally run this on both our baseline 6 second model as well as a 24 second music generation model, which maintains\nall the same training hyperparameters and model size as our base model and only differs in that the output dimension is\n2048\u00d7160\u00d71. In Table 6, we show that DITTO beats all other inversion methods across all guidance scales and model sizes,\nwith the exception of the highest guidance scale on the 6 second base model, for which it performs slightly worse than null-text\ninversion. Notably, DITTO\u2019s superior performance on the 24 second model shows that scaling the number of free parameters\nwith the image size (as xT is the same shape as the output spectrogram) helps maintain reconstruction quality in the presence\nof high guidance, while methods that do not scale with the image size (like null-text inversion) do not have this benefit.\nQualitatively, we find that null-text inversion exhibits unique semantic artifacts in the reconstructed audio, such as replacing\nsung vocals with trumpets or tambourines with hi-hats, while DITTO avoids this failure case. As all the training data for the\nbase model was on purely instrumental music, this shows that DITTO allows TTM diffusion models to interact with real\naudio outside the distribution of their training data. In further work, we hope to explore more complicated edits that require\ninverted inputs (which is common in the image domain) and thus compare against the EDICT-based approach.\nTable 6. Inversion results across context size and guidance strength. DITTO performs SOTA reconstruction in most cases and noticeably\nscales with context size.\nMSE (\u2193)\n6 seconds\n24 seconds\nw = 0\nw = 1\nw = 4\nw = 7.5\nw = 0\nw = 1\nw = 4\nw = 7.5\nNa\u00a8\u0131ve\n0.0678\n0.0668\n0.0714\n0.0787\n0.1044\n0.1042\n0.1071\n0.1122\nDDIM\n0.0115\n0.0072\n0.0192\n0.0334\n0.0089\n0.0072\n0.0115\n0.0179\nNT\n0.0043\n0.0072\n0.0055\n0.0072\n0.0057\n0.0072\n0.0057\n0.0060\nDITTO (ours)\n0.0011\n0.0010\n0.0025\n0.0075\n0.0011\n0.0011\n0.0015\n0.0023\nE. Reference-Free Looping\nWhile we generally focus on long-form reference-based loop generation, where we seamlessly take existing audio and blend\nit back into itself, we note that DITTO can also be used for short-form reference-free loop generation, where we seek to\ngenerate a short musical loop unconditionally. This framework is similar to the reference-based looping, but instead defines\nthe generated audio to loop back into itself, rather than into some fixed reference audio. More formally, we define Mgen,1\nand Mgen,2 as two o sized masks over the generated spectrogram, and set f(x0) = Mgen,1 \u2299 x0, y = Mgen,2 \u2299 x0, and\nL \u221d \u2225f(x0) \u2212 y\u22252\n2, such that the model optimizes to match the overlap region of its own generation during DITTO. We\nnote that by setting Mgen,2 to occur earlier in the spectrogram (rather than one of the edges), we can generate loops of\nlengths that are less than or equal to the total context window (in our case, 6 seconds). In Figure 6, we show spectrograms of\nreference-free looping with an o = 0.5 second overlap and a total of two repetitions, with the loop boundary shown in red.\nF. Musical Structure Transfer\nWhile in the main paper, we focus our musical structure control task as controlling high-level musical form through simple\nmusical phrase diagrams (like \u201cABA\u201d), we can also directly transfer the structure of an existing song to our generation with\n16\nDITTO: Diffusion Inference-Time T -Optimization for Music Generation\nFigure 6. Reference-free loop generation with an overlap of o = 0.5 seconds. Loop boundary is shown in red.\nDITTO through a similar process. Namely, instead of generating a target self-similarity matrix based on a given phrase\ndiagram, we can instead set y = T(y)T(y)\u22a4, where y is the mel-spectrogram of a real song and T(\u00b7) is our MFCC-based\ntimbre-extraction function. In this way, using L \u221d \u2225f(x0) \u2212 y\u22252\n2 we can use DITTO to generate music that matches the\nfine-grained self-similarity matrix of an existing musical fragment. Note that here we omit the 2D Savitzky-Golay step over\nthe output self-similarity matrix, as here we want to directly match the intra-phrase similarity structures (rather than trying\nto capture broad musical form). We show examples of spectrograms with the target and generated self-similarity matrices\nin Fig. 7, where target self-similarity matrices are extracted from songs from the Free Music Archive dataset (Defferrard\net al., 2017).\nG. Alternative Sampling Methods\nUnlike previous works on diffusion latent optimization (Wallace et al., 2023a), DITTO imposes no restrictions on the\nsampling process used to perform the optimization procedure, thus freeing us to choose any performant diffusion model\nsampling algorithm. Namely, we explore using DPM-Solver++ (Lu et al., 2022), a SOTA diffusion sampler for improving\nsample quality in conditional diffusion settings. Using outpainting and intensity control as test cases, in Table 7 we show\nMSE and FAD results. We interestingly find that DDIM is better than DPM++ for the intensity control task, yet DPM++ is\nslightly better for the outpainting task. We invite future work on discovering both theoretically and empirically how different\ndiffusion sampling algorithms effect the noise latent optimization process.\nTable 7. Comparison of different samplers for DITTO. DDIM works solidly better than DPM++ for the intensity task, and DPM++\npreforms slightly better for outpainting.\nTarget\nSampler\nMSE\nFAD\nIntensity\nDDIM\n4.77\n10.53\nIntensity\nDPM++\n6.30\n11.04\nOutpainting\nDDIM\n\u2013\n9.19\nOutpainting\nDPM++\n\u2013\n9.12\n17\nDITTO: Diffusion Inference-Time T -Optimization for Music Generation\nTarget Musical Structure\nGenerated Musical Structure\nTarget Musical Structure\nGenerated Musical Structure\nTarget Musical Structure\nGenerated Musical Structure\nTarget Musical Structure\nGenerated Musical Structure\nFigure 7. Musical Structure Transfer using self-similarity MFCC matrices extracted from real musical audio as the target.\nH. Multi-Objective DITTO\nInspired by (Wu et al., 2023a), we can leverage the flexibility of DITTO to incorporate multiple feature matching criteria for\na multi-objective optimization setup:\nx\u2217\nT = arg min\nxT\n1\nM\nM\nX\ni=1\n\u03bbiLi (fi(x0), yi) ,\n(14)\nwhere we include additional \u03bbi weights to balance the different scales of each loss function. Given DITTO\u2019s generality,\nthis allows us to combine both editing and control signals at the same time, effectively unlocking the ability to iteratively\ncompose long-form music with fine-grained temporal control. Here, we experiment with both Intensity+Structure, showing\nthe combination of multiple reference-free controls, and Intensity+Outpainting, showing how reference-free controls can be\ncomposed with reference-based editing methods. For both experiments, we set \u03bbintensity = 1/40 and all other \u03bbi = 1, as\nintensity is calculated in the raw dB space. For the Intensity+Outpainting control, we use an overlap of o = 2 seconds and\nonly optimize the intensity curve for the nonoverlapping section, having a similar effect to the \u201cdon\u2019t care\u201d regions in Wu\net al. (2023a). In Figures 8 and 9, we show spectrograms and output features for both experiments.\nI. Reusing Optimized Latents\nA key bottleneck of inference-time optimization methods like DITTO is the apparent need for the optimization procedure\nto generate a single output that matches the given feature, thus limiting its scalability. In order to mitigate this effect and\naccelerate the creative workflow for users, we explore how we can reuse optimized latents x\u2217\nT to generate diverse outputs\nthat follow the initial optimized feature signal.\nA natural idea to add reusability to optimized latents is to treat each x\u2217\nT as the mean of some normal distribution N(x\u2217\nT , \u03c32)\nwithin the model\u2019s latent space for some hyperparameter \u03c32, and then sample an xT \u223c N(x\u2217\nT , \u03c32) at inference time without\nre-optimizing. We find that this process leads to considerable divergence from the optimized feature in practice, and leave\n18\nDITTO: Diffusion Inference-Time T -Optimization for Music Generation\n0\n200\n400\n17.5\n15.0\n12.5\n10.0\n7.5\n5.0\nTarget Intensity\n0\n200\n400\n17.5\n15.0\n12.5\n10.0\n7.5\n5.0\nGenerated Intensity\n0\n200\n400\n0\n100\n200\n300\n400\n500\nTarget Musical Structure\n0\n200\n400\n0\n100\n200\n300\n400\n500\nGenerated Musical Structure\n0\n200\n400\n17.5\n15.0\n12.5\n10.0\n7.5\n5.0\nTarget Intensity\n0\n200\n400\n17.5\n15.0\n12.5\n10.0\n7.5\n5.0\nGenerated Intensity\n0\n200\n400\n0\n100\n200\n300\n400\n500\nTarget Musical Structure\n0\n200\n400\n0\n100\n200\n300\n400\n500\nGenerated Musical Structure\n0\n200\n400\n17.5\n15.0\n12.5\n10.0\n7.5\n5.0\nTarget Intensity\n0\n200\n400\n17.5\n15.0\n12.5\n10.0\n7.5\n5.0\nGenerated Intensity\n0\n200\n400\n0\n100\n200\n300\n400\n500\nTarget Musical Structure\n0\n200\n400\n0\n100\n200\n300\n400\n500\nGenerated Musical Structure\n0\n200\n400\n17.5\n15.0\n12.5\n10.0\n7.5\n5.0\nTarget Intensity\n0\n200\n400\n17.5\n15.0\n12.5\n10.0\n7.5\n5.0\nGenerated Intensity\n0\n200\n400\n0\n100\n200\n300\n400\n500\nTarget Musical Structure\n0\n200\n400\n0\n100\n200\n300\n400\n500\nGenerated Musical Structure\nFigure 8. Output spectrograms, intensity curves, and MFCC self-similarity matrices for multi-objective DITTO with intensity and structure\nset as the feature extractors.\n0\n200\n400\n600\n800\n18\n16\n14\n12\n10\n8\n6\n4\nTarget Intensity\nOutput Intensity\n0\n200\n400\n600\n800\n18\n16\n14\n12\n10\n8\n6\n4\nTarget Intensity\nOutput Intensity\n0\n200\n400\n600\n800\n18\n16\n14\n12\n10\n8\n6\n4\nTarget Intensity\nOutput Intensity\n0\n200\n400\n600\n800\n18\n16\n14\n12\n10\n8\n6\n4\nTarget Intensity\nOutput Intensity\nFigure 9. Output spectrograms and intensity curves for multi-objective DITTO with outpainting and intensity set as the feature extractors.\nThe overlap is set to o = 2 seconds, and intensity control is only applied over the non-overlapping section.\n19\nDITTO: Diffusion Inference-Time T -Optimization for Music Generation\nTable 8. Loss on samples generated with stochastic sampling from x\u2217\nT . We observe that DITTO latents natively can act as generalized\nfeature priors, using FreeDoM on optimized latents to significantly improve feature adherence, thus showing how optimization-based and\nguidance-based methods can be used in conjuction for high-quality and efficient control.\nOptimization\nInference\nFeature\nL\nL\nSampler\nSampler\n(Fixed Prompt)\nDDIM\nDDPM\nIntensity\n24.5120\n13.8316\nDDIM\nDDPM+FreeDoM\nIntensity\n16.9780\n11.2481\nDDIM\nDDPM\nMelody\n2.7973\n2.7441\nDDIM\nDDPM+FreeDoM\nMelody\n1.8482\n1.8710\nDDIM\nDDPM\nMusical Structure\n0.2952\n0.2643\nDDIM\nDDPM+FreeDoM\nMusical Structure\n0.0251\n0.0235\nthis to future work to explore further. Instead, we consider the case where we sample stochastic trajectories starting from\nx\u2217\nT , which in practice is as simple as switching to a stochastic sampling algorithm at inference time such as DDPM in (12)\n(note that we still use deterministic samplers during DITTO as stochastic samplers tend to make the optimization process\nconsiderably harder). Additionally, we also explore the case when the initial prompt ctext used during DITTO is varied,\nadding another source of stochasticity.\nIn this experiment, we compare two possible methods for reusing optimized latents for sampling stochastic trajectories: 1)\nafter performing DITTO with DDIM, we sample using DDPM at inference time and 2) we use DDIM for optimization and\nDDPM for inference, but then additionally include the FreeDoM (Yu et al., 2023) guidance update in each DDPM step. To\ntest reusability, after optimizing for each x\u2217\nT given a target signal y and some text condition ctext, we generate B samples\nx(i)\n0\nusing x\u2217\nT as the starting latent and our stochastic sampling algorithm of choice, and measure 1\nB\nPB\ni=1 L(f(x(i)\n0 , y)), or\nthe average loss over the stochastic samples, where no optimization is occuring. We perform this experiment both where\neach x(i)\n0\nis generated with a random prompt ci, and when each prompt is fixed to the initial prompt ci = ctext to measure\nthe effect of additional stochasticity from conditioning.\nIn Table 8, we show results for intensity, melody, and musical structure control with a batch size B = 10. Notably, while\nswitching to baseline DDPM during sampling predictably worsens the feature adherence, using FreeDoM with DDPM\nand starting at x\u2217\nT yields significantly improved feature adherence to the optimized target. This presents a useful marriage\nof guidance-based and optimization-based approaches, as DITTO latents can act as reasonable feature priors by utilizing\nFreeDoM to guide the trajectory from the strong starting point.\nJ. Diffusion Latents and Low-Frequency Content\nIn Si et al. (2023), the authors discover that much of the low-frequency (in the 2D pixel domain) content of TTI model\ngenerations are determined exceedingly early on in the sampling process, where further sampling steps only produce high-\nfrequency information and improve quality. This presents a compelling case for why DITTO has such strong expressivity:\nbecause many target controls for TTM generation like intensity, melody and musical structure are low-frequency features in\nthe spectrogram domain (i.e. most high-frequency 2D content in spectrograms address audio quality factors), optimizing xT\nto target these features is well within the diffusion model\u2019s latent space which already encodes low-frequency information\nin the first place. This is compounded by the fact that music tags and captions generally only address high-level stylistic\ninformation, leaving everything that is not captured by the text captions (such as time-varying intensity, melody, and\nstructure) to be incorporated into the initialization.\nTo validate this proposed justification, we generate 5K batches (B = 10) of samples from our base diffusion model, where\nhalf of the batches (2.5K) have random initializations and random prompts while the other half have the same initialization\nxT (and still random prompts). For each group, we measure variance within each batch of the intensity, melody, and\nmusical structure features extracted from the batch outputs. Shown in Fig. 10, we find a statistically significant effect across\nall features that fixing the initialization significantly reduces the intra-batch feature variance. This serves as empirical\njustification that to a certain extent, the model output\u2019s salient musical features are already determined at initialization.\n20\nDITTO: Diffusion Inference-Time T -Optimization for Music Generation\nVariable Latent\nFixed Latent\n0\n1\n2\n3\n4\n5\nIntra-batch Intensity Variance\nIntensity\nVariable Latent\nFixed Latent\n0.00\n0.02\n0.04\n0.06\n0.08\nIntra-batch Melody Variance\nMelody\nVariable Latent\nFixed Latent\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\nIntra-batch Musical Structure Variance\nMusical Structure\nFigure 10. Intra-batch variance for model generations both with and without fixing the initial latent. We find a stiatistically significant\neffect that fixing the latent reduces feature variance, showing that xT already encodes a great deal of feature information.\nK. Model Pre-training\nFor our spectrogram generation model, we follow an identical training processed to default TTM as to Music ControlNet (Wu\net al., 2023a). We use a convolutional UNet (Ronneberger et al., 2015) with 5 2D-convolution ResNet (He et al., 2016)\nblocks with [64, 64, 128, 128, 256] feature channels per block with a stride of 2 in between downsampling blocks. The UNet\ninputs Mel-scaled (Stevens et al., 1937) spectrograms clipped to a dynamic range of 160 dB and scaled to [\u22121, 1] computed\nfrom 22.05 kHz audio with a hop size of 256 (i.e., frame rate fk \u2248 86 Hz), a window size of 2048, and 160 Mel bins. For\nour genre, mood, and tempo global style control ctext, we use learnable class-conditional embeddings with dimension of 256\nthat are injected into the inner two ResNet blocks of the U-Net via cross-attention. We use a cosine noise schedule with\n1000 diffusion steps that are injected via sinusoidal embeddings with a learnable linear transformation summed directly\nwith U-Net features in each block. We set our output time dimension to 512 or \u22486 seconds, yielding a 512\u00d7160\u00d71 output\ndimension. We use an L1 training objective between predicted and actual added noise, an Adam optimizer with learning rate\nto 10\u22125 with linear warm-up and cosine decay. Due to limited data and efficiency considerations, we instantiate a relatively\nsmall model of 41M parameters and pre-train with distributed data parallel for 5 days on 32 A100 GPUs with a batch size of\n24 per GPU. Finally, we also use a use a BigVGAN vocoder (Lee et al., 2022) modified with a DAC discriminator (Kumar\net al., 2023), trained with an AdamW optimizer with learning rate 0.0001, exponential learning rate decay on both our\ndiscriminator and generator optimizer, batch size of 48 per GPU, and 1536 channels for the initial upsampling layer that was\ntrained on 8 A100 GPUs for 5 days.\n21\n"
  },
  {
    "title": "EmerDiff: Emerging Pixel-level Semantic Knowledge in Diffusion Models",
    "link": "https://arxiv.org/pdf/2401.11739.pdf",
    "upvote": "16",
    "text": "Published as a conference paper at ICLR 2024\nEMERDIFF:\nEMERGING\nPIXEL-LEVEL\nSEMANTIC\nKNOWLEDGE IN DIFFUSION MODELS\nKoichi Namekata1,2, Amirmojtaba Sabour1,2, Sanja Fidler1,2,3, Seung Wook Kim1,2,3\n1University of Toronto, 2Vector Institute, 3NVIDIA\nkoichi.namekata@mail.utoronto.ca, {amsabour, fidler, seung}@cs.toronto.edu\nABSTRACT\nDiffusion models have recently received increasing research attention for their re-\nmarkable transfer abilities in semantic segmentation tasks. However, generating\nfine-grained segmentation masks with diffusion models often requires additional\ntraining on annotated datasets, leaving it unclear to what extent pre-trained dif-\nfusion models alone understand the semantic relations of their generated images.\nTo address this question, we leverage the semantic knowledge extracted from Sta-\nble Diffusion (SD) and aim to develop an image segmentor capable of generating\nfine-grained segmentation maps without any additional training. The primary dif-\nficulty stems from the fact that semantically meaningful feature maps typically\nexist only in the spatially lower-dimensional layers, which poses a challenge in\ndirectly extracting pixel-level semantic relations from these feature maps.\nTo\novercome this issue, our framework identifies semantic correspondences between\nimage pixels and spatial locations of low-dimensional feature maps by exploiting\nSD\u2019s generation process and utilizes them for constructing image-resolution seg-\nmentation maps. In extensive experiments, the produced segmentation maps are\ndemonstrated to be well delineated and capture detailed parts of the images, in-\ndicating the existence of highly accurate pixel-level semantic knowledge in diffu-\nsion models. Project page: https://kmcode1.github.io/Projects/\nEmerDiff/\nFigure 1: EmerDiff is an unsupervised image segmentor solely built on the semantic knowledge\nextracted from a pre-trained diffusion model. The obtained fine-detailed segmentation maps suggest\nthe presence of highly accurate pixel-level semantic knowledge in diffusion models.\n1\narXiv:2401.11739v1  [cs.CV]  22 Jan 2024\nPublished as a conference paper at ICLR 2024\n1\nINTRODUCTION\nIn recent years, diffusion models (Ho et al., 2020; Dhariwal & Nichol, 2021) have emerged as the\nstate-of-the-art generative models for synthesizing high-quality images. Notably, the internal rep-\nresentations of pre-trained diffusion models have been found semantically enriched, demonstrating\nimpressive transfer abilities in semantic segmentation tasks (Baranchuk et al., 2022; Karazija et al.,\n2023; Li et al., 2023a; Xu et al., 2023a). However, the previous success in producing fine-grained\nsegmentation maps often relies on incorporation of additional knowledge such as mask annota-\ntions (Baranchuk et al., 2022; Xu et al., 2023a) and hand-crafted priors (Karazija et al., 2023; Ma\net al., 2023; Wu et al., 2023), leaving it unclear to what extent the pre-trained diffusion models alone\nunderstand the semantic relations of their generated images. To address this question, we present\nan unsupervised image segmentor that can generate fine-grained segmentation maps by solely lever-\naging the semantic knowledge extracted from a pre-trained diffusion model. Specifically, our work\nis built on Stable Diffusion (Rombach et al., 2022), a large-scale text conditioned diffusion model\ncapable of generating diverse high-resolution images.\nThe common unsupervised approach to capture semantic relations from a diffusion model is to apply\nk-means on its semantically meaningful feature maps. This technique has been demonstrated to\nproduce cluster maps that are semantically aligned (Baranchuk et al., 2022; Patashnik et al., 2023;\nXu et al., 2023a). While this method intuitively visualizes the semantic awareness of diffusion\nmodels, the obtained semantic relations are often coarse because of the fact that the semantically\nmeaningful feature maps typically reside only in the spatially low-dimensional layers (Collins et al.,\n2020; Baranchuk et al., 2022; Luo et al., 2023). Nevertheless, diffusion models possess the ability\nto construct high-resolution images based on their semantic knowledge embedded in low-resolution\nfeature maps. This motivates us to analyze how the semantically meaningful low-resolution feature\nmaps influence the output images through the generation process, which we hypothesize to be a\ncrucial step for extracting pixel-level semantic knowledge from the diffusion models.\nTo this end, we investigate the influence of local change in the values of the low-resolution feature\nmaps on the pixel values of the generated images. Our key discovery is that when we perturb the\nvalues of a sub-region of low-resolution feature maps, the generated images are altered in a way\nthat only the pixels semantically related to that sub-region are notably changed. Consequently, we\ncan automatically identify the semantic correspondences between image pixels and a sub-region of\nlow-dimensional feature maps by simply measuring the change in the pixel values.\nBuilding on this insight, our proposed image segmentor can generate fine-grained segmentation\nmaps without the need for any additional knowledge. First, we generate low-resolution segmentation\nmaps (e.g., 16 \u00d7 16) by applying k-means on low-dimensional feature maps. Then, we build image-\nresolution segmentation maps (e.g., 512 \u00d7 512) in a top-down manner by mapping each image pixel\nto the most semantically corresponding low-resolution mask. These semantic correspondences are\nextracted from the diffusion models leveraging the aforementioned finding.\nThe effectiveness of our framework is extensively evaluated on multiple scene-centric datasets such\nas COCO-Stuff (Caesar et al., 2018), PASCAL-Context (Mottaghi et al., 2014), ADE20K (Zhou\net al., 2019) and Cityscapes (Cordts et al., 2016) both qualitatively and quantitatively. Although the\nunderlying diffusion model is never trained on annotated datasets, our framework produces segmen-\ntation maps that align surprisingly well with the detailed parts of the images, indicating the existence\nof highly accurate pixel-level semantic knowledge in the diffusion models.\n2\nRELATED WORKS\nGenerative models for semantic segmentation. The use of generative models for semantic seg-\nmentation dates back to the era of GANs, where Collins et al. (2020) discovered applying k-means\non the StyleGAN\u2019s (Karras et al., 2019) intermediate feature maps yields clusters aligning well\nwith semantic objects. Following that, the prior works (Li et al., 2021; Tritrong et al., 2021; Xu\n& Zheng, 2021; Zhang et al., 2021; Li et al., 2022b) exploited such semantically meaningful fea-\nture maps to learn semantic segmentation with minimal supervision. Notably, diffusion models\nexhibit similar properties, where clusters of their intermediate feature maps consistently group se-\nmantic objects (Baranchuk et al., 2022; Xu et al., 2023a). These feature maps are utilized in various\ndownstream tasks, including keypoint matching (Hedlin et al., 2023; Luo et al., 2023; Tang et al.,\n2\nPublished as a conference paper at ICLR 2024\n2023a; Zhang et al., 2023) and semantic segmentation (Baranchuk et al., 2022; Li et al., 2023a; Xu\net al., 2023a), outperforming GAN-based counterparts. Additionally, cross-attention layers of text-\nconditioned diffusion models (Balaji et al., 2022; Rombach et al., 2022; Saharia et al., 2022) are\nused for determining object layouts (Hertz et al., 2022; Patashnik et al., 2023; Tang et al., 2023b),\nwhere delimited layouts serve as (pseudo-) segmentation masks (Karazija et al., 2023; Ma et al.,\n2023; Wu et al., 2023). However, such semantically meaningful feature maps usually exist in low-\ndimensional layers, whose spatial resolution is significantly lower than the image resolution. To\nobtain upsampled and refined segmentation maps, prior literature has incorporated post-processing\ntools such as boundary refinement techniques (Kr\u00a8ahenb\u00a8uhl & Koltun, 2011; Barron & Poole, 2016;\nAraslanov & Roth, 2020; Wang et al., 2023) which, however, relies on hand-crafted priors. In con-\ntrast, our framework successfully produces fine-grained segmentation masks (e.g., 512 \u00d7 512) from\nlow-resolution feature maps (e.g., 16 \u00d7 16) without the need for any additional knowledge.\nUnsupervised semantic segmentation. Unsupervised semantic segmentation is the task of group-\ning pixels of unlabeled images into semantically meaningful concepts without seeing annotated\ndatasets. Previous studies have focused on either segmenting only salient objects (Van Gansbeke\net al., 2021; Melas-Kyriazi et al., 2022a; Shin et al., 2022; Wang et al., 2022; Sim\u00b4eoni et al., 2023;\nZadaianchuk et al., 2023) or segmenting entire scenes (Ji et al., 2019; Cho et al., 2021; Hamilton\net al., 2022; Seitzer et al., 2022; Wen et al., 2022; Yin et al., 2022; Li et al., 2023b), where our\nstudy is of the latter category. Typically, those frameworks are composed of two parts: 1. train an\nimage encoder that produces pixel embeddings through self-supervised learning. 2. learn concept\nembeddings that are used to group pixels into a pre-defined number of semantic concepts. During\ninference, the pixels are classified into one of these concepts by matching their pixel embeddings\nwith the closest concept embeddings. The current state-of-the-art approaches for scene segmen-\ntation (Hamilton et al., 2022; Seitzer et al., 2022; Wen et al., 2022) are built on the pre-trained\nself-supervised ViTs like DINO (Caron et al., 2021). There also exist several studies that utilize\nGANs\u2019 latent spaces for segmenting foreground objects (Voynov et al., 2020; Abdal et al., 2021;\nMelas-Kyriazi et al., 2022b; Feng et al., 2023; Oldfield et al., 2023); however, these approaches\nonly work in narrow visual domains and are not applicable to scene-centric images.\nOpen-vocabulary semantic segmentation. Open-vocabulary semantic segmentation aims to seg-\nment images according to arbitrary user-defined vocabularies during inference. Models are typically\ntrained with only text-image pairs (Xu et al., 2022a; Zhou et al., 2022; Cha et al., 2023; Mukhoti\net al., 2023; Ranasinghe et al., 2023; Xu et al., 2023b), or combination of unlabeled/labeled annota-\ntions and text-image supervision (Ghiasi et al., 2022; Li et al., 2022a; Xu et al., 2022b; Liang et al.,\n2023; Xu et al., 2023a;c). The majority of these models are built on image encoders of pre-trained\nvision language models like CLIP (Radford et al., 2021), but learn to produce feature representations\nthat exhibit better pixel-level alignment with text embeddings. However, the existing annotation-free\nmodels, which are trained without annotated datasets, tend to produce noisy segmentation masks. To\novercome this issue, we integrate our framework into these models. Concretely, for each segmen-\ntation mask produced from our framework, we compute its mask embedding through their image\nencoders and classify them by the text embeddings. By combining our framework\u2019s segmentation\nabilities with their classification abilities, we achieve significantly better mIoU.\n3\nMETHODS\nAs illustrated in Figure 2, our goal is to generate fine-grained segmentation maps by solely leverag-\ning the semantic knowledge extracted from pre-trained diffusion models. To achieve this, we begin\nby generating low-resolution segmentation maps by applying k-means on the semantically meaning-\nful low-dimensional feature maps (Section 3.2). Next, we construct image-resolution segmentation\nmaps by mapping each image pixel to the most semantically corresponding low-resolution mask.\nTo find the semantic correspondences between the image pixels and the masks, we exploit the dif-\nfusion model\u2019s mechanism of generating high-resolution images from their low-resolution feature\nmaps (Section 3.3). In the following sections, we first provide an overview of the properties of\ndiffusion models (Section 3.1) and then delve into further details of our approach.\n3.1\nPRELIMINARIES\nDiffusion models are trained to generate images by taking successive denoising steps from pure\nGaussian noise, where each denoising step is commonly performed with U-Net backbones (Ron-\n3\nPublished as a conference paper at ICLR 2024\nFigure 2: Overview of our framework. green: we first construct low-resolution segmentation maps\nby applying k-means on semantically meaningful low-dimensional feature maps. orange: Next, we\ngenerate image-resolution segmentation maps by mapping each pixel to the most semantically cor-\nresponding low-resolution mask, where semantic correspondences are identified by the modulated\ndenoising process.\nneberger et al., 2015) containing downward and upward paths. Specifically, our framework is built\non Stable Diffusion (SD) (Rombach et al., 2022), where the denoising steps are performed in a spa-\ntially lower-dimensional latent space, and final generated images are obtained by decoding denoised\nlatents with another neural network. The SD\u2019s U-Net architecture takes both noisy latents and text\ncaptions (an empty string in our experiments) as inputs and processes them through a stack of mod-\nular blocks, where each block consists of a residual block (He et al., 2016), a self-attention layer and\na cross-attention layer (Vaswani et al., 2017). These blocks belong to one of the four spatial resolu-\ntion levels (namely, 8 \u00d7 8, 16 \u00d7 16, 32 \u00d7 32, and 64 \u00d7 64), where the components of our interest\nare modular blocks at resolution 16 \u00d7 16 on the upward path, which especially contain semantically\nenriched representations (Luo et al., 2023) and are utilized in the prior literature (Karazija et al.,\n2023; Patashnik et al., 2023). Specifically, there are three consecutive modular blocks at resolution\n16 \u00d7 16 on the upward path, where Voynov et al. (2023) have observed that the first cross-attention\nlayer is primarily responsible for manipulating contents, while the last cross-attention layer exerts\nmore control over appearance. In the subsequent sections, we make use of these semantically mean-\ningful layers to produce fine-grained segmentation maps.\n3.2\nCONSTRUCTING LOW-RESOLUTION SEGMENTATION MAPS\nTo handle real images, we first invert real images into a particular number of denoising steps (each\nspecified by timestep t = 1 \u00b7 \u00b7 \u00b7 T where larger timesteps correspond to noisier images) through\nDDPM-based inversion (Huberman-Spiegelglas et al., 2023), which guarantees perfect reconstruc-\ntion with the scheduled noise. Next, we extract query vectors from the first cross-attention layer of\nupward 16 \u00d7 16 modular blocks at timestep tf, which will be referred to as our low-dimensional\nfeature maps. Intuitively, query vectors are trained to directly interact with text tokens; hence, their\nrepresentations should be semantically aware. Finally, we apply k-means on the extracted feature\nmaps, obtaining K clusters serving as low-resolution segmentation masks.\n3.3\nBUILDING IMAGE-RESOLUTION SEGMENTATION MAPS\nThus far, we have constructed low-resolution segmentation maps (e.g., 16 \u00d7 16), which are 32 times\nlower in resolution than the original image (e.g., 512 \u00d7 512). Our next goal is to build image-\nresolution segmentation maps from low-resolution segmentation maps by identifying the semantic\ncorrespondences between the image pixels and the low-resolution masks. To this end, we begin\nby observing how the low-dimensional layers in SD influence the pixels of their generated images.\nSpecifically, we modulate the values of a sub-region of feature maps at 16\u00d716 cross-attention layers\nand observe how this local change effects the pixel values of the generated images.\nIn the official SD\u2019s implementation, a cross-attention layer projects inputs into query, key, and value\nvectors (denote Q, K, V ) and computes f\n\u0010\n\u03c3\n\u0010\nQKT\n\u221a\nd\n\u0011\n\u00b7 V\n\u0011\n\u2208 Rhw\u00d7d, where d is the dimension\nof query vectors, hw is the spatial dimension of modular blocks, \u03c3 is a softmax function, and f\nis a fully-connected layer. To modulate the cross-attention layer, we replace this computation by\n4\nPublished as a conference paper at ICLR 2024\nFigure 3: Visualization of modulated denoising process. First row: original image. Second row:\nlow-resolution modulation mask M \u2208 {0, 1}h\u00d7w. Third row: obtained difference map d \u2208 RH\u00d7W ,\nwhere H/h = W/w = 32\nf\n\u0010\n\u03c3\n\u0010\nQKT\n\u221a\nd\n\u0011\n\u00b7 V\n\u0011\n+ cM \u2208 Rhw\u00d7d where c \u2208 R is a scalar value and M \u2208 {0, 1}hw\u00d71 is a binary\nmask specifying the spatial locations to modulate. Verbally, we uniformly add a constant offset c to\na sub-region (specified by M) of feature maps. During the modulated denoising process, we apply\nthis modulation to a specific cross-attention layer at timestep tm.\nTo observe changes in pixel values, we first run the above modulated denoising process with two\noffsets c = \u2212\u03bb, +\u03bb separately, obtaining two altered images I\u2212, I+ \u2208 RH\u00d7W \u00d73. We then compute\nthe difference maps d by taking Euclidean distance over RGB dimension d = ||I\u2212 \u2212 I+||2 \u2208\nRH\u00d7W . As shown in Figure 3, the pixels semantically related to the modulated sub-region changed\nprominently, while the other pixels remained roughly the same. Therefore, the obtained difference\nmap can be interpreted as the strength of the semantic correspondences between the image pixels\nand the sub-region of the low-resolution feature maps.\nBased on this observation, we can compute the strength of semantic correspondences between every\npair of image pixels and the low-resolution segmentation masks. In detail, for each low-resolution\nmask M i \u2208 {0, 1}hw\u00d71, we produce a difference map di \u2208 RH\u00d7W , where di\nx,y \u2208 R represents\nthe strength of semantic correspondence between pixel (x, y) and mask i. To build image-resolution\nsegmentation maps, we label each pixel (x, y) with the low-resolution mask k having the strongest\nsemantic correspondence (i.e., k = argmaxi di\nx,y).\nFor further improvements, we fix the attention maps QKT (i.e., inject original attention maps) of\nall the self/cross attention layers during the modulated denoising process. Since the attention maps\nrepresent pixel affinities and strongly influence object layouts, attention injection is a commonly\nused technique in image editing to preserve the structure of images (Tumanyan et al., 2023). Finally,\nafter computing the difference maps, we apply Gaussian filtering to suppress pixelated artifacts.\n4\nEXPERIMENTS\nIn this section, we evaluate our produced segmentation masks both qualitatively and quantitatively.\nWe also conduct hyperparameter analysis in Appendix D.\n4.1\nIMPLEMENTATION DETAILS\nThroughout the experiments, we use the official Stable Diffusion v1.4 checkpoint with DDPM sam-\npling scheme of 50 steps (for clarity purposes, we denote timesteps out of T = 1000). To generate\nlow-resolution segmentation maps, we extract feature maps at timestep tf = 1 (minimum noise). We\napply modulation to the third cross-attention layer of 16 \u00d7 16 upward blocks at timestep tm = 281\nand \u03bb = 10. As discussed earlier, this layer is responsible for controlling appearance. The effects of\nvarying hyperparameters are discussed in Appendix D.\nRuntime analysis. The most computationally expensive part of our method is the modulated de-\nnoising process, which runs independently for each mask. However, we only need to execute the\nmodulated denoising process from the timestep to apply modulation (i.e. modulation timestep tm),\n5\nPublished as a conference paper at ICLR 2024\nFigure 4: Qualitative comparison with naively upsampled low-resolution segmentation maps.\nFigure 5: Varying the number of segmentation masks. Our framework consistently groups objects\nin a semantically meaningful manner.\nTable 1: Results of unsupervised semantic segmentation under traditional evaluation strategy.\nEvaluated on full COCO-Stuff-27 (Caesar et al., 2018). ACSeg is taken from the original paper. IIC,\nPiCIE, and TransFGU from Yin et al. (2022). DINO from Koenig et al. (2023). Other results from\nSeitzer et al. (2022). Some works are evaluated on curated datasets (Ji et al., 2019), which generally\ngives higher mIoU than being evaluated on the full datasets (Yin et al., 2022).\nmIoU (\u2191)\nmIoU (\u2191)\nIIC (Ji et al., 2019)\n2.4\nTransFGU (Yin et al., 2022)\n16.2\nPiCIE (Cho et al., 2021)\n11.9\nSlotCon (Wen et al., 2022)\n18.3\nDINO ViT-B/8 (Caron et al., 2021)\n13.0\nSTEGO (Hamilton et al., 2022)\n26.8\nSegDiscover (Huang et al., 2022)\n14.3\nDINOSAUR (Seitzer et al., 2022)\n24.0\nACSeg (Li et al., 2023b)\n16.4\nOurs\n26.6\nrequiring 15 denoising steps in our hyperparameter settings. Furthermore, our method does not in-\nvolve backpropagation, and the entire denoising process is operated in the latent space (except the\nlast decoding part). Therefore, parallelization can be performed effectively.\n4.2\nQUALITATIVE ANALYSIS\nFigure 1 showcases the examples of our produced segmentation maps (More results in Appendix E).\nOur segmentation maps are well delineated and successfully distinguish moderately small ob-\njects (e.g., person, building) in the scene. To further demonstrate the effectiveness of our pipeline,\nwe compare our segmentation maps with the naively up-sampled (via bilinear interpolation) low-\nresolution segmentation maps. As visualized in Figure 4, the naively upsampled segmentation maps\nare coarse and hard to interpret. In contrast, our segmentation maps are much clearer despite shar-\ning the same low-resolution maps. Finally, we vary the number of masks generated per image. As\nillustrated in Figure 5, we obtain segmentation maps that are semantically interpretable even when\nthe number of masks is as small as 2.\n4.3\nQUANTITATIVE ANALYSIS\nTo quantitatively evaluate our segmentation masks, we apply our framework to two downstream\ntasks: unsupervised semantic segmentation and annotation-free open vocabulary segmentation. In\nthis evaluation, our framework generates 30 segmentation masks per image.\nUnsupervised semantic segmentation.\nWe evaluate our framework on standard segmentation\ndatasets. The traditional evaluation protocol requires pixels to be classified into the same num-\nber of semantic concepts as the dataset classes so that the concepts can be subsequently matched\nwith the dataset classes through Hungarian matching. As in prior work (Hamilton et al., 2022), we\nextend our framework by generating pixel/concept embeddings, where each pixel will be classified\ninto the concept with the closest embeddings. To generate pixel embeddings, we first create a mask\n6\nPublished as a conference paper at ICLR 2024\nFigure 6: Visualizations of unsupervised semantic segmentation under our modified evaluation\nstrategy. More examples in Figure 11 in Appendix.\nTable 2:\nResults of unsupervised semantic segmentation under our modified evaluation\nstrategy.\nEvaluated on ADE20K (AD150) (Zhou et al., 2019), PASCAL-Context (PC59,\nPC459) (Mottaghi et al., 2014), COCO-Stuff (CS171, CS27) (Caesar et al., 2018), and\nCityscapes (City19) (Cordts et al., 2016). MDC (Cho et al., 2021), PiCIE (Cho et al., 2021), DINO,\nand STEGO are trained solely on images, while CLIP (Radford et al., 2021), TCL (Cha et al., 2023),\nand CLIPpy (Ranasinghe et al., 2023) are trained on text-image pairs. For CLIP, we follow Zhou\net al. (2022) to modify the image encoder to output pixel-wise embeddings. For SD, we naively\nup-sample low-resolution segmentation maps (via bilinear interpolation, see Figure 4) and produce\npixel embeddings following the same procedure as ours.\nmIoU(\u2191)\nBackbone\nAD150\nPC59\nPC459\nCS171\nCS27\nCity19\nMDC\n-\n-\n-\n-\n-\n8.6\n14.0\nPiCIE\n-\n-\n-\n-\n-\n11.7\n15.1\nDINO\nDINO ViT-B/8\n19.1\n30.0\n10.1\n19.0\n32.2\n34.6\nSTEGO\nDINO ViT-B/8\n-\n-\n-\n13.8\n36.6\n34.6\nCLIP\nCLIP ViT-B/16\n22.0\n36.4\n14.0\n23.9\n34.1\n33.7\nTCL\nCLIP ViT-B/16\n20.8\n35.5\n11.3\n23.5\n31.9\n32.6\nCLIPpy\nT5 + DINO ViT-B/16\n24.0\n38.0\n15.6\n25.9\n34.1\n28.3\nSD\nSD v1.4\n29.1\n41.5\n20.6\n27.6\n42.1\n32.3\nOurs\nSD v1.4\n33.1\n45.7\n25.1\n30.5\n45.8\n37.1\nembedding of each segmentation mask by utilizing SD\u2019s low-dimensional feature maps (details are\nin appendix B). Each pixel then adopts its corresponding mask embedding as its own pixel embed-\nding. For concept embeddings, we run k-means on the pixel embeddings across the entire dataset\nand extract the desired number of cluster centroids.\nHowever, as demonstrated in Table 1, we observe that our model performs on par with the recent\nDINO-based baselines: STEGO (Hamilton et al., 2022) and DINOSAUR (Seitzer et al., 2022). This\nis attributed to the limitation of the traditional evaluation protocol that mIoU is sensitive to how\nwell the concept embeddings align with the pre-defined dataset classes, which is problematic when\nthere are various ways to semantically group objects into a fixed number of groups (See Figure 9 in\nAppendix for concrete examples).\nTo demonstrate the strength of our framework, we relax the restrictions of the traditional evaluation\nprotocol by allowing access to the annotations while building concept embeddings. Specifically, we\ntake the average of pixel embeddings belonging to the same ground truth labels and set the obtained\n7\nPublished as a conference paper at ICLR 2024\nFigure 7: Annotation-free open-vocabulary semantic segmentation.\nWe combine our fine-\ngrained class-agnostic segmentation masks (green) with the baseline\u2019s coarse text-aligned pixel em-\nbeddings (blue) to produce text-aligned fine-grained segmentation maps (orange).\nTable 3: Comparison between baselines and baselines + ours in annotation-free open vocab-\nulary semantic segmentation. Evaluated on ADE20K(AD150) (Zhou et al., 2019), PASCAL-\nContext(PC59, PC459) (Mottaghi et al., 2014), COCO-Stuff(CS171) (Caesar et al., 2018), and\nCityscapes(City19) (Cordts et al., 2016). For a fair comparison, we re-evaluate TCL, MaskCLIP,\nand CLIPpy with the same prompt engineering. The results of other works are also put for refer-\nence, where OVSegmentor is taken from the original paper, and GroupViT from Cha et al. (2023)\nmIoU (\u2191)\nAD150\nPC59\nPC459\nCS171\nCity19\nOVSegmentor (Xu et al., 2023b)\n5.6\n20.4\n-\n-\n-\nGroupViT (Xu et al., 2022a)\n9.2\n23.4\n-\n15.3\n11.1\nMaskCLIP (Zhou et al., 2022)\n11.5\n25.4\n4.25\n15.3\n20.9\nMaskCLIP + Ours\n15.9\n33.2\n6.54\n20.7\n26.5\nTCL (Cha et al., 2023)\n14.6\n30.9\n5.22\n18.7\n20.9\nTCL + Ours\n17.4\n35.4\n6.49\n21.8\n23.4\nCLIPpy (Ranasinghe et al., 2023)\n12.6\n28.0\n4.64\n16.6\n9.76\nCLIPpy + Ours\n12.9\n29.0\n4.88\n17.2\n9.24\nmean embeddings as concept embeddings. As illustrated in Figure 6, this modification ensures that\nconcept embeddings represent each dataset class, and mIoU difference primarily comes from the\naccuracy and disentanglement capabilities of pixel embeddings. Since our modified evaluation pro-\ntocol only requires a model outputting pixel-wise embeddings, it can be applied to other baselines,\nincluding STEGO (Hamilton et al., 2022) as well as self-supervised image encoders.\nAs presented in Table 2, our modified evaluation protocol reveals a clear difference between the pre-\nvious methods and our framework. We observe that the DINO-based models (DINO and STEGO)\ngenerally perform poorly when the number of classes in a dataset is large, while the diffusion-based\nmodels (SD and ours, where SD is the naively up-sampled low-resolution masks as in Figure 4)\nhandle them relatively well. This might be because DINO is originally trained on curated object-\ncentric datasets like ImageNet (Russakovsky et al., 2015), whereas SD is trained on diverse images,\nmaking itself more robust to various domains. Furthermore, we observe that STEGO performs on\npar with DINO, which is consistent with the follow-up results (Koenig et al., 2023) that STEGO\nhas similar or even inferior linear probing performance to DINO. Nevertheless, our method consis-\ntently outperforms all the baselines. Additionally, we conduct comparisons with text-aligned image\nencoders (CLIP, TCL, CLIPpy) and still observe the performance gaps, attesting our performance\ngains are not solely attributed to the availability of text-image pairs during the pre-training. Lastly,\nwe see a clear improvement between our framework and SD, where they share the same mask em-\nbeddings and only differ in mask shapes. This validates the effectiveness of our pipeline.\nAnnotation-free open-vocabulary semantic segmentation. In this study, we combine our frame-\nwork with the existing annotation-free open-vocabulary segmentation models, where both ap-\nproaches are trained on image-text pairs only without access to annotated datasets.\nAs visual-\nized in Figure 7, the existing baselines produce text-aligned but coarse pixel embeddings, while\nour framework produces well delineated but class-agnostic masks. This motivates us to combine\nour segmentation masks with their pixel embeddings, aiming to produce class-aware fine-grained\nsegmentation masks.\n8\nPublished as a conference paper at ICLR 2024\nFigure 8: Failure cases. Segmentation masks occasionally failed to distinguish extremely small\nobjects (e.g., small desks, animal legs, human\u2019s face parts).\nIn detail, we integrate our framework into three publicly available annotation-free baselines con-\ntaining image and text encoders: MaskCLIP (Zhou et al., 2022), TCL (Cha et al., 2023), and\nCLIPpy (Ranasinghe et al., 2023). MaskCLIP (without heuristic refinement) is the most standard\nbaseline, where it modifies the pre-trained CLIP image encoder (Radford et al., 2021) to output\npixel-wise embeddings without additional training. TCL and CLIPpy are structured similarly to\nMaskCLIP, but trained to produce better pixel-level representations. To combine our framework\nwith these baselines, we first generate mask embeddings of each mask by computing the average of\npixel embeddings produced from the baseline\u2019s image encoder. We then classify each mask by find-\ning the closest text embedding to its mask embedding. Following convention Radford et al. (2021),\nprompt templates (e.g., \u201dA photo of a {}\u201d) are used to produce text embeddings. We provide the list\nof templates in Appendix C.\nAs presented in Table 3, we mostly observe the performance gain after being combined with our\nframework, assuring the quality of our segmentation masks. Notably, the performance gain of\nMaskCLIP is substantial, exhibiting competitive performance with the recent baselines (before be-\ning combined with our method). On the other hand, the performance gain of CLIPpy is marginal.\nWe attribute this to their over-smoothed pixel embeddings (See Figure 7).\n5\nLIMITATION AND CONCLUSION\nIn this paper, we developed an unsupervised image segmentor that can generate fine-grained seg-\nmentation maps by solely leveraging the semantic knowledge extracted from a pre-trained diffusion\nmodel. The extensive experiments validated its effectiveness, suggesting the presence of highly\naccurate pixel-level semantic knowledge in diffusion models.\nAs a limitation, our framework occasionally struggles to distinguish extremely small objects (e.g.,\nanimal legs, human faces) as illustrated in Figure 8, which might be because the detailed parts are\ncompressed together in low-dimensional layers, and our framework fails to separate them when\ngenerating low-resolution segmentation maps. Additionally, the underlying feature representations\nmay contain not only object meanings but also other attributes such as spatial location and colors,\nwhich lead some objects such as sky and ground to be over-segmented. For practical use, treating our\nproduced masks as pseudo-masks and integrating them into weakly-supervised frameworks could\nbe a promising direction.\nLastly, our study is built on Stable Diffusion, since it is the most commonly studied diffusion model\nthat can generate diverse images. However, our fundamental idea of modulating semantically mean-\ningful feature maps can be potentially applied to various generative models, which we leave for\nfuture work. We hope that our findings help further understand the inner workings of diffusion\nmodels and also encourage the research direction that utilizes generative models for discriminative\ntasks.\nACKNOWLEDGEMENTS\nThe authors acknowledge support by NSERC and the Vector Institute. SF acknowledges the Canada\nCIFAR AI Chair award.\nREFERENCES\nRameen Abdal, Peihao Zhu, Niloy J Mitra, and Peter Wonka. Labels4free: Unsupervised segmen-\ntation using stylegan. In Proceedings of the IEEE/CVF International Conference on Computer\n9\nPublished as a conference paper at ICLR 2024\nVision, pp. 13970\u201313979, 2021.\nNikita Araslanov and Stefan Roth.\nSingle-stage semantic segmentation from image labels.\nIn\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4253\u20134262,\nJune 2020.\nYogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika\nAittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models\nwith an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022.\nDmitry Baranchuk, Andrey Voynov, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko. Label-\nefficient semantic segmentation with diffusion models. In International Conference on Learning\nRepresentations, 2022. URL https://openreview.net/forum?id=SlxSY2UZQT.\nJonathan T Barron and Ben Poole. The fast bilateral solver. In European conference on computer\nvision, pp. 617\u2013632. Springer, 2016.\nHolger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context.\nIn Computer vision and pattern recognition (CVPR), 2018 IEEE conference on. IEEE, 2018.\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou, Julien Mairal, Piotr Bojanowski, and\nArmand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of\nthe IEEE/CVF international conference on computer vision, pp. 9650\u20139660, 2021.\nJunbum Cha, Jonghwan Mun, and Byungseok Roh. Learning to generate text-grounded mask for\nopen-world semantic segmentation from only image-text pairs. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 11165\u201311174, 2023.\nJang Hyun Cho, Utkarsh Mall, Kavita Bala, and Bharath Hariharan. Picie: Unsupervised semantic\nsegmentation using invariance and equivariance in clustering. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 16794\u201316804, 2021.\nEdo Collins, Raja Bala, Bob Price, and Sabine Susstrunk. Editing in style: Uncovering the local\nsemantics of gans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 5771\u20135780, 2020.\nMarius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo\nBenenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic\nurban scene understanding. In Proc. of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2016.\nPrafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances\nin neural information processing systems, 34:8780\u20138794, 2021.\nQianli Feng, Raghudeep Gadde, Wentong Liao, Eduard Ramon, and Aleix Martinez. Network-free,\nunsupervised semantic segmentation with synthetic images. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 23602\u201323610, 2023.\nGolnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Scaling open-vocabulary image segmentation\nwith image-level labels. In European Conference on Computer Vision, pp. 540\u2013557. Springer,\n2022.\nMark Hamilton, Zhoutong Zhang, Bharath Hariharan, Noah Snavely, and William T. Freeman. Un-\nsupervised semantic segmentation by distilling feature correspondences. In International Confer-\nence on Learning Representations, 2022. URL https://openreview.net/forum?id=\nSaKO6z6Hl0c.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n770\u2013778, 2016.\nEric Hedlin, Gopal Sharma, Shweta Mahajan, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi,\nand Kwang Moo Yi. Unsupervised semantic correspondence using stable diffusion. 2023.\n10\nPublished as a conference paper at ICLR 2024\nAmir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or.\nPrompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626,\n2022.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\nneural information processing systems, 33:6840\u20136851, 2020.\nHaiyang Huang, Zhi Chen, and Cynthia Rudin. Segdiscover: Visual concept discovery via unsuper-\nvised semantic segmentation. arXiv preprint arXiv:2204.10926, 2022.\nInbar Huberman-Spiegelglas, Vladimir Kulikov, and Tomer Michaeli. An edit friendly ddpm noise\nspace: Inversion and manipulations. arXiv preprint arXiv:2304.06140, 2023.\nXu Ji, Joao F Henriques, and Andrea Vedaldi. Invariant information clustering for unsupervised\nimage classification and segmentation. In Proceedings of the IEEE/CVF international conference\non computer vision, pp. 9865\u20139874, 2019.\nLaurynas Karazija, Iro Laina, Andrea Vedaldi, and Christian Rupprecht. Diffusion Models for Zero-\nShot Open-Vocabulary Segmentation. arXiv preprint, 2023.\nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative\nadversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, pp. 4401\u20134410, 2019.\nAlexander Koenig, Maximilian Schambach, and Johannes Otterbach. Uncovering the inner work-\nings of stego for safe unsupervised semantic segmentation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 3788\u20133797, 2023.\nPhilipp Kr\u00a8ahenb\u00a8uhl and Vladlen Koltun. Efficient inference in fully connected crfs with gaussian\nedge potentials. Advances in neural information processing systems, 24, 2011.\nBoyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen Koltun, and Rene Ranftl. Language-driven\nsemantic segmentation. In International Conference on Learning Representations, 2022a. URL\nhttps://openreview.net/forum?id=RriDjddCLN.\nDaiqing Li, Junlin Yang, Karsten Kreis, Antonio Torralba, and Sanja Fidler. Semantic segmenta-\ntion with generative models: Semi-supervised learning and strong out-of-domain generalization.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n8300\u20138311, 2021.\nDaiqing Li, Huan Ling, Seung Wook Kim, Karsten Kreis, Sanja Fidler, and Antonio Torralba. Big-\ndatasetgan: Synthesizing imagenet with pixel-wise annotations. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 21330\u201321340, 2022b.\nDaiqing Li, Huan Ling, Amlan Kar, David Acuna, Seung Wook Kim, Karsten Kreis, Antonio Tor-\nralba, and Sanja Fidler. Dreamteacher: Pretraining image backbones with deep generative models,\n2023a.\nKehan Li, Zhennan Wang, Zesen Cheng, Runyi Yu, Yian Zhao, Guoli Song, Chang Liu, Li Yuan, and\nJie Chen. Acseg: Adaptive conceptualization for unsupervised semantic segmentation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7162\u2013\n7172, 2023b.\nFeng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan Zhao, Hang Zhang, Peizhao Zhang,\nPeter Vajda, and Diana Marculescu. Open-vocabulary semantic segmentation with mask-adapted\nclip. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npp. 7061\u20137070, 2023.\nGrace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holynski, and Trevor Darrell. Diffusion\nhyperfeatures: Searching through time and space for semantic correspondence. arXiv, 2023.\nChaofan Ma, Yuhuan Yang, Chen Ju, Fei Zhang, Jinxiang Liu, Yu Wang, Ya Zhang, and Yanfeng\nWang. Diffusionseg: Adapting diffusion towards unsupervised object discovery. arXiv preprint\narXiv:2303.09813, 2023.\n11\nPublished as a conference paper at ICLR 2024\nLuke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and Andrea Vedaldi. Deep spectral methods: A\nsurprisingly strong baseline for unsupervised semantic segmentation and localization. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8364\u20138375,\n2022a.\nLuke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and Andrea Vedaldi.\nFinding an unsu-\npervised image segmenter in each of your deep generative models.\nIn International Confer-\nence on Learning Representations, 2022b. URL https://openreview.net/forum?id=\nUg-bgjgSlKV.\nRoozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja Fidler,\nRaquel Urtasun, and Alan Yuille. The role of context for object detection and semantic segmen-\ntation in the wild. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n2014.\nJishnu Mukhoti, Tsung-Yu Lin, Omid Poursaeed, Rui Wang, Ashish Shah, Philip HS Torr, and\nSer-Nam Lim. Open vocabulary semantic segmentation with patch aligned contrastive learning.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n19413\u201319423, 2023.\nJames Oldfield, Christos Tzelepis, Yannis Panagakis, Mihalis Nicolaou, and Ioannis Patras. Panda:\nUnsupervised learning of parts and appearances in the feature maps of GANs. In The Eleventh\nInternational Conference on Learning Representations, 2023. URL https://openreview.\nnet/forum?id=iUdSB2kK9GY.\nOr Patashnik, Daniel Garibi, Idan Azuri, Hadar Averbuch-Elor, and Daniel Cohen-Or. Localiz-\ning object-level shape variations with text-to-image diffusion models.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision (ICCV), 2023.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pp.\n8748\u20138763. PMLR, 2021.\nKanchana Ranasinghe, Brandon McKinzie, Sachin Ravi, Yinfei Yang, Alexander Toshev, and\nJonathon Shlens. Perceptual grouping in contrastive vision-language models. ICCV, 2023.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition, pp. 10684\u201310695, 2022.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomed-\nical image segmentation. In Medical Image Computing and Computer-Assisted Intervention\u2013\nMICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceed-\nings, Part III 18, pp. 234\u2013241. Springer, 2015.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual\nrecognition challenge. International journal of computer vision, 115:211\u2013252, 2015.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\ntext-to-image diffusion models with deep language understanding. Advances in Neural Informa-\ntion Processing Systems, 35:36479\u201336494, 2022.\nMaximilian Seitzer, Max Horn, Andrii Zadaianchuk, Dominik Zietlow, Tianjun Xiao, Carl-Johann\nSimon-Gabriel, Tong He, Zheng Zhang, Bernhard Sch\u00a8olkopf, Thomas Brox, et al. Bridging the\ngap to real-world object-centric learning. arXiv preprint arXiv:2209.14860, 2022.\nGyungin Shin, Samuel Albanie, and Weidi Xie. Unsupervised salient object detection with spectral\ncluster voting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 3971\u20133980, 2022.\n12\nPublished as a conference paper at ICLR 2024\nOriane Sim\u00b4eoni, Chlo\u00b4e Sekkat, Gilles Puy, Anton\u00b4\u0131n Vobeck`y, \u00b4Eloi Zablocki, and Patrick P\u00b4erez.\nUnsupervised object localization: Observing the background to discover objects. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3176\u20133186, 2023.\nLuming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. Emergent\ncorrespondence from image diffusion. arXiv preprint arXiv:2306.03881, 2023a.\nRaphael Tang, Linqing Liu, Akshat Pandey, Zhiying Jiang, Gefei Yang, Karun Kumar, Pontus\nStenetorp, Jimmy Lin, and Ferhan Ture. What the DAAM: Interpreting stable diffusion using\ncross attention.\nIn Proceedings of the 61st Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), pp. 5644\u20135659, Toronto, Canada, July 2023b. As-\nsociation for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.310. URL https:\n//aclanthology.org/2023.acl-long.310.\nNontawat Tritrong, Pitchaporn Rewatbowornwong, and Supasorn Suwajanakorn. Repurposing gans\nfor one-shot semantic part segmentation. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition, pp. 4475\u20134485, 2021.\nNarek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for\ntext-driven image-to-image translation. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pp. 1921\u20131930, June 2023.\nWouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, and Luc Van Gool.\nUnsu-\npervised semantic segmentation by contrasting object mask proposals. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision, pp. 10052\u201310062, 2021.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-\ntion processing systems, 30, 2017.\nAndrey Voynov, Stanislav Morozov, and Artem Babenko. Object segmentation without labels with\nlarge-scale generative models. In International Conference on Machine Learning, 2020. URL\nhttps://api.semanticscholar.org/CorpusID:235417600.\nAndrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir Aberman. P+: Extended textual condi-\ntioning in text-to-image generation. 2023.\nXudong Wang, Rohit Girdhar, Stella X Yu, and Ishan Misra. Cut and learn for unsupervised object\ndetection and instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 3124\u20133134, 2023.\nY. Wang, X. Shen, S. Hu, Y. Yuan, J. L. Crowley, and D. Vaufreydaz.\nSelf-supervised trans-\nformers for unsupervised object discovery using normalized cut. In 2022 IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR), pp. 14523\u201314533, Los Alamitos,\nCA, USA, jun 2022. IEEE Computer Society.\ndoi: 10.1109/CVPR52688.2022.01414.\nURL\nhttps://doi.ieeecomputersociety.org/10.1109/CVPR52688.2022.01414.\nXin Wen, Bingchen Zhao, Anlin Zheng, Xiangyu Zhang, and XIAOJUAN QI. Self-supervised\nvisual representation learning with semantic grouping. In Alice H. Oh, Alekh Agarwal, Danielle\nBelgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022.\nURL https://openreview.net/forum?id=H3JObxjd8S.\nWeijia Wu, Yuzhong Zhao, Mike Zheng Shou, Hong Zhou, and Chunhua Shen. Diffumask: Syn-\nthesizing images with pixel-level annotations for semantic segmentation using diffusion models,\n2023.\nJianjin Xu and Changxi Zheng. Linear semantics in generative adversarial networks. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9351\u20139360, 2021.\nJiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, and Xiaolong\nWang. Groupvit: Semantic segmentation emerges from text supervision. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18134\u201318144, 2022a.\n13\nPublished as a conference paper at ICLR 2024\nJiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. Open-\nvocabulary panoptic segmentation with text-to-image diffusion models. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2955\u20132966, 2023a.\nJilan Xu, Junlin Hou, Yuejie Zhang, Rui Feng, Yi Wang, Yu Qiao, and Weidi Xie. Learning open-\nvocabulary semantic segmentation models from natural language supervision. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2935\u20132944, 2023b.\nMengde Xu, Zheng Zhang, Fangyun Wei, Yutong Lin, Yue Cao, Han Hu, and Xiang Bai. A simple\nbaseline for open-vocabulary semantic segmentation with pre-trained vision-language model. In\nEuropean Conference on Computer Vision, pp. 736\u2013753. Springer, 2022b.\nMengde Xu, Zheng Zhang, Fangyun Wei, Han Hu, and Xiang Bai. Side adapter network for open-\nvocabulary semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 2945\u20132954, 2023c.\nZhaoyuan Yin, Pichao Wang, Fan Wang, Xianzhe Xu, Hanling Zhang, Hao Li, and Rong Jin. Trans-\nfgu: a top-down approach to fine-grained unsupervised semantic segmentation. In European\nconference on computer vision, pp. 73\u201389. Springer, 2022.\nAndrii Zadaianchuk, Matthaeus Kleindessner, Yi Zhu, Francesco Locatello, and Thomas Brox.\nUnsupervised semantic segmentation with self-supervised object-centric representations.\nIn\nThe Eleventh International Conference on Learning Representations, 2023.\nURL https:\n//openreview.net/forum?id=1_jFneF07YC.\nJunyi Zhang, Charles Herrmann, Junhwa Hur, Luisa Polania Cabrera, Varun Jampani, Deqing Sun,\nand Ming-Hsuan Yang. A tale of two features: Stable diffusion complements dino for zero-shot\nsemantic correspondence. 2023.\nYuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-Francois Lafleche, Adela Barriuso, Antonio\nTorralba, and Sanja Fidler. Datasetgan: Efficient labeled data factory with minimal human effort.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n10145\u201310155, 2021.\nBolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba.\nSemantic understanding of scenes through the ade20k dataset. International Journal of Computer\nVision, 127:302\u2013321, 2019.\nChong Zhou, Chen Change Loy, and Bo Dai. Extract free dense labels from clip. In European\nConference on Computer Vision, pp. 696\u2013712. Springer, 2022.\nA\nADDITIONAL IMPLEMENTATION DETAILS\nIn order to process real images with SD, each dimension of images is required to be a multiple of\n64. Furthermore, the size of images should be adjusted so that it won\u2019t be too large or too small to\nprocess with SD. To meet these requirements, we resize the images to keep pixel count roughly 5122\nwhile keeping the original height-to-width ratio. We then round up each dimension to the nearest\ninteger divisible by 64. When computing mIoU, the obtained segmentation maps are resized back\nto the original size via nearest neighboring.\nB\nADDITIONAL DETAILS OF UNSUPERVISED SEMANTIC SEGMENTATION\nImplementation details. The traditional evaluation protocol (Ji et al., 2019) requires that the model\nclassifies pixels into the same number of semantic concepts as the dataset classes so that it can\nsubsequently match the semantic concepts with the pre-defined dataset classes through Hungarian\nmatching (Here we maximize mIoU). To fulfill this requirement, previous works learn both pixel\nembeddings and concept embeddings so that it can classify each pixel into the concept having the\nclosest concept embedding from the pixel embedding. In particular, the current state-of-the-art\nmodel, STEGO (Hamilton et al., 2022), first trains an image encoder that produces pixel embeddings\n14\nPublished as a conference paper at ICLR 2024\nFigure 9: Drawbacks of the traditional evaluation protocol of unsupervised semantic segmen-\ntation. Evaluated on COCO-Stuff-27. Our framework partitions a person into head, arm, and body\nbut there is only one pre-defined class for person in the dataset, forcing other parts to be mismatched\nwith irrelevant classes.\nFigure 10: Unsupervised semantic segmentation under our modified unsupervised semantic\nsegmentation evaluation protocol. Evaluated on COCO-Stuff-27. In contrast to Figure 9, our\nframework partitions objects following the pre-defined classes in the dataset (e.g., one group for a\nperson), evaluating the quality of our segmentaion masks more appropriately.\nand then clusters pixel embeddings over the entire dataset, obtaining cluster centroids that serve as\nthe concept embeddings.\nTo adapt our framework to this task, we follow STEGO\u2019s procedure and extend our framework by\ngenerating pixel/concept embeddings. To create pixel embeddings, we generate a mask embedding\nfor each produced mask using SD\u2019s feature maps. Specifically, we generate a mask embedding\ne \u2208 Rc of a mask M \u2208 {0, 1}H\u00d7W by taking the average of SD\u2019s feature maps F \u2208 Rc\u00d7h\u00d7w inside\nthe masked region m \u2208 {0, 1}h\u00d7w where F is the query vectors extracted from the first cross-\nattention layer of upward 16 \u00d7 16 at timestep t = 200, and m is the M\u2019s low-resolution mask. Note\nthat these designs are largely borrowed from the previous work (Karazija et al., 2023) that produces\nmask embeddings using SD\u2019s feature maps for classification purposes. Once the mask embeddings\nare generated, each pixel embedding is defined as the embedding of the mask it belongs to. To\ngenerate concept embeddings under the traditional evaluation protocol, we run k-means on the pixel\nembeddings across the entire dataset and extract their cluster centroids.\nIssues of traditional evaluation protocol. To attain high mIoU in this evaluation protocol, it unwill-\ningly becomes crucial to generate concept embeddings that align well with the pre-defined dataset\nclasses. Since we are not allowed to access ground truth labels for building concept embeddings, this\nrequirement becomes unreasonably strict and makes evaluation unreliable, especially when there are\nmultiple valid ways to semantically partition objects. For instance, COCO-Stuff 27 (Caesar et al.,\n2018) is the popular dataset used in unsupervised semantic segmentation, where it only consists of\none class referring to person; however, as visualized in Figure 9, our framework produces at least\nthree semantic concepts for a person: head, body, and arms. Since only one concept can be matched\nwith the person class, other parts are forced to be mismatched with irrelevant labels, resulting in a\ndecrease in mIoU.\nModifying traditional evaluation protocol. Since our focus is not on estimating dataset classes but\non properly evaluating the quality of our produced segmentation masks, we pre-calculate concept\nembeddings such that each embedding corresponds to each dataset class. To achieve this, we relax\nthe restriction of the traditional evaluation protocol by allowing access to the ground truth annota-\ntions while constructing concept embeddings. Specifically, for each dataset class, we aggregate all\n15\nPublished as a conference paper at ICLR 2024\nFigure 11: Examples of unsupervised semantic segmentation on COCO171 and ADE150K, evalu-\nated under our modified evaluation protocol.\npixel embeddings belonging to the class from the entire dataset and compute their average. The ob-\ntained mean embedding serves as the concept embedding of this class. As visualized in Figure 10,\nthis modified evaluation protocol enables our framework to partition the objects according to the\npre-defined class categories in the dataset, and the performance difference primarily comes from the\nquality of underlying pixel embeddings rather than the concept embeddings. Additional qualitative\nresults can be found in Figure 11.\nC\nADDITIONAL DETAILS OF OPEN-VOCABULARY SEMANTIC SEGMENTATION\nImplementation details. As discussed in Cha et al. (2023), there exists no unified evaluation proto-\ncol for open-vocabulary semantic segmentation, and different studies often employ varying prompt\nengineering strategies. For a fair comparison, we re-evaluated our baselines with the same class\nlabels and prompt templates. Specifically, following Mukhoti et al. (2023), we use 7 prompt tem-\nplates: \u201ditap of a {}.\u201d, \u201da bad photo of a {}.\u201d, \u201da origami {}.\u201d, \u201da photo of the large {}.\u201d, \u201da {}\nin a video game.\u201d,\u201dart of the {}.\u201d, \u201da photo of the small {}.\u201d, where {} is replaced with the class\n16\nPublished as a conference paper at ICLR 2024\nTable 4: Effects of varying the number of masks on open-vocabulary segmentation and unsupervised\nsemantic segmentation tasks.\nmIoU (\u2191)\nOpen-vocabulary seg.\nUnsupervised seg.\nMaskCLIP + Ours\nTCL + Ours\nOurs\n# masks\nAD150\nCS171\nCity19\nAD150\nCS171\nCity19\nAD150\nCS171\nCity19\n10\n15.7\n21.5\n24.2\n17.2\n22.3\n21.8\n30.9\n30.0\n34.0\n20\n16.0\n21.1\n25.9\n17.6\n22.1\n23.3\n32.9\n30.4\n36.3\n30\n15.9\n20.7\n26.5\n17.4\n21.8\n23.4\n33.1\n30.5\n37.1\n40\n15.6\n20.4\n26.4\n17.3\n21.6\n23.5\n33.4\n30.3\n37.3\nTable 5: Effects of extracting feature maps from each cross-attention layer in 16 \u00d7 16 upward\nblock. Evaluated on ADE150K. No significant differences in performance.\nmIoU (\u2191)\nlayer 1\nlayer 2\nlayer 3\nUnsupervised seg.\n33.1\n33.1\n32.8\nnames. Note that the purpose of this evaluation is not to achieve the state-of-the-art results in open-\nvocabulary semantic segmentation, but quantitatively evaluate the preciseness of our segmentation\nmaps by combining them with the noisy feature maps produced by the baselines.\nD\nHYPERPARAMETER ANALYSIS\nIn this section, we analyze how the selection of the hyperparameters in our framework affects the\nsegmentation quality. In this analysis, unsupervised semantic segmentation is evaluated under our\nmodified evaluation protocol.\nNumber of masks per image. We first ensure that our quantitative results are not sensitive to the\nnumber of masks generated per image within a reasonable range (Here we experiment between 10 to\n40). As presented in Table 4, we observe only a marginal difference in mIoU for both unsupervised\nsemantic segmentation and open-vocabulary semantic segmentation, confirming the fairness and\nrobustness of our experiment setup.\nSelection of cross-attention layer to extract feature maps. In Section 3.2, we extract feature maps\nfrom one of the cross-attention layers to build low-resolution segmentation maps. Our selection is\nbased on the previous observation (Luo et al., 2023; Karazija et al., 2023) that layers at 16 \u00d7 16\nblocks are most semantically meaningful. Note that there are three cross-attention layers at 16 \u00d7 16\nresolution on the upward path. However, as reported in Table 5, we observed similar performance\nregardless of which 16 \u00d7 16 cross attention layer was selected.\nTimesteps for extracting feature maps. We experiment with varying the timesteps of extracting\nfeature maps. Recall that the extracted feature maps are used for generating low-resolution segmen-\ntation maps, therefore the feature maps are desired to clearly capture all the contents presented in the\nimages. Intuitively, adding large noise (i.e., large timestep) ambiguates object layouts and should be\navoided. Indeed, as visualized in Figure 12, when the timestep increases, our framework struggles to\ndistinguish neighbouring objects in the images. Table 6 also highlights the performance degradation\nwhen adding larger noise.\nModulation timestep and strength. The influence of text captions on the generated images is\nknown to vary throughout the denoising process (Balaji et al., 2022; Hertz et al., 2022; Patashnik\net al., 2023). Given that our modulation alters the feature maps of cross-attention layers, where the\ntext captions interact with the U-Net\u2019s feature maps, the influence of our modulation is also expected\nto be different throughout the timesteps. As visualized in Figure 13, when the timestep is too small,\nour modulation did not effectively affect the pixel values of the corresponding semantic objects and\nproduced coarse segmentation masks. Conversely, when the timestep is too large, some segmen-\n17\nPublished as a conference paper at ICLR 2024\nFigure 12: Varying the timesteps of extracting feature maps. As the timestep grows, the generated\nsegmentation maps fail to distinguish neighboring objects.\nTable 6: Varying the timestep of extracting feature maps. Evaluated on unsupervised semantic\nsegmentation and open-vocabulary semantic segmentation (MaskCLIP + Ours) w/ ADE20K\nmIoU (\u2191)\nTimestep\n1\n201\n401\n601\n801\nUnsupervised seg.\n33.1\n32.8\n31.8\n29.8\n26.9\nOpen-vocabulary seg.\n15.9\n15.8\n15.7\n15.2\n13.9\nFigure 13: Effects of modulation timestep tm vs modulation strength \u03bb.\ntation masks were absorbed by others, likely because detailed semantic parts are not distinguished\nat large timesteps. The performance difference is also quantitatively verified in Table 7, where the\nearly-middle timesteps achieve the best performance. Interestingly, our framework consistently gen-\nerates visually decent segmentation maps when the modulation strength \u03bb is large enough. However,\nas reported in Table 8, slight degraded performance is observed when increasing \u03bb excessively. Nev-\nertheless, we find that early-middle timesteps produce perceptually plausible segmentation masks,\nworking effectively with both small and large \u03bb.\nSelection of cross-attention layer to modulate. There are three cross-attention layers in 16 \u00d7 16\nupward modular blocks, where the first and the third ones are known to control generated con-\ntents and appearance respectively (Voynov et al., 2023). In our experiments, we modulate the third\ncross-attention layer by replacing its original computation f\n\u0010\n\u03c3\n\u0010\nQKT\n\u221a\nd\n\u0011\n\u00b7 V\n\u0011\nwith the modulated\ncomputation f\n\u0010\n\u03c3\n\u0010\nQKT\n\u221a\nd\n\u0011\n\u00b7 V\n\u0011\n+ cM (See 3.3 for more details). Here, we experiment with differ-\n18\nPublished as a conference paper at ICLR 2024\nTable 7: Varying the modulation timestep tm. Evaluated on unsupervised semantic segmentation\nw/ ADE20K\nmIoU (\u2191)\ntm\n1\n81\n281\n481\n681\n881\nUnsupervised seg.\n29.0\n31.8\n33.1\n33.1\n31.5\n27.4\nTable 8: Varying the modulation strength \u03bb. Evaluated on unsupervised semantic segmentation\nw/ ADE20K\nmIoU (\u2191)\n\u03bb\n1\n10\n100\n1000\nUnsupervised seg.\n32.7\n33.1\n32.8\n32.6\nFigure 14:\nEffects of modulating different cross-attention layers vs different computation\n(f (\u03c3 \u00b7 V )+cM, f (\u03c3 \u00b7 V + cM)) vs different \u03bb. For cross-attention layers, we experiment with the\nthree different layers in 16 \u00d7 16 upward modular blocks. Note that we abbreviate \u03c3\n\u0010\nQKT\n\u221a\nd\n\u0011\nto \u03c3 for convenience.\nent modulated computation and cross-attention layers. Specifically, for modulated computation, we\nconsider f\n\u0010\n\u03c3\n\u0010\nQKT\n\u221a\nd\n\u0011\n\u00b7 V + cM\n\u0011\n, which adds an offset c before the fully connected layer f. As\nvisualized in Figure 14, both computations produce visually plausible segmentation maps, indicat-\ning that our framework is not sensitive to the place of offset injection. For the choice of different\ncross attention layers, we observe a clear difference between the first layer and the third layer, likely\nbecause these layers are responsible for different attributes of the generated images and image pixels\nare affected differently. Overall, the third cross-attention layer produces relatively decent segmen-\ntation maps regardless of the modulated computations and \u03bb. We conjecture that modulating the\nthird layer naturally leads to changing the values of the pixels of semantic objects, which might be\nattributed to the fact that the third layer is controlling appearance.\n19\nPublished as a conference paper at ICLR 2024\nTable 9: Ablating attention injection. Evaluated on unsupervised semantic segmentation and open-\nvocabulary semantic segmentation (MaskCLIP + Ours) w/ ADE20K\nmIoU (\u2191)\nUnsupervised seg.\nOpen-vocabulary seg.\nOurs w/o attention injection\n31.8\n15.6\nOurs\n33.1\n15.9\nAttention injection. In Section 3.3, we fix the attention maps of all the attention layers to the\noriginal ones during the modulated denoising process. This is a technique used in image editing to\npreserve the structure of the original images. Figure 15 presents the comparison between our frame-\nwork with and without the attention injection. While both produced visually plausible segmentation\nmaps, our framework with attention injection preserves more detailed structures than the other (See\nperson in the figure). The effectiveness is also quantitatively evident in Table 9.\nFigure 15: Ablating attention injection.\nE\nADDITIONAL QUALITATIVE ANALYSIS\nThe additional qualitative results of ADE150K (Zhou et al., 2019), PASCAL-Context (Mottaghi\net al., 2014) and COCO-Stuff (Caesar et al., 2018) are shown in Figure 16, 17, 18, respectively.\n20\nPublished as a conference paper at ICLR 2024\nFigure 16: Examples of our produced segmentation maps on ADE20K (Zhou et al., 2019)\n21\nPublished as a conference paper at ICLR 2024\nFigure 17: Examples of our produced segmentation maps on PASCAL-Context (Mottaghi et al.,\n2014)\n22\nPublished as a conference paper at ICLR 2024\nFigure 18: Examples of our produced segmentation maps on COCO-Stuff (Caesar et al., 2018)\n23\n"
  },
  {
    "title": "Make-A-Shape: a Ten-Million-scale 3D Shape Model",
    "link": "https://arxiv.org/pdf/2401.11067.pdf",
    "upvote": "15",
    "text": "Make-A-Shape: a Ten-Million-scale 3D Shape Model\nKa-Hei Hui*1\nAditya Sanghi*2\nArianna Rampini2\nKamal Rahimi Malekshan2\nZhengzhe Liu1\nHooman Shayani2\nChi-Wing Fu1\n*These authors contributed equally.\n1The Chinese University of Hong Kong\n2Autodesk AI Lab\nhttps://edward1997104.github.io/make-a-shape/\nFigure 1. Make-A-Shape is a large 3D generative model trained on over 10 millions diverse 3D shapes. As demonstrated above, it exhibits\nthe capability of unconditionally generating a large variety of 3D shapes over a wide range of object categories, featuring intricate geometric\ndetails, plausible structures, nontrivial topologies, and clean surfaces.\nAbstract\nSignificant progress has been made in training large gen-\nerative models for natural language and images. Yet, the\nadvancement of 3D generative models is hindered by their\nsubstantial resource demands for training, along with inef-\nficient, non-compact, and less expressive representations.\nThis paper introduces Make-A-Shape, a new 3D genera-\ntive model designed for efficient training on a vast scale,\ncapable of utilizing 10 millions publicly-available shapes.\nTechnical-wise, we first innovate a wavelet-tree represen-\ntation to compactly encode shapes by formulating the sub-\nband coefficient filtering scheme to efficiently exploit coef-\nficient relations. We then make the representation generat-\nable by a diffusion model by devising the subband coeffi-\ncients packing scheme to layout the representation in a low-\nresolution grid. Further, we derive the subband adaptive\ntraining strategy to train our model to effectively learn to\ngenerate coarse and detail wavelet coefficients. Last, we\nextend our framework to be controlled by additional input\nconditions to enable it to generate shapes from assorted\nmodalities, e.g., single/multi-view images, point clouds, and\nlow-resolution voxels. In our extensive set of experiments,\nwe demonstrate various applications, such as unconditional\ngeneration, shape completion, and conditional generation\non a wide range of modalities. Our approach not only sur-\npasses the state of the art in delivering high-quality results\nbut also efficiently generates shapes within a few seconds,\noften achieving this in just 2 seconds for most conditions.\n1. Introduction\nLarge-scale generative models have increasingly become\nhighly capable of generating realistic outputs [72, 74, 75,\n101], leading to numerous commercial applications in de-\nsign, marketing, e-commerce, and more. This success has\nled to the question of whether it is possible to develop a\nlarge-scale 3D generative model, which could potentially\nexhibit intriguing emergent properties and facilitate a vari-\nety of applications. However, most current 3D generative\nmodels have lagged behind, being either limited in quality,\nfocused on small 3D datasets [6, 18, 28, 32, 56, 82, 100,\n105] or allowing a single condition [25, 33, 40, 43, 62, 98].\nTraining large generative models in 3D, compared to\narXiv:2401.11067v1  [cs.CV]  20 Jan 2024\nFigure 2. Make-A-Shape is able to generate a large variety of shapes for diverse input modalities: single-view images (rows 1 & 2),\nmulti-view images (rows 3 & 4), point clouds (rows 5 & 6), voxels (rows 7 & 8), and incomplete inputs (last row). The resolution of the\nvoxels in rows 7 & 8 are 163 and 323, respectively. In the top eight rows, odd columns show the inputs whereas even columns show the\ngenerated shapes. In the last row, columns 1 & 4 show the partial input whereas the remaining columns show the diverse completed shapes.\nFigure 3. Reconstructing the SDF of a shape (a) using different\nmethods: (b) Point-E [62], (c) Shap-E [33], (d) coarse coefficients\nC0 [28], and (e) our wavelet-tree representation. Our approach (e)\ncan more faithfully reconstruct the shape\u2019s structure and details.\n2D images, faces several significant challenges. First, hav-\ning an extra spatial dimension in 3D substantially increases\nthe number of input variables that require a neural net-\nwork to model, resulting far more network parameters.\nThis is particularly evident in U-Net-based diffusion mod-\nels [24, 83, 84], which generate memory-intensive feature\nmaps that are often too large for GPUs to process, thus pro-\nlonging the training time [26]. Second, scaling a genera-\ntive model to 3D introduces data handling complexities not\npresent with 2D images. Most storage and data handling for\ntraining large models takes place on cloud services such as\nAWS or Azure. 3D data escalates the cost of storage and\ntime to download the data in each training iteration. Third,\nthere are many ways to represent 3D shapes. It remains un-\nclear which one best achieves high representation quality\nwhile maintaining a good representation compactness for\nefficient training.\nRecent large-scale generative models for 3D shapes\ntackle these issues through two main strategies. The first\nemploys lossy input representations to reduce the number\nof input variables that the model must process. However,\nit comes at the expense of omitting important details, thus\nfailing to faithfully capture the shape.\nKey examples of\nthis strategy are Point-E [62], which utilizes point clouds\nas input, and Shap-E [33], which uses latent vectors. Their\ntrade-off is evident in Figure 3 and Table 1, where a signifi-\ncant loss of detail is often observed when reconstructing the\nground-truth signed distance function (SDF). The second\nstrategy employs multi-view images to represent the geom-\netry [25, 40, 43, 98]. In this approach, a generative net-\nwork utilizes differentiable rendering to produce images of\nthe generated shape for comparing with ground-truth multi-\nview images to facilitate learning. These methods gener-\nally require extensive training time, as they use differen-\ntiable rendering for loss calculation, which can be slow and\nTable 1.\nComparing different 3D representations on the GSO\ndataset [15] in terms of Intersection Over Union (IOU) and pro-\ncessing time. \u201cExtra Network\u201d denotes the need for training mul-\ntiple networks to obtain SDF; \u201cProcess Time\u201d refers to the time\nrequired to convert from one representation to SDF; and \u201cInput\nVariable\u201d reports the number of floating-point numbers adopted in\neach representation. It is noteworthy that our representation has\na similar parameter count as Shap-E [33], yet it does not need an\nextra network and achieves faster conversion.\nRepresentation\nIOU\nInput Variables\nExtra Network\nProcess Time\nGround-truth SDF (2563)\n1.0\n16777216\nNo\n\u2212\nPoint-E [62]\n0.8642\n12288\nYes\n\u223c1 second\nShap-E [33]\n0.8576\n1048576\nYes\n\u223c5 minutes\nCoarse Component [28]\n0.9531\n97336\nNo\n\u223c1 second\nWavelet tree (ours)\n0.9956\n1129528\nNo\n\u223c1 second\nmay not capture full geometry in one training example. Our\nframework, on average, processes 2x to 6x more training\nshapes in one day than these methods, despite utilizing a\nless powerful GPU (A10G vs. A100), as detailed in Table 2.\nThese issues arise due to the lack of a suitable 3D rep-\nresentation that is expressive, compact, and efficient to\nlearn.\nIn this work, we introduce a new 3D representa-\ntion, the wavelet-tree representation, designed for encoding\n3D shapes for large-scale model training. This representa-\ntion employs a wavelet decomposition on a high-resolution\nSDF grid to yield a coarse coefficient subband and multiple\nmultiscale detail coefficient subbands. Beyond [28], which\ndiscards all detail subbands for efficient generation, we de-\nsign a family of techniques to enable large model training,\nconsidering both coarse and detail coefficients: (i) subband\ncoefficients filtering to identify and retain information-rich\ndetail coefficients in the detail subbands, such that our rep-\nresentation can compactly include more shape details; (ii)\nsubband coefficients packing to rearrange the wavelet-tree\nrepresentation in a low-resolution spatial grid, such that the\nre-arranged representation can become diffusible, i.e., gen-\neratable by a diffusion model; and (iii) subband adaptive\ntraining strategy to enable efficient model training on both\ncoarse and detail coefficients, such that the training can at-\ntend to the overall shape and also the important but sparse\nshape details.\nBesides, we formulate various condition-\ning mechanisms to accommodate flexibly input conditions,\nsuch as point clouds, voxels, and images. Hence, our new\nrepresentation, while being compact, can faithfully retain\nmost shape information and facilitate effective training of a\nlarge generative model on over millions of 3D shapes.\nWith the above technical contributions, we can generate\na representation that is notably expressive, capable of en-\ncoding a shape with minimal loss; for example, a 2563 grid\ncan be bijectively encoded into the wavelet-tree represen-\ntation in around one second, yet with an IoU of 99.56%.\nSimultaneously, our representation is compact, character-\nized by a low number of input variables. This is almost\nTable 2.\nEfficiency comparison with state-of-the-art methods.\nThe results for rows 4-6 are sourced from the concurrent works,\nDMV3D [98], Instant3D [40], and LRM [25], respectively. For\nsingle-view, we present inference time for both 10 and 100 itera-\ntions (iter.), with the latter being the optimal hyperparameter for\nquality, as per our ablation study. For multi-view, 10 iterations\nis identified as the optimal. For the training time, since different\nmethods use different number of GPUs, we compare their training\nspeed by the number of training shapes that it can process in one\nday divided by the number of GPUs used. Note that training time\nis not available for Point-E [62] and Shap-E [33].\nMethod\nInference time # Training shapes in 1 day / GPU\nPoint-E [62]\n\u223c 31 sec\n\u2212\nShape-E [33]\n\u223c 6 sec\n\u2212\nOne-2-3-45 [43]\n\u223c 45 sec\n\u223c 50k (A10G)\nDMV3D [98]\n\u223c 30 sec\n\u223c 110k (A100)\nInstant3D [40]\n\u223c 20 sec\n\u223c 98k (A100)\nLRM [25]\n\u223c 5 sec\n\u223c 74k (A100)\nOurs (single-view 10 iter.)\n\u223c 2 sec\nOurs (single-view 100 iter.)\n\u223c 8 sec\n\u223c 290k (A10G)\nOurs (multi-view 10 iter.)\n\u223c 2 sec\n\u223c 250k (A10G)\nakin to lossy representations like latent vectors [33], yet it\nachieves this without necessitating additional training of an\nautoencoder, while having higher quality, as shown in Ta-\nble 1. Last, our representation is efficient, enabling efficient\nstreaming and training. For instance, streaming and loading\na sophisticatedly compressed 2563 SDF grid takes 266 mil-\nliseconds, while our representation requires only 184 mil-\nliseconds for the same process. The 44.5% reduction in data\nloading time is crucial for large-scale model training.\nOverall, our generative model can be trained effectively,\nenabling also fast inference and taking just few seconds to\ngenerate high-quality shapes, as compared to existing meth-\nods reported in Table 2. We name our proposed genera-\ntion framework Make-A-Shape. This framework facilitates\nthe training of an unconditional generative model and ex-\ntended models under different input conditions on an exten-\nsive dataset comprising 10 million 3D shapes over a wide\nrange of object categories. It successfully produces a range\nof plausible shapes, as illustrated in Figures 1 and 2.\n2. Related Work\nNeural Shape Representations. In recent years, there has\nbeen significant research in integrating deep learning with\nvarious 3D representations. Initial works such as [54, 95]\nfocused on volumetric representations for different tasks\nthrough the use of 3D convolutional networks. Multiview\nCNNs were also explored [66, 86], by means of first render-\ning 3D shapes into multiple images, upon which 2D CNNs\nare applied for use in downstream applications.\nSubsequently, deep learning methods for point clouds\nwere introduced in PointNet [67], and later, additional in-\nductive biases such as convolution were adopted, as seen\nin [68, 90]. Neural networks can also be employed to rep-\nresent 3D shapes by predicting either signed distance func-\ntions (SDFs) or occupancy fields. These representations,\ntypically known as neural implicit representations, have\nbeen a popular subject of exploration [6, 56, 63]. Other\nexplicit representations such as meshes have been explored\nin works such as [21, 53, 61, 88]. Another prevalent 3D\nrepresentation, boundary representation (BREP), has only\nbeen examined recently in studies such as [31, 32, 38, 93]\nfor discriminative and generative applications.\nRecently, some works [28, 47] have investigated the use\nof wavelets to decompose an SDF signal into multi-scale\nwavelet coefficients.\nThese methods, however, filter out\nhigh-frequency details to enhance learning efficiency, albeit\nat the expense of shape fidelity. In this work, we introduce\na novel representation known as the wavelet-tree represen-\ntation. We consider both coarse and information-rich detail\ncoefficients to compactly yet nearly losslessly encode 3D\nshapes. Enabled by various techniques that we shall intro-\nduce, our representation enables high-quality shape genera-\ntion, while remaining compact for scalability across a large\n3D data comprising over millions of shapes.\n3D Generative Models. The initial efforts in the field of\n3D generative models primarily concentrated on Generative\nAdversarial Networks (GANs) [19, 92]. Subsequently, au-\ntoencoders are trained and GANs are then utilized to pro-\ncess the latent spaces of these autoencoders, enabling gen-\nerative models on representations such as point clouds [1]\nand implicit representations [6, 29, 106]. More recent stud-\nies [2, 18, 79] incorporated GANs with differentiable ren-\ndering, where multiple rendered views are employed as the\nloss signal. There has also been a focus on normalizing\nflows [35, 76, 100] and Variational Autoencoder (VAE)-\nbased generative models [60]. Autoregressive models have\ngained popularity in 3D generative modeling and have been\nextensively explored [8, 59, 61, 77, 87, 99, 103].\nWith recent advances in diffusion model for high-quality\nimage generation, there has also been immense interest in\ndiffusion models for 3D context. Most approaches first train\na Vector-Quantized-VAE (VQ-VAE) on a 3D representation\nsuch as triplane [10, 64, 82], implicit form [9, 41, 104]\nand point cloud [33, 102], before employing the diffusion\nmodel to the latent space. Direct training on a 3D repre-\nsentation has been less explored. Some recent studies fo-\ncus on point clouds [52, 62, 108], voxels [107], and neu-\nral wavelet coefficients [28, 47]. Our work employs a dif-\nfusion model directly on the 3D representation, thereby\navoiding information loss associated with the VQ-VAE. Be-\nsides formulating our wavelet-tree representation, we pro-\npose a scheme to convert it into a format that can be dif-\nfusible, or effectively generatable by a diffusion model. Our\napproach demonstrates great efficiency at high resolutions\nFigure 4. Overview of our generative approach. (a) A shape is first encoded into a truncated signed distance field (TSDF), then decomposed\ninto multi-scale wavelet coefficients in a wavelet-tree structure. We design the subband coefficient filtering procedure to exploit the relations\namong coefficients and extract information-rich coefficients to build our wavelet-tree representation. (b) We propose the subband coefficient\npacking scheme to rearrange our wavelet-tree representation into a regular grid structure of manageable spatial resolution, so that we can\nadopt a denoising diffusion model to effectively generate the representation. (c) Further, we formulate the subband adaptive training\nstrategy to effectively balance the shape information in different subbands and address the detail coefficient sparsity. Hence, we can\nefficiently train our model on millions of 3D shapes. (d) Our framework can be extended to condition on various modalities.\ncompared to [107], produces cleaner manifold surfaces\nthan [52, 62, 108] and captures far more details than [28].\nConditional 3D Models. Existing conditional models in\n3D can be categorized in two groups. The first group lever-\nages large 2D conditional image generative models, such\nas Stable Diffusion [74] or Imagen [75], to optimize a 3D\nscene or object. These methods create 3D shapes and con-\nvert them to images using a differentiable renderer, such\nthat the images can be either compared to multiple images\nor aligned with the distribution of a large text-to-3D genera-\ntive model. The initial exploration in this area was centered\naround text-to-3D, as seen in [30, 57, 65]. This approach\nwas later expanded to include images [13, 55, 96] and multi-\nview images [12, 44, 69, 81]. Recent methods have also\nincorporated additional conditions such as sketches [58].\nOverall, this approach unavoidably requires an expensive\noptimization, which limits practical applications.\nThe second group of methods focuses on training a con-\nditional generative model with data that is either paired\nwith a condition or used in a zero-shot manner.\nPaired\nconditional generative models explore various conditions\nsuch as point cloud [103, 105], image [33, 62, 103, 105],\nlow-resolution voxels\n[5, 7], sketches [17, 20, 37, 51]\nand text [33, 62]. More recently, zero-shot methods have\ngained popularity, with a focus on text [46, 76, 77, 97]\nand sketches [78]. In this work, our primary focus is on\ntraining a large, paired conditional generative model. This\nmodel offers fast generation, as it eliminates the need for\nscene optimization. Our approach also facilitates the easy\nincorporation of assorted conditions, e.g., point clouds, low-\nresolution voxels, and images. Besides, it enables both un-\nconditional applications and zero-shot tasks like shape com-\npletion.\n3. Overview\nFigure 4 provides an overview of our shape-generative\nframework, designed to create a large-scale 3D generative\nmodel capable of efficient training on millions of 3D shapes.\nThe complexity of 3D data makes efficient training at this\nscale extremely challenging, particularly when considering\nthe need to optimize both the quality of shape generation\nand the speed of training. Our approach comprises four\nmain components, detailed in Sections 4 to 7.\n(i) Wavelet-tree representation.\nWe first formulate a\ncompact, efficient and expressive 3D representation to sup-\nport large-scale shape training. Importantly, we first en-\ncode each shape into a high-resolution truncated signed dis-\ntance field (TSDF) and decompose the TSDF into multi-\nscale wavelet coefficients. We design a subband coefficient\nfiltering procedure that exploits the relationships among co-\nefficients, allowing us to retain information-rich wavelet\ncomponents (both coarse and detail) in our wavelet-tree rep-\nresentation, enabling a faithful yet compact representation\nFigure 5. Wavelet decomposition of the input shape, represented\nas a TSDF, recursively into coarse coefficients Ci and detail coef-\nficients {DLH\ni\n, DHL\ni\n, DHH\ni\n}. Note that in the 3D case, there will\nbe seven subbands of detail coefficients in each decomposition.\nof the 3D shape for efficient storage and streaming.\n(ii) Diffusible Wavelet-tree Representation.\nNext, we\ntransform the wavelet-tree representation into a format that\nis more compatible with diffusion models. Though our rep-\nresentation achieves compact shape encoding, its irregu-\nlar format hinders effective shape learning and generation.\nThis motivates us to design the subband coefficient packing\nscheme to rearrange the coefficients into a regular grid of a\nmanageable spatial resolution for shape generation.\n(iii) Subband Adaptive Training Strategy. Further, we ex-\nploit methods to train the model on the diffusible wavelet-\ntree representation.\nIn general, shape information varies\nacross subbands and scales, with detail coefficients being\nhighly sparse yet rich in the shape details. Hence, training\nwith a standard uniform Mean Squared Error (MSE) loss\nmight lead to model collapse or inefficient learning of the\ndetails. To address this, we introduce the subband adaptive\ntraining strategy, which selectively focuses on coefficients\nin various subbands. This approach allows for an effective\nbalance of shape information over coarse to fine subbands\nduring the training and encourages the model to learn both\nthe structural and detailed aspects of shapes.\n(iv) Extension for Conditional Generation. Finally, be-\nyond unconditional generation, we extend our method to\nsupport conditional generation of shapes, following condi-\ntions such as single-/multi-view images, voxels, and point\nclouds. In essence, we encode the specified conditions into\nlatent vectors and then collectively employ multiple mech-\nanisms to inject these vectors into our generation network.\n4. Wavelet-tree Representation\nTo build our representation, we transform a 3D shape\ninto a truncated signed distance function (TSDF) with a\nresolution of 2563.\nWe decompose the TSDF using a\nwavelet transform1 into a coarse coefficient C0 and a set\nof three detail coefficients {D0, D1, D2}.\nThe process\n1Following the approach in [28], we use biorthogonal wavelets with 6\nand 8 moments.\nFigure 6. Overview of Parent-child relation. A wavelet tree is\nformed for each coefficient in C0 as the root, with a coarser-level\ncoefficient as parent and the finer-level coefficients as children.\nof obtaining these coefficients involves first transforming\nTSDF into C2 and its associated detail coefficients D2 =\nDLH\n2\n, DHL\n2\n, DHH\n2\n. Then, we decompose C2 into C1 and\nD1 = DLH\n1\n, DHL\n1\n, DHH\n1\n, and subsequently, C1 is decom-\nposed into C0 and D0 = DLH\n0\n, DHL\n0\n, DHH\n0\n. This pro-\ncess is depicted in Figure 5. For simplicity, we present our\nmethod using 2D illustrations, yet the actual computation\nis performed in 3D with seven subband volumes (instead of\nthree subband images, in the 2D case) of detail coefficients\nin each decomposition. It is important to note that the detail\ncoefficients contain high-frequency information. Further-\nmore, this representation is lossless and can be bijectively\nconverted to a TSDF through inverse wavelet transforms.\nWavelet-tree and Coefficient relation.\nBuilding upon\nthe neural wavelet representation as proposed in [28], we\npropose to exploit the relationships between wavelet coef-\nficients for our 3D representation. Generally, each coarse\ncoefficient in C0, referred to as a parent, and its associated\ndetail coefficients in D0, known as children, reconstruct the\ncorresponding coefficients in C1 through an inverse wavelet\ntransform. This parent-child relation relationship extends\nbetween D0 and D1, and so forth, as shown by the arrows\nleading from D0 to D1 in Figure 6. Additionally, coef-\nficients sharing the same parent are termed siblings. By\naggregating all descendants of a coefficient in C0, we can\nconstruct a wavelet coefficient tree or simply a wavelet tree,\nwith a coefficient in C0 serving as its root. This concept is\nfurther illustrated in Figure 6.\nObservations.\nWe have identified four notable observa-\ntions regarding the wavelet coefficients:\n(i) If a coefficient\u2019s magnitude is smaller than a thresh-\nold (say, 1/32 of the largest coefficient in a subband),\nits children will likely have small magnitudes. Small\nmagnitude means low contribution to the shape, so\nFigure 7. The detail component part of our representation. We ex-\ntract and pack informative coefficients from D0 and D1, indicated\nin yellow boxes, along with their spatial locations to form our rep-\nresentation\u2019s detail component.\nthese coefficients have little impact on the shape. We\nempirically studied this observation in the D0 sub-\nbands of 1,000 random shapes and found that more\nthan 96.1% of the coefficients satisfy this hypothesis.\n(ii) The values of sibling coefficients are positively cor-\nrelated. We evaluated the correlation coefficients be-\ntween all pairs of sibling coefficients in 1,000 random\nshapes and found a positive correlation value of 0.35.\n(iii) Coefficients in C0 are mostly non-zeros, with a mean\nmagnitude of 2.2, while the mean magnitude of detail\ncoefficients in D0 are much closer to zero, implying\nthat C0 contains most of the shape information.\n(iv) Most coefficients in D2 are insignificant. By empir-\nically setting them to zeros in inverse wavelet trans-\nforms, we can reconstruct the TSDFs faithfully for\n1,000 random shapes with 99.64% IoU accuracy.\nSubband Coefficient Filtering.\nBased on the observa-\ntions, we design the subband coefficient filtering procedure\nto locate and pack information-rich coefficients when build-\ning our representation. First, we keep all the coefficients in\nC0, take them as the coarse component in our representa-\ntion, and exclude all the coefficients in D2, following ob-\nservations (iii) and (iv). Second, C0 alone is insufficient to\ncapture the details; compare Figure 3 (d) vs. (e). We need\nthe coefficients in D0 and D1. However, simply including\nall coefficients in D0 and D1 will lead to a bulky represen-\ntation. Hence, we aim for a compact representation that can\nretain details by following observations (i) and (ii) to exploit\nthe coefficient relations in D0 and D1.\nProcedure-wise, since the subbands of D0 share the same\nresolution and are positively correlated, as analyzed in ob-\nservation (iii), we collectively examine the coefficient lo-\ncations in all subbands of D0 together. For each coefficient\nlocation, we examine the sibling coefficients in DLH\n0\n, DHL\n0\n,\nand DHH\n0\n, selecting the one with the largest magnitude. We\nconsider its magnitude value as the measure of information\nfor that coefficient location. Next, we filter out the top K\ncoefficient locations with the highest information content\n(refer to Figure 7 on the left) and store their location coor-\ndinates and associated coefficient values in D0, along with\ntheir children\u2019s coefficient values in D1. This forms the de-\ntail component in our wavelet-tree representation, as illus-\ntrated in Figure 7 on the right. Together with the coarse\ncomponent, i.e., C0, we construct our wavelet-tree repre-\nsentation. Despite excluding all the coefficients in D2 and\nselecting the top K coefficients in D0, our representation\ncan effectively achieve an impressive mean IOU of 99.56%.\nTo efficiently process millions of 3D shapes, we utilize\nour wavelet-tree representation for encoding each shape,\nsubsequently storing the results in the cloud. This consti-\ntutes a one-time preprocessing step. This aspect is particu-\nlarly crucial for large-scale training, as it results in a 44.5%\nreduction in both data streaming and loading, a significant\nimprovement over directly using the 2563 SDF, as pointed\nout in the introduction. Moreover, we deliberately avoid us-\ning more sophisticated compression techniques due to the\ndecompression overhead they incur, which can significantly\nslow down the model training.\n5. Diffusible Wavelet-tree Representation\nNext, we develop a representation that can be effec-\ntively trained and generated by a diffusion-based genera-\ntive model. This diffusion model is based on the DDPM\nframework [24], which formulates the generative process\nas a Markov chain.\nThis chain comprises two key pro-\ncesses: (i) a forward process, which incrementally intro-\nduces noise into a data sample x0 over T time steps, even-\ntually transforming it into a unit Gaussian distribution, de-\nnoted as p(xT ) \u223c N(0, I); and (ii) a reverse process, which\nis characterized by a generator network \u03b8 tasked to progres-\nsively remove noise from a noisy sample. In our approach,\nthe generator network is designed to predict the original\ndiffusion target x0 directly from the noisy sample xt, ex-\npressed as f\u03b8(xt, t) \u2243 x0. To achieve this, we employ a\nmean-squares loss objective.\nChallenges.\nOur wavelet-tree representation, while being\ncompact and efficient for data streaming, it encounters spe-\ncific challenges during training. This representation con-\nsists of a coarse component, C0, structured as a grid, and a\ndetail component containing three irregular arrays. The de-\ntail component is derived from D0 and D1, as illustrated in\nFigure 7 (right). A straightforward approach is to directly\ntreat this representation as the diffusion target and predict\nthe coarse and detail components using a two-branch net-\nwork. However, it is hard to accurately predict the detail co-\nefficient locations while balancing multiple objectives. We\nempirically observed that this approach struggles with con-\nvergence and leads to the collapse of model training.\nAnother approach we tried is to flatten the extracted co-\nFigure 8.\nDiffusible wavelet representation.\nFirst, we unpack\nand flatten the coefficients in our wavelet-tree representation (left).\nFollowing observation (iii), we channel-wise concatenate sibling\ncoefficients to reduce the spatial resolution (right). Here we con-\ncatenate each coefficient in C0 with its three children in D0 and\nthe reshaped descendants in D1 (each of size 1\u00d71\u00d74).\nefficients in our representation to avoid the irregularities in\nthe detail component. As Figure 8 (left) shows, we first ar-\nrange the coarse component C0 at top left, then pack D0\nand D1 successively around C0 by arranging the extracted\ndetail coefficients at their respective locations in each sub-\nband while leaving the remaining locations as zeros.\nIn\nthis way, the input representation becomes a regular 2D\ngrid for the DDPM to model. However, this representa-\ntion is spatially very large. The current U-Net architecture,\nwidely adopted by existing diffusion models, creates GPU\nmemory-intensive feature maps, which can lead to out-of-\nmemory issues and result in low computational intensity,\nthus leading to poor utilization of the accelerator [26]. Con-\nsequently, model training remains intractable.\nSubband Coefficients Packing.\nTo address these chal-\nlenges, we draw inspiration from recent work on efficient\n2D image generation [26]. Our motivation is further bol-\nstered by observation (iii), which highlights the relation-\nship between coefficients. This insight enables us to effec-\ntively pack sibling subband coefficients, exhibiting similar\nstructures (as illustrated in D0 and D1 of Figure 6), along\nwith their corresponding children. These children are re-\nshaped into a 1 \u00d7 1 \u00d7 4 format, as demonstrated in Fig-\nure 8 (right), and integrated into the channel dimension.\nIn this approach, the resulting representation (the diffusion\nmodel target) adopts a grid structure with reduced spatial\nresolution but an increased number of channels.\nHence,\nthis allows us to circumvent the use of memory-intensive\nfeature maps, avoiding out-of-memory issues and enabling\nmore efficient computation when training on millions of 3D\nshapes. Using this strategy in our 3D representation can ap-\nproximately lead to a cubic-order speedup and a significant\nreduction in GPU memory usage, estimated to be around\n64x, when applied to the same network architecture.\n6. Subband Adaptive Training Strategy\nIn this step, our goal is to effectively train our diffusion\nmodel on this diffusible wavelet-tree representation. A key\nchallenge for our model is ensuring that it can proficiently\ngenerate both the coarse and detailed components of the\nrepresentation. An obvious choice for this purpose would\nbe to employ a standard mean squared error (MSE) loss on\nthe coefficient set X:\nLMSE(X) :=\n1\n|X|\nX\nx0\n||f\u03b8(xt, t) \u2212 x0||2 , x0 \u2208 X,\n(1)\nwhere xt is the noised coefficient of x0 at time step t. Em-\npirically, we observed that naively applying MSE to all co-\nefficients results in significant quality degradation during\ntraining, as demonstrated in our ablation studies. We at-\ntribute this to two primary factors. Firstly, there is an im-\nbalance in the number of coefficients across different scales.\nIn the 2D case, the ratio of |C0| : |D0| : |D1| is 1 : 3 : 12,\nand in the 3D case, it is 1 : 7 : 56. Consequently, using\na uniform MSE loss tends to disproportionately emphasize\nthe fine-detail coefficients, even though the core shape in-\nformation is more densely represented in C0 than in D0\nand D1. Secondly, the majority of the detail coefficients\nare close to zeros, with only a few having high magnitude\nor information which contain maximum high-frequency in-\nformation about the shape. Therefore, uniformly sampling\na loss across these coefficients can result in a sub-optimal\ntraining mechanism due to the imbalance in the number of\nhigh-magnitude detail coefficients.\nAn initial approach to tackle the issue involves defining\nthree separate MSE losses for C0, D0, and D1, then com-\nbining these losses with equal weights. However, this ap-\nproach still treats the losses on detail coefficients uniformly,\nwithout resolving the imbalance issue due to sparsity in the\ndetail subbands. This oversight is empirically evidenced by\nthe subpar performance observed in our ablation study.\nSubband Adaptive Training.\nTo address these issues, we\ndevelop a subband adaptive training strategy. This approach\nis specifically designed to focus more effectively on the\nhigh magnitude detail coefficients while still maintaining a\nbalanced consideration for the other remaining detail co-\nefficients. This ensures that they are not completely over-\nlooked. Specifically, for each subband in D0, say DLH\n0\n, we\nfirst locate the coefficient of the largest magnitude in DLH\n0\n.\nDenoting v as its magnitude value, we then identify all co-\nefficients in DLH\n0\nwith magnitude larger than v/32; we re-\ngard these coefficients as important, and record their spatial\nlocations into coordinate set P LH\n0\n. Similarly, we can ob-\ntain coordinate sets P HL\n0\nfor DHL\n0\nand P HH\n0\nfor DHH\n0\n. As\nsibling coefficients are positively-correlated, we union the\nthree coordinate sets into coordinate set P0, which records\nthe spatial locations of the important detail coefficients.\nOn the other hand, we define P \u2032\n0 as the coordinate set\ncomplement to P0 in the spatial domain of the D0 subband.\nUsing P0 and P \u2032\n0, we can then formulate the training loss as\nLMSE(C0)+ 1\n2\nh X\nD\nLMSE(D[P0])+\nX\nD\nLMSE(R(D[P \u2032\n0]))\ni\n,\n(2)\nwhere D is a subband in {D0, D1}; D[P] denotes D\u2019s coef-\nficients at locations in P; and R is a function that randomly\npicks some of the coefficients in D[P \u2032\n0]. Importantly, the\nnumber of coefficients picked by R is |P0|, such that we can\nbalance the last two terms in our loss with the same number\nof coefficients. Note that using random sampling can effec-\ntively regularize the less important small coefficients while\nnot completely ignoring them in the training. Also, P0 in-\ncludes far less coefficients than the whole domain of D0\n(only \u223c 7.4%), so the model training can effectively focus\non the crucial information while attending to the details.\nEfficient Loss Computation.\nTo calculate the losses, we\nmay store P0 and P \u2032\n0 as irregularly-sized coordinate sets and\nemploy slicing to process the generation target and network\nprediction. Doing so is, however, highly inefficient, as it re-\nquires distinct computations for different data samples and\nprevents us from leveraging code compilation features for\ntraining speedup.\nTo improve the model training efficiency, we propose to\nuse a fixed-size binary mask to represent the coordinate set,\nwhere a value of one indicates the associated location as\nselected. The MSE loss can then be efficiently calculated by\nmasking both the generation target and network prediction.\nThis approach eliminates the need for irregular operations\nand allows for efficient use of code compilation in PyTorch\nto accelerate the training.\n7. Extension for Conditional Generation\n. Our framework is versatile and can be extended beyond\nunconditional generation to accommodate conditional gen-\neration across various modalities. To achieve this, we adopt\ndifferent encoder for each modality that transforms a given\ncondition into a sequence of latent vectors. Subsequently,\nthese vectors are injected into the generator using multi-\nple conditioning mechanisms. We also use a classifier-free\nguidance mechanism [23], which has empirically demon-\nstrated greater effectiveness in conditional settings.\nCondition Latent Vectors. We deliberately convert all in-\nput conditions into a sequence of latent vectors, which we\ncall condition latent vectors, to preserve the generality of\nFigure 9. Our generator network progressively downsamples input\ncoefficients to a bottleneck feature volume (middle). This volume\ngoes through attention layers and deconvolution for upsampling\nto predict the denoised coefficients. If the condition latent vectors\nare available, we simultaneously transform these vectors and adopt\nthem at three locations in our architecture: (i) concatenating with\nthe input noised coefficients (the green arrow); (ii) conditioning\nthe convolution and deconvolution blocks (the blue arrows); and\n(iii) cross-attention with the bottleneck volume (the red arrows).\nour conditioning framework. This approach eliminates the\nneed to devise new specific condition mechanisms to diffu-\nsion model for each modality, thereby enabling our frame-\nwork to function seamlessly across various modalities. Our\nencoder for different modality are described below:\n(i) Single-view image. Given a rendered image of a 3D\nmodel, we utilize the pre-trained CLIP L-14 image\nencoder [70] to process the image. The latent vectors\nextracted from just before the pooling layer of this en-\ncoder are then used as the conditional latent vectors.\n(ii) Multi-view images. We are provided with four images\nof a 3D model, each rendered from one of 55 pre-\ndefined camera poses (selected randomly). To gen-\nerate the conditional latent vectors, we first use the\nCLIP L-14 image encoder to process each rendered\nimage individually to produce an image latent vec-\ntor. Considering the camera poses, we maintain 55\ntrainable camera latent vectors, each corresponding\nto one camera pose and matching the dimensionality\nof the latent vectors encoded by the CLIP image en-\ncoder. For each encoded image latent vector, we re-\ntrieve the corresponding trainable camera latent vec-\ntor based on the camera pose of the image. This cam-\nera vector is then added to each image latent vector in\nthe sequence in an element-wise fashion. Finally, the\nfour processed sequences of latent vectors are con-\ncatenated to form the conditional latent vectors.\n(iii) 3D point cloud. We utilize three Multi-Layer Percep-\ntron (MLP) layers to first transform the given point\ncloud into feature vectors like PointNet [67]. These\nvectors are then aggregated using the PMA block\nform the Set Transformer layer [39], resulting in se-\nqunece of latent vectors that serve as the condition.\n(iv) Voxels. We utilize two 3D convolution layers to pro-\ngressively downsample the input 3D voxels into a 3D\nfeature volume.\nThis volume is subsequently flat-\ntened to form the desired conditional latent vectors.\nNetwork Architecture. Figure 9 illustrates the network ar-\nchitecture of our generator. The main branch, highlighted\nby yellow boxes, adopts a U-ViT architecture [26]. The\nnetwork uses multiple ResNet convolution layers for down-\nsampling our noised coefficients into a feature bottleneck\nvolume, as shown in the middle part of Figure 9. Follow-\ning this step, we apply a series of attention layers to the\nvolume.\nThe volume is then upscaled using various de-\nconvolution layers to produce the denoised coefficients. A\nkey feature of our architecture is the inclusion of learnable\nskip-connections between the convolution and deconvolu-\ntion blocks, which have been found to enhance stability and\nfacilitate more effective information sharing [27].\nMoreover, when condition latent vectors are available,\nwe integrate them into our generation network at three dis-\ntinct locations in the U-ViT architecture, as depicted in the\nbottom part of Figure 9. Initially, these latent vectors are\nprocessed through MLP layers and a pooling layer to yield\na single latent vector (highlighted by the green arrow in\nthe left section of Figure 9). This vector is subsequently\nconcatenated as additional channels of the input noise co-\nefficients. Second, following a similar process, we convert\ncondition latent vectors to another latent vector. However,\nthis vector is utilized to condition the convolution and de-\nconvolution layers via modulating the affine parameters of\ngroup normalization layers [14]. This integration is repre-\nsented by the blue arrows in Figure 9. Lastly, to condition\nthe bottleneck volume, an additional positional encoding is\napplied to the condition latent vectors in an element-wise\nfashion. These vectors are then used in a cross-attention\noperation with the bottleneck volume, as indicated by the\nred arrows in Figure 9.\n8. Results\nIn this section, we begin by presenting the experimental\nsetup, followed by both quantitative and qualitative results\nobtained using various input conditions. Further, we show-\ncase that our generative model is adaptable to shape com-\npletion tasks without additional training. Last, we present\ncomprehensive analysis and ablations on our framework.\n8.1. Experimental Setup\nDataset.\nWe compile a new very large-scale dataset,\nconsisting of more than 10 million 3D shapes, from 18\nexisting publicly-available sub-datasets: ModelNet [89],\nShapeNet [3], SMLP [48], Thingi10K [109], SMAL [110],\nCOMA [73], House3D [94], ABC [36], Fusion 360 [91],\n3D-FUTURE\n[16],\nBuildingNet\n[80],\nDeformingTh-\nings4D [42], FG3D [45], Toys4K [85], ABO [11], In-\nfinigen [71], Objaverse [12], and two subsets of Objaver-\nseXL [12] (Thingiverse and GitHub).\nAmong the sub-\ndatasets, some contain specific object classes, e.g., CAD\nmodels (ABC and Fusion 360), furniture (ShapeNet, 3D-\nFUTURE, ModelNet, FG3D, and ABO), humans (SMLP\nand DeformingThings4D), animals (SMAL and Infinigen),\nplants (Infinigen), faces (COMA), houses (BuildingNet and\nHouse3D), etc. Beyond these, the Objaverse and Objaver-\nseXL datasets encompass generic objects collected from the\ninternet, thereby not only covering the aforementioned cate-\ngories but also offering a more diverse range of objects. For\nthe data split, we randomly divided each sub-dataset into\ntwo parts: a training set consisting of 98% of the shapes\nand a testing set comprising the remaining 2%. The final\ntrain and test sets were then compiled by combining the re-\nspective train and test sets from each sub-dataset.\nFor each shape in our dataset, we generate a TSDF and\nits wavelet-tree representation for model training. On the\nother hand, we prepare various additional inputs for the con-\nditional generation tasks. For image inputs, we randomly\nsampled 55 pre-defined camera poses and rendered 55 im-\nages for each object according to these poses, using the\nscripts provided by [33]. For voxel inputs, we prepared two\nsets of voxels at different resolutions (163 and 323) for each\n3D object and trained separate models for each resolution.\nLastly, we randomly sampled 25,000 points on the surface\nof each 3D shape to generate the point cloud input.\nTraining Details.\nWe train our shape model using the\nAdam Optimizer [34] with a learning rate of 1e-4 and a\nbatch size of 96. To stabilize the training, we employ an\nexponential moving average with a decay rate of 0.9999 in\nthe model updates, in line with existing 2D large-scale dif-\nfusion models [74]. Our model is trained on 48 \u00d7 A10G\nwith 2M-4M iterations, depending on the input condition.\nEvaluation Dataset. For qualitative evaluation, we provide\nvisual results based on the inputs in the unseen test set of\nour compiled large-scale dataset. For quantitative evalua-\ntion, we prepare two evaluation sets for metric computation.\nIn particular, we randomly select 50 shapes from the test set\nof each sub-dataset to form the first evaluation set for com-\nputing the metrics. We refer to this dataset as \u201cOur Val\u201d\ndataset throughout the remainder of the paper. We also uti-\nlize the Google Scanned Objects (GSO) dataset to create an\nadditional evaluation set to evaluate the cross-domain gen-\neralization capability for our method, noting that our model\nhas not been trained on this dataset. Please note that while\na subset of the Google Scanned Objects (GSO) dataset has\nbeen used as an evaluation set in One-2345 [43], we have\nincluded all objects from this dataset in our study to ensure\na more comprehensive evaluation.\nEvaluation Metrics. In the conditional task, we evaluate\nFigure 10. Visual comparisons for the Image-to-3D generation task reveal that our method outperforms three major generative models:\nPoint-E [62], Shap-E [33], and One-2-3-45 [43]. Our single-view model generates more accurate shapes compared to these baselines, and\nthe multi-view model further enhances shape fidelity with additional view information.\nFigure 11. Our model demonstrates the capability to generate var-\nied results from a single input image, accurately resembling the\nvisible portions while offering diversity in unseen areas.\nTable 3. Quantitative evaluation of the Image-to-3D task shows\nthat our single-view model excels the baslines, achieving the high-\nest IoU and lowest LFD metrics. Incorporating additional infor-\nmation, our multi-view model further enhances performance.\nMethod\nGSO Dataset\nOur Val Dataset\nLFD \u2193\nIoU \u2191\nLFD \u2193\nIoU \u2191\nPoint-E [62]\n5018.73\n0.1948\n6181.97\n0.2154\nShap-E [33]\n3824.48\n0.3488\n4858.92\n0.2656\nOne-2-3-45 [43]\n4397.18\n0.4159\n5094.11\n0.2900\nOurs (Single view)\n3406.61\n0.5004\n4071.33\n0.4285\nOurs (Multi view)\n1890.85\n0.7460\n2217.25\n0.6707\nperformance by comparing the similarity between the gen-\nerated shape and the associated ground-truth shape using\ntwo metrics: (i) Intersection over Union (IoU), which mea-\nsures the volume ratio of the intersection to the union be-\ntween the voxelized volumes; and (ii) Light Field Distance\n(LFD) [4], which assesses the similarity between two sets\nof images rendered from various viewpoints.\nFor the unconditional task, we adopt the method de-\nscribed in [104] to evaluate the generation performance us-\ning the Frechet Inception Distance (FID) [22]. In particular,\nwe render an image for each shape from Our Val set. Fol-\nlowing this, we apply the same rendering process to a gen-\nerated set of shapes of equivalent size. We then compute a\nfeature vector for each rendered image and assess the dif-\nference in the distribution of these feature vectors between\nthe two sets as the final metric.\n8.2. Quantitative Comparison with Other Large\nGenerative Models\nIn this experiment, we contrast our method with other large\nimage-to-3D generative models. Our analysis encompasses\ntwo distinct model settings: single-view and multi-view.\nThe single-view model uses a solitary image, which serves\nas an input for our wavelet generative model.\nIn cases\nwhere multiple images are accessible, our multi-view model\ncomes into play. This model uses four images along with\nthe camera parameter as the condition.\nWe present the quantitative results in Table 3 and the\nqualitative comparison results in Figure 10. As shown in\nTable 3, our single-view model significantly outperforms\nall baseline models by a considerable margin in both the\nIoU and LFD metrics. It is noteworthy that LFD, being a\nrotation-insensitive metric, indicates that our results do no\ndepend on the alignment between the generated shapes and\nthe ground-truth shapes. Additionally, Figure 10 reveals\nthat our method captures not only global structures but also\nfine local details, as well as more intricate geometric pat-\nterns, compared to the baselines. This is particularly evident\nin rows 7-8 of Figure 10, showing that our method is able\nto more accurately capture the geometric patterns of lines\nthan the other baselines. Furthermore, rows 3-4 showcase\nhow our model effectively captures the rough coat pattern\nof the shape, further demonstrating its superiority. Finally,\nwe would like to highlight that the geometry produced by\nour method features clean and smooth surfaces, in contrast\nto those generated by other baseline methods.\nOn the other hand, when provided with additional three\nviews, our model exhibits a substantial improvement in re-\nsults. Both LFD and IoU metrics, as indicated in Table 3,\nshow that the generative model captures a greater extent of\nthe global shape. This improvement is expected, since the\nnetwork can access more information in the form of mul-\ntiple views; however, four images still constitute a limited\nset of views for completely reconstructing a shape. More-\nover, multiple views aid in better capturing local geomet-\nric details, as demonstrated in Figure 10 rows 3-4 and 7-8.\nAlso, it is worthy to mention recent advancements, such as\nZero123 [44] and Zero123-XL [12], which have the capa-\nbility to generate multiple views from a single view. This\nadvancement potentially allows our multi-view model to\noperate effectively with only a single view available since a\nsingle-view can be converted to a multi-view using Zero123\nor Zero123-XL, after which our model can be applied. Yet,\nexploring this possibility remains a topic for future research.\nWe attribute this success to two major factors. Firstly, we\nposit that our wavelet-tree representation is almost lossless,\nas discussed in Table 1, compared to Point-E and Shape-E.\nThis characteristic likely makes the upper bound of our rep-\nresentation much easier to capture using a diffusion model.\nMoreover, our adaptive training scheme enables our net-\nFigure 12. Our single-view conditioned generation model yields a wide variety of shapes. Our model adeptly generates both CAD objects\nlike screws, chairs, and cars, as well as organic forms such as humans, animals, and plants.\nwork to capture more local details and geometric patterns,\nby efficiently learning to model the information-rich coeffi-\ncients in the detail coefficient subbands.\n8.3. Image-to-3D Generation\nIn this section, we present additional qualitative results on\nsingle-view conditioning from our val set. As depicted in\nFigure 12, it is evident that our method can generate objects\nfrom a variety of categories. This includes CAD objects,\nsuch as the screw shown in row 1, furniture like the chair in\nrow 5, and buildings exemplified by the house in row 4. Ad-\nditionally, our method effectively represents organic shapes,\nincluding humans (as seen in rows 4 and 6), animals (with\nthe dog in row 3), and plants (illustrated by the Christmas\ntree and mushroom in row 3).\nOur model, being a generative one, can produce multiple\nvariations for a given condition, as demonstrated in Figure\n11 using single-view images. In addition to the visible re-\ngions that can be faithfully reconstructed by our approach,\nit further imaginatively reconstructs the parts that are invis-\nible. For instance, in Figure 11, rows 2 and 4, the top of\nthe CAD object is invisible, leading the model to imagine\nit. This trade-off between adhering to visible parts and cre-\natively interpreting invisible ones could be further explored\nby adjusting the classifier-free guidance weight. We leave\nthis aspect for future work.\nWe also present additional results for the multi-view ap-\nproach in Figure 13. Again, it demonstrates that our method\nis able to generate objects across diverse categories. More-\nover, there is a noticeable alignment of the objects with the\ninput images, which is more pronounced compared to the\nsingle-view approach. Our method is also capable of gen-\nFigure 13. Our multi-view conditioned generation model can utilize multi-view information to create diverse and coherent shapes with\ncomplex topologies, exemplified by the CAD objects in the first two rows.\nTable 4. The quantitative evaluation reveals that our model\u2019s per-\nformance is not significantly impacted by the number of points of\ninputs. Even with inputs of 5000 points, it manages to deliver rea-\nsonable reconstructions, though trained on 25000-point inputs.\nMetrics\nNumber of Points\n2500\n5000\n10000\n25000\nLFD \u2193\n1857.84\n1472.02\n1397.39\n1368.90\nIoU \u2191\n0.7595\n0.8338\n0.8493\n0.8535\nerating objects with highly complicated topologies and in-\ntricate geometries, as demonstrated by the CAD examples\n(see the second object in rows 1-2 and the first object in\nrows 5-6).\n8.4. Point-to-3D Generation\nIn this experiment, our objective is to take a point cloud as\ninput and produce a TSDF following its geometry. Our en-\ncoder, comprising a PointNet [67] and a PMA block from\nthe Set Transformer [39], is adept at handling a variable\nnumber of points. This versatility allows us to train the\nmodel with 25,000 points, while also giving us the flexi-\nbility to input an arbitrary number of points during testing.\nWe conduct an ablation study to assess how the quality\nof generation is influenced by different sets of points, as\ndetailed in Table 4. Our findings reveal that an increase in\nthe number of points leads to improved IoU results on our\nval set. Notably, even with sparse point clouds with as few\nas 5,000 points, our model achieves a reasonable IoU.\nThis analysis is also visually represented illustrated in\nFigure 14. Here, we observe that certain details are lost\nwhen a lower number of points are used, as evident in row 2.\nHowever, it\u2019s worth mentioning that, in general, our method\nperforms well even with fewer points. We also present addi-\ntional visual results in Figure 15. These results demonstrate\nthat our method performs consistently well across various\ntypes of object and showcasing robustness predominantly\nto the number of points.\nFigure 14. Visual comparisons, based on the number of input points, highlight our model\u2019s ability to robustly generate thin structures, like\nthe deer horn or the chair leg, with a reasonable number of points (\u2265 5000).\nTable 5.\nOur method is quantitatively compared with tradi-\ntional voxel upsampling techniques, specifically nearest neigh-\nbour upsampling and trilinear interpolation, followed by marching\ncubes [49] for mesh extraction. Our generative model significantly\nsurpasses these two baselines in both Light Field Distance (LFD)\nand Intersection Over Union (IoU) metrics.\nSetting\nMethods\nLFD \u2193\nIoU \u2191\nOurs\n2266.41\n0.687\nNearest\n6408.82\n0.2331\nVoxel (163)\nTrilinear\n6132.99\n0.2373\nOurs\n1580.98\n0.7942\nNearest\n3970.49\n0.4677\nVoxel (323)\nTrilinear\n3682.83\n0.4719\n8.5. Voxel-to-3D Generation\nNext, we explore the use of low-resolution voxels as input\nfor our model, which outputs a Signed Distance Function\nfollowing its geometry. We trained two distinct models on\ndifferent voxel resolutions: 163 and 323. Both models em-\nploy the same encoder, which utilizes two 3D convolutional\nblocks to downsample the input into a conditional latent\nvector.\nThe qualitative results for both resolutions are displayed\nin Figure 16. From this analysis, we observe three main\npoints. First, our method successfully creates smooth and\nclean surfaces. Second, despite the ambiguity present in\nsome examples for both voxel 163 (as seen in the human\nexamples in row 3) and voxel 323 (as seen in the crown ex-\namples in row 5), our method produces plausible shapes.\nFinally, our method also performs well with disjoint ob-\njects (as demonstrated in the human examples in row 3) and\nscenes (as shown in the room examples in row 1 and row 7).\nWe further compare our method with traditional ap-\nproaches for converting low-resolution voxels to meshes.\nFor the baselines, we first employ interpolation techniques\nsuch as nearest neighbor and trilinear interpolation, fol-\nlowed by the use of marching cubes [49] to derive the\nmeshes. Importantly, our approach is the first large-scale\ngenerative model to tackle this task. The quantitative and\nqualitative results of this comparison are presented in Ta-\nble 5 and Figure 17. It is evident that, among the baseline\nmethods, trilinear interpolation outperforms nearest neigh-\nbor, which is intuitively reasonable. Our method easily sur-\npasses both of these traditional methods in terms of both\nIoU and LFD metrics.\n8.6. 3D Shape Completion\nFurthermore, our trained unconditional generative model\ncan be employed for completion tasks in a zero-shot fash-\nion. In this context, the objective is to generate multiple\nvariations of a partial input shape. Given a shape, we first\nidentify a region, consisting of a set of vertices on the mesh,\nthat we aim to regenerate and subsequently discard it. The\nremaining part of the mesh is retained as the partial input as\nshown in Figure 18 (leftmost column). Subsequently, this\npartial shape is transformed into a TSDF, which is then con-\nverted into our diffusible wavelet representation with a grid\nstructure. Based on the earlier selected region for regen-\neration, we construct a binary mask that identifies missing\nregions to complete. Utilizing the selection mask, we adopt\nthe approach described in [50] to regenerate the wavelet co-\nefficients in these masked regions using our trained uncon-\nditional model.\nFigure 18 shows visual examples of our completion ap-\nFigure 15. Our point cloud conditioned generation results demonstrate that our model is capable of producing shapes with intricate\ngeometric details while preserving the complex topologies of the input point clouds.\nproach.\nIt is evident that, in general, our unconditional\nmodel can generate semantically meaningful parts in the\nmasked regions, as illustrated by the animal heads produced\nin the third row and the varied chair backs in the first row.\nAdditionally, these generated parts exhibit coherent connec-\ntions to the given input of the partial shape, exemplified by\nthe body region of the animal in the fifth row. It is also note-\nworthy that our model can produce multiple variations for\nthe same input partial shape, indicating the diverse distribu-\ntion captured in our generative model.\n8.7. Ablation Studies\nClassifier-free Guidance. As previously mentioned, we\nemploy classifier-free guidance, as detailed in [23], to en-\nhance the quality of conditioned samples. A crucial hyper-\nparameter in this classifier-free guidance, during inference,\nTable 6. We quantitatively analyse the performance of our condi-\ntional generation models on different guidance weights.\nGuidance Weight (w)\nModel\nMetrics\n1\n2\n3\n4\n5\nLFD \u2193\n4395.15\n4071.33\n4121.14\n4192.30\n4295.28\nSingle-view\nIoU \u2191\n0.3706\n0.4285\n0.4348\n0.4289\n0.4202\nLFD \u2193\n2378.48\n2310.30\n2413.18\n2522.03\n2639.69\nMulti-view\nIoU \u2191\n0.6322\n0.6595\n0.6488\n0.6317\n0.6148\nLFD \u2193\n1701.17\n1683.95\n1769.93\n1900.48\n2029.59\nVoxels (32)\nIoU \u2191\n0.7636\n0.7771\n0.7659\n0.7483\n0.7323\nLFD \u2193\n2453.69\n2347.04\n2426.40\n2556.62\n2724.72\nVoxels (16)\nIoU \u2191\n0.6424\n0.6726\n0.6614\n0.6452\n0.6289\nLFD \u2193\n1429.37\n1432.95\n1521.55\n1658.03\n1830.78\nPoints\nIoU \u2191\n0.8380\n0.8379\n0.8207\n0.8002\n0.7781\nFigure 16. Our voxel-conditioned generative model excels in creating high-quality outputs from low-resolution inputs, imaginatively\nintroducing various plausible geometric patterns. This is exemplified by the creation of holes in the crown, which are not available in the\ninitial inputs.\nis the scale parameter or guidance weight, denoted as w.\nThis parameter plays a key role in managing the trade-off\nbetween the generation\u2019s fidelity to the input conditions and\nthe diversity of the generated output.\nWe experiment to explore the effect of the guidance\nweight parameter on the quality of samples generated by\nvarious conditional models. The guidance weight parame-\nter was systematically adjusted in a linear progression from\n1.0 to 5.0. It is important to note that, for efficient eval-\nuation, an inference timestep of 100 was consistently em-\nployed across all experiments. The results of this study are\npresented in Table 6.\nEmpirically, we observe that a guidance weight of 2.0\nis optimal for most conditional generation tasks. However,\nwhen the model is conditioned on point cloud data, a lower\nguidance weight of 1.0 yields better results. This contrasts\nwith the text-to-image scenarios, which typically require a\nlarger value for the guidance weight. We suspect this differ-\nence is attributable to the nature of the input conditions we\nuse, such as images and point clouds, which contain more\ninformation and thus make it more challenging to generate\ndiverse samples compared to text-based inputs. Note that\nwe adopt these identified optimal values as fixed hyperpa-\nrameters for all subsequent inferences in the remainder of\nour experiments, as well as for the generation of qualitative\nresults.\nInference Time Step Analysis. Furthermore, we also pro-\nFigure 17. In comparison with meshes generated from interpola-\ntion using nearest neighbor upsampling and trilinear interpolation,\nour generation results display notably smoother surfaces.\nTable 7. We quantitatively evaluate the performances of generation\nmodels with different inference time steps.\nInference Time step (t)\nModel\nMetrics\n10\n100\n500\n1000\nLFD \u2193\n4312.23\n4071.33\n4136.14\n4113.10\nSingle-view\nIoU \u2191\n0.4477\n0.4285\n0.4186\n0.4144\nLFD \u2193\n2217.25\n2310.30\n2369.15\n2394.17\nMulti-view\nIoU \u2191\n0.6707\n0.6595\n0.6514\n0.6445\nLFD \u2193\n1580.98\n1683.95\n1744.48\n1763.91\nVoxels (323)\nIoU \u2191\n0.7943\n0.7771\n0.7700\n0.7667\nLFD \u2193\n2266.41\n2347.04\n2375.89\n2373.42\nVoxels (163)\nIoU \u2191\n0.6870\n0.6726\n0.6620\n0.6616\nLFD \u2193\n1368.90\n1429.37\n1457.89\n1468.91\nPoint Cloud\nIoU \u2191\n0.8535\n0.8380\n0.8283\n0.8287\nUnconditional\nFID \u2193\n371.32\n85.25\n74.60\n68.54\nvide a detailed analysis of the inference timesteps for both\nour conditional and unconditional models. Specifically, we\nevaluate the generation models under the same settings as\nabove but with varying timesteps, namely 10, 100, 500, and\n1000.\nTable 7 presents the quantitative results for our differ-\nent generative models using various time steps during infer-\nence. Specifically, we empirically find that a small time step\n(10) suffices for conditions with minimal ambiguity, such as\nmulti-view images, voxels, and point clouds. As ambiguity\nrises, the required number of time steps to achieve satisfac-\ntory sample quality also increases. For the unconditional\nFigure 18. When provided with partial inputs (left), our uncondi-\ntional generative model completes the missing regions in a coher-\nent and semantically meaningful way. Additionally, it is capable\nof generating multiple variations of the completed shapes, many\nof which significantly differ from the original input (right), high-\nlighting the diverse shape distribution learned.\nTable 8. Employing Mean Squared Error (MSE) directly or sepa-\nrately for each subband resulted in worse performance compared\nto using just the coarse component [28], despite a higher theo-\nretical representation capacity.\nIn contrast, our subband adap-\ntive training strategy led to significant improvements in both Light\nField Distance (LFD) and Intersection Over Union (IoU) metrics.\nSettings\nMetrics\nLFD \u2193\nIoU \u2191\nCoarse component only [28]\n2855.41\n0.5919\nOurs (MSE)\n3191.49\n0.5474\nOurs (subband-based MSE)\n2824.28\n0.5898\nOurs\n2611.60\n0.6105\nmodel, which has no condition, the optimal time step is the\nmaximum one (1000). Similarly to the guidance weight, we\nconsider the optimal time step as a hyper-parameter, which\nis utilized in all experiments.\nAblation of Adaptive Training Strategy. In this experi-\nment, we use our multi-view model and initially compare\nour proposed representation with the generation results ob-\ntained using only the coarse component of our representa-\ntion. This approach was adopted as the generation target\nin [28]. This is shown in the first row of Table 8. Further-\nmore, to illustrate the effectiveness of our adopted subband\nadaptive training strategy, we also compare it with two other\nbaseline training objectives. First, we apply a direct MSE\n(Mean Squared Error) loss uniformly across all our coef-\nficients on our diffusible representation. This approach is\nequivalent to the objective of the classical diffusion model.\nSecond, we contrast our strategy with a method that com-\nputes three separate MSE (Mean Squared Error) losses for\nC0, D0, and D1, and then sums them directly. It is impor-\ntant to note that separate models are necessary for each of\nthese experiments, leading to increased computational re-\nsource costs. Additionally, convergence can take up to a\nfew weeks, so we decided to implement an early stop at\n750k iterations and compare the results at this stage.\nFrom Table 8, it is observable that using solely the coarse\ncomponent can still yield plausible results. This finding is\nin alignment with the discovery reported in [28]. However,\nnaively applying MSE loss uniformly across all coefficients\nresults in a performance drop, despite the upper bound of\nthe representation capacity being significantly higher. We\nobserve that the initial attempt to separate the loss compu-\ntation improves the performance of the trained generator.\nHowever, the quality of the results remains similar to those\ngenerated using only the coarse component. By adopting\nour proposed subband adaptive training scheme, we achieve\na significant improvement over the approach in [28].\n9. Limitations and Future Works\nOur approach exhibits the following limitations: (i) While\nour unconditional model is capable of generating a diverse\nvariety of shapes from various sub-datasets, it can not en-\nsure a balanced representation of objects across different\ncategories during sampling. Hence, the learned 3D shape\ndistribution is inherently imbalanced, evident in the dispro-\nportionate representation of CAD models. We can utilize\na large zero-shot language model like ChatGPT for anno-\ntating object categories, enabling the application of diverse\ndata augmentation methods to balance the training data ac-\ncording to these categories. (ii) Our generation network,\ntrained on a heterogeneous mix of datasets, does not uti-\nlize the category label as an extra condition. Hence, our\nunconditional model may occasionally generate implausi-\nble shapes or introduce noise into the outputs. Identifying\nand mitigating these anomalies represents a compelling di-\nrection for future research. It is particularly intriguing to\nconsider the development of data-driven metrics for assess-\ning the visual plausibility of generated 3D shapes, espe-\ncially in the context of the available large-scale 3D dataset.\n(iii) At present, our primary focus lies in direct genera-\ntion of 3D geometry. An interesting avenue for future ex-\nploration involves generating textures together on the ge-\nometry, with the aim of achieving this without relying on\ncomputationally-expensive optimizations.\n10. Conclusion\nIn summary, this paper presents Make-A-Shape, a novel\n3D generative framework trained on a vast dataset of over\n10 millions publicly-available 3D shapes, capable of pro-\nducing high-quality 3D shapes impressively within 2 sec-\nonds. Central to our approach is the introduction of a fam-\nily of new techniques.\nThis includes the subband coef-\nficient filtering scheme to help construct a compact, ex-\npressive, and efficient wavelet-tree representation that ef-\nfectively encodes a 2563 SDF with minimal information\nloss. Then, we adeptly model the wavelet-tree represen-\ntation by our diffusion-based generative model using our\nsubband coefficient packing scheme, and further derive the\nsubband adaptive training strategy to achieve model training\nthat can effectively attends to both coarse and sparse detail\ncoefficients. Besides, we also extend Make-A-Shape to take\noptional condition inputs of various modalities.\nOur extensive experiments demonstrate the model\u2019s su-\nperiority in synthesizing high-quality 3D shapes across vari-\nous challenging conditions, including single/multi-view im-\nages, point clouds, and low-resolution voxels, all while re-\nquiring minimal resource demands during the training. Re-\nmarkably, our model not only outperforms existing base-\nlines quantitatively but also demonstrates zero-shot appli-\ncations such as partial shape completion. We believe our\nwork will pave the way for future research in other 3D rep-\nresentations to enable large-scale 3D model training.\nReferences\n[1] Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and\nLeonidas Guibas. Learning representations and generative\nmodels for 3d point clouds. In International conference on\nmachine learning, pages 40\u201349. PMLR, 2018. 4\n[2] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki\nNagano, Boxiao Pan, Shalini De Mello, Orazio Gallo,\nLeonidas J Guibas, Jonathan Tremblay, Sameh Khamis,\net al. Efficient geometry-aware 3d generative adversarial\nnetworks. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 16123\u2013\n16133, 2022. 4\n[3] Angel X Chang, Thomas Funkhouser, Leonidas Guibas,\nPat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese,\nManolis Savva, Shuran Song, Hao Su, et al.\nShapenet:\nAn information-rich 3d model repository. arXiv preprint\narXiv:1512.03012, 2015. 10\n[4] Ding-Yun Chen, Xiao-Pei Tian, Yu-Te Shen, and Ming\nOuhyoung. On visual similarity based 3d model retrieval.\nIn Computer graphics forum, pages 223\u2013232. Wiley Online\nLibrary, 2003. 12\n[5] Qimin Chen, Zhiqin Chen, Hang Zhou, and Hao Zhang.\nShaddr: Real-time example-based geometry and texture\ngeneration via 3d shape detailization and differentiable ren-\ndering. arXiv preprint arXiv:2306.04889, 2023. 5\n[6] Zhiqin Chen and Hao Zhang.\nLearning implicit fields\nfor generative shape modeling.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5939\u20135948, 2019. 1, 4\n[7] Zhiqin Chen, Vladimir G Kim, Matthew Fisher, Noam\nAigerman, Hao Zhang, and Siddhartha Chaudhuri. Decor-\ngan: 3d shape detailization by conditional refinement. In\nProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 15740\u201315749, 2021. 5\n[8] An-Chieh Cheng, Xueting Li, Sifei Liu, Min Sun, and\nMing-Hsuan Yang. Autoregressive 3d shape generation via\ncanonical mapping. In European Conference on Computer\nVision, pages 89\u2013104. Springer, 2022. 4\n[9] Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexan-\nder G Schwing, and Liang-Yan Gui. Sdfusion: Multimodal\n3d shape completion, reconstruction, and generation.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 4456\u20134465, 2023. 4\n[10] Gene Chou, Yuval Bahat, and Felix Heide. Diffusion-sdf:\nConditional generative modeling of signed distance func-\ntions. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 2262\u20132272, 2023. 4\n[11] Jasmine Collins, Shubham Goel, Kenan Deng, Achlesh-\nwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas\nF Yago Vicente, Thomas Dideriksen, Himanshu Arora,\net al. Abo: Dataset and benchmarks for real-world 3d ob-\nject understanding. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n21126\u201321136, 2022. 10\n[12] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana\nEhsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:\nA universe of annotated 3d objects.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 13142\u201313153, 2023. 5, 10, 12\n[13] Congyue Deng, Chiyu Jiang, Charles R Qi, Xinchen Yan,\nYin Zhou, Leonidas Guibas, Dragomir Anguelov, et al.\nNerdi: Single-view nerf synthesis with language-guided\ndiffusion as general image priors. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 20637\u201320647, 2023. 5\n[14] Prafulla Dhariwal and Alexander Nichol. Diffusion mod-\nels beat gans on image synthesis. NeurIPS, 34:8780\u20138794,\n2021. 10\n[15] Laura Downs, Anthony Francis, Nate Koenig, Brandon\nKinman, Ryan Hickman, Krista Reymann, Thomas B\nMcHugh, and Vincent Vanhoucke. Google scanned objects:\nA high-quality dataset of 3d scanned household items. In\n2022 International Conference on Robotics and Automa-\ntion (ICRA), pages 2553\u20132560. IEEE, 2022. 3\n[16] Huan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binqiang\nZhao, Steve Maybank, and Dacheng Tao.\n3d-future: 3d\nfurniture shape with texture. International Journal of Com-\nputer Vision, 129:3313\u20133337, 2021. 10\n[17] Chenjian Gao, Qian Yu, Lu Sheng, Yi-Zhe Song, and\nDong Xu. Sketchsampler: Sketch-based 3d reconstruction\nvia view-dependent depth sampling. In Computer Vision\u2013\nECCV 2022: 17th European Conference, Tel Aviv, Israel,\nOctober 23\u201327, 2022, Proceedings, Part I, pages 464\u2013479.\nSpringer, 2022. 5\n[18] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen,\nKangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja\nFidler. Get3d: A generative model of high quality 3d tex-\ntured shapes learned from images. Advances In Neural In-\nformation Processing Systems, 35:31841\u201331854, 2022. 1,\n4\n[19] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville,\nand Yoshua Bengio. Generative adversarial nets. Advances\nin neural information processing systems, 27, 2014. 4\n[20] Benoit Guillard, Edoardo Remelli, Pierre Yvernay, and Pas-\ncal Fua.\nSketch2mesh: Reconstructing and editing 3d\nshapes from sketches. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, pages 13023\u2013\n13032, 2021. 5\n[21] Rana Hanocka, Amir Hertz, Noa Fish, Raja Giryes, Shachar\nFleishman, and Daniel Cohen-Or. Meshcnn: a network with\nan edge. ACM Transactions on Graphics (ToG), 38(4):1\u2013\n12, 2019. 4\n[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. Advances in neural information processing systems,\n30, 2017. 12\n[23] Jonathan Ho and Tim Salimans. Classifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 9, 16\n[24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural informa-\ntion processing systems, 33:6840\u20136851, 2020. 3, 7\n[25] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou,\nDifan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and\nHao Tan. Lrm: Large reconstruction model for single image\nto 3d. arXiv preprint arXiv:2311.04400, 2023. 1, 3, 4\n[26] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans.\nsimple diffusion: End-to-end diffusion for high resolution\nimages. arXiv preprint arXiv:2301.11093, 2023. 3, 8, 10\n[27] Zhongzhan Huang, Pan Zhou, Shuicheng Yan, and Liang\nLin.\nScalelong: Towards more stable training of diffu-\nsion model via scaling network long skip connection. arXiv\npreprint arXiv:2310.13545, 2023. 10\n[28] Ka-Hei Hui, Ruihui Li, Jingyu Hu, and Chi-Wing Fu. Neu-\nral wavelet-domain diffusion for 3D shape generation. In\nACM SIGGRAPH Asia, pages 1\u20139, 2022. 1, 3, 4, 5, 6, 18,\n19\n[29] Moritz Ibing, Isaak Lim, and Leif Kobbelt. 3d shape gener-\nation with grid-based implicit functions. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 13559\u201313568, 2021. 4\n[30] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter\nAbbeel, and Ben Poole. Zero-shot text-guided object gen-\neration with dream fields. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 867\u2013876, 2022. 5\n[31] Pradeep Kumar Jayaraman, Aditya Sanghi, Joseph G Lam-\nbourne, Karl DD Willis, Thomas Davies, Hooman Shayani,\nand Nigel Morris. Uv-net: Learning from boundary rep-\nresentations. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 11703\u2013\n11712, 2021. 4\n[32] Pradeep Kumar Jayaraman, Joseph G Lambourne, Nishkrit\nDesai, Karl DD Willis, Aditya Sanghi, and Nigel JW Mor-\nris. Solidgen: An autoregressive model for direct b-rep syn-\nthesis. arXiv preprint arXiv:2203.13944, 2022. 1, 4\n[33] Heewoo Jun and Alex Nichol.\nShap-e:\nGenerat-\ning conditional 3D implicit functions.\narXiv preprint\narXiv:2305.02463, 2023. 1, 3, 4, 5, 10, 11, 12\n[34] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980,\n2014. 10\n[35] Roman Klokov, Edmond Boyer, and Jakob Verbeek. Dis-\ncrete point flow networks for efficient point cloud genera-\ntion. In European Conference on Computer Vision, pages\n694\u2013710. Springer, 2020. 4\n[36] Sebastian Koch, Albert Matveev, Zhongshi Jiang, Francis\nWilliams, Alexey Artemov, Evgeny Burnaev, Marc Alexa,\nDenis Zorin, and Daniele Panozzo. Abc: A big cad model\ndataset for geometric deep learning.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 9601\u20139611, 2019. 10\n[37] Di Kong, Qiang Wang, and Yonggang Qi.\nA diffusion-\nrefinement model for sketch-to-point modeling.\nIn Pro-\nceedings of the Asian Conference on Computer Vision,\npages 1522\u20131538, 2022. 5\n[38] Joseph G Lambourne, Karl DD Willis, Pradeep Kumar\nJayaraman, Aditya Sanghi, Peter Meltzer, and Hooman\nShayani. Brepnet: A topological message passing system\nfor solid models. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n12773\u201312782, 2021. 4\n[39] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Se-\nungjin Choi, and Yee Whye Teh. Set transformer: A frame-\nwork for attention-based permutation-invariant neural net-\nworks. In International conference on machine learning,\npages 3744\u20133753. PMLR, 2019. 9, 14\n[40] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun\nLuan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg\nShakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with\nsparse-view generation and large reconstruction model.\narXiv preprint arXiv:2311.06214, 2023. 1, 3, 4\n[41] Muheng Li, Yueqi Duan, Jie Zhou, and Jiwen Lu.\nDiffusion-sdf: Text-to-shape via voxelized diffusion.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 12642\u201312651, 2023.\n4\n[42] Yang Li, Hikari Takehara, Takafumi Taketomi, Bo Zheng,\nand Matthias Nie\u00dfner. 4dcomplete: Non-rigid motion es-\ntimation beyond the observable surface. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion, pages 12706\u201312716, 2021. 10\n[43] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Zexiang\nXu, Hao Su, et al. One-2-3-45: Any single image to 3d\nmesh in 45 seconds without per-shape optimization. arXiv\npreprint arXiv:2306.16928, 2023. 1, 3, 4, 10, 11, 12\n[44] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot one image to 3d object. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 9298\u20139309, 2023. 5, 12\n[45] Xinhai Liu, Zhizhong Han, Yu-Shen Liu, and Matthias\nZwicker.\nFine-grained 3d shape classification with hier-\narchical part-view attention. IEEE Transactions on Image\nProcessing, 30:1744\u20131758, 2021. 10\n[46] Zhengzhe Liu, Peng Dai, Ruihui Li, Xiaojuan Qi, and Chi-\nWing Fu. Iss: Image as stetting stone for text-guided 3d\nshape generation. arXiv preprint arXiv:2209.04145, 2022.\n5\n[47] Zhengzhe Liu, Jingyu Hu, Ka-Hei Hui, Xiaojuan Qi, Daniel\nCohen-Or, and Chi-Wing Fu.\nExim: A hybrid explicit-\nimplicit representation for text-guided 3d shape generation.\nACM Transactions on Graphics (TOG), 42(6):1\u201312, 2023.\n4\n[48] Matthew Loper, Naureen Mahmood, Javier Romero, Ger-\nard Pons-Moll, and Michael J. Black. SMPL: A skinned\nmulti-person linear model. ACM Trans. Graphics (Proc.\nSIGGRAPH Asia), 34(6):248:1\u2013248:16, 2015. 10\n[49] William E Lorensen and Harvey E Cline. Marching cubes:\nA high resolution 3d surface construction algorithm.\nIn\nSeminal graphics: pioneering efforts that shaped the field,\npages 347\u2013353. 1998. 15\n[50] Andreas Lugmayr, Martin Danelljan, Andres Romero,\nFisher Yu, Radu Timofte, and Luc Van Gool. Repaint: In-\npainting using denoising diffusion probabilistic models. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 11461\u201311471, 2022.\n15\n[51] Zhaoliang Lun, Matheus Gadelha, Evangelos Kalogerakis,\nSubhransu Maji, and Rui Wang. 3d shape reconstruction\nfrom sketches via multi-view convolutional networks. In\n2017 International Conference on 3D Vision (3DV), pages\n67\u201377. IEEE, 2017. 5\n[52] Shitong Luo and Wei Hu.\nDiffusion probabilistic mod-\nels for 3d point cloud generation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 2837\u20132845, 2021. 4, 5\n[53] Jonathan Masci, Davide Boscaini, Michael Bronstein, and\nPierre Vandergheynst. Geodesic convolutional neural net-\nworks on riemannian manifolds.\nIn Proceedings of the\nIEEE international conference on computer vision work-\nshops, pages 37\u201345, 2015. 4\n[54] Daniel Maturana and Sebastian Scherer. Voxnet: A 3d con-\nvolutional neural network for real-time object recognition.\nIn 2015 IEEE/RSJ international conference on intelligent\nrobots and systems (IROS), pages 922\u2013928. IEEE, 2015. 4\n[55] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and\nAndrea Vedaldi.\nRealfusion: 360deg reconstruction of\nany object from a single image.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8446\u20138455, 2023. 5\n[56] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-\nbastian Nowozin, and Andreas Geiger.\nOccupancy net-\nworks: Learning 3d reconstruction in function space. In\nProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 4460\u20134470, 2019. 1,\n4\n[57] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and\nRana Hanocka. Text2mesh: Text-driven neural stylization\nfor meshes. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 13492\u2013\n13502, 2022. 5\n[58] Aryan Mikaeili, Or Perel, Mehdi Safaee, Daniel Cohen-\nOr, and Ali Mahdavi-Amiri.\nSked: Sketch-guided text-\nbased 3d editing.\nIn Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, pages 14607\u2013\n14619, 2023. 5\n[59] Paritosh Mittal, Yen-Chi Cheng, Maneesh Singh, and Shub-\nham Tulsiani.\nAutosdf:\nShape priors for 3d comple-\ntion, reconstruction and generation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 306\u2013315, 2022. 4\n[60] Kaichun Mo, Paul Guerrero, Li Yi, Hao Su, Peter Wonka,\nNiloy Mitra, and Leonidas J Guibas. Structurenet: Hier-\narchical graph networks for 3d shape generation.\narXiv\npreprint arXiv:1908.00575, 2019. 4\n[61] Charlie Nash, Yaroslav Ganin, SM Ali Eslami, and Peter\nBattaglia. Polygen: An autoregressive generative model of\n3d meshes. In International conference on machine learn-\ning, pages 7220\u20137229. PMLR, 2020. 4\n[62] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela\nMishkin, and Mark Chen. Point-e: A system for generat-\ning 3d point clouds from complex prompts. arXiv preprint\narXiv:2212.08751, 2022. 1, 3, 4, 5, 11, 12\n[63] Jeong Joon Park, Peter Florence, Julian Straub, Richard\nNewcombe, and Steven Lovegrove.\nDeepsdf: Learning\ncontinuous signed distance functions for shape representa-\ntion. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition, pages 165\u2013174, 2019.\n4\n[64] Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc\nPollefeys, and Andreas Geiger. Convolutional occupancy\nnetworks.\nIn Computer Vision\u2013ECCV 2020: 16th Euro-\npean Conference, Glasgow, UK, August 23\u201328, 2020, Pro-\nceedings, Part III 16, pages 523\u2013540. Springer, 2020. 4\n[65] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv\npreprint arXiv:2209.14988, 2022. 5\n[66] Charles R Qi, Hao Su, Matthias Nie\u00dfner, Angela Dai,\nMengyuan Yan, and Leonidas J Guibas. Volumetric and\nmulti-view cnns for object classification on 3d data. In Pro-\nceedings of the IEEE conference on computer vision and\npattern recognition, pages 5648\u20135656, 2016. 4\n[67] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.\nPointnet: Deep learning on point sets for 3d classification\nand segmentation. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 652\u2013\n660, 2017. 4, 9, 14\n[68] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J\nGuibas. Pointnet++: Deep hierarchical feature learning on\npoint sets in a metric space. Advances in neural information\nprocessing systems, 30, 2017. 4\n[69] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren,\nAliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Sko-\nrokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123:\nOne image to high-quality 3d object generation us-\ning both 2d and 3d diffusion priors.\narXiv preprint\narXiv:2306.17843, 2023. 5\n[70] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In International conference on machine learning,\npages 8748\u20138763. PMLR, 2021. 9\n[71] Alexander Raistrick, Lahav Lipson, Zeyu Ma, Lingjie Mei,\nMingzhe Wang, Yiming Zuo, Karhan Kayan, Hongyu Wen,\nBeining Han, Yihan Wang, et al.\nInfinite photorealistic\nworlds using procedural generation.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 12630\u201312641, 2023. 10\n[72] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott\nGray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya\nSutskever. Zero-shot text-to-image generation. In Interna-\ntional Conference on Machine Learning, pages 8821\u20138831.\nPMLR, 2021. 1\n[73] Anurag Ranjan, Timo Bolkart, Soubhik Sanyal, and\nMichael J. Black. Generating 3D faces using convolutional\nmesh autoencoders. In European Conference on Computer\nVision (ECCV), pages 725\u2013741, 2018. 10\n[74] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 1, 5, 10\n[75] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Sali-\nmans, et al. Photorealistic text-to-image diffusion models\nwith deep language understanding. Advances in Neural In-\nformation Processing Systems, 35:36479\u201336494, 2022. 1,\n5\n[76] Aditya Sanghi, Hang Chu, Joseph G. Lambourne, Ye Wang,\nChin-Yi Cheng, Marco Fumero, and Kamal Rahimi Malek-\nshan. CLIP-Forge: Towards zero-shot text-to-shape gener-\nation. In CVPR, pages 18603\u201318613, 2022. 4, 5\n[77] Aditya Sanghi, Rao Fu, Vivian Liu, Karl DD Willis,\nHooman Shayani, Amir H Khasahmadi, Srinath Sridhar,\nand Daniel Ritchie. Clip-sculptor: Zero-shot generation of\nhigh-fidelity and diverse shapes from natural language. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 18339\u201318348, 2023.\n4, 5\n[78] Aditya Sanghi,\nPradeep Kumar Jayaraman,\nArianna\nRampini, Joseph Lambourne, Hooman Shayani, Evan\nAtherton, and Saeid Asgari Taghanaki.\nSketch-a-shape:\nZero-shot sketch-to-3d shape generation.\narXiv preprint\narXiv:2307.03869, 2023. 5\n[79] Katja Schwarz, Axel Sauer, Michael Niemeyer, Yiyi Liao,\nand Andreas Geiger. Voxgraf: Fast 3d-aware image synthe-\nsis with sparse voxel grids. Advances in Neural Information\nProcessing Systems, 35:33999\u201334011, 2022. 4\n[80] Pratheba Selvaraju, Mohamed Nabail, Marios Loizou,\nMaria Maslioukova, Melinos Averkiou, Andreas Andreou,\nSiddhartha Chaudhuri, and Evangelos Kalogerakis. Build-\ningnet: Learning to label 3d buildings. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion, pages 10397\u201310407, 2021. 10\n[81] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie\nLi, and Xiao Yang. Mvdream: Multi-view diffusion for 3d\ngeneration. arXiv preprint arXiv:2308.16512, 2023. 5\n[82] J Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner,\nJiajun Wu, and Gordon Wetzstein. 3D neural field genera-\ntion using triplane diffusion. In CVPR, pages 20875\u201320886,\n2023. 1, 4\n[83] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In International confer-\nence on machine learning, pages 2256\u20132265. PMLR, 2015.\n3\n[84] Yang Song and Stefano Ermon. Generative modeling by\nestimating gradients of the data distribution. Advances in\nneural information processing systems, 32, 2019. 3\n[85] Stefan Stojanov, Anh Thai, and James M Rehg.\nUsing\nshape to categorize: Low-shot learning with an explicit\nshape bias. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 1798\u20131808,\n2021. 10\n[86] Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik\nLearned-Miller. Multi-view convolutional neural networks\nfor 3d shape recognition. In Proceedings of the IEEE in-\nternational conference on computer vision, pages 945\u2013953,\n2015. 4\n[87] Yongbin Sun, Yue Wang, Ziwei Liu, Joshua Siegel, and\nSanjay Sarma. Pointgrow: Autoregressively learned point\ncloud generation with self-attention. In Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer\nVision, pages 61\u201370, 2020. 4\n[88] Nitika Verma, Edmond Boyer, and Jakob Verbeek. Feast-\nnet: Feature-steered graph convolutions for 3d shape anal-\nysis. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 2598\u20132606, 2018. 4\n[89] Kashi Venkatesh Vishwanath, Diwaker Gupta, Amin Vah-\ndat, and Ken Yocum. Modelnet: Towards a datacenter emu-\nlation environment. In 2009 IEEE Ninth International Con-\nference on Peer-to-Peer Computing, pages 81\u201382. IEEE,\n2009. 10\n[90] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma,\nMichael M Bronstein, and Justin M Solomon. Dynamic\ngraph cnn for learning on point clouds. ACM Transactions\non Graphics (tog), 38(5):1\u201312, 2019. 4\n[91] Karl DD Willis, Yewen Pu, Jieliang Luo, Hang Chu, Tao\nDu, Joseph G Lambourne, Armando Solar-Lezama, and\nWojciech Matusik. Fusion 360 gallery: A dataset and en-\nvironment for programmatic cad construction from human\ndesign sequences. ACM Transactions on Graphics (TOG),\n40(4):1\u201324, 2021. 10\n[92] Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman,\nand Josh Tenenbaum. Learning a probabilistic latent space\nof object shapes via 3d generative-adversarial modeling.\nAdvances in neural information processing systems, 29,\n2016. 4\n[93] Rundi Wu, Chang Xiao, and Changxi Zheng. Deepcad: A\ndeep generative network for computer-aided design models.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 6772\u20136782, 2021. 4\n[94] Yi Wu, Yuxin Wu, Georgia Gkioxari, and Yuandong Tian.\nBuilding generalizable agents with a realistic and rich 3d\nenvironment. arXiv preprint arXiv:1801.02209, 2018. 10\n[95] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu,\nLinguang Zhang, Xiaoou Tang, and Jianxiong Xiao.\n3d\nshapenets: A deep representation for volumetric shapes.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 1912\u20131920, 2015. 4\n[96] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang,\nand Zhangyang Wang. Neurallift-360: Lifting an in-the-\nwild 2d photo to a 3d object with 360 views. arXiv e-prints,\npages arXiv\u20132211, 2022. 5\n[97] Jiale Xu, Xintao Wang, Weihao Cheng, Yan-Pei Cao, Ying\nShan, Xiaohu Qie, and Shenghua Gao. Dream3d: Zero-\nshot text-to-3d synthesis using 3d shape prior and text-to-\nimage diffusion models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 20908\u201320918, 2023. 5\n[98] Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Ji-\nahao Li, Zifan Shi, Kalyan Sunkavalli, Gordon Wetzstein,\nZexiang Xu, et al. Dmv3d: Denoising multi-view diffu-\nsion using 3d large reconstruction model. arXiv preprint\narXiv:2311.09217, 2023. 1, 3, 4\n[99] Xingguang Yan, Liqiang Lin, Niloy J Mitra, Dani Lischin-\nski, Daniel Cohen-Or, and Hui Huang.\nShapeformer:\nTransformer-based shape completion via sparse representa-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 6239\u20136249,\n2022. 4\n[100] Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu,\nSerge Belongie, and Bharath Hariharan.\nPointflow: 3d\npoint cloud generation with continuous normalizing flows.\nIn Proceedings of the IEEE/CVF international conference\non computer vision, pages 4541\u20134550, 2019. 1, 4\n[101] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong,\nGunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku,\nYinfei Yang, Burcu Karagol Ayan, et al. Scaling autore-\ngressive models for content-rich text-to-image generation.\narXiv preprint arXiv:2206.10789, 2(3):5, 2022. 1\n[102] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Goj-\ncic, Or Litany, Sanja Fidler, and Karsten Kreis. Lion: La-\ntent point diffusion models for 3d shape generation. arXiv\npreprint arXiv:2210.06978, 2022. 4\n[103] Biao Zhang, Matthias Nie\u00dfner, and Peter Wonka. 3dilg: Ir-\nregular latent grids for 3d generative modeling. Advances in\nNeural Information Processing Systems, 35:21871\u201321885,\n2022. 4, 5\n[104] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Pe-\nter Wonka.\n3dshape2vecset: A 3d shape representation\nfor neural fields and generative diffusion models.\narXiv\npreprint arXiv:2301.11445, 2023. 4, 12\n[105] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter\nWonka. 3DShape2VecSet: A 3D shape representation for\nneural fields and generative diffusion models. 42(4), 2023.\n1, 5\n[106] Xinyang Zheng, Yang Liu, Pengshuai Wang, and Xin Tong.\nSdf-stylegan: Implicit sdf-based stylegan for 3d shape gen-\neration. In Computer Graphics Forum, pages 52\u201363. Wiley\nOnline Library, 2022. 4\n[107] Xin-Yang Zheng, Hao Pan, Peng-Shuai Wang, Xin Tong,\nYang Liu, and Heung-Yeung Shum.\nLocally attentional\nsdf diffusion for controllable 3d shape generation. arXiv\npreprint arXiv:2305.04461, 2023. 4, 5\n[108] Linqi Zhou, Yilun Du, and Jiajun Wu. 3d shape generation\nand completion through point-voxel diffusion. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 5826\u20135835, 2021. 4, 5\n[109] Qingnan Zhou and Alec Jacobson.\nThingi10k:\nA\ndataset of 10,000 3d-printing models.\narXiv preprint\narXiv:1605.04797, 2016. 10\n[110] Silvia Zuffi,\nAngjoo Kanazawa,\nDavid Jacobs,\nand\nMichael J. Black. 3D menagerie: Modeling the 3D shape\nand pose of animals. In IEEE Conf. on Computer Vision\nand Pattern Recognition (CVPR), 2017. 10\n"
  },
  {
    "title": "WARM: On the Benefits of Weight Averaged Reward Models",
    "link": "https://arxiv.org/pdf/2401.12187.pdf",
    "upvote": "15",
    "text": "WARM: On the Benefits of Weight Averaged\nReward Models\nAlexandre Ram\u00e9, Nino Vieillard, L\u00e9onard Hussenot, Robert Dadashi, Geoffrey Cideron, Olivier Bachem, Johan Ferret\nGoogle DeepMind\nAligning large language models (LLMs) with human preferences through reinforcement learning (RLHF)\ncan lead to reward hacking, where LLMs exploit failures in the reward model (RM) to achieve seemingly\nhigh rewards without meeting the underlying objectives. We identify two primary challenges when\ndesigning RMs to mitigate reward hacking: distribution shifts during the RL process and inconsistencies\nin human preferences. As a solution, we propose Weight Averaged Reward Models (WARM), first fine-\ntuning multiple RMs, then averaging them in the weight space. This strategy follows the observation\nthat fine-tuned weights remain linearly mode connected when sharing the same pre-training. By\naveraging weights, WARM improves efficiency compared to the traditional ensembling of predictions,\nwhile improving reliability under distribution shifts and robustness to preference inconsistencies. Our\nexperiments on summarization tasks, using best-of-\ud835\udc41 and RL methods, shows that WARM improves the\noverall quality and alignment of LLM predictions; for example, a policy RL fine-tuned with WARM has a\n79.4% win rate against a policy RL fine-tuned with a single RM.\nKeywords: Alignment, RLHF, Reward Modeling, Model Merging\n1. Introduction\nReward modeling. Conversational assistants such as Gemini [1] or GPT-4 [2] have revolutionized the\nAI community and beyond. These LLMs are capable of completing novel and intricate tasks, including\nmathematics, coding, and tool use [3]. These advancements are underpinned by a systematic three\nstage training procedure: pre-training by next token prediction [4, 5, 6], supervised fine-tuning (SFT)\nto learn to follow instructions [7, 8, 9], and ultimately, reinforcement learning (RL) to maximize a\nreward encapsulating the desired behaviors [10]. However, defining such rewards for real-world tasks\nis non-trivial [11]. In reinforcement learning from human feedback (RLHF) [12, 13, 14, 15], rewards\nare reward models (RMs), trained on binary preference datasets to emulate human judgment. The\nenhancement of LLM capabilities from RL is strongly tied to the quality of the RMs [16].\nReward hacking. Particularly insidious in RLHF [17, 18] is the reward hacking issue [19, 20, 21, 22]\n(a.k.a. reward overoptimization), arising from reward misspecification [23, 24] between the proxy\nRM and actual human preferences. While optimizing for the RM initially provides improvements, in\nlater stages the policy (i.e., the LLM being trained) usually learns to exploit loopholes in the RM and\nachieves high rewards without truly fulfilling the intended objectives, as illustrated in Figure 1(b).\nThis reward hacking phenomenon poses numerous issues. First, it degrades performances, manifesting\nas linguistically flawed [25] or unnecessarily verbose [26] outputs, which do not reflect true human\npreferences. Second, it complicates checkpoint selection due to the unreliability of the proxy RM,\nechoing Goodhart\u2019s Law [27]: \u201cwhen a measure becomes a target, it ceases to be a good measure\u201d.\nThird, it can engender sycophancy [28, 29] or amplify social biases, reflecting the limited and skewed\ndemographics of feedback providers [30, 31]. Lastly and most critically, misalignment [32, 33] due\nto reward hacking can escalate into safety risks [19, 34, 35], in particular given the rapid integration\nof LLMs in everyday life and critical decision-making. Such concerns underscore the need to mitigate\nreward hacking to ensure the beneficial and safe deployment of LLMs.\nCorresponding author: alexandrerame@google.com\narXiv:2401.12187v1  [cs.LG]  22 Jan 2024\nWARM: On the Benefits of Weight Averaged Reward Models\nUse RL to maximize reward \nby updating weights.\nRL fine-tune step\nGenerate output by feeding \nan unlabeled input data point\nCompute reward\nAssign a reward to the \nmodel\u2019s output.\nSFT \nSample from policy\nMultiple RM \nfine-tunings with \ndifferent \nhyperparams\nHuman or AI pairwise feedback.\nCollect preference dataset\nWARM:\nWeight Averaged \nReward Model\nReward function\nWeight averaging\nRL fine-tuning\nAligned LLM \n(a) WARM procedure with \ud835\udc40 = 3.\n0\n2000\n4000\n6000\n8000\n# steps\n4\n5\n6\n7\n8\n9\n10\n11\nControl reward\nWARM M = 10\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\n(b) WARM mitigates reward hacking.\nFigure 1 | Figure 1(a) illustrates the alignment process with WARM. From a SFT-ed LLM, we apply RL fine-tuning\nto optimize a proxy reward model (RM), in line with RLHF [12]. The innovation of WARM lies in the design of\nthe proxy RM, which is the weight average (WA) of \ud835\udc40 individual RMs, each fine-tuned from a shared pre-trained\nLLM on the same preference dataset, but with slight differences such as diverse hyperparameters. This WA\napproach is efficient, while enhancing the reliability under distribution shifts and robustness under inconsistent\npreferences. Figure 1(b) showcases the impact during RL alignment. The control reward (detailed in Section 5)\ninitially increases but eventually deteriorates, a phenomenon called reward hacking [19]. However, when\nWARM serves as the proxy RM, increasing \ud835\udc40 (the number of averaged RMs) significantly improves absolute\nresults while delaying the collapse, as indicated by the control rewards maintaining higher values for longer\nduring training. Same plot with KL as the \ud835\udc65-axis in Figure 8(a) and with label corruption in Figure 18.\nChallenges. Two primary challenges underlie reward hacking. The first major issue are the distribu-\ntion shifts encountered by the RM [36, 37]. Indeed, the generations from the policy might deviate\nsubstantially from those in the offline preference dataset, posing an out-of-distribution (OOD) chal-\nlenge. Moreover, those distribution shifts are accentuated by the policy drift during the RL procedure:\nthe policy moves away from its SFT initialization, continually altering the distribution of predictions\nthe RM needs to interpret reliably. Second, preferences are inconsistent: the binary labels in the\npreference dataset are noisy. Indeed, human labelers often rely on simpler criteria (length, bullet\npoints, politeness) over more nuanced indicators. Moreover, errors can be exacerbated for complex\ntasks requiring specific expertise [38], and because of the multi-objective nature of alignment [39]\nrequiring handling the heterogeneity of human opinions. Overall, this results in a low inter-labeler\nagreement (72.6% for InstructGPT [40]), altering the robustness of the RM.\nGoal and ensembling baseline. Designing good RMs must meet a tripartite objective: guiding RL\nefficiently, reliably scoring generations despite the distribution shifts, and providing robust signals\namidst label noise. To address these challenges, the seminal work on RLHF from Christiano et al. [12]\nand more recent works [41, 42] leveraged prediction ensembling (ENS) [43], averaging the rewards\nfrom multiple RMs. ENS improves the reliability of the reward and mitigates hacking risks [41, 42].\nYet, ENS suffers from memory and inference overhead causing efficiency challenges; we will also\nshow that ENS fails to improve robustness to label noise in the preference datasets.\nWARM. In this paper, we propose weight averaged reward models (WARM), a simple, efficient and\nscalable strategy to obtain a reliable and robust RM by combining multiple RMs. Starting from a\nshared pre-trained LLM, we launch multiple RM fine-tunings: in practice, the different runs have\ndifferent hyperparameters (as in grid search), and see the preference data in different orders, thus\n2\nWARM: On the Benefits of Weight Averaged Reward Models\nleading to diverse RMs. A key contribution is how the different RMs are merged: by linear interpolation\nin the weight space. This follows the findings from the linear mode connectivity (LMC) [44, 45] and\nweight averaging (WA) literature [46, 47, 48]: under shared pre-training, the different weights can\nbe linearly interpolated despite the non-linearities in the architecture.\nOn the benefits of WARM. Firstly, WARM stands out for its efficiency and practicality. By requiring\na single model at inference time, it provides a scalable approximation to the traditional, costlier\nensembling of predictions, without its memory and inference burdens. Secondly, WARM improves\nreliability by inheriting from the generalization abilities of WA under distribution shifts, a quality\nwell-documented in the OOD literature for supervised learning [47, 48, 49]. Lastly, WARM improves\nrobustness to label corruption. We show that WA selects the invariant predictive mechanisms [50, 51]\nacross different runs [52, 53], thus naturally diminishing the memorization of corrupted samples,\noccurring in each run in different ways. In contrast, ENS simply memorizes the corrupted samples.\nWe also explain why reducing memorization when modeling noisy preferences enhances stability in\nthe RL process. These multifaceted benefits of WARM are further explored in Section 4.\nWe summarize our contributions as follows.\n1. Innovation in reward modeling. We introduce WARM, the first instance of weight averaging for\nreward modeling. This novel strategy efficiently mitigates reward hacking, improves reliability\nunder distribution shifts and robustness to label corruption.\n2. Theoretical and empirical insights into weight averaging. We validate linear mode connectivity\nfor reward models trained on binary preference datasets. Moreover, we reveal a key difference\nbetween weight and prediction averaging, that appears clearly under label corruption; weight\naveraging only maintains the invariant predictive mechanisms across runs, thereby diminishing\nmemorization and enhancing the focus on generalizable features.\nOur experiments on summarization tasks in Section 5 confirm that WARM improves performance\nwithout any memory or inference overhead, either when used as the reward selector in best-of-\ud835\udc41,\nor as the proxy RM in RL. WARM mitigates reward hacking, and thus provides better downstream\npolicies; specifically, it leads to a win rate of 79.4% (according to the preference oracle metric) against\na policy trained with a standard RM.\n2. Context and challenges\n2.1. Context\nLLMs. We consider an LLM \ud835\udc53\ud835\udf03 of a fixed non-linear architecture parameterized by \ud835\udf03, usually a\nTransformer with attention layers [54]. It defines a policy by mapping prompt inputs \ud835\udc65 to \ud835\udc53\ud835\udf03(\ud835\udc65).\nFollowing the foundation model paradigm [55] and the success of transfer learning [56], the weights\n\ud835\udf03 are first pre-trained [4] on the vast amount of web data into \ud835\udf03\ud835\udc5d\ud835\udc61, before supervised fine-tuning\n(SFT) [7] to learn to follow instructions into \ud835\udf03\ud835\udc60 \ud835\udc53\ud835\udc61. However, the high cost and limited scope of\ninstruction data (i.e., prompts and responses) can create a misalignment [19, 32, 33] between the\nLLM and its intended application. Reinforcement learning (RL) as a third step in the training process\nof LLMs was shown to help alignment of LLMs with the intended usage [40].\nRMs. A notable aspect of RL is the absence of supervised samples to be imitated by the policy; instead,\nthe focus shifts to maximizing the reward of generated samples, that should measure their quality.\nThe challenge is that the oracle reward, perfectly encapsulating the desired behaviors, is not given by\nthe environment. The key innovation from RLHF [12] is that this reward is the output of a reward\nmodel (RM), trained in a supervised way to predict and thus reflect human preferences. Specifically,\n3\nWARM: On the Benefits of Weight Averaged Reward Models\nan RM is an LLM \ud835\udc5f\ud835\udf19 parameterized by \ud835\udf19, predicting a single scalar as the reward \ud835\udc5f\ud835\udf19(\ud835\udc65, \ud835\udc66) for a prompt\n\ud835\udc65 and generation \ud835\udc66. The weights \ud835\udf19 are usually initialized from \u0000\ud835\udf03\ud835\udc60 \ud835\udc53\ud835\udc61, \ud835\udf14\u0001, where the final linear layer \ud835\udf14\nis added on top of the extracted features from the SFT model \ud835\udf03\ud835\udc60 \ud835\udc53\ud835\udc61. Then \ud835\udf19 is trained on a preference\ndataset D\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b = {\ud835\udc65\ud835\udc51, \ud835\udc66+\n\ud835\udc51 , \ud835\udc66\u2212\n\ud835\udc51 }\ud835\udc37\n\ud835\udc51=1 where the generation \ud835\udc66+\n\ud835\udc51 has been preferred over \ud835\udc66\u2212\n\ud835\udc51 to continue \ud835\udc65\ud835\udc51.\nUsually human labelers evaluate those generations, but recent works on RLAIF [57, 58] showed\nthat similar performances can be obtained by prompting an LLM for AI feedback. Following the\nBradley-Terry [59] assumption about the distribution of preferences, and by framing the problem\nas binary classification, the maximum likelihood principle motivates learning \ud835\udf19 by minimizing the\nfollowing negative log-likelihood loss (where \ud835\udf0e is the logistic function):\nL\ud835\udc45\n\u0000\ud835\udc5f\ud835\udf19, D\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b\n\u0001 = \u2212\ud835\udd3c(\ud835\udc65,\ud835\udc66+,\ud835\udc66\u2212)\u2208D\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b\n\u0002\nlog \ud835\udf0e\u0000\ud835\udc5f\ud835\udf19\n\u0000\ud835\udc65, \ud835\udc66+\u0001 \u2212 \ud835\udc5f\ud835\udf19(\ud835\udc65, \ud835\udc66\u2212)\u0001\u0003\n.\n(1)\n.\nReward inference. With this RM, the literature suggests applying any kind of RL algorithm (usually\nREINFORCE [60] or PPO [61]) to fine-tuned \ud835\udf03\ud835\udc60 \ud835\udc53\ud835\udc61 into \ud835\udf03\ud835\udc5f\ud835\udc59, as analyzed in Section 5.2. A training-free\nalternative is best-of-\ud835\udc41 (BoN) sampling, analyzed in Section 5.1, which returns the generation that\nhas the highest reward among \ud835\udc41 generations from \ud835\udf03\ud835\udc60 \ud835\udc53\ud835\udc61. Both methods aim to align the policy with\nhuman preferences. Yet, the reward misspecification [23] between the proxy RM and the true human\npreferences can lead to reward hacking [19, 20, 21, 22], where the policy exploits loopholes in the\nproxy RM to artificially increase the score without matching human preferences.\n2.2. Challenges in reward modeling\nWhen handling rich inputs such as text, or when assessing complex behaviours, designing rewards\naligned with human preferences is a complex challenge for two main reasons, described below.\nDistribution shifts. The primary challenge is the distribution shifts resulting from the offline nature\nof preference data. Indeed, the generations in the preference dataset and those from the policy \ud835\udf03\ud835\udc60 \ud835\udc53\ud835\udc61 do\nnot necessarily follow the same distributions, and the shifts can become even more pronounced due to\nmodel drift during RL. The OOD generalization literature has extensively analyzed the repercussions of\nthese shifts. Firstly, they often lead to a reduction in performance [62, 63]. RMs (of limited capacity)\ntrained on narrow data distributions may rely on spurious correlations [51] or a limited number of\nfeatures [64], thus failing when encountering OOD examples [65, 66]. Secondly, they complicate the\nselection of RMs, as ID validation metrics may poorly correlate with real-world OOD performances\n[67, 68] and the ability to guide the RL [41]. Lastly, RMs can become poorly calibrated [69] in OOD\nscenarios [70, 71], and predict more extreme values as rewards. Such miscalibration exacerbates\nthe problem in a negative feedback loop, further intensifying model drift and distribution shifts. In\nconclusion, limited data coverage during reward modeling reduces the reliability of the RM and\nfacilitates reward hacking [36] in regions where the RM is badly specified.\nInconsistent preferences. The second major challenge is the label noise in preference datasets.\nHuman labelers, often grappling with fatigue, misunderstandings [72, 73] and imperfect incentives\n[74], might default to simpler criteria such as length, bullet points, or politeness rather than more\ncausal indicators. This tendency is exacerbated for complex tasks [38] or when considering multiple\nobjectives, ranging from harmlessness [75] to engagement [76] and representing the heterogeneity\nof human opinions. Consequently, these factors lead to low inter-rater agreement, where human\ndata appears as an imperfect representation of the underlying ground truth [77, 78]. To mitigate\nthese issues, there has been a shift towards AI-generated preferences [57, 58], which, while reducing\nhuman labor costs, introduces its own set of noise and failure cases, such as sensitivity to prompting\nstrategies [79, 80]. These layers of noise and inconsistency challenge the robustness of the RM, and\nits ability to provide stable signals.\n4\nWARM: On the Benefits of Weight Averaged Reward Models\nWith this in mind, a good RM should ideally satisfy the three following properties.\nProperty 1: efficiency. The RM should incur no memory or inference overhead. Then the policy\ncan be optimized efficiently.\nProperty 2: reliability. The RM should reliably reward predictions despite the distribution\nshifts. Then the policy can explore away from its initialization while relying on the RM.\nProperty 3: robustness. The RM should be robust to the label inconsistencies in binary\npreferences. Then the policy can learn from robust signals given by the RM.\n2.3. Existing approaches\nTo tackle those issues, previous works have explored a few research directions, further detailed in\nour related work from Appendix A.2. During RL, the standard strategy is to encourage the policy to\nremain close to its SFT initialization with Kullback-Leibler (KL) regularization [81, 82]; KL reduces\nmodel drift [83, 84] but can cause underfitting and adds an extra hyperparameter (the regularization\nstrength \ud835\udefc). Collecting, labelling and then training on new data (reflecting the evolving policy)\ncan improve the reliability of the RM [16]. Yet it poses significant efficiency challenges due to the\ncontinuous requirement for human annotation and computational resources. In contrast, active\nlearning strategies [85, 86] proactively enrich the preference dataset by seeking out a diverse set of\ngenerations and potential failure cases. Concurrent work [87] suggests applying label smoothing\nand flipping. Finally, and most similar to WARM, prediction ensembling (ENS) [43] strategies average\nthe logits from \ud835\udc40 RMs. From a bias-variance perspective [88], ENS reduces the variance term when\nmembers are sufficiently diverse [89], and thus favors reliability under distribution shifts where\nvariance is the key issue [47]. From a RL perspective, ENS was shown to mitigate hacking risks\n[12, 41, 42]. Despite its advantages, ENS faces efficiency challenges; the memory and inference\ncosts grow linearly with \ud835\udc40, making ENS incompatible with the scaling trend in RMs, where larger\narchitectures consistently perform better [90]. Moreover, we will also show in Section 4.2 that ENS\nfails to improve robustness to preference inconsistencies.\n3. WARM\n3.1. Weight averaging of reward models\nFacing those challenges in reward modeling and the limitations from existing approaches, we propose\nWeight Averaged Reward Models (WARM). WARM is a simple and efficient strategy that combines\nmultiple models without the memory and inference overheads of prediction ensembling, enhancing\nreward reliability (under distribution shifts) and robustness (amidst noisy preference dataset). WARM\nis illustrated in Figure 1(a) and described below.\n1. Shared pre-trained initialization. For a given pre-trained LLM, each RM is initialized from\n\u0000\ud835\udf03\ud835\udc60 \ud835\udc53\ud835\udc61, \ud835\udf14\u0001 combining SFT weights and a linear probed [91] classifier.\n2. Diverse fine-tunings. We run \ud835\udc40 RM fine-tunings, optimizing Equation (1) with diverse hyperpa-\nrameters (as in a grid search), yielding \ud835\udc40 weights {\ud835\udf19\ud835\udc56}\ud835\udc40\n\ud835\udc56=1.\n3. Weight averaging. We average those \ud835\udc40 weights together to form \ud835\udf19WARM = 1\n\ud835\udc40\n\u00cd\ud835\udc40\n\ud835\udc56=1 \ud835\udf19\ud835\udc56.\n5\nWARM: On the Benefits of Weight Averaged Reward Models\nThen \ud835\udc5f\ud835\udf19WARM serves as the proxy RM to guide the RL procedure, as efficiently as an individual RM, but\nwith the enhanced reliability and robustness provided by the WA strategy, that leverages the strengths\nand mitigates the weaknesses of the individual RMs.\n3.2. Linear mode connectivity\nCompared to ENS, the main difference lies in how WARM combines the different RMs: we do so\nthrough linear interpolation in the weight space. It relies on the linear mode connectivity (LMC) [44, 45]\nproperty across fine-tuned weights, i.e., the fact that the accuracy of the interpolated model is at\nleast as good as the interpolation of the individual accuracies. Precisely, by defining the pairwise\naccuracy of an RM \ud835\udc5f\ud835\udf19 w.r.t. a dataset D as Acc\u0000\ud835\udc5f\ud835\udf19, D\u0001 = \ud835\udd3c(\ud835\udc65,\ud835\udc66+,\ud835\udc66\u2212)\u2208D\n\u0002\n1\ud835\udc5f\ud835\udf19(\ud835\udc65,\ud835\udc66+)\u2265\ud835\udc5f\ud835\udf19(\ud835\udc65,\ud835\udc66\u2212)\n\u0003\n, the following\nObservation 1 underpins the success of WARM.\nObservation 1 (LMC). Given two fine-tuned weights \ud835\udf191 and \ud835\udf192 with a shared pre-training and a test\ndataset D\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61, then for all \ud835\udf06 \u2208 [0, 1],\nAcc\u0000\ud835\udc5f(1\u2212\ud835\udf06)\u00b7\ud835\udf191+\ud835\udf06\u00b7\ud835\udf192, D\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61\n\u0001 \u2265 (1 \u2212 \ud835\udf06) \u00d7 Acc\u0000\ud835\udc5f\ud835\udf191, D\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61\n\u0001 + \ud835\udf06 \u00d7 Acc\u0000\ud835\udc5f\ud835\udf192, D\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61\n\u0001\n.\n(2)\nWe empirically validate this LMC in Figure 3, by evaluating interpolated RMs on OOD test samples.\nThis follows similar observations for multi-class classification in the context of computer vision [44, 45],\nwhich led to a plethora of weight averaging (WA) works such as the model soups [46, 47, 48] variants\n(detailed in our related work in Appendix A.1).\nRemark 1 (Importance of pre-training and linear probing). The efficacy of WA can be surprising\ngiven the non-linearities [54] and permutation symmetries [92] in deep neural network architectures.\nWA is actually possible only because of the shared pre-training which constrains the divergence during\nfine-tunings [45], such as the weights remain in convex regions of the loss valley [93]. In contrast, the\nLMC does not hold when training weights from scratch [45], even if the random initialization is shared.\nFor these reasons and to facilitate the LMC, we follow [47, 48] and use linear probing to initialize the\nclassifier \ud835\udf14; compared to random initialization, such linear probing prevents feature distortion [91].\n3.3. Sources of diversity\nOn one hand, WARM requires shared pre-training so that the fine-tuned weights remain linearly\nconnected. On the other hand, weights must not be identical: actually, the diversity across those\nfine-tuned weights significantly contributes to the accuracy gains observed in WA [47]. Overall, an\neffective WARM requires a delicate trade-off between ensuring LMC and diversity across weights.\nIn practice, we use the following sources of diversity [94], leading the RM fine-tunings to diverse\nyet linearly connected models. First, the different fine-tunings see the data samples in different\norders. Second, we sample slightly different hyperparameters, notably different learning rates and\ndropout probabilities, as detailed in Appendix B.3. Third, we investigate a new source of diversity in\ninitialization named Baklava, illustrated in Figure 2. Specifically, we initialize the RMs\u2019 featurizers\nfrom different checkpoints {\ud835\udf03\ud835\udc60 \ud835\udc53\ud835\udc61\n\ud835\udc56 }\ud835\udc40\n\ud835\udc56=1 collected along a given SFT trajectory. Baklava relaxes the shared\ninitialization constraint from model soups [46] to simply sharing the same pre-training: Baklava is\nactually an efficient alternative to model ratatouille [48] but without the need of multiple auxiliary\ntasks. Overall, Baklava increases diversity compared to only initializing from the last SFT checkpoint,\nwhile adhering to the shared pre-training requisite for LMC, without incurring any overhead.\n6\nWARM: On the Benefits of Weight Averaged Reward Models\nSFT\nReward\nmodelings\nWeight\naveraging\n\ud835\udf03\ud835\udc5d\ud835\udc61\n\ud835\udf03\ud835\udc60 \ud835\udc53\ud835\udc61\n1\n\ud835\udf03\ud835\udc60 \ud835\udc53\ud835\udc61\n2\n\ud835\udf03\ud835\udc60 \ud835\udc53\ud835\udc61\n\ud835\udc40\n\ud835\udf191\n\ud835\udf192\n\ud835\udf19\ud835\udc40\n\ud835\udf19WARM = 1\n\ud835\udc40\n\u00cd\ud835\udc40\n\ud835\udc56=1 \ud835\udf19\ud835\udc56\nFigure 2 | Baklava diversity procedure. Starting from a pre-trained LLM \ud835\udf03\ud835\udc5d\ud835\udc61, we consider different checkpoints\n{\ud835\udf03\ud835\udc60 \ud835\udc53\ud835\udc61\n\ud835\udc56 }\ud835\udc40\n\ud835\udc56=1 along a single SFT run (dashed arrow\n) collected at different number of SFT training steps. Those\ncheckpoints serve as initializations for \ud835\udc40 RM fine-tunings on the preference dataset (thick solid arrows\n)\nto learn the {\ud835\udf19\ud835\udc56}\ud835\udc40\n\ud835\udc56=1. Finally, those RMs are weight averaged (dotted arrows\n) into the final model \ud835\udf19WARM.\nFollowing the culinary analogy from model soups [46] and model ratatouille [48], we named this method\nBaklava because of its diamond geometric shape.\nRemark 2 (Moving average). Following stochastic weight average [95] or moving average [96], we\nalso tried to average checkpoints collected along a single RM fine-tuning. Though interesting because less\ncostly for training, the lower results in Figure 3(a) suggest that the accuracy-diversity trade-off was not\nfavorable: incorporating early checkpoints would compromise individual accuracies, and considering only\nlater checkpoints would not bring the necessary diversity. As a result, we opted to use in WARM only the\nlast checkpoint from each RM fine-tuning.\n4. On the benefits of WARM\nWe now explore the properties and benefits from the WARM strategy, previously described in Section 3.\nWe ground our analysis on the empirical comparison between WA and ENS for reward modeling, and\na novel general theoretical comparison in Section 4.3.\nExperimental setup. We leverage the TL;DR summarization benchmark [97], a standard in reward\nmodeling for LLMs, that we briefly describe below and further detail in Appendix B. The goal of the\nRMs is to score summaries such as they are ranked properly. In training, we use the dataset D\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b\nfrom Stiennon et al. [14] where the candidate summaries are generated by GPT-3 [6] variants. To\nobtain the labels, we follow the RLAIF procedure from [58], where a PaLM-L [98] is prompted with\nchain-of-thought [99] to generate feedback mimicking human preferences. This strategy performs\nsimilarly to human labelers with similar inter-agreement, and will be useful in Section 5 as an oracle\nmetric. The RMs are PaLM-XXS models, pre-trained and SFT-ed on the preferred summaries from\nD\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b, on which we plug a linear probed [91] classification layer. We train the RMs for 10k steps on\nD\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b, with hyperparameters and procedure detailed in Appendix B.3. We report accuracies of those\nRMs on a novel out-of-distribution (OOD) test dataset D\ud835\udc5c\ud835\udc5c\ud835\udc51 with 92k pairwise comparisons where the\nsummaries are generated by multiple PaLM-XS policies with high temperature, some of which are\npre-trained only, others SFT-ed and others RLHF-ed.\n4.1. 1st order analysis: weight averaging for reliable and more efficient ensembling\nPrevious works [46, 47, 95] have argued that the best way to understand WA is as an efficient\napproximation of ENS, as clarified in Observation 2.\nObservation 2 (WA and ENS: 1st order analysis). Weight averaging and prediction ensembling perform\nsimilarly: i.e., for all \ud835\udf06 \u2208 [0, 1] and a test dataset D\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61,\nAcc\u0000\ud835\udc5f(1\u2212\ud835\udf06)\u00b7\ud835\udf191+\ud835\udf06\u00b7\ud835\udf192, D\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61\n\u0001 \u2248 Acc\u0000(1 \u2212 \ud835\udf06) \u00d7 \ud835\udc5f\ud835\udf191 + \ud835\udf06 \u00d7 \ud835\udc5f\ud835\udf192, D\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61\n\u0001\n.\n(3)\n7\nWARM: On the Benefits of Weight Averaged Reward Models\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.758\n0.759\n0.760\n0.761\n0.762\n0.763\n0.764\n0.765\nAcc.\nWA\nENS\nDiag\n(a) 1 RM fine-tuning at 2\ndifferent training steps.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.758\n0.759\n0.760\n0.761\n0.762\n0.763\n0.764\n0.765\nAcc.\nWA\nENS\nDiag\n(b) 2 RM fine-tunings with\nshared config.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.758\n0.759\n0.760\n0.761\n0.762\n0.763\n0.764\n0.765\nAcc.\nWA\nENS\nDiag\n(c) 2 RM fine-tunings with\ndifferent learning rates.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.758\n0.759\n0.760\n0.761\n0.762\n0.763\n0.764\n0.765\nAcc.\nWA\nENS\nDiag\n(d) 2 RM fine-tunings with\ndifferent inits: Baklava.\nFigure 3 | Experiments under distribution shifts validating Observations 1 and 2 on the TL;DR summa-\nrization benchmark [97]. We report the accuracies on D\ud835\udc5c\ud835\udc5c\ud835\udc51 when interpolating between two RM weights \ud835\udf191\nand \ud835\udf192 with the coefficient \ud835\udf06 sliding between 0 and 1. WA stands for weight averaging \ud835\udc5f(1\u2212\ud835\udf06)\u00b7\ud835\udf191+\ud835\udf06\u00b7\ud835\udf192 while ENS\ncombines the predictions (1\u2212 \ud835\udf06) \u00d7\ud835\udc5f\ud835\udf191 + \ud835\udf06 \u00d7\ud835\udc5f\ud835\udf192; Diag is the interpolated accuracy (1 \u2212 \ud835\udf06) \u00d7Acc\u0000\ud835\udc5f\ud835\udf191\n\u0001 + \ud835\udf06 \u00d7Acc\u0000\ud835\udc5f\ud835\udf192\n\u0001.\nWe consider sources of increasing diversity [94] between \ud835\udf191 and \ud835\udf192: in Figure 3(a), they are collected at\ndifferent number of training steps (8k and 10k) along a single RM fine-tuning; in Figure 3(b), they are from\ntwo independant RM fine-tunings, with the exact same config, but seeing the data in different orders; in\nFigure 3(c), they have different learning rates (1e-4 and 4e-5); in Figure 3(d), they are initalized from different\nSFT checkpoints collected at different number of SFT steps (8k and 12k), per Baklava introduced in Figure 2.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.45\n0.46\n0.47\n0.48\n0.49\nAcc.\nWA\nENS\nDiag\n(a) Train (corrupt).\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.894\n0.896\n0.898\n0.900\n0.902\nAcc.\nWA\nENS\nDiag\n(b) Train (clean).\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.766\n0.768\n0.770\n0.772\n0.774\n0.776\n0.778\nAcc.\nWA\nENS\nDiag\n(c) Validation (ID).\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.708\n0.710\n0.712\n0.714\n0.716\nAcc.\nWA\nENS\nDiag\n(d) Test (OOD).\nFigure 4 | Corruption experiment validating Observation 3. We consider \ud835\udf191 and \ud835\udf192, two RMs fine-tuned\nindependently with the same config as in Figure 3(b), but this time with 25% of the training labels corrupted.\nWe then report the performances of their WA and ENS on the different data subsets. We observe that WA\nreduces memorization of the corrupted labels in Figure 4(a), and still performs slightly worse than ENS on the\nclean training samples in Figure 4(b); yet, the performances of WA w.r.t. ENS improves as we move away from\nthe training distribution, in particular on D\ud835\udc5c\ud835\udc5c\ud835\udc51 in Figure 4(d) where WA generalizes better.\nTheoretically, a simple Taylor expansion can justify this similarity when \u2225\ud835\udf191 \u2212 \ud835\udf192\u2225 \u226a 1. Empirically,\nthis is validated in Figure 3 where the accuracy curves on D\ud835\udc5c\ud835\udc5c\ud835\udc51 for WA and ENS closely match. This\nsimilarity justifies that WA is a variance reduction method; then, because variance is the dominant\nissue under distribution shifts [47], this explains the significant gains in Figure 3 over the individual\nRMs \ud835\udf191 and \ud835\udf192 (validating Observation 1), in particular when weights are sufficiently diverse. This\nsuggests improved reliability in WARM, with efficiency benefits over ENS: indeed, WA maintains a\nsingle set of weights, removing the memory and inference overheads from ENS.\n8\nWARM: On the Benefits of Weight Averaged Reward Models\n4.2. 2nd order analysis: weight averaging for more robust ensembling\nA surprising fact remains unexplained. WA is slightly superior to ENS under distribution shifts,\nwhich one can see on the plots from Figure 3, and more consistently in Figure B.1 from model soups\n[46] or in Figure 1 from DiWA [47]. More generally, WA is the state-of-the-art strategy for OOD\ngeneralization, consistently outperforming ENS; yet, this was not explained in previous works, thus\nurging for new insights about the difference between WA and ENS.\nCorruption setup. To refine our understanding on the difference between WA and ENS, we propose\na new setup where 25% of the binary labels are swapped in training. We then report the per-subset\naccuracies on Figure 4, enriched in Appendix C.1 and aggregated in Figure 5. On the corrupted\nsubset of training data, the accuracy curve for WA is below the expected accuracies, while it is above\non all other subsets. More precisely, we make the following Observation 3.\nObservation 3 (WA and ENS: 2nd order analysis). The accuracy gains of WA over ENS grow as data\nmoves away from the training distribution.\n\u2022 WA \u226a ENS on train corrupt: WA is far worse than ENS on train samples with swapped labels,\nshowing reduced memorization and improved robustness to label corruption.\n\u2022 WA \u2264 ENS on train clean: WA is worse than ENS on train samples with correct labels.\n\u2022 WA \u2a86 ENS on ID val: WA is better or similar to ENS on samples without distribution shifts.\n\u2022 WA \u2265 ENS on OOD test: WA is far better than ENS on test samples from new distributions, showing\nbetter reliability under distribution shifts.\n0.06\n0.05\n0.04\n0.03\n0.02\n0.01\n0.00\n0.01\n0.02\nAccuracy gain of WA over ENS\n0\n50\n100\n150\n200\nDensity\nTrain corrupted\nTrain clean\nVal ID\nTest OOD\nTrain corrupted\nTrain clean\nVal ID\nTest OOD\nFigure 5 | Histograms of the differ-\nences in accuracy between WA and\nENS on different data subsets.\nOverall, this suggests that weight averaging memorizes less and\ngeneralizes better than ensembling predictions.\n4.3. Weight averaging enforces invariance across runs\nWe now provide theoretical support to this Observation 3. In brief,\nour simplifying assumptions suggest that WA acts as a regulariza-\ntion towards the predictive mechanisms that are invariant across\nruns, i.e., learned simultaneously in each independent run. Then,\nin contrast with ENS, WA would improve robustness to corruption\nbecause it would underweight the run-specific features (with low\nprobability of being learned) inducing memorization.\nSetup. We follow Lin et al. [53], and consider a simplified binary classification setup with labels\n\ud835\udc66 \u2208 {\u22121, 1}, related to \ud835\udc39 features {\ud835\udc67 \ud835\udc57}\ud835\udc39\n\ud835\udc57=1 such as \ud835\udc67 \ud835\udc57 \u2208 \u211d\ud835\udc51. From inputs \ud835\udc65, we train a binary classifier\n\ud835\udc5f(\ud835\udc65) = \ud835\udf14\u22ba \ud835\udc53 (\ud835\udc65). Following [53], we make three key assumptions. First, features orthogonality: we\nassume that {\ud835\udc67 \ud835\udc57}\ud835\udc39\n\ud835\udc57=1 are orthogonal, i.e., (\ud835\udc67 \ud835\udc57)\u22ba\ud835\udc67 \ud835\udc57\u2032 = 0 when \ud835\udc57 \u2260 \ud835\udc57\u2032. Second, input as bag of features: we\nassume that the input \ud835\udc65 =\n\u0002\n\ud835\udc65 \ud835\udc57\u0003 \ud835\udc39\n\ud835\udc57=1 \u2208 \u211d\ud835\udc39\u00d7\ud835\udc51 can be represented as the concatenation of \ud835\udc65 \ud835\udc57 generated\nby \ud835\udc65 \ud835\udc57 \u223c N \u0000\ud835\udc66 \u00b7 \ud835\udc67 \ud835\udc57, \ud835\udf0e \u00b7 I\ud835\udc51\n\u0001 with \ud835\udf0e \u226a 1. Finally, the binary featurizer assumption: we consider that the\nfeaturizer \ud835\udc53 =\n\u0002\n\ud835\udc53 \ud835\udc57\u0003 \ud835\udc39\n\ud835\udc57=1 \u2208 {0, 1}\ud835\udc39 is a binary selector of the features that make the input. For example,\nif \ud835\udc66 = 1, \ud835\udc39 = 3, \ud835\udc65 \u2248 [\ud835\udc671, \ud835\udc672, \ud835\udc673], and \ud835\udc53 = [1, 0, 1] learns to extract the first and third features, then\n\ud835\udc53 (\ud835\udc65) \u2248 \ud835\udc671 + \ud835\udc673. We denote \ud835\udc5d\ud835\udc57 the probability that the featurizer \ud835\udc53 learns to use the \ud835\udc57-th feature\ndimension (associated with \ud835\udc67 \ud835\udc57); this means \ud835\udc53 \ud835\udc57 is 1 with probability \ud835\udc5d\ud835\udc57 and 0 otherwise. Moreover,\nfor infinite training samples and under some constraint on \ud835\udf0e, Lemma 5 in [53] proved that, to learn\n\ud835\udc5f = \ud835\udf14\u22ba \ud835\udc53, the optimal linear fit \ud835\udf14 on the features selected from \ud835\udc53 would be \ud835\udf14 = \u00cd\ud835\udc39\n\ud835\udc57=1 \ud835\udc53 \ud835\udc57 \u00b7 \ud835\udc67 \ud835\udc57.\n9\nWARM: On the Benefits of Weight Averaged Reward Models\nResults. We consider \ud835\udc40 RMs {\ud835\udc5f\ud835\udc56 = \ud835\udf14\u22ba\n\ud835\udc56 \ud835\udc53\ud835\udc56}\ud835\udc40\n\ud835\udc56=1, and compare the limit behaviours of their predic-\ntion ensembling \ud835\udc5f\ud835\udc38\ud835\udc41\ud835\udc46\n\ud835\udc40\nand weight averaging \ud835\udc5f\ud835\udc4a \ud835\udc34\n\ud835\udc40\nwhen \ud835\udc40 \u2192 \u221e. In this limit case, the averaged\nprediction \ud835\udc5f\ud835\udc38\ud835\udc41\ud835\udc46\n\ud835\udc40\n=\n1\n\ud835\udc40\n\u00cd\ud835\udc40\n\ud835\udc56=1 \ud835\udf14\u22ba\n\ud835\udc56 \ud835\udc53\ud835\udc56 for an input \ud835\udc65 from label \ud835\udc66 tends towards the expected prediction\n\ud835\udd3c[\ud835\udc5f(\ud835\udc65)] = \ud835\udd3c[\ud835\udf14\u22ba \ud835\udc53 (\ud835\udc65)] = \ud835\udd3c{ \ud835\udc53 \ud835\udc57}\ud835\udc39\n\ud835\udc57=1\n\u0002\u0000 \u00cd\ud835\udc39\n\ud835\udc57=1 \ud835\udc53 \ud835\udc57 \u00b7 \ud835\udc67 \ud835\udc57\u0001\u22ba(\u00cd\ud835\udc39\n\ud835\udc57\u2032=1 \ud835\udc53 \ud835\udc57\u2032 \u00b7 \ud835\udc65 \ud835\udc57\u2032)\n\u0003\n\u2248 \ud835\udc66 \u00b7 \u00cd\ud835\udc39\n\ud835\udc57=1 \ud835\udc5d\ud835\udc57 \u00b7 |\ud835\udc67 \ud835\udc57|2, using \ud835\udc65 \ud835\udc57\u2032 \u2248 \ud835\udc66 \u00b7 \ud835\udc67 \ud835\udc57\u2032 thus\n(\ud835\udc67 \ud835\udc57)\u22ba\ud835\udc65 \ud835\udc57\u2032 \u2248 0 when \ud835\udc57 \u2260 \ud835\udc57\u2032, and ( \ud835\udc53 \ud835\udc57)2 = \ud835\udc53 \ud835\udc57.\n\ud835\udc5f\ud835\udc38\ud835\udc41\ud835\udc46\n\ud835\udc40\n(\ud835\udc65) \u2212\u2212\u2212\u2212\u2212\u2192\n\ud835\udc40\u2192\u221e \ud835\udd3c[\ud835\udc5f(\ud835\udc65)] \u2248 \ud835\udc66 \u00b7\n\ud835\udc39\n\u2211\ufe01\n\ud835\udc57=1\n\ud835\udc91\ud835\udc8b \u00b7 |\ud835\udc67 \ud835\udc57|2.\n(4)\nIn contrast, when considering \ud835\udc5f\ud835\udc4a \ud835\udc34\n\ud835\udc40\n=\n\u0010\n1\n\ud835\udc40\n\u00cd\ud835\udc40\n\ud835\udc56=1 \ud835\udf14\ud835\udc56\n\u0011\u22ba\u0010\n1\n\ud835\udc40\n\u00cd\ud835\udc40\n\ud835\udc56=1 \ud835\udc53\ud835\udc56\n\u0011\nwith \ud835\udc40\n\u2192\n\u221e, we have\n1\n\ud835\udc40\n\u00cd\ud835\udc40\n\ud835\udc56=1 \ud835\udc53\ud835\udc56 \u2212\u2212\u2212\u2212\u2212\u2192\n\ud835\udc40\u2192\u221e \ud835\udd3c[ \ud835\udc53] =\n\u0002\n\ud835\udc5d\ud835\udc57\n\u0003 \ud835\udc39\n\ud835\udc57=1 and 1\n\ud835\udc40\n\u00cd\ud835\udc40\n\ud835\udc56=1 \ud835\udf14\ud835\udc56 \u2212\u2212\u2212\u2212\u2212\u2192\n\ud835\udc40\u2192\u221e \ud835\udd3c[\ud835\udf14] = \u00cd\ud835\udc39\n\ud835\udc57=1 \ud835\udc5d\ud835\udc57 \u00b7 \ud835\udc67 \ud835\udc57, and thus:\n\ud835\udc5f\ud835\udc4a \ud835\udc34\n\ud835\udc40 (\ud835\udc65) \u2212\u2212\u2212\u2212\u2212\u2192\n\ud835\udc40\u2192\u221e\n\u00a9\u00ad\n\u00ab\n\ud835\udc39\n\u2211\ufe01\n\ud835\udc57=1\n\ud835\udc5d\ud835\udc57 \u00b7 \ud835\udc67 \ud835\udc57\u00aa\u00ae\n\u00ac\n\u22ba\n\u00a9\u00ad\n\u00ab\n\ud835\udc39\n\u2211\ufe01\n\ud835\udc57\u2032=1\n\ud835\udc5d\ud835\udc57\u2032 \u00b7 \ud835\udc65 \ud835\udc57\u2032\u00aa\u00ae\n\u00ac\n\u2248 \ud835\udc66 \u00b7\n\ud835\udc39\n\u2211\ufe01\n\ud835\udc57=1\n\ud835\udc912\n\ud835\udc8b \u00b7 |\ud835\udc67 \ud835\udc57|2.\n(5)\nInterpretation. For ENS, the coefficient for a given feature is \ud835\udc91\ud835\udc8b, the same as the probability of\nthis information being used by any individual network. In contrast, WA involves the square of the\nprobability \ud835\udc912\n\ud835\udc8b . Thus WA reduces the reliance on features with low probability, related to minor\nspecific information (such as noise or context) which can be used to fit the corrupted training samples;\nthis would reduce memorization, and thus explains the robustness of WA under label corruption.\nReciprocally, WA tends to prioritize the most probable features, favoring the mechanisms that are\nconsistently learned, in other words the mechanisms invariant across runs. Overall, WA acts as a\nregularization, improving robustness under label corruption by tackling run-specific mechanisms\nfavoring memorization, and improving reliability under distribution shifts by preserving run-invariant\nmechanisms favoring generalization.\nRemark 3 (Invariance). We argue that weight averaging only keeps the invariant predictive mechanisms\nacross runs. This is in analogy with the invariance literature [50], popular for domain generalization\n[51, 100] under spurious correlations, where the key idea is that the predictive mechanisms which are\ninvariant across domains are the causal ones that are stable under distribution shifts. This theoretically\nconnects two key paradigms for OOD generalization, ensembling and invariance, and shows that weight\naveraging actually benefits from both.\nRemark 4 (Extension to a deeper structure with \ud835\udc3f layers). We obtain a square in \ud835\udc912\n\ud835\udc8b due to our\nsimplified two-layer architecture. Yet, in full generality, using a deeper structure with \ud835\udc3f layers would\nlead to \ud835\udc91\ud835\udc73\n\ud835\udc8b . Intuitively, WA applies an AND-mask on the information, that need to be found both in the\nprevious feature space and the next layer weights.\nRemark 5 (From reward robustness to learnability). When applied to the design of RMs in WARM,\nwe now argue that WA facilitates WARM\u2019s stability [87] by mitigating the reliance on some non-robust\nfeatures. Indeed, WA makes the WARM reward more robust to small (potentially adversarial [101])\nperturbations [102], i.e., smoother [103] in the input space. This relates to the Lipschitzness property\nof the reward [104, 105, 106], where the difference in predicted rewards is bounded by the distance in\ninput space. Fortunately, such smoothness is useful in RL [107], in particular for the stability of the\npolicy gradient [108] because \u201csharp changes in reward value are hard to represent and internalize\u201d\n[109]. This is studied in Lipschitzness is all you need [109] where the authors argue that \u201cthe local\nLipschitzness of the reward is a sine qua non condition for good performance\u201d, required \u201cto even learn\nanything\u201d. In summary, robustness improves stability and hinders the cascade of errors occurring when\nminor input variations can cause large reward differences.\n10\nWARM: On the Benefits of Weight Averaged Reward Models\nIn conclusion, we summarize the benefits from WARM. First, WARM is efficient, incurring no memory\nor computation costs, as it returns a single model. Second, WARM reduces variance while leveraging\nmechanisms invariant across runs, thus improving its reliability under distribution shifts. Lastly,\nWARM also addresses label corruption, thereby augmenting robustness to noisy preferences.\n5. Experiments\nTo empirically validate WARM\u2019s benefits described in previous section, we train PaLM-XXS RMs on\nthe TL;DR summarization benchmark [97] where preference labels are generated by a PaLM-L model\nprompted with chain-of-thought [99]. This AI labeling approach, increasingly common in recent\nresearch [26, 41, 110] as an efficient alternative to human assessments, is motivated by studies\n[57, 58] indicating that it correlates well with human preferences: critically, it provides an automatic\npairwise oracle preference metric to evaluate reward hacking (in a similar fashion to the distillation\nsetup from [17], discussed in Appendix C.4). In addition, we leverage a PaLM-XS RM for pointwise\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nKL : log(N)\nN\n1\nN\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\nControl reward gain\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\n(a) PaLM (clean).\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nKL : log(N)\nN\n1\nN\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nControl reward gain\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\n(b) PaLM (corrupt).\n0\n1\n2\n3\n4\n5\n6\nKL : log(N)\nN\n1\nN\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nControl reward gain\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\n(c) T5 (clean).\n0\n1\n2\n3\n4\n5\n6\nKL : log(N)\nN\n1\nN\n0.1\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nControl reward gain\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\n(d) T5 (corrupt).\nFigure 6 | Control reward for BoN experiments: clean preference dataset in Figures 6(a) and 6(c) and 25%\ncorruptions in Figures 6(b) and 6(d). We consider two SFT policies to generate candidate summaries: one\nbased on PaLM architecture [98], the other on T5 architecture [111]. The \ud835\udc65-axis is the KL between the BoN\npolicy and the SFT policy; the \ud835\udc66-axis represents the control reward gains w.r.t. to an RM \ud835\udf191, which was the\nbest individual RM on D\ud835\udc5c\ud835\udc5c\ud835\udc51. The blue lines represent WARM with \ud835\udc40 weights: WARM performs higher than the\nindividual RMs (in yellows) or when ensembling their predictions (ENS in red). We report the absolute control\nrewards for those experiments in Figure 15, where the values range roughly between 3 and 7.\n0\n1\n2\n3\n4\n5\n6\nKL : log(N)\nN\n1\nN\n0.750\n0.775\n0.800\n0.825\n0.850\n0.875\n0.900\n0.925\nWin ratio vs. SFT\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\n(a) SFT (clean).\n0\n1\n2\n3\n4\n5\n6\nKL : log(N)\nN\n1\nN\n0.70\n0.75\n0.80\n0.85\n0.90\nWin ratio vs. SFT\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\n(b) SFT (corrupt).\n0\n1\n2\n3\n4\n5\n6\nKL : log(N)\nN\n1\nN\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\n0.50\nWin ratio vs. WARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\ny=0.5\n(c) WARM (clean).\n0\n1\n2\n3\n4\n5\n6\nKL : log(N)\nN\n1\nN\n0.300\n0.325\n0.350\n0.375\n0.400\n0.425\n0.450\n0.475\n0.500\nWin ratio vs. WARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\ny=0.5\n(d) WARM (corrupt).\nFigure 7 | Oracle preference metric for BoN experiments on T5 generations: clean preference dataset in\nFigures 7(a) and 7(c) and 25% corruptions in Figures 7(b) and 7(d). We plot the win rates for different values\nof \ud835\udc41 vs. two reference strategies: SFT (i.e., random selection or equivalently BoN with \ud835\udc41 = 1), or selecting the\nbest summary according to WARM \ud835\udc40 = 6. We observe that all strategies beat the SFT reference (they are all\nabove 50% win rate), but that none beat the WARM \ud835\udc40 = 6 reference.\n11\nWARM: On the Benefits of Weight Averaged Reward Models\ncontrol reward reaching 80.1% accuracy on the OOD dataset D\ud835\udc5c\ud835\udc5c\ud835\udc51. As verified in our experiments, this\ncontrol RM also detects hacking, as it benefits from a larger architecture and a disjoint pretraining\ncompared to the PaLM-XXS RMs of interest. Below, we explore two key scenarios: in Section 5.1,\nWARM reranks outputs in best-of-\ud835\udc41 (BoN); in Section 5.2, WARM guides the RL procedure.\n5.1. Best-of-\ud835\udc41 experiments\nSetup. We start with best-of-\ud835\udc41 (BoN) sampling experiments in Figures 6 and 7. Given a dataset\nof \ud835\udc37 text prompts, for each prompt we generate \ud835\udc41 summaries from a SFT policy, and then returns\nthe summary with the highest reward according to different RMs. We actually consider two SFT\npolicies; one based on PaLM architecture [98] (\ud835\udc41 = 8, \ud835\udc37 = 15000), the other on T5 architecture [111]\n(\ud835\udc41 = 1000, \ud835\udc37 = 1000). For the \ud835\udc65-axis, we plot the KL between the BoN policy and the SFT policy,\nwhich can be approximated by log(\ud835\udc41) \u2212 \ud835\udc41\u22121\n\ud835\udc41\n[112, 113]. BoN is effective [16], especially in the low-KL\nregime (i.e., for small \ud835\udc41). We consider two setups, without (clean setup) and with (corrupt setup)\n25% label corruption in the preference datasets for reward modeling, and denote in each setup the\nweights {\ud835\udf19\ud835\udc56}\ud835\udc40\n\ud835\udc56=1 sorted in decreasing accuracy on D\ud835\udc5c\ud835\udc5c\ud835\udc51.\nControl reward. Figure 6 shows that, in terms of pointwise control reward, WARM performs con-\nsistently better than ENS (only with \ud835\udc40 = 2 for computational reasons) and the two best individual\nRMs \ud835\udf191 and \ud835\udf192; moreover, the gains get bigger for \ud835\udc40 = 6. As a side note, we also observe that the\nindividual RM \ud835\udf192 performs better in BoN in Figure 6(c) than \ud835\udf191 though \ud835\udf191 was better than \ud835\udf192 on\nD\ud835\udc5c\ud835\udc5c\ud835\udc51, highlighting that selecting the appropriate individual RM is not trivial [41].\nOracle preference. In Figure 7, we leverage the pairwise oracle preference [58] metric to validate\nbetter performance with WARM. We observe in Figures 7(a) and 7(b) that summaries selected with\nWARM have a win rate of up to 92.5% against the random selection of a summary (from SFT). We\nalso see in Figures 7(c) and 7(d) that reciprocally, all selection strategies have a win rate lower than\n50% against the summaries selected by WARM \ud835\udc40 = 6.\n5.2. RL experiments\nSetup. For RL fine-tuning of policies, we follow [58] and use their modified version of REINFORCE [60]\nwith a baseline value score for variance reduction, a simpler algorithm than PPO [61] yet still effective\nfor LLMs. Both policy and value LLMs are PaLM-XS, initialized from the same SFT model. We then\ngenerate samples with the policy, compute the reward with the RMs and update the weights to\noptimize this reward. More details are available in Appendix B.4. To reduce forgetting and encourage\nthe policy to remain close to its SFT initialization, we incorporate a KL regularization [81, 82]\ncontrolled by a coefficient \ud835\udefc, ablated in Figure 8(c), yet otherwise set to 0.003 in the clean setup and\n0.01 in the corrupt setup. This KL serves as the \ud835\udc65-axis in our plots to estimate model drift, as done in\nthe literature; same curves with the number of training steps as the \ud835\udc65-axis in Figures 1(b) and 18.\nControl reward. In Figure 8, we observe reward hacking; as the policy moves away from its SFT\ninitialization, the control reward collapses. Critically, WARM improves performances: in particular,\nincreasing \ud835\udc40 pushes the Pareto front of solutions to the top left in Figures 8(a) and 8(b). In comparison,\npolicies trained with ENS (with \ud835\udc40 = 2 for computational reasons) are still susceptible to early reward\nhacking, while reaching absolute control rewards significantly worse than with WARM (even with\n\ud835\udc40 = 2). In Figure 8(c), we confirm that the \ud835\udefc hyperparameter plays a crucial role; low values of \ud835\udefc\nsuch as 0.001 correspond to high KL, while high values of \ud835\udefc such as 0.01 entail low KL but a risk of\nunderfitting. From a practical perspective, this highlights that the optimal value of \ud835\udefc for WARM is\nlower than for a single RM; this is because WARM can mitigate reward hacking, and thus the optimal\npolicies are obtained for larger values of KL.\n12\nWARM: On the Benefits of Weight Averaged Reward Models\n0\n200\n400\n600\n800\n1000\nKL\n3\n4\n5\n6\n7\n8\n9\n10\n11\nControl reward\nWARM M = 10\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\n(a) RL (clean).\n0\n25\n50\n75\n100\n125\n150\n175\nKL\n4\n5\n6\n7\nControl reward\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\n(b) RL (corrupt).\n0\n250\n500\n750\n1000\n1250\n1500\n1750\nKL\n2\n4\n6\n8\n10\nControl reward\nWARM \n= 0.01\nWARM \n= 0.003\nWARM \n= 0.001\nInd \n= 0.01\nInd \n= 0.003\nInd \n= 0.001\n(c) Ablating \ud835\udefc for RL (clean).\nFigure 8 | Control reward for RL experiments: clean preference dataset in Figures 8(a) and 8(c) and 25%\ncorruptions in Figure 8(b). The blue lines show the RL fine-tuning of policies when averaging \ud835\udc40 weights as the\nRM; the darker, the higher the \ud835\udc40. It performs higher than when RL fine-tuning with the individual RMs (in\nyellows) or when ensembling their predictions (in red). Figure 8(c) shows results of policies RL fine-tuned with\nWARM \ud835\udc40 = 6 or \ud835\udf191, for different values of \ud835\udefc controlling the KL regularization strength.\n2000\n3000\n4000\n5000\n6000\n# steps\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nWin ratio vs. SFT\nWARM M = 10\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\ny=0.5\n(a) SFT.\n2000\n3000\n4000\n5000\n6000\n# steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nWin ratio vs. WARM M = 6 at step 3500\nWARM M = 10\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\ny=0.5\n(b) WARM \ud835\udc40 = 6.\n2000\n3000\n4000\n5000\n6000\n# steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nWin ratio vs. Ind \n1 at step 3000\nWARM M = 10\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\ny=0.5\n(c) \ud835\udf191 (best individual RM).\nFigure 9 | Oracle preference metric for RL experiments: clean preference dataset. We plot the win rates\nalong RL fine-tuning against three reference policies: the SFT policy, the policy RL fine-tuned with WARM\n\ud835\udc40 = 6 after 3500 steps, and the policy RL fine-tuned with \ud835\udf191 after 3000 steps. Figure 19 reports results when\ncomparing policies at fixed number of training steps.\nOracle preference. In Figure 9, we compare the different policies according to our pairwise oracle\npreference AI labeler [58]. In Figure 9(a), the reference policy is the SFT initialization; all the RL\nfine-tuned policies outperform this baseline, with WARM \ud835\udc40 = 6 reaching a win rate of 99.8% after\n3500 steps (the highest win rate among all policies). We use this policy as the reference in Figure 9(b);\nno other policy could beat it. Interestingly, we observe that using \ud835\udc40 = 10 rewards can delay reward\nhacking but does not improve the peak performance; we speculate this is related to our weight\nselection procedure, as the weights {\ud835\udf19\ud835\udc56}10\n\ud835\udc56=7 have lower individual accuracy on D\ud835\udc5c\ud835\udc5c\ud835\udc51 than {\ud835\udf19\ud835\udc56}6\n\ud835\udc56=1 (more\ndetails in Figure 10). Finally, in Figure 9(c), the reference policy is obtained after 3000 steps of\nRL fine-tuning with \ud835\udf191 (the best individual RM on D\ud835\udc5c\ud835\udc5c\ud835\udc51). There is a large region of steps in which\npolicies trained WARM (even with \ud835\udc40 = 2) beat this approach; the previous reference from Figure 9(b)\nactually has a 79.4% win rate against it.\n13\nWARM: On the Benefits of Weight Averaged Reward Models\n6. Discussion\nBenefits. WARM represents a flexible and pragmatic method to improve the alignment of AI with\nhuman values and societal norms. This paper has detailed several of its benefits, and below, we delve\ninto additional, more exploratory advantages. WARM follows the updatable machine learning paradigm\n[114], eliminating the need for inter-server communication, thus enabling embarrassingly simple\nparallelization [115] of RMs. This facilitates its use in federated learning scenario [116] where the data\nshould remain private; moreover, WA would add a layer of privacy and bias mitigation by reducing\nthe memorization of private preference [52]. Then, a straightforward extension of WARM would\ncombine RMs trained on different datasets, for example, coming from different (clusters of) labelers.\nThis diversity could help WARM performances, but also from a multi objective perspective [117]; by\nnon-uniform interpolation of RMs, we could learn a set of personalized policies [39]. Furthermore,\nas WA has been shown to limit catastrophic forgetting [118, 119], WARM could seamlessly support\niterative and evolving preferences. Finally, a promising research direction is extending WARM to direct\npreference optimization (DPO) strategies [120], where averaging the RMs casts back to averaging\nthe DPO policies [121].\nLimitations. WARM, while innovative, does face some limitations, notably two when compared to\nprediction ensembling methods; first, prediction ensembling can benefit from the diversity brought\nby combining RMs from various architectures and pre-trainings; second, prediction ensembling\ncan incorporate prediction disagreement into the reward to provide uncertainty estimation and\nlimit model drift. However, it\u2019s been noted in [41] that simple averaging of logits often performs\ncomparably to more complex prediction aggregation functions that include uncertainty elements.\nAnother limitation is that, while WARM effectively reduces certain types of memorization, it does\nnot completely eradicate all forms of spurious correlations or biases inherent in the preference data.\nFor instance, if each individual RM predominantly relies on summary length as a criterion, WARM\nis likely to replicate this tendency. Therefore, alternative methods (from the OOD generalization\nliterature?) might be required, for example those based on invariance regularization [51, 100] or\nlast layer retraining [122]. Finally, WARM only enhances reward modeling without tackling the other\nchallenges in RLHF [18]; thus, to mitigate the safety risks [19, 34, 35] from misalignment [32, 33],\nWARM must be considered within the larger context of responsible AI.\n7. Conclusion\nIn conclusion, we introduce Weight Averaged Reward Models (WARM) to address two critical chal-\nlenges in reward modeling: reliability under distribution shifts and robustness under label corruption.\nBy averaging the weights of multiple RMs obtained from diverse fine-tunings, WARM appears as an\nefficient solution to mitigate reward hacking in reinforcement learning from human feedback. Our\nempirical results demonstrate its effectiveness when applied to summarization. We anticipate that\nWARM will contribute to more aligned, transparent, and effective AI systems, encouraging further\nexploration in reward modeling.\n14\nWARM: On the Benefits of Weight Averaged Reward Models\nReferences\n[1] Google Gemini Team. Gemini: A family of highly capable multimodal models. 2023. (p. 1)\n[2] OpenAI. Gpt-4 technical report. 2023. (p. 1)\n[3] S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece\nKamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general\nintelligence: Early experiments with gpt-4. arXiv preprint, 2023. (p. 1)\n[4] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\nunderstanding by generative pre-training. 2018. (pp. 1 and 3)\n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. In NAACL, 2019. (p. 1)\n[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,\nJeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\nBenjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei. Language models are few-shot learners. In NeurIPS, 2020. (pp. 1, 7, and 27)\n[7] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In ICLR,\n2022. (pp. 1 and 3)\n[8] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei,\nAtharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan\nPathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson,\nKirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir\nParmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh\nPuri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A,\nSumanta Patro, Tanay Dixit, and Xudong Shen. Super-NaturalInstructions: Generalization via\ndeclarative instructions on 1600+ NLP tasks. In ACL, 2022. (p. 1)\n[9] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B Hashimoto. Stanford Alpaca: An instruction-following LLaMA model,\n2023. (p. 1)\n[10] Paul Roit, Johan Ferret, Lior Shani, Roee Aharoni, Geoffrey Cideron, Robert Dadashi, Matthieu\nGeist, Sertan Girgin, L\u00e9onard Hussenot, Orgad Keller, et al. Factually consistent summarization\nvia reinforcement learning with textual entailment feedback. In ACL, 2023. (p. 1)\n[11] Lev McKinney, Yawen Duan, David Krueger, and Adam Gleave. On the fragility of learned\nreward functions. arXiv preprint, 2023. (p. 1)\n[12] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep\nreinforcement learning from human preferences. In NeurIPS, 2017. (pp. 1, 2, 3, 5, and 27)\n[13] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul\nChristiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv\npreprint, 2019. (pp. 1 and 27)\n15\nWARM: On the Benefits of Weight Averaged Reward Models\n[14] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec\nRadford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback.\nNeurIPS, 2020. (pp. 1, 7, and 27)\n[15] Jeff Wu, Long Ouyang, Daniel M Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul\nChristiano. Recursively summarizing books with human feedback. arXiv preprint, 2021. (pp. 1\nand 27)\n[16] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher,\nCristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy\nFu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,\nSaghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel\nKloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,\nDiana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,\nIgor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,\nAlan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh\nTang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen\nZhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,\nSergey Edunov, and Thomas Scialom. LLaMA 2: Open foundation and fine-tuned chat models.\narXiv preprint, 2023. (pp. 1, 5, 12, and 27)\n[17] Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization.\nIn ICML, 2023. (pp. 1, 11, and 33)\n[18] Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, J\u00e9r\u00e9my Scheurer, Javier\nRando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, et al. Open problems\nand fundamental limitations of reinforcement learning from human feedback. TMLR, 2023.\n(pp. 1 and 14)\n[19] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man\u00e9.\nConcrete problems in AI safety. arXiv preprint, 2016. (pp. 1, 2, 3, 4, and 14)\n[20] Jack Clark and Dario Amodei. Faulty Reward Functions in the Wild. https://openai.com\n/research/faulty-reward-functions, 2016. (pp. 1 and 4)\n[21] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy\nJones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny\nHernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown,\nJack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan. A general language assistant as a\nlaboratory for alignment. arXiv preprint, 2021. (pp. 1 and 4)\n[22] Joar Max Viktor Skalse, Nikolaus H. R. Howe, Dmitrii Krasheninnikov, and David Krueger.\nDefining and characterizing reward gaming. In NeurIPS, 2022. (pp. 1 and 4)\n[23] Alexander Pan, Kush Bhatia, and Jacob Steinhardt. The effects of reward misspecification:\nMapping and mitigating misaligned models. In ICLR, 2022. (pp. 1 and 4)\n[24] Nathan Lambert and Roberto Calandra. The alignment ceiling: Objective mismatch in rein-\nforcement learning from human feedback. arXiv preprint, 2023. (p. 1)\n[25] Mike Lewis, Denis Yarats, Yann N Dauphin, Devi Parikh, and Dhruv Batra. Deal or no deal?\nend-to-end learning for negotiation dialogues. arXiv preprint, 2017. (p. 1)\n16\nWARM: On the Benefits of Weight Averaged Reward Models\n[26] Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. A long way to go: Investigating\nlength correlations in rlhf. arXiv preprint, 2023. (pp. 1 and 11)\n[27] Marilyn Strathern. Improving ratings: audit in the british university system. European Review,\n1997. (p. 1)\n[28] Ethan Perez, Sam Ringer, Kamil\u02d9e Luko\u0161i\u00afut\u02d9e, Karina Nguyen, Edwin Chen, Scott Heiner, Craig\nPettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, et al. Discovering language model\nbehaviors with model-written evaluations. arXiv preprint, 2022. (p. 1)\n[29] Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R\nBowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott R Johnston, et al. Towards\nunderstanding sycophancy in language models. arXiv preprint, 2023. (p. 1)\n[30] Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, and Tatsunori\nHashimoto. Whose opinions do language models reflect? In ICML, 2023. (p. 1)\n[31] Jochen Hartmann, Jasper Schwenzow, and Maximilian Witte. The political ideology of conver-\nsational ai: Converging evidence on chatgpt\u2019s pro-environmental, left-libertarian orientation.\narXiv preprint, 2023. (p. 1)\n[32] Jessica Taylor, Eliezer Yudkowsky, Patrick LaVictoire, and Andrew Critch. Alignment for\nadvanced machine learning systems. Ethics of AI, 2016. (pp. 1, 3, and 14)\n[33] Richard Ngo, Lawrence Chan, and Soren Mindermann. The alignment problem from a deep\nlearning perspective. arXiv preprint, 2022. (pp. 1, 3, 14, and 27)\n[34] Dan Hendrycks and Mantas Mazeika. X-risk analysis for AI research. arXiv preprint, 2022.\n(pp. 1 and 14)\n[35] Dan Hendrycks. Natural selection favors AIs over humans. arXiv preprint, 2023. (pp. 1 and 14)\n[36] Simon Zhuang and Dylan Hadfield-Menell. Consequences of misaligned AI. NeurIPS, 2020.\n(pp. 2 and 4)\n[37] Daniel Shin, Anca Dragan, and Daniel S. Brown. Benchmarks and algorithms for offline\npreference-based reward learning. TMLR, 2023. (p. 2)\n[38] Samuel R Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamile\nLukosuite, Amanda Askell, Andy Jones, Anna Chen, et al. Measuring progress on scalable\noversight for large language models. arXiv preprint, 2022. (pp. 2 and 4)\n[39] Alexandre Ram\u00e9, Guillaume Couairon, Mustafa Shukor, Corentin Dancette, Jean-Baptiste\nGaya, Laure Soulier, and Matthieu Cord. Rewarded soups: towards pareto-optimal alignment\nby interpolating weights fine-tuned on diverse rewards. In NeurIPS, 2023. (pp. 2, 14, and 26)\n[40] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models\nto follow instructions with human feedback. NeurIPS, 2022. (pp. 2, 3, and 27)\n[41] Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex D\u2019Amour, DJ Dvijotham,\nAdam Fisch, Katherine Heller, Stephen Pfohl, Deepak Ramachandran, et al. Helping or herding?\nreward model ensembles mitigate but do not eliminate reward hacking. arXiv preprint, 2023.\n(pp. 2, 4, 5, 11, 12, 14, and 27)\n17\nWARM: On the Benefits of Weight Averaged Reward Models\n[42] Thomas Coste, Usman Anwar, Robert Kirk, and David Krueger. Reward model ensembles help\nmitigate overoptimization. arXiv preprint, 2023. (pp. 2, 5, and 27)\n[43] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell.\nSimple and scalable\npredictive uncertainty estimation using deep ensembles. In NeurIPS, 2017. (pp. 2 and 5)\n[44] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M. Roy, and Michael Carbin. Linear\nmode connectivity and the lottery ticket hypothesis. In ICML, 2020. (pp. 3, 6, and 26)\n[45] Behnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang. What is being transferred in transfer\nlearning? In NeurIPS, 2020. (pp. 3, 6, and 26)\n[46] Mitchell Wortsman, Gabriel Ilharco, Samir Yitzhak Gadre, Rebecca Roelofs, Raphael Gontijo-\nLopes, Ari S. Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and\nLudwig Schmidt. Model soups: averaging weights of multiple fine-tuned models improves\naccuracy without increasing inference time. In ICML, 2022. (pp. 3, 6, 7, 9, 26, and 28)\n[47] Alexandre Ram\u00e9, Matthieu Kirchmeyer, Thibaud Rahier, Alain Rakotomamonjy, Patrick Galli-\nnari, and Matthieu Cord. Diverse weight averaging for out-of-distribution generalization. In\nNeurIPS, 2022. (pp. 3, 5, 6, 7, 8, 9, 26, and 28)\n[48] Alexandre Ram\u00e9, Kartik Ahuja, Jianyu Zhang, Matthieu Cord, L\u00e9on Bottou, and David Lopez-\nPaz. Model Ratatouille: Recycling diverse models for out-of-distribution generalization. In\nICML, 2023. (pp. 3, 6, 7, 26, and 28)\n[49] Junbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung Lee,\nand Sungrae Park. SWAD: Domain generalization by seeking flat minima. In NeurIPS, 2021.\n(pp. 3 and 26)\n[50] Krikamol Muandet, David Balduzzi, and Bernhard Sch\u00f6lkopf. Domain generalization via\ninvariant feature representation. In ICML, 2013. (pp. 3 and 10)\n[51] Martin Arjovsky, L\u00e9on Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimiza-\ntion. arXiv preprint, 2019. (pp. 3, 4, 10, and 14)\n[52] Kerem Zaman, Leshem Choshen, and Shashank Srivastava. Fuse to forget: Bias reduction and\nselective memorization through model fusion. arXiv preprint, 2023. (pp. 3, 14, and 26)\n[53] Yong Lin, Lu Tan, Yifan Hao, Honam Wong, Hanze Dong, Weizhong Zhang, Yujiu Yang, and\nTong Zhang. Spurious feature diversification improves out-of-distribution generalization. In\nICLR, 2024. (pp. 3, 9, and 26)\n[54] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. (pp. 3 and 6)\n[55] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von\nArx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the\nopportunities and risks of foundation models. arXiv preprint, 2021. (pp. 3 and 26)\n[56] Maxime Oquab, Leon Bottou, Ivan Laptev, and Josef Sivic. Learning and transferring mid-level\nimage representations using convolutional neural networks. In CVPR, 2014. (p. 3)\n[57] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,\nAnna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional AI:\nHarmlessness from AI feedback. arXiv preprint, 2022. (pp. 4 and 11)\n18\nWARM: On the Benefits of Weight Averaged Reward Models\n[58] Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu,\nColton Bishop, Victor Carbune, and Abhinav Rastogi. RLAIF: Scaling reinforcement learning\nfrom human feedback with ai feedback. arXiv preprint, 2023. (pp. 4, 7, 11, 12, 13, 27, and 28)\n[59] Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the\nmethod of paired comparisons. Biometrika, 1952. (p. 4)\n[60] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforce-\nment learning. Reinforcement learning, 1992. (pp. 4, 12, and 28)\n[61] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms. arXiv preprint, 2017. (pp. 4 and 12)\n[62] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In ICLR, 2021.\n(p. 4)\n[63] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay\nBalsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee,\nEtienne David, Ian Stavness, Wei Guo, Berton Earnshaw, Imran Haque, Sara M Beery, Jure\nLeskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang.\nWILDS: A benchmark of in-the-wild distribution shifts. In ICML, 2021. (p. 4)\n[64] Mohammad Pezeshki, S\u00e9kou-Oumar Kaba, Yoshua Bengio, Aaron Courville, Doina Precup, and\nGuillaume Lajoie. Gradient starvation: A learning proclivity in neural networks. In NeurIPS,\n2020. (p. 4)\n[65] Firas Laakom, Jenni Raitoharju, Alexandros Iosifidis, and Moncef Gabbouj. Learning distinct\nfeatures helps, provably. arXiv preprint, 2021. (p. 4)\n[66] Niv Nayman, Avram Golbert, Asaf Noy, Tan Ping, and Lihi Zelnik-Manor. Diverse ImageNet\nmodels transfer better. arXiv preprint, 2022. (p. 4)\n[67] Alexander D\u2019Amour, Katherine Heller, Dan Moldovan, Ben Adlam, Babak Alipanahi, Alex Beutel,\nChristina Chen, Jonathan Deaton, Jacob Eisenstein, Matthew D Hoffman, et al. Underspeci-\nfication presents challenges for credibility in modern machine learning. JMLR, 2020. (pp. 4\nand 26)\n[68] Damien Teney, Yong Lin, Seong Joon Oh, and Ehsan Abbasnejad. ID and OOD performance\nare sometimes inversely correlated on real-world datasets. In NeurIPS Workshop, 2023. (p. 4)\n[69] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural\nnetworks. In ICML, 2017. (p. 4)\n[70] Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua\nDillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model\u2019s uncertainty?\nevaluating predictive uncertainty under dataset shift. In NeurIPS, 2019. (p. 4)\n[71] Yoav Wald, Amir Feder, Daniel Greenfeld, and Uri Shalit. On calibration and out-of-domain\ngeneralization. In NeurIPS, 2021. (p. 4)\n[72] Herbert A Simon. Bounded rationality. Utility and probability, 1990. (p. 4)\n[73] Rohin Shah, Noah Gundotra, Pieter Abbeel, and Anca Dragan. On the feasibility of learning,\nrather than assuming, human biases for reward inference. In ICML, 2019. (p. 4)\n19\nWARM: On the Benefits of Weight Averaged Reward Models\n[74] Timo Kaufmann, Sarah Ball, Jacob Beck, Eyke H\u00fcllermeier, and Frauke Kreuter. On the\nchallenges and practices of reinforcement learning from real human feedback. 2023. (p. 4)\n[75] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath,\nBen Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. Red teaming language models\nto reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint, 2022. (p. 4)\n[76] Robert Irvine, Douglas Boubert, Vyas Raina, Adian Liusie, Vineet Mudupalli, Aliaksei Korshuk,\nZongyi Liu, Fritz Cremer, Valentin Assassi, Christie-Carol Beauchamp, et al. Rewarding chatbots\nfor real-world engagement with millions of users. arXiv preprint, 2023. (p. 4)\n[77] Condorcet. Essai sur l\u2019application de l\u2019analyse \u00e0 la probabilit\u00e9 des d\u00e9cisions rendues \u00e0 la\npluralit\u00e9 des voix. 1785. (p. 4)\n[78] Silviu Pitis. Failure modes of learning reward models for llms and other sequence models. In\nICML, 2023. (p. 4)\n[79] Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. Quantifying language models\u2019\nsensitivity to spurious features in prompt design or: How i learned to start worrying about\nprompt formatting. arXiv preprint, 2023. (p. 4)\n[80] Moran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna Shahaf, and Gabriel Stanovsky.\nState of what art? a call for multi-prompt llm evaluation. arXiv preprint, 2023. (p. 4)\n[81] Natasha Jaques, Shixiang Gu, Dzmitry Bahdanau, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Richard E\nTurner, and Douglas Eck. Sequence tutor: Conservative fine-tuning of sequence generation\nmodels with kl-control. In ICML, 2017. (pp. 5 and 12)\n[82] Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized markov decision\nprocesses. In ICML, 2019. (pp. 5 and 12)\n[83] Angeliki Lazaridou, Anna Potapenko, and Olivier Tieleman. Multi-agent communication meets\nnatural language: Synergies between functional and structural language learning. In ACL,\n2020. (p. 5)\n[84] Yuchen Lu, Soumye Singhal, Florian Strub, Aaron Courville, and Olivier Pietquin. Countering\nlanguage drift with seeded iterated learning. In ICML, 2020. (p. 5)\n[85] Siddharth Reddy, Anca Dragan, Sergey Levine, Shane Legg, and Jan Leike. Learning human\nobjectives by evaluating hypothetical behavior. In ICML, 2020. (pp. 5 and 27)\n[86] William Saunders, Girish Sastry, Andreas Stuhlm\u00fcller, and Owain Evans. Trial without error:\nTowards safe reinforcement learning via human intervention. In AAMAS, 2018. (p. 5)\n[87] Binghai Wang et al. Secrets of rlhf in large language models part ii: Reward modeling. arXiv\npreprint, 2023. (pp. 5, 10, and 27)\n[88] Ron Kohavi, David H Wolpert, et al. Bias plus variance decomposition for zero-one loss\nfunctions. In ICML, 1996. (p. 5)\n[89] Naonori Ueda and Ryohei Nakano. Generalization error of ensemble estimators. In ICNN,\n1996. (p. 5)\n[90] Sandipan Kundu, Yuntao Bai, Saurav Kadavath, Amanda Askell, Andrew Callahan, Anna Chen,\nAnna Goldie, Avital Balwit, Azalia Mirhoseini, Brayden McLean, et al. Specific versus general\nprinciples for constitutional ai. arXiv preprint, 2023. (p. 5)\n20\nWARM: On the Benefits of Weight Averaged Reward Models\n[91] Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang.\nFine-tuning can distort pretrained features and underperform out-of-distribution. In ICLR,\n2022. (pp. 5, 6, 7, and 28)\n[92] Samuel K. Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. Git re-basin: Merging\nmodels modulo permutation symmetries. In ICLR, 2022. (p. 6)\n[93] Almog Gueta, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, and Leshem Choshen.\nKnowledge is a region in weight space for fine-tuned language models. In EMNLP, 2023. (p. 6)\n[94] Raphael Gontijo-Lopes, Yann Dauphin, and Ekin Dogus Cubuk. No one representation to rule\nthem all: Overlapping features of training methods. In ICLR, 2022. (pp. 6 and 8)\n[95] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson.\nAveraging weights leads to wider optima and better generalization. In UAI, 2018. (pp. 7 and 26)\n[96] Devansh Arpit, Huan Wang, Yingbo Zhou, and Caiming Xiong. Ensemble of averages: Improving\nmodel selection and boosting performance in domain generalization. In NeurIPS, 2021. (pp. 7\nand 26)\n[97] Michael V\u00f6lske, Martin Potthast, Shahbaz Syed, and Benno Stein. Tl; dr: Mining reddit to\nlearn automatic summarization. In ACL Workshop, 2017. (pp. 7, 8, and 11)\n[98] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. PaLM 2 technical report.\narXiv preprint, 2023. (pp. 7, 11, 12, 27, 28, and 30)\n[99] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny\nZhou. Chain-of-Thought prompting elicits reasoning in large language models. In NeurIPS,\n2022. (pp. 7, 11, and 27)\n[100] Alexandre Ram\u00e9, Corentin Dancette, and Matthieu Cord. Fishr: Invariant gradient variances\nfor out-of-distribution generalization. In ICML, 2022. (pp. 10 and 14)\n[101] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Good-\nfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint, 2013. (p. 10)\n[102] Yao-Yuan Yang, Cyrus Rashtchian, Hongyang Zhang, Ruslan Salakhutdinov, and Kamalika\nChaudhuri. Adversarial robustness through local lipschitzness. arXiv preprint, 2020. (p. 10)\n[103] Mihaela Rosca, Theophane Weber, Arthur Gretton, and Shakir Mohamed. A case for new\nneural network smoothness constraints. In NeurIPS ICBINB, 2020. (p. 10)\n[104] Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness of a classifier\nagainst adversarial manipulation. NeurIPS, 2017. (p. 10)\n[105] Jure Sokoli\u0107, Raja Giryes, Guillermo Sapiro, and Miguel RD Rodrigues. Robust large margin\ndeep neural networks. IEEE Transactions on Signal Processing, 2017. (p. 10)\n[106] Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized\nsmoothing. In ICML, 2019. (p. 10)\n[107] Roland Hafner and Martin Riedmiller. Reinforcement learning in feedback control: Challenges\nand benchmarks from technical process control. Machine learning, 2011. (p. 10)\n21\nWARM: On the Benefits of Weight Averaged Reward Models\n[108] Matteo Pirotta, Marcello Restelli, and Luca Bascetta. Policy gradient in lipschitz markov\ndecision processes. Machine Learning, 2015. (p. 10)\n[109] Lionel Blond\u00e9, Pablo Strasser, and Alexandros Kalousis. Lipschitzness is all you need to tame\noff-policy generative adversarial imitation learning. Machine Learning, 2022. (p. 10)\n[110] Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos\nGuestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for\nmethods that learn from human feedback. arXiv preprint, 2023. (p. 11)\n[111] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. JMLR, 2020. (pp. 11, 12, 30, and 33)\n[112] Jacob Hilton. KL divergence of max-of-n, 2023. (p. 12)\n[113] Ahmad Beirami, Alekh Agarwal, Jonathan Berant, Alexander D\u2019Amour, Jacob Eisenstein, Chirag\nNagpal, and Ananda Theertha Suresh. Theoretical guarantees on the best-of-n alignment\npolicy. arXiv preprint, 2024. (p. 12)\n[114] Colin Raffel. Building Machine Learning Models Like Open Source Software. ACM, 2023.\n(p. 14)\n[115] Margaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah A Smith, and\nLuke Zettlemoyer. Branch-Train-Merge: Embarrassingly parallel training of expert language\nmodels. arXiv preprint, 2022. (p. 14)\n[116] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.\nCommunication-efficient learning of deep networks from decentralized data. In AISTATS, 2017.\n(p. 14)\n[117] Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A.\nSmith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better\nrewards for language model training. In NeuriPS, 2023. (p. 14)\n[118] Zafir Stojanovski, Karsten Roth, and Zeynep Akata. Momentum-based weight interpolation of\nstrong zero-shot models for continual learning. In NeurIPS Workshop, 2022. (pp. 14 and 26)\n[119] Steven Vander Eeckt et al. Weight averaging: A simple yet effective method to overcome\ncatastrophic forgetting in automatic speech recognition. arXiv preprint, 2022. (p. 14)\n[120] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and\nChelsea Finn. Direct preference optimization: Your language model is secretly a reward model.\narXiv preprint, 2023. (pp. 14 and 27)\n[121] Maxime Labonne. NeuralBeagle14-7B. https://huggingface.co/mlabonne/NeuralBe\nagle14-7B-GGUF, 2024. (pp. 14 and 27)\n[122] Polina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson. Last layer re-training is\nsufficient for robustness to spurious correlations. In ICLR, 2023. (p. 14)\n[123] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common\ncorruptions and perturbations. In ICLR, 2019. (p. 26)\n22\nWARM: On the Benefits of Weight Averaged Reward Models\n[124] John R. Zech, Marcus A. Badgeley, Manway Liu, Anthony B. Costa, Joseph J. Titano, and\nEric Karl Oermann. Variable generalization performance of a deep learning model to detect\npneumonia in chest radiographs: A cross-sectional study. PLOS Medicine, 2018. (p. 26)\n[125] Alex J DeGrave, Joseph D Janizek, and Su-In Lee. AI for radiographic COVID-19 detection\nselects shortcuts over signal. Nature Machine Intelligence, 2021. (p. 26)\n[126] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Hanna Hajishirzi, Ali Farhadi,\nHongseok Namkoong, and Ludwig Schmidt. Robust fine-tuning of zero-shot models. In CVPR,\n2022. (p. 26)\n[127] Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi,\nSimon Kornblith, Ali Farhadi, and Ludwig Schmidt. Patching open-vocabulary models by\ninterpolating weights. In NeurIPS, 2022. (p. 26)\n[128] Shachar Don-Yehiya, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, and Leshem\nChoshen. ColD fusion: Collaborative descent for distributed multitask finetuning. In ACL,\n2023. (p. 26)\n[129] Nikolaos Dimitriadis, Pascal Frossard, and Fran\u00e7ois Fleuret. Pareto manifold learning: Tackling\nmultiple tasks via ensembles of single-task models. arXiv preprint, 2022. (Not cited.)\n[130] Mustafa Shukor, Corentin Dancette, Alexandre Ram\u00e9, and Matthieu Cord. Unival: Unified\nmodel for image, video, audio and language. TMLR, 2023. (p. 26)\n[131] Francesco Croce, Sylvestre-Alvise Rebuffi, Evan Shelhamer, and Sven Gowal. Seasoning model\nsoups for robustness to adversarial and natural distribution shifts. In CVPR, 2023. (p. 26)\n[132] Jeevesh Juneja, Rachit Bansal, Kyunghyun Cho, Jo\u00e3o Sedoc, and Naomi Saphra. Linear\nconnectivity reveals generalization strategies. In ICLR, 2023. (p. 26)\n[133] Evgenii Nikishin, Pavel Izmailov, Ben Athiwaratkun, Dmitrii Podoprikhin, Timur Garipov,\nPavel Shvechikov, Dmitry Vetrov, and Andrew Gordon Wilson. Improving stability in deep\nreinforcement learning with weight averaging. 2018. (p. 26)\n[134] Jean-Baptiste Gaya, Laure Soulier, and Ludovic Denoyer. Learning a subspace of policies for\nonline adaptation in reinforcement learning. In ICLR, 2022. (p. 26)\n[135] Daniel Lawson and Ahmed H Qureshi. Merging decision transformers: Weight averaging for\nforming multi-task policies. In ICLR RRL Workshop, 2023. (p. 26)\n[136] Michael Noukhovitch, Samuel Lavoie, Florian Strub, and Aaron Courville. Language model\nalignment with elastic reset. In NeurIPS, 2023. (p. 26)\n[137] Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt,\nHannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. In ICLR, 2023.\n(p. 26)\n[138] Nico Daheim, Nouha Dziri, Mrinmaya Sachan, Iryna Gurevych, and Edoardo M Ponti. Elastic\nweight removal for faithful and abstractive dialogue generation. arXiv preprint, 2023. (p. 26)\n[139] Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee. Learning from noisy\nlabels with deep neural networks: A survey. TNNLS, 2022. (p. 26)\n[140] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding\ndeep learning requires rethinking generalization. ICLR, 2017. (p. 26)\n23\nWARM: On the Benefits of Weight Averaged Reward Models\n[141] Ryutaro Tanno, Ardavan Saeedi, Swami Sankaranarayanan, Daniel C Alexander, and Nathan\nSilberman. Learning from noisy labels by regularized estimation of annotator confusion. In\nCVPR, 2019. (p. 26)\n[142] Neel Jain, Ping-yeh Chiang, Yuxin Wen, John Kirchenbauer, Hong-Min Chu, Gowthami\nSomepalli, Brian R Bartoldson, Bhavya Kailkhura, Avi Schwarzschild, Aniruddha Saha, et al.\nNeftune: Noisy embeddings improve instruction finetuning. arXiv preprint, 2023. (p. 26)\n[143] Aritra Ghosh, Himanshu Kumar, and P Shanti Sastry. Robust loss functions under label noise\nfor deep neural networks. In AAAI, 2017. (p. 26)\n[144] Xiaobo Xia, Tongliang Liu, Bo Han, Mingming Gong, Jun Yu, Gang Niu, and Masashi Sugiyama.\nSample selection with uncertainty of losses for learning with noisy labels. In ICLR, 2022. (p. 26)\n[145] Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning\ndata-driven curriculum for very deep neural networks on corrupted labels. In ICML, 2018.\n(p. 26)\n[146] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi\nSugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels.\nNeurIPS, 2018. (p. 26)\n[147] Maryam Sabzevari. Ensemble learning in the presence of noise. PhD thesis, Universidad Aut\u00f3noma\nde Madrid, 2019. (p. 26)\n[148] Andrew Y Ng, Stuart Russell, et al. Algorithms for inverse reinforcement learning. In ICML,\n2000. (p. 27)\n[149] W Bradley Knox, Stephane Hatgis-Kessell, Sigurdur Orn Adalgeirsson, Serena Booth, Anca\nDragan, Peter Stone, and Scott Niekum. Learning optimal advantage from preferences and\nmistaking it for reward. arXiv preprint, 2023. (p. 27)\n[150] Peter Barnett, Rachel Freedman, Justin Svegliato, and Stuart Russell. Active reward learning\nfrom multiple teachers. arXiv preprint, 2023. (p. 27)\n[151] Sian Gooding and Hassan Mansoor. The impact of preference agreement in reinforcement\nlearning from human feedback: A case study in summarization. arXiv preprint, 2023. (p. 27)\n[152] Lei Li, Yekun Chai, Shuohuan Wang, Yu Sun, Hao Tian, Ningyu Zhang, and Hua Wu. Tool-\naugmented reward modeling. In ICLR, 2023. (p. 27)\n[153] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang\nGan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models\nwith factually augmented rlhf. arXiv preprint, 2023. (p. 27)\n[154] Anonymous. RIME: Robust preference-based reinforcement learning with noisy human prefer-\nences. In Submitted to ICLR, 2023. (p. 27)\n[155] Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello,\nMichal Valko, and R\u00e9mi Munos. A general theoretical paradigm to understand learning from\nhuman preferences. arXiv preprint, 2023. (p. 27)\n[156] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory\ncost. In ICML, 2018. (pp. 27 and 28)\n24\nWARM: On the Benefits of Weight Averaged Reward Models\n[157] Noam Razin, Hattie Zhou, Omid Saremi, Vimal Thilak, Arwen Bradley, Preetum Nakkiran,\nJoshua Susskind, and Etai Littwin. Vanishing gradients in reinforcement finetuning of language\nmodels. arXiv preprint, 2023. (p. 28)\n25\nWARM: On the Benefits of Weight Averaged Reward Models\nWARM: On the Benefits of Weight Averaged Reward Models\nSupplementary material\nThis supplementary material is organized as follows:\n\u2022 Appendix A enriches our related work.\n\u2022 Appendix B clarifies some experimental details.\n\u2022 Appendix C enriches our experiments.\nA. Related work\nThis paper leverages the insights from the OOD generalization literature, in particular from linear\nmode connectivity (see Appendix A.1), and applies them to the design of efficient, reliable and robust\nreward models (see Appendix A.2).\nA.1. Out-of-distribution generalization, linear mode connectivity and memorization\nLMC in fine-tuning. Fine-tuning foundation models [55] into specialized models that generalize\nwell to new distributions is critical for many real-world applications [123, 124, 125]. Recently,\ndifferent variants of weight averaging (WA) were able to improve performance, such as moving\naverage [49, 95, 96], WiSE fine-tuning [126], model soups [46], DiWA [47] and model ratatouille\n[48]. These works rely on the LMC [44, 45] across fine-tuned weights, which was extended to fine-\ntunings on different tasks [48, 127, 128], modalities [130] or with different losses [47, 131], although\n[132] highlighted some limitations. WA was also used recently in RL setups [39, 133, 134, 135, 136],\nin particular in RLHF in [39, 136] but only to combine policies and not rewards.\nInsights into WA. Specifically, WA comes with several benefits. First, WA flattens the loss landscape\n[49]. Second, WA approximates prediction ensembling, thus reduces variance of the estimator\n[46, 47] and tackles model misspecification [67]. Third, WA combines models\u2019 abilities [137, 138],\nwhich can be useful for multi-task [127], multi-objective [39] or in continual learning [118] setups.\nLastly, it has recently been shown that WA can provide some benefits under spurious correlations\n[52, 53], with a phenomenon called FalseFalseTrue in [53]. These works [52, 53] share similarities\nwith our memorization experiments from Section 4.2, but we are the first to analyze WA regularization\nproperties under label corruption, and their consequences on generalization. In contrast, in [52] the\nnetworks are trained on different datasets while the theory in [53] is actually mostly developed for\nprediction ensembling.\nMemorization. Traditional approaches [139] tackling memorization of corrupted labels [140] usually\nrequire explicit regularization [141], specific data augmentation [142], loss adjustment [143] or\nsample selection [144]. Some other strategies are based on ensembling: they filter out potentially\ncorrupted samples with self-labeling filtering [145, 146] or bagging diversity procedures [147]. As\nfar as we know, with WA we propose the first strategy combining multiple models trained on the\nsame dataset that manages to tackle corruption.\n26\nWARM: On the Benefits of Weight Averaged Reward Models\nA.2. Reward modeling\nOne of the central challenge in aligning LLMs is the absence of explicit rewards from the environment,\na.k.a. the outer alignment challenge [33]. While Inverse Reinforcement Learning [148] attempts\nto derive a reward model (RM) from expert demonstrations, most recent efforts [12, 13, 14, 15,\n40] primarily focus on learning from human preferences. Despite its importance to enhance LLM\nperformances post-RL and for safe deployment in real-world applications, how to best design RMs\nhas arguably receive less attention than it warrants. Some research [149] seeks to refine the loss\nfunction from Equation (1). Other approaches are more data oriented: for example, LLaMA-2 [16]\ninvolves continual learning of the RM to adjust to new generation distributions; [85, 150] follow an\nactive learning paradigm [151]. Augmenting rewards with tools [152] or additional information\n[153] represents an even more recent and very promising trend. Limited efforts have been made at\nthe intersection of label corruption and reward modeling; [154] tried to filter the preference dataset\nfor small academic locomotion tasks, while the concurrent [87] suggests applying label smoothing\nand flipping. Actually, reward ensembling is the most discussed method to mitigate reward hacking\n[41, 42]; we show that WARM can beat ENS while removing its overheads. Finally, following DPO\n[120], a recent trend merges reward modeling with policy learning; though, the policies still tend to\nhack the preference data [155], and thus require only a few training steps and very small learning\nrates. The WA of DPO policies, theoretically equivalent to the WA of RMs, is a promising research\ndirection with already significant empirical results on public benchmarks, as demonstrated in [121].\nB. Implementation details\nB.1. Dataset details\nFor summarization, we use the Reddit TL;DR dataset [14], containing posts from Reddit that have\nbeen filtered to ensure high quality. The training summaries from [14] are generated by OpenAI\nGPT-3 [6] variants. The dataset contains 123k posts, and \u223c5% is held out as the ID validation set.\nTo generate the candidate responses in the OOD dataset D\ud835\udc5c\ud835\udc5c\ud835\udc51 with 92k pairwise comparisons, we\nconsidered multiple PaLM-XS policies with high temperature, some of which are pre-trained only,\nothers SFT-ed and others RLHF-ed; the goal was to get a diverse set of summaries.\nB.2. AI labeling details\nWhile the ideal approach for evaluating our models would involve human preferences, we resort to\nthe cheaper AI labeling procedure from RLAIF [58]. We query an instruct fine-tuned PaLM-L [98]\nLLM1, prompted to generate preference mimicking human preferences. Specifically, we follow the\n\u201cDetailed + CoT 0-shot\u201d prompting strategy from RLAIF [58], the best one according to their results,\ninvolving zero-shot prompting with chain-of-thought [99], a maximum decoding length of 512 tokens\nand temperature \ud835\udc47 = 0.0 (i.e., greedy decoding). To avoid position bias, we run the AI labeler in the\ntwo possible orderings. This strategy was shown to perform similarly to human labellers, with similar\ninter-agreement. For the corruption experiments, we swap the labels for 25% of the training samples.\nB.3. Reward modeling details\nThe RMs are PaLM-XXS models [98]. They are first pre-trained, and then supervised fine-tuned on\nthe Reddit TL;DR dataset for 12k steps with a batch size of 128 and the Adafactor [156] optimizer\n1Available through Google Cloud\u2019s Vertex AI https://cloud.google.com/vertex-ai/docs/generative-ai/\nlearn/models.\n27\nWARM: On the Benefits of Weight Averaged Reward Models\nwith a learning rate of 10\u22125. Following the Baklava recipe, we actually launch the reward modeling\nfrom different checkpoints along this SFT fine-tuning, at steps {8k, 10k, 12k}; taking a too-early\ncheckpoint would drastically reduce RM accuracy, as observed in [157]. To convert this LLM into a\nclassifier, we plug a linear probed [91] classification layer (the same for all RMs); said differently,\neven though the featurizers are actually from different SFT checkpoints, they share the same linear\nprobed classification linear layer. As explained in [91], it prevents features from moving too much\naway from their initializations, which facilitates the LMC required for WA.\nWe train all RMs for 10k steps, a batch size of 128, the Adafactor [156] optimizer, a learning rate\nsampled in {1e-5,4e-5,1e-4}, and a dropout probability in {0.05, 0.1}. This follows the practical\nrecommandations from [47] to leverage hyperparameters in a mild range to preserve the LMC.\nTraining for a longer number of steps could help, as it did not alter the LMC in previous works [48].\nIn practice, for the main experiments with clean labels, we launch 10 reward modelings; when ranked\nin decreasing accuracy on D\ud835\udc5c\ud835\udc5c\ud835\udc51, we denote them {\ud835\udf19\ud835\udc56}10\n\ud835\udc56=1. Therefore, the RMs named \ud835\udf191 and \ud835\udf192 in the\ndifferent plots are the two best according to their individual performances under distribution shifts.\nThen, WARM \ud835\udc40 = 2 is actually the RM defined per \ud835\udf191+\ud835\udf192\n2\n, while ENS \ud835\udc40 = 2 averages their predictions.\nMore generally, WARM with \ud835\udc40 weights is the WA of the \ud835\udc40 best weights {\ud835\udf19\ud835\udc56}\ud835\udc40\n\ud835\udc56=1. The main motivation\nof this weight selection procedure is to remove potentially bad RMs, as validated in Figure 10, in\nwhich we consider different permutations across those 10 RMs. As a side note, we speculate that a\ngreedy procedure as in [46] could further improve performances.\n2\n4\n6\n8\n10\nM\n0.752\n0.754\n0.756\n0.758\n0.760\n0.762\n0.764\n0.766\n0.768\nOOD Acc.\nFrom best to worst\nFirst best and then random\nRandom permutation v1\nRandom permutation v2\nFrom worst to best\nFigure 10 | Analysis of the weight selection procedure. We plot the accuracy resulting from averaging \ud835\udc40\nweights (out of 10), where these weights are chosen based on various selection procedures. This effectively\nvalidates that choosing models from best to worst serves as a reliable heuristic.\nB.4. Reinforcement learning details\nBoth policy and value models are PaLM-XS [98], initialized from the same SFT model. We then\ngenerate samples from the policy with temperature \ud835\udc47 = 0.9, batch size of 128, the Adafactor [156]\noptimizer, a learning rate of 10\u22125 and a policy warmup of 2k steps. We set \ud835\udefc = 0.003 for the KL\nregularization in the main experiment without label corruption, and \ud835\udefc = 0.01 with label corruption.\nFollowing [58], we used a modified version of REINFORCE [60] with a baseline value function for\nvariance reduction.\n28\nWARM: On the Benefits of Weight Averaged Reward Models\nC. Additional experiments\nC.1. 2nd order analysis: weight averaging for more robust ensembling\n0.0\n0.5\n1.0\n0.43\n0.44\n0.45\n0.46\nAcc.\nWA\nENS\nDiag\n0.0\n0.5\n1.0\n0.46\n0.47\n0.48\n0.49\n0.50\n0.0\n0.5\n1.0\n0.34\n0.36\n0.38\n0.40\n0.0\n0.5\n1.0\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\n0.50\n0.0\n0.5\n1.0\n0.450\n0.475\n0.500\n0.525\n0.550\n0.575\nAcc.\n0.0\n0.5\n1.0\n0.320\n0.325\n0.330\n0.335\n0.0\n0.5\n1.0\n0.370\n0.375\n0.380\n0.385\n0.390\n0.395\n0.0\n0.5\n1.0\n0.43\n0.44\n0.45\n0.46\nFigure 11 | Train (corrupt). More results enriching Figure 4(a) with different pairs of RMs.\n0.0\n0.5\n1.0\n0.886\n0.888\n0.890\n0.892\n0.894\nAcc.\nWA\nENS\nDiag\n0.0\n0.5\n1.0\n0.890\n0.892\n0.894\n0.896\n0.0\n0.5\n1.0\n0.835\n0.840\n0.845\n0.850\n0.855\n0.0\n0.5\n1.0\n0.8800\n0.8825\n0.8850\n0.8875\n0.8900\n0.8925\n0.8950\n0.0\n0.5\n1.0\n0.890\n0.895\n0.900\n0.905\nAcc.\n0.0\n0.5\n1.0\n0.8425\n0.8450\n0.8475\n0.8500\n0.8525\n0.8550\n0.8575\n0.0\n0.5\n1.0\n0.878\n0.880\n0.882\n0.884\n0.886\n0.888\n0.0\n0.5\n1.0\n0.888\n0.890\n0.892\n0.894\n0.896\nFigure 12 | Train (clean). More results enriching Figure 4(b) with different pairs of RMs.\n0.0\n0.5\n1.0\n0.7725\n0.7750\n0.7775\n0.7800\n0.7825\n0.7850\nAcc.\nWA\nENS\nDiag\n0.0\n0.5\n1.0\n0.760\n0.765\n0.770\n0.775\n0.0\n0.5\n1.0\n0.74\n0.75\n0.76\n0.77\n0.0\n0.5\n1.0\n0.74\n0.75\n0.76\n0.77\n0.78\n0.0\n0.5\n1.0\n0.74\n0.75\n0.76\n0.77\n0.78\nAcc.\n0.0\n0.5\n1.0\n0.770\n0.772\n0.774\n0.776\n0.778\n0.780\n0.0\n0.5\n1.0\n0.780\n0.785\n0.790\n0.0\n0.5\n1.0\n0.7700\n0.7725\n0.7750\n0.7775\n0.7800\n0.7825\n0.7850\nFigure 13 | Validation (ID). More results enriching Figure 4(c) with different pairs of RMs.\n0.0\n0.5\n1.0\n0.710\n0.712\n0.714\n0.716\nAcc.\nWA\nENS\nDiag\n0.0\n0.5\n1.0\n0.704\n0.706\n0.708\n0.710\n0.712\n0.0\n0.5\n1.0\n0.700\n0.705\n0.710\n0.715\n0.720\n0.725\n0.0\n0.5\n1.0\n0.695\n0.700\n0.705\n0.710\n0.715\n0.720\n0.725\n0.0\n0.5\n1.0\n0.690\n0.695\n0.700\n0.705\n0.710\n0.715\nAcc.\n0.0\n0.5\n1.0\n0.718\n0.720\n0.722\n0.724\n0.726\n0.0\n0.5\n1.0\n0.718\n0.720\n0.722\n0.724\n0.726\n0.0\n0.5\n1.0\n0.708\n0.710\n0.712\n0.714\n0.716\n0.718\nFigure 14 | Test (OOD). More results enriching Figure 4(d) with different pairs of RMs.\n29\nWARM: On the Benefits of Weight Averaged Reward Models\nC.2. BoN experiments\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nKL : log(N)\nN\n1\nN\n4.00\n4.25\n4.50\n4.75\n5.00\n5.25\n5.50\n5.75\nControl reward\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\n(a) PaLM (clean).\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nKL : log(N)\nN\n1\nN\n3.0\n3.5\n4.0\n4.5\n5.0\nControl reward\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\n(b) PaLM (corrupt).\n0\n1\n2\n3\n4\n5\n6\nKL : log(N)\nN\n1\nN\n3\n4\n5\n6\n7\nControl reward\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\n(c) T5 (clean).\n0\n1\n2\n3\n4\n5\n6\nKL : log(N)\nN\n1\nN\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\nControl reward\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\n(d) T5 (corrupt).\nFigure 15 | Same as Figure 8, but with absolute values of the control reward for BoN experiments. We\nconsider two SFT policies to generate candidate summaries: one based on PaLM architecture [98], the other\non T5 architecture [111]. In both cases, we observe that WARM performs better than ENS and the individual\nnetworks in terms of pointwise control RM.\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nKL : log(N)\nN\n1\nN\n0.00\n0.02\n0.04\n0.06\nControl reward gain\nWARM Baklava M = 2\nENS Baklava M = 2\nInd \n3 Baklava\nInd \n1\n(a) Baklava with PaLM.\n0\n1\n2\n3\n4\n5\n6\nKL : log(N)\nN\n1\nN\n0.00\n0.05\n0.10\n0.15\nControl reward gain\nWARM Baklava M = 2\nENS Baklava M = 2\nInd \n3 Baklava\nInd \n1\n(b) Baklava with T5.\nFigure 16 | Control reward for BoN experiments (clean setup) with Baklava when the two fine-tunings \ud835\udf191\nand \ud835\udf193 have different featurizer initializations, collected respectively at steps 12k and 8k from a shared SFT.\n30\nWARM: On the Benefits of Weight Averaged Reward Models\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nKL : log(N)\nN\n1\nN\n0.38\n0.40\n0.42\n0.44\n0.46\n0.48\n0.50\nWin ratio vs. WARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\ny=0.5\n(a) PaLM.\n0\n1\n2\n3\n4\n5\n6\nKL : log(N)\nN\n1\nN\n0.1\n0.2\n0.3\n0.4\n0.5\nWin ratio vs. WARM M = 6 with N = 1000\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\ny=0.5\n(b) T5 vs. WARM w/ \ud835\udc41 = 1000.\nFigure 17 | Oracle preference metric for BoN experiments (clean setup). Figure 17(a) confirms Figure 7(c)\nbut on generations from PaLM SFT. Figure 17(b) shows win rates for BoN on T5 generations for WARM with\n\ud835\udc40 = 6 and always \ud835\udc41 = 1000 for BoN vs. other RMs with 1 \u2264 \ud835\udc41 \u2264 1000. We validate that BoN limits reward\nhacking compared to RL, as performances get better when increasing \ud835\udc41.\nC.3. RL experiments\nC.3.1. Experiments with corrupted preference dataset\n0\n2000\n4000\n6000\n8000\n10000\n# steps\n4\n5\n6\n7\nControl reward\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\nFigure 18 | RL experiments. Same as Figure 1(b) but with 25% corruption in the preference dataset.\n31\nWARM: On the Benefits of Weight Averaged Reward Models\nC.3.2. Experiments with clean preference dataset\n2000\n3000\n4000\n5000\n6000\n# steps\n0.0\n0.2\n0.4\n0.6\n0.8\nWin ratio vs. WARM M = 6\nWARM M = 10\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\ny=0.5\n(a) WARM \ud835\udc40 = 6.\n2000\n3000\n4000\n5000\n6000\n# steps\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nWin ratio of WARM M = 6 vs. Ind \n1\n= 0.01\n= 0.003\n= 0.001\ny=0.5\n(b) Impact of \ud835\udefc.\nFigure 19 | Oracle preference metric for RL experiments at fixed number of training steps (clean setup).\nFigure 19(a) plots the win rate of the policy with WARM \ud835\udc40 = 6 vs. the other policies, all at the same number of\ntraining steps. Figure 19(b) shows the win rate of WARM \ud835\udc40 = 6 against the policy trained with a single RM \ud835\udf191\n(the best according to OOD accuracy) along training for different values of \ud835\udefc controlling the KL regularization\nstrength.\n0\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n# steps\n2\n3\n4\n5\n6\n7\n8\n9\n10\nControl reward\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\n(a) Control reward vs. training steps.\n0\n100\n200\n300\n400\n500\n600\n700\nKL\n2\n3\n4\n5\n6\n7\n8\n9\n10\nControl reward\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\n(b) Control reward vs. KL.\nFigure 20 | Control reward for RL experiments with \ud835\udefc = 0.01 (clean setup).\n32\nWARM: On the Benefits of Weight Averaged Reward Models\n0\n2000\n4000\n6000\n8000\n# steps\n7.5\n5.0\n2.5\n0.0\n2.5\n5.0\n7.5\n10.0\nControl reward\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\n(a) Control reward vs. training steps.\n0\n500\n1000\n1500\n2000\nKL\n7.5\n5.0\n2.5\n0.0\n2.5\n5.0\n7.5\n10.0\nControl reward\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\n(b) Control reward vs. KL.\nFigure 21 | Control reward for RL experiments with \ud835\udefc = 0.001 (clean setup).\nC.4. Distillation experiments\nIn Figure 22 we reproduce the distillation setup from [17], where the control PaLM-XS RM generates\nthe labels to train PaLM-XXS RMs. As a side note, we observed that distillation changes the diversity\nacross fine-tuned RMs, thus potentially altering the significance of the distillation setup, motivating\nus in exploring the more realistic RLAIF setup.\n0\n1\n2\n3\n4\n5\n6\nKL : log(N)\nN\n1\nN\n0.075\n0.050\n0.025\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\nControl reward gain\nWARM M = 6\nWARM M = 2\nENS M = 2\nInd \n2\nInd \n1\nFigure 22 | BoN experiment in the distillation setup from [17]. The labels in the preference dataset are\ngiven by the control RM, the same RM which gives the \ud835\udc66-axis. The candidate summaries are generated by a\nSFT with the T5 architecture [111]. The blue lines represent WARM with \ud835\udc40 weights: WARM performs higher\nthan the individual RMs (in yellows) or when ensembling their predictions (ENS in red).\n33\n"
  },
  {
    "title": "OK-Robot: What Really Matters in Integrating Open-Knowledge Models for Robotics",
    "link": "https://arxiv.org/pdf/2401.12202.pdf",
    "upvote": "9",
    "text": "OK-Robot:\nWhat Really Matters in Integrating Open-Knowledge\nModels for Robotics\nPeiqi Liu*1\nYaswanth Orru*1\nJay Vakil2\nChris Paxton2\nNur Muhammad Mahi Shafiullah\u20201\nLerrel Pinto\u20201\nNew York University1, AI at Meta2\nhttps://ok-robot.github.io\n1\n2\n3\n\u201cOK Robot, move the Takis on the desk to the nightstand\u201d\n1. VoxelMap \nNavigationPlan\n(\u201cTakis on the desk\u201d)\n1. VoxelMap\nNavigationPlan\n(\u201cTakis on the desk\u201d)\n1. VoxelMap \nNavigationPlan\n(\u201cTakis on the desk\u201d)\n3. Drop \nPrimitive\nDrop(<RGBD>, \u201cnightstand\u201d)\n\u201cMove the soda can to \nthe box\u201d\n1\n2\n3\n\u201cMove the purple \nshampoo to the red bag\u201d\n1\n2\n3\n\u201cMove the white meds \nbox to the trash bin\u201d\n1\n2\n3\n1. VoxelMap \nNavigationPlan\n(\u201cTakis on the desk\u201d)\n2. Lang-SAM + \nAnyGrasp\nGrasp(<RGBD>, \u201cTakis\u201d)\nFig. 1: OK-Robot is an Open Knowledge robotic system, which integrates a variety of learned models trained on publicly available data, to\npick and drop objects in real-world environments. Using Open Knowledge models such as CLIP, Lang-SAM, AnyGrasp, and OWL-ViT,\nOK-Robot achieves a 58.5% success rate across 10 unseen, cluttered home environments, and 82.4% on cleaner, decluttered environments.\narXiv:2401.12202v2  [cs.RO]  29 Feb 2024\nAbstract\u2014 Remarkable progress has been made in recent years\nin the fields of vision, language, and robotics. We now have\nvision models capable of recognizing objects based on language\nqueries, navigation systems that can effectively control mobile\nsystems, and grasping models that can handle a wide range of\nobjects. Despite these advancements, general-purpose applications\nof robotics still lag behind, even though they rely on these\nfundamental capabilities of recognition, navigation, and grasping.\nIn this paper, we adopt a systems-first approach to develop a new\nOpen Knowledge-based robotics framework called OK-Robot. By\ncombining Vision-Language Models (VLMs) for object detection,\nnavigation primitives for movement, and grasping primitives\nfor object manipulation, OK-Robot offers a integrated solution\nfor pick-and-drop operations without requiring any training. To\nevaluate its performance, we run OK-Robot in 10 real-world\nhome environments. The results demonstrate that OK-Robot\nachieves a 58.5% success rate in open-ended pick-and-drop tasks,\nrepresenting a new state-of-the-art in Open Vocabulary Mobile\nManipulation (OVMM) with nearly 1.8\u00d7 the performance of\nprior work. On cleaner, uncluttered environments, OK-Robot\u2019s\nperformance increases to 82%. However, the most important\ninsight gained from OK-Robot is the critical role of nuanced\ndetails when combining Open Knowledge systems like VLMs with\nrobotic modules. We published our code and robot videos on\nhttps://ok-robot.github.io to encourage further investigation.\nI. INTRODUCTION\nCreating a general-purpose robot has been a longstanding\ndream of the robotics community. With the increase in data-\ndriven approaches and large robot models, impressive progress\nis being made [1\u20134]. However, current systems are brittle,\nclosed, and fail when encountering unseen scenarios. Even\nthe largest robotics models can often only be deployed in\npreviously seen environments [5, 6]. The brittleness of these\nsystems is further exacerbated in settings where little robotic\ndata is available, such as in unstructured home environments.\nThe poor generalization of robotic systems lies in stark\ncontrast to large vision models [7\u201310], which show capabilities\nof semantic understanding [11\u201313], detection [7, 8], and\nconnecting visual representations to language [9, 10, 14]\nAt the same time, base robotic skills for navigation [15],\ngrasping [16\u201319], and rearrangement [20, 21] are fairly mature.\nHence, it is perplexing that robotic systems that combine\nmodern vision models with robot-specific primitives perform\nso poorly. To highlight the difficulty of this problem, the\nrecent NeurIPS 2023 challenge for open-vocabulary mobile\nmanipulation (OVMM) [22] registered a success rate of 33%\nfor the winning solution [23].\nSo what makes open-vocabulary robotics so hard? Unfortu-\nnately, there isn\u2019t a single challenge that makes this problem\nhard. Instead, inaccuracies in different components compound\nand together results in an overall drop. For example, the quality\nof open-vocabulary retrievals of objects in homes is dependent\non the quality of query strings, navigation targets determined\nby VLMs may not be reachable to the robot, and the choice\nof different grasping models may lead to large differences in\ngrasping performance. Hence, making progress on this problem\nrequires a careful and nuanced framework that both integrates\n* Denotes equal contribution and \u2020 denotes equal advising.\nCorrespondence to: mahi@cs.nyu.edu\nVLMs and robotics primitives, while being flexible enough to\nincorporate newer models as they are developed by the VLM\nand robotics community.\nWe present OK-Robot, an Open Knowledge Robot that\nintegrates state-of-the-art VLMs with powerful robotics prim-\nitives for navigation and grasping to enable pick-and-drop.\nHere, Open Knowledge refers to learned models trained on\nlarge, publicly available datasets. When placed in a new home\nenvironment, OK-Robot is seeded with a scan taken from an\niPhone. Given this scan, dense vision-language representations\nare computed using LangSam [24] and CLIP [9] and stored\nin a semantic memory. Then, when a language-query for an\nobject to be picked comes in, semantic memory is queried\nwith the language embedding to find that object. After this,\nnavigation and picking primitives are applied sequentially to\nmove to the desired object and pick it up. A similar process\ncan be carried out for dropping the object.\nTo study OK-Robot, we tested it in 10 real world home\nenvironments. Through our experiments, we found that on\na unseen natural home environment, a zero-shot deployment\nof our system achieves 58.5% success on average. However,\nthis success rate is largely dependant on the \u201cnaturalness\u201d of\nthe environment, as we show that with improving the queries,\ndecluttering the space, and excluding objects that are clearly\nadversarial (too large, too translucent, too slippery), this success\nrate reaches 82.4%. Overall, through our experiments, we make\nthe following observations:\n\u2022 Pre-trained\nVLMs\nare\nhighly\neffective\nfor\nopen-\nvocabulary navigation: Current open-vocabulary vision-\nlanguage models such as CLIP [9] or OWL-ViT [8] offer\nstrong performance in identifing arbitrary objects in the real\nworld, and enable navigating to them in a zero-shot manner\n(see Section II-A.)\n\u2022 Pre-trained grasping models can be directly applied to\nmobile manipulation: Similar to VLMs, special purpose\nrobot models pre-trained on large amounts of data can be\napplied out of the box to approach open-vocabulary grasping\nin homes. These robot models do not require any additional\ntraining or fine-tuning (see Section II-B.)\n\u2022 How components are combined is crucial: Given the\npretrained models, we find that they can be combined with\nno training using a simple state-machine model. We also\nfind that using heuristics to counteract the robot\u2019s physical\nlimitations can lead to a better success rate in the real world\n(see Section II-D.)\n\u2022 Several challenges still remain: While, given the im-\nmense challenge of operating zero-shot in arbitrary homes,\nOK-Robot improves upon prior work, by analyzing the\nfailure modes we find that there are significant improvements\nthat can be made on the VLMs, robot models, and robot\nmorphology, that will directly increase performance of open-\nknowledge manipulation agents (see Section III-D).\nTo encourage and support future work in open-knowledge\nrobotics, we have shared the code and modules for OK-Robot,\nand are committed to supporting reproduction of our results.\nMore information along with robot videos and the code are\navailable on our project website: https://ok-robot.github.io.\nII. TECHNICAL COMPONENTS AND METHOD\nOur method, on a high level, solves the problem described\nby the query: \u201cPick up A (from B) and drop it on/in C\u201d,\nwhere A is an object and B and C are places in a real-world\nenvironment such as homes. The system we introduce is a\ncombination of three primary subsystems combined on a Hello\nRobot: Stretch. Namely, these are the open-vocabulary object\nnavigation module, the open-vocabulary RGB-D grasping\nmodule, and the dropping heuristic. In this section, we describe\neach of these components in more details.\nA. Open-home, open-vocabulary object navigation\nThe first component of our method is an open-home, open-\nvocabulary object navigation model that we use to map a home\nand subsequently navigate to any object of interest designated\nby a natural language query.\nScanning the home: For open vocabulary object navigation, we\nfollow the approach from CLIP-Fields [27] and assume a pre-\nmapping phase where the home is \u201cscanned\u201d manually using an\niPhone. This manual scan simply consists of taking a video of\nthe home using the Record3D app on the iPhone, which results\nin a sequence of posed RGB-D images and takes less than one\nminute for a new room. Once collected, the RGB-D images,\nalong with the camera pose and positions, are exported to\nour library for map-building. To ensure our semantic memory\ncontains both the objects of interest as well as the navigable\nsurface and any obstacles, we capture the floor surface alongside\nthe objects and receptacles in the environment.\nDetecting objects: On each frame of the scan, we run an\nopen-vocabulary object detector. We chose OWL-ViT [8] over\nDetic [7] as the object detector since we found OWL-ViT to\nperform better in preliminary queries. We apply the detector on\nevery frame, and extract each of the object bounding box, CLIP-\nembedding, detector confidence, and pass these information\nonto the object memory module. We further refine the bounding\nboxes into object masks with Segment Anything (SAM) [28].\nNote that, in many cases, open-vocabulary object detectors\nrequire a set of natural language object queries to be detected.\nWe supply a large set of such object queries, derived from the\noriginal Scannet200 labels [29] and presented in Appendix B,\nto help the detector captures most common objects in the scene.\nObject-centric semantic memory: We use an object-centric\nmemory similar to Clip-Fields [27] and OVMM [25] that we\ncall the VoxelMap. VoxelMap is built by back-projecting the\nobject masks in real-world coordinates using the depth image\nand the pose collected by the camera. This process giving\nus a point cloud where each point has an associated CLIP\nsemantic vector. Then, we voxelize the point cloud to a 5 cm\nresolution. For each voxel, we calculate the detector-confidence\nweighted average for the CLIP embeddings that belong to that\nvoxel. This VoxelMap builds the base of our object memory\nmodule. Note that the representation created this way remains\nstatic after the first scan, and cannot be adapted during the\nrobot\u2019s operation. This inability to dynamically create a map\nis discussed in our limitations section (Section V).\nQuerying the memory module: Our semantic object memory\ngives us a static world representation represented as possibly\nnon-empty voxels in the world, and a semantic vector in CLIP\nspace associated with each voxel. Given a language query, we\nfirst convert it to a semantic vector using the CLIP language\nencoder. Then, we find the voxel where the dot product between\nthe encoded embedding and the voxel\u2019s associated embedding is\nmaximized. Since each voxel is associated with a real location\nin the home, this lets us find the location where a queried\nobject is most likely to be found, similar to Figure 2(a).\nWe also implement querying for \u201cA on B\u201d by interpreting\nit as \u201cA near B\u201d. We do so by selecting top-10 points for\nquery A and top-50 points for query B. Then, we calculate the\n10 \u00d7 50 pairwise L2 distances and pick the A-point associated\nwith the shortest (A, B) distance. Note that during the object\nnavigation phase we use this query only to navigate to the\nobject approximately, and not for manipulation. This approach\ngives us two advantages: our map can be as lower resolution\nthan those in prior work [26, 27, 30], and we can deal with\nsmall movements in object\u2019s location after building the map.\nNavigating to objects in the real world: Once our navigation\nmodel gives us a 3D location coordinate in the real world, we\nuse that as a navigation target for our robot to initialize our\nmanipulation phase. Going and looking at an object [15, 27,\n31] can be done while remaining at a safe distance from the\nobject itself. In contrast, our navigation module must place the\nrobot at an arms length so that the robot can manipulate the\ntarget object afterwards. Thus, our navigation method has to\nbalance the following objectives:\n1) The robot needs to be close enough to the object to\nmanipulate it,\n2) The robot needs some space to move its gripper, so there\nneeds to be a small but non-negligible space between the\nrobot and the object, and,\n3) The robot needs to avoid collision during manipulation,\nand thus needs to keep its distance from all obstacles.\nWe use three different navigation score functions, each associ-\nated with one of the above points, and evaluate them on each\npoint of the space to find the best position to place the robot.\nLet a random point be \u2212\u2192x , the closest obstacle point as\n\u2212\u2192x obs, and the target object as \u2212\u2192\nxo. We define the following\nthree functions s1, s2, s3 to capture our three criterion. We\ndefine s as their weighted sum. The ideal navigation point\n\u2212\u2192x \u2217 is the point in space that minimizes s(\u2212\u2192x ), and the ideal\ndirection is given by the vector from \u2212\u2192\nx\u2217 to \u2212\u2192\nxo.\ns1(\u2212\u2192x ) = ||\u2212\u2192x \u2212 \u2212\u2192\nxo||\ns2(\u2212\u2192x ) = 40 \u2212 min(||\u2212\u2192x \u2212 \u2212\u2192\nxo||, 40)\ns3(\u2212\u2192x ) =\n(\n1/||\u2212\u2192x \u2212 \u2212\u2192x obs||,\nif ||\u2212\u2192x \u2212 \u2212\u2192x obs||0 \u2264 30\n0,\notherwise\ns(\u2212\u2192x ) = s1(\u2212\u2192x ) + 8s2(\u2212\u2192x ) + 8s3(\u2212\u2192x )\n(a) Open-vocabulary object localization \nusing VoxelMap\n(b) Open-vocabulary navigation planning \nusing VoxelMap and heuristics weighted A*\nUmbrella\nBrown hat\nBed\nBrown bag\nOrange \nlaundry bag\nPink powder \nbottle\nRed body \nspray\nPlant\nStart point\nBrown hat\nFig. 2: Open-vocabulary, open knowledge object localization and navigation in the real-world. We use the VoxelMap [25] for localizing\nobjects with natural language queries, and use an A* algorithm similar to USANet [26] for path planning.\nTo navigate to this target point safely from any other point in\nspace, we follow a similar approach to [26, 32] by building\nan obstacle map from our captured posed RGB-D images. We\nbuild a 2D, 10cm\u00d710cm grid of obstacles over which we\nnavigate using the A* algorithm. To convert our VoxelMap\nto an obstacle map, we first set a floor and ceiling height.\nPresence of occupied voxels in between them implies the grid\ncell is occupied, while presence of neither ceiling nor floor\nvoxels mean that the grid cell is unexplored. We mark both\noccupied or unexplored cells as not navigable. Around each\noccupied point, we mark any point within a 20 cm radius\nas also non-navigable to account for the robot\u2019s radius and a\nturn radius. During A* search, we use the s3 as a heuristic\nfunction on the node costs to navigate further away from any\nobstacles, which makes our generated paths similar to ideal\nVoronoi paths [33] in our experiments.\nB. Open-vocabulary grasping in the real world\nGrasping or physically interacting with arbitrary objects in\nthe real world is much more complex than open-vocabulary\nnavigation. We opt for using a pre-trained grasping model to\ngenerate grasp poses in the real world, and filter them with\nlanguage-conditioning using a modern VLM.\nGrasp perception: Once the robot reaches the object location\nusing the navigation method outlined in Section II-A, we use a\npre-trained grasping model, AnyGrasp [19], to generate a grasp\nfor the robot. We point the robot\u2019s RGB-D head camera towards\nthe object\u2019s 3D location, given to us by the semantic memory,\nand capture an RGB-D image from it (Figure 3, column 1). We\nbackproject and convert the depth image to a pointcloud and\npass this information to the grasp generation model. Our grasp\ngeneration model, AnyGrasp, generates all collision free grasps\n(Figure 3 column 2) for a parallel jaw gripper in a scene given\na single RGB image and a pointcloud. AnyGrasp provides us\nwith grasp point, width, height, depth, and a \u201cgraspness score\u201d,\nindicating uncalibrated model confidence in each grasp.\nFiltering grasps using language queries: Once we get\nall proposed grasps from AnyGrasp, we filter them using\nLangSam [24]. LangSam [24] segments the captured image and\nfinds the desired object mask with a language query (Figure 3\ncolumn 3). We project all the proposed grasp points onto the\nimage and find the grasps that fall into the object mask (Figure 3\ncolumn 4). We pick the best grasp using a heuristic. Given a\ngrasp score S and the angle between the grasp normal and floor\nnormal \u03b8, the new heuristic score is S \u2212 (\u03b84/10). This heuristic\nbalances high graspness scores with finding flat, horizontal\ngrasps. We prefer horizontal grasps because they are robust\nto small calibration errors on the robot, while vertical grasps\nneeds better hand-eye calibration to be successful. Robustness\nto hand-eye calibration errors lead to higher success as we\ntransport the robot to different homes during our experiments.\nGrasp execution: Once we identify the best grasp (Figure 3\ncolumn 5), we use a simple pre-grasp approach [34] to grasp our\nintended object. If \u2212\u2192p is the grasp point and \u2212\u2192a is the approach\nvector given by the grasping model, our robot gripper follows\nthe following trajectory:\n\u27e8\u2212\u2192p \u2212 0.2\u2212\u2192a , \u2212\u2192p \u2212 0.08\u2212\u2192a , \u2212\u2192p \u2212 0.04\u2212\u2192a , \u2212\u2192p \u27e9\nPut simply, our method approaches the object from a pre-grasp\nposition in a line with progressively smaller motions. Moving\nslower as we approach the object helps the robot not knock\nover light objects. Once we reach the predicted grasp point, we\nclose the gripper in a close loop fashion to get a solid grip on\nthe object without crushing it. After grasping the object, we lift\nup the robot arm, retract it fully, and rotate the wrist to have\nthe object tucked over the body. This behavior maintains the\nrobot footprint while ensuring the object is held securely by\nthe robot and doesn\u2019t fall while navigating to the drop location.\nRobot view\nAnyGrasp proposals\nLangSam mask\nGrasp \ufb01ltering\nFinal grasp\nFig. 3: Open-vocabulary grasping in the real world. From left to right, we show the (a) robot POV image, (b) all suggested grasps from\nAnyGrasp [19], (c) object mask given label from LangSam [24], (d) grasp points filtered by the mask, and (e) grasp chosen for execution.\nC. Dropping heuristic\nAfter picking up an object, we find and navigatte to the drop\nlocation using the same methods described in Section II-A.\nUnlike in HomeRobot\u2019s baseline implementation [25] that\nassumes that the drop-off location is a flat surface, we extend\nour heuristic to cover concave objects such as sink, bins, boxes,\nand bags. First, we segment the point cloud P captured by the\nrobot\u2019s head camera using LangSam [24] similar to Section II-B\nusing the drop language query. Then, we align that segmented\npoint cloud such that X-axis is aligned with the way the robot is\nfacing, Y-axis is to its left and right, and the Z-axis of the point\ncloud is aligned with the floor normal. Then, we normalize\nthe point cloud so that the robot\u2019s (x, y) coordinate is (0, 0),\nand the floor plane is at z = 0. We call this pointcloud Pa.\nOn the aligned, segmented point cloud, we consider the (x, y)\ncoordinates for each point, and find the median values xm and\nym on each axis. Finally, we find a drop height using zmax =\n0.2 + max{z | (x, y, z) \u2208 Pa; 0 \u2264 x \u2264 xm; |y \u2212 ym| < 0.1}\non the segmented, aligned pointcloud. We add a small buffer\nof 0.2 to the height to avoid collisions between the robot and\nthe drop location. Finally, we move the robot gripper above\nthe drop point, and open the gripper to drop the object. While\nthis heuristic doesn\u2019t explicitly reason about clutter, in our\nexperiments it performs well on average.\nD. Deployment in homes\nOur navigation, pick, and drop primitives are combined to\ncreate our robot method that can be applied in any novel home.\nFor a new home environment, we \u201cscan\u201d the room in under\na minute. Then, it takes less than five minutes to process the\nscan into our VoxelMap. Once that is done, the robot can\nbe immediately placed at the base and start operating. From\narriving into a completely novel environment to start operating\nautonomously in it, our system takes under 10 minutes on\naverage to complete the first pick-and-drop task.\nTransitioning between modules: The transition between\ndifferent modules is predefined and happens automatically\nonce a user specifies the object to pick and where to drop it.\nSince we do not implement error detection or correction, our\nstate machine model is a simple linear chain of steps leading\nfrom navigating to object, to grasping, to navigating to goal,\nand to dropping the object at the goal to finish the task.\nProtocol for home experiments: To run our experiment in a\nnovel home, we move the robot to a previously unobserved\nroom. We record the scene and create our VoxelMap. Concur-\nrently, we pick between 10-20 objects arbitrarily in each scene\nthat can fit in the robot gripper. These are objects found in the\nscene, and are not chosen ahead of time. We come up with\na language query for each chosen object using GPT-4V [35]\nto keep the queries consistent and free of experimenter bias.\nWe query our navigation module to filter out all the navigation\nfailures; i.e. objects that our semantic memory module could not\nlocate properly. Then, we execute pick-and-drop on remaining\nobjects sequentially without resets between trials.\nFig. 4: All the success and failure cases in our home experiments, aggregated over all three cleaning phases, and broken down by mode of\nfailure. From left to right, we show the application of the three components of OK-Robot, and show a breakdown of the long-tail failure\nmodes of each of the components.\nIII. EXPERIMENTS\nWe evaluate our method in two set of experiments. On\nthe first set of experiments, we evaluate between multiple\nalternatives for each of our navigation and manipulation\nmodules. These experiments give us insights about which\nmodules to use and evaluate in a home environment as a\npart of our method. On the next set of experiments, we took\nour robots to 10 homes and ran 171 pick-and-drop experiments\nto empirically evaluate how our method performs in completely\nnovel homes, and to understand the failure modes of our system.\nThrough these experiments, we look to answer a series\nof questions regarding the capabilities and limits of current\nOpen Knowledge robotic systems, as embodied by OK-Robot.\nNamely, we ask the following:\n1) How well can such a system tackle the challenge of pick\nand drop in arbitrary homes?\n2) How well do alternate primitives for navigation and\ngrasping compare to the recipe presented here for building\nan Open Knowledge robotic system?\n3) How well can our current systems handle unique chal-\nlenges that make homes particularly difficult, such as\nclutter, ambiguity, and affordance challenges?\n4) What are the failure modes of such a system and its\nindividual components in real home environments?\nA. Results of home experiments\nOver the 10 home environment, OK-Robot achieved a\n58.5% success rates in completing full pick-and-drops. Notably,\nthis success rate is over novel objects sourced from each home\nwith our zero-shot algorithm. As a result, each success and\nfailure of the robot tells us something interesting about applying\nopen-knowledge models in robotics, which we analyze over the\nnext sections. In Appendix E, we provide the details of all our\nhome experiments and results from the same. In Appendix C\nwe show a subset of the target objects and in Appendix D we\nshow snapshots of homes where OK-Robot was deployed.\nSnippets of our experiments are in Figure 1, and full videos\nare presented on our project website.\nReproduction of our system: Beyond the home experiment\nresults presented here, we also reproduced OK-Robot in two\nhomes in Pittsburgh, PA, and Fremont, CA. These homes\nwere larger and more complex: a cluttered, actively-used home\nkitchen environment, and a large, controlled test apartment\nused in prior work [22, 25]. In Appendix Figure 12, we show\nthe robot performing pick-and-drop in these two environments.\nThese homes were different from our initial ten experiments\nin a few ways. Both were larger compared to the average NY\nhomes, requiring more robot motion to navigate to different\ngoals. The PA environment (Figure 12 top) notably had much\nmore clutter. However, given only a scan, OK-Robot was able\nto successfully pick and drop objects like stuffed lion, plush\ncactus, toy drill, or green water bottle in both environments.\nB. Ablations over system components\nApart from the navigation and manipulation strategies used\nin OK-Robot, we also evaluated a number of alternative open\nVoxelMap\nClip fields\nUSA Net\nSemantic memory module\n0\n20\n40\n60\n80\n100\nAnyGrasp\nAnyGrasp\nOpen Source\nTop down\nGrasping module\nFig. 5: Ablation experiment using different semantic memory and\ngrasping modules, with the bars showing average performance and\nthe error bars showing standard deviation over the environments.\nvocabulary navigation and grasping modules. We compared\nthem by evaluating them in three different environments in our\nlab. Apart from VoxelMap [25], we evaluate CLIP-Fields [27],\nand USA-Net [26] for semantic navigation. For grasping\nmodule, we consider AnyGrasp and its open-source baseline,\nOpen Graspness [19], Contact-GraspNet [16], and Top-Down\ngrasp heuristic from home-robot [25]. More details about them\nare provided in Appendix Section A.\nIn Figure 5, we see their comparative performance in three\nlab environments. For semantic memory modules, we see that\nVoxelMap, used in OK-Robot and described in Sec. II-A,\noutperforms other semantic memory modules by a small margin.\nIt also has much lower variance compared to the alternatives,\nmeaning it is more reliable. As for grasping modules, AnyGrasp\nclearly outperforms other grasping methods, performing almost\n50% better in a relative scale over the next best candidate, top-\ndown grasp. However, the fact that a heuristic-based algorithm,\ntop-down grasp from HomeRobot [25] beats the open-source\nAnyGrasp baseline and Contact-GraspNet shows that building\na truly general-purpose grasping model remains difficult.\nC. Impact of clutter, object ambiguity, and affordance\nWhat makes home environments especially difficult com-\npared to lab experiments is the presence of physical clutter,\nlanguage-to-object mapping ambiguity, and hard-to-reach posi-\ntions. To gain a clear understanding of how such factors play\ninto our experiments, we go through two \u201cclean-up\u201d processes\nin each environment. During the clean-up, we pick a subset of\nobjects that are free from ambiguity from the previous rounds,\nclean the clutter around objects, and generally relocated them\nin an accessible locations. The two clean-up rounds at each\nenvironment gives us insights about the performance gap caused\nby the natural difficulties of a home-like environment.\nWe show a complete analysis of the tasks listed section III-A\nwhich failed in various stages in Figure 6. As we can see from\nthis breakdown, as we clean up the environment and remove\nthe ambiguous objects, the navigation accuracy goes up, and\nthe total error rate goes down from 15% to 12% and finally\nall the way down to 4%. Similarly, as we clean up clutters\nfrom the environment, we find that the manipulation accuracy\nalso improves and the error rates decrease from 25% to 16%\nand finally 13%. Finally, since the drop-module is agnostic\n0\n20\n40\n60\n80\n100\nPercentage of trials\nhigh\nlow\nnone\nCleanup level\n82\n71\n58\n4\n12\n15\n13\n16\n25\nSuccess\nNavigation failure\nManipulation failure\nPlacing failure\nFig. 6: Failure modes of our method in novel homes, broken down\nby the failures of the three modules and the cleanup levels.\nof the label ambiguity or manipulation difficulty arising from\nclutter, the failure rate of the dropping primitive stays roughly\nconstant through the three phases of cleanup.\nD. Understanding the performance of OK-Robot\nWhile our method can show zero-shot generalization in\ncompletely new environments, we probe OK-Robot to better\nunderstand its failure modes. Primarily, we elaborate on how\nour model performed in novel homes, what were the biggest\nchallenges, and discuss potential solutions to them.\nWe first show a coarse-level breakdown of the failures, only\nconsidering the three high level modules of our method in\nFigure 6. We see that generally, the leading cause of failure is\nour manipulation failure, which intuitively is the most difficult\nas well. However, at a closer look, we notice a long tail of\nfailure causes presented in figure 4.\nThe three leading causes of failures are failing to retrieve the\nright object to navigate to from the semantic memory (9.3%),\ngetting a difficult pose from the manipulation module (8.0%),\nand robot hardware difficulties (7.5%). In this section, we go\nover the analysis of the failure modes presented in Figure 4\nand discuss the most frequent cases.\nNatural language queries for objects: One of the primary\nreasons our OK-Robot can fail is when a natural language\nquery given by the user doesn\u2019t retrieve the intended object\nfrom the semantic memory. In Figure 7 we show how some\nqueries may fail while semantically very similar but slightly\nmodified wording of the same query might succeed.\nGenerally, this has been the case for scenes where there\nare multiple visually or semantically similar objects, as shown\nin the figure. There are other cases where some queries may\npass while other very similar queries may fail. An interactive\nsystem that gets confirmation from the user as it retrieves an\nobject from memory would avoid such issues.\nGrasping module limitations: One failure mode of our\nmanipulation module comes from executing grasps from a\npre-trained manipulation model\u2019s output based on a single\nRGB-D image. Moreover, this model wasn\u2019t even designed for\nthe Hello Robot: Stretch gripper. As a result, sometimes the\nproposed grasps are unreliable or unrealistic (Figure 8).\nSometimes, the grasp is infeasible given the robot joint limits,\nor is simply too far from the robot body. Developing better\ngrasp perception models or heuristics will let us sample better\ngrasps for a given object.\nFig. 7: Samples of failed or ambiguous language queries into our\nsemantic memory module. Since the memory module depends on\npretrained large vision language model, its performance shows\nsusceptibility to particular \u201cincantations\u201d similar to current LLMs.\nIn other cases, the model generates a good grasp pose, but\nas the robot is executing the grasping primitive, it collides with\nsome minor environment obstacle. Since we apply the same\ngrasp trajectory in every case instead of planning the grasp\ntrajectory, some such failures are inevitable. Grasping models\nthat generates a grasp trajectory as well as a pose may solve\nsuch issues.\nFinally, our grasping module categorically struggles with\nflat objects, like chocolate bars and books, since it\u2019s difficult\nto grasp them off a surface with a two-fingered gripper.\nRobot hardware limitations: While our robot of choice, a\nHello Robot: Stretch, is able to pick-and-drop a variety of\nobjects, certain hardware limitations also dictate what our\nsystem can and cannot manipulate. For example, the fully\nextended robot arm has a 1 kg payload limit, and thus our\nmethod is unable to pick objects like a full dish soap bottle.\nSimilarly, objects that are far from navigable floor space, i.e.\nin the middle of a bed, or on high places, are difficult for the\nrobot to reach because of the reach limits of the arm. The\nrobot hardware or the RealSense camera can occasionally get\nmiscalibrated over time, especially during continuous home\noperations. This miscalibration can lead to manipulation errors\nsince that module requires hand-eye coordination in the robot.\nThe robot base wheels have small diameters and in some cases\nstruggle to move smoothly between carpet and floor.\nIV. RELATED WORKS\nA. Vision-Language models for robotic navigation\nEarly applications of pre-trained open-knowledge models in\nrobotics has been in open-vocabulary navigation. Navigating to\nvarious objects is an important task which has been looked at\nin a wide range of previous works [25, 31, 36], as well as in\nObject is transparent, so  \npointcloud is imperfect, \nso grasp is imperfect\nTop-down grasp on tall \ncounter is unreachable\nDiagonal grasp on a \ncylindrical object is \nunstable\nFine grasps on small \nobjects are vulnerable to \ncalibration errors\nGrasps on round objects \nare unstable when not \nperfectly diametrical\nGrasps on \ufb02at objects \ncollide with env if not \nperfectly executed\nFig. 8: Samples of failures of our manipulation module. Most failures\nstem from using only a single RGB-D view to generate the grasp and\nthe limiting form-factor of a large two-fingered parallel jaw gripper.\nthe context of longer pick-and-place tasks [37, 38]. However,\nthese methods have generally been applied to relatively small\nnumbers of objects [39]. Recently, Objaverse [40] has shown\nnavigation to thousands of object types, for example, but\nmuch of this work has been restricted to simulated or highly\ncontrolled environments.\nThe early work addressing this problem builds upon repre-\nsentations derived from pre-trained vision language models,\nsuch as SemAbs [41], CLIP-Fields [27], VLMaps [42], NLMap-\nSayCan [43], and later, ConceptFusion [44] and LERF [30].\nMost of these models show object localization in pre-mapped\nscenes, while CLIP-Fields, VLMaps, and NLMap-SayCan\nshow integration with real robots for indoor navigation tasks.\nUSA-Nets [26] extends this task to include an affordance\nmodel, navigating with open-vocabulary queries while doing\nobject avoidance. ViNT [45] proposes a foundation model for\nrobotic navigation which can be applied to vision-language\nnavigation problems. More recently, GOAT [31] was proposed\nas a modular system for \u201cgoing to anything\u201d and navigating\nto any object in any environment given either language or\nimage queries. ConceptGraphs [46] proposed an open scene\ngraph representation capable of handling complex queries using\nLLMs. Any such open-vocabulary embodied model has the\npotential to improve modular systems like OK-Robot.\nB. Pretrained robot manipulation models\nWhile humans can frequently look at objects and immediately\nknow how to grasp it, such grasping knowledge is not easily\naccessible to robots. Over the years, there has been many\nworks that has focused on creating such a general robot grasp\ngeneration model [1, 47\u201352] for arbitrary objects and potentially\ncluttered scenes via learning methods. Our work focuses on\nmore recent iterations of such methods [16, 19] that are trained\non large grasping datasets [18, 53]. While these models only\nperform one task, namely grasping, they predict grasps across\na large object surface and thus enable downstream complex,\nlong-horizon manipulation tasks [20, 21, 54].\nMore recently, there is a set of general-purpose manipulation\nmodels moving beyond just grasping [55\u201359]. Some of these\nworks perform general language-conditioned manipulation\ntasks, but are largely limited to a small set of scenes and\nobjects. HACMan [60] demonstrates a larger range of object\nmanipulation capabilities, focused on pushing and prodding. In\nthe future, such models could expand the reach of our system.\nC. Open vocabulary robot systems\nMany recent works have worked on language-enabled\ntasks for complex robot systems. Some examples include\nlanguage conditioned policy learning [55, 61\u201363], learning goal-\nconditioned value functions [3, 64], and using large language\nmodels to generate code [65\u201367]. However, a fundamental\ndifference remains between systems which aim to operate\non arbitrary objects in an open-vocab manner, and systems\nwhere one can specify one among a limited number of goals\nor options using language. Consequently, Open-Vocabulary\nMobile Manipulation has been proposed as a key challenge for\nrobotic manipulation [25]. There has previously been efforts to\nbuild such a system [68, 69]. However, unlike such previous\nwork, we try to build everything on an open platform and ensure\nour method can work without having to re-train anything for a\nnovel home. Recently, UniTeam [23] won the 2023 HomeRobot\nOVMM Challenge [22] with a modular system doing pick-\nand-place to arbitrary objects, with a zero-shot generalization\nrequirement similar to ours.\nIn parallel, recently, there have been a number of papers\ndoing open-vocabulary manipulation using GPT or especially\nGPT4 [35]. GPT4V can be included in robot task planning\nframeworks and used to execute long-horizon robot tasks,\nincluding ones from human demonstrations [70]. Concept-\nGraphs [46] is a good recent example, showing complex\nobject search, planning, and pick-and-place capabilities to open-\nvocabulary objects. SayPlan [71] also shows how these can use\nused together with a scene graph to handle very large, complex\nenvironments, and multi-step tasks; this work is complementary\nto ours, as it doesn\u2019t handle how to implement pick and place.\nV. LIMITATIONS, OPEN PROBLEMS AND\nREQUESTS FOR RESEARCH\nWhile our method shows significant success in completely\nnovel home environments, it also shows many places where\nsuch methods can improve. In this section, we discuss a few\nof such potential improvement in the future.\nA. Live semantic memory and obstacle maps\nAll the current semantic memory modules and obstacle map\nbuilders build a static representation of the world, without\na good way of keeping it up-to-date as the world changes.\nHowever, homes are dynamic environments, with many small\nchanges over the day every day. Future research that can build\na dynamic semantic memory and obstacle map would unlock\npotential for continuous application of such pick-and-drop\nmethods in a novel home out of the box.\nB. Grasp plans instead of proposals\nCurrently, the grasping module proposes generic grasps\nwithout taking the robot\u2019s body and dynamics into account.\nSimilarly, given a grasp pose, often the open loop grasping\ntrajectory collides with environmental obstacles, which can be\neasily improved by using a module to generate grasp plans\nrather than grasp poses only.\nC. Improving interactivity between robot and user\nOne of the major causes of failure in our method is in\nnavigation: where the semantic query is ambiguous and the\nintended object is not retrieved from the semantic memory. In\nsuch ambiguous cases, interaction with the user would go a\nlong way to disambiguate the query and help the robot succeed\nmore often.\nD. Detecting and recovering from failure\nCurrently, we observe a multiplicative error accumulation\nbetween our modules: if any of our independent components\nfail, the entire process fails. As a result, even if our modules\neach perform independently at or above 80% success rate,\nour final success rate can still be below 60%. However, with\nbetter error detection and retrying algorithms, we can recover\nfrom much more single-stage errors, and similarly improve our\noverall success rate [23].\nE. Robustifying robot hardware\nWhile Hello Robot - Stretch [72] is an affordable and\nportable platform on which we can implement such an open-\nhome system for arbitrary homes, we also acknowledge that\nwith robust hardware such methods may have vastly enhanced\ncapacity. Such robust hardware may enable us to reach high\nand low places, and pick up heavier objects. Finally, improved\nrobot odometry will enable us to execute much more finer\ngrasps than is possible today.\nACKNOWLEDGMENTS\nNYU authors are supported by grants from Amazon, Honda,\nand ONR award numbers N00014-21-1-2404 and N00014-21-\n1-2758. NMS is supported by the Apple Scholar in AI/ML\nFellowship. LP is supported by the Packard Fellowship. Our\nutmost gratitude goes to our friends and colleagues who helped\nus by hosting our experiments in their homes. Finally, we\nthank Siddhant Haldar, Paula Pascual and Ulyana Piterbarg for\nvaluable feedback and conversations.\nREFERENCES\n[1]\nLerrel Pinto and Abhinav Gupta. Supersizing Self-\nsupervision: Learning to Grasp from 50K Tries and\n700 Robot Hours. 2015. arXiv: 1509.06825 [cs.LG].\n[2]\nSergey Levine, Peter Pastor, Alex Krizhevsky, Julian\nIbarz, and Deirdre Quillen. \u201cLearning hand-eye coordi-\nnation for robotic grasping with deep learning and large-\nscale data collection\u201d. In: The International journal of\nrobotics research 37.4-5 (2018), pp. 421\u2013436.\n[3]\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen\nChebotar, Omar Cortes, Byron David, Chelsea Finn,\nKeerthana Gopalakrishnan, Karol Hausman, Alex Her-\nzog, et al. \u201cDo as I can, not as I say: Grounding\nlanguage in robotic affordances\u201d. In: Conference on\nRobot Learning (CoRL) (2022).\n[4]\nNur Muhammad Mahi Shafiullah, Anant Rai, Haritheja\nEtukuru, Yiqian Liu, Ishan Misra, Soumith Chintala, and\nLerrel Pinto. On Bringing Robots Home. 2023. arXiv:\n2311.16098 [cs.RO].\n[5]\nAnthony Brohan, Noah Brown, Justice Carbajal, Yev-\ngen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana\nGopalakrishnan, Karol Hausman, Alex Herzog, Jasmine\nHsu, et al. \u201cRt-1: Robotics transformer for real-world\ncontrol at scale\u201d. In: arXiv preprint arXiv:2212.06817\n(2022).\n[6]\nAnthony Brohan, Noah Brown, Justice Carbajal, Yevgen\nChebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding,\nDanny Driess, Avinava Dubey, Chelsea Finn, et al. \u201cRt-2:\nVision-language-action models transfer web knowledge\nto robotic control\u201d. In: arXiv preprint arXiv:2307.15818\n(2023).\n[7]\nXingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp\nKr\u00a8ahenb\u00a8uhl, and Ishan Misra. \u201cDetecting twenty-\nthousand classes using image-level supervision\u201d. In:\nEuropean Conference on Computer Vision. Springer.\n2022, pp. 350\u2013368.\n[8]\nMatthias Minderer, Alexey Gritsenko, Austin Stone, et al.\n\u201cSimple Open-Vocabulary Object Detection with Vision\nTransformers\u201d. In: European Conference on Computer\nVision. Springer. 2022, pp. 728\u2013755.\n[9]\nAlec Radford, Jong Wook Kim, Chris Hallacy, et al.\n\u201cLearning Transferable Visual Models From Natural\nLanguage Supervision\u201d. In: International Conference on\nMachine Learning (ICML). Vol. 139. 2021, pp. 8748\u2013\n8763.\n[10]\nKenneth Marino, Mohammad Rastegari, Ali Farhadi,\nand Roozbeh Mottaghi. \u201cOk-vqa: A visual question\nanswering benchmark requiring external knowledge\u201d.\nIn: Proceedings of the IEEE/cvf conference on computer\nvision and pattern recognition. 2019, pp. 3195\u20133204.\n[11]\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, et\nal. Flamingo: a Visual Language Model for Few-Shot\nLearning. 2022. arXiv: 2204.14198 [cs.CV].\n[12]\nShilong Liu, Zhaoyang Zeng, Tianhe Ren, et al. Ground-\ning DINO: Marrying DINO with Grounded Pre-Training\nfor Open-Set Object Detection. 2023. arXiv: 2303.05499\n[cs.CV].\n[13]\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. Visual Instruction Tuning. 2023. arXiv: 2304.08485\n[cs.CV].\n[14]\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. \u201cLanguage models\nare unsupervised multitask learners\u201d. In: OpenAI Blog\n(2019).\n[15]\nTheophile Gervet, Soumith Chintala, Dhruv Batra, Jiten-\ndra Malik, and Devendra Singh Chaplot. \u201cNavigating\nto objects in the real world\u201d. In: Science Robotics 8.79\n(2023), eadf6991.\n[16]\nMartin Sundermeyer, Arsalan Mousavian, Rudolph\nTriebel, and Dieter Fox. \u201cContact-graspnet: Efficient 6-\ndof grasp generation in cluttered scenes\u201d. In: 2021 IEEE\nInternational Conference on Robotics and Automation\n(ICRA). IEEE. 2021, pp. 13438\u201313444.\n[17]\nJeffrey Mahler, Jacky Liang, Sherdil Niyaz, Michael\nLaskey, Richard Doan, Xinyu Liu, Juan Aparicio Ojea,\nand Ken Goldberg. Dex-Net 2.0: Deep Learning to Plan\nRobust Grasps with Synthetic Point Clouds and Analytic\nGrasp Metrics. 2017. arXiv: 1703.09312 [cs.RO].\n[18]\nHao-Shu Fang, Chenxi Wang, Minghao Gou, and\nCewu Lu. \u201cGraspnet-1billion: a large-scale benchmark\nfor general object grasping\u201d. In: Proceedings of the\nIEEE/CVF conference on computer vision and pattern\nrecognition. 2020, pp. 11444\u201311453.\n[19]\nHao-Shu Fang, Chenxi Wang, Hongjie Fang, Minghao\nGou, Jirong Liu, Hengxu Yan, Wenhai Liu, Yichen Xie,\nand Cewu Lu. \u201cAnygrasp: Robust and efficient grasp\nperception in spatial and temporal domains\u201d. In: IEEE\nTransactions on Robotics (2023).\n[20]\nAnkit Goyal, Arsalan Mousavian, Chris Paxton, Yu-\nWei Chao, Brian Okorn, Jia Deng, and Dieter Fox. \u201cIfor:\nIterative flow minimization for robotic object rearrange-\nment\u201d. In: Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition. 2022,\npp. 14787\u201314797.\n[21]\nWeiyu Liu, Yilun Du, Tucker Hermans, Sonia Chernova,\nand Chris Paxton. StructDiffusion: Language-Guided\nCreation of Physically-Valid Structures using Unseen\nObjects. 2023. arXiv: 2211.04604 [cs.RO].\n[22]\nSriram Yenamandra, Arun Ramachandran, Mukul\nKhanna, et al. \u201cThe HomeRobot Open Vocab Mobile\nManipulation Challenge\u201d. In: Thirty-seventh Conference\non Neural Information Processing Systems: Competition\nTrack. 2023. URL: https : / / aihabitat . org / challenge /\n2023 homerobot ovmm/.\n[23]\nAndrew Melnik, Michael B\u00a8uttner, Leon Harz, Lyon\nBrown, Gora Chand Nandi, Arjun PS, Gaurav Kumar\nYadav, Rahul Kala, and Robert Haschke. \u201cUniTeam:\nOpen Vocabulary Mobile Manipulation Challenge\u201d. In:\narXiv preprint arXiv:2312.08611 (2023).\n[24]\nLuca Medeiros. Lang Segment Anything. https://github.\ncom/luca-medeiros/lang-segment-anything. 2023.\n[25]\nSriram Yenamandra, Arun Ramachandran, Karmesh\nYadav, et al. \u201cHomeRobot: Open Vocabulary Mobile\nManipulation\u201d. In: arXiv preprint arXiv:2306.11565\n(2023). URL: https://github.com/facebookresearch/home-\nrobot.\n[26]\nBenjamin Bolte, Austin Wang, Jimmy Yang, Mustafa\nMukadam, Mrinal Kalakrishnan, and Chris Paxton. USA-\nNet: Unified Semantic and Affordance Representations\nfor Robot Memory. 2023. arXiv: 2304.12164 [cs.RO].\n[27]\nNur Muhammad Mahi Shafiullah, Chris Paxton, Lerrel\nPinto, Soumith Chintala, and Arthur Szlam. CLIP-Fields:\nWeakly Supervised Semantic Fields for Robotic Memory.\n2023. arXiv: 2210.05663 [cs.RO].\n[28]\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, et al.\n\u201cSegment Anything\u201d. In: ICCV. 2023, pp. 4015\u20134026.\n[29]\nDavid Rozenberszki, Or Litany, and Angela Dai.\nLanguage-Grounded Indoor 3D Semantic Segmentation\nin the Wild. 2022. arXiv: 2204.07761 [cs.CV].\n[30]\nJustin Kerr, Chung Min Kim, Ken Goldberg, Angjoo\nKanazawa, and Matthew Tancik. \u201cLerf: Language embed-\nded radiance fields\u201d. In: Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision. 2023,\npp. 19729\u201319739.\n[31]\nMatthew Chang, Theophile Gervet, Mukul Khanna, Sri-\nram Yenamandra, Dhruv Shah, So Yeon Min, Kavit Shah,\nChris Paxton, Saurabh Gupta, Dhruv Batra, et al. \u201cGoat:\nGo to any thing\u201d. In: arXiv preprint arXiv:2311.06430\n(2023).\n[32]\nChenguang Huang, Oier Mees, Andy Zeng, and Wolfram\nBurgard. \u201cAudio Visual Language Maps for Robot Nav-\nigation\u201d. In: arXiv preprint arXiv:2303.07522 (2023).\n[33]\nSantiago Garrido, Luis Moreno, Mohamed Abderrahim,\nand Fernando Martin. \u201cPath planning for mobile robot\nnavigation using voronoi diagram and fast marching\u201d. In:\n2006 IEEE/RSJ International Conference on Intelligent\nRobots and Systems. IEEE. 2006, pp. 2376\u20132381.\n[34]\nSudeep Dasari, Abhinav Gupta, and Vikash Kumar.\nLearning Dexterous Manipulation from Exemplar Object\nTrajectories and Pre-Grasps. 2023. arXiv: 2209.11221\n[cs.RO].\n[35]\nOpenAI. GPT-4 Technical Report. 2023. arXiv: 2303.\n08774 [cs.CL].\n[36]\nArsalan Mousavian, Alexander Toshev, Marek Fi\u02c7ser,\nJana Ko\u02c7seck\u00b4a, Ayzaan Wahid, and James Davidson.\n\u201cVisual Representations for Semantic Target Driven\nNavigation\u201d. In: 2019 International Conference on\nRobotics and Automation (ICRA). IEEE. May 2019,\npp. 8846\u20138852.\n[37]\nValts Blukis, Chris Paxton, Dieter Fox, Animesh Garg,\nand Yoav Artzi. \u201cA persistent spatial semantic repre-\nsentation for high-level natural language instruction\nexecution\u201d. In: Conference on Robot Learning. PMLR.\n2022, pp. 706\u2013717.\n[38]\nSo Yeon Min, Devendra Singh Chaplot, Pradeep Raviku-\nmar, Yonatan Bisk, and Ruslan Salakhutdinov. \u201cFilm:\nFollowing instructions in language with modular meth-\nods\u201d. In: arXiv preprint arXiv:2110.07342 (2021).\n[39]\nMatt Deitke, Dhruv Batra, Yonatan Bisk, Tommaso\nCampari, Angel X Chang, Devendra Singh Chaplot,\nChangan Chen, Claudia P\u00b4erez D\u2019Arpino, Kiana Ehsani,\nAli Farhadi, et al. \u201cRetrospectives on the embodied ai\nworkshop\u201d. In: arXiv preprint arXiv:2210.06849 (2022).\n[40]\nMatt Deitke, Dustin Schwenk, Jordi Salvador, Luca\nWeihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt,\nKiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi.\n\u201cObjaverse: A universe of annotated 3d objects\u201d. In:\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition. 2023, pp. 13142\u201313153.\n[41]\nHuy Ha and Shuran Song. Semantic Abstraction:\nOpen-World 3D Scene Understanding from 2D Vision-\nLanguage Models. 2022. arXiv: 2207.11514 [cs.CV].\n[42]\nChenguang Huang, Oier Mees, Andy Zeng, and Wolfram\nBurgard. \u201cVisual language maps for robot navigation\u201d.\nIn: 2023 IEEE International Conference on Robotics\nand Automation (ICRA). IEEE. 2023, pp. 10608\u201310615.\n[43]\nBoyuan Chen, Fei Xia, Brian Ichter, Kanishka Rao,\nKeerthana Gopalakrishnan, Michael S. Ryoo, Austin\nStone, and Daniel Kappler. \u201cOpen-vocabulary Queryable\nScene Representations for Real World Planning\u201d. In:\narXiv preprint arXiv:2209.09874. 2022.\n[44]\nKrishna Murthy Jatavallabhula, Alihusein Kuwajerwala,\nQiao Gu, Mohd Omama, Tao Chen, Shuang Li, Ganesh\nIyer, Soroush Saryazdi, Nikhil Keetha, Ayush Tewari,\net al. \u201cConceptfusion: Open-set multimodal 3d mapping\u201d.\nIn: arXiv preprint arXiv:2302.07241 (2023).\n[45]\nDhruv Shah, Ajay Sridhar, Nitish Dashora, Kyle Sta-\nchowicz, Kevin Black, Noriaki Hirose, and Sergey\nLevine. \u201cViNT: A Foundation Model for Visual Navi-\ngation\u201d. In: 7th Annual Conference on Robot Learning\n(CoRL). 2023.\n[46]\nQiao Gu, Alihusein Kuwajerwala, Sacha Morin, Krishna\nMurthy Jatavallabhula, Bipasha Sen, Aditya Agarwal,\nCorban Rivera, William Paul, Kirsty Ellis, Rama Chel-\nlappa, et al. \u201cConceptgraphs: Open-vocabulary 3d scene\ngraphs for perception and planning\u201d. In: arXiv preprint\narXiv:2309.16650 (2023).\n[47]\nAbhinav\nGupta,\nAdithyavairavan\nMurali,\nDhiraj\nPrakashchand Gandhi, and Lerrel Pinto. \u201cRobot Learning\nin Homes: Improving Generalization and Reducing\nDataset Bias\u201d. In: Advances in Neural Information\nProcessing Systems 31 (2018), pp. 9094\u20139104.\n[48]\nJeffrey Mahler, Jacky Liang, Sherdil Niyaz, Michael\nLaskey, Richard Doan, Xinyu Liu, Juan Aparicio Ojea,\nand Ken Goldberg. \u201cDex-Net 2.0: Deep Learning to Plan\nRobust Grasps with Synthetic Point Clouds and Analytic\nGrasp Metrics\u201d. In: Robotics: Science and Systems (RSS).\n2017.\n[49]\nJeffrey Mahler, Matthew Matl, Xinyu Liu, Albert Li,\nDavid Gealy, and Ken Goldberg. Dex-Net 3.0: Com-\nputing Robust Robot Vacuum Suction Grasp Targets in\nPoint Clouds using a New Analytic Model and Deep\nLearning. 2018. arXiv: 1709.06670 [cs.RO].\n[50]\nDmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian\nIbarz, Alexander Herzog, Eric Jang, Deirdre Quillen,\nEthan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke,\net al. \u201cQT-Opt: Scalable deep reinforcement learning for\nvision-based robotic manipulation\u201d. In: arXiv preprint\narXiv:1806.10293 (2018).\n[51]\nYuzhe Qin, Rui Chen, Hao Zhu, Meng Song, Jing Xu,\nand Hao Su. S4G: Amodal Single-view Single-Shot SE(3)\nGrasp Detection in Cluttered Scenes. 2019. arXiv: 1910.\n14218 [cs.RO].\n[52]\nArsalan Mousavian, Clemens Eppner, and Dieter Fox. 6-\nDOF GraspNet: Variational Grasp Generation for Object\nManipulation. 2019. arXiv: 1905.10520 [cs.CV].\n[53]\nClemens Eppner, Arsalan Mousavian, and Dieter Fox.\n\u201cAcronym: A large-scale grasp dataset based on sim-\nulation\u201d. In: 2021 IEEE International Conference on\nRobotics and Automation (ICRA). IEEE. 2021, pp. 6222\u2013\n6227.\n[54]\nI. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu,\nJ. Tremblay, and D. Fox. \u201cProgprompt: Generating\nsituated robot task plans using large language models\u201d.\nIn: 2023 IEEE International Conference on Robotics\nand Automation (ICRA). 2023, p. 11523.\n[55]\nMohit Shridhar, Lucas Manuelli, and Dieter Fox.\n\u201cPerceiver-Actor: A multi-task transformer for robotic\nmanipulation\u201d. In: CoRL. PMLR. 2023, pp. 785\u2013799.\n[56]\nPriyam Parashar, Jay Vakil, Sam Powers, and Chris Pax-\nton. \u201cSpatial-Language Attention Policies for Efficient\nRobot Learning\u201d. In: arXiv preprint arXiv:2304.11235\n(2023).\n[57]\nNur Muhammad Shafiullah, Zichen Cui, Ariuntuya Arty\nAltanzaya, and Lerrel Pinto. \u201cBehavior Transformers:\nCloning k modes with one stone\u201d. In: Advances in neural\ninformation processing systems 35 (2022), pp. 22955\u2013\n22968.\n[58]\nZichen Jeff Cui, Yibin Wang, Nur Muhammad Mahi\nShafiullah, and Lerrel Pinto. From Play to Policy:\nConditional Behavior Generation from Uncurated Robot\nData. 2022. arXiv: 2210.10047 [cs.RO].\n[59]\nTheophile Gervet, Zhou Xian, Nikolaos Gkanatsios,\nand Katerina Fragkiadaki. \u201cAct3D: 3D Feature Field\nTransformers for Multi-Task Robotic Manipulation\u201d. In:\nConference on Robot Learning. PMLR. 2023, pp. 3949\u2013\n3965.\n[60]\nWenxuan Zhou, Bowen Jiang, Fan Yang, Chris Paxton,\nand David Held. \u201cLearning Hybrid Actor-Critic Maps\nfor 6D Non-Prehensile Manipulation\u201d. In: arXiv preprint\narXiv:2305.03942 (2023).\n[61]\nMohit Shridhar, Lucas Manuelli, and Dieter Fox. \u201cCLI-\nPort: What and where pathways for robotic manipula-\ntion\u201d. In: CoRL. PMLR. 2022, pp. 894\u2013906.\n[62]\nCorey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar,\nJonathan Tompson, Sergey Levine, and Pierre Sermanet.\n\u201cLearning latent plans from play\u201d. In: CoRL. PMLR.\n2020, pp. 1113\u20131132.\n[63]\nCorey Lynch and Pierre Sermanet. \u201cLanguage Condi-\ntioned Imitation Learning over Unstructured Data\u201d. In:\nRobotics: Science and Systems (2021). URL: https://arxiv.\norg/abs/2005.07648.\n[64]\nWenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li,\nJiajun Wu, and Li Fei-Fei. \u201cVoxPoser: Composable 3D\nValue Maps for Robotic Manipulation with Language\nModels\u201d. In: CoRL. 2023.\n[65]\nJacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol\nHausman, Brian Ichter, Pete Florence, and Andy Zeng.\n\u201cCode as Policies: Language model programs for em-\nbodied control\u201d. In: icra. IEEE. 2023, pp. 9493\u20139500.\n[66]\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar,\nChaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anand-\nkumar. \u201cVoyager: An Open-Ended Embodied Agent\nwith Large Language Models\u201d. In: arXiv preprint arXiv:\nArxiv-2305.16291 (2023).\n[67]\nIshika Singh, Valts Blukis, Arsalan Mousavian, Ankit\nGoyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse\nThomason, and Animesh Garg. \u201cProgPrompt: Generat-\ning Situated Robot Task Plans using Large Language\nModels\u201d. In: ICRA. IEEE. 2023, pp. 11523\u201311530.\n[68]\nNaoki Yokoyama, Alex Clegg, Joanne Truong, Eric\nUndersander, Tsung-Yen Yang, Sergio Arnaud, Sehoon\nHa, Dhruv Batra, and Akshara Rai. \u201cASC: Adaptive\nSkill Coordination for Robotic Mobile Manipulation\u201d.\nIn: arXiv preprint arXiv:2304.00410 (2023).\n[69]\nAustin Stone, Ted Xiao, Yao Lu, et al. Open-World\nObject Manipulation using Pre-trained Vision-Language\nModels. 2023. arXiv: 2303.00905 [cs.RO].\n[70]\nNaoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi,\nJun Takamatsu, and Katsushi Ikeuchi. \u201cGPT-4V(ision)\nfor Robotics: Multimodal Task Planning from Human\nDemonstration\u201d. In: arXiv preprint arXiv:2311.12015\n(2023).\n[71]\nKrishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-\nChakra, Ian Reid, and Niko Suenderhauf. \u201cSayplan:\nGrounding large language models using 3d scene\ngraphs for scalable task planning\u201d. In: arXiv preprint\narXiv:2307.06135 (2023).\n[72]\nCharles C Kemp, Aaron Edsinger, Henry M Clever,\nand Blaine Matulevich. \u201cThe design of Stretch: A\ncompact, lightweight mobile manipulator for indoor\nhuman environments\u201d. In: 2022 International Conference\non Robotics and Automation (ICRA). IEEE. 2022,\npp. 3150\u20133157.\n[73]\nNils Reimers and Iryna Gurevych. Sentence-BERT:\nSentence Embeddings using Siamese BERT-Networks.\n2019. arXiv: 1908.10084 [cs.CL].\n[74]\nBen Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng.\n\u201cNerf: Representing scenes as neural radiance fields for\nview synthesis\u201d. In: European Conference on Computer\nVision (ECCV) 65.1 (2020), pp. 99\u2013106.\nAPPENDIX A\nDESCRIPTION OF ALTERNATE SYSTEM COMPONENTS\nIn this section, we provide more details about the alternate\nsystem components that we evaluated in Section III-B.\nAlternate semantic navigation strategies: We evaluate the\nfollowing semantic memory modules:\n\u2022 VoxelMap [25]: VoxelMap converts every detected object\nto a semantic vector and stores such info into an associated\nvoxel. Occupied voxels serve as an obstacle map.\n\u2022 CLIP-Fields [27]: CLIP-Fields converts a sequence of posed\nRGB-D images to a semantic vector field by using open-\nlabel object detectors and semantic language embedding\nmodels. The result associates each point in the space with\ntwo semantic vectors, one generated via a VLM [9], and\nanother generated via a language model [73], which is then\nembedded into a neural field [74].\n\u2022 USA-Net [26]: USA-Net generates multi-scale CLIP fea-\ntures and embeds them in a neural field that also doubles\nas a signed distance field. As a result, a single model can\nsupport both object retrieval and navigation.\nWe compare them in the same three environments with a fixed\nset of queries, the results of which are shown in Figure 5.\nAlternate grasping strategies: Similarly, we compare multiple\ngrasping strategies to find out the best grasping strategy for\nour method.\n\u2022 AnyGrasp [19]: AnyGrasp is a single view RGB-D based\ngrasping model. It is trained on the GraspNet dataset which\ncontains 1B grasp labels.\n\u2022 Open Graspness [19]: Since the AnyGrasp model is free\nbut not open source, we use an open licensed baseline\ntrained on the same dataset.\n\u2022 Contact-GraspNet [16]: We use Contact-GraspNet as a\nprior work baseline, which is trained on the Acronym [53]\ndataset. One limitation of Contact-GraspNet is that it was\ntrained on a fixed camera view for a tabletop setting. As a\nresult, in our application with a moving camera and arbitrary\nlocations, it failed to give us meaningful grasps.\n\u2022 Top-down grasp [25]: As a heuristic based baseline, we\ncompare with the top-down heuristic grasp provided in the\nHomeRobot project.\nAPPENDIX B\nSCANNET200 TEXT QUERIES\nTo\ndetect\nobjects\nin\na\ngiven\nhome\nenvironment\nusing\nOWL-ViT,\nwe\nuse\nthe\nScannet200\nlabels.\nThe\nfull label set is here: [\u2019shower head\u2019, \u2019spray\u2019,\n\u2019inhaler\u2019, \u2019guitar case\u2019, \u2019plunger\u2019,\n\u2019range hood\u2019, \u2019toilet paper dispenser\u2019,\n\u2019adapter\u2019, \u2019soy sauce\u2019, \u2019pipe\u2019, \u2019bottle\u2019,\n\u2019door\u2019, \u2019scale\u2019, \u2019paper towel\u2019, \u2019paper\ntowel roll\u2019, \u2019stove\u2019, \u2019mailbox\u2019,\n\u2019scissors\u2019, \u2019tape\u2019, \u2019bathroom stall\u2019,\n\u2019chopsticks\u2019, \u2019case of water bottles\u2019,\n\u2019hand sanitizer\u2019, \u2019laptop\u2019, \u2019alcohol\ndisinfection\u2019, \u2019keyboard\u2019, \u2019coffee\nmaker\u2019, \u2019light\u2019, \u2019toaster\u2019, \u2019stuffed\nanimal\u2019, \u2019divider\u2019, \u2019clothes dryer\u2019,\n\u2019toilet seat cover dispenser\u2019, \u2019file\ncabinet\u2019, \u2019curtain\u2019, \u2019ironing board\u2019,\n\u2019fire extinguisher\u2019, \u2019fruit\u2019, \u2019object\u2019,\n\u2019blinds\u2019, \u2019container\u2019, \u2019bag\u2019, \u2019oven\u2019,\n\u2019body wash\u2019, \u2019bucket\u2019, \u2019cd case\u2019, \u2019tv\u2019,\n\u2019tray\u2019, \u2019bowl\u2019, \u2019cabinet\u2019, \u2019speaker\u2019,\n\u2019crate\u2019, \u2019projector\u2019, \u2019book\u2019, \u2019school\nbag\u2019, \u2019laundry detergent\u2019, \u2019mattress\u2019,\n\u2019bathtub\u2019, \u2019clothes\u2019, \u2019candle\u2019, \u2019basket\u2019,\n\u2019glass\u2019, \u2019face wash\u2019, \u2019notebook\u2019, \u2019purse\u2019,\n\u2019shower\u2019, \u2019power outlet\u2019, \u2019trash bin\u2019,\n\u2019paper bag\u2019, \u2019water dispenser\u2019, \u2019package\u2019,\n\u2019bulletin board\u2019, \u2019printer\u2019, \u2019windowsill\u2019,\n\u2019disinfecting wipes\u2019, \u2019bookshelf\u2019,\n\u2019recycling bin\u2019, \u2019headphones\u2019, \u2019dresser\u2019,\n\u2019mouse\u2019, \u2019shower gel\u2019, \u2019dustpan\u2019, \u2019cup\u2019,\n\u2019storage organizer\u2019, \u2019vacuum cleaner\u2019,\n\u2019fireplace\u2019, \u2019dish rack\u2019, \u2019coffee kettle\u2019,\n\u2019fire alarm\u2019, \u2019plants\u2019, \u2019rag\u2019, \u2019can\u2019,\n\u2019piano\u2019, \u2019bathroom cabinet\u2019, \u2019shelf\u2019,\n\u2019cushion\u2019, \u2019monitor\u2019, \u2019fan\u2019, \u2019tube\u2019,\n\u2019box\u2019, \u2019blackboard\u2019, \u2019ball\u2019, \u2019bicycle\u2019,\n\u2019guitar\u2019, \u2019trash can\u2019, \u2019hand sanitizers\u2019,\n\u2019paper towel dispenser\u2019, \u2019whiteboard\u2019,\n\u2019bin\u2019, \u2019potted plant\u2019, \u2019tennis\u2019,\n\u2019soap dish\u2019, \u2019structure\u2019, \u2019calendar\u2019,\n\u2019dumbbell\u2019, \u2019fish oil\u2019, \u2019paper cutter\u2019,\n\u2019ottoman\u2019, \u2019stool\u2019, \u2019hand wash\u2019, \u2019lamp\u2019,\n\u2019toaster oven\u2019, \u2019music stand\u2019, \u2019water\nbottle\u2019, \u2019clock\u2019, \u2019charger\u2019, \u2019picture\u2019,\n\u2019bascketball\u2019, \u2019sink\u2019, \u2019microwave\u2019,\n\u2019screwdriver\u2019, \u2019kitchen counter\u2019, \u2019rack\u2019,\n\u2019apple\u2019, \u2019washing machine\u2019, \u2019suitcase\u2019,\n\u2019ladder\u2019, \u2019ping pong ball\u2019, \u2019window\u2019,\n\u2019dishwasher\u2019, \u2019storage container\u2019,\n\u2019toilet paper holder\u2019, \u2019coat rack\u2019, \u2019soap\ndispenser\u2019, \u2019refrigerator\u2019, \u2019banana\u2019,\n\u2019counter\u2019, \u2019toilet paper\u2019, \u2019mug\u2019, \u2019marker\npen\u2019, \u2019hat\u2019, \u2019aerosol\u2019, \u2019luggage\u2019,\n\u2019poster\u2019, \u2019bed\u2019, \u2019cart\u2019, \u2019light switch\u2019,\n\u2019backpack\u2019, \u2019power strip\u2019, \u2019baseball\u2019,\n\u2019mustard\u2019, \u2019bathroom vanity\u2019, \u2019water\npitcher\u2019, \u2019closet\u2019, \u2019couch\u2019, \u2019beverage\u2019,\n\u2019toy\u2019, \u2019salt\u2019, \u2019plant\u2019, \u2019pillow\u2019, \u2019broom\u2019,\n\u2019pepper\u2019, \u2019muffins\u2019, \u2019multivitamin\u2019,\n\u2019towel\u2019, \u2019storage bin\u2019, \u2019nightstand\u2019,\n\u2019radiator\u2019, \u2019telephone\u2019, \u2019pillar\u2019, \u2019tissue\nbox\u2019, \u2019vent\u2019, \u2019hair dryer\u2019, \u2019ledge\u2019,\n\u2019mirror\u2019, \u2019sign\u2019, \u2019plate\u2019, \u2019tripod\u2019,\n\u2019chair\u2019, \u2019kitchen cabinet\u2019, \u2019column\u2019,\n\u2019water cooler\u2019, \u2019plastic bag\u2019, \u2019umbrella\u2019,\n\u2019doorframe\u2019, \u2019paper\u2019, \u2019laundry hamper\u2019,\n\u2019food\u2019, \u2019jacket\u2019, \u2019closet door\u2019, \u2019computer\ntower\u2019, \u2019stairs\u2019, \u2019keyboard piano\u2019,\n\u2019person\u2019, \u2019table\u2019, \u2019machine\u2019, \u2019projector\nscreen\u2019, \u2019shoe\u2019].\nAPPENDIX C\nSAMPLE OBJECTS FROM OUR TRIALS\nDuring our experiments, we tried to sample objects that\ncan plausibly be manipulated by the Hello Robot: Stretch\ngripper from the home environments. As a result, OK-Robot\nencountered a large variety of objects with different shapes\nand visual features. A subsample of such objects are presented\nin the Figures 9, 10.\nAPPENDIX D\nSAMPLE HOME ENVIRONMENTS FROM OUR TRIALS\nWe show snapshots from a subset of home environments\nwhere we evaluated our method in Figures 11. Additionally, in\nFigure 12 we show the two home environments in Pittsburgh,\nPA, and Fremont, CA, where we reproduced the OK-Robot\nsystem.\nAPPENDIX E\nLIST OF HOME EXPERIMENTS\nA full list of experiments in homes can be found in Table I.\nArm smartphone holder\nGray toy dragon\nToy plant\nTangled earphones\nWhite pretzel\nPlaying cards\nBlue gloves\nToy cactus\nToy grapes\nMedicine bottles\nGrey rag\nBlue hair oil bottle\nBlue pretzel pack\nToothpaste\nWhite shirt\nBlue body wash\nFig. 9: Sample objects on our home experiments, sampled from each home environment, which OK-Robot was able to pick and drop\nsuccessfully.\nPurple strap\nBrown trail mix bag\nWhite Apple bag\nSteel wool\nBlue eyeglass case\nFlu!y headbands\nYogurt drinks\nLotion pump\nTranslucent grey cup\nBlue bag\nGold wrapped chocolate\nBlack head band\nBlue hair gel tube\nYellow ginger paste packet\nBlack face wash\nSmall hand sanitizer\nFig. 10: Sample objects on our home experiments, sampled from each home environment, which OK-Robot failed to pick up successfully.\n\u201cpurple lightbulb box to sofa chair\u201d\n\u201ccooking oil bottle to marble surface\u201d\n\u201cpower adapter to chair\u201d\n\u201cblue gloves to sink\u201d\n\u201cpurple shampoo to white rack\u201d\n\u201cmilk bottle to chair\u201d\n\u201cherbal tea can to box\u201d\n\u201cMcDonalds paper bag to stove\u201d\nFig. 11: Eight out of the ten New York home environments in which we evaluated OK-Robot. In each figure caption, we show the queries\nthat the system is being evaluated on.\nReproducibility experiments in Pittsburgh, PA\nReproducibility experiments Fremont, CA\nFig. 12: Home environments outside of New York where we successfully reproduced OK-Robot. We ensured that OK-Robot can function\nin these homes by trying pick-and-drop on a number of objects in the homes.\nTABLE I: A list of all tasks in the home enviroments, along with their categories and success rates out of 10 trials.\nPick object\nPlace location\nResult\nHome 1\nCleanup level: none\nsilver cup\nwhite table\nSuccess\nblue eye glass case\nchair\nSuccess\nprinted paper cup, coffee cup [white table]\nManipulation failure\nsmall red and white medication\nChair\nSuccess\npower adapter\nGrey Bed\nSuccess\nwrapped paper\nNavigation failure\nblue body wash\nstudy table\nSuccess\nblue air spray\nwhite table\nSuccess\nblack face wash\nManipulation failure\nyellow face wash\nchair\nSuccess\nbody spray\nNavigation failure\nsmall hand sanitizer\nManipulation failure\nblue inhaler device(window)\nwhite table\nSuccess\ninhaler box(window)\ndust bin\nSuccess\nmultivitamin container\nNavigation failure\nred towel\nwhite cloth bin (air conditioner)\nSuccess\nwhite shirt\nwhite cloth bin (air conditioner)\nSuccess\nCleanup level: low\nsilver cup\nwhite table\nSuccess\nblue eye glass case\nNavigation failure\nprinted paper cup, coffee cup [white table]\ndust bin\nSuccess\nsmall red and white medication\nChair\nSuccess\npower adapter\nNavigation failure\nblue body wash\nwhite table\nSuccess\nblue air spray\nwhite table\nSuccess\nyellow face wash\nwhite table\nSuccess\nsmall hand sanitizer\nstudy table\nSuccess\nblue inhaler device(window)\nManipulation failure\ninhaler box(window)\ndust bin\nSuccess\nred towel\nwhite cloth bin(air conditioner)\nSuccess\nwhite shirt\nwhite cloth bin(air conditioner)\nSuccess\nCleanup level: high\nsilver cup\nwhite table\nSuccess\nprinted paper cup, coffee cup [white table]\ndust bin\nSuccess\nblue body wash\nwhite table\nSuccess\nblue air spray\nwhite table\nSuccess\nyellow face wash\nManipulation failure\nsmall hand sanitizer\nManipulation failure\ninhaler box(window)\ndust bin\nSuccess\nwhite shirt\nwhite cloth bin(air conditioner)\nSuccess\nHome 2\nCleanup level: None\nfanta can\ndust bin\nSuccess\ntennis ball\nsmall red shopping bag\nSuccess\nblack head band [bed]\nManipulation failure\npurple shampoo bottle\nwhite rack\nSuccess\ntoothpaste\nsmall red shopping bag\nSuccess\nContinued on the next page\nPick object\nPlace location\nResult\norange packaging\ndust bin\nSuccess\ngreen hair cream jar [white rack]\nNavigation failure\ngreen detergent pack [white rack]\nwhite table\nSuccess\nblue moisturizer [white rack]\nNavigation failure\ngreen plastic cover\nNavigation failure\nstorage container\nManipulation failure\nblue hair oil bottle\nwhite rack\nSuccess\nblue pretzels pack\nwhite rack\nSuccess\nblue hair gel tube\nManipulation failure\nred bottle [white rack]\nbrown desk\nSuccess\nblue bottle [air conditioner]\nwhite cloth bin(air conditioner)\nSuccess\nwallet\nManipulation failure\nCleanup level: low\nfanta can\nblack trash can\nSuccess\ntennis ball\nred target bag\nSuccess\nblack head band [bed]\nred target bag\nSuccess\npurple shampoo bottle\nred target bag\nSuccess\ntoothpaste\nred target bag\nSuccess\norange packaging\nblack trash can\nSuccess\ngreen detergent pack [white rack]\nManipulation failure\nblue moisturizer [white rack]\nNavigation failure\nblue hair oil bottle\nwhite rack\nSuccess\nblue pretzels pack\nwhite rack\nSuccess\nwallet\nManipulation failure\nCleanup level: high\nfanta can\nblack trash can\nSuccess\npurple shampoo bottle\nsmall red shopping bag\nSuccess\norange packaging\nblack trash can\nSuccess\nblue moisturizer [white rack]\nwhite rack\nSuccess\nblue hair oil bottle\nManipulation failure\nblue hair gel tube\ndust bin\nSuccess\nred bottle [white rack]\ntarget bag\nPlacing failure\nblue bottle [air conditioner]\nwhite cloth bin(air conditioner)\nSuccess\nHome 3\nCleanup level: none\napple\nwhite plate\nSuccess\nice cream\nwhite and green bag\nSuccess\ngreen lime juice bottle\nred basket\nSuccess\nyellow packet\nManipulation failure\nred packet\nManipulation failure\norange can\ncard board box\nSuccess\ncooking oil bottle\nManipulation failure\npasta sauce\nManipulation failure\norange box [stove]\nManipulation failure\ngreen bowl\nsink\nSuccess\nwashing gloves\ngreen bag [card board box]\nSuccess\nsmall oregano bottle\nred basket\nSuccess\nyellow noodles packet [stove]\nred basket\nSuccess\nblue dish wash bottle\ncard board box\nSuccess\nscrubber\nNavigation failure\ndressing salad bottle\nNavigation failure\nContinued on the next page\nPick object\nPlace location\nResult\nCleanup level: low\napple\nwhite plate\nSuccess\nice cream\nred basket\nSuccess\ngreen lime juice bottle\nred basket\nSuccess\nyellow packet\ngreen bag\nSuccess\nred packet\nManipulation failure\norange can\ncard board box\nSuccess\ncooking oil bottle\nmarble surface [red basket]\nSuccess\ngreen bowl\nManipulation failure\nwashing gloves\nsink\nSuccess\nsmall oregano bottle\nred basket\nSuccess\nyellow noodles packet [stove]\nManipulation failure\nblue dish wash bottle\ncard board box\nSuccess\nCleanup level: high\napple\nwhite plate\nSuccess\nice cream\nred basket\nSuccess\ngreen lime juice bottle\nred basket\nSuccess\norange can\ncard board box\nSuccess\ncooking oil bottle\nManipulation failure\nwashing gloves\nsink\nSuccess\nsmall oregano bottle\nred basket\nSuccess\nyellow noodles packet [stove]\nred basket\nSuccess\nblue dish wash bottle\ncard board box\nSuccess\nHome 4\nCleanup level: none\npepsi\nblack chair\nSuccess\nbirdie\ncloth bin\nSuccess\nblack hat\nNavigation failure\nowl like wood carving\nbed\nSuccess\nred inhaler\nManipulation failure\nblack sesame seeds\nManipulation failure\nbanana\nManipulation failure\nloose-leaf herbal tea jar\nblack chair\nSuccess\nred pencil sharpener\nNavigation failure\nfast-food French fries container\nblue shopping bag [metal drying rack]\nPlacing failure\nmilk\nplastic storage drawer unit\nSuccess\nsocks[bed]\nNavigation failure\npurple gloves\nManipulation failure\ntarget bag\ncloth bin\nSuccess\nmuffin\ngrey bed\nSuccess\ntissue paper\ntable\nSuccess\ngrey eyeglass box\nManipulation failure\nCleanup level: low\npepsi\nbasket\nSuccess\nbirdie\nwhite drawer\nSuccess\nowl like wood carving\nNavigation failure\nred inhaler\nplastic storage drawer unit\nSuccess\nblack sesame seeds\nbed\nSuccess\nloose-leaf herbal tea jar\ntable\nSuccess\nfast-food French fries container\nchair\nSuccess\nContinued on the next page\nPick object\nPlace location\nResult\nmilk\nchair\nSuccess\npurple gloves\nbasket\nSuccess\ntarget bag\nbasket\nPlacing failure\nmuffin\ntable\nSuccess\ntissue paper\nManipulation failure\ngrey eyeglass box\nNavigation failure\nCleanup level: high\npepsi\nbasket\nSuccess\nbirdie\nbed\nSuccess\nred inhaler\nplastic storage drawer unit\nSuccess\nblack sesame seeds\ndesk\nSuccess\nbanana\nManipulation failure\nloose-leaf herbal tea jar\nManipulation failure\nmilk\nchair\nSuccess\npurple gloves\nbasket\nSuccess\ntarget bag\nbasket\nSuccess\nmuffin\nbed\nSuccess\nHome 5\nCleanup level: none\ntiger balm topical ointment\nNavigation failure\npink shampoo\ntrader joes shapping bag\nSuccess\naveeno sunscreen protective lotion\ntrader joes shapping bag\nSuccess\nsmall yellow nozzle spray\nManipulation failure\nblack hair care spray\nManipulation failure\ngreen hand sanitizer\nManipulation failure\nwhite hand sanitizer\nNavigation failure\nwhite bowl [ketchup]\nblack sofa chair\nSuccess\nblue bowl\nManipulation failure\nblue sponge\ntrader joes shapping bag\nSuccess\nketchup\nManipulation failure\nwhite salt\nManipulation failure\nblack pepper\nblack drawer\nSuccess\nblue bottle\nNavigation failure\npurple light bulb box\ntrader joes shopping bag\nSuccess\nwhite plastic bag\nbed\nSuccess\nrag\nwhite rack\nSuccess\nCleanup level: low\npink shampoo\nNavigation failure\naveeno sunscreen protective lotion\nManipulation failure\nsmall yellow nozzle spray\nManipulation failure\nwhite bowl [ketchup]\nblack sofa chair\nSuccess\nblue sponge\nbed\nSuccess\nketchup\ntrader joes shopping bag\nSuccess\nwhite salt\ntrader joes shopping bag\nSuccess\nblack pepper\nNavigation failure\nblue bottle\nblack sofa chair\nSuccess\npurple light bulb box\nManipulation failure\nrag\nwhite rack\nSuccess\nCleanup level: high\npink shampoo\ntrader joes shopping bag\nSuccess\nContinued on the next page\nPick object\nPlace location\nResult\ngreen hand sanitizer\nblack sofa chair\nSuccess\nwhite bowl [ketchup]\nManipulation failure\nblue sponge\nbed\nSuccess\nketchup\nblack drawer\nSuccess\nwhite salt\nwhite drawer\nSuccess\npurple light bulb box\ntrader joes shopping bag\nSuccess\nrag\nblack sofa chair\nSuccess\nHome 6\nCleanup level: none\ntranslucent grey cup\nManipulation failure\ngreen mouth spray box\nstove\nSuccess\ngreen eyeglass container\nchair\nSuccess\nblue bag\nManipulation failure\nblack burn ointment box\nNavigation failure\nwhite vitamin bottle\nNavigation failure\nMcDonald\u2019s paper bag\nstove\nSuccess\npurple medicine packaging\nchair\nSuccess\ngrey rag\nsink\nSuccess\nsparkling water can [sink]\ncountertop\nSuccess\ngold wrapped chocolate\nManipulation failure\nlemon tea carton\ntable\nSuccess\nmetallic golden beverage can\ntable\nSuccess\nred bottle\ntable\nSuccess\ntea milk bottle\nNavigation failure\nnyu water bottle [sink]\ntable\nSuccess\nwhite hand wash\nNavigation failure\nCleanup level: low\ntranslucent grey cup\nNavigation failure\ngreen mouth spray box\nManipulation failure\nblue bag\nbrown box\nSuccess\nblack burn ointment box\nbrown box\nSuccess\nMcDonald\u2019s paper bag\nNavigation failure\ngrey rag\nsink\nSuccess\nsparkling water can [sink]\nchair\nSuccess\nlemon tea carton\nstove\nSuccess\nmetallic golden beverage can\nNavigation failure\nred bottle\nbrown box\nSuccess\nnyu water bottle [sink]\ntable\nSuccess\nwhite hand wash\nsink\nSuccess\nCleanup level: high\nblue bag\nbrown box\nSuccess\nblack burn ointment box\nManipulation failure\ngrey rag\nsink\nSuccess\nsparkling water can [sink]\nchair\nSuccess\nlemon tea carton\ntable\nSuccess\nmetallic golden beverage can\nstove\nSuccess\nred bottle\nNavigation failure\nnyu water bottle [sink]\nManipulation failure\nwhite hand wash\nManipulation failure\nHome 7\nContinued on the next page\nPick object\nPlace location\nResult\nCleanup level: none\nblue plastic bag roll\nNavigation failure\ngreen bag\nbasket[window]\nSuccess\ntoy cactus\ndesk\nSuccess\ntoy van\nchair\nSuccess\nbrown medical bandage\nchair\nSuccess\npower adapter\nNavigation failure\nred herbal tea\nbrown cardboard box\nSuccess\napple juice box\nbrown cardboard box\nSuccess\npaper towel\nblue cardboard box\nSuccess\ntoy bear\nbed blanket\nSuccess\nyellow ball\nbed blanket\nSuccess\nblack pants\nbasket[window]\nSuccess\npurple water bottle\ndesk\nSuccess\nblue eyeglass case\nManipulation failure\nbrown toy monkey\nNavigation failure\nblue hardware box [table]\nblue cardboard box\nSuccess\ngreen zandu balm container\nblue cardboard box\nSuccess\nCleanup level: low\ngreen bag\nbasket\nSuccess\ntoy cactus\nbasket\nSuccess\ntoy van\nchair\nSuccess\nbrown medical bandage\nManipulation failure\nred herbal tea\nbrown box\nSuccess\napple juice box\nbrown box\nSuccess\npaper towel\nbasket\nSuccess\ntoy bear\ndesk\nSuccess\npurple water bottle\ndesk\nSuccess\nblue eyeglass case\nManipulation failure\ngreen zandu balm container\nblue cardboard box\nSuccess\nCleanup level: high\ngreen bag\nstool [window]\nSuccess\ntoy cactus\ntable\nSuccess\ntoy van\nwhite basket\nSuccess\nred herbal tea\nbrown cardboard box\nSuccess\napple juice box\nbrown cardboard box\nSuccess\npaper towel\nblue cardboard box\nSuccess\ntoy bear\nwhite basket\nSuccess\nyellow ball\nbed\nSuccess\npurple water bottle\nblack tote bag\nSuccess\ngreen zandu balm container\nblue cardboard box\nSuccess\nHome 8\nCleanup level: none\ncyan air spray\nbrown shelf [sink]\nSuccess\nblue gloves\nkitchen sink\nSuccess\nblue peanut butter\nblack stove\nSuccess\nnutella\ntable\nSuccess\ngreen bag\nbrown shelf [sink]\nSuccess\ngreen bandage box\ntrash can\nSuccess\ngreen detergent\nkitchen sink\nSuccess\nblack \u2018red pepper sauce\u2019\nManipulation failure\nContinued on the next page\nPick object\nPlace location\nResult\nred bag\nchair\nSuccess\nblack bag\nchair\nSuccess\nred spray [brown shelf]\nkitchen countertop\nSuccess\nsteel wool\nManipulation failure\nwhite aerosol\ntrash can\nSuccess\nwhite pretzel\nblack stove\nSuccess\npurple crisp\nkitchen countertop\nSuccess\nplastic bowl\nManipulation failure\nplaying card\nmicrowave\nSuccess\nCleanup level: low\ncyan air apray\nchair\nSuccess\nblue gloves\nsink\nSuccess\nblue peanut butter\nNavigation failure\ngreen bag\nbrown shelf\nSuccess\ngreen bandage box\nbrown shopping bag\nSuccess\ngreen detergent\nmicrowave\nSuccess\nred bag\nManipulation failure\nblack bag\nchair\nSuccess\nwhite aerosol\ntrash can\nSuccess\nwhite pretzel\nblack stove\nSuccess\npurple crisp\nkitchen countertop\nSuccess\nplastic bowl\nManipulation failure\nplaying card\nmicrowave\nSuccess\nCleanup level: high\ncyan air apray\nbrown shelf [sink]\nSuccess\nblue gloves\nstove\nSuccess\nblue peanut butter\nblack stove\nSuccess\ngreen bag\nbrown shelf [sink]\nSuccess\ngreen bandage box\nmicrowave\nSuccess\ngreen detergent\nManipulation failure\nblack bag\nchair\nSuccess\nwhite aerosol\ntable\nSuccess\npurple crisp\nchair\nSuccess\nplaying card\nmicrowave\nSuccess\nHome 9\nCleanup level: none\ntoy grapes\nblack laundry bag\nSuccess\npurple strap\nManipulation failure\nred foggy body spray\nManipulation failure\narm smartphone holder\nbed\nSuccess\nmedicine bottle\nManipulation failure\nyogurt beverage\nNavigation failure\nblue shaving cream can\nNavigation failure\nblue cup\ntable\nSuccess\npurple tape\nManipulation failure\nblack shoe brush\nNavigation failure\nfluffy headband\nManipulation failure\nblack water bottle\nbrown shopping bag\nPlacing failure\nyellow eyeglass case\nblack chair\nSuccess\npaper cup\nManipulation failure\nlotion pump\nManipulation failure\nContinued on the next page\nPick object\nPlace location\nResult\nnasal spray\nManipulation failure\nplastic bag\ntrash basket\nSuccess\nCleanup level: low\ntoy grapes\nManipulation failure\nred foggy body spray\nbrown paper bag\nSuccess\narm smartphone holder\nbrown paper bag\nSuccess\nyogurt beverage\ndesk\nSuccess\nblue shaving cream can\nblack bag\nSuccess\nblue cup\nblack chair\nSuccess\nblack shoe brush\nManipulation failure\nfluffy headband\nNavigation failure\nblack water bottle\nfolded chair\nSuccess\nnasal spray\nNavigation failure\nplastic bag\ntrash basket\nSuccess\nCleanup level: high\nred foggy body spray\nbrown paper bag\nSuccess\narm smartphone holder\nManipulation failure\nyogurt beverage\ndesk\nSuccess\nblue shaving cream can\nblack bag\nSuccess\nblue cup\nblack chair\nSuccess\nblack water bottle\nwhite bed\nSuccess\nnasal spray\nfolded chair\nSuccess\nplastic bag\ntrash basket\nSuccess\nHome 10\nCleanup level: none\ngrey toy dragon\nbed\nSuccess\npurple body spray\nManipulation failure\nhand sanitizer\nshelf\nSuccess\ntoy plant\nbed [shelf]\nSuccess\nbrown trail mix bag\nManipulation failure\nhanging blue shirt\ncloth bin\nSuccess\nwhite apple bag\nManipulation failure\nwhite and pink powder bottle\ntable\nSuccess\ncough syrup bottle\nshelf\nSuccess\ntangled ear phones\noffice chair\nSuccess\nred deodrant stick[table]\nchair\nSuccess\nblack body spray\nchair\nSuccess\nhair treatment medicine bottle\nManipulation failure\ngreen tea package\nchair\nSuccess\nportable speaker [green tea package]\noffice chair\nSuccess\nwooden workout gripper\nNavigation failure\nbrown box\nNavigation failure\nblue bulb adapter\noffice chair\nSuccess\ngame controller\noffice chair\nSuccess\nCleanup level: low\ngrey toy dragon\norange bag\nSuccess\npurple body spray\ntable\nSuccess\nhand sanitizer\nNavigation failure\ntoy plant\nbed\nSuccess\nbrown trail mix bag\nManipulation failure\nContinued on the next page\nPick object\nPlace location\nResult\nwhite and pink powder bottle\nblack chair [bed]\nSuccess\ncough syrup bottle\nshelf [bed]\nSuccess\nred deodrant stick[table]\nbed [rack]\nSuccess\nblack body spray\nrack [bed]\nPlacing failure\ngreen tea package\norange bag\nSuccess\nbrown box\nblack chair [bed]\nSuccess\nblue bulb adapter\nManipulation failure\nCleanup level: high\npurple body spray\norange bag\nSuccess\ntoy plant\nbed\nSuccess\nwhite and pink powder bottle\nNavigation failure\ncough syrup bottle\nshelf [bed]\nSuccess\nred deodrant stick[table]\nNavigation failure\nblack body spray\nblack chair\nSuccess\ngreen tea package\ntable\nSuccess\nblue bulb adapter\nshelf\nSuccess\n"
  },
  {
    "title": "StreamVoice: Streamable Context-Aware Language Modeling for Real-time Zero-Shot Voice Conversion",
    "link": "https://arxiv.org/pdf/2401.11053.pdf",
    "upvote": "8",
    "text": ""
  },
  {
    "title": "UltrAvatar: A Realistic Animatable 3D Avatar Diffusion Model with Authenticity Guided Textures",
    "link": "https://arxiv.org/pdf/2401.11078.pdf",
    "upvote": "6",
    "text": "UltrAvatar: A Realistic Animatable 3D Avatar Diffusion Model with\nAuthenticity Guided Textures\nMingyuan Zhou* 1, Rakib Hyder* 1, Ziwei Xuan 1, Guojun Qi 1,2\n1OPPO US Research Center, InnoPeak Technology, Inc., USA,\n2Westlake University, China\n{mingyuan.zhou,rakib.hyder,ziwei.xuan}@innopeaktech.com,\nguojunq@gmail.com\n\u201cLeonardo \nDiCaprio\u201d\n\u201cAn elder \nwhite woman \nwith delicate \nlines and \nwrinkles\u201d\nFigure 1. UltrAvatar. Our method takes a text prompt or a single image as input to generate realistic animatable 3D Avatars with PBR\ntextures, which are compatible with various rendering engines, our generation results in a wide diversity, high quality, and excellent fidelity.\nAbstract\nRecent advances in 3D avatar generation have gained\nsignificant attentions. These breakthroughs aim to produce\nmore realistic animatable avatars, narrowing the gap be-\ntween virtual and real-world experiences. Most of existing\nworks employ Score Distillation Sampling (SDS) loss, com-\nbined with a differentiable renderer and text condition, to\nguide a diffusion model in generating 3D avatars. How-\never, SDS often generates oversmoothed results with few\nfacial details, thereby lacking the diversity compared with\nancestral sampling. On the other hand, other works gen-\nerate 3D avatar from a single image, where the challenges\nof unwanted lighting effects, perspective views, and inferior\nimage quality make them difficult to reliably reconstruct the\n3D face meshes with the aligned complete textures. In this\npaper, we propose a novel 3D avatar generation approach\ntermed UltrAvatar with enhanced fidelity of geometry, and\nsuperior quality of physically based rendering (PBR) tex-\ntures without unwanted lighting. To this end, the proposed\napproach presents a diffuse color extraction model and an\n* Equal contribution.\nauthenticity guided texture diffusion model. The former re-\nmoves the unwanted lighting effects to reveal true diffuse\ncolors so that the generated avatars can be rendered under\nvarious lighting conditions. The latter follows two gradient-\nbased guidances for generating PBR textures to render di-\nverse face-identity features and details better aligning with\n3D mesh geometry. We demonstrate the effectiveness and\nrobustness of the proposed method, outperforming the state-\nof-the-art methods by a large margin in the experiments.\n1. Introduction\nGenerating 3D facial avatars is of significant interest in the\ncommunities of both computer vision and computer graph-\nics.\nRecent advancements in deep learning have greatly\nenhanced the realism of AI-generated avatars. Although\nmulti-view 3D reconstruction methods, such as Multi-View\nStereo (MVS) [66] and Structure from Motion (SfM) [65],\nhave facilitated avatar generation from multiple images cap-\ntured at various angles, generating realistic 3D avatars from\nfew images, like a single view taken by user or particularly\ngenerated from text prompts, proves significantly challeng-\n1\narXiv:2401.11078v1  [cs.CV]  20 Jan 2024\ning due to the limited visibility, unwanted lighting effects\nand inferior image quality.\nPrevious works have attempted to overcome these chal-\nlenges by leveraging available information contained in\nthe single view image.\nFor example, [24, 46, 79] fo-\ncused on estimating parameters of the 3D Morphable Model\n(3DMM) model by minimizing landmark loss and photo-\nmetric loss, and other approaches train a self-supervised\nnetwork to predict 3DMM parameters [20, 25, 64, 80].\nThese methods are sensitive to occlusions and lighting con-\nditions, leading to susceptible 3DMM parameters or gen-\neration of poor quality textures. Moreover, many existing\nworks [20, 25, 79, 80] rely on prefixed texture basis [31] to\ngenerate facial textures. Although these textures are often\nreconstructed jointly with lighting parameters, the true face\ncolors and skin details are missing in the underlying texture\nbasis and thus are unable to be recovered. Alternatively,\nother works [8, 10, 37, 73] employ neural radiance render-\ning field (NeRF) to generate 3D Avatar, but they are compu-\ntationally demanding and not amenable to mesh-based an-\nimation of 3D avatars. They also may lack photo-realism\nwhen being rendered from previously unobserved perspec-\ntives.\nGenerative models [29, 30, 43, 53, 61, 75] designed for\n3D avatars have shown promising to generate consistent 3D\nmeshes and textures. However, these models do not account\nfor unwanted lighting effects that prevent access to true face\ncolors and could result in deteriorated diffuse textures. On\nthe other hand, some works use the SDS loss [56, 75, 76] to\ntrain a 3D avatar by aligning the rendered view with the tex-\ntures generated by a diffusion model. The SDS may lead to\nthe oversmoothed results that lack the diversity in skin and\nfacial details compared with the original 2D images sam-\npled from the underlying diffusion model.\nTo address these challenges, we propose a novel ap-\nproach to create 3D animatable avatars that are more re-\nalistic in diffuse colors and skin details. First, our approach\ncan take either a text prompt or a single face image as in-\nput. The text prompt is fed into a generic diffusion model\nto create a face image, or the a single face image can also\nbe input into our framework. It is well known that sepa-\nrating lighting from the captured colors in a single image\nis intrinsically challenging. To obtain high quality textures\nthat are not contaminated by the unwanted lighting, our key\nobservation is that the self-attention block in the diffusion\nmodel indeed captures the lighting effects, this enables us\nto reveal the true diffuse colors by proposing a diffuse color\nextraction (DCE) model to robustly eliminate the lighting\nfrom the texture of the input image.\nIn addition, we propose to train an authenticity guided\ntexture diffusion model (AGT-DM) that are able to gener-\nate high-quality complete facial textures that align with the\n3D face meshes. Two gradient guidances are presented to\nenhance the resultant 3D avatars \u2013 a photometric guidance\nand an edge guidance that are added to classifier-free diffu-\nsion sampling process. This can improve the diversity of the\ngenerated 3D avatars with more subtle high-frequency de-\ntails in their facial textures across observed and unobserved\nviews.\nThe key contributions of our work are summarized be-\nlow.\n\u2022 We reveal the relationship between the self-attention fea-\ntures and the lighting effects, enabling us to propose\na novel model for extracting diffuse colors by remov-\ning lighting effects in a single image. Our experiments\ndemonstrate this is a robust and effective approach, suit-\nable for tasks such as removing specular spotlights and\nshadows.\n\u2022 We introduce an authenticity guided diffusion model to\ngenerate PBR textures. It can provide high-quality com-\nplete textures that well align with 3D meshes without\nsusceptible lighting effects. The sampling process fol-\nlows two gradient-based guidances to retain facial details\nunique to each identity, which contributes to the improved\ngeneration diversity.\n\u2022 We build a novel 3D avatar generative framework Ul-\ntrAvatar upon the proposed DCE model and the AGT-\nDM. Our experiments demonstrate high quality diverse\n3D avatars with true colors and sharp details (see Fig. 1).\n2. Related Work\nImage-to-Avatar Generation:\nInitially, avatar genera-\ntion was predominantly reliant on complex and costly scan-\nning setups, limiting its scalability [7, 13, 33, 48]. This chal-\nlenge has shifted towards utilizing common image inputs\nlike a single photo [20, 25, 80] or video [16, 28, 32, 79].\nAdditionally, the representation of 3D head has diversified,\nranging from the mesh-based parametric models [46, 79] to\nthe fluid neural implicit functions like NeRFs [8, 37, 73].\nThe introduction of advanced neural networks, especially\nGenerative Adversarial Networks (GANs) [40] have led to\nthe embedding of 3D attributes directly into these genera-\ntive models [17, 21, 52] and the use of generative model\nas a prior to generate 3D avatars [29, 30, 43, 44] etc. Some\ntechniques [10, 23] also can create the riggable avatars from\na single image. Nevertheless, these existing methods rely on\nthe provided images for mesh and texture generation, often\nencounter errors in mesh creation and issues with diffuse\ntexture generation due to the lighting like specular high-\nlights and shadows on the images. Our proposed method\nadeptly addresses and mitigates these challenges, ensuring\nmore consistent and accurate results.\nText-to-3D Generation:\nText-to-3D generation is a pop-\nular research topic that builds on the success of text-\nto-image\nmodels\n[51,\n58\u201360].\nDreamFusion\n[56],\n2\nAuthenticity Guided Texture Diffusion Model\n\u201ca photo of an old \nwhite man with \ndeep wrinkles\u201d\nImage \ud835\udc70\nInput Text\nGeneric\nDiffusion\nSpecular\nRoughness\nNormal\nDiffuse\nDiffuse Color \nExtraction \nDiffusion Model\nFLAME-Based \nMesh Generator\nMasked Initial Texture \n\ud835\udc7d \u2299 \ud835\udc70\ud835\udc8e\n\ud835\udc74(\ud835\udf37\u2217, \ud835\udf4d\u2217, \ud835\udf3d\u2217)\nEdge Guidance\nPhotometric\n Guidance\nImage \ud835\udc70\ud835\udc85\nRelighting Result\n\u00d7 \ud835\udc75 \nDenoising U-Net\n\ud835\udc6b\ud835\udc94\n\ud835\udc6b\ud835\udc93\n\ud835\udc6b\ud835\udc8f\n\ud835\udc6b\ud835\udc85\n\ud835\udc81\ud835\udfce\n\ud835\udc81\ud835\udc75\nEncoder\nEdge\nDetector\n\ud835\udc7d\n\ud835\udc6a \ud835\udc70\ud835\udc85\n\ud835\udf37\u2217\n\ud835\udf4d\u2217\n\ud835\udf3d\u2217\n\ud835\udc84\u2217\nVisibility Mask\n\ud835\udc70\ud835\udc8e\nInitial Texture\nAfter (\ud835\udc13 \u2212 \ud835\udc75) Steps \nLatent Inpainting\nFigure 2. The Overview of UltrAvatar. First, we feed a text prompt into a generic diffusion model (SDXL [55]) to produce a face image.\nAlternatively, the face image can also be directly input into our framework. Second, our DCE model takes the face image to extract its\ndiffuse colors Id by eliminating lighting. The Id is then passed to the mesh generator and the edge detector to generate the 3D mesh,\ncamera parameters and the edge image. With these predicted parameters, the initial texture and the corresponding visibility mask can be\ncreated by texture mapping. Lastly, we input the masked initial texture into our AGT-DM to generate the PBR textures. A relighting result\nusing the generated mesh and PBR textures is shown here.\nMagic3D [47], Latent-NeRF [49], AvatarCLIP [36], Clip-\nFace [9], Rodin [70], DreamFace [76] etc., uses the text\nprompt to guide the 3D generation.\nMost of these ap-\nproaches use SDS loss to maintain consistency between\nthe images generated by the diffusion model and 3D ob-\nject. However, SDS loss significantly limits the diversity of\ngeneration. Our approach upholds the powerful image gen-\neration capabilities from diffusion models trained on large\nscale data, facilitating diversity. Simultaneously, it ensures\na high degree of fidelity between the textual prompts and\nthe resulting avatars without depending on SDS loss.\nGuided Diffusion Model:\nA salient feature of diffusion\nmodels lies in their adaptability post-training, achieved by\nguiding the sampling process to tailor outputs. The con-\ncept of guided diffusion has been extensively explored in\na range of applications, encompassing tasks like image\nsuper-resolution [18, 27, 63], colorization [19, 62], deblur-\nring [18, 71], and style-transfer [26, 41, 42]. Recent studies\nhave revealed that the diffusion U-Net\u2019s intermediate fea-\ntures are rich in information about the structure and content\nof generated images [12, 34, 42, 57, 69]. We discover that\nthe attention features can represent lighting in the image and\npropose a method to extract the diffuse colors from a given\nimage. Additionally, we incorporated two guidances to en-\nsure the authenticity and realism of the generated avatars.\n3. The Method\nAn overview of our framework is illustrated in Fig. 2. We\ntake a face image as input or use the text prompt to generate\na view I of the avatar with a diffusion model. Then, we in-\ntroduce a DCE model to recover diffuse colors by eliminat-\ning unwanted lighting from the generated image. This pro-\ncess is key to generating high quality textures without being\ndeteriorated by lighting effects such as specularity and shad-\nows. This also ensures the generated avatars can be properly\nrendered under new lighting conditions. We apply a 3D face\nmodel (e.g., a 3DMM model) to generate the mesh aligned\nwith the resultant diffuse face image. Finally, we train an\nAGT-DM with several decoders to generate PBR textures,\nincluding diffuse colors, normal, specular, and roughness\ntextures. This complete set of PBR textures can well align\nwith the 3D mesh, as well as preserve the face details unique\nto individual identity.\n3.1. Preliminaries\nDiffusion models learn to adeptly transform random noise\nwith condition y into a clear image by progressively remov-\ning the noise. These models are based on two essential pro-\ncesses. The forward process initiates with a clear image x0\nand incrementally introduces noise, culminating in a noisy\nimage xT , and the backward process works to gradually re-\nmove the noise from xT , restoring the clear image x0. The\nstable diffusion (SD) model [55, 60] operates within the la-\ntent space z = E(x) by encoding the image x to a latent\nrepresentation. The final denoised RGB image is obtained\nby decoding the latent image through x0 = D(z0). To carry\nout the sequential denoising, the network \u03f5\u03b8 is rigorously\ntrained to predict noise at each time step t by following the\nobjective function:\n3\nmin\n\u03b8\nEz\u223cE(x),t,\u03f5\u223cN (0,1)||\u03f5 \u2212 \u03f5\u03b8(zt, t, \u03c4(y))||2\n2.\n(1)\nwhere the \u03c4(\u00b7) is the conditioning encoder for an input\ncondition y, such as a text embedding, zt represents the\nnoisy latent sample at the time step t. The noise prediction\nmodel in SD is based on the U-Net architecture, where each\nlayer consists of a residual block, a self-attention block, and\na cross-attention block, as depicted in Fig. 4. At a denoising\nstep t, the features \u03d5l\u22121\nt\nfrom the previous (l \u2212 1)-th layer\nare relayed to the residual block to produce the res-features\nf l\nt. Within the self-attention block, the combined features\n(\u03d5l\u22121\nt\n+f l\nt) through the residual connection are projected to\nthe query features ql\nt, key features kl\nt and value features vl\nt.\nThe above res-features f l\nt contributes to the content of the\ngenerated image and the attention features hold substantial\ninformation that contributes to the overall structure layout,\nwhich are normally used in image editing [34, 42, 57, 69].\nDiffusion models possess the pivotal feature of employ-\ning guidance to influence the reverse process for generat-\ning conditional samples. Typically, classifier guidance can\nbe applied to the score-based models by utilizing a dis-\ntinct classifier. Ho et al. [35] introduce the classifier-free\nguidance technique, blending both conditioned noise pre-\ndiction \u03f5\u03b8(zt, t, \u03c4(y)) and unconditioned noise prediction\n\u03f5\u03b8(zt, t, \u2205), to extrapolate one from another,\n\u02dc\u03f5\u03b8(zt, t, \u03c4(y)) = \u03c9\u03f5\u03b8(zt, t, \u03c4(y)) + (1 \u2212 \u03c9)\u03f5\u03b8(zt, t, \u2205).\n(2)\nwhere \u2205 is the embedding of a null text and \u03c9 is the guid-\nance scale.\n3.2. Diffuse Color Extraction via Diffusion Features\nOur approach creates a 3D avatar from a face image I that is\neither provided by a user or generated by a diffusion model\n[55, 60] from a text prompt. To unify the treatment of those\ntwo cases, the DDIM inversion [22, 68] with non-textual\ncondition is applied to the image that results in a latent noise\nzI\nT at time step T from which the original image I is then\nreconstructed through the backward process. This gives rise\nto a set of features from the diffusion model.\nThe given image I, no matter user-provided or SD-\ngenerated, typically contains shadows, specular highlights,\nand lighting effects that are hard to eliminate. To render\na relightable and animatable 3D avatar, it usually requires\na diffuse texture map with these lighting effects removed\nfrom the image, which is a challenging task. For this, we\nmake a key observation that reveals the relation between the\nself-attention features and the lighting effects in the image,\nand introduce a DCE model to eliminate the lighting.\nFirst, we note that the features f l\nt in each layer contain\nthe RGB details as discussed in [67, 69]. The self-attention\nSample number\nInput Image \ud835\udc70\nQuery Features Key Features \nRes-Features \nResult \ud835\udc70\ud835\udc85\nGround Truth\n\ud835\udc92\n\ud835\udc8c\n0\n3000\nIntensity\nIntensity\nIntensity\nColor Distribution\nInput \ud835\udc70\nResult \ud835\udc70\ud835\udc85\nGround Truth\nMask \ud835\udc7a\nFigure 3. Features Visualization. We render a high-quality data\nwith PBR textures under a complex lighting condition to image I,\nand also render its corresponding ground truth diffuse color image.\nWe input the I to our DCE model to produce result Id. The S is the\nsemantic mask. We apply DDIM inversion and sampling on these\nimages and extract the features. To visualize the features, we ap-\nply PCA on the extracted features to check the first three principal\ncomponents. The attention features and res-features shown here\nare all from the 8-th layer at upsampling layers in the U-Net at time\nstep 101. From the extracted query and key features of I, we can\nclearly visualize the lighting. The colors and extracted query and\nkey features of the result Id closely match those from the ground\ntruth image, which demonstrates our method effectively removes\nthe lighting. All res-features do not present too much lighting. We\nalso show the color distributions of these three images, illustrating\nthat the result Id can eliminate shadows and specular points, mak-\ning its distribution similar to the ground truth.\nfeatures ql\nt and kl\nt reflect the image layout, with similar re-\ngions exhibiting similar values. Beyond this, our finding is\nthat the variations in these self-attention features ql\nt and kl\nt\nindeed reflect the variations caused by the lighting effects\nsuch as shading, shadows, and specular highlights within a\nsemantic region. This is illustrated in Fig. 3. This is not hard\nto understand. Consider a pixel on the face image, its query\nfeatures ought to align with the key features from the same\nfacial part so that its color can be retrieved from the rele-\nvant pixels. With the lighting added to the image, the query\nfeatures must vary in the same way as the variation caused\nby the lighting effects. In this way, the lighted colors could\nbe correctly retrieved corresponding to the lighting pattern\n\u2013 shadows contribute to the colors of nearby shadowed pix-\nels, while highlights contribute to the colors of nearby high-\nlighted ones.\nTo eliminate lighting effects, one just needs to remove\nthe variation in the self-attention (query and key) features\n4\nDiffusion U-Net\nUpSample Layer\nDecoder\nRes-features\nImage \ud835\udc70\ud835\udc85\nEncoder\nDDIM \nInversion\n\u00d7 \ud835\udc7b\nImage \ud835\udc70\n\ud835\udc9b\ud835\udc7b\n\ud835\udc70\n\ud835\udc9b\ud835\udfce\n\ud835\udc70\nQuery\nMask \ud835\udc7a\nEncoder\nDDIM \nInversion\nFace\nParsing\nKey\n\u00d7 \ud835\udc7b\n\ud835\udc9b\ud835\udc7b\n\ud835\udc7a\n\ud835\udc9b\ud835\udfce\n\ud835\udc7a\nRes- \nBlock\n\ud835\udc87\ud835\udc95\n\ud835\udc8d\nSelf-\nAttention\n\u2026\n\ud835\udc92\ud835\udc95\n\ud835\udc8d\n\ud835\udc8c\ud835\udc95\n\ud835\udc8d\n\ud835\udc97\ud835\udc95\n\ud835\udc8d\n\ud835\udf53\ud835\udc95\n\ud835\udc8d\u2212\ud835\udfcf\n\ud835\udc9b\ud835\udc7b\n\ud835\udc70\nCopy To\nCopy To\nCopy To\n\u0de1\ud835\udc81\ud835\udfce\n\ud835\udc70\nFigure 4. DCE Model. The input image I is fed to the face parsing\nmodel to create the semantic mask S. We apply DDIM inversion\non the I and S to get initial noise zI\nT and zS\nT , then we progressively\ndenoise the zI\nT and zS\nT to extract and preserve the res-features and\nattention features separately. Lastly, we progressively denoise the\nzI\nT one more time, copying the res-features and attention features\nfrom storage at certain layers (as discussed in Sec. 4) during sam-\npling to produce \u02c6zI\n0, the final result Id will be generated from de-\ncoding the \u02c6zI\n0.\nwhile still keeping these features aligned with the semantic\nstructure. Fig. 4 summarizes the idea. Specifically, first we\nchoose a face parsing model to generate a semantic mask\nS for the image I. The semantic mask meets the above\ntwo requirements since it perfectly aligns with the semantic\nstructure by design and has no variation within a semantic\nregion. Then we apply the DDIM inversion to S resulting in\na latent noise zS\nT at time step T, and obtain the self-attention\nfeatures of S via the backward process starting from zS\nT for\nfurther replacing ql\nt and kl\nt of the original I. Since the se-\nmantic mask has uniform values within a semantic region,\nthe resultant self-attention features are hypothesized to con-\ntain no lighting effects (see Fig. 3), while the face details are\nstill kept in the features f l\nt of the original image I. Thus, by\nreplacing the query and key features ql\nt and kl\nt with those\nfrom the semantic mask in the self-attention block, we are\nable to eliminate the lighting effects from I and keep its dif-\nfuse colors through the backward process starting from the\nlatent noise zI\nT used for generating I.\nThis approach can be applied to eliminate lighting effects\nfrom more generic images other than face images, and we\nshow more results in the Appendix.\n3.3. 3D Avatar Mesh Generation\nWe employ the FLAME [46] model as our geometry rep-\nresentation of 3D avatars. FLAME is a 3D head template\nmodel, which is trained from over 33, 000 scans. It is char-\nacterized by the parameters for identity shape \u03b2 , facial ex-\npression \u03c8 and pose parameters \u03b8. With these parameters.\nFLAME generates a mesh M(\u03b2, \u03c8, \u03b8) consisting 5023 ver-\ntices and 9976 faces, including head, neck, and eyeballs\nmeshes. We adopt MICA [80] for estimating shape code\n\u03b2\u2217 of the FLAME model from the diffuse image Id, which\nexcels in accurately estimating the neutral face shape and\nis robust to expression, illumination, and camera changes.\nWe additionally apply EMOCA [20] to obtain the expres-\nsion code \u03c8\u2217, pose parameters \u03b8\u2217 and camera parameters\nc\u2217, which is employed for subsequent 3D animation/driv-\ning applications. Note that we do not use the color texture\ngenerated by the EMOCA combining the FLAME texture\nbasis. It cannot accurately present the true face color, lacks\nskin details and contains no PBR details, such as diffuse\ncolors, normal maps, specularity, and roughness textures,\nwhich can be derived below with our AGT-DM.\n3.4. Authenticity Guided Texture Diffusion Model\nGiven the current estimated mesh M(\u03b2\u2217, \u03c8\u2217, \u03b8\u2217), camera\nparameters c\u2217 and the lighting-free face image Id, one can\ndo the texture mapping of the latter onto the mesh, and then\nproject the obtained mesh texture to an initial texture UV\nmap Im. Since Id is only a single view of the face, the\nresultant Im is an incomplete UV texture map of diffuse\ncolors, and we use V to denote its visible mask in the UV\ncoordinates. The UV texture map may also not perfectly\nalign with the mesh due to the errors in the estimated face\npose, expression and camera pose by EMOCA.\nTo address the above challenges, we train an AGT-DM\nthat can 1) inpaint the partially observed texture UV map\nIm to the full UV coordinates, and 2) improve the alignment\nbetween the texture map and the UV coordinates. More-\nover, the model can output more PBR details beyond the dif-\nfuse color textures, including normal, specular and rough-\nness maps from the given Im and V . We will enhance the\noutput PBR results with two guidance signals based on the\nphotometric and edge details, so that the identity and more\nfacial features, such as subtle wrinkles and pores, can be\nrecovered from the AGT-DM.\nTo this end, we use the online 3DScan dataset [1] that\nconsists of 3D face scans alongside multiple types of PBR\ntexture maps (in 4K and 8K) including diffuse colors, nor-\nmal maps, specularity and roughness textures on the entire\nhigh-quality meshes of 3D scans. We process this dataset\n(details in the Appendix) into a training dataset and train\nan inpainting SD model with them, the U-net of the origi-\nnal SD is finetuned over the groundtruth diffuse UV maps\nfrom the dataset. To generate PBR textures, the SD en-\ncoder is frozen and the SD decoder is copied and finetuned\nover the dataset for each type of PBR texture, except that\nthe PBR decoder Dd for diffuse texture directly inherits\n5\nfrom the original SD decoder. Then we can use the fine-\ntuned SD model to inpaint the masked diffuse color map\nV \u2299 Im alongside the other PBR textures.\nBecause the\ntraining dataset have ideally aligned meshes and texture de-\ntails, the resultant inpainting diffusion model can improve\nthe alignment between the output PBR textures and meshes,\nas well as facial details. Denoising the noisy masked texture\nlatent ZN following latent inpainting, more accurate align-\nment can be generated between textures and meshes since\nthe denoising could corrupt the misalignments in Im and al-\nlow the inpainting diffusion model to correct them through\ndiffusion sampling.\nTo further enhance the PBR textures with more facial\ndetails, we employ two energy functions to guide the sam-\npling process of the inpainting diffusion model. The first\nis the photometric guidance GP with the following energy\nfunction,\nGP = \u03c9photo||Vd \u2299 (R(M(\u03b2\u2217, \u03c8\u2217, \u03b8\u2217), Dd(zt), c\u2217) \u2212 Id)||2\n2\n+ \u03c9lpipsLlpips(Vd \u2299 (R(M(\u03b2\u2217, \u03c8\u2217, \u03b8\u2217), Dd(zt), c\u2217)), Vd \u2299 Id))\n(3)\nwhere Vd is the mask over the visible part of rendered face,\nas shown in Fig. 6, and the R(\u00b7) is a differential renderer of\nthe avatar face based on the current estimate of the mesh M,\nthe diffuse color texture map Dd(zt) at a diffusion time step\nt, the Llpips(.) is the perceptual loss function (LPIPS [77]).\nThe minimization of this photometric energy will align the\nrendered image with the original image.\nThe second is the edge guidance with the following edge\nenergy function,\nGE = |||Vd \u2299 (C(R(M(\u03b2\u2217, \u03c8\u2217, \u03b8\u2217), D(zt), c\u2217)) \u2212 C(Id))||2\n2\n(4)\nwhere C(\u00b7) is the canny edge detection function [15]. While\nthe edges contain high-frequency details, as shown in Fig.\n2, this guidance will help retain the facial details such as\nwrinkles, freckles, pores, moles and scars in the image Id,\nmaking the generated avatars look more realistic with high\nfidelity.\nWe integrate the two guidances through the gradients of\ntheir energy functions into the sampling of classifier-free\nguidance below,\n\u02dc\u03f5\u03b8(zt, t, \u03c4(y)) = \u03c9\u03f5\u03b8(zt, t, \u03c4(y)) + (1 \u2212 \u03c9)\u03f5\u03b8(zt, t, \u2205)\n+ \u03c9p\u2207ztGP + \u03c9e\u2207ztGE.\n(5)\nWe demonstrate the effectiveness in Fig 6.\n4. Experiments\n4.1. Setup and Baselines\nExperimental Setup.\nWe used SDXL[55] as our text-\nto-image generation model. We used pretrained BiSeNet\n[4, 74] for generating face parsing mask, S. In our DCE\nmodule, we use the standard SD-2.1 base model and apply\nthe DDIM sampling with 20 time steps. We preserve the\nres-features from 4th to 11th upsampling layers in the U-net\nextracted from the I, and inject into the DDIM sampling of\nI the query and key features from the 4th to 9th upsampling\nlayers extracted from S. We choose not to inject query and\nkey features from all layers because we find injecting them\nto the last few layers sometimes slightly would change the\nidentity.\nFor our AGT-DM, we finetune the U-Net from SD-2.1-\nbase model on our annotated 3DScan store dataset to gener-\nate diffuse color texture map. We attach \u201cA UV map of\u201d to\nthe text prompt during finetuning to generate FLAME UV\nmaps. We train three decoders to output normal, specular\nand roughness maps. More training details are presented in\nthe Appendix.\nIn the AGT-DM, we use T = 200, N = 90, \u03c9 = 7.5,\n\u03c9p = 0.1, \u03c9photo = 0.4, \u03c9lpips = 0.6 and \u03c9e = 0.05. In the\ninitial T \u2212 N denoising steps, our approach adopts a latent\nspace inpainting technique akin to the method described in\n[11], utilizing a visibility mask. During the final N steps,\nwe apply the proposed photometric and edge guidances to\nrectify any misalignments and ensure a coherent integration\nbetween the observed and unobserved regions of the face.\nAfter the inference, we pass the resultant latent code to nor-\nmal, specular and roughness decoders to obtain the corre-\nsponding PBR texture maps. We then pass the texture to\na pretrained Stable Diffusion super-resoltuion network [60]\nto get 2K resolution texture.\nBaselines.\nWe\nshow\ncomparisons\nagainst\ndifferent\nstate-of-the-art\napproaches\nfor\ntext-to-avatar\ngenera-\ntion (Latent3d [14], CLIPMatrix [38], Text2Mesh[50],\nCLIPFace[9], DreamFace[76]) and image-to-avatar gen-\neration (FlameTex[24], PanoHead [8]). Details about the\ncomparisons are included in the Appendix.\n4.2. Results and Discussion\nWe demonstrate our text/image generated realistic avatars in\nFig. 1 and 5. Note that, we do not have those images in the\ntraining data for our AGT-DM. Generated results demon-\nstrate rich textures maintaining fidelity with the given text\nprompt/image. Furthermore, due to our DCE model and\nAGT-DM\u2019s capabilities to extract diffuse color texture and\nPBR details, we can correctly render relighted avatars from\nany lighting condition. Since, AGT-DM enforces consis-\ntency across the observed and unobserved region, our ren-\ndered avatars look equally realistic from different angles\nwithout any visible artifacts.\nPerformance Analysis.\nFor comparison, we randomly\nselect 40 text prompts, ensuring a comprehensive represen-\ntation across various age groups, ethnicities and genders,\nas well as including a range of celebrities.\nFor Dream-\n6\n\u201cA middle-aged East Asian man with \npronounced wrinkles, piercing eyes.\u201d\n\u201cA middle-aged Caucasian woman \nwith sun-kissed skin, subtle smile lines\u201d\n\u201cA young white girl with blue eyes, \na dusting of freckles across her nose\u201d\n\u201cA middle-age Middle Eastern man\u201d\n\u201cMorgan_Freeman_\u201d\n\u201cBrad Pitt\u201d\n\u201cMark Zuckerberg\u201d\n\u201cWill Smith\u201d\nFigure 5. Results of generating random identities and celebrities. We input the text prompts into the generic SDXL to create 2D\nface images. Our results showcase the reconstructed high-quality PBR textures which are also well-aligned with the meshes, exhibit high\nfidelity, and maintain the identity and facial details. To illustrate the quality of our generation, we relight each 3D avatar under various\nenvironment maps.\nImage \ud835\udc70\ud835\udc85\n\ud835\udc7d\ud835\udc85\n\ud835\udc7d\ud835\udc85 \u2299 \ud835\udc70\ud835\udc85\n\ud835\udc7d\n\ud835\udc7d \u2299 \ud835\udc70\ud835\udc8e\nOn \ud835\udc70\ud835\udc85\nOn UV Map\nOnly with \ud835\udc6e\ud835\udc77\nWith \ud835\udc6e\ud835\udc77 + \ud835\udc6e\ud835\udc6c\nWithout \ud835\udc6e\ud835\udc77 and \ud835\udc6e\ud835\udc6c\nMasks\nFigure 6. Analysis of the guidances in the AGT-DM. Three PBR\ntextures generation scenarios from image Id by our AGT-DM are\nshown: one without GP and GE, one only with GP , and another\nwith both GP and GE. It clearly demonstrates that the identity and\nfacial details are effectively maintained through these two guid-\nances.\nFace and UltrAvatar, we render the generated meshes from\n50 different angles under 5 different lighting conditions.\nFor PanoHead, we provide 200 images generated by SDXL\nusing the same 40 text prompts. UltrAvatar can generate\nhigh-quality facial asset from text prompt within 2 minutes\n(compared to 5 minutes for DreamFace) on a single Nvidia\nA6000 GPU.\nWe evaluate the perceptual quality of the rendered im-\nages using standard generative model metrics FID and KID.\nSimilar to CLIPFace, we evaluate both of these metrics with\nrespect to masked FFHQ images [39] (without background,\neyes and mouth interior) as ground truth. For text-to-avatar\ngeneration, we additionally calculate CLIP score to measure\nsimilarity between text prompts and rendered images. We\nreport the average score from two different CLIP variants,\n\u2018ViT-B/16\u2019 and \u2018ViT-L/14\u2019.\nAmong the text-to-avatar generation approaches in Ta-\nble 1, DreamFace performs very well on maintaining sim-\nilarity between text and generated avatars. However, the\ngenerated avatars by DreamFace lack realism and diversity.\nOur proposed UltrAvatar performs significantly better than\nDreamFace in terms of perceptual quality (more results are\nshown in the Appendix). Furthermore, in Fig. 7, we demon-\nstrate that DreamFace fails to generate avatars from chal-\nlenging prompts (e.g. big nose, uncommon celebrities). It\nis important to note that the results from DreamFace repre-\nsent its best outputs from multiple runs. Our UltrAvatar also\noutperforms other text-to-avatar approaches in terms of per-\nceptual quality and CLIP score, as reported in Table 1. In\nthe task of image-to-avatar generation, PanoHead achieves\nimpressive performance in rendering front views. However,\nthe effectiveness of PanoHead is heavily dependent on the\naccuracy of their pre-processing steps, which occasionally\n7\nMethod\nFID \u2193\nKID \u2193\nCLIP Score \u2191\nDreamFace [76]\n76.70\n0.061\n0.291 \u00b1 0.020\nClipFace\u2217 [9]\n80.34\n0.032\n0.251 \u00b1 0.059\nLatent3d\u2217[14]\n205.27\n0.260\n0.227 \u00b1 0.041\nClipMatrix\u2217[38]\n198.34\n0.180\n0.243 \u00b1 0.049\nText2Mesh\u2217[50]\n219.59\n0.185\n0.264 \u00b1 0.044\nFlameTex\u2217[24]\n88.95\n0.053\n-\nPanoHead [8]\n48.64\n0.039\n-\nUtrAvatar (Ours)\n45.50\n0.029\n0.301 \u00b1 0.023\nTable 1. Comparison of methods based on FID, KID, and CLIP\nScore metrics, \u2217 results are from CLIPFace.\n\u201cA black man with a big and wide nose\u201d\nOur Results\nDreamFace \n\u201cAngela Merkel\u201d\nFigure 7. Comparison to DreamFace. Our results achieve bet-\nter alignment with the text prompt than DreamFace, especially for\nextreme prompts.\nfail to provide precise estimation. Furthermore, the NeRF-\nbased PanoHead approach is limited in relighting. Consid-\nering the multi-view rendering capabilities, UltrAvatar out-\nperforms PanoHead in image-to-avatar task as shown in Ta-\nble 1.\nIn addition, we automate text-to-avatar performance as-\nsessment utilizing GPT-4V(ision) [5, 6]. GPT-4V is recog-\nnized for its human-like evaluation capabilities in vision-\nlanguage tasks [72, 78]. We evaluate models on a 5-point\nLikert scale.\nThe criteria for assessment include photo-\nrealism, artifact minimization, skin texture quality, textual\nprompt alignment, and the overall focus and sharpness of\nthe image. As illustrated in Fig. 8, UltrAvatar demonstrates\nsuperior capabilities in generating lifelike human faces. It\nnot only significantly reduces artifacts and enhances sharp-\nness and focus compared to DreamFace and PanoHead but\nalso maintains a high level of photo-realism and fidelity in\ntext-prompt alignment.\n4.3. Ablation Studies.\nIn Fig. 6, we illustrate the impact of different guidances on\nthe AGT-DM performance. The photometric guidance en-\nforces the similarity between the generated texture and the\nsource image. Additionally, the edge guidance enhances the\ndetails in the generated color texture.\nFigure 8. Qualitative evaluation by GPT-4V. Our framework has\noverall better performance.\n\u201cMoana Waialiki\u201d\n\u201cJoker from DC\u201d\n\u201cHiccup Horrendous \nHaddock III\u201d\n\u201cElsa from Frozen\u201d\nFigure 9.\nResults of out-of-domain avatar generation.\nOur\nframework has capability to generate out-of-distribution animation\ncharacters or non-human avatars.\nOut-of-domain Generation.\nUltrAvatar can generate\navatars from the image/prompt of animation characters,\ncomic characters and other non-human characters. We have\nshown some results in Fig. 9.\nAnimation and Editing\nSince our generated avatars are\nmesh-based models, we can animate our generated avatars\nby changing the expressions and poses. We can also per-\nform some texture editing using the text prompt in the AGT-\nDM. We have included the animation and editing results in\nthe Appendix.\n5. Conclusion\nWe introduced a novel approach to 3D avatar generation\nfrom either a text prompt or a single image. At the core\nof our method is the DCE Model designed to eliminate un-\nwanted lighting effects from a source image, as well as a\ntexture generation model guided by photometric and edge\nsignals to retain the avatar\u2019s PBR details. Compared with\nthe other SOTA approaches, we demonstrate our method\ncan generate 3D avatars that display heightened realistic,\nhigher quality, superior fidelity and more extensive diver-\nsity.\n8\nReferences\n[1] 3DScan store. https://www.3dscanstore.com/. 5,\n12\n[2] Wrap4d. https://www.russian3dscanner.com/\nwrap4d/. 12\n[3] Hyperhuman. https://hyperhuman.deemos.com/. 12\n[4] Using modified BiSeNet for face parsing in PyTorch.\nhttps://github.com/zllrunning/face-parsing.PyTorch. 6\n[5] ChatGPT can now see, hear, and speak.\nhttps :\n//openai.com/blog/chatgpt-can-now-see-\nhear-and-speak, 2023. 8, 12\n[6] GPT-4V(ision) system card. https://cdn.openai.\ncom/papers/GPTV_System_Card.pdf, 2023. 8, 12\n[7] Oleg Alexander, Mike Rogers, William Lambeth, Jen-Yuan\nChiang, Wan-Chun Ma, Chuan-Chang Wang, and Paul De-\nbevec. The Digital Emily Project: Achieving a Photorealistic\nDigital Actor. IEEE Computer Graphics and Applications,\n30(4):20\u201331, 2010. 2\n[8] Sizhe An, Hongyi Xu, Yichun Shi, Guoxian Song, Umit Y\nOgras, and Linjie Luo.\nPanoHead: Geometry-Aware 3D\nFull-Head Synthesis in 360\u00b0. In CVPR, pages 20950\u201320959,\n2023. 2, 6, 8, 12\n[9] Shivangi Aneja, Justus Thies, Angela Dai, and Matthias\nNie\u00dfner. ClipFace: Text-guided Editing of Textured 3D Mor-\nphable Models. In ACM SIGGRAPH 2023 Conference Pro-\nceedings, pages 1\u201311, 2023. 3, 6, 8, 12\n[10] ShahRukh Athar, Zexiang Xu, Kalyan Sunkavalli, Eli\nShechtman, and Zhixin Shu. RigNeRF: Fully Controllable\nNeural 3D Portraits. In CVPR, pages 20364\u201320373, 2022. 2\n[11] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended\nLatent Diffusion. ACM TOG, 42(4):1\u201311, 2023. 6, 12\n[12] Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta,\nYaniv Taigman, Devi Parikh, Dani Lischinski, Ohad Fried,\nand Xi Yin.\nSpaText: Spatio-Textual Representation for\nControllable Image Generation.\nIn CVPR, pages 18370\u2013\n18380, 2023. 3\n[13] George Borshukov and John P Lewis. Realistic Human Face\nRendering for \u201cThe Matrix Reloaded\u201d. In ACM Siggraph\n2005 Courses, pages 13\u2013es. 2005. 2\n[14] Zehranaz Canfes, M Furkan Atasoy, Alara Dirik, and Pinar\nYanardag. Text and Image Guided 3D Avatar Generation and\nManipulation. In CVPR, pages 4421\u20134431, 2023. 6, 8, 12\n[15] John Canny. A Computational Approach to Edge Detection.\nIEEE TPAMI, (6):679\u2013698, 1986. 6\n[16] Chen Cao, Tomas Simon, Jin Kyu Kim, Gabe Schwartz,\nMichael Zollhoefer, Shun-Suke Saito, Stephen Lombardi,\nShih-En Wei, Danielle Belko, Shoou-I Yu, et al. Authen-\ntic volumetric avatars from a phone scan. ACM TOG, 41(4):\n1\u201319, 2022. 2\n[17] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,\nBoxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J\nGuibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient\nGeometry-aware 3D Generative Adversarial Networks. In\nCVPR, pages 16123\u201316133, 2022. 2\n[18] Hyungjin Chung, Jeongsol Kim, Michael Thompson Mc-\ncann, Marc Louis Klasky, and Jong Chul Ye.\nDiffusion\nPosterior Sampling for General Noisy Inverse Problems. In\nICLR, 2022. 3\n[19] Hyungjin Chung,\nByeongsu Sim,\nDohoon Ryu,\nand\nJong Chul Ye.\nImproving Diffusion Models for Inverse\nProblems using Manifold Constraints. NeurIPS, 35:25683\u2013\n25696, 2022. 3\n[20] Radek Dan\u02c7e\u02c7cek, Michael J Black, and Timo Bolkart.\nEMOCA: Emotion Driven Monocular Face Capture and An-\nimation. In CVPR, pages 20311\u201320322, 2022. 2, 5\n[21] Yu Deng, Jiaolong Yang, Jianfeng Xiang, and Xin Tong.\nGRAM: Generative Radiance Manifolds for 3D-Aware Im-\nage Generation. In CVPR, pages 10673\u201310683, 2022. 2\n[22] Prafulla Dhariwal and Alexander Nichol. Diffusion Models\nBeat GANs on Image Synthesis. NeurIPS, 34:8780\u20138794,\n2021. 4\n[23] Zheng Ding,\nXuaner Zhang,\nZhihao Xia,\nLars Jebe,\nZhuowen Tu, and Xiuming Zhang. DiffusionRig: Learning\nPersonalized Priors for Facial Appearance Editing. In CVPR,\npages 12736\u201312746, 2023. 2\n[24] Haven\nFeng.\nPhotometric\nFLAME\nfitting.\nhttps://github.com/HavenFeng/photometric optimization,\n2019. 2, 6, 8, 12\n[25] Yao Feng, Haiwen Feng, Michael J Black, and Timo Bolkart.\nLearning an Animatable Detailed 3D Face Model from In-\nThe-Wild Images. ACM TOG, 40(4):1\u201313, 2021. 2\n[26] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\nAmit Haim Bermano, Gal Chechik, and Daniel Cohen-or.\nAn Image is Worth One Word: Personalizing Text-to-Image\nGeneration using Textual Inversion. In ICLR, 2022. 3\n[27] Sicheng Gao, Xuhui Liu, Bohan Zeng, Sheng Xu, Yan-\njing Li, Xiaoyan Luo, Jianzhuang Liu, Xiantong Zhen, and\nBaochang Zhang. Implicit Diffusion Models for Continuous\nSuper-Resolution. In CVPR, pages 10021\u201310030, 2023. 3\n[28] Xuan Gao, Chenglai Zhong, Jun Xiang, Yang Hong, Yudong\nGuo, and Juyong Zhang. Reconstructing Personalized Se-\nmantic Facial NeRF Models from Monocular Video. ACM\nTOG, 41(6):1\u201312, 2022. 2\n[29] Baris Gecer, Stylianos Ploumpis, Irene Kotsia, and Stefanos\nZafeiriou. GANFIT: Generative Adversarial Network Fitting\nfor High Fidelity 3D Face Reconstruction. In CVPR, pages\n1155\u20131164, 2019. 2\n[30] Baris\nGecer,\nAlexandros\nLattas,\nStylianos\nPloumpis,\nJiankang\nDeng,\nAthanasios\nPapaioannou,\nStylianos\nMoschoglou, and Stefanos Zafeiriou. Synthesizing Coupled\n3D Face Modalities by Trunk-Branch Generative Adversar-\nial Networks.\nIn ECCV, pages 415\u2013433. Springer, 2020.\n2\n[31] Thomas Gerig, Andreas Morel-Forster, Clemens Blumer,\nBernhard Egger, Marcel Luthi, Sandro Sch\u00a8onborn, and\nThomas Vetter. Morphable Face Models - An Open Frame-\nwork. In 2018 13th IEEE International Conference on Auto-\nmatic Face & Gesture Recognition (FG 2018), pages 75\u201382.\nIEEE, 2018. 2\n[32] Philip-William Grassal,\nMalte Prinzler,\nTitus Leistner,\nCarsten Rother, Matthias Nie\u00dfner, and Justus Thies. Neu-\nral Head Avatars from Monocular RGB Videos. In CVPR,\npages 18653\u201318664, 2022. 2\n9\n[33] Kaiwen Guo, Peter Lincoln, Philip Davidson, Jay Busch,\nXueming Yu, Matt Whalen, Geoff Harvey, Sergio Orts-\nEscolano, Rohit Pandey, Jason Dourgarian, et al. The re-\nlightables: Volumetric performance capture of humans with\nrealistic relighting. ACM TOG, 38(6):1\u201319, 2019. 2\n[34] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-Prompt Image\nEditing with Cross-Attention Control. In ICLR, 2022. 3, 4\n[35] Jonathan Ho and Tim Salimans. Classifier-Free Diffusion\nGuidance. In NeurIPS 2021 Workshop on Deep Generative\nModels and Downstream Applications, 2021. 4\n[36] Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang\nCai, Lei Yang, and Ziwei Liu. AvatarCLIP: Zero-Shot Text-\nDriven Generation and Animation of 3D Avatars. ACM TOG,\n41(4):1\u201319, 2022. 3\n[37] Yang Hong, Bo Peng, Haiyao Xiao, Ligang Liu, and Juy-\nong Zhang. HeadNeRF: A Real-time NeRF-based Paramet-\nric Head Model. In CVPR, pages 20374\u201320384, 2022. 2\n[38] Nikolay Jetchev. ClipMatrix: Text-controlled Creation of 3D\nTextured Meshes. arXiv preprint arXiv:2109.12922, 2021. 6,\n8, 12\n[39] Tero Karras, Samuli Laine, and Timo Aila.\nA Style-\nBased Generator Architecture for Generative Adversarial\nNetworks. In CVPR, pages 4401\u20134410, 2019. 7\n[40] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila. Analyzing and Improv-\ning the Image Quality of StyleGAN. In CVPR, pages 8110\u2013\n8119, 2020. 2\n[41] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli\nShechtman, and Jun-Yan Zhu.\nMulti-concept customiza-\ntion of text-to-image diffusion. In CVPR, pages 1931\u20131941,\n2023. 3\n[42] Gihyun Kwon and Jong Chul Ye.\nDiffusion-based Image\nTranslation using Disentangled Style and Content Represen-\ntation. In ICLR, 2022. 3, 4\n[43] Alexandros Lattas, Stylianos Moschoglou, Baris Gecer,\nStylianos\nPloumpis,\nVasileios\nTriantafyllou,\nAbhijeet\nGhosh, and Stefanos Zafeiriou.\nAvatarMe: Realistically\nRenderable 3D Facial Reconstruction \u201cin-the-wild\u201d.\nIn\nCVPR, pages 760\u2013769, 2020. 2\n[44] Alexandros\nLattas,\nStylianos\nMoschoglou,\nStylianos\nPloumpis,\nBaris Gecer,\nJiankang Deng,\nand Stefanos\nZafeiriou. FitMe: Deep Photorealistic 3D Morphable Model\nAvatars. In CVPR, pages 8629\u20138640, 2023. 2\n[45] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBLIP-2: Bootstrapping Language-Image Pre-training with\nFrozen Image Encoders and Large Language Models. 2023.\n12\n[46] Tianye Li, Timo Bolkart, Michael J Black, Hao Li, and Javier\nRomero. Learning a model of facial shape and expression\nfrom 4D scans. ACM TOG, 36(6), 2017. 2, 5\n[47] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3D: High-Resolution\nText-to-3D Content Creation.\nIn CVPR, pages 300\u2013309,\n2023. 3\n[48] Stephen Lombardi, Jason Saragih, Tomas Simon, and Yaser\nSheikh. Deep Appearance Models for Face Rendering. ACM\nTOG, 37(4):1\u201313, 2018. 2\n[49] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and\nDaniel Cohen-Or. Latent-NeRF for Shape-Guided Genera-\ntion of 3D Shapes and Textures. In CVPR, pages 12663\u2013\n12673, 2023. 3\n[50] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and\nRana Hanocka. Text2Mesh: Text-Driven Neural Stylization\nfor Meshes. In CVPR, pages 13492\u201313502, 2022. 6, 8, 12\n[51] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,\nPranav Shyam,\nPamela Mishkin,\nBob Mcgrew,\nIlya\nSutskever, and Mark Chen. GLIDE: Towards Photorealistic\nImage Generation and Editing with Text-Guided Diffusion\nModels. In Int. Conf. Machine Learn., pages 16784\u201316804.\nPMLR, 2022. 2\n[52] Roy\nOr-El,\nXuan\nLuo,\nMengyi\nShan,\nEli\nShecht-\nman, Jeong Joon Park, and Ira Kemelmacher-Shlizerman.\nStyleSDF: High-Resolution 3D-Consistent Image and Ge-\nometry Generation. In CVPR, pages 13503\u201313513, 2022.\n2\n[53] Foivos Paraperas Papantoniou, Alexandros Lattas, Stylianos\nMoschoglou, and Stefanos Zafeiriou.\nRelightify:\nRe-\nlightable 3d faces from a single image via diffusion models.\narXiv preprint arXiv:2305.06077, 2023. 2\n[54] Pascal Paysan, Reinhard Knothe, Brian Amberg, Sami\nRomdhani, and Thomas Vetter. A 3d face model for pose\nand illumination invariant face recognition. In 2009 sixth\nIEEE international conference on advanced video and sig-\nnal based surveillance, pages 296\u2013301. Ieee, 2009. 12\n[55] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach. SDXL: Improving Latent Diffusion Mod-\nels for High-Resolution Image Synthesis.\narXiv preprint\narXiv:2307.01952, 2023. 3, 4, 6\n[56] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall. DreamFusion: Text-to-3D using 2D Diffusion. In ICLR,\n2022. 2\n[57] Konpat Preechakul, Nattanat Chatthee, Suttisak Wizad-\nwongsa, and Supasorn Suwajanakorn.\nDiffusion Autoen-\ncoders: Toward a Meaningful and Decodable Representa-\ntion. In CVPR, pages 10619\u201310629, 2022. 3, 4\n[58] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-Shot Text-to-Image Generation. In Int. Conf. Machine\nLearn., pages 8821\u20138831. PMLR, 2021. 2\n[59] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical Text-Conditional Image Gen-\neration with CLIP Latents. arXiv preprint arXiv:2204.06125,\n1(2):3, 2022.\n[60] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-Resolution Image\nSynthesis with Latent Diffusion Models. In CVPR, pages\n10684\u201310695, 2022. 2, 3, 4, 6\n[61] Will Rowan, Patrik Huber, Nick Pears, and Andrew Keeling.\nText2Face: A Multi-Modal 3D Face Model. arXiv preprint\narXiv:2303.02688, 2023. 2\n10\n[62] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,\nJonathan Ho, Tim Salimans, David Fleet, and Mohammad\nNorouzi.\nPalette: Image-to-Image Diffusion Models.\nIn\nACM SIGGRAPH 2022 Conference Proceedings, pages 1\u2013\n10, 2022. 3\n[63] Chitwan Saharia, Jonathan Ho, William Chan, Tim Sali-\nmans, David J Fleet, and Mohammad Norouzi. Image Super-\nResolution via Iterative Refinement. IEEE TPAMI, 45(4):\n4713\u20134726, 2022. 3\n[64] Soubhik Sanyal, Timo Bolkart, Haiwen Feng, and Michael J\nBlack. Learning to Regress 3D Face Shape and Expression\nfrom an Image without 3D Supervision.\nIn CVPR, pages\n7763\u20137772, 2019. 2\n[65] Johannes L Schonberger and Jan-Michael Frahm. Structure-\nfrom-Motion Revisited. In CVPR, pages 4104\u20134113, 2016.\n1\n[66] Steven M Seitz, Brian Curless, James Diebel, Daniel\nScharstein, and Richard Szeliski. A Comparison and Eval-\nuation of Multi-View Stereo Reconstruction Algorithms. In\nCVPR, pages 519\u2013528. IEEE, 2006. 1\n[67] Chenyang Si, Ziqi Huang, Yuming Jiang, and Ziwei Liu.\nFreeU: Free Lunch in Diffusion U-Net.\narXiv preprint\narXiv:2309.11497, 2023. 4\n[68] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. In ICLR, 2021. 4\n[69] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali\nDekel.\nPlug-and-Play Diffusion Features for Text-Driven\nImage-to-Image Translation. In CVPR, pages 1921\u20131930,\n2023. 3, 4\n[70] Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin\nBao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang\nWen, Qifeng Chen, et al. Rodin: A Generative Model for\nSculpting 3D Digital Avatars Using Diffusion.\nIn CVPR,\npages 4563\u20134573, 2023. 3\n[71] Jay Whang, Mauricio Delbracio, Hossein Talebi, Chitwan\nSaharia, Alexandros G Dimakis, and Peyman Milanfar. De-\nblurring via Stochastic Refinement. In CVPR, pages 16293\u2013\n16303, 2022. 3\n[72] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang,\nChung-Ching Lin, Zicheng Liu, and Lijuan Wang. The Dawn\nof LMMs: Preliminary Explorations with GPT-4V(ision),\n2023. 8, 12\n[73] Yu Yin, Kamran Ghasedi, HsiangTao Wu, Jiaolong Yang,\nXin Tong, and Yun Fu. NeRFInvertor: High Fidelity NeRF-\nGAN Inversion for Single-shot Real Image Animation. In\nCVPR, pages 8539\u20138548, 2023. 2\n[74] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao,\nGang Yu, and Nong Sang. BiSeNet: Bilateral Segmentation\nNetwork for Real-time Semantic Segmentation. In ECCV,\npages 325\u2013341, 2018. 6\n[75] Hao Zhang, Yao Feng, Peter Kulits, Yandong Wen, Jus-\ntus Thies, and Michael J Black.\nText-Guided Generation\nand Editing of Compositional 3D Avatars. arXiv preprint\narXiv:2309.07125, 2023. 2\n[76] Longwen Zhang, Qiwei Qiu, Hongyang Lin, Qixuan Zhang,\nCheng Shi, Wei Yang, Ye Shi, Sibei Yang, Lan Xu, and\nJingyi Yu. DreamFace: Progressive Generation of Animat-\nable 3D Faces under Text Guidance. ACM TOG, 42(4), 2023.\n2, 3, 6, 8, 12\n[77] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The Unreasonable Effectiveness of\nDeep Features as a Perceptual Metric. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 586\u2013595, 2018. 6\n[78] Xinlu Zhang, Yujie Lu, Weizhi Wang, An Yan, Jun Yan,\nLianke Qin, Heng Wang, Xifeng Yan, William Yang Wang,\nand Linda Ruth Petzold. GPT-4V(ision) as a Generalist Eval-\nuator for Vision-Language Tasks, 2023. 8, 12\n[79] Yufeng Zheng, Victoria Fern\u00b4andez Abrevaya, Marcel C\nB\u00a8uhler, Xu Chen, Michael J Black, and Otmar Hilliges. I\nM Avatar: Implicit Morphable Head Avatars from Videos.\nIn CVPR, pages 13545\u201313555, 2022. 2\n[80] Wojciech Zielonka, Timo Bolkart, and Justus Thies. Towards\nMetrical Reconstruction of Human Faces. In ECCV, pages\n250\u2013269. Springer, 2022. 2, 5\n11\nAppendix\nA. Experimental Setup\nA.1. Dataset\nWe use the 3DScan dataset comprising 188 super-high qual-\nity commercial data samples from the [1], encompassing\na diverse array of skin colors, skin tones, genders, ages,\nand ethnicities. For each identity, the data includes high-\nquality 3D head and eyeball meshes, along with diffuse,\nnormal, roughness, and specular textures in high resolution\n(4K and 8K). Notably, the textures provided are identical to\nground truth and the diffuse map doesn\u2019t contain any light-\ning effects. We register all 3D meshes from the dataset to\nthe FLAME mesh format and align all of the texture maps\nwith FLAME UV mapping through commercial Wrap4D\n[2]. We annotate the dataset based on individual identity\nattributes: skin color, gender, ethnicity, and age.\nA.2. PBR Texture Decoders Training\nWe use the dataset to train separate decoders for normal,\nspecular and roughness textures estimation. We directly ap-\nply variational autoencoder (VAE) from SD-2.1-base model\nfor diffuse texture, we freeze the encoder, take the diffuse\ntexture as input and finetune 3 separate decoders to generate\nother maps over the dataset. We optimize the decoders by\nminimizing the loss function LD = ||D{n,s,r}(E(Im)) \u2212\nI{n,s,r}||2\n2 + \u03bbLlpips(D{n,s,r}(E(Im))), I{n,s,r})), where\nDn, Ds and Dr are normal, specular and roughness de-\ncoders, E(.) is the SD-2.1-base encoder, In, Is, Ir and Im\ncorrespond to normal, specular, roughness and diffuse tex-\nture maps respectively.\nA.3. Inpainting\nWe perform latent inpainting in the first (T \u2212 N) steps of\nthe diffusion denoising process. We downsample the visi-\nbility mask V to the latent visibility mask V \u2217 and encode\nIm into the latent code zm = E(Im).\nSimilar to [11],\nat each denoising step, we apply inpainting by updating\nz\u2217\nt = V \u2217 \u2299 (zm + \u03f5t) + (1 \u2212 V \u2217) \u2299 zt, where zt is the\ndenoised latent and \u03f5t is the scheduled noise for time-step t.\nWhen the image is used as the input, we use BLIP-2 [45]\nto generate caption which would eventually be fed to our\nAGT-DM. For neutral face mesh generation, we set expres-\nsion and pose parameters to zero.\nB. Evaluation Details\nB.1. Baselines\nLatent3d [14] uses text or image-based prompts to change\nthe shape and texture of a 3D model. It combines CLIP\nand a 3D GAN with a differentiable renderer to adjust the\ninput latent codes for specific attribute manipulation while\nkeeping the other attributes unchanged.\nCLIPMatrix [38] leverages CLIP text embeddings to cre-\nate high-resolution, articulated 3D meshes controlled by\ntext prompts.\nText2Mesh[50] stylizes 3D meshes based on text prompts,\nusing a neural style field network and CLIP model for style\nediting. It does not rely on pre-trained models or specialized\ndatasets.\nCLIPFace[9] uses text to control 3D faces\u2019 expressions and\nappearance. It combines 3D models and a generative model\nto make expressive, textured, and articulated faces with ad-\nversarial training and differentiable rendering.\nDreamFace [76] is a progressive text-guided method de-\nsigned to generate personalized, animatable 3D face assets\ncompatible with CG pipelines, enabling users to customize\nfaces with specific shapes, textures, and detailed anima-\ntions. Since DreamFace does not have any implementation\npublicly available, we used their website UI [3] to generate\nand download meshes with all PBR textures.\nFlameTex[24] is a PCA-based texturing model tailored for\nthe FLAME model, developed using 1500 randomly se-\nlected images from the FFHQ dataset and the base texture\nis from the Basel Face Model [54].\nPanoHead [8] makes view-consistent 360\u00b0 images of full\nhuman heads from unstructured images. It uses novel 3D\nGAN training and feature entanglement resolution tech-\nniques to create avatars from single images.\nB.2. Qualitative Comparison\nWe present eights samples respectively generated from our\nUltrAvatar, DreamFace and PanoHead under one lighting\ncondition in our quantitative comparison experiment for\nqualitative visualization, in Fig. 10. There are correspond-\ning eight prompts which are from our 40 prompts used in\ncomparison experiment.\nWe use Unreal Engine for ren-\ndering. We display results from three viewpoints (frontal\nview, left view at -45 degree angle, right view at 45 degree\nangle) for each method. Additionally, the middle images\nfrom PanoHead results, generated from the input prompts,\nare their and our inputs.\nIn the comparison, UltrAvatar\ndelivers higher quality results and achieves more accurate\nalignment between the input texts and the generated avatars.\nPanoHead provides satisfactory results but in a low resolu-\ntion, there are many artifacts along the edges and bound-\naries when zoomed, moreover, it is incapable of producing\nanimatable avatars.\nB.3. Evaluation from the GPT4-V\nThe recently released GPT-4V(sion) [5, 6] is recognized\nas an effective evaluation tool with outstanding human-\nalignment for images [72, 78].\nWe leverage GPT-4V to\nqualitatively rate the rendered images of generated avatars.\nWe request that GPT-4V conduct assessments based on the\nfive criteria: photo-realism, artifact minimization, skin tex-\n12\nEnvironment Map\n\u201cA European Caucasian man with \nfreckles, and green eyes, who is smiling.\u201d\n\u201cA middle-aged Asian man.\u201d\n\u201cA young Indian man with medium-sized \neyes and a clear, smooth complexion.\u201d\n\u201cOprah Winfrey.\u201d\n\u201cA Hispanic woman with arched eyebrows \nand a straight nose.\u201d\n\u201cA middle-aged Caucasian man with a fair, \nlined complexion.\u201d\n\u201cRobert Downey Jr.\u201d\n\u201cA young black man.\u201d\nUltrAvatar\nDreamFace\nPanoHead\nFigure 10. Qualitative Comparison. We show some results from our quantitative comparison experiment involving DreamFace and PanoHead. UltrAvatar\nproduces higher quality, greater diversity, better fidelity results, clearly outperforms the state-of-the-art methods.\n13\n\u201cRed eyes\u201d\n\u201cTeal hair\u201d\n\u201cGreen star tattoo on the neck\u201d\n\u201cBlue eyes\u201d\n\u201cTom Cruise\u201d\n\u201cWill Smith\u201d\n\u201cWrinkles and crow's feet on the face\u201d\n\u201cTattoo on the face\u201d\n10\nFigure 11. Texture Editing Results. Our AGT-DM has capability to execute texture editing, editing results are shown here, including\nchanging eye and hair colors, aging effects, and adding tattoos.\nture quality, textual prompt alignment, and the overall fo-\ncus and sharpness of the images. We define the situations\nin which a high or a low score will be assigned, prompt the\nGPT-4V API with the following instructions and draw the\ncomparison figure using five-point Likert scale based on the\navaerage score for each criterion.\nPlease act as a professional photography critic.\nYou are provided a picture of a human face.\nPlease rate it from five dimensions.\n1. Reality. Please score this picture for how\nmuch it looks like a real person. The score\nrange is 1-5. A score of 1 means the poorest\nreality and the picture is assumed to be fake\n, while a score of 5 means the highest\nreality and the picture captures a real\nperson.\n2. Alignment. Please score how much this picture\nreflects the theme {image_prompt}. The score\nrange is 1-5. A score of 1 means the provided\npicture does not reflect the theme at all,\nwhile a score of 5 means the picture\nperfectly reflects the theme.\n3. Focus and Sharpness. Please score how good\nthis portrait picture from the perspective of\nfocus and sharpness. The score range is 1-5.\nA score of 1 means the picture has a soft\nfocus and lacks sharpness, while a score of 5\nmeans it provides perfect focus and\nsharpness as a high-quality portrait.\n4. Artifacts. Please score the extent of\nartifacts in this picture. The score range is\n1-5. A score of 1 means the picture has\nunbearable amount of artifacts, while a score\nof 5 means the picture is almost artifact-\nfree.\n5. Texture. Please score the extent that the\npicture correctly exhibits the texture of the\nskin. A score of 1 means the skin in the\npicture looks extremely different from real\nhumans, while a score of 5 means the skin in\nthe picture looks very genuine.\nPlease evaluate the provided picture and return\nthe score to me in the following format:\n\u2019\u2019\u2019\nReality: []; Alignment: []; Focus and Sharpness:\n[]; Artifacts: []; Texture: [].\nYou should strictly follow the above format and\nput your actual evaluation score based on the\nabove criteria into each \u2019[]\u2019. Note this is\nvery important to my career. You should be as\nfair as possible.\nC. Additional Results\nC.1. Editing\nOur AGT-DM has the capability to perform texture editing\nthrough text prompts. To facilitate editing in our AGT-DM,\nwe set lower values (\u03c9p = 0.01, \u03c9e = 0.005) to our pho-\ntometric and edge guidance scalars to loosen the guidance\ncontrols and enable more effective editing. The editing re-\nsults, shown in Fig. 11, illustrate the efficacy.\nC.2. Out-of-Domain Generation\nUltrAvatar is capable of producing high-quality fictional\ncharacters, comic figures and diverse out-of-domain char-\nacters. The results, shown in Fig. 12, illustrate the high\nquality, extensive diversity and excellent fidelity of our Ul-\ntrAvatar generation.\nC.3. Animation\nWe show five animated video sequences to demonstrate the\nanimatability of generated avatars. From two source videos,\nwe extract the motion parameters (expression codes and\npose codes) from EMOCA, and then apply these to animate\nour generated avatars. Each animations is rendered from\ntwo different viewpoints under a specific lighting condition.\n14\n\u201cBatman\u201d\n\u201cWoody from Toy Story\u201d\n\u201cGreen Lantern\u201d\n\u201cRaya from Raya and the Last Dragon\u201d\n\u201cFiona from Shrek\u201d\n\u201cNight King\u201d\nFigure 12. Out-of-Domain Generation. UltrAvatar is able to generate high-quality fictional Characters, comic figures and diverse out-of-\ndomain characters.\nInput Images\nResults\nSemantic Masks\nFigure 13. DCE Results. The results indicate that our DCE model\nis effective in removing specular highlights and shadows.\nC.4. Ablation\nDCE Model.\nWe show more diffuse color extraction re-\nsults in the Fig. 13. We select three images of different\nobjects with specular highlights and shadows, and create se-\nmantic masks as input for our DCE model, the results pro-\nvide a better demonstration of the efficiency and accuracy\nof our DCE model in handling a range of lighting removal\ntasks.\nGenerated Mesh\nTexture Result \n(from EMOCA albedo model)\nResult \n(from EMOCA with texture mapping)\nGenerated Texture \nwith inpainting only\nGenerated Texture with \ninpainting and guidances\nFigure 14. Comparison with EMOCA and Texture Inpainting\n(w/o authenticity guidance). EMOCA and texture inpainting pro-\nduce textures with misalignment between the texture and the mesh\nand inconsistency in the boundary areas.\nAGT-DM.\nIn our comparative analysis, we evaluate our\nmethod against EMOCA and the texture inpainting tech-\nnique that does not incorporate authenticity guidance,\nshown in Fig. 14. Both EMOCA and the non-authenticity-\nguided texture inpainting tend to yield textures with incon-\nsistencies, primarily suffering from issues of misalignment\nbetween the generated texture and mesh and mismatch be-\ntween the visible and inpainted regions. Our AGT-DM ex-\ncels in generating consistent textures while effectively elim-\ninating misalignment between the mesh and the texture,\nthereby enhancing the overall coherence and quality of the\noutput.\n15\n"
  },
  {
    "title": "Single-View 3D Human Digitalization with Large Reconstruction Models",
    "link": "https://arxiv.org/pdf/2401.12175.pdf",
    "upvote": "4",
    "text": "Template-Free Single-View 3D Human\nDigitalization with Diffusion-Guided LRM\nZhenzhen Weng1, Jingyuan Liu2, Hao Tan2, Zhan Xu2, Yang Zhou2\nSerena Yeung-Levy1, Jimei Yang2\n1Stanford University, 2Adobe Research\n1{zzweng,syyeung}@stanford.edu,\n2{jingyliu,hatan,zhaxu,yazhou,jimyang}@adobe.com\nDense Novel Views\nInput Image\nHigh Quality Reconstruction\nIn-the-Wild Results\nTriplane NeRF\nDiffusion Model\nNovel view\nNovel view\nFig. 1: We present Human-LRM, a template-free large reconstruction model for feed-\nforward 3D human digitalization from a single image. Trained on a vast dataset com-\nprising multi-view capture and 3D scans, our model generalizes across a broader range\nof scenarios. Guided by dense novel views generated by a conditional diffusion model,\nour model can generate high-fidelity full body humans from a single image. Our project\nwebpage is at https://zzweng.github.io/humanlrm.\nAbstract. Reconstructing 3D humans from a single image has been\nextensively investigated. However, existing approaches often fall short\non capturing fine geometry and appearance details, hallucinating oc-\ncluded parts with plausible details, and achieving generalization across\nunseen and in-the-wild datasets. We present Human-LRM, a diffusion-\nguided feed-forward model that predicts the implicit field of a human\nfrom a single image. Leveraging the power of the state-of-the-art recon-\nstruction model (i.e., LRM) and generative model (i.e Stable Diffusion),\nour method is able to capture human without any template prior, e.g.,\nSMPL, and effectively enhance occluded parts with rich and realistic\ndetails. Our approach first uses a single-view LRM model with an en-\nhanced geometry decoder to get the triplane NeRF representation. The\nnovel view renderings from the triplane NeRF provide strong geometry\nand color prior, from which we generate photo-realistic details for the\narXiv:2401.12175v2  [cs.CV]  14 Mar 2024\n2\nWeng et al.\noccluded parts using a diffusion model. The generated multiple views\nthen enable reconstruction with high-quality geometry and appearance,\nleading to superior overall performance comparing to all existing human\nreconstruction methods.\n1\nIntroduction\nReconstructing 3D human models from a single image is an important research\ntopic in computer vision with an array of practical applications. These appli-\ncations encompass areas such as AR/VR, asset creation, relighting, and many\nmore. A plethora of techniques have been developed to address this challenging\ntask, each with its own set of advantages and limitations. Parametric reconstruc-\ntion methods, a.k.a. human mesh recovery (HMR) [13, 23, 58] regress pose and\nshape parameters of SMPL (Skinned Multi-Person Linear) human body mesh\nmodel [29], which does not include clothing details. This limits their utility in\napplications requiring realistic and detailed human representations. Conversely,\nimplicit volume reconstruction methods [39,40] capture fine-grained clothing de-\ntails with their pixel-aligned features but do not generalize across various poses.\nRecent hybrid approaches [51\u201353, 61\u201363] combine the advantages of paramet-\nric and implicit reconstruction methods by using the predicted SMPL body\nmesh as conditioning to guide the full clothed reconstruction. However, these\nSMPL-conditioned methods face inevitable limitations: SMPL prediction errors\npropagate to the subsequent full reconstruction stage, resulting in misalignment\nbetween the reconstructed mesh and the input image. The misalignment is exac-\nerbated when the poses are complex (Fig. 2 (a)). These errors are often irrepara-\nble and cannot be fully fixed by post-hoc optimization [51,52,63]. Moreover, these\nworks typically do not learn the appearances. For the works that perform joint\nprediction of geometry and appearances, the appearance predictions are blurry\nespecially on the occluded part (Fig. 2 (b)).\nMeanwhile, there have been various works that use NeRF [31] as a represen-\ntation to learn geometry as well as texture of humans, but these works typically\noverfit to single scenes [1, 19], which is not generalizable to new observations.\nRecently, feed-forward NeRF prediction models such as Large Reconstruction\nModel (LRM) [17] has been proposed, which enables generalizable 3D recon-\nstructions from arbitrary single image inputs. However, directly applying LRM\nto humans yields sub-optimal results even with fine-tuning (Fig. 2 (c)). Primar-\nily, the reconstructed surfaces tend to be coarse, not preserving enough details.\nSecond, the occluded part has collapsed appearances and appear blurry.\nIn this work, we present Human-LRM, a feed-forward model that predicts\nthe geometry and appearance of the human from a single image. We draw from\nthe insight that diffusion models yield high-quality, novel view hallucinations for\noccluded parts, while 3D reconstruction provide strong geometry and color prior\nto ensure the diffusion model\u2019s multi-view consistency. To that end, we design\na novel three-stage approach. We first use an enhanced Large Reconstruction\nModel [17] for single view NeRF prediction. The predicted NeRF captures the\nHuman-LRM\n3\nPIFu (ICCV 19)\nPIFu-HD (CVPR 20)\nICON (CVPR 22)\n(a) Geometry comparison to volumetric reconstruction methods\n(b) Appearance comparison to textured volumetric \nreconstruction methods\nPIFu (ICCV 19)\nGTA (Neurips 23)\nSIFU (CVPR 24)\nECON (CVPR 23)\nHuman-LRM (Ours)\nHuman-LRM (Ours)\nSHERF (ICCV 23)\nHuman-LRM (Ours)\n(c) Comparison to generalizable NeRF methods\n(*:LRM is finetuned on human data)\nD-IF (ICCV 2023) \nInput Image\nInput Image\nInput Image\nLRM* (ICLR 24)\nNormals from \nrendered depth\nFig. 2: Comparison of Human-LRM with SoTA single-view human reconstruction\nmethods on in-the-wild images. Compared to volumetric reconstruction methods, our\nmethod achieves superior generalizability to challenging poses (a) and higher fidelity\nappearance prediction (b). Compared to generalizable human NeRF methods (c), our\nresult achieves much better geometry quality.\ncoarse geometry and color of the person but lacks details. We then employ a\nnovel-view guided reconstruction stage to improve the overall quality of the per-\nson. Specifically, we leverage the generative capability of diffusion models to hal-\nlucinate high resolution novel views of the person. Outputs from the first stage\ncontain geometry and appearance information, and are used as conditioning to\nensure multi-view consistency during diffusion. Lastly, the novel-view genera-\ntions are used to guide the higher-fidelity geometry and appearance prediction.\nSince our method does not require any human template, we can easily scale up\ntraining by including multi-view human datasets to achieve improved generaliza-\ntion capability. Additionally, unlike existing models that predicts appearance in\na deterministic way, Human-LRM leverages the generative power from diffusion\nmodels to achieve higher-quality reconstruction.\nOur contributions can be summarized as below:\n\u2013 We introduce Human-LRM, a feed-forward model for reconstructing humans\nwith detailed geometry and appearance quality from a single image. Being\ntrained on an extensive dataset (more than 10K identities) including both\nmulti-view RGB data and 3D scans, our model attains substantially en-\nhanced generalizability and excels across a wider spectrum of scenarios.\n\u2013 As the core of our method, we propose a conditional diffusion model for gen-\nerating high-resolution novel views. Raw renderings from single-view LRM\nserve as spatial condition to provide geometry and appearance prior. Addi-\ntional reference networks help preserve the identity of the person. The gen-\n4\nWeng et al.\nerated novel views contain rich details, and are effective in guiding the final\nreconstruction of the human with high-fidelity geometry and appearance.\n\u2013 We perform extensive comparisons to existing single-view human reconstruc-\ntion works\n[1, 11, 16, 17, 27, 34, 37, 39, 40, 51\u201353, 61, 61\u201363]. We show that\nHuman-LRM outperforms previous methods significantly on a comprehen-\nsive evaluation set.\n2\nRelated Work\nParametric reconstruction. Many 3D human reconstruction works [13, 23,\n25,58] are built on mesh-based parametric body models, e.g., SMPL [29]. Given\nan input image, these methods, referred as Human Mesh Recovery (HMR), em-\nploy neural networks to predict the SMPL shape and pose parameters from\nwhich the target human body mesh is constructed. This SMPL-conditioned ap-\nproach greatly reduces the network output complexity and also can be adapted\nfor weakly-supervised training with 2D pose estimates via differentiable mesh\nrasterization [23, 48]. As SMPL models minimally-clothed human bodies with\na smooth mesh of fixed topology, it prevents these methods from reconstruct-\ning detailed geometry and texture. Nevertheless, the predicted SMPL mesh is a\nvery good proxy for the fully clothed reconstruction as it captures the base body\nshape and depicts its pose structure. The promise of HMR motivates follow-up\nworks to predict 3D offsets [2, 30, 33, 65] or build another layer of geometry on\ntop of the base body mesh to accommodate clothed human shapes [5,22]. How-\never, this \u201cbody+offset\" strategy lacks the flexibility to represent a wide-range\nof clothing types.\nImplicit reconstruction. Implicit-functions offer a topology-agnostic represen-\ntation for modeling human shapes. PiFU [39] uses pixel-aligned image features\nto predict 3D occupancy values and colors from sampled 3D points in a prede-\nfined grid. Building on this, PIFuHD [40] develops a high-resolution module to\npredict geometric and texture details with additional front-back normal maps as\ninput. While producing expressive reconstruction results for simple inputs like\nstanding humans against clean background, such models are not able to gener-\nalize well to in-the-wild scenarios and often yield broken and messy shapes on\nchallenging poses and lightings due to their limited model capacity and lack of\na holistic representation.\nHybrid reconstruction. An emerging type of approach leverages parametric\nbody models (e.g. SMPL [29]) to improve the generalizability of fully-supervised\nimplicit reconstruction methods. Starting from a given image and an estimated\nSMPL mesh, ICON [52] regresses shapes from locally-queried features to general-\nize to unseen poses. Wang et al. [45] extends ICON with a GAN-based generative\ncomponent. ECON [51] leverages variational normal integration and shape com-\npletion to preserve the details of loose clothing. D-IF [53] additionally models\nthe uncertainty of the occupancy through an adaptive uncertainty distribution\nfunction. GTA [61] uses a hybrid prior fusion strategy combining 3D spatial and\nSMPL prior-enhanced features. SIFU [62] further enhance the 3D features using\nHuman-LRM\n5\nside-view conditioned features. All these methods leverage SMPL prior and al-\nthough the incorporation of SMPL does enhance generalizability to large poses,\nthese methods are also constrained by the accuracy of SMPL predictions. Any\nerrors in the estimated SMPL parameters have a cascading effect on the subse-\nquent mesh reconstruction stage.\nHuman NeRFs. Neural Radiance Fields (NeRF) [31] marks a pivotal milestone\nin 3D reconstruction. NeRF empowers the learning of a 3D representation of an\nobject solely from 2D observations. While there exist several notable works that\nfocus on reconstructing human NeRF, these efforts often center around the single\nvideo [47] or image [19,49] fine-tuning setting at the cost of substantial compu-\ntational time, ranging from tens of minutes to hours. In contrast, our focus lies\non a feed-forward paradigm that radically reduces the time required for a model\nto predict a human NeRF from a single image, typically in mere seconds. A few\nrecent works [14, 24] also employ a feed-forward paradigm for generalizability,\nutilizing SMPL as a geometric prior and aggregating features from sparse obser-\nvations, yet they necessitate multiple views. A closer related work [18] considers\nfeed-forward human NeRF prediction from a single image. Nonetheless, their\nmethod replies on ground truth SMPL body meshes that limit their model rep-\nresentation power. Our method is completely template-free, making NeRF-based\nhuman reconstruction more accessible and practical for various scenarios.\nDiffusion-based novel view synthesis. Many recent works leverage diffusion\nmodels for novel view synthesis [9,26, 28,42,43, 46,50]. Maintaining multi-view\nconsistency in geometry and colors for the generated images remains a challenge.\nTo improve multi-view consistency, Zero123++ [42] uses reference attention to\npreserve the global information from the input image. SSDNeRF [10], Viewset\nDiffusion [43] and SyncDreamer [28] model the joint probability distribution of\nmulti-view images. GNVS [9] and ReconFusion [50] use predicted 3D latent or\nrenderings as conditioning to the diffusion model. We use the geometry and ap-\npearance renderings from a NeRF prediction as well as global information from\nthe input image and triplane features to ensure multi-view consistency. In con-\ntrast to works [9,26,42] that focus on novel view synthesis, we also reconstruct\nthe geometry. In contrast to ReconFusion [50], our method is feed-forward.\n3\nMethod\nHuman-LRM (overview in Figure 3) consists of three stages:\n\u2013 Stage I is built on top of LRM [17] that consists of two building blocks: a\ntransformer-based triplane decoder and triplane NeRF. In Sec. 3.1, we briefly\nintroduce triplane prediction as our model backbone and then in Sec. 3.2, we\nintroduce our improved triplane NeRF to enhance the surface reconstruction\nquality of humans.\n\u2013 Stage II takes the coarse renderings from the triplane NeRF and uses a\nspecialized diffusion model to generate high-fidelity novel views of the person\n(Sec. 3.3).\n6\nWeng et al.\nNovel View Coarse Renderings\nTriplanes\nSDF MLP\nRGB MLP\nQueried \nFeatures\nNovel View \ndirection\nNovel View Generations\nInput Image\nTriplane NeRF\nMulti-view \nReconstruction\nStage III: Multi-View Feed-Forward\nReconstruction (Section 3.4)\nStage II: Diffusion-Based Novel-View Generation\n(Section 3.3)\nStage I: Single-View Feed-Forward Reconstruction \n(Section 3.1-3.2)\nInput Image \nReference\nWeights\nDepth\nRGB\nEncoder\nTriplane\nDecoder\nFine Geometry and Appearance\nKey\nQuery\nValue\nGlobal \nGeometry/Appearance \nReference\nReference Attention\nReference Attention\nReference Attention\nNoise\nFig. 3: Overview of Human-LRM. Given a single image, we encode the image using\nViT [7], and employ a transformer to decode a triplane representation [8], followed\nby SDF and RGB MLPs for volumetric rendering of RGB and depths from novel\nviewpoints. Next, we use a conditional diffusion model to generate novel-views of the\nperson conditioning on the coarse geometry renderings. From the dense views generated\nby the diffusion model, we then use a multi-view reconstruction model to generate\nreconstruction of the person with fine geometry and textures.\n\u2013 Lastly, we reconstruct the person with fine geometry and appearance lever-\naging the diffused novel views as guide (Sec. 3.4).\n3.1\nSingle-view Triplane Decoder\nGiven an RGB image xinput as input, LRM first applies a pre-trained vision\ntransformer (ViT), DINO [7] to encode the image to patch-wise feature tokens\n{hi \u2208 R768}n\ni=1, where i denotes the i-th image patch, n is the total number of\npatches, and 768 is the latent dimension.\nIt then uses a transformer module to decode the image tokens into a 3D tri-\nplane [8]. Specifically, the decoder updates learnable tokens to the final triplane\nfeatures via camera modulation and cross-attention with the image tokens, sim-\nilar to the design of PerceiverIO [20]. More specifically, each transformer layer\ncontains a cross-attention, a self-attention, and a multi-layer perceptron (MLP)\nsub-layer, where the input tokens to each sub-layer are modulated [32] by the\ncamera features c. The cross-attention layer attends from the triplane features to\nthe image tokens, which can help link image information to the triplane. Then,\nthe self-attention layer further models the intra-modal relationships across the\nspatially-structured triplane entries.\nTriplane [8] is used as an efficient 3D representation. A triplane T contains\nthree axis-aligned feature planes TXY, TYZ and TXZ. In our implementation,\neach plane is of dimension hT \u00d7wT \u00d7dT where hT \u00d7wT is the spatial resolution,\nand dT is the number of feature channels. For any 3D point in the NeRF object\nbounding box [\u22121, 1]3 , we can project it onto each of the planes and query the\ncorresponding point features Txy, Tyz, Txz via bilinear interpolation, which is\nthen decoded for rendering (Section 3.2).\nHuman-LRM\n7\nIn short, given an input image I1 \u2208 RH\u00d7W \u00d73, we train an encoder E and\ndecoder D s.t. {hi}n\ni=1 = E(I1), and TXY, TYZ, TXZ = D({hi}n\ni=1, c)\n3.2\nTriplane NeRF\nTraditional neural volume rendering methods (as used in LRM [17]) model ge-\nometry through a generalized density function. The extraction of this geometry\nis achieved using a random level set of the density function, which often results in\nreconstructions that are noisy and of low fidelity. Hence, to improve the fidelity\nof the reconstructions, we predict Signed Distance Functions (SDF) instead of\ndensity. Specifically, we use two MLPs (i.e. \u201cSDF MLP\" and \u201cRGB MLP\" in Fig-\nure 3) to predict SDF and RGB from the point features queried from the triplane\nrepresentation T. The SDF MLP takes the point features and output SDF and\na latent vector hp. The RGB MLP takes the point features, latent vector and\nnormals at sampled points \u02c6np (computed from predicted SDF using finite differ-\nences) and output RGB values. That is, (hp, SDF) = MLPSDF(Txy, Tyz, Txz),\nRGB = MLPRGB(Txy, Tyz, Txz, hp, \u02c6np). For a ray r emanating from a camera\nposition o in direction v \u2208 R3, ||v|| = 1, defined by r(t) = o + tv, t \u2265 0, the\ncolor of the corresponding pixel in the rendered image is computed via numerical\nintegration\nI(r) =\nM\nX\ni=1\n\u03b1i\nY\ni>j\n(1 \u2212 \u03b1j)RGBi, \u03b1i = 1 \u2212 e\u2212\u03c3i\u03b4i\n(1)\nwhere \u03c3i is the density converted from SDF using VolSDF [54], and \u03b4i is the dis-\ntance between samples. Normals can be rendered using the same formula where\nwe integrate over predicted normals at sampled points instead.\nTraining objective. Our training data contains multiple views and their re-\nspective camera parameter per human. For each human, we randomly choose\na few side views, and render a random \u02c6x \u2208 Rh\u00d7w\u00d73 patch on each view. The\nground truth RGB values for the patch is x \u2208 Rh\u00d7w\u00d73. In addition, we render\nthe predicted depths and normals of the patch \u02c6d \u2208 Rh\u00d7w\u00d73 and \u02c6n \u2208 Rh\u00d7w,\nand supervise with depths maps d \u2208 Rh\u00d7w and normal maps n \u2208 Rh\u00d7w\u00d73. The\nsupervising depth and normal maps can be either ground-truth renderings or\noff-the-shelf predictions. The training objective of our single-view reconstruc-\ntion method is computed over losses from V rendered views, with the input view\nas well as (V \u2212 1) side views. Overall, the training objective is to minimize L,\nL = 1\nV\nV\nX\nv=1\n(LMSE(\u02c6xv, xv)\n(2)\n+ \u03bblpipsLLPIPS(\u02c6xv, xv)\n(3)\n+ \u03bbnLMSE(\u02c6nv, nv) + \u03bbdLDSI(\u02c6dv, dv))\n(4)\n+ \u03bbeikLEikonal\n(5)\nSubscript v means that the corresponding variable is for the vth supervising\nview. LMSE is the normalized pixel-wise L2 loss, LLPIPS is the perceptual image\n8\nWeng et al.\npatch similarity [60], and LDSI is the scale invariant depth loss [4]. LEikonal is\nthe Eikonal regularization [15] computed using SDF values of the sampled points\nalong the rays. \u03bblpips, \u03bbn, \u03bbd, and \u03bbeik are weight coefficients.\n3.3\nDiffusion-Based Novel View Generations\nThe triplane NeRF from Section 3.2 captures the geometry and appearance very\nwell through large scale training. However, it often has collapsed reconstruction\non the unseen parts, leading to blurry appearance on the back side of the person.\nTo address this limitation, we leverage the power of diffusion models to generate\nhigh-fidelity and realistic novel views as guidance for reconstructing the occluded\npart. Specifically, we condition on the novel-view coarse renderings rendered from\nthe triplane NeRF, and generate high-quality novel views using a diffusion model.\nThe coarse renderings consist of rendered RGB, depth and weights sum (i.e.\nsum of the weights in Eqn. 1). RGB and depth renderings provide the appearance\nand geometry information from the novel view. We use weights sum as a proxy\nfor the certainty for the rendered content, as parts that are visible often has high\nweights sum. In other words, weights sum is for the diffusion model to learn to\nhallucinate the less certain parts.\nUsing coarse renderings as the only conditioning to the diffusion model often\nresults in generations that do not preserve the identity of the person. Hence,\nwe additionally pass information from the input image and triplanes to the\ndenoiser through reference attention, to make the denoiser be aware of the global\ninformation of the person. Reference attention [59] essentially modifies each of\nthe self-attention layers in the UNet denoiser by concatenating the key and value\nwith that of the reference.\nFormally, the objective for diffusion model training is to minimize Ldiffusion.\nrcoarse = (\u02c6xv, \u02c6dv, \u02c6wv)\n(6)\nLdiffusion = Et\u223c[1,T ][||v \u2212 \u02c6v\u03b8(xnoised\nv\n; rcoarse, xinput, T, t)||2]\n(7)\nwhere \u02c6v\u03b8 is a UNet [38] (with trainable weights \u03b8) that does v-prediction. xnoised\nv\nis the noised GT view, xinput is the input image, and T is the triplanes.\n3.4\nNovel-View Guided Feed-Forward Reconstruction\nOnce we obtain dense novel view generations of the human as well as their\nviewpoints from Sec. 3.3, we use a multi-view reconstruction model to reconstruct\nthe human. The multi-view model is trained on the same data with the same\nobjective as the single-view model in Sec. 3.1. In contrast to the single-view\nmodel, the multi-view model incorporates camera conditioning within the ViT\nencoder through modulation [32]. We refer readers to LRM [17] for more details\nregarding the camera modulation. The triplane decoder in the multi-view model\nmaintains the same architecture as the single-view model, with the exception\nthat it does not take camera conditioning, as it is moved to ViT encoder.\nHuman-LRM\n9\n4\nExperiments\nTraining data. Our complete training set consists of 1,426 high-quality scans\n(500 from THuman 2.0 [56] and 926 from Alloy++), as well as around 8,000\nposed multi-view captures from HuMMan [6] v1.0. THuman 2.0 and HuMMan\nboth contain adults with simple clothing. Thus, to further evaluate the gener-\nalization capability, we collect Alloy++ from Human Alloy [3] and our internal\ncapture. Each scan from Human Alloy has around 40K polygons and our inter-\nnal capture, 100K polygons. The quality of those scans are similar to that of\nRenderPeople [36] (100K polygons). Alloy++ contains humans with more chal-\nlenging clothing, poses, as well as little kids.\nEvaluation sets. We evaluate on 20 humans from THuman 2.0 and 20 humans\nfrom Alloy++, each with renderings from 3 evenly spaced viewpoints. In addi-\ntion, we create an evaluation set from X-Human [41]. We randomly sample 2\nframes per sequence, which results in 460 frames from 20 human subjects, all\nwith distinct poses. The X-Human testset serves as an out-of-domain evaluation\nset as none of the models have seen images from this dataset during training.\nData preprocessing. For each scan from THuman 2.0 and Alloy++, we cen-\nter it by the origin and scale them so the longest side has length 1.8. We render\neach human scan from 32 randomly sampled viewpoints with the same camera\npointing toward the origin. For HuMMan v1.0, there are 10 cameras per pose.\nIn total, there are 16K, 14K, and 80K distinct input images from the training\nsplit of THuman 2.0, Alloy++ and HuMMan v1.0, respectively.\nImplementation details. We train our coarse single-view model and multi-\nview reconstruction model with 16 A100 GPUs for 7 days. We use batch size\nof 4 per GPU. During training, we sample rays on a random 64 by 64 patch.\nWe use importance sampling where we sample 48 coarse and 64 fine samples\nalong the rays. \u03bblpips = 2, \u03bbd = \u03bbn = 1, \u03bbeik = 0.5. We use cosine learning rate\nscheduler with initial learning rate of 2e \u2212 5. In SDF-density conversion, we use\nscheduled hyperparameters following [55] for stable optimization. For diffusion\nmodel, we initialize from the Stable Diffusion 2.0 UNet which was trained with\nv-prediction, and adapt the input layers for our input dimension. We then fine-\ntune the UNet on a single A6000 GPU with batch size of 4 and learning rate\nof 5e \u2212 5. We use a separate UNet for triplane conditioning, which was initial-\nized from the same model. The input layer is updated and rest of the models\nare frozen. During inference, we use 100 steps of Ancestral sampling with Euler\nmethod steps.\nInference time. It takes about 0.7 second for the image encoder and triplane\ndecoder to get the triplane representation from the input image(s), and 1.3 sec-\nonds to render a 256 by 256 image from the triplanes. Sampling from diffusion\nmodel takes about 5 seconds on a single A6000 GPU.\n4.1\nGeometry Comparisons\nWe compare to existing single-view human reconstruction methods PIFu [39],\nPIFu-HD [40], Pamir [63], ICON [52], ECON [51], D-IF [53], GTA [61], and\n10\nWeng et al.\nTable 1: Geometry comparison to existing single-view reconstruction methods. For\nall models requiring SMPL, PIXIE [13] is used for SMPL-X estimation.\n(a) Fair comparison of all methods on THuman 2.0 (with the same train-\ntest split). To further compare with SoTA methods that can work without\nSMPL, we additionally trained ablated GTA model (\u201cw/o SMPL\") taking\nout the SMPL-related features. (\u2020 We have verified with the authors of\nGTA [61] and SIFU [62] that the evaluation on THuman 2.0 is correct.)\nTHuman 2.0\nAlloy++\nX-Human\nModel\nSMPL - Free\nChamfer \u2193 P2S \u2193 NC \u2193\nChamfer \u2193 P2S \u2193 NC \u2193\nChamfer \u2193 P2S \u2193 NC \u2193\nPIFu [39]\n\u2713\n1.59\n1.53\n0.088\n1.89\n1.64\n0.099\n1.92\n1.89\n0.112\nPamir [63]\n\u2717\n3.39\n3.42\n0.129\n2.90\n3.02\n0.112\n2.78\n2.62\n0.102\nICON [52]\n\u2717\n3.26\n3.14\n0.128\n2.34\n2.26\n0.102\n2.24\n2.32\n0.104\nECON [51]\n\u2717\n3.48\n3.21\n0.114\n2.42\n2.31\n0.106\n2.28\n2.34\n0.109\nD-IF [53]\n\u2717\n3.34\n3.75\n0.126\n2.49\n2.52\n0.118\n2.32\n2.47\n0.109\nGTA [61] \u2020\n\u2717\n3.18\n3.86\n0.131\n2.26\n2.92\n0.134\n2.22\n2.13\n0.108\nSIFU [62] \u2020\n\u2717\n2.66\n3.63\n0.112\n2.68\n2.46\n0.109\n2.10\n2.08\n0.083\nGTA [61] retrained w/o SMPL\n\u2713\n1.84\n1.66\n0.089\n1.92\n1.73\n0.091\n1.89\n1.90\n0.080\nHuman-LRM (Stage I)\n\u2713\n1.27\n1.41\n0.074\n1.58\n1.48\n0.072\n1.23\n1.21\n0.065\nHuman-LRM (Full)\n\u2713\n1.26\n1.37\n0.071\n1.55\n1.43\n0.067\n1.21\n1.20\n0.064\n(b) Comparison of off-the-shelf models. The size of each training set is italicized.\nTHuman 2.0\nAlloy++\nX-Human\nModel\nTraining Data\nSMPL - Free\nChamfer \u2193 P2S \u2193 NC \u2193\nChamfer \u2193 P2S \u2193 NC \u2193\nChamfer \u2193 P2S \u2193 NC \u2193\nPIFu [39]\nRenderPeople-442\n\u2713\n2.13\n1.97\n0.134\n2.24\n2.19\n0.120\n1.96\n1.90\n0.120\nPIFu-HD [40]\nRenderPeople-450\n\u2713\n1.65\n1.68\n0.091\n1.94\n1.99\n0.099\n1.92\n2.04\n0.123\nPamir [63]\nTwindom [44]-900 + DeepHuman [64]-600\n\u2717\n3.46\n3.23\n0.123\n2.42\n2.38\n0.109\n2.51\n2.32\n0.107\nICON [52]\nRenderPeople-450\n\u2717\n3.28\n3.17\n0.120\n2.23\n2.29\n0.099\n2.37\n2.19\n0.102\nLRM [17]\nObjaverse [12] + MVImgNet [57] -730K in total\n\u2713\n2.36\n2.08\n0.109\n1.74\n1.82\n0.083\n2.21\n2.09\n0.103\nHuman-LRM (Stage I) THuman 2-500, Alloy++ -926, HuMMan v1-8000\n\u2713\n1.26\n1.40\n0.070\n1.58\n1.47\n0.068\n1.34\n1.31\n0.082\nHuman-LRM (Full) THuman 2-500, Alloy++ -926, HuMMan v1-8000\n\u2713\n1.24\n1.36\n0.068\n1.52\n1.43\n0.067\n1.20\n1.19\n0.062\nTable 2: Appearance comparison on THuman 2.0. Baseline numbers are from [62].\nModel\nDiffusion-based\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nARCH++ [16]\n\u2717\n10.6600\n-\n-\nPIFu [39]\n\u2717\n18.0934\n0.9117\n0.1372\nImpersonator++ [27]\n\u2717\n16.4791\n0.9012\n0.1468\nTEXTure [37]\n\u2713\n16.7869\n0.8740\n0.1435\nMagic123 [34]\n\u2713\n14.5013\n0.8768\n0.1880\nS3F [11]\n\u2717\n14.1212\n0.8840\n0.1868\nHumanSGD [1]\n\u2713\n17.3651\n0.8946\n0.1300\nGTA [61]\n\u2717\n18.0500\n-\n-\nSIFU [62]\n\u2717\n22.1024\n0.9236\n0.0794\nHuman-LRM (Stage I)\n\u2717\n22.8902\n0.9207\n0.0782\nHuman-LRM (Full)\n\u2713\n24.8050\n0.9364\n0.0604\nSIFU [62]. 1 For works requiring SMPL parameters as input to their model, we\nuse SMPL-X predictions from PIXIE [13]. To further compare to methods that\ncan work without SMPL, we additionally trained ablated versions of GTA and\nSIFU by taking out the SMPL-related features.\nFollowing previous works, we report Chamfer distance, Point-to-Surface (P2S)\nand Normal Consistency (NC). First, we compare with their public pretrained\nmodels. As some of the baseline methods are trained on the commercially avail-\nable RenderPeople, we opt for THuman 2.0, a publicly available dataset with a\nsimilar scale, to ensure a fair comparison. For GTA and SIFU, we evaluate their\nreleased models that are trained on THuman 2.0. We re-train other baselines on\nthe same dataset for fair comparison.\nWe report the quantitative results in Table 1. \u201cHuman-LRM (Stage I)\" is\nour single-view reconstruction stage as described in Section 3.1. \u201cHuman-LRM -\n1 Wang et al. [45] is another related work but we couldn\u2019t compare with it as there is no code\nrelease and their authors also informed us that their model checkpoints got lost.\nHuman-LRM\n11\nHuman-LRM\nSIFU (CVPR 24)\nGTA (Neurips 23)\nPIFu (ICCV 19)\nFig. 4: Geometry and appearance comparison with PIFu [39], GTA [61] and SIFU [62]\non in-the-wild images.\n(Full)\" is the final reconstruction result following the novel-view guided recon-\nstruction described in Section 3.4.\nAs shown by Table 1, the geometry predicted by our method consistently\noutperforms previous works, including works that are prior-free (PIFu, PIFu-\nHD, SMPL-free GTA) as well as works that require SMPL prior (Pamir, ICON\nand ECON, GTA, SIFU). The performance of SMPL-guided methods is generally\naffected by the errors from the predicted SMPL parameters. Even though [51\u2013\n53, 61, 62] utilize an optimization algorithm to optimize the SMPL parameters\nto match the predicted image normals, the errors in the SMPL parameters on\nsome images are still significant. Our method does not rely on a human mesh\ntemplate such as SMPL and thus does not suffer from this problem. As shown\nby Fig. 4 and Fig. 5, our method demonstrates exceptional generalizability to\nchallenging cases such as people in difficult poses.\n4.2\nAppearance Comparisons\nComparison to volumetric reconstruction methods. A few volumetric reconstruc-\ntion methods (PIFu [39], PIFu-HD [40], GTA [61], and SIFU [62]) consider joint\nreconstruction of geometry and color. We compare to those works in Table 2\nand show that the visual quality from our final mesh renderings are better than\nprevious methods, including the works that use diffusion models. 2 Qualitatively\n(Fig. 4), Human-LRM produces much better appearances, especially on the oc-\ncluded part. Better geometry quality also helps with better color prediction, as\nshown by the 3rd row.\nComparison to generalizable Human NeRF methods. Generalizable Human NeRF\nworks (e.g. NHP [24], MPS-NERF [14], SHERF [18]) focus is rendering qual-\nity instead of geometry. These works assume access to GT SMPL parameters,\n2 We are not able to compare to PIFu-HD [40]\u2019s appearance as their color inference module is not\nreleased.\n12\nWeng et al.\nPiFu-HD\nPiFu\nECON\nHuman-LRM (Stage I)\nLRM (Finetuned)\nGTA\nSIFU\nPiFu-HD\nPiFu\nECON\nHuman-LRM (Stage I)\nLRM (Finetuned)\nGTA\nSIFU\nHuman-LRM (Full)\nHuman-LRM (Full)\nFig. 5: Comparison of our single-view reconstruction model to previous volumetric\nreconstruction methods: PIFu [39], PIFu-HD [40], ECON [51], LRM [17], GTA [61],\nand SIFU [62]. All models are trained on THuman 2.0. For each example we show the\ngeometry (colored by vertex normals) from 4 views.\nwhich is impractical in in-the-wild scenarios. With estimated SMPL parameters\ntheir rendering quality declines significantly. For quantitative comparison to the\nSoTA method SHERF [18], we train our model on HuMMan v1.0 [6] and eval-\nuate the quality of novel view renderings by SSIM, PSNR and LPIPS following\ntheir evaluation protocol. Compared to SHERF with estimated SMPL parame-\nters, SSIM, PSNR and LPIPS from Human-LRM are 30% higher (better), 10%\nhigher (better), 45% lower (better), respectively. (Full table can be found in\nSupplementary.) The huge performance gap can be attributed to the fact that\nSHERF\u2019s pixel-aligned feature extraction relies heavily on the pixel alignment\nbetween the SMPL vertices and RGB. In contrast, our model does not rely on a\npose prior, which makes it more resilient and adaptable for in-the-wild scenarios.\nThis robustness is not only demonstrated through notably improved quantitative\nresults but also through the qualitative results illustrated in Fig. 6. In addition,\nthe geometry quality from Human-LRM is significantly better than SHERF, as\nshown in Fig. 2 (c).\nInput image\nHuman-LRM\nw/ est SMPL\nInput image\nHuman-LRM\nSHERF \nSHERF \nw/ est SMPL\nw/ GT SMPL\nw/ GT SMPL\nFig. 6: Novel view renderings results on HuMMan v1.0.\nHuman-LRM\n13\n4.3\nAblations\nAblations for single-view reconstruction method (Stage I).\nEffect of GT normal and depth. Although for our best model, we do use ground\ntruth normal and depth maps from geometry, we can replace them with es-\ntimated ones to achieve comparable performance. We additionally experiment\nwith supervising with predicted normals and depths from off-the-shelf predic-\ntors. While there is no notable drop in quantitative performance (Table 3), the\nsurface details are better with GT normal and depth supervision (Figure 7).\nEffect of predicting SDF. LRM [17] uses a single MLP to predict the density (\u03c3)\nand RGB, followed by NeRF volumetric rendering (i.e. experiment \u201cpredict \u03c3\"\nin Table 3). We noticed that the geometry predicted by this approach tended to\nbe more rudimentary in detail (Figure 7).\nEffect of the scale of training data. To further showcase the increased general-\nization ability of our method with more training data, we train with additional\ntraining data from Alloy++ (926 scans). As shown in Table 3, our model shows\nthe best performance, and this performance continues to enhance as we incor-\nporate additional training data. This underscores the significance of expanding\nthe scale of model training to improve generalizability.\nSupervise with estimated \nnormal, depth\nSupervise with GT \nnormal, depth\nPredict density \ninstead of SDF\nWithout normal, \ndepth supervision\nInput image\nFig. 7: Ablations for the single-view reconstruction model (Stage I). We show the effect\nof using estimated vs. ground truth normal and depth as supervision as well as using\na simple MLP as in LRM [17] to predict the density instead of SDF.\nStage I\nStage II\nStage III\nStage I\nStage II\nStage III\nInput Image\nInput Image\nFig. 8: Example novel view results after each stage. Results for Stage I and Stage III\nare mesh renderings. Results for Stage II are diffusion model outputs (i.e. images).\nAblations for diffusion-guided reconstruction model (Stage II-III).\n14\nWeng et al.\nEffect of number of novel views. We experiment with varying number of novel\nviews during Stage II and evaluate the final geometry quality. For each exper-\niment, we generate novel views from viewpoints evenly spaced around the 0-\nelevation level of the human. As shown in Table 4, with more views during Stage\nII, our final reconstruction gets increasingly accurate. We report final results\n(Table 1) using 12 views as reconstruction guide.\nEffect of diffusion model conditioning. In Table 4 we investigate the effect of the\neach diffusion model conditioning. As shown, each one of the proposed condi-\ntioning is useful in improving the generation quality and final geometry quality.\nTable 3: Ablations of our single-view reconstruction model (Stage I). Top: Effect of\nusing depth and normal maps (\u201cd.n.\") for supervision and predicting SDFs. Bottom:\nEffect of the scale of training data.\nTHuman 2.0\nModel\nTraining Data\nChamfer \u2193 P2S \u2193\nNC \u2193\nEst. d.n.\nTHuman 2.0, HuMMan v1.0\n1.35\n1.20\n0.076\nNo d.n.\nTHuman 2.0, HuMMan v1.0\n1.34\n1.20\n0.075\nPredict \u03c3\nTHuman 2.0, HuMMan v1.0\n1.28\n1.22\n0.072\nFull Model\nTHuman 2.0, HuMMan v1.0\n1.25\n1.20\n0.069\nSmall training set\nTHuman 2.0\n1.27\n1.41\n0.074\nMedium training set\nTHuman 2.0, HuMMan v1.0\n1.25\n1.20\n0.069\nLarge training set\nTHuman 2.0, Alloy++, HuMMan v1.0\n1.22\n1.02\n0.065\nTable 4: Ablations of diffusion-guided reconstruction model on THuman 2.0.\n(a) Effect of number of diffused views on the final geometry quality.\nNormal Consistency \u2193\n# of diffused views Chamfer \u2193\nP2S \u2193\nOverall\nFront\nRight\nBack\nLeft\n2\n1.994\n2.070\n0.0967\n0.0530\n0.1560\n0.0651 0.1119\n4\n1.389\n1.485\n0.0753\n0.0507\n0.1020\n0.0639 0.0840\n8\n1.284\n1.391\n0.0714\n0.0480\n0.0974\n0.0591 0.0810\n12\n1.264\n1.376\n0.0707\n0.0467\n0.0974\n0.0576 0.0809\n(b) Effect of diffusion model conditioning on diffusion model\noutput quality and final geometry quality.\nDiffusion Output Quality\nFinal Geometry Quality\nConditioning\nPSNR\u2191\nSSIM\u2191 LPIPS\u2193 Chamfer \u2193\nP2S \u2193\nNC \u2193\nw/o depth\n24.926\n0.937\n0.059\n1.399\n1.674\n0.0744\nw/o weights sum\n24.918\n0.936\n0.059\n1.355\n1.479\n0.0753\nw/o input image\n23.338\n0.931\n0.067\n1.345\n1.473\n0.0752\nw/o triplanes\n24.909\n0.937\n0.059\n1.271\n1.390\n0.0712\nFull\n25.067\n0.938\n0.058\n1.264\n1.376\n0.0707\n5\nConclusion and Future Work\nWe introduced a novel approach for reconstructing human NeRFs from a single\nimage. What sets our approach apart from previous implicit volumetric human\nreconstruction methods is its remarkable scalability, making it adaptable for\ntraining on large and diverse multi-view RGB datasets. Additionally, we pro-\nposed a coarse-to-fine reconstruction strategy guided by dense novel generations\nHuman-LRM\n15\nfrom a diffusion model. The dense novel views serve a strong geometry and tex-\nture guide that effectively enhances the overall quality of the final reconstruction.\nAlthough Human-LRM excels in capturing global geometry, it still falls short\nin preserving finer facial and hand details. Future directions include utilizing\nmore powerful representation than triplanes or additional refinement techniques.\nAcknowledgement The authors would like to thank Chun-Hao Huang for the\ninsightful discussions and valuable suggestions, which significantly contributed\nto the progression and shaping of this paper.\nSupplementary Materials\nA. Comparison to Generalizable Human NeRF Methods\nWe include additional novel view renderings results from SHERF [18] and Human-\nLRM on HuMMan v1.0 [6] in Figure 9. We include qualitative comparisons\nbetween SHERF\u2019s normals (computed from estimated depths) and our normal\npredictions. As shown, the geometry quality from Human-LRM is significantly\nbetter than SHERF.\nTable 5: Comparison to generalizable human NeRF methods on HuMMan v1.0 [6].\nTop section: Feed-forward methods that use GT SMPL parameters during inference.\nBottom section: methods that do not use GT SMPL during inference.\nMethod\nGT SMPL PSNR \u2191 SSIM \u2191 LPIPS \u2193\nNHP [24]\n\u2713\n18.99\n0.84\n0.18\nMPS-NERF [14]\n\u2713\n17.44\n0.82\n0.19\nSHERF [18]\n\u2713\n20.83\n0.89\n0.12\nSHERF [18]\n\u00d7\n14.46\n0.79\n0.20\nOurs\n\u00d7\n18.98\n0.82\n0.11\nTHuman 2.0\nAlloy ++\nX-Human\nMethod\nDepth Error (\u2193) Normal Error (\u2193) Depth Error (\u2193) Normal Error (\u2193) Depth Error (\u2193) Normal Error (\u2193)\nDPT\n2.44 \u00b1 0.84\n-\n2.91 \u00b1 1.24\n-\n3.17 \u00b1 1.20\n-\nZoeDepth\n2.20 \u00b1 0.96\n-\n2.41 \u00b1 1.01\n-\n2.08 \u00b1 0.88\n-\nHDNet\n2.27 \u00b1 0.80\n0.58 \u00b1 0.07\n2.37 \u00b1 1.04\n0.47 \u00b1 0.06\n2.30 \u00b1 0.83\n0.48 \u00b1 0.06\nHuman-LRM (Ours)\n0.91 \u00b1 0.39\n0.38 \u00b1 0.04\n1.79 \u00b1 0.78\n0.41 \u00b1 0.05\n1.28 \u00b1 0.49\n0.33 \u00b1 0.04\nTable 6: Comparison with HDNet [21], ZoeDepth [4], and DPT [35]. We report mean\nand standard deviation (\u00b1) across all test samples.\nB. Comparison to SoTA Depth and Normal Estimation Methods\nWe compare the depth quality of our reconstructed geometry to state-of-the-art\ndepth estimation works ZoeDepth [4] and DPT [35]. Furthermore, we include a\n16\nWeng et al.\nInput Image\nSHERF\nHuman-LRM\nNovel View Rendering\nHuman-LRM\nSHERF\nPredicted Normals\nFig. 9: Additional comparison to SHERF.\ncomparison of the depth and normal from our predicted geometry with HDNet\n[21], a method tailored for human surface reconstruction. The results (Table 6)\ndemonstrate that our approach surpasses all baseline methods across various\ndatasets. We include qualitative results for depth estimation in Figure 10, and\nnormal estimation in Figure 11.\nC. Personal and human subjects data.\nIn our experiments, we use public datasets THuman 2.0 3 [56], X-Human 4 [41],\nHumman v1.0 5 [6], and commercially available dataset Alloy 6 [3]. These datasets\nare widely used for human reconstruction research and we direct to their respec-\ntive website for information about their data collection procedures.\nD. Ethics statement.\nHuman-LRM is capable of transforming a single image into a 3D human. While\nHuman-LRM\u2019s results are not yet advanced enough to deceive human percep-\ntion, it\u2019s important to remain vigilant about possible ethical concerns. While we\n3 https://github.com/ytrock/THuman2.0-Dataset\n4 https://skype-line.github.io/projects/X-Avatar/\n5 https://caizhongang.com/projects/HuMMan/\n6 https://humanalloy.com\nHuman-LRM\n17\nGT Depth\nOurs\nHDNet\nInput Image\nZoeDepth\nDPT\nFig. 10: Depth comparison to HDNet [21], ZoeDepth [4] and DPT [35]. Red color\nmeans the region is closer.\ndiscourage such practices, there is a potential risk that the 3D human models\ncreated could be used to produce deceptive content.\nReferences\n1. AlBahar, B., Saito, S., Tseng, H.Y., Kim, C., Kopf, J., Huang, J.B.: Single-image\n3d human digitization with shape-guided diffusion. In: SIGGRAPH Asia (2023)\n2. Alldieck, T., Magnor, M., Bhatnagar, B.L., Theobalt, C., Pons-Moll, G.: Learning\nto reconstruct people in clothing from a single rgb camera. In: Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 1175\u2013\n1186 (2019)\n3. Alloy, H.: Human alloy (2023), https://humanalloy.com/\n4. Bhat, S.F., Birkl, R., Wofk, D., Wonka, P., M\u00fcller, M.: Zoedepth: Zero-shot transfer\nby combining relative and metric depth. arXiv preprint arXiv:2302.12288 (2023)\n5. Bhatnagar, B.L., Tiwari, G., Theobalt, C., Pons-Moll, G.: Multi-garment net:\nLearning to dress 3d people from images. In: Proceedings of the IEEE/CVF inter-\nnational conference on computer vision. pp. 5420\u20135430 (2019)\n6. Cai, Z., Ren, D., Zeng, A., Lin, Z., Yu, T., Wang, W., Fan, X., Gao, Y., Yu, Y.,\nPan, L., et al.: Humman: Multi-modal 4d human dataset for versatile sensing and\nmodeling. In: European Conference on Computer Vision. pp. 557\u2013577. Springer\n(2022)\n18\nWeng et al.\nGT Normal\nOurs\nHDNet\nInput Image\nFig. 11: Normal comparison to HDNet [21].\n7. Caron, M., Touvron, H., Misra, I., J\u00e9gou, H., Mairal, J., Bojanowski, P., Joulin,\nA.: Emerging properties in self-supervised vision transformers. In: Proceedings of\nthe IEEE/CVF international conference on computer vision. pp. 9650\u20139660 (2021)\n8. Chan, E.R., Lin, C.Z., Chan, M.A., Nagano, K., Pan, B., De Mello, S., Gallo,\nO., Guibas, L.J., Tremblay, J., Khamis, S., et al.: Efficient geometry-aware 3d\ngenerative adversarial networks. In: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. pp. 16123\u201316133 (2022)\n9. Chan, E.R., Nagano, K., Chan, M.A., Bergman, A.W., Park, J.J., Levy, A., Aittala,\nM., De Mello, S., Karras, T., Wetzstein, G.: Generative novel view synthesis with\n3d-aware diffusion models. arXiv preprint arXiv:2304.02602 (2023)\n10. Chen, H., Gu, J., Chen, A., Tian, W., Tu, Z., Liu, L., Su, H.: Single-stage diffu-\nsion nerf: A unified approach to 3d generation and reconstruction. arXiv preprint\narXiv:2304.06714 (2023)\n11. Corona, E., Zanfir, M., Alldieck, T., Bazavan, E.G., Zanfir, A., Sminchisescu, C.:\nStructured 3d features for reconstructing relightable and animatable avatars. arXiv\npreprint arXiv:2212.06820 (2022)\n12. Deitke, M., Schwenk, D., Salvador, J., Weihs, L., Michel, O., VanderBilt, E.,\nSchmidt, L., Ehsani, K., Kembhavi, A., Farhadi, A.: Objaverse: A universe of\nannotated 3d objects. In: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition. pp. 13142\u201313153 (2023)\n13. Feng, Y., Choutas, V., Bolkart, T., Tzionas, D., Black, M.J.: Collaborative regres-\nsion of expressive bodies using moderation. In: 2021 International Conference on\n3D Vision (3DV). pp. 792\u2013804. IEEE (2021)\n14. Gao, X., Yang, J., Kim, J., Peng, S., Liu, Z., Tong, X.: Mps-nerf: Generalizable 3d\nhuman rendering from multiview images. IEEE Transactions on Pattern Analysis\nand Machine Intelligence (2022)\n15. Gropp, A., Yariv, L., Haim, N., Atzmon, M., Lipman, Y.: Implicit geometric reg-\nularization for learning shapes. arXiv preprint arXiv:2002.10099 (2020)\nHuman-LRM\n19\n16. He, T., Xu, Y., Saito, S., Soatto, S., Tung, T.: Arch++: Animation-ready clothed\nhuman reconstruction revisited. In: Proceedings of the IEEE/CVF international\nconference on computer vision. pp. 11046\u201311056 (2021)\n17. Hong, Y., Zhang, K., Gu, J., Bi, S., Zhou, Y., Liu, D., Liu, F., Sunkavalli, K.,\nBui, T., Tan, H.: Lrm: Large reconstruction model for single image to 3d. arXiv\npreprint arXiv:2311.04400 (2023)\n18. Hu, S., Hong, F., Pan, L., Mei, H., Yang, L., Liu, Z.: Sherf: Generalizable human\nnerf from a single image. arXiv preprint arXiv:2303.12791 (2023)\n19. Huang, Y., Yi, H., Xiu, Y., Liao, T., Tang, J., Cai, D., Thies, J.: Tech: Text-guided\nreconstruction of lifelike clothed humans. arXiv preprint arXiv:2308.08545 (2023)\n20. Jaegle, A., Borgeaud, S., Alayrac, J.B., Doersch, C., Ionescu, C., Ding, D., Koppula,\nS., Zoran, D., Brock, A., Shelhamer, E., et al.: Perceiver io: A general architecture\nfor structured inputs & outputs. arXiv preprint arXiv:2107.14795 (2021)\n21. Jafarian, Y., Park, H.S.: Learning high fidelity depths of dressed humans by watch-\ning social media dance videos. In: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. pp. 12753\u201312762 (2021)\n22. Jiang, B., Zhang, J., Hong, Y., Luo, J., Liu, L., Bao, H.: Bcnet: Learning body and\ncloth shape from a single image. In: Computer Vision\u2013ECCV 2020: 16th European\nConference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XX 16. pp. 18\u2013\n35. Springer (2020)\n23. Kanazawa, A., Black, M.J., Jacobs, D.W., Malik, J.: End-to-end recovery of human\nshape and pose. In: Proceedings of the IEEE conference on computer vision and\npattern recognition. pp. 7122\u20137131 (2018)\n24. Kwon, Y., Kim, D., Ceylan, D., Fuchs, H.: Neural human performer: Learning\ngeneralizable radiance fields for human performance rendering. Advances in Neural\nInformation Processing Systems 34, 24741\u201324752 (2021)\n25. Li, Z., Liu, J., Zhang, Z., Xu, S., Yan, Y.: Cliff: Carrying location information in\nfull frames into human pose and shape estimation. In: European Conference on\nComputer Vision. pp. 590\u2013606. Springer (2022)\n26. Liu, R., Wu, R., Van Hoorick, B., Tokmakov, P., Zakharov, S., Vondrick, C.: Zero-\n1-to-3: Zero-shot one image to 3d object. In: Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision. pp. 9298\u20139309 (2023)\n27. Liu, W., Piao, Z., Tu, Z., Luo, W., Ma, L., Gao, S.: Liquid warping gan with\nattention: A unified framework for human image synthesis. IEEE Transactions on\nPattern Analysis and Machine Intelligence 44(9), 5114\u20135132 (2021)\n28. Liu, Y., Lin, C., Zeng, Z., Long, X., Liu, L., Komura, T., Wang, W.: Syncdreamer:\nGenerating multiview-consistent images from a single-view image. arXiv preprint\narXiv:2309.03453 (2023)\n29. Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black, M.J.: Smpl: A skinned\nmulti-person linear model. In: Seminal Graphics Papers: Pushing the Boundaries,\nVolume 2, pp. 851\u2013866 (2023)\n30. Ma, Q., Yang, J., Ranjan, A., Pujades, S., Pons-Moll, G., Tang, S., Black,\nM.J.: Learning to dress 3d people in generative clothing. In: Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6469\u2013\n6478 (2020)\n31. Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng,\nR.: Nerf: Representing scenes as neural radiance fields for view synthesis. Commu-\nnications of the ACM 65(1), 99\u2013106 (2021)\n32. Peebles, W., Xie, S.: Scalable diffusion models with transformers. In: Proceedings\nof the IEEE/CVF International Conference on Computer Vision. pp. 4195\u20134205\n(2023)\n20\nWeng et al.\n33. Pons-Moll, G., Pujades, S., Hu, S., Black, M.J.: Clothcap: Seamless 4d clothing\ncapture and retargeting. ACM Transactions on Graphics (ToG) 36(4), 1\u201315 (2017)\n34. Qian, G., Mai, J., Hamdi, A., Ren, J., Siarohin, A., Li, B., Lee, H.Y., Skorokhodov,\nI., Wonka, P., Tulyakov, S., et al.: Magic123: One image to high-quality 3d object\ngeneration using both 2d and 3d diffusion priors. arXiv preprint arXiv:2306.17843\n(2023)\n35. Ranftl, R., Bochkovskiy, A., Koltun, V.: Vision transformers for dense prediction.\nCoRR abs/2103.13413 (2021), https://arxiv.org/abs/2103.13413\n36. RenderPeople: Renderpeople (2018), https://www.renderpeople.com\n37. Richardson, E., Metzer, G., Alaluf, Y., Giryes, R., Cohen-Or, D.: Texture: Text-\nguided texturing of 3d shapes. arXiv preprint arXiv:2302.01721 (2023)\n38. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomed-\nical image segmentation. In: Medical Image Computing and Computer-Assisted\nIntervention\u2013MICCAI 2015: 18th International Conference, Munich, Germany, Oc-\ntober 5-9, 2015, Proceedings, Part III 18. pp. 234\u2013241. Springer (2015)\n39. Saito, S., Huang, Z., Natsume, R., Morishima, S., Kanazawa, A., Li, H.: Pifu:\nPixel-aligned implicit function for high-resolution clothed human digitization. In:\nProceedings of the IEEE/CVF international conference on computer vision. pp.\n2304\u20132314 (2019)\n40. Saito, S., Simon, T., Saragih, J., Joo, H.: Pifuhd: Multi-level pixel-aligned im-\nplicit function for high-resolution 3d human digitization. In: Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 84\u201393\n(2020)\n41. Shen, K., Guo, C., Kaufmann, M., Zarate, J., Valentin, J., Song, J., Hilliges, O.:\nX-avatar: Expressive human avatars. Computer Vision and Pattern Recognition\n(CVPR) (2023)\n42. Shi, R., Chen, H., Zhang, Z., Liu, M., Xu, C., Wei, X., Chen, L., Zeng, C., Su, H.:\nZero123++: a single image to consistent multi-view diffusion base model (2023)\n43. Szymanowicz, S., Rupprecht, C., Vedaldi, A.: Viewset diffusion:(0-) image-\nconditioned 3d generative models from 2d data. arXiv preprint arXiv:2306.07881\n(2023)\n44. Twindom: Twindom, https://web.twindom.com/\n45. Wang, J., Yoon, J.S., Wang, T.Y., Singh, K.K., Neumann, U.: Complete 3d human\nreconstruction from a single incomplete image. In: Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition. pp. 8748\u20138758 (2023)\n46. Watson, D., Chan, W., Martin-Brualla, R., Ho, J., Tagliasacchi, A., Norouzi, M.:\nNovel view synthesis with diffusion models. arXiv preprint arXiv:2210.04628 (2022)\n47. Weng, C.Y., Curless, B., Srinivasan, P.P., Barron, J.T., Kemelmacher-Shlizerman,\nI.: Humannerf: Free-viewpoint rendering of moving people from monocular video.\nIn: Proceedings of the IEEE/CVF conference on computer vision and pattern\nRecognition. pp. 16210\u201316220 (2022)\n48. Weng, Z., Wang, K.C., Kanazawa, A., Yeung, S.: Domain adaptive 3d pose aug-\nmentation for in-the-wild human mesh recovery. In: 2022 International Conference\non 3D Vision (3DV). pp. 261\u2013270. IEEE (2022)\n49. Weng, Z., Wang, Z., Yeung, S.: Zeroavatar: Zero-shot 3d avatar generation from a\nsingle image. arXiv preprint arXiv:2305.16411 (2023)\n50. Wu, R., Mildenhall, B., Henzler, P., Park, K., Gao, R., Watson, D., Srinivasan,\nP.P., Verbin, D., Barron, J.T., Poole, B., et al.: Reconfusion: 3d reconstruction\nwith diffusion priors. arXiv preprint arXiv:2312.02981 (2023)\nHuman-LRM\n21\n51. Xiu, Y., Yang, J., Cao, X., Tzionas, D., Black, M.J.: Econ: Explicit clothed humans\noptimized via normal integration. In: Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition. pp. 512\u2013523 (2023)\n52. Xiu, Y., Yang, J., Tzionas, D., Black, M.J.: Icon: Implicit clothed humans obtained\nfrom normals. In: 2022 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR). pp. 13286\u201313296. IEEE (2022)\n53. Yang, X., Luo, Y., Xiu, Y., Wang, W., Xu, H., Fan, Z.: D-if: Uncertainty-aware\nhuman digitization via implicit distribution field. In: Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision. pp. 9122\u20139132 (2023)\n54. Yariv, L., Gu, J., Kasten, Y., Lipman, Y.: Volume rendering of neural implicit\nsurfaces. Advances in Neural Information Processing Systems 34, 4805\u20134815 (2021)\n55. Yariv, L., Hedman, P., Reiser, C., Verbin, D., Srinivasan, P.P., Szeliski, R., Barron,\nJ.T., Mildenhall, B.: Bakedsdf: Meshing neural sdfs for real-time view synthesis.\narXiv preprint arXiv:2302.14859 (2023)\n56. Yu, T., Zheng, Z., Guo, K., Liu, P., Dai, Q., Liu, Y.: Function4d: Real-time human\nvolumetric capture from very sparse consumer rgbd sensors. In: IEEE Conference\non Computer Vision and Pattern Recognition (CVPR2021) (June 2021)\n57. Yu, X., Xu, M., Zhang, Y., Liu, H., Ye, C., Wu, Y., Yan, Z., Zhu, C., Xiong, Z.,\nLiang, T., et al.: Mvimgnet: A large-scale dataset of multi-view images. In: Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\npp. 9150\u20139161 (2023)\n58. Zhang, H., Tian, Y., Zhou, X., Ouyang, W., Liu, Y., Wang, L., Sun, Z.: Pymaf: 3d\nhuman pose and shape regression with pyramidal mesh alignment feedback loop.\nIn: Proceedings of the IEEE/CVF International Conference on Computer Vision.\npp. 11446\u201311456 (2021)\n59. Zhang, L.: Reference-only control. https://github.com/Mikubill/sd-webui-\ncontrolnet/discussions/1236 (2023)\n60. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable\neffectiveness of deep features as a perceptual metric. In: Proceedings of the IEEE\nconference on computer vision and pattern recognition. pp. 586\u2013595 (2018)\n61. Zhang, Z., Sun, L., Yang, Z., Chen, L., Yang, Y.: Global-correlated 3d-decoupling\ntransformer for clothed avatar reconstruction. Advances in Neural Information Pro-\ncessing Systems 36 (2024)\n62. Zhang, Z., Yang, Z., Yang, Y.: Sifu: Side-view conditioned implicit function for\nreal-world usable clothed human reconstruction. arXiv preprint arXiv:2312.06704\n(2023)\n63. Zheng, Z., Yu, T., Liu, Y., Dai, Q.: Pamir: Parametric model-conditioned implicit\nrepresentation for image-based human reconstruction. IEEE transactions on pat-\ntern analysis and machine intelligence 44(6), 3170\u20133184 (2021)\n64. Zheng, Z., Yu, T., Wei, Y., Dai, Q., Liu, Y.: Deephuman: 3d human reconstruction\nfrom a single image. In: Proceedings of the IEEE/CVF International Conference\non Computer Vision. pp. 7739\u20137749 (2019)\n65. Zhu, H., Zuo, X., Wang, S., Cao, X., Yang, R.: Detailed human shape estima-\ntion from a single image by hierarchical mesh deformation. In: Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition. pp. 4491\u20134500\n(2019)\n"
  },
  {
    "title": "Fast Registration of Photorealistic Avatars for VR Facial Animation",
    "link": "https://arxiv.org/pdf/2401.11002.pdf",
    "upvote": "1",
    "text": "Fast Registration of Photorealistic Avatars for VR Facial Animation\nChaitanya Patel1,2\nShaojie Bai2\nTe-Li Wang2\nJason Saragih2\nShih-En Wei2\n1Stanford University\n2Meta Reality Labs\nhttps://chaitanya100100.github.io/FastRegistration\nHMC Images\nPhotorealistic Avatar\nRegistered Expression and Head Pose\nVR headset with Face Cameras\nFast  \nRegistration\nFigure 1. On consumer VR headsets, oblique mouth views and a large image domain gap hinder high quality face registration.\nAs shown, the subtle lip shapes and jaw movement are often hardly observed. Under this setting, our method is capable of efficiently\nand accurately registering facial expression and head pose of unseen identities in VR with their photorealisitic avatars [5].\nAbstract\nVirtual Reality (VR) bares promise of social interac-\ntions that can feel more immersive than other media.\nKey to this is the ability to accurately animate a photore-\nalistic avatar of one\u2019s likeness while wearing a VR head-\nset. Although high quality registration of person-specific\navatars to headset-mounted camera (HMC) images is\npossible in an offline setting, the performance of generic\nrealtime models are significantly degraded. Online reg-\nistration is also challenging due to oblique camera views\nand differences in modality. In this work, we first show\nthat the domain gap between the avatar and headset-\ncamera images is one of the primary sources of diffi-\nculty, where a transformer-based architecture achieves\nhigh accuracy on domain-consistent data, but degrades\nwhen the domain-gap is re-introduced. Building on this\nfinding, we develop a system design that decouples the\nproblem into two parts: 1) an iterative refinement mod-\nule that takes in-domain inputs, and 2) a generic avatar-\nguided image-to-image style transfer module that is con-\nditioned on current estimation of expression and head\npose. These two modules reinforce each other, as im-\nage style transfer becomes easier when close-to-ground-\ntruth examples are shown, and better domain-gap re-\nmoval helps registration.\nOur system produces high-\nquality results efficiently, obviating the need for costly\noffline registration to generate personalized labels. We\nvalidate the accuracy and efficiency of our approach\nthrough extensive experiments on a commodity headset,\ndemonstrating significant improvements over direct re-\ngression methods as well as offline registration.\n1. Introduction\nPhotorealistic avatar creation has seen much progress in\nrecent years. Driven by advances in neural representa-\ntions and inverse rendering [19, 20, 27, 28], highly ac-\ncurate representations of individuals can now be gener-\nated even from limited captures such as phone scans [5]\nwhile supporting real time rendering for interactive ap-\nplications [30]. An emerging use case for such avatars is\nfor enabling social interactions in Virtual Reality (VR).\nThis application presents a particular problem where\nthe user\u2019s face is typically occluded from the environ-\nment by the VR headset. As such, it relies on headset-\nmounted cameras to drive a user\u2019s avatar. While accu-\nrate results have been demonstrated, they have been re-\nstricted to person-specific cases, where correspondences\nrelating the avatar to the headset mounted cameras rely\non the use of additional elaborate capture rigs [30].\nHighly accurate tracking in the more general case re-\nmains an open problem.\nIn this work, we demonstrate that generic facial ex-\n1\narXiv:2401.11002v1  [cs.CV]  19 Jan 2024\npression registration can be both accurate and efficient\non unseen identities, without relying on an additional\ncapture device to provide avatar-to-image correspon-\ndences. For this, we first demonstrate that acurate re-\nsults are possible when the modalities of the headset-\nmounted cameras and the user\u2019s avatar match, using a\nnovel transformer-based network that iteratively refines\nexpression estimation and head pose. Building on of this\nfinding, we propose to learn a cross-identity domain-\ntransfer function from the camera\u2019s domain to that of\nthe avatar. The core challenge here lies in the high accu-\nracy required of the domain transfer due to the challeng-\ning viewpoints of the face presented by headset mounted\ncameras; even a few pixels error can lead to significant\neffects in the estimated avatar\u2019s expression. A key de-\nsign in our method is that the iterative expression and\nhead pose estimation, and domain transfer reinforce one\nanother.\nOn the one hand, higher quality of domain\ntransfer results make the iterative refining easier. A sim-\nilar reinforcement holds in the other direction, from a\ndesign that conditions the domain-transfer function with\nmultiple avatar renderings, where the expressions and\nhead pose can be flexibly configured, including the one\nestimated from the refinement step. When the refined\nestimation is close to ground truth, the domain-transfer\nnetwork can easily reason locally using the input HMC\nimages and conditioning images.\nTo demonstrate the efficacy of our approach, we per-\nform experiments on a dataset of 208 identities, each\ncaptured in a multiview capture system [19] as well\nas a modified QuestPro headset [22], where the latter\nwas used to provide ground truth correspondence be-\ntween the driving cameras and the avatars. Compared\nto a direct regression method, our iterative constructions\nshows significantly improved robustness against novel\nappearance variations in unseen identities.\nIn summary, the contribution of this work include:\n\u2022 A demonstration that accurate and efficient generic\nface registration is achievable under matching camera-\navatar domains with an iterative transformer-based ar-\nchitecture on a neural rendering model.\n\u2022 A generalizing domain-transfer network that in flexi-\nbly conditioned on photorealistic avatar renderings of\nunseen identities.\n\u2022 The first generic expression estimation system from\ncommodity headset cameras that outperforms regres-\nsion methods and approaches person-specific level ac-\ncuracy on unseen identities without preprocessing.\nThe remaining of the paper is structured as follows. In\nthe next section a literature review is presented. Then, in\n\u00a73, we outline our method for generic facial expression\nestimation. In \u00a74, we demonstrate the efficacy of our\napproach through extensive experiments. We conclude\nin \u00a75 with a discussion of future work.\n2. Related Work\n2.1. VR Face Tracking\nWhile face tracking is a long studied problem, track-\ning faces of VR users from head mounted cameras\n(HMCs) poses an unique challenge.\nThe difficulty\nmainly comes from restrictions in camera placement and\nocclusion caused by the headset. Sensor images only\nafford oblique and partially overlapping views of facial\nparts. Previous work explored different ways to circum-\nvent these difficulties. In [16], a camera was attached\non a protruding mount to acquire a frontal view of the\nlower face, but with a non-ergonomic hardware design.\nIn [29], the outside-in third-person view camera limits\nthe range of a user\u2019s head pose. Both of these works\nrely on RGBD sensors to directly register the lower face\nwith a geometry-only model. To reduce hardware re-\nquirements, [23] used a single RGB sensor for the lower\nface and performed direct regression of blendshape coef-\nficients. The training dataset comprised of subjects per-\nforming a predefined set of expressions and sentences\nthat had associated artist-generated blendshape coeffi-\ncients.\nThe inconsistencies between subject\u2019s perfor-\nmances with the blendshape-labeled animation limited\nanimation fidelity.\nA VR face tracking system on a consumer headset\n(Oculus Rift) with photoreaslitic avatars [19] was firstly\npresented in [30]. They introduced two novelties: (1)\nThe concept of a training- and tracking-headset, where\nthe former has a super-set of cameras of the latter. After\ntraining labels were obtained from the training headset,\nthe auxiliary views from better positioned cameras can\nbe discarded, and a regression model taking only track-\ning headset\u2019s input was built. They also employed (2)\nanalysis-by-synthesis with differentiable rendering and\nstyle transfer to precisely register parameterized pho-\ntorealistic face models to HMC images, bridging the\nRGB-to-IR domain gap. The approach was extended\nin [27] via jointly learning the style-transfer and regis-\ntration together, instead of an independent CycleGAN-\nbased module. Although highly accurate driving was\nachieved, both [30] and [27] relied on person-specific\nmodels, the registration process required hours to days\nof training, and required the training headset with aux-\niliary camera views to produce ground truth. As such,\nthey cannot be used in a live setting where speed is re-\nquired and only cameras on consumer headsets are avail-\n2\nFigure 2. Examples of HMC images and corresponding ground truth expression rendered on their avatars from the offline regis-\ntration method [27], which utilizes augmented cameras with better frontal views (highlighted in green). In this work, we aim to\nefficiently register faces using cameras on consumer headsets, which only have oblique views (highlighted in red). In such views,\ninformation about subtle expressions (e.g., lip movements) are often cover very few pixels or even not visible.\nable. In this work, we demonstrate that a system trained\non a pre-registered dataset of multiple identities can gen-\neralize well to unseen identities\u2019 HMC captures within\nseconds. These efficiently generated image-label pairs\ncan be later used to adapt an unconditioned face tracker\nand make the animation more precise.\n2.2. Image Style Transfer\nThe goal of image style transfer is to render an image\nin a target style domain provided by conditioning infor-\nmation, while retaining semantic and structural content\nfrom an input\u2019s content. Convolutional neural features\nstarted to be utilized [11] to encode content and style in-\nformation. Pix2pix [13] learns conditional GANs along\nwith L1 image loss to encourage high-frequency sharp-\nness, with an assumption of availability of paired ground\ntruth. To alleviate the difficulty of acquiring paired im-\nages, CycleGAN [35] introduced the concept of cycle-\nconsistency, but each model is only trained for a spe-\ncific pair of domains, and suffers from semantic shifts\nbetween input and output. StarGAN [7] extends the con-\ncept to a fixed set of predefined domains. For more con-\ntinuous control, many explored text conditioning [2] or\nimages conditioning [1, 6, 8, 18, 31]. These settings usu-\nally have information imbalance between input and out-\nput space, where optimal output might not be unique. In\nthis work, given a latent-space controlled face avatar [5],\nalong with a ground-truth generation method [27], our\nimage style transfer problem can be simply directly su-\npervised, with conditioning images rendered from the\navatar to address the imbalance information problem.\n2.3. Learning-based Iterative Face Registration\nA common approach for high-precision face tracking in-\nvolves a cascade of regressors that use image features\nextracted from increasingly registered geometry. One of\nthe first methods to use this approach used simple lin-\near models raw image pixels [26], which was extended\nby using SIFT features [33]. Later methods used more\npowerful regressors, such as binary trees [4, 14] and in-\ncorporated the 3D shape representation into the formu-\nlation. Efficiency could be achieved by binary features\nand linear models [25].\nWhile these face tracking methods use current esti-\nmates of geometry to extract relevant features from im-\nages, similar cascade architectures have also been ex-\nplored for general detection and registration. In those\nworks, instead of extracting features using current es-\ntimates of geometry, the input data is augmented with\nrenderings of the current estimate of geometry, which\nsimplifies the backbone of the regressors in leveraging\nmodern convolutional deep learning architectures. For\nexample, Cascade Pose Regression [9] draws 2D Gaus-\nsians centered at the current estimates of body keypoints,\nwhich are concatenated with the original input, acting as\na kind of soft attention map. Similar design in [3] was\nused for 3D heatmap prediction. Xia et al. [32] applied\nvision transformer [10] to face alignment with landmark\nqueries. In this work, we demonstrate a transformer-\nbased network that doesn\u2019t require any guidance from\nlandmark to predict precise corrections of head pose and\nexpression from multiview images.\n3. Method\nWe aim to register the avatar model presented in [5]\nto multi-view HMC images denoted H = {Hc}c\u2208C,\nwhere each camera view Hc \u2208 Rh\u00d7w is a monochrome\ninfrared (IR) image and C is the set of available cam-\neras on a consumer VR headset (in this work, we primar-\nily focus on Meta\u2019s Quest Pro [22], see the Appendix).\nThey comprise a patchwork of non-overlapping views\nbetween each side of the upper and lower face. Some\nexamples are shown in Fig. 2. Due to challenging cam-\nera angles and headset donning variations, it is difficult\nfor the subtle facial expressions to be accurately recog-\nnized by machine learning models (e.g., see Fig. 7).\n3\nHMC input\n<latexit sha1_base64=\"YPN6HWASVu1hWz7o\nAMZz2HVXQI=\">AB8nicbVDLSgMxFL1TX7W+qi7dBIvgqsxIUZdFXbisYB8wHUomzbShmWRI\nMkIZ+hluXCji1q9x59+YaWehrQcCh3PuJeMOFMG9f9dkpr6xubW+Xtys7u3v5B9fCo2WqCG0\nTyaXqhVhTzgRtG2Y47SWK4jktBtObnO/+0SVZlI8mlCgxiPBIsYwcZKfj/GZkwz+5mg2rNr\nbtzoFXiFaQGBVqD6ld/KEkaU2EIx1r7npuYIMPKMLprNJPNU0wmeAR9S0VOKY6yOaRZ+jMKkMU\nSWfMGiu/t7IcKz1NA7tZB5RL3u5+J/npya6DjImktRQRYfRSlHRqL8fjRkihLDp5ZgopjNis\ngYK0yMbaliS/CWT14lnYu6d1lvPDRqzZuijKcwCmcgwdX0IR7aEbCEh4hld4c4z4rw7H4vRk\nlPsHMfOJ8/d8uRYw=</latexit>D\n<latexit sha1_base64=\"EBxAT+s+NlsMW\n2HPQL2Dn1n6Cg=\">AB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyIr2VREJcV7APaoWTST\nBuaZMYkUyhDv8ONC0Xc+jHu/Bsz7Sy09UDgcM693JMTxJxp47rfTmFldW19o7hZ2tre2d\n0r7x80dZQoQhsk4pFqB1hTziRtGY4bceKYhFw2gpGt5nfGlOlWSQfzSmvsADyUJGsLG\nS3xXYDAnm6d205/bKFbfqzoCWiZeTCuSo98pf3X5EkGlIRxr3fHc2PgpVoYRTqelbqJpj\nMkID2jHUokF1X46Cz1FJ1bpozBS9kmDZurvjRQLrScisJNZSL3oZeJ/Xicx4bWfMhknhk\noyPxQmHJkIZQ2gPlOUGD6xBPFbFZEhlhYmxPJVuCt/jlZdI8q3qX1YuH80rtJq+jCEdw\nDKfgwRXU4B7q0ACT/AMr/DmjJ0X5935mI8WnHznEP7A+fwBpyuSCQ=</latexit>F0\n<latexit sha1_base64=\"YPgT\nxVWmuRUIeVcCQ4kWsuVsIM=\">AB9XicbVDLSgMxFL1TX7W+qi\n7dBIvgqsyIr2XRTZcV7APasWQyaRuaSYko5Sh/+HGhSJu/Rd3/\no2ZdhbaeiDkcM695OQEMWfauO63U1hZXVvfKG6WtrZ3dvfK+wct\nLRNFaJNILlUnwJpyJmjTMNpJ1YURwGn7WB8m/ntR6o0k+LeTGLq\nR3go2IARbKz0AskD/Uksldan/bLFbfqzoCWiZeTCuRo9MtfvVC\nSJKLCEI617npubPwUK8MIp9NSL9E0xmSMh7RrqcAR1X46Sz1FJ1Y\nJ0UAqe4RBM/X3RojnUWzkxE2I73oZeJ/Xjcxg2s/ZSJODBVk/t\nAg4chIlFWAQqYoMXxiCSaK2ayIjLDCxNiSrYEb/HLy6R1VvUuqx\nd35XaTV5HEY7gGE7BgyuoQR0a0AQCp7hFd6cJ+fFeXc+5qMFJ\n985hD9wPn8ADZyS4w=</latexit>H\n<latexit sha1_base\n64=\"0JYDAyG74tUJaTrY+oUtAQoY2U=\">A\nAB8nicbVDLSgMxFL1TX7W+qi7dBIvgqsy\nIr2XRjcuK9gHToWTSTBuaSYkI5Shn+HGhS\nJu/Rp3/o2ZdhbaeiBwOdecu4JE860cd1vp\n7Syura+Ud6sbG3v7O5V9w/aWqaK0BaRXKp\nuiDXlTNCWYbTbqIojkNO+H4Nvc7T1RpJs\nWjmSQ0iPFQsIgRbKzk92JsRgTz7GHar9bcu\njsDWiZeQWpQoNmvfvUGkqQxFYZwrLXvuYk\nJMqwMI5xOK71U0wSTMR5S31KBY6qDbBZ5ik\n6sMkCRVPYJg2bq740Mx1pP4tBO5hH1opeL/\n3l+aqLrIGMiSQ0VZP5RlHJkJMrvRwOmKDF8\nYgkmitmsiIywsTYliq2BG/x5GXSPqt7l/\nWL+/Na46aowxHcAyn4MEVNOAOmtACAhKe4\nRXeHO8O/Ox3y05BQ7h/AHzucPjuiRcw=\n</latexit>S\nConditioning \nRenders\n<latexit sha1_base64=\"U5NgBVzIfpRXf\n+zdbcOGRje1VE=\">AB8nicbVDLSgMxFM3UV62vqks3wSK4KjPia1kUxGUF+4DpUDJp\ng3NJENyRyhDP8ONC0Xc+jXu/Bsz7Sy09UDgcM695NwTJoIbcN1vp7Syura+Ud6sbG3v7O\n5V9w/aRqWashZVQuluSAwTXLIWcBCsm2hG4lCwTji+zf3OE9OGK/kIk4QFMRlKHnFKwEp\n+LyYwokRkd9N+tebW3RnwMvEKUkMFmv3qV2+gaBozCVQY3zPTSDIiAZOBZtWeqlhCaFjM\nmS+pZLEzATZLPIUn1hlgCOl7ZOAZ+rvjYzExkzi0E7mEc2il4v/eX4K0XWQcZmkwCSdfx\nSlAoPC+f14wDWjICaWEKq5zYrpiGhCwbZUsSV4iycvk/Z3busXzyc1xo3R1ldISO0Sny\n0BVqoHvURC1EkULP6BW9OeC8O/Ox3y05BQ7h+gPnM8feyeRZg=</latexit>F\nDomain-transferred \nImage\nRegistration \nResults\nHMC  \ninput\nInitialization\n<latexit sha1_base64=\"YPgT\nxVWmuRUIeVcCQ4kWsuVsIM=\">AB9XicbVDLSgMxFL1TX7W+qi\n7dBIvgqsyIr2XRTZcV7APasWQyaRuaSYko5Sh/+HGhSJu/Rd3/\no2ZdhbaeiDkcM695OQEMWfauO63U1hZXVvfKG6WtrZ3dvfK+wct\nLRNFaJNILlUnwJpyJmjTMNpJ1YURwGn7WB8m/ntR6o0k+LeTGLq\nR3go2IARbKz0AskD/Uksldan/bLFbfqzoCWiZeTCuRo9MtfvVC\nSJKLCEI617npubPwUK8MIp9NSL9E0xmSMh7RrqcAR1X46Sz1FJ1Y\nJ0UAqe4RBM/X3RojnUWzkxE2I73oZeJ/Xjcxg2s/ZSJODBVk/t\nAg4chIlFWAQqYoMXxiCSaK2ayIjLDCxNiSrYEb/HLy6R1VvUuqx\nd35XaTV5HEY7gGE7BgyuoQR0a0AQCp7hFd6cJ+fFeXc+5qMFJ\n985hD9wPn8ADZyS4w=</latexit>H\n<latexit sha1_base64=\"ODIPRNUV1jtMHcRquMzZ2Xxvt6Y=\">A\nAB/XicbVA7T8MwGHR4lvIKj43FokJiqhLEa6xgYSyIPqQmqhzHa06dmQ7SCWK+CsDCDEyv9g49/gtBmg5STLp7vk8XJIwq7Tjf1sLi0v\nLKamWtur6xubVt7+y2lUglJi0smJDdACnCKCctTUj3UQSFAeMdILRdeF3HohUVPB7PU6IH6MBpxHFSBupb+97Q6QzLxAsVOPYXNldnvftml\nN3JoDzxC1JDZRo9u0vLxQ4jQnXmCGleq6TaD9DUlPMSF71UkUShEdoQHqGchQT5WeT9Dk8MkoIyHN4RpO1N8bGYpVkc1MxkgP1axXiP95v\nVRHl35GeZJqwvH0oShlUAtYVAFDKgnWbGwIwpKarBAPkURYm8KqpgR39svzpH1Sd8/rZ7entcZVWUcFHIBDcAxcAEa4AY0Qtg8AiewSt4s\n56sF+vd+piOLljlzh74A+vzB4Amles=</latexit> \u02c6R\n<latexit sha1_base64=\"C5qavcp1J/HSu+8jqnjMFuJ5Xjk=\">ACG3icbZBLSwMxEMezPmt9rXr0EiyCBym7xdex6MVj\nFfuAdlmy2bQNzSZLki3Upd/Di1/FiwdFPAke/DZm2z3U1oGQP7+ZYWb+Qcyo0o7zYy0tr6yurRc2iptb2zu79t5+Q4lEYlLHgnZCpAijHJS1Qz0olQVHASDMY3GT5pBIRQV/0KOYeBHqcdqlGmDfLvSCQL1SgyX/o49vXpLBjOg3sDfLvklJ1JwEXh\n5qIE8qj59lcnFDiJCNeYIaXarhNrL0VSU8zIuNhJFIkRHqAeaRvJUSUl05uG8NjQ0LYFdI8ruGEznakKFLZcqYyQrqv5nMZ/C/XTnT3ykspjxNOJ4O6iYMagEzo2BIJcGajYxAWFKzK8R9JBHWxs6iMcGdP3lRNCpl96J8fndWql7ndhTAITgCJ8AFl6A\nKbkEN1AEGT+AFvIF369l6tT6sz2npkpX3HIA/YX3/AiY5o10=</latexit>zt, vt, Rt\n<latexit sha1_base64=\"pUAcs/czsIuYlY6M4Nky81KXDto=\n\">ACDXicbVDLSsNAFJ3UV62vqEs3g1WoICURX8uiG5cV7APaECaTSTt0kgkzk0IN/QE3/obF4q4de/Ov3HSZlFbDwxzOde7\nr3HixmVyrJ+jMLS8srqWnG9tLG5tb1j7u41JU8EJg3MGRdtD0nCaEQaipG2rEgKPQYaXmD28xvDYmQlEcPahQTJ0S9iAYUI6U\nl1zyqdD3OfDkK9Zc+jl3rFM4qQ62cuGbZqloTwEVi56QMctRd87vrc5yEJFKYISk7thUrJ0VCUczIuNRNJIkRHqAe6WgaoZBIJ5\n1cM4bHWvFhwIV+kYITdbYjRaHMtOVIVJ9Oe9l4n9eJ1HBtZPSKE4UifB0UJAwqDjMoE+FQrNtIEYUH1rhD3kUBY6QBLOgR7/\nuRF0jyr2pfVi/vzcu0mj6MIDsAhqAbXIEauAN10AYPIEX8AbejWfj1fgwPqelBSPv2Qd/YHz9Aonum94=</latexit>(z0, v0)\n<latexit sha1_base64=\"pUAcs/czsIuYlY6M4Nky81KXDto=\n\">ACDXicbVDLSsNAFJ3UV62vqEs3g1WoICURX8uiG5cV7APaECaTSTt0kgkzk0IN/QE3/obF4q4de/Ov3HSZlFbDwxzOde7\nr3HixmVyrJ+jMLS8srqWnG9tLG5tb1j7u41JU8EJg3MGRdtD0nCaEQaipG2rEgKPQYaXmD28xvDYmQlEcPahQTJ0S9iAYUI6U\nl1zyqdD3OfDkK9Zc+jl3rFM4qQ62cuGbZqloTwEVi56QMctRd87vrc5yEJFKYISk7thUrJ0VCUczIuNRNJIkRHqAe6WgaoZBIJ5\n1cM4bHWvFhwIV+kYITdbYjRaHMtOVIVJ9Oe9l4n9eJ1HBtZPSKE4UifB0UJAwqDjMoE+FQrNtIEYUH1rhD3kUBY6QBLOgR7/\nuRF0jyr2pfVi/vzcu0mj6MIDsAhqAbXIEauAN10AYPIEX8AbejWfj1fgwPqelBSPv2Qd/YHz9Aonum94=</latexit>(z0, v0)\n<latexit sha1_base64=\"o46uQtWRs1arhHjmaqB7OKq7M=\">ACDXicbVDLSsNAFJ3UV62vqEs3g1WoICURX8ui\nG5cV+oI2hMlk0g6dZMLMpFBDf8CNv+LGhSJu3bvzb5y0WdTqgWEO59zLvfd4MaNSWda3UVhaXldK6XNja3tnfM3b2W5InApIk546LjIUkYjUhTUcVIJxYEhR4jbW94m/ntERGS8qihxjFxQtSPaEAxUlpyzaNKz+PMl+NQf+nDxG2cwnlpJUT\n1yxbVWsK+JfYOSmDHX/Or5HCchiRmSMqubcXKSZFQFDMyKfUSWKEh6hPupGKCTSafXTOCxVnwYcKFfpOBUne9IUSiz7XRliNRALnqZ+J/XTVRw7aQ0ihNFIjwbFCQMKg6zaKBPBcGKjTVBWFC9K8QDJBWOsCSDsFePkvaZ1V7cvqxf1\n5uXaTx1EB+AQVIANrkAN3IE6aAIMHsEzeAVvxpPxYrwbH7PSgpH37INfMD5/APnenCY=</latexit>(zT , vT )\nFigure 3. Overview of the method. We decouple the problem\ninto a avatar-conditioned image-to-image style transfer mod-\nule S and a iterative refinement module F. Module F0 initial-\nizes both modules by directly esimating on HMC input H.\nSetting.\nWe\ndenote\nthe\navatar\u2019s\ndecoder\nmodel\nfrom [5] as D. Following the same setting as in [5],\ngiven an input expression code z \u2208 R256, viewpoint v \u2208\nR6, and identity information of the ith subject, Ii, the\ndecoder is able to render this subject\u2019s avatar from the\ndesignated viewpoint by R = D(z, v|Ii) \u2208 Rh\u00d7w\u00d73.\nSpecifically, when we use v = vc; i.e., the viewpoint\nof a particular head-mounted camera (HMC), we\u2019ll ob-\ntain Rc = D(z, vc|Ii) \u2208 Rh\u00d7w\u00d73, which has the same\nsize as the corresponding Hc \u2208 Rh\u00d7w, except the latter\nis monochromatic. Following [5], the identity informa-\ntion Ii for a specific identity i is provided as multi-scale\nuntied bias maps to the decoder neural network. In this\npaper, we assume Ii is available for both training and\ntesting identities, either from the lightstage or a phone\nscanning1; and that the calibrations of all head-mounted\ncameras are known. We utilize the method in [27] to\nestablish ground truth HMC image-to-(z,v) correspon-\ndences, which relies a costly optimization process and\nan augmented additional camera set, C\u2032, which provides\nenhanced visbility. The examples are highlighted in the\ngreen boxes in Fig. 2. Our goal in this work is to estimate\nthe same optimal z and v for new identities leveraging\nthe avatar model (i.e., registration), while using only the\noriginal camera set C, highlighted in red boxes in Fig. 2.\n3.1. A Simplified Case: Matching Input Domain\nAccurate VR face registration entails exact alignment\nbetween Hc and Rc for each head-mounted camera c.\nHowever, a vital challenge here is their enormous do-\nmain gap: H = {Hc}c\u2208C are monochrome infrared im-\n1In this work we differentiate between unseen identities for avatar\ngeneration vs. unseen identities for HMC driving. We always as-\nsume an avatar for a new identity is available through methods in prior\nworks, and evaluate the performance of expression estimation methods\non unseen HMC images of that identity.\nTable 1. Registration accuracy in simplified setting: The er-\nrors are averaged across all frames in the test set. Augmented\ncameras means the use of camera set C\u2032 (which has better\nlower-face visbility) instead of C. Frontal Image L1 describes\nexpression prediction error, while rotation and translation er-\nrors describe the headpose prediction error.\nAug.\nCams\nFrontal\nImage L1\nRot.\nErr.\n(deg.)\nTrans.\nErr.\n(mm)\nSpeed\nOffline [27]\n\u2717\n0.784\n0.594\n0.257\n\u223c1 day\nRegression [12]\n\u2717\n2.920\n\u2212\n\u2212\n7ms\nRegression [12]\n\u2713\n2.902\n\u2212\n\u2212\n7ms\nOur F0(Rgt|D)\n\u2717\n1.652\n0.660\n0.618\n0.4sec\nOur F0(Rgt|D)\n\u2713\n1.462\n0.636\n0.598\n0.4sec\nOur F0(H|D)\n\u2717\n2.851\n1.249\n1.068\n0.4sec\nages with nearfield lighting and strong shadows, while\nR = {Rc}c\u2208C are renderings of an avatar built from\nuniformly lit colored images in the visible spectrum.\n[27, 30] utilized a style-transfer network to bridge this\ngap in a identity-specific setting. To simplify the prob-\nlem in the generic, multi-identity case, we first ask the\nquestion: what performance is possible when there is\nno domain difference? In order to study this, we re-\nplace H with Rgt = D(zgt, vgt) obtained from the\ncostly method in [27], which can be seen as a perfectly\ndomain-transferred result from H to the 3D avatar ren-\ndering space, that exactly retains expression. To extract\n(zgt, vgt) from Rgt, a na\u00a8\u0131ve way is to build a regres-\nsion CNN, such as MobileNetV3 [12], which can be ex-\ntremely efficient. Alternatively, given D is differentiable\nand the inputs are in the same domain, another straight-\nforward approach is to optimize (z, v) to fit to Rgt us-\ning pixel-wise image losses. As we show in Table 1,\nthe regression model is extremely lightweight but fails to\ngeneralize well; whereas this offline method (unsurpris-\ningly) generates low error, at the cost of extremely long\ntime to converge. Note that despite the simplification\nwe make on the input domain difference (i.e., assum-\ning access to Rgt rather than H), the registration is still\nchallenging due to the inherent oblique viewing angles,\nheadset donning variations and the need to generalize to\nunseen identities.\nIn contrast, we argue that a carefully designed func-\ntion that leverages avatar model (i.e., D) information,\nwhich we denote as F0(\u00b7|D), achieves a good balance:\n(1) it\u2019s feed-forward (no optimization needed for unseen\nidentities) so its speed can afford online usage; (2) it uti-\nlizes the renderings of D as a feedback to compare with\ninput Hc and minimize misalignment. Before we later\n4\nHMC Images\nStyle-transferred \nHMC Images\nPer-view \nConcat.\n<latexit sha1_base64=\"YPN6H\nWASVu1hWz7oAMZz2HVXQI=\">AB8nicbVDLSgMxFL1TX7W+qi7dB\nIvgqsxIUZdFXbisYB8wHUomzbShmWRIMkIZ+hluXCji1q9x59+YaWe\nhrQcCh3PuJeMOFMG9f9dkpr6xubW+Xtys7u3v5B9fCo2WqCG0Ty\naXqhVhTzgRtG2Y47SWK4jktBtObnO/+0SVZlI8mlCgxiPBIsYwc\nZKfj/GZkwz+5mg2rNrbtzoFXiFaQGBVqD6ld/KEkaU2EIx1r7npuY\nIMPKMLprNJPNU0wmeAR9S0VOKY6yOaRZ+jMKkMUSWfMGiu/t7IcK\nz1NA7tZB5RL3u5+J/npya6DjImktRQRYfRSlHRqL8fjRkihLDp5Zg\nopjNisgYK0yMbaliS/CWT14lnYu6d1lvPDRqzZuijKcwCmcgwdX0\nIR7aEbCEh4hld4c4z4rw7H4vRklPsHMfOJ8/d8uRYw=</latex\nit>D\nAlignment \nFeatures\nPositional \nEmbedding\nCamera \nEmbedding\nLearned \nQuery\nTransformer Encoder\nTransformer Decoder\n<latexit sha1_base64=\"OD\nIPRNUV1jtMHcRquMzZ2Xxvt6Y=\">AB/XicbVA7T8MwGH\nR4lvIKj43FokJiqhLEa6xgYSyIPqQmqhzHa06dmQ7SCWK+\nCsDCDEyv9g49/gtBmg5STLp7vk8XJIwq7Tjf1sLi0vL\nKamWtur6xubVt7+y2lUglJi0smJDdACnCKCctTUj3UQSFA\neMdILRdeF3HohUVPB7PU6IH6MBpxHFSBupb+97Q6QzLxAsV\nOPYXNldnvftmlN3JoDzxC1JDZRo9u0vLxQ4jQnXmCGleq6\nTaD9DUlPMSF71UkUShEdoQHqGchQT5WeT9Dk8MkoIyHN4R\npO1N8bGYpVkc1MxkgP1axXiP95vVRHl35GeZJqwvH0oShl\nUAtYVAFDKgnWbGwIwpKarBAPkURYm8KqpgR39svzpH1Sd8/\nrZ7entcZVWUcFHIBDcAxcAEa4AY0Qtg8AiewSt4s56sF\n+vd+piOLljlzh74A+vzB4Amles=</latexit> \u02c6R\n<latexit sha1_base64=\"3yt\nnthd4jVb4o6cghUVupjMBU=\">AB+XicbVC7TsMwFL0pr\n1JeAUaWiAqJqUoQr7GChbEg+pDaKHIcp7Xq2JHtVKqi/gkLA\nwix8ids/A1OmwEKR7J8dM698vEJU0aVdt0vq7Kyura+Ud2sb\nW3v7O7Z+wcdJTKJSRsLJmQvRIowyklbU81IL5UEJSEj3XB8W\n/jdCZGKCv6opynxEzTkNKYaSMFtj0IBYvUNDFX/jALdGDX3Y\nY7h/OXeCWpQ4lWYH8OIoGzhHCNGVKq7mp9nMkNcWMzGqDTJ\nEU4TEakr6hHCVE+fk8+cw5MUrkxEKaw7UzV39u5ChRTgzmS\nA9UsteIf7n9TMdX/s5WmCceLh+KMOVo4RQ1ORCXBmk0NQV\nhSk9XBIyQR1qasminBW/7yX9I5a3iXjYv783rzpqyjCkdwDK\nfgwRU04Q5a0AYME3iCF3i1cuvZerPeF6MVq9w5hF+wPr4BKqO\nUBQ=</latexit>Rt\nCurrent \nRendering\n<latexit sha1_base64=\"YPg\nTxVWmuRUIeVcCQ4kWsuVsIM=\">AB9XicbVDLSgMxFL1TX\n7W+qi7dBIvgqsyIr2XRTZcV7APasWQyaRuaSYko5Sh/+HGhS\nJu/Rd3/o2ZdhbaeiDkcM695OQEMWfauO63U1hZXVvfKG6Wtr\nZ3dvfK+wctLRNFaJNILlUnwJpyJmjTMNpJ1YURwGn7WB8m/\nntR6o0k+LeTGLqR3go2IARbKz0AskD/Uksldan/bLFbfqzoC\nWiZeTCuRo9MtfvVCSJKLCEI617npubPwUK8MIp9NSL9E0xmS\nMh7RrqcAR1X46Sz1FJ1YJ0UAqe4RBM/X3RojnUWzkxE2I73o\nZeJ/Xjcxg2s/ZSJODBVk/tAg4chIlFWAQqYoMXxiCSaK2ayI\njLDCxNiSrYEb/HLy6R1VvUuqxd35XaTV5HEY7gGE7Bgyuo\nQR0a0AQCp7hFd6cJ+fFeXc+5qMFJ985hD9wPn8ADZyS4w=<\n/latexit>H\n<latexit sha1_base64=\"/ehm\n+4gahrDYVG4QcoI95JyomVY=\">ACDHicbVDLSgMxFM34rPV\ndelmsAgVpMyIr2XRjcsK9gHtUDJpg3NJENyp1CHfoAbf8WNC0\nXc+gHu/Bsz7Sxq64WQwzncu89fsSZBsf5sZaWV1bX1nMb+c2t\n7Z3dwt5+XctYEVojkvV9LGmnAlaAwacNiNFcehz2vAHt6neGF\nKlmRQPMIqoF+KeYAEjGAzVKRLbV/yrh6F5ksex04nSWGhjgx\nLqfsTMpeBG4Giraqfw3e5KEodUAOFY65brROAlWAEjnI7z7V\njTCJMB7tGWgQKHVHvJ5JixfWyYrh1IZ4Ae8LOdiQ41Ol2xhli\n6Ot5LSX/01oxBNdewkQUAxVkOiIuQ3STpOxu0xRAnxkACaKmV\n1t0scKEzD5U0I7vzJi6B+VnYvyxf358XKTRZHDh2iI1RCLrpC\nFXSHqiGCHpCL+gNvVvP1qv1YX1OrUtW1nOA/pT19Qv+S5w8</l\natexit>(zt, vt)\nCNN\nCNN\nCurrent \nEstimation\n<latexit sha1_base64=\"/ehm+4gahrDYVG4QcoI95JyomVY=\">ACDHicbVDLSgMxFM34rPVdelmsAgVpMyIr2XRjcsK9gHtU\nDJpg3NJENyp1CHfoAbf8WNC0Xc+gHu/Bsz7Sxq64WQwzncu89fsSZBsf5sZaWV1bX1nMb+c2t7Z3dwt5+XctYEVojkvV9LGmnAlaAwacNiNFcehz2vAHt6neGFKlmRQPMIqoF+KeYAEjGAzVKRLbV/yrh6F5ksex04nSWGhjgxLqfsTMpeBG4Giraqfw3e5KE\nodUAOFY65brROAlWAEjnI7z7VjTCJMB7tGWgQKHVHvJ5JixfWyYrh1IZ4Ae8LOdiQ41Ol2xhli6Ot5LSX/01oxBNdewkQUAxVkOiIuQ3STpOxu0xRAnxkACaKmV1t0scKEzD5U0I7vzJi6B+VnYvyxf358XKTRZHDh2iI1RCLrpCFXSHqiGCHpCL+gNvVvP1qv1Y\nX1OrUtW1nOA/pT19Qv+S5w8</latexit>(zt, vt)\nCurrent \nEstimation\nshared \nweights\n<latexit sha1_base64=\"zh4sxqC/xFZgUMRYeJ8CXh7CQnw=\">ACG3icbVDLSgMxFM34rPU16tJNsAgVpMwUX8uiLlxWsA/olJLJpG1o5kFyp1CH/ocbf8WN\nC0VcCS78GzPtLTtgZDOfdy7z1uJLgCy/oxlpZXVtfWcxv5za3tnV1zb7+uwlhSVqOhCGXTJYoJHrAacBCsGUlGfFewhju4Sf3GkEnFw+ABRhFr+6QX8C6nBLTUMctF5YJINhxQ+Gpka+/5HcgVO8wBhq46RjFqySNQGeJ3ZGCihDtWN+OV5IY58FQAVRqmVbEbQTIoFTwcZ5J1YsInRAeqylaUB8ptrJ5LYxPtaKh7uh1C8APFH\n/diTEV+l2utIn0FezXiou8loxdK/aCQ+iGFhAp4O6scAQ4jQo7HJKIiRJoRKrnfFtE8koaDjzOsQ7NmT50m9XLIvSuf3Z4XKdRZHDh2iI1RENrpEFXSHqiGKHpCL+gNvRvPxqvxYXxOS5eMrOcA/YPx/Qt+z6G+</latexit>(\u2206zt, \u2206vt)\nEstimation \nupdates\nConcatenation\nAddition\nFigure 4. Iterative refinement module F. For each view c \u2208 C, a shared CNN encodes the alignment information between the\ncurrent rendering Rt,c and input images Hc along with style-transferred images \u02c6Rc into a feature grid. After adding learnable grid\npositional encoding and camera-view embedding, the grid features concatenated with the current estimate (zt, vt) and are flattened\ninto a sequence of tokens. These tokens are processed by a transformer module with a learnable decoder query to output residual\nupdates to the estimation.\nSWA\nSWA\nSWA\nConditioning \nImages from \nestimated\nStyle-transferred \nImage\n<latexit sha1_base64=\"ODIPRNUV1jtMHcRquMzZ2Xxvt6Y=\">AB/XicbVA7T8MwGHR4lvIKj43FokJiqhLEa6xgYSyIPqQmqhzHa\na06dmQ7SCWK+CsDCDEyv9g49/gtBmg5STLp7vk8XJIwq7Tjf1sLi0vLKamWtur6xubVt7+y2lUglJi0smJDdACnCKCctTUj3UQSFAeMdILRdeF3HohUVPB7PU6IH6MBpxHFSBupb+97Q6QzLxAsVOPYXNldnvftmlN3JoDzxC1JDZRo9u0vLxQ4jQnXmCGleq6TaD9DUlPMSF7\n1UkUShEdoQHqGchQT5WeT9Dk8MkoIyHN4RpO1N8bGYpVkc1MxkgP1axXiP95vVRHl35GeZJqwvH0oShlUAtYVAFDKgnWbGwIwpKarBAPkURYm8KqpgR39svzpH1Sd8/rZ7entcZVWUcFHIBDcAxcAEa4AY0Qtg8AiewSt4s56sF+vd+piOLljlzh74A+vzB4Amles=</latexit\n> \u02c6R\n<latexit sha1_base64=\"YP\ngTxVWmuRUIeVcCQ4kWsuVsIM=\">AB9XicbVDLSgMxFL\n1TX7W+qi7dBIvgqsyIr2XRTZcV7APasWQyaRuaSYko5Sh\n/+HGhSJu/Rd3/o2ZdhbaeiDkcM695OQEMWfauO63U1hZXV\nvfKG6WtrZ3dvfK+wctLRNFaJNILlUnwJpyJmjTMNpJ1YUR\nwGn7WB8m/ntR6o0k+LeTGLqR3go2IARbKz0AskD/Uksld\nan/bLFbfqzoCWiZeTCuRo9MtfvVCSJKLCEI617npubPwUK\n8MIp9NSL9E0xmSMh7RrqcAR1X46Sz1FJ1YJ0UAqe4RBM/X\n3RojnUWzkxE2I73oZeJ/Xjcxg2s/ZSJODBVk/tAg4chIl\nFWAQqYoMXxiCSaK2ayIjLDCxNiSrYEb/HLy6R1VvUuqxd3\n5XaTV5HEY7gGE7BgyuoQR0a0AQCp7hFd6cJ+fFeXc+5q\nMFJ985hD9wPn8ADZyS4w=</latexit>H\nHMC input\nSWA\nSWA\nSliding Window \nAttention\n=\nPreset expr.\nFigure 5. Style transfer module S. Given an estimate of\n(z0, v0), conditioning images are generated from the same es-\ntimate and M other key expressions, concatenated channel-\nwise and encoded by a U-Net encoder. Input HMC image is\nencoded by a separate U-Net encoder. Sliding window based\nattention [24] modules are used to fuse input features and con-\nditioning features to compensate for the misalignment between\nthem. These fused features are provided as the skip connection\nin the U-Net decoder to output style-transferred image.\ndescribe F0 in \u00a7 3.3, we report the results of aforemen-\ntioned methods under this simplified setting in Table 1.\nSpecifically, we show that F0 can achieve perfor-\nmance approaching that of offline registration [27]. In\ncontrast, na\u00a8\u0131ve direct regressions perform substantially\nworse, even with the augmented set of cameras. This\nhighlights the importance of conditioning face registra-\ntion learning with information about the target identity\u2019s\navatar (in our case, D). But importantly, when reverting\nback to the real problem, by replacing Rgt with H, the\nperformance of F0 also degrades significantly. This ob-\nservation demonstrates the challenge posed by input do-\nmain gap difference, and motivates us to decouple style\ntransfer problem from registration, as we describe next.\n3.2. Overall Design\nIn light of the observation in \u00a73.1, we propose to de-\ncouple the problem into the learning of two modules:\nan iterative refinement module, F, and a style transfer\nmodule, S. The goal of F is to produce an iterative up-\ndate to the estimate expression z and headpose v of a\ngiven frame. However, as Table 1 shows, conditioning\non avatar model D alone is not sufficient; good perfor-\nmance of such F relies critically on closing the gap be-\ntween H and Rgt. Therefore, module F shall rely on\nstyle transfer module S for closing this monochromatic\ndomain gap. Specifically, in addition to raw HMC im-\nages H, we also feed a domain-transferred version of\nthem (denoted \u02c6R), produced by S, as input to F. In-\ntuitively, \u02c6R should then resemble avatar model D\u2019s ren-\nderings with the same facial expression as in H. (And as\nTable 1 shows, if \u02c6R \u2248 Rgt, one can obtain really good\nregistration.) Differing from the common style transfer\nsetting, here the conditioning information that provides\n\u201cstyle\u201d to S is the entire personalized model D(\u00b7|Ii) it-\nself. As such, we have the options of providing various\nconditioning images to S by choosing expression and\nviewpoints to render. Through out experiments, we find\nthat selecting frames with values closer to (zgt, vgt) im-\nproves the quality of S\u2019s style transfer output.\n5\nTherefore,\na desirable mutual reinforcement is\nformed: the better S performs, the lower the errors of F\nare on face registration; in turn, the better F performs,\nthe closer rendered conditioning images will be to the\nground truth, simplifying the problem for S. An initial-\nization (z0, v0) = F0(H) for this reinforcement pro-\ncess can be provided by any model that directly works\non monochromatic inputs H. Fig. 3 illustrates the over-\nall design of our system. In what follows, we will de-\nscribe the design of each module.\n3.3. Transformer-based Iterative Refinement\nNetwork\nThe role of the iterative refinement module, F, is to pre-\ndict the updated parameters (zt+1, vt+1) from input and\ncurrent rendering:\n[zt+1, vt+1] = F\n\u0010\nH, \u02c6R, Rt\n\u0011\n, Rt = D(zt, vt) (1)\nwhere t \u2208 [1, T] is number of steps and \u02c6R = S(H)\nis the style transferred result (see Fig. 4). F can eas-\nier reason about the misalignment between input H and\ncurrent rendering D(zt, vt), with the aid of S(H) to\nbridge the domain gap. In Fig. 4, we show the hybrid-\ntransformer [10] based architecture of F. We will show\nin \u00a74.2 that this hybrid-transformer structure is a crucial\ndesign choice for achieving generalization across iden-\ntities. The transformer layers help to fuse feature pyra-\nmid from multiple camera views while avoiding model\nsize explosion and information bottleneck. Output of the\nmodel is treated as (\u2206zt, \u2206vt) and added to (zt, vt)\nto yield the new estimate for the next iteration. Fig. 6\nshows the progression of Rt over the steps. This itera-\ntive refinement module is trained to minimize:\nLF = \u03bbfrontLfront + \u03bbhmcLhmc,\n(2)\nwhere\nLhmc =\nT\nX\nt=1\nX\nc\u2208C\n\u2225D(zt, vt,c|Ii) \u2212 D(zgt, vgt,c|Ii)\u22251\nLfront =\nT\nX\nt=1\n\u2225D(zt, vfront|Ii) \u2212 D(zgt, vfront|Ii)\u22251\nHere, vfront is a predefined frontal view of the rendered\navatar (see Fig. 2 for an example). While Lhmc encour-\nages alignment between the predicted and input HMC\nimages, Lfront promotes an even reconstruction over the\nentire face to combat effects of oblique viewing angles\nin the HMC images.\nWhile F0 could be any module that works on HMC\nimages H for the purpose of providing {z0, v0}, for\nconsistency, we simply set F0 to also be iterative refin-\ning, where the internal module is the same as Eq. (1),\nexcept without \u02c6R as input.\n3.4. Avatar-conditioned Image-to-image Style\nTransfer\nThe goal of the style transfer module, S, is to directly\ntransform raw IR input images H into \u02c6R that resembles\nthe avatar rendering Rgt of that original expression. Our\nsetting differs from the methods in the literature in that\nour style-transferred images need to recover identity-\nspecific details including skin-tone, freckles, etc., that\nare largely missing in the IR domain; meanwhile, the\nillumination differences and oblique view angle across\nidentities imply any minor changes in the inputs could\nmap to a bigger change in the expression. These issues\nmake the style transfer problem ill-posed without highly\ndetailed conditioning.\nTo this end, we design a novel style transfer ar-\nchitecture that utilizes the prior registration estimation\ngiven by F0. Specifically, we can utilize F0 that was\ntrained directly on monochrome images H, to obtain\nan estimate of (z0, v0) for the current frame.\nAddi-\ntionally, we choose M \u201creference conditioning expres-\nsions\u201d: (zk1, ..., zkM ) to cover a range of reference ex-\npressions; e.g., mouth open, squinting eyes, closed eyes,\netc., which we find to significantly help mitigate am-\nbiguities in style-transferring extreme expressions (we\nshow examples of these conditioning reference expres-\nsions in the Appendix).\nFormally, given the current\nframe HMC image H, we compute\n\u02c6R = S (H, (z0, zk1, ..., zkM ), v0) .\n(3)\nWith a better estimation of (z0, v0) provided by F0,\nthese conditioning images become closer to ground\ntruth, thereby simplifying the style transfer learning task\nof S. Fig. 5 shows the UNet-based architecture of S.\nA U-Net decoder decodes the input images features into\nan RGB image \u02c6R with skip connections from the com-\nbined features. This style transfer module is trained with\na simple image L1 loss:\nLS = \u2225 \u02c6R \u2212 Rgt\u22251.\n(4)\n4. Experiments\nWe perform experiments on a dataset of 208 identities\n(32M frames in total), each captured in a lightstage [19]\nas well as a modified Quest-Pro headset [22] with aug-\nmented camera views. The avatars are generated for all\nidentities with a unified latent expression space using\nthe method from [5]. We utilize the extensive offline\n6\nInput HMC\nGround Truth\nCurrent estimation\n<latexit sha1_base64=\"8Z9Z9uSagzlkliW2uquSDVyS6M=\">A\nACEXicbVDLSsNAFJ3UV42vqks3wSJ0VRLxtRGKblxWsA9oQplMJu3QeYSZiVJCfsGNv+LGhSJu3bnzb5y2WjrgeEezrmXufeECSVKu\n+63VpaXldK6/bG5tb2zuV3b2EqlEuIUEFbIbQoUp4bilia4m0gMWUhxJxdT/zOPZaKCH6nxwkOGBxwEhMEtZH6lZrP8QMSjEeZ\nX6o8p4XmCpopMbMlKzq5bmtL91+perW3SmcReIVpAoKNPuVLz8SKGWYa0ShUj3PTXSQakJoji3/VThBKIRHOCeoRwyrIJselHuHBklcm\nIhzePamaq/JzLI1GRB08mgHqp5byL+5/VSHV8EGeFJqjFHs4/ilDpaOJN4nIhIjDQdGwKRJGZXBw2hEibEG0Tgjd/8iJpH9e9s/rp7U\nm1cVXEUQYH4BDUgAfOQPcgCZoAQewTN4BW/Wk/VivVsfs9aSVczsgz+wPn8A0fmdng=</latexit>t = 0\nError maps\n<latexit sha1_base64=\"8Z9Z9uSagzlkliW2uquSDVyS6M=\">A\nACEXicbVDLSsNAFJ3UV42vqks3wSJ0VRLxtRGKblxWsA9oQplMJu3QeYSZiVJCfsGNv+LGhSJu3bnzb5y2WjrgeEezrmXufeECSVKu\n+63VpaXldK6/bG5tb2zuV3b2EqlEuIUEFbIbQoUp4bilia4m0gMWUhxJxdT/zOPZaKCH6nxwkOGBxwEhMEtZH6lZrP8QMSjEeZ\nX6o8p4XmCpopMbMlKzq5bmtL91+perW3SmcReIVpAoKNPuVLz8SKGWYa0ShUj3PTXSQakJoji3/VThBKIRHOCeoRwyrIJselHuHBklcm\nIhzePamaq/JzLI1GRB08mgHqp5byL+5/VSHV8EGeFJqjFHs4/ilDpaOJN4nIhIjDQdGwKRJGZXBw2hEibEG0Tgjd/8iJpH9e9s/rp7U\nm1cVXEUQYH4BDUgAfOQPcgCZoAQewTN4BW/Wk/VivVsfs9aSVczsgz+wPn8A0fmdng=</latexit>t = 0\n<latexit sha1_base64=\"iTHAkSwpDjFvRDp/MxWUd+vlhH4=\">ACEXicbVDLSsNAFJ3UV42vqks3wSJ0VRLxtRGKblxWsA9oQplMJ\nu3QeYSZiVJCfsGNv+LGhSJu3bnzb5y2WjrgeEezrmXufeECSVKu+63VpaXldK6/bG5tb2zuV3b2EqlEuIUEFbIbQoUp4bilia4m0gMWUhxJxdT/zOPZaKCH6nxwkOGBxwEhMEtZH6lZrP8QMSjEeZX6o8p4XmCpopMbMlKzq5bmtL71+perW3SmcReIVpAoKNPuVLz8SKG\nWYa0ShUj3PTXSQakJoji3/VThBKIRHOCeoRwyrIJselHuHBklcmIhzePamaq/JzLI1GRB08mgHqp5byL+5/VSHV8EGeFJqjFHs4/ilDpaOJN4nIhIjDQdGwKRJGZXBw2hEibEG0Tgjd/8iJpH9e9s/rp7Um1cVXEUQYH4BDUgAfOQPcgCZoAQewTN4BW/Wk/VivVsfs9aSVc\nzsgz+wPn8A032dnw=</latexit>t = 1\n<latexit sha1_base64=\"pbOw3pRwKIuQOQX189tys48XcI=\">ACEXicbVDLSsNAFJ34rPEVdekmWISuSlJ8bYSiG5cV7APSUCaTS\nTt0HmFmopSQX3Djr7hxoYhbd+78G6ePhbYeGO7hnHuZe0+UqK0531bS8srq2vrpQ17c2t7Z9fZ28pkUmEm0hQITsRVJgSjpuaIo7qcSQRS3o+H12G/fY6mI4Hd6lOKQwT4nCUFQG6nVLocPyDBGORx3o1UEfihqYLGasRMyct+Udj6stZzyl7Vm8BdJP6MlMEMjZ7z1Y0Fyh\njmGlGoVOB7qQ5zKDVBFBd2N1M4hWgI+zgwlEOGVZhPLircY6PEbiKkeVy7E/X3RA6ZGi9oOhnUAzXvjcX/vCDTyUWYE5mGnM0/SjJqKuFO47HjYnESNORIRBJYnZ10QBKiLQJ0TYh+PMnL5JWreqfVU9vT8r1q1kcJXAIjkAF+OAc1MENaIAmQOARPINX8GY9WS/Wu/UxbV2yZj\nMH4A+szx/VAZ2g</latexit>t = 2\n<latexit sha1_base64=\"TeT3Xxq412fgHxh/FkoVWVQ+/s=\">ACEXicbVDLSsNAFJ3UV62vqEs3wSB0VRLfG6HoxmUF+4A2lMlk2\ng6dR5iZKCXkF9z4K25cKOLWnTv/xkmbhVYPDPdwzr3MvSeMKVHa876s0sLi0vJKebWytr6xuWVv7SUSCTCTSokJ0QKkwJx01NMWdWGLIQorb4fgq9t3WCoi+K2exDhgcMjJgCojdS3qz2O75FgDPIo7YUq6/qBqYJGasJMSV0/yr64qhvu17Nm8L5S/yCuKBAo29/9iKBEo\na5RhQq1fW9WAcplJogirNKL1E4hmgMh7hrKIcMqyCdXpQ5B0aJnIGQ5nHtTNWfEylkKl/QdDKoR2rey8X/vG6iB+dBSnicaMzR7KNBQh0tnDweJyISI0nhkAkidnVQSMoIdImxIoJwZ8/+S9pHdb809rJzbFbvyziKIM9sA+qwAdnoA6uQM0AQIP4Am8gFfr0Xq23qz3WvJKm\nZ2wS9YH9/WhZ2h</latexit>t = 3\n<latexit sha1_base64=\"iTHAkSwpDjFvRDp/MxWUd+vlhH4=\">ACEXicbVDLSsNAFJ3UV42vqks3wSJ0VRLxtRGKblxWsA9oQplMJ\nu3QeYSZiVJCfsGNv+LGhSJu3bnzb5y2WjrgeEezrmXufeECSVKu+63VpaXldK6/bG5tb2zuV3b2EqlEuIUEFbIbQoUp4bilia4m0gMWUhxJxdT/zOPZaKCH6nxwkOGBxwEhMEtZH6lZrP8QMSjEeZX6o8p4XmCpopMbMlKzq5bmtL71+perW3SmcReIVpAoKNPuVLz8SKG\nWYa0ShUj3PTXSQakJoji3/VThBKIRHOCeoRwyrIJselHuHBklcmIhzePamaq/JzLI1GRB08mgHqp5byL+5/VSHV8EGeFJqjFHs4/ilDpaOJN4nIhIjDQdGwKRJGZXBw2hEibEG0Tgjd/8iJpH9e9s/rp7Um1cVXEUQYH4BDUgAfOQPcgCZoAQewTN4BW/Wk/VivVsfs9aSVc\nzsgz+wPn8A032dnw=</latexit>t = 1\n<latexit sha1_base64=\"pbOw3pRwKIuQOQX189tys48XcI=\">ACEXicbVDLSsNAFJ34rPEVdekmWISuSlJ8bYSiG5cV7APSUCaTS\nTt0HmFmopSQX3Djr7hxoYhbd+78G6ePhbYeGO7hnHuZe0+UqK0531bS8srq2vrpQ17c2t7Z9fZ28pkUmEm0hQITsRVJgSjpuaIo7qcSQRS3o+H12G/fY6mI4Hd6lOKQwT4nCUFQG6nVLocPyDBGORx3o1UEfihqYLGasRMyct+Udj6stZzyl7Vm8BdJP6MlMEMjZ7z1Y0Fyh\njmGlGoVOB7qQ5zKDVBFBd2N1M4hWgI+zgwlEOGVZhPLircY6PEbiKkeVy7E/X3RA6ZGi9oOhnUAzXvjcX/vCDTyUWYE5mGnM0/SjJqKuFO47HjYnESNORIRBJYnZ10QBKiLQJ0TYh+PMnL5JWreqfVU9vT8r1q1kcJXAIjkAF+OAc1MENaIAmQOARPINX8GY9WS/Wu/UxbV2yZj\nMH4A+szx/VAZ2g</latexit>t = 2\n<latexit sha1_base64=\"TeT3Xxq412fgHxh/FkoVWVQ+/s=\">ACEXicbVDLSsNAFJ3UV62vqEs3wSB0VRLfG6HoxmUF+4A2lMlk2\ng6dR5iZKCXkF9z4K25cKOLWnTv/xkmbhVYPDPdwzr3MvSeMKVHa876s0sLi0vJKebWytr6xuWVv7SUSCTCTSokJ0QKkwJx01NMWdWGLIQorb4fgq9t3WCoi+K2exDhgcMjJgCojdS3qz2O75FgDPIo7YUq6/qBqYJGasJMSV0/yr64qhvu17Nm8L5S/yCuKBAo29/9iKBEo\na5RhQq1fW9WAcplJogirNKL1E4hmgMh7hrKIcMqyCdXpQ5B0aJnIGQ5nHtTNWfEylkKl/QdDKoR2rey8X/vG6iB+dBSnicaMzR7KNBQh0tnDweJyISI0nhkAkidnVQSMoIdImxIoJwZ8/+S9pHdb809rJzbFbvyziKIM9sA+qwAdnoA6uQM0AQIP4Am8gFfr0Xq23qz3WvJKm\nZ2wS9YH9/WhZ2h</latexit>t = 3\nFigure 6. Progression of iterative refinement: we show intermediate results D(zt, vt) in F on a validation identity.\nregistration pipeline in [27] to generate high-quality la-\nbels. We held out 26 identities as validation set. We use\nT = 3 refinement iterations during training and M = 4\nkey expressions are used to provide conditioning images\nfor style transfer, which is operating at 192 \u00d7 192 res-\nolution. See the Appendix for more details on model\narchitecture and training.\n4.1. Comparison with Baselines\nAs discussed, there are two obvious types of methods\nto compare for general face registration: (1) the same\noffline registration method in [27], but only using the\ncamera set C. Its performance anchors the challenge\nfrom camera angles, if computing time is not limited.\nThe training here is only across frames from the same\nidentity, so less prior knowledge it can leverage from\nother identities\u2019 images.\n(2) Direct regression: us-\ning the same set of ground truth label, we train a Mo-\nbileNetV3 [12] to directly regress HMC images to ex-\npression codes z. This method represents a online model\nthat could use in a realtime system where the use of D is\nprohibited. Table 2 summarized the comparison. The of-\nfline method achieves good averaged frontal image loss.\nAlbeit its high precision, it has common failure modes\nin lower jaw and inner mouth, where the observation is\npoor, as shown in Fig. 7. In comparison, our method\ncould leverage the learning from cross-identity dataset,\nproducing a more uniformly distributed error and even\nbetter head pose estimation. Our method is also much\nfaster due to its feed-forward design. On the other hand,\nthe direct regression method performs notably worse on\naverage, as expected. We also provide relaxed condi-\ntions (e.g. Rgt as input, or using augmented cameras),\nand interestingly the method fails to improve, while our\nmethod can leverage these conditions significantly.\nTable 2.\nComparison with direct regression and offline\nmethods. The errors are the averages of all frames in the test\nset. Augmented view means the use of camera set C\u2032 instead\nof C.\nAug.\ncams\nInput\nFrontal\nImage\nL1\nRot.\nErr.\n(deg.)\nTrans.\nErr.\n(mm)\nSpeed\nOffline [27]\n\u2717\nH\n1.713\n2.400\n2.512\n\u223c1 day\nRegression [12]\n\u2717\nH\n2.956\n\u2212\n\u2212\n7ms\nRegression [12]\n\u2717\nRgt\n2.920\n\u2212\n\u2212\n7ms\nRegression [12]\n\u2713\nH\n2.967\n\u2212\n\u2212\n7ms\nRegression [12]\n\u2713\nRgt\n2.902\n\u2212\n\u2212\n7ms\nOurs (F+S)\n\u2717\nH\n2.655\n0.947\n0.886\n0.4s\nOurs (F+S)\n\u2713\nH\n2.399\n0.917\n0.845\n0.4s\n4.2. Ablation Studies\nIn this section, we ablate the design of F and S. See the\nAppendix for more detailed analysis and experiments.\nIterative Refinement Module F.\nWe design a sim-\nple baseline where we remove the transformer layers of\nF and trivially fuse the features {F c}c\u2208C followed by\nan MLP network. We train this baseline on the simpli-\nfied case of matching modalities (similar to F0(Rgt)).\nIt fails to learn the iterative refinement of (z, v) and\nachieves frontal image L1 of 3.65, rotation error of 5.09\ndegrees and translation error of 5.84mm. These errors\nare significantly higher than that of F0 model as shown\nin Table 1. This shows that the transformer is able to\nbetter fuse information from multiview features.\nStyle Transfer Module S.\nIn Fig. 8, we compare the\nresults of our style transfer module S with baselines.\nStyTr2 [8] is one of the recent style transfer methods\nthat leverages the power of vision transformers [10] with\nlarge datasets.\nFor comparison, we feed Rgt as the\n7\n(a) HMC input\n (b) G.T.\n(c) Regression [12] (d) Offline [24]\n (e) Ours\n (f) Offline [24]\n(g) Ours\nFigure 7. Qualitative Results: we compare different methods by evaluating (b,c,d,e) frontal rendering (with error maps), and (f,g)\nerror maps in HMC viewpoints. See the Appendix for more examples.\nInput HMC\nOurs\nOurs\nGround \nTruth\nWithout\nStyTr  [8]\n2\n<latexit sha1_base64=\"EBxAT+s+NlsMW2HPQL2Dn1n6Cg=\">AB9HicbVDLS\ngMxFL1TX7W+qi7dBIvgqsyIr2VREJcV7APaoWTSTBuaZMYkUyhDv8ONC0Xc+jHu/Bsz7Sy09UDgcM693JMTxJxp47rfTmFldW19o7hZ2tre2d0r7x80dZQoQhsk4pFqB1\nhTziRtGY4bceKYhFw2gpGt5nfGlOlWSQfzSmvsADyUJGsLGS3xXYDAnm6d205/bKFbfqzoCWiZeTCuSo98pf3X5EkGlIRxr3fHc2PgpVoYRTqelbqJpjMkID2jHUo\nkF1X46Cz1FJ1bpozBS9kmDZurvjRQLrScisJNZSL3oZeJ/Xicx4bWfMhknhkoyPxQmHJkIZQ2gPlOUGD6xBPFbFZEhlhYmxPJVuCt/jlZdI8q3qX1YuH80rtJq+jCEd\nwDKfgwRXU4B7q0ACT/AMr/DmjJ0X5935mI8WnHznEP7A+fwBpyuSCQ=</latexit>F0\nFigure 8. Ablation on style transfer results. We compare our\nresults with a generic style transfer method as well as with our\nbaseline method without the estimates by F0.\nstyle image and H as content image. Although the con-\ntents of input is well-preserved, StyTr2 fails to bridge\nthe domain gap. To show the benefit of iterative feed-\nback from F0, we train a baseline style transfer model\nS (H, (zk1, ..., zkM ), vmean), where only the condition-\ning images of M key expressions rendered with mean\nviewpoint vmean (computed from the dataset) are pro-\nvided. Although it produces better style transfer than\nStyTr2 [8], it smooths out high-frequency details includ-\ning freckles, teeth, soft-tissue deformations near eyes\nand nose. These high-frequency details are crucial for\nanimating subtle expressions. Our style transfer model\nS is able to retain such details by leveraging the estimate\nprovided by F0.\n5. Conclusions and Future Work\nIn this paper, we present a lightweight generic method\nfor registering photorealistic 3D avatars on monochro-\nmatic images of consumer Virtual Reality (VR) head-\nset cameras. We show that closing the domain gap be-\ntween avatar\u2019s rendering and headset images is a key to\nachieve high registration quality. Motivated by this, we\ndecompose the problem into two modules, style trans-\nfer and iterative refinement, and present a system where\none reinforces the other. Extensive experiments on real\ncapture data show that our system achieves superior reg-\nistration quality than direct regression methods and can\nafford online usage. We believe this provides a viable\npath for efficiently generating high quality image-label\npairs on the fly to adapt real-time facial expression en-\ncoders with on-device compute in the future.\n8\nReferences\n[1] Jie An, Siyu Huang, Yibing Song, Dejing Dou, Wei Liu,\nand Jiebo Luo.\nArtflow: Unbiased image style trans-\nfer via reversible neural flows.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 862\u2013871, 2021. 3\n[2] Tim Brooks, Aleksander Holynski, and Alexei A. Efros.\nInstructpix2pix: Learning to follow image editing in-\nstructions. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR),\npages 18392\u201318402, 2023. 3\n[3] Adrian Bulat and Georgios Tzimiropoulos. How far are\nwe from solving the 2d & 3d face alignment problem?\n(and a dataset of 230,000 3d facial landmarks). In Pro-\nceedings of the IEEE International Conference on Com-\nputer Vision (ICCV), 2017. 3\n[4] Chen Cao, Qiming Hou, and Kun Zhou. Displaced dy-\nnamic expression regression for real-time facial tracking\nand animation. ACM Trans. Graph., 33(4), 2014. 3\n[5] Chen Cao, Tomas Simon, Jin Kyu Kim, Gabe Schwartz,\nMichael Zollhoefer, Shun-Suke Saito, Stephen Lom-\nbardi, Shih-En Wei, Danielle Belko, Shoou-I Yu, Yaser\nSheikh, and Jason Saragih. Authentic volumetric avatars\nfrom a phone scan. ACM Trans. Graph., 41(4), 2022. 1,\n3, 4, 6\n[6] Haibo Chen, Zhizhong Wang, Huiming Zhang, Zhiwen\nZuo, Ailin Li, Wei Xing, Dongming Lu, et al.\nArtis-\ntic style transfer with internal-external learning and con-\ntrastive learning. Advances in Neural Information Pro-\ncessing Systems, 34:26561\u201326573, 2021. 3\n[7] Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo\nHa, Sunghun Kim, and Jaegul Choo. Stargan: Unified\ngenerative adversarial networks for multi-domain image-\nto-image translation. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition, 2018.\n3\n[8] Yingying Deng, Fan Tang, Weiming Dong, Chongyang\nMa, Xingjia Pan, Lei Wang, and Changsheng Xu. Stytr2:\nImage style transfer with transformers. In IEEE Con-\nference on Computer Vision and Pattern Recognition\n(CVPR), 2022. 3, 7, 8\n[9] Piotr Doll\u00b4ar, Peter Welinder, and Pietro Perona.\nCas-\ncaded pose regression. In 2010 IEEE Computer Soci-\nety Conference on Computer Vision and Pattern Recog-\nnition, pages 1078\u20131085, 2010. 3\n[10] Alexey\nDosovitskiy,\nLucas\nBeyer,\nAlexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer,\nGeorg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil\nHoulsby. An image is worth 16x16 words: Transformers\nfor image recognition at scale. ArXiv, abs/2010.11929,\n2020. 3, 6, 7, 1\n[11] Leon A. Gatys, Alexander S. Ecker, and Matthias\nBethge. Image style transfer using convolutional neu-\nral networks. In IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2016. 3\n[12] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh\nChen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun\nZhu, Ruoming Pang, Vijay Vasudevan, et al. Searching\nfor mobilenetv3. In Proceedings of the IEEE/CVF in-\nternational conference on computer vision, pages 1314\u2013\n1324, 2019. 4, 7\n[13] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A\nEfros. Image-to-image translation with conditional ad-\nversarial networks. CVPR, 2017. 3\n[14] Vahid Kazemi and Josephine Sullivan. One millisecond\nface alignment with an ensemble of regression trees. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2014. 3\n[15] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai,\nJoan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil\nHoulsby. Big transfer (bit): General visual representa-\ntion learning. In Computer Vision\u2013ECCV 2020: 16th Eu-\nropean Conference, Glasgow, UK, August 23\u201328, 2020,\nProceedings, Part V 16, pages 491\u2013507. Springer, 2020.\n1\n[16] Hao Li, Laura Trutoiu, Kyle Olszewski, Lingyu Wei,\nTristan Trutna, Pei-Lun Hsieh, Aaron Nicholls, and\nChongyang Ma.\nFacial performance sensing head-\nmounted display. ACM Transactions on Graphics (TOG),\n34(4):47:1\u201347:9, 2015. 2\n[17] Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu\nChen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han. On\nthe variance of the adaptive learning rate and beyond.\narXiv preprint arXiv:1908.03265, 2019. 3\n[18] Songhua Liu, Tianwei Lin, Dongliang He, Fu Li, Meiling\nWang, Xin Li, Zhengxing Sun, Qian Li, and Errui Ding.\nAdaattn: Revisit attention mechanism in arbitrary neural\nstyle transfer. In Proceedings of the IEEE/CVF interna-\ntional conference on computer vision, pages 6649\u20136658,\n2021. 3\n[19] Stephen Lombardi, Jason Saragih, Tomas Simon, and\nYaser Sheikh. Deep appearance models for face render-\ning. ACM Trans. Graph., 37(4):68:1\u201368:13, 2018. 1, 2,\n6\n[20] Stephen Lombardi,\nTomas Simon,\nJason Saragih,\nGabriel Schwartz, Andreas Lehrmann, and Yaser Sheikh.\nNeural volumes: Learning dynamic renderable volumes\nfrom images. arXiv preprint arXiv:1906.07751, 2019. 1\n[21] Stephen Lombardi, Tomas Simon, Gabriel Schwartz,\nMichael Zollhoefer, Yaser Sheikh, and Jason Saragih.\nMixture of volumetric primitives for efficient neural ren-\ndering. ACM Trans. Graph., 40(4), 2021. 2\n[22] Meta Inc.\nMeta Quest Pro: Premium Mixed Reality.\nhttps://www.meta.com/ie/quest/quest-\npro/, 2023. 2, 3, 6\n[23] Kyle Olszewski, Joseph J. Lim, Shunsuke Saito, and\nHao Li. High-fidelity facial and speech animation for\nvr hmds. ACM Transactions on Graphics (TOG), 35(6):\n1\u201314, 2016. 2\n9\n[24] Niki Parmar, Prajit Ramachandran, Ashish Vaswani, Ir-\nwan Bello, Anselm Levskaya, and Jonathon Shlens.\nStand-alone self-attention in vision models.\n2019.\n5,\n2\n[25] Shaoqing Ren, Xudong Cao, Yichen Wei, and Jian Sun.\nFace alignment at 3000 fps via regressing local binary\nfeatures. In 2014 IEEE Conference on Computer Vision\nand Pattern Recognition, pages 1685\u20131692, 2014. 3\n[26] Jason Saragih and Roland Goecke. Iterative error bound\nminimisation for aam alignment. In Proceedings of the\n18th International Conference on Pattern Recognition -\nVolume 02, page 1196\u20131195, USA, 2006. IEEE Com-\nputer Society. 3\n[27] Gabriel Schwartz, Shih-En Wei, Te-Li Wang, Stephen\nLombardi, Tomas Simon, Jason Saragih, and Yaser\nSheikh. The eyes have it: An integrated eye and face\nmodel for photorealistic facial animation. ACM Trans.\nGraph., 39(4), 2020. 1, 2, 3, 4, 5, 7\n[28] Aliaksandra Shysheya, Egor Zakharov, Kara-Ali Aliev,\nRenat Bashirov, Egor Burkov, Karim Iskakov, Alek-\nsei Ivakhnenko, Yury Malkov, I. Pasechnik, Dmitry\nUlyanov, Alexander Vakhitov, and Victor S. Lempitsky.\nTextured neural avatars. 2019 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), pages\n2382\u20132392, 2019. 1\n[29] Justus Thies, Michael Zollh\u00a8ofer, Marc Stamminger,\nChristian Theobalt, and Matthias Niessner.\nFacevr:\nReal-time gaze-aware facial reenactment in virtual real-\nity. ACM Transactions on Graphics (TOG), 37(2):25:1\u2013\n25:15, 2018. 2\n[30] Shih-En Wei, Jason Saragih, Tomas Simon, Adam W.\nHarley, Stephen Lombardi, Michal Perdoch, Alexan-\nder Hypes, Dawei Wang, Hernan Badino, and Yaser\nSheikh. Vr facial animation via multiview image trans-\nlation. ACM Trans. Graph., 38(4), 2019. 1, 2, 4\n[31] Xiaolei Wu, Zhihao Hu, Lu Sheng, and Dong Xu. Style-\nformer: Real-time arbitrary style transfer via paramet-\nric style composition. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages\n14618\u201314627, 2021. 3\n[32] Jiahao Xia, Weiwei Qu, Wenjian Huang, Jianguo Zhang,\nXi Wang, and Min Xu. Sparse local patch transformer\nfor robust face alignment and landmarks inherent relation\nlearning. In 2022 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pages 4042\u2013\n4051, 2022. 3\n[33] Xuehan Xiong and Fernando De la Torre. Supervised\ndescent method and its applications to face alignment.\n2013 IEEE Conference on Computer Vision and Pattern\nRecognition, pages 532\u2013539, 2013. 3\n[34] Yi Zhou, Connelly Barnes, Lu Jingwan, Yang Jimei, and\nLi Hao. On the continuity of rotation representations in\nneural networks. In The IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2019. 3\n[35] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A.\nEfros. Unpaired image-to-image translation using cycle-\nconsistent adversarial networks. In IEEE International\nConference on Computer Vision (ICCV), 2017. 3\n10\nFast Registration of Photorealistic Avatars for VR Facial Animation\nSupplementary Material\n6. More Qualitative Results\nWe show more qualitative results on test identities in\nFig. 13 and Fig. 14 comparing against regression and\noffline methods. More results can be found in the ac-\ncompanying video. Overall, the regression method has\nthe larger error in terms of expression, often failing to\ncapture subtle mouth shapes and the amount of teeth/-\ntongue that is visible. On the other hand, offline meth-\nods that allow for a slowly optimizing the expression\ncode and head pose lead to lowest expression error over-\nall. However, when key face areas are not well observed\nin the HMC images (e.g. row 1,3 in Fig. 13 and row\n1,3,4,5,8 in Fig. 14), our method often estimates bet-\nter expressions. Our method is also superior in head\npose estimation. For example, in row 3,5 of Fig. 13,\nwhile our method has slightly high frontal error (expres-\nsion), the offline method has higher head pose error, in-\ndicated by higher image error in the HMC perspective\n(column (f) and (g)). This is often caused by the style-\ntransfer module compensating for registration error in\nit\u2019s person-specific training regime [27] where the model\ncan overfit more easily. In contrast, our style transfer\nmodule is trained across a diverse set of identities, and\ndoes not overfit as easily, resulting in better retained fa-\ncial structure, that in turn, leads to more accurate head\npose. Fig. 12 shows some failure cases of our method,\nwhich is usually caused by uncommon expressions, oc-\ncluded mouth regions from HMC cameras, and extreme\nhead poses.\n7. Design Ablation\nThis section provides a detailed description of the archi-\ntecture of the Iterative Refinement module F and Style\nTransfer module S. Additionally, we conduct ablation\nexperiments to validate key design choices.\n7.1. Iterative refinement module\nThe iterative refinement module F has \u223c28M trainable\nparameters. The CNN is based on ResNetV2-50 [15]\nwhich takes as input images of size 128 \u00d7 128 for each\ncamera view and outputs 512\u00d74\u00d74 grid features. After\nadding learnable patch embedding and view embedding,\nand concatenating the current estimate (zt, vt), the se-\nquence of |C| \u00d7 4 \u00d7 4 feature tokens are processed by a\nViT-based transformer module [10] that outputs the up-\ndate (\u2206zt, \u2206vt). The transformer module consists of 6\nencoder layers and 4 decoder layers operating on 512-\ndim tokens. F0 follows the same architecture as F ex-\ncept without the style-transfer images \u02c6R as input.\nKey to our design is the application of the transformer\non the grid of features from all camera views. We val-\nidate this design by comparing it\u2019s performance against\nF0( \u02c6Rgt) with the following settings (see Table 3):\n\u2022 w/o transformer, where we replace the transformer\nwith an MLP. In this case, the 512 \u00d7 4 \u00d7 4 grid fea-\ntures from all four camera views are simply concate-\nnated and processed by an MLP that outputs the up-\ndate (\u2206zt, \u2206vt). This trivial concatenation results in\na 2x increase in the number of trainable parameters.\n\u2022 w/o grid features, where we average pool grid fea-\ntures to get a single 512-dim feature for each camera\nview and use the same transformer design to process\n|C| tokens.\n\u2022 w/o transformer & w/o grid features, where we use\nan MLP to process the concatenation of pooled fea-\ntures from all camera views.\nWe observe that processing grid features using trans-\nformers results in superior generalization while requir-\ning fewer parameters compared to using an MLP with\ntrivial concatenation.\nPooling grid features also per-\nforms significantly worse than our model. This is partic-\nularly detrimental in the oblique viewing angle of head-\nset cameras because even minor variations in input pix-\nels can result in more significant changes in expression.\nTransformers operating on grid tokens can effectively\npreserve fine-grained information and extract subtle ex-\npression details.\nTable 3. Ablation on the design of F\nAug.\nCams\nFrontal\nImage L1\nRot.\nErr.\n(deg.)\nTrans.\nErr.\n(mm)\nOur F0( \u02c6Rgt|D)\n\u2717\n1.652\n0.660\n0.618\nw/o transformer\n\u2717\n2.533\n2.335\n2.023\nw/o grid features\n\u2717\n2.786\n2.818\n3.081\nw/o transformer &\nw/o grid features\n\u2717\n3.645\n5.090\n5.839\n7.2. Style transfer module\nThe style transfer module, S, has \u223c25M trainable pa-\nrameters and operates at an image resolution of 192 \u00d7\n1\nFigure 9. Conditioning Expressions for S: Four conditioning\nexpressions (zk1, ..., zk4) for three different identities.\n192. Both the input encoder and the conditioning en-\ncoder, as well as the decoder, follow the UNet archi-\ntecture. We train a single style transfer network for all\ncamera views by incorporating a learnable view embed-\nding at each layer of the UNet. Since the conditioning\nimages are generated using the avatar model, D, we also\nhave access to their foreground masks and projected UV\nimages of their guide mesh [21], which are also input\nto the conditioning encoder along with the rendered im-\nages.\nFig. 9 illustrates the four key conditioning expres-\nsions (zk1, ..., zk4) utilized in our experiments. These\nexpressions were selected to cover extremes of the ex-\npression space, to compensate for information defi-\nciency in style transfer conditioning while the estimate\nz0 is suboptimal. Sliding Window Attention (SWA) [24]\nis based on the cross-attention layer of the transformer\nwhere each grid feature of the input branch cross-attends\nto a 5\u00d75 neighborhood around the aligned feature of the\nconditioning branch. SWA compensates for missregis-\ntration when the estimate v0 is suboptimal.\nWe validate our design by comparing it with the fol-\nlowing baselines:\n\u2022 w/o SWA, where we simply concatenate the fea-\ntures of input branch with the features of conditioning\nbranch at each layer.\n\u2022 w/o key conditioning expressions, where only the\nTable 4. Ablation on the design of S\nImage L1 Error\nOur S\n2.55\nw/o SWA\n2.82\nw/o key cond. expressions\n2.75\nw/o F0\n2.99\nconditioning corresponding to the current estimate\n(z0, v0) is used.\n\u2022 w/o F0, where conditioning is comprised only of the\nfour key expressions rendered using the average view-\npoint per-camera, vmean.\nTable 4 shows the L1 error between the foreground pix-\nels of the groundtruth image and the predicted style\ntransferred image, as evaluated on the test set. The larger\nerror of style-transfer without F0 validates our motiva-\ntion that a better style transfer can be achieved by pro-\nviding conditioning closer to the groundtruth (zgt, vgt).\nWhen not incorporating SWA or key conditioning ex-\npressions, the model performs poorly when the esti-\nmates v0 and z0 are suboptimal respectively, resulting\nin higher error. We show more style transfer results on\ntest identities in Fig. 11.\n8. HMC Details\n(a) Training Headset\n(b) Tracking Headset\nCamera set C\nCamera set C\u2019\nOther cameras\nFigure 10. HMC details: We use all cameras on training head-\nset to establish ground truth in this work. Camera sets C and\nC\u2032 used in the main paper are annotated.\nIn this work, we follow the concept of training head-\nset and tracking headsets in [30], where the former has\na superset of cameras of the latter (see Fig. 10). In this\nwork, we use a more recent and advanced VR consumer\nheadset QuestPro [22] as the tracking headset, and aug-\nment it with additional cameras on a extended structure\nas the training headset. As shown in Fig. 10 (a), there are\n10 cameras on the training headset. We use all of them to\nestablish ground truth with the method in [27]. Camera\nset C on the tracking headset and the constructed cam-\n2\nWithout\nInput HMC\nStyTr2\nOurs\nOurs\nGroundtruth\n<latexit sha1_base64=\"EBxAT+s+NlsMW2HPQL2Dn1n6Cg=\">AB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyIr2VREJcV7APaoWTSTB\nuaZMYkUyhDv8ONC0Xc+jHu/Bsz7Sy09UDgcM693JMTxJxp47rfTmFldW19o7hZ2tre2d0r7x80dZQoQhsk4pFqB1hTziRtGY4bceKYhFw2gpGt5nfGlOlWSQfzSmvsADyUJGsLGS3xXYDAnm6d205/bKFbfqzoCWiZeTCuSo98pf3X5EkGlIRxr3fHc2PgpVoYRTqelbqJpjMkID2\njHUokF1X46Cz1FJ1bpozBS9kmDZurvjRQLrScisJNZSL3oZeJ/Xicx4bWfMhknhkoyPxQmHJkIZQ2gPlOUGD6xBPFbFZEhlhYmxPJVuCt/jlZdI8q3qX1YuH80rtJq+jCEdwDKfgwRXU4B7q0ACT/AMr/DmjJ0X5935mI8WnHznEP7A+fwBpyuSCQ=</latexit>F0\nFigure 11. More Qualitative Results on Style Transfer: We compare our results with a generic style transfer method as well as\nwith our baseline method without the estimates by F0.\nera set C\u2032 used for comparison in the main paper are\nalso annotated in the Fig. 10. Note we exclude the cy-\nclopean camera on the tracking headset from the camera\nset C due to limited observation and extreme illumina-\ntion. We also focus on mouth area and did not compare\nagainst the other 2 eye cameras on the training headset.\nAll cameras are synchronized and capture at 72 fps.\n9. Training Details\nOur model is trained in phases, where F0 is first trained,\nfollowed by S, which takes the pre-trained F0\u2019s output\nas input. The error distribution of the estimates (z0, v0)\nprovided by F0 to S will vary between training and test-\ning due to the generalization gap inherent in F0. To ad-\ndress this discrepancy, we introduce random Gaussian\nnoise to the estimates when training S. Similarly, we\nadd random Gaussian noise the the prediction of S when\ntraining F. F is trained for T = 3 refinement itera-\ntions. To stabilize training the gradients of each iteration\nare not backpropagated to prior iterations; we detach the\npredictions (zt+1, vt+1) before passing them as input to\nthe next iteration.\nBoth F and F0 are trained for 200K steps with a\nminibatch size of 4 using the RAdam optimizer [17].\nWeight decay is set to 10\u22124, and the initial learning rate\nis set to 3\u00d710\u22124. This learning rate is then gradually de-\ncayed to 3 \u00d7 10\u22126 using a cosine scheduler. S is trained\nsimilarly except that the weight decay is set to 3\u00d710\u22124.\nThe rotation component of viewpoint v is converted to\na 6D-rotation representation [34] before passing it to the\nnetwork. Both loss weights \u03bbhmc and \u03bbfront are set to 1.\n3\n(a) HMC input\n (b) G.T.\n(c) Regression (d) Offline\n (e) Ours\n (f) Offline \n(g) Ours\nFigure 12. Failure cases of our methods: we compare different methods by evaluating (b,c,d,e) frontal rendering (with error\nmaps), and (f,g) error maps in HMC viewpoints.\n4\n(a) HMC input\n (b) G.T.\n(c) Regression (d) Offline\n (e) Ours\n (f) Offline \n(g) Ours\nFigure 13. More Qualitative Results (1/2): we compare different methods by evaluating (b,c,d,e) frontal rendering (with error\nmaps), and (f,g) error maps in HMC viewpoints.\n5\n(a) HMC input\n (b) G.T.\n(c) Regression (d) Offline\n (e) Ours\n (f) Offline \n(g) Ours\nFigure 14. More Qualitative Results (2/2): we compare different methods by evaluating (b,c,d,e) frontal rendering (with error\nmaps), and (f,g) error maps in HMC viewpoints.\n6\n"
  },
  {
    "title": "Scaling Face Interaction Graph Networks to Real World Scenes",
    "link": "https://arxiv.org/pdf/2401.11985.pdf",
    "upvote": "1",
    "text": "SCALING FACE INTERACTION GRAPH NETWORKS TO\nREAL WORLD SCENES\nTatiana Lopez-Guevara, Yulia Rubanova, William F. Whitney, Tobias Pfaff,\nKimberly Stachenfeld, Kelsey R. Allen\nGoogle DeepMind\nABSTRACT\nAccurately simulating real world object dynamics is essential for various appli-\ncations such as robotics, engineering, graphics, and design. To better capture\ncomplex real dynamics such as contact and friction, learned simulators based\non graph networks have recently shown great promise (Allen et al., 2023; 2022).\nHowever, applying these learned simulators to real scenes comes with two major\nchallenges: first, scaling learned simulators to handle the complexity of real world\nscenes which can involve hundreds of objects each with complicated 3D shapes,\nand second, handling inputs from perception rather than 3D state information. Here\nwe introduce a method which substantially reduces the memory required to run\ngraph-based learned simulators. Based on this memory-efficient simulation model,\nwe then present a perceptual interface in the form of editable NeRFs which can\nconvert real-world scenes into a structured representation that can be processed\nby graph network simulator. We show that our method uses substantially less\nmemory than previous graph-based simulators while retaining their accuracy, and\nthat the simulators learned in synthetic environments can be applied to real world\nscenes captured from multiple camera angles. This paves the way for expanding\nthe application of learned simulators to settings where only perceptual information\nis available at inference time.\n1\nINTRODUCTION\nSimulating rigid body dynamics is an important but challenging task with broad applications ranging\nfrom robotics to graphics to engineering. Widely used analytic rigid body simulators in robotics such\nas Bullet (Coumans, 2015), MuJoCo (Todorov et al., 2012), and Drake (Tedrake, 2019) can produce\nplausible predicted trajectories in simulation, but system identification is not always sufficient to\nbridge the gap between real world scenes and these simulators (Wieber et al., 2016; Stewart & Trinkle,\n1996; Fazeli et al., 2017; Lan et al., 2022; Parmar et al., 2021; Guevara et al., 2017). This is due,\nin part, to the challenges of estimating fine-grained surface structures of objects which often have\nlarge impacts on their associated dynamics (Bauza & Rodriguez, 2017). This fundamental issue\ncontributes to the well-documented sim-to-real gap between outcomes from analytical solvers and\nreal-world experiments.\nLearned simulators have shown the potential to fill the sim-to-real gap (Allen et al., 2023; 2022) by\nrepresenting rigid body dynamics with graph neural networks. These fully learned simulators can be\napplied directly to real world object trajectories, and do not assume any analytical form for rigid body\ncontacts. As a result, they can learn to be more accurate than system identification with an analytic\nsimulator even with reasonably few real world trajectories.\nHowever, real world scenes present major challenges for learned simulators. First, learned simulators\ngenerally assume access to full state information (the positions, rotations, and exact shapes of all\nobjects) in order to simulate a trajectory. This information must be inferred from a collection of\nsensor measurements. Second, learned simulators can be memory intensive, especially for the\nkinds of intricate, irregular objects that often comprise real-world scenes. The currently best-\nperforming graph-based methods operate on explicit surface representations, i.e. point clouds or\ntriangulated meshes (Pfaff et al., 2021). The induced graphs of these methods tend to consume vast\namounts of GPU memory for complex object geometries, or when there are many objects in the\n1\narXiv:2401.11985v1  [cs.LG]  22 Jan 2024\nscene. Consequently, results are generally shown for scenes containing fewer than 10 objects with\nreasonably simple object geometries.\nHere we propose a simple, yet surprisingly effective modification (FIGNet*) to the learned, mesh-\nbased FIGNet rigid body simulator (Allen et al., 2023) that can address these challenges with\nrepresenting and simulating real world scenes:\n\u2022 FIGNet* consumes much less memory, while maintaining translation and rotation rollout\naccuracy. This allows us to train FIGNet* on datasets with more objects with complex\ngeometries such as Kubric MOVi-C, which FIGNet cannot train on due to memory cost.\n\u2022 We connect a NeRF perceptual front-end (Barron et al., 2022) to FIGNet*, and show that\nwe can simulate plausible trajectories for complex, never-before-seen objects in real world\nscenes.\n\u2022 We show that despite training FIGNet* on simulated rigid body dynamics with ground-truth\nmeshes, the model is robust to noisy mesh estimates obtained from real-world NeRF data.\n2\nRELATED WORK\nLearned simulators\nattempt to replicate analytical simulators by employing a learned function\napproximator. Typically, they are trained using ground truth state information, and consequently\ncannot be directly applied to visual input data. The representation of state varies depending on the\nmethod, but can range from point clouds (Li et al., 2019; Sanchez-Gonzalez et al., 2020; Mrowca\net al., 2018; Linkerh\u00a8agner et al., 2023), to meshes (Pfaff et al., 2021; Allen et al., 2023), to signed\ndistance functions (SDFs) (Le Cleac\u2019h et al., 2023). Subsequently, learned function approximators\nsuch as multi-layer perceptrons (MLPs) (Li et al., 2021), graph neural networks (GNNs) (Battaglia\net al., 2018; Sanchez-Gonzalez et al., 2018), or continuous convolutional kernels (Ummenhofer et al.,\n2019) can be employed to model the temporal evolution of the state. Our approach follows the\nmesh-based state representation options, but aims to provide a more efficient graph neural network\ndynamics model.\nBridging simulators to perception.\nMultiple approaches aim to bridge these learned simulators to\nperceptual data. Some approaches are \u201cend-to-end\u201d \u2013 they train a perceptual input system jointly with\na dynamics model, often assuming access to ground truth state information like object masks (Janner\net al., 2019; Driess et al., 2022; Shi et al., 2022; Xue et al., 2023; Whitney et al., 2023). Others first\nlearn a perceptual encoder and decoder, and then fix these to train a dynamics model in latent space\n(Li et al., 2021).\nMost related to our approach are methods that use neural radiance fields to reconstruct 3D scenes from\n2D multi-view scenes to enable simulation. Some of these assume hand-crafted but differentiable\ndynamics models (Qiao et al., 2023; 2022; Mengyu et al., 2022), while others learn the dynamics\nmodel separately from state information Guan et al. (2022). We similarly aim to simply apply our\npre-trained learned simulators to real scenes by using a NeRF perceptual front-end. We show that this\napproach can work without fine-tuning even when simulators are trained only from synthetic data.\n3\nMETHOD\n3.1\nFIGNET*\nFIGNet* closely follows the method of Face Interaction Graph Networks (FIGNet) (Allen et al.,\n2023) which is a graph neural network approach designed for modeling rigid body dynamics.\nIn FIGNet, each object is represented as a triangulated mesh M made of triangular mesh faces\n{FM} with mesh vertices {VM}. A scene graph G then consists of O objects, each with their\nown triangulated meshes Mo. At any given time t, M t\no can be represented using the object\u2019s\ntransformation matrix, M t\no = Rt\no \u00d7 Mo. A simulation trajectory is represented as a sequence of\nscene graphs G = (Gt0, Gt1, Gt2, . . . ) constructed from these meshes. FIGNet is then a simulator S\nparameterized by neural network weights \u0398, trained to predict the next state of the physical system\n\u02dcGt+1 based on the previous two scene graphs {Gt, Gt\u22121}, ie Gt+1 = S\u0398(Gt, Gt\u22121). We train with\n2\na mean-squared-error loss on the predicted positions of the vertices for each object {VM}. During\ninference, S\u0398 can be recursively applied to yield a rollout of any length T.\nobject-node\nnode-node\nface-face\nFIGNet* Connectivity\n\u2713\n\u2713\n\u2717\nvm\nvo\nvm\nvm\nvo\nvm\nfs\nfr\nfs\nfr\nFigure 1: Architectural changes:\nFIGNet* with respect to FIGNet.\nFIGNet consists of two types of nodes (mesh nodes {VM} and\nobject nodes {VO}), and three types of bi-directional edges.\nThe mesh nodes {VM} have input features vM,features\ni\n= [xt\ni \u2212\nxt\u22121\ni\n, pi, ai, f t\ni ], where xt\ni is the position of the node at time\nt, pi are static object properties like density and friction, ai\nis a binary \u201cstatic\u201d feature that indicates whether the node is\nsubject to dynamics (e.g. the moving objects), or its position\nis set externally (e.g. the floor), and f t\ni = ki(xt+1\ni\n\u2212 xt\ni) is a\nfeature that indicates how much kinematic nodes are going to\nmove at the next time step. Object nodes {VO} use the same\nfeature description, with their positions xt\ni being the object\u2019s\ncenter of mass.\nThe three types of bi-directional edges include node-node,\nobject-node, and face-face edges. Node-node edges vm \u2192 vm\nconnect surface mesh nodes on a single object to one another.\nObject-node edges vo \u2192 vm connect object nodes vo to each\nmesh vertex vm of that object. Face-face edges connect faces\non one sender object fs to another receiver object fr. See Figure 1.\nConceptually, the node-node edges enable the propagation of messages locally along an object\u2019s\nsurface. However, in the case of rigid body collisions, collision information needs to be propagated\ninstantaneously from one side of the object to the other, irrespective of the mesh complexity. Object-\nnode edges enable this by having a single virtual object node vo at the center of each object which\nhas bidirectional edges to each mesh node vm on the object\u2019s surface. Finally, to model the collision\ndynamics between rigid objects, face-face edges convey information about face interactions between\nobjects. FIGNet proposes a special hypergraph architecture for how to incorporate face-face edges\ninto an Encode-Process-Decode graph network architecture. We defer further details of the FIGNet\napproach to (Allen et al., 2023).\nThis approach works remarkably well for rigid body shapes but becomes intractably expensive as\nthe complexity of each object mesh grows, since this will add a significant number of node-node\n(surface mesh) edges. Empirically, node-node edges often account for more than 50% of the total\nedges in FIGNet. FIGNet* makes a simple modification to FIGNet which removes the node-node\n(surface mesh) edges, keeping everything else identical. Surprisingly, this does not hurt the accuracy\nof FIGNet*, but dramatically improves memory and runtime performance for the rigid body settings\nexamined in this paper. This works for rigid body dynamics because the collision edges reason\nabout the local geometry of two objects involved in contact, and this information can then be directly\nbroadcasted to the whole shape using object-node edges.\nThis simple change to FIGNet unlocks the ability to train on much more complex scenes than was\npreviously possible, as larger scenes fit into accelerator memory during training. We can therefore\nrun FIGNet* on meshes extracted from real-world scenes, as well as simulations with more complex\nobject geometries than previously possible.\n3.2\nCONNECTING FIGNET* TO PERCEPTION\nIn this section we describe the procedure used to connect FIGNet* to the real world. We leverage\nNeural Radiance Fields (NeRFs) (Mildenhall et al., 2021; Barron et al., 2022) as a perceptual front\nend to (1) extract the meshes required by FIGNet* for simulation and (2) re-render the scene with the\ntransformations predicted by FIGNet* (Figure 2). This approach shares similarities with the method\npresented in (Qiao et al., 2023), however, here we demonstrate its implementation using a learned\nsimulator.\n3\nGt : t-h\nGt\nF\nNeRF\n\u0398\nS\nPERCEPTION\nDYNAMICS\nG t+1\n(x, d)\n(c, \u03c3)\n\u03a6\nFIGNet*\nFigure 2: Perception Pipeline. We demonstrate a two-way coupling approach, integrating FIGNet*\nwith real-world scenes through NeRF. Initially, a static NeRF scene is trained using a collection of\nimages capturing a real-world scene, enabling the extraction of the necessary meshes for FIGNet*.\nUpon obtaining the rollout trajectory, we derive a set of rigid body transformations, which are then\nutilized to edit the original NeRF. See subsection 3.2 for details.\n3.2.1\nFROM NERF TO FIGNET*\nLearning a Neural Radiance Field:\nWe first learn a NeRF from W sparse input views {I}W\n1 and\ntheir associated camera intrinsics K and extrinsics. This representation models a view-dependent\nappearance function F\u03a6 that maps a 3D location x = (x, y, z) and a viewing direction d to a radiance\ncolor c and a density \u03c3.\nF\u03a6 : (x, d) \u2192 (c, \u03c3)\n(1)\nThe geometries of all the objects in a scene represented by a NeRF are implicitly captured by F\u03a6. We\nonly care about the density \u03c3 for the geometry and can ignore the color c and the viewing direction\nd. We slightly abuse the notation and define F \u03c3\n\u03a6(x) \u2192 \u03c3 to denote the subpart of the NeRF that\nevaluates the density only.\nMesh Extraction:\nTo extract the mesh of an individual object from the implicit function F \u03c3\n\u03a6, we\nfirst need to define a volumetric boundary of the object.\nWe begin by generating N binary segmentation masks, each capturing the object\u2019s shape from one\nof N distinct viewpoints. Each mask is created by calling XMEM (Cheng & Schwing, 2022) with\nthe corresponding RGB image and a point prompt located at the center of the object. XMEM then\nidentifies and labels all active pixels belonging to the object in each mask at the prompted location,\nresulting in a set of N segmentation masks {mn}N\n1 that capture the object\u2019s shape from various\nperspectives. Empirically, we found that for simple objects like spheres, as few as two views from\ndifferent angles are sufficient to accurately segment the object. However, one can use additional\nviews for increased robustness or to capture finer details, particularly for more complex shapes.\nWe use the same procedure as described in (Cen et al., 2023) to unproject the pixels of the 2D masks\ninto 3D points by leveraging the estimated depth z(mn) from the NeRF and the known camera\nintrinsics from which each mask was generated:\nxmn = z(mn) \u2217 K\u22121 \u00b7 (x(mn), y(mn), 1)T\n(2)\nThe volumetric boundary Vo \u2208 R2\u00d73 can be then obtained as\nVo = {min(xmn), max(xmn)}N\n1\n(3)\nTo extract the mesh of the object Mo within the volume Vo, we employ the Marching Cubes algorithm\n(m cubes) (Lorensen & Cline, 1998). This algorithm uses samples of the density field from a regular\ngrid of J points inside the boundary xj \u2208 Vo as \u03c3o = {F \u03c3\n\u03a6(xj)}J\n1 and a threshold value \u03c3thrs. To\nmanage the potentially high number of vertices and faces in the generated mesh, we perform an\n4\nadditional decimation step (decimate). We employ the Quadric Error Metric Decimation method\nby Garland and Heckbert (Garland & Heckbert, 1997). This technique preserves the primary features\nof the mesh while allowing us to control the final mesh complexity through a user-specified target\nnumber of faces nf.\nMo = decimate( m cubes(\u03c3o, \u03c3thrs), nf)\n(4)\nBuilding the Graph\nTo specify the object whose motion we want to simulate, we define the mesh\nMo as the active object in the graph, with all other objects considered static. We then repeat the same\nmesh extraction procedure described above on an offset version of the scene volume (Vo \u2212 \u2206xVo)\nto obtain the passive mesh Mpassive representing the static environment with ai set to True. Both\nmeshes are used to construct the initial graph Gt for FIGNet and FIGNet*. We do not infer static\nproperties like mass, friction, elasticity, etc for meshes extracted from the scene. Instead we use the\ndefault parameters provided in Table 3. Future work will be needed to infer these properties from\nobject dynamics.\nWe generate the history Gt\u22121 using the same mesh but shifted downwards by a \u2206z amount twice to\nsimulate an object being dropped vertically.\n3.2.2\nFROM FIGNET* TO NERF\nWe obtain a rollout trajectory by iteratively applying FIGNet* over T time steps. Starting from the\ninitial graph and its history to obtain (Gt+1, Gt+2, \u00b7 \u00b7 \u00b7 , Gt+T ). This can be equivalently seen as a\nsequence of rigid transformations (Rt+1\no\n, Rt+2\no\n, \u00b7 \u00b7 \u00b7 , Rt+T\no\n) that are applied to Mo.\nGiven the bounding volume of each object Vo and a rigid transformation Rt at time t, we can reuse\nthe static NeRF function F\u03a6 to render the rollout by editing the original static NeRF described\nby F\u03a6 via ray bending (Jambon et al., 2023). We restrict the bending of the ray b to be the rigid\ntransformation returned by FIGNet* as\n\u02c6\nF\u03a6 : ( b(x, Rt\no ), d) \u2192 (c, \u03c3),\n(5)\nwhere b(x, Rt\no ) can be either\nbmove(x, Rt\no) =\n\uf8f1\n\uf8f2\n\uf8f3\nRt\no \u00d7 x\nif x \u2208 Vo,\n(Rt\no)\u22121 \u00d7 x\nif x \u2208 Rt\no \u00d7 Vo,\nx\notherwise.\n(6)\nor\nbduplicate(x, Rt\no) =\n\u001a(Rt\no)\u22121 \u00d7 x\nif x \u2208 Rt\no \u00d7 Vo,\nx\notherwise.\n(7)\nmeaning that the active object has the option to be either moved or copy-pasted during the rollout.\nWe then generate the final sequence of rollout images from a chosen viewpoint \u02c6d across all time steps.\nThis involves applying NeRF\u2019s classic volume rendering pipeline with the transformed radiance\nfield \u02c6\nF\u03a6 incorporating object movement. At each step, we adjust the radiance field based on the\napplied rigid transformation, effectively capturing the dynamic appearance of the object throughout\nthe rollout sequence { \u02c6\nF\u03a6(b(x, Rt ), \u02c6d)}k\nt=1.\n4\nRESULTS\nWe test FIGNet* on both simulated and real data. In simulation, we show that FIGNet* outperforms\nFIGNet in memory consumption and runtime while maintaining accuracy for a standard rigid body\ndynamics benchmark (Greff et al., 2022). For real data, we show that FIGNet* can be run on views\nof real scenes collected from multiple cameras, making plausible trajectories despite training in\nsimulation on perfect state information.\n5\nGround\ntruth\nFIGNet*\nFigure 3: Qualitative results for simulation. FIGNet* rollout for complex MOVi-C simulation\nwhich could not be represented in memory for FIGNet.\n4.1\nSIMULATION\nFor our simulation results, we use the MOVi-B and MOVi-C Kubric datasets (Greff et al., 2022). In\nboth setups, multiple rigid objects are tossed together onto the floor using the PyBullet (?) simulator\nto predict trajectories. MOVi-B consists of scenes involving 3-10 objects selected from 11 different\nshapes being tossed. The shapes include teapots, gears, and torus knots, with a few hundred up to just\nover one thousand vertices per object. MOVi-C consists of scenes involving 3-10 objects selected\nfrom 1030 different shapes taken from the Google Scanned Objects dataset (Downs et al., 2022).\nMOVi-C shapes tend to be more complex than MOVi-B shapes, and have up to several thousand or\ntens of thousands of vertices.\nWe report four metrics in Table 2: peak memory consumption, runtime per simulation step, translation\nerror, and rotation error. Translation and rotation root-mean-squared error (RMSE) are calculated\nwith respect to the ground truth state after 50 rollout steps.\nTable 1: Comparison metrics for FIGNet and FIGNet* on Kubric MOVi-B and MOVi-C\nDataset\nModel\nMemory (MiB)\nRuntime (ms)\nTrans. Err. (m)\nRot. Err. (deg)\nEdge Count (#)\nMOVi-B\nFIGNet\n63.38 \u00b1 3.32\n26.38 \u00b1 0.73\n0.14 \u00b1 0.01\n14.99 \u00b1 0.67\n24514 \u00b1 906\nFIGNet*\n50.08 \u00b1 3.37\n19.41 \u00b1 0.24\n0.13 \u00b1 0.01\n15.96 \u00b1 0.87\n8630 \u00b1 714\nMOVi-C\nFIGNet\nOOM\n\u2013\n\u2013\n\u2013\n\u2013\nFIGNet*\n71.79 \u00b1 6.39\n20.42 \u00b1 0.64\n0.18 \u00b1 0.01\n19.82 \u00b1 0.64\n11401 \u00b1 975\nFor MOVi-B, FIGNet* matches FIGNet\u2019s performance in translation and rotation error, performing\nslightly better in translation, and slightly worse on rotation. However, FIGNet* uses significantly\nless memory than FIGNet while also having a 20% faster runtime. These differences in memory\nconsumption and runtime allow us to train FIGNet* on the much more complex MOVi-C dataset\n(example trajectory in Figure 3), which causes OOM errors when attempting to train FIGNet even\nwith 16 A100 GPUs. On MOVi-C, the memory consumption is higher, but runtime remains almost as\nfast. Similarly, since MOVi-C is more complex than MOVi-B, the translation and rotation errors for\nFIGNet* are higher, but not significantly so.\nOverall, this suggests that FIGNet* is a viable alternative to FIGNet. It maintains accuracy while\nsignificantly reducing memory consumption and runtime, allowing us to train FIGNet* on more\ncomplex datasets than can be fit into FIGNet memory.\n4.2\nREAL WORLD\nWe present our results on linking FIGNet* with real-world scene inputs. Note that this is a proof-of-\nconcept only, that is we do not compare to real ground truth dynamics, instead leaving that for future\n6\nwork. For comparisons between FIGNet and FIGNet* on real data, FIGNet models were trained in\nsimulation on Kubric MOVi-B, while FIGNet* models were trained in simulation on Kubric MOVi-C.\nFor our real-world results, we used two scenes: our custom-made KITCHEN scene filled with common\nelements such as fruits and baskets (See Appendix C for details), the GARDEN-outdoor and KITCHEN\nCOUNTER-indoor scenes introduced in (Barron et al., 2022) and the FIGURINES scene introduced in\n(Kerr et al., 2023). These scenes consist of 360-degree image sets captured with different cameras.\nWe used a MipNerf360 (Barron et al., 2022) implementation for the NeRF front end.\nt\nKITCHEN\nGARDEN\nFIGURINES\nK. COUNTER\nFigure 4: Qualitative results for real world scenes. Left: Initial NeRF rendering of the static\nreal-world scene. The desired active object is outlined in red, with a red arrow indicating its intended\nstarting position. Right: FIGNet* rollouts simulating the object\u2019s motion for k = 30 time steps\n(rendered from a different viewpoint) after being dropped from the initial position. The complete\ntrajectory is traced in yellow. Here we used bduplicate as the ray bending function meaning the active\nobject is copy pasted into the starting position at the beginning of the rollout (See the website for\nvideos and Appendix B for details on the mesh extraction procedure described in subsection 3.2).\nQualitative Results.\nWe show qualitative FIGNet* rollouts on both real world scenes using the\nfull pipeline described in subsection 3.2. For all the scenes, we manually selected 2 views of the\nactive object (highlighted in the red boxes) to compute the bounding volume Vo and the subsequent\nmesh Mo (See Appendix B). By creating a history based on downward vertical displacement of\nthe chosen mesh, we are effectively simulating a motion similar to dropping. Figure 4 illustrates\nthe bouncing behaviors of various objects falling onto other objects. Note the sharp rotation of the\norange at the end of the bounce (last frame) in the KITCHEN scene, and how rendering with the\n7\ntransformed \u02c6\nF\u03a6 works when the orange is flipped upside down. We can observe similar results for the\nFIGURINES scene, where we selected two views of the dog figurine with long thing legs and simulate\na dropping motion onto a duck. Our perception pipeline can realistically simulate and re-render the\ndropping motion of objects captured within these real scenes by reusing the static NeRF scene with\nthe FIGNet* transformations 1.\nFIGNet*\n20K FACES\n40K FACES\nFIGNet\nOOM\nnf\nnf\nDecimating faces from [494234] to [20000]\nDecimating faces from [940579] to [20000]\nMorange\nMpassive\n10K FACES\nnf\n1K FACES\nnf\nFigure 5: FIGNet and FIGNet* comparison for different levels of decimation: High-quality meshes\nlead to out-of-memory issues on FIGNet, while lower resolutions result in implausible trajectories\n(e.g., orange penetrating the basket). Notably, FIGNet*\u2019s performance gracefully degrades with mesh\nquality, indicating enhanced robustness and memory efficiency. The gray mesh depicts the passive\nobject, and the colored mesh corresponds to the active object.\nEffect of decimation.\nThe marching cube algorithm often results in oversampled meshes charac-\nterized by an elevated node count. While the implementation of a controllable parameter for mesh\ndecimation (nf) is an effective strategy to address this challenge, it is important to note that the extent\nof decimation can adversely affect the quality of simulations, especially in cases involving complex\ngeometries. The advantage of using FIGNet* lies in its reduced memory requirements, which permits\na less rigorous decimation process in comparison to FIGNet. To demonstrate this, we simulated a\n1See https://sites.google.com/view/fignetstar/ for videos.\n8\nscene with two distinct levels of decimation (Figure 5). This experiment highlights instances where\nFIGNet\u2019s memory capacity is exceeded, showcasing the benefits of FIGNet* in such scenarios.\nEffect of perception noise.\nReal-world meshes extracted from pipelines like NeRF, primarily\noptimized for rendering quality, often exhibit noise and imperfections (Figure 6). Unlike the clean\ntraining data used for FIGNet* and FIGNet, these meshes are far from ideal. Nevertheless, both\nmodels can successfully handle rollouts even with such challenging real-world data.\n5\nDISCUSSION\nWe showed that a surprisingly simple modification to FIGNet, the removal of the surface mesh\nedges, allowed us to create a model with low enough memory consumption to support training on\nunprecedentedly complex scenes. This unlocked the ability to interface FIGNet* with real world\nscenes by using a combination of Neural Radiance Fields (NeRFs) and object selection (XMem) to\nconvert real scenes into object-based mesh representations. In combination with volumetric NeRF\nediting, this allowed us to simulate videos of alternative physical futures for real scenes.\nWe believe that this explicitly 3D approach to video editing and generation has significant promise\nfor robotics and graphics applications. It allows a model to be pre-trained from simulation data, while\nstill generalizing to real scenes. FIGNet* generalizes surprisingly well to noisy meshes extracted from\nNeRFs, especially considering that it was trained in simulation with nearly perfect state information\n(positions, rotations, and shapes of objects). We imagine that this approach could further support\nfuture applications including \u201cvirtualization\u201d of real scenes, where users may be interested in editing\nthose scenes and simulating possible future outcomes.\nThere are many exciting directions for future work with FIGNet*. In particular, while fine-tuning\na pre-trained FIGNet* model to a real video was outside the scope of this paper, we believe this\nis a natural next step. Since FIGNet* is entirely composed of neural networks, fine-tuning from\nreal world dynamics directly into the weights of FIGNet* could be a viable alternative to system\nidentification for robotics. Future work will be needed to determine the details of how to perform\nfine-tuning in a data efficient manner.\nREFERENCES\nKelsey R Allen, Tatiana Lopez Guevara, Yulia Rubanova, Kim Stachenfeld, Alvaro Sanchez-Gonzalez,\nPeter Battaglia, and Tobias Pfaff. Graph network simulators can learn discontinuous, rigid contact\ndynamics. In 6th Annual Conference on Robot Learning, 2022. URL https://openreview.\nnet/forum?id=rbIzq-I84i_.\nKelsey R. Allen, Yulia Rubanova, Tatiana Lopez-Guevara, William Whitney, Alvaro Sanchez-\nGonzalez, Peter Battaglia, and Tobias Pfaff. Learning rigid dynamics with face interaction graph\nnetworks. In International Conference on Learning Representations, 2023.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\nJonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Mip-nerf\n360: Unbounded anti-aliased neural radiance fields. CVPR, 2022.\nPeter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi,\nMateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al.\nRelational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261,\n2018.\nMaria Bauza and Alberto Rodriguez. A probabilistic data-driven model for planar pushing. In IEEE\nInternational Conference on Robotics and Automation (ICRA), pp. 3008\u20133015, 2017.\nJiazhong Cen, Zanwei Zhou, Jiemin Fang, Chen Yang, Wei Shen, Lingxi Xie, Dongsheng Jiang,\nXiaopeng Zhang, and Qi Tian. Segment anything in 3d with nerfs. In NeurIPS, 2023.\n9\nHo Kei Cheng and Alexander G. Schwing. XMem: Long-term video object segmentation with an\natkinson-shiffrin memory model. In ECCV, 2022.\nErwin Coumans. Bullet physics simulation. In ACM SIGGRAPH 2015 Courses, pp. 7, 2015.\nLaura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann,\nThomas B McHugh, and Vincent Vanhoucke. Google scanned objects: A high-quality dataset of\n3d scanned household items. arXiv preprint arXiv:2204.11918, 2022.\nDanny Driess, Zhiao Huang, Yunzhu Li, Russ Tedrake, and Marc Toussaint. Learning multi-object\ndynamics with compositional neural radiance fields. arXiv preprint arXiv:2202.11855, 2022.\nNima Fazeli, Elliott Donlon, Evan Drumwright, and Alberto Rodriguez. Empirical evaluation of\ncommon contact models for planar impact. In 2017 IEEE international conference on robotics and\nautomation (ICRA), pp. 3418\u20133425. IEEE, 2017.\nMichael Garland and Paul S Heckbert. Surface simplification using quadric error metrics. In\nProceedings of the 24th annual conference on Computer graphics and interactive techniques, pp.\n209\u2013216, 1997.\nKlaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David J\nFleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, et al.\nKubric: A scalable\ndataset generator. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 3749\u20133761, 2022.\nShanyan Guan, Huayu Deng, Yunbo Wang, and Xiaokang Yang. Neurofluid: Fluid dynamics\ngrounding with particle-driven neural radiance fields, 2022.\nTatiana L\u00b4opez Guevara, Nicholas Kenelm Taylor, Michael Gutmann, Subramanian Ramamoorthy,\nand Kartic Subr. Adaptable pouring: Teaching robots not to spill using fast but approximate fluid\nsimulation. In 1st Conference on Robot Learning 2017, pp. 77\u201386, 2017.\nCl\u00b4ement Jambon, Bernhard Kerbl, Georgios Kopanas, Stavros Diolatzis, George Drettakis, and\nThomas Leimk\u00a8uhler. Nerfshop: Interactive editing of neural radiance fields. Proceedings of the\nACM on Computer Graphics and Interactive Techniques, 6(1), 2023.\nMichael Janner, Sergey Levine, William T. Freeman, Joshua B. Tenenbaum, Chelsea Finn, and Jiajun\nWu. Reasoning about physical interactions with object-oriented prediction and planning, 2019.\nJustin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, and Matthew Tancik. Lerf: Language\nembedded radiance fields. In International Conference on Computer Vision (ICCV), 2023.\nLei Lan, Danny M Kaufman, Minchen Li, Chenfanfu Jiang, and Yin Yang. Affine body dynamics:\nFast, stable & intersection-free simulation of stiff materials. ACM Trans. Graph, 2022.\nSimon Le Cleac\u2019h, Hong-Xing Yu, Michelle Guo, Taylor Howell, Ruohan Gao, Jiajun Wu, Zachary\nManchester, and Mac Schwager. Differentiable physics simulation of dynamics-augmented neural\nobjects. IEEE Robotics and Automation Letters, 8(5):2780\u20132787, 2023.\nYunzhu Li, Jiajun Wu, Russ Tedrake, Joshua B. Tenenbaum, and Antonio Torralba. Learning\nparticle dynamics for manipulating rigid bodies, deformable objects, and fluids. In International\nConference on Learning Representations, 2019.\nYunzhu Li, Shuang Li, Vincent Sitzmann, Pulkit Agrawal, and Antonio Torralba. 3d neural scene\nrepresentations for visuomotor control. arXiv preprint arXiv:2107.04004, 2021.\nJonas Linkerh\u00a8agner, Niklas Freymuth, Paul Maria Scheikl, Franziska Mathis-Ullrich, and Gerhard\nNeumann. Grounding graph network simulators using physical sensor observations. arXiv preprint\narXiv:2302.11864, 2023.\nWilliam E Lorensen and Harvey E Cline. Marching cubes: A high resolution 3d surface construction\nalgorithm. In Seminal graphics: pioneering efforts that shaped the field, pp. 347\u2013353. 1998.\n10\nChu Mengyu, Liu Lingjie, Zheng Quan, Franz Erik, Seidel Hans-Peter, Theobalt Christian, and\nZayer Rhaleb. Physics informed neural fields for smoke reconstruction with sparse data. ACM\nTransactions on Graphics, 41(4):119:1\u2013119:14, aug 2022.\nBen Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and\nRen Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications\nof the ACM, 65(1):99\u2013106, 2021.\nDamian Mrowca, Chengxu Zhuang, Elias Wang, Nick Haber, Li F Fei-Fei, Josh Tenenbaum, and\nDaniel L Yamins. Flexible neural representation for physics prediction. Advances in neural\ninformation processing systems, 31, 2018.\nMihir Parmar, Mathew Halm, and Michael Posa. Fundamental challenges in deep learning for stiff\ncontact dynamics. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS), pp. 5181\u20135188. IEEE, 2021.\nTobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter Battaglia. Learning mesh-based\nsimulation with graph networks. In International Conference on Learning Representations, 2021.\nYi-Ling Qiao, Alexander Gao, and Ming C. Lin. Neuphysics: Editable neural geometry and physics\nfrom monocular videos. In Conference on Neural Information Processing Systems (NeurIPS),\n2022.\nYi-Ling Qiao, Alexander Gao, Yiran Xu, Yue Feng, Jia-Bin Huang, and Ming C. Lin. Dynamic\nmesh-aware radiance fields. ICCV, 2023.\nAlvaro Sanchez-Gonzalez, Nicolas Heess, Jost Tobias Springenberg, Josh Merel, Martin Riedmiller,\nRaia Hadsell, and Peter Battaglia. Graph networks as learnable physics engines for inference and\ncontrol. In International Conference on Machine Learning, pp. 4470\u20134479. PMLR, 2018.\nAlvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Peter\nBattaglia. Learning to simulate complex physics with graph networks. In International Conference\non Machine Learning, pp. 8459\u20138468. PMLR, 2020.\nJohannes L Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition, pp. 4104\u20134113, 2016.\nHaochen Shi, Huazhe Xu, Zhiao Huang, Yunzhu Li, and Jiajun Wu. Robocraft: Learning to see,\nsimulate, and shape elasto-plastic objects with graph networks, 2022.\nD Stewart and JC J.C. Trinkle. An implicit time-stepping scheme for rigid body dynamics with\nCoulomb friction. International Journal for Numerical Methods in Engineering, 39(15):2673\u20132691,\n1996.\nRuss Tedrake. Drake: Model-based design and verification for robotics, 2019. URL https:\n//drake.mit.edu.\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.\nIn 2012 IEEE/RSJ international conference on intelligent robots and systems, pp. 5026\u20135033.\nIEEE, 2012.\nBenjamin Ummenhofer, Lukas Prantl, Nils Thuerey, and Vladlen Koltun. Lagrangian fluid simulation\nwith continuous convolutions. In International Conference on Learning Representations, 2019.\nWilliam F. Whitney, Tatiana Lopez-Guevara, Tobias Pfaff, Yulia Rubanova, Thomas Kipf, Kimberly\nStachenfeld, and Kelsey R. Allen. Learning 3d particle-based simulators from rgb-d videos, 2023.\nPierre-Brice Wieber, Russ Tedrake, and Scott Kuindersma. Modeling and control of legged robots.\nIn Springer handbook of robotics, pp. 1203\u20131234. Springer, 2016.\nHaotian Xue, Antonio Torralba, Joshua B. Tenenbaum, Daniel LK Yamins, Yunzhu Li, and Hsiao-\nYu Tung. 3d-intphys: Towards more generalized 3d-grounded visual intuitive physics under\nchallenging scenes, 2023.\n11\nAppendix\nA\nDECIMATION EXPERIMENTS\nWe qualitatively evaluated the impact of mesh decimation on rollouts for FIGNet and FIGNet* in the\nKITCHEN scene (Figure 5). With higher quality meshes (lower decimation), FIGNet tends to run out\nof memory, whereas lower quality meshes (higher decimation) often result in unrealistic rollouts. In\nsuch cases, objects (orange) may pass through solid objects (basket), as observed with meshes of 1k\nfaces. In contrast, FIGNet*\u2019s rollout trajectories exhibit a graceful degradation with increased levels\nof decimation, maintaining relative stability even at very high decimation levels (nf = 1000, which\nmeans approximately 1% of the original faces are preserved)\nSide-view\nBottom-view\nSide-view\nEXTRACTED MESHES (MARCHING CUBES)\nFROM PLATES\nFROM ORANGE\nFigure 6: Noisy meshes extracted from NeRF, including the orange object on the left missing its\nbottom face (from Figure 4) and the plates (from Figure 11). Notably, both FIGNet and FIGNet* can\nhandle rollouts even with such mesh imperfections, demonstrating their robustness to real-world data\nchallenges.\n40K FACES\nNONE\nnf\n20K FACES\n1K FACES\nMpassive\nnf\nnf\nnf\nFigure 7: Effect of the decimation parameter on the mesh quality. Left: no decimation. Right: high\ndecimation.\nB\nIMAGE SEGMENTATIONS\nWe provide some examples of how the mesh extraction procedure described in subsection 3.2 works\nin Figure 8 and Figure 9.\nC\nKITCHEN SCENE DETAILS\nWe collected 1027 images of a KITCHEN scene that included different elements such as apples,\noranges, baskets and plates. We extracted the images from a video recorded with an iPhone 14 Pro at\n60fps and HEVC format (Figure 10). We used COLMAP (Schonberger & Frahm, 2016) to estimate\nthe camera poses from the images.\n12\n2D MASKS OF OBJECT FROM DIFFERENT VIEWS\nm1\nm2\nMorange\nKITCHEN - PLATES\nm1\nm2\nm3\nMESHES\nMplates\nKITCHEN - ORANGE\nFIGURINES - DOG\nm1\nm2\nMdog\nFigure 8: Left: Selected views to generate the objects masks for the FIGURINES and KITCHEN scenes.\nThe top row corresponds to the rendered image in RGB with each orange mask {mn}N\n1 (overlaid in\nlight orange) obtained by XMEM\u2019s (Cheng & Schwing, 2022). The bottom row illustrates the same\nprocedure for the plates on the same scene. Note that partial segmentations from different views can\nalso be used to build the volumetric boundary of the object. Right: the obtained mesh Mo from each\nof the masks after decimation.\nDEPTH MASKS FROM DIFFERENT VIEWS\nm1\nm2\nVOLUMETRIC BOX\nVo\nz(m1)\nz(m2)\nxm1\nxm2\nFigure 9: Visualizing the generation of the orange\u2019s volumetric box from depth masks in the KITCHEN\nscene.\nD\nIMPLEMENTATION DETAILS\nD.1\nHYPER-PARAMETERS\nFIGNet* is trained identically to FIGNet (Allen et al., 2023).\nMLPs for Encoder, Processor, Decoder\nWe use MLPs with 2 hidden layers, and 128 hidden and\noutput sizes (except the decoder MLP, with an output size of 3). All MLPs, except for those in the\ndecoder, are followed by a LayerNorm(Ba et al., 2016) layer.\n13\nFigure 10: Example frames from the KITCHEN scene video.\nOptimization\nAll models are trained to 1M steps with a batch size of 128 across 8 TPU devices.\nWe use Adam optimizer, and an an exponential learning rate decay from 1e-3 to 1e-4.\nTable 2: NeRF Training Parameters\nType\nParameter\nValue\nGeneral\nnear\n0.\nGeneral\nfar\n1e6\nGeneral\nlr delay steps\n100\nGeneral\nbatch size\n65536\nGeneral\nlr init\n1e-2\nGeneral\nlr final\n1e-3\nGeneral\nadam beta1\n0.9\nGeneral\nadam beta2\n0.99\nGeneral\nadam eps\n1e-15\nGeneral\ncast rays in eval step\nTrue\nGeneral\ncast rays in train step\nTrue\nGeneral\nnum glo features\n4\nModel\nsampling strategy\n((0, 0, 64), (0, 0, 64), (1, 1, 32))\nModel\ngrid params per level\n(1, 4)\nHash\nhash map size\n2097152\nHash\nscale supersample\n1.\nHash\nmax grid size\n8192\nMLP\nnet depth\n1\nMLP\nnet width\n64\nMLP\ndisable density normals\nTrue\nMLP\ndensity activation\n@math.safe exp\nMLP\nbottleneck width\n15\nMLP\nnet depth viewdirs\n2\nMLP\nnet width viewdirs\n64\nTable 3: Default Physical Parameters\nModel\nType\nMass\nFriction\nRestitution\nFIGNet*\nActive\n1e-3\n0.5\n0.5\nFIGNet*\nPassive\n0\n0.5\n0.3\nFIGNet\nActive\n1.0\n0.8\n0.7\nFIGNet\nPassive\n0\n0.5\n0.3\nE\nADDITIONAL ROLLOUTS FOR THE REAL WORLD SCENES\nWe provide additional rollout examples for the KITCHEN scene in Figure 11 and in the website.\nPLATES-FLOOR: Duplicating the stack of plates on the right and shifting their initial position to\n14\nthe left. ORANGE-BASKET: The top orange from the stack of fruits is duplicated and dropped on\ntop of a basket of oranges. Note the correct depth ordering of the orange with respect to the basket.\nORANGE-TABLE: the orange is dropped on the table.\nt\nPLATES-FLOOR\nORANGE-BASKET\nKITCHEN\nORANGE-TABLE\nFigure 11: Additional examples of FIGNet* rollouts on the KITCHEN scene. The final row was\ngenerated using the bmove ray bending function (moving the orange from the fruit tower to the starting\nposition), while the other rows used bduplicate (copy-pasting the object).\nF\nEXAMPLE ROLLOUTS FOR MOVI-C\nAdditional simulation rollouts of FIGNet* on Kubric MOVI-C.\n15\nGround\ntruth\nFIGNet*\nGround\ntruth\nFIGNet*\nGround\ntruth\nFIGNet*\nFigure 12: Rollout of FIGNet* Kubric MOVi-C.\n16\n"
  }
]