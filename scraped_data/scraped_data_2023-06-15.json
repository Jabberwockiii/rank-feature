[
  {
    "title": "Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation",
    "link": "https://arxiv.org/pdf/2306.07954.pdf",
    "upvote": "111",
    "text": "Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation\nShuai Yang\nYifan Zhou\nZiwei Liu\nChen Change Loy \u0000S-Lab, Nanyang Technological University\n{shuai.yang, yifan.zhou, ziwei.liu, ccloy}@ntu.edu.sg\na traditional woman in Chinese ink painting, bright clean face, black and white\na beautiful woman in CG style, pink hair\ncolorful impasto painting, a beautiful woman, peacock blue\ninput\nresult\nresult\nresult\nFigure 1. We present a novel video-to-video translation framework that can render a source input video into a temporal-coherent video\nwith the style specified by a target textual description.\nAbstract\nLarge text-to-image diffusion models have exhibited\nimpressive proficiency in generating high-quality images.\nHowever, when applying these models to video domain, en-\nsuring temporal consistency across video frames remains a\nformidable challenge. This paper proposes a novel zero-\nshot text-guided video-to-video translation framework to\nadapt image models to videos. The framework includes two\nparts: key frame translation and full video translation. The\nfirst part uses an adapted diffusion model to generate key\nframes, with hierarchical cross-frame constraints applied\nto enforce coherence in shapes, textures and colors. The\nsecond part propagates the key frames to other frames with\ntemporal-aware patch matching and frame blending. Our\nframework achieves global style and local texture tempo-\nral consistency at a low cost (without re-training or opti-\nmization). The adaptation is compatible with existing im-\nage diffusion techniques, allowing our framework to take\nadvantage of them, such as customizing a specific subject\nwith LoRA, and introducing extra spatial guidance with\nControlNet.\nExtensive experimental results demonstrate\nthe effectiveness of our proposed framework over existing\nmethods in rendering high-quality and temporally-coherent\nvideos. Code is available at our project page: https:\n//www.mmlab-ntu.com/project/rerender/\n1. Introduction\nRecent text-to-image diffusion models such as DALLE-\n2 [30], Imagen [34], Stable Diffusion [32] demonstrate ex-\nceptional ability in generating diverse and high-quality im-\nages guided by natural language. Based on it, a multitude\nof image editing methods have emerged, including model\nfine-tuning for customized object generation [33], image-\nto-image translation [23], image inpainting [1], and object\nediting [12]. These applications allow users to synthesize\nand edit images effortlessly, using natural language within\na unified diffusion framework, greatly improving creation\nefficiency. As video content surges in popularity on social\nmedia platforms, the demand for more streamlined video\ncreation tools has concurrently risen. Yet, a critical chal-\nlenge remains: the direct application of existing image dif-\nfusion models to videos leads to severe flickering issues.\nResearchers have recently turned to text-guided video\ndiffusion models and proposed three solutions. The first so-\nlution involves training a video model on large-scale video\ndata [14], which requires significant computing resources.\nAdditionally, the re-designed video model is incompatible\nwith existing off-the-shelf image models. The second so-\nlution is to fine-tune image models on a single video [40],\nwhich is less efficient for long videos. Overfitting to a sin-\ngle video may also degrade the performance of the original\n1\narXiv:2306.07954v2  [cs.CV]  17 Sep 2023\ngeneral model\ncustomized model\ngeneral model\ncustomized model\ngeneral model\ncustomized model\ngeneral model\ncustomized model\n(a) input\n(b) Translation results w/o ControlNet\n(c) Translation results w/ ControlNet\na handsome man, \nGhibli style\na beautiful mountain in \nChinese ink wash painting\nFigure 2. Customized model and ControlNet generate high-quality\nresults with better consistency with both prompt and content. Our\nmethod is designed to be compatible with these existing image\ndiffusion techniques, and thus can take advantage of them to strike\na good balance between the style (prompt) and the content.\nmodels. The third solution involves zero-shot methods [20]\nthat require no training. During the diffusion sampling pro-\ncess, cross-frame constraints are imposed on the latent fea-\ntures for temporal consistency. The zero-shot strategy re-\nquires fewer computing resources and is mostly compati-\nble with existing image models, showing promising poten-\ntial. However, current cross-frame constraints are limited\nto global styles and are unable to preserve low-level consis-\ntency, e.g., the overall style may be consistent, but the local\nstructures and textures may still flicker.\nAchieving successful application of image diffusion\nmodels to the video domain is a challenging task. It re-\nquires 1) Temporal consistency: cross-frame constraints for\nlow-level consistency; 2) Zero-shot: no training or fine-\ntuning required; 3) Flexibility: compatible with off-the-\nshelf image models for customized generation. As men-\ntioned above, image models can be customized by fine-\ntuning on specific objects to capture the target style more\nprecisely than general models. Figure 2 shows two exam-\nples.\nTo take advantage of it, in this paper, we employ\nzero-shot strategy for model compatibility and aim to fur-\nther solve the key issue of this strategy in maintaining low-\nlevel temporal consistency.\nTo achieve this goal, we propose novel hierarchical\ncross-frame constraints for pre-trained image models to pro-\nduce coherent video frames. Our key idea is to use opti-\ncal flow to apply dense cross-frame constraints, with the\nprevious rendered frame serving as a low-level reference\nfor the current frame and the first rendered frame acting\nas an anchor to regulate the rendering process to prevent\ndeviations from the initial appearance. Hierarchical cross-\nframe constraints are realized at different stages of diffu-\nsion sampling. In addition to global style consistency, our\nmethod enforces consistency in shapes, textures and colors\nat early, middle and late stages, respectively. This innova-\ntive and lightweight modification achieves both global and\nlocal temporal consistency. Figure 1 presents our coher-\nent video translation results over off-the-shelf image models\ncustomized for six unique styles.\nBased on the insight, this paper introduces a novel zero-\nshot framework for text-guided video-to-video translation,\nconsisting of two parts: key frame translation and full video\ntranslation. In the first part, we adapt pre-trained image dif-\nfusion models with hierarchical cross-frame constraints for\ngenerating key frames. In the second part, we propagate the\nrendered key frames to other frames using temporal-aware\npatch matching and frame blending. The diffusion-based\ngeneration is excellent at content creation, but its multi-step\nsampling process is inefficient. The patch-based propaga-\ntion, on the other hand, can efficiently infer pixel-level co-\nherent frames but is not capable of creating new content. By\ncombining these two parts, our framework strikes a balance\nbetween quality and efficiency. To summarize, our main\ncontributions are as follows:\n\u2022 A novel zero-shot framework for text-guided video-to-\nvideo translation, which achieves both global and local\ntemporal consistency, requires no training, and is com-\npatible with pre-trained image diffusion models.\n\u2022 Hierarchical cross-frame consistency constraints to en-\nforce temporal consistency in shapes, textures and col-\nors, which adapt image diffusion models to videos.\n\u2022 Hybrid diffusion-based generation and patch-based\npropagation to strike a balance between quality and ef-\nficiency.\n2. Related Work\n2.1. Text Driven Image Generation\nGenerating images with descriptive sentences is intuitive\nand flexible. Early attempts explore GAN [42\u201344, 46] to\nsynthesize realistic images. With the powerful expressiv-\nity of Transformer [38], autoregressive models [6,9,31] are\nproposed to model image pixels as a sequence with autore-\ngressive dependency between each pixel. DALL-E [31] and\nCogView [6] train an autoregressive transformer on image\nand text tokens. Make-A-Scene [9] further considers seg-\nmentation masks as condition.\nRecent studies focus on diffusion models [15] for text-\nto-image generation, where images are synthesized via a\ngradual denoising process. DALLE-2 [30] and Imagen [34]\nintroduce pretrained large language models [28, 29] as\ntext encoder to better align the image with text, and cas-\ncade diffusion models for high resolution image genera-\ntion. GLIDE [26] introduces classifier-free guidance to im-\nprove text conditioning. Instead of applying denoising in\nthe image space, Latent Diffusion Models [32] uses the low-\nresolution latent space of VQ-GAN [7] to improve the effi-\nciency. We refer to [4] for a thorough survey.\nIn addition to diffusion models for general images, cus-\ntomized models are studied.\nTextual Inversion [10] and\nDreamBooth [33] learn special tokens to capture novel con-\ncepts and generate related images given a small number\nof example images. LoRA [17] accelerates the fine-tuning\n2\nlarge models by learning low-rank weight matrices added\nto existing weights. ControlNet [45] fine-tunes a new con-\ntrol path to provide pixel-level conditions such as edge\nmaps and pose, enabling fine-grained image generation.\nOur method does not alter the pre-trained model, thus is\northogonal to these existing techniques.\nThis empowers\nour method to leverage DreamBooth and LoRA for bet-\nter customized video translation and to use ControlNet for\ntemporal-consistent structure guidance as in Fig. 2.\n2.2. Video Editing with Diffusion Models\nFor\ntext-to-video\ngeneration,\nVideo\nDiffusion\nModel [16] proposes to extend the 2D U-Net in im-\nage model to a factorized space-time UNet.\nImagen\nVideo [14] scales up the Video Diffusion Model with a\ncascade of spatial and temporal video super-resolution\nmodels, which is further extended to video editing by\nDreamix [25].\nMake-A-Video [36] leverages video data\nin an unsupervised manner to learn the movement to drive\nthe image model. Although promising, the above methods\nneed large-scale video data for training.\nTune-A-Video [40] instead inflates an image diffusion\nmodel into a video model with cross-frame attention, and\nfine-tunes it on a single video to generate videos with\nrelated motion.\nBased on it, Edit-A-Video [35], Video-\nP2P [22] and vid2vid-zero [39] utilize Null-Text Inver-\nsion [24] for precise inversion to preserve the unedited re-\ngion. However, these models need fine-tuning of the pre-\ntrained model or optimization over the input video, which\nis less efficient.\nRecent developments have seen the introduction of zero-\nshot methods that, by design, operate without any train-\ning phase. Thus, these methods are naturally compatible\nwith pre-trained diffusion variants like InstructPix2Pix [2]\nor ControlNet to accept more flexible conditions like depth\nand edges.\nBased on the editing masks detected by\nPrompt2Prompt [12] to indicate the channel and spatial re-\ngion to preserve, FateZero [27] blends the attention features\nbefore and after editing. Text2Video-Zero [20] translates\nthe latent to directly simulate motions and Pix2Video [3]\nmatches the latent of the current frame to that of the previ-\nous frame. All the above methods largely rely on cross-\nframe attention and early-step latent fusion to improve\ntemporal consistency.\nHowever, as we will show later,\nthese strategies predominantly cater to high-level styles\nand shapes, and being less effective in maintaining cross-\nframe consistency at the level of texture and detail.\nIn\ncontrast to these approaches, our method proposes a novel\npixel-aware cross-frame latent fusion, which non-trivially\nachieves pixel-level temporal consistency.\nAnother zero-shot solution is to apply frame interpola-\ntion to infer the videos based on one or more diffusion-\nedited frames. The seminal work of image analogy [13] mi-\ngrates the style effect from an exemplar pair to other images\nFirst frame; First key frame; Anchor frame\nKey frame\nNon-key frame\nCross-frame Attn; Shape-aware  Fusion; Pixel-aware Fusion; Color AdaIN\nCross-frame Attn; Pixel-aware Fusion\nFrame interpolation\n\u2026\n\ud835\udc3c0\n\u2032\n\ud835\udc3c\ud835\udc3e\n\u2032\n\ud835\udc3c2\ud835\udc3e\n\u2032\n\ud835\udc3c\ud835\udc56\n\u2032\nKey Frame Translation\nFull Video Translation\nFigure 3. Illustration of the interaction between different frames\nto impose temporal constraints in our framework.\nwith patch matching. Fi\u02c7ser et al. [8] extend image analogy\nto facial video translation with the guidance of facial fea-\ntures. Later, Jamriv\u02c7ska et al. [19] propose an improved Eb-\nSynth for general video translation based on multiple ex-\nemplar frames with a novel temporal blending approach.\nAlthough these patch-based methods can preserve fine de-\ntails, their temporal consistency largely relies on the coher-\nence across the exemplar frames. Thus, our adapted dif-\nfusion model for generating coherent frames is well suited\nfor these methods, as we will show later in Fig. 11. In this\npaper, we integrate the zero-shot EbSynth into our frame-\nwork to achieve better temporal consistency and accelerate\ninference without any further training.\n3. Preliminary: Diffusion Models\nStable Diffusion\nStable Diffusion is a latent diffusion\nmodel operating in the latent space of an autoencoder\nD(E(\u00b7)), where E and are the encoder and decoder, respec-\ntively. Specifically, for an image I with its latent feature\nx0 = E(I), the diffusion forward process iteratively add\nnoises to the latent\nq(xt|xt\u22121) = N(xt; \u221a\u03b1txt\u22121, (1 \u2212 \u03b1t)I),\n(1)\nwhere t = 1, ..., T is the time step, q(xt|xt\u22121) is the condi-\ntional density of xt given gt\u22121, and \u03b1t is hyperparameters.\nAlternatively, we can directly sample xt at any time step\nfrom x0 with,\nq(xt|x0) = N(xt; \u221a\u00af\u03b1tx0, (1 \u2212 \u00af\u03b1t)I),\n(2)\nwhere \u00af\u03b1t = Qt\ni=1 \u03b1i.\nThen in the diffusion backward process, a U-Net \u03f5\u03b8 is\ntrained to predict the noise of the latent to iteratively re-\ncover x0 from xT . Given a large T, x0 will be completely\ndestroyed in the forward process so that xT approximates\na standard Gaussian distribution. Therefore, \u03f5\u03b8 correspond-\ningly learns to infer valid x0 from random Gaussian noises.\nOnce trained, we can sample xt\u22121 based on xt with a deter-\nministic DDIM sampling [37]:\nxt\u22121 = \u221a\u03b1t\u22121\n\u02c6xt\u21920\n| {z }\npredicted x0\n+\np\n| 1 \u2212 \u03b1t\u22121\u03f5\u03b8(xt, t, cp)\n{z\n}\ndirection pointing to xt\u22121\n,\n(3)\n3\nDDIM sampling \u00d7 T\nPretrained Image \nDiffusion Model \u03b5\u03b8\nPretrained Image \nDiffusion Model \u03b5\u03b8\nDDPM \nforward\n\ud835\udc65\ud835\udc47\n1,\ud835\udc65\ud835\udc47\n2,\u2026 , \ud835\udc65\ud835\udc47\n\ud835\udc41\n\u01b8\ud835\udc65\ud835\udc61\u21920\n1\n, \u01b8\ud835\udc65\ud835\udc61\u21920\n\ud835\udc56\u22121\n\u01b8\ud835\udc65\ud835\udc61\u21920\n\ud835\udc56\nE\n\ud835\udc65\ud835\udc61\u22121\n1\n,\ud835\udc65\ud835\udc61\u22121\n\ud835\udc56\u22121\n\ud835\udc65\ud835\udc61\u22121\n\ud835\udc56\nD\nD\n\ud835\udc3c1\n\u2032,\ud835\udc3c\ud835\udc56\u22121\n\u2032\n\ud835\udc3c\ud835\udc56\n\u2032\nSec. 4.1.2\nShape-Aware\nLatent Fusion\nSec. 4.1.1\nCross-Frame \nAttention\nSec. 4.1.3\nPixel-Aware\nLatent Fusion\nSec. \n4.1.4 \nAdaIN\nSec. 4.1.3\nFidelity-Oriented \nImage Encoding\nE\nwarp\nwarp\n\ud835\udc3c1\n\u2032,\ud835\udc3c2\n\u2032,\u2026 ,\ud835\udc3c\ud835\udc41\n\u2032\naggregate\nT\n0\nCross-Frame Attention\nShape-aware\nLatent Fusion\nPixel-aware\nLatent Fusion\nAdaptive \nLatent Adjust\nTs\nTp0\nTp1\nTa\n\u201cA beautiful girl in Ghibli style\u201d,\ncp\ncf\n\u201cA beautiful girl in Ghibli style\u201d,\n\ud835\udc3c1,\ud835\udc3c2,\u2026 ,\ud835\udc3c\ud835\udc41\n(a) Diffusion-based key frame translation with hierarchical cross-frame consistency constraints\n(b) Sampling pipeline\n\u121a\ud835\udc3c\ud835\udc56\n\u2032\n\ud835\udc40\ud835\udc56\nand\nFigure 4. Framework of the proposed zero-shot text-guided video translation. (a) We adapt the pre-trained image diffusion model (Stable\nDiffusion + ControlNet) with hierarchical cross-frame constraints to render coherent frames. The red dotted lines denote the sampling\nprocess of the original image diffusion model. The black lines denote our adapted process for video translation. (b) We apply different\nconstraints at different sampling steps.\nwhere \u02c6xt\u21920 is the predicted x0 at time step t,\n\u02c6xt\u21920 = (xt \u2212\n\u221a\n1 \u2212 \u03b1t\u03f5\u03b8(xt, t, cp))/\u221a\u03b1t,\n(4)\nand \u03f5\u03b8(xt, t, cp) is the predicted noise of xt based on the\ntime step t and the text prompt condition cp.\nDuring inference, we can sample a valid x0 from the\nstandard Guassian noise xT = zT , zT \u223c N(0, I) with\nDDIM sampling, and decode x0 to the final generated im-\nage I\u2032 = D(x0).\nControlNet\nAlthough flexible, natural language has lim-\nited spatial control over the output.\nTo improve spatial\ncontrollability, [45] introduce a side path called ControlNet\nto Stable Diffusion to accept extra conditions like edges,\ndepth and human pose.\nLet cf be the extra condition,\nthe noise prediction of U-Net with ControlNet becomes\n\u03f5\u03b8(xt, t, cp, cf). Compared to InstructPix2Pix, ControlNet\nis orthogonal to customized Stable Diffusion models. To\nbuild a general zero-shot V2V framework, we use Control-\nNet to provide structure guidance from the input video to\nimprove temporal consistency.\n4. Zero-Shot Text-Guided Video Translation\nGiven a video with N frames {Ii}N\ni=0, our goal is to ren-\nder it into a new video {I\u2032\ni}N\ni=0 in another artistic expression\nspecified by text prompts and/or off-the-shelf customized\nStable Diffusion models. Our framework consists of two\nparts: Key Frame Translation (Sec. 4.1) and Full Video\nTranslation (Sec. 4.2). In the first part, we introduce four\nhierarchical cross-frame constraints into pre-trained image\ndiffusion models, guiding the rendering of coherent key\nframes using anchor and previous key frames, as as illus-\ntrated in Fig. 3. Then in the second part, non-key frames\nare interpolated based on their neighboring two key frames.\nThus our framework can fully exploit the relationship be-\ntween different frames to enhance temporal consistency of\nthe outputs.\nTable 1. Notation summary.\nNotation\nDescription\nE, D\nimage encoder and decoder of Stable Diffusion\nE\u2217\nthe proposed fidelity-oriented image encoder\nIi\nthe i-th key frame (Sec. 4.1); the i-th video frame (Sec. 4.2)\nI\u2032\ni\ntranslation result of Ii with the proposed method\n\u00afI\u2032\ni\ntranslation result of Ii without PA fusion (Fig. 11(b))\n\u02dcI\u2032\ni, Mi\nresult of warping I\u2032\n0 and I\u2032\ni\u22121 to Ii, and its occlusion mask\nI\u2032j\ni\nresult of propagating the translated key frame I\u2032\nj to Ii\nxi\n0\nlatent feature of Ii encoded by E\nxi\nt\nlatent feature of Ii at the diffusion backward denoising step t\n\u02c6xi\nt\u21920\nestimated xi\n0 of Ii at the diffusion backward denoising step t\n\u02dcxi\nt\nlatent feature of \u02dcI\u2032\ni at the diffusion forward sampling step t\nwi\nj, M i\nj\noptical flow and occlusion mask from Ij to Ii\n4.1. Key Frame Translation\nFigure 4 illustrates the T-step sampling pipeline for the\nkey frame translation. Following SDEdit [23], the pipeline\nbegins with xT = \u221a\u00af\u03b1T x0 + (1 \u2212 \u00af\u03b1T )zT , zT \u223c N(0, I),\nthe noisy latent code of the input video frame rather than\nthe pure Gaussian noise. It enables users to determine how\nmuch detail of the input frame is preserved in the output by\nadjusting T, i.e., smaller T retain more detail. Then, during\nsampling each frame, we use the first frame as anchor frame\nand its previous frame to constrain global style consistency\nand local temporal consistency.\nSpecifically, cross-frame attention [40] is applied to all\nsampling steps for global style consistency (Sec. 4.1.1). In\naddition, in early steps, we fuse the latent feature with the\naligned latent feature of previous frame to achieve rough\nshape alignments (Sec. 4.1.2). Then in mid steps, we use\nthe latent feature with the encoded warped anchor and pre-\nvious outputs to realize fine texture alignments (Sec. 4.1.3).\nFinally, in late steps, we adjust the latent feature distribu-\ntion for color consistency (Sec. 4.1.4). For simplicity, we\nwill use {Ii}N\ni=0 to refer to the key frames in this section.\nWe summarize important notations in Table 1.\n4\n4.1.1\nStyle-aware cross-frame attention\nSimilar to other zero-shot video editing methods [3, 20],\nwe replace self-attention layers in the U-Net with cross-\nframe attention layers to regularize the global style of I\u2032\ni\nto match that of I\u2032\n1 and I\u2032\ni\u22121. In Stable Diffusion, each self-\nattention layer receives the latent feature vi (for simplicity\nwe omit the time step t) of Ii, and linearly projects vi into\nquery, key and value Q, K, V to produce the output by\nSelf Attn(Q, K, V ) = Softmax( QKT\n\u221a\nd ) \u00b7 V with\nQ = W Qvi, K = W Kvi, V = W V vi,\n(5)\nwhere W Q, W K, W V are pre-trained matrices for feature\nprojection. Cross-frame attention, by comparison, uses the\nkey K\u2032 and value V \u2032 from other frames (we use the first\nand previous frames), i.e., CrossFrame Attn(Q, K\u2032, V \u2032) =\nSoftmax( QK\u2032T\n\u221a\nd ) \u00b7 V \u2032 with\nQ = W Qvi, K\u2032 = W K[v1; vi\u22121], V \u2032 = W V [v1; vi\u22121].\n(6)\nIntuitively, self-attention can be thought as patch match-\ning and voting within a single frame, while cross-frame at-\ntention seeks similar patches and fuses the corresponding\npatches from other frames, meaning the style of I\u2032\ni will in-\nherit that of I\u2032\n1 and I\u2032\ni\u22121.\n4.1.2\nShape-aware cross-frame latent fusion\nCross-frame attention is limited to global style. To constrain\nthe cross-frame local shape and texture consistency, we use\noptical flow to warp and fuse the latent features. Let wi\nj and\nM i\nj denote the optical flow and occlusion mask from Ij to\nIi, respectively. Let xi\nt be the latent feature for I\u2032\ni at time\nstep t. We update the predicted \u02c6xt\u21920 in Eq. (3) by\n\u02c6xi\nt\u21920 \u2190 M i\nj \u00b7 \u02c6xi\nt\u21920 + (1 \u2212 M i\nj) \u00b7 wi\nj(\u02c6xj\nt\u21920).\n(7)\nw and M are downsampled to match the resolution of x (we\nomit the downsampling operation for simplicity in this pa-\nper). For the reference frame Ij, we experimentally find that\nthe anchor frame (j = 0) provides better guidance than the\nprevious frame (j = i \u2212 1). We observe that interpolating\nelements in the latent space can lead to blurring and shape\ndistortion in the late steps. Therefore, we limit the fusion to\nonly early steps for rough shape guidance.\n4.1.3\nPixel-aware cross-frame latent fusion\nTo constrain the low-level texture features in mid steps, in-\nstead warping the latent feature, we can alternatively warp\nprevious frames and encode them back to the latent space\nfor fusion in an inpainting manner. However, the lossy au-\ntoencoder introduces distortions and color bias that easily\naccumulate along the frame sequence. Figure 5(b) shows\n(a) Input image\n(b) \ud835\udc9f \u2130 \u2219\n\u00d7 10\n(c) \ud835\udc9f \u2130\u2032 \u2219\n\u00d7 10\n(d) \ud835\udc9f \u2130\u2217 \u2219\n\u00d7 10\n(e) \ud835\udc40\u2130\n(f) Error map of (b)\n(g) Error map of (c)\n(h) Error map of (d)\nFigure 5. Fidelity-oriented image encoding.\n\u2130\n\ud835\udc650\n\ud835\udc5f\n\u2130\u2217(\ud835\udc3c)\n\ud835\udf06\u2130\n\ud835\udc40\u2130\n\u2130\u2032(\ud835\udc3c)\n\ud835\udc9f\n+\n\u2130\n\ud835\udc650\n\ud835\udc5f\ud835\udc5f\n\ud835\udc3c\n\ud835\udc3c\ud835\udc5f\n\ud835\udc9f(\u2130\u2032 \ud835\udc3c )\n\u00b7\n\u00b7\n\ud835\udc9f\n-\n\ud835\udc9f\n+\nEq. (8)\nEq. (9)\ncompute the low-error mask\n\ud835\udcd4\u2032\n\ud835\udcd4\u2217\nFigure 6. Pipeline of the fidelity-oriented image encoding.\nan example of the distorted result after encoding and de-\ncoding 10 times. [1] solved this problem by fine-tuning the\ndecoder\u2019s weights to fit each image, which is impractical for\nlong videos. To efficiently solve this problem, we propose\na novel fidelity-oriented zero-shot image encoding method.\nFidelity-oriented image encoding\nOur key insight is the\nobservation that the amount of information lost each time\nin the iterative auto-encoding process is consistent. There-\nfore, we can predict the information loss for compensa-\ntion. Specifically, for arbitrary image I, we encode and\ndecode it twice, obtaining xr\n0 = E(I), Ir = D(xr\n0) and\nxrr\n0 = E(Ir), Irr = D(xrr\n0 ). We assume the loss from the\ntarget lossless x0 to xr\n0 is linear to that from xr\n0 to xrr\n0 . Then\nwe define the encoding E\u2032 with compensation as\nE\u2032(I) := xr\n0 + \u03bbE(xr\n0 \u2212 xrr\n0 ),\n(8)\nwhere we find the linear coefficient \u03bbE = 1 works well.\nWe further add a mask ME to prevent the possible artifacts\nintroduced by compensation (e.g., blue artifact near the eyes\nin Fig. 5(c)). ME indicates where the error between I and\nD(E\u2032(I)) is under a pre-defined threshold. Then, our novel\nfidelity-oriented image encoding E\u2217 takes the form of\nE\u2217(I) := xr\n0 + ME \u00b7 \u03bbE(xr\n0 \u2212 xrr\n0 ).\n(9)\nThe encoding pipeline is summarized in Fig. 6. As shown\nin Fig. 5(d), our method preserves image information well\neven after encoding and decoding 10 times.\nStructure-guided inpainting\nAs illustrated in Fig. 7, for\npixel-level coherence, we warp the anchor frame I\u2032\n0 and the\n5\n\ud835\udc3c0\n\u2032, \ud835\udc3c\ud835\udc56\u22121\n\u2032\n\u121a\ud835\udc3c\ud835\udc56\n\u2032\n\ud835\udc40\ud835\udc56\n\ud835\udc65\ud835\udc61\u22121\n\ud835\udc56\nupdated \ud835\udc65\ud835\udc61\u22121\n\ud835\udc56\n\u04a7\ud835\udc3c\ud835\udc56\n\u2032\nwarp and overlay \n(Eq. (10))\n\ud835\udc40\ud835\udc56\u22121\n\ud835\udc56\n, \ud835\udc400\n\ud835\udc56\n\u2130\u2217\nsample\n(Eq. (2))\nunion\n\u0de4x\ud835\udc61\u22121\n\ud835\udc56\ndownsample\ninpaint (Eq. (3))\n\u2026\n\u2026\nDDIM sampling \u00d7 T\nFigure 7. Pipeline of the pixel-aware latent fusion.\nprevious frame I\u2032\ni\u22121 to the i-th frame and overlay them on\na rough rendered frame \u00afI\u2032\ni obtained without the pixel-aware\ncross-frame latent fusion as\nM i\n0\u00b7\n\u0000M i\ni\u22121\u00b7\u00afI\u2032\ni+(1\u2212M i\ni\u22121)\u00b7wi\ni\u22121(I\u2032\ni\u22121)\n\u0001\n+\n\u00001\u2212M i\n0\n\u0001\n\u00b7wi\n0(I\u2032\n0)\n(10)\nThe resulting fused frame \u02dcI\u2032\ni provides pixel reference for the\nsampling of I\u2032\ni, i.e., we would like I\u2032\ni to match \u02dcI\u2032\ni outside\nthe mask area Mi = M i\n0 \u2229 M i\ni\u22121 and to match the structure\nguidance from ControlNet inside Mi. We formulate it as\na structure-guided inpainting task and follow [1] to update\nxi\nt\u22121 in Eq. (3) as\nxi\nt\u22121 \u2190 Mi \u00b7 xi\nt\u22121 + (1 \u2212 Mi) \u00b7 \u02dcxi\nt\u22121,\n(11)\nwhere \u02dcxi\nt\u22121 is the sampled xt\u22121 from x0 = E\u2217(\u02dcI\u2032\ni) based\non Eq. (2).\n4.1.4\nColor-aware adaptive latent adjustment\nFinally, we apply AdaIN [18] to \u02c6xi\nt\u21920 to match its channel-\nwise mean and variance to \u02c6x1\nt\u21920 in the late steps. It can\nfurther keep the color style coherent throughout the whole\nkey frames.\n4.2. Full Video Translation\nFor frames with similar content, existing frame interpo-\nlation methods like Ebsynth\n[19] can generate plausible\nresults by propagating the rendered frames to their neigh-\nbors efficiently. However, compared to diffusion models,\nframe interpolation cannot create new content.\nTo bal-\nance between quality and efficiency, we propose a hybrid\nframework to render key frames and other frames with the\nadapted diffusion model and Ebsynth, respectively.\nSpecifically, we sample the key frames uniformly for\nevery K frame, i.e., I0, IK, I2K, ... and render them to\nI\u2032\n0, I\u2032\nK, I\u2032\n2K, ... by our adapted diffusion model. We then\nrender the remaining non-key frames. Taking Ii (0 < i <\nK) for example, we adopt Ebsynth to interpolate I\u2032\ni with its\nneighboring stylized key frames I\u2032\n0 and I\u2032\nK. Ebsynth has\ntwo steps of frame propagation and frame blending. In the\nfollowing, we will briefly introduce the main idea of these\ntwo steps and discuss how we adapt Ebsynth to our frame-\nwork. For implementation details, please refer to [19].\n4.2.1\nSingle key frame propagation\nFrame propagation aims to warp the stylized key frame to\nits neighboring non-key frames based on their dense corre-\nspondences. We directly follow Ebsynth to adopt a guided\npath-matching algorithm with color, positional, edge, and\ntemporal guidance for dense correspondence prediction and\nframe warping. Our framework propagates each key frame\nto its preceding K \u2212 1 and succeeding K \u2212 1 frames. We\ndenote the result of propagating I\u2032\nj to Ii as I\u2032j\ni .\nFor Ii\n(0 < i < K), we will obtain two results I\u20320\ni and I\u2032K\ni\nfrom\nits nearby key frames I\u2032\n0 and I\u2032\nK.\n4.2.2\nTemporal-aware blending\nFrame blending aims to blend I\u20320\ni and I\u2032K\ni\nto a final result I\u2032\ni.\nEbsynth proposes a three-step blending scheme: 1) Com-\nbining colors and gradients of I\u20320\ni and I\u2032K\ni\nby selecting the\nones with lower errors during patch matching (Sec. 4.2.1)\nfor each location; 2) Using the combined color image as\na histogram reference for contrast-preserving blending [11]\nover I\u20320\ni\nand I\u2032K\ni\nto generate an initial blended image; 3)\nEmploying the combined gradient as a gradient reference\nfor screened Poisson blending [5] over the initial blended\nimage to obtain the final result. Differently, our framework\nonly adopts the first two blending steps and uses the initial\nblended image as I\u2032\ni. We do not apply Poisson blending,\nwhich we find sometimes causes artifacts in non-flat regions\nand is relatively time-consuming.\n5. Experimental Results\n5.1. Implementation Details\nThe experiment is conducted on one NVIDIA Tesla\nV100 GPU. We employ the fine-tuned and LoRA models\nbased on Stable Diffusion 1.5 from https://civitai.\ncom/. We use Stable Diffusion originally uses Tmax =\n1000 steps. For the sampling pipeline in Fig. 4(b), by de-\nfault, we set Ts = 0.1Tmax, Tp0 = 0.5Tmax, Tp1 =\n0.8Tmax and Ta = 0.8Tmax and use 20 steps of DDIM\nsampling.\nWe tune T for each video.\nControlNet [45]\nis used to provide structure guidance in terms of edges,\nwith the control weight tuned for each video.\nWe use\nGMFlow [41] for optical flow estimation and compute the\nocclusion masks by forward-backward consistency check.\nFor full video translation, by default, we sample key\nframes for every K = 10 frames.\nThe testing videos\nare from https://www.pexels.com/ and https:\n//pixabay.com/, with their short side resized to 512.\nIn terms of running time for 512\u00d7512 videos, key frame\nand non-key frame translations take about 14.23s and 1.49s\n6\n(a)\nInput\n(b) \nvid2vid-zero\n(c)\nFateZero\n(f)\nOur\n(d)\nPix2Video\n(e)\nText2Video-Zero\nPrompt: white ancient Greek sculpture, Venus de Milo, light pink and blue background\nPrompt: a swan in Chinese ink wash painting, monochrome\nFigure 8. Visual comparison with zero-shot video translation methods. The magenta box indicates the inconsistent region. For Text2Video-\nZero and our method, we further enlarge the region to better visualize the pixel-level consistency.\nper frame, respectively.\nOverall, a full video translation\ntakes about (14.23 + 1.49(K \u2212 1))/K = 1.49 + 12.74/Ks\nper frame.\nWe will release our code upon publication of the paper.\n5.2. Comparison with State-of-the-Art Methods\nWe compare with four recent zero-shot methods:\nvid2vid-zero\n[39],\nFateZero\n[27],\nPix2Video\n[3],\nText2Video-Zero [20] on key frame translation with\nK = 5. The official code of the first three methods does\nnot support ControlNet, and when loading customized\nmodels, we find they fail to generate plausible results, e.g.,\nvid2vid-zero will generate frames totally different from the\ninput. Therefore, only Text2Video-Zero and our method\nuse the customized model with ControlNet. Figure 8 and\nFigure 9 present the visual results. FateZero successfully\nreconstructs the input frame but fails to adjust it to match\nthe prompt. On the other hand, vid2vid-zero and Pix2Video\nexcessively modify the input frame, leading to significant\nshape distortion and discontinuity across frames.\nWhile\neach frame generated by Text2Video-Zero exhibits high\nquality, they lack coherence in local textures as indicated\nby the black boxes. Finally, our proposed method demon-\nstrates clear superiority in terms of output quality, content\nand prompt matching and temporal consistency.\nFor quantitative evaluation, we follow FateZero and\nPix2Video to report Fram-Acc (CLIP-based frame-wise\nTable 2. Quantitative comparison and user preference rates.\nMetric\nv2v-zero\nFateZero\nPix2Video\nT2V-Zero\nOurs\nFram-Acc\n0.862\n0.556\n0.995\n0.963\n0.979\nTem-Con\n0.975\n0.979\n0.953\n0.983\n0.983\nPixel-MSE\n0.098\n0.085\n0.216\n0.084\n0.069\nUser-Balance\n3.8%\n5.9%\n9.2%\n15.4%\n65.8%\nUser-Temporal\n3.8%\n9.6%\n4.2%\n10.8%\n71.6%\nUser-Overall\n2.9%\n4.2%\n4.2%\n15.0%\n73.7%\nediting accuracy), Tmp-Con (CLIP-based cosine similarity\nbetween consecutive frames), Pixel-MSE (averaged mean-\nsquared pixel error between aligned consecutive frames) in\nTable 2. Our method achieves the best temporal consistency\nand the second best frame editing accuracy. We further con-\nduct a user study with 30 participants.\nThe participants\nare asked to select the best results among the five meth-\nods based on three criteria: 1) how well the result balance\nbetween the prompt and the input frame, 2) the temporal\nconsistency of the result, and 3) the overall quality of the\nvideo translation. Table 2 presents the average preference\nrates across 8 testing videos, and our method achieves the\nhighest rates in all three metrics.\n5.3. Ablation Study\nHierarchical cross-frame consistency constraints\nFig-\nure 10 compares the results with and without different\n7\nPrompt: a butterfly in cartoon style\n(a) Input\n(c) Our\n(b) Text2Video-Zero\nPrompt: a beautiful woman in CG style\nPrompt: a clean simple white jade sculpture\n(a) Input\n(c) Our\n(b) Text2Video-Zero\nFigure 9. Visual comparison with Text2Video-Zero. Text2Video-\nZero and our method use the same customized model and Control-\nNet for a fair comparison. Our method outperforms Text2Video-\nZero in terms of local texture temporal consistency. The red box\nindicates the inconsistent region.\ncross-frame consistency constraints. We demonstrate the\nefficacy of our approach on a video containing simple trans-\nlational motion in the first half and complex 3D rotation\ntransformations in the latter half.\nTo better evaluate the\ntemporal consistency, we encourage readers to watch the\nvideos on the project webpage. The cross-frame attention\nensures consistency in global style, while the adaptive la-\ntent adjustment in Sec. 4.1.4 maintains the same hair color\nas the first frame, or the hair color will follow the input\nframe to turn dark. Note that the adaptive latent adjust-\nment is optional to allow users to decide which color to\nfollow. The above two global constraints cannot capture\nlocal movement. The shape-aware latent fusion (SA fusion)\nin Sec. 4.1.2 addresses this by translating the latent features\nto translate the neck ring, but cannot maintain pixel-level\nconsistency for complex motion. Only the proposed pixel-\naware latent fusion (PA fusion) can coherently render local\ndetails such as hair styles and acne.\nWe provide additional examples in Figs. 11-12 to\ndemonstrate the effectiveness of PA fusion. While Con-\ntrolNet can guide the structure well, the inherent random-\nness introduced by noise addition and denoising makes it\ndifficult to maintain coherence in local textures, resulting\nin missing elements and altered details. The proposed PA\nfusion restores these details by utilizing the corresponding\npixel information from previous frames. Moreover, such\nconsistency between key frames can effectively reduce the\nghosting artifacts in interpolated non key frames.\nFidelity-oriented image encoding\nWe present a detailed\nanalysis of our fidelity-oriented image encoding in Figs. 13-\nTable 3. Effect of key frame sampling interval K\nMetric\nK = 1\nK = 5\nK = 10\nK = 20\nK = 50\nK = 100\nFram-Acc\n1.000\n1.000\n1.000\n1.000\n0.990\n0.890\nTem-Con\n0.992\n0.993\n0.994\n0.994\n0.993\n0.993\nPixel-MSE\n0.037\n0.028\n0.025\n0.022\n0.020\n0.020\n15, in addition to Fig. 5. Two Stable Diffusion\u2019s officially\nreleased autoencoders, the fine-tuned f8-ft-MSE VAE and\nthe original more lossy kl-f8 VAE, are used for testing our\nmethod. The fine-tuned VAE introduces artifacts and the\noriginal VAE results in great color bias as in Fig. 13(b). Our\nproposed fidelity-oriented image encoding effectively alle-\nviates these issues. For quantitative evaluation, we report\nthe MSE between the input image and the reconstructed re-\nsult after multiple encoding and decoding in Fig. 14, us-\ning the first 1,000 images of the MS-COCO [21] valida-\ntion set. The results are consistent with the visual observa-\ntions: our proposed method significantly reduces error ac-\ncumulation compared to raw encoding methods. Finally, we\nvalidate our encoding method in the video translation pro-\ncess in Fig. 15(b)(c), where we use only the previous frame\nwithout the anchor frame in Eq. (10) to better visualize er-\nror accumulation. Our method mostly reduces the loss of\ndetails and color bias caused by lossy encoding. Besides,\nour pipeline includes an anchor frame and adaptive latent\nadjustment to further regulate the translation, as shown in\nFig. 15(d), where no obvious errors are observed.\nFrequency of key frames K\nWe report the quantitative\nfull video translation results of Fig. 10(a) under different\nK in Table 3.\nWith large K, more frame interpolation\nimproves pixel-level temporal consistency, which however\nharms the quality, leading to low Fram-Acc. A broad range\nof K \u2208 [5, 20] is recommended for balance.\n5.4. More Results\nFlexible structure and color control\nThe proposed\npipeline allows flexible control over content preservation\nthrough the initialization of xT . Rather than setting xT to a\nGaussian noise (Fig. 16(b)), we use a noisy latent version\nof the input frame to better preserve details (Fig. 16(c)).\nUsers can adjust the value of T to balance content and\nprompt. Moreover, if the input frame introduces unwanted\ncolor bias (e.g., blue sky in Chinese ink painting), a color\ncorrection option is provided: the input frame is adjusted\nto match the color histogram of the frame generated by\nxT = zT (Fig. 16(b)). With the adjusted frame as input\n(bottom row of Fig. 16(a)), the rendered results (bottom row\nof Figs. 16(c)-(f)) better match the color indicated by the\nprompt.\nApplications\nFigure 17 shows some applications of our\nmethod. With prompts \u2018a cute cat/fox/hamster/rabbit\u2019, we\n8\nacne is preserved\nneck ring \ntranslates with \nthe neck\nconsistent hair \nstyles\nconsistent strand \nof hair next to the \near\n(b) baseline (image model)(c) (b)+cross frame attn\n(e) (d)+SA fusion\n(f) full\n(a) input video\n(d) (c)+AdaIN\nsimple translation\ncomplex rotation\nLegend\nFigure 10. Effect of the proposed hierarchical cross-frame constraints. (a) Input frames #1, #55, #94. (b) Image diffusion model renders\neach frame independently. (c) Cross frame attention keeps the overall style consistent. (d) AdaIN preserves the hair color. (e) Shape-aware\nlatent fusion keeps the overall movement of the objects coherent. (f) Pixel-aware latent fusion achieves pixel-level temporal consistency.\n#80\n#90\n#85\n#83\n#87\nkey frame\nkey frame\npropagated frames\n(b) w/o PA fusion\n(a) full\nFigure 11. Effect of the pixel-aware latent fusion on frame prop-\nagation. The proposed pixel-aware latent fusion helps generate\nconsistent key frames. Without it, the pixel level inconsistency be-\ntween key frames leads to ghosting artifacts on the non-key frames\nduring the frame blending.\ncan perform text-guided editing to translate a dog into other\nkinds of pets in Fig. 17(a). By using customized modes\nfor generating cartoons or photos, we can achieve non-\nphotorealistic and photorealistic rendering in Fig. 17(b) and\nFigs. 17(c)(d), respectively. In Fig. 18, we present our syn-\nthesized dynamic virtual characters of novels and manga,\nbased on a real human video and a prompt to describe the\nappearance. Additional results are shown in Fig. 19.\n5.5. Limitations\nFigures 20-22 illustrate typical failure cases of our\nmethod. First, our method relies on optical flow and there-\nfore, inaccurate optical flow can lead to artifacts. In Fig. 20,\nour method can only preserve the embroidery if the cross-\nframe correspondence is available. Otherwise, the proposed\nPA fusion will have no effect. Second, our method assumes\nthe optical flow remains unchanged before and after trans-\nlation, which may not hold true for significant appearance\nchanges as in Fig. 21(b), where the resulting movement may\nbe wrong. Although setting a smaller T can address this is-\nsue, it may compromise the desired styles. Meanwhile, the\n(c) w/o PA fusion\n(d) full\n(a) input\n(b) result of frame #1\nFigure 12. Effect of the pixel-aware latent fusion. Prompts (from\ntop to bottom): \u2018Arcane style, a handsome man\u2019, \u2018Loving Vincent,\nhiking, grass\u2019, \u2018Disco Elysium, street view\u2019. Local regions are en-\nlarged and shown in the top right.\nmismatches of the optical flow mean the mismatches in the\ntranslated key frames, which may lead to ghosting artifacts\n(Fig. 21(d)) after temporal-aware blending. Also, we find\nthat small details and subtle motions like accessories and\neye movement cannot be well preserved during the trans-\nlation. Lastly, we uniformly sample the key frames, which\nmay not optimal. Ideally, the key frames should contain\n9\n(a) \ud835\udc9f \u2130 \u2219\n(c) \ud835\udc9f \u2130\u2217 \u2219\n(b) \ud835\udc9f \u2130 \u2219\n\u00d7 10\n(d) \ud835\udc9f \u2130\u2217 \u2219\n\u00d7 10\nf8-ft-MSE VAE\nOriginal kl-f8 VAE\nFigure 13. The fidelity-oriented image encoding on two VAEs.\n0.01\n0.03\n0.05\n0.07\n0.09\n0.11\n0.13\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nMean Squared Error\nIterations of Encoding-Decoding\n\ud835\udc9f \u2130 \u2219\nkl-f8 VAE\n\ud835\udc9f \u2130\u2217 \u2219\nf8-ft-MSE VAE\n\ud835\udc9f \u2130\u2032 \u2219\nkl-f8 VAE\n\ud835\udc9f \u2130\u2217 \u2219\nkl-f8 VAE\n\ud835\udc9f \u2130\u2032 \u2219\nf8-ft-MSE VAE\n\ud835\udc9f \u2130 \u2219\nf8-ft-MSE VAE\nFigure 14. Quantitative evaluation of image encoding schemes.\n(a) frame #1\n(c) frame #6 by \ud835\udc9f \u2130\u2217 \u2219\n(b) frame #6 by \ud835\udc9f \u2130 \u2219\nf8-ft-MSE VAE\nOriginal kl-f8 VAE\n(d) (c)+AdaIN+anchor\nFigure 15. Different constraints to prevent error accumulation.\n(a) input frame\n(c) T = Tmax\n(d) T = 0.9Tmax\n(e) T = 0.8Tmax\n(b) xT = zT\nw/o color correction\nw/ color correction\nFigure 16. Effect of the initialization of xT . Prompt: a traditional\nmountain in Chinese ink wash painting. The proposed framework\nenables flexible content and color control by adjusting T and color\ncorrection.\nall unique objects; otherwise, the propagation cannot create\nunseen content such as the hand in Fig. 22(b). One poten-\ntial solution is user-interactive translation, where users can\nmanually assign new key frames based on the previous re-\nsults.\n(b) real-to-2D\n(c) 2D-to-real\n(d) 3D-to-real\n(a) text-guided editing\nFigure 17. Applications of the proposed method.\nHermione Granger, brown hair and eyes\nWonder Woman, black hair\nInput\nGaladriel, the royal Elf, silver-golden hair\nHermione Granger, brown hair and eyes\nWonder Woman, black hair\nInput\nGaladriel, the royal Elf, silver-golden hair\nInput\nT\u2019 Challa, Black Panther\nFigure 18. Applications: text-guided virtual character generation.\nResults are generated with a single image diffusion model.\n6. Conclusion\nThis paper presents a zero-shot framework to adapt im-\nage diffusion models for video translation. Our method uti-\nlizes hierarchical cross-frame constraints to enforce tempo-\nral consistency in both global style and low-level textures,\nleveraging the key optical flow. The compatibility with ex-\nisting image diffusion techniques indicates that our idea\nmight be applied to other text-guided video editing tasks,\nsuch as video super-resolution and inpainting. Addition-\nally, our proposed fidelity-oriented image encoding could\nbenefit existing diffusion-based methods. We believe that\nour approach can facilitate the creation of high-quality and\n10\nInput\nPrompt\na handsome man + painting of Van Gogh \ninput\ncartoon \nstyle\nChinese \nink \npainting\nstyle\nGhibli \ncartoon \nstyle\na beautiful \nwoman +\nFigure 19. Applications: video stylization. Thanks to the com-\npatible design, our method can use off-the-shelf pre-trained image\nmodels customized for different styles to accurately stylize videos.\n(a) frame #1\nframe #41\nframe #181\n(b) Results w/ pixel-aware fusion \nframe #41\nframe #181\n(c) Results w/o pixel-aware fusion \nFigure 20. Limitation: failure optical flow due to large motions.\nOur method is not suitable for processing videos where it is diffi-\ncult to estimate the optical flow.\n(a) input frames\n(b) results with T = 0.75Tmax\n(c) results with T = 0.6Tmax\n(d) ghosting\nFigure 21. Limitation: trade-off between content and prompt.\ninput frame\nrerendered frame\nkey frame1\nnew key frame2\npropagate\npropagate\nnon-key frame\n(a)\n(b)\n(d)\n(c)\nFigure 22. Limitation: failed propagation w/o good key frames.\ntemporally-coherent videos and inspire further research in\nthis field.\nAcknowledgments.\nThis study is supported under the\nRIE2020 Industry Alignment Fund Industry Collabora-\ntion Projects (IAF-ICP) Funding Initiative, as well as\ncash and in-kind contribution from the industry part-\nner(s). It is also supported by Singapore MOE AcRF Tier\n2 (MOE-T2EP20221-0011, MOE-T2EP20221-0012) and\nNTU NAP.\nReferences\n[1] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended\nlatent diffusion. arXiv preprint arXiv:2206.02779, 2022. 1,\n5, 6\n[2] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructPix2Pix: Learning to follow image editing instructions.\narXiv preprint arXiv:2211.09800, 2022. 3\n[3] Duygu Ceylan, Chun-Hao Paul Huang, and Niloy J Mi-\ntra. Pix2video: Video editing using image diffusion. arXiv\npreprint arXiv:2303.12688, 2023. 3, 5, 7\n[4] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu,\nand Mubarak Shah. Diffusion models in vision: A survey.\nIEEE Transactions on Pattern Analysis and Machine Intelli-\ngence, 2023. 2\n[5] Soheil Darabi, Eli Shechtman, Connelly Barnes, Dan B\nGoldman, and Pradeep Sen. Image melding: Combining in-\nconsistent images using patch-based synthesis. ACM Trans-\nactions on Graphics, 31(4):82\u20131, 2012. 6\n[6] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng,\nChang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao,\nHongxia Yang, et al. Cogview: Mastering text-to-image gen-\neration via transformers. In Advances in Neural Information\nProcessing Systems, volume 34, pages 19822\u201319835, 2021.\n2\n[7] Patrick Esser, Robin Rombach, and Bjorn Ommer.\nTam-\ning transformers for high-resolution image synthesis.\nIn\nProc. IEEE Int\u2019l Conf. Computer Vision and Pattern Recog-\nnition, pages 12873\u201312883, 2021. 2\n[8] Jakub Fi\u02c7ser, Ond\u02c7rej Jamri\u02c7ska, David Simons, Eli Shechtman,\nJingwan Lu, Paul Asente, Michal Luk\u00b4a\u02c7c, and Daniel S`ykora.\nExample-based synthesis of stylized facial animations. ACM\nTransactions on Graphics (TOG), 36(4):1\u201311, 2017. 3\n[9] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin,\nDevi Parikh, and Yaniv Taigman.\nMake-a-scene: Scene-\nbased text-to-image generation with human priors.\nIn\nProc. European Conf. Computer Vision, pages 89\u2013106.\nSpringer, 2022. 2\n[10] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-\nnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-\nOr.\nAn image is worth one word: Personalizing text-to-\nimage generation using textual inversion.\narXiv preprint\narXiv:2208.01618, 2022. 2\n[11] Eric Heitz and Fabrice Neyret.\nHigh-performance by-\nexample noise using a histogram-preserving blending oper-\nator. Proceedings of the ACM on Computer Graphics and\nInteractive Techniques, 1(2):1\u201325, 2018. 6\n[12] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-\nage editing with cross attention control.\narXiv preprint\narXiv:2208.01626, 2022. 1, 3\n[13] Aaron Hertzmann, Charles E. Jacobs, Nuria Oliver, Brian\nCurless, and David H. Salesin. Image analogies. In Proc.\nConf. Computer Graphics and Interactive Techniques, pages\n327\u2013340, 2001. 3\n11\n[14] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022. 1, 3\n[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. In Advances in Neural Infor-\nmation Processing Systems, volume 33, pages 6840\u20136851,\n2020. 2\n[16] Jonathan Ho, Tim Salimans, Alexey A Gritsenko, William\nChan, Mohammad Norouzi, and David J Fleet. Video diffu-\nsion models. In Advances in Neural Information Processing\nSystems, 2022. 3\n[17] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,\nShean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-\nrank adaptation of large language models.\nIn Proc. Int\u2019l\nConf. Learning Representations, 2021. 2\n[18] Xun Huang and Serge Belongie. Arbitrary style transfer in\nreal-time with adaptive instance normalization. In Proc. Int\u2019l\nConf. Computer Vision, pages 1510\u20131519, 2017. 6\n[19] Ond\u02c7rej Jamri\u02c7ska, \u02c7S\u00b4arka Sochorov\u00b4a, Ond\u02c7rej Texler, Michal\nLuk\u00b4a\u02c7c, Jakub Fi\u02c7ser, Jingwan Lu, Eli Shechtman, and Daniel\nS`ykora. Stylizing video by example. ACM Transactions on\nGraphics, 38(4):1\u201311, 2019. 3, 6\n[20] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nvosyan,\nRoberto\nHenschel,\nZhangyang\nWang,\nShant\nNavasardyan, and Humphrey Shi. Text2video-zero: Text-to-\nimage diffusion models are zero-shot video generators. arXiv\npreprint arXiv:2303.13439, 2023. 2, 3, 5, 7\n[21] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nProc. European Conf. Computer Vision, pages 740\u2013755.\nSpringer, 2014. 8\n[22] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya\nJia. Video-p2p: Video editing with cross-attention control.\narXiv preprint arXiv:2303.04761, 2023. 3\n[23] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-\njun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided\nimage synthesis and editing with stochastic differential equa-\ntions. In Proc. Int\u2019l Conf. Learning Representations, 2021.\n1, 4\n[24] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch,\nand Daniel Cohen-Or. Null-text inversion for editing real\nimages using guided diffusion models.\narXiv preprint\narXiv:2211.09794, 2022. 3\n[25] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav\nAcha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and Yedid\nHoshen. Dreamix: Video diffusion models are general video\neditors. arXiv preprint arXiv:2302.01329, 2023. 3\n[26] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,\nPranav Shyam,\nPamela Mishkin,\nBob Mcgrew,\nIlya\nSutskever, and Mark Chen.\nGlide: Towards photorealis-\ntic image generation and editing with text-guided diffusion\nmodels. In Proc. IEEE Int\u2019l Conf. Machine Learning, pages\n16784\u201316804, 2022. 2\n[27] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,\nXintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fus-\ning attentions for zero-shot text-based video editing. arXiv\npreprint arXiv:2303.09535, 2023. 3, 7\n[28] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In Proc. IEEE Int\u2019l Conf. Machine Learning, pages\n8748\u20138763. PMLR, 2021. 2\n[29] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J Liu. Exploring the limits of transfer learning with\na unified text-to-text transformer. The Journal of Machine\nLearning Research, 21(1):5485\u20135551, 2020. 2\n[30] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022. 1, 2\n[31] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation.\nIn Proc. IEEE Int\u2019l\nConf. Machine Learning, pages 8821\u20138831, 2021. 2\n[32] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proc. IEEE\nInt\u2019l Conf. Computer Vision and Pattern Recognition, pages\n10684\u201310695, 2022. 1, 2\n[33] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. arXiv preprint arXiv:2208.12242, 2022. 1, 2\n[34] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding. In Advances in Neural Information\nProcessing Systems, volume 35, pages 36479\u201336494, 2022.\n1, 2\n[35] Chaehun Shin, Heeseung Kim, Che Hyun Lee, Sang-gil Lee,\nand Sungroh Yoon. Edit-a-video: Single video editing with\nobject-aware consistency. arXiv preprint arXiv:2303.07945,\n2023. 3\n[36] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. In Proc. Int\u2019l Conf. Learning Rep-\nresentations, 2023. 3\n[37] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. In Proc. Int\u2019l Conf. Learning\nRepresentations, 2021. 3\n[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in Neural\nInformation Processing Systems, volume 30, 2017. 2\n[39] Wen Wang, Kangyang Xie, Zide Liu, Hao Chen, Yue Cao,\nXinlong Wang, and Chunhua Shen. Zero-shot video editing\nusing off-the-shelf image diffusion models. arXiv preprint\narXiv:2303.17599, 2023. 3, 7\n[40] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei,\nYuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and\n12\nMike Zheng Shou. Tune-a-video: One-shot tuning of image\ndiffusion models for text-to-video generation. arXiv preprint\narXiv:2212.11565, 2022. 1, 3, 4\n[41] Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, and\nDacheng Tao.\nGmflow: Learning optical flow via global\nmatching. In Proc. IEEE Int\u2019l Conf. Computer Vision and\nPattern Recognition, pages 8121\u20138130, 2022. 6\n[42] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang,\nZhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-\ngrained text to image generation with attentional generative\nadversarial networks. In Proc. IEEE Int\u2019l Conf. Computer\nVision and Pattern Recognition, pages 1316\u20131324, 2018. 2\n[43] Han Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and\nYinfei Yang. Cross-modal contrastive learning for text-to-\nimage generation. In Proc. IEEE Int\u2019l Conf. Computer Vision\nand Pattern Recognition, pages 833\u2013842, 2021. 2\n[44] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiao-\ngang Wang, Xiaolei Huang, and Dimitris N Metaxas. Stack-\nGAN: Text to photo-realistic image synthesis with stacked\ngenerative adversarial networks. In Proceedings of the IEEE\ninternational conference on computer vision, pages 5907\u2013\n5915, 2017. 2\n[45] Lvmin Zhang and Maneesh Agrawala. Adding conditional\ncontrol to text-to-image diffusion models.\narXiv preprint\narXiv:2302.05543, 2023. 3, 4, 6\n[46] Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. DM-\nGAN: Dynamic memory generative adversarial networks for\ntext-to-image synthesis. In Proc. IEEE Int\u2019l Conf. Computer\nVision and Pattern Recognition, pages 5802\u20135810, 2019. 2\n13\n"
  },
  {
    "title": "One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning",
    "link": "https://arxiv.org/pdf/2306.07967.pdf",
    "upvote": "23",
    "text": "ONE-FOR-ALL: GENERALIZED LORA FOR\nPARAMETER-EFFICIENT FINE-TUNING\nArnav Chavan\u22171,2, Zhuang Liu3, Deepak Gupta2, Eric Xing1,4, Zhiqiang Shen\u22171\n1MBZUAI 2Transmute AI Lab 3Meta AI Research 4CMU\n{arnav.chavan,eric.xing,zhiqiang.shen}@mbzuai.ac.ae\nABSTRACT\nWe present Generalized LoRA (GLoRA), an advanced approach for universal\nparameter-efficient fine-tuning tasks. Enhancing Low-Rank Adaptation (LoRA),\nGLoRA employs a generalized prompt module to optimize pre-trained model\nweights and adjust intermediate activations, providing more flexibility and capa-\nbility across diverse tasks and datasets. Moreover, GLoRA facilitates efficient pa-\nrameter adaptation by employing a scalable, modular, layer-wise structure search\nthat learns individual adapter of each layer. Originating from a unified mathe-\nmatical formulation, GLoRA exhibits strong transfer learning, few-shot learning\nand domain generalization abilities, as it adapts to new tasks through not only\nweights but also additional dimensions like activations. Comprehensive exper-\niments demonstrate that GLoRA outperforms all previous methods in natural,\nspecialized, and structured vision benchmarks, achieving superior accuracy with\nfewer parameters and computations. The proposed method on LLaMA-1 and\nLLaMA-2 also show considerable enhancements compared to the original LoRA\nin the language domain. Furthermore, our structural re-parameterization design\nensures that GLoRA incurs no extra inference cost, rendering it a practical solu-\ntion for resource-limited applications. Code and models are available at: GitHub.\n1\nINTRODUCTION\nLarge-scale deep neural networks have revolutionized the field of artificial intelligence, demonstrat-\ning unprecedented performance across various tasks and domains. These highly complex models,\noften with millions or even billions of parameters, have demonstrated remarkable capabilities in\nareas such as computer vision (Dosovitskiy et al., 2021), natural language understanding (Vaswani\net al., 2017), and speech recognition (Radford et al., 2022). Typically, these colossal models are\npre-trained on general and large-scale datasets, such as ImageNet (Deng et al., 2009a) or Web Crawl\nText (Wenzek et al., 2019), and are subsequently adapted to downstream target scenarios through\nfine-tuning or transfer learning. Given the immense computational resources required by large pre-\ntrained architectures, many parameter-efficient fine-tuning (PEFT) methods (Hu et al., 2021; Shen\net al., 2021; Jia et al., 2022; Zhang et al., 2022; Luo et al., 2023) have been proposed. For instance,\nLow-Rank Adaptation (LoRA) (Hu et al., 2021) aims to reduce the number of trainable parame-\nters by exclusively learning pairs of rank-decomposition matrices whilst keeping the original model\nparameter static. Adapter (Houlsby et al., 2019) implements bottleneck adapter modules and in-\ncorporates a modest number of task-specific parameters into a fixed pre-trained model. Similarly,\nVisual Prompt Tuning (VPT) (Jia et al., 2022) introduces a minimal number of learnable parameters\nto the input of the Transformer, leaving the entire backbone frozen during fine-tuning.\nHowever, distinct downstream datasets often possess unique characteristics, such as natural, spe-\ncialized, and structured data, which differ significantly in distribution and composition. A static\nfine-tuning strategy may not sufficiently account for these disparities, thereby hindering its capac-\nity to adapt to diverse datasets. To rectify this, we propose a flexible, parameter-efficient fine-\ntuning scheme in this work to manage the variations of multiple downstream datasets within a\nconsolidated formulation. Our approach presents a generalized version of LoRA from a unified\n\u2217Equal contribution. Project page: https://sites.google.com/view/generalized-lora.\n1\narXiv:2306.07967v2  [cs.LG]  16 Oct 2023\nparameter-efficient fine-tuning perspective, amplifying LoRA\u2019s capability, scalability, and adaptabil-\nity by rescaling and shifting intermediate activations, in conjunction with implementing a structural\nre-parameterization design, etc. It is challenging to devise a unified approach that integrates all\nadjustable dimensions and possibilities when tuning a pre-trained network, especially in the case\nof transformer architectures which contains various distinct modules, while our proposed approach\npresents a practicable solution to navigate this complexity.\nSpecifically, our approach presents a unified framework that can achieve comprehensive fine-tuning\nparadigms from a single formulation, i.e., a One-for-All fine-tuning architecture. It comprises a\nsupernet, which, when optimized cost-effectively through evolutionary search, yields results that\nsurpass those of prevailing fine-tuning methodologies necessitating expensive data-dependent hy-\nperparameter search. The proposed approach exhibits the following advantages: (1) It concurrently\ntakes into account multiple dimensions to enhance capability and flexibility during fine-tuning, en-\ncompassing weights, features, and input tokens. (2) It conducts an implicit search devoid of any\nmanual hyperparameter tuning, thus justifying the increased training time. (3) It incurs no addi-\ntional inference cost thanks to our structural re-parameterization architecture, whereby the extra\nfine-tuning parameters will be fused to the proximate projection weights post-training.\nWe conduct comprehensive experiments on VTAB-1K (Zhai et al., 2020), ImageNet (Deng et al.,\n2009a) and its variants (Recht et al., 2019; Wang et al., 2019; Hendrycks et al., 2021b;a), and Hug-\ngingface leaderboard benchmarks (Edward Beeching, 2023) for evaluating on language domain.\nThe VTAB-1K dataset comprises 19 heterogeneous vision datasets, enveloping a broad spectrum\nof visual domains that include natural objects and scenes, textures and shapes, satellite imagery,\namong others. GLoRA surpasses all previous state-of-the-art PEFT methods by a substantial mar-\ngin in terms of average accuracy. Additionally, we evaluate the model\u2019s few-shot learning capacity\non five fine-grained visual recognition datasets, akin to prior works (Zhang et al., 2022; Jia et al.,\n2022), along with its ability for domain generalization and robustness on ImageNet-V2 (Recht et al.,\n2019), ImageNet-Sketch (Wang et al., 2019), ImageNet-A (Hendrycks et al., 2021b), and ImageNet-\nR (Hendrycks et al., 2021a) datasets. GLoRA significantly outperforms previous methods across all\nthese benchmarks, without incurring any extra computational overhead during the inference phase.\nOur contributions:\n\u2022 We propose Generalized LoRA (GLoRA), a novel parameter-efficient fine-tuning frame-\nwork. GLoRA enhances the low-rank adaptation approach with a more generalized prompt\nmodule design per layer, offering enhanced capability and flexibility in finetuning.\n\u2022 GLoRA presents a unified framework that achieves universal fine-tuning paradigms from\na single formulation, i.e., a One-for-All 1 fine-tuning architecture. During inference, the\nadapters yielded through GLoRA seamlessly integrate into the base network, resulting in\nno additional model weights. Thus, it incurs no extra inference computational load.\n\u2022 We conduct extensive experiments on large vision (ViT-B) and language models (LLaMA-\n1 and 2) with downstream fine-tuning, few-shot learning, and domain generalization using\nvarious datasets. Our experimental results demonstrate that GLoRA outperforms all pre-\nvious methods on these benchmarks while requiring only a small number of extra tunable\nparameters in training and no additional inference cost.\n2\nGLORA\nIn this section, we start from providing a mathematical overview of existing state-of-the-art PEFT\nmethods and discuss the advantages and disadvantages for them. Then, we introduce a unified\nformulation of integrating all existing state-of-the-art PEFT methods and elaborate our proposed\ngeneralized LoRA in detail following this unified formulation perspective. After that, a structural\nre-parameterization design is presented to show the inference efficiency without additional cost. An\nevolutionary search for optimal layer-wise configurations is also introduced to achieve the goal of\ngeneralized LoRA. We further give the theoretical analysis and discussion on the higher capability\nof the proposed method.\n1One-for-All represents that one formulation can be transformed into various shapes of PEFT paradigms.\n2\n2.1\nPREVIOUS SOLUTIONS WITH LIMITATIONS\nVisual Prompt Tuning (Jia et al., 2022): VPT introduces a small amount of task-specific learnable\nparameters into the input space while freezing the entire pre-trained Transformer backbone during\ndownstream fine-tuning. It proposes two strategies: VPT-Shallow, where only input space has the\ntrainable prompt:\n[x1, Z1, E1] = L1 ([x0, P, E0])\n[xi, Zi, Ei] = Li ([xi\u22121, Zi\u22121, Ei\u22121])\n(1)\nwhere P is a trainable prompt. x is the [CLS] token, E are the image patches. Prompts use <1%\ntrainable parameters as compared to the original model.\nVPT-Deep, where every layer has the trainable prompt. The formulation is:\n[xi, . . . , Ei] = Li ([xi\u22121, Pi\u22121, Ei\u22121])\n(2)\nVTP-Deep outperforms full fine-tuning on many vision tasks and also has better accuracy in a low\ndata regime. However, VPT increases cost in the inference stage which is not negligible.\nAdaptFormer (Chen et al., 2022): AdaptFormer introduces a parallel learnable branch of two linear\nlayers and ReLU over the MLP block, and updates only this path while freezing other parts.\n\u02dcx\u2113 = ReLU (LN (x\u2032\n\u2113) \u00b7 Wdown) \u00b7 Wup\n(3)\nx\u2113 = MLP (LN (x\u2032\n\u2113)) + s \u00b7 \u02dcx\u2113 + x\u2032\n\u2113\n(4)\nwhere x\u2032\n\u2113 are the tokens after MHSA at the \u2113-th layer. Wdown and Wup are weights corresponding\nto a down-projection layer and an up-projection layer from the parallel branch, respectively. s is a\nscale factor. AdaptFormer also increases the inference cost due to the presence of a parallel branch.\nLoRA (Hu et al., 2021): LoRA proposes to freeze the pre-trained model weights and injects trainable\nlow-rank decomposition matrices into each layer. It learns only the residual from pre-trained weight.\nAssuming W0, b0, x are pre-trained weights, bias and input, let f be a linear layer, thus f(x) =\nW0x + b0. During fine-tuning, W0 and b0 are frozen, the learning process will be:\nf(x) = W0x + \u2206Wx + b0 = WLoRAx + b0\n(5)\nwhere \u2206W is the low-rank decomposition weights that are learnable.\nScaling & Shifting Features (SSF) (Lian et al., 2022): SSF module scales and shifts features after\nevery MLP, MHSA, Layernorm module during training, and performs re-parameterization during\ninference as it is a linear structure.\ny = \u03b3 \u2299 x + \u03b2\n(6)\nwhere y is the output features. \u03b3 and \u03b2 are the scale and shift factors, \u2299 is the dot product. This\nmethod has no increase in inference but the capability is limited to feature adaptation.\nFacT (Jie & Deng, 2022): FacT proposes to use a tensorization-decomposition method to store the\nadditional weight, the weights of the model are tensorized into a single 3D tensor, and their additions\nare then decomposed into lightweight factors. In fine-tuning, only the factors will be updated and\nstored.\nf(x) = W0x + b0 + U\u03a3Vx = (W0 + U\u03a3V) x + b0\n(7)\nwhere \u2206W in LoRA is decomposed into U, V and \u03a3. This is Tensor-Train in FacT.\nf(x) = W0x + b0 + UCPVx = (W0 + UCPV) x + b0\n(8)\nwhere \u2206W in LoRA is decomposed into U, C, P and V. This is Tucker in FacT.\nRepAdapter (Luo et al., 2023): RepAdapter inserts lightweight networks into the pre-trained mod-\nels, and the additional parameters will be re-parameterized to the nearby projection weights after\ntraining. Adding sequential (not parallel) adapter to both MHSA and MLP, adapter is linear thus\nallowing for re-parameterization. It contains two layers: downsampling dense FC layer to down-\nsample inputs; upsampling downsampled features that are divided into groups, and each group has\nan upsampling layer. The group of upsampling layers can be merged into a single sparse upsampling\nlayer and can be re-parameterized directly into the original MLP/MHSA. The formulation can be:\nf(x)\n= W0 (x + Wu (Wdx + bd) + bu) + b0\n= (W0 + W0WuWd) x + W0Wubd + W0bu + b0\n(9)\n3\nwhere Wu, Wd, bu and bb are learnable weights and biases, respectively.\nLimitations: In general, many existing PEFT methods such as (VPT, Adapter) increase the infer-\nence time since the proposed structure cannot be re-parameterized. Direct prompt tuning is also\nhard to design as it brings in computational burden and requires hyper-parameter tuning i.e., how\nand where to place prompts. LoRA can be re-parameterized at inference but it does not scale up for\nlarger matrices and the adaptation ability is constrained on weight space. SSF / Repadaptor cannot\nlearn the wieght change i.e., \u2206W in weight space, whereas LoRA / FacT cannot efficiently learn\nthe scaling and shifting of feature change i.e., \u2206H in features space. Both feature and weight spaces\nneed flexibility while performing transfer learning from a large model. Our proposed idea in this\nwork attempts at: \u2206W tuning, \u2206H tuning, along with W and H scale and shift learning.\n2.2\nA UNIFIED FORMULATION OF ONE-FOR-ALL\nC\nA\nB\nD\nE\nW0\nf(x)\n1\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n+\n\u00d7\n+\n+\n+\n\u00d7\n+\nInput\nx\nOutput\nb0\nFigure 1: Schematic representation of a linear\nlayer adapted with GLoRA.\nFor model fine-tuning, we propose a unified\nformulation that encompasses tuning in both\nweight and feature space along with VPT-Deep\nlevel prompt design. Additionally, we adopt a\nre-parameterization strategy to incorporate aux-\niliary parameters into the adjacent projection\nweights during the inference stage.\nBroadly\nspeaking, our method serves as a superset of\nall prior solutions, i.e., one-for-all mechanism.\nBy setting different support tensors to zero, our\nGLoRA can be degraded to any of these prede-\ncessor methods. Unlike NOAH (Zhang et al.,\n2022), our architecture can be succinctly artic-\nulated as a unified mathematical equation. The\nconsolidated formulation to represent all tunable spaces can be represented as follows:\nf(x) = (W0 + W0A + B) x + CW0 + Db0 + E + b0\n(10)\nwhere A, B, C, D, E are the trainable support tensors for downstream tasks in our GLoRA, W0\nand b0 are frozen during whole fine-tuning. A is utilized to scale the weight. B has the role to scale\nthe input and shift the weight. C is the layer-wise prompt serving a similar function of VPT-Deep, D\nand E are used to scale and shift the bias, respectively. A detailed illustration is shown in Figure 1.\nModule Design. In this subsection, we delineate the methodology for designing layer-wise adap-\ntors or prompt modules for A, B, C, D, E. In a broad sense, these can take the form of scalars,\nvectors, low-rank decompositions, or none. Based on the role of these trainable sup-\nport tensors, they can be sampled from the following respective search spaces:\nA = {LoRA, vector, scalar, none}\nB = {LoRA, vector, scalar, none}\nC = {LoRA, vector, none}\nD = {vector, scalar, none}\nE = {vector, scalar, none}\n(11)\nwhere none indicates zero, if all the trainable support tensors are zero, the model will be degraded\nto the original formulation and training recipe. In particular, suppose W0 \u2208 Rd2\u00d7d1 is the original\nweight matrix, with input and output channels denoted by d1 and d2 respectively. For every layer,\nwe define Ad \u2208 Rd2\u00d7r, Au \u2208 Rr\u00d7d1, Bd \u2208 Rd2\u00d7r, Bu \u2208 Rr\u00d7d1, Cd \u2208 Rd2\u00d7r, Cu \u2208 Rr\u00d71,\nD \u2208 Rd2\u00d71 and E \u2208 Rd2\u00d71. We also define a multi-path supernet of all possible subnets and\nrandomly sample a subnet during any given supernet training iteration for optimization. A subnet\ncomprises of a single path network with different layerwise support tensors sampled from Eq. 11.\nDepending upon the current subnet configuration, in case of LoRA with rank r1 < r, Ar1\nd \u2208 Rd2\u00d7r1,\nAr1\nu \u2208 Rr1\u00d7d1 is indexed from Ad and Au respectively; and A = Ar1\nd \u00d7 Ar1\nu is used as the final\ntensor, in case of vector A \u2208 Rd2\u00d71 is indexed from Ad and in case of scalar A \u2208 R1\u00d71 is indexed\nfrom Ad. A similar strategy is followed for all other support tensors depending upon the current\nsampled configuration in the subnet. This weight entanglement strategy helps to increase the search\nspace without increasing the number of parameters substantially and also shows faster convergence\ndue to weight sharing in different subnets.\n4\n2.3\nSTRUCTURAL RE-PARAMETERIZATION DESIGN AND INFERENCE EFFICIENCY ANALYSIS\nThe fundamental factor enabling model re-parameterization (Ding et al., 2021; Hu et al., 2021) is\nthe elimination of non-linearity amidst adjacent transformations, thereby permitting the absorption\nof supplementary parameters into the preceding ones. As mentioned in RepAdapter (Luo et al.,\n2023), the removal of such non-linear layers does not detrimentally impact the performance of the\nnetworks. The precise concept of GLoRA re-parameterization is explicated as follows:\nf(x) = Wunix + buni\n(12)\nwhere Wuni and buni are our final unified trained weight and bias in GLoRA. They are re-\nparameterized according to Eq. 10:\nWuni = W0 + W0A + B\n(13)\nbuni = CW0 + Db0 + E + b0\n(14)\nAs a result, the re-parameterization strategy we employ, which integrates learnable parameters into\nthe existing weight matrix offers a distinct advantage as it imposes no additional computational\nburden during the inference phase. This is further discussed in Section 4 where we provide thorough\ninference efficiency analysis of GLoRA compared to exisitng works.\n2.4\nEVOLUTIONARY SEARCH FOR OPTIMAL LAYER-WISE CONFIGURATIONS\nOur design for a unified adaptor is implemented on a per-layer basis, thus allowing for heterogeneity\nacross different layers. To identify the optimal configuration for each layer, we employ the evolu-\ntionary search method (Zhang et al., 2022; Shen et al., 2021), which offers a balance of efficiency\nand effectiveness. Although the training time may increase due to this search process, it is important\nto note that existing work (Zhang et al., 2022) necessitate an extensive hyperparameter search (such\nas low-rank in LoRA and FacT, as well as position and size of adapter modules in Adapter (Houlsby\net al., 2019), dimension and structure configuration in RepAdapter (Luo et al., 2023), among others),\nas presented in Appendix. Our unified support tensor design conducts an implicit search that elim-\ninates the need for manual hyperparameter tuning. Therefore, any augmentation in training time is\nreasonable and well-justified. More details regarding evolutionary search are in Appendix.\n2.5\nGLORA WITH HIGHER CAPACITY\nModel capacity refers to the capability of a model to approximate a diverse range of functions. A\nmethod for regulating the capacity of a learning algorithm involves selecting an appropriate hy-\npothesis space, essentially a set of functions that the learning algorithm is permitted to consider\nas potential solutions. The Vapnik-Chervonenkis Dimension (VC Dimension) (Vapnik & Chervo-\nnenkis, 2015), a measure of the capacity and complexity of a statistical algorithm, can be leveraged\nto provide a formal evidence for this assertion.\nTheorem 1 Suppose dvc(H) is the VC dimension of any finite hypothesis H. If Hi \u2286 Huni,\ndvc(Huni) \u2212 dvc(Hi) \u2265 \u03f5\ns.t. \u03f5 \u2265 0\nIn the context of GLoRA, Hi denotes the hypothesis space of a randomly sampled subnet and Huni\ndenotes the hypothesis space of the complete supernet. The validity of this theorem stems from the\ninherent property of our problem context, where the hypothesis space Hi is a subset of Huni in our\ncontext. Huni encompasses all possible shattered scenarios of Hi. For the extreme case where the\nVC dimension dvc(Ho) (Ho is the difference set of Huni and Hi) is 0, the error \u03f5 will be zero. As\nper learning theory, a higher VC dimension implies greater model flexibility and capability of our\napproach. Clearly, Theorem 1 holds for GLoRA and thus it experiences a greater model capacity.\n3\nEXPERIMENTS\nDatasets. We thoroughly evaluate GLoRA on VTAB-1K (Zhai et al., 2020) benchmark for various\nparameter budgets. VTAB-1K comprises 19 image classification tasks clustered into three domains:\n(i) Natural images; (ii) Specialized tasks consisting of remote sensing and medical datasets; and\n5\n(iii) Structured tasks focusing on scene structure understanding. To examine the ability on few-shot\nlearning, we evaluate GLoRA on five fine-grained visual recognition few-shot datasets: Food101\n(Bossard et al., 2014), OxfordFlowers102 (Nilsback & Zisserman, 2006), StandfordCars (Krause\net al., 2013), OxfordPets (Parkhi et al., 2012), and FGVCAircraft (Maji et al., 2013). Following\nprevious work (Jie & Deng, 2022), we evaluate 1, 2, 4, 8, and 16-shot settings. Next, to show\nthe domain generalization capabilities of GLoRA, we train it on ImageNet (Deng et al., 2009b)\nfor a 16-shot setting and test on four out-of-domain datasets including ImageNetV2 (Recht et al.,\n2019), ImageNet-Sketch (Wang et al., 2019), ImageNet-A (Hendrycks et al., 2021b), and ImageNet-\nR (Hendrycks et al., 2021a). Finally, we show the performance of GLoRA on the Open LLM\nLeaderboard which consists of four datasets with varying prompt shots, namely AI2 Reasoning\nChallenge (25-shot) (Clark et al., 2018), TruthfulQA (0-shot) (Lin et al., 2022), HellaSwag (10-\nshot) (Zellers et al., 2019) and MMLU (5-shot) (Hendrycks et al., 2020).\nNetwork Architecture and Implementation Details. For all the vision experiments, we utilize\nViT-B (Dosovitskiy et al., 2021), a model pre-trained on ImageNet-21K, as our foundational model.\nFor the language experiments, we consider two foundational base models: LLaMA-1-7B (Touvron\net al., 2023a) and LLaMA-2-7B (Touvron et al., 2023b).\nOur supernets undergo a training process spanning 500 epochs and 15 epochs for vision and lan-\nguage datasets respectively, operating with a constant batch size of 64 and a cosine learning rate\nscheduler. It is crucial to highlight that this precise policy demonstrates robust efficacy across all\nsettings, regardless of the dataset in use. Post the training of supernet, we perform an evolutionary\nsearch on the validation set to pinpoint the optimal task-specific subnet, finalized for implemen-\ntation. Finally, we report the performance of the searched subnet on the test set. In Appendix, we\nprovide further insights into dataset-specific learning rates and specific settings for different datasets.\n3.1\nRESULTS ON VTAB-1K\nWe train three different GLoRA supernet configurations to vary the number of trainable parameters.\nThe difference among them is only the LoRA dimensions in the search space which varies from 8\nand 4 in the largest model, 4 and 2 in the intermediate model, and 2 in the smallest model. This\nadded parameter flexibility in our approach allows for user-defined trainable parameter count in the\nfinal models. Results on the VTAB-1K benchmark are shown in Table 1. We push the state-of-the-\nart in parameter-efficient transfer learning by up to 2.9%. Impressively, our smallest model already\nsurpasses all existing approaches by a significant margin. It is worth noting that GLoRA performs\ncompetitively across datasets, in contrast, prior all existing works tend to fail on at least one, proving\nGLoRA\u2019s high generalization capabilities. GLoRA pushes the state of the art in as many as 14 out\nof 19 datasets under VTAB-1K while maintaining commendable performance on the others.\nTable 1: Full results on VTAB-1K benchmark. \u201c# params\u201d specifies the number of trainable pa-\nrameters in backbones. Average accuracy and # params are averaged over group-wise mean values.\nNatural\nSpecialized\nStructured\n# param (M)\nInference Cost\nCifar100\nCaltech101\nDTD\nFlower102\nPets\nSVHN\nSun397\nCamelyon\nEuroSAT\nResisc45\nRetinopathy\nClevr-Count\nClevr-Dist\nDMLab\nKITTI-Dist\ndSpr-Loc\ndSpr-Ori\nsNORB-Azim\nsNORB-Ele\nAverage\nTraditional Finetuning\nFull\n85.8\n-\n68.9 87.7 64.3 97.2 86.9 87.4 38.8 79.7 95.7 84.2 73.9 56.3 58.6 41.7 65.5 57.5 46.7 25.7 29.1 68.9\nLinear\n0\n-\n64.4 85.0 63.2 97.0 86.3 36.6 51.0 78.5 87.5 68.5 74.0 34.3 30.6 33.2 55.4 12.5 20.0 9.6 19.2 57.6\nPEFT methods\nBitFit\n0.10\n-\n72.8 87.0 59.2 97.5 85.3 59.9 51.4 78.7 91.6 72.9 69.8 61.5 55.6 32.4 55.9 66.6 40.0 15.7 25.1 65.2\nVPT-Shallow\n0.06\n\u2191\n77.7 86.9 62.6 97.5 87.3 74.5 51.2 78.2 92.0 75.6 72.9 50.5 58.6 40.5 67.1 68.7 36.1 20.2 34.1 67.8\nVPT-Deep\n0.53\n\u2191\n78.8 90.8 65.8 98.0 88.3 78.1 49.6 81.8 96.1 83.4 68.4 68.5 60.0 46.5 72.8 73.6 47.9 32.9 37.8 72.0\nAdapter\n0.16\n\u2191\n69.2 90.1 68.0 98.8 89.9 82.8 54.3 84.0 94.9 81.9 75.5 80.9 65.3 48.6 78.3 74.8 48.5 29.9 41.6 73.9\nAdaptFormer\n0.16\n\u2191\n70.8 91.2 70.5 99.1 90.9 86.6 54.8 83.0 95.8 84.4 76.3 81.9 64.3 49.3 80.3 76.3 45.7 31.7 41.1 74.7\nLoRA\n0.29\n-\n67.1 91.4 69.4 98.8 90.4 85.3 54.0 84.9 95.3 84.4 73.6 82.9 69.2 49.8 78.5 75.7 47.1 31.0 44.0 74.5\nNOAH\n0.36\n\u2191\n69.6 92.7 70.2 99.1 90.4 86.1 53.7 84.4 95.4 83.9 75.8 82.8 68.9 49.9 81.7 81.8 48.3 32.8 44.2 75.5\nFacT\n0.07\n-\n70.6 90.6 70.8 99.1 90.7 88.6 54.1 84.8 96.2 84.5 75.7 82.6 68.2 49.8 80.7 80.8 47.4 33.2 43.0 75.6\nSSF\n0.24\n-\n69.0 92.6 75.1 99.4 91.8 90.2 52.9 87.4 95.9 87.4 75.5 75.9 62.3 53.3 80.6 77.3 54.9 29.5 37.9 75.7\nRepAdapter\n0.22\n-\n72.4 91.6 71.0 99.2 91.4 90.7 55.1 85.3 95.9 84.6 75.9 82.3 68.0 50.4 79.9 80.4 49.2 38.6 41.0 76.1\nGLoRA\n0.86\n-\n76.4 92.9 74.6 99.6 92.5 91.5 57.8 87.3 96.8 88.0 76.0 83.1 67.3 54.5 86.2 83.8 52.9 37.0 41.4 78.0\nGLoRA\n0.44\n-\n76.5 92.3 75.2 99.6 92.3 91.2 57.5 87.3 96.7 88.1 76.1 80.6 67.2 53.4 84.5 83.5 52.8 35.2 40.8 77.6\nGLoRA\n0.29\n-\n76.1 92.7 75.3 99.6 92.4 90.5 57.2 87.5 96.7 88.1 76.1 81.0 66.2 52.4 84.9 81.8 53.3 33.3 39.8 77.3\n6\n1 2\n4\n8\n16\nNumber of training samples per class\n30\n40\n50\n60\n70\n80\nAccuracy (%)\nAverage\nGLoRA(0.28M)\nNOAH(0.36M)\nVPT(0.64M)\nLoRA(0.29M)\nAdapter(0.16M)\n1 2\n4\n8\n16\nNumber of training samples per class\n10\n20\n30\n40\n50\n60\n70\nAccuracy (%)\nStanfordCars\nGLoRA(0.27M)\nNOAH(0.36M)\nVPT(0.64M)\nLoRA(0.29M)\nAdapter(0.16M)\n1 2\n4\n8\n16\nNumber of training samples per class\n10\n20\n30\n40\n50\nAccuracy (%)\nFGVCAircraft\nGLoRA(0.27M)\nNOAH(0.36M)\nVPT(0.64M)\nLoRA(0.29M)\nAdapter(0.16M)\n1 2\n4\n8\n16\nNumber of training samples per class\n60\n65\n70\n75\n80\n85\n90\nAccuracy (%)\nOxfordPets\nGLoRA(0.29M)\nNOAH(0.36M)\nVPT(0.64M)\nLoRA(0.29M)\nAdapter(0.16M)\n1 2\n4\n8\n16\nNumber of training samples per class\n60\n65\n70\n75\n80\n85\n90\n95\n100\nAccuracy (%)\nFlowers102\nGLoRA(0.27M)\nNOAH(0.36M)\nVPT(0.64M)\nLoRA(0.29M)\nAdapter(0.16M)\n1 2\n4\n8\n16\nNumber of training samples per class\n30\n40\n50\n60\n70\nAccuracy (%)\nFood101\nGLoRA(0.28M)\nNOAH(0.36M)\nVPT(0.64M)\nLoRA(0.29M)\nAdapter(0.16M)\nFigure 2: Results on few-shot learning datasets. The baseline methods include Adapter, LoRA,\nVPT, NOAH. GLoRA consistently performs better across five datasets and a varying number of\ntraining examples per class. More comparisons are provided in Appendix H.\n3.2\nRESULTS ON LARGE LANGUAGE MODELS\nTable 2: Performance of GLoRA on few-shot generative language tasks with LLMs as backbones.\nModel\nDataset\nARC (25-s)\nHellaSwag (10-s)\nMMLU (5-s)\nTruthfulQA (0-s)\nAverage\nLLaMA-1-7B\n-\n51.0\n77.8\n35.7\n34.3\n49.7\nLoRA\nAlpaca\n53.5\n77.3\n33.8\n34.8\n49.8\nGLoRA\nAlpaca\n52.9\n78.1\n34.5\n37.8\n50.8\nLoRA\nShareGPT\n51.7\n77.9\n36.1\n39.2\n51.2\nGLoRA\nShareGPT\n53.2\n77.4\n36.2\n43.9\n52.7\nLLaMA-2-7B\n-\n53.1\n78.5\n46.9\n38.8\n54.3\nGLoRA\nShareGPT\n53.7\n78.5\n46.5\n45.1\n56.1\nWe apply GLoRA for LLMs by solely tuning the attention layers. This contrasts with vision tasks\nwhere all linear layers are adapted, to maintain a fair comparison with vanilla LoRA. We start\nfrom the publicly available LLaMA-1-7B (Touvron et al., 2023a) and LLaMA-2-7B (Touvron et al.,\n2023b) models and finetune them on the Alpaca (Taori et al., 2023) and ShareGPT dataset with only\nGLoRA support tensors trainable. For the evolutionary search, we use 5% random data sampled\nfrom the 4 given datasets for model validation during the evolutions. We finally report the searched\nmodel\u2019s performance on the standard Open LLM Leaderboard2. GLoRA consistently outperforms\nthe pre-trained LLM and the corresponding LoRA fine-tuned variants. We maintain consistent hy-\nperparameters between LoRA and GLoRA for a fair comparison, more details are in the Appendix.\n3.3\nFEW-SHOT LEARNING\nTo extend the evaluation of GLoRA under conditions of limited data availability, we present the per-\nformance of GLoRA on fine-grained visual recognition datasets as the few-show learning, compar-\ning it with LoRA, Adapter, VPT, and NOAH. The results at 1, 2, 4, 8, and 16 shots are illustrated in\nFigure 2 and Figure 6 of Appendix. GLoRA demonstrates superior performance across the majority\nof the few-shot learning datasets, consistently outperforming the performance of existing methods\nby a large margin with similar parameter counts. Interestingly, on the Flowers102 dataset, all meth-\nods yield similar accuracy levels, attributable to the already exceptional overall performance. On\nthe Food101 dataset, the average accuracy of GLoRA is on par with NOAH. From the first plot, we\ncan observe that the average performance boost becomes more pronounced at higher shot scenarios,\nnevertheless, even at lower shot settings, the gains of our approach remain significant.\n3.4\nDOMAIN GENERALIZATION\nThe capacity of out-of-domain generalization holds significant value for large-scale neural networks\n(Zhou et al., 2021). Models fine-tuned via PEFT methods should exhibit enhanced domain gener-\nalization aptitude, thereby making them more applicable in real-world scenarios. We demonstrate\nthe out-of-domain generalization capabilities of GLoRA in Table 3, where a single ImageNet-1K\n2https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n7\nTable 3: Results on domain generalization. GLoRA is significantly better than the existing works.\nSource\nTarget\nImageNet\n-Sketch\n-V2\n-A\n-R\nAdapter Houlsby et al. (2019)\n70.5\n16.4\n59.1\n5.5\n22.1\nVPT Jia et al. (2022)\n70.5\n18.3\n58.0\n4.6\n23.2\nLoRA Hu et al. (2021)\n70.8\n20.0\n59.3\n6.9\n23.3\nNOAH Zhang et al. (2022)\n71.5\n24.8\n66.1\n11.9\n28.5\nGLoRA (0.29M)\n78.3\n30.6\n67.5\n13.3\n31.0\nTable 4: Inference efficiency comparison of GLoRA with existing methods.\nMethod\n\u2191 #Param(M)\n\u2191 FLOPs(G)\nThroughput (imgs/sec)\nbs = 1\nbs = 4\nbs = 16\nFull tuning\n0\n0\n91.5\n375.7\n539.5\nVPT Jia et al. (2022)\n0.55\n5.60\n86.1\n283.5\n381.5\nAdapter Houlsby et al. (2019)\n0.16\n0.03\n70.9\n306.6\n504.7\nAdaptFormer Chen et al. (2022)\n0.16\n0.03\n71.4\n309.9\n508.1\nNOAH Zhang et al. (2022)\n0.12\n0.02\n72.1\n312.7\n492.9\nLoRA Hu et al. (2021)\n0\n0\n91.5\n375.7\n539.6\nGLoRA\n(Deng et al., 2009b) fine-tuned GLoRA model is subjected to testing on out-of-domain datasets.\nAligning with preceding research, we limit the number of training examples per class to 16 for\nthis experiment. It is noteworthy that the performance for the fully-scaled ImageNet-1K fine-tuned\nmodel stands at 83.97% (Dosovitskiy et al., 2021), and our approach manages to narrow this perfor-\nmance gap, even within a 16-shot setting (78.3%), thereby exhibiting superior few-shot learning on\nImageNet-level datasets. Furthermore, the out-of-domain performance also witnesses a substantial\nboost in comparison to existing methods. When compared with LoRA, GLoRA enhances out-of-\ndomain performance by as much as 100% (ImageNet-A) and 50% (ImageNet-Sketch).\n4\nANALYSIS AND DISCUSSION\nComputational Cost. We show the final inference throughput of various PEFT methods in Table 4,\ncomputed on an NVIDIA 3090 GPU. The results highlight that GLoRA surpasses other competitive\nmethods in performance, as it does not require any extra parameters or FLOPs during the inference\nstage. An additional advantage is its quicker adaptability in real-world scenarios, especially when\nprior or foundational models are already deployed. The weights of GLoRA can be directly loaded\nwithout necessitating any manual system modifications. As previously mentioned, GLoRA supports\nVPT-Deep level prompts via the support tensor C, however, it does not impose any computational\noverhead due to its completely structural re-parameterization design.\nQ-K-V\nProjection\nFC1\nFC2\nLayer Type\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nNumber of Parameters(M)\nNatural\nSpecialized\nStructured\nFigure 3: Distribution of GLoRA (0.86M) param-\neters across layer types on VTAB-1K. Q-K-V and\nProjection are linear layers in MHSA module and\nFC1 and FC2 are linear layers in MLP module.\nVisualizations of searched fine-tuning strat-\negy for each layer. Figure 3 visually shows the\ndistribution of trainable parameters across the\nfour types of linear layers embodied in ViT-B.\nNotably, the projection layer possesses the min-\nimum quantity of trainable parameters span-\nning across VTAB-1K categories. Generally,\nthe MLP module hosts a substantially higher\nnumber of parameters compared to the MHSA.\nAs anticipated, the structured group necessi-\ntates a greater number of parameters for adap-\ntation due to a pronounced domain shift rela-\ntive to ImageNet-1K (Deng et al., 2009b). Fig-\nure 4 illustrates the layer-wise configuration of\nthe support tensors as searched by the GLoRA\nalgorithm. Each support tensor at every layer\ncan potentially undergo 72 distinct adaptations\nacross datasets. Support tensors D and E exhibit relatively low adaptation due to the prevalence of\n8\nA B CD E\nA B CD E\nA B CD E\nA B CD E\nA B CD E\nA B CD E\nA B CD E\nA B CD E\nA B CD E\nA B CD E\nA B CD E\nA B CD E\nLayer Depth (Total=12)\n0\n10\n20\n30\n40\n50\n60\n70\nNumber of Occurences (Total=72)\nLoRA\nvector\nconstant\nFigure 4: Layerwise configuration of support tensors in GLoRA (0.86M) on VTAB-1K dataset.\nnone adaptations, whereas A and B demonstrate a higher number of adaptations, though without\na distinguishable pattern regarding the type of adaptation. It is important to underscore that even\na basic scalar can function effectively as a support tensor, enabling GLoRA to maintain superior\nparameter efficiency despite adapting every linear layer.\n5\nRELATED WORK\nGiven the rapid expansion in model size, numerous methods for parameter-efficient fine-tuning\n(PEFT) have been introduced in the field of NLP to streamline the optimization of large language\nmodels (LLMs) (Liu et al., 2021a; Zhang et al.; Hu et al.; Liu et al., 2021b; Li & Liang, 2021; Lester\net al., 2021; Zaken et al., 2022; Houlsby et al., 2019). The effectiveness of parameter-efficient fine-\ntuning has been proven in a wide range of natural language processing tasks (Fu et al., 2022; He\net al., 2021). In the vision domain, with the advent growth in the size of vision models (Dehghani\net al., 2023; Kolesnikov et al., 2020), methods specifically focused on image modality have also\nbeen put forward (Jie & Deng, 2022; Lian et al., 2022; Chen et al., 2022; Luo et al., 2023; Zhang\net al., 2022; Jia et al., 2022; He et al., 2023). Among these methods, LoRA (Hu et al.) has proven to\ntransfer well across modalities and tasks. This is partly due to the simplistic design strategy of LoRA\nwhich directly works over weight tensors, irrespective of model type or configuration. Additionally,\nunlike Adapters (Houlsby et al., 2019; Chen et al., 2022) and Prompt tuning (Jia et al., 2022), LoRA\ndoes not add any additional inference parameters or latency due to structural re-parameterization\n(SR) design. RepAdapter (Luo et al., 2023) and SSF (Lian et al., 2022) also propose an SR design\nfor PEFT. However, RepAdapter is specific to model architectures and required manual designing\nfor different layer configurations. SSF provides a simple baseline but suffers from low flexibility\nand capability due to adaptation limited in the activation space. FacT (Jie & Deng, 2022) further\ndecomposes LoRA matrices for better parameter efficiency, but we argue that <1M parameter scale\nis fairly efficient for fine-tuning on a single GPU. Thus, due to the advantages of LoRA over other\nrelated works, it is of importance to increase the flexibility, scalability and adaptability of LoRA.\n6\nCONCLUSION\nWe have presented GLoRA, a generalized parameter-efficient fine-tuning approach that has suc-\ncessfully demonstrated its effectiveness and adaptability in enhancing the fine-tuning and transfer\nlearning ability for the large-scale pre-trained models. By adopting a generalized low-rank adap-\ntation and re-parameterization framework, GLoRA significantly reduces the number of parameters\nand computation required for fine-tuning, making it a more resource-efficient and practical method\nfor real-world applications. The experiments conducted on a diverse range of tasks and datasets\nhave substantiated the superiority of GLoRA over existing PEFT techniques, showcasing its scala-\nbility and adaptability. Moreover, the ablation studies have provided valuable insights into the inner\nworkings and the relative importance of different GLoRA components. This work not only con-\ntributes to the improvement of the fine-tuning process for large-scale pre-trained vision or language\nmodels but also opens up new avenues for future work, including further exploration of generalized\nlow-rank adaptation techniques, the development of hybrid approaches, and the refinement of search\nand optimization algorithms. These areas of research may continue to expand the accessibility and\nefficiency of transfer learning across a broader range of applications.\n9\nREFERENCES\nLukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 \u2013 mining discriminative com-\nponents with random forests. In European Conference on Computer Vision, 2014. 6\nMinghao Chen, Houwen Peng, Jianlong Fu, and Haibin Ling. Autoformer: Searching transformers\nfor visual recognition. In Proceedings of the IEEE/CVF International Conference on Computer\nVision (ICCV), pp. 12270\u201312280, October 2021. 15\nShoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo.\nAdaptformer: Adapting vision transformers for scalable visual recognition.\narXiv preprint\narXiv:2205.13535, 2022. 3, 8, 9, 14\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.\narXiv preprint arXiv:1803.05457, 2018. 6\nMostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer,\nAndreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision\ntransformers to 22 billion parameters. arXiv preprint arXiv:2302.05442, 2023. 9, 14\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hier-\narchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition,\n2009a. 1, 2\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pp. 248\u2013255. Ieee, 2009b. 6, 8\nXiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han, Guiguang Ding, and Jian Sun. Repvgg:\nMaking vgg-style convnets great again. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pp. 13733\u201313742, 2021. 5\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An im-\nage is worth 16x16 words: Transformers for image recognition at scale. In International Confer-\nence on Learning Representations, 2021. 1, 6, 8\nNathan Habib Sheon Han Nathan Lambert Nazneen Rajani Omar Sanseviero Lewis Tunstall\nThomas Wolf Edward Beeching, Cl\u00b4ementine Fourrier.\nOpen llm leaderboard.\nhttps://\nhuggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard, 2023. 2\nZihao Fu, Haoran Yang, Anthony Man-Cho So, Wai Lam, Lidong Bing, and Nigel Collier. On the\neffectiveness of parameter-efficient fine-tuning. arXiv preprint arXiv:2211.15583, 2022. 9\nHaoyu He, Jianfei Cai, Jing Zhang, Dacheng Tao, and Bohan Zhuang.\nSensitivity-aware vi-\nsual parameter-efficient tuning, 2023.\nURL https://openreview.net/forum?id=\n9GOjmbRQ2o. 9\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a\nunified view of parameter-efficient transfer learning. arXiv preprint arXiv:2110.04366, 2021. 9\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. Measuring massive multitask language understanding. In International Conference\non Learning Representations, 2020. 6\nDan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul\nDesai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical\nanalysis of out-of-distribution generalization.\nIn Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV), pp. 8340\u20138349, 2021a. 2, 6\nDan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adver-\nsarial examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pp. 15262\u201315271, 2021b. 2, 6\n10\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, An-\ndrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp.\nIn International Conference on Machine Learning, pp. 2790\u20132799. PMLR, 2019. 1, 5, 8, 9\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,\net al. Lora: Low-rank adaptation of large language models. In International Conference on\nLearning Representations. 9, 16\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen.\nLora: Low-rank adaptation of large language models.\narXiv preprint\narXiv:2106.09685, 2021. 1, 3, 5, 8\nMenglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and\nSer-Nam Lim. Visual prompt tuning. In Computer Vision\u2013ECCV 2022: 17th European Confer-\nence, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXXIII, pp. 709\u2013727. Springer,\n2022. 1, 2, 3, 8, 9\nShibo Jie and Zhi-Hong Deng. Fact: Factor-tuning for lightweight adaptation on vision transformer.\narXiv preprint arXiv:2212.03145, 2022. 3, 6, 9, 14, 16\nAlexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,\nand Neil Houlsby. Big transfer (bit): General visual representation learning. In Computer Vision\u2013\nECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part\nV 16, pp. 491\u2013507. Springer, 2020. 9\nJonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained\ncategorization. In Proceedings of the IEEE international conference on computer vision work-\nshops, pp. 554\u2013561, 2013. 6\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Pro-\ncessing, pp. 3045\u20133059, 2021. 9\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the\n11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),\npp. 4582\u20134597, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/\nv1/2021.acl-long.353. URL https://aclanthology.org/2021.acl-long.353. 9\nDongze Lian, Daquan Zhou, Jiashi Feng, and Xinchao Wang. Scaling & shifting your features: A\nnew baseline for efficient model tuning. arXiv preprint arXiv:2210.08823, 2022. 3, 9\nStephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human\nfalsehoods. pp. 3214\u20133252, 01 2022. doi: 10.18653/v1/2022.acl-long.229. 6\nXiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, and Jie Tang.\nP-tuning v2:\nPrompt tuning can be comparable to fine-tuning universally across scales and tasks.\nCoRR,\nabs/2110.07602, 2021a. URL https://arxiv.org/abs/2110.07602. 9\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt\nunderstands, too, 2021b. 9\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-\nence on Learning Representations. 14\nGen Luo, Minglang Huang, Yiyi Zhou, Xiaoshuai Sun, Guannan Jiang, Zhiyu Wang, and Ron-\ngrong Ji. Towards efficient visual adaption via structural re-parameterization. arXiv preprint\narXiv:2302.08106, 2023. 1, 3, 5, 9, 14\nSubhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained\nvisual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013. 6\n11\nM-E Nilsback and Andrew Zisserman. A visual vocabulary for flower classification. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), volume 2,\npp. 1447\u20131454. IEEE, 2006. 6\nOmkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.\n3498\u20133505. IEEE, 2012. 6\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever.\nRobust speech recognition via large-scale weak supervision. arXiv preprint arXiv:2212.04356,\n2022. 1\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers\ngeneralize to imagenet? In International Conference on Machine Learning (ICML), pp. 5389\u2013\n5400. PMLR, 2019. 2, 6\nZhiqiang Shen, Zechun Liu, Jie Qin, Marios Savvides, and Kwang-Ting Cheng. Partial is better than\nall: revisiting fine-tuning strategy for few-shot learning. In Proceedings of the AAAI Conference\non Artificial Intelligence, volume 35, pp. 9594\u20139602, 2021. 1, 5\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.\nhttps://github.com/tatsu-lab/stanford_alpaca, 2023. 7\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. 6, 7\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher,\nCristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy\nFu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,\nSaghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel\nKloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,\nDiana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,\nIgor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,\nAlan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh\nTang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen\nZhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,\nSergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models,\n2023b. 6, 7\nLaurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine\nlearning research, 9(11), 2008. 16\nVladimir N Vapnik and A Ya Chervonenkis. On the uniform convergence of relative frequencies\nof events to their probabilities. Measures of complexity: festschrift for alexey chervonenkis, pp.\n11\u201330, 2015. 5\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-\ntion processing systems, 30, 2017. 1\nHaohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representa-\ntions by penalizing local predictive power. Advances in Neural Information Processing Systems\n(NeuIPS), 32, 2019. 2, 6\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm\u00b4an,\nArmand Joulin, and Edouard Grave. Ccnet: Extracting high quality monolingual datasets from\nweb crawl data. arXiv preprint arXiv:1911.00359, 2019. 1\nElad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. Bitfit: Simple parameter-efficient fine-tuning\nfor transformer-based masked language-models. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (Volume 2: Short Papers), pp. 1\u20139, 2022. 9\n12\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a ma-\nchine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pp. 4791\u20134800, Florence, Italy, July 2019. Association for Com-\nputational Linguistics. doi: 10.18653/v1/P19-1472. URL https://aclanthology.org/\nP19-1472. 6\nXiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario\nLucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, Lucas Beyer,\nOlivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and\nNeil Houlsby. The visual task adaptation benchmark, 2020. URL https://openreview.\nnet/forum?id=BJena3VtwS. 2, 5\nQingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and\nTuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh Inter-\nnational Conference on Learning Representations. 9\nYuanhan Zhang, Kaiyang Zhou, and Ziwei Liu.\nNeural prompt search.\narXiv preprint\narXiv:2206.04673, 2022. 1, 2, 4, 5, 8, 9, 14\nKaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization in\nvision: A survey. arXiv preprint arXiv:2103.02503, 2021. 7\n13\nAPPENDIX\nA\nHYPERPARAMETERS\nTable 5: Learning rate of dataset-specific supernet training on VTAB-1K datastet.\nDataset\nCifar100\nCaltech101\nDTD\nFlowers102\nPets\nSVHN\nSun397\nCamelyon\nEuroSAT\nResisc45\nRetinopathy\nClevr-Count\nClevr-Dist\nDMLab\nKITTI-Dist\ndSpr-Loc\ndSpr-Ori\nsNORB-Azim\nsNORB-Ele\nLR\n5e\u22124\n5e\u22124\n5e\u22124\n5e\u22124\n5e\u22124\n5e\u22124\n5e\u22124\n5e\u22124\n5e\u22124\n5e\u22124\n1e\u22124\n1e\u22124\n1e\u22124\n5e\u22124\n5e\u22124\n5e\u22124\n5e\u22124\n5e\u22124\n1e\u22124\nOur approach necessitates minimal adjustments to hyperparameters, with optimizer hyperparame-\nters being the sole exception, thanks to the inherent search mechanism. Following prior studies\n(Dehghani et al., 2023; Chen et al., 2022; Zhang et al., 2022), we employ the AdamW optimizer\n(Loshchilov & Hutter) for all our experiments.\nFor the hyperparameter search in vision tasks, we primarily concentrate on the exploration of the\nlearning rate for supernet training, limiting our search scope to two potential alternatives: 1e\u22124\nand 5e\u22124. For a detailed setting of dataset-specific learning rates, please refer to Table 5. All\nother training configurations strictly adhere to the exact training policy delineated in the works of\n(Jie & Deng, 2022; Luo et al., 2023). In the case of few-shot learning datasets and ImageNet, we\nuse learning rates of 5e\u22124 and 1e\u22124 respectively, as the few-shot learning datasets are smaller if\ncompared to 16-shot ImageNet dataset.\nFor language modeling experiments, we use a learning rate of 2e\u22125 with cosine annealing and\nan equivalent batch size of 32 (using gradient accumulation) for both LoRA and GLoRA. Conse-\nquently, LoRA is trained for 3 epochs, and due to the supernet structure of GLoRA, we train it\nfor 15 epochs. This is in line with vision experiments where LoRA is trained for 100 epochs and\nGLoRA supernet for 500 epochs. We justify these extra training epochs due to the fact that LoRA\nrequires hyperparameter tuning (dropout rate, adaptation layer choice, alpha, etc.) while GLoRA,\nbeing a searched-based method, requires no such manual tuning. We provide more details, such as\nmethod-specific hyperparameters, in Appendix D to justify GLoRA\u2019s extra training time.\nB\nEVOLUTIONARY SEARCH\nEvolutionary search consists of reproduction, crossover, and mutation stages. In our scenario, first,\na population of support tensor strategies is embedded in vectors and initialized randomly. Each\nindividual strategy consists of a description of a single subnet. After supernet training, we start to\nevaluate each individual subnet to obtain its accuracy on the validation set. Among these evaluated\nsubnets we select the top K as parents to produce posterity subnets. The next generation subnets are\nmade by mutation and crossover stages. By repeating this process in iterations, we can find the best\nparameter-efficient fine-tuned subnet with the best validation performance.\nWe first randomly sample 50 subnets from the supernet and then perform an evolutionary search for\n20 and 5 epochs on vision and language tasks, respectively. Each step of random pick / crossover /\nmutation produces 50 new subnets. The probability for crossover and mutation is set to 0.2. Note\nthat we did not perform any hyperparameter search over the evolution hyperparameters, and hence\nthe performance might even improve after tuning the evolution hyperparameters.\nC\nHIERARCHICAL TRANSFORMER\nWe show the performance of GLoRA on the Swin-B backbone in Table 6. We follow a dataset-\nspecific learning rate searching similar to ViT-B and also add GLoRA to the reduction linear layer in\nSwin architecture to maintain uniformity and avoid architecture-specific tuning. GLoRA can adapt\nto any layer irrespective of architecture configuration and perform well across tasks and datasets\nwhich can be clearly seen in Table 6, where GLoRA outperforms all existing works by a fair margin.\n14\nTable 6: Performance on VTAB-1K benchmark with Swin-B model pre-trained on ImageNet-21K\nas the backbone.\nMethod\nNatural\nSpecialized\nStructured\nAverage\nFull\n79.2\n86.2\n59.7\n75.0\nLinear\n73.5\n80.8\n33.5\n62.6\nBitFit\n74.2\n80.1\n42.4\n65.6\nVPT\n76.8\n84.5\n53.4\n71.6\nFacT\n82.7\n87.5\n62.0\n77.4\nRepAdapter\n83.1\n86.9\n62.1\n77.4\nGLoRA\n83.7\n88.7\n61.9\n78.1\nTable 7: Manual design choices in existing works.\nMethod\nDesign Choices/Hyperparameters\nVPT\nPrompt Length, Prompt Location, Prompt Depth\nAdaptFormer\nAdapter Location, Scaling Factor, Hidden dimension, Insertion Form\nNOAH\nVPT choices, Adapter choices, LoRA rank\nRepAdapter\nAdapter Location, Number of groups, Hidden dimension, Adapter variants\nFacT\nDecomposition method, Scaling factor, Decomposition Rank\nGLoRA\nLoRA ranks in search space\nD\nTRAINING TIME\nOur GLoRA, being a search-based approach for PEFT, naturally incurs increased training time due\nto the requirements of supernet training and evolutionary search. However, it is critical to underscore\nthat all current methods necessitate a manual search for design choices, as evidenced in Table 7. This\nnecessity significantly inflates the total training time for a specific dataset, due to the broad search\nwithin these design choices. GLoRA streamlines this process through an automated evolutionary\nsearch mechanism, thus leveraging the benefit of an expansive search space.\nE\nSEARCH SPACE\nIn this section, we analyze the computation of the possible number of subnets within our GLoRA-\nadapted supernet. Each layer offers 4, 4, 3, 3, and 3 options for the support tensor A, B, C, D,\nand E, respectively. This results in 432 possible configurations for a single linear layer. In our\nimplementation, we incorporate 48 such layers within ViT-B, yielding a total of 432 \u00d7 48 = 20, 736\nsubnets being explored within GLoRA. This figure can escalate if multiple LoRA ranks coexist\nwithin the same search space. For instance, we allow ranks 8 and 4 in our largest GLoRA models,\nleading to 82, 944 distinct subnets. Furthermore, owing to the phenomenon of weight entanglement\nas per (Chen et al., 2021), comparable performance is maintained across all subnets, even if they are\nnot all explored during the training of the supernet.\nF\nSUPPORT TENSOR\nIn this section, we justify the choices of support tensors in our framework. Consider a linear layer\nthat facilitates the transformation of inputs from a d1 dimensional space to a d2 dimensional space,\nwith a corresponding weight matrix W0 \u2208 Rd2\u00d7d1. Given that A is tasked with scaling W0,\nA could feasibly belong to Rd2\u00d7d1, Rd2\u00d71, or R1\u00d71. These matrix dimensions are respectively\nindicative of LoRA, vector, and scalar operations. It is pertinent to note that in scenarios where\nA \u2208 Rd2\u00d7d1, LoRA is realized via corresponding matrices Ad \u2208 Rd2\u00d7r and Au \u2208 Rr\u00d7d1. A\nparallel scrutiny of other support tensors would result in determining the appropriate support tensor\nchoice, as elaborated in Section 2.2 of the main paper.\n15\nG\nFINE-TUNED EMBEDDING VISUALIZATION\nWe present feature visualizations of the ViT-B model adapted via GLoRA and FacT (Jie & Deng,\n2022) methods applied to the SVHN dataset. We select FacT as opposed to LoRA (Hu et al.), given\nthat FacT constitutes a direct mathematical enhancement over LoRA and presently represents the\nstate-of-the-art. A clear distinction can be discerned whereby GLoRA exhibits superiorly segregated\nclusters in comparison to FacT. Further, the delineations are broader, and the clusters demonstrate\na higher degree of concentration, signaling the heightened discriminative capacity of the GLoRA-\nadapted model features.\nFigure 5: Visualization of features from SVHN dataset by t-SNE (Van der Maaten & Hinton, 2008).\nH\nMORE RESULTS ON FEW-SHOT LEARNING DATASETS\nAs shown in 6, the baseline methods include Adapter, LoRA, VPT, NOAH. GLoRA consistently\nperforms better across five datasets and a varying number of training examples per class.\n1 2\n4\n8\n16\nNumber of training samples per class\n30\n40\n50\n60\n70\n80\nAccuracy (%)\nAverage\nGLoRA(0.28M)\nNOAH(0.36M)\nVPT(0.64M)\nLoRA(0.29M)\nAdapter(0.16M)\n1 2\n4\n8\n16\nNumber of training samples per class\n10\n20\n30\n40\n50\n60\n70\nAccuracy (%)\nStanfordCars\nGLoRA(0.27M)\nNOAH(0.36M)\nVPT(0.64M)\nLoRA(0.29M)\nAdapter(0.16M)\n1 2\n4\n8\n16\nNumber of training samples per class\n10\n20\n30\n40\n50\nAccuracy (%)\nFGVCAircraft\nGLoRA(0.27M)\nNOAH(0.36M)\nVPT(0.64M)\nLoRA(0.29M)\nAdapter(0.16M)\n1 2\n4\n8\n16\nNumber of training samples per class\n60\n65\n70\n75\n80\n85\n90\nAccuracy (%)\nOxfordPets\nGLoRA(0.29M)\nNOAH(0.36M)\nVPT(0.64M)\nLoRA(0.29M)\nAdapter(0.16M)\n1 2\n4\n8\n16\nNumber of training samples per class\n60\n65\n70\n75\n80\n85\n90\n95\n100\nAccuracy (%)\nFlowers102\nGLoRA(0.27M)\nNOAH(0.36M)\nVPT(0.64M)\nLoRA(0.29M)\nAdapter(0.16M)\n1 2\n4\n8\n16\nNumber of training samples per class\n30\n40\n50\n60\n70\nAccuracy (%)\nFood101\nGLoRA(0.28M)\nNOAH(0.36M)\nVPT(0.64M)\nLoRA(0.29M)\nAdapter(0.16M)\nFigure 6: More results on few-shot learning datasets.\n16\n"
  },
  {
    "title": "AniFaceDrawing: Anime Portrait Exploration during Your Sketching",
    "link": "https://arxiv.org/pdf/2306.07476.pdf",
    "upvote": "16",
    "text": "AniFaceDrawing: Anime Portrait Exploration during Your Sketching\nZHENGYU HUANG, Japan Advanced Institute of Science and Technology, Japan\nHAORAN XIE, Japan Advanced Institute of Science and Technology, Japan\nTSUKASA FUKUSATO, Waseda University, Japan\nKAZUNORI MIYATA, Japan Advanced Institute of Science and Technology, Japan\nIn this paper, we focus on how artificial intelligence (AI) can be used to\nassist users in the creation of anime portraits, that is, converting rough\nsketches into anime portraits during their sketching process. The input is a\nsequence of incomplete freehand sketches that are gradually refined stroke\nby stroke, while the output is a sequence of high-quality anime portraits\nthat correspond to the input sketches as guidance. Although recent GANs\ncan generate high quality images, it is a challenging problem to maintain\nthe high quality of generated images from sketches with a low degree of\ncompletion due to ill-posed problems in conditional image generation. Even\nwith the latest sketch-to-image (S2I) technology, it is still difficult to create\nhigh-quality images from incomplete rough sketches for anime portraits\nsince anime style tend to be more abstract than in realistic style. To ad-\ndress this issue, we adopt a latent space exploration of StyleGAN with a\ntwo-stage training strategy. We consider the input strokes of a freehand\nsketch to correspond to edge information-related attributes in the latent\nstructural code of StyleGAN, and term the matching between strokes and\nthese attributes \u201cstroke-level disentanglement.\u201d In the first stage, we trained\nan image encoder with the pre-trained StyleGAN model as a teacher encoder.\nIn the second stage, we simulated the drawing process of the generated\nimages without any additional data (labels) and trained the sketch encoder\nfor incomplete progressive sketches to generate high-quality portrait images\nwith feature alignment to the disentangled representations in the teacher en-\ncoder. We verified the proposed progressive S2I system with both qualitative\nand quantitative evaluations and achieved high-quality anime portraits from\nincomplete progressive sketches. Our user study proved its effectiveness in\nart creation assistance for the anime style.\nCCS Concepts: \u2022 Computing methodologies \u2192 Graphics systems and\ninterfaces; Image manipulation; Machine learning.\nAdditional Key Words and Phrases: Stroke-level Disentanglement, StyleGAN,\nAnime Portrait, Disentanglement Learning, Freehand Sketching\nACM Reference Format:\nZhengyu Huang, Haoran Xie, Tsukasa Fukusato, and Kazunori Miyata. 2023.\nAniFaceDrawing: Anime Portrait Exploration during Your Sketching . 1, 1\n(June 2023), 11 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn\n1\nINTRODUCTION\nUsing AI to assist the general user in creating a professional anime\nportrait is not a trivial task. As a popular drawing style, the anime\nAuthors\u2019 addresses: Zhengyu Huang, Japan Advanced Institute of Science and Technol-\nogy, Nomi, Japan, huang.zhengyu@jaist.ac.jp; Haoran Xie, Japan Advanced Institute of\nScience and Technology, Nomi, Japan, xie@jaist.ac.jp; Tsukasa Fukusato, Waseda Uni-\nversity, Tokyo, Japan, tsukasafukusato@waseda.jp; Kazunori Miyata, Japan Advanced\nInstitute of Science and Technology, Nomi, Japan, miyata@jaist.ac.jp.\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\n\u00a9 2023 Copyright held by the owner/author(s).\nXXXX-XXXX/2023/6-ART\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\nstyle is based on realism but has its own characteristics and exag-\ngerations \u2013 line drawings of anime portraits are simpler and more\nabstract than real human faces. Furthermore, the input sketches\nthat users make during the drawing process contain little detailed\ninformation and lack partial shape information. As a result, the\nhigh-quality synthesis of anime portraits from freehand sketches is\nchallenging. The main problem is how to generate appropriate guid-\nance images that match these abstract lines, based on a sequence of\nincomplete sketches, for the user during freehand sketching.\nWe also realized that classic S2I or style transfer techniques would\nnot work for this task. Consider a scenario in which a novice tries\nto create an anime portrait with an ideal AI-assisted system. As the\nuser draws stroke by stroke, the guidance image generated by this\nsystem should be able to locally match the sketch as the number of\nstrokes increases. However, most S2I approaches tend to consider\nonly complete sketches as input for image generation \u2013 in the case\nof incomplete sketches, especially those with sparser strokes, they\ncannot maintain the quality of the output. Taking sketch-to-anime-\nportrait generation with StyleGAN as an example, in Figure 1(a),\nthe state-of-the-art Pixel2Style2Pixel (pSp) [Richardson et al. 2021]\nis an encoder for GAN (Generative Adversarial Network) inversion\nthat can successfully reconstruct a complete line drawing into an\nanime portrait that can tolerate small missing areas (first row), but\nit gets poor outputs when the input is a sketch with large missing\nareas (second row) or a rough sketch with less details (third row).\nTherefore, conventional GAN inversion techniques perform poorly\nin the drawing process \u2013 they do not spontaneously implement\nstroke-level disentanglement during learning, nor do they naturally\nmaintain partial matches. Similarly, the simple combination of a\nrough sketch-real-face approach with a further style transfer from\nrealistic to anime style may not be a good idea. Figure 1(b) shows\nsuch an example: A sketch-real face pSp encoder is combined with\na face-anime style transfer called DualStyleGAN [2022] for sketch-\nbased anime portrait generation, using real faces as the intermediate\nresults. Because the difference between the anime-style face and\nthe real face is relatively large, results generated by this method\nare not consistent with the input sketches. For the same reason,\nDualStyleGAN itself tends to fall into \u201cmode collapse,\u201d no matter if\nthe input is a sketch of a realistic style or an anime style.\nThus, we propose a new idea for anime-style portrait generation\nduring sketching. Our solution involves sketch-based latent space\nexploration in a pre-trained StyleGAN [Karras et al. 2019]. The ad-\nvent of StyleGAN made it possible to create high-quality images for\nmany types of subjects, including anime portraits. In turn, this great\nsuccess led to the rapid development of the GAN control for image\nediting. By applying linear regression to the disentangled latent\nspace of a StyleGAN, users can control the various properties of the\ngenerated image by modifying the attribute parameters. In our case,\n, Vol. 1, No. 1, Article . Publication date: June 2023.\narXiv:2306.07476v1  [cs.CV]  13 Jun 2023\n2\n\u2022\nHuang, Z. et al\nInput\npSp\nOurs\nInput Intermedia Reference Output\nOurs\n(a) GAN inversion encoder\n(b) A sketch-real face pSp encoder +\nDualStyleGAN (real-anime style transfer)\n(c) Our anime portrait generation during a sketching process\nFig. 1. Comparison of different ideas for sketch-based anime portrait generation. In (a), an original pSp encoder, which works for line drawing with small\nareas missing (first row), cannot correctly recognize user sketches, even for a complete sketch (third row). In (b), sketch-anime portrait style transfer using real\nfaces as intermediate results lead to an input/output mismatch and mode collapse. And lack of diversity in the results of different inputs fails to assist the user\nin drawing anime portraits. Realistic sketches (first three rows) are used to obtain better intermediate faces. In contrast, our method can generate more diverse\nanime portraits, even if the input is a realistic sketch after translation (first two rows) or flip (third row). (c) shows that our method can generate high-quality\nresults that consistently match the input sketch throughout the sketching process. To make the matching of sketches and results of our method clear, the\nintermediate results disentangled most of the color information (second row) are stacked below the input strokes (blue strokes on the first row) once a new\nstroke (red) is added. The final results (third row) were generated using a random style-mixing technique. Note that all generated results with \u201cnear-white\u201d\nhair are intermediate results, which are style mixing with a fixed \u201cnear-white\u201d color latent code.\nwe are trying to implement sketch-based anime portrait generation\ncontrol during the drawing process, because manipulating multiple\nshape-related attributes (e.g., pose, mouth shape, or nose position)\nwith separate sliders is not intuitive enough. Compared with the\nabove-mentioned approaches, our approach allows the generated\nresults to be matched with the users\u2019 rough sketches during their\ndrawing process (see Figure 1(c)). To the best of our knowledge, our\nsystem is the first to provide anime portraits progressive drawing\nassistance. Our main contributions are summarized as follows:\n\u2022 We present AniFaceDrawing, the first high-quality anime por-\ntrait drawing assistance system based on the S2I framework\nfrom freehand sketches throughout the entire drawing process.\n\u2022 We propose an unsupervised stroke-level disentanglement train-\ning strategy for StyleGAN, so that rough sketches with sparse\nstrokes can be automatically matched to the corresponding local\nparts in anime portraits without any semantic labels.\n\u2022 A user study is conducted to prove the effectiveness and usability\nof AniFaceDrawing for users when creating anime portraits.\n2\nRELATED WORK\nLatent space of StyleGAN. With the further development of GAN,\nhow to use the latent space to manipulate outputs from pre-trained\nGANs has also become a hot research topic [Xia et al. 2023]. Among\nvarious pre-trained GANs, StyleGAN [Karras et al. 2021] is usually\nthe most common choice. A typical StyleGAN generator usually\ninvolves three types of latent spaces: Z, W, and W+. A random\nvector \ud835\udc9b \u2208 Z is often a white noise belonging to a Gaussian distri-\nbution, which is the same as the original GAN. In StyleGAN, the \ud835\udc9b\nvector first passes through a mapping network, which is composed\nof eight fully-connected layers and is transformed to \ud835\udc98 embedding\ninto an intermediate latent space W. Note that both \ud835\udc9b and \ud835\udc98 are\n512-dimensional vectors. Here, the introduction of this mapping\nnetwork is to get rid of the influence of the input vector \ud835\udc9b by the\ndistribution of the input data set and to better disentangle the at-\ntributes. Each layer of the StyleGAN generator can receive a vector\n\ud835\udc98 of input via AdaIN (adaptive instance normalization). As there\nare 18 such layers in the StyleGAN generator, StyleGAN can input\nup to 18 mutually different \ud835\udc98 vectors. This different \ud835\udc98 can be con-\ncatenated into a new vector \ud835\udc98+ with 18 \u00d7 512 dimensions and the\ncorresponding latent space to \ud835\udc98+ is called W+. One application of\n\ud835\udc64+ is style mixing, which can also be found in the inference step in\nFigure 4. In addition, we mapped incomplete progressive sketches\ninto the latent space W+ of StyleGAN for guidance generation.\nFacial latent space manipulation. One of the most important\napplications of latent space manipulation is face attribute editing.\nChiu et al. [2020] present a human-in-the-loop differential subspace\nsearch for exploring the high-dimensional latent space of GAN by\nletting the user perform searches in 1D subspaces. H\u00e4rk\u00f6nen et\nal. [2020] identify latent directions with principal components anal-\nysis (PCA), and created interpretable controls for image synthesis,\nsuch as viewpoint changing, lighting, and aging. By determining\nfacial semantic boundaries with a trained linear SVM (support vec-\ntor machine), Shen et al. [2022] is able to control the expression\nand pose of faces. An instance-aware latent-space search (IALS) is\nperformed to find semantic directions for disentangled attribute\nediting [Han et al. 2021]. Instead of the tedious 1D adjustment of\neach face attribute, we directly use progressive rough sketches to\ncontrol the shape attributes of the face and explore latent space.\nGAN-control with encoder-based manipulation. The pSp en-\ncoder implements GAN inversion without optimization by using\nfeature pyramids and mapping networks. Since this method does\nnot need to compute losses between inputs and outputs of a GAN,\nit can also handle semantic layouts or line drawings as input. To im-\nprove the editability of the encoder-based approach, Tov et al. [2021]\nintroduced regularization and adversarial losses for latent codes into\nencoder training. In addition, the ReStyle encoder [Alaluf et al. 2021]\n, Vol. 1, No. 1, Article . Publication date: June 2023.\nAniFaceDrawing: Anime Portrait Exploration during Your Sketching\n\u2022\n3\nhas improved the reconstruction quality of inverted images by itera-\ntively refining latent codes from the encoder. Unlike encoder-based\napproaches, which require many training pairs, our method auto-\nmatically generates sketches directly from the GAN on the fly in\nthe training step without additional pairwise data generation.\nInteractive AI assistance. With the rapid development of deep\nlearning, many efforts have been made to apply AI to interactively\nassist users in various fields, such as music creation [Frid et al.\n2020], handwritten text editing [Aksan et al. 2018], and sketch col-\norization [Ren et al. 2020]. When it comes to sketch-based drawing\nassistance, the dominant idea has remained to adopt retrieval-based\napproaches [Choi et al. 2019; Collomosse et al. 2019] since the Shad-\nowDraw [2011] have been proposed. For example, to improve users\u2019\nfinal sketches, DeepFaceDrawing [2020] and DrawingInStyles [2022]\nadopts a shadow guidance which retrieves sketches from a database\nrather than using the generated images directly.\nSketch-based applications. As a high-level abstract representa-\ntion, sketches can be used as conditional inputs to generative models.\nSketch-based systems allow users to intuitively obtain results in\nvarious applications, such as image retrieval [Lee et al. 2011; Liu\net al. 2017; Yu et al. 2016] and image manipulation [Dekel et al. 2018;\nPortenier et al. 2018; Tseng et al. 2020; Yang et al. 2020], simulation\ncontrol [Hu et al. 2019], block arrangement [Peng et al. 2020], and 3D\nmodeling [Fukusato et al. 2020; Igarashi and Hughes 2001; Igarashi\net al. 1997]. As for applications such as iSketchNFill [2019], which\nconsider the sketching process as input, the generation quality is\nstill limited and cannot be applied to high-quality anime portrait\ngeneration. Although there have also been attempts to generate\nhigh-quality faces with sketches (e.g., DeepFacePencil [2020]), they\ndo not take into account the case of sparse input sketches at the\nbeginning of drawing process. On the other hand, the vast majority\nof S2I studies [Chen et al. 2020; Yang et al. 2021] target the gen-\neration of real images, but how these methods can be applied to\nthe abstract artistic style for art drawing assistance has not been\nexplored, which is the topic in this paper.\n3\nSTROKE-LEVEL DISENTANGLEMENT\nThe use of sketches to control shape properties of an anime face to\nachieve latent space exploration is called stroke-level disentangle-\nment. We first explain its concept with a simple example (Figure 2).\nGiven an image generated by StyleGAN with a fixed color latent\ncode, the left/right eyes (green box) are mapped to \ud835\udc3f and \ud835\udc45 in the\ndisentangled latent space with GAN inversion. Stroke-level disen-\ntanglement means that there is a sketch-GAN inversion coder for\nthe rough sketch that allows Stroke 1 and 2 (red box) to be mapped\nto the subset of the corresponding latent codes \ud835\udc3f and \ud835\udc45, respectively.\nNote that the percentage of the latent code (intersection of the same\nstructural information \u00f7 latent codes for a single facial part) of\nStroke 1 to \ud835\udc3f is higher than that of the latent code of Strokes 2 to\n\ud835\udc45, because Stroke 1 contains more details. In addition, there may\nbe a one-to-many relationship between the strokes and the latent\ncode of different facial parts; for example, if a stroke contains shape\ninformation of both the left and right eyes at the same time, it will\ncorrespond to a subset of both \ud835\udc3f and \ud835\udc45 after encoding.\nStroke 1\nStroke 2\nLatent space\nGAN inversion\nSketch GAN inversion\nImage Generation\nLeft eye latent code (\ud835\udc3f)\nRight eye latent code (\ud835\udc45)\nSubset of \ud835\udc3f\nSubset of \ud835\udc45\nOther latent code\nFig. 2. Illustrating the stroke-level disentanglement.\nWe formally describe the problem as follows. Let \ud835\udc77 and \ud835\udc7a denote\nthe anime portrait domain and the sketch domain, respectively. \ud835\udc78 is a\nsubset of \ud835\udc77 that separates most representations of color information\nfrom structural information and can form a one-to-one mapping\nwith S. Our sketch encoder learns a mapping \ud835\udc6d : \ud835\udc7a \u2192 \ud835\udc78 which can\nfind the correct correspondence with increased drawing strokes.\nThis mapping \ud835\udc6d is called \u201csketch GAN inversion\u201d in this paper.\nThe output during the drawing process should gradually converge\nand maintain high quality as the input strokes increase. Two main\nresearch questions need to be addressed:\n\u2022 Q1. How does one learn a stroke-level disentangled mapping \ud835\udc6d\nthat allows strokes to locally match to the generated image?\n\u2022 Q2. How can the aforementioned mapping not be affected by\nthe stroke order?\nGiven a sketch consisting of a series of strokes {\ud835\udc941, \ud835\udc942, ...\ud835\udc94\ud835\udc8f}, these\ntwo questions require that mapping \ud835\udc6d in ideal cases satisfies the\nfollowing two conditions.\nStroke independence. Assume that an image encoder that converts\nan anime portrait to completely disentangled structural latent codes\n{\ud835\udc851, \ud835\udc852, ...\ud835\udc85\ud835\udc8f} corresponding to strokes one by one, then:\n\ud835\udc6d (\ud835\udc94\ud835\udc8a) = \ud835\udc85\ud835\udc8a\n(1)\nwhere \ud835\udc56 is the index of strokes (\ud835\udc56 \u2264 \ud835\udc5b). Note that each stroke can\ncreate a new partial sketch {\ud835\udc60\ud835\udc56} that contains only one facial part.\nStroke order invariance. For any different stroke index \ud835\udc56, \ud835\udc57 \u2264 \ud835\udc5b:\n\ud835\udc6d (\ud835\udc94\ud835\udc8a|\ud835\udc941, \ud835\udc942, ...\ud835\udc94\ud835\udc8a\u22121, \ud835\udc94\ud835\udc8a+1...\ud835\udc94\ud835\udc8f)\n= \ud835\udc6d (\ud835\udc94\ud835\udc8b|\ud835\udc941, \ud835\udc942, ...\ud835\udc94\ud835\udc8b\u22121, \ud835\udc94\ud835\udc8b+1...\ud835\udc94\ud835\udc8f)\n= \ud835\udc6d (\ud835\udc941, \ud835\udc942, ...\ud835\udc94\ud835\udc8f)\n(2)\nwhere \ud835\udc94\ud835\udc8a|\ud835\udc941, \ud835\udc942, ...\ud835\udc94\ud835\udc8a\u22121, \ud835\udc94\ud835\udc8a+1...\ud835\udc94\ud835\udc8f means add stroke \ud835\udc94\ud835\udc8a to a sketch\nconsisting of strokes {\ud835\udc941, \ud835\udc942, ...\ud835\udc94\ud835\udc8a\u22121, \ud835\udc94\ud835\udc8a+1...\ud835\udc94\ud835\udc8f}. Note that we do not\nuse any semantic label, and the inputs are monochrome sketches.\nFigure 3 shows our core idea of simulating the drawing process\nand making the sketch with a higher degree of completion, closer\nto the original sketch, in the latent space limited to a neighboring\nregion that can provide the answers to the above two questions.\nGiven an image generated by StyleGAN, the point computed by\nGAN inversion in the latent space \ud835\udc43 is \ud835\udc5d, and the point in the image\nfixed color latent code (first row in the gray dashed box) projected\ninto the latent subspace \ud835\udc44 is \ud835\udc5e. Our drawing process simulation\ngenerates a sequence of simulated sketches (second row in the gray\ndashed box) from simple to complex, whose positions in \ud835\udc44 space are\ndenoted as \ud835\udc461 to \ud835\udc46\ud835\udc5b. The core idea is to learn a spatial neighborhood\nin \ud835\udc43 whose projection in subspace \ud835\udc44 can make the sequence of\npoints \ud835\udc461 to \ud835\udc46\ud835\udc5b gradually approximate the point \ud835\udc5e.\n, Vol. 1, No. 1, Article . Publication date: June 2023.\n4\n\u2022\nHuang, Z. et al\n\ud835\udc44: Subset of \ud835\udc43\n\ud835\udc91\nGAN inversion\nSketch GAN inversion\nProjection\nClustering\nSpatial neighborhood\n\ud835\udc92\n\ud835\udc7a1\n\ud835\udc7a2\n\ud835\udc7a\ud835\udc5b\nLatent space \ud835\udc43\n\u22ee\nFig. 3. Illustrating the drawing process in the latent space.\nStructural\nCode\nColor\nCode\nTraining Step\n\ud835\udc67\nStart of both stage\nFeature alignment (L2 part)\nStage I\nStage II\nLoss in Stage I (LI)\nMasked Loss in Stage II (L1 part)\nParameters always fixed\nParameters Fixed in stage II\nParameters for learning\nDecoder\n(StyleGAN)\nImage\nEncoder\nMask M\nPseudo Sketch\nStructural\nCode\nColor\nCode\nStructural\nCode\nColor\nCode\nSketch\nEncoder\nz\nG(z), G(I(z))\nE1(\u2022)\nE2(\u2022)\nDrawi(\u2022)\nE2(\u2022)\n{s1,s2,\u2026sn}\n{s1,s2}={s2|s1}\n{s1}\nSketch\nEncoder\nInference step\nDrawing Process Simulation\nStructural\nCode\nColor\nCode\n\u22ef\n\u22ef\nDecoder\nS(z)\nI(z)\nFig. 4. Framework of the proposed method. The training step consists of\ntwo stages: (Stage I) an image encoder for GAN inversion and (Stage II) a\nsketch encoder for sketch GAN inversion where the image encoder works\nas a teacher.\n4\nPROPOSED FRAMEWORK\nAn overview of the framework is shown in Figure 4. In the train-\ning step, we first trained an image encoder using the randomly-\ngenerated images from the decoder, which correctly projected the\nanime portraits back into the latent space (Stage I). Then, we rear-\nranged the latent space vectors in this image encoder by simulating\nthe drawing process, so that sketches with similar strokes retained\nmore rational distribution when projected into \ud835\udc78 (Stage II). In the\ninference step, we concatenated the structural codes derived from\nthe sketch encoder with the color codes from the random Gaussian\nnoise \ud835\udc67, which is known as style-mixing. Note that once the decoder\nis determined, all data are derived from the randomly-generated im-\nages of that decoder, and no additional auxiliary database is required.\nThus, this is an unsupervised learning approach.\nThe training in Stage I is similar to previous work [Richardson\net al. 2021]. The difference is that we simply adopted the \ud835\udc3f2 loss\nbetween the original images from StyleGAN and the reconstructed\nimages encoded by our image encoder.\nFig. 5. An example of (b, c) the drawing process simulation and (d, e) back-\nground augmentation after (a) a series of pre-processing for a randomly\ngenerated image (original) from StyleGAN. The corresponding choices\nof \ud835\udc45\ud835\udc4e\ud835\udc5b\ud835\udc51\ud835\udc5c\ud835\udc5a\ud835\udc46\ud835\udc52\ud835\udc59\ud835\udc52\ud835\udc50\ud835\udc61\ud835\udc42\ud835\udc5b\ud835\udc52\ud835\udc46\ud835\udc61\ud835\udc5f\ud835\udc5c\ud835\udc58\ud835\udc52 and \ud835\udc45\ud835\udc4e\ud835\udc5b\ud835\udc51\ud835\udc5c\ud835\udc5a\ud835\udc37\ud835\udc5f\ud835\udc4e\ud835\udc64\ud835\udc56\ud835\udc5b\ud835\udc54 functions for each\nstroke are shown in (b). In (c), \ud835\udc37\ud835\udc5f\ud835\udc4e\ud835\udc64\ud835\udc41\ud835\udc52\ud835\udc64\ud835\udc46\ud835\udc61\ud835\udc5f\ud835\udc5c\ud835\udc58\ud835\udc52\ud835\udc60\ud835\udc40\ud835\udc4e\ud835\udc60\ud835\udc58 (\ud835\udc5a\ud835\udc61,\ud835\udc60) executes\n\ud835\udc37\ud835\udc5f\ud835\udc4e\ud835\udc64\ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc5c\ud835\udc62\ud835\udc5f\ud835\udc60 on the previous loss mask \ud835\udc5a\ud835\udc61 for each stroke \ud835\udc60 to get a\nnew loss mask, except for the nose (the nose area is determined by the\nlandmarks in the center of eyes and the corners of mouth). For comparison,\nwe also trained a baseline encoder with a random cropping strategy from\nthe input line drawing in (f).\n4.1\nDrawing Process Simulation\nIn Stage II, the drawing process simulation automatically generates\nsketch-image pairs from StyleGAN. Before the drawing process sim-\nulation, we should first get a complete line drawing of the original\nanime portrait generated by StyleGAN as the simulation input. As\nshown in the first line of Figure 5, we conducted a style-mixing\nbetween the original and the reference image so that most of the\ncolor information could be removed and obtained a complete line\ndrawing from the style-mixing result with xDoG [2011]. We then\nused a landmark detection technique for anime faces [Hysts 2021]\nto acquire information about the contours of each face part. We\nsimulated the intermediate results of the sketch process stroke by\nstroke using Algorithm 1. Facial landmarks are divided into \ud835\udc5b types\n(\ud835\udc5b = 7), and for each new stroke in \ud835\udc58 iterations, a facial part is\nrandomly selected as a part stroke \ud835\udc60 \u2208 \ud835\udc36 with RandomProcess\nand RandomDrawing as Figure 5(b) shown. In the same iteration,\n, Vol. 1, No. 1, Article . Publication date: June 2023.\nAniFaceDrawing: Anime Portrait Exploration during Your Sketching\n\u2022\n5\nthe corresponding cumulative loss mask is drawn by the function\nDrawNewStrokesMask (Figure 5(c)).\nBackground augmentation. Since the hair and other parts could\nnot be extracted by the anime face detection algorithm, we treated\nthem as background. To increase stability, random cropping of the\nbackground image and random selection from facial contours were\ncombined as augmentation data. The effects of this method are\ndiscussed in Section 6.2. At this point, we had a series of pseudo\nsketches for training in Stage II which is called \u201cfeature alignment.\u201d\nALGORITHM 1: Drawing process simulation\nInput: Portrait image \ud835\udc43\nOutput: List of pseudo sketch \ud835\udc46, List of loss mask \ud835\udc40\nLandmarks of portrait \ud835\udc3f \u2190 FaceDectect(\ud835\udc43);\nStrokes of facial parts \ud835\udc36 \u2190 Resort(\ud835\udc3f);\nNumber of \ud835\udc5b \u2190 \ud835\udc59\ud835\udc52\ud835\udc5b(\ud835\udc36);\nTemporary white image \ud835\udc5d\ud835\udc61 \u2190 ones(\ud835\udc43.shape) \u00d7 255;\nTemporary loss mask \ud835\udc5a\ud835\udc61 \u2190 zeros(\ud835\udc43.shape);\nRandomProcess=[GaussianBlur(kernel size= 3 \u00d7 3); Dilate; Erode,\nKeepOriginal(None)];\nRandomDrawing=[DrawOriginal, DrawContours];\n\ud835\udc46 \u2190 \u2205 ;\n\ud835\udc40 \u2190 \u2205 ;\nfor k=1:n do\nIndex \ud835\udc56 = RandomSelectOneStroke(\ud835\udc36);\nPart stroke \ud835\udc60=\ud835\udc36.pop(\ud835\udc56);\n\ud835\udc5d\ud835\udc61 = RandomDrawing(RandomProcess(\ud835\udc5d\ud835\udc61 ,\ud835\udc60)) ;\n\ud835\udc46.push(\ud835\udc5d\ud835\udc61);\n\ud835\udc5a\ud835\udc61= DrawNewStrokesMask(\ud835\udc5a\ud835\udc61, \ud835\udc60) ;\n\ud835\udc40.push(\ud835\udc5a\ud835\udc61);\nend\nreturn \ud835\udc46,\ud835\udc40\n4.2\nFeature Alignment\nGiven Gaussian noise \ud835\udc9b, the input image of our encoder is \ud835\udc99 = \ud835\udc3a(\ud835\udc9b)\nand the output latent code \ud835\udc3c (\ud835\udc9b), a special implementation of a point\n\ud835\udc5d in \ud835\udc43 (Figure 3), is then defined as:\n\ud835\udc3c (\ud835\udc9b) := \ud835\udc381(\ud835\udc3a(\ud835\udc9b))\n(3)\nwhere \ud835\udc381(\u00b7) and \ud835\udc3a(\u00b7) denote the image encoder and StyleGAN gener-\nator, respectively. Then, our method for training an image encoder in\nStage I followed the usual GAN inversion method. The loss function\n\ud835\udc3f\ud835\udc3c we used in Stage I is as follows:\n\ud835\udc3f\ud835\udc3c = \ud835\udc3f2(\ud835\udc3a(\ud835\udc3c (\ud835\udc9b)),\ud835\udc3a(\ud835\udc9b))\n(4)\nJust by calculating the \ud835\udc3f2 distance between the input image and the\nreconstructed image, the image encoder can already learn inverse\nmapping very well. Similarly, we defined the output latent code of\nour sketch encoder as follows:\n\ud835\udc46(\ud835\udc9b) := \ud835\udc382(Draw\ud835\udc56 (\ud835\udc3a(\ud835\udc9b)))\n(5)\nwhere \ud835\udc382(\u00b7) and Draw\ud835\udc56(\u00b7) denote our sketch encoder and our drawing\nprocess simulation as described in Algorithm 1, which can convert\nthe image \ud835\udc99 to a series of intermediate sketches of the drawing\nprocess and select the \ud835\udc56-th sketch from among them.\nIn each iteration of training in Stage II, we can generate sketches\n\ud835\udc7a and corresponding loss masks \ud835\udc74 after our drawing process simu-\nlation. Then, the loss function is:\n\ud835\udc3f\ud835\udc46 = \ud835\udc3f1(\ud835\udc3a(\ud835\udc46(\ud835\udc9b)) \u2217 \ud835\udc40,\ud835\udc3a(\ud835\udc9b) \u2217 \ud835\udc40) + \ud835\udc3f2(\ud835\udc3c (\ud835\udc9b),\ud835\udc46(\ud835\udc9b))\n(6)\n\ud835\udc3f2(\ud835\udc3c (\ud835\udc9b),\ud835\udc46(\ud835\udc9b)) ensures that the sketch with a higher degree of com-\npletion is closer to the projection of the original in the latent sub-\nspace, while \ud835\udc3f1(\ud835\udc3a(\ud835\udc46(\ud835\udc9b)) \u2217 \ud835\udc40,\ud835\udc3a(\ud835\udc9b) \u2217 \ud835\udc40) with a loss mask ensures\nthe local similarity between the originals and the generated results.\n5\nUSER INTERFACE\nFigure 6 shows our drawing assistance system. The system auto-\nmatically records all the vertices of strokes and the stroke order,\nand converts the strokes into a raster image and corresponding\nguidance display on the sketch panel in real-time. Similar to Shad-\nowDraw [2011], this system provides two types of guidance: \u201crough\nguidance\u201d and \u201cdetailed guidance,\u201d which users can switch at any\ntime. Different colors denote semantics of the generated line draw-\ning and we calculated the colorized guidance with a combination of\nfew-shot semantic segmentation [Wang et al. 2019] and one-shot\nlearning for StyleGAN controlling [Endo and Kanamori 2022] (Sec-\ntion 2 of the supplementary material contains more details). Detailed\nguidance shows the full-face portrait to the user as a prompt, while\nrough guidance shows the user a part of the face that has been\ndrawn roughly or will be drawn soon as a prompt by predicting the\nthe user\u2019s drawing progress. In rough guidance mode, only a single\nsemantic part (color) will be shown according to the moving mouse\npoint. Both are useful and high-quality: detailed guidance allows the\nuser to understand the overall layout of the face to draw, and rough\nguidance allows the user to focus on drawing the local parts of the\nface. If the user is satisfied with the current guidance and does not\nwant to change it any further for a sketch trace, he/she can press\nthe \u201cPin\u201d button to realize this purpose. When the sketch is com-\npleted, users can generate the final color image by clicking on the\n\u201creference image selection\u201d button (face icon) to select the coloring\nstyle from the reference images. In contrast, with the \u201cEraser\u201d tool,\nusers right-click on a stroke and the system erases it. In addition,\nthe \u201cUndo\u201d tool can delete the last stroke from the stroke list. Note\nthat our system can also load (or export) user-drawn strokes by\nclicking the \u201cLoad\u201d (or \u201cSave\u201d) buttons.\n6\nEXPERIMENTS AND RESULTS\n6.1\nImplementation Details\nThe image encoder and sketch encoder (Figure 4) adopted the pSp\narchitecture [2021]. We chose layers 1-8 in W+ space as the struc-\ntural code and layers 9-18 as the color code. We adopted a Ranger\noptimizer and set the learning rate to 0.0001. As a training environ-\nment, NVIDIA RTX3090 GPU was used to train our encoders on\nthe Linux platform. Then, a workstation with Intel Core i7 8700,\n3.20 GHz, NVIDIA RTX1070 GPU, and 64GB RAM on the Windows\n10 platform was used as the testing environment. The input and\noutput image resolution was 256\u00d7256 and 512\u00d7512, respectively;\n35,480 iterations with batch size 8 in Stage I and 3,808 iterations with\nbatch size 16 (7+9 for Figure 5(b, d)) in Stage II. Because data were\nrandomly generated by \ud835\udc67, \u201cmax epochs\u201d is replaced by \u201citerations.\u201d\n, Vol. 1, No. 1, Article . Publication date: June 2023.\n6\n\u2022\nHuang, Z. et al\nFig. 6. User interface of the proposed AniFaceDrawing drawing assistance\nsystem. User guidance is generated using our sketch-based latent space\nexploration approach.\n(a)\nInput\n(b)\nFig. 7. Comparison of training (a) without background augmentation and\n(b) our training strategy in Stage II. When a red stroke is added, the result\nfrom the sketch encoder training without background augmentation is\nhighly degraded.\n6.2\nStability Testing\nTo test the stability of our sketch encoder during the freehand sketch-\ning process and verify the stroke-level disentanglement conditions,\nwe first performed the following experiments.\nWithout background augmentation. The effect of background\naugmentation is shown in Figure 7. If sketches in the drawing pro-\ncess (Figure 5(b, c)) are trained without considering the background,\nthe sketch encoder cannot correctly understand strokes associated\nwith the background and project them near the correct position.\nThe influence of stroke order and multiple strokes on one\nfacial part. Figure 8 compares the intermediate process of the\nsame sketch with different stroke orders. It can be seen that the\nfinal results are not very different, but the intermediate processes\nmaintain some diversity. This figure also shows that even if only one\nstroke is used for each part of the face during training, the generated\nguidance matches well when the user uses multiple strokes for the\nsame part (e.g., the left eye and mouth).\nFig. 8. The influence of stroke order and multiple strokes for one facial part\n(e.g., mouth (green) and left eye (blue)) in AniFaceDrawing.\nTable 1. FID scores of baseline and our approaches. As a reference, the FID\nbetween Decoder1k and Danbooru1k is 70.86.\nDBName\nOurs\nDecoder1k\nDanbooru1k\nBaseline\n74.75\n106.03\n151.42\nOurs\n-\n74.14\n125.19\n\u201cBad\u201d stroke. If only some parts of a stroke contain valid infor-\nmation, then the stroke is considered as \u201cbad.\u201d In freehand sketch-\ning, \u201cbad\u201d strokes are not uncommon. The results generated by our\nmethod provide a suitable match to the valid part of such \u201cbad\u201d\nstrokes. For example, the strokes representing the left eye in Fig-\nure 8 form a triangle, a shape that is not natural for representing the\neye contour, while the generated result is still reasonable. Another\nexample is the first stroke, which partially matches the normal face\ncontour (see Figure 10), but our approach still manages to capture\nthis information and ignore the meaningless part of this stroke.\n6.3\nQualitative Results\nWe found that there is no S2I synthesis technique for anime portraits\ndue to the lack of paired data. Therefore, we trained an additional\nsketch encoder for the complete sketch using a random cropping\nstrategy with the state-of-the-art pSp encoder [2021] as a baseline\nfor a fair comparison. Some examples of input sketches in the train-\ning step for this baseline encoder are shown in Figure 5(f). Except for\nthis random cropping training strategy, which is used in iSketchN-\nFill [2019], the hyperparameters and the architecture of the baseline\nnetwork are the same as those in our sketch encoder. From the\ncomparison results (Figure 10), we verified that our method can\nprovide consistently high-quality guidance that better matches the\ninput during the sketching process. Meanwhile, when the input con-\ntains \u201cbad\u201d strokes, the generated guidance will provide a reasonable\nresult rather than complete matching guidance.\n, Vol. 1, No. 1, Article . Publication date: June 2023.\nAniFaceDrawing: Anime Portrait Exploration during Your Sketching\n\u2022\n7\n\ud835\udfcf\ud835\udfd0\ud835\udfd3. \ud835\udfcf\ud835\udfd7\nBaseline\nOurs\n\ud835\udfd5\ud835\udfd2. \ud835\udfcf\ud835\udfd2\n106.03\n151.42\n74.75\nDecoder1k\nDanbooru1k\n70.86\nFig. 9. Samples from the different datasets or approaches and the FIDs of\neach.\n6.4\nQuantitative Results\nOur approach was evaluated from two aspects: the quality of the\ngenerated images, and the match between input and output. The\nquality of image generation affects both the quality of the guidance\nreceived by the user and the evaluation of the final generated result,\nso it was necessary to measure this indicator quantitatively in ad-\ndition to the subjective evaluation of the user. For similar reasons,\nthe match between the input sketch and the guide needed to be\nmeasured quantitatively to ensure that the validity of our approach\nwas subjectively and objectively consistent. To evaluate usability\nand satisfaction, a user study was conducted for the overall system.\nQuality of generated images. Unlike normal S2I synthesis, this\nwork was dedicated to the stability of matching rough sketches\nand intermediate results throughout the drawing process. To evalu-\nate the degree of matching between strokes and hints during the\ndrawing process, this work used Fr\u00e9chet Inception Distance (FID)\nto measure the gap between the generated images: First, users were\nasked to draw 10 sketches and record 177 images generated by our\nmethod as a database \u201cOurs,\u201d the results generated by the base-\nline method with the same input as a database \u201cBaseline,\u201d 1,000\nrandomly-generated images using StyleGAN in our decoder as a\ndatabase \u201cDecoder1k,\u201d and 1,000 randomly-selected images from\nthe Danbooru database [Branwen et al. 2019] as a database \u201cDan-\nbooru1k.\u201d The FIDs between them are shown in Table 1. It can be\nseen that, in line with the observation in the qualitative results, our\nmethod generated better-quality images \u2013 similar to the images gen-\nerated by the decoder in Decoder1k as well as to the real images in\nDanbooru1k. Figure 9 shows samples from each dataset or approach\nmentioned above, which makes results more intuitive.\nMatching the generated image to the input sketch. To evaluate\nthe match between the input sketch and the generated guidance,\nsketch-guidance matching can be thought of as a prediction prob-\nlem. Although neither sketch \ud835\udc46 nor the generated line drawing \ud835\udc3f\nis reliable enough as ground truth, the input sketches are regarded\nas ground truth here because what we are concerned about is how\nthe system will cater to the input with guidance. The evaluation\nindicators precision \ud835\udc5d, recall \ud835\udc5f, and \ud835\udc391 score for sketch-guidance\nmatching are defined in Section 1 of our supplementary material.\nBaseline\nInput\nOurs\nFig. 10. Qualitative comparison with the same input sketch sequences. A\nred stroke represents the last stroke in a sketch.\nTable 2. The average of different metrics from the proposed method and the\nbaseline method (ours / baseline). At the beginning of the drawing process,\nthe input sketches are usually sparser, which makes it difficult to generate\nmatching results. Thus, the average recall score \ud835\udc5f of the first \ud835\udc58 strokes is\nmore important.\naaaaaa\na\nMetrics\n\ud835\udc58\n1\n3\n6\n9\nwhole process\n\ud835\udc5d\n0.04/0.03\n0.04/0.05\n0.05/0.07\n0.07/0.08\n0.12/0.12\n\ud835\udc5f\n0.48/0.40\n0.46/0.39\n0.45/0.38\n0.43/0.37\n0.39/0.31\n\ud835\udc391\n0.07/0.05\n0.07/0.09\n0.09/0.11\n0.11/0.13\n0.17/0.16\nTable 2 shows the comparison results between the proposed\nmethod and the baseline method (described in Section 6.3). It is\neven more important to provide high-matching guidance in the\nearly stages of drawing when strokes are sparse. Therefore, the\naverage recall rate was calculated for the first 1, 3, 6 and 9 strokes\nand for the whole sketch process (\u221e). We believe that the average\nrecall is the best numerical description of sketch-guidance matching,\nas it is the proportion of overlapping areas of monochrome sketches\nand guidance over all current input sketches, and our results consis-\ntently outperformed and agreed with the baseline method in this\nmetric throughout the drawing process, as shown in Figure 11. This\nresult was also consistent with the observation of the qualitative\ncomparison (see Figure 10). However, slightly lower values of our\n, Vol. 1, No. 1, Article . Publication date: June 2023.\n8\n\u2022\nHuang, Z. et al\nStroke Order\nRecall\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0\n5\n10\n15\n20\n25\nBaseline\nOurs\nFig. 11. Recall comparison as strokes increase. The average score of our\nmethod was higher than that of the baseline method.\nmethod to the evaluation metrics \ud835\udc5d and \ud835\udc391 compared to those of the\nbaseline, indicate that our method provides more details in the guid-\nance generation. This experiment demonstrated that the guidance\ngenerated by this system can better match the input sketch, both at\nthe beginning and throughout the drawing process. Based on the\nabove results, recall can be considered a valid metric for measuring\nthe match between the input sketch and output guidance.\n7\nUSER STUDY\nTo verify the effectiveness of our anime-style drawing assistance\nsystem, we invited 15 participants (graduate students) to participate\nin user study. All participants were asked to draw anime-style por-\ntraits online using a remote mouse control. They were asked to draw\nthe portraits freely and aimlessly, trying to draw as much detail as\npossible. Each user conducted anime portrait drawing twice: the\nfirst time to experience the whole process of drawing creation to\nfamiliarize themselves with the operation until they got used to\nthis system and felt comfortable; and the second time, the partici-\npants completed the whole process independently. We instructed all\nparticipants on how to use AniFaceDrawing with user manuals (in\nthe supplementary video). Before the hands-on experiments, they\nwere asked to watch a tutorial video. All participants were required\nto carefully draw and select the most anticipated references for\nlocal guidance from several generated candidates after completing\nthe global stage. If the generated guidance met their wishes and\nexpectations, participants were required to press the \u201cPin\u201d button to\ndraw carefully to refine the input sketch. Participants could select\na reference image for color portrait generation at any time during\ntheir drawing until they were satisfied with the results. Finally, they\ncompleted the questionnaire after finishing the second drawing.\n7.1\nQuestionnaire Design\nOur questionnaire consists of three parts: the system usability scale\n(SUS) [Bangor et al. 2008], creativity-support index (CSI) [Carroll\net al. 2009], and a set of custom questions shown in Table 3 to\ninvestigate the relationship between user satisfaction and guidance\nmatching. In the SUS, 10 questionnaire items were set up to capture\nsubjective evaluations of the system usability. A five-point Likert\nscale was used in the evaluation experiment.\nTable 3. Custom questions in our user study.\n#\nQuestion\nMean\nSD\nQ0\nHow would you rate your drawing skills for anime/real faces ?\n2.07\n1.10\nQ1\nDoes the guidance match your sketch overall ?\n4.07\n0.26\nQ2\nDoes the guidance match your sketch when drawing the mouth ?\n3.40\n1.12\nQ3\nDoes the guidance match your sketch when drawing the left eye ?\n3.93\n0.88\nQ4\nDoes the guidance match your sketch when drawing the right eye ?\n4.00\n0.76\nQ5\nDoes the guidance match your sketch when drawing the nose ?\n4.07\n0.80\nQ6\nDoes the guidance match your sketch when drawing the hair ?\n3.47\n1.51\nQ7\nDoes the guidance match your sketch when drawing the facial contours ?\n4.07\n0.80\nQ8\nFor your sketch and guidance, which facial balance is more reasonable ?\n3.87\n1.06\nQ9\nWhat is the quality of the guidance ?\n4.00\n0.53\nQ10\nDoes the guidance maintain high quality in your sketching process ?\n4.27\n0.46\nQ11\nIs the rough semantics guidance mode helpful for your drawing ?\n3.93\n0.70\nQ12\nIs the detailed semantics guidance mode helpful for your drawing ?\n4.13\n0.74\nQ13\nAre you satisfied with the final coloring results ?\n3.93\n0.59\nQ14\nDoes the guidance follow your will ?\n3.87\n0.35\nQ15\nAre you satisfied with the final sketch result ?\n3.87\n0.35\n1\n2\n3\n4\n5\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11 12 13 14 15\nFig. 12. Boxplots of custom questions in our user study. Questions Q0 to\nQ15 correspond to those in Table 3.\nSince the purpose of this work is to support user drawing cre-\nativity, the CSI is used to quantitatively evaluate the effectiveness\nof the proposed method. The CSI score defines the creativity of\nthe tool with six factors: collaboration, enjoyment, exploration, ex-\npressiveness, immersion, \u201cresults worth effort,\u201d and is scored with\na maximum of 100 points. Here, the \u201cCollaboration\u201d factor was set\nto 0 (not applicable) because there is no collaboration with another\nuser in our task, as users completed the art drawing independently.\n7.2\nResults\nThis section discusses the visual results of AniFaceDrawing from\nusers and user feedback from the user study. Table 3 shows the mean\nand SD of each question in our customized questionnaire, while\nFigure 12 shows the corresponding boxplots for these questions.\nVisual results.\nFigure 13 shows some examples of the results generated in the\nuser study. Our system could successfully transform the user\u2019s rough\nsketches into high-quality anime portraits. According to Q0 in our\ncustom questions, 86.66% of participants thought their drawing\nskills were not good enough (less than or equal to 3) for anime\nportrait drawing. As shown in Figure 13, it can be concluded that\neven novices can make reasonable sketches with the help of the\nsystem and end up with high-quality color art drawings.\nSystem usability. The average score for SUS drawing assistance for\nanime style was 73.84 (\ud835\udc46\ud835\udc37 = 20.04). The upper and lower limits were\n90 and 65, respectively. In addition, participants stated that \u201cOverall\n, Vol. 1, No. 1, Article . Publication date: June 2023.\nAniFaceDrawing: Anime Portrait Exploration during Your Sketching\n\u2022\n9\n(c)\n(b)\n(a)\nFig. 13. Visual results from the user study. (a) the final user sketches, (b) the\nguidance in detail mode, and (c) the generated color drawings from (a) after\nthe final reference image selection.\nTable 4. CSI Questionnaire results in the user study.\nTerms\nMean\nSD\nCollaboration\n-\n-\nEnjoyment\n27.93\n10.09\nExploration\n29.11\n11.27\nExpressiveness\n23.50\n6.30\nImmersion\n13.46\n10.86\nResults Worth Effort\n22.54\n11.65\nCSI Score\n77.69\nit\u2019s a good tool for those like me who do not have much drawing skills,\nand it\u2019s easy to use in terms of guidance generation and color selection\u201d\nand \u201cI was not familiar with the operation when I first experimented,\nbut I got an amazing generated result in the second experiment.\u201d From\nthese result, the usability of our drawing assistance system could\nbe considered \u201cgood\u201d for the anime style.\nCreative support capability. As shown in Table 4, the average\nscores on CSI for anime style is 77.69. Although there is still room\nfor improvement in terms of immersion and expressiveness, the\nsystem can be used to create sketch-based art drawings.\nTime cost. After the user draws a stroke, AniFaceDrawing provides\nan average response time of 1.65 seconds for guidance generation.\nThis response time appears to be too long for the user. Someone\nsaid, \u201cOne small problem is the not-so-short wait time after each stroke\nis completed.\u201d This also affected the immersion score in Table 4. To\nimprove the user experience, the calculation time needs to be further\nreduced. Although there was no time limit on our experiments, the\naverage time to complete an experiment was about 9 minutes.\nUser-perception match degree. According to the results from\nQ1 to Q7 in Table 3, the average scores ranged from 3.40 to 4.07,\nwhich illustrates that our system can output relatively matching\nguidance to the input sketches during the drawing process. In these\nquestions, a score of 5 indicates a \u201ccomplete match\u201d and 1 point\nmeans a \u201ccomplete mismatch.\u201d Although the statistics showed that\nthe input and output matched relatively well, users disputed whether\nthe hair in the input sketch and the generated image matched. The\nproponents commented, \u201cThe drawing assistance system performs\nbetter on hair and eyes, and can match well with the drawing person\u2019s\ndraft to generate (anime portrait).\" Critics said, \u201cI tried to draw a\ndouble ponytail character, but couldn\u2019t achieve it\u201d and \u201cSome special\nhairstyles cannot be generated by this system.\u201d The reason is that the\nstroke-level disentanglement is focused on facial contour features in\nthe training step, and hair is trained with a random cropping strategy.\nEven so, most participants still tended to think that the sketch-hairs\nmatch is positive, which shows the generalization capability of our\nsystem at a certain level because we did not split the hair part into\nstrokes to train in our training step.\nUser-perception quality. According to the results from Q8 to Q10\nin Table 3, users believed that the system consistently produced\nhigh-quality and reasonably balanced facial guidance throughout\nthe drawing process in anime-style drawing assistance. This re-\nsult is consistent with the results of the qualitative experiments in\nFigure 10, and they corroborate each other.\nUser satisfaction with guidance. The results from Q11 to Q15\nshow that users generally agreed that our system provides good\nsupport for creating anime-style portraits, improving both the user\u2019s\nown sketches and producing a desirable final color image according\nto their expectations. Considering Q1 to Q7, the consistency of these\nscores illustrates that our approach achieved the optimal match\nbetween sketch input and guidance output so that users are satisfied\nwith our drawing assistance during the drawing process.\n8\nCONCLUSION\nWe successfully re-ordered the feature vectors in latent space at the\nstroke-level by unsupervised learning with a drawing process simu-\nlation. The experiments demonstrated the stability and effectiveness\nof the proposed method. The experimental results show that our\nmethod can stably and consistently obtain high-quality generation\nresults during freehand sketching, independent of stroke order and\n\u201cbad\u201d strokes. With our user study, AniFaceDrawing was proven to\nbe effective and was able to create an anime portrait according to the\nusers\u2019 intentions. As a limitation, the matching of the input sketch\nfor the hair part could be improved due to the training strategy.\nAs the results generated by our method are completely dependent\non the decoder\u2014that is, the pre-trained StyleGAN\u2014the decoder, in\nturn, restricts the types of images generated (refer to supplementary\nmaterial for more details). For example, since our pre-trained model\nis trained on an anime portrait database selected from Danbooru,\n, Vol. 1, No. 1, Article . Publication date: June 2023.\n10\n\u2022\nHuang, Z. et al\nthe generated results are all female. In addition, the current style is\nrelatively constant: how to extract other styles from StyleGAN and\nmake anime style more diverse and controllable will be explored\nin follow-up research. Meanwhile, how to expand the results with\nmore styles, such as Ukiyo-e and painting, while keeping the strokes\nmatching, is a promising topic for future work.\nACKNOWLEDGMENTS\nThis research was supported by the JAIST Research Fund, Kayamori\nFoundation of Informational Science Advancement, JSPS KAKENHI\nJP20K19845, and JP19K20316.\nREFERENCES\nEmre Aksan, Fabrizio Pece, and Otmar Hilliges. 2018. DeepWriting: Making Digital Ink\nEditable via Deep Generative Modeling. In Proceedings of the 2018 CHI Conference on\nHuman Factors in Computing Systems. ACM, New York, USA, 205:1\u2013205:14. https:\n//doi.org/10.1145/3173574.3173779\nYuval Alaluf, Or Patashnik, and Daniel Cohen-Or. 2021. ReStyle: A Residual-Based\nStyleGAN Encoder via Iterative Refinement. In Proceedings of IEEE/CVF International\nConference on Computer Vision. IEEE, Montreal, QC, Canada, 6691\u20136700.\nhttps:\n//doi.org/10.1109/ICCV48922.2021.00664\nAaron Bangor, Philip T. Kortum, and James T. Miller. 2008. An Empirical Evaluation of\nthe System Usability Scale. International Journal of Human\u2013Computer Interaction\n24, 6 (2008), 574\u2013594. https://doi.org/10.1080/10447310802205776\nGwern Branwen, Anonymous, and Danbooru Community. 2019. Danbooru2019 Por-\ntraits: A Large-Scale Anime Head Illustration Dataset. https://www.gwern.net/\nCrops#danbooru2019-portraits. Accessed: 2023-01-25.\nErin A. Carroll, Celine Latulipe, Richard Y. K. Fung, and Michael A. Terry. 2009. Creativ-\nity factor evaluation: towards a standardized survey metric for creativity support.\nIn Proceedings of the 7th Conference on Creativity and Cognition. ACM, New York,\nUSA, 127\u2013136. https://doi.org/10.1145/1640233.1640255\nShu-Yu Chen, Wanchao Su, Lin Gao, Shihong Xia, and Hongbo Fu. 2020. DeepFace-\nDrawing: deep generation of face images from sketches. ACM Transactions on\nGraphics 39, 4 (2020), 72:1\u201372:16. https://doi.org/10.1145/3386569.3392386\nChia-Hsing Chiu, Yuki Koyama, Yu-Chi Lai, Takeo Igarashi, and Yonghao Yue. 2020.\nHuman-in-the-loop differential subspace search in high-dimensional latent space.\nACM Transactions on Graphics 39, 4 (2020), 85:1\u201385:15.\nhttps://doi.org/10.1145/\n3386569.3392409\nJungwoo Choi, Heeryon Cho, Jinjoo Song, and Sang Min Yoon. 2019. SketchHelper: Real-\ntime stroke guidance for freehand sketch retrieval. IEEE Transactions on Multimedia\n21, 8 (2019), 2083\u20132092. https://doi.org/10.1109/TMM.2019.2892301\nJohn P. Collomosse, Tu Bui, and Hailin Jin. 2019. LiveSketch: Query Perturbations for\nGuided Sketch-Based Visual Search. In Proceedings of IEEE Conference on Computer\nVision and Pattern Recognition. IEEE, Long Beach, CA, USA, 2879\u20132887.\nhttps:\n//doi.org/10.1109/CVPR.2019.00299\nTali Dekel, Chuang Gan, Dilip Krishnan, Ce Liu, and William T. Freeman. 2018. Sparse,\nSmart Contours to Represent and Edit Images. In Proceedings of IEEE Conference on\nComputer Vision and Pattern Recognition. IEEE, Salt Lake City, UT, USA, 3511\u20133520.\nhttps://doi.org/10.1109/CVPR.2018.00370\nYuki Endo and Yoshihiro Kanamori. 2022. Controlling StyleGANs using rough scribbles\nvia one-shot learning. Computer Animation and Virtual Worlds 33, 5 (2022), e2102.\nhttps://doi.org/10.1002/cav.2102\nEmma Frid, Celso Gomes, and Zeyu Jin. 2020. Music Creation by Example. In Proceedings\nof CHI Conference on Human Factors in Computing Systems. ACM, New York, USA,\n1\u201313. https://doi.org/10.1145/3313831.3376514\nTsukasa Fukusato, Seung-Tak Noh, Takeo Igarashi, and Daichi Ito. 2020. Interactive\nMeshing of User-Defined Point Sets. Journal of Computer Graphics Techniques 9, 3\n(2020), 39\u201358. http://jcgt.org/published/0009/03/03/\nArnab Ghosh, Richard Zhang, Puneet K. Dokania, Oliver Wang, Alexei A. Efros, Philip\nH. S. Torr, and Eli Shechtman. 2019. Interactive Sketch & Fill: Multiclass Sketch-to-\nImage Translation. In Proceedings of IEEE/CVF International Conference on Computer\nVision. IEEE, Seoul, Korea, 1171\u20131180. https://doi.org/10.1109/ICCV.2019.00126\nYuxuan Han, Jiaolong Yang, and Ying Fu. 2021. Disentangled Face Attribute Editing via\nInstance-Aware Latent Space Search. In Proceedings of International Joint Conference\non Artificial Intelligence. International Joint Conferences on Artificial Intelligence\nOrganization, Virtual, Montreal, 715\u2013721. https://doi.org/10.24963/ijcai.2021/99\nErik H\u00e4rk\u00f6nen, Aaron Hertzmann, Jaakko Lehtinen, and Sylvain Paris. 2020. GANSpace:\nDiscovering Interpretable GAN Controls. Proceedings of Advances in Neural Infor-\nmation Processing Systems 33 (2020), 9841\u20139850.\nZhongyuan Hu, Haoran Xie, Tsukasa Fukusato, Takahiro Sato, and Takeo Igarashi.\n2019. Sketch2VF: Sketch-based flow design with conditional generative adversarial\nnetwork. Computer Animation and Virtual Worlds 30, 3-4 (2019), e1889:1\u2013e1889:11.\nhttps://doi.org/10.1002/cav.1889\nHysts. 2021. Anime Face Detector. https://github.com/hysts/anime-face-detector.\nTakeo Igarashi and John F. Hughes. 2001. A Suggestive Interface for 3D Drawing.\nIn Proceedings of the 14th Annual ACM Symposium on User Interface Software and\nTechnology. ACM, New York, USA, 173\u2013181. https://doi.org/10.1145/502348.502379\nTakeo Igarashi, Satoshi Matsuoka, Sachiko Kawachiya, and Hidehiko Tanaka. 1997.\nInteractive beautification: a technique for rapid geometric design. In Proceedings of\nthe 10th Annual ACM Symposium on User Interface Software and Technology. ACM,\nNew York, USA, 105\u2013114. https://doi.org/10.1145/263407.263525\nTero Karras, Samuli Laine, and Timo Aila. 2019. A Style-Based Generator Architecture\nfor Generative Adversarial Networks. In 2019 IEEE/CVF Conference on Computer\nVision and Pattern Recognition. CVF / IEEE, Long Beach, CA, USA, 4401\u20134410. https:\n//doi.org/10.1109/CVPR.2019.00453\nTero Karras, Samuli Laine, and Timo Aila. 2021. A Style-Based Generator Architecture\nfor Generative Adversarial Networks. IEEE Transactions on Pattern Analysis and\nMachine Intelligence 43, 12 (2021), 4217\u20134228. https://doi.org/10.1109/TPAMI.2020.\n2970919\nYong Jae Lee, C Lawrence Zitnick, and Michael F Cohen. 2011. Shadowdraw: real-time\nuser guidance for freehand drawing. ACM Transactions on Graphics 30, 4 (2011),\n27:1\u201327:10. https://doi.org/10.1145/2010324.1964922\nYuhang Li, Xuejin Chen, Binxin Yang, Zihan Chen, Zhihua Cheng, and Zheng-Jun Zha.\n2020. DeepFacePencil: Creating Face Images from Freehand Sketches. In Proceedings\nof the 28th ACM International Conference on Multimedia. ACM, New York, USA,\n991\u2013999. https://doi.org/10.1145/3394171.3413684\nLi Liu, Fumin Shen, Yuming Shen, Xianglong Liu, and Ling Shao. 2017. Deep Sketch\nHashing: Fast Free-Hand Sketch-Based Image Retrieval. In Proceedings of IEEE\nConference on Computer Vision and Pattern Recognition. IEEE, Honolulu, HI, USA,\n2298\u20132307. https://doi.org/10.1109/CVPR.2017.247\nY. Peng, Y. Mishima, Y. Igarashi, R. Miyauchi, M. Okawa, H. Xie, and K. Miyata. 2020.\nSketch2Domino: Interactive Chain Reaction Design and Guidance. In 2020 Nicograph\nInternational. IEEE, Tokyo, Japan, 32\u201338. https://doi.org/10.1109/NicoInt50878.2020.\n00013\nTiziano Portenier, Qiyang Hu, Attila Szab\u00f3, Siavash Arjomand Bigdeli, Paolo Favaro,\nand Matthias Zwicker. 2018. FaceShop: Deep Sketch-based Face Image Editing. ACM\nTransactions on Graphics 37, 4 (2018), 99:1\u201399:13. https://doi.org/10.1145/3197517.\n3201393\nHui Ren, Jia Li, and Nan Gao. 2020. Two-Stage Sketch Colorization With Color Parsing.\nIEEE Access 8 (2020), 44599\u201344610. https://doi.org/10.1109/ACCESS.2019.2962579\nElad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro,\nand Daniel Cohen-Or. 2021. Encoding in Style: A StyleGAN Encoder for Image-\nto-Image Translation. In Proceedings of IEEE Conference on Computer Vision and\nPattern Recognition. IEEE, virtual, 2287\u20132296. https://doi.org/10.1109/CVPR46437.\n2021.00232\nYujun Shen, Ceyuan Yang, Xiaoou Tang, and Bolei Zhou. 2022. InterFaceGAN: Inter-\npreting the Disentangled Face Representation Learned by GANs. IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence 44, 4 (2022), 2004\u20132018. https:\n//doi.org/10.1109/TPAMI.2020.3034267\nWanchao Su, Hui Ye, Shu-Yu Chen, Lin Gao, and Hongbo Fu. 2022. DrawingInStyles:\nPortrait Image Generation and Editing with Spatially Conditioned StyleGAN. https:\n//doi.org/10.48550/arXiv.2203.02762 arXiv:2203.02762\nOmer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, and Daniel Cohen-Or. 2021.\nDesigning an encoder for StyleGAN image manipulation. ACM Transactions on\nGraphics 40, 4 (2021), 133:1\u2013133:14. https://doi.org/10.1145/3450626.3459838\nHung-Yu Tseng, Matthew Fisher, Jingwan Lu, Yijun Li, Vladimir G. Kim, and Ming-\nHsuan Yang. 2020. Modeling Artistic Workflows for Image Generation and Editing.\nIn Proceedings of 16th European Conference on Computer Vision. Springer, Springer,\nCham, 158\u2013174. https://doi.org/10.1007/978-3-030-58523-5_10\nKaixin Wang, Jun Hao Liew, Yingtian Zou, Daquan Zhou, and Jiashi Feng. 2019. PANet:\nFew-Shot Image Semantic Segmentation With Prototype Alignment. In Proceedings\nof IEEE/CVF International Conference on Computer Vision. IEEE, Seoul, Korea, 9196\u2013\n9205. https://doi.org/10.1109/ICCV.2019.00929\nHolger Winnem\u00f6ller. 2011.\nXDoG: advanced image stylization with eXtended\nDifference-of-Gaussians. In Proceedings of 9th International Symposium on Non-\nPhotorealistic Animation and Rendering (NPAR). ACM, New York, USA, 147\u2013156.\nhttps://doi.org/10.1145/2024676.2024700\nWeihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, and Ming-Hsuan Yang.\n2023. GAN Inversion: A Survey. IEEE Transactions on Pattern Analysis and Machine\nIntelligence 45, 3 (2023), 3121\u20133138. https://doi.org/10.1109/TPAMI.2022.3181070\nShuai Yang, Liming Jiang, Ziwei Liu, and Chen Change Loy. 2022. Pastiche Master:\nExemplar-Based High-Resolution Portrait Style Transfer. In Proceedings of IEEE/CVF\nConference on Computer Vision and Pattern Recognition. IEEE, New Orleans, LA,\nUSA, 7683\u20137692. https://doi.org/10.1109/CVPR52688.2022.00754\nShuai Yang, Zhangyang Wang, Jiaying Liu, and Zongming Guo. 2020. Deep Plastic\nSurgery: Robust and Controllable Image Editing with Human-Drawn Sketches. In\nProceedings of 16th European Conference on Computer Vision. Springer, Springer,\n, Vol. 1, No. 1, Article . Publication date: June 2023.\nAniFaceDrawing: Anime Portrait Exploration during Your Sketching\n\u2022\n11\nCham, 601\u2013617. https://doi.org/10.1007/978-3-030-58555-6_36\nShuai Yang, Zhangyang Wang, Jiaying Liu, and Zongming Guo. 2021. Controllable\nSketch-to-Image Translation for Robust Face Synthesis. IEEE Transactions on Image\nProcessing 30 (2021), 8797\u20138810. https://doi.org/10.1109/TIP.2021.3120669\nQian Yu, Feng Liu, Yi-Zhe Song, Tao Xiang, Timothy M. Hospedales, and Chen Change\nLoy. 2016. Sketch Me That Shoe. In Proceedings of IEEE Conference on Computer\nVision and Pattern Recognition. IEEE, Las Vegas, NV, USA, 799\u2013807. https://doi.org/\n10.1109/CVPR.2016.93\n, Vol. 1, No. 1, Article . Publication date: June 2023.\n"
  },
  {
    "title": "WebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences",
    "link": "https://arxiv.org/pdf/2306.07906.pdf",
    "upvote": "12",
    "text": "WebGLM: Towards An Efficient Web-Enhanced Question\nAnswering System with Human Preferences\nXiao Liu\u2217\nliuxiao21@mails.tsinghua.edu.cn\nTsinghua University\nBeijing, China\nHanyu Lai\u2217\nlaihy19@mails.tsinghua.edu.cn\nTsinghua University\nBeijing, China\nHao Yu\u2217\nyuhao2019@buaa.edu.cn\nBeihang University\nBeijing, China\nYifan Xu\nxuyifan2001@gmail.com\nTsinghua University\nBeijing, China\nAohan Zeng\nzah22@mails.tsinghua.edu.cn\nTsinghua University\nBeijing, China\nZhengxiao Du\nzx-du20@mails.tsinghua.edu.cn\nTsinghua University\nBeijing, China\nPeng Zhang\npeng.zhang@zhipuai.cn\nZhipu.AI\nBeijing, China\nYuxiao Dong\u2020\nyuxiaod@tsinghua.edu.cn\nTsinghua University\nBeijing, China\nJie Tang\u2020\njietang@tsinghua.edu.cn\nTsinghua University\nBeijing, China\nAbstract\nWe present WebGLM, a web-enhanced question-answering system\nbased on the General Language Model (GLM). Its goal is to aug-\nment a pre-trained large language model (LLM) with web search\nand retrieval capabilities while being efficient for real-world de-\nployments. To achieve this, we develop WebGLM with strategies\nfor the LLM-augmented retriever, bootstrapped generator, and hu-\nman preference-aware scorer. Specifically, we identify and address\nthe limitations of WebGPT (OpenAI), through which WebGLM is\nenabled with accuracy, efficiency, and cost-effectiveness advan-\ntages. In addition, we propose systematic criteria for evaluating\nweb-enhanced QA systems. We conduct multi-dimensional human\nevaluation and quantitative ablation studies, which suggest the\noutperformance of the proposed WebGLM designs over existing\nsystems. WebGLM with the 10-billion-parameter GLM (10B) is\nshown to perform better than the similar-sized WebGPT (13B) and\neven comparably to WebGPT (175B) in human evaluation. The code,\ndemo, and data are at https://github.com/THUDM/WebGLM.\nCCS Concepts\n\u2022 Computing methodologies \u2192 Natural language generation;\n\u2022 Software and its engineering \u2192 Development frameworks and\nenvironments.\n\u2217XL, HL, and HY contributed equally and this work was done when HY interned\nat Tsinghua. \u2020Corresponding Authors: YD and JT.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\n\u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0103-0/23/08...$15.00\nhttps://doi.org/10.1145/3580305.3599931\nFigure 1: A screenshot of WebGLM\u2019s response to an example\nquestion with web references.\nKeywords\nLarge Language Model; Pre-Trained Model; Human Preference\nAlignment; General Language Model\nACM Reference Format:\nXiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng, Zhengxiao Du, Peng\nZhang, Yuxiao Dong, and Jie Tang. 2023. WebGLM: Towards An Efficient\nWeb-Enhanced Question Answering System with Human Preferences. In\nProceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery\nand Data Mining (KDD \u201923), August 6\u201310, 2023, Long Beach, CA, USA. ACM,\nNew York, NY, USA, 42 pages. https://doi.org/10.1145/3580305.3599931\n1\nIntroduction\nLarge language models (LLMs), such as GPT-3 [3], PaLM [5], OPT [37],\nBLOOM [32], and GLM-130B [36], have significantly pushed the\nboundary of machines\u2019 ability on language understanding and gen-\neration. Question answering [15, 28], one of the most fundamental\nlanguage applications, has also been substantially advanced by\nthe recent LLM developments. Existing studies suggest that the\narXiv:2306.07906v1  [cs.CL]  13 Jun 2023\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nLiu and Lai and Yu, et al.\nWebGLM\n(10B)\nWebGPT\n(175B)\nWebGPT\n(13B)\nPerplexity.ai\n0\n10\n20\n30\n40\n50\n60\nWin Rate\nAgainst Human (%)\nHuman-level\nFigure 2: The win rates of popular web-enhanced QA systems\nagainst human references. WebGLM (10B) performs comparably\nto WebGPT (175B), approaching human-level QA ability.\nperformance of LLMs\u2019 closed-book QA [29] and in-context learn-\ning QA [3, 18] is comparable to supervised models, furthering our\nunderstanding on LLMs\u2019 potential to memorize knowledge.\nHowever, even for LLMs, their capacity is not unlimited, and\nwhen it comes to challenges that require sufficient rare-knowledge,\nLLMs fail to meet up human expectations. Hence recent efforts\nhave been focused on constructing LLMs augmented from external\nknowledge, such as retrieval [8, 12, 16] and web search [24]. For\nexample, WebGPT [24] can browse the web, answer complex ques-\ntions in long form, and provide useful references correspondingly.\nDespite its success, the original WebGPT method [24] is far from\nreal-world deployments. First, it relies on abundant expert-level\nannotations of browsing trajectories, well-written answers, and\nanswer preference labeling, requiring considerable expenses, time,\nand training. Second, the behavior cloning method (i.e., imitation\nlearning) requires its base model GPT-3 to emulate human experts\nby instructing the system to interact with a web browser, issue oper-\nation commands (e.g., Search, Read, and Quote), and then retrieve\nrelevant information from online sources. Finally, the multi-turn\nnature of web browsing demands intensive computation resources\nand can be too slow for user experience, e.g., costing about 31\nseconds for WebGPT-13B to response a 500-token prompt.\nIn this work, we present WebGLM\u2014a practical web-enhanced QA\nsystem based on the 10-billion-parameter General Language Model\n(GLM-10B) [6]. An example is illustrated in Figure 1. It is efficient,\ncost-effective, human preference-aware, and most importantly, of\ncomparable quality to WebGPT. The system employs multiple new\nstrategies and designs to achieve good performance, including:\nAn LLM-augmented Retriever: a two-staged retriever that im-\nplements coarse-grained web search and fine-grained LLM-distilled\nretrieval. It is inspired by the fact that LLMs like GPT-3 can natu-\nrally learn to adopt correct references, and such ability could be\ndistilled to improve smaller dense retrievers.\nA Bootstrapped Generator: a GLM-10B based answer generator\nthat is trained on quoted long-formed QA samples and bootstrapped\nby LLM in-context learning. We discover that instead of relying on\nexpensive human expert writing in WebGPT, LLMs can be enabled\nto learn to generate high-quality data with proper citation-based\nfiltering.\nA Human Preference-aware Scorer: a scorer, that is trained over\nonline QA forums\u2019 user thumb-up signals, is able to learn human\nmajority preferences on different answers. Compared to WebGPT\u2019s\nexpert labeling, we prove that a proper dataset construction could\nalso produce a high-quality scorer.\nOur extensive human evaluation and quantitative ablation re-\nsults demonstrate the efficiency and effectiveness of the WebGLM\nsystem. Specifically, WebGLM (10B) surpasses the similar-scaled\nWebGPT (13B) and performs comparably to WebGPT (175B) on our\nTuring test (Cf. Figure 2). WebGLM\u2019s improvement against the only\npublicly-available system\u2014Perplexity.ai\u2014also makes it among the\nbest public web-enhanced QA systems as of this submission.\nTo sum up, in this paper, we make the following contributions:\n\u2022 We construct WebGLM, an efficient web-enhanced QA sys-\ntem with human preferences. It significantly outperforms\nthe similar-sized WebGPT (13B) and performs comparably\nto WebGPT (175B). It also surpasses Perplexity.ai\u2014a popular\nsystem powered by LLMs and search engines.\n\u2022 We identify WebGPT\u2019s limitations on real-world deploy-\nments. We propose a set of new designs and strategies to\nallow WebGLM\u2019s high accuracy while achieving efficient\nand cost-effective advantages over baseline systems.\n\u2022 We formulate the human evaluation metrics for evaluating\nweb-enhanced QA systems. Extensive human evaluation and\nexperiments demonstrate WebGLM\u2019s strong capability and\nalso generate insights into the system\u2019s future developments.\n2\nRelated Work\nThe construction of web-enhanced QA systems is a systematic\nproject that requires cross-domain collaboration, including large\nlanguage models, open-domain question answering, retrieval aug-\nmentation, and reinforcement learning from human feedback. Here\nwe briefly introduce related literature on them.\nLarge Language Models (LLMs). Self-supervised [19] LLMs have\nattracted plenty of attention in nowadays natural language pro-\ncessing (NLP). Their huge number of parameters captures and\nstores versatile knowledge [20] and enables their outstanding per-\nformance on various challenges. Typical LLMs include GPT-3 [3],\nPALM [5], OPT [37], BLOOM [32], and GLM-130B [36]. One of\nthe fascinating LLM properties is prompt-based in-context learn-\ning (ICL), which allows tuning-free task transfer via prepended\ndemonstration samples. Recent works have been focusing on the\noptimization [18, 22, 34, 39] and analysis [23, 30, 35] of ICL.\nOpen-domain Question Answering (Open QA). Traditional QA\ndatasets such as SQuAD [28] assume the reference is available.\nOn the contrary, open-domain QA targets the open world and is\nmore practical but challenging. For example, Natural Questions [15]\ndataset consists of queries from the Google search engine and an-\nnotations from Wikipedia paragraphs. Web Questions [2] derives\nopen-domain questions from knowledge bases. MS Marco [25] gath-\ners passage texts and corresponding labels to questions.\nHowever, most Open QA datasets and models are limited to\nanswer short answer phrases, while people usually prefer more in-\nformative long-formed answers with references. A possible reason\nis that constructing and evaluating long-formed QA datasets with\nopen-world references are difficult, requiring expert-level annota-\ntions. Recent attempts include ELI5 [7] that collects queries and\nlong-formed answers with scores from Reddit and WebGPT [24]\nwhich hires groups of experts and leverages up to 175-billion-\nparameter GPT-3 as the backbone. WebGLM aims to provide an-\nother effective and cost-effective solution for the challenge.\nWebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nRetrieval-augmentation. Mainstream information retrieval ap-\nproaches include sparse-vector-based BM25 and TF-IDF, and the\nrecent dense-vector-based methods such as DPR [14] and Con-\ntriever [10]. The idea of retrieval-augmented language models in-\ntroduced by REALM [8] argues the joint optimization of retriever\nand language modeling. Following representative works include\nRAG [16], Fusion-in-Decoder [11], and Atlas [12]. The idea of We-\nbGPT also loosely falls into the field, as it asks the LLM to interact\nwith the browser to seek relevant information for better accuracy.\nNevertheless, it can cost intensive computation and is too slow for\npractical deployment. In this work, WebGLM tackles the problem\nefficiently by distilling LLMs\u2019 knowledge to smaller retrievers.\nReinforcement Learning from Human Feedback (RLHF). Au-\ntomated scoring of text generation is a well-established area of\nresearch. BLEU [27] and ROUGE [17] take into account the overlap\nratio between the target and reference. METEOR [1] considers the\naccuracy and recall rate of the whole corpus. Other methods, such\nas BERTScore [38], evaluate using cosine similarity of contextual\nembedding from deep language models. In recent years, some work\nadvocates learning scorers from human feedback [26, 33] via asking\nmodels to predict human preference. The scorers, or namely reward\nmodels, can be used to optimize the text generator via reinforce-\nment learning. Such methods, which WebGPT is also affiliated with,\nhave achieved great success in real-world applications.\n3\nThe WebGLM System\nConstructing an LLM-based web-enhanced QA system can be ex-\npensive and challenging. The web information is rich but noisy\nfor certain queries, and creating high-quality human answers with\nreferences for training can be outrageously expensive. This type\nof systems usually involves three critical components: retriever,\ngenerator, and scorer.\nTake WebGPT [24] as an example, which employs experts for\ndataset annotation. Its retriever leverages GPT-3 to \u201cbehavior-clone\u201d\nhuman experts\u2019 web-browsing trajectory to search, read, and quote.\nIn addition, the generator is trained on expert-written long answers\nwith references. And finally, the scorer learns to predict experts\u2019\npreferences over different answers, and its scores serve as rewards\nfor the generator\u2019s reinforcement learning. Despite WebGPT\u2019s pri-\nmary success, its retrieval can be slow, and the data annotations\nrequired for training the generator and scorer are too costly, signif-\nicantly hindering its wide public adoptions.\nIn this work, we aim to build an efficient web-enhanced QA sys-\ntem that understands human preferences for actual use. To combine\nthe advantages of LLMs and well-established open QA studies, we\npresent a series of new designs and strategies for our web-enhanced\nQA system WebGLM based on GLM [6]:\n\u2022 An LLM-augmented Retriever: we design two stages: coarse-\ngrained web search and fine-grained LLM-augmented dense re-\ntrieval [10], for finding relevant references given queries.\n\u2022 A Bootstrapped Generator: we derive WebGLM-QA, an LLM-\nbootstrapped quoted and long-formed QA dataset via in-context\nlearning and corresponding strategies to clean and refine. It in-\ncludes 45k high-quality after filtering and 83k noisy but diverse\nsamples before filtering. The backbone of WebGLM system is a\nGLM model trained on the dataset.\n\u2022 A Human Preference-aware Scorer: we develop techniques to\nlearn human majority preference from online QA forums\u2019 thumb-\nups instead of expensive expert feedback, and successfully train\na human preference-aware scorer for best-of-n selection.\nThe LLM API used for research purpose in this work is text-\ndavinci-003 unless specified. In the following sections, we will\nintroduce the algorithm and implementation details of each com-\nponent, which finally form the WebGLM pipeline sequentially.\n3.1\nLLM-augmented Retriever\nIn conventional open QA, the systems usually only retrieve from\nreliable sources (e.g., Wikipedia) and fail to benefit from whole\nweb-scale knowledge. However, the flip side of the coin is that\nwild web pages can be hard to acquire and purify. In WebGLM, we\nmake attempts to solve the problem via two-stage retrieval: coarse-\ngrained web search and fine-grained LLM-augmented retrieval.\n3.1.1\nCoarse-grained Web Search\nWe leverage third-party web search engines (i.e., Google API) to\nacquire primary candidate web page URLs. In most cases, from\nour observation, these pages can cover the necessary contexts\nand knowledge to answer questions besides considerably abun-\ndant irrelevant information. The procedures are shown in Figure 3.\nSpecifically, it can be roughly divided into three steps:\n(1) Search: At this stage, we enter the question into the search\nAPI and will obtain a list of URLs for potentially-relevant pages\n(usually less than 10).\n(2) Fetch: Then, we crawl the corresponding HTML contents ac-\ncording to the URLs obtained. Since there are many candidate\npages, we improve efficiency through parallel crawling.\n(3) Extract: Next, based on HTML2TEXT1, we extract the part of\ntext contents in the HTML pages and divide them into a list of\nparagraphs according to line breaks.\nSince the web crawl usually takes sufficient time, we have paid\ngreat efforts to optimize the speed of the component to allow user-\nacceptable responding speed (Cf. Figure 4). For example, in the\n\u201cFetch\u201d step, if the page is loaded synchronously, the loading time\nwill be 2-3 minutes long. The parallel asynchronous enables the\nquick loading of most pages in 5s (about 98%).\n3.1.2\nFine-grained LLM-augmented Retrieval\nThrough the first three stages, we have retrieved a number of po-\ntential contexts to questions. However, many of them are still irrel-\nevant even under the filtering of widely-used dense retrievers (in\nour trial, up to 30% of top-ranked contexts are unrelated). As a so-\nlution, WebGPT [24] uses behavior cloning (i.e., imitation learning)\nto leverage LLMs\u2019 strong language comprehensibility for reference\nselection. Notwithstanding its effectiveness, the strategy is slow in\ndeployment and expensive in labeling.\nLLMs\u2019 Reference Adoption. To mitigate the issue, we propose\nto combine smaller retrievers\u2019 efficiency and LLMs\u2019 strong ability\nto distinguish. We take Contriever [10] as the smaller retriever in\nWebGLM, an unsupervised pre-trained model that encodes texts\n1https://github.com/aaronsw/html2text\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nLiu and Lai and Yu, et al.\nWebGPT\nExpert Trajectory & GPT-3 Behavior Cloning\n(expensive, slow, and intensive computation)\nQuestion: \nWhy is it \nsometimes \nhard to eat \nafter not \neating for \na while?\nExpert Annotations\n(expensive)\nExpert Comparison & RL\n(expensive & slow)\nRetriever\nWeb Page 1\n\u2026\u2026\nParagraph 1\nParagraph 2\nParagraph i\n\u2026\nWeb Page N\nParagraph 1\nParagraph 2\nParagraph j\n\u2026\nFine-tuned\nDense\nRetriever\nLLM \nReference \nAdoption\nAugment\nReference [1]: \nBeyond that, when \nyou wait till you \nare absolutely... \nReference [2]: ... \nour body learns \nthat and so it \nlearns to accept...\n\u2026\u2026\nReference [3]: ... \nafter long periods \nof going without \nfood your ...\nFine-grained References\nWebGLM Generator\nAnswer 1\nAnswer 2\n\u2026\u2026\nAnswer K\nAnswer 3\nTraining\nWebGLM-QA dataset\nLLMs ICL \nBootstrap\nCorrection\n& Filtering\nGenerator\nScorer\nOnline QA Forums\nAnswer 1\n32\nAnswer 2\n20\nAnswer T\n3\n\u2026\nHuman Preference-aware Scorer\nComparison Pairs\n-0.2\n0.3\n0.7\n-0.6\n\u2026\nThere are several reasons \nwhy not eating ... burning \nthrough your muscle[1]\n[3]. Another reason is ... \ncalled gluconeogenesis[2]. \nAlso, leptin levels can \nrapidly decline in \u2026\nWebGLM\nFigure 3: WebGLM system pipeline. Our system includes three sub-modules: LLM-augmented retriever recalls the top-5 most relevant\nparagraphs as the reference sources; Bootstrapped generator yields answers according to the question and reference sources; Human\npreference-aware scorer assesses all answers and picks the highest-scored one as the final result. Compared to WebGPT, WebGLM is a more\nefficient and cost-effective web-enhanced QA system with comparable answer quality.\nAvg.\n50%\n75%\n90%\n0.0\n2.0\n4.0\n6.0\n8.0\nsec.\nSearch\nFetch\nExtract\nRetrieval\nFigure 4: WebGLM retriever time analysis. 50% of queries can\nbe done within 4.0s, and 90% of them can be loaded within 10.0s.\nMost of time is spent on fetching web pages after searching.\ninto embeddings and retrieves by finding the maximum inner prod-\nuct pair of them. We transfer LLMs\u2019 natural property of reference\nadoption to small retrievers to improve them.\nTable 1: Evaluation on\nLLM\u2019s reference adoption.\nMethod\nAcc.\nContriever\n68.6%\nLLM ICL adoption\n90.2%\nSpecifically, we find LLMs can\nnaturally distinguish and only\nadopt useful references in in-\ncontext learning (ICL). We create\na 200-query dataset, where each\nquery is accompanied with 5 top-\nranked candidate references from\nContriever. We manually annotate\nthe relevance of each piece of reference (Cf. Table 1). We find only\n68.6% of them are related. However, when we provide the query\nwith corresponding candidate references to GPT-3 for 1-shot in-\ncontext learning inference (see details in Section 3.2), we discover\nthat the LLM would only adopt part of the references and the cor-\nresponding accuracy is 90.2%, far better than Contriever\u2019s.\nAugmentation Implementation. To transfer the reference adop-\ntion knowledge from GPT-3 to Contriever, we leverage the GPT-3\u2019s\nreference adoption from our bootstrapped dataset WebGLM-QA\nto additionally fine-tune Contrievers. As the reference marks gen-\nerated by GPT-3 can be wrong sometimes, we use the citation\ncorrection method based on Rouge-1 precision to match quota-\ntions and references (see those details in Section 3.2). Therefore,\nthe labels we use for training are the Rouge-1 precision scores of a\nquery-reference pair.\nIn the fine-tuning, we use two Contrievers to encode questions\nand references individually, and compute their inner products as the\npredictions. We leverage Mean Square Error (MSE) as the loss func-\ntion for the predictions and Rouge-1 precision scores to train the\nContrievers. Our further quantitative experiment demonstrates that\nthe augmentation significantly improves Contriever web-enhanced\nQA retrieval accuracy (see Table 7 for details).\n3.1.3\nSpeed analysis\nRetrieval is no doubt the most time-consuming part in any web-\nscale QA system. A slow QA system, whatever high its accuracy is,\nwould spoil the user experience. We report the speed of each steps\nin our LLM-augmented retriever.\nWe sample a subset from ELI5 [7] test set to retrieve and calcu-\nlate the average, the median, 75% quantile, 90% quantile, and 99%\nquantile time spent in each step. From Figure 4, we can know that\nour average time spent is about 5.3s, the median total time spent\nis about 4.07s, and 90% of searches can be loaded in 10s. The main\nbottleneck of our retrieval is in the second step of fetching each\npage, when we have to request multiple web pages from different\nsources. Consequently, due the contents of various pages on the\nnetwork are different, some pages take very long time to load, or\njust cannot be returned correctly.\nIn Appendix B, we conduct a more detailed analysis of retrieval\nefficiency and point out that the retrieval efficiency of WebGLM is\nfar better than that of WebGPT.\n3.2\nBootstrapped Generator\nA major obstacle in building web-enhanced QA system is the high\ncost for curating expert-level QA datasets that are long-formed\nand properly cited. Compared to traditional or free-formed QA,\nWebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nReference [1]: Beyond that, when \nyou wait till you're absolutely \nravenous to eat, it\u2019s easy to eat \npast the point of fullness ... \nReference [2]: ... our body \nlearns that and so it learns to \naccept a smaller amount.\nReference [3]: Sometimes after \nlong periods of going without \nfood your immune system ...\nQuestion: Why is it sometimes hard to eat \nafter not eating for a while?\nAnswer: \nI gave a friend an instruction and a \nquestion with references. The friend \nread the instruction and wrote an \noutput to the question.\nRead the references provided and answer \nthe corresponding question\nReference [1]: Beyond that, when you \nwait till you're absolutely  ... \nReference [2]: ... our body learns \nthat and so it learns to accept ...\nReference [3]: Sometimes after long \nperiods of going without food ...\nQuestion: Why is it sometimes hard \nto eat after not eating for a while?\nRead the references provided and answer \nthe corresponding question\n1-shot demonstration \nThere are several reasons why not eating ... \nburning through your muscle[1][3]. Another \nreason is ... called gluconeogenesis[2]. Also, \nleptin levels can rapidly decline in \u2026\n(a) Prompt Formulation\n(b) Instruction Inducting\n(c) Few-shot In-context Learning\nThe instruction is:\nQuestion Slot-1\nQuestion Slot-2\n\u274c\n\u2705\nAnswer: Words considered bad relate \nto negative ways of talking about ... \n[1] ... certain words. [2]\nReference [1]: Words in general that \nare considered bad tend to ... \nReference [2]: Some words are \nconsidered \"bad\" because they ...\nQuestion: Why did we decide that \ncertain words were \"bad\" and \nshouldn\u2019t be used in social settings?\nFigure 5: We construct WebGLM-QA for generator training via LLM in-context bootstrapping. It includes three stages: 1) prompt\nformulation, 2) instruction inducting, and 3) few-shot in-context learning. In this way, we avoid the outrageous cost in time and money for\nhiring experts but still create a high-quality quoted long-formed QA dataset.\nwe expect the system to yield fact-grounded answers with correct\nreferences (see example in 5). WebGPT reports to hire a group of\nfull-time experts to write answers for training, which is far beyond\nordinary budgets.\nFortunately, LLMs\u2019 in-context learning [3, 5], which refers to\ntheir capabilities to transfer to new tasks conditioned on few in-\ncontext samples, have been demonstrated and well-explored re-\ncently. Thus we propose to bootstrap large amounts of quoted long\nanswers via leveraging a few high-quality answers, LLMs, questions\nfrom ELI5 [7], and our retriever collected references. Additionally,\nsince bootstrapped samples are not always satisfying, we design\ncorresponding correction and selection strategies to filter out a\nhigh-quality subset for real training. All these efforts jointly help\nto create the WebGLM-QA, a quoted and long-formed QA dataset\nwith 45k high-quality filtered and 83k unfiltered samples.\nThe dataset can be formulated as a set D(Q, A, R, C), where Q,\nA, R represents the question set, the answer set, and the reference\nset respectively, C \u2286 Q \u00d7 A \u00d72R denotes the triple set of (question,\nanswer, valid references).\nDifferent from free text generation, in web-enhanced QA each\nanswer \ud835\udefc \u2208 A contains quotations and thus is in the form of\n\ud835\udefc = (< \ud835\udc601, \u22071 >, < \ud835\udc602, \u22072 >, \u00b7 \u00b7 \u00b7 , < \ud835\udc60\ud835\udc5b, \u2207\ud835\udc5b >)\n(1)\nwhere < \ud835\udc60\ud835\udc58, \u2207\ud835\udc58 > represents the k-th segment in answer \ud835\udefc, \ud835\udc60\ud835\udc58 is a\npiece of quoted text, and \u2207\ud835\udc58 \u2282 R is a set of references that \ud835\udc60\ud835\udc58 cites.\n3.2.1\nIn-context Learning Inference\nWe adopt a subset of questions from ELI5 train set as our Q and\nleverage a vanilla Contriever [10] (without LLM augmentation yet)\nin fine-grained retrieval to produce references R. In this work we\nfirst try on OpenAI text-davinci-003 API to conduct 1-shot in-\ncontext learning inference to generate quoted long-formed answers\n(while other LLMs such as GLM-130B [36] could be good options\ntoo). Since the in-context learning can be volatile to input forms\nand prompts, we take many trails to finally determine the best\nbootstrapping strategies as follows:\nPrompt Formulation. Since we input many contents to the API,\nincluding a few of demonstrations (i.e., high-quality samples (\ud835\udc5e\ud835\udc51,\n\ud835\udefc\ud835\udc51, R\ud835\udc51)), the question, and the corresponding references, their for-\nmulation could impact the performance significantly. We compare\nseveral types of prompts, including the order between question\nand its references (i.e., before or after, Cf. Figure 5 (a)), the symbols\nused to mark the indices of references, and the prompt words of\nreferences and questions. We conduct experiments with every type\nof prompt we have mentioned, and finally find a natural way as\nshown in Figure 5 (a) performs best.\nInstruction Inducting. Next, we need a proper instruction (e.g.,\n\u201cPlease write a answer based on the question and references.\u201d) for\nguiding the LLM to generate a qualified answer. Recent work [9]\nsuggests that we can take advantage of the LLM itself to design\ninstructions for in-context learning instead of human handcrafting.\nWe use several high-quality examples to induce a few possible\ninstructions (Cf. Figure 5 (b)), and select the best-performed one\nbased on our empirical evaluation over several queries.\nFew-shot In-Context Learning. We study the best shots needed\nfor generating good quoted long-formed answers. Because the ref-\nerence parts often occupies much of sequence length, we notice\nthat one-shot learning can surpass few-shot learning in terms of\nanswer\u2019s quality in most time. Hence we finally choose to inference\nwith one-shot demonstration sample as shown in Figure 5 (c), and\nfinally 83k various queries and their answers have been collected.\nWe record the details of choosing prompts and instructions in\nAppendix C.\n3.2.2\nCitation Correction\nWe have produced a large amount of well-written quoted long-\nformed answers using GPT-3 in-context learning. However, in our\nexamination, we observe that the answers sometimes cite the wrong\nor invalid (i.e., nonexistent) references in their citation numbers.\nAs a result, to correct the citation relationships are crucial for the\nquality of WebGLM-QA dataset.\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nLiu and Lai and Yu, et al.\nDespite the fact that the citation numbers can be wrong, the\ncontents quoted in the answer are often correct. Thus we propose\nto amend the citation number according to the quotation similarity\nto references, by splitting an answer into few segments by generated\ncitation numbers and match then to references. For a question \ud835\udc5e,\nour retrieved references are defined as R and our answer can be\ndefined as \ud835\udefc. We define text segments S = {\ud835\udc601,\ud835\udc602, \u00b7 \u00b7 \u00b7 ,\ud835\udc60\ud835\udc5b}, and for\neach pair (\ud835\udc60, \u2207) \u2208 S \u00d7 R, we compute citation match scores \ud835\udc53 (\ud835\udc60,\ud835\udc5f)\nfor \ud835\udc5f \u2208 R. We pick a threshold \ud835\udc47, and the final citation \ud835\udc5f for each\nsegment (\ud835\udc60, \u2207) \u2208 \ud835\udefc can be described as:\n\u2207\ud835\udc56 = {\ud835\udc5f|\ud835\udc53 (\ud835\udc60\ud835\udc56,\ud835\udc5f) \u2265 \ud835\udc47 },\ud835\udc5f \u2208 R\nFor our application, we finally adopt Rouge-1 score as the \ud835\udc53 and\nthe threshold \ud835\udc47 selection is introduced in the Section 3.2.3.\n3.2.3\nFiltering\nAfter correction, we further investigate more issues that could\npotentially influence the dataset quality. And in short, we discover\nthat most of them are related or could be solved via checking the\ncitation quality. We will discard a piece of generated sample if it\npresents any problems in the following:\n\u2022 Hallucination [13]: the answer leverages the internal knowl-\nedge of LLMs instead of references, which is not factual-grounded\nand sometimes severely wrong. It can be identified via the low\noverlapping ratio between all references and the answer.\n\u2022 Few citations: when an answer cites too few of the provided\nreferences, it usually presents poor reference relevance and thus\noften not informative and factual-grounded enough.\n\u2022 Low citation accuracy: if an answer have too many wrong\ncitation numbers, we assume it as a low-quality one.\nWe calculate the F1 for the similarity and overlapping calculation.\nWe test Rouge-L (whose best threshold is 0.4) and Rouge-1 (whose\nbest one is 0.57) on a set of manually checked samples, and find\nthat Rouge-1 is better. It is due to the fact that LLMs would often\nrewrite and paraphrase the reference contents including exchang-\ning phrase orders. In that case, a high-quality answer may hold a\nhigh informative Rouge-1 score, but a low Rouge-L score, which\ncomputes the longest common subsequence co-occurrence.\nAfter all the filtering conditions mentioned above, the number of\nsamples drops from 83k to 45k, which becomes a high quality quoted\nlong-formed QA dataset for web-hanced QA system training. We\ntrain the GLM [6], a type of bidirectional LM that is pre-trained on\nautoregressive blanking infilling (including a 10-billion-parameter\nand a 2-billion-parameter one), over the WebGLM-QA as our back-\nbone generator.\n3.3\nHuman Preference-aware Scorer\nIn preliminary testing, our bootstrapped generator under beam-\nsearch decoding strategy already performs satisfyingly in many\ncases. However, recent literature [24, 26, 33] demonstrates that\naligning human purposes and preference to LLMs are crucial for\nexpert-level text generation. WebGPT reports to recruit many ex-\nperts to provide comparison and ranking over generated answers\nand make use of the feedback to train a reward model (RM) for pick-\ning best-of-n (i.e., 16/32/64) generated candidates and additionally\noptimize the generator via reinforcement learning (RL).\nNevertheless, such expert annotations could be expensive to\nacquire and the RL would consume much computation resource.\nIn this work, as a competitive substitute, we propose to build a\nhuman preference-aware scorer based on massive user feedback\n(e.g., thumb-ups) from online QA forums. Under appropriate designs\nand elaborate data cleaning, we show in our experiments that such\nscorer also significantly improve the alignment-level of answers\nand the scoring in real human evaluation.\nData collection and preprocessing. We first collect QA pairs and\ncorresponding user thumb-ups from online QA forums. Despite\ntheir diversity, these answers are of so various lengths and qual-\nities that the scorer would learn little from them without proper\npreprocessing.\nOur preprocessing includes the following requirements:\n\u2022 High quality feedback: we define the answer with more than\n3 thumb-ups as an answer with valid feedback. We pick out\nquestions with 8 or more valid answers as qualified ones.\n\u2022 Length-bias mitigation: we notice that the score prefers longer\nanswers rather than the better ones in preliminary study, as is\nalso indicated in literature [26, 33]. To mitigate the bias, for each\nqualified question, we use the median length \ud835\udc65 of all the answers\nas the threshold to truncate longer answers and discard those\nlengths are less than \ud835\udc65/2.\n\u2022 Contrast augmentation: after sorting the answers by their\nthumb-ups, the gaps between neighboring answers turn out nar-\nrow. Scorers trained on such uninformative dataset present poor\nperformance. To increase the contrast between answers for com-\nparison training, we select a pair of answers of more than 5 in\nrank positions. In each pair, the answer with greater amount of\nlikes is the better response.\nAfter our prepossessing, there are 93k questions and 249k com-\nparison pairs in total, with 230k pairs as the training set and 19k\npairs as the test set. Next, we introduce the implementation details\nfor training our human preference-scorer. The backbone model for\ntraining scorer is a 6-billion-parameter GLM.\nSupervised fine-tuning (SFT). In SFT step, we leverage the Reddit\nTL; DR dataset for first fine-tuning the scorer following [33]. We\ntrain 16 epochs with cosine learning rate decay and 2.83e-5 as\nbeginning learning rate. We use the SFT model for initialization of\ncomparison training.\nComparison training. We pass pairs of comparison data to the\nmodel to yield a scalar score for each of the question-answer pair\nand maximize the gap between their scores. We use a linear head\nwith the input dimension of hidden size and the output dimension\nof 1 to produce the score.\nDuring the training, we find that the scorer tends to overfit\nquickly. Therefore, we freeze first 70% transformer layers and lever-\nage other techniques such as dropouts and large batch size for\nregularization. Notwithstanding, the scorer would overfit after 1-\n1.5 epochs anyway. After the training completes, we calibrate its\npredictions to standard normal distribution based on the training\nset reward distribution.\nWebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\n4\nHuman Evaluation Criteria\nAutomatic metrics to score model-generated answers can perform\nwell in terms of short-formed ones. However, for open-domain\nlong-formed QA with references, the answers and rationales can\nbe subjective and versatile, especially for those questions that start\nwith \"HOW\" and \"WHY.\" As a result, human evaluation is vitally\nneeded, for which there have been many studies [4, 31].\nTo evaluate WebGLM and appropriately compare it to other\nsimilar models, we introduce a human evaluation criteria system\nto evaluate both references and answers. We adopt both binary\n(for those objective metrics, e.g., truthfulness) and four-level score\n(for those subjective metrics, e.g., fluency) balancing objectivity\nand scale in human evaluation. The four-level score is applied as is\nsuggested in the literature that it avoid human annotators to keep\nabsolutely neutral [31]. For each criterion we mention below, an\narrow follows. up arrow (\u2191) means higher score performs better,\nwhile down arrow (\u2193) denotes lower score performs better.\n4.1\nReference Evaluation\nIn this section, we introduce human evaluation criteria on refer-\nences. The evaluation is done on per question-reference pair.\nRelevancy ([0, 3], \u2191). For retrieved documents or references related\nto a question, the more related, the higher relevancy score should\nbe. Specifically, different references to a question can share high\nrelevancy scores simultaneously.\nDensity ([0, 3], \u2191). To evaluate how much useful information is in\na piece of reference, we need to estimate its information density.\nBoth relevancy and density are criteria to evaluate informa-\ntiveness, but there is difference between them. Relevancy can be\nregarded as a \"recall metric\" for informativeness, while density can\nbe regarded as a \"precision metric\".\nTruthfulness ([0, 1], \u2191). Retrieved references can be factually\nwrong even they are closely associated to the question. It is be-\ncause the web information sources are open and could contain\nuser-submitted information without correctness check. As a result,\nthe truthfulness of a piece of reference should be evaluated, and its\nevaluation does not consider the question.\nToxicity ([0, 1], \u2193). Web texts could involve violent, pornographic,\noffensive words or other improper elements. Thus, it is necessary\nto assess toxicity of references retrieved.\nSocial Bias ([0, 1], \u2193). Potential biases on the internet could related\nto genders, races, nations, and ages. We should also exclude them\nfrom our system.\n4.2\nAnswer Evaluation\nIn this section, we introduce human evaluation criteria on answers,\nwhich are evaluated triple-wise (i.e., (question, answer, references)).\nFluency ([0, 3], \u2191). Fluency measures the quality of generated\ntext itself only, without taking questions and references into ac-\ncount [4]. It concerns only elements such as grammar, word, and\nphrase choices that are affiliated to the language aspect.\nCorrectness ([0, 3], \u2191). Correctness measures the coherence of\nthe answer and its corresponding question. If an answer solves\nthe question satisfyingly, we say it holds a high correctness. Addi-\ntionally, when we score the correctness of an answer, we should\ntake factual consistency into account. For example, contradicting\ncommon sense or defying logic will decrease the correctness.\nCitation Accuracy ([0, 3], \u2191). The metric only considers the rela-\ntionships between an answer and its references. When an answer\ncontains citation marks, we should check if it is correct. Citation\nmistakes or missing citation will both decrease the accuracy.\nTruthfulness ([0, 1], \u2191). Similar to truthfulness in the reference\nevaluation, truthfulness of an answer measures whether the text\nof the answer is factually sound, including the factual consistency\nof the answer and whether the answer contains contradictions or\nhallucinate information.\nObjectivity ([0, 1], \u2191). The metric only concerns the relationships\nbetween an answer and its references. When references provided,\nmodels are supposed to generate answers according to these refer-\nences without its using its latent knowledge from pre-training. If we\ncan find all the information of an answer from provided references,\nwe say it is objective.\nRedundancy ([0, 1], \u2193). Within the limited text length, duplicate\ncontent will reduce informativeness. As the lower redundancy, the\nhigher quality of the answer, we take it into our consideration.\nThe detail of the metrics and the meaning of the score can be\nfound in the Appendix H.\n5\nExperiment\nIn this section, we conduct experiments employing the metrics\nmentioned in Section 4 to evaluate and analyze the quality of the\nresponses generated, including those from WebGLM and other\nsimilar systems. We also report quantitative ablation studies on\ncertain components in WebGLM.\n5.1\nMain Results\nWe conduct the major evaluation using the 272 questions provided\non WebGPT [24] demo website2, as the WebGPT is not publicly\navailable and selected questions are generally complicated and\ncloser enough to real human questions.\nHuman Evaluation Setup. We recruited 15 master-degree level\nexperts to conduct human evaluation. For each question, we aggre-\ngate all the search results and answers from different models into\none table, enabling the annotators to effectively compare them and\nunify the annotation standards. We evaluate the performance of our\nmodel and other different models from various dimensions through\nhuman evaluation. We also compare and analyze the results from\ndifferent perspectives as follows. The main results are shown in\nTable 2.\nWebGLM Reference vs Other References. Although the search\nresults of WebGLM are slightly inferior to WebGPT-175B, its per-\nformance is far better than that of Perplexity.ai and WebGPT-13B. It\nis worth mentioning that the WebGLM retrieval process only uses\nsome traditional, word-based algorithms and two Contrievers with\na cumulative parameter amount of no more than 300M. WebGLM\nis significantly superior to WebGPT in computing performance and\n2https://openaipublic.blob.core.windows.net/webgpt-answer-viewer/index.html\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nLiu and Lai and Yu, et al.\nTable 2: Main results based on human evaluation metrics. Human evaluation results of generations on questions provided on the\nWebGPT demo website. For reference evaluation, Rel., Den., Tru., Tox\u2193., and Soc. Bias\u2193 are the abbreviations corresponding to Relevancy,\nDensity, Truthfulness, Toxicity, and Social Bias. For answer evaluation, Flu., Cor., Cit. Acc., Obj., Tru., Red.\u2193 correspond to Fluency, Correctness,\nCitation Accuracy, Objectivity, Truthfulness, and Redundancy.\nModel\nReference Evaluation\nAnswer Evaluation\nRel.\nDen.\nTru.\nTox.\u2193\nSoc. Bias\u2193\nFlu.\nCor.\nCit. Acc.\nObj.\nTru.\nRed.\u2193\nWebGPT (175B)\n2.512\n2.660\n0.996\n0.015\n0.006\n2.457\n2.889\n2.837\n0.990\n0.975\n0.087\nPerplexity.ai\n1.652\n1.636\n0.955\n0.005\n0.001\n2.718\n2.321\n2.512\n0.726\n0.975\n0.032\nWebGPT (13B)\n1.782\n1.766\n0.998\n0.008\n0.016\n2.692\n2.102\n2.769\n0.974\n0.872\n0.051\nWebGLM (10B)\n1.980\n2.226\n0.983\n0.002\n0.002\n2.829\n2.810\n2.757\n0.943\n0.998\n0.021\ntime consumption. Its performance is far superior to that of the 13B\nmodel and close to that of the 175B model.\nWebGLM vs Other Systems. Finally, we compare our system with\nthe results of WebGPT-13B, Perplexity.ai, and WebGPT-175B. Our\nsystem has achieved the highest performance in fluency, truthful-\nness, and redundancy. At the same time, we are close to WebGPT-\n175B in the correctness metric with a score of 2.81, which is far\nhigher than that of Perplexity.ai and WebGPT-13B, indicating that\nour system can still achieve superior performance at a lower cost.\n5.2\nTuring Test\nTo further compare our performance, we design a Turing test [21]\nto check the answers\u2019 quality.\nSetup. We randomly sampled 200 items from the 272 questions\nthat WebGPT has displayed on their official web page. For each\nquestion, we shuffle the answers generated by WebGLM, WebGPT-\n175B, WebGPT-13B, and Perplexity.ai, and remove citation marks\nfrom them for fairness. We next mix an answer written by humans\ninto these answers and ask evaluators to rank the answers by their\nquality, such as correctness, informativeness, and truthfulness.\nPerplexity.ai\nWebGPT-13B\nWebGPT-175B\nWebGLM\nHuman\nPerplexity.ai\nWebGPT-13B\nWebGPT-175B\nWebGLM\nHuman\n0.50\n0.68\n0.45\n0.45\n0.21\n0.32\n0.50\n0.23\n0.30\n0.18\n0.55\n0.77\n0.50\n0.55\n0.45\n0.55\n0.70\n0.45\n0.50\n0.43\n0.79\n0.82\n0.55\n0.57\n0.50\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nFigure 6: Win rates between systems. Numbers denote the rate\nthat the answers from corresponding source from the first column\nare better than ones from corresponding source from the first row.\nResult. For each pair of answers (\ud835\udc34, \ud835\udc35), if evaluators prefer \ud835\udc34 to\n\ud835\udc35, we call \ud835\udc34 wins and \ud835\udc35 loses. Firstly, we compare each pair of the\nanswers, the win rate is shown in Figure 6. Besides, We calculate\nthe win rates against humans for each system. The result is shown\nin Figure 2. We hold a 43% win rate, definitely beat Perplexity.ai\nwith a 21% win rate and WebGPT-13B with an 18% win rate, and\nalmost draw with WebGPT-175B with a 45% win rate.\n5.3\nTest on QA Benchmarks\nWe randomly sample 400 questions on Natural Question and Web\nQuestion, and evaluate WebGLM and Perplexity.ai on them. The\nresults in Table 3 show that WebGLM outperform Perplexity.ai.\nTable 3: Open QA Performance on NaturalQuestions and We-\nbQuestions. Perplexity.ai is evaluated on sampled subsets because\nthe website prohibits crawling.\nNatural Questions\nWeb Questions\nWebGLM\n60.8\n63.5\nPerplexity.ai (sample)\n57.3\n57.5\nGPT3-175B\n29.9\n41.5\nIn addition, we conducted experiments on the full validation\nsplit of TriviaQA (same as WebGPT). Following the testing method\nemployed by WebGPT, we first generated a long answer for each\nquestion using WebGLM. We then used Google Bigbird, fine-tuned\non the TriviaQA training set3, to answer TriviaQA questions based\non the output of WebGLM. To address potential test-train overlap\nissues mentioned in WebGPT, we also conducted TriviaQA tests on\ndifferent train-test splits. The results are summarized in Table 4.\n5.4\nAblation Study\nIn this section, we study the major improvements and strategies\nin WebGLM, including the bootstrapped dataset filtering, scorer\ntraining, LLM-augmented retriever and some other minor topics.\n5.4.1\nWebGLM-QA Filtering Since we build our training dataset\nbased on LLM in-context bootstrapping, the dataset quality could be\nessential for WebGLM\u2019s success. We randomly sample 210 examples\nfrom these versions of our dataset to verify the filtering strategies\nthey are based on, including 1) None, 2) Rouge-L filtered, and 3)\nRouge-1 filtered.\nWe randomly shuffle all the samples and distribute them to eval-\nuators, and then collect and calculate the average score of each\nmetric. The sample results are shown in Table 5 We analyze this re-\nsult from two perspectives. One is the absolute performance of our\n3https://huggingface.co/google/bigbird-base-trivia-itc\nWebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nTable 4: WebGLM, WebGPT and other comparison methods on TriviaQA. The setting follows WebGPT [24] Appendix G.\nMethod\nTotal\nQuestion\noverlap\nNo question\noverlap\nAnswer\noverlap\nAnswer\noverlap only\nNo overlap\nBigbird + WebGLM (Ours)\n70.80%\n86.40%\n67.10%\n78.70%\n73.60%\n49.30%\nGPT-3 175B\n58.70%\n75.90%\n52.90%\n67.30%\n61.60%\n39.00%\nGPT-3 175B + WebGPT 175B BC\n69.50%\n86.30%\n65.30%\n78.40%\n73.20%\n52.40%\nUnitedQA-E\n68.90%\n89.30%\n62.70%\n78.60%\n70.60%\n44.30%\nUnitedQA (hybrid model)\n70.50%\n-\n-\n-\n-\n-\nTable 5: Ablation study on different dataset filtering strategies in creating the bootstrapped generator.\nFiltering\nMethod\nReference Evaluation\nAnswer Evaluation\nRel.\nDen.\nTru.\nTox.\u2193\nSoc. Bias\u2193\nFlu.\nCor.\nCit. Acc.\nTru.\nObj.\nRed.\u2193\nNone\n1.711\n1.619\n0.991\n0.011\n0.011\n2.872\n2.636\n2.370\n2.810\n0.805\n0.134\nRouge-L\n1.833\n1.728\n0.994\n0.022\n0.010\n2.731\n2.680\n2.573\n2.896\n0.841\n0.181\nRouge-1\n1.832\n1.751\n0.993\n0.010\n0.012\n2.826\n2.694\n2.688\n2.919\n0.890\n0.120\nTable 6: Ablation study on different dataset filtering strate-\ngies, based on GLM-2B\u2019s post-training evaluation\nFlu.\nCor.\nCit. Acc.\nObj.\nTru.\nRed.\u2193\nNone\n2.610\n2.738\n2.655\n0.961\n0.961\n0.063\nRouge-L\n2.604\n2.742\n2.727\n0.952\n0.975\n0.034\nRouge-1\n2.852\n2.738\n2.743\n0.976\n0.970\n0.044\nfinal version of the dataset. The other is comparing the performance\nof our different versions of datasets.\nWe find that our dataset holds a high factual consistency and\ncorrectness, and the majority of our data are judged as perfectly\ncorrect. We have also noticed that the information relevancy and\ndensity are considerably improved when we apply a filter method\nand when we change Rouge-L to Rouge-1. As for the answer, we find\nthat correctness has great improvement when we apply any one\nof the two filters, and factual consistency has a great improvement\nwhen we change the Rouge-L filter to Rouge-1. Besides, objectivity\nis also one of the most important criteria that we care about, and we\nfind that it\u2019s more likely to discard subjective answers with a Rouge-\n1 filter than with a Rouge-L filter. As a result, our experiments show\nthat citation accuracy is closely related to the reference quality and\nanswer quality, so our filter method is effective.\nBesides, We train the GLM-2B models on each dataset and eval-\nuate them with our designed metrics to see the impact of these\ndatasets on our model\u2019s performance. We show the results in Ta-\nble 6. We find that the answers of the three models showed little\ndifference in the correctness metric. However, the performance\nof the model trained by rouge-1 was better in fluency, citation\naccuracy, and objectivity metrics. This result further proves the\nadvantages of the dataset of rouge-1. Therefore, we decide to train\nour 10B model on the dataset of rouge-1.\n5.4.2\nLLM-augmented Retriever In terms of the usefulness of ref-\nerences, we have compared our method with traditional methods\nsuch as BM25, TF-IDF, and the original version of Contriver.\nTable 7: Performance of LLM-augmented Retriever (Ours).\n\u201cN-NDCG\u201d refers to Normalized NDCG.\nMetric(%)\nTF-IDF\nBM25\nContriever\nOurs\nAccuracy\n46.85\n40.33\n18.54\n69.36\nSpearman\n9.92\n-20.94\n-1.58\n62.26\nNDCG\n82.54\n76.28\n81.16\n91.99\nN-NDCG\n46.05\n26.77\n41.75\n75.29\nWe collect 22000 examples from WebGLM-QA, and for each\nquestion, we calculate Rouge-1 precision score \ud835\udc5d of corresponding\nanswer \ud835\udc4e and each of the reference \ud835\udc5f, and then label the reference-\nanswer pair (\ud835\udc5f,\ud835\udc4e) as \ud835\udc5d. Finally, we gain a training dataset containing\n20000 examples and a test dataset containing 2000 examples.\nFor all answers to the same question, we compare the order\npredicted by retrieve methods with the answer relevancy order.\nThe results are shown in Table 7. We notice that before the LLM\ntask augmentation, the Contriever performs even poorer than tradi-\ntional lexical-based approaches. After augmenting knowledge from\nGPT-3\u2019s reference adoption labeling, we find that ours, which holds\na 69.36 pair-wise choosing accuracy and 62.26 spearman index,\nperforms best. The evidence strongly advocates that the LLM aug-\nmentation is vital when we use pre-trained smaller dense retrievers\nin practice.\n5.4.3\nHuman Preference-aware Scorer In this section we compare\nseveral different scorer training strategies and datasets. We discover\nthat proper task formulation and larger and more diverse dataset\nyield better results.\nBaseline and data preprocessing. We first train RoBERTa-large\nin the classification task and the regression task formulation, and\nthe 6-billion-parameter GLM on the ELI5\u2019s training set (with thumb-\nups) as our baselines. In the classification task, we collect all items\nwhose count of answers is not less than 10 from ELI5. For each\ncollected question, we label top-5-voted answers as positive, and\nrandomly pick 5 answers from other questions as negative examples.\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nLiu and Lai and Yu, et al.\n0%\n20%\n40%\n60%\n80%\n100%\nproportion of the ranking in the answers\n0.5\n0.0\n0.5\nscore\nClassification (RoBERTa)\nRegression (RoBERTa)\nRM (Eli5)\nRM (WebGLM)\nFigure 7: Average score of answers in ELI5 test set. It is sorted\nby likes in the ELI5 test set. The best answer is around 0% and the\nworst answer is around 100%.\nIn the regression task, we collect all items whose count of answers is\nnot less than 5 from ELI5. For each collected question, we complete\nthe following steps:\n(1) for each answer to this question, supposing its corresponding\nup-vote is \ud835\udc62, we firstly label this answer as log2 (\ud835\udc62 + 1).\n(2) Then, we scale labels of all answers to this question to [0, 1].\n(3) Let \ud835\udc65 be the summation of the answers\u2019 label, we randomly pick\n\u230a\ud835\udc65\u230b answers from other questions as negative examples with\nlabel \u22121.\nIn order to obtain a large train set (which has been suggested very\nimportant in [33]), we adopt a relatively loose screening method,\nwhich selects the questions with more than 5 answers and answers\nwith no less than 100 words in length. Our large train set includes\n28.2k questions and 191.6k pairs. We use the ELI5 test set with\nthumb-ups for our final evaluations.\nMetrics. We select three metrics to measure the ability of the\nreward model to distinguish responses of different quality, namely\naccuracy, Spearman coefficient, and NDCG (Normalized Discounted\nCumulative Gain). Accuracy refers to the accuracy of selecting\nbetter answers in pairs. Spearman and NDCG measure the sorting\nability of the model.\nThe ranking evaluation of different models is shown in Table 9.\nWe find that WebGLM human preference-aware scorer performs\nbest on accuracy and Spearman coefficient. Under the same amount\nof training tokens, the performance of the reward model is slightly\nworse than that of RoBERTa classification and RoBERTa regression,\nbut after increasing the amount of training, the performance of the\nreward model will increase significantly.\nFigure 7 shows the average reward of the answers at different\npositions in the sequence sorted by likes in the ELI5 test set. The\nbest answer is around 0% and the worst answer is around 100%.\nWe find that the curve of the WebGLM Human Preference-aware\nScorer is more discriminative than other models, and the rewards\nof the best answer are higher than that of others.\n5.4.4\nAblation Study on Each Component We added some exper-\niments to conduct ablation studies on each component. We com-\npared the three sub-modules of the system: Retriever, Generator,\nand Scorer. The results are shown in Table 8.\nIn the Retriever module, we compared the performance on the\nsettings of WebGPT-175B, WebGLM, and non-retrieval. From the\nTable 8, the performance on WebGLM retrieval is similar to that of\nWebGPT-175B and significantly better than non-retrieval.\nRegarding the Generator module, we compared the response\nquality of WebGLM and GPT-3 on WebGLM retrieval setting. We\nfound that WebGLM performed slightly better than GPT-3 in flu-\nency, correctness, accuracy, citation accuracy, objectivity, and truth-\nfulness.\nIn terms of Scorer, we compared the response quality of We-\nbGLM removing and retaining Reward Models. The results show\nthat by WebGLM-10B top-p sampling and reward model scoring\nmethod, We found through the human evaluation results that the\nanswers scored high by the reward model excel the original re-\nsults in fluency, correctness, citation accuracy, truthfulness, and\nredundancy. It shows the importance of the reward model scoring\nmechanism to model performance.\n6\nConclusion\nWe build the LLM-based question-answering system\u2014WebGLM\u2014\nwith a web retrieval method. We propose a fast and cost-effective\nmethod to retrieve valuable information from the Internet. We lever-\nage GPT-3\u2019s in-context learning ability to build a LLM-bootstrapped\nquoted and long-form QA dataset, which is used to train our model.\nFurther, we train a human preference-aware scorer and use it to\ngive marks to responses generated by our model. For each question,\nthe scorer can select the highest-scored response from candidates,\nthus obtaining a final answer humans prefer the most. We con-\nduct extensive experiments, including both the human evaluation\nand the Turing test, to demonstrate the competitive performance\nof WebGLM with some of the pioneering web-enhanced question\nanswering systems like Perplexity.ai and WebGPT.\nACKNOWLEDGEMENT\nThis work is supported by Technology and Innovation Major Project\nof the Ministry of Science and Technology of China under Grant\n2022ZD0118600 and 2022ZD0118601, NSF of China for Distinguished\nYoung Scholars (No. 61825602), NSF of China (No. 62276148), and a\nresearch fund from Zhipu.AI.\nReferences\n[1] Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for\nMT evaluation with improved correlation with human judgments. In Proceedings\nof the acl workshop on intrinsic and extrinsic evaluation measures for machine\ntranslation and/or summarization. 65\u201372.\n[2] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Seman-\ntic parsing on freebase from question-answer pairs. In Proceedings of the 2013\nconference on empirical methods in natural language processing. 1533\u20131544.\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot learners. Advances in neural\ninformation processing systems 33 (2020), 1877\u20131901.\n[4] Asli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao. 2020. Evaluation of text\ngeneration: A survey. arXiv preprint arXiv:2006.14799 (2020).\n[5] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav\nMishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Se-\nbastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways.\narXiv preprint arXiv:2204.02311 (2022).\n[6] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and\nJie Tang. 2022. GLM: General language model pretraining with autoregressive\nblank infilling. In Proceedings of the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers). 320\u2013335.\nWebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nTable 8: Ablation study on different sub-modules (Scorer, Retriever, and Generator) in WebGLM.\nMethod\nFlu.\nCor.\nCit. Acc.\nObj.\nTru.\nRed.\u2193\nScorer Ablation\nNo Scorer\n2.797\n2.757\n2.723\n0.961\n0.970\n0.039\nHuman Preference-aware Scorer (Ours)\n2.829\n2.810\n2.757\n0.943\n0.998\n0.021\nRetriever Ablation (w.o. RM)\nNo Retriever\n2.364\n1.982\n-\n-\n0.645\n0.091\nWebGPT Retriever\n2.750\n2.884\n2.808\n0.981\n0.980\n0.038\nContriever\n2.761\n2.732\n2.721\n0.963\n0.930\n0.043\nLLM-augmented Retriever (Ours)\n2.797\n2.757\n2.723\n0.961\n0.970\n0.039\nGenerator Ablation (w.o. RM)\nGPT-3 (text-davinci-003, zero-shot)\n2.751\n2.752\n2.607\n0.927\n0.966\n0.034\nBootstrapped Generator (Ours)\n2.797\n2.757\n2.723\n0.961\n0.970\n0.039\nWebGLM (Ours)\n2.829\n2.810\n2.757\n0.943\n0.998\n0.021\nTable 9: Different scorers\u2019 performance on ELI5 test set.\nAccuracy\nSpearman\nN-NDCG\nClassification (RoBERTa)\n0.552\n0.129\n0.319\nRegression (RoBERTa)\n0.569\n0.164\n0.352\nRM (ELI5)\n0.568\n0.197\n0.406\nRM (WebGLM)\n0.596\n0.241\n0.367\n[7] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and\nMichael Auli. 2019. ELI5: Long Form Question Answering. In Proceedings of the\n57th Annual Meeting of the Association for Computational Linguistics. 3558\u20133567.\n[8] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020.\nRetrieval augmented language model pre-training. In International conference on\nmachine learning. PMLR, 3929\u20133938.\n[9] Or Honovich, Uri Shaham, Samuel R Bowman, and Omer Levy. 2022. Instruction\ninduction: From few examples to natural language task descriptions. arXiv\npreprint arXiv:2205.10782 (2022).\n[10] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bo-\njanowski, Armand Joulin, and Edouard Grave. 2022. Unsupervised Dense Infor-\nmation Retrieval with Contrastive Learning. Transactions on Machine Learning\nResearch (2022).\n[11] Gautier Izacard and \u00c9douard Grave. 2021. Leveraging Passage Retrieval with\nGenerative Models for Open Domain Question Answering. In Proceedings of the\n16th Conference of the European Chapter of the Association for Computational\nLinguistics: Main Volume. 874\u2013880.\n[12] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni,\nTimo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard\nGrave. 2022. Few-shot learning with retrieval augmented language models. arXiv\npreprint arXiv:2208.03299 (2022).\n[13] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii,\nYe Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in\nnatural language generation. Comput. Surveys 55, 12 (2023), 1\u201338.\n[14] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey\nEdunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-\nDomain Question Answering. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP). 6769\u20136781.\n[15] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur\nParikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton\nLee, et al. 2019. Natural questions: a benchmark for question answering research.\nTransactions of the Association for Computational Linguistics 7 (2019), 453\u2013466.\n[16] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,\nNaman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel,\net al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks.\nAdvances in Neural Information Processing Systems 33 (2020), 9459\u20139474.\n[17] Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries.\nIn Text summarization branches out. 74\u201381.\n[18] Jiachang Liu, Dinghan Shen, Yizhe Zhang, William B Dolan, Lawrence Carin,\nand Weizhu Chen. 2022. What Makes Good In-Context Examples for GPT-3?.\nIn Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on\nKnowledge Extraction and Integration for Deep Learning Architectures. 100\u2013114.\n[19] Xiao Liu, Fanjin Zhang, Zhenyu Hou, Li Mian, Zhaoyu Wang, Jing Zhang, and Jie\nTang. 2021. Self-supervised learning: Generative or contrastive. IEEE Transactions\non Knowledge and Data Engineering 35, 1 (2021), 857\u2013876.\n[20] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and\nJie Tang. 2021. GPT understands, too. arXiv preprint arXiv:2103.10385 (2021).\n[21] Michael L Mauldin. 1994. Chatterbots, tinymuds, and the turing test: Entering\nthe loebner prize competition. In AAAI, Vol. 94. 16\u201321.\n[22] Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022.\nNoisy Channel Language Model Prompting for Few-Shot Text Classification.\nIn Proceedings of the 60th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers). 5316\u20135330.\n[23] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh\nHajishirzi, and Luke Zettlemoyer. 2022. Rethinking the Role of Demonstrations:\nWhat Makes In-Context Learning Work? arXiv preprint arXiv:2202.12837 (2022).\n[24] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina\nKim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al.\n2021. Webgpt: Browser-assisted question-answering with human feedback. arXiv\npreprint arXiv:2112.09332 (2021).\n[25] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan\nMajumder, and Li Deng. 2016. MS MARCO: A Human Generated MAchine\nReading COmprehension Dataset. choice 2640 (2016), 660.\n[26] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022.\nTraining language models to follow instructions with human feedback. Advances\nin Neural Information Processing Systems 35 (2022), 27730\u201327744.\n[27] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a\nmethod for automatic evaluation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Computational Linguistics. 311\u2013318.\n[28] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.\nSQuAD: 100,000+ Questions for Machine Comprehension of Text. In Proceed-\nings of the 2016 Conference on Empirical Methods in Natural Language Processing.\n2383\u20132392.\n[29] Adam Roberts, Colin Raffel, and Noam Shazeer. 2020. How Much Knowledge\nCan You Pack Into the Parameters of a Language Model?. In Proceedings of the\n2020 Conference on Empirical Methods in Natural Language Processing (EMNLP).\n5418\u20135426.\n[30] Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2022. Learning To Retrieve\nPrompts for In-Context Learning. In Proceedings of the 2022 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies. 2655\u20132671.\n[31] Ananya B Sai, Akash Kumar Mohankumar, and Mitesh M Khapra. 2022. A survey\nof evaluation metrics used for NLG systems. ACM Computing Surveys (CSUR) 55,\n2 (2022), 1\u201339.\n[32] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel\nHesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias\nGall\u00e9, et al. 2022. Bloom: A 176b-parameter open-access multilingual language\nmodel. arXiv preprint arXiv:2211.05100 (2022).\n[33] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea\nVoss, Alec Radford, Dario Amodei, and Paul F Christiano. 2020. Learning to\nsummarize with human feedback. Advances in Neural Information Processing\nSystems 33 (2020), 3008\u20133021.\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nLiu and Lai and Yu, et al.\n[34] Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui\nZhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, et al. 2022. Selective\nannotation makes language models better few-shot learners. arXiv preprint\narXiv:2209.01975 (2022).\n[35] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2021. An\nExplanation of In-context Learning as Implicit Bayesian Inference. In International\nConference on Learning Representations.\n[36] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding,\nZhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b: An open\nbilingual pre-trained model. arXiv preprint arXiv:2210.02414 (2022).\n[37] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui\nChen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt:\nOpen pre-trained transformer language models. arXiv preprint arXiv:2205.01068\n(2022).\n[38] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi.\n2019. BERTScore: Evaluating Text Generation with BERT. In International Con-\nference on Learning Representations.\n[39] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate\nbefore use: Improving few-shot performance of language models. In International\nConference on Machine Learning. PMLR, 12697\u201312706.\nWebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nA\nAdditional Experimental Results\nA.1\nWebGLM vs Others in WebGPT Reference\nWe compared the generation results of WebGLM-Rouge1, WebGPT-\n175B, and GPT-3 on the WebGPT-175B references. For GPT-3, we\nalso use the method of automatically constructing datasets to gener-\nate responses for the WebGPT samples to compare the effect of the\nWebGLM system. Specifically, we use the references of WebGPT to\nlet GPT-3 do in-context learning to answer questions according to\nthe search results. We use human evaluation to compare the quality\nof the three answers. The experimental results are shown in Ta-\nble 10. Although our model size is more than ten times smaller than\nGPT-3 and WebGPT-175B, we can effectively compensate for the\nimpact of the model size and achieve competitive performance in\nthe retrieval paradigm. Our model matches WebGPT-175B and GPT-\n3 on correctness, citation accuracy, objectivity, and truthfulness\nmetrics and outperforms them on fluency and redundancy.\nTable 10: Ablation study on different Generators based on\nWebGPT references\nGenerator\nFlu.\nCor.\nCit. Acc.\nbj.\nTru.\nRed.\nGPT-3 In-Context\n2.801\n2.883\n2.726\n0.966\n0.975\n0.024\nWebGPT-175B\n2.457\n2.889\n2.837\n0.990\n0.975\n0.087\nWebGLM-10B-Rouge1\n2.750\n2.884\n2.808\n0.981\n0.980\n0.038\nB\nDetailed Efficiency Analysis\nAt the retrieval stage, we only search for one time, then take the\nfirst few results links to fetch the web pages in parallel. We then\nextract all paragraphs and rank these paragraphs by Contriever,\nand finally take the top 5 paragraphs as references. Let \ud835\udc61\ud835\udc60, \ud835\udc61\ud835\udc53 , \ud835\udc61\ud835\udc52,\nand \ud835\udc61\ud835\udc5f denote the time we consume in four steps, so the total time\nwe consume is \ud835\udc61\ud835\udc60 + \ud835\udc61\ud835\udc53 + \ud835\udc61\ud835\udc52 + \ud835\udc61\ud835\udc5f.\nWebGPT simulates the operations in a virtual browser environ-\nment while obtaining references. For the 272 questions they showed,\nwe count the types of actions and the average number of generated\ntokens as shown in Table 11 and 12. Then we calculate the average\ntime it takes to browse. Assuming that the total time \ud835\udc40 (\ud835\udc40 is either\nWebGPT-175B or WebGPT-13B) takes to generate commands in the\nbrowsing process of each question is expected to be \ud835\udc61\ud835\udc50 (\ud835\udc40), the time\n\ud835\udc40 consumes \ud835\udc47 (\ud835\udc40) satisfies the following equations.\n\ud835\udc47 (WebGPT-175B) = \ud835\udc61\ud835\udc50 (WebGPT-175B) + \ud835\udc61\ud835\udc60 \u2217 3.82 + \ud835\udc61\ud835\udc53 \u2217 6.96 (2)\n\ud835\udc47 (WebGPT-13B) = \ud835\udc61\ud835\udc50 (WebGPT-13B) + \ud835\udc61\ud835\udc60 \u2217 4.05 + \ud835\udc61\ud835\udc53 \u2217 7.56\n(3)\nWe test the efficiency of GPT-3. With a 500-token prompt, the\n175B model generates about 20 tokens per second, and the 13B\nmodel generates 100 tokens per second, meaning that:\n\ud835\udc61\ud835\udc50 (WebGPT-175B) = 580.08 tokens/query\n20 tokens/second\n= 29 seconds\n(4)\n\ud835\udc61\ud835\udc50 (WebGPT-13B) = 580.08 tokens/query\n100 tokens/second = 5.8 seconds\n(5)\nIn practice, \ud835\udc61\ud835\udc60, \ud835\udc61\ud835\udc53 , \ud835\udc61\ud835\udc52, and \ud835\udc61\ud835\udc5f are about 1.81, 2.38, 0.29, and 0.89\nrespectively. So we consume 5.36 seconds for one query on average.\nNevertheless, assuming in the same network circumstance, the time\nconsumption of WebGPT models can be calculated by Equation 2\nand 3.\n\ud835\udc47 (WebGPT-175B) = 52.48 seconds\n(6)\n\ud835\udc47 (WebGPT-13B) = 31.12 seconds\n(7)\nTherefore, WebGPT-175B costs 52.48 seconds, and WebGPT-13B\ncosts 31.12 seconds. Our efficiency can be about 10 times that of\nWebGPT-175B and 6 times that of WebGPT-13B.\nTable 11: Efficiency statistics for browsing stage in WebGPT-\n175B. Average count per query, tokens per action, and tokens\nper query (the product of the first two terms) are displayed\nin this table.\naction\ncount/query\ntokens/action\ntokens/query\nsearch\n3.82\n9.80\n37.46\nclick_link\n6.96\n5.00\n34.82\nquote\n3.49\n124.49\n434.80\nback\n5.35\n1.00\n5.35\nscroll_down\n11.41\n4.00\n45.63\nscroll_up\n1.62\n4.00\n6.49\ntop\n0.49\n1.00\n0.49\nend\n0.43\n3.00\n1.29\nfind_in_page\n0.13\n5.11\n0.68\ninvalid\n0.12\n111.09\n13.07\ntokens\n580.08\ngenerating speed\n20 tokens/second\naction time\n29s\ntotal time\n52s\nTable 12: Efficiency statistics for browsing stage in WebGPT-\n13B. Average count per query, tokens per action, and tokens\nper query (the product of the first two terms) are displayed\nin this table.\naction\ncount/query\ntokens/action\ntokens/query\nsearch\n4.05\n9.65\n39.08\nclick_link\n7.56\n5.00\n37.81\nquote\n3.44\n125.85\n433.08\nback\n5.90\n1.00\n5.90\nscroll_down\n10.30\n4.00\n41.21\nscroll_up\n2.01\n4.00\n8.04\ntop\n0.32\n1.00\n0.32\nend\n0.44\n3.00\n1.33\nfind_in_page\n0.21\n5.04\n1.06\ninvalid\n0.10\n136.58\n13.06\ntokens\n580.89\ngenerating speed\n100 tokens/second\naction time\n5.8s\ntotal time\n31s\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nLiu and Lai and Yu, et al.\nC\nChoice of Prompts and Instructions\nFirstly, we attempt the zero-shot approach for bootstrapping data.\nTo produce data with appropriate citation marks, we require specific\ninstructions. We experiment with several methods, however, they\nare all limited in their effectiveness.\nUse a mark for each helpful reference you cited,\nsuch as [1]. Limitation: bootstrapped data contain mixed usage\nof [1][2] and [1, 2].\nUse a mark for each helpful reference you cited, such\nas [1]. If there are multiple citations at one position,\nplease use a format like [1][2][3]. Limitation: bootstrapped\ndata contain citations of useless references.\nUse a mark for each helpful reference you cited, such\nas [1]. If there are multiple citations at one position,\nplease use a format like [1][2][3]. If a reference is\nuseless, do not cite it. Limitation: useless references are still\ncited. This method do not work.\nWe then select few-shot context to bootstrap data. If we provide\ntoo many references or in-context examples, it is easy to exceed the\ntoken count limit. Therefore, we choose to use an 1-shot example\nand 5 references. We also include some useless references in the\nexample, which are not cited in the answer.\nAfter that, We conduct experiments on prompts and demonstrate\nthat placing the question after the references is the most effective\napproach.\nRegarding instruction induction for in-context learning, we ex-\nperiment with the previously mentioned examples as well as some\nnew ones, such as:\nAnswer the question based on the following references\nwith citations. Use a mark for each helpful reference\nyou cited, such as [1]. If there are multiple citations\nat one position, please use a format like [1][2][3]. If\na reference is useless, do not cite it.\nI will provide you with some references. Based on the\nreferences, please answer my question. Pay attention\nthat you should be objective, and you should not use\nyour knowledge. Use a mark for each helpful reference\nyou cited, such as [1]. If there are multiple citations\nat one position, please use a format like [1][2][3]. If\na reference is useless, do not cite it.\nHowever, these instructions are too verbose, and in the presence\nof examples, the model\u2019s performance is not significantly impacted\nby the instructions. Therefore, we adopt a more natural approach\nto generate instructions[9] to produce a natural instruction that is\ninterpretable by the model.\nFinally, we use a very concise instruction: Read the references\nprovided and answer the corresponding question.\nIn addition, we compared models trained with different prompt\nstrategies, and the results are shown in the Table 13. From the\n\"Correctness\" column, we can see the significant difference that the\norder of references and question in the prompt makes.\nD\nDataset Examples\nAn example of WebGLM-QA is shown in Table 14.\nTable 13: The performance with training data bootstrapped\nby difference prompt strategies.\nPrompt\nFlu.\nCor.\nCit. Acc.\nObj.\nTru.\nRed.\nWebGLM Prompt\n2.797\n2.757\n2.723\n0.961\n0.970\n0.039\nQuestion before Reference\n2.633\n2.518\n2.700\n0.933\n0.970\n0.058\n3-Reference\n2.658\n2.412\n2.819\n0.933\n0.930\n0.065\nE\nRetrieval Example\nAn example of retrieved references from each system is shown in\nTable 15 and Table 16.\nF\nAnswer Examples\nSome examples consisting of only answers are shown in Table\n17 and Table 18. We remove citation marks for all the answers\nto evaluate answers only. In this example, WebGLM\u2019s answer is\nconsistent with the question, while the answers of two WebGPT\nmodels are beside the point in the conclusion.\nG\nReward Model Example\nWebGLM performs better after implementing the reward model.\nAn example is shown in Table 19.\nH\nCriteria Details\nThe criteria details of human evaluation are shown in Table 20 and\nTable 21.\nFigure 8: WebGLM web demo page\nI\nDemo\nWe provide a web demo as shown in Figure 8. Meanwhile, some\nreal examples are shown in Figure 9 and the later figures.\nWebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nTable 14: Dataset Example. The fifth reference is not cited because it is unrelated to the question(dirty data). In the training\nprocedure, we need these unrelated references to train our model to pick related information.\nQuestion\nWhy were the cities that are capital cities chosen as capitals?\nReferences\n[1] Reasons differ from state to state as to why the capital city was chosen. In some cases, state officials wanted the capital\nlocated in the geographic center of their states. Jefferson City and Pierre were chosen as capitals of Missouri and South Dakota\nrespectively for this reason. In other instances, a capital site may be picked because of its location relative to other cities.\nFlorida chose the city of Tallahassee to be its capital in 1824 because it was situated halfway between St. Augustine and\nPensacola, the state\u2019s two largest cities at the time. Some capital locations were picked because they were centers of trade and\ntransportation. This is why Albany, located near the Hudson River, was picked as New York\u2019s capital.\n[2] - The selection or founding of a \"neutral\" capital city, one unencumbered by regional or political identities, was meant to\nrepresent the unity of a new state when Ankara, Bern, Bras\u00edlia, Canberra, Madrid, Ottawa and Washington became capital\ncities. Sometimes, the location of a new capital city was chosen to terminate actual or potential squabbling between various\nentities, such as in the cases of Bras\u00edlia, Canberra, Ottawa, Washington, Wellington and Managua.\n[3] - However a country decides on its capital, that city becomes an important symbol of nationhood. While being a home to\nits inhabitants and a place for tourists to visit, it is also the city chosen to represent the entire country to the world.\n[4] State capitals are important because they house the governments of their states, with a meeting place for the state legislature.\nMany meeting buildings are called capitols, which are distinguished from capitals in that capitol refers to buildings while\ncapital always references a city. Cities are chosen to be the capital for differing reasons, sometimes because it was located in\nthe center of the state, like Jefferson City, or was a center of commerce, like Albany. Today, many capitals are not the largest\ncity in their states and can be overshadowed by bigger cities, but retain important historical value.\n[5] (Not Cited) 8. \"Capital cities: How are they chosen and what do they represent?\". BBC News. 6 December 2017. Archived\nfrom the original on 23 March 2022. Retrieved 26 June 2022.\nAnswer\nThe reasons why cities are chosen as capitals vary from state to state. In some cases, state officials wanted the capital located in\nthe geographic center of their state[1][4], while in other instances, a capital site may be picked because of its location relative\nto other cities[1]. Some capital cities were picked because they were centers of trade and transportation[1], or to represent the\nunity of a new state[1][2][4]. In some cases, the location of a new capital city was chosen to terminate actual or potential\nsquabbling between various entities[1][2]. Cities are chosen to be the capital because they house the governments of their\nstates, with a meeting place for the state legislature[1][4]. Today, many capitals are not the largest city in their states and can\nbe overshadowed by bigger cities, but retain important historical value[4]. The capital city also represents the entire country\nto the world[3].\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nLiu and Lai and Yu, et al.\nTable 15: Retrieved Reference Example (WebGLM and Perplexity AI)\nQuestion\nWhat is the difference between sigma bonds and pi bonds?\nReferences\nfrom\nWebGLM\n[1] Sigma bonds are often formed by the combination of s orbitals in different atoms whereas pi bonds are formed from\nthe combination of p and similar orbitals in different atoms.\n[2] The main differences between sigma and pi bonds has to do with their formation and strength. Sigma bonds are formed\nby direct head to head overlap between orbitals while pi bonds are formed by side to side overlap, usually between p\norbitals. This difference in formation leads to a difference in strength. Sigma bonds are stronger than pi bonds because the\ndirect head to head overlap provides a bigger (and therefore stronger) overlap than the side to side overlap of pi bonds.\nAdditionally, sigma bonds form single bonds and can exist without a pi bond present; however, a sigma bond must already\nbe formed for a pi bond to form.\n[3] Sigma and pi bonds are two types of covalent bonds formed by the overlapping of atomic orbitals. Sigma bonds form\nby the direct head to head overlap of atomic orbitals and can occur between s-s, p-p and s-p orbitals. Pi bonds form by the\nside to side overlap of p orbitals.\n[4] The sigma bond is stronger than the Pi bond, according to quantum mechanics, since the orbital paths of Pi bonds are\nparallel, resulting in less overlap between the p-orbitals. Two atomic orbitals come into contact with each other through\ntwo areas of overlap, forming Pi bonds.\nReferences\nfrom\nPerplexity AI\n[1] Sigma bond is a chemical bond formed by the linear or co-axial overlapping of the atomic orbitals of two atoms. A pi\nbond is a type of covalent bond that exists between atoms where the electrons are on top and bottom of the axis connecting\nthe nuclei of the joined atoms.\n[2] The sigma bond is stronger than the Pi bond, according to quantum mechanics, since the orbital paths of Pi bonds are\nparallel, resulting in less overlap between ...\n[3] A \ud835\udf0e bond has cylindrical symmetry; a \ud835\udf0b bond has a nodal plane that includes the bond axis. ... A \ud835\udf0e bond comes from\nthe end-to-end overlap of the ...\nWebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nTable 16: Retrieved Reference Example (WebGPTs)\nQuestion\nWhat is the difference between sigma bonds and pi bonds?\nReferences\nfrom\nWebGPT-175B\n[1] Pi bonds are formed by the sidewise positive (same phase) overlap of atomic orbitals along a direction perpendicular to\nthe internuclear axis. During the formation of \ud835\udf0b bonds, the axes of the atomic orbitals are parallel to each other whereas\nthe overlapping is perpendicular to the internuclear axis. This type of covalent bonding is illustrated below. [Image: Pi\nBonds] Pi Bonds are generally weaker than sigma bonds, owing to the significantly lower degree of overlapping. Generally,\ndouble bonds consist of one sigma and one pi bond, whereas a typical triple bond is made up of two \ud835\udf0b bonds and one \ud835\udf0e\nbond. It is important to note that a combination of sigma and pi bonds is always stronger than a single sigma bond.\n[2] In chemistry, sigma bonds (\ud835\udf0e bonds) are the strongest type of covalent chemical bond. They are formed by head-on\noverlapping between atomic orbitals. Sigma bonding is most simply defined for diatomic molecules using the language and\ntools of symmetry groups. In this formal approach, a \ud835\udf0e-bond is symmetrical with respect to rotation about the bond axis.\nBy this definition, common forms of sigma bonds are \ud835\udc60 +\ud835\udc60, \ud835\udc5d\ud835\udc67 + \ud835\udc5d\ud835\udc67,\ud835\udc60 + \ud835\udc5d\ud835\udc67\ud835\udc4e\ud835\udc5b\ud835\udc51\ud835\udc51\ud835\udc672 +\ud835\udc51\ud835\udc672 (where z is defined as the axis of the\nbond or the internuclear axis). Quantum theory also indicates that molecular orbitals (MO) of identical symmetry actually\nmix or hybridize. As a practical consequence of this mixing of diatomic molecules, the wavefunctions \ud835\udc60 + \ud835\udc60\ud835\udc4e\ud835\udc5b\ud835\udc51\ud835\udc5d\ud835\udc67 + \ud835\udc5d\ud835\udc67\nmolecular orbitals become blended. The extent of this mixing (or hybridization or blending) depends on the relative\nenergies of the MOs of like symmetry.\n[3] A sigma bond is stronger than a pi bond. The reason is that the overlapping of atomic orbitals can take place to a\ngreater extent during the formation of a sigma bond, whereas overlapping of orbitals occurs to a smaller extent during\nthe formation of a pi bond. A pi bond between two atoms is formed only in addition to a sigma bond. The reason is that\nthe atoms constituting a single bond prefer to form a strong sigma bond rather than a weak pi bond. Thus, a pi bond is\nalways present in molecules with multiple bonds, i.e., double or triple bonds. In other words, a single bond cannot be a pi\nbond. There can be free rotation of atoms around the sigma bonds. Free rotation of atoms around pi bonds is not possible\nbecause it involves breaking the pi bonds.\nReferences\nfrom\nWebGPT-13B\n[1] What is the difference between Sigma Bond and pi Bond? \u2022 Sigma bonds are formed by head to head overlapping of\norbitals, whereas pi bonds are formed by the lateral overlapping. \u2022 Sigma bonds are stronger than pi bonds. \u2022 Sigma bonds\ncan be formed between both s and p orbitals whereas pi bonds are mostly formed between p and d orbitals. \u2022 The single\ncovalent bonds between atoms are sigma bonds. When there are multiple bonds between atoms, pi bonds can be seen. \u2022\npi bonds result in unsaturated molecules. \u2022 Sigma bonds allow free rotation of atoms whereas pi bonds restrict the free\nrotation.\n[2] A sigma bond (\ud835\udf0e bond) is a bond formed by the overlap of orbitals in an end-to-end fashion, with the electron density\nconcentrated between the nuclei of the bonding atoms. A pi bond (\ud835\udf0b bond) is a bond formed by the overlap of orbitals in a\nside-by-side fashion with the electron density concentrated above and below the plane of the nuclei of the bonding atoms.\n[3] In a sigma bond, the electrons are in orbitals between the nuclei of the bonding atoms (in the plane of electron density),\nwhere electron density is greatest. The end-to-end axial overlap of electrons attracts the nuclei to form a strong bond. In a\npi bond, the p-orbitals overlap side-to-side above and below the nuclei (equatorial plane), so this attraction is more of a\n\"sideways\" pull as opposed to a direct line of pulling. Additionally, the election density is spread out over a greater area.\nThus, sigma bonds are stronger than pi bonds.\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nLiu and Lai and Yu, et al.\nTable 17: Answer Example (1)\nQuestion\nWhen electronics show black, the pixels are not off. Wouldn\u2019t that use less energy, and show truer black?\nWebGLM\ndisplaying black on OLED screens does use less power than displaying white, because the LEDs that make up each pixel\nare off when displaying black. However, this is not the case for LCD monitors, where white is actually slightly more\nenergy efficient than black. This is because LCD monitors rely on an array of thin-tube fluorescent bulbs that provide a\nconstant source of light to create a white screen, and to make it black, LCDs rely on a diffuser to block this light. Therefore,\ndisplaying black does not necessarily use less energy, and may not even show truer black than white.\nWebGPT-175B\nMost computer and mobile device displays do not use black pixels to save energy. Instead, they use an array of thin-tube\nfluorescent bulbs to provide a constant source of light to create a white screen. To make it black, LCDs rely on a diffuser to\nblock this light. As a result, LCDs use more energy than CRTs to display a black screen. Measuring a 17-inch (43-centimeter)\nLCD monitor, Schindler found that white required 22.6 watts, while black came in a tad higher at 23.2 watts. With a 20-inch\n(50.8-centimeter) LCD, black required 6 percent more energy than white. In contrast, AMOLED displays don\u2019t have a\nbacklight at all. Instead, each pixel is like a tiny red, blue, or green colored light. If the screen is instructed to display black,\nit doesn\u2019t need to block any light, it simply doesn\u2019t light up any of the little colored sub-pixels. As a result, black pixels\nsave you a lot of power because those pixels can be turned completely off.\nWebGPT-13B\nIt depends on the resting state of the LCD as to whether they require energy to stop light or to allow light to pass through,\"\nHeap explains. \"This is why screen tests show that some CCFL (cold cathode fluorescent lamp) LCD screens save energy\nand some use a bit more. All of the scientific test data we have come across shows a slight saving on black LCD screens,\nwhich suggests that the rest state on many LCD screens does not allow light through.\" Heap also points out that a large\nnumber of Blackle users come from India and South America, where CRTs are still commonly sold. Most LCD devices use\na backlight which is always on. When an LCD is black, it is doing full light output, and then also blocking all of that light\nto create the appearence of black. If a monitor is displaying all-black, most will reduce power to the backlight accordingly,\nprimarily to produce a deeper black, and of course also to save power.\nPerplexity AI\nWhile it is true that black pixels use less energy than white pixels on some displays, such as OLEDs, this is not always the\ncase. On LCDs, for example, black pixels still require power to be lit up. Additionally, the amount of energy used by a pixel\ndepends on its color intensity, so even if a pixel is black it may still use some energy.\nHuman Writen\nIt\u2019s not true for all types of screens. For LCD screens, energy cost is not definitely related to brightness. The principle of\nthe LCD screen is that there is a white LED panel at the back, and the front uses the LCD panel to cover the pixels one by\none. Turn down the backlight brightness to save power. The screen is adjusted to internal black, but the LCD panel is\nshielded to black, which will not save power. The power saving has nothing to do with the background color.\nWebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nTable 18: Answer Example (2)\nQuestion\nWhy does it seem like when I watch something the second time around, it goes by faster than the first time I watched it?\nWebGLM\nIt seems like when you watch something the second time around, it goes by faster than the first time you watched it\nbecause you are focusing on the little details instead, which might make it feel like time is going by faster. Another reason\ncould be that when you know what\u2019s coming, your mind breaks it up and you have intervals/milestones, and know when\nit\u2019s going to end, which helps you digest it easier. However, if you go back a third time, it may be ruined because all the\nthings you could discover have been ruined by the second rerun.\nWebGPT-175B\nThere are several different ideas about why this happens. One theory is that when you are cognitively engaged, meaning\nyou are thinking about the task at hand and are busy doing it, you don\u2019t have the opportunity to notice the passage of\ntime. When you\u2019re not engaged, like when you are bored, you have the time to think about the passage of time and so it\nseems like it goes by slower. Another theory is that the chemicals in your brain release dopamine when you are having\nfun, and it impacts how your brain perceives time. When you are having fun and your brain is releasing dopamine, time\nseems to pass faster.\nWebGPT-13B\nThere are two main reasons time seems to pass faster the second time around. First, information our brains absorbs and\nprocesses increases with each viewing, so the more information there is, the slower time goes. Second, as we age, the\nrate at which we process visual information slows down, and this is what makes time \u2018speed up\u2019 as we grow older. The\ncomplexity of the information we examine and the networks of neurons in our brain increase as we age, and electrical\nsignals must traverse greater distances, causing signal processing to take more time. This is what leads to time passing\nmore rapidly.\nPerplexity AI\nThe phenomenon of time seeming to pass faster when watching something for a second time is likely due to the recency\neffect, which amplifies the importance of recent stimuli or observations. Additionally, paying more attention to time\npassing can make it seem like it\u2019s going by faster, as can familiarity with the content. Finally, life-threatening situations,\neye movements, tiredness, hypnosis, age and emotions can all warp our perception of time.\nHuman Writen\nWhen humans repeatedly look at the same thing, the brain will quickly recall the previous memory when it sees the same\ncontent. After people recall quickly in the brain, they can quickly grasp its contents, and feel the time has become faster.\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nLiu and Lai and Yu, et al.\nTable 19: Comparison Example of GLM-10B-RM and GLM-10B-Rouge1\nQuestion\nWhy is it that soap can wash almost anything off of your hands but doesn\u2019t wash its fragrance off of you?\nReferences\n[1] Soap is able to clean hands and dishes because of some pretty nifty chemistry. Soap molecules have on one\nend what\u2019s known as a polar salt, which is hydrophilic, or attracted to water. The other end of the molecule\nis a nonpolar chain of fatty acids or hydrocarbons, which is hydrophobic\u2014meaning that it\u2019s repelled by water\nbut attracted to grease and other oily substances. When you wash your hands, the soap forms something like a\nmolecular bridge between the water and the dirty, germ-laden oils on your hands, attaching to both the oils and\nthe water and lifting the grime off and away. Soaps can also link up with the fatty membranes on the outside of\nbacteria and certain viruses, lifting the infectious agents off and even breaking them apart. Once the oily dirt\nand germs are off your hands, the soap molecules thoroughly surround them and form tiny clusters, known as\nmicelles, that keep them from attaching to anything else while they wash down the drain.\n[2] First, the hydrophilic ends of the soap molecule attach to the water, then the hydrophobic ends attach to the\noils, pathogens, or other debris on the hand. After the soap has bonded with the germs on your hand the water\ncan then wash it down the drain. This works because the soap molecule can attach to the germs more strongly\nthan the germs can hold onto your skin.\n[3] Therefore, when you wash with soap and water the soap molecules grab on to the fat molecules and pulls\nthem off the skin into the rinse water. When your rinse the skin the fat and germs are removed from the skin.\nTherefore, the main reason why soap works to cleanse and sanitise is it literally washes everything off the skin.\nWebGLM-10B-RM\nSoap is able to cleanse and sanitise hands and bodies because of its unique chemistry. Soap molecules have on one\nend a polar salt, which is attracted to water, and on the other end a nonpolar chain of fatty acids or hydrocarbons,\nwhich is hydrophobic and attracted to grease and other oily substances. When you wash your hands, the soap\nforms a molecular bridge between the water and the dirty, germ-laden oils on your hands, attaching to both\nthe oils and the water and lifting the grime off and away.[1] Soap molecules can also link up with the fatty\nmembranes on the outside of bacteria and certain viruses, lifting the infectious agents off and even breaking\nthem apart. Once the oily dirt and germs are off your hands, the soap molecules thoroughly surround them and\nform tiny clusters, known as micelles, that keep them from attaching to anything else while they wash down the\ndrain.[2] Therefore, the main reason why soap works to cleanse and sanitise is it literally washes everything off\nthe skin.[3] However, while soap can wash almost anything off of your hands, it cannot wash its fragrance off of\nyou because fragrances are not dirt and are not removed by rinsing.\nWebGLM-10B-Rouge1\nThe main reason why soap works to cleanse and sanitise is because of its molecules, which have on one end a\npolar salt, which is hydrophilic, or attracted to water, and on the other end a nonpolar chain of fatty acids or\nhydrocarbons, which are hydrophobic, meaning they are repelled by water but attracted to grease and other\noily substances[1]. Soap molecules form a molecular bridge between the water and the dirty, germ-laden oils on\nyour hands, attaching to both the oils and the Water and lifting the grime off and away[1][2]. Then, the soap\nmolecules surround the oily dirt and germs and form tiny clusters, known as micelles, that keep them from\nattaching to anything else while they wash down the drain[1]. However, the fragrance of soap is not removed\nthis way because it is not bound to the fat molecules that are removed from the skin when you wash with soap\nand water[3].\nWebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nTable 20: Criteria Details For References\nRelevancy\nEvaluating how a reference text is related to the question and how much information of the question can be found\nin the reference.\n0\nThe reference is almost not related to the question.\n1\nA few information or answers related to question can be found in the reference.\n2\nMost of the answers to the question can be found in the reference, but a small amount of reasoning or external\ninformation is still needed.\n3\nThe complete answer to the question can be found in the reference\nDensity\nEvaluating the proportion of information related to question in a reference text.\n0\nReference content is almost irrelevant to questions and answers.\n1\nReference has most content irrelevant to the problem, but there is still a small amount of content related to the\nproblem.\n2\nReference has a few irrelevant contents.\n3\nReference is closely related to the problem, and almost the whole reference focuses on the problem.\nTruthfulness\nEvaluating whether the text itself violates common sense, logic or contradictions. There is no need to pay attention\nto whether it is relevant to the question.\n0\nThe text contains statements that are contrary to the facts, and the authenticity is low.\n1\nThe text has no statements contrary to the facts and is highly authentic.\nKeep Blank\nIf the evaluator cannot determine the authenticity through his or her own knowledge, leave it blank.\nToxicity \u2193\nEvaluating whether It involves violence, pornography, indecent words and other similar elements.\n0\nThe above element does not exist.\n1\nOne or more of the above elements exist.\nSocial Bias \u2193\nEvaluating whether there are prejudices against specific social groups, such as racial discrimination and gender\ndiscrimination.\n0\nThe above element does not exist.\n1\nOne or more of the above elements exist.\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nLiu and Lai and Yu, et al.\nTable 21: Criteria Details For Answers\nFluency\nEvaluating whether grammar, spelling, word usage, etc. conform to people\u2019s grammar habits (please do not consider\nany semantic factors).\n0\nThere are major errors in grammar and spelling, which make the text difficult to read.\n1\nThere are small errors in grammar and spelling, which will slightly affect understanding.\n2\nThere are a few grammatical, spelling or case errors that do not affect understanding.\n3\nFluent language, correct grammar, no mistakes, easy to read.\nCorrectness\nEvaluating whether the question is correctly answered.\n0\nNo answer, or the answer is irrelevant or wrong.\n1\nA few answers are given, but they are particularly incomplete or fragmented. The question is basically not answered.\n2\nBasically answer the questions, but there are a few mistakes or omissions.\n3\nAnswer the question perfectly.\nCitation Accuracy\nEvaluating whether the reference marks in the answer are accurate.\n0\nThe reference marks are basically wrong or there is no reference label.\n1\nThere are a large number of missing and wrong marks.\n2\nThere are a few missing and wrong marks.\n3\nThe reference marks are completely accurate.\nObjectivity\nEvaluating whether all the answers come from references.\n0\nThere is external knowledge in the answer which does not come from references.\n1\nAll answers can be based on the reference.\nTruthfulness\nEvaluating whether the text itself violates common sense, logic or contradictions. There is no need to pay attention\nto whether it is relevant to the question.\n0\nThe text contains statements that are contrary to the facts, and the authenticity is low.\n1\nThe text has no statements contrary to the facts and is highly authentic.\nKeep Blank\nIf the evaluator cannot determine the authenticity through his or her own knowledge, leave it blank.\nRedundancy \u2193\nEvaluating whether there is redundancy in the answer, such as repeating the same sentence or the same fact\nrepeatedly.\n0\nThere is no redundancy.\n1\nThere is redundancy.\nWebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nFigure 9: Real Example: How to balance career and hobbies?\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nLiu and Lai and Yu, et al.\nFigure 10: Real Example: FL Studio and Cubase, which is better?\nWebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nFigure 11: Real Example: Is attention better than CNN?\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nLiu and Lai and Yu, et al.\nFigure 12: Real Example: How to survive in the first-tier cities without a high-salary work?\nWebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nFigure 13: Real Example: What do you think of the 3.5 version of Genshin Impact?\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nLiu and Lai and Yu, et al.\nFigure 14: Real Example: transformers are originated from NLP, but why they can be applied in CV?\nWebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nFigure 15: Real Example: Who proposed Music Transformer? How does it work?\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nLiu and Lai and Yu, et al.\nFigure 16: Real Example: What is the backbone of Toolformer?\nWebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nFigure 17: Real Example: Why CyGames succeed? What games have they launched?\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nLiu and Lai and Yu, et al.\nFigure 18: Real Example: When will the COVID-19 disappear?\nWebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nFigure 19: Real Example: Who is the president of United States now?\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nLiu and Lai and Yu, et al.\nFigure 20: Real Example: Tell me about the movie Black Panther 2\nWebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nFigure 21: Real Example: What is Hogwarts Legacy?\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nLiu and Lai and Yu, et al.\nFigure 22: Real Example: What is google bard?\nWebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nFigure 23: Real Example: What is the most popular AI technology in 2023?\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nLiu and Lai and Yu, et al.\nFigure 24: Real Example: Tell me the two teams of NBA all-star in 2023.\nWebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nFigure 25: Real Example: What is copilot?\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nLiu and Lai and Yu, et al.\nFigure 26: Real Example: What is the core technique chatgpt use?\nWebGLM: Towards An Efficient Web-Enhanced Question Answering System with Human Preferences\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nFigure 27: Real Example: Where does the code data used to train copilot come from?\nKDD \u201923, August 6\u201310, 2023, Long Beach, CA, USA.\nLiu and Lai and Yu, et al.\nFigure 28: Real Example: What is the model behind Perplexity AI?\n"
  },
  {
    "title": "TART: A plug-and-play Transformer module for task-agnostic reasoning",
    "link": "https://arxiv.org/pdf/2306.07536.pdf",
    "upvote": "10",
    "text": "Tart: A plug-and-play Transformer module for\ntask-agnostic reasoning\nKush Bhatia\u2020\u2217\nAvanika Narayan\u2020\u2217\nChristopher De Sa\u2021\nChristopher R\u00e9\u2020\n\u2020 Department of Computer Science, Stanford University\n\u2021 Department of Computer Science, Cornell University\n{kushb, avanika, chrismre}@cs.stanford.edu, cdesa@cs.cornell.edu\nAbstract\nLarge language models (LLMs) exhibit in-context learning abilities which enable the same\nmodel to perform several tasks without any task-specific training. In contrast, traditional\nadaptation approaches, such as fine-tuning, modify the underlying models for each specific\ntask. In-context learning, however, consistently underperforms task-specific tuning approaches\neven when presented with the same examples. While most existing approaches (e.g., prompt\nengineering) focus on the LLM\u2019s learned representations to patch this performance gap, our\nanalysis actually reveal that LLM representations contain sufficient information to make good\npredictions. As such, we focus on the LLM\u2019s reasoning abilities and demonstrate that this\nperformance gap exists due to their inability to perform simple probabilistic reasoning tasks.\nThis raises an intriguing question: Are LLMs actually capable of learning how to reason in a\ntask-agnostic manner? We answer this in the affirmative and propose Tart which generically\nimproves an LLM\u2019s reasoning abilities using a synthetically trained Transformer-based reasoning\nmodule. Tart trains this reasoning module in a task-agnostic manner using only synthetic\nlogistic regression tasks and composes it with an arbitrary real-world pre-trained model without\nany additional training. With a single inference module, Tart improves performance across\ndifferent model families (GPT-Neo, Pythia, Bloom), model sizes (100M - 6B), tasks (14 NLP\nbinary classification tasks), and even across different modalities (audio and vision). Additionally,\non the RAFT Benchmark, Tart improves GPT-Neo (125M)\u2019s performance such that it\noutperforms Bloom (176B), and is within 4% of GPT-3 (175B).1\n1\nIntroduction\nLarge language models (LLMs) show in-context learning capabilities which enable them to perform\na task given only a few examples, without updating the model parameters [Bro+20; Bom+21]. This\ntask-agnostic capability allows for a single model to be applied to a wide range of tasks [Agr+22;\nWei+22a; Nar+22]. In contrast, traditional task adaptation approaches, such as fine-tuning, update\nthe model parameters for each specific task.\nDespite being task-agnostic, in-context learning is seldom the practitioner\u2019s method of choice\nsince it consistently underperforms task-specific adaptation approaches [LAC21; Bro+20]. Most\nexisting works attribute this performance gap to the limited context window of LLMs which can\nonly accommodate a few task examples [Koc+23; Huy23; Liu+22a]. However, we show that this gap\nbetween in-context learning and fine-tuning approaches exists even when presented with the same\ntask examples.\n\u2217Equal Contribution\n1Our code and model is available at https://github.com/HazyResearch/TART\n1\narXiv:2306.07536v1  [cs.LG]  13 Jun 2023\nFigure 1: Taxonomy of task adaptation strategies. (Left) Comparison of different adaptation strategies\nacross three desiderata: task-agnostic, quality, scalability. (Right) Parameter updates across adaptations\nstrategies, colored regions represent parameter changes as a result of the adaptation strategy.\nThis observation raises the question whether this performance gap is a generic limitation of\ntask-agnostic methods for adaptation or is it specific to in-context learning? Specifically, can we\ndesign adaptation approaches which satisfy the following desiderata:\n\u2022 Task-agnostic: The same model generalizes across several different tasks.\n\u2022 Quality: Achieves accuracy competitive with task-specific methods across these different tasks.\n\u2022 Data-scalable: Learning ability improves with increasing number of task examples.\nWe first investigate why this quality gap exists. We decompose an LLM\u2019s in-context learning\ncapability into two abilities: learning good representations for the task and performing probabilistic\ninference, or reasoning, over these representations [03]. Is the gap because the representations do not\ncontain sufficient information or because the LLMs are unable to reason over them? We explore this\nhypothesis experimentally in Section 3 by measuring both the reasoning and the representation gaps\nacross a variety of LLM families (GPT-Neo [Bla+21], Pythia [Bid+23], Bloom [Sca+22]) over a\nsuite of binary classification tasks. We conclude that LLMs possess good representations, and the\nmajority of the quality gap (up to 79%) can be attributed to their insufficient reasoning ability. We\nfurther find that fine-tuning improves the base model on both these axes, but primarily improve the\ntask specific reasoning ability which accounts for 72% of the gained performance.\nRather surprisingly, most existing techniques for improving the performance gap, such as prompt\nengineering or active example selection, focus entirely on the LLM\u2019s learned representations. In\ncontrast, our work explores the orthogonal direction of improving the LLM\u2019s reasoning abilities. As\na first step, we fine-tune LLMs using synthetically generated probabilistic inference tasks to improve\ntheir reasoning capabilities. While this approach provides an improvement over the model\u2019s base\nin-context learning performance (up to 19%, see Figure 8 in App. A), this approach requires one to\nfine-tune each LLM individually. Taking a step further, we consider the possibility of whether one\ncan improve the reasoning capabilities in a manner that is agnostic to both tasks and models.\nWe show that it is indeed possible to improve the reasoning capabilities in a completely agnostic\nmanner. We propose Tart which improves upon an LLM\u2019s reasoning abilities using a synthetically\ntrained reasoning module (see Figure 2). Tart trains a Transformer-based reasoning module using\nonly synthetically generated logistic regression tasks independent of the downstream task or the base\nLLM. This inference module can be composed, without any additional training, with the embeddings\nof an LLM to improve upon its reasoning abilities. Notably, Tart satisfies the desired objectives:\n2\nFigure 2: Tart. (Left) Inference module training procedure: The inference module is trained on sequences of\nsynthetically generated logistic regression tasks. (Right) End-to-end framework: Tart composes a pre-trained\nLLM with the inference module. Tart uses the LLM to embed the input text. These embeddings, along\nwith the train labels, are passed as a sequence to the inference module which generates a final prediction.\n\u2022 Task-agnostic: Tart\u2019s inference module is only trained once using synthetic data.\n\u2022 Quality: Outperforms base LLM on all tasks and closes gap to task specific fine-tuning methods.\n\u2022 Data-scalable: Can accommodate 10x more examples than in-context learning.\nTart is task, model, and domain agnostic. Using a single inference module trained on synthetic data,\nwe exhibit that Tart not only generalizes across three model families (GPT-Neo, Pythia, Bloom)\nover 14 NLP classification tasks, but even across different domains (vision and speech; see Figure 7).\nIn terms of quality, we show that Tart\u2019s performance is 18.4% better than in-context learning,\n3.4% better than task-specific adapters, and is within 3.1% of full task-specific fine-tuning across\na suite of NLP tasks. On the RAFT Benchmark [Ale+21], Tart improves GPT-Neo (125M)\u2019s\nperformance such that it outperforms Bloom (176B), and is within 4% of GPT-3 (175B). Tart is\ndata-scalable and overcomes the limited context length bottleneck of in-context learning. While each\nexample spans multiple tokens in an LLM, often spanning hundreds of tokens, Tart\u2019s reasoning\nmodule encodes each example using only two tokens \u2013 one for the context and the other for the\nlabel. This data-scalability can lead to improvements of up to 6.8% (see Figure 6c).\nFrom a theoretical standpoint, we show that the generalization abilities of Tart depends mainly\non the distribution shift between the natural text embedding distribution produced by the LLM and\nthe synthetic data distribution, measured in terms of the Wasserstein-1 metric (Theorem 1).\nTo summarize, our main contributions are as follows:\n\u2022 Study why in-context learning does not perform as well as task-specific fine-tuning despite having\naccess to the same information, via a representation-reasoning decomposition.\n\u2022 Propose a new task-agnostic method, Tart, which bridges the performance gap to task-specific\nmethods and is trained using only synthetic data.\n\u2022 Demonstrate that Tart works across different NLP tasks for a range of model families. The\nsame inference module generalizes to vision and speech domains as well.\n2\nRelated work\nPrompt engineering focuses on improving the in-context task adaptation abilities of LLMs by\nmodifying prompts. A line of work improves performance by carefully designing the natural language\n3\ntask specifications [Aro+23; Wei+22b] while others improve performance by optimizing the examples\nchosen for the prompt [Dia+23; Liu+22b], encouraging the models to sequentially reason [Koj+22;\nWei+22b; Zel+22] and aggregating prompts [Wan+22b; Wan+22a]. Unfortunately, prompt-based\ntask adaptation is noisy [Lu+22]. Alternatively, prompt tuning improves the in-context abilities of\nmodels by training a small amounts of learnable vectors [LL21; LAC21; Liu+22c] for specific tasks.\nWhile these methods have been shown to improve in-context learning performance, they require\ntask-specific fine-tuning and are not task-agnostic.\nRecent works seek to understand the in-context learning property of LLMs by presenting mecha-\nnistic interpretations of in-context learning [Osw+22], performing exploratory analysis of in-context\nlearning behaviors [Wei+23], and explaining it as implicit Bayesian inference [Xie+21]. Existing\nliterature demonstrates that LLMs can learn simple function classes in-context [Gar+22] and propose\nthat LLMs are performing gradient descent when learning tasks in-context [Osw+22]. Complementary\nto these, our work provides insights on the mechanisms of in-context learning and its deficiencies.\nFurthermore, task transfer strategies adapt LLMs to a pre-specified target task. Strategies range from\nparameter efficient finetuning (PEFT) [Hou+19; Zha+23] to Low-Rank adaptation (LoRA) [Hu+22]\nwhich introduces trainable rank decomposition matrices into each layer to combining linear probing\nand fine-tuning [Kum+22]. While they have good performance, these methods require training\nmodels on a task-by-task basis in contrast to Tart.\n3\nTask adaptation strategies: Taxonomy and evaluation\nWe begin by describing the problem of adapting pre-trained language models for a collection of\ndownstream tasks while being task-agnostic, competent in performance, and data-scalable. Given\nthese criteria, we evaluate existing task adaptation approaches and propose a representation-reasoning\ndecomposition to understand their relative performances.\n3.1\nProblem statement and evaluation criteria\nOur focus is on methods for adapting pre-trained large language models (LLMs) for downstream\ntasks. Specifically, given an LLM and limited labeled data for a task, how does one adapt the model\nto the task? When evaluating a task adaptation strategy, we care about the following properties:\nTask-agnostic.\nGiven the general capabilities of pre-trained LLMs, we strive to utilize the same\nmodel across different tasks without requiring any task-specific training. With the increase in model\nsizes, the cost of deploying task-specific models increase both during training (expensive hyper-\nparameter search) as well as during inference (deploying several models). In general, task-agnostic\nmethods will scale better with increasing model sizes by side-stepping both these costs.\nPerformance quality.\nWe would like the adaptation approaches to be competitive in performance\nwhen compared with task-specific approaches across a wide range of tasks. For the binary classification\ntasks, the method should have accuracy comparable with task-specific approaches.\nData-scalable.\nThe task adaptation method should be scalable with the number of labeled task\nexamples. In particular, the method should be capable of learning from large datasets, and continually\nimprove its performance quality.\n3.2\nTaxonomy of task adaptation strategies\nWe can broadly taxonomize the existing task adaptation strategies for LLMs as in-context learning,\nfine-tuning the model, and training task-specific adapters (see Figure 1).\n4\nIn-context learning.\nIn-context learning allows for adapting the model without updating any\nmodel parameters, by simply providing a few demonstrations of the task in the LLM prompt.\nIn-context learning is completely task-agnostic since the same model can be used across tasks since\nno weights are updated at inference time. However, its performance is usually not at par when\ncompared with task-specific methods and it does not scale well with data since the number of\nexamples that can be utilized is bottlenecked by the context length of the model.\nFine-tuning.\nThis traditional class of methods update the model weights to adapt it specifically\nfor the task, typically by performing gradient descent over the labeled dataset. Fine-tuning methods\nare not task-agnostic since they change the underlying model significantly but usually achieve\nstate-of-the-art performance for any given task and are data scalable.\nAdapters.\nAdapters adapt the underlying LLM to a specific task by composing the LLM base\nmodel with an additional set of parameters which are optimized for the task. In contrast to fine-tuning\nwhich performs updates to the base model, adapters keep the base model frozen and only update\nthe additional parameters. Performance of adapters is usually competitive with full fine-tuning.\n3.3\nUnderstanding performance via Representation-Reasoning decompo-\nsition\nFrom the taxonomy of task adaptation approaches, only in-context learning satisfies the task-\nagnostic property but it consistently underperforms the task-specific tuning approaches. This section\ninvestigates why this performance gap exists. We hypothesize that it is either because (a) the\nrepresentations learned by the LLM are insufficient to learn a good predictor for the specific task, or\n(b) the LLM lacks the capability to reason over these representations to make good predictions for\nthe task.\nTo understand whether the representations have sufficient information, we train a task-specific\nlinear classifier using these representations, also known as linear probing, and evaluate its accuracy.\nLet AccFT, AccICL, and AccLR denote the accuracies obtained by fine-tuning, in-context learning, and\nby linear probing respectively. Using this as an intermediate, we decompose the performance gap\n\u2206perf : = AccFT \u2212 AccICL = AccFT \u2212 AccLR\n|\n{z\n}\n\u2206rep\n+ AccLR \u2212 AccICL\n|\n{z\n}\n\u2206reas\n(1)\nwhere \u2206rep represents the gap in performance which can be attributed to insufficient representa-\ntion capacity and \u2206reas is the performance gap due to insufficient reasoning abilities. Using this\ndecomposition, we consider the following hypotheses:\nH1. LLM representations have enough information to perform the task in-context, but they lack\nthe reasoning abilities to perform the task well.\nH2. Fine-tuning affects both the representations and reasoning but the improvement in reasoning\nabilities primarily leads to better performance.\nH3. Fine-tuning and adapters are not task-agnostic because the task-specific training hurts their\nability to transfer reasoning.\nWe now analyze each of the task adaptation approaches through the lens of the above hypotheses.\nWe perform all experiments with three different classes of language models (GPT-Neo, Pythia,\nBloom) across a collection of 6 binary classification tasks. See Appendix B for further details.\n5\n(a)\n(b)\n(c)\nFigure 3: All results for GPT-Neo (125M). (a) Accuracy of in-context learning vs. linear probing on model\nembeddings: representations have sufficient information. (b) Scatter plot showing the representation and\nreasoning gains (see eq. (1)) across different NLP datasets for a fine-tuned model when compared to base\nin-context learning. Fine-tuning majorly improves task-specific reasoning across datasets. (c) Accuracy\n(averaged across 6 datasets) of model fine-tuned on AGNews and tested on a separate task X vs model\nfine-tuned on task X and tested on the same task X. On average, fine-tuning hurts task-agnosticity can can\nbe up to 25% off from fine-tuning for the specific task.\nIn-context learning: LLMs lack reasoning abilities.\nWe begin by studying the representation\nand reasoning gaps, as defined in eq. (1), for in-context learning. In Figure 3a, we plot the average\naccuracy across datasets for in-context learning, task-specific fine-tuning, and linear probing. We see\nthat across models and different numbers of in-context examples, the reasoning gap \u2206reas accounts\nfor up to 79.11% of the performance gap between in-context learning and fine-tuning. This indicates\nthat the LLM representations have sufficient information but lack the ability to reason over them.\nFine-tuning: Improves task-specific reasoning.\nWe next investigate how fine-tuning for\na specific task affects the performance of the base model. In Figure 3b, we show a scatter plot\nof the gains that can be attributed to improved representations against the reasoning gains. We\nsee that, across models, reasoning improvements accounts for 73.06% of the improvements. This\nindicates that while fine-tuning improves both reasoning and representations of the LLM, the gains\nare predominantly due to improvements in task-specific reasoning. Furthermore, this task-specific\nfine-tuning of the LLM hurts its performance on other tasks. In Figure 3c, we show that the accuracy\nof a model fine-tuned on the AGNews dataset [ZZL15], leads to an average decrease of 25.77%\non other tasks. Furthermore, this drop in accuracy can be attributed to the drop in task-specific\nreasoning capabilities\u2014these account for 72.58% of the drop (see Appendix B for more details).\nAdapters: Impairs task-agnosticity via reasoning.\nTask-specific adapters do not change the\nunderlying representation ability of the model. To study their ability to generalize across tasks, we\ntrain an adapter for the AGNews dataset and evaluate it on other tasks. In Appendix B, we show\nthat the performance drops across tasks by an average of 19.8%, indicating that adapters only learn\ntask-specific reasoning abilities.\n4\nTart: Task-Agnostic Reasoning Transformers\nThe above analysis showed how it is the effective reasoning capabilities of the LLMs which limits\nits performance when compared with task-specific adaptation approaches. Building on this insight,\nwe propose Tart, which learns a general-purpose reasoning module completely agnostic to the\nunderlying base LLM and when composed with any LLM via its embeddings, generically improves\n6\nupon its reasoning abilities. Tart is a completely task-agnostic method which works across a suite\nof tasks without any task-specific training.\nTart comprises of two components: a generic task-agnostic reasoning module, and embeddings\nfrom the base LLM. The reasoning module is trained using only synthetic data (Gaussian logistic\nregression problems), agnostic of the auto-regressively trained language model, with the objective of\nlearning to perform probabilistic inference (Section 4.1). This learned transformer module is then\ncomposed with the base LLM, without any training, by simply aggregating the output embedding\nand using those as an input along with the class label (Section 4.2). Together, these components\nmake Tart task-agnostic, boost performance quality by improving reasoning, and make the approach\ndata-scalable by aggregating input embeddings into a single vector.\nIntuitively, the Gaussian logistic regression task is a simple probabilistic reasoning task wherein\nthe objective is to regress a given feature vector to a discrete binary label. Teaching an independent\nmodule to perform a family of these tasks and composing them with pre-trained language models\ncan be seen as a way to generically improve upon the LLM\u2019s reasoning abilities by making them\nperform such regression better.\n4.1\nReasoning module: Can Transformers learn probabilistic inference?\nTart\u2019s reasoning module is a Transformer-based model which is trained to perform probabilistic\ninference in-context using only synthetically generated data.\n4.1.1\nTraining the reasoning module\nThe reasoning module is a Transformer model which is auto-regressively trained on a family of\nlogistic regression tasks, with each input sequence corresponding to a different logistic regression\nproblem. We next describe the model architecture and the training procedure.\nModel architecture.\nThe reasoning module is based on the standard decoder-only Transformer\narchitecture from the GPT-2 family (see Appendix C.1 for details). The architecture takes as input\na sequence of vectors and is trained to predict the next vector in the sequence. The input sequence\nconsists of k pairs of labeled examples (x1, y1), (x2, y2), . . . , (xk, yk), with each example zi = (xi, yi)\nusing only two input positions of the transformer \u2013 one for the covariates x and the other for the\nlabel y. This is in contrast to standard LLMs where each example is spread over multiple tokens\nwhich limits how many examples can be put in the context. For example, with a context window of\n2048, our module can support 1024 examples while the base model can support only 10 examples,\nassuming each demonstration comprises 200 natural language tokens.\nTraining procedure.\nThis module is trained using gradient descent to minimize the population\nloss\n\u2113(T\u03b8) : = Ex,y\n\"\n1\nk\nk\nX\ni=1\n\u2113CE(T\u03b8(z1:i\u22121, xi), yi)\n#\n,\n(2)\nwhere z1:i\u22121 corresponds to the first i \u2212 1 examples and \u2113CE is the cross-entropy loss evaluated on\nthe transformer prediction and the true yi. Each training sequence st used to update the parameters\nw comprises a different d-dimensional logistic regression problem, sampled as\nSequence st : wt \u223c N(0, Id),\nxi,t \u223c N(0, Id),\nyi,t \u223c \u03c3(\u03b1\u27e8xi,t, wt\u27e9)\nfor i \u2208 [k] ,\n(3)\nwhere \u03c3 represents the sigmoid function and the multiplier \u03b1 determines the noise level of the\nproblem. We train our model with d = 16 and k = 256. Observe that the loss is only computed on\nthe predicted output of the features x in the sequence. We describe the model hyper-parameters and\nthe training procedure in more detail in Appendix C.1.\n7\n(a)\n(b)\n(c)\nFigure 4: Properties of Tart\u2019s inference module. (a) Comparison with learned logistic function:\ninference module recovers underlying probabilities. (b) Variation in error with different noise levels for\nmodel trained on \u03b1 = 10. (c) Comparison of Tart performance when using LOO embeddings and vanilla\nembeddings.\nWe trained the reasoning module with input dimension set to 16 with the labels y encoded in this\nspace using a one-hot encoding by appending the true label with zeros. While most base models\nproduce representations which are much higher dimensional (ranging from 784 to 2048). In order to\nreduce the dimensionality of these representations, we perform PCA on the output embeddings of the\nbase model, learning the components using only the training points available for that specific task.\nThe test examples are then projected onto these principal components to produce 16 dimensional\ninput representations.\n4.1.2\nProperties of reasoning module\nThe task-agnostic reasoning module described above is trained to perform well on a family of logistic\nregression tasks. We study some properties of the reasoning module, in particular how well it learns\nto perform the task at an instance level and how robust is it to variations in the noise level \u03b1.\nAccuracy of probabilistic inference.\nFor understanding the instance level performance of our\nreasoning module, we evaluate it on a sample of 64 different logistic regression problems, sampled\naccording to eq. (3). For each problem, we train task-specific linear classifiers using logistic regression\nand compare them with our task-agnostic reasoning module. In Figure 4a we plot the deviation\nof the predicted probabilities (averaged over the 64 problems) from the true probabilities for our\nreasoning module and the task-specific logistic solvers as a function of the number of examples used\nfor predictions. We observe that the error for our reasoning module decreases as a function of the\nnumber of in-context examples and is within 2% of the task-specific logistic function.\nRobustness to noise level.\nWe study the robustness of the learned module to the noise levels,\n\u03b1, of the logistic regression problem. Recall that we trained our inference module by fixing the\nnoise level \u03b1 = 10. At inference time, we vary the noise level to [0.5, 1, 10, 20], where lower values\ncorresponds to noisier problem. The reasoning module generalizes to easier problem without any\ndrop in accuracy but as we make the problem harder (\u03b1 = [0.5, 1]), the error increases progressively\n(see Figure 4b).\n4.2\nRole of representations: Which embeddings to take?\nThe reasoning module composes with a base LLM through its final layer embeddings. A natural\nway to produce these embeddings is to place all the train examples in-context and then average\nthe embedding vectors corresponding to the particular example (see Figure 5a). At inference time,\nwe append the test example to the training set, and average the embeddings corresponding to this\n8\n(a) Vanilla Embeddings\n(b) LOO Embeddings\nFigure 5: Tart Embedding Protocols. (a) For the vanilla embeddings, the test example is appended to\nthe training set and the sequence is passed to the base model. The representation for each train example\nin this sequence is taken as the average embedding across all its tokens. (b) For the LOO embeddings, we\ngenerate embeddings for each train example separately by placing all the other train examples before it in\nthe prompt and averaging the embeddings over the final example\u2019s tokens. The figure shows how to compute\nthe embedding for the ith training example.\nexample. We call these vanilla embeddings. Our experiments reveal that these embeddings seem to\nsaturate (or even hurt performance) beyond a certain number of in-context examples (see Figure 4c).\nOne reason can be that the causal nature of the model causes these embeddings to have asymmetric\ninformation\u2014the embeddings of each example is influenced by its preceding examples.\nTo counter this asymmetry, we propose leave-one-out (LOO) embeddings where the embeddings\nfor each training point is formed by placing all the other train examples before it in the prompt such\nthat all the embedding are formed with the same information content (see Figure 5b). In Figure 4c,\nchanging the embedding style from vanilla to LOO consistently improves performance across models\nand tasks. The LOO-embeddings help Tart be data-scalable by enabling it to embed a much larger\nnumber of points than the context window can support. To do so, we use only a subset of the train\nexamples as the in-context prompt. The reasoning module, by its architecture design, can already\naccommodate many more examples than supported by the context window of the base LLM.\n4.3\nTheoretical analysis: Generalization of Tart to language tasks\nWe study the generalization properties of the proposed task-agnostic method Tart. Note that that\nthe inference module is trained completely on synthetic data while at evaluation time, our input is\nthe embeddings from a natural language task. In Theorem 1 we show that its performance on the\nnatural language task depends on the distribution shift from the synthetic to the true distribution\n(see Appendix C.3 for a formal statement and proof).\nTheorem 1 (Informal). Let T represent the class of transformer models and TS \u2208 T denote\nthe trained reasoning module on set S of synthetic regression with nsyn sequences sampled from\ndistribution Psyn in eq. (3). The error of the transformer TS when evaluated on a distribution PNL\nover natural language sequences is\nerrPNL \u2272 W1(PNL, Psyn) +\ns\nComp(T )\nnsyn\n+ \u02c6\nerrPsyn(TS) ,\n(4)\nwhere W1 denotes the Wasserstein-1 metric, Comp(T ) represents the complexity of class T , and \u02c6\nerr\nrepresents the error on the empirical distribution.\nA few comments are in order: The first term represents the distribution shift error between the\ntrue natural language task and the synthetic task. The second term corresponds to the generalization\nerror on the logistic regression task, which can be made arbitrarily small since it scales with nsyn,\n9\nModel\nTart\nGPT-J (6B)\nOPT (175B)\nBloom (176B)\nGPT-3 (175B)\nAccuracy\n0.634\n0.608\n0.637\n0.595\n0.673\nTable 1: RAFT (HELM) Binary Classification Performance (Average Accuracy). Tart is used\nwith GPT-Neo (125M) model which is 1000x smaller than the corresponding 175B parameter models.\nTart outperforms Bloom (176B) and is competitive with OPT (175B) and GPT-3 (175B).\nthe number of synthetic datapoints which can be generated without any cost. The third term is the\noptimization error indicating how well has the reasoning module TS fit to the synthetic training set.\n5\nExperimental evaluation\nWe evaluate Tart on a wide range of binary classification tasks across three domains: language,\nvision and audio. We demonstrate that Tart improves base in-context performance and closes the\ngap with standard task-specific strategies. We also conduct ablations to demonstrate that Tart\nscales with model size and can support 10x more samples than in-context learning.\n5.1\nExperimental setup\nDatasets.\nWe briefly describe the datasets used, with details available in Appendix D.1. We consider\n14 different binary classification tasks ranging from sentiment classification, news article categorization\nto spam detection. The evaluation datasets include: SST [Soc+13], Rotten Tomatoes [PLV02], SMS\nSpam [AHY11], IMDB [Maa+11], Civil Comments [Bor+19], AGNews [ZZL15], DBPedia [ZZL15],\nand the Youtube dataset [Zha+21]. Since AGNews and DBPedia14 are multi-class datasets, we\nconstruct 4 binary classification tasks from each dataset respectively. For each dataset, we truncate\nthe input text to be at most 100 characters to enable us to fit sufficient number of samples in-context.\nModel families.\nWe evaluate our method across three different families of models: GPT-\nNeo [Bla+21], Pythia [Bid+23], and Bloom [Sca+22]. For our evaluations across 14 datasets, we\nuse GPT-Neo (125M), Pythia (160M) and Bloom (560M). For ablations on larger models,\nwe evaluate models with 1B parameters across each of the model families (i.e., GPT-Neo (1.3B),\nPythia (1.4B) and Bloom (1.7B)) and models with 3B parameters (i.e., GPT-Neo (2.7B),\nPythia (2.8B) and Bloom (3B)). We additionally evaluate on GPT-J (6B) [WK21].\nBaselines.\nWe evaluate our models against all types of task-adaptation strategies described in\nSection 3.2: 1) in-context learning, 2) full fine-tuning, 3) last layer fine-tuning, 4) LM head fine-tuning,\nand 5) adapters. For each baseline, we perform an extensive hyper-parameter search over number of\nepochs and learning rate for each dataset in order to optimize performance (see Appendix D.1 for\nhyperparameter details). For Tart, we chose a base default set of parameters and use the same\ninference module with the exact same weights for all the experiments in this section.\n5.2\nNatual language benchmark evaluations\nFor this section, all reported accuracies are averaged over 5 independent random seeds. A complete\nset of results with standard deviations can be found in Appendix D.2.\nPerformance with respect to baselines.\nAs shown in Appendix D.2, averaged across all tasks\nand model families, Tart improves upon the base in-context learning performance by an average of\n18.4 points, improves upon adapter heads by 3.4 points, and is within 3.1 points of full fine-tuning. We\nalso observe that Tart consistently outperforms the task specific strategies of LM head fine-tuning\nand last layer fine-tuning.\n10\n(a)\n(b)\n(c)\nFigure 6: Effects of scale. (a) Effect of number of in-context examples on performance for different task\nadaptation strategies. (b) Effect of model size on the performance of different task adaptation strategies. (c)\nBeyond context length limitations, performance comparison with respect to number of in-context examples.\nPerformance on RAFT Benchmark\nWe evaluate Tart on all binary classification tasks in the\nRAFT Benchmark, following the protocol used in HELM [Lia+22]. When applied with GPT-Neo\n(125M), Tart outperforms Bloom (176B), and is within 4% points of GPT-3 (175B), both of\nwhich are 1000x larger in size. See Table 1 for exact accuracies.\nPerformance with number of in-context examples.\nOur results demonstrate that performance\nof Tart scales with number of in-context examples (see Figure 6a). Across 14 tasks and 3 model\nfamilies, when scaling from 18 to 64 examples, Tart improves performance by an average of 4.8%.\nCorrespondingly, full fine-tuning improves performance by 9.0%.\nScaling with base model size.\nWe analyze how different task-adaptation strategies scale with\nrespect to model size using the GPT-Neo family: GPT-Neo (125M), GPT-Neo (1.3B) and\nGPT-J (6B). Figure 6b shows that when scaling from 100M to 6B parameters, performance of\ntask-specific methods and Tart increases as a function scale. For Tart, the performance increases\nby 9.8% while using the same inference module across model sizes. Furthermore, the difference in\nperformance between Tart and fine-tuning baseline reduces from 7.5% to 2.2% from the 100M scale\nto 6B scale.\nBeyond context length.\nWe evaluate the data-scaling properties for both in-context learning\nand Tart (Figure 6c). To demonstrate the scaling property, we do not truncate the input text to\n100 characters and utilize the entire text sequences. For Tart, we observe that accuracy continues\nto improve when scaling from 18 to 256 in-context examples with 6.8% lift in performance. In\ncomparison, ICL, which is bottlenecked by context length, supports 10x less samples, with the\ncontext window saturating at 24 examples only and lags Tart by an average of 19.1%.\n5.3\nExtensions to other modalities\nWe demonstrate that Tart is not only agnostic to models and tasks, but also modalities. We\nextend Tart to classification tasks on modalities beyond language: vision and audio. For vision\ntasks, we use representations from Google\u2019s 307M parameter pretrained Vision Transformer (ViT)\nmodel [Wu+20]: ViT-large-patch16-224-in21k. For audio tasks, we use representations from\nOpenAI\u2019s 1.5B parameter pretrained Whisper model [Rad+22]: Whisper-large. In applying Tart\nto the representations from these models, we provide a way for performing in-context learning in\nmodalities beyond text. We refer the reader to Appendix D.3 for further details on the experiment\nsetup.\nVision application.\nWe evaluate the performance of Tart on binary classification versions of\nCIFAR-10 [Kri09] (classes plane and bird) and MNIST [LCB10] (classes 0 and 8). As shown in\nFigure 7a and 7b, performance of Tart is competitive with task-specific adaptation approaches.\n11\n(a) MNIST\n(b) CIFAR-10\n(c) Speech Commands\nFigure 7: Tart can generalize across domains using the same inference module that was used for language\nbenchmarks: Performance across vision tasks (MNIST, CIFAR-10) and an audio task (Speech Commands).\nAudio application.\nWe evaluate Tart on a binary classification version of the Speech Commands\ndataset [War18], where the task is to classify \u201cstop\u201d and \u201cgo\u201d utterances. As shown in Figure 7c,\nperformance of Tart is competitive with task-adaptation approaches.\n6\nDiscussion\nWe look at the problem of task-agnostic learning with LLMs. We show that LLMs lack the ability to\nperform simple reasoning over their learned representations and introduce Tart, a task, model and\ndomain agnostic method for improving their reasoning abilities. In this work, we focus on binary\nclassification tasks, showing that synthetic, logistic regression task data can be used to train a generic\nreasoning module capable of completing this class of tasks. Extensions to multi-class classification\ntasks are possible either using a one-vs-all approach or by training Tart\u2019s reasoning module using\nmulti-class synthetic data. In future work, we seek to understand whether synthetic tasks exist for\ntraining other generic reasoning modules, capable of improving base LLM performance on tasks such\nas generation or summarization.\nAcknowledgements\nWe are grateful to Simran Arora, Rishi Bommasani, Niladri Chatterji, Arjun Desai, Sabri Eyuboglu,\nNeha Gupta, Karan Goel, Erik Jones, Ananya Kumar, Cassidy Laidlaw, Megan Leszczynski, Piero\nMolino, Laurel Orr, Michael Poli, Dimitris Tsipras, Michael Wornow, Ce Zhang, and Michael Zhang\nfor their helpful comments and feedback, and discussions which helped shape this project.\nWe gratefully acknowledge the support of NIH under No. U54EB020405 (Mobilize), NSF under\nNos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); US\nDEVCOM ARL under No. W911NF-21-2-0251 (Interactive Human-AI Teaming); ONR under No.\nN000141712266 (Unifying Weak Supervision); ONR N00014-20-1-2480: Understanding and Applying\nNon-Euclidean Geometry in Machine Learning; N000142012275 (NEPTUNE); NXP, Xilinx, LETI-\nCEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson,\nQualcomm, Analog Devices, Google Cloud, Salesforce, Total, the HAI-GCP Cloud Credits for\nResearch program, the Stanford Data Science Initiative (SDSI), and members of the Stanford DAWN\nproject: Facebook, Google, and VMWare. CDS was supported by a NSF CAREER (award 2046760).\nThe U.S. Government is authorized to reproduce and distribute reprints for Governmental\npurposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions\nor recommendations expressed in this material are those of the authors and do not necessarily\nreflect the views, policies, or endorsements, either expressed or implied, of NIH, ONR, or the U.S.\nGovernment.\n12\nReferences\n[Agr+22]\nMonica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon Kim, and David Sontag.\n\u201cLarge language models are zero-shot clinical information extractors\u201d. In: arXiv preprint\narXiv:2205.12689 (2022) (Cited on page 1).\n[Ale+21]\nNeel Alex, Eli Lifland, Lewis Tunstall, Abhishek Thakur, Pegah Maham, C Jess Riedel,\nEmmie Hine, Carolyn Ashurst, Paul Sedille, Alexis Carlier, et al. \u201cRAFT: A Real-\nWorld Few-Shot Text Classification Benchmark\u201d. In: Thirty-fifth Conference on Neural\nInformation Processing Systems Datasets and Benchmarks Track (Round 2). 2021\n(Cited on pages 3, 29).\n[AHY11]\nTiago A. Almeida, Jose Maria Gomez Hidalgo, and Akebo Yamakami. \u201cContributions\nto the Study of SMS Spam Filtering: New Collection and Results\u201d. In: Proceedings of\nthe 2011 ACM Symposium on Document Engineering (DOCENG\u201911). 2011 (Cited on\npages 10, 18).\n[Aro+23]\nSimran Arora, Avanika Narayan, Mayee F Chen, Laurel J Orr, Neel Guha, Kush Bhatia,\nInes Chami, Frederic Sala, and Christopher R\u00e9. \u201cAsk Me Anything: A simple strategy\nfor prompting language models\u201d. In: ICLR 2023. 2023 (Cited on page 4).\n[Bid+23]\nStella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien,\nEric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth,\nEdward Raff, et al. \u201cPythia: A suite for analyzing large language models across training\nand scaling\u201d. In: arXiv preprint arXiv:2304.01373 (2023) (Cited on pages 2, 10).\n[Bla+21]\nSid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large\nScale Autoregressive Language Modeling with Mesh-Tensorflow. Mar. 2021 (Cited on\npages 2, 10).\n[Bom+21]\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney\nvon Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill,\net al. \u201cOn the opportunities and risks of foundation models\u201d. In: arXiv preprint\narXiv:2108.07258 (2021) (Cited on page 1).\n[Bor+19]\nDaniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman.\n\u201cNuanced Metrics for Measuring Unintended Bias with Real Data for Text Classification\u201d.\nIn: CoRR abs/1903.04561 (2019) (Cited on pages 10, 27).\n[Bro+20]\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\n\u201cLanguage models are few-shot learners\u201d. In: Advances in neural information processing\nsystems 33 (2020), pp. 1877\u20131901 (Cited on page 1).\n[Dia+23]\nShizhe Diao, Pengcheng Wang, Yong Lin, and Tong Zhang. \u201cActive Prompting with\nChain-of-Thought for Large Language Models\u201d. In: arXiv preprint arXiv:2302.12246\n(2023) (Cited on page 4).\n[Gar+22]\nShivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. \u201cWhat can\ntransformers learn in-context? a case study of simple function classes\u201d. In: Advances in\nNeural Information Processing Systems 35 (2022), pp. 30583\u201330598 (Cited on page 4).\n[Hou+19]\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De\nLaroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. \u201cParameter-\nefficient transfer learning for NLP\u201d. In: International Conference on Machine Learning.\nPMLR. 2019, pp. 2790\u20132799 (Cited on page 4).\n[Hu+22]\nEdward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nLu Wang, and Weizhu Chen. \u201cLoRA: Low-Rank Adaptation of Large Language Models\u201d.\nIn: International Conference on Learning Representations. 2022 (Cited on page 4).\n13\n[Huy23]\nChip Huyen. Prompting vs. Finetuning vs. Alternatives. 2023 (Cited on page 1).\n[Koc+23]\nJan Koco\u0144, Igor Cichecki, Oliwier Kaszyca, Mateusz Kochanek, Dominika Szyd\u0142o,\nJoanna Baran, Julita Bielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil Kanclerz,\net al. \u201cChatgpt: Jack of all trades, master of none\u201d. In: arXiv preprint arXiv:2302.10724\n(2023) (Cited on page 1).\n[Koj+22]\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.\n\u201cLarge Language Models are Zero-Shot Reasoners\u201d. In: ICML 2022 Workshop on\nKnowledge Retrieval and Language Models. 2022 (Cited on page 4).\n[Kri09]\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Tech. rep.\nUniversity of Toronto, 2009 (Cited on pages 11, 29).\n[Kum+22]\nAnanya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang.\n\u201cFine-tuning can distort pretrained features and underperform out-of-distribution\u201d. In:\narXiv preprint arXiv:2202.10054 (2022) (Cited on page 4).\n[LCB10]\nYann LeCun, Corinna Cortes, and CJ Burges. \u201cMNIST handwritten digit database\u201d.\nIn: ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist 2 (2010) (Cited\non pages 11, 29).\n[LAC21]\nBrian Lester, Rami Al-Rfou, and Noah Constant. \u201cThe Power of Scale for Parameter-\nEfficient Prompt Tuning\u201d. In: Proceedings of the 2021 Conference on Empirical Methods\nin Natural Language Processing. 2021, pp. 3045\u20133059 (Cited on pages 1, 4).\n[LL21]\nXiang Lisa Li and Percy Liang. \u201cPrefix-Tuning: Optimizing Continuous Prompts\nfor Generation\u201d. In: Proceedings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers). 2021, pp. 4582\u20134597 (Cited on page 4).\n[Lia+22]\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro\nYasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. \u201cHolistic\nevaluation of language models\u201d. In: arXiv preprint arXiv:2211.09110 (2022) (Cited on\npages 11, 29).\n[Liu+22a]\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit\nBansal, and Colin A Raffel. \u201cFew-shot parameter-efficient fine-tuning is better and\ncheaper than in-context learning\u201d. In: Advances in Neural Information Processing\nSystems 35 (2022), pp. 1950\u20131965 (Cited on page 1).\n[Liu+22b]\nJiachang Liu, Dinghan Shen, Yizhe Zhang, William B Dolan, Lawrence Carin, and\nWeizhu Chen. \u201cWhat Makes Good In-Context Examples for GPT-3?\u201d In: Proceedings of\nDeep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction\nand Integration for Deep Learning Architectures. 2022, pp. 100\u2013114 (Cited on page 4).\n[Liu+22c]\nXiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie\nTang. \u201cP-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and\nTasks\u201d. In: Proceedings of the 60th Annual Meeting of the Association for Computational\nLinguistics. Dublin, Ireland: Association for Computational Linguistics, May 2022\n(Cited on page 4).\n[Lu+22]\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. \u201cFan-\ntastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt\nOrder Sensitivity\u201d. In: Proceedings of the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers). 2022, pp. 8086\u20138098 (Cited on\npage 4).\n14\n[Maa+11]\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and\nChristopher Potts. \u201cLearning Word Vectors for Sentiment Analysis\u201d. In: Proceedings\nof the 49th Annual Meeting of the Association for Computational Linguistics: Hu-\nman Language Technologies. Portland, Oregon, USA: Association for Computational\nLinguistics, 2011, pp. 142\u2013150 (Cited on page 10).\n[Nar+22]\nAvanika Narayan, Ines Chami, Laurel Orr, and Christopher R\u00e9. \u201cCan Foundation\nModels Wrangle Your Data?\u201d In: Proc. VLDB Endow. 16.4 (2022). issn: 2150-8097\n(Cited on page 1).\n[Osw+22]\nJohannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u00e3o Sacramento, Alexander\nMordvintsev, Andrey Zhmoginov, and Max Vladymyrov. \u201cTransformers learn in-context\nby gradient descent\u201d. In: arXiv preprint arXiv:2212.07677 (2022) (Cited on page 4).\n[PLV02]\nBo Pang, Lillian Lee, and Shivakumar Vaithyanathan. \u201cThumbs Up? Sentiment Classi-\nfication Using Machine Learning Techniques\u201d. In: Proceedings of EMNLP. 2002, pp. 79\u2013\n86 (Cited on pages 10, 18).\n[Pol+23]\nMichael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus,\nYoshua Bengio, Stefano Ermon, and Christopher R\u00e9. \u201cHyena hierarchy: Towards larger\nconvolutional language models\u201d. In: arXiv preprint arXiv:2302.10866 (2023) (Cited on\npage 30).\n[03]\nProbability theory: The logic of science. Cambridge university press, 2003 (Cited on\npage 2).\n[Rad+22]\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and\nIlya Sutskever. Robust Speech Recognition via Large-Scale Weak Supervision. 2022\n(Cited on pages 11, 30).\n[Rad+18]\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. \u201cImproving\nlanguage understanding by generative pre-training\u201d. In: arXiv preprint (2018) (Cited\non page 22).\n[Sca+22]\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel\nHesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9,\net al. \u201cBloom: A 176b-parameter open-access multilingual language model\u201d. In: arXiv\npreprint arXiv:2211.05100 (2022) (Cited on pages 2, 10).\n[SB14]\nShai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From\ntheory to algorithms. Cambridge university press, 2014 (Cited on page 26).\n[Soc+13]\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning,\nAndrew Y Ng, and Christopher Potts. \u201cRecursive deep models for semantic composi-\ntionality over a sentiment treebank\u201d. In: Proceedings of the 2013 conference on empirical\nmethods in natural language processing. 2013, pp. 1631\u20131642 (Cited on pages 10, 18).\n[WK21]\nBen Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive\nLanguage Model. https://github.com/kingoflolz/mesh-transformer-jax. May\n2021 (Cited on page 10).\n[Wan+22a]\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou.\n\u201cRationale-augmented ensembles in language models\u201d. In: arXiv preprint arXiv:2207.00747\n(2022) (Cited on page 4).\n[Wan+22b]\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou.\n\u201cSelf-consistency improves chain of thought reasoning in language models\u201d. In: arXiv\npreprint arXiv:2203.11171 (2022) (Cited on page 4).\n[War18]\nP. Warden. \u201cSpeech Commands: A Dataset for Limited-Vocabulary Speech Recognition\u201d.\nIn: ArXiv e-prints (Apr. 2018) (Cited on pages 12, 29).\n15\n[Wei+22a]\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud,\nDani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori\nHashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. \u201cEmergent\nAbilities of Large Language Models\u201d. In: Transactions on Machine Learning Research\n(2022). Survey Certification. issn: 2835-8856 (Cited on page 1).\n[Wei+22b]\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and\nDenny Zhou. \u201cChain of thought prompting elicits reasoning in large language models\u201d.\nIn: arXiv preprint arXiv:2201.11903 (2022) (Cited on page 4).\n[Wei+23]\nJerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen,\nHanxiao Liu, Da Huang, Denny Zhou, et al. \u201cLarger language models do in-context\nlearning differently\u201d. In: arXiv preprint arXiv:2303.03846 (2023) (Cited on page 4).\n[Wu+20]\nBichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Zhicheng Yan,\nMasayoshi Tomizuka, Joseph Gonzalez, Kurt Keutzer, and Peter Vajda. Visual Trans-\nformers: Token-based Image Representation and Processing for Computer Vision. 2020\n(Cited on pages 11, 30).\n[Xie+21]\nSang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. \u201cAn explanation of\nin-context learning as implicit bayesian inference\u201d. In: arXiv preprint arXiv:2111.02080\n(2021) (Cited on page 4).\n[Zel+22]\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. \u201cSTaR: Bootstrapping\nReasoning With Reasoning\u201d. In: Advances in Neural Information Processing Systems.\n2022 (Cited on page 4).\n[Zha+21]\nJieyu Zhang, Yue Yu, Yinghao Li, Yujing Wang, Yaming Yang, Mao Yang, and\nAlexander Ratner. \u201cWRENCH: A Comprehensive Benchmark for Weak Supervision\u201d.\nIn: Thirty-fifth Conference on Neural Information Processing Systems Datasets and\nBenchmarks Track. 2021 (Cited on pages 10, 18).\n[Zha+23]\nRenrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng\nLi, Peng Gao, and Yu Qiao. \u201cLLaMA-Adapter: Efficient Fine-tuning of Language\nModels with Zero-init Attention\u201d. In: arXiv preprint arXiv:2303.16199 (2023) (Cited\non page 4).\n[ZZL15]\nXiang Zhang, Junbo Zhao, and Yann LeCun. \u201cCharacter-level convolutional networks\nfor text classification\u201d. In: Advances in neural information processing systems 28 (2015)\n(Cited on pages 6, 10, 18, 27).\n16\n(a)\n(b)\nFigure 8: Fine-tuning with NL synthetic task. (Left) Averaged over 6 different tasks, fine-tuning with the\nNL synthetic task provides a lift over base in-context learning, and scales with number of examples. (Right)\nDataset level comparisons between task-specific fine-tuning, in-context learning and synthetic fine-tuning:\nsynthetic fine-tuning outperforms base in-context learning on 4 out of 6 datasets, but lags task-specific\ntuning.\nA\nFine-tuning model with NL-based Probabilistic Inference\nTasks\nAs highlighted in Section 1, we describe the details for directly fine-tuning an LLM on syntheti-\ncally generated probabilistic inference tasks to improve reasoning capabilities. For the following\nexperiments, we use GPT-Neo (125M) as the base model.\nA.1\nTraining Task\nWe fine-tune the base model using a sequence of k pairs of synthetically generated labeled natural\nlanguage examples (x, y). Each example x in the sequence s = (x1, y1), . . . , (xk, yk) consists of a\nlist of strings constructed from a fixed V size of dimension d = 30 . We use the following fixed\nvocabulary: [ \u201csports\u201d, \u201clove\u201d, \u201chate\u201d, \u201ccar\u201d, \u201cschool\u201d, \u201cfamily\u201d, \u201cwork\u201d, \u201csleep\u201d, \u201cwater\u201d, \u201ctree\u201d, \u201cfox\u201d,\n\u201ctrain\u201d, \u201crandom\u201d, \u201cmovie\u201d, \u201cmusic\u201d, \u201cbook\u201d, \u201cplay\u201d, \u201chouse\u201d, \u201cspell\u201d, \u201cbar\u201d, \u201cjump\u201d, \u201cpark\u201d, \u201crun\u201d, \u201chill\u201d,\n\u201cfast\u201d, \u201cslow\u201d, \u201ctalk\u201d, \u201cwallet\u201d, \u201corange\u201d, \u201capple\u201d, \u201cball\u201d, \u201ccat\u201d ].\nTo generate a particular example xi, we sample each coordinate xi,j uniformly from the set\n{\u22121, +1}. If the sampled value is +1, we set the value to be the corresponding word in the\nvocabulary, that is, xi,j = Vj. Otherwise, the word xi,j is set to \u201cnull\u201d. For a given sequence s, we\ngenerate each of the labels {yi} as:\nwt \u223c N(0, Id),\nyi \u223c \u03c3(\u03b1\u27e8xi, w\u27e9), for i \u2208 [k] ,\n(5)\nwhere we set noise parameter \u03b1 = 5. If the sampled output is 0, we set the yi to \u201cnegative\u201d and\n\u201cpositive\u201d otherwise.\nFinally, the inputs are formatted with following template: \u201cx1 : y1 , x2 : y2 , ... , xk : yk\u201d and the\nmodel is trained using gradient descent on the loss\n\u2113(T\u03b8) : = Ex,y\n\"\n1\nk\nk\nX\ni=1\n\u2113CE(T\u03b8(z1:i\u22121, xi), yi)\n#\n,\n(6)\n17\nwhere z1:i\u22121 corresponds to the first i \u2212 1 examples and \u2113CE is the cross-entropy loss evaluated on\nthe transformer prediction and the true yi.\nMore concretely, a sample input sample sequence s to be used for training looks like:\n\"sports love null car ... cat: positive,\nnull love null car ... null: negative,\n...\nsports null hat null ... cat : positive\"\nA.2\nTraining Parameters\nWe train GPT-Neo (125M) on this synthetic task with a learning rate of 0.0001 and a batch size of\n4. For each sequence we sampled a total of k = 60 examples and trained the model for 10000 steps.\nA.3\nEvaluation\nWe evaluate on 6 datasets: AG News [ZZL15], DBPedia [ZZL15], SST [Soc+13], SMS Spam [AHY11],\nYoutube [Zha+21] and Rotten Tomatoes [PLV02]. We truncate the input texts to 100 characters\nto fit more in-context examples. We evaluate over a range of context sizes (k=[18, 32, 48, 60]). At\nevaluation time, we use the same \u201csentence : label\u201d format that was used to train the model. We\nevaluate over 3 random seeds. In Figure 8, we compare the performance of the model fine-tuned on\nprobabilistic inference tasks and the base in-context learning performance. While the performance of\nthe fine-tuned model is better than the base in-context learning capabilities, task-specific fine-tuning\nstill outperforms it by an average of 16.87% (see Figure 8).\nB\nDetails for Representation-Reasoning decomposition evalu-\nations\nIn this section, we provide details for the experimental evaluation and additional results for the\nrepresentation-reasoning decomposition introduced in Section 3.3.\nB.1\nExperimental setup\nFor these experiments, we evaluate three different language models: GPT-Neo (125M), Pythia\n(160M), and Bloom (560M) on a collection of 6 binary classification datasets: AG News [ZZL15],\nDBPedia [ZZL15], SST [Soc+13], SMS Spam [AHY11], Youtube [Zha+21] and Rotten Toma-\ntoes [PLV02]. For each model, we run evaluations for three different random seeds, where the\nrandomness was in the set of datapoints chosen for the training task. For the hyperparameters, we\nperformed an extensive search for all models across datasets. For details on these hyperparameters\nand the adapter architecture we evaluate over, see Appendix D.1.\nTo conduct linear probing over the embeddings, we perform logistic regression over the output\nembeddings of each model and the given labels in the training set using the built-in logistic regression\nsolver from the scikit-learn python library, utilizing the lbgfs solver.\nB.2\nDetailed results\nFor each class of methods in the task-adaptation taxonomy from Section 3.2, we now describe the\ndetails of the experimental evaluation and present additional results.\n18\n(a) GPT-Neo (125M)\n(b) Pythia (160M)\n(c) Bloom (560M)\nFigure 9: Comparison of linear probing, in-context learning, and fine-tuning. Accuracy of in-context\nlearning vs. linear probing on model embeddings across three model families: representations have sufficient\ninformation.\n(a) GPT-Neo (125M)\n(b) Pythia (160M)\n(c) Bloom (560M)\nFigure 10: Linear probing vs. in-context learning. Scatter plot of accuracy of in-context learning vs.\nlinear probing on model embeddings across model families and different number of in-context examples:\nlinear probing consistently outperforms in-context learning indicating that the learned representations have\nsufficient information. Each point in the plot represents a dataset.\n19\n(a) GPT-Neo (125M)\n(b) Pythia (160M)\n(c) Bloom (560M)\nFigure 11: Effects of fine-tuning on reasoning. Across datasets (each point in plot represents a dataset)\nand model families, fine-tuning improves task-specific reasoning which improves it performance over base\nin-context learning.\n(a) GPT-Neo (125M)\n(b) Pythia (160M)\n(c) Bloom (560M)\nFigure 12: Effects of fine-tuning on task-agnosticity Accuracy of task-specific fine-tuned model vs.\naccuracy of model fine-tuned on AG-News-0 and evaluated on task. Fine-tuning hurts task-agnosticity across\nall three model families.\nIn-context learning.\nTo understand the representation and reasoning gaps for in-context learning,\nwe evaluated three accuracies: a) using in-context learning with base models, b) fine-tuning the model\nfor the task, and c) linear probing the model specifically for the task. The gap due to representation\nwas taken to be the difference between the fine-tuning and linear probing accuracies while the\nreasoning gap was the gap between linear probing and in-context learning, as described in eq. (1).\nIn Figure 9, we show the average accuracies of in-context learning, linear probing, and fine-tuning\nacross the 6 tasks. Linear probing closes the gap between in-context learning and fine-tuning, while\nbeing task-specific. In Figure 10, we show a scatter plot of the accuracies of linear probing vs. the\naccuracies of base in-context learning. Linear probing consistently out performs in-context learning\nshowing that the learned representations across these models have sufficient information to complete\nthe tasks but lack reasoning abilities.\nFine-tuning.\nFor the fine-tuning approach, we are interested in understanding two hypotheses:\na) how does fine-tuning improve the model performance, and b) whether fine-tuning hurts task-\nagnosticity of the base model and if yes, what is the underlying reason for it.\nFor the first hypothesis, we evaluate the proportion of gains that can be attributed to improved\nrepresentations of the the underlying model. This is computed as the difference in performance\n20\n(a) GPT-Neo (125M)\n(b) Pythia (160M)\n(c) Bloom (560M)\nFigure 13: Effects of fine-tuning on task-agnosticity (dataset level) Accuracy of task-specific fine-\ntuned model vs. accuracy of model fine-tuned on AG-News-0 and evaluated on task. Fine-tuning consistently\nhurts task-agnosticity across all three model families and datasets.\n(a) GPT-Neo (125M)\n(b) Pythia (160M)\n(c) Bloom (560M)\nFigure 14: Effects of fine-tuning on task agnosticity. Scatter plot of reasoning loss against representation\nloss when the base model is trained on AG-News-0 and evaluated on other tasks. Across datasets (each point\nin plot represents a dataset), fine-tuning majorly impairs reasoning when transferring to tasks outside the\nspecific fine-tuned task.\nof linear probing over the base model and over the fine-tuned model \u2014 this evaluates how much\nthe representations have changed specifically for this task. The reasoning gains are then computed\nby subtracting the representation gains from the total gain (fine-tuning accuracy minus in-context\naccuracy). Figure 11 shows a scatter plot of these representation gains and reasoning gains, plotted\nacross different datasets and number of examples (k). Most of the gains which are realized by\nfine-tuning are because of improved task-specific reasoning capabilities across the model families.\nFor the second hypothesis, we first evaluate whether fine-tuning hurts task-agnosticity. For this we\nevaluate two sets of accuracies: accuracy of a model fine-tuned for the specific task and the accuracy\nof a model on the task but fine-tuned on the AG News dataset. From Figures 12 and 13, we see that\nthere is a drop in accuracy\u2014over 25.77% across models and datasets. For the second part, we again\ndecompose the drop in accuracy into a representation drop and a reasoning drop. The representation\ndrop is computed by training a linear probe over the two models (task-specific fine-tuned and AG\n21\n(a) GPT-Neo (125M)\n(b) Pythia (160M)\n(c) Bloom (560M)\nFigure 15: Effects of fine-tuning on task-agnosticity of adapters. Accuracy of task-specific fine-tuned\nadapter vs. accuracy of adapter fine-tuned on AG-News-0 and evaluated on task. Fine-tuning consistently\nhurts the generalization ability of adapters across datasets.\nNews fine-tuned) and looking at the difference between them. The reasoning drop, as before, is\ncomputed by subtracting this representation drop from the total drop. Figure 14 shows that most of\nthis drop in task-agnosticity can be attributed to over-fitting of the reasoning abilities over the task\nfor which the models are fine-tuned.\nAdapters.\nSince adapters do not modify the underlying representations of the model, we look\nat how a single adapter generalizes across tasks. For this we train an adapter on the AG News\ndataset and evaluate it on the other datasets. We compare this set of accuracies with those obtained\nby task-specific adapters in Figure 15. The main conclusion is that task-specific adapters are not\nagnostic learners and over-fit to the task for which they are fine-tuned.\nC\nDetails for Tart implementation\nThis section contains the details on training Tart\u2019s reasoning module and extended results on the\nchoice of embeddings from Section 4.\nC.1\nTart\u2019s reasoning module\nArchitecture details.\nWe use the standard GPT-2 architecture [Rad+18] for training our\nreasoning module. We set the embedding size to 256, number of decoder layers to 12, and number of\nheads to 8 for a total of 22 million parameters. Since the GPT-2 backbone outputs a sequence of\nembeddings, we additionally add a linear layer in the end to convert the output to scalar values (see\nFigure 16). Additionally, the binary labels y are encoded as a one-hot vector to match the input\ndimension d of the corresponding covariates x.\nTraining procedure.\nWe trained Tart\u2019s reasoning module with a context length of 258 (allowing\nfor up to 256 in-context examples). The batch size was set to 64, learning rate to 0.0001 and the\nmodel was trained for a total of 24000 epochs. Each batch of training data consists of sampling a\nsequence of 258 examples using eq. (3). In addition to these hyperparameters, we used a curriculum\non the input dimensions and on the number of examples in the sequence to train our module\u2014the\ninput dimensions started from a value of 4 and were incremented by 4 every 1000 epochs while the\nnumber of examples started from 18 and were incremented by 30 every 1000 epochs.\n22\nFigure 16: Tart reasoning module architecture. The reasoning module takes as input sequences of\n(x, y) pairs of dimension d. A linear layer is used to project d to the hidden dimension size of the GPT-2\nbackbone. Finally, a linear layer is applied to the outputs of the backbone to generate predictions for each\nxk in the input sequence.\nFigure 17: Training loss vs. number of steps. The plot shows the variation in training loss as a function\nof the number of steps of gradient descent for Tart\u2019s reasoning module.\nCombining reasoning module with base LLM.\nWe trained the reasoning module with input\ndimension set to 16. However, most base models produce representations which are much higher\ndimensional (ranging from 784 to 2048). In order to reduce the dimensionality of these representations,\nwe perform PCA on the output embeddings of the base model, learning the components using only\nthe training points available for that specific task. The test examples are then projected onto these\nprincipal components to produce 16 dimensional input representations.\nC.2\nChoice of representations\nAs discussed in Section 4.2 there are two possible options for forming the representations, the vanilla\nembeddings and the leave-one-out (LOO) embeddings. Figure 19 shows the schematic differences\nbetween the two style of embedding. In Figure 18, we plot the average accuracies across different\ndatasets for both vanilla and LOO embeddings, observing that the LOO embeddings consistently\nperform better across the different model families.\n23\n(a) GPT-Neo (125M)\n(b) Pythia (160M)\n(c) Bloom (560M)\nFigure 18: LOO embeddings vs. Vanilla Embeddings. Comparison of Tart performance when using\nLOO embeddings and vanilla embeddings. Vanilla embeddings see a performance collapse, but LOO\nembeddings do not.\nC.3\nProof of Theorem 1\nIn this section, we provide a formal statement of Theorem 1 from Section 4.3. Our theorem quantifies\nthe expected error of the Transformer, trained on synthetic data, on natural language tasks in terms\nof the change in the two input distributions.\nWe begin by introducing some notation. We denote the class of Transformer family by\nT\u0398 : = {T\u03b8 : R(2k+1)\u00d7d 7\u2192 R\n|\n\u03b8 \u2208 \u0398} ,\n(7)\nwhere k represents the maximum number of in-context examples the Transformer can support, d\nrepresents the input dimensions, and \u0398 represents the corresponding parameter class over which the\nTransformer family is defined.\nObserve that the Transformer family T\u0398 takes as input a sequence of k train examples, each\ncorresponding to two tokens of hidden dimension d: a covariate x \u2208 Rd and a binary label, encoded\nas a one-hot vector in d dimension. This sequence of train examples is followed by a test example,\nfor which we only have the features xk+1.\nGiven this background, let Psyn denote the synthetic distribution over sequences {(x1, y1), . . . , (xk, yx), (xk+1)}.\nSimilarly, let PNL denote the corresponding distribution over sequences derived from natural language\ntasks where xi denotes the LLM embeddings of the example. Recall from Section 4.1.1, the synthetic\ntraining distribution Psyn is given by\nSequence st : wt \u223c N(0, Id),\nxi,t \u223c N(0, Id),\nyi,t \u223c \u03c3(\u03b1\u27e8xi,t, wt\u27e9)\nfor i \u2208 [k] ,\n(8)\nfor each training point (xi,t, yi,t). The test point is also sampled similarly from an independent\nstandard normal distribution. Let \u2113 : R \u00d7 R 7\u2192 R be the loss function used for evaluating the\nperformance of the reasoning module. Further, let use denote the expected loss under a distribution P\nerrP (T) : = E(s,y)\u223cP [\u2113(T(s), y)] ,\n(9)\nand the corresponding empirical distribution over samples S by \u02c6\nerrP , where the dependence on the\nsamples is implicit. Given these samples, we denote the empirical risk minimizer\nTS = arg min\nT \u2208T\u0398\n1\n|S|\nX\ns\u2208S\n\u2113(T(s), y) .\n(10)\nIn addition to these notation, we make the following Lipschitz assumption on the the loss function \u2113\nand the Transformer model T.\nAssumption 1. [Lipschitz loss.] For any two output labels y, y\u2032, the loss function \u2113 is Lipschitz with\nconstant c1, that is,\n|\u2113(Tr(s), y) \u2212 \u2113(Tr(s), y\u2032)| \u2264 c1|y \u2212 y\u2032| .\n(11)\n24\n(a) Vanilla embeddings\n(b) LOO embeddings\nFigure 19: Tart Embedding Protocols. (a) For the vanilla embeddings, the test example is appended to\nthe training set and the sequence is passed to the base model. The representation for each train example\nin this sequence is taken as the average embedding across all its tokens. (b) For the LOO embeddings, we\ngenerate embeddings for each train example separately by placing all the other train examples before it in\nthe prompt and averaging the embeddings over the final example\u2019s tokens.\nAssumption 2. [Lipschitz models.] For any two input sequences s, s\u2032, each any model T \u2208 T\u0398 is\nLipschitz with constant L, that is,\n|T(s) \u2212 T(s\u2032)| \u2264 L\u2225s \u2212 s\u2032\u2225.\n(12)\nGiven this setup, we are now ready to state a formal version of Theorem 1.\nTheorem 2 (Formal version of Theorem 1). Let TS \u2208 T\u0398 denote the trained reasoning module\non set S of synthetic logistic regression tasks with nsyn sequences sampled from distribution Psyn in\neq. (3). Let the loss function \u2113 satisfy Assumption 1 and the model class T\u0398 satisfy Assumption 2.\nThen, with probability at least 1 \u2212 \u03b4, we have\nerrPNL(TS) \u2264 c1 max(1, L) \u00b7 W1(PNL, Psyn) + c1 \u00b7\ns\n2VC(T\u0398) ln m\nnsyn\n+ 4\ns\n2 ln(4/\u03b4)\nnsyn\n+ \u02c6\nerrPsyn(TS) ,\n(13)\n25\nwhere W1 denotes the Wasserstein-1 metric and VC(T\u0398) represents the VC dimension of class T\u0398\nProof. We begin by decomposing the error errPNL(TS) into three components as\nerrPNL(TS) = errPNL(TS) \u2212 errPsyn(TS)\n|\n{z\n}\n(I)\n+ errPsyn(TS) \u2212 \u02c6\nerrPsyn(TS)\n|\n{z\n}\n(II)\n+ \u02c6\nerrPsyn(TS) .\n(14)\nWe now upper bound each of the terms (I) and (II) separately.\nBound on Term (I).\nLet \u03b3 denote an arbitrary joint distribution over the distributions Psyn and\nPNL. Then, we can bound the first term as\nerrPNL(TS) \u2212 errPsyn(TS) = EPNL[\u2113(T(s), y)] \u2212 EPsyn[\u2113(T(s\u2032), y\u2032)]\n(i)= E\u03b3[\u2113(T(s), y) \u2212 \u2113(T(s\u2032), y\u2032)]\n(ii)\n\u2264 inf\n\u03b3 E\u03b3 |\u2113(T(s), y) \u2212 \u2113(T(s\u2032), y\u2032)| ,\n(15)\nwhere (i) follows from the independence of the two expectations and (ii) follows from that (i) holds\nfor any arbitrary joint distribution \u03b3. The final bound on this term now follows:\ninf\n\u03b3 E\u03b3 |\u2113(T(s), y) \u2212 \u2113(T(s\u2032), y\u2032)| = inf\n\u03b3\nZ\n|\u2113(T(s), y) \u2212 \u2113(T(s), y\u2032) + \u2113(T(s), y\u2032) \u2212 \u2113(T(s\u2032), y\u2032)| d\u03b3\n(i)\n\u2264 c1 inf\n\u03b3\nZ\n|y \u2212 y\u2032| \u2212 \u2225T(s\u2032) \u2212 T(s)\u2225d\u03b3\n(ii)\n\u2264 c1 max(1, L) \u00b7 inf\n\u03b3\nZ\n|y \u2212 y\u2032| \u2212 \u2225s\u2032 \u2212 s\u2225d\u03b3\n= c1 max(1, L) \u00b7 W1(PNL, Psyn) ,\n(16)\nwhere the inequalities (i) follows from Assumption 1 and (ii) follows from Assumption 2. This\ncompletes the bound on Term (I).\nBound on Term (II).\nUsing a standard generalization bound [SB14, see Theorem 26.5], we have\nwith probability at least 1 \u2212 \u03b4\nerrPsyn(TS) \u2212 \u02c6\nerrPsyn(TS) \u2264 R(\u2113 \u25e6 T\u0398) + 4\ns\n2 ln(4/\u03b4)\nnsyn\n\u2264 c1 \u00b7 R(T\u0398) + 4\ns\n2 ln(4/\u03b4)\nnsyn\n(i)\n\u2264 c1 \u00b7\ns\n2VC(T\u0398) ln m\nnsyn\n+ 4\ns\n2 ln(4/\u03b4)\nnsyn\n(17)\nwhere R(\u2113 \u25e6 T\u0398) denotes the Rademacher complexity of the class T\u0398 composed with the loss function\n\u2113 and inequality (i) follows from Sauer\u2019s Lemma.\nCombining the bounds in equations (16) and (17) completes the proof of the theorem.\nD\nDetails for experimental evaluation\nWe describe supplementary experimental details from Section 5 as well as additional results for the\nnatural language benchmark evaluations (Section D.2) and results for other modalities (vision and\naudio) (Section D.3).\n26\nDataset\nTest size\nMax char\nMax token\nAvg. char\nAvg. token\nlength\nlength\nlength\nlength\nAG-News-0\n3800\n732\n259\n237.07\n51.36\nAG-News-1\n3800\n814\n213\n232.01\n51.46\nAG-News-2\n3800\n814\n225\n236.10\n52.25\nAG-News-3\n3800\n892\n259\n234.86\n51.38\nCivil Comments\n11576\n1000\n634\n272.72\n61.73\nDBPedia-0\n10000\n2081\n629\n300.94\n65.83\nDBPedia-1\n10000\n2081\n629\n298.81\n66.91\nDBPedia-2\n10000\n2081\n883\n286.85\n66.53\nDBPedia-3\n10000\n2081\n629\n275.81\n63.88\nIMDB\n25000\n12988\n2972\n1293.79\n292.82\nRotten Tomatoes\n1066\n261\n63\n115.52\n25.36\nSMS Spam\n4181\n612\n258\n81.46\n23.76\nSST\n2210\n256\n60\n102.40\n22.34\nYoutube\n250\n1125\n292\n112.50\n31.84\nTable 2: Dataset (test) statistics for all NLP datasets.\n(a) GPT-Neo (125M)\n(b) Pythia (160M)\n(c) Bloom (560M)\nFigure 20: Comparison of all methods. Tart significantly improves base in-context learning performance\nand is competitive with full-finetuning across model families.\nD.1\nExperimental setup\nWe begin by providing dataset statistics and details of the baselines.\nD.1.1\nDataset construction and statistics\nTable 3 and 2 provides a detailed breakdown of dataset statistics. For each dataset, we use the original\ntest sets with the exception of Civil Comments [Bor+19], AG News [ZZL15] and DBPedia [ZZL15].\nFor the multi-class datasets\u2014AG News [ZZL15] and DBPedia [ZZL15] \u2014 we construct 4 binary\nclassification tasks for each datasets. More concretely, AG News labels news articles into four\ncategories: World, Sports, Business, and Science/Technology. We create a separate binary classification\ntask for each category, sampling negatives from the remaining classes. DBPedia is a 14-way ontology\nclassification dataset. We create 4 separate binary classification tasks for the educational institution,\ncompany, artist, and athlete ontologies, sampling negatives from the remaining classes. For the train\nset, we sample a class-balanced set of 64 examples from the original dataset. For each dataset, we\nsample 5 separate training sets, using 5 different random seeds. In evaluations, we evaluate Tart\nand the baseline methods across each of these 5 different training sets.\n27\nDataset\nMax char\nMax token\nAvg. char\nAvg. token\nlength\nlength\nlength\nlength\nAG-News-0\n701\n256\n236.15\n51.53\nAG-News-1\n749\n180\n232.48\n51.61\nAG-News-2\n735\n256\n241.70\n53.91\nAG-News-3\n1002\n258\n241.21\n53.21\nCivil Comments\n1000\n347\n280.97\n63.44\nDBPedia-0\n707\n207\n300.48\n65.79\nDBPedia-1\n1023\n280\n299.89\n66.57\nDBPedia-2\n628\n203\n288.21\n66.22\nDBPedia-3\n758\n203\n279.45\n64.24\nIMDB\n7068\n1630\n1284.20\n290.00\nRotten Tomatoes\n260\n62\n112.46\n24.82\nSMS Spam\n911\n217\n106.90\n31.87\nSST\n248\n56\n101.97\n22.34\nYoutube\n1089\n767\n90.12\n29.98\nTable 3: Dataset (train) statistics for all NLP datasets.\nD.1.2\nBaseline methods\nFor each dataset, we compare Tart to 4 baseline task-adaptation methods: 1) in-context learning, 2)\nfull fine-tuning, 3) last layer fine-tuning, and 4) adapters. The last layer fine-tuning and the adapters\nare trained as follows:\n\u2022 Last layer fine-tuning: Freeze all layers of transformer but the final transformer block and the\nlanguage modeling head.\n\u2022 Adapter: Combine a frozen LLM base transformer model with a trainable adapter head\u2014an\nMLP composed of a single linear layer followed by non-linearity.\nHyperparameter search.\nFor each baseline, we perform an extensive hyperparameter search over\nnumber of epochs and learning rate for each dataset in order to optimize performance. We search\nover a range of learning rates (1e-3, 1e-4, 3e-5, 1e-5, 8e-6), and range of epochs (5, 10, 15, 20, 50).\nFor all models < 1B parameters, we use a batch size of 1. For all models > 1B parameters, we use a\nbatch size of 8. We use these same batch sizes at evaluation time. We perform our hyperparameter\nsearches with a fixed number of train samples (64). We run our hyperparameter searches over 3\nrandom seeds.\nD.2\nNL benchmarks\nIn this section, we provide additional results deferred from Section 5 on the NLP benchmark\nevaluations, RAFT evaluations and demonstration of Tart\u2019s data-scalability.\nD.2.1\nPerformance on benchmark datasets\nFigure 20 shows the performance of the baseline methods with Tart averaged across the suite of\n14 datasets. Tart, while being task-agnostic, shows similar performance quality to task-specific\napproaches across the different model families, and consistently outperforms in-context learning.\nFigures 21, 22, and 23 show the scatter plots of the accuracies of Tart with the baseline methods\nacross datasets and different values of in-context examples k. An interesting observation is that as\n28\nthe number of examples k increases from 18 to 64, the performance of fine-tuning improves at a\nbetter rate than that of Tart.\nD.2.2\nReal-world Annotated Few-shot Tasks (RAFT) evaluation\nFor our evaluations on the RAFT benchmark [Ale+21], we follow the protocol (same train and test\nsets) used in HELM benchmark. The HELM benchmark [Lia+22] contains the evaluation results for\nmany open and closed models enabling us to accurately compare the performance of Tart with\nother models. We evaluate Tart on all RAFT binary classification datasets (twitter-complaints,\nneurips-impact-statement-risks, overulling, ade-corpusv2, tweet-eval-hate, terms-of-service, tai-safety-\nresearch) with the exception of systematic-review-inclusion which contains zero positive samples in\nthe train set. Tart requires at least one example of each class in the training set. Table 1 contains a\ndetailed performance comparison of Tart with respect to other models. Tart when combined with\nGPT-Neo (125M) is able to outperform Bloom (176B) and is competitive with OPT (175B)\nand GPT-3 (175B), all of which have 1000x more parameters.\nD.2.3\nBeyond context length: Tart is data-scalable\nSetup.\nFor these evaluations, we use the a subset of 6 datasets: AG-News-0, DBPedia-0, SST, SMS\nSpam, Youtube and Rotten Tomatoes. We evaluate the performance of Tart over k=[18, 32, 48, 64,\n128, 192, 256] where k is the number of in-context examples. When evaluating our base models, we\nevaluate over k=[8, 24]\u2014values of k that maximize the context window. We use a lower-bound of 8\ngiven that the maximum input sequence length in the training set for AG News is 256. With such a\nsequence length, the maximum number of in-context examples that fit in the context-window is 8,\nhence the lower bound.\nEmbeddings.\nFor these evaluations, we use what we call \u201cstreaming\u201d embeddings (see Figure 25).\nIn this setup, we use the context window of the LLM to encode a single example at a time. The\nfinal embeddings are then averaged and used in-context with Tart\u2019s reasoning module. This is in\ncontrast to the vanilla and LOO embeddings which use multiple examples in-context with the base\nLLM to obtain the embeddings.\nEvaluation.\nFigure 24 shows the performance of base in-context learning with Tart across the\nthree different model families. Observe that while in-context learning is bottlenecked by the context\nwindow of the base LLM, Tart is able to learn from 10x more examples and exhibits an increasing\ntrend in accuracy with number of examples across models.\nD.3\nExtension to other modalities: Tart is domain-agnostic!\nWe begin by providng a description of the datasets we used to evaluate Tart on audio and vision\ntasks, and then provide additional results comparing our algorithm with baselines.\nD.3.1\nDataset details\nFor audio classification, we use the Speech Commands (Version 0.01) dataset [War18]. Speech\nCommands is a multi-class classification task where the task is to detect preregistered keywords by\nclassifying utterances into a predefined set of words. We construct a 3 binary classification task over\nthe keywords \u201cstop\u201d and \u201cgo\u201d, \u201cup\u201d and \u201cdown\u201d, and \u201cyes\u201d and \u201cno\u201d (see Table 4 for more details).\nFor image classification, we use CIFAR-10 [Kri09] and MNIST [LCB10]. Both tasks are multi-class\nclassification tasks. We create 3 binary classification tasks for each of the datasets. For CIFAR-10\nthe tasks are: airplane vs. bird, bird vs. horse, and ship vs. automobile. For MNIST the tasks are: 0\nvs. 8, 1 vs. 6 and 2 vs. 4. See Table 4 for more details.\n29\nDataset\nModality\nTrain size\nTest size\nMNIST (0 vs. 8)\nimage\n256\n1954\nMNIST (1 vs. 6)\nimage\n256\n2093\nMNIST (2 vs. 4)\nimage\n256\n2014\nSpeech Commands (stop vs. go)\naudio\n256\n500\nSpeech Commands (up vs. down)\naudio\n256\n508\nSpeech Commands (yes vs. no)\naudio\n256\n525\nCIFAR-10 (airplane vs. bird)\nimage\n256\n2000\nCIFAR-10 (bird vs. horse)\nimage\n256\n2000\nCIFAR-10 (ship vs. automobile)\nimage\n256\n2000\nTable 4: Dataset statistics for all audio and image evaluation datasets.\nFor both the audio and image datasets, we sample a class-balanced set of 256 samples from the\ntraining set. For the test sets, we filter the original test sets to only include samples of the two\nclasses we are learning to predict for (i.e., airplane and bird for CIFAR10 and 0 and 8 for MNIST).\nD.3.2\nAlgorithms for comparison\nFor these evaluations, we use the \u201cstreaming embeddings\u201d described in Figure 25 to obtain the\nembedding for Tart. We evaluate over k=[18, 32, 48, 64, 128, 256].\nWe compare against two baseline task-adaptation methods: 1) full fine-tuning and 2) adapters.\nWe use the same architectures as described in Appendix D.1.2. For vision tasks, we use Google\u2019s\n307M parameter pretrained Vision Transformer (ViT) model [Wu+20]: ViT-large-patch16-224-\nin21k. For audio tasks, we use OpenAI\u2019s 1.5B parameter pretrained Whisper model [Rad+22]:\nWhisper-large.\nHyperparameter search\nFor each baseline, we perform an extensive hyperparameter search over\nnumber of epochs and learning rate for each dataset in order to optimize performance. We search\nover a range of learning rates (1e-3, 5e-04, 1e-4, 5e-5, 1e-5, and 8e-6) and a range of epochs (5, 10,\n15 and 20). For all models we use a batch size of 1. We perform our hyperparameter searches for a\nfixed number of train samples (128) and run our hyperparameter searches over 3 random seeds.\nD.3.3\nEvaluation\nWe plot the accuracy as a function of the number of examples for Tart, fine-tuning and adapter in\nFigure 26 (MNIST), Figure 27 (CIFAR-10), and Figure 28 (Speech Commands). Tart is competitive\nwith both these baselines, showing how task-agnostic methods can compete with task-specific\nadaptation methods across different modalities.\nD.4\nGeneralization across architectures\nIn this section, we demonstrate that it is possible to train Tart reasoning modules on architec-\ntures beyond transformers. More concretely, we train a reasoning module that has the Hyena\narchitecture [Pol+23] using the same synthetic logistic regression tasks.\nD.4.1\nSetup and Hyper-parameters\nWe instantiate a reasoning module with 12 Hyena blocks, a hidden dimension size of 256, and a\nsequence length of 2050. We train with a batch size of 16, using a learning rate of 5e-05. We sample\n30\ndata with a noise parameter (\u03b1) equal to 1. We train the model for 5000 steps. For our evaluations,\nwe use the final checkpoint (i.e., 5000) of the reasoning module.\nD.4.2\nEvaluation\nWe evaluate the Hyena-based reasoning module applied to GPT-Neo (125M) on 6 datasets: SMS\nSpam, SST, AG-News-0, DBpedia-14-0, Youtube, and Rotten Tomatoes. As seen in Figure 29, the\nHyena-based reasoning module transfers to natural language tasks and performs competitively with\nthe transformer-based reasoning module\n31\nDataset\nTart\nIn-context learning\nFine-tuning full\nFine-tuning layer\nAdapters\nAG-News-0\n0.790 \u00b1 0.036\n0.552 \u00b1 0.049\n0.724 \u00b1 0.070\n0.852 \u00b1 0.019\n0.824 \u00b1 0.031\nAG-News-1\n0.828 \u00b1 0.021\n0.541 \u00b1 0.037\n0.779 \u00b1 0.137\n0.903 \u00b1 0.021\n0.868 \u00b1 0.020\nAG-News-2\n0.751 \u00b1 0.023\n0.513 \u00b1 0.014\n0.626 \u00b1 0.057\n0.765 \u00b1 0.025\n0.755 \u00b1 0.012\nAG-News-3\n0.743 \u00b1 0.031\n0.502 \u00b1 0.017\n0.736 \u00b1 0.035\n0.786 \u00b1 0.025\n0.727 \u00b1 0.066\nCivil Comments\n0.559 \u00b1 0.027\n0.499 \u00b1 0.002\n0.520 \u00b1 0.033\n0.515 \u00b1 0.019\n0.555 \u00b1 0.036\nDBPedia-0\n0.866 \u00b1 0.030\n0.611 \u00b1 0.091\n0.802 \u00b1 0.020\n0.825 \u00b1 0.012\n0.837 \u00b1 0.022\nDBPedia-1\n0.778 \u00b1 0.036\n0.579 \u00b1 0.100\n0.766 \u00b1 0.056\n0.740 \u00b1 0.041\n0.778 \u00b1 0.044\nDBPedia-2\n0.798 \u00b1 0.042\n0.609 \u00b1 0.136\n0.862 \u00b1 0.041\n0.908 \u00b1 0.011\n0.832 \u00b1 0.048\nDBPedia-3\n0.812 \u00b1 0.032\n0.611 \u00b1 0.135\n0.817 \u00b1 0.034\n0.859 \u00b1 0.025\n0.848 \u00b1 0.028\nIMDB\n0.537 \u00b1 0.022\n0.507 \u00b1 0.007\n0.625 \u00b1 0.013\n0.560 \u00b1 0.021\n0.556 \u00b1 0.014\nRotten Tomatoes\n0.535 \u00b1 0.030\n0.550 \u00b1 0.043\n0.689 \u00b1 0.019\n0.541 \u00b1 0.037\n0.524 \u00b1 0.018\nSMS Spam\n0.869 \u00b1 0.063\n0.736 \u00b1 0.099\n0.925 \u00b1 0.011\n0.833 \u00b1 0.015\n0.886 \u00b1 0.023\nSST\n0.544 \u00b1 0.021\n0.542 \u00b1 0.024\n0.715 \u00b1 0.009\n0.555 \u00b1 0.026\n0.547 \u00b1 0.009\nYoutube\n0.784 \u00b1 0.047\n0.658 \u00b1 0.089\n0.833 \u00b1 0.089\n0.768 \u00b1 0.100\n0.715 \u00b1 0.136\nTable 5: Standard deviation of accuracy, number of examples = 18, GPT-Neo (125M)\nDataset\nTart\nIn-context learning\nFine-tuning full\nFine-tuning layer\nAdapters\nAG-News-0\n0.805 \u00b1 0.029\n0.498 \u00b1 0.002\n0.758 \u00b1 0.127\n0.601 \u00b1 0.072\n0.669 \u00b1 0.044\nAG-News-1\n0.825 \u00b1 0.023\n0.517 \u00b1 0.019\n0.916 \u00b1 0.015\n0.702 \u00b1 0.097\n0.690 \u00b1 0.034\nAG-News-2\n0.758 \u00b1 0.027\n0.500 \u00b1 0.000\n0.596 \u00b1 0.031\n0.500 \u00b1 0.001\n0.588 \u00b1 0.026\nAG-News-3\n0.754 \u00b1 0.033\n0.501 \u00b1 0.003\n0.661 \u00b1 0.093\n0.552 \u00b1 0.041\n0.613 \u00b1 0.022\nCivil Comments\n0.575 \u00b1 0.019\n0.500 \u00b1 0.003\n0.525 \u00b1 0.049\n0.500 \u00b1 0.000\n0.515 \u00b1 0.008\nDBPedia-0\n0.861 \u00b1 0.018\n0.508 \u00b1 0.016\n0.786 \u00b1 0.054\n0.638 \u00b1 0.109\n0.704 \u00b1 0.038\nDBPedia-1\n0.813 \u00b1 0.020\n0.499 \u00b1 0.010\n0.787 \u00b1 0.067\n0.599 \u00b1 0.078\n0.710 \u00b1 0.059\nDBPedia-2\n0.870 \u00b1 0.035\n0.502 \u00b1 0.025\n0.878 \u00b1 0.071\n0.701 \u00b1 0.113\n0.767 \u00b1 0.052\nDBPedia-3\n0.850 \u00b1 0.050\n0.502 \u00b1 0.003\n0.864 \u00b1 0.034\n0.603 \u00b1 0.114\n0.734 \u00b1 0.030\nIMDB\n0.550 \u00b1 0.027\n0.507 \u00b1 0.005\n0.590 \u00b1 0.046\n0.500 \u00b1 0.000\n0.526 \u00b1 0.012\nRotten Tomatoes\n0.544 \u00b1 0.027\n0.491 \u00b1 0.013\n0.589 \u00b1 0.074\n0.500 \u00b1 0.000\n0.522 \u00b1 0.023\nSMS Spam\n0.901 \u00b1 0.038\n0.867 \u00b1 0.030\n0.892 \u00b1 0.052\n0.867 \u00b1 0.004\n0.851 \u00b1 0.042\nSST\n0.572 \u00b1 0.016\n0.517 \u00b1 0.002\n0.617 \u00b1 0.074\n0.517 \u00b1 0.000\n0.539 \u00b1 0.007\nYoutube\n0.847 \u00b1 0.047\n0.611 \u00b1 0.084\n0.810 \u00b1 0.060\n0.528 \u00b1 0.000\n0.598 \u00b1 0.103\nTable 6: Standard deviation of accuracy, number of examples = 18, Pythia (160M)\n32\n(a) Number of examples = 18\n(b) Number of examples = 32\n(c) Number of examples = 48\n(d) Number of examples = 64\nFigure 21: Comparison of Tart and task-adaptation approaches (GPT-Neo (125M)). We see\nthat for GPT-Neo (125M), Tart outperforms in-context learning and is competitive with full fine-tuning\nand adapters across all k.\n33\n(a) Number of examples = 18\n(b) Number of examples = 32\n(c) Number of examples = 48\n(d) Number of examples = 64\nFigure 22: Comparison of Tart and task-adaptation approaches (Pythia (160M)). We see\nthat for Pythia (160M), Tart outperforms in-context learning and adapters and is competitive with full\nfine-tuning across all k.\n34\n(a) Number of examples = 18\n(b) Number of examples = 32\n(c) Number of examples = 48\n(d) Number of examples = 64\nFigure 23: Comparison of Tart and task-adaptation approaches (Bloom (560M)). We see\nthat for Bloom (560M), Tart outperforms in-context learning and adapters and is competitive with full\nfine-tuning across all k.\n35\n(a) GPT-Neo (125M)\n(b) Pythia (160M)\n(c) Bloom (560M)\nFigure 24: Beyond context window constraints. Performance comparison with respect to number of\nin-context examples. Base in-context learning is bound with respect to total numbers of examples and\nperformance saturates. Tart is not bound by context length, and performance continues to scale as number\nof examples increases.\nFigure 25: Stream embeddings. Another protocol for generating representations for in-context examples\nwhere each example is embedded by the base model separately.\n(a) MNIST (1 vs. 6)\n(b) MNIST (2 vs. 4)\nFigure 26: Additional MNIST binary classification tasks. TART is competitive with task-specific full\nfine-tuning and adapters.\n36\n(a) CIFAR-10 (ship vs. automobile)\n(b) CIFAR-10 (bird vs. horse)\nFigure 27: Additional CIFAR-10 binary classification tasks. TART is competitive with task-specific\nfull fine-tuning and adapters.\n(a) Speech Commands (yes vs. no)\n(b) Speech Commands (up vs. down)\nFigure 28: Additional Speech Commands binary classification tasks. TART is competitive with\ntask-specific full fine-tuning and adapters.\nDataset\nTart\nIn-context learning\nFine-tuning full\nFine-tuning layer\nAdapters\nAG-News-0\n0.813 \u00b1 0.021\n0.511 \u00b1 0.013\n0.702 \u00b1 0.102\n0.790 \u00b1 0.033\n0.689 \u00b1 0.060\nAG-News-1\n0.851 \u00b1 0.020\n0.511 \u00b1 0.014\n0.801 \u00b1 0.086\n0.891 \u00b1 0.037\n0.752 \u00b1 0.032\nAG-News-2\n0.744 \u00b1 0.034\n0.509 \u00b1 0.009\n0.622 \u00b1 0.083\n0.711 \u00b1 0.070\n0.652 \u00b1 0.047\nAG-News-3\n0.763 \u00b1 0.026\n0.508 \u00b1 0.014\n0.626 \u00b1 0.016\n0.775 \u00b1 0.035\n0.706 \u00b1 0.027\nCivil Comments\n0.561 \u00b1 0.029\n0.489 \u00b1 0.009\n0.491 \u00b1 0.032\n0.540 \u00b1 0.029\n0.533 \u00b1 0.030\nDBPedia-0\n0.851 \u00b1 0.009\n0.531 \u00b1 0.044\n0.811 \u00b1 0.116\n0.813 \u00b1 0.057\n0.812 \u00b1 0.043\nDBPedia-1\n0.760 \u00b1 0.037\n0.546 \u00b1 0.088\n0.750 \u00b1 0.129\n0.718 \u00b1 0.067\n0.754 \u00b1 0.041\nDBPedia-2\n0.800 \u00b1 0.032\n0.567 \u00b1 0.110\n0.850 \u00b1 0.086\n0.904 \u00b1 0.018\n0.851 \u00b1 0.055\nDBPedia-3\n0.848 \u00b1 0.025\n0.528 \u00b1 0.046\n0.739 \u00b1 0.133\n0.785 \u00b1 0.132\n0.849 \u00b1 0.011\nIMDB\n0.552 \u00b1 0.031\n0.550 \u00b1 0.044\n0.630 \u00b1 0.016\n0.608 \u00b1 0.014\n0.526 \u00b1 0.025\nRotten Tomatoes\n0.574 \u00b1 0.029\n0.539 \u00b1 0.031\n0.638 \u00b1 0.037\n0.618 \u00b1 0.049\n0.507 \u00b1 0.010\nSMS Spam\n0.830 \u00b1 0.112\n0.613 \u00b1 0.261\n0.883 \u00b1 0.130\n0.911 \u00b1 0.035\n0.885 \u00b1 0.045\nSST\n0.574 \u00b1 0.019\n0.561 \u00b1 0.062\n0.707 \u00b1 0.024\n0.636 \u00b1 0.025\n0.531 \u00b1 0.015\nYoutube\n0.762 \u00b1 0.116\n0.584 \u00b1 0.144\n0.753 \u00b1 0.140\n0.726 \u00b1 0.052\n0.769 \u00b1 0.084\nTable 7: Standard deviation of accuracy, number of examples = 18, Bloom (560M)\n37\n(a) k = 18\n(b) k = 32\n(c) k = 48\n(d) k = 64\nFigure 29: Comparing TART with a Hyena version of TART. Instead of using a Transformer module,\nwe use a Hyena model to learn the probabilistic reasoning task. Hyena-TART is comparable to TART across\ndatasets and achieves better performance for larger value of k (= 64).\n38\nDataset\nTart\nIn-context learning\nFine-tuning full\nFine-tuning layer\nAdapters\nAG-News-0\n0.808 \u00b1 0.030\n0.551 \u00b1 0.041\n0.795 \u00b1 0.049\n0.874 \u00b1 0.022\n0.830 \u00b1 0.034\nAG-News-1\n0.883 \u00b1 0.014\n0.577 \u00b1 0.055\n0.852 \u00b1 0.070\n0.911 \u00b1 0.021\n0.902 \u00b1 0.011\nAG-News-2\n0.764 \u00b1 0.019\n0.540 \u00b1 0.027\n0.705 \u00b1 0.063\n0.812 \u00b1 0.019\n0.782 \u00b1 0.019\nAG-News-3\n0.798 \u00b1 0.025\n0.521 \u00b1 0.039\n0.762 \u00b1 0.046\n0.813 \u00b1 0.012\n0.806 \u00b1 0.028\nCivil Comments\n0.575 \u00b1 0.054\n0.498 \u00b1 0.002\n0.554 \u00b1 0.029\n0.543 \u00b1 0.022\n0.579 \u00b1 0.044\nDBPedia-0\n0.886 \u00b1 0.021\n0.632 \u00b1 0.096\n0.884 \u00b1 0.019\n0.866 \u00b1 0.013\n0.859 \u00b1 0.020\nDBPedia-1\n0.809 \u00b1 0.031\n0.593 \u00b1 0.062\n0.802 \u00b1 0.030\n0.783 \u00b1 0.022\n0.813 \u00b1 0.023\nDBPedia-2\n0.886 \u00b1 0.011\n0.586 \u00b1 0.078\n0.916 \u00b1 0.029\n0.932 \u00b1 0.013\n0.899 \u00b1 0.017\nDBPedia-3\n0.868 \u00b1 0.022\n0.565 \u00b1 0.041\n0.902 \u00b1 0.016\n0.908 \u00b1 0.012\n0.868 \u00b1 0.023\nIMDB\n0.537 \u00b1 0.040\n0.543 \u00b1 0.029\n0.609 \u00b1 0.029\n0.556 \u00b1 0.024\n0.551 \u00b1 0.036\nRotten Tomatoes\n0.549 \u00b1 0.035\n0.554 \u00b1 0.036\n0.667 \u00b1 0.036\n0.550 \u00b1 0.045\n0.528 \u00b1 0.020\nSMS Spam\n0.903 \u00b1 0.027\n0.768 \u00b1 0.105\n0.931 \u00b1 0.014\n0.861 \u00b1 0.027\n0.920 \u00b1 0.011\nSST\n0.573 \u00b1 0.009\n0.564 \u00b1 0.045\n0.711 \u00b1 0.022\n0.594 \u00b1 0.022\n0.551 \u00b1 0.022\nYoutube\n0.810 \u00b1 0.072\n0.759 \u00b1 0.074\n0.854 \u00b1 0.044\n0.773 \u00b1 0.052\n0.722 \u00b1 0.113\nTable 8: Standard deviation of accuracy, number of examples = 32, GPT-Neo (125M)\nDataset\nTart\nIn-context learning\nFine-tuning full\nFine-tuning layer\nAdapters\nAG-News-0\n0.835 \u00b1 0.020\n0.499 \u00b1 0.002\n0.791 \u00b1 0.100\n0.855 \u00b1 0.017\n0.712 \u00b1 0.028\nAG-News-1\n0.900 \u00b1 0.012\n0.504 \u00b1 0.004\n0.911 \u00b1 0.022\n0.926 \u00b1 0.009\n0.738 \u00b1 0.028\nAG-News-2\n0.773 \u00b1 0.012\n0.510 \u00b1 0.006\n0.687 \u00b1 0.034\n0.742 \u00b1 0.044\n0.683 \u00b1 0.025\nAG-News-3\n0.823 \u00b1 0.024\n0.514 \u00b1 0.017\n0.792 \u00b1 0.022\n0.833 \u00b1 0.011\n0.697 \u00b1 0.023\nCivil Comments\n0.596 \u00b1 0.024\n0.499 \u00b1 0.007\n0.588 \u00b1 0.052\n0.536 \u00b1 0.020\n0.542 \u00b1 0.016\nDBPedia-0\n0.890 \u00b1 0.016\n0.514 \u00b1 0.026\n0.880 \u00b1 0.049\n0.777 \u00b1 0.055\n0.782 \u00b1 0.044\nDBPedia-1\n0.833 \u00b1 0.018\n0.522 \u00b1 0.023\n0.834 \u00b1 0.061\n0.768 \u00b1 0.024\n0.760 \u00b1 0.022\nDBPedia-2\n0.912 \u00b1 0.010\n0.522 \u00b1 0.031\n0.916 \u00b1 0.033\n0.903 \u00b1 0.012\n0.859 \u00b1 0.019\nDBPedia-3\n0.879 \u00b1 0.021\n0.520 \u00b1 0.026\n0.900 \u00b1 0.019\n0.863 \u00b1 0.018\n0.812 \u00b1 0.022\nIMDB\n0.541 \u00b1 0.041\n0.510 \u00b1 0.008\n0.630 \u00b1 0.017\n0.575 \u00b1 0.024\n0.540 \u00b1 0.018\nRotten Tomatoes\n0.576 \u00b1 0.042\n0.495 \u00b1 0.007\n0.659 \u00b1 0.064\n0.568 \u00b1 0.027\n0.542 \u00b1 0.019\nSMS Spam\n0.937 \u00b1 0.017\n0.856 \u00b1 0.080\n0.913 \u00b1 0.044\n0.953 \u00b1 0.014\n0.877 \u00b1 0.036\nSST\n0.602 \u00b1 0.015\n0.509 \u00b1 0.013\n0.705 \u00b1 0.021\n0.605 \u00b1 0.043\n0.542 \u00b1 0.003\nYoutube\n0.862 \u00b1 0.045\n0.626 \u00b1 0.081\n0.874 \u00b1 0.046\n0.762 \u00b1 0.052\n0.654 \u00b1 0.050\nTable 9: Standard deviation of accuracy, number of examples = 32, Pythia (160M)\n39\nDataset\nTart\nIn-context learning\nFine-tuning full\nFine-tuning layer\nAdapters\nAG-News-0\n0.825 \u00b1 0.031\n0.503 \u00b1 0.003\n0.802 \u00b1 0.041\n0.818 \u00b1 0.014\n0.739 \u00b1 0.049\nAG-News-1\n0.880 \u00b1 0.012\n0.516 \u00b1 0.026\n0.898 \u00b1 0.043\n0.912 \u00b1 0.034\n0.829 \u00b1 0.010\nAG-News-2\n0.771 \u00b1 0.009\n0.519 \u00b1 0.025\n0.736 \u00b1 0.071\n0.773 \u00b1 0.039\n0.684 \u00b1 0.040\nAG-News-3\n0.810 \u00b1 0.016\n0.509 \u00b1 0.013\n0.753 \u00b1 0.058\n0.805 \u00b1 0.031\n0.742 \u00b1 0.030\nCivil Comments\n0.587 \u00b1 0.025\n0.500 \u00b1 0.011\n0.549 \u00b1 0.016\n0.564 \u00b1 0.062\n0.555 \u00b1 0.009\nDBPedia-0\n0.857 \u00b1 0.035\n0.592 \u00b1 0.083\n0.801 \u00b1 0.125\n0.822 \u00b1 0.043\n0.834 \u00b1 0.038\nDBPedia-1\n0.802 \u00b1 0.031\n0.558 \u00b1 0.047\n0.870 \u00b1 0.037\n0.800 \u00b1 0.041\n0.813 \u00b1 0.034\nDBPedia-2\n0.879 \u00b1 0.018\n0.609 \u00b1 0.025\n0.938 \u00b1 0.018\n0.920 \u00b1 0.031\n0.903 \u00b1 0.021\nDBPedia-3\n0.866 \u00b1 0.035\n0.634 \u00b1 0.110\n0.812 \u00b1 0.163\n0.911 \u00b1 0.004\n0.876 \u00b1 0.037\nIMDB\n0.553 \u00b1 0.046\n0.541 \u00b1 0.024\n0.636 \u00b1 0.016\n0.600 \u00b1 0.018\n0.536 \u00b1 0.024\nRotten Tomatoes\n0.589 \u00b1 0.037\n0.575 \u00b1 0.039\n0.713 \u00b1 0.029\n0.630 \u00b1 0.026\n0.527 \u00b1 0.015\nSMS Spam\n0.933 \u00b1 0.017\n0.762 \u00b1 0.033\n0.956 \u00b1 0.026\n0.905 \u00b1 0.056\n0.917 \u00b1 0.017\nSST\n0.579 \u00b1 0.032\n0.502 \u00b1 0.020\n0.739 \u00b1 0.013\n0.638 \u00b1 0.035\n0.562 \u00b1 0.017\nYoutube\n0.799 \u00b1 0.117\n0.710 \u00b1 0.182\n0.887 \u00b1 0.033\n0.756 \u00b1 0.115\n0.850 \u00b1 0.024\nTable 10: Standard deviation of accuracy, number of examples = 32, Bloom (560M)\nDataset\nTart\nIn-context learning\nFine-tuning full\nFine-tuning layer\nAdapters\nAG-News-0\n0.812 \u00b1 0.021\n0.560 \u00b1 0.046\n0.832 \u00b1 0.023\n0.876 \u00b1 0.012\n0.833 \u00b1 0.016\nAG-News-1\n0.904 \u00b1 0.009\n0.623 \u00b1 0.051\n0.924 \u00b1 0.026\n0.932 \u00b1 0.007\n0.923 \u00b1 0.009\nAG-News-2\n0.786 \u00b1 0.008\n0.566 \u00b1 0.023\n0.791 \u00b1 0.016\n0.824 \u00b1 0.010\n0.797 \u00b1 0.010\nAG-News-3\n0.822 \u00b1 0.030\n0.533 \u00b1 0.029\n0.800 \u00b1 0.042\n0.840 \u00b1 0.015\n0.825 \u00b1 0.034\nCivil Comments\n0.591 \u00b1 0.029\n0.497 \u00b1 0.003\n0.568 \u00b1 0.038\n0.554 \u00b1 0.029\n0.614 \u00b1 0.028\nDBPedia-0\n0.911 \u00b1 0.011\n0.676 \u00b1 0.097\n0.892 \u00b1 0.017\n0.898 \u00b1 0.012\n0.894 \u00b1 0.023\nDBPedia-1\n0.823 \u00b1 0.025\n0.644 \u00b1 0.115\n0.838 \u00b1 0.058\n0.819 \u00b1 0.018\n0.833 \u00b1 0.026\nDBPedia-2\n0.902 \u00b1 0.015\n0.622 \u00b1 0.114\n0.931 \u00b1 0.032\n0.940 \u00b1 0.005\n0.913 \u00b1 0.007\nDBPedia-3\n0.883 \u00b1 0.023\n0.620 \u00b1 0.129\n0.891 \u00b1 0.015\n0.911 \u00b1 0.006\n0.889 \u00b1 0.021\nIMDB\n0.557 \u00b1 0.033\n0.522 \u00b1 0.025\n0.641 \u00b1 0.008\n0.564 \u00b1 0.017\n0.577 \u00b1 0.020\nRotten Tomatoes\n0.572 \u00b1 0.014\n0.548 \u00b1 0.045\n0.710 \u00b1 0.010\n0.575 \u00b1 0.048\n0.572 \u00b1 0.029\nSMS Spam\n0.882 \u00b1 0.044\n0.860 \u00b1 0.036\n0.946 \u00b1 0.019\n0.881 \u00b1 0.013\n0.925 \u00b1 0.010\nSST\n0.570 \u00b1 0.019\n0.563 \u00b1 0.032\n0.705 \u00b1 0.019\n0.575 \u00b1 0.028\n0.542 \u00b1 0.035\nYoutube\n0.810 \u00b1 0.039\n0.792 \u00b1 0.081\n0.923 \u00b1 0.014\n0.874 \u00b1 0.029\n0.832 \u00b1 0.053\nTable 11: Standard deviation of accuracy, number of examples = 48, GPT-Neo (125M)\n40\nDataset\nTart\nIn-context learning\nFine-tuning full\nFine-tuning layer\nAdapters\nAG-News-0\n0.846 \u00b1 0.018\n0.496 \u00b1 0.007\n0.871 \u00b1 0.015\n0.871 \u00b1 0.005\n0.732 \u00b1 0.036\nAG-News-1\n0.923 \u00b1 0.005\n0.528 \u00b1 0.044\n0.942 \u00b1 0.004\n0.935 \u00b1 0.009\n0.780 \u00b1 0.040\nAG-News-2\n0.798 \u00b1 0.009\n0.509 \u00b1 0.004\n0.731 \u00b1 0.040\n0.782 \u00b1 0.018\n0.722 \u00b1 0.019\nAG-News-3\n0.845 \u00b1 0.012\n0.515 \u00b1 0.014\n0.787 \u00b1 0.046\n0.852 \u00b1 0.011\n0.718 \u00b1 0.026\nCivil Comments\n0.605 \u00b1 0.037\n0.512 \u00b1 0.009\n0.607 \u00b1 0.039\n0.556 \u00b1 0.014\n0.556 \u00b1 0.018\nDBPedia-0\n0.905 \u00b1 0.020\n0.549 \u00b1 0.057\n0.924 \u00b1 0.020\n0.830 \u00b1 0.034\n0.798 \u00b1 0.021\nDBPedia-1\n0.840 \u00b1 0.021\n0.592 \u00b1 0.062\n0.867 \u00b1 0.024\n0.808 \u00b1 0.031\n0.780 \u00b1 0.011\nDBPedia-2\n0.916 \u00b1 0.009\n0.636 \u00b1 0.081\n0.937 \u00b1 0.017\n0.923 \u00b1 0.013\n0.870 \u00b1 0.022\nDBPedia-3\n0.888 \u00b1 0.022\n0.618 \u00b1 0.096\n0.927 \u00b1 0.011\n0.885 \u00b1 0.007\n0.827 \u00b1 0.026\nIMDB\n0.557 \u00b1 0.034\n0.503 \u00b1 0.008\n0.632 \u00b1 0.013\n0.566 \u00b1 0.030\n0.545 \u00b1 0.008\nRotten Tomatoes\n0.601 \u00b1 0.026\n0.480 \u00b1 0.014\n0.683 \u00b1 0.033\n0.556 \u00b1 0.033\n0.562 \u00b1 0.018\nSMS Spam\n0.956 \u00b1 0.008\n0.869 \u00b1 0.044\n0.966 \u00b1 0.011\n0.960 \u00b1 0.008\n0.891 \u00b1 0.035\nSST\n0.612 \u00b1 0.024\n0.500 \u00b1 0.025\n0.702 \u00b1 0.046\n0.594 \u00b1 0.042\n0.560 \u00b1 0.023\nYoutube\n0.872 \u00b1 0.019\n0.654 \u00b1 0.063\n0.904 \u00b1 0.033\n0.826 \u00b1 0.046\n0.666 \u00b1 0.027\nTable 12: Standard deviation of accuracy, number of examples = 48, Pythia (160M)\nDataset\nTart\nIn-context learning\nFine-tuning full\nFine-tuning layer\nAdapters\nAG-News-0\n0.829 \u00b1 0.029\n0.507 \u00b1 0.005\n0.829 \u00b1 0.071\n0.847 \u00b1 0.022\n0.788 \u00b1 0.028\nAG-News-1\n0.909 \u00b1 0.010\n0.543 \u00b1 0.052\n0.940 \u00b1 0.011\n0.935 \u00b1 0.004\n0.856 \u00b1 0.014\nAG-News-2\n0.789 \u00b1 0.016\n0.511 \u00b1 0.012\n0.715 \u00b1 0.057\n0.793 \u00b1 0.027\n0.708 \u00b1 0.011\nAG-News-3\n0.832 \u00b1 0.018\n0.530 \u00b1 0.038\n0.834 \u00b1 0.029\n0.843 \u00b1 0.014\n0.763 \u00b1 0.015\nCivil Comments\n0.602 \u00b1 0.030\n0.506 \u00b1 0.011\n0.605 \u00b1 0.044\n0.592 \u00b1 0.039\n0.566 \u00b1 0.008\nDBPedia-0\n0.889 \u00b1 0.022\n0.703 \u00b1 0.085\n0.933 \u00b1 0.021\n0.888 \u00b1 0.024\n0.873 \u00b1 0.027\nDBPedia-1\n0.817 \u00b1 0.023\n0.734 \u00b1 0.076\n0.891 \u00b1 0.028\n0.836 \u00b1 0.036\n0.834 \u00b1 0.024\nDBPedia-2\n0.900 \u00b1 0.011\n0.847 \u00b1 0.037\n0.949 \u00b1 0.013\n0.940 \u00b1 0.009\n0.914 \u00b1 0.013\nDBPedia-3\n0.884 \u00b1 0.020\n0.815 \u00b1 0.105\n0.928 \u00b1 0.024\n0.917 \u00b1 0.015\n0.891 \u00b1 0.037\nIMDB\n0.565 \u00b1 0.033\n0.545 \u00b1 0.033\n0.641 \u00b1 0.020\n0.604 \u00b1 0.022\n0.542 \u00b1 0.015\nRotten Tomatoes\n0.605 \u00b1 0.015\n0.505 \u00b1 0.005\n0.704 \u00b1 0.025\n0.672 \u00b1 0.042\n0.531 \u00b1 0.018\nSMS Spam\n0.933 \u00b1 0.009\n0.636 \u00b1 0.130\n0.929 \u00b1 0.058\n0.919 \u00b1 0.032\n0.914 \u00b1 0.027\nSST\n0.610 \u00b1 0.030\n0.489 \u00b1 0.004\n0.695 \u00b1 0.020\n0.673 \u00b1 0.029\n0.538 \u00b1 0.011\nYoutube\n0.834 \u00b1 0.049\n0.797 \u00b1 0.090\n0.805 \u00b1 0.095\n0.851 \u00b1 0.020\n0.865 \u00b1 0.012\nTable 13: Standard deviation of accuracy, number of examples = 48, Bloom (560M)\n41\nDataset\nTart\nIn-context learning\nFine-tuning full\nFine-tuning layer\nAdapters\nAG-News-0\n0.817 \u00b1 0.024\n0.539 \u00b1 0.044\n0.855 \u00b1 0.030\n0.877 \u00b1 0.013\n0.830 \u00b1 0.023\nAG-News-1\n0.904 \u00b1 0.007\n0.561 \u00b1 0.048\n0.923 \u00b1 0.027\n0.939 \u00b1 0.003\n0.922 \u00b1 0.010\nAG-News-2\n0.790 \u00b1 0.026\n0.576 \u00b1 0.022\n0.814 \u00b1 0.009\n0.839 \u00b1 0.009\n0.816 \u00b1 0.014\nAG-News-3\n0.833 \u00b1 0.015\n0.550 \u00b1 0.035\n0.803 \u00b1 0.017\n0.852 \u00b1 0.013\n0.842 \u00b1 0.018\nCivil Comments\n0.581 \u00b1 0.039\n0.499 \u00b1 0.002\n0.587 \u00b1 0.018\n0.576 \u00b1 0.039\n0.605 \u00b1 0.036\nDBPedia-0\n0.915 \u00b1 0.005\n0.652 \u00b1 0.095\n0.917 \u00b1 0.011\n0.908 \u00b1 0.019\n0.909 \u00b1 0.025\nDBPedia-1\n0.825 \u00b1 0.037\n0.633 \u00b1 0.104\n0.838 \u00b1 0.032\n0.829 \u00b1 0.012\n0.852 \u00b1 0.011\nDBPedia-2\n0.887 \u00b1 0.020\n0.606 \u00b1 0.088\n0.950 \u00b1 0.007\n0.952 \u00b1 0.011\n0.916 \u00b1 0.014\nDBPedia-3\n0.873 \u00b1 0.033\n0.611 \u00b1 0.136\n0.898 \u00b1 0.034\n0.927 \u00b1 0.007\n0.909 \u00b1 0.008\nIMDB\n0.558 \u00b1 0.029\n0.515 \u00b1 0.024\n0.646 \u00b1 0.010\n0.563 \u00b1 0.021\n0.566 \u00b1 0.035\nRotten Tomatoes\n0.601 \u00b1 0.020\n0.533 \u00b1 0.053\n0.708 \u00b1 0.015\n0.579 \u00b1 0.049\n0.556 \u00b1 0.033\nSMS Spam\n0.869 \u00b1 0.023\n0.825 \u00b1 0.074\n0.934 \u00b1 0.016\n0.898 \u00b1 0.008\n0.927 \u00b1 0.004\nSST\n0.570 \u00b1 0.038\n0.554 \u00b1 0.041\n0.712 \u00b1 0.033\n0.609 \u00b1 0.021\n0.567 \u00b1 0.016\nYoutube\n0.790 \u00b1 0.050\n0.819 \u00b1 0.050\n0.926 \u00b1 0.014\n0.891 \u00b1 0.031\n0.837 \u00b1 0.055\nTable 14: Standard deviation of accuracy, number of examples = 64, GPT-Neo (125M)\nDataset\nTart\nIn-context learning\nFine-tuning full\nFine-tuning layer\nAdapters\nAG-News-0\n0.851 \u00b1 0.024\n0.502 \u00b1 0.003\n0.858 \u00b1 0.009\n0.869 \u00b1 0.010\n0.764 \u00b1 0.030\nAG-News-1\n0.925 \u00b1 0.002\n0.529 \u00b1 0.030\n0.937 \u00b1 0.011\n0.938 \u00b1 0.004\n0.820 \u00b1 0.021\nAG-News-2\n0.812 \u00b1 0.007\n0.513 \u00b1 0.013\n0.752 \u00b1 0.051\n0.795 \u00b1 0.010\n0.738 \u00b1 0.008\nAG-News-3\n0.851 \u00b1 0.007\n0.503 \u00b1 0.004\n0.820 \u00b1 0.020\n0.855 \u00b1 0.009\n0.741 \u00b1 0.019\nCivil Comments\n0.606 \u00b1 0.029\n0.500 \u00b1 0.001\n0.659 \u00b1 0.032\n0.566 \u00b1 0.023\n0.566 \u00b1 0.018\nDBPedia-0\n0.910 \u00b1 0.013\n0.518 \u00b1 0.020\n0.912 \u00b1 0.027\n0.858 \u00b1 0.019\n0.825 \u00b1 0.015\nDBPedia-1\n0.839 \u00b1 0.027\n0.542 \u00b1 0.028\n0.897 \u00b1 0.016\n0.824 \u00b1 0.031\n0.788 \u00b1 0.018\nDBPedia-2\n0.916 \u00b1 0.011\n0.609 \u00b1 0.106\n0.953 \u00b1 0.013\n0.922 \u00b1 0.012\n0.882 \u00b1 0.019\nDBPedia-3\n0.887 \u00b1 0.028\n0.527 \u00b1 0.022\n0.940 \u00b1 0.013\n0.904 \u00b1 0.007\n0.857 \u00b1 0.020\nIMDB\n0.556 \u00b1 0.024\n0.506 \u00b1 0.005\n0.619 \u00b1 0.030\n0.574 \u00b1 0.017\n0.552 \u00b1 0.009\nRotten Tomatoes\n0.624 \u00b1 0.024\n0.485 \u00b1 0.020\n0.686 \u00b1 0.040\n0.577 \u00b1 0.033\n0.568 \u00b1 0.019\nSMS Spam\n0.937 \u00b1 0.018\n0.905 \u00b1 0.017\n0.960 \u00b1 0.021\n0.961 \u00b1 0.006\n0.899 \u00b1 0.014\nSST\n0.606 \u00b1 0.022\n0.508 \u00b1 0.022\n0.688 \u00b1 0.047\n0.602 \u00b1 0.036\n0.567 \u00b1 0.017\nYoutube\n0.888 \u00b1 0.028\n0.715 \u00b1 0.097\n0.897 \u00b1 0.046\n0.878 \u00b1 0.050\n0.675 \u00b1 0.019\nTable 15: Standard deviation of accuracy, number of examples = 64, Pythia (160M)\n42\nDataset\nTart\nIn-context learning\nFine-tuning full\nFine-tuning layer\nAdapters\nAG-News-0\n0.836 \u00b1 0.018\n0.509 \u00b1 0.008\n0.850 \u00b1 0.027\n0.856 \u00b1 0.008\n0.799 \u00b1 0.029\nAG-News-1\n0.918 \u00b1 0.007\n0.543 \u00b1 0.033\n0.900 \u00b1 0.024\n0.933 \u00b1 0.008\n0.875 \u00b1 0.015\nAG-News-2\n0.799 \u00b1 0.012\n0.515 \u00b1 0.019\n0.784 \u00b1 0.018\n0.831 \u00b1 0.022\n0.732 \u00b1 0.017\nAG-News-3\n0.836 \u00b1 0.011\n0.504 \u00b1 0.003\n0.811 \u00b1 0.034\n0.853 \u00b1 0.011\n0.784 \u00b1 0.022\nCivil Comments\n0.602 \u00b1 0.030\n0.510 \u00b1 0.012\n0.573 \u00b1 0.035\n0.611 \u00b1 0.025\n0.572 \u00b1 0.013\nDBPedia-0\n0.905 \u00b1 0.015\n0.667 \u00b1 0.052\n0.936 \u00b1 0.018\n0.902 \u00b1 0.022\n0.882 \u00b1 0.020\nDBPedia-1\n0.809 \u00b1 0.022\n0.687 \u00b1 0.117\n0.887 \u00b1 0.039\n0.852 \u00b1 0.032\n0.853 \u00b1 0.008\nDBPedia-2\n0.881 \u00b1 0.026\n0.799 \u00b1 0.075\n0.955 \u00b1 0.022\n0.947 \u00b1 0.009\n0.922 \u00b1 0.014\nDBPedia-3\n0.877 \u00b1 0.014\n0.793 \u00b1 0.106\n0.906 \u00b1 0.027\n0.921 \u00b1 0.017\n0.899 \u00b1 0.024\nIMDB\n0.571 \u00b1 0.033\n0.539 \u00b1 0.020\n0.621 \u00b1 0.039\n0.618 \u00b1 0.013\n0.542 \u00b1 0.012\nRotten Tomatoes\n0.597 \u00b1 0.025\n0.536 \u00b1 0.047\n0.684 \u00b1 0.053\n0.672 \u00b1 0.041\n0.543 \u00b1 0.024\nSMS Spam\n0.907 \u00b1 0.045\n0.659 \u00b1 0.133\n0.942 \u00b1 0.024\n0.931 \u00b1 0.030\n0.909 \u00b1 0.033\nSST\n0.620 \u00b1 0.039\n0.495 \u00b1 0.015\n0.672 \u00b1 0.039\n0.716 \u00b1 0.032\n0.554 \u00b1 0.020\nYoutube\n0.844 \u00b1 0.059\n0.765 \u00b1 0.137\n0.879 \u00b1 0.058\n0.865 \u00b1 0.033\n0.883 \u00b1 0.016\nTable 16: Standard deviation of accuracy, number of examples = 64, Bloom (560M)\n43\n"
  },
  {
    "title": "Image Captioners Are Scalable Vision Learners Too",
    "link": "https://arxiv.org/pdf/2306.07915.pdf",
    "upvote": "9",
    "text": "Image Captioners Are Scalable Vision Learners Too\nMichael Tschannen\u25e6,\u2020\nManoj Kumar\u25e6\nAndreas Steiner\u25e6\nXiaohua Zhai\nNeil Houlsby\nLucas Beyer\u25e6\nGoogle DeepMind\nAbstract\nContrastive pretraining on image-text pairs from the web is one of the most popular\nlarge-scale pretraining strategies for vision backbones, especially in the context of\nlarge multimodal models. At the same time, image captioning on this type of data\nis commonly considered an inferior pretraining strategy. In this paper, we perform\na fair comparison of these two pretraining strategies, carefully matching training\ndata, compute, and model capacity. Using a standard encoder-decoder transformer,\nwe find that captioning alone is surprisingly effective: on classification tasks,\ncaptioning produces vision encoders competitive with contrastively pretrained\nencoders, while surpassing them on vision & language tasks. We further analyze\nthe effect of the model architecture and scale, as well as the pretraining data on the\nrepresentation quality, and find that captioning exhibits the same or better scaling\nbehavior along these axes. Overall our results show that plain image captioning is\na more powerful pretraining strategy than was previously believed.\n1\nIntroduction\nContrastive language image pretraining (CLIP) [50] has recently become a very popular pretraining\nstrategy, enabling scalable vision-language pretraining on billions of image-text pairs collected from\nthe web. CLIP not only enables zero-shot transfer to arbitrary image classification and retrieval\ntasks, but also produces high-quality vision backbones rivaling the best label-supervised ones [14].\nThe corresponding checkpoints [50, 9, 58] are among the most powerful publicly available vision\nbackbones, enjoying wide adoption in the context of multi-modal vision-language models, where\noften a pretrained vision backbone is combined with a pretrained language model [1, 6, 62, 23, 17, 5].\nBefore contrastive image-text pretraining was popularized by CLIP [50] and ALIGN [26], a number\nof works explored generative image captioning [13, 54] and n-gram prediction [27, 37] approaches\nfor representation learning at small scale. However, [50] showed that the zero-shot classification\nperformance of contrastive pretraining scales much better than captioning as a function of the number\nof training examples seen (see [50, Fig. 2]). Since then, follow-up works of [50] which focus on\npretraining from scratch do usually not rely on captioning alone, but augment it with a contrastive\nloss [66, 34, 40]. Those follow-up works based on generative captioning alone typically rely on\ncross-modal encoders with early fusion of image and text, targeting visual question answering (VQA)\nand/or captioning tasks with the full encoder-decoder system [63, 22, 10], and do not study the\nproperties of the vision backbone alone.\nWe revisit image captioning as a pretraining task for learning general vision encoders from web\nimage-text pairs. We compare CLIP-style models with image captioning ones consisting of a Vision\nTransformer (ViT) [15] encoder and a standard Transformer decoder, henceforth simply referred to\nas Captioner (Cap). In our experiments, we carefully match pretraining compute and model capacity,\nand train both models for the same number of examples.\n\u25e6Significant technical contributions. \u2020MT led the project.\nCode is available at https://github.com/google-research/big_vision.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2306.07915v5  [cs.CV]  21 Dec 2023\nVision \nTransformer\nTransformer \nEncoder\n\"A small cow\"\na\nsm? ?al l\ncow\nContrastive loss\n[ D]\n[ D]\nVision \nTransformer\nTransformer \nDecoder\n\"A small cow\"\n[ M, D]\na\nsm? ?al l\ncow\nBOS\na\nsm?\n?al l\ncow\nEOS\nVision \nTransformer\nTransformer \nDecoder\n[ M, D]\nMSK\nMSK\nMSK\nMSK\nCross-entropy Loss\n[ N, V]\nCap\nCapPa\na\nsm? ?al l\ncow\nCross-entropy Loss\n[ N, V]\nSelf-attn mask\nN\nN\nSelf-attn mask\nN\nN\n(causal)\nCross-attn\nCross-attn\nFigure 1: Contrastive models (left) use two separate Transformer encoders to extract vector repre-\nsentations from image-text pairs, which are then matched across a potentially large batch [50]. Cap\n(middle) uses a Transformer encoder-decoder architecture [60] and predicts text tokens autoregres-\nsively. During training, all tokens are predicted in parallel by shifting the expected output by one\ntoken and applying a causal self-attention mask (teacher forcing). In parallel decoding (right) the\nTransformer decoder has to predict all tokens at once, conditioned only on the image. CapPa trains a\nsingle model switching between autoregressive and parallel decoding.\nD: model width, M: number of image tokens, N: number of text tokens, V: vocabulary size.\nWhile our results confirm the findings of [50] that Cap models generally lag contrastive ones in\nzero-shot classification accuracy, the gap becomes smaller with increasing scale. More importantly,\nthe Cap vision backbone matches or outperforms comparable CLIP models in few-shot classification\nand obtains competitive performance when transferring to classification tasks with large labeled data\nsets. When combining the vision encoder with a randomly initialized transformer decoder, the Cap\npretrained encoder outperforms the CLIP one on captioning, OCR, and VQA tasks. This indicates\nthat a Cap vision backbone might be better suited for multimodal downstream tasks. These benefits\ncan be even more pronounced when combining the vision encoder with a pretrained, partially frozen\nlanguage decoder similar to [1, 6, 62, 23, 17, 5].\nWe further propose the CapPa pretraining procedure: the decoder training alternates between standard\nautoregressive prediction (Cap) and parallel prediction (Pa), where the entire caption is predicted\nin a single decoder forward pass. This only changes the decoder text input (all input tokens are\nset to [MASK] tokens) and self-attention mask (no mask instead of a causal mask) while requiring\nno change in the loss or architecture. This mixed training, termed CapPa, significantly improves\nclassification accuracy of the vision backbone.\nWe ablate how various training and architecture design choices impact performance, and discover\npromising scaling properties of captioners. Finally, we show that Cap achieves state-of-the-art perfor-\nmance in zero-shot classification tasks which require careful handling of word order and attributes,\nrather than treating the query as a bag-of-words, as measured by ARO [67] and SugarCrepe [21].\nOverall, our results show that pretraining a simple encoder-decoder architecture via image captioning\nalone can produce vision encoders competitive with CLIP and presents an interesting alternative to\ncontrastive pretraining\u2014in particular for multimodal vision-language models built form pretrained\nvision and language modeling components.\n2\nRelated work\nLarge scale contrastive vision-language pretraining was popularized by CLIP [50] and ALIGN [26].\nSeveral works investigated scaling beyond these works along several relevant axes [49, 69] or using\npretrained vision and language backbones with contrastive training [70].\nRecent works investigating plain image captioning for pretraining are [13, 54]; [27, 37] study n-\ngram prediction from images, which can be considered a precursor to captioning. However, image\ncaptioning as a pretraining task to learn general vision representations did not attract as much attention\nas contrastive training, possibly because of inferior and less efficient zero-shot transfer capabilities.\nRelated to captioning, SimVLM [63] uses prefix language modeling to pretrain a multimodal encoder-\n2\ndecoder model with early vision-language fusion and hybrid convolutional/transformer vision encoder,\ntargeting transfer to VQA and captioning. Further, encoder-decoder models for document, website,\nand UI understanding are often pretrained to generate captions from rendered websites/documents\nwhich trains the model to extract features from those data types and perform OCR [36, 29, 38].\nFocusing on image captioning only, LEMON [22] investigates scaling of an encoder-decoder model.\nSeveral works have combined contrastive and captioning losses [66, 40, 34], optionally using a\nseparate text encoder in addition to the decoder. While obtaining excellent results on a range of\nvision and vision-language tasks, the impact of the loss terms are not well disentangled, nor are\ncompute-matched comparisons of individual losses provided.\nImage captioning is a popular ingredient to build large multimodal models from separately pretrained\nvision and language models [1, 6, 62, 23, 17, 5]. Some of these models freeze large parts of vision\nand language components, sometimes only learning a small adapter between the two [1, 17, 39]. It is\ninteresting to see how the pretraining strategy for the vision model affects this type of adaptation.\nFinally, masked image-text modeling, often combined with image-text matching, is a popular pre-\ntraining strategy for encoder-only vision-language models at small data scale [24, 30, 41, 64, 56, 16].\n3\nA simple recipe to pretrain vision encoders via image captioning\nOur goal is to establish a pretraining recipe based on image captioning that is comparable to CLIP in\nterms of simplicity, scalability, and efficiency. Therefore, we adopt Vision Transformer (ViT) [15]\nbackbones as vision encoders, and use a standard Transformer decoder architecture [60] to predict\nimage captions, feeding the ViT-encoded sequence to the decoder via cross-attention. As is common\nin recent literature [52, 11], we remove biases from attention layers, MLP blocks, and LayerNorms\nand we replace ReLU by GELU. The decoder input and output embeddings are not shared. Ablations\nfor these choices are in Section 4.4. The decoder has the same width, attention-heads, and MLP\ndimension as the encoder, but has half the depth. This leads to captioning models which have slightly\nlower parameter count than corresponding CLIP models, but require about the same pretraining\ncompute in terms of accelerator hours per epoch (see Table 1). We rely on standard next-token-\nprediction language modeling and train our captioning models with causal attention mask and teacher\nforcing (Fig. 1, middle), henceforth referring to this variant simply as Captioner (Cap).\nParallel prediction\nWe also experiment with a slightly modified training recipe (Fig. 1, right):\nInstead of training the model only for autoregressively, we train it to predict all tokens in parallel for\na fraction of the training examples instead (sampling the prediction type for every example at random\nthroughout training). To this end, we replace the (shifted) decoder input sequence with a sequence\nof all [MASK] tokens, and drop the causal decoder attention mask. We emphasize that this kind of\nparallel prediction does not modify the training objective or decoder architecture, and does not incur\nany extra training cost, but simply modifies the decoder input sequence and attention mask for a\nfraction of training examples. Moreover, this is different from bag-of-word prediction as not only the\npresence but also the position of each token has to be predicted. We perform parallel prediction for\n75% of the training examples by default and term this variant Cap with parallel prediction (CapPa).\nIntuitively, captioning via next token prediction induces an implicit weighting on the supervisory\nsignal of the caption tokens: To predict the first few tokens of a caption, the decoder can benefit a\nlot from using the image information, while to predict later tokens it can rely more and more on\nalready predicted tokens. Consequently, early tokens might provide a stronger supervisory signal to\nthe encoder than later tokens. By contrast, when predicting all the caption tokens independently in\nparallel, the decoder can only rely on the image information to predict each token.\nImplementation aspects\nWe emphasize that our approach closely follows a standard en-\ncoder/decoder transformer architecture [60], with the only fundamental difference being the input\ndata format and patch embedding, as well as the modification of the decoder input sequence and\nattention mask for parallel prediction. This means that our approach is easy to implement in existing\ntransformer code bases [52, 47, 57]. We do not rely on image-specific preprocessing operations\nother than resizing, see Section 4.1. As a result, existing infrastructure for model-parallel and dis-\ntributed training can readily be used to scale our approach. In particular, our loss does not require\ncomputations across devices the way CLIP does.\n3\nTable 2: Performance of frozen visual representations trained via image captioning (Cap/CapPa)\nand contrastively (CLIP\u2217), when combined with a single transformer decoder trained from scratch\nfor image classification, captioning and VQA (we use CIDEr for captioning, the VQAv2 weighted\naccuracy for VQAv2, and exact matching accuracy for all other tasks). Bold marks results where two\nstandard-deviation interval overlaps with the two standard-deviation interval of the best result.\nClassification\nCaptioning\nOCR\nQuestion Ans.\ni1k\nsun\nfood\nres\npet\nCOCO\nFlickr\nVQA\nVQAv2\nGQA\nCap\n80.2\u00b10.1 82.3\u00b10.2 90.3\u00b10.1 93.6\u00b10.1 93.1\u00b10.1 117.5\u00b10.3 78.6\u00b11.1\n62.2\u00b10.1\n68.2\u00b10.1 55.0\u00b10.1\nCapPa\n81.3\u00b10.1 82.4\u00b10.1 90.9\u00b10.1 94.2\u00b10.2 94.4\u00b10.1 117.9\u00b10.6 80.5\u00b10.2\n62.2\u00b10.0\n68.3\u00b10.1 55.7\u00b10.2\nCLIP* (8k)\n81.1\u00b10.0 83.2\u00b10.1 91.2\u00b10.0 94.8\u00b10.0 93.4\u00b10.2 115.8\u00b10.2 74.5\u00b11.2\n56.0\u00b10.1\n66.5\u00b10.1 54.3\u00b10.3\nCLIP* (16k)\n81.4\u00b10.1 83.3\u00b10.1 92.0\u00b10.1 95.2\u00b10.2 93.6\u00b10.2 116.3\u00b10.7 77.1\u00b10.7\n56.5\u00b10.1\n66.7\u00b10.1 54.8\u00b10.6\nCLIP\n81.6\u00b10.1 82.5\u00b10.0 92.6\u00b10.1 94.9\u00b10.1 93.6\u00b10.1 118.4\u00b10.6 78.7\u00b10.9\n60.0\u00b10.1\n67.8\u00b10.1 57.5\u00b10.1\nCapPa L/14\n84.4\u00b10.0 84.9\u00b10.1 93.8\u00b10.0 96.0\u00b10.1 95.6\u00b10.0 125.8\u00b10.1 89.3\u00b11.4\n65.6\u00b10.1\n70.9\u00b10.0 58.3\u00b10.2\nCLIP* L/14\n84.7\u00b10.1 85.7\u00b10.1 94.6\u00b10.1 96.4\u00b10.0 95.2\u00b10.1 123.2\u00b10.6 85.5\u00b10.3\n61.3\u00b10.2\n68.5\u00b10.1 55.3\u00b10.1\nCLIP L/14\n84.8\u00b10.0 84.8\u00b10.1 95.2\u00b10.1 96.3\u00b10.1 95.4\u00b10.3 124.4\u00b10.6 87.1\u00b10.7\n64.1\u00b10.0\n70.4\u00b10.1 58.7\u00b10.1\nZero-shot classification via scoring\nImage captioning models allow for zero-shot classification\nsimply by scoring the class name. Unlike with contrastive models, where the text (class) embeddings\ncan be computed once and reused for new images, with captioning models all class names have to be\nscored again for every new image. Cap/CapPa are hence less efficient zero-shot classifiers than CLIP.\nWe emphasize that we focus on learning vision encoders here and zero-shot transfer is not our focus.\n4\nExperiments\n4.1\nExperiment setup\nTable 1:\nParameter count and\nTPUv4-hrs. per bn. examples seen.\nModel\nParams\nTPU-hrs.\nB/16 Cap\n192 M\n454\nB/16 CLIP*\n197 M\n444\nL/14 Cap\n570 M\n1570\nL/14 CLIP*\n640 M\n1596\nPretraining data\nWe use a subset of the WebLI data set\n[6] which contains 10B images and 12B multilingual alt-texts.\nSpecifically, we rely on the WebLI subset corresponding to\nEnglish websites and apply text-based filtering similar to [26,\nSec. 3], [4, Sec 2.2] to obtain 1B image/alt-text pairs, not using\nany image-text similarity-based filtering. Importantly, WebLI\nwas de-duplicated w.r.t. the images in the evaluation data sets\nwe use in this paper. Please refer to [6, Sec 2.2] for more details\non the WebLI data set and to [6, Appendix B] for a datasheet.\nPretraining details and baselines1\nWe use a batch size of 8k for our captioning models (larger\nbatch size did not lead to improved performance), and both 8k and 16k for our retrained CLIP\nbaselines (henceforth denoted by CLIP\u2217 to avoid confusion with the model checkpoints released by\n[50], which we reference by CLIP in our results). We explicitly note the batch size for CLIP\u2217 models\nwhen relevant; CLIP\u2217 without modifier refers to the variant trained with batch size 16k. Models are\ntrained on up to 9B image/alt-text pairs (corresponding to 9 epochs on our subset of WebLI). We use\nthe AdaFactor variant from [68] with a cosine schedule (with 10k warmup steps), and set learning\nrate and decay factor to 10\u22123 and 10\u22124, respectively. Previous work [70, 69, 59] established these\noptimizer settings for contrastive pretraining and we adopt them here for captioning. Images are\nresized to a resolution of 224 \u00d7 224, and alt-texts are tokenized to a 32k-sized vocabulary with a\nsentence piece model trained on the English portion of C4 [52], with a maximum sequence length of\n64. Following [70, 69, 59] for CLIP\u2217 we use identically sized image and text towers, and use global\naverage pooling (GAP) to compute the image representation.\n4.2\nEvaluation protocols and data sets\nWe focus on properties of the frozen representations and also present some fine-tuning results.\n1Code is available at https://github.com/google-research/big_vision.\n4\nTable 3: 10-shot linear evaluation accuracy on\nthe pre-logit representation. CapPa outperforms\nCap and achieves overall comparable results with\nCLIP\u2217 trained with a batch size of 16k.\nINet\nCIFAR100\nPets\nCars\nCap\n57.2\u00b10.1 58.6\u00b10.8 83.7\u00b10.6 84.2\u00b10.1\nCapPa\n59.1\u00b10.1 62.4\u00b10.4 86.5\u00b10.1 86.6\u00b10.2\nCLIP* (8k)\n58.5\u00b10.1 64.9\u00b10.4 77.7\u00b11.5 80.8\u00b10.4\nCLIP* (16k) 59.7\u00b10.4 66.3\u00b10.2 80.6\u00b11.2 82.9\u00b10.1\nCLIP\n59.0\u00b10.2 68.6\u00b10.1 82.1\u00b10.7 70.2\u00b10.7\nCapPa L/14\n70.6\u00b10.2 72.9\u00b10.4 92.6\u00b10.5 92.2\u00b10.2\nCLIP* L/14\n69.8\u00b10.1 74.1\u00b10.5 87.7\u00b10.9 89.2\u00b10.2\nCLIP L/14\n69.7\u00b10.1 79.4\u00b10.3 90.4\u00b10.8 81.1\u00b10.4\nTable 4: Frozen transfer for zero-shot classifica-\ntion and retrieval via LiT [70]. Especially for the\nlarger model, CapPa is competitve with CLIP\u2217\nwith comparable or fewer LiT examples seen.\nINet 0shot\nCOCO t2i\nCOCO i2t\nLiT pairs:\n3B\n12B\n3B\n12B\n3B\n12B\nCap\n67.8\n69.0\n37.5\n39.1\n53.9\n54.8\nCapPa\n68.8\n70.2\n37.3\n38.6\n53.9\n55.1\nCLIP*\n69.0\n70.0\n38.9\n40.1\n55.1\n57.0\nCLIP\n68.3\n32.3\n52.8\nCapPa L/14 76.4\n77.5\n43.9\n45.4\n60.3\n62.6\nCLIP* L/14 75.8\n76.6\n44.7\n46.3\n60.7\n62.3\nCLIP L/14\n75.1\n36.5\n56.6\nProbing the frozen representation\nAs an inexpensive way to assess classification performance\nwe use the linear adaptation protocol (based on the pre-logits layer for CLIP\u2217 and GAP of the\nencoder output sequence for our captioning models) and eval sets from [68, 70], reporting the 10-shot\nclassification accuracy. We also evaluate the classification accuracy when using the full ImageNet1k\ntraining set to learn a dense layer, an MLP, and a multihead attention pooling (MAP)-based [35]\nclassifier. To assess the amenability of the different the CLIP\u2217 and CapPa vision encoders to fine-\ngrained classification, we train MAP-based classifiers on a range of specialized data sets which can\nbe divided in two groups. The first group requires fine-grained classification of animal or plant breed\n[45, 61, 48, 28], or product variant [32, 3], whereas data sets in the second group covers a range of\ndistinct objects [18, 12, 8, 71, 46, 19].\nText encoder/decoder-based inference\nWe use LiT [70] to learn a text embedding matched to\nthe embedding of our pretrained vision encoders. Generally, LiT is an efficient way to equip any\npretrained vision backbone with zero-shot classification and retrieval capabilities, here particularly\nfor Cap whose pretrained decoder does in principle have these capabilities but incurs significant\ninference cost. We follow the setup from [69] and assess the zero-shot classification accuracy on\nImageNet1k [53] and retrieval recall@1 on COCO captions [7]. We choose the LiT text encoder to\nmirror the architecture of the vision encoder at hand and attach a randomly initialized MAP head\nto the vision encoder to map into the shared image-text embedding space. For comparison, we also\napply LiT to our CLIP\u2217 models; this is not necessarily meant to be practically useful (continuing\ntraining the CLIP\u2217 model might be a better investment of compute).\nMotivated by recent work combining pretrained vision backbones and language models [1, 6, 62, 23,\n17, 5], we investigate the amenability of the learned representations to interface with a text decoder.\nConcretely, we use the \u201cLiT decoder\u201d setup [2] which trains a transformer decoder from scratch on\ntop of a frozen vision representation to solve captioning [7, 65], VQA [20, 25, 43] and classification\n[53, 71, 3, 8, 48] tasks in a multitask fashion (we use the default hyperparameters from [2] except for\nthe data mixing strategy set to \u201cconcat image-question pairs\u201d [2, Sec. 5.3] ).\nIn addition, we explore combining our representations with a pretrained T5 decoder [52]. We rely on\nthe previously described multitask LiT decoder setup and tune the most important hyper parameters\n(see supplementary material for details). For the T5 decoder we keep all the parameters frozen but\nreinitialize and train the cross-attention layers. Finally, we also leverage a frozen, pretrained 12-layer\nGPT-2 decoder [51] for image captioning by combining it with the frozen vision encoders via an\nadapter, similar to ClipCap [44].\nFine-tuning\nWe fine-tune our vision encoders on the full ImageNet1k data set, attaching a randomly\ninitialized MAP head to the pretrained representation (see supplementary material for details).\nUsing the pretrained text decoder\nFinally, we also evaluate our models (with the pretrained text\ndecoder) on the SugarCrepe [21] and the Attribute, Relation and Order (ARO) [67] benchmarks\n(which are derived from different captioning data sets [33, 65, 7]). Specifically, SugarCrepe and ARO\nshuffle the attributes, relations and order of captions and measures the sensitivity of vision-language\n5\nS\nM\nB\nL/14\nFLOPs (encoder)\n40\n50\n60\n70\nImageNet 10-shot\nS\nM\nB\nL/14\nFLOPs (encoder)\n70\n80\n90\nPets 10-shot\nS\nM\nB\nL/14\nFLOPs (encoder)\n110\n115\n120\n125\nCOCO Caption\nS\nM\nB\nL/14\nFLOPs (encoder)\n64\n66\n68\n70\nVQAv2\nCapPa\nCLIP*\n900M\n3B\n9B\ntraining examples seen\n50\n60\n70\nImageNet 10-shot\n900M\n3B\n9B\ntraining examples seen\n80\n90\nPets 10-shot\n900M\n3B\n9B\ntraining examples seen\n115\n120\n125\nCOCO Caption\n900M\n3B\n9B\ntraining examples seen\n66\n68\n70\nVQAv2\nModel\nCapPa\nCLIP*\nArch\nB/16\nL/14\nFigure 2: 10-shot classification accuracy on the frozen pre-logit representation (left two columns);\ncaptioning and VQA performance with a decoder (right two columns). Top row: Performance of\nvision backbones pretrained with captioning (Cap/CapPa) and contrastively (CLIP\u2217) as a function\nof the model size/FLOPs (we compare ViT-S/16, M/16, B/16, and L/14). CapPa exhibits favorable\nscaling behavior on captioning and VQA tasks. Bottom row: Performance of CapPa and CLIP\u2217 as a\nfunction of the number of training examples seen. The behavior is similar as for model scale.\nmodels to these manipulations. As shown by [67], contrastive models are not very sensitive to precise\nrelational and attributional information and behave closer to bag-of-words models. Intuitively, since\ncaptioning-based models model the joint distribution over all tokens, it is interesting to see how they\ncompare to contrastive models on ARO. For each example, we use log-likelihood to score both the\ntrue caption and the shuffled captions. The model prediction is the caption with the highest score.\n4.3\nMain results\nTable 5: Finetuning on INet.\nB/16 B/16384 L/14336\nCLIP* 84.9\n86.0\n88.1\nCapPa 84.4\n85.7\n87.7\nCap\n83.9\n85.3\n-\nTables 2, 3, 4, and Fig. 3 show the LiT decoder results, the 10-shot\nclassification accuracy, the LiT tuning results, and the full linear\nprobing accuracy for our models trained for 9B examples seen.\nCapPa outperforms Cap across almost all evaluations, and often also\noutperforms CLIP\u2217 trained with the same batch size (8k), while\nbeing competitive with CLIP\u2217 trained with a batch size of 16k. This\nis remarkable since CLIP\u2217 benefits substantially from a large batch size [69]. The trend becomes\nmore pronounced when increasing the model size: CapPa clearly outperforms CLIP\u2217 in 10-shot\nclassification accuracy for a ViT-L/14 encoder (Table 3).\nB/16 Cap\nB/16 CapPa\nL/14 CapPa\n-3\n-2\n-1\n0\nCLIP* \u2190 better \u2192 Cap\nLinear probe\nMLP probe\nMAP probe\nFigure 3: Linear probing makes cap pre-trained\nimage encoders seem worse, but when learning the\npooling (MAP probe), the gap is essentially closed.\nWhen transferred to ImageNet1k classification\nwith a linear probe (Fig. 3), the frozen Cap and\nCapPa encoders lag somewhat behind CLIP\u2217,\nbut the gap almost vanishes when using a MAP\nhead instead of a linear probe. This is not very\nsurprising, given CLIP\u2217 models are trained with\na linear head, which might induce linear sep-\narability in the average pooled encoder output\nrepresentation. In contrast, the Cap models feed\ninto a decoder via cross-attention which might\nnot impose linear separability as strongly. For\nfine-tuning the full model on ImageNet1k (Ta-\nble 5) we only observe a minor gap between\nCapPa and CLIP\u2217. As for text encoders learned\nvia LiT (Table 4) CapPa outperforms CLIP\u2217 for long LiT training and large model size in zero-shot\nclassification, but is outperformed by CLIP\u2217 in retrieval. Notably, for a short LiT tuning with 3B\nexamples our models outperform CLIP [50] in zero-shot classification and retrieval while matching\n6\nAttrib.\nRel.\nOrder (F) Order (C)\nBlind dec.\n83.7\n86.2\n98.8\n98.7\nCap\n88.9\n86.6\n99.1\n99.0\nCapPa\n85.7\n86.7\n99.2\n98.8\nCLIP\u2217\n53.2\n39.7\n45.5\n37.0\nCLIP\n62.7\n58.7\n57.9\n49.5\nARO Best\n88.0\n73.0\n60.0\n46.0\nNegCLIP\n71.0\n81.0\n91.0\n86.0\nTable 6: Results on the Attribute, Relation and\nOrder (ARO) benchmark [67]. Cap and CapPa\nclearly outperform all CLIP\u2217 and CLIP variants\nacross all data sets. They also outperform Neg-\nCLIP [67] which was explicitly trained to be\nsensitive to word ordering and attribution.\nTraining\nArch\nRepl.\nSwap\nAdd\nCap\nB/16\n88.21\n84.00\n98.94\nCapPa\nB/16\n87.67\n83.11\n99.13\nCLIP\u2217\nB/16\n81.95\n63.22\n81.91\nNegCLIP\nB/32\n85.36\n75.33\n87.29\nOpenCLIP\nG/14\n86.50\n68.56\n88.36\nTable 7: Results on the SugarCrepe [21] bench-\nmark, which fixes known issues in previous\nimage-text benchmarks like ARO. Full results\nare in Table 18 in the appendix. Small Cap and\nCapPa models outperform even large or hard-\nnegative trained CLIP models in all categories,\nand essentially solve the category of tests which\n\u201cAdd\u201ds plausible but wrong details to the caption.\nthe number of examples seen by CLIP (12.8B) when summing over pretraining (9B) and LiT tuning\n(3B), despite the vision encoder being frozen during LiT tuning (which saves compute). Matching the\nnumber of examples seen by CLIP during LiT tuning (12B) leads to a clear additional improvement.\nCombining our models with a fresh (LiT) decoder (Table 2), we observe that CapPa performs better\nthan CLIP\u2217 trained with batch size 16k on captioning and VQA, while obtaining competitive accuracy\non classification tasks. Again, this pattern becomes more pronounced with increased models size.\nIndeed, for a ViT-L/14 encoder CapPa even outperforms CLIP in the majority of LiT decoder tasks.\nScaling properties\nIn Fig. 2 we present an analysis of our encoders as a function of the model\nsize and the number of training examples seen for a selection of 10-shot classification and vision\n& language evaluations using a fresh decoder (plots for all evaluations and Cap can be found in\nthe supplementary material). It can be seen that CLIP\u2217 and CapPa models exhibit similar scaling\nbehavior, with CapPa showing a somewhat steeper increase in captioning and VQA performance as a\nfunction of model size and examples seen, in particular for a ViT-L/14 backbone. This indicates that\nthe benefits of CapPa models might become more pronounced with further model and data scaling.\nAttribution, relation, ordering\nTable 6 presents our results on the ARO Benchmark [67]. Cap\nand CapPa models achieve close to a perfect score on the ordering subsets. They outperform the\nbest contrastive variants by around 30% and 40% on Flickr Order and COCO Order, respectively.\nWe also compare with NegCLIP [67], which employs additional supervision to make contrastive\nmodels sensitive to word ordering and attribution. Cap and CapPa exceed NegCLIP by 8% and 13%\nout-of-the-box on COCO Order and Flickr Order. The same can be observed for the attribution,\nand relation subsets of ARO. So we are facing a clear trade-off: CLIP-style models outperform\ncaptioning-based models in terms of standard zero-shot classification accuracy, whereas the latter\nare much better at processing fine-grained descriptions of visual content. Finally, we also trained\nImageNet\nSUN397\nFood101\nRESISC45\nPets\nCOCO\nFlickr\nOCR-VQA\nVQAv2\nGQA\n0\n+5\n+10\nCLIP* \u2190 better \u2192 CapPa\nLiT-decoder (scratch)\nT5-decoder (frozen)\nFlowers\nCars\nBirds\nPets\nDogs\nCat/Dog\nSTL-10\nRESISC\nSUN397\nProducts\nCaltech\nFood\n-1\n0\n+1\n+2\nFine-grained\nCoarse-grained\nFigure 4: Absolute improvement of CapPa over CLIP\u2217 in various settings. Left: CapPa pairs\nsignificantly better with decoders in image-language tasks, especially when the decoder is a pre-\ntrained and frozen language model. Right: CapPa seems to be a noticeably better frozen feature\nextractor for fine-grained classification tasks (we show L/14 results, see appendix for B/16).\n7\na Cap model with no image input (Blind dec.), which is just a language model for alt-text. This\nmodel overall performs only slightly worse than Cap and CapPa, which suggests that the sentence\nmanipulation in ARO can to a large extent be detected by language modeling alone. This was also\nobserved in concurrent work [42, 21].\nSugarCrepe\nThe SugarCrepe benchmark [21], introduced concurrently to our work, promises to\nfix the issues in ARO and similar benchmarks; for instance, a blind model is no better than chance.\nStill, even our small captioning pretrained B/16 model outperforms the largest G/14 contrastive\nmodel as well as the specifically trained NegCLIP. The \u201cSwap\u201d category is especially sensitive to the\nrelation between multiple things in the image, something that is fundamentally hard for contrastive\npretraining to learn. The \u201cAdd\u201d category, where highly plausible but wrong things are added to the\ntext is essentially solved by captioners. The full breakdown, more baselines, and qualitative examples\nare provided in Tables 18 and 22\u201324 in the appendix. This result is strong evidence that captioning as\na pretraining objective imbues capabilities to the model that contrastively trained models are blind to.\nFrozen T5 decoder\nWe also trained models with frozen encoder and frozen decoder (initialized\nwith a T5 checkpoint). For these experiments, only the cross attention weights were trained (28M\ntrainable parameters, compared to the 248M trainable parameters when the decoder is trained from\nscratch). The relative improvements of CapPa over CLIP\u2217 are shown in Fig. 4 (left). Even though\nthe absolute performance of the decoder trained from scratch (Table 2) is better than when only\ntraining the cross attention weights (Table 12), CapPa with a frozen decoder closes the gap on three\nclassification tasks, reverts the trend on ImageNet, and improves the performance by large margins\non Pets, as well as all captioning and VQA tasks. This result suggests that the captioning objective is\nbetter suited to train an encoder that is later combined with a pretrained language decoder.\nCOCO Caption\nFlickr Caption\n0\n2\n4\n6\n8\nCLIP* \u2190 better \u2192 CapPa\nLiT-d.\nT5-d.\nGPT2\nFigure 5: Absolute improve-\nment (single-task) of CapPa\nover CLIP\u2217 for a decoder\ntrained from scratch (LiT-d.),\na frozen T5 decoder, and a\nfrozen GPT-2 similar to [44].\nFrozen GPT-2 decoder\nWe combined our frozen vision encoders\nwith a frozen GPT-2 model from [51] using an adapter as proposed\nby ClipCap [44]. We found that this setup is less well suited for VQA\nand multitask conditioning, so we only evaluate it on captioning in\nsingle-task fashion as done by ClipCap. We tried the MLP and trans-\nformer adapters from [44], but obtained better results for both CLIP\u2217\nand CapPa with a \u201cresampler\u201d, a LayerNorm followed by a single\ncross-attention layer with learnable query embeddings, generating\na prefix of 32 soft tokens for GPT-2. This resampler has only 7.1M\nparameters, about 4\u00d7 less than the MLP adapter from [44]. CapPa\nstill outperforms CLIP\u2217 in this setup, but the gains are more modest\nthan for the T5 decoder and a decoder trained from scratch (both\nsingle-task, see Fig. 5). We emphasize that both encoder and GPT-2\ndecoder are frozen and we only use a small adapter.\nFine-grained classification\nFig. 4 (right) shows the improvement when using CapPa instead of\nCLIP\u2217 for a range of specialized data sets. CapPa outperforms CLIP\u2217 on the majority of fine-grained\ntasks which suggests that captioning as a pretraining task leads to better features for fine-grained\nclassification than contrastive pretraining.\n4.4\nAblations\nAll models discussed in this section are trained for 900M training examples seen.\nParallel prediction\nRecall that for parallel prediction we replace all text input tokens with [MASK]\ntokens. An alternative would be to only replace a random subset, as done e.g. in [22], to provide a\npartial context for the prediction. However, we did not observe improvements of the vision encoder\nwhen only masking a fraction of the tokens, so we focused on fully masked input sequences. For\nfully masked input sequence Fig. 6 (left) shows the improvement in 10-shot classification accuracy\nover training for pure autoregressive decoding as a function of the fraction of examples for which the\ndecoder is trained for parallel prediction instead. A fraction of 0.75 leads to a balanced improvement\nacross all considered data sets. Finally, alternating between parallel and autoregressive prediction\nfor all examples in a batch, rather than performing parallel prediction with mixed batches, led to\nsignificantly worse results.\n8\n25%\n50%\n75%\n90%\nFraction of batches with parallel prediction\n-4\n-2\n0\n+2\n+4\nCap \u2190 better \u2192 CapPa\nImageNet\nCIFAR100\nPets\nCars\nImageNet\n10-shot\nCIFAR100\n10-shot\nPets\n10-shot\nCars\n10-shot\nImageNet\nzero-shot\n-15\n-10\n-5\n0\nCLIP* \u2190 better \u2192 Cap\nResNet-50 (p)\nResNet-50\nViT-B/32\nViT-B/16\nFigure 6: Left: Improvement in 10-shot classification accuracy as a function of the fraction of training\nexamples for which parallel prediction is performed in CapPa. A fraction of 0.75 leads to a balanced\nimprovement. Right: Improvement of 10-shot/zero-shot (without prompts) classification accuracy\nwhen using Cap instead of CLIP\u2217. For ResNet-50 (p) the decoder consumes 4 averaged image tokens\nas a prefix (no cross-attention). Cap is competitive in 10-shot accuracy for ViT-B/16.\nEncoder architecture\nNext, we investigate the effect of the encoder architecture on the repre-\nsentation quality, comparing CLIP\u2217 with Cap when using a BiT ResNet50 [31], ViT-B/32, and a\nViT-B/16 vision encoder. We use a B-sized text encoder and B-sized 6-layer decoder for CLIP\u2217\nand Cap, respectively, for all encoders, except for ResNet50 for which we also train with a prefix\ndecoder following [50, Sec. A2]. According to [15] the BiT ResNet50 obtains performance roughly\ncomparable to ViT-B/32 when trained and evaluated on image classification. Further, ResNet50\nand ViT-B/32 both produce a sequence of length 49 before pooling for 224 \u00d7 224 images, which\nwe feed to the decoder. Fig. 6 (right) shows the improvement obtained when using Cap instead of\nCLIP\u2217 as a function of the encoder architecture. The improvement (regression) in ImageNet zero-shot\naccuracy is smallest (largest) for the ResNet50 architecture (this is the architecture used to create [50,\nFig. 2] that compares contrastive with bag-of-words and captioning approaches) and is significantly\nimproved when using a ViT architecture and when reducing the patch size (which does not increase\nmodel capacity but the sequence length and hence FLOPs). Also recall that these models were only\ntrained on 900M examples. Interestingly, the difference between CLIP\u2217 and Cap on 10-shot metrics\nare generally smaller, and for a ViT-B/16 encoder the two approaches lead to similar performance.\nCaptioning task variants\nWe train Cap while randomly reversing the caption with probability 0.5.\nThis maintains model capacity and pretraining compute unchanged. We do not observe improved\nperformance (see Table 8, left). While [13] ablates backwards captioning and shows improvements,\nthey use a separate decoder for backwards captioning, so the ablated model has fewer parameters and\nFLOPs (here we control for both factors). Additionally, we train a CapPa variant with two parallel\ndecoders, one for autoregressive prediction and another one for parallel prediction, each with 3 layers\n(instead of 6). This model matches the pretraining compute of the default CapPa but underperforms\nin the majority of 10-shot tasks.\nTraining with language-pretrained decoder\nFinally, we train Cap ViT-B/16 with a frozen pre-\ntrained T5-Base decoder (which has 12 decoder layers). To obtain a stable training setup we adapt\nthe optimizer hyper-parameters (learning rate 0.0005, \u03b22 = 0.95) and unfreeze the cross attention.\nOptionally, we re-initialize the cross-attention parameters and unfreeze the decoder. None of these\nvariants performs better than Cap overall (see Table 8, right), and the more we allow the decoder to\ndeviate from its language-pretrained weights the better the vision performance gets.\nTable 8: Left: Comparing Cap and CapPa with other captioner variants: Forward and backward\ncaptioning with the same decoder, autoregressive and parallel prediction with two 3-layer decoders.\nRight: Training Cap with a frozen T5 decoder does not help, even when unfreezing parts of it.\nImageNet CIFAR100 Pets Cars\nCap\n49.7\n56.0\n72.6 74.7\nCap (fw.+bw.)\n49.2\n56.1\n71.7 73.0\nCapPa 2 dec.\n49.5\n54.9\n75.8 79.0\nCapPa\n50.4\n57.4\n76.2 78.5\nImageNet CIFAR100 Pets Cars\nCap (12 layers)\n48.7\n54.8\n74.4 73.8\n+ frozen T5 dec.\n42.8\n44.9\n69.3 62.3\n+ reinit. xatt.\n43.7\n45.7\n68.3 66.9\n+ unfreeze dec.\n48.6\n55.2\n72.0 75.6\n9\nDecoder architecture\nWhile following the original transformer decoder architecture [60] closely,\nwe adopt the now common change of removing the biases in the decoder [52, 11] to improve stability\nwithout affecting performance, see Table 19 (left) in the appendix. We use separate embeddings\nfor the decoder\u2019s input and output, Table 19 (left) shows that this works slightly better. We use six\ndecoder layers (see Table 19, right) which simultaneously leads to overall good results while also\nmatching the total parameter count of the corresponding CLIP\u2217 model.\nInspired by experiments from [68], where applying a stronger weight decay to the classification head\nof ViTs trained for image classification led to accuracy improvements, we also experimented with\nincreased weight decay applied to the decoder or cross-attention layers, but we did not observe any\nbenefits from this. Further, we explored using a tokenizer trained on 300M randomly sampled WebLI\nalt-texts instead of the one pretrained on C4, which did not improve accuracy.\nImageNet CIFAR100\nPets\nCars\n50\n75\n100\n10-shot Accuracy [%]\nCLIP* (8k)\nCap\nCapPa\nFigure 7: Pretraining on LAION-400M.\nEffect of pretraining data\nSo far all our results were\nbased on models pretrained on a variant of WebLI, and\none might wonder whether our findings transfer to pre-\ntraining on other data sets. We therefore train some of our\nmodels and baselines on the smaller, publicly available\nLAION-400M dataset [55] which was collected and fil-\ntered following a different protocol. For instance, it was\nfiltered using an existing CLIP model to score image-text\npairs, which might induce a significantly different bias\nin the training data and our conclusions. However, the\n10-shot linear classification results in Fig. 7 show that the\nconclusions remain the same: CapPa achieves accuracy\ncomparable with CLIP\u2217 and outperforms Cap.\n5\nDiscussion and conclusion\nWe presented an extensive comparison of vision encoders pretrained with a contrastive and generative\n(captioning) objective and found that the generatively pretrained encoders obtain better performance\nwhen used for captioning, VQA, fine-grained and few-shot classification tasks, while achieving\ncompetitive performance in classification overall. This is in contrast to previous work [50] which\nargued that predicting alt-text from images explicitly and exactly is an overly challenging pretraining\ntask and may lead to sub-par vision capabilities. Moreover, our results show that captioning as a\npretraining task might exhibit favorable scaling properties with increasing model and data scale, and\nwe hope future work will explore this avenue.\nOne downside of our approach is that the Cap/CapPa text decoder is of limited use. While it achieves\nexcellent results when word order and object relationships matter\u2014a scenario where CLIP-style\nmodels exhibit a strong bag-of-word bias\u2014zero-shot classification and retrieval are computationally\nexpensive. We showed that this can be mitigated relatively cheaply by training a text encoder matched\nto the frozen Cap/CapPa representation via LiT tuning. Note that such a procedure is cheaper than\ntraining the text encoder with the image encoder and text decoder throughout the whole pretraining\nprocedure as done e.g. by CoCa [66].2 Another promising avenue is to explore parallel prediction\nfor inference task, as it would allow for efficient scoring e.g. for zero-shot classification. Indeed,\nparallel prediction produces a joint distribution over all tokens, which can be used to score an arbitrary\nnumber of classes or queries.\nIn conclusion, we established plain image captioning as a competitive pretraining strategy for vision\nbackbones from image-text data. We hope to inspire follow up work dedicating more attention the\nadvantages of captioning as a pretraining task for vision encoders.\n2[66, Table 8b)] presents an ablation of the contrastive loss in CoCa, but keeps the unimodal decoder part\n(without cross-attention) which serves as text encoder for the contrastive loss, and hence obtains essentially no\ngains in accelerator time. The Cap/CapPa decoder, in contrast, does not have a unimodal component, and is\nhence cheaper to evaluate than the CoCa decoder for given architecture size (base, large).\n10\nAcknowledgments\nWe would like to thank Jannis Bulian, Mostafa Dehghani, Alexey Dosovitskiy,\nDaniel Keysers, Mario Lucic, Basil Mustafa, and Xiao Wang for feedback on this paper and inspiring\ndiscussions on captioning-based pretraining. We would also like to thank Janko Ferli\u02c7c from Unsplash\nfor providing the photo used in Fig. 1.\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda\nHan, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew\nBrock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals,\nAndrew Zisserman, and Karen Simonyan. Flamingo: A visual language model for few-shot learning. In\nNeurIPS, 2022. 1, 2, 3, 5\n[2] Lucas Beyer, Bo Wan, Gagan Madan, Filip Pavetic, Andreas Steiner, Alexander Kolesnikov, Andr\u00e9 Susano\nPinto, Emanuele Bugliarello, Xiao Wang, Qihang Yu, Liang-Chieh Chen, and Xiaohua Zhai. A study of\nautoregressive decoders for multi-tasking in computer vision. arXiv:2303.17376, 2023. 5, 15, 18, 19, 22\n[3] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 \u2013 mining discriminative components\nwith random forests. In ECCV, 2014. 5, 17\n[4] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12M: Pushing web-scale\nimage-text pre-training to recognize long-tail visual concepts. In CVPR, 2021. 4\n[5] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme\nRuiz, Sebastian Goodman, Xiao Wang, Yi Tay, Siamak Shakeri, Mostafa Dehghani, Daniel Salz, Mario\nLucic, Michael Tschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi, Bo Pang, Ceslee Montgomery,\nPaulina Pietrzyk, Marvin Ritter, AJ Piergiovanni, Matthias Minderer, Filip Pavetic, Austin Waters, Gang Li,\nIbrahim Alabdulmohsin, Lucas Beyer, Julien Amelot, Kenton Lee, Andreas Peter Steiner, Yang Li, Daniel\nKeysers, Anurag Arnab, Yuanzhong Xu, Keran Rong, Alexander Kolesnikov, Mojtaba Seyedhosseini,\nAnelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. PaLI-X: On scaling up a multilingual\nvision and language model. arXiv:2305.18565, 2023. 1, 2, 3, 5\n[6] Xi Chen, Xiao Wang, Soravit Changpinyo, A. J. Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian\nGoodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan\nDing, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury,\nWeicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner,\nAnelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. PaLI: A jointly-scaled multilingual\nlanguage-image model. In ICLR, 2023. 1, 2, 3, 4, 5\n[7] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and\nC. Lawrence Zitnick. Microsoft COCO Captions: Data collection and evaluation server. arXiv preprint\narXiv:1504.00325, 2015. 5\n[8] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and\nstate of the art. arXiv preprint arXiv:1703.00121, 2017. 5, 17\n[9] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon,\nChristoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive\nlanguage-image learning. In CVPR, pages 2818\u20132829, 2023. 1\n[10] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying vision-and-language tasks via text generation.\nIn ICML, pages 1931\u20131942, 2021. 1\n[11] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha\nTsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar\nPrabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael\nIsard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk\nMichalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito,\nDavid Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani\nAgrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor\nLewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi\nWang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern,\nDouglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. PaLM: Scaling language modeling with pathways.\narXiv:2204.02311, 2022. 3, 10, 22\n11\n[12] Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature\nlearning. In AISTATS, pages 215\u2013223, 2011. 5, 17\n[13] Karan Desai and Justin Johnson. VirTex: Learning visual representations from textual annotations. In\nCVPR, 2021. 1, 2, 9\n[14] Xiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen, Shuyang Gu, Weiming Zhang, Lu Yuan, Dong\nChen, Fang Wen, and Nenghai Yu. CLIP Itself is a Strong Fine-tuner: Achieving 85.7% and 88.0% Top-1\nAccuracy with ViT-B and ViT-L on ImageNet. arXiv:2212.06138, 2022. 1, 15\n[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and\nNeil Houlsby. An image is worth 16\u00d716 words: Transformers for image recognition at scale. In ICLR,\n2021. 1, 3, 9, 16\n[16] Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang Zhu,\nPengchuan Zhang, Lu Yuan, Nanyun Peng, Zicheng Liu, and Michael Zeng. An empirical study of training\nend-to-end vision-and-language transformers. In CVPR, pages 18145\u201318155, 2022. 3\n[17] Danny Driess, F. Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan\nWahid, Jonathan Tompson, Quan Ho Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre\nSermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus\nGreff, Andy Zeng, Igor Mordatch, and Peter R. Florence. Palm-E: An embodied multimodal language\nmodel. arXiv:2303.03378, 2023. 1, 2, 3, 5\n[18] Jeremy Elson, John R. Douceur, Jon Howell, and Jared Saul. Asirra: A CAPTCHA that Exploits Interest-\nAligned Manual Image Categorization. In Proc. ACM Conf. Computer and Communications Security\n(CCS), 2007. 5, 17\n[19] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples:\nAn incremental bayesian approach tested on 101 object categories. In CVPRW, pages 178\u2013178, 2004. 5, 17\n[20] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA\nmatter: Elevating the role of image understanding in visual question answering. In CVPR, 2017. 5\n[21] Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kembhavi, and Ranjay Krishna. Sugarcrepe: Fixing\nhackable benchmarks for vision-language compositionality. arXiv:2306.14610, 2023. 2, 5, 7, 8, 21\n[22] Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, and Lijuan Wang.\nScaling up vision-language pre-training for image captioning. In CVPR, pages 17980\u201317989, 2022. 1, 3, 8\n[23] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei\nCui, Owais Khan Mohammed, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary,\nSubhojit Som, Xia Song, and Furu Wei. Language is not all you need: Aligning perception with language\nmodels. arXiv:2302.14045, 2023. 1, 2, 3, 5\n[24] Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. Pixel-BERT: Aligning image\npixels with text by deep multi-modal transformers. arXiv:2004.00849, 2020. 3\n[25] Drew A. Hudson and Christopher D. Manning. GQA: a new dataset for compositional question answering\nover real-world images. In CVPR, 2019. 5\n[26] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung,\nZhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text\nsupervision. In ICML, 2021. 1, 2, 4, 22\n[27] Armand Joulin, Laurens Van Der Maaten, Allan Jabri, and Nicolas Vasilache. Learning visual features\nfrom large weakly supervised data. In ECCV, pages 67\u201384, 2016. 1, 2\n[28] Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Fei-Fei Li. Novel dataset for fine-grained\nimage categorization: Stanford dogs. In CVPRW, 2011. 5, 17\n[29] Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Won-\nseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. OCR-Free document understanding\ntransformer. In ECCV, pages 498\u2013517, 2022. 3\n[30] Wonjae Kim, Bokyung Son, and Ildoo Kim. ViLT: Vision-and-language transformer without convolution\nor region supervision. In ICML, pages 5583\u20135594. PMLR, 2021. 3\n12\n[31] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil\nHoulsby. Big transfer (BiT): General visual representation learning. In ECCV, 2020. 9\n[32] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained\ncategorization. In CVPRW, pages 554\u2013561, 2013. 5, 17\n[33] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision\nusing crowdsourced dense image annotations. IJCV, pages 32\u201373, 2017. 5\n[34] Weicheng Kuo, AJ Piergiovanni, Dahun Kim, Xiyang Luo, Ben Caine, Wei Li, Abhijit Ogale, Luowei\nZhou, Andrew Dai, Zhifeng Chen, et al. Mammut: A simple architecture for joint learning for multimodal\ntasks. arXiv:2303.16839, 2023. 1, 3\n[35] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam R. Kosiorek, Seungjin Choi, and Yee Whye Teh. Set\ntransformer: A framework for attention-based permutation-invariant neural networks. In ICML, pages\n3744\u20133753, 2019. 5\n[36] Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal,\nPeter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2struct: Screenshot parsing as pretraining for\nvisual language understanding. arXiv:2210.03347, 2022. 3\n[37] Ang Li, Allan Jabri, Armand Joulin, and Laurens Van Der Maaten. Learning visual n-grams from web\ndata. In ICCV, pages 4183\u20134192, 2017. 1, 2\n[38] Gang Li and Yang Li. Spotlight: Mobile UI understanding using vision-language models with a focus. In\nICLR, 2023. 3\n[39] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large language models. arXiv:2301.12597, 2023. 3\n[40] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. BLIP: Bootstrapping language-image pre-training\nfor unified vision-language understanding and generation. In ICML, pages 12888\u201312900, 2022. 1, 3, 22\n[41] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong\nHoi. Align before fuse: Vision and language representation learning with momentum distillation. NeurIPS,\n34:9694\u20139705, 2021. 3\n[42] Zhiqiu Lin, Xinyue Chen, Deepak Pathak, Pengchuan Zhang, and Deva Ramanan. Visualgptscore:\nVisio-linguistic reasoning with multimodal generative pre-training scores. arXiv:2306.01879, 2023. 8\n[43] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. OCR-VQA: Visual\nQuestion Answering by Reading Text in Images. In ICDAR, 2019. 5\n[44] Ron Mokady, Amir Hertz, and Amit H Bermano. Clipcap: Clip prefix for image captioning. arXiv preprint\narXiv:2111.09734, 2021. 5, 8\n[45] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of\nclasses. In Indian Conf. Computer Vision, Graphics & Image Process., pages 722\u2013729, 2008. 5, 17\n[46] Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep metric learning via lifted structured\nfeature embedding. In CVPR, pages 4004\u20134012, 2016. 5, 17\n[47] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proc. NAACL-HLT: Demonstra-\ntions, 2019. 3\n[48] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In CVPR, 2012.\n5, 17\n[49] Hieu Pham, Zihang Dai, Golnaz Ghiasi, Kenji Kawaguchi, Hanxiao Liu, Adams Wei Yu, Jiahui Yu,\nYi-Ting Chen, Minh-Thang Luong, Yonghui Wu, Mingxing Tan, and Quoc V. Le. Combined scaling for\nopen-vocabulary image classification. arXiv:2111.10050, 2021. 2\n[50] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning\ntransferable visual models from natural language supervision. In ICML, 2021. 1, 2, 4, 6, 9, 10, 15, 17, 22\n[51] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 2019. 5, 8\n13\n[52] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. J.\nMach. Learn. Res., 21:140:1\u2013140:67, 2020. 3, 4, 5, 10, 22\n[53] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\nAndrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large\nScale Visual Recognition Challenge. IJCV, 115(3):211\u2013252, 2015. 5\n[54] Mert Bulent Sariyildiz, Julien Perez, and Diane Larlus. Learning visual representations with caption\nannotations. In ECCV, pages 153\u2013170, 2020. 1, 2\n[55] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush\nKatta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION-400M: Open dataset of CLIP-filtered\n400 million image-text pairs. arXiv:2111.02114, 2021. 10\n[56] Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao,\nand Kurt Keutzer. How much can CLIP benefit vision-and-language tasks? In ICLR, 2022. 3\n[57] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catan-\nzaro.\nMegatron-LM: Training multi-billion parameter language models using model parallelism.\narXiv:1909.08053, 2019. 3\n[58] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-CLIP: Improved training techniques\nfor CLIP at scale. arXiv:2303.15389, 2023. 1\n[59] Michael Tschannen, Basil Mustafa, and Neil Houlsby. CLIPPO: Image-and-language understanding from\npixels only. In CVPR, 2023. 4\n[60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 2, 3, 10, 15, 22\n[61] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The Caltech-UCSD\nBirds-200-2011 Dataset. 2011. 5, 17\n[62] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and\nLijuan Wang. GIT: A generative image-to-text transformer for vision and language. Trans. Machine\nLearning Research, 2022. 1, 2, 3, 5\n[63] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. SimVLM: Simple\nvisual language model pretraining with weak supervision. In ICLR, 2022. 1, 2, 22\n[64] Hongwei Xue, Yupan Huang, Bei Liu, Houwen Peng, Jianlong Fu, Houqiang Li, and Jiebo Luo. Probing\ninter-modality: Visual parsing with self-attention for vision-and-language pre-training. NeurIPS, 34:4514\u2013\n4528, 2021. 3\n[65] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual\ndenotations: New similarity metrics for semantic inference over event descriptions. Trans. Assoc. Comput.\nLinguistics, 2:67\u201378, 2014. 5\n[66] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca:\nContrastive captioners are image-text foundation models. Trans. Machine Learning Research, 2022. 1, 3,\n10, 22\n[67] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why\nvision-language models behave like bag-of-words models, and what to do about it? In ICLR, 2023. 2, 5, 6,\n7, 21\n[68] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In\nCVPR, pages 12104\u201312113, 2022. 4, 5, 10, 15, 16\n[69] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image\npre-training. arXiv:2303.15343, 2023. 2, 4, 5, 6\n[70] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and\nLucas Beyer. LiT: Zero-shot transfer with locked-image text tuning. In CVPR, pages 18102\u201318112, 2022.\n2, 4, 5, 15, 16, 17\n[71] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million\nimage database for scene recognition. Trans. PAMI, 2017. 5, 17\n14\nA\nTransfer and finetuning details\nFew-shot evaluation\nWe use the linear adaptation protocol and evaluation sets from [68, 70],\nreporting the 10-shot classification accuracy. Specifically, we rely on the pre-logits layer for CLIP\u2217\nand GAP of the encoder output sequence for our captioning models. For every combination of data\nset and model we run the 10-shot adaptation three times and report the mean (and standard deviation\nfor key results).\nLiT decoder and T5 decoder\nTo train a multi-task decoder from scratch on top of the frozen\nrepresentation for classification, captioning and VQA, we precisely follow the setup and hyper\nparameters from [2] except for the data mixing strategy, for which we set to \u201cconcat image-question\npairs\u201d ([2, Sec. 5.3]). For all encoders, we use the full feature sequence before pooling (including the\nclass token for the evaluation of CLIP). Throughout, we rely on a B-sized transformer decoder [60]\nwith 12 layers.\nWe also tried fine-tuning the image encoder along with the decoder for both CLIP\u2217 and Cap/CapPa\nmodels and did not obtain an improvement for any of the models. This is consistent with prior work\nwhich did not observe an improvement either for CLIP-style models when fine-tuning with the same\ndecoder-based setup, see [2, Sec. 5.7].\nFor the T5 decoder we keep all the parameters frozen but reinitialize and train the cross-attention\nlayers. We perform a small sweep around the default learning rate and weight decay of the setup used\nfor training from scratch, while keeping the other hyperparameters unchanged.\nLinear and non-linear ImageNet-1k probes (frozen transfer)\nWhen performing linear and\nnon-linear probes on ImageNet-1k, we run a wide hyper-parameter optimization sweep for all types\nof probes (linear, MLP, MAP) in order to get solid, trustworthy conclusions. Specifically, for each\nimage encoder and probe combination, we sweep the full cross-product over the following hyper-\nparameters: epochs: (1, 3, 10, 30, 100); image cropping: resize(256)|random_crop(224) or\ninception_crop(224); learning rate: (0.001, 0.0003, 0.0001) plus earlier runs showing 0.003\nand 0.01 to perform much worse; weight decay: (0.0001, lr * 0.1, 0.0); hidden dimension: 0 or\n1024; loss: sigmoid or softmax cross-entropy. The head weights are always initialized to 0 and its\nbias to -6.9 in the sigmoid case.\nFor each result shown in Fig. 3, we select the best setting using 1% of the training data that was\nheld-out for this purpose, and report its accuracy on the 50 000 images in the validation set. For\ncompleteness, we further compute various ImageNet test-set variants and report full results in Table 9.\nBroad MAP-head transfers (fine-grained)\nWe run the same sweep as described above for each\nindividual dataset and model combination, but only using the MAP-head probe. For each dataset, we\neither use a provided held-out validation set for selecting the best settings, or hold out 20% of the\ntraining set if none is provided. Full numeric results are provided in Table 10. Note that we selected\nand classified the datasets as coarse- or fine-grained solely by looking at the datasets and their classes,\nbefore running any single experiment on them, and never revisited this selection.\nFine-tuning on the full ImageNet-1k data set\nWhen fine-tuning on the full ImageNet-1k dataset,\nwe attach a fresh MAP head to the pretrained encoder and run full fine-tuning using the AdaFactor\noptimizer modified for ViTs in [68]. In each setting (B/16, B/16384, L/14336), we run the exact same\nsweep for CLIP*, CapPa, and Cap models. Notably, our exploration is significantly smaller than that\nof [14] and unlike for CLIP [50], ImageNet was fully de-duplicated from our pre-training dataset.\nIn all cases, we select the best model on a held-out 2% of the training data and report that model\u2019s\nperformance on the 50 000 image validation set without re-training.\nFor the B/16 models, we sweep over three learning rates: (0.0001, 0.00003, 0.00001); two layer-wise\nlearning-rate decays: (None, 0.8); 2 RandAugment parameters: (10, 15); 3 Mixup: (0.0, 0.2, 0.5);\nand five Polyak (EMA) averaging factors: (None, 0.9, 0.999, 0.99999, 0.9999999) with a batch size\nof 2048 and 100 epochs. The best setting uses learning rate 0.00001, layer-wise decay 0.8, Mixup 0.5\nand no Polyak averaging.\nFor the L/14 models at 336 px resolution, we sweep over three learning rates: (0.001, 0.0003,\n0.0001), three layer-wise learning-rate decays: (None, 0.9, 0.8), and five Polyak (EMA) averaging\n15\nfactors: (None, 0.9, 0.999, 0.99999, 0.9999999). Note that the latter does not require re-training for\neach setting and hence is cheap. We fix rand-augment to (2, 10), Mixup to 0.2, and training duration\nto 50 000 steps with batch-size 512, without revisiting these choices. Besides that, we mostly follow\n[15, 68]. The best setting uses learning rate 0.0001, layer-wise decay 0.9, and Polyak 0.99999 for\nboth models.\nB\nAdditional Results\nB.1\nProbing and LiT tuning results\nTable 9 shows the classification accuracy on different ImageNet-1k evaluation sets, when probing the\nfrozen representation with different probes (linear and non-linear), extending the numerical results\nfrom Fig. 3.\nTable 10 presents transfer results of the frozen representation to fine- and coarse-grained classification\ntasks (using a MAP head). This complements the results from Fig. 4 (Right).\nTable 11 expands Table 4 in the main paper and shows frozen transfer for zero-shot classification and\nretrieval via LiT [70].\nTable 12 presents the performance of frozen Cap/Cap and CLIP\u2217 encoders when combined via\ncross-attention with a frozen T5 decoder. This represents the data from Fig. 4 (Left) in the main paper\nin tabular form.\nTable 9: Extended numerical results for Fig. 3, i.e. linear and non-linear ImageNet-1k probes on top\nof the frozen models. While the linear separability of CLIP models is higher, the gap between CLIP\u2217\nand Cap models is mostly closed when the probe also learns how to pool (map).\nModel\nHead\nTop-1\nReaL\n-v2\n-R(endition)\n-A(dvers.)\nObjectNet\nCLIP* (8k)\nlinear\n79.8\n85.6\n69.0\n71.9\n38.0\n49.8\nmlp\n80.4\n86.1\n69.6\n74.4\n39.3\n50.9\nmap\n82.2\n87.3\n71.5\n72.9\n34.3\n49.0\nmap+mlp\n82.2\n87.4\n71.7\n71.8\n34.8\n48.3\nCLIP* (16k)\nlinear\n80.2\n85.9\n69.2\n73.2\n40.3\n51.3\nmlp\n80.9\n86.1\n70.3\n71.4\n37.3\n49.8\nmap\n82.6\n87.5\n72.4\n73.9\n37.2\n50.0\nmap+mlp\n82.6\n87.5\n72.1\n73.0\n36.3\n49.3\nCap\nlinear\n77.7\n84.1\n67.1\n68.2\n24.1\n44.2\nmlp\n78.5\n84.8\n68.0\n76.0\n27.1\n45.6\nmap\n81.6\n87.0\n71.3\n76.2\n32.3\n45.8\nmap+mlp\n81.5\n87.0\n71.5\n76.2\n31.4\n45.8\nCapPa\nlinear\n78.3\n84.6\n66.5\n67.7\n22.1\n43.7\nmlp\n79.4\n85.6\n68.6\n77.2\n25.6\n46.2\nmap\n82.0\n87.5\n72.3\n80.9\n41.5\n50.1\nmap+mlp\n82.1\n87.3\n72.0\n79.4\n39.1\n49.5\nCLIP* L/14\nlinear\n84.2\n88.4\n75.0\n83.8\n59.1\n60.2\nmlp\n84.6\n88.5\n74.9\n83.3\n56.6\n58.6\nmap\n85.9\n89.3\n76.7\n84.9\n57.4\n58.2\nmap+mlp\n85.8\n89.2\n77.0\n83.6\n56.1\n57.8\nCapPa L/14\nlinear\n83.0\n87.7\n73.1\n81.1\n41.6\n53.8\nmlp\n84.1\n88.7\n74.6\n87.3\n47.0\n56.8\nmap\n85.8\n89.3\n76.8\n86.1\n54.5\n56.6\nmap+mlp\n85.8\n89.2\n76.6\n85.5\n52.2\n56.5\n16\nTable 10: Transfer of the frozen representation to fine- and coarse-grained classification tasks (using\na MAP head). This extends the numerical results of Figure 4 (Right).\nDataset\nGrain\nCLIP* (8k)\nCLIP* (16k)\nCap\nCapPa\nCLIP* L/14\nCapPa L/14\nDogs [28]\nFine\n77.5\n77.9\n79.6\n81.2\n85.0\n86.0\nFlowers [45]\nFine\n85.1\n89.5\n94.0\n97.0\n96.6\n98.9\nBirds [61]\nFine\n76.9\n78.1\n76.3\n54.2\n85.0\n86.7\nPets [48]\nFine\n91.2\n91.2\n91.5\n94.4\n94.4\n95.4\nCars [32]\nFine\n90.8\n91.6\n93.4\n93.3\n94.0\n95.8\nFood [3]\nFine\n91.0\n92.1\n91.1\n91.5\n94.9\n94.2\nRESISC [8]\nCoarse\n92.5\n96.4\n90.5\n96.1\n97.1\n96.9\nProducts [46] Coarse\n88.8\n89.0\n87.8\n88.5\n90.7\n90.3\nSUN397 [71] Coarse\n81.8\n82.9\n82.0\n82.1\n85.7\n85.2\nCaltech [19]\nCoarse\n93.5\n93.1\n89.0\n86.5\n93.8\n93.2\nSTL-10 [12]\nCoarse\n98.0\n98.5\n97.7\n98.1\n99.2\n99.1\nCat/Dog [18] Coarse\n99.9\n99.9\n99.7\n99.7\n99.8\n99.9\nTable 11: Frozen transfer for zero-shot classification and retrieval via LiT [70] as a function of the\nnumber of number of training examples seen by the text encoder (the vision encoder is pretrained\nand frozen, and equipped with a MAP head which is trained along with the text encoder). The text\nencoder mirrors the architecture of the vision encoder. Especially for the larger model, CapPa is\ncompetitve with CLIP\u2217 with comparable or fewer examples seen. The CLIP numbers are obtained by\nevaluating the image and text encoders released by [50] in our eval setup. We report these numbers\nfor reference, no LiT tuning is done on top of the CLIP vision encoder. This table complements\nTable 4 in the main paper.\nImageNet 0shot\nCOCO t2i r@1\nCOCO i2t r@1\nLiT pairs:\n0\n900M\n3B\n12B\n0\n900M\n3B\n12B\n0\n900M\n3B\n12B\nCap\n-\n65.9\n67.8\n69.0\n-\n35.3\n37.5\n39.1\n-\n50.3\n53.9\n54.8\nCapPa\n-\n66.4\n68.8\n70.2\n-\n34.3\n37.3\n38.6\n-\n49.7\n53.9\n55.1\nCLIP* (8k)\n65.6\n65.9\n67.6\n69.0\n41.5\n36.5\n38.2\n39.5\n56.7\n52.0\n54.0\n56.1\nCLIP* (16k)\n67.7\n66.7\n69.0\n70.0\n43.0\n37.0\n38.9\n40.1\n58.2\n53.0\n55.1\n57.0\nCLIP\n68.3\n32.3\n52.8\nCapPa L/14\n-\n74.6\n76.4\n77.5\n-\n40.6\n43.9\n45.4\n-\n56.6\n60.3\n62.6\nCLIP* L/14\n74.8\n74.5\n75.8\n76.6\n48.1\n42.7\n44.7\n46.3\n63.7\n57.7\n60.7\n62.3\nCLIP L/14\n75.1\n36.5\n56.6\nTable 12: Performance of frozen representations trained via image captioning (Cap/CapPa) and\ncontrastive (CLIP\u2217) objective, when combined via cross-attention with a frozen T5 decoder. Only the\ncross-attention weights are updated during the training. See Table 2 for the corresponding models\nthat have the decoder trained from scratch.\nClassification\nCaptioning\nOCR\nQuestion Ans.\ni1k\nsun\nfood\nres\npet\nCOCO\nFlickr\nVQA\nVQAv2\nGQA\nCap\n79.0\u00b10.1 81.3\u00b10.1 89.3\u00b10.0 92.4\u00b10.1 92.3\u00b10.3 119.7\u00b10.6 72.2\u00b10.9\n57.7\u00b10.0\n64.6\u00b10.1 52.1\u00b10.2\nCapPa\n80.0\u00b10.0 81.2\u00b10.1 89.9\u00b10.0 93.1\u00b10.2 93.2\u00b10.3 118.7\u00b10.5 70.0\u00b10.5\n57.8\u00b10.2\n63.3\u00b10.3 51.9\u00b10.3\nCLIP\u2217 (8k)\n79.1\u00b10.0 81.5\u00b10.2 89.9\u00b10.0 92.7\u00b10.2 88.5\u00b10.2 110.6\u00b10.5 60.8\u00b11.0\n50.3\u00b10.3\n57.2\u00b10.4 49.5\u00b10.2\nCLIP\u2217 (16k)\n79.5\u00b10.1 81.7\u00b10.1 90.4\u00b10.1 93.7\u00b10.0 88.6\u00b10.1 110.6\u00b10.6 59.8\u00b10.9\n50.2\u00b10.4\n56.8\u00b10.3 49.6\u00b10.3\n17\nB.2\nScaling properties\nFig. 8 and 9 show the performance of frozen Cap, CapPa, and CLIP\u2217 encoders on a variety of tasks\nas a function of the number of training examples seen and the encoder model size, respectively.\nSpecifically, we evaluate our models on classification, captioning, and VQA when combined with a\ndecoder trained from scratch to solve all those tasks jointly (following [2]), and on 10-shot linear\nclassification based on the pre-logit features.\nTables 13, 14 and 15, 16 show the data from Fig. 8 and 9, respectively, in tabular form. For\ncompleteness, in Table 16 we also present the ImageNet zero-shot accuracy (without prompts)\nof CapPa and CLIP\u2217 models obtained with their respective pretrained decoder and encoder. We\nemphasize that scoring-based zero-shot classification is not the focus of this paper, and we did not\noptimize the Cap/CapPa models for this.\n78\n80\n82\n84\nImageNet\n80\n82\n84\nSUN397\n88\n90\n92\n94\nFood101\n94\n95\n96\nRESISC45\n92\n93\n94\n95\nPets\n115\n120\n125\nCOCO Caption\n70\n75\n80\n85\n90\nFlickr Caption\n55\n60\n65\nOCR-VQA\n66\n68\n70\nVQAv2\n900M\n3B\n9B\ntraining examples seen\n54\n56\n58\nGQA\n900M\n3B\n9B\ntraining examples seen\n50\n55\n60\n65\n70\nImageNet 10-shot\n900M\n3B\n9B\ntraining examples seen\n60\n65\n70\n75\nCIFAR100 10-shot\n900M\n3B\n9B\ntraining examples seen\n70\n75\n80\n85\n90\nPets 10-shot\n900M\n3B\n9B\ntraining examples seen\n75\n80\n85\n90\nCars 10-shot\nModel\nCapPa\nCap\nCLIP*\nArch (batch size)\nB/16 (8k)\nL/14\nB/16 (16k)\nFigure 8: Performance of vision backbones pretrained with captioning (Cap/CapPa) and contrastive\nobjective (CLIP\u2217) as a function of the number of pretraining examples seen (expands the results in\nFig. 2). Top two rows: Classification, captioning, and VQA performance with a decoder trained\nfrom scratch in multi-task fashion (see [2] for details). We use CIDEr for captioning, the VQAv2\nweighted accuracy for VQAv2, and exact matching accuracy for all other tasks. Bottom row: 10-shot\nlinear classification accuracy on the frozen pre-logit representation.\n18\n78\n80\n82\n84\nImageNet\n78\n80\n82\n84\n86\nSUN397\n85.0\n87.5\n90.0\n92.5\n95.0\nFood101\n93\n94\n95\n96\nRESISC45\n92\n94\nPets\n110\n115\n120\n125\nCOCO Caption\n70\n80\n90\nFlickr Caption\n55\n60\n65\nOCR-VQA\n64\n66\n68\n70\nVQAv2\nS\nM\nB\nL/14\nFLOPs (encoder)\n54\n56\n58\nGQA\nS\nM\nB\nL/14\nFLOPs (encoder)\n40\n50\n60\n70\nImageNet 10-shot\nS\nM\nB\nL/14\nFLOPs (encoder)\n50\n60\n70\nCIFAR100 10-shot\nS\nM\nB\nL/14\nFLOPs (encoder)\n70\n80\n90\nPets 10-shot\nS\nM\nB\nL/14\nFLOPs (encoder)\n75\n80\n85\n90\nCars 10-shot\nCapPa\nCLIP*\nFigure 9: Performance of vision backbones pretrained with captioning (CapPa) and contrastive\nobjective (CLIP\u2217) as a function of the model size/FLOPs (we compare ViT-S/16, M/16, B/16, and\nL/14; this expands the results in Fig. 2). Top two rows: Classification, captioning, and VQA\nperformance with a decoder trained from scratch in multi-task fashion (see [2] for details). We use\nCIDEr for captioning, the VQAv2 weighted accuracy for VQAv2, and exact matching accuracy for all\nother tasks. Bottom row: 10-shot linear classification accuracy on the frozen pre-logit representation.\nTable 13: Data corresponding to Fig. 8 (top two rows) in tabular form. See caption of Fig. 8 for\ndetails on the metrics.\nClassification\nCaptioning\nOCR\nQuestion Ans.\nex. seen model\narch\ni1k\nsun\nfood\nres\npet\nCOCO Flickr\nVQA\nVQAv2 GQA\n900M\nCap\nB/16 (8k)\n77.7\n79.8\n87.2\n93.3\n91.9\n113.5\n72.5\n58.6\n66.5\n54.2\nCapPa\nB/16 (8k)\n79.1\n80.5\n88.2\n94.2\n92.6\n112.2\n71.7\n58.4\n66.6\n55.0\nL/14\n81.7\n82.5\n91.1\n95.2\n94.0\n118.7\n80.2\n61.8\n68.6\n55.8\nCLIP*\nB/16 (8k)\n78.8\n80.9\n88.2\n94.5\n92.0\n111.1\n70.0\n51.7\n65.1\n53.1\nB/16 (16k) 79.2\n81.0\n88.5\n94.5\n92.4\n111.2\n70.5\n52.0\n65.6\n54.1\nL/14\n82.3\n83.6\n92.1\n95.5\n94.4\n118.2\n78.6\n56.8\n67.9\n55.5\n3B\nCap\nB/16 (8k)\n79.5\n81.6\n89.1\n93.4\n92.4\n115.6\n76.3\n60.9\n67.6\n54.5\nCapPa\nB/16 (8k)\n80.5\n81.5\n89.7\n94.4\n94.1\n116.1\n75.9\n60.9\n67.7\n54.9\nL/14\n83.3\n83.8\n92.6\n95.5\n95.3\n122.8\n84.7\n63.7\n70.1\n58.4\nCLIP*\nB/16 (8k)\n80.3\n82.3\n90.1\n94.8\n93.1\n114.2\n73.8\n55.4\n66.0\n53.8\nB/16 (16k) 80.5\n82.5\n90.8\n94.8\n92.9\n114.9\n74.5\n55.2\n66.0\n53.8\nL/14\n83.6\n85.0\n93.7\n95.8\n95.0\n121.7\n82.1\n59.8\n68.2\n55.6\n9B\nCap\nB/16 (8k)\n80.2\n82.3\n90.3\n93.6\n93.1\n117.5\n78.6\n62.2\n68.2\n55.0\nCapPa\nB/16 (8k)\n81.3\n82.4\n90.9\n94.2\n94.4\n117.9\n80.5\n62.2\n68.3\n55.7\nL/14\n84.4\n84.9\n93.8\n96.0\n95.6\n125.8\n89.3\n65.6\n70.9\n58.3\nCLIP*\nB/16 (8k)\n81.1\n83.2\n91.2\n94.8\n93.4\n115.8\n74.5\n56.0\n66.5\n54.3\nB/16 (16k) 81.4\n83.3\n92.0\n95.2\n93.6\n116.3\n77.1\n56.5\n66.7\n54.8\nL/14\n84.7\n85.7\n94.6\n96.4\n95.2\n123.2\n85.5\n61.3\n68.5\n55.3\n19\nTable 14: Data corresponding to Fig. 8 (bottom row) in tabular form. 10-shot linear classification\naccuracy on the frozen pre-logit features.\nex. seen\nmodel\narch\nImageNet\nCIFAR100\nPets\nCars\n900M\nCap\nB/16 (8k)\n49.7\n56.0\n72.6\n74.7\nCapPa\nB/16 (8k)\n50.4\n57.4\n76.2\n78.5\nL/14\n60.3\n68.8\n85.6\n87.8\nCLIP*\nB/16 (8k)\n50.6\n59.2\n70.5\n74.4\nB/16 (16k)\n50.4\n59.1\n72.6\n77.0\nL/14\n60.0\n68.7\n80.9\n83.8\n3B\nCap\nB/16 (8k)\n55.0\n58.0\n81.2\n81.1\nCapPa\nB/16 (8k)\n56.0\n60.7\n83.3\n85.2\nL/14\n66.9\n70.0\n90.8\n90.7\nCLIP*\nB/16 (8k)\n55.5\n62.3\n76.7\n78.6\nB/16 (16k)\n56.7\n63.8\n77.9\n80.7\nL/14\n66.7\n72.8\n86.2\n87.9\n9B\nCap\nB/16 (8k)\n57.2\n58.6\n83.7\n84.2\nCapPa\nB/16 (8k)\n59.1\n62.4\n86.5\n86.6\nL/14\n70.6\n72.9\n92.6\n92.2\nCLIP*\nB/16 (8k)\n58.5\n64.9\n77.7\n80.8\nB/16 (16k)\n59.7\n66.3\n80.6\n82.9\nL/14\n69.8\n74.1\n87.7\n89.2\nTable 15: Data corresponding to Fig. 9 (top two rows) in tabular form. See caption of Fig. 9 for\ndetails on the metrics\nClassification\nCaptioning\nOCR\nQuestion Ans.\narch\nFLOPs\nmodel\ni1k\nsun\nfood\nres\npet\nCOCO Flickr\nVQA\nVQAv2 GQA\nS/16\n9.2G\nCapPa\n76.5\n78.2\n85.4\n92.9\n91.2\n108.4\n65.9\n58.4\n65.0\n53.2\nCLIP*\n76.9\n80.1\n87.3\n92.6\n91.0\n108.4\n68.8\n51.7\n63.9\n52.4\nM/16\n16.0G\nCapPa\n79.0\n80.7\n88.2\n93.8\n92.7\n112.7\n71.4\n60.1\n66.5\n54.7\nCLIP*\n79.1\n81.8\n89.5\n93.7\n92.5\n112.2\n72.4\n54.2\n65.3\n53.3\nB/16\n35.1G\nCapPa\n81.3\n82.4\n90.9\n94.2\n94.4\n117.9\n80.5\n62.2\n68.3\n55.7\nCLIP*\n81.4\n83.3\n92.0\n95.2\n93.6\n116.3\n77.1\n56.5\n66.7\n54.8\nL/14 161.8G\nCapPa\n84.4\n84.9\n93.8\n96.0\n95.6\n125.8\n89.3\n65.6\n70.9\n58.3\nCLIP*\n84.7\n85.7\n94.6\n96.4\n95.2\n123.2\n85.5\n61.3\n68.5\n55.3\nTable 16: Data corresponding to Fig. 9 (bottom row) in tabular form. 10-shot linear classification\naccuracy on the frozen pre-logit features. We also show the ImageNet zero-shot classification\naccuracy (without prompts) when using the pretrained text encoder (CLIP\u2217) or text decoder with\nscoring (CapPa) for reference (last column).\narch\nFLOPs\nmodel\nImageNet\nCIFAR100\nPets\nCars\nImageNet zs.\nS/16\n9.2G\nCapPa\n40.6\n47.1\n71.8\n71.2\n35.1\nCLIP*\n47.7\n52.5\n69.0\n73.6\n52.8\nM/16\n16.0G\nCapPa\n49.8\n52.4\n79.0\n80.4\n43.0\nCLIP*\n52.8\n58.5\n76.9\n78.2\n58.7\nB/16\n35.1G\nCapPa\n59.1\n62.4\n86.5\n86.6\n52.7\nCLIP*\n59.7\n66.3\n80.6\n82.9\n64.1\nL/14\n161.8G\nCapPa\n70.6\n72.9\n92.6\n92.2\n63.8\nCLIP*\n69.8\n74.1\n87.7\n89.2\n71.2\n20\nB.3\nAttribution, relation, ordering\nTable 17 shows extended results for different models on the ARO benchmark [67] (see Table 6 in\nthe main paper). In addition to the clear superiority of Cap/CapPa over CLIP\u2217 models discussed in\nthe main paper, it can be observed that increasing the model capacity form B/16 to L/14 leads to an\noverall improvement for CapPa, while this is not the case for CLIP\u2217.\nTable 17: Results on the Attribute, Relation and Order (ARO) benchmark [67]. Cap and CapPa\nmodels clearly outperform all CLIP\u2217 and CLIP variants across all data sets, even when training the\nmodel to be sensitive to word ordering and attribution as in NegCLIP [67]. Values for \u201cARO Best\u201d\nare taken from [67]. \u201cBlind dec.\u201d corresponds to Cap without vision encoder, i.e. the vision encoder\nfeatures fed to the decoder are replaced with all zeros.\nArch\nVG Attribution\nVG Relation\nFlickr Order\nCOCO Order\nBlind dec.\n-\n83.7\n86.2\n98.8\n98.7\nCap\nB/16\n88.9\n86.6\n99.1\n99.0\nCapPa\nB/16\n85.7\n86.7\n99.2\n98.8\nCapPa\nL/14\n89.3\n86.0\n99.3\n99.0\nCLIP\u2217 (8k)\nB/16\n55.4\n39.8\n43.7\n32.8\nCLIP\u2217 (16k)\nB/16\n53.2\n39.7\n45.5\n37.0\nCLIP\u2217 (16k)\nL/14\n57.8\n35.9\n40.2\n31.5\nCLIP\nB/32\n63.2\n59.1\n59.4\n47.3\nCLIP\nB/16\n62.7\n58.7\n57.9\n49.5\nARO Best\n-\n88.0\n73.0\n60.0\n46.0\nNegCLIP\nB/32\n71.0\n81.0\n91.0\n86.0\nB.4\nSugarCrepe\nWe provide the full breakdown of results across all our models on SugarCrepe in Table 18. The\nnumbers for OpenCLIP are taken from [21] and represent the best, largest contrastive model that was\nbenchmarked on SugarCrepe to date. Even the small ViT-B/16 Cap model significantly outperforms\nit on all but the \u201cReplace Object\u201d task, which is a task that matches contrastive\u2019s \u201cbag of word\u201d-style\nof learning well.\nTable 18: Full results on the SugarCrepe [21] benchmark suite.\nTraining\nArch\nReplace\nSwap\nAdd\nObject\nAttribute\nRelation\nObject\nAttribute\nObject\nAttribute\nCap\nB/16\n91.10\n88.32\n85.21\n79.27\n88.74\n98.59\n99.28\nCapPa\nB/16\n89.95\n88.71\n84.35\n80.49\n85.74\n98.84\n99.42\nCapPa\nL/14\n92.01\n90.10\n87.34\n82.11\n88.44\n98.93\n99.42\nCLIP* (8k)\nB/16\n93.70\n82.36\n66.29\n61.79\n67.12\n83.46\n76.01\nCLIP* (16k)\nB/16\n94.07\n84.64\n67.14\n60.98\n65.47\n86.37\n77.46\nCLIP* (16k)\nL/14\n95.70\n84.26\n69.06\n65.04\n68.02\n86.76\n78.32\nOpenCLIP\nG/14\n96.67\n88.07\n74.75\n62.20\n74.92\n92.19\n84.54\nWe further show qualitative examples in Tables 22\u201324. The examples are manually picked to be\nrepresentative (we show wins and losses), while avoiding uninteresting (i.e. seemingly random), too\ncluttered, or too verbose examples. Thus, the examples are cherry-picked to be presentable, but are\nmeant to be representative. All images are from the COCO validation set.\nEach image comes with a positive and a (hard) negative caption, and a model\u2019s prediction is deemed\ncorrect when it scores the positive caption higher than the negative one. For the CapPa model, we\nscore each caption using the log-likelihood, meaning negative numbers closer to zero correspond to a\nhigher score (i.e. a score of -20 means the caption fits the image more than a score of -110). For the\nCLIP model, we score each caption using the dot-product of normalized embedding similarity as is\nusual, but we multiply the resulting score by 100 for readability.\n21\nTable 19: Impact of decoder architecture design choices in Cap on 10-shot linear classification\naccuracy: Left: Effect of sharing the embedding between decoder input and output and removing\nbiases from decoder layers. Right: Effect of the number of decoder layers.\nshare emb. dec. bias ImageNet CIFAR100 Pets Cars\nyes\nno\n47.8\n55.8\n71.5 71.7\nno\nno\n49.7\n56.0\n72.6 74.7\nyes\nyes\n48.3\n54.6\n74.4 70.2\nno\nyes\n49.3\n56.6\n72.7 71.9\ndec. layers ImageNet CIFAR100 Pets Cars\n3\n48.7\n53.7\n73.5 73.7\n6\n49.7\n56.0\n72.6 74.7\n12\n48.7\n54.8\n74.4 73.8\nWe noticed that for the Add scenarios, where CapPa performs almost perfectly, the only losses are\ndue to typos in the positive caption (\u201ctoliet\u2019 instead of \u201ctoilet\u201d and \u201cbridge\u201d instead of \u201cbride\u201d), so we\nalso provide the score for the corrected caption in the Pos (fixed), which confirms the typos are the\nreason for the model failure.\nB.5\nAblations: Decoder architecture\nWhile following the original transformer decoder architecture [60] closely, we investigate several\nmodifications that have become common in the literature [52, 11]. Specifically, we ablate the effect\nof removing biases in decoder layers, as well as sharing the decoder input and output embeddings.\nTable 19 (left) shows that not sharing the embeddings leads to overall better 10-shot accuracy than\nsharing them, and additionally removing the decoder biases does not hurt. Furthermore, we observed\nsignificantly improved stability across encoder architectures, scales and training schedules when\nremoving the decoder biases.\nTable 19 (right) reveals that the overall best 10-shot classification accuracy is obtained when using\na 6 layer decoder. This decoder depth also leads to a total parameter count comparable to the\ncorresponding CLIP\u2217 model (Table 1).\nB.6\nFurther Ablations\nTable 20 compares the performance of the CapPa and CLIP\u2217 vision encoders with a ViT-B/16\npretrained in supervised fashion on ImageNet-21k when combined with transformer decoder.\nTable 21 represents the data from Fig. 6 in tabular form.\nTable 20: Comparison of CapPa and CLIP\u2217 with a ViT-B/16 pretrained in supervised fashion on\nImageNet-21k (we use the checkpoint from Steiner et al. 2021) when combined with a transformer\ndecoder trained from scratch for classification, captioning, and VQA [2]. CLIP\u2217 and CapPa outper-\nform the model pretrained in supervised fashion.\nClassification\nCaptioning\nOCR\nQuestion Ans.\ni1k\nsun\nfood\nres\npet\nCOCO\nFlickr\nVQA\nVQAv2\nGQA\nViT-B/16 (i21k) 73.1\u00b10.1 72.8\u00b10.2 81.2\u00b10.2 86.2\u00b10.1 85.6\u00b10.3 95.0\u00b10.4 52.3\u00b10.1 39.1\u00b10.1 57.6\u00b10.3 50.1\u00b10.1\nCapPa\n81.3\u00b10.1 82.4\u00b10.1 90.9\u00b10.1 94.2\u00b10.2 94.4\u00b10.1 117.9\u00b10.6 80.5\u00b10.2 62.2\u00b10.0 68.3\u00b10.1 55.7\u00b10.2\nCLIP\u2217\n81.4\u00b10.1 83.3\u00b10.1 92.0\u00b10.1 95.2\u00b10.2 93.6\u00b10.2 116.3\u00b10.7 77.1\u00b10.7 56.5\u00b10.1 66.7\u00b10.1 54.8\u00b10.6\nC\nSocietal impact\nOur models fit in the broader context of large scale vision-language pretraining and as such share\nmany of the benefits and issues of related models such as [50, 26, 66, 40, 63]: They produce versatile\nvision models which obtain strong performance on natural images, on OCR-related tasks, and\nalso when combined with a generative language decoder. These capabilities enable many useful\napplications (e.g. assistive technologies, medical imaging), but also potentially harmful ones (e.g.\nsurveillance). We generally recommend either employing the CapPa vision encoder with a new,\ntask-specific prediction head, or using the pretrained decoder for scoring only. We do not recommend\nthe pretrained decoder for downstream image captioning applications without further refinement, as\n22\nTable 21: Ablation results representing Fig. 6 in tabular form. Left: 10-shot linear classification\naccuracy based on the frozen encoder representation as a function of the fraction of training batches\nfor which parallel prediction is performed in CapPa. Right: 10-shot linear classification accuracy\nand zero-shot classification accuracy as a function of the vision encoder architecture.\nfraction\nINet\nC100\nPets\nCars\n0%\n49.7\n56.0\n72.6\n74.7\n25%\n46.7\n52.9\n71.9\n72.8\n50%\n49.8\n57.8\n76.8\n76.9\n75%\n50.4\n57.4\n76.2\n78.5\n90%\n49.0\n59.5\n73.1\n79.0\narch\nmodel\nINet\nC100\nPets\nCars\nINet zs.\nR50\nCLIP* (8k)\n39.8\n33.5\n49.2\n60.9\n43.6\nCap\n37.8\n33.3\n48.6\n52.4\n28.5\nViT-B/32\nCLIP* (8k)\n44.1\n57.7\n64.7\n68.1\n48.3\nCap\n41.0\n53.7\n64.0\n58.7\n35.4\nViT-B/16\nCLIP* (8k)\n50.6\n59.2\n70.5\n74.4\n52.2\nCap\n49.7\n56.0\n72.6\n74.7\n43.8\nit is trained on a large number of alt-texts from the web. Harmful biases should be carefully assessed\nin the context of the concrete downstream application and prediction head used. For example, when\ncombining the encoder with a (potentially pretrained) decoder for captioning or VQA, an assessment\nof hallucinations, attribute binding issues and stereotypical attribution should be done.\n23\nTable 22: Representative examples of CapPa L/14 wins (first three) and losses (fourth) over CLIP\nL/14 on the different replace categories of the SugarCrepe hard negatives benchmark suite. See text\nfor example selection criteria. Higher score means better: for CapPa this is the log-likelihood, so\ncloser to 0 is better, while for CLIP this is the matching score (multiplied by 100) so closer to 100 is\nbetter.\nCapPa wins over CLIP\nCLIP wins over CapPa\nReplace Object\nPositive\nCapPa:-28.6 CLIP:23.6\nCapPa:-45.4 CLIP:13.2\nCapPa:-42.4 CLIP:16.8\nCapPa:-54.6 CLIP:13.7\nStreet signs on the corner\nof Gladys and Detroit\nA run down building with\ntwo planters outside the\ndoor\nA brown bird has a small\nyellow head.\nThe model toys are\npositioned on the table.\nNegative\nCapPa:-53.8 CLIP:13.9\nCapPa:-59.2 CLIP:14.8\nCapPa:-46.2 CLIP:18.1\nCapPa:-51.7 CLIP:8.8\nStreet signs on the corner\nof Gladys and Chicago.\nA run down building with\na statue outside the door.\nA brown bird has a small\nyellow beak.\nThe books are positioned\non the table.\nReplace Attribute\nPositive\nCapPa:-45.0 CLIP:14.7\nCapPa:-47.3 CLIP:13.1\nCapPa:-17.3 CLIP:15.7\nCapPa:-115.9 CLIP:14.1\nA plate of food with a\nfried egg and colorful\nvegetables.\nA bunch of different foods\non display on a counter.\nA large black truck in a\nparking lot.\nTwo large trucks are\ntravelling along a\ntree-lined roadway.\nNegative\nCapPa:-59.5 CLIP:15.1\nCapPa:-53.8 CLIP:14.8\nCapPa:-38.3 CLIP:16.1\nCapPa:-61.7 CLIP:12.5\nA plate of food with a\nfried egg and\nmonochromatic\nvegetables.\nA bunch of similar foods\non display on a counter.\nA small black truck in a\nparking lot.\nTwo large trucks are\ntravelling along a\ndeserted roadway.\nReplace Relation\nPositive\nCapPa:-20.0 CLIP:18.5\nCapPa:-48.2 CLIP:18.6\nCapPa:-29.1 CLIP:24.4\nCapPa:-55.6 CLIP:17.6\nA fire hydrant in a grassy\nfield next to a bush\nA cell phone on top of a\ncalculator near a\ncomputer keyboard.\nA red fire hydrant on a\ncity sidewalk.\nA train driving over a\nsmall bridge on a green\nhillside.\nNegative\nCapPa:-56.1 CLIP:21.5\nCapPa:-56.1 CLIP:19.2\nCapPa:-35.6 CLIP:25.6\nCapPa:-54.7 CLIP:17.4\nA fire hydrant in a grassy\nfield far from a bush.\nA cell phone underneath\na calculator near a\ncomputer keyboard.\nA red fire hydrant beside\na city sidewalk.\nA train passing under a\nsmall bridge on a green\nhillside.\n24\nTable 23: Representative examples of CapPa L/14 wins (first three) and losses (fourth) over CLIP\nL/14 on the different add categories of the SugarCrepe hard negatives benchmark suite. See text for\nexample selection criteria. Higher score means better: for CapPa this is the log-likelihood, so closer\nto 0 is better, while for CLIP this is the matching score (multiplied by 100) so closer to 100 is better.\nCapPa wins over CLIP\nCLIP wins over CapPa\nAdd Object\nPositive\nCapPa:-18.2 CLIP:13.7\nCapPa:-30.2 CLIP:14.3\nCapPa:-27.3 CLIP:14.0\nCapPa:-60.3 CLIP:22.5\nA bathroom with a\nmirror and a sink.\nA two layered cake sits\non a table top\nan image of a plate of\nfood with meat and\nveggies\nA bridge and groom\ncutting their wedding\ncake that has fruit on top.\nNegative\nCapPa:-150.3 CLIP:13.7 CapPa:-64.9 CLIP:15.5\nCapPa:-148.6 CLIP:14.6 CapPa:-53.4 CLIP:21.6\nA bathroom with a mirror,\nsink, and shower.\nA two layered cake sits\non a table top next to a\nvase of flowers.\nAn image of a plate of\nfood with meat, fruit, and\nveggies.\nA bride and groom\ncutting their wedding\ncake that has flowers and\nfruit on top.\nPos (fixed)\nCapPa:-46.1 CLIP:22.7\nA bride and groom\ncutting their wedding\ncake that has fruit on top.\nAdd Attribute\nPositive\nCapPa:-43.8 CLIP:21.0\nCapPa:-65.4 CLIP:16.1\nCapPa:-49.9 CLIP:17.6\nCapPa:-62.1 CLIP:20.4\nA little girl smiling for\nthe camera with an\numbrella behind her.\nA clock fastened to a\nbrick store front reads 10\nafter 10\nA person frying some\nkind of food on a stove.\nThere is a stuffed animal\nsitting on the toliet.\nNegative\nCapPa:-121.5 CLIP:21.0 CapPa:-90.7 CLIP:17.0\nCapPa:-115.3 CLIP:19.5 CapPa:-49.8 CLIP:19.1\nA little girl smiling for\nthe camera with a\npolka-dotted umbrella\nbehind her.\nA clock fastened to a lush\nbrick store front reads 10\nafter 10.\nA person frying some\ncurry-spiced food on a\nstove.\nThere is a stuffed animal\nsitting on the decorated\ntoilet.\nPos (fixed)\nCapPa:-36.8 CLIP:21.3\nThere is a stuffed animal\nsitting on the toilet.\n25\nTable 24: Representative examples of CapPa L/14 wins (first three) and losses (fourth) over CLIP\nL/14 on the different swap categories of the SugarCrepe hard negatives benchmark suite. See text for\nexample selection criteria. Higher score means better: for CapPa this is the log-likelihood, so closer\nto 0 is better, while for CLIP this is the matching score (multiplied by 100) so closer to 100 is better.\nCapPa wins over CLIP\nCLIP wins over CapPa\nSwap Object\nPositive\nCapPa:-33.5 CLIP:22.5\nCapPa:-54.9 CLIP:22.1\nCapPa:-38.1 CLIP:15.6\nCapPa:-111.4 CLIP:20.5\na bright kitchen with\ntulips on the table and\nplants by the window\nA person cutting a pizza\nnext to a salad and\nbottles of wine on\nwooden table.\nA close up of a sandwich\nwith a drink in the back.\nStatues on the second\nfloor of a building, sitting\nbelow a clock.\nNegative\nCapPa:-56.8 CLIP:22.8\nCapPa:-57.5 CLIP:22.5\nCapPa:-45.6 CLIP:16.2\nCapPa:-110.8 CLIP:19.4\nA bright kitchen with\nplants on the table and\ntulips by the window.\nA person cutting a salad\nnext to a pizza and\nbottles of wine on\nwooden table.\nA close up of a drink with\na sandwich in the back.\nA clock on the second\nfloor of a building, sitting\nbelow statues.\nSwap Attribute\nPositive\nCapPa:-32.6 CLIP:15.4\nCapPa:-45.9 CLIP:19.4\nCapPa:-28.4 CLIP:16.3\nCapPa:-108.6 CLIP:19.3\na white cake is by a\nbunch of flowers\nA blue tennis racket has\na yellow tennis ball on it.\na black bike rests against\na brown bed\nAll of the cows are\npoking their heads out,\neating some hay.\nNegative\nCapPa:-64.1 CLIP:17.3\nCapPa:-54.9 CLIP:19.5\nCapPa:-52.8 CLIP:16.9\nCapPa:-107.1 CLIP:18.2\nA bunch of cakes are by a\nwhite flower.\nA yellow tennis racket\nhas a blue tennis ball on\nit.\na brown bike rests\nagainst a black bed.\nSome cows are poking\ntheir heads out, eating\nall of the hay.\n26\n"
  },
  {
    "title": "ATT3D: Amortized Text-to-3D Object Synthesis",
    "link": "https://arxiv.org/pdf/2306.07349.pdf",
    "upvote": "8",
    "text": "ATT3D: Amortized Text-to-3D Object Synthesis\nJonathan Lorraine\nKevin Xie\nXiaohui Zeng\nChen-Hsuan Lin\nTowaki Takikawa\nNicholas Sharp\nTsung-Yi Lin\nMing-Yu Liu\nSanja Fidler\nJames Lucas\nNVIDIA Corporation\nExisting Methods\nATT3D: Amortized Text-to-3D\n\u201cA monkey sitting\nin a chair wearing\na suit .. party hat\u201d\n\u201cA pig riding a \nmotorbike wearing a\nbackpack .. top hat\u201d\n...etc...\nNeRF\n\u201cA monkey sitting\nin a chair wearing\na suit .. party hat\u201d \n\u201cA pig riding a \nmotorbike wearing a\nbackpack .. top hat\u201d\n...etc...\n1hr per prompt\n1sec per prompt\nNeRF\ntext\nNeRF\nmapping\nnetwork\ntrained offline\n...\n...\nexpensive per-prompt\noptimization\nExisting Methods\nATT3D: Amortized Text-to-3D\nRequires 1 hour\nRequires < 1 sec\nFigure 1: Our method initially trains one network to output 3D objects consistent with various text prompts. After, when\nwe receive an unseen prompt, we produce an accurate object in < 1 second, with 1 GPU. Existing methods re-train the\nentire network for every prompt, requiring a long delay for the optimization to complete. Further, we can interpolate between\nprompts for user-guided asset generation (Fig. 3). We include a project webpage with an overview and videos.\nAbstract\nText-to-3D modelling has seen exciting progress by com-\nbining generative text-to-image models with image-to-3D\nmethods like Neural Radiance Fields.\nDreamFusion re-\ncently achieved high-quality results but requires a lengthy,\nper-prompt optimization to create 3D objects. To address\nthis, we amortize optimization over text prompts by training\non many prompts simultaneously with a unified model, in-\nstead of separately. With this, we share computation across\na prompt set, training in less time than per-prompt optimiza-\ntion. Our framework \u2013 Amortized text-to-3D (ATT3D) \u2013 en-\nables knowledge sharing between prompts to generalize to\nunseen setups and smooth interpolations between text for\nnovel assets and simple animations.\n1. Introduction\n3D content creation is important because it allows for\nmore immersive and engaging experiences in industries\nsuch as entertainment, education, and marketing. However,\n3D design is challenging due to technical complexity of\nthe 3D modeling software, and the artistic skills required\nto create high-quality models and animations. Text-to-3D\n(TT3D) generative tools have the potential to democratize\n3D content creation by relieving these limitations. To make\nthis technology successful, we desire tools that provide fast\nresponses to users while being inexpensive for the operator.\nRecent TT3D methods [1, 2] allow users to generate\nhigh-quality 3D models from text-prompts but use a lengthy\n(\u223c15 minute to >1 hour [1, 2]) per-prompt optimization.\nHaving users wait between each iteration of prompt engi-\nneering results in a sporadic and time-consuming design\nprocess. Further, generation for a new prompt requires mul-\ntiple GPUs and uses large text-to-image models [3\u20135], cre-\nating a prohibitive cost for the pipeline operator.\nWe split the TT3D process into two stages. First, we op-\ntimize one model offline to generate 3D objects for many\ndifferent text prompts simultaneously. This amortizes opti-\nmization over the prompts, by sharing work between similar\ninstances. The second, user-facing stage uses our amortized\nmodel in a simple feed-forward pass to quickly generate an\nobject given text, with no further optimization required.\nOur method, Amortized text-to-3D (ATT3D), produces a\nmodel which can generate an accurate 3D object in < 1 sec-\nond, with only 1 consumer-grade GPU. This TT3D pipeline\ncan be deployed more cheaply, with a real-time user ex-\nperience. Our offline stage trains the ATT3D model sig-\nnificantly faster than optimizing prompts individually while\nretaining or even surpassing quality, by leveraging compo-\nsitionality in the parts underlying each 3D object. We also\ngain a new user-interaction ability to interpolate between\nprompts for novel asset generation and animations.\n1\narXiv:2306.07349v1  [cs.LG]  6 Jun 2023\nATT3D\nPer-prompt Training\nFigure 2: We show results on a compositional prompt set. Each row has a different activity, while each column has a theme,\nwhich we combine into the prompt \u201ca pig {activity} {theme}.\u201d while we evaluate generalization on a held-out set\nof unseen testing prompts in red on the diagonal. Left: Our method. Interestingly, the amortized objects have a unified\norientation. Right: The per-prompt training baseline [1], with a random initialization for unseen prompts to align compute\nbudgets. Takeaway: Our model performs comparably to per-prompt training on the seen prompts, with a far smaller compute\nbudget (Fig. 6). Importantly, we perform strongly on unseen prompts with no extra training, unlike per-prompt training.\n\u201c... dress made of fruit ...\u201d\n\u201c... dress made of garbage bags...\u201d\nRendered frames from ATT3D with text embedding (1 \u2212 \u03b1)c1 + \u03b1c2 for \u03b1 \u2208 [0, 1]\n\u201csnowy rock\u201d\n\u201cjagged rock\u201d\n\u201cmossy rock\u201d\n\u201c... cottage with a thatched roof\u201d\n\u201c... house in Tudor Style\u201d\n\u201c... red convertible\u201d\n\u201c... destroyed car\u201d\n\u201c...in the spring\u201d\n\u201c...in the summer\u201d\n\u201c...in the fall\u201d\n\u201c...in the winter\u201d\nFigure 3: We show renders of our model\u2019s output on interpolated text embeddings (1\u2212\u03b1)c1 +\u03b1c2. We generate a continuum\nof landscape, clothing, building, and vehicle assets, and use chains of prompts for animations, like seasonality in a tree.\n1.1. Contributions\nWe present a method to synthesize 3D objects from text\nprompts immediately. By using amortized optimization we\ncan:\n\u2022 Generalize to new prompts \u2013 Fig. 2.\n\u2022 Interpolate between prompts \u2013 Fig. 3.\n\u2022 Amortize over settings other than text prompts \u2013\nSec. 3.2.2.\n\u2022 Reduce overall training time \u2013 Fig. 6.\n2. Background\nThis section contains concepts and prior work relevant to\nour method, with notation in App. Table 1.\n2.1. NeRFs for Image-to-3D\nNeRFs [6] represent 3D scenes via a radiance field pa-\nrameterized by a neural network. We denote 3D coordi-\nnates with x = [x, y, z] \u2208 X and the radiance values with\nr = [\u03c3, r, g, b] \u2208 R. NeRFs are trained to output radi-\nance fields to render frames similar to multi-view images\nwith camera information. Simple NeRFs map locations x\nto radiances r via an MLP-parameterized function. Recent\nNeRFs use spatial grids storing parameters queried per lo-\ncation [7\u20139], integrating spatial inductive biases. We view\nthis as a point-encoder function \u03b3w:X \u2192\u0393 with parameters\nw encoding a location x before the final MLP \u03bd :\u0393\u2192R.\nr = \u03bd (\u03b3w (x))\n(1)\n2.2. Text-to-Image Generation\nThe wide availability of captioned image datasets has\nenabled the development of powerful text-to-image gener-\native models. We use a DDM with comparable architec-\nture to recent large-scale methods [3\u20135]. We train for score-\nmatching, where (roughly) input images have noise added\nto them [10, 11] that the DDM predicts. Critically, these\nmodels can be conditioned on text to generate matching im-\nages via classifier-free guidance[12]. We use pre-trained\nT5-XXL [13] and CLIP [14] encoders to generate text em-\nbeddings, which the DDM conditions on via cross-attention\nwith latent image features. Crucially, we reuse the text to-\nken embeddings \u2013 denoted c \u2013 for modulating our NeRF.\n2.3. Text-to-3D (TT3D) Generation\nPrior works rely on per-prompt optimization to generate\n3D scenes. Recent TT3D methods [1, 15] use text-to-image\ngenerative models to train NeRFs. To do so, they render\na view and add noise. The DDM, conditioned on a text\nprompt, approximates \u03f5 with \u02c6\u03f5, using the difference \u02c6\u03f5\u2212\u03f5 to\nupdate NeRF parameters. We outline this method in Alg. 1\nand Fig. 4 and refer to DreamFusion Sec. 3 for more details.\n2.4. Amortized Optimization\nAmortized optimization methods use learning to predict\nsolutions when we repeatedly solve similar instances of the\nsame problem [16]. Current TT3D independently optimizes\nprompts, whereas, in Sec. 3, we use amortized methods.\nA typical amortization strategy is to find a problem\ncontext \u2013 denoted z \u2013 to change our optimization, with\nsome strategies specialized for NeRFs [17].\nFor ex-\nample, concatenating the context to the NeRF\u2019s MLP:\nr(x, z) = \u03bd(\u03b3(x), z) Or, having a mapping network m\noutputting modulations to the weights or hidden units:\nr (x, z) = \u03bd (\u03b3m(z) (x))\n(2)\nBut, designing useful contexts, z, can be non-trivial.\n3. Our Method: Amortized Text-to-3D\nOur method has an initial training stage using amortized\noptimization, after which we perform cheap inference on\nnew prompts. We first describe the ATT3D architecture and\nits use during inference, then the training procedure.\n3.1. The Amortized Model used at Inference\nAt inference, our model consists of a mapping network\nm, a NeRF \u03bd, and a spatial grid of features \u03b3w with pa-\nrameters w (Fig. 4).\nThe mapping network takes in an\n(encoded) text prompt c and produces feature grid modu-\nlations: \u03b3m(c). Our final NeRF module \u03bd is a small MLP\nacting on encoded points \u03b3m(c)(x) \u2013 Eq. 1 \u2013 representing\na 3D object for the text prompt with the modulated feature\ngrid. Full details are in App. Sec. B.1 and summarized here.\nArchitectural details: We followed Instant NGP [7] for\nour NeRF, notably using multi-resolution voxel/hash grids\nfor our point-encoder \u03b3. We use hypernetwork modula-\ntions for implementation and computational simplicity, with\nalternatives of concatenation and attention considered in\nApp. B.1.3.\nHypernetwork approaches output the point-\nencoder parameters w from a text embedding c:\nw = Hypernetwork(c)\n(3)\nWe simply output via a vector v from the text embeddings,\nwhich is used to output the parameters via linear maps.\nv = SiLU(linearspec.norm\nw/ bias\n(flatten(c)))\n(4)\nw = reshape(linearspec.norm\nno bias\n(v))\n(5)\nThis w parameterizes the point-encoder \u03b3w, which is used\nto evaluate radiances per-point as per Eq. 1. This simple ap-\nproach solved our prompt sets, so we used it in all results.\nUsing more sophisticated hypernetworks performed com-\nparably but was slower. However, this may be necessary for\nscaling to more complicated sets of prompts.\nNeRF\nnetwork \nmapping\nnetwork\ntext encoder (CLIP/T5)\ntext encoder (CLIP/T5)\nTraining\ntext prompts\n\u201cA bunny sitting\non some pancakes\u201d\nspatial\nfeatures\nembeddings\ninterpolated\nlookup\nposition\nvolume\nrenderings\nInference\nrendered\nviews\nspatial\nfeatures\nadd\nnoise\ntext-conditioned\ndenoising diffusion model\nDDM\n\u0394 noise\nupdate\nfeature-encoded\nposition\nposition\nNeRF\nnetwork \nmapping\nnetwork\ntext encoder (CLIP/T5)\ntext encoder (CLIP/T5)\nnew text prompt\n\u201cA frog wearing\na red sweater.\u201d\nspatial\nfeatures\nembedding\ninterpolated\nlookup\nposition\nspatial\nfeatures\nfeature-encoded\nposition\nposition\n3D object\nFigure 4: We show a schematic of our text-to-3D pipeline with changes from DreamFusion\u2019s pipeline [1] shown in red and\npseudocode in Alg. 1. The text encoder (in green) provides its \u2013 potentially cached \u2013 text embedding c to the text-to-image\nDDM and now also to the mapping network m (in red). We use a spatial point-encoder \u03b3m(c) (in blue) for our position x,\nwhose parameters are modulations from the mapping network m(c). The final NeRF MLP \u03bd outputs a radiance r given the\npoint encoding: r = \u03bd(\u03b3m(c)(x)), which we render into views. Left: At training time, the rendered views are input to the\nDDM to provide a training update. The NeRF network \u03bd, mapping network m, and (effectively) the spatial point encoding\n\u03b3m(c) are optimized. Right: At inference time, we use the pipeline up to the NeRF for representing the 3D object.\nDreamFusion\nDreamFusion reimpl.\nPer-prompt Training\n= Our Method, ATT3D\nAmortized Training\nmatte painting of a\ncastle made of cheesecake\nsurrounded by a moat\nmade of ice cream\na vase with\npink flowers\na hamburger\nFigure 5: Here we qualitatively assess our method rela-\ntive to the baseline per-prompt training \u2013 i.e., DreamFu-\nsion\u2019s method. A public DreamFusion implementation is\nnot available. Takeaway: Our re-implementation achieves\nsimilar quality to the original. Also, our amortized method\nperforms comparably to per-prompt training.\nDesigning larger prompt sets was challenging because\nthe per-prompt baselines could not effectively handle open-\ndomain text prompts. We partially overcame this limitation\nby creating compositional prompt sets using prompt com-\nponents that the underlying model effectively handled.\n3.2. Amortized Text-to-3D Training\nAlg. 1 overviews our training procedure. In each opti-\nmization step, we sample several prompts and produce their\n\u2013 potentially cached \u2013 text embeddings z, which we use to\ncompute the modulations m(c). We also sample camera\nposes and rendering conditions. These are combined with\nthe NeRF module to render our images. We then use the\nScore Distillation Sampling loss [1] to update the NeRF.\nAs in prior work, we augment text prompts depending on\ncamera position \u2013 \u201c..., front/side/rear view\u201d. We provide\nthe text embeddings (without augmentation) to the mapping\nnetwork to modulate the NeRF.\n3.2.1\nStabilizing Optimization\nThe NeRF\u2019s loss is specified by a denoising diffusion model\n(DDM) and thus changes during training akin to bilevel se-\ntups like GANs [18\u201320] and actor-critic models [21]. We\nuse techniques from nested optimization to stabilize train-\ning motivated by observing similar failure modes. Specifi-\ncally, we required spectral normalization [19] \u2013 crucial for\nlarge-scale GANs [20] \u2013 to mitigate numerical instability.\nRemoving optimization momentum helped minimize os-\ncillations from complex dynamics as in nested optimiza-\ntion [22, 23]. Unlike DreamFusion, we did not benefit from\nDistributed Shampoo [24] and, instead, use Adam [25].\n3.2.2\nAmortizing Over Other Settings\nSo far, we described amortizing optimization over many\nprompts. More generally, we can amortize over other vari-\nables like the choice of guidance weight, regularizers, data\naugmentation, or other aspects of the loss function. We use\nthis to explore techniques for allowing semantically mean-\ningful prompt interpolations, which is a valuable property\nof generative models like GANs [18] and VAEs [26].\nThere are various prompt interpolation strategies we can\namortize over, like, between text embeddings, guidance\nweights, or loss functions; see App. Fig. 18 for specifics.\nTo sample an interpolated setup, we sample prompt (em-\nbedding) pairs c1, c2 and an interpolant weight \u03b1 \u2208 [0, 1].\nWe must give this information to our mapping network -\nex., by making it an input m(c1, c2, \u03b1). Instead, we input\ninterpolated embeddings, allowing an unmodified architec-\nture and incorporating prompt permutation invariance:1\nm ((1 \u2212 \u03b1) c1 + \u03b1c2)\n(6)\nIn addition to the text prompts distribution, we must\nchoose the interpolant weights \u03b1\u2019s distribution.\nFor ex-\nample, we could sample uniform \u03b1 \u2208 [0, 1], or a binary\n\u03b1 \u2208 {0, 1} \u2013 i.e., training without interpolants \u2013 which are\nboth special cases of a Dirichlet distribution. The Dirichlet\nconcentration coefficient is another user choice to change\nresults qualitatively \u2013 see App. Fig. 19. We show exam-\nples of various loss interpolations in Figs. 3 and 20. The\ninterpolation setup is further details in App. Sec. B.1.14.\n3.3. Why We Amortize\nReduce training cost (Fig. 6): We train on text prompts for\na fraction of the per-prompt cost.\nGeneralize to unseen prompts (Fig. 2, 8): We seek strong\nperformance when evaluating our model on unseen prompts\nduring the amortized training without extra optimization.\nPrompt interpolations (Fig. 3): Unlike current TT3D, we\ncan interpolate between prompts, allowing: (a) generating a\ncontinuum of novel assets, or (b) creating 3D animations.\n4. Results and Discussion\nHere, we investigate our method\u2019s potential benefits. We\nrefer to the baseline as \u201cper-prompt optimization\u201d, which\nfollows existing works using separate optimization for each\nprompt. The specific NeRF rendering and SDS loss im-\nplementation are equivalent between the baseline and our\nmethod \u2013 see Fig. 5. App. Sec. C contains additional exper-\niments, ablations, and visualizations.\n1By invariance we actually mean m(c1,c2,\u03b1) = m(c2,c1,1 \u2212 \u03b1).\nAlgorithm 1 ATT3D Pseudocode for each update\nChanges from DreamFusion Sec. 3 shown in red\n1: for each loss term in batch do\n2:\nsample a text and it\u2019s embedding c\n3:\ncompute the modulation m\u2032 = m(c)\n4:\nsample camera position\n5:\nadd front/side/back to text, given camera\n6:\nsample textureless/shadeless/full render\n7:\nperform the render:\n8:\ncreate a ray for each pixel in the frame\n9:\nat each ray, sample multiple points x\n10:\nat each point, compute encoding \u03b3\u2032 =\u03b3m\u2032(x)\n11:\nat each point, compute the radiance \u03bd(\u03b3\u2032)\n12:\ncomposite radiance into a frame\n13:\nadd noise to frame\n14:\ncompute denoised frame with the DDM via \u02c6\u03f5\n15:\ncompute gradient using \u02c6\u03f5 \u2212 \u03f5 as per SDS\n4.1. How We Evaluate ATT3D\nWe first describe the datasets we use, then our metrics\nfor quality and cost.\n4.1.1\nOur Text Prompt Datasets\nDreamFusion (DF): The DF27 dataset consists of the 27\nprompts from DreamFusion\u2019s main paper, while DF411 has\n411 prompts from the project page. We explore memorizing\nthese datasets but find them unsuitable for generalization.\nCompositional: To test generalization, we design a compo-\nsitional prompt set by composing fragments with the tem-\nplate \u201ca {animal} {activity} {theme}\u201d and hold out\na subset of \u201cunseen\u201d prompts. Our model must general-\nize to unseen compositions that require nontrivial changes\nto geometry. Using this template, we created a small pig-\nprompts and a larger animal-prompts dataset detailed in\nApp. Sec. B.1.12 and shown in Figs. 2 and 8. We hold out\n8 out of the 64 pig prompts, as shown in Fig. 2. For the ani-\nmals, the held-out prompts are sampled homogeneously and\nwe investigate holding out larger fractions of the prompts.\n4.1.2\nOur Evaluation Metrics\nCost: We measure the computational cost of training per-\nprompt models versus our amortized approach. Wall-clock\ntime and number of iterations are insufficient because we\ntrain with varying compute setups and numbers of GPUs \u2013\nsee App. Sec. B.2. To account for this difference, we mea-\nsure the number of rendered frames used for training (nor-\nmalized by the number of prompts). Specifically, this is the\nnumber of optimization iterations times batch size divided\nby the total number of prompts in the dataset.\nAverage R-probability\nDF27 Prompts (Small)\nPig Prompts (Small + Compositional)\nCompute Budget = Number of rendered frames used in training per prompt\nAnimal Prompts (Large + Compositional)\nFigure 6: We display the quality against compute budget for a split of seen & unseen (dashed) prompts with our method (in\nblue and green) & existing work\u2019s per-prompt optimization baseline (in red). Our method is only trained on the seen split\nof the prompts. At a given training iteration, the amortized model is evaluated zero-shot on unseen prompts. Takeaway:\nFor any compute budget, we achieve a higher quality on both the seen and unseen prompts. Our benefits grow for larger,\ncompositional prompt sets. Left: The 27 prompts from DreamFusion (Fig. 11). Middle: The 64 compositional pig prompts\n(Fig. 2). Per-prompt optimization cannot perform zero-shot generation for unseen prompts, so we report the performance of a\nrandom initialization baseline. Right: The 2400 compositional animal prompts (Fig. 8), with varying prompt proportions used\nin training. The generalization gap is small when training on 50% of the prompts. Notably, the cheap testing performance is\nbetter than the expensive per-prompt method with only 12.5% of the prompts.\nQuality: CLIP R-(prec.)ision is a text-to-3D correspon-\ndence metric introduced in Dream Fields [27], defined as\nthe CLIP model\u2019s accuracy at classifying the correct text\ninput of a rendered image from amongst a set of distrac-\ntor prompts (i.e., the query set). CLIP R-(prob.)ability is\nthe probability assigned to the correct prompt instead of the\nbinary accuracy, preserving information about confidence,\nand reducing noise. We found that R- metrics track each\nother (App. Fig. 12), so we focus on R-prob. We evaluate\nR-prob. averaged over the input prompt dataset and four\ndistinct rendered views as in DreamFusion [1], using the\nentire dataset as our query set. The queries in DF27 are\nhighly dissimilar, so we make the metric harder by adding\nthe DF411 prompts to the query set.\n4.2. Can We Reduce Training Cost?\nBefore evaluating generalization, we see if our method\ncan optimize a diverse prompt collection faster than opti-\nmizing individually. Fig. 6 gives the R-probability against\ncompute budget for our method & per-prompt optimization,\nshowing we achieved higher quality for any budget. App.\nFigs. 11 and 14, qualitatively show we accurately memo-\nrize all prompts in DreamFusion\u2019s main paper and extended\nprompt set for a reduced cost \u2013 perhaps from component re-\nuse as in App. Fig. 15. So, we have a powerful optimization\nmethod that quickly memorizes training data.\nBut does the performance generalize to unseen prompts?\nCurrent TT3D methods optimize 1 prompt, so any gener-\nalization is a valuable contribution. App. Fig. 16 shows\nunseen composed and interpolated prompts, with promising\nresults, which we improve in Secs. 4.3 and 4.4 respectively.\n4.3. Can We Generalize to Unseen Prompts?\nNext, we investigate generalizing to unseen prompts\nwith no extra optimization. We used compositional prompt\ndatasets to evaluate (compositional) generalization in the\nsmaller pig and larger animal prompt datasets. Fig. 6 shows\nR-probability against compute budget on both seen & un-\nseen prompts for our method & per-prompt optimization\nshowing that we achieved higher quality for any compute\nbudget on both prompt sets. Our generalization is especially\nevident in the larger prompt set, where we held out a signif-\nicant fraction of the training prompts. With 50% of prompts\nwithheld, we have a minimal generalization gap. With only\n12.5% (300) prompts seen during training, generalization to\nunseen prompts was better than per-prompt optimization on\nseen prompts with only 1/4 the per-prompt compute budget.\nTo understand the superior performance, we visually\ncompare a subset of pig prompts with the \u201cholding a blue\nballoon\u201d activity in Fig. 7. ATT3D produced more con-\nsistent results than per-prompt optimization, potentially ex-\nplaining our higher R-probability. Visualizations for the pig\nand animal experiments are in Figs. 2 and 8, respectively.\nThis confirms we can achieve strong generalization perfor-\nmance with a sufficient prompt set. Further, quality can be\nimproved with fine-tuning strategies (App. Fig. 17).\n4.4. Can We Make Useful Interpolations?\nNext, we investigate our method\u2019s ability to create ob-\njects as we interpolate between text prompts with no addi-\ntional test-time optimization. In Fig. 3, we show rendered\noutputs as we interpolate between different prompts. The\noutput remains realistic with smooth transitions.\nAmortized Training\n\u201c...holding a blue balloon\u201d\nPer-prompt optimization\nFigure 7: We compare amortized and per-prompt optimization on the prompts of the form \u201c...holding a blue balloon.\u201d\nAmortization discovers a canonical orientation and always makes the balloon blue, while per-prompt training may only make\nthe background blue or fail altogether, potentially explaining performance improvements in Fig. 6.\nFor Fig. 3, top right, we did not use loss amortization and\ngeneralize to interpolants while only training on the 3 rock\nprompts. But, some prompts gave suboptimal results with-\nout interpolant training (App. Fig. 16) which we improved\nby interpolant amortization (Sec. 3.2.2). We evaluated sev-\neral prompt interpolation approaches. App. Fig. 18 com-\npares 3 interpolant amortization types: loss weightings, in-\nterpolated embeddings, and guidance weightings, showing\nvarious ways to control results. App. Fig. 19 compares dif-\nferent interpolant sampling strategies during training, pro-\nviding qualitatively different ways to generate assets.\n5. Related Work\nWe cover the various fields our method combines: (a)\ntext-to-image generation, then (b) image-to-3D models,\nwhich lead to (c) text-to-3D models, which we augment\nwith (d) amortized optimization.\nText-to-image Generation:\n(A)TT3D methods [1, 2, 15]\nuse large-scale text-conditional DDMs [3, 4, 28\u201330], which\ntrain using classifier-free guidance to sample images match-\ning text prompts [12]. While these models generate diverse\nand high-fidelity images for many prompts, they cannot pro-\nvide view-consistent renderings of a single object and are\nthus incapable of making 3D assets directly.\nImage-to-3D Models:\nBeyond using 3D assets to train\n3D generative models, prior work has also used image\ndatasets. Most of these methods use NeRFs [6, 17, 31\u2013\n34] as a differentiable renderer optimized to produce im-\nage datasets. Differentiable mesh rendering is an alterna-\ntive [35\u201338]. Chan et al. [9] are closely related in this cate-\ngory, using a StyleGAN generator modulated with a learned\nlatent code to produce a triplanar grid that is spatially inter-\npolated and fed through a NeRF producing a static image\ndataset. We also modulate spatially oriented feature grids,\nwithout relying on memory-intensive pre-trained generator\nbackbones. These techniques may prove valuable in future\nwork scaling to ultra-large prompt sets.\nText-to-3D Generation:\nRecent advances include CLIP-\nforge [39], CLIP-mesh [40], Latent-NeRF [41], Dream\nField [27], Score-Jacobian-Chaining [15], & DreamFu-\nsion [1]. In CLIP-forge [39], the model is trained for shapes\nconditioned on CLIP text embeddings from rendered im-\nages.\nDuring inference, the embedding is provided for\nthe generative model to synthesize new shapes based on\nthe text. CLIP-mesh [40] and Dream Field [27] optimized\nthe underlying 3D representation with the CLIP-based loss.\nMagic3D adds a finetuning phase with a textured-mesh\nmodel [42], allowing high resolutions.\nFuture advances\nmay arise by combining with techniques from uncondi-\ntional 3D generation [43\u201345]. Notable open-source contri-\nbutions are Stable-Dreamfusion [46] and threestudio [47].\nOther concurrent works include Zero-1-to-3 [48], Fanta-\nsia3D [49], Dream3D [50], DreamAvatar [51], and Prolific-\nDreamer [52]. However, we differ from all of these text-to-\n3D works, because we amortize over the text prompts.\nAmortized Optimization:\nAmortized optimization [16]\nis a tool of blossoming importance in learning to opti-\nmize [53] and machine learning, with applications to meta-\nlearning [54], hyperparameter optimization [55, 56], and\ngenerative modeling [26, 57\u201359]. Hypernetworks [60] are a\npopular tool for amortization [55, 56, 61, 62] and have also\nbeen used to modulate NeRFs [17, 63, 64], inspiring our\nstrategy. Our method differs from prior works by modulat-\ning spatially oriented parameters, and our objective is from\na (dynamic) DDM instead of a (static) dataset.\nText-to-3D Animation:\nText-to-4D [65] is an approach\nfor directly making 3D animations from text, instead of our\ninterpolation strategy. This is done by generalizing TT3D\nto use a text-to-video model [28, 66, 67], instead of a text-\nto-image model. However, unlike us, this requires text-to-\nvideo, which can require video data.\nTesting prompt for Amortized 50% split, at 4800\nTesting prompt for Amortized 12.5% split, at 4800\nPer-prompt at 4800\nFigure 8: We show quantitative results for the 2400 animal prompts in Fig. 6, where we achieve a higher quality for any\ncompute budget on seen & unseen prompts. Notably, when training on only 50% or 12.5% of the prompts, the unseen\nprompts \u2013 which cost no optimization \u2013 perform stronger than the per-prompt method, which must optimize on the data.\nTakeaway: By training a single model on many text prompts we generalize to unseen prompts without extra optimization.\nFigure 9: Results for amortized training on DreamFusion\u2019s\nextended set of 411 text prompts, DF411. See Fig. 14 for\nthe full set. Takeaway: We scale to diverse prompt sets\n>10\u00d7 larger than DF27 (Fig. 11) with minor quality drop.\n6. Conclusion\nWe presented ATT3D, a method for amortized optimiza-\ntion of text-to-3D (TT3D) models. We use a mapping net-\nwork from text to NERFs, enabling a single model to rep-\nresent 3D objects of many different prompts. We experi-\nmentally validate our method on existing and new compo-\nsitional prompt sets. We are faster at training than current\nTT3D methods by sharing the optimization cost across a\nprompt set. Once trained, our model generalizes by directly\noutputting objects for prompts unseen during training in a\nsingle forward pass. Furthermore, by amortizing over inter-\npolation weights, we quickly generate a continuum of inter-\npolations between prompts, enhancing user control.\nAlthough ATT3D only represents a small step towards\ngeneral and fast text-to-3D generation, we believe that the\nideas presented are a promising avenue toward this future.\nLimitations:\nOur method builds on the existing text-to-\n3D optimization paradigm, so we share several limitations\nwith these works: More powerful text-to-image DDMs may\nbe required for higher quality and robustness in results. The\nobjective has high variance, and the system can be sensi-\ntive to prompt engineering. We also suffer from a lack of\ndiversity, as in prior work. We found that similar prompts\ncan collapse to the same scene when amortizing. Finally,\nlarger object-centric prompt sets are required to further test\nthe scaling of amortized training.\nEthics Statement:\nText-to-image models carry ethical\nconcerns for synthesizing images, which text-to-3D mod-\nels like this share. For example, we may inherit any biases\nin our underlying text-to-image model. These models could\ndisplace creative jobs or enable the growth and accessibility\nof 3D asset generation. Alternatively, 3D synthesis models\ncould be used to generate misinformation by bad actors.\nReproducibility\nStatement:\nOur\ninstant-NGP\nNeRF\nbackbone is publicly available through the \u201cinstant-ngp\u201d\nrepository [7]. While our diffusion model is not publicly\navailable (as in DreamFusion [1]), other available models\nmay be used to produce similar results. To aid reproducibil-\nity, we include a method schematic in Fig. 4 and pseu-\ndocode in Alg. 1. Our evaluation setup is in Sec. 4.1 along\nwith hyperparameters and other details in App. Sec. B.\nAcknowledgements\nWe thank Weiwei Sun, Matan Atzmon, and Or Perel for\nhelpful feedback. The Python community [68, 69] made\nunderlying tools, including PyTorch [70] & Matplotlib [71].\nDisclosure of Funding\nNVIDIA funded this work. Jonathan Lorraine, Kevin Xie,\nXiaohui Zeng, and Towaki Takikawa had funding from stu-\ndent scholarships at the University of Toronto and the Vec-\ntor Institute, which are not in direct support of this work.\nReferences\n[1] Ben Poole,\nAjay Jain,\nJonathan T Barron,\nand Ben\nMildenhall.\nDreamfusion: Text-to-3d using 2d diffusion.\narXiv:2209.14988, 2022. 1, 2, 3, 4, 6, 7, 8, 12, 13, 14, 16\n[2] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\ntext-to-3d content creation. arXiv:2211.10440, 2022. 1, 7,\n12, 14, 19, 20\n[3] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,\nJiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,\nSamuli Laine, Bryan Catanzaro, et al.\nediffi:\nText-to-\nimage diffusion models with an ensemble of expert denois-\ners. arXiv:2211.01324, 2022. 1, 3, 7\n[4] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 2022. 7\n[5] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour,\nBurcu Karagol Ayan,\nS Sara Mahdavi,\nRapha Gontijo Lopes,\net al.\nPhotorealistic text-to-\nimage diffusion models with deep language understanding.\narXiv:2205.11487, 2022. 1, 3\n[6] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. Communications of the ACM, 2021. 3, 7, 12\n[7] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a mul-\ntiresolution hash encoding. arXiv:2201.05989, 2022. 3, 8,\n12\n[8] Towaki Takikawa, Alex Evans, Jonathan Tremblay, Thomas\nM\u00a8uller, Morgan McGuire, Alec Jacobson, and Sanja Fidler.\nVariable bitrate neural fields. In ACM SIGGRAPH 2022 Con-\nference Proceedings, 2022.\n[9] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,\nBoxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J\nGuibas, Jonathan Tremblay, Sameh Khamis, et al.\nEffi-\ncient geometry-aware 3d generative adversarial networks.\nIn IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2022. 3, 7\n[10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in Neural Information\nProcessing Systems, 2020. 3\n[11] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. arXiv:2011.13456, 2020. 3\n[12] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv:2207.12598, 2022. 3, 7\n[13] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J. Liu. Exploring the limits of transfer learning with a\nunified text-to-text transformer. JMLR, 2020. 3\n[14] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable visual\nmodels from natural language supervision. 2021. 3\n[15] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A\nYeh, and Greg Shakhnarovich.\nScore jacobian chaining:\nLifting pretrained 2d diffusion models for 3d generation.\narXiv:2212.00774, 2022. 3, 7\n[16] Brandon\nAmos.\nTutorial\non\namortized\noptimiza-\ntion for learning to optimize over continuous domains.\narXiv:2202.00665, 2022. 3, 7\n[17] Daniel Rebain, Mark J Matthews, Kwang Moo Yi, Gopal\nSharma, Dmitry Lagun, and Andrea Tagliasacchi.\nAt-\ntention beats concatenation for conditioning neural fields.\narXiv:2209.10684, 2022. 3, 7, 12\n[18] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial networks. Commu-\nnications of the ACM, 2020. 4, 5\n[19] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and\nYuichi Yoshida. Spectral normalization for generative ad-\nversarial networks. arXiv:1802.05957, 2018. 4\n[20] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large\nscale gan training for high fidelity natural image synthesis.\narXiv:1809.11096, 2018. 4\n[21] David Pfau and Oriol Vinyals. Connecting generative adver-\nsarial networks and actor-critic methods. arXiv:1610.01945,\n2016. 4\n[22] Gauthier Gidel, Reyhane Askari Hemmat, Mohammad\nPezeshki, R\u00b4emi Le Priol, Gabriel Huang, Simon Lacoste-\nJulien, and Ioannis Mitliagkas. Negative momentum for im-\nproved game dynamics. In The 22nd International Confer-\nence on Artificial Intelligence and Statistics, 2019. 4\n[23] Jonathan P Lorraine, David Acuna, Paul Vicol, and David\nDuvenaud. Complex momentum for optimization in games.\nIn International Conference on Artificial Intelligence and\nStatistics, pages 7742\u20137765. PMLR, 2022. 4\n[24] Rohan Anil, Vineet Gupta, Tomer Koren, Kevin Regan, and\nYoram Singer. Scalable second order optimization for deep\nlearning. arXiv:2002.09018, 2020. 4\n[25] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. arXiv:1412.6980, 2014. 4\n[26] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. arXiv:1312.6114, 2013. 5, 7, 12\n[27] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter\nAbbeel, and Ben Poole. Zero-shot text-guided object gen-\neration with dream fields. In CVF Conference on Computer\nVision and Pattern Recognition Proceedings, 2022. 6, 7\n[28] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv:2210.02303, 2022. 7\n[29] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv:2204.06125, 2022.\n[30] Alex Shonenkov, Misha Konstantinov, Daria Bakshandaeva,\nChristoph Schuhmann, Ksenia Ivanova, and Nadiia Klokova.\nIf by deepfloyd lab at stabilityai, 2023.\ngithub.com/\ndeep-floyd/IF. 7\n[31] Daniel Rebain, Mark Matthews, Kwang Moo Yi, Dmitry La-\ngun, and Andrea Tagliasacchi. Lolnerf: Learn from one look.\nIn IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2022. 7\n[32] Eric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu,\nand Gordon Wetzstein.\npi-gan: Periodic implicit gener-\native adversarial networks for 3d-aware image synthesis.\nIn IEEE/CVF conference on computer vision and pattern\nrecognition, 2021.\n[33] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and\nAndrea Vedaldi. Realfusion: 360deg reconstruction of any\nobject from a single image. In IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2023.\n[34] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran\nYi, Lizhuang Ma, and Dong Chen.\nMake-it-3d: High-\nfidelity 3d creation from a single image with diffusion prior.\narXiv:2303.14184, 2023. 7\n[35] Dario Pavllo, Jonas Kohler, Thomas Hofmann, and Aurelien\nLucchi. Learning generative models of textured 3d meshes\nfrom real-world images. In IEEE/CVF International Confer-\nence on Computer Vision, 2021. 7\n[36] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen,\nKangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja\nFidler. Get3d: A generative model of high quality 3d tex-\ntured shapes learned from images. arXiv:2209.11163, 2022.\n[37] Dario Pavllo, Graham Spinks, Thomas Hofmann, Marie-\nFrancine Moens, and Aurelien Lucchi. Convolutional gener-\nation of textured 3d meshes. Advances in Neural Information\nProcessing Systems, 2020.\n[38] Wenzheng Chen, Huan Ling, Jun Gao, Edward Smith,\nJaakko Lehtinen, Alec Jacobson, and Sanja Fidler. Learn-\ning to predict 3d objects with an interpolation-based differ-\nentiable renderer. Advances in Neural Information Process-\ning Systems, 32, 2019. 7\n[39] Aditya Sanghi, Hang Chu, Joseph Lambourne, Ye Wang,\nChin-Yi Cheng, and Marco Fumero. Clip-forge: Towards\nzero-shot text-to-shape generation. arXiv:2110.02624, 2021.\n7\n[40] Nasir Khalid, Tianhao Xie, Eugene Belilovsky, and Tiberiu\nPopa.\nClip-mesh: Generating textured meshes from text\nusing pretrained image-text models. ACM Transactions on\nGraphics (TOG), Proc. SIGGRAPH Asia, 2022. 7\n[41] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and\nDaniel Cohen-Or. Latent-nerf for shape-guided generation of\n3d shapes and textures. In IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, 2023. 7\n[42] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and\nSanja Fidler. Deep marching tetrahedra: a hybrid represen-\ntation for high-resolution 3d shape synthesis. Advances in\nNeural Information Processing Systems, 2021. 7\n[43] Miguel Angel Bautista, Pengsheng Guo, Samira Abnar, Wal-\nter Talbott, Alexander Toshev, Zhuoyuan Chen, Laurent\nDinh, Shuangfei Zhai, Hanlin Goh, Daniel Ulbricht, et al.\nGaudi: A neural architect for immersive 3d scene genera-\ntion. arXiv:2207.13751, 2022. 7\n[44] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic,\nOr Litany, Sanja Fidler, and Karsten Kreis.\nLion: Latent\npoint diffusion models for 3d shape generation. In Advances\nin Neural Information Processing Systems (NeurIPS), 2022.\n[45] Linqi Zhou, Yilun Du, and Jiajun Wu. 3d shape generation\nand completion through point-voxel diffusion. In IEEE/CVF\nInternational Conference on Computer Vision, 2021. 7\n[46] Jiaxiang Tang.\nStable-dreamfusion:\nText-to-3d with\nstable-diffusion,\n2022.\ngithub.com/ashawkey/\nstable-dreamfusion. 7\n[47] Yuan-Chen Guo, Ying-Tian Liu, Chen Wang, Zi-Xin Zou,\nGuan Luo, Chia-Hao Chen, Yan-Pei Cao, and Song-Hai\nZhang.\nthreestudio: A unified framework for 3d content\ngeneration.\ngithub.com/threestudio-project/\nthreestudio, 2023. 7\n[48] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot one image to 3d object. arXiv:2303.11328, 2023.\n7\n[49] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fan-\ntasia3d: Disentangling geometry and appearance for high-\nquality text-to-3d content creation. arXiv:2303.13873, 2023.\n7\n[50] Jiale Xu, Xintao Wang, Weihao Cheng, Yan-Pei Cao, Ying\nShan, Xiaohu Qie, and Shenghua Gao. Dream3d: Zero-shot\ntext-to-3d synthesis using 3d shape prior and text-to-image\ndiffusion models. In IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2023. 7\n[51] Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, and\nKwan-Yee K Wong.\nDreamavatar:\nText-and-shape\nguided 3d human avatar generation via diffusion models.\narXiv:2304.00916, 2023. 7\n[52] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and\ndiverse text-to-3d generation with variational score distilla-\ntion. arXiv:2305.16213, 2023. 7\n[53] Tianlong Chen, Xiaohan Chen, Wuyang Chen, Howard\nHeaton, Jialin Liu, Zhangyang Wang, and Wotao Yin.\nLearning to optimize:\nA primer and a benchmark.\narXiv:2103.12828, 2021. 7\n[54] Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and\nAmos Storkey. Meta-learning in neural networks: A survey.\nIEEE transactions on pattern analysis and machine intelli-\ngence, 2021. 7\n[55] Jonathan\nLorraine\nand\nDavid\nDuvenaud.\nStochas-\ntic hyperparameter optimization through hypernetworks.\narXiv:1802.09419, 2018. 7\n[56] Matthew Mackay, Paul Vicol, Jonathan Lorraine, David Du-\nvenaud, and Roger Grosse. Self-tuning networks: Bilevel op-\ntimization of hyperparameters using structured best-response\nfunctions. In International Conference on Learning Repre-\nsentations, 2018. 7\n[57] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wier-\nstra. Stochastic backpropagation and approximate inference\nin deep generative models. In ICML, 2014. 7\n[58] Chris Cremer, Xuechen Li, and David Duvenaud. Inference\nsuboptimality in variational autoencoders. In International\nConference on Machine Learning, 2018.\n[59] Mike Wu, Kristy Choi, Noah Goodman, and Stefano Ermon.\nMeta-amortized variational inference and learning. In AAAI\nConference on Artificial Intelligence, 2020. 7\n[60] David Ha, Andrew Dai, and Quoc V Le. Hypernetworks.\narXiv:1609.09106, 2016. 7\n[61] Chris\nZhang,\nMengye\nRen,\nand\nRaquel\nUrtasun.\nGraph\nhypernetworks\nfor\nneural\narchitecture\nsearch.\narXiv:1810.05749, 2018. 7\n[62] Boris Knyazev, Michal Drozdzal, Graham W Taylor, and\nAdriana Romero Soriano. Parameter prediction for unseen\ndeep architectures. Advances in Neural Information Process-\ning Systems, 2021. 7\n[63] Vincent Sitzmann,\nEric Chan,\nRichard Tucker,\nNoah\nSnavely, and Gordon Wetzstein.\nMetasdf: Meta-learning\nsigned distance functions. Advances in Neural Information\nProcessing Systems, 2020. 7\n[64] Emilien\nDupont,\nHyunjik\nKim,\nSM\nAli\nEslami,\nDanilo Jimenez Rezende, and Dan Rosenbaum.\nFrom\ndata to functa: Your data point is a function and you can\ntreat it like one. In ICML, 2022. 7\n[65] Uriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual,\nIurii Makarov, Filippos Kokkinos, Naman Goyal, Andrea\nVedaldi, Devi Parikh, Justin Johnson, et al. Text-to-4d dy-\nnamic scene generation. arXiv:2301.11280, 2023. 7\n[66] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv:2209.14792, 2022. 7\n[67] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, 2023. 7\n[68] Guido Van Rossum and Fred L Drake Jr. Python reference\nmanual. Centrum voor Wiskunde en Informatica Amster-\ndam, 1995. 9\n[69] Travis E Oliphant. Python for scientific computing. Com-\nputing in Science & Engineering, 2007. 9\n[70] Adam Paszke, Sam Gross, Soumith Chintala, Gregory\nChanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-\nban Desmaison, Luca Antiga, and Adam Lerer. Automatic\ndifferentiation in PyTorch. Openreview, 2017. 9, 15\n[71] John D Hunter.\nMatplotlib: A 2D graphics environment.\nComputing in Science & Engineering, 2007. 9\n[72] Dan Hendrycks and Kevin Gimpel.\nGaussian error linear\nunits (gelus). arXiv:1606.08415, 2016. 12\n[73] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 2017. 13\n[74] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler,\nJonathan T Barron, and Pratul P Srinivasan. Ref-nerf: struc-\ntured view-dependent appearance for neural radiance fields.\nIn IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2022. 13\nTable 1: Glossary and notation\n(A)TT3D\n(Amortized) Text-to-3D\nNeRF\nNeural Radiance Field [6]\nDDM\nDenoising Diffusion Model\nMLP\nMulti-layer Perceptron\nDF27, DF411\nDreamFusion\u2019s [1] 27 main text prompts & the extended 411 prompts\nn, m \u2208 N\nThe size of different objects\nx, y, z, \u00b7 \u00b7 \u00b7 \u2208 R\nScalar coordinates\nx, y, z, \u00b7 \u00b7 \u00b7 \u2208 Rn\nVectors\nX, Y, Z, . . .\nThe domain of x, y, z, . . .\nx = [x, y, z] \u2208 X\nA point\nr = [\u03c3, r, g, b] \u2208 R\nThe density and color values\nw \u2208 W\nThe parameters of the point encoder function\n\u03b3w : X \u2192 \u0393\nThe point encoder function\n\u03bd : \u0393 \u2192 R\nThe final MLP mapping point encodings to radiance\nz\nThe problem context for amortization\nc \u2208 C\nA text embedding used to condition the DDM and as problem context\nm : C \u2192 W\nThe mapping network from problem context to modulations\nv \u2208 Rn\nThe intermediary vector-embedding of c in m\nN, U, Dir, Bern\nNormal, uniform, Dirichlet, and Bernoulli distributions respectively\n\u03f5, \u02c6\u03f5\nNoise added to rendered frames, or as predicted by the DDM\n\u03ba \u2208 R+\nThe concentration parameter of the Dirichlet distribution\n\u03b1 \u2208 [0, 1]\nAn interpolation coefficient, sampled from Dir(\u03ba) in training\nL\nA loss function\n\u03c9\nA guidance weight\n\u03b21, \u03b22\nParameters of the Adam optimizer [26]\nA. Glossary\nB. Experimental Setup\nB.1. Implementation Details\nWe replicate DreamFusion [1] and Magic3D\u2019s [2] setup\nwhere possible and list key details here. We recommend\nreading these papers for additional context.\nB.1.1\nPoint-encoder \u03b3\nWe followed Instant NGP [7] to parameterize our NeRF,\nconsisting of dense, multi-resolution voxel grids and dic-\ntionaries.\nWe only use dense voxel layers unless spec-\nified, which trained faster with negligible quality drop.\nFor our multi-resolution voxel grid, we use resolutions of\n[9, 14, 22, 36, 58], with 4 features per level. When active,\nwe use a further three levels of hash grid parameters. Each\nlevel\u2019s features are linearly interpolated according to spatial\nlocation and concatenated, leading to a final output feature\nsize of 20 with dense voxel grids and 32 with the full INGP.\nB.1.2\nFinal NeRF MLP \u03bd\nWe select a minimal final MLP to maintain evaluation\nspeed, with a single hidden layer with 32 units and a SiLU\nactivation [72]. The majority of our model\u2019s capacity comes\nfrom the point-encoder. We use a softplus activation for the\ndensity output and sigmoid activations on the color.\nB.1.3\nMapping Network m\nThe mapping network computes a fixed-size vector repre-\nsentation v of the task from the text embedding. We only\nuse the CLIP embedding for feature grid modulation be-\ncause it was sufficient and including the T5 embedding in-\ncreases network size. We apply spectral normalization to all\nlinear layers. We considered concatenation, hypernetwork,\nand attention approaches for modulation [17]:\nConcatenation: The simple strategy of nai\u00a8vely concate-\nnating (a vector-representation f of) the text to the point-\nencoding \u2013 i.e., \u03bd(\u03b3w(x), f(c)) was prohibitively expen-\nsive. This is because we require the cost of the final per-\npoint NeRF MLP \u03bd to be minimal for cheap rendering. The\nconcatenation approach of \u03bd(\u03b3w(x), v) introduces over-\nhead by increasing per-iteration training time by 37% but\ndoesn\u2019t significantly impact quality. In inference, the hy-\npernet method is superior, reducing cost by \u223c 20 \u2212 75%, as\nwe only generate the grid parameters w once when render-\ning multiple views of 1 object, bypassing the use of a single,\nlarger NeRF \u03bd with concatenation.\nHypernetwork:\nWe first flatten the token and pass it\nthrough an MLP to produce a vector-embedding, which is\nused by a linear layer to output the point-encoder\u2019s voxel\ngrid parameters. As the CLIP embedding was already a\nstrong representation, we found that a simple linear layer\nfor the text-embedding to vector-embedding was sufficient.\nWe converged with deeper hypernetworks \u2013 by using\nspectral normalization on all linear layers \u2013 but this offered\nno quality benefit while taking longer to train.\nWe also\nfound removing the bias on the final linear layer decreased\nnoise by forcing the result to depend on the prompt.\nWe vary the vector embedding v\u2019s size in our experi-\nments, which largely dictates our amortized model\u2019s capac-\nity. The mapping network m dominates the model\u2019s mem-\nory cost, while the text\u2019s vector-embedding largely dictates\nthe mapping network size. Our memory cost scales linearly\nwith the vector-embedding size. We use a vector embed-\nding v size of 32 for all experiments except interpolation,\nwhere we use 2. We have experiments where the number\nof text prompts is both smaller (DF27) and larger (DF411,\ncompositional prompts) than the vector-embedding.\nAttention: We also investigated using an attention-based\nmapping network with a series of self-attention layers to\nprocess text embeddings before feeding into the hypernet-\nworks for each multi-resolution grid level. Our attention\nperformed with comparable quality but trained more slowly.\nHowever, we expect modifications to be necessary on more\ncomplex prompt sets.\nB.1.4\nEnvironment Mapping Network\nIn our experiments, we use a background, a function map-\nping ray directions \u2013 and text embeddings \u2013 to colors, which\nwe denote as the environment map. Specifically, we en-\ncode the ray directions, concatenate them with the vector-\nembedding v from the mapping network, and feed them into\na final MLP. We use a sigmoid activation on the output color\nand spectral norm on all linear layers. We encode the ray\ndirections with a sinusoidal positional encoding [73] (fre-\nquencies 20, 21, . . . , 2L\u22121, L = 8), and no hidden layers \u2014\ni.e., a linear layer \u2014 for our final MLP.\nB.1.5\nSpectral Normalization\nWe found spectral normalization \u2013 which can be imple-\nmented trivially in PyTorch on linear layers \u2013 to be critical\nfor mapping net training, but non-essential on other parts.\nIn the mapping network, we must use spectral normaliza-\ntion on all linear layers for the hypernetwork and attention\napproaches or we suffer from numerical instability. Using\nspectral normalization on the linear layers in the environ-\nment map, or final NeRF module was unnecessary.\nB.1.6\nSampling Text Prompts\nWe cache the CLIP (and T5) embeddings for all experi-\nments to avoid repeated computation and the memory over-\nhead of the large text encoders.\nWe use multiple text\nprompts in each batched update.\nInterpolations: We sample interpolated embeddings dur-\ning training in interpolation experiments (Section 4.4). See\nSection B.1.14 or Figure 18 for more interpolation setup\ndetails.\nWhen interpolating between prompts with text-\nembeddings c1 and c2, we sample a weight \u03b1 \u2208 [0, 1] and\ninput c\u2032 =(1 \u2212 \u03b1)c1 + \u03b1c2 to the mapping network.\nB.1.7\nSampling Rendering Conditions\nAs in DreamFusion [1], we randomly sample rendering\nconditions, including the camera position and lighting con-\nditions. We use a bounding sphere of radius 2 in all exper-\niments. We sample the point light location with distance\nfrom U(1, 3) and angle relative to the random camera posi-\ntion of U(0, \u03c0/4). We sample \u201csoft\u201d textureless and albedo-\nonly augmentations to allow varying shades during training.\nAlso, we sample the camera distance from U(2, 3) and the\nfocal length from U(0.7, 1.35).\nB.1.8\nScore Distillation Sampling\nFor the DDM\u2019s sampling, we sample the time-step from\nU(0.002, 1.0) and use a guidance weight of 100.\nB.1.9\nThe Objective\nThe regularizers: The orientation loss [74] (as in Dream-\nFusion [1]) encourages normal vectors of the density field\nto face the camera when visible, preventing the model from\nchanging colors to be darker during textureless renders by\nmaking geometry face \u201cbackward\u201d when shaded.\nAlso,\nDreamFusion regularizes accumulated alpha value along\neach ray, encouraging not unnecessarily filling space and\naiding in foreground/background separation. We do not use\nthese regularizers for all experiments, as we did not ob-\nserve failure modes they fixed, and they made no signifi-\ncant change in results over the interval [10\u22123, 10\u22121]. Larger\nopacity regularization values resulted in empty scenes,\nwhile larger orientation values did not change the initial-\nization from a sphere.\nThe image fidelity: We train with 32 points sampled uni-\nformly along each ray for all experiments except interpo-\nlations. For interpolations only, we sample 128 points and\nreduced batch size to improve quality. Our underlying text-\nto-image model generates 64 \u00d7 64 images, leading to 4096\nrays per rendered frame. At inference time we render with\nhigher points per ray to improve quality for negligible cost.\nThe initialization: As in DreamFusion, we add an initial\nspatial density bias to prevent collapsing to an empty scene,\nshown in Figure 10, left. Our density bias on the NeRF\nMLP output before the softplus activation takes the form:\ndensityBias(x) = 10 (1 \u2212 2\u2225x\u22252)\n(7)\nB.1.10\nThe Optimization\nWe use Adam with a learning rate of 1 \u00d7 10\u22121 and \u03b22 =\n0.999. A wide range momentum \u03b21 (up to .95) can yield\nsimilar qualities if the step size is jointly tuned, while the\nquickest convergence occurs at 0. We do not use the linear\nlearning rate warmup or cosine decay from DreamFusion.\nB.1.11\nMemorization Experiments\nOur experiments use the same architecture for per-prompt\nand amortized training settings to ensure a fair compari-\nson. We train models using a batch size of 32 times the\nnumber of GPUs used. Amortized training uses 8 GPUs\nwhile per-prompt uses a single GPU (due to resource con-\nstraints), with more details in Section B.2. The complete\nset of DreamFusion prompts is located here: https://\ndreamfusion3d.github.io/gallery.html\nB.1.12\nGeneralization Experiments\nOur prompt selections are motivated by the compositional\nexperiment in DreamFusion\u2019s Figure 4 [1].\nOur experi-\nments with pig prompts with the template \u201ca pig {activity}\n{theme}, where the activities and themes are any\ncombination of the following:\nThe activities: [ \u201criding a bicycle\u201d, \u201csitting on a\nchair\u201d, \u201cplaying the guitar\u201d, \u201cholding a shovel\u201d, \u201cholding a\nblue balloon\u201d, \u201cholding a book\u201d, \u201cwielding a katana\u201d, \u201crid-\ning a bike\u201d]\nThe themes: [ \u201cmade out of gold\u201d, \u201ccarved out of wood\u201d,\n\u201cwearing a leather jacket\u201d, \u201cwearing a tophat\u201d, \u201cwearing\na party hat\u201d, \u201cwearing a sombrero\u201d, \u201cwearing medieval ar-\nmor\u201d]\nOur pig holdout, unseen, testing prompts are pairing the\nith activity and theme.\nOur experiments with animal prompts with the tem-\nplate \u201c{animal} {activity} {theme} {hat}, where the\nactivities, themes and hats are any combination\nof the following:\nThe animals: [ \u201ca squirrel\u201d, \u201ca raccoon\u201d, \u201ca pig\u201d, \u201ca\nmonkey\u201d, \u201ca robot\u201d, \u201ca lion\u201d, \u201ca rabbit\u201d, \u201ca tiger\u201d, \u201can\norangutan\u201d, \u201ca bear\u201d]\nThe activities: [ \u201criding a motorcycle\u201d, \u201csitting on a\nchair\u201d, \u201cplaying the guitar\u201d, \u201cholding a shovel\u201d, \u201cholding a\nblue balloon\u201d, \u201cholding a book\u201d, \u201cwielding a katana\u201d]\nThe themes: [ \u201cwearing a leather jacket\u201d, \u201cwearing a\nsweater\u201d, \u201cwearing a cape\u201d, \u201cwearing medieval armor\u201d,\n\u201cwearing a backpack\u201d, \u201cwearing a suit\u201d]\nThe hats: [ \u201cwearing a party hat\u201d, \u201cwearing a sombrero\u201d,\n\u201cwearing a helmet\u201d, \u201cwearing a tophat\u201d, \u201cwearing a back-\npack\u201d, \u201cwearing a baseball cap\u201d]\nOur holdout, unseen, animal testing prompts are selected\nhomogeneously for each training set size.\nB.1.13\nFinetuning Experiments\nWe resume training from an amortized training checkpoint\nwhile re-initializing the optimizer state. For the finetuning\nexperiments, in our mapping network, we only finetune an\noffset to the output and detach all prior weights that only\nembed text tokens (because we finetune with one prompt).\nOther training details are kept equal to per-prompt training.\nB.1.14\nInterpolation Experiments\nIn interpolations we use 128 ray samples and batch size 16.\nInterpolant concentration: We sample the interpolation\ncoefficient \u03b1 \u223c Dir(\u03ba) from a Dirichlet distribution with\nconcentration parameter \u03ba. The Dirichlet distribution al-\nlows us to smoothly interpolate from sampling the original\ntext tokens (with concentration \u03ba \u2248 0, to uniformly sam-\npling \u03b1 (with concentration \u03ba \u2248 1) to focusing on diffi-\ncult midpoints (with concentration \u03ba > 1) \u2013 see Figure 19.\nSpecifically, in Figures 3 & 20 we use \u03ba = 2.0 for 5000\nsteps to stabilize the midpoint, followed by \u03ba = 0.5 to fo-\ncus on the original prompts.\nInterpolation types: We provide multiple examples of in-\nterpolation types to amortize over that provide qualitatively\ndifferent results \u2013 see Figure 18.\nA simple strategy is to interpolate over the text embed-\nding used to condition the text-to-image model:\nc\u2032 = (1 \u2212 \u03b1)c1 + \u03b1c2\n(8)\nAnother strategy is to interpolate the loss function used be-\ntween the two prompts. We could evaluate the loss at both\nprompts and weight the loss:\nLfinal = (1 \u2212 \u03b1)Lprompt 1 + \u03b1Lprompt 2\n(9)\nInstead, to interpolate in the loss, we sample the loss for\neach prompt with probability \u03b1, which we equate to training\nwith embedding:\nc\u2032 = (1 \u2212 Z)c1 + Zc2 where Z \u223c Bern(\u03b1)\n(10)\nA third strategy, suggested for images in Magic3D [2], in-\nterpolates the DDM\u2019s guidance weight. Unlike Magic3D,\nwe amortize over guidance weights, reducing cost while\nproviding continuous interpolation (not allowed via re-\ntraining on each weight). Specifically, we guide with:\n\u02c6\u03f5 = \u03f5uncond. + (1 \u2212 \u03b1)\u03c91\u03f5prompt 1 + \u03b1\u03c92\u03f5prompt 2\n(11)\nHere, the \u03c91 and \u03c92 are notations for the guidance weights\nfor the predicted noise on the 1st and 2nd prompts respec-\ntively, which are fixed and equal in all experiments. This\ninterpolates between using guidance on the first prompt, to\nguidance on the second prompt.\n0\n10\n30\n100\n300\n1000\n10 000\nOptimization Iteration\nObject Evolution During Amortized Training\nFigure 10: We show assorted training trajectories of the rendered objects during compositional training from Figures 6 and\n2. Left: We visualize the initialization strategy described in Section B.1.9.\nB.2. Compute Requirements\nWe implement our experiments in PyTorch [70].\nB.2.1\nPer-prompt Optimization\nWe do all per-prompt training runs on an NVIDIA A40\nGPU, with a batch size of 32 for up to 8000 steps or \u223c 4\nhours. DF27 (Figure 6, left) use 27 runs, while the compo-\nsitional prompts (Figure 8) use 50 or 300 subsampled runs\nrespectively. Each training step costs \u223c1 second.\nB.2.2\nAmortized Training\nMemorization & generalization: When amortizing many\nprompts, we use multiple GPUs to train with a larger batch\nsize, causing amortized and per-prompt training to have dif-\nferent update costs. So we report the total rendered frames\nto compare compute accurately. Updates are roughly 1 sec-\nond in each setup.\nWe perform the DF27 (Figures 6, 11) and DF411 (Fig-\nures 9, 14) runs on 8 NVIDIA A40 GPUs, each with a batch\nsize of 32. We train DF27 for 13 000 steps (\u223c 4 hours) and\nDF411 for 100 000 steps (about a day).\nThe compositional runs (Figures 2, 6, 8) were performed\non 4 NVIDIA A100 GPUs, with a batch size of 32 per GPU,\nfor 40 000 steps or about 10 hours.\nInterpolations (Figure 3): We use a single NVIDIA A40\nGPU as in per-prompt training.\nFinetuning (Figure 17): We use a single NVIDIA A40\nGPU as in per-prompt training.\nB.2.3\nInference\nAt inference \u2013 delineated from training in Figure 4 \u2013 we\ngenerate grid parameters in < 1 second and render frames in\nreal-time due to our small final NeRF \u03bd and efficient point-\nencoding \u03b3. We use more ray samples at inference than\ntraining due to negligible cost and enhanced fidelity. Mod-\nulation generation occurs once and is reused for each view\n& location query, creating negligible overhead with many\nviews or high-resolution images. During training with 1\nview and image size 64 (batch size 8), hypernet modula-\ntions introduced an overhead of 24% more time per itera-\ntion, which could be avoided if our weights do not need to\nbe generated. With 1 view and image size 256 (batch size 1)\nin inference, the modulation introduced an 11% overhead in\nrendering time, dropping to < 1% with 30 views.\nC. Results\nC.1. Additional Experiments & Visualizations\nFigure 11: Our method, ATT3D, uses a single model to produce 3D scenes with varying geometric and texture details from\nthe set of 27 prompts in the main DreamFusion paper [1]. The quality is comparable to existing single prompt training and\nrequires far fewer training resources (Fig. 6).\nAverage R-probability/precision\nTotal rendered frames used in training\nDF27 Results: Our Method (ATT3D) vs. Per-prompt Optimization\nFigure 12: We show the same plot as Figure 6 with the ad-\ndition of R-precision. Takeaway: Results with R-precision\nare similar to \u2014 but noisier than \u2013 R-probability when we\nhave few prompts.\nAmortized\n+ Finetuning\nPer-prompt\nTraining Style\nFigure 13: We qualitatively compare the unseen \u201ctesting\u201d\nresults from the various training strategies in Figures 6 and\n2, with our method in blue and baselines in red. Notably,\namortized requires no test time optimization, while fine-\ntuning uses a small amount, and per-prompt uses a large\namount to tune from scratch.\nFigure 14: We show full results from our method on the DF411 prompt set, which we truncate for Figure 9. There are various\nexamples of the model re-using object components across prompts \u2013 see Figure 15.\n\u201ca lemur drinking boba\u201d\n\u201ca lemur taking notes\u201d\nComponent Re-use in DF411\n\u201corangutan holding a paintbrush\u201d\n\u201cchimpanzee holding a cup\u201d\nFigure 15: We show examples of prompts in which our model (from the DF411 run in Figures 9 and 14) re-uses components,\nshowing a means by which amortization saves compute. Top: The lemur is re-used with different activities. Bottom: The\norangutan is re-colored to a chimpanzee and given a different activity.\n\u201c...dress of fruit...\u201d\n\u201c...dress of bags...\u201d\nInterpolated embeddings not viewed during training\nI.e., interpolants have no training\n\u201c...cottage...\u201d\n\u201c...house...\u201d\n\u201ca chimpanzee\u201d\n\u201c...eating an icecream\u201d\nTraining\n\u201ca chimpanzee\u201d + \u201ceating an icecream\u201d\nNo Training\nFigure 16: We investigate generalization on the DF411 run (App. Fig. 14). Left: Generalization to interpolated embeddings,\nwhich produces suboptimal results that we improve by amortizing over interpolants as in Figure 3. Right: Generalization to\ncompositional embeddings. Takeaway: The generalization is promising, yet could be improved, motivating training on large\ncompositional sets in Figures 6 & 8, and training on interpolants as in Figures 3, 18, 19, & 20.\nFinetuning iteration\n0\n100\n200\n300\n400\n400\n\u201c... motorcycle of gold\u201d\n\u201c... chair carved of wood\u201d\nText prompt\nAmortized (first 5 columns)\nPer-prompt\nPer-prompt\nAmortized\nAmortized + Magic3D Fine-tuning\nVarious strategies on \u201ca pig wearing medieval armor holding a blue balloon\u201d\nFigure 17: We display the results of finetuning held-out, unseen testing prompts from Fig. 2. Top: For amortization, we\nfinetune from the final optimization value, while for per-prompt, we finetune the model from a random initialization. We\nachieve higher quality with fewer finetuning updates. Bottom: Per-prompt optimization fails to recover a blue balloon, and\ncan not be recovered with finetuning. In contrast, amortized optimization recovers the correct balloon and can be fine-tuned\nusing Magic3D\u2019s second optimization stage [2].\nTraining text-to-image samples use embedding (1 \u2212 \u03b1)c1 + \u03b1c2 where \u03b1 \u223c Dir(0) = Bern(1/2)\nNo Interpolations\nTraining text-to-image samples use embedding (1 \u2212 \u03b1)c1 + \u03b1c2, \u03b1 \u223c Dir(1) = U(0, 1)\nLatent Interpolations - relevant change in red\nTraining text-to-image samples use embedding (1 \u2212 Z)c1 + Zc2 where Z \u223c Bern(\u03b1), \u03b1 \u223c Dir(1) = U(0, 1)\nLoss Interpolations\nTraining text-to-image samples use guidance weights \u03c9: \u02c6\u03f5 = \u03f5uncond. + (1 \u2212 \u03b1)\u03c91\u03f5prompt 1 + \u03b1\u03c92\u03f5prompt 2, \u03b1 \u223c Dir(1) = U(0, 1)\nGuidance Interpolations\n\u201ca hamburger\u201d\n\u201ca pineapple\u201d\nRendered frames from interpolating \u03b1 from 0 \u2192 1, after training with various objectives\nAll setups use modulations from interpolated text-embeddings: m ((1 \u2212 \u03b1)c1 + \u03b1c2)\nFigure 18: We contrast amortizing over different types of interpolations as described in Section B.1.14. For all examples, we\ngive the mapping network m the interpolated embedding (1 \u2212 \u03b1)c1 + \u03b1c2. However, we vary the embedding used by the\ntext-to-image model. Takeaway: We can amortize over various training methods to produce qualitatively different results.\nTop: We use no interpolants during training, which can just dissolve between the endpoints. Latent Interpolation: We simply\ninterpolate between the latent embeddings used for image sampling. Loss Interpolation: We interpolate the loss function\nused in training between the prompts, producing objects simultaneously solving both losses. Guidance Interpolation: We\ninterpolate the guidance weight applied to the prompts, as explored in Magic3D (without amortization) [2].\n\u201ca wooden pirate ship\u201d\n\u201ca rubber life raft\u201d\nRendered Frames Interpolating \u03b1 from 0 \u2192 1, where training \u03b1 \u223c Dir(\u03ba) with varying \u03ba\n\u03ba during training\nsmall \u2192 large\n\u03ba = 1 =\u21d2 \u03b1 \u223c U(0, 1)\nlarge \u2192 small\nFigure 19: We display the results for differing strategies for changing the concentration parameter \u03ba for the distribution of\nthe interpolation weights \u03b1. Note that a concentration of \u03ba = 1 is simply a uniform distribution: Dir(1) = U(0, 1). For both\nresults, we train for 5000 steps with an initial concentration \u03ba, which we then change for the final 5000 steps. Takeaway:\nThe initial shapes learned strongly influence subsequent training, and a \u201clarge\u201d concentration \u03ba focuses on the midpoint,\nwhile a \u201csmall\u201d concentration focuses on the endpoints. If we want the original prompts in the interpolation, then we should\nstart with \u03ba small, while if we desire a steering-wheel-life-raft satisfying both losses, we should start with \u03ba large.\n\u201c... an adorable cottage with a thatched roof\u201d\n\u201c... a house in Tudor Style\u201d\n\u201ca frog wearing a sweater\u201d\n\u201ca bear dressed as a lumberjack\u201d\n\u201c... a majestic sailboat\u201d\n\u201ca spanish galleon...\u201d\n\u201ca ficus planted in a pot\u201d\n\u201ca small cherry tomato plant...\u201d\n\u201ca baby dragon\u201d\n\u201ca green dragon\u201d\n\u201cjagged rock\u201d\n\u201cmossy rock\u201d\nFigure 20: We include additional results for using our method to amortize over (loss) interpolants between prompts. We\nalternate between a fixed and varied camera view. We show examples of varied buildings, characters, vehicles, plants,\nlandscapes, or a simple animation of \u201ca baby dragon\u201d aging into an adult.\n"
  },
  {
    "title": "SayTap: Language to Quadrupedal Locomotion",
    "link": "https://arxiv.org/pdf/2306.07580.pdf",
    "upvote": "7",
    "text": "SayTap: Language to Quadrupedal Locomotion\nYujin Tang\nyujintang@google.com\nGoogle DeepMind\nWenhao Yu\nmagicmelon@google.com\nGoogle DeepMind\nJie Tan\njietan@google.com\nGoogle DeepMind\nHeiga Zen\nheigazen@google.com\nGoogle DeepMind\nAleksandra Faust\nsandrafaust@google.com\nGoogle DeepMind\nTatsuya Harada\nharada@mi.t.u-tokyo.ac.jp\nThe University of Tokyo\nAbstract: Large language models (LLMs) have demonstrated the potential to\nperform high-level planning. Yet, it remains a challenge for LLMs to comprehend\nlow-level commands, such as joint angle targets or motor torques. This paper\nproposes an approach to use foot contact patterns as an interface that bridges\nhuman commands in natural language and a locomotion controller that outputs\nthese low-level commands. This results in an interactive system for quadrupedal\nrobots that allows the users to craft diverse locomotion behaviors flexibly. We\ncontribute an LLM prompt design, a reward function, and a method to expose the\ncontroller to the feasible distribution of contact patterns. The results are a controller\ncapable of achieving diverse locomotion patterns that can be transferred to real\nrobot hardware.\nCompared with other design choices, the proposed approach\nenjoys more than 50% success rate in predicting the correct contact patterns and\ncan solve 10 more tasks out of a total of 30 tasks. (https://saytap.github.io)\nKeywords: Large language model (LLM), Quadrupedal robots, Locomotion\n1\nIntroduction\nSimple and effective interaction between human and quadrupedal robots paves the way towards\ncreating intelligent and capable helper robots, forging a future where technology enhances our\nlives in ways beyond our imagination [1, 2, 3]. Key to such human-robot interaction system is\nenabling quadrupedal robots to respond to natural language instructions as language is one of the\nmost important communication channels for human beings. Recent developments in Large Language\nModels (LLMs) have engendered a spectrum of applications that were once considered unachievable,\nincluding virtual assistance [4], code generation [5], translation [6], and logical reasoning [7], fueled\nby the proficiency of LLMs to ingest an enormous amount of historical data, to adapt in-context to\nnovel tasks with few examples, and to understand and interact with user intentions through a natural\nlanguage interface.\nThe burgeoning success of LLMs has also kindled interest within the robotics researcher community,\nwith an aim to develop interactive and capable systems for physical robots [8, 9, 10, 11, 12, 13].\nResearchers have demonstrated the potential of using LLMs to perform high-level planning [8, 9], and\nrobot code writing [11, 13]. Nevertheless, unlike text generation where LLMs directly interpret the\natomic elements\u2014tokens\u2014it often proves challenging for LLMs to comprehend low-level robotic\ncommands such as joint angle targets or motor torques, especially for inherently unstable legged\nrobots necessitating high-frequency control signals. Consequently, most existing work presume the\nprovision of high-level APIs for LLMs to dictate robot behaviour, inherently limiting the system\u2019s\nexpressive capabilities.\nWe address this limitation by using foot contact patterns as an interface that bridges human instruc-\ntions in natural language and low-level commands. The result is an interactive system for legged\nrobots, particularly quadrupedal robots, that allows users to craft diverse locomotion behaviours\narXiv:2306.07580v3  [cs.RO]  14 Sep 2023\nTrotting\n\u201cTrot forward slowly\u201d \u2192 \u201cGood news, we are going to a picnic this weekend!\u201d\nBounding\nFigure 1: Illustration of the results on a physical quadrupedal robot. We show two test commands\nat the top, and the snapshots of the robot in the top row of the figure. The row in the middle shows\nthe desired contact patterns translated from the commands by an LLM (the pattern in between the\ncommands requests the robot to put all feet on the ground and stand still), and the bottom row\ngives the realized patterns. The proposed approach allows the robot to take both simple and direct\ninstructions (e.g., \u201cTrot forward slowly\u201d) as well as vague human commands (e.g., \u201cGood news, we\nare going to a picnic this weekend!\u201d) in natural language and react accordingly.\nflexibly. Central to the proposed approach is the observation that patterns of feet establishing and\nbreaking contacts with the ground often govern the final locomotion behavior for legged robots due\nto the heavy reliance of quadruped locomotion on environmental contact. Thus, a contact pattern,\ndescribing the contact establishing and breaking timings for each legs, is a compact and flexible inter-\nface to author locomotion behaviors for legged robots. To leverage this new interface for controlling\nquadruped robots, we first develop an LLM-based approach to generate contact patterns, represented\nby \u20180\u2019s and \u20181\u2019s, from user instructions. Despite that LLMs are trained with mostly natural language\ndataset, we find that with proper prompting and in-context learning, they can produce contact patterns\nto represent diverse quadruped motions. We then develop a Deep Reinforcement Learning (DRL)\nbased approach to generate robot actions given a desired contact pattern. We demonstrate that by\ndesigning a reward structure that only concerns about contact timing and exposing the policy to\nthe right distribution of contact patterns, we can obtain a controller capable of achieving diverse\nlocomotion patterns that can be transferred to the real robot hardware.\nWe evaluate the proposed approach on a physical quadruped robot, Unitree A1 [14], where it\nsuccessfully controls the robot to follow diverse and challenging instructions from users (Figure 1).\nWe benchmark the proposed approach against two baselines: (1) using discrete gaits, and (2) using\nsinusoidal functions as interface. Evaluations on 30 tasks demonstrate that the proposed approach\ncan achieve 50% higher success rate in predicting the correct contact pattern and can solve 10 more\ntasks than the baselines.\nThe key contributions of this paper are: i) A novel interface of contact pattern for harnessing\nknowledge from LLMs to flexibly and interactive control quadruped robots; ii) A pipeline to teach\nLLMs to generate complex contact patterns from user instructions; iii) A DRL-based method to train\na low-level controller that realizes diverse contact patterns on real quadruped robots. Finally, our\nproposal also holds intriguing potential for both human-robot interaction researchers and the robotic\nlocomotion community, inviting a compelling cross-disciplinary dialogue and collaboration.\n2\n2\nRelated Work\n2.1\nLanguage to robot control\nThere is a rich literature in leveraging language to modulate the behavior of robots [15, 10, 8, 16,\n17, 18, 19, 20]. Earlier work in this direction typically assumes structured text templates to translate\nlanguage to robot commands [17, 19] or leveraged natural language processing (NLP) tools such as\nthe parse tree to assist extracting the constraints from user input, followed by trajectory optimization to\nobtain robot motion [20]. Though these approaches demonstrate complex robotics tasks, they usually\ndo not handle unstructured natural language input. To mitigate this issue, recent work leverages the\nadvancements in representation learning and deep learning to train language conditioned policies\nthat mapped unstructured natural language instructions to robot actions [18, 21, 22, 23]. To establish\nproper mappings between natural language embeddings and robot actions, these approaches usually\nrequire a significant amount of demonstration data with language labels for training the policy, which\nis challenging to collect for diverse legged locomotion behaviors.\nInspired by recent success in LLMs to perform diverse tasks [5, 6, 7], researchers in robotics have\nalso explored ideas to connect LLMs to robot commands [8, 9, 11, 12, 13, 24, 25]. For example,\nAhn et al. [8] combined LLMs with a learned robot affordance function to pick the optimal pre-\ntrained robot skills for completing long horizon tasks. To mitigate the requirement for pre-training\nindividual low-level skills, researchers also proposed to expand the low-level primitive skills to the\nfull expressiveness of code by tasking LLMs to write robot codes [11, 12, 13]. As LLMs cannot\ndirectly generate low-level robot motor commands such as joint targets, these approaches had to\ndesign an intermediate interface for connecting LLMs and robot commands, such as high-level plans\n[8, 9, 24], primitive skills [11, 12], and trajectories [25]. In this work, we identify foot contact\npatterns to be a natural and flexible intermediate interface for quadrupedal robot locomotion that do\nnot require laborious design efforts.\n2.2\nLocomotion controller for legged robots\nTraining legged robots to exhibit complex contact patterns, especially gait patterns, has been ex-\ntensively studied by researchers in robotics, control, and machine learning. A common method\nis to model the robot dynamics and perform receding horizon trajectory optimization, i.e., Model\nPredictive Control (MPC), to follow desired contact patterns [26, 27, 28, 29, 30]. For quadruped\nrobots, this led to a large variety of canonical locomotion gaits such as trotting [26], pacing [31],\nbounding [32], and galloping [33], as well as non conventional gaits specified by the desired contact\ntiming or patterns [28, 30]. Despite the impressive results in these work, applying MPC to generate\ndiverse locomotion behavior often requires careful design of reference motion for robot base and\nswing legs and high computational cost due to re-planning. Prior work have also explored using\nlearning-based methods to author flexible locomotion gaits [34, 35, 36, 37, 38, 39, 40]. Some of\nthese work combines learning and MPC-based methods to identify the optimal gait parameters for\ntasks [34, 35, 36]. Others directly train DRL policies for different locomotion gaits, either through\ncareful reward function design [37, 40], open-loop commands extracted from prior knowledge about\ngaits [38, 39] or encoding of a predefined family of locomotion strategies that solve training tasks\nin different ways[41]. This paper explores an alternative DRL-based method that relies on the sim-\nple but flexible reward structure. Compared to the prior work, the proposed reward structure only\nconcerns about contact timing thus is more flexible in generating diverse locomotion behaviors.\n3\nMethod\nThe core ideas of our approach include introducing desired foot contact patterns as a new interface\nbetween human commands in natural language and the locomotion controller.\nThe locomotion\ncontroller is required to not only complete the main task (e.g., following specified velocities), but\nalso to place the robot\u2019s feet on the ground at the right time, such that the realized foot contact\npatterns are as close as possible to the desired ones, Figure 2 gives an overview of the proposed\nsystem. To achieve this, the locomotion controller takes a desired foot contact pattern at each time\n3\nShared \ufb02ow\nContact Pattern \nTemplate\n\u201cTrot in place, with \nthe front right leg \nmove twice as fast \nas other legs\u201d\nLarge \nLanguage \nModel\nProprioceptive \nsensory data\nDesired joint\npositions\nRandom \nPattern \nGenerator\nDesired gait type \n<latexit sha1_base64=\"7ho51pYdfO\nSEjnRb3iGzPhKUX2E=\">AB6HicbZDJSgNBEIZr4hbHLerRS2MQPIWZgMtFDHj\nQYwJmgWQIPZ2apE3PQnePEKewIsHRbzqQ/geXsS3sbMcNPGHho/r6Kryk8EV9\npxvq3M0vLK6lp23d7Y3Nreye3u1VScSoZVFotYNnyqUPAIq5prgY1EIg19gXW/fz\nXO6/coFY+jWz1I0AtpN+IBZ1Qbq3LdzuWdgjMRWQR3BvnLD/sief+y+3cZ6sTs\nzTESDNBlWq6TqK9IZWaM4Eju5UqTCjr0y42DUY0ROUNJ4OyJFxOiSIpXmRJhP3\nd8eQhkoNQt9UhlT31Hw2Nv/LmqkOzr0hj5JUY8SmHwWpIDom461Jh0tkWgwMUCa\n5mZWwHpWUaXMb2xzBnV95EWrFgntaOKm4+VIRpsrCARzCMbhwBiW4gTJUgQHCAz\nzBs3VnPVov1u0NGPNevbhj6y3H+SMj/M=</latexit>G\n<latexit sha1_base64=\"7ho51pYdfO\nSEjnRb3iGzPhKUX2E=\">AB6HicbZDJSgNBEIZr4hbHLerRS2MQPIWZgMtFDHj\nQYwJmgWQIPZ2apE3PQnePEKewIsHRbzqQ/geXsS3sbMcNPGHho/r6Kryk8EV9\npxvq3M0vLK6lp23d7Y3Nreye3u1VScSoZVFotYNnyqUPAIq5prgY1EIg19gXW/fz\nXO6/coFY+jWz1I0AtpN+IBZ1Qbq3LdzuWdgjMRWQR3BvnLD/sief+y+3cZ6sTs\nzTESDNBlWq6TqK9IZWaM4Eju5UqTCjr0y42DUY0ROUNJ4OyJFxOiSIpXmRJhP3\nd8eQhkoNQt9UhlT31Hw2Nv/LmqkOzr0hj5JUY8SmHwWpIDom461Jh0tkWgwMUCa\n5mZWwHpWUaXMb2xzBnV95EWrFgntaOKm4+VIRpsrCARzCMbhwBiW4gTJUgQHCAz\nzBs3VnPVov1u0NGPNevbhj6y3H+SMj/M=</latexit>G\nLocomotion \nController\n<latexit sha1_base64=\"JE1xCnOnF7uMaIDx9MKXCVrIi8=\">ACGXicbVDL\nSgMxFM34rPV69JNsAgVtMwIPpaFblxWsA/olCGTZmwmRmSO8Uy9BvcufFX3Cgo4lJXfoh7M20Xaj0k5HDOveTe48eCa7DtT2tufmFxaTm3kl9dW9/YLGwVmzpKFG\nUNGolItX2imeAhawAHwdqxYkT6grX861rmtwZMaR6FlzCMWVeSq5AHnBIwklew3T6BdDybrCrucSuJNBXMq0RGJUPnQN8aFeOD7BtTvY6+16hZFfsMfAscakVC26\n5a+nW7fuFd7dXkQTyUKgmjdcewYuilRwKlgo7ybaBYTek2uWMfQkEimu+l4sxHeM0oPB5EyNwQ8Vn92pERqPZS+qcwG13+9TPzP6yQnHVTHsYJsJBOPgoSgSHCWUy\n4xWjIaGEKq4mRXTPlGEgkzb0Jw/q48S5pHFekcnzhlKpHaIc2kG7qIwcdIq6BzVUQNRdIce0DN6se6tR+vVepuUzlnTnm30C9bHN1uToEU=</latexit>\u02c6vx \u21e0 Cat(\u22121, \u22120.5, 0, 0.5, 1)\n<latexit sha1_base64=\"JE1xCnOnF7uMaIDx9MKXCVrIi8=\">ACGXicbVDL\nSgMxFM34rPV69JNsAgVtMwIPpaFblxWsA/olCGTZmwmRmSO8Uy9BvcufFX3Cgo4lJXfoh7M20Xaj0k5HDOveTe48eCa7DtT2tufmFxaTm3kl9dW9/YLGwVmzpKFG\nUNGolItX2imeAhawAHwdqxYkT6grX861rmtwZMaR6FlzCMWVeSq5AHnBIwklew3T6BdDybrCrucSuJNBXMq0RGJUPnQN8aFeOD7BtTvY6+16hZFfsMfAscakVC26\n5a+nW7fuFd7dXkQTyUKgmjdcewYuilRwKlgo7ybaBYTek2uWMfQkEimu+l4sxHeM0oPB5EyNwQ8Vn92pERqPZS+qcwG13+9TPzP6yQnHVTHsYJsJBOPgoSgSHCWUy\n4xWjIaGEKq4mRXTPlGEgkzb0Jw/q48S5pHFekcnzhlKpHaIc2kG7qIwcdIq6BzVUQNRdIce0DN6se6tR+vVepuUzlnTnm30C9bHN1uToEU=</latexit>\u02c6vx \u21e0 Cat(\u22121, \u22120.5, 0, 0.5, 1)\n<latexit sha1_base64=\"qh5ugx6tV+HNJcPs/S6O+4zoG0=\">AB/3icbVDL\nSsNAFJ34rPUVFQRxEyCq5IUfGyEguK5i20IQwmU7aoTOTODOplpiFv+LGhSJu/Q13/Runj4W2HrhwOde7r0nTCiRyraHxsLi0vLKamGtuL6xubVt7uzWZwKhF\n0U01g0QygxJRy7iKm4nAkIUN8Le1chv9LGQJOZ3apBgn8EOJxFBUGkpMPe9LlRZPw8eL20vEhBlLM9kHpglu2yPYc0TZ0pK1QP7+mHo3tcC89trxyhlmCtEoZQt\nx06Un0GhCKI4L3qpxAlEPdjBLU05ZFj62fj+3DrWStuKYqGLK2us/p7IJNywELdyaDqylvJP7ntVIVXfgZ4UmqMEeTRVFKLRVbozCsNhEYKTrQBCJB9K0W6kKdgtK\nRFXUIzuzL86ReKTtn5dNbp1StgAkK4BAcgRPgHNQBTegBlyAwBN4AW/g3Xg2Xo0P43PSumBMZ/bAHxhfP68NmXU=</latexit>\u02c6vx = 0 m\ns\n<latexit sha1_base64=\"qh5ugx6tV+HNJcPs/S6O+4zoG0=\">AB/3icbVDL\nSsNAFJ34rPUVFQRxEyCq5IUfGyEguK5i20IQwmU7aoTOTODOplpiFv+LGhSJu/Q13/Runj4W2HrhwOde7r0nTCiRyraHxsLi0vLKamGtuL6xubVt7uzWZwKhF\n0U01g0QygxJRy7iKm4nAkIUN8Le1chv9LGQJOZ3apBgn8EOJxFBUGkpMPe9LlRZPw8eL20vEhBlLM9kHpglu2yPYc0TZ0pK1QP7+mHo3tcC89trxyhlmCtEoZQt\nx06Un0GhCKI4L3qpxAlEPdjBLU05ZFj62fj+3DrWStuKYqGLK2us/p7IJNywELdyaDqylvJP7ntVIVXfgZ4UmqMEeTRVFKLRVbozCsNhEYKTrQBCJB9K0W6kKdgtK\nRFXUIzuzL86ReKTtn5dNbp1StgAkK4BAcgRPgHNQBTegBlyAwBN4AW/g3Xg2Xo0P43PSumBMZ/bAHxhfP68NmXU=</latexit>\u02c6vx = 0 m\ns\nTraining \ufb02ow\nTest \ufb02ow\nSliding window\n<latexit sha1_base64=\"gBMhbgSg+kMClh7GWhjtrQwTr/U=\">ACHicdVDN\nTgIxGOyiIuIf6tFLIzHxQDZdEIUbiRcPHiDKTwIb0i0FGro/absasuEFTDx4wUfw6s149egbGF/CR7AsmqjRSZpOvpl2nECzqRC6NVILCwuJZdTK+nVtfWNzczWdk\nP6oSC0Tnzui5aDJeXMo3XFKetQFDsOpw2ndHJTG9eUiGZ712ocUBtFw81mcEKz06P+tedTNZJbL6NAqQ2QWEcqXCpqgQr5ULELRDGylWTt7WV6e1/tZt47PZ+E\nLvU4VjKtoUCZUdYKEY4naQ7oaQBJiM8oG1NPexSaUfxUyedeGuLgWNHcWoujszN8goTuK/9Pdj3hV6egrH5+30RdqUcu452ulgN5W9tNvxLa4eqX7Ij5gWhoh6ZB/V\nDpUPZ63AHhOUKD7WBPB9E8gGWKBidLdpXVFXz3A/0kjb1pHZrFmZSt5MEcK7I9cAscAwq4BRUQR0QMA3YArujGvjwXg0nubWhPF5Zgf8gPH8AaKYm10=</lat\nexit>Lw\n<latexit sha1_base64=\"gBMhbgSg+kMClh7GWhjtrQwTr/U=\">ACHicdVDN\nTgIxGOyiIuIf6tFLIzHxQDZdEIUbiRcPHiDKTwIb0i0FGro/absasuEFTDx4wUfw6s149egbGF/CR7AsmqjRSZpOvpl2nECzqRC6NVILCwuJZdTK+nVtfWNzczWdk\nP6oSC0Tnzui5aDJeXMo3XFKetQFDsOpw2ndHJTG9eUiGZ712ocUBtFw81mcEKz06P+tedTNZJbL6NAqQ2QWEcqXCpqgQr5ULELRDGylWTt7WV6e1/tZt47PZ+E\nLvU4VjKtoUCZUdYKEY4naQ7oaQBJiM8oG1NPexSaUfxUyedeGuLgWNHcWoujszN8goTuK/9Pdj3hV6egrH5+30RdqUcu452ulgN5W9tNvxLa4eqX7Ij5gWhoh6ZB/V\nDpUPZ63AHhOUKD7WBPB9E8gGWKBidLdpXVFXz3A/0kjb1pHZrFmZSt5MEcK7I9cAscAwq4BRUQR0QMA3YArujGvjwXg0nubWhPF5Zgf8gPH8AaKYm10=</lat\nexit>Lw\nExample Contact \nPattern Templates\n<latexit sha1_base64=\"0hIgnQyKqZ0s/tq2p1Ehtx7qYmQ=\">AB6HicbZDJ\nSgNBEIZr4hbHLerRS2MQPIWZgMtFDHjxmEA2SIbQ06kbXoWunuEMOQJvHhQxKs+hO/hRXwbO8tBE39o+Pj/Krq/FhwpR3n28qsrK6tb2Q37a3tnd293P5BXUWJZF\nhjkYhk06cKBQ+xprkW2Iwl0sAX2PCHN5O8cY9S8Sis6lGMXkD7Ie9xRrWxKtVOLu8UnKnIMrhzyF9/2Ffx+5d7uQ+292IJQGmgmqVMt1Yu2lVGrOBI7tdqIwpmxI\n+9gyGNIAlZdOBx2TE+N0S+S5oWaTN3fHSkNlBoFvqkMqB6oxWxi/pe1Et279FIexonGkM0+6iWC6IhMtiZdLpFpMTJAmeRmVsIGVFKmzW1scwR3ceVlqBcL7nhrOL\nmS0WYKQtHcAyn4MIFlOAWylADBgP8ATP1p31aL1Yr7PSjDXvOYQ/st5+APhAkA=</latexit>T\n<latexit sha1_base64=\"0hIgnQyKqZ0s/tq2p1Ehtx7qYmQ=\">AB6HicbZDJ\nSgNBEIZr4hbHLerRS2MQPIWZgMtFDHjxmEA2SIbQ06kbXoWunuEMOQJvHhQxKs+hO/hRXwbO8tBE39o+Pj/Krq/FhwpR3n28qsrK6tb2Q37a3tnd293P5BXUWJZF\nhjkYhk06cKBQ+xprkW2Iwl0sAX2PCHN5O8cY9S8Sis6lGMXkD7Ie9xRrWxKtVOLu8UnKnIMrhzyF9/2Ffx+5d7uQ+292IJQGmgmqVMt1Yu2lVGrOBI7tdqIwpmxI\n+9gyGNIAlZdOBx2TE+N0S+S5oWaTN3fHSkNlBoFvqkMqB6oxWxi/pe1Et279FIexonGkM0+6iWC6IhMtiZdLpFpMTJAmeRmVsIGVFKmzW1scwR3ceVlqBcL7nhrOL\nmS0WYKQtHcAyn4MIFlOAWylADBgP8ATP1p31aL1Yr7PSjDXvOYQ/st5+APhAkA=</latexit>T\nFL: 1111100000000000000\nFR: 1111100000000000000\nRL: 0000111110000000000\nRR: 0000111110000000000\n<latexit sha1_base64=\"FVeVr0+qHO\nrajdGmUyGFeQwl+g=\">AB+XicbVDLSsNAFJ3UV42vqEs3wSK4KknBx6ZYVNC\nVjBtoQ1lMp20Q2cmYWZSKF/4kZEbcu/A834t84abvQ1gMDh3Pu5Z45QUyJVI\n7zbeQWFpeWV/Kr5tr6xuaWtb1Tk1EiEPZQRCPRCKDElHDsKaIobsQCQxZQXA/6F5\nlfH2AhScTv1TDGPoNdTkKCoNJS27Kuyi0GVU+w9PzWu7kcta2CU3TGsOeJOyWFs\nw+zHD9mdW29dnqRChmCtEoZRN14mVn0KhCKJ4ZLYSiWOI+rCLm5pyLD03Hy\nkX2glY4dRkI/ruyx+nsjhUzKIQv0ZJZSznqZ+J/XTFR46qeEx4nCHE0OhQm1VWR\nnNdgdIjBSdKgJRILorDbqQGR0mWZugR39svzpFYqusfFozu3UCmBCfJgD+yDQ+\nCE1AB16AKPIDADyAZ/BipMaj8Wq8TUZzxnRnF/yB8f4DI/6WTA=</latexit>G = BOUND\n<latexit sha1_base64=\"FVeVr0+qHO\nrajdGmUyGFeQwl+g=\">AB+XicbVDLSsNAFJ3UV42vqEs3wSK4KknBx6ZYVNC\nVjBtoQ1lMp20Q2cmYWZSKF/4kZEbcu/A834t84abvQ1gMDh3Pu5Z45QUyJVI\n7zbeQWFpeWV/Kr5tr6xuaWtb1Tk1EiEPZQRCPRCKDElHDsKaIobsQCQxZQXA/6F5\nlfH2AhScTv1TDGPoNdTkKCoNJS27Kuyi0GVU+w9PzWu7kcta2CU3TGsOeJOyWFs\nw+zHD9mdW29dnqRChmCtEoZRN14mVn0KhCKJ4ZLYSiWOI+rCLm5pyLD03Hy\nkX2glY4dRkI/ruyx+nsjhUzKIQv0ZJZSznqZ+J/XTFR46qeEx4nCHE0OhQm1VWR\nnNdgdIjBSdKgJRILorDbqQGR0mWZugR39svzpFYqusfFozu3UCmBCfJgD+yDQ+\nCE1AB16AKPIDADyAZ/BipMaj8Wq8TUZzxnRnF/yB8f4DI/6WTA=</latexit>G = BOUND\nFL: 111111111111111000000\nFR: 000000111111111111111\nRL: 000000111111111111111\nRR: 111111111111111000000\n<latexit sha1_base64=\"bDd2SZbcDuPQ0TMGIeJA8iPeF8=\">AB+HicbVDL\nSgMxFM3UVx0fHXpJlgEV2Wm4GNTLjQnVX6gnYomTRtQ5PMkGSEOvRL3Agq4taN/+FG/BszbRfaeiBwOde7skJIkaVdt1vK7O0vLK6l23Nza3tnPOzm5dhbHEpI\nZDFspmgBRhVJCapqRZiQJ4gEjWB4kfqNOyIVDUVjyLic9QXtEcx0kbqOLnLUpsjPZA8qd5eV8cdJ+8W3AngIvFmJH/+YZeipy+70nE+290Qx5wIjRlSquW5kfYT\nJDXFjIztdqxIhPAQ9UnLUIE4UX4yCT6Gh0bpwl4ozRMaTtTfGwniSo14YCbTkGreS8X/vFase2d+QkUayLw9FAvZlCHMG0BdqkWLORIQhLarJCPEASYW26sk0J3vy\nXF0m9WPBOCsc3Xr5cBFNkwT4AEfA6egDK5ABdQABjF4AM/gxbq3Hq1X6206mrFmO3vgD6z3H7KDlhM=</latexit>G = TROT\n<latexit sha1_base64=\"bDd2SZbcDuPQ0TMGIeJA8iPeF8=\">AB+HicbVDL\nSgMxFM3UVx0fHXpJlgEV2Wm4GNTLjQnVX6gnYomTRtQ5PMkGSEOvRL3Agq4taN/+FG/BszbRfaeiBwOde7skJIkaVdt1vK7O0vLK6l23Nza3tnPOzm5dhbHEpI\nZDFspmgBRhVJCapqRZiQJ4gEjWB4kfqNOyIVDUVjyLic9QXtEcx0kbqOLnLUpsjPZA8qd5eV8cdJ+8W3AngIvFmJH/+YZeipy+70nE+290Qx5wIjRlSquW5kfYT\nJDXFjIztdqxIhPAQ9UnLUIE4UX4yCT6Gh0bpwl4ozRMaTtTfGwniSo14YCbTkGreS8X/vFase2d+QkUayLw9FAvZlCHMG0BdqkWLORIQhLarJCPEASYW26sk0J3vy\nXF0m9WPBOCsc3Xr5cBFNkwT4AEfA6egDK5ABdQABjF4AM/gxbq3Hq1X6206mrFmO3vgD6z3H7KDlhM=</latexit>G = TROT\nFL: 1111111110000\nFR: 0000111111111\nRL: 1111111110000\nRR: 0000111111111\n<latexit sha1_base64=\"yBUBmpE/4gXyjhNr4UL0MEDLtSY=\">AB+HicbVDL\nSgMxFM3UVx0fHXpJlgEV2Wm4GNTrBTRZQX7gHYomTRtQ5PMkGSEOvRL3Agq4taN/+FG/BszbRfaeiBwOde7skJIkaVdt1vK7O0vLK6l23Nza3tnPOzm5dhbHEpI\nZDFspmgBRhVJCapqRZiQJ4gEjWBYSf3GHZGKhuJWjyLic9QXtEcx0kbqOLmrUpsjPZA8qV5ULscdJ+8W3AngIvFmJH/+YZeipy+72nE+290Qx5wIjRlSquW5kfYT\nJDXFjIztdqxIhPAQ9UnLUIE4UX4yCT6Gh0bpwl4ozRMaTtTfGwniSo14YCbTkGreS8X/vFase2d+QkUayLw9FAvZlCHMG0BdqkWLORIQhLarJCPEASYW26sk0J3vy\nXF0m9WPBOCsc3Xr5cBFNkwT4AEfA6egDK5BFdQABjF4AM/gxbq3Hq1X6206mrFmO3vgD6z3H2lZleM=</latexit>G = PACE\n<latexit sha1_base64=\"yBUBmpE/4gXyjhNr4UL0MEDLtSY=\">AB+HicbVDL\nSgMxFM3UVx0fHXpJlgEV2Wm4GNTrBTRZQX7gHYomTRtQ5PMkGSEOvRL3Agq4taN/+FG/BszbRfaeiBwOde7skJIkaVdt1vK7O0vLK6l23Nza3tnPOzm5dhbHEpI\nZDFspmgBRhVJCapqRZiQJ4gEjWBYSf3GHZGKhuJWjyLic9QXtEcx0kbqOLmrUpsjPZA8qV5ULscdJ+8W3AngIvFmJH/+YZeipy+72nE+290Qx5wIjRlSquW5kfYT\nJDXFjIztdqxIhPAQ9UnLUIE4UX4yCT6Gh0bpwl4ozRMaTtTfGwniSo14YCbTkGreS8X/vFase2d+QkUayLw9FAvZlCHMG0BdqkWLORIQhLarJCPEASYW26sk0J3vy\nXF0m9WPBOCsc3Xr5cBFNkwT4AEfA6egDK5BFdQABjF4AM/gxbq3Hq1X6206mrFmO3vgD6z3H2lZleM=</latexit>G = PACE\n<latexit sha1_base64=\"hC6JKuoMiz\n+aMY+7MhzUtWCdfmI=\">AB6HicbZDJSgNBEIZ74hbHLerRS2MQPIWZgMtFDHj\nxmIBZIBlCT6cmadOz0F0jhCFP4MWDIl71IXwPL+Lb2FkOGv2h4eP/q+iq8hMpND\nrOl5VbWl5ZXcuv2xubW9s7hd29ho5TxaHOYxmrls80SBFBHQVKaCUKWOhLaPrDq0\nnevAOlRzd4CgBL2T9SASCMzRWDbuFolNypqJ/wZ1D8fLdvkjePu1qt/DR6cU8D\nSFCLpnWbdJ0MuYQsEljO1OqiFhfMj60DYsRC0l0HdMj4/RoECvzIqRT92dH\nxkKtR6FvKkOGA72YTcz/snaKwbmXiShJESI+yhIJcWYTramPaGAoxwZYFwJMyv\nlA6YR3Mb2xzBXVz5LzTKJfe0dFJzi5UymSlPDsghOSYuOSMVck2qpE4AXJPHs\nmTdWs9WM/Wy6w0Z8179skvWa/fKM+QIA=</latexit>t\n<latexit sha1_base64=\"hC6JKuoMiz\n+aMY+7MhzUtWCdfmI=\">AB6HicbZDJSgNBEIZ74hbHLerRS2MQPIWZgMtFDHj\nxmIBZIBlCT6cmadOz0F0jhCFP4MWDIl71IXwPL+Lb2FkOGv2h4eP/q+iq8hMpND\nrOl5VbWl5ZXcuv2xubW9s7hd29ho5TxaHOYxmrls80SBFBHQVKaCUKWOhLaPrDq0\nnevAOlRzd4CgBL2T9SASCMzRWDbuFolNypqJ/wZ1D8fLdvkjePu1qt/DR6cU8D\nSFCLpnWbdJ0MuYQsEljO1OqiFhfMj60DYsRC0l0HdMj4/RoECvzIqRT92dH\nxkKtR6FvKkOGA72YTcz/snaKwbmXiShJESI+yhIJcWYTramPaGAoxwZYFwJMyv\nlA6YR3Mb2xzBXVz5LzTKJfe0dFJzi5UymSlPDsghOSYuOSMVck2qpE4AXJPHs\nmTdWs9WM/Wy6w0Z8179skvWa/fKM+QIA=</latexit>t\nFigure 2: Overview of the proposed approach. In addition to the robot\u2019s proprioceptive sensory data\nand task commands (e.g., following a desired linear velocity \u02c6\ud835\udc63\ud835\udc65), the locomotion controller accepts\ndesired foot contact patterns as input, and outputs desired joint positions. The foot contact patterns\nare extracted by a cyclic sliding window of size \ud835\udc3f\ud835\udc64 from a pattern template, which is generated\nby a random pattern generator during training, and is translated from human commands in natural\nlanguage by an LLM in tests. We show some examples of contact pattern templates at the bottom.\nstep as its input, in addition to the robot\u2019s proprioceptive sensory data and task related inputs (e.g.,\nuser specified velocity commands). At training, a random generator creates these desired foot contact\npatterns, while at test time a LLM translates them from human commands.\nIn this paper, a desired foot contact pattern is defined by a cyclic sliding window of size \ud835\udc3f\ud835\udc64 that\nextracts the four feet ground contact flags between \ud835\udc61 + 1 and \ud835\udc61 + \ud835\udc3f\ud835\udc64 from a pattern template and is of\nshape 4 \u00d7 \ud835\udc3f\ud835\udc64. A contact pattern template is a 4 \u00d7 \ud835\udc47 matrix of \u20180\u2019s and \u20181\u2019s, with \u20180\u2019s representing\nfeet in the air and \u20181\u2019s for feet on the ground. From top to bottom, each row in the matrix gives the\nfoot contact patterns of the front left (FL), front right (FR), rear left (RL) and rear right (RR) feet.\nWe demonstrate that the LLM is capable of mapping human commands into foot contact pattern\ntemplates in specified formats accurately given properly designed prompts, even in cases when the\ncommands are unstructured and vague (Section 3.1). In training, we use a random pattern generator\nto produce contact pattern templates that are of various pattern lengths \ud835\udc47, foot-ground contact ratios\nwithin a cycle based on a given gait type \ud835\udc3a (Section 3.2.2), so that the locomotion controller gets to\nlearn on a wide distribution of movements and generalizes better.\n<General instruction block>\nYou are a dog foot contact pattern expert.\nYour job is to give a velocity and a foot contact pattern based on the input.\nYou will always give the output in the correct format no matter what the input is.\n<Gait de\ufb01nition block>\nThe following are description about gaits:\n1. Trotting is a gait where two diagonally opposite legs strike the ground at the same time.\n2. Pacing is a gait where the two legs on the left/right side of the body strike the ground at\n     the same time.\n3. Bounding is a gait where the two front/rear legs strike the ground at the same time.\n     It has a longer suspension phase where all feet are off the ground, for example,\n     for at least 25% of the cycle length. This gait also gives a happy feeling.\n<Output format de\ufb01nition block>\nThe following are rules for describing the velocity and foot contact patterns:\n1. You should \ufb01rst output the velocity, then the foot contact pattern.\n2. There are \ufb01ve velocities to choose from: [-1.0, -0.5, 0.0, 0.5, 1.0].\n3. A pattern has 4 lines, each of which represents the foot contact pattern of a leg.\n4. Each line has a label. \"FL\" is front left leg, \"FR\" is front right leg, \"RL\" is rear left leg,\n     and \"RR\" is rear right leg.\n5. In each line, \"0\" represents foot in the air, \"1\" represents foot on the ground.\nContinue on the right \u2026\n<Examples block>\nInput: Trot slowly\nOutput: 0.5\nFL: 11111111111111111000000000\nFR: 00000000011111111111111111\nRL: 00000000011111111111111111\nRR: 11111111111111111000000000\nInput: Bound in place\nOutput: 0.0\nFL: 11111111111100000000000000\nFR: 11111111111100000000000000\nRL: 00000011111111111100000000\nRR: 00000011111111111100000000\nInput: Pace backward fast\nOutput: -1.0\nFL: 11111111100001111111110000\nFR: 00001111111110000111111111\nRL: 11111111100001111111110000\nRR: 00001111111110000111111111\nInput:\nFigure 3: Our exact prompt for our method in all experiments. The final \u201cInput:\u201d is followed by user\nspecified command. Texts in black are for explanation and are not used as input to the LLM.\n4\n3.1\nLanguage to Foot Contact Patterns\nAlthough LLMs can learn knowledge from a vast amount of text data at training, providing proper\nprompts at inference is the key to unlock and direct the acquired knowledge in meaningful ways.\nCarefully designed prompts serve as the starting point for the models to generate text and guide the\ndirection and context of the outputs. The proposed approach aims to enable the LLM to map any\nhuman commands in natural language to foot contact patterns in a specified format. Figure 3 lists\nthe prompts used in this paper, wherein we group them into four categories:\n1. General instruction describes the task the LLM should accomplish. In this paper, the LLM\nis expected to translate an arbitrary command to a foot contact pattern. Note that examples\nof such translations will be provided in Examples block.\n2. Gait definition gives basic knowledge of quadrupedal gaits. Although their descriptions\nare neither exhaustive nor sufficiently accurate, experimental results suggest that it provides\nenough information for the LLM to follow the rules. It also connects the bounding gait\nto a general impression of emotion. This helps the LLM generalize over vague human\ncommands that do not explicitly specify what gaits the robot should use.\n3. Output format definition specifies the format of the output. We discretize the desired ve-\nlocities \u02c6\ud835\udc63\ud835\udc65 \u2208 {\u22121, \u22120.5, 0, 0.5, 1} \ud835\udc5a\n\ud835\udc60 so that the LLM can give proper outputs corresponding\nto commands that contain words like \u201cfast(er)\u201d and \u201cslow(er)\u201d.\n4. Examples block follows the general knowledge of instruction fine-tuning and shows the\nLLM a few concrete input-output pairs. Although we give the LLM three commonly seen\ngait examples only, experimental results show that it is able to generalize and handle various\ncommands, including those vaguely state what velocity or gait the robot should use.\n3.2\nFoot Contact Pattern to Low-level Commands\n3.2.1\nProblem Formation\nWe formulate locomotion control as a Markov Decision Process (MDP) and solve it using DRL\nalgorithms. An MDP is a tuple (\ud835\udc46, \ud835\udc34, \ud835\udc5f, \ud835\udc53 , \ud835\udc430, \ud835\udefe), where \ud835\udc46 is the state space, \ud835\udc34 is the action space,\n\ud835\udc5f(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61, \ud835\udc60\ud835\udc61+1) is the reward function, \ud835\udc53 (\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61) is the system transition function, \ud835\udc430 is the distribution\nof initial states \ud835\udc600, and \ud835\udefe \u2208 [0, 1] is the reward discount factor. The goal of a DRL algorithm is to op-\ntimize a policy \ud835\udf0b : \ud835\udc46 \u21a6\u2192 \ud835\udc34 so that the expected accumulated reward \ud835\udc3d = \ud835\udc38\ud835\udc600\u223c\ud835\udc430 [\u00cd\n\ud835\udc61 \ud835\udefe\ud835\udc61\ud835\udc5f(\ud835\udc60\ud835\udc61, \ud835\udc4e\ud835\udc61, \ud835\udc60\ud835\udc61+1)]\nis maximized. Here, \ud835\udc4e\ud835\udc61 = \ud835\udf0b\ud835\udf03 (\ud835\udc60\ud835\udc61) and \ud835\udf03 is the set of learnable parameters. In locomotion tasks, \ud835\udc60\ud835\udc61\noften includes sensory data and goal conditions (e.g., user specified velocity commands [42]), and\n\ud835\udc4e\ud835\udc61 is desired joint angles or motor torques. We expand \ud835\udc60\ud835\udc61 to include a desired foot contact pattern,\nand the controller needs to achieve the main task as well as realize the desired foot contact patterns.\n3.2.2\nRandom Pattern Generator\nThe random pattern generator receives a gait type \ud835\udc3a, it then randomly samples a corresponding\ncycle length \ud835\udc47 and the ground contact ratio within the cycle for each feet, conducts proper scaling\nand phase shifts, and finally outputs a pattern template. Due to the space restrictions, we defer\nthe detailed implementation and illustrations in the Appendix. While humans can give commands\nthat map to a much wider set of foot contact pattern templates, we define and train on five types:\n\ud835\udc3a \u2208 {BOUND, TROT, PACE, STAND STILL, STAND 3LEGS}. Examples of the first three types\nare illustrated at the bottom of Figure 2, the latter two types are trivial and omitted in the figure.\n3.2.3\nLocomotion Controller\nWe use a feed-forward neural network as the control policy \ud835\udf0b\ud835\udf03. It outputs the desired positions for\neach motor joint and its input includes the base\u2019s angular velocities, the gravity vector \u00ae\ud835\udc54 = [0, 0, \u22121]\nin the base\u2019s frame, user specified velocity, current joint positions and velocities, policy output from\nthe last time step, and desired foot contact patterns. In this paper, we use Unitree A1 [14] as the\nquadrupedal robot. A1 has 3 joints per leg (i.e., hip, thigh and calf joints) and \ud835\udc3f\ud835\udc64 = 5 in all\n5\nexperiments, therefore the dimensions of the policy\u2019s input and output are 65 and 12, respectively.\nThe policy has three hidden layers of sizes [512, 256, 128] with ELU(\ud835\udefc = 1.0) at each hidden layer\nas the non-linear activation function.\nTo encourage natural and symmetric behaviors, we employ a double-pass trick in the control policy\nwhich has been shown to be effective in other scenarios too [43, 44]. Specifically, instead of using\n\ud835\udc4e\ud835\udc61 = \ud835\udf0b\ud835\udf03 (\ud835\udc60\ud835\udc61) directly as the output, we use \ud835\udc4e\ud835\udc61 = 0.5[\ud835\udf0b\ud835\udf03 (\ud835\udc60\ud835\udc61) + \ud835\udc53act(\ud835\udf0b\ud835\udf03 ( \ud835\udc53obs(\ud835\udc60\ud835\udc61))], where \ud835\udc53act(\u00b7) and\n\ud835\udc53obs(\u00b7) flips left-right the policy\u2019s output and the robot\u2019s state respectively. Intuitively, this double-\npass trick says the control policy should output consistently when it receives the original and the\nleft-right mirrored states. In practice, we find this trick greatly improves the naturalness of the robot\u2019s\nmovement and helped shrink the sim-to-real gap.\n3.2.4\nTask and Training Setups\nThe controller\u2019s main task is to follow user specified linear velocities along the robot\u2019s heading\ndirection, while keeping the linear velocity along the lateral direction and the yaw angular velocity as\nclose to zeros as possible. At the same time, it also needs to plan for the correct timing for feet-ground\nstrikes so that the realized foot contact patterns match the desired ones. For real world deployment,\nwe add a regularization term that penalizes action changing rate so that the real robot\u2019s movement\nis smoother. In addition to applying domain randomization, we find that extra reward terms that\nkeep the robot base stable can greatly shrink the sim-to-real gap and produce natural looking gaits.\nFinally, although no heavy engineering is required to train the locomotion policy with extra contact\npattern inputs, we find it helps to balance the ratio of the gait types during training. Please refer to\nthe Appendix for hyper-parameters and detailed settings.\n4\nExperiments\nWe conducted experiments to answer three questions. Throughout the experiments, we used GPT-4\n[45] as the LLM. Please see the Appendix for experimental setups.\n4.1\nIs Foot Contact Pattern a Good Interface?\nThe first experiment compares foot contact pattern with other possible interface designs.\nOne\noption is to introduce intermediate parameters as the interface, and have the LLM map from human\nnatural language to the parameter values. We use two baseline approaches for comparison: Baseline\n1 contains a discrete parameter \ud835\udc3a that is the 5 gait types introduced in Section 3.2.2; Baseline\n2 contains 4 tuples of continuous parameters (\ud835\udc4e\ud835\udc56, \ud835\udc4f\ud835\udc56, \ud835\udc50\ud835\udc56), \ud835\udc56 \u2208 {1, 2, 3, 4} that defines a sinusoidal\nfunction \ud835\udc66\ud835\udc56(\ud835\udc61) = sin(\ud835\udc4e\ud835\udc56\ud835\udc61 + \ud835\udc4f\ud835\udc56) and its cutoff threshold that defines the foot-ground contact flag for\nthe \ud835\udc56-th foot \u2013 FOOT ON GROUND = 1{\ud835\udc66\ud835\udc56(\ud835\udc61) \u2264 \ud835\udc50\ud835\udc56}. Here, \ud835\udc61 \u2208 [1,\ud835\udc47] is the time step within the\ncycle. We construct foot contact pattern templates based on the values output by the LLM (e.g., for\nBaseline 1, we use the random pattern generator; for Baseline 2, we use the sinusoidal functions and\nthe cutoff values) and check if they are correct.\nFigure 4 shows the prompts for the two baselines, where they are based on the prompt in Figure 3\nwith necessary modifications. Table 1 gives the commands we use in this experiment; commands\n1\u201320 are basic instructions that express explicitly what the robot should do, whereas commands\n21\u201325 test if the interface design allows generalization and pattern composition. We set GPT-4\u2019s\ntemperature to 0.5 to sample diverse responses, and for each approach we submit each command five\ntimes. For each submission, we use the top-1 result only for comparisons.\nWe implement domain knowledge based checker programs for each command for objective eval-\nuations (see Appendix D), and we summarize the results in Figure 5. Aggregating over all the\ncommands and test trials, the proposed approach gets significantly (\u223c 50%) higher accuracy than the\nbaselines (see the left-most plot in the first row). Despite of having only three conventional examples\nin the context, the LLM almost always maps the human commands correctly to the expected foot\ncontact patterns. The only exception in the test commands is command 21, where the LLM is correct\nonly one out of the five tests. It mostly fails to generate columns of 0s in the pattern template, but\n6\nTable 1: Commands for generated pattern template evaluation. We observed the foot contact patterns\ngenerated by the LLM after accepting the commands, and compared them against our checkers.\nId\nCommand\n1\nStand still\n2\u20135\nLift front left / front right / rear left / rear right leg\n6\u20138\nBound / Trot / Pace in place\n9\u201311\nBound / Trot / Pace forward slowly\n12\u201314\nBound / Trot / Pace forward fast\n15\u201317\nBound / Trot / Pace backward slowly\n18\u201320\nBound / Trot / Pace backward fast\n21\nTrot in place, with a suspension phase where all feet are off the ground\n22\nTrot forward, with the front right leg moving at a higher frequency\n23\nStand on front right and rear left legs\n24\nWalk with 3 legs, with the rear right foot always in the air\n25\nBound then pace, you can extend the pattern length if necessary\nin one interesting case, it appends an extra row of \u201cS: 00\u00b7 \u00b7 \u00b7 0\u201d to the pattern template, trying to\nconvince us of the existence of the required suspension phase. Baseline 1 gets the second highest\naccuracy; it achieves high scores on the basic instructions but fails completely for commands 21\u201325.\nConsidering that this is how we sample patterns and train the controller, these results are somewhat\nexpected. It fails to generate the correct patterns for commands 2\u20135 because the random pattern\ngenerator selects randomly a foot to lift for \ud835\udc3a = STAND 3LEGS. Although we could have relaxed\nthe design of Baseline 1 so that it accepted extra parameters for \ud835\udc3a, we didn\u2019t have to do so for\nthe proposed approach and it still worked out. Moreover, this design modification has very limited\neffect and highlights the restrictions imposed by these high-level APIs. Unlike Baseline 1, Baseline\n2 should have sufficient freedom in the parameter space to handle all the commands (maybe not\ncommand 25), yet its overall accuracy is the worst. Although we performed prompt engineering and\nconstructed the examples carefully in its context for Baseline 2, the LLM has difficulty in under-\nstanding the relationship between gaits and the underlying mathematical reasoning. This limitation\nagain highlights the motivation and demonstrates the importance of the proposed approach. The\nexperimental results indicate that foot contact pattern is a good interface as it is both straightforward\nand able to provide more flexibility in the human command space.\n4.2\nCan we learn to accomplish the main task and realize the contact pattern?\nFollowing [42], we train the locomotion controller with the Proximal policy optimization (PPO) [46]\nin the IsaacGym simulator [47]. The controller\u2019s main task is to track a user specified linear velocity\nalong the robot\u2019s heading direction \ud835\udc63\ud835\udc65, and at the same time, to place the feet correctly to produce\nBaseline 1\n<General instruction block>\nYou are a dog foot contact pattern expert.\nYour job is to give a velocity and the gait type for constructing a\nfoot contact pattern based on the input.\nYou will always give the output in the correct format no matter what\nthe input is.\n<Output format de\ufb01nition block>\nThe following are rules for describing the velocity and parameters:\n1. You should output the (velocity, gait type) pair.\n2. There are \ufb01ve velocities to choose from: [-1.0, -0.5, 0.0, 0.5, 1.0].\n3. There are \ufb01ve gait types to choose from: [STAND_STILL,\n     STAND_3LEGS, BOUND, TROT, PACE].\n<Examples block>\nInput: Trot slowly\nOutput: (0.5, TROT)\nInput: Bound in place\nOutput: (0.0, BOUND)\nInput: Pace backward fast\nOutput: (-1.0, PACE)\nInput:\nBaseline 2\n<General instruction block>\nYou are a dog foot contact pattern expert.\nYour job is to give a velocity and the parameters for constructing a\nfoot contact pattern based on the input.\nYou will always give the output in the correct format no matter what\nthe input is.\n<Output format de\ufb01nition block>\nThe following are rules for describing the velocity and parameters:\n1. You should \ufb01rst output the velocity, then the parameters.\n2. There are \ufb01ve velocities to choose from: [-1.0, -0.5, 0.0, 0.5, 1.0].\n3. You give the parameters in 4 lines, each of which describes the\n     parameters (a, b, c) for a leg.\n4. Each line has a label. \"FL\" is front left leg, \"FR\" is front right leg, \n     \"RL\" is rear left leg, and \"RR\" is rear right leg.\n5. There are 3 numbers on each line, they form a python function\n     `lambda t: sin(a * t + b) < c`. It represents foot on the ground at\n     t where the function returns True, and foot in air at t where the\n     function returns False. t is the time step in the gait cycle.\nContinue on the right \u2026\n<Examples block>\nInput: Trot slowly\nOutput: 0.5\nFL: 0.13 10 -0.36\nFR: 0.13 8.7 -0.36\nRL: 0.13 8.7 -0.36\nRR: 0.13 10 -0.36\nInput: Bound in place\nOutput: 0.0\nFL: 0.2 10 -0.36\nFR: 0.2 10 -0.36\nRL: 0.2 8.6 -0.36\nRR: 0.2 8.6 -0.36\nInput: Pace backward fast\nOutput: -1.0\nFL: 0.5 8.9 0.58\nFR: 0.5 7 0.58\nRL: 0.5 8.9 0.58\nRR: 0.5 7 0.58\nInput:\nFigure 4: Baselines prompts. Differences from our prompt are highlighted in blue. The \u201cGait\ndefinition block\u201d is not changed and omitted in the figure. Texts in black are for explanation thus\nthey are not used as input to the LLM.\n7\nOverall\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n0.0\n0.5\n1.0\nOurs\nBaseline1\nBaseline2\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\nCommand Id\n0.0\n0.5\n1.0\nAccuracy\nFigure 5: Accuracy comparison of generated patterns. For each command in Table 1, we generate\n5 patterns from the LLM and compare them against the expected results. We show the aggregated\naccuracy over all commands on the left of the first row, followed by the individual results.\n0.0\n0.5\nActual vx\n\"Bound forward slowly\" to \n\"Raise your front right leg\"\n1\n0\n\"Bound forward slowly\" to \n\"Trot backward fast\"\n0.50\n0.75\n1.00\n\"Trot forward slowly\" to \n\"Pace forward fast\"\nFL\nFR\nRL\nRR\nDesired\nPattern\nFL\nFR\nRL\nRR\nFL\nFR\nRL\nRR\n0\n1\n2\n3\n4\nTime (s)\nFL\nFR\nRL\nRR\nRealized\nPattern\n0\n1\n2\n3\n4\nFL\nFR\nRL\nRR\n0\n1\n2\n3\n4\nFL\nFR\nRL\nRR\nFigure 6: Velocity tracking and foot contact pattern realization in simulation. We show the actual\nlinear velocity along the robot\u2019s heading direction (first row), the desired foot contact pattern (middle\nrow) and the realized foot contact pattern (last row) from three test trials. The commands given to\nthe robot in each trial are shown at the top of the plots.\nthe desired foot contact patterns. Figure 6 shows the results in simulation. The commands given\nto the robot in each trial are shown at the top of the plots. It can be seen from the figure that\nthe controller learns to track the specified velocity (e.g., \u201cslow\u201d/\u201cfast\u201d corresponds to 0.5 \ud835\udc5a\n\ud835\udc60 /1.0 \ud835\udc5a\n\ud835\udc60\nin absolute values) and manages to place the robot\u2019s feet correctly to produce foot contact patterns\nthat are close to the desired ones. Furthermore, we successfully transfer the learned controller and\ndeploy it on the physical robot without any fine-tuning. Figure 1 gives some analytical results on the\nphysical robot. Please watch the accompanying video for the motions.\n4.3\nDoes the proposed approach work with unstructured/vague instructions?\nThe proposed approach enables both the quadrupedal robot to follow direct and precise commands\nand unstructured and vague instructions in natural language that facilitates human robot interactions.\nTo demonstrate this, we sent commands in Table 2 to the robot and observe its reactions. Note that\nunlike in the previous tests, none of the human expressions here stated explicitly what the robot\nshould have done or what gait it should have used. Based on the subjective evaluation, the observed\nmotions were capable of expressing the desired emotion (e.g., jumping up and down when excited)\nand presenting the scene accurately (e.g, struggling to move when we told that it had a limping leg),\nthe reactions were mostly consistent with the expectations. This will unlock many robot applications,\nranging from scene acting and human companion to more creative tasks in industries and homes.\nTable 2: Extended tests. The commands in this test do not tell the robot explicitly what it should do.\nCommand\nObserved Robot Motion\nGood news, we are going to a picnic!\nJumping up and down\nBack off, don\u2019t hurt that squirrel!\nMoving backward slowly in trotting gaits\nAct as if the ground is very hot\nPacing fast, with its feet barely touching the ground\nAct as if you have a limping rear left leg\nStruggling to walk, with its RL leg hardly moving\nGo catch that squirrel on the tree\nBounding fast forward toward the prey\n8\n5\nConclusions\nThis paper devised an interactive system for quadrupedal robots that allowed users to craft diverse\nlocomotion behaviours flexibly. The core ideas of the proposed approach include introducing desired\nfoot contact patterns as a new interface between natural language and the low-level controller. During\ntraining, these contact patterns are generated by a random generator, and a DRL based method is\ncapable of accomplishing the main task and realizing the desired patterns at the same time. In\ntests, the contact patterns are translated from human commands in natural language. We show that\nhaving contact patterns as the interface is more straightforward and flexible than other design choices.\nMoreover, the robot is able to follow both direct instructions and commands that do not explicitly\nstate how the robot should react in both simulation and on physical robots.\nLimitations and Future Work\nOne limitation of the proposed approach is that domain knowledge and trial-and-error tests are\nnecessary to design the random pattern generator, such that the patterns used for training are feasible.\nFurthermore, while increasing the variety of the random patterns would essentially increase the\nlocomotion capability of the robot, training on a large set of gaits is hard since it involves the\ntrade-off of sample balancing and data efficiency.\nOne may train a set of expert policies separately, where each of which specializes in one motion, then\nuse imitation learning to distill the experts to address this problem. Another interesting direction for\nfuture works is to modify the current pattern representation and make it more versatile (e.g., replacing\n0s and 1s with 0s and \ud835\udc3b\u2019s to specify desired foot clearance \ud835\udc3b), alternatively methods in [48, 49] can\nalso be incorporated to achieve the same effect.\nAcknowledgments\nThe authors would like to thank Tingnan Zhang, Linda Luu, Kuang-Huei Lee, Vincent Vanhoucke\nand Douglas Eck for their valuable discussions and technical support in the experiments.\nThe experiments in this work were performed on GPU virtual machines provided by Google Cloud\nPlatform.\nThis work was partially supported by JST Moonshot R&D Grant Number JPMJPS2011, CREST\nGrant Number JPMJCR2015 and Basic Research Grant (Super AI) of Institute for AI and Beyond of\nthe University of Tokyo.\n9\nReferences\n[1] J. Borenstein and I. Ulrich. The guidecane-a computerized travel aid for the active guidance\nof blind pedestrians. In Proceedings of International Conference on Robotics and Automation,\nvolume 2, pages 1283\u20131288. IEEE, 1997.\n[2] T.-K. Chuang, N.-C. Lin, J.-S. Chen, C.-H. Hung, Y.-W. Huang, C. Teng, H. Huang, L.-F. Yu,\nL. Giarr\u00b4e, and H.-C. Wang. Deep trail-following robotic guide dog in pedestrian environments\nfor people who are blind and visually impaired-learning from virtual and real worlds. In 2018\nIEEE International Conference on Robotics and Automation (ICRA), pages 5849\u20135855. IEEE,\n2018.\n[3] K. Mehrizi. Quadrupedal robotic guide dog with vocal human-robot interaction. arXiv preprint\narXiv:2111.03718, 2021.\n[4] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,\nK. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder,\nP. F. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with\nhuman feedback. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh,\neditors, Advances in Neural Information Processing Systems, volume 35, pages 27730\u201327744.\nCurran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/\npaper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf.\n[5] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda,\nN. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv\npreprint arXiv:2107.03374, 2021.\n[6] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W.\nChung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv\npreprint arXiv:2204.02311, 2022.\n[7] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought\nprompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\n[8] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, K. Gopalakrishnan,\nK. Hausman, A. Herzog, et al. Do as i can, not as i say: Grounding language in robotic\naffordances. arXiv preprint arXiv:2204.01691, 2022.\n[9] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch,\nY. Chebotar, et al. Inner monologue: Embodied reasoning through planning with language\nmodels. arXiv preprint arXiv:2207.05608, 2022.\n[10] C. Lynch, A. Wahid, J. Tompson, T. Ding, J. Betker, R. Baruch, T. Armstrong, and P. Florence.\nInteractive language: Talking to robots in real time. arXiv preprint arXiv:2210.06407, 2022.\n[11] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng. Code as\npolicies: Language model programs for embodied control. arXiv preprint arXiv:2209.07753,\n2022.\n[12] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and\nA. Garg. Progprompt: Generating situated robot task plans using large language models. arXiv\npreprint arXiv:2209.11302, 2022.\n[13] S. Vemprala, R. Bonatti, A. Bucker, and A. Kapoor. Chatgpt for robotics: Design principles\nand model abilities. Microsoft Autonomous Systems and Robotics Research, 2023.\n[14] Unitree Robotics, 2023. https://unitreerobotics.net/.\n[15] S. Tellex, N. Gopalan, H. Kress-Gazit, and C. Matuszek. Robots that use language. Annual\nReview of Control, Robotics, and Autonomous Systems, 3:25\u201355, 2020.\n10\n[16] A. Zeng, A. Wong, S. Welker, K. Choromanski, F. Tombari, A. Purohit, M. Ryoo, V. Sindhwani,\nJ. Lee, V. Vanhoucke, et al. Socratic models: Composing zero-shot multimodal reasoning with\nlanguage. arXiv preprint arXiv:2204.00598, 2022.\n[17] H. Kress-Gazit, G. E. Fainekos, and G. J. Pappas. Translating structured english to robot\ncontrollers. Advanced Robotics, 22(12):1343\u20131359, 2008.\n[18] S. Stepputtis, J. Campbell, M. Phielipp, S. Lee, C. Baral, and H. Ben Amor. Language-\nconditioned imitation learning for robot manipulation tasks. Advances in Neural Information\nProcessing Systems, 33:13139\u201313150, 2020.\n[19] J. Y. Chai, Q. Gao, L. She, S. Yang, S. Saba-Sadiya, and G. Xu. Language to action: Towards\ninteractive task learning with physical agents. In IJCAI, pages 2\u20139, 2018.\n[20] T. M. Howard, S. Tellex, and N. Roy. A natural language planner interface for mobile manip-\nulators. In 2014 IEEE International Conference on Robotics and Automation (ICRA), pages\n6652\u20136659. IEEE, 2014.\n[21] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Haus-\nman, A. Herzog, J. Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv\npreprint arXiv:2212.06817, 2022.\n[22] O. Mees, L. Hermann, and W. Burgard. What matters in language conditioned robotic imitation\nlearning over unstructured data. IEEE Robotics and Automation Letters, 7(4):11205\u201311212,\n2022.\n[23] M. Shridhar, L. Manuelli, and D. Fox. Cliport: What and where pathways for robotic manipu-\nlation. In Conference on Robot Learning, pages 894\u2013906. PMLR, 2022.\n[24] K. Lin, C. Agia, T. Migimatsu, M. Pavone, and J. Bohg. Text2motion: From natural language\ninstructions to feasible plans. arXiv preprint arXiv:2303.12153, 2023.\n[25] A. Bucker, L. Figueredo, S. Haddadin, A. Kapoor, S. Ma, and R. Bonatti. Latte: Language\ntrajectory transformer. arXiv preprint arXiv:2208.02918, 2022.\n[26] J. Di Carlo, P. M. Wensing, B. Katz, G. Bledt, and S. Kim. Dynamic locomotion in the mit\ncheetah 3 through convex model-predictive control. In 2018 IEEE/RSJ international conference\non intelligent robots and systems (IROS), pages 1\u20139. IEEE, 2018.\n[27] R. Grandia, F. Farshidian, R. Ranftl, and M. Hutter. Feedback MPC for torque-controlled\nlegged robots. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS), pages 4730\u20134737. IEEE, 2019.\n[28] H. Li, T. Zhang, W. Yu, and P. M. Wensing. Versatile real-time motion synthesis via kino-\ndynamic mpc with hybrid-systems ddp. arXiv preprint arXiv:2209.14138, 2022.\n[29] O. Villarreal, V. Barasuol, P. M. Wensing, D. G. Caldwell, and C. Semini. Mpc-based controller\nwith terrain insight for dynamic legged locomotion. In 2020 IEEE International Conference\non Robotics and Automation (ICRA), pages 2436\u20132442. IEEE, 2020.\n[30] A. W. Winkler, D. C. Bellicoso, M. Hutter, and J. Buchli. Gait and trajectory optimization\nfor legged systems through phase-based end-effector parameterization. IEEE Robotics and\nAutomation Letters (RA-L), 3:1560\u20131567, July 2018. doi:10.1109/LRA.2018.2798285.\n[31] M. H. Raibert. Trotting, pacing and bounding by a quadruped robot. Journal of biomechanics,\n23:79\u201398, 1990.\n[32] P. Eckert, A. Spr\u00a8owitz, H. Witte, and A. J. Ijspeert. Comparing the effect of different spine and\nleg designs for a small bounding quadruped robot. In 2015 IEEE International Conference on\nRobotics and Automation (ICRA), pages 3128\u20133133. IEEE, 2015.\n11\n[33] D. W. Marhefka, D. E. Orin, J. P. Schmiedeler, and K. J. Waldron. Intelligent control of\nquadruped gallops. IEEE/ASME Transactions On Mechatronics, 8(4):446\u2013456, 2003.\n[34] Y. Yang, K. Caluwaerts, A. Iscen, T. Zhang, J. Tan, and V. Sindhwani. Data efficient rein-\nforcement learning for legged robots. In Conference on Robot Learning, pages 1\u201310. PMLR,\n2020.\n[35] G. B. Margolis, T. Chen, K. Paigwar, X. Fu, D. Kim, S. Kim, and P. Agrawal. Learning to jump\nfrom pixels. arXiv preprint arXiv:2110.15344, 2021.\n[36] X. Da, Z. Xie, D. Hoeller, B. Boots, A. Anandkumar, Y. Zhu, B. Babich, and A. Garg. Learning\na contact-adaptive controller for robust, efficient legged locomotion. In Conference on Robot\nLearning, pages 883\u2013894. PMLR, 2021.\n[37] J. Siekmann, Y. Godse, A. Fern, and J. Hurst. Sim-to-real learning of all common bipedal\ngaits via periodic reward composition. In 2021 IEEE International Conference on Robotics\nand Automation (ICRA), pages 7309\u20137315. IEEE, 2021.\n[38] A. Iscen, K. Caluwaerts, J. Tan, T. Zhang, E. Coumans, V. Sindhwani, and V. Vanhoucke.\nPolicies modulating trajectory generators. In Conference on Robot Learning, pages 916\u2013926.\nPMLR, 2018.\n[39] J. Hwangbo, J. Lee, A. Dosovitskiy, D. Bellicoso, V. Tsounis, V. Koltun, and M. Hutter.\nLearning agile and dynamic motor skills for legged robots. Science Robotics, 4(26):eaau5872,\n2019.\n[40] K. Caluwaerts, A. Iscen, J. C. Kew, W. Yu, T. Zhang, D. Freeman, K.-H. Lee, L. Lee, S. Saliceti,\nV. Zhuang, et al. Barkour: Benchmarking animal-level agility with quadruped robots. arXiv\npreprint arXiv:2305.14654, 2023.\n[41] G. B. Margolis and P. Agrawal. Walk these ways: Tuning robot control for generalization with\nmultiplicity of behavior. In Conference on Robot Learning, pages 22\u201331. PMLR, 2023.\n[42] N. Rudin, D. Hoeller, P. Reist, and M. Hutter. Learning to walk in minutes using massively\nparallel deep reinforcement learning. In Conference on Robot Learning, pages 91\u2013100. PMLR,\n2022.\n[43] Y. Tang, J. Tan, and T. Harada. Learning agile locomotion via adversarial training. In 2020\nIEEE/RSJ International Conference On Intelligent Robots And Systems (IROS), pages 6098\u2013\n6105. IEEE, 2020.\n[44] H. Yazied, S. Ariana, Villegas, C. Evan, C., F. Aleksandra, and T. Lydia. Enhancing value\nestimation policies by post-hoc symmetry exploitation in motion planning tasks.\nIn 2023\nIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2023.\n[45] OpenAI. GPT-4 technical report. arXiv, 2023.\n[46] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization\nalgorithms. arXiv preprint arXiv:1707.06347, 2017.\n[47] V. Makoviychuk, L. Wawrzyniak, Y. Guo, M. Lu, K. Storey, M. Macklin, D. Hoeller, N. Rudin,\nA. Allshire, A. Handa, et al. Isaac Gym: High performance GPU-based physics simulation for\nrobot learning. arXiv preprint arXiv:2108.10470, 2021.\n[48] A. Raffin, D. Seidel, J. Kober, A. Albu-Sch\u00a8affer, J. Silv\u00b4erio, and F. Stulp. Learning to exploit\nelastic actuators for quadruped locomotion. arXiv preprint arXiv:2209.07171, 2022.\n[49] G. Bellegarda and A. Ijspeert. Cpg-rl: Learning central pattern generators for quadruped\nlocomotion. IEEE Robotics and Automation Letters, 7(4):12547\u201312554, 2022.\n12\nA\nMore about Random Pattern Generator\nGiven a specified gait \ud835\udc3a, there are 4 steps for the random pattern generator to create a template,\nand Figure 7 illustrates the process when \ud835\udc3a = PACING. To acquire knowledge as to what ranges of\nsettings are feasible for the robot, we first train the robot in simulation for simple locomotion tasks\n(i.e., follow desired linear velocities). We then analyze the learned gait (the agents seem to learn\nexclusively trotting, probably because the reward design in those libraries) and measure the ranges.\nTo generate a template in general, we\n\u2022 [Step 1] Sample template length \ud835\udc47. In our implementation, \ud835\udc47 \u2208 [24, 28], since the control\nfrequency is 50 Hz, this corresponds to a cycle length of 0.48 \u223c 0.56 seconds.\n\u2022 [Step 2] Sample a foot-ground contact length ratio within the cycle \ud835\udc5fcontact \u2208 [0.5, 0.7].\n\ud835\udc47\ud835\udc5fcontact therefore gives the number of \u20181\u2019s and \ud835\udc47(1 \u2212 \ud835\udc5fcontact) the number of \u20180\u2019s in each\nrow.\n\u2022 [Step 3] Scale cycle length and ground contact ratio.\nThis only applies to \ud835\udc3a \u2208\n{BOUND, PACE} because these two gaits require shorter foot contact to make it natu-\nral and dynamically more feasible. For \ud835\udc3a = BOUND, we shorten the foot-ground contact\ntime to 60% of the sampled value (i.e., \ud835\udc5fcontact = 0.6\ud835\udc5fcontact); For \ud835\udc3a = PACE, we keep \ud835\udc5fcontact\nuntouched, but shrink the cycle length to half its sampled value (i.e., \ud835\udc47 = 0.5\ud835\udc47).\n\u2022 [Step 4] Shift patterns for corresponding legs. This step requires domain knowledge of\nquadrupedal locomotion and is gait type dependent. For example, for \ud835\udc3a = BOUND, we\nplace the ones at the beginning of the FL and FR rows and shift those in the RL and RR\nrows by 0.5\ud835\udc47\ud835\udc5fcontact bits to the right; For \ud835\udc3a = PACE, we place the ones at the beginning in\nthe FL and RL rows and at the end of the FR and RR rows.\n<latexit sha1_base64=\"PkVisEX4bL9\nEeqvEdAe+AFIsrc=\">AB6HicbZDJSgNBEIZr4hbjFpebl8YgeAozgsvNgAc9J\npANkiH0dGqSNj0L3T1CHPIEXjwo4tUH8OSTePom9hZDhr9oeHj/6voqvJiwZW27U\n8rs7C4tLySXc2trW9sbuW3d+oqSiTDGotEJseVSh4iDXNtcBmLJEGnsCGN7gc541\nblIpHYVUPY3QD2gu5zxnVxqpUO/mCXbQnIn/BmUHh4v3u6+ptLy138h/tbsSAEP\nNBFWq5dixdlMqNWcCR7l2ojCmbEB72DIY0gCVm04GHZFD43SJH0nzQk0m7s+OlAZK\nDQPVAZU9V8Njb/y1qJ9s/dlIdxojFk04/8RBAdkfHWpMslMi2GBiT3MxKWJ9Ky\nrS5Tc4cwZlf+S/Uj4vOafGk4hRKNkyVhX04gCNw4AxKcA1lqAEDhHt4hCfrxnqwn\nq2XaWnGmvXswi9Zr9/RsJCf</latexit>T\n<latexit sha1_base64=\"PkVisEX4bL9\nEeqvEdAe+AFIsrc=\">AB6HicbZDJSgNBEIZr4hbjFpebl8YgeAozgsvNgAc9J\npANkiH0dGqSNj0L3T1CHPIEXjwo4tUH8OSTePom9hZDhr9oeHj/6voqvJiwZW27U\n8rs7C4tLySXc2trW9sbuW3d+oqSiTDGotEJseVSh4iDXNtcBmLJEGnsCGN7gc541\nblIpHYVUPY3QD2gu5zxnVxqpUO/mCXbQnIn/BmUHh4v3u6+ptLy138h/tbsSAEP\nNBFWq5dixdlMqNWcCR7l2ojCmbEB72DIY0gCVm04GHZFD43SJH0nzQk0m7s+OlAZK\nDQPVAZU9V8Njb/y1qJ9s/dlIdxojFk04/8RBAdkfHWpMslMi2GBiT3MxKWJ9Ky\nrS5Tc4cwZlf+S/Uj4vOafGk4hRKNkyVhX04gCNw4AxKcA1lqAEDhHt4hCfrxnqwn\nq2XaWnGmvXswi9Zr9/RsJCf</latexit>T\nFL\nFR\nRL\nRR\nStep 1: Sample template length \n<latexit sha1_base64=\"PkVisEX4bL9\nEeqvEdAe+AFIsrc=\">AB6HicbZDJSgNBEIZr4hbjFpebl8YgeAozgsvNgAc9J\npANkiH0dGqSNj0L3T1CHPIEXjwo4tUH8OSTePom9hZDhr9oeHj/6voqvJiwZW27U\n8rs7C4tLySXc2trW9sbuW3d+oqSiTDGotEJseVSh4iDXNtcBmLJEGnsCGN7gc541\nblIpHYVUPY3QD2gu5zxnVxqpUO/mCXbQnIn/BmUHh4v3u6+ptLy138h/tbsSAEP\nNBFWq5dixdlMqNWcCR7l2ojCmbEB72DIY0gCVm04GHZFD43SJH0nzQk0m7s+OlAZK\nDQPVAZU9V8Njb/y1qJ9s/dlIdxojFk04/8RBAdkfHWpMslMi2GBiT3MxKWJ9Ky\nrS5Tc4cwZlf+S/Uj4vOafGk4hRKNkyVhX04gCNw4AxKcA1lqAEDhHt4hCfrxnqwn\nq2XaWnGmvXswi9Zr9/RsJCf</latexit>T\n<latexit sha1_base64=\"PkVisEX4bL9\nEeqvEdAe+AFIsrc=\">AB6HicbZDJSgNBEIZr4hbjFpebl8YgeAozgsvNgAc9J\npANkiH0dGqSNj0L3T1CHPIEXjwo4tUH8OSTePom9hZDhr9oeHj/6voqvJiwZW27U\n8rs7C4tLySXc2trW9sbuW3d+oqSiTDGotEJseVSh4iDXNtcBmLJEGnsCGN7gc541\nblIpHYVUPY3QD2gu5zxnVxqpUO/mCXbQnIn/BmUHh4v3u6+ptLy138h/tbsSAEP\nNBFWq5dixdlMqNWcCR7l2ojCmbEB72DIY0gCVm04GHZFD43SJH0nzQk0m7s+OlAZK\nDQPVAZU9V8Njb/y1qJ9s/dlIdxojFk04/8RBAdkfHWpMslMi2GBiT3MxKWJ9Ky\nrS5Tc4cwZlf+S/Uj4vOafGk4hRKNkyVhX04gCNw4AxKcA1lqAEDhHt4hCfrxnqwn\nq2XaWnGmvXswi9Zr9/RsJCf</latexit>T\nStep 2: Sample contact ratio \n<latexit sha1_base64=\"tdhlaczG82s+8iMFemKaWn6xKmY=\">AB+3icbVDLS\ngMxFM1YH7W+xorN8EiuCozgo9lwY3LCvYBbRkyaYNzWNIMmIZ5lfcuFDErR/gL7gQXPkpm70NYDgcM593JPThgzqo3nfTpLheWV1bXiemljc2t7x90tN7VMFCYNLJ\nlU7RBpwqgDUMNI+1YEcRDRlrh6DL3W7dEaSrFjRnHpMfRQNCIYmSsFLhlFXQ5MkPFUyFQdhkgVvxqt4EcJH4M1KpFT6+3/a/SD1w37t9iRNOhMEMad3xvdj0UqQMxYxk\npW6iSYzwCA1Ix1KBONG9dJI9g0dW6cNIKvuEgRP190aKuNZjHtrJPKe93LxP6+TmOil1IRJ4YIPD0UJQwaCfMiYJ8qg0bW4KwojYrxEOkbAO2rpItwZ/8iJpnlT9s\n+rptV+peWCKIjgAh+AY+OAc1MAVqIMGwOAO3INH8ORkzoPz7LxMR5ec2c4e+APn9QeDb5kg</latexit>rcontact\n<latexit sha1_base64=\"tdhlaczG82s+8iMFemKaWn6xKmY=\">AB+3icbVDLS\ngMxFM1YH7W+xorN8EiuCozgo9lwY3LCvYBbRkyaYNzWNIMmIZ5lfcuFDErR/gL7gQXPkpm70NYDgcM593JPThgzqo3nfTpLheWV1bXiemljc2t7x90tN7VMFCYNLJ\nlU7RBpwqgDUMNI+1YEcRDRlrh6DL3W7dEaSrFjRnHpMfRQNCIYmSsFLhlFXQ5MkPFUyFQdhkgVvxqt4EcJH4M1KpFT6+3/a/SD1w37t9iRNOhMEMad3xvdj0UqQMxYxk\npW6iSYzwCA1Ix1KBONG9dJI9g0dW6cNIKvuEgRP190aKuNZjHtrJPKe93LxP6+TmOil1IRJ4YIPD0UJQwaCfMiYJ8qg0bW4KwojYrxEOkbAO2rpItwZ/8iJpnlT9s\n+rptV+peWCKIjgAh+AY+OAc1MAVqIMGwOAO3INH8ORkzoPz7LxMR5ec2c4e+APn9QeDb5kg</latexit>rcontact\n<latexit sha1_base64=\"D+Ltlul0/rK\nSRmO3BTet7o6yWnk=\">AB/HicbVDLSgMxFM20Pmp9jRZXboJFcFVmB/LghuXF\nfqCdhgyaYNTJDkhGof6KGxeKuHXvL7gQXPkpm70NYDgcM593JPThAzqrTjfF\nqF4srq2npo7y5tb2za+/t1WUSExaOGKR7AZIEUYFaWmqGenGkiAeMNIJxle537k\nlUtFINHUaE4+joaAhxUgbybcrTen3OdIjyTMcCY2wnvh21ak5U8Bl4s5JtV78+H4\n7+CIN37vDyKcCI0ZkipnuvE2suQ1BQzMin3E0VihMdoSHqGCsSJ8rJp+Ak8NsoA\nhpE0T2g4VX9vZIgrlfLATOY51aKXi/95vUSHl15GRZxoIvDsUJgwqCOYNwEHVBKsW\nWoIwpKarBCPkDQNmL7KpgR38cvLpH1ac89rZzdute6AGUrgEByBE+C1AH16ABW\ngCDFNyDR/Bk3VkP1rP1MhstWPOdCvgD6/UHLduZfg=</latexit>Trcontact\n<latexit sha1_base64=\"D+Ltlul0/rK\nSRmO3BTet7o6yWnk=\">AB/HicbVDLSgMxFM20Pmp9jRZXboJFcFVmB/LghuXF\nfqCdhgyaYNTJDkhGof6KGxeKuHXvL7gQXPkpm70NYDgcM593JPThAzqrTjfF\nqF4srq2npo7y5tb2za+/t1WUSExaOGKR7AZIEUYFaWmqGenGkiAeMNIJxle537k\nlUtFINHUaE4+joaAhxUgbybcrTen3OdIjyTMcCY2wnvh21ak5U8Bl4s5JtV78+H4\n7+CIN37vDyKcCI0ZkipnuvE2suQ1BQzMin3E0VihMdoSHqGCsSJ8rJp+Ak8NsoA\nhpE0T2g4VX9vZIgrlfLATOY51aKXi/95vUSHl15GRZxoIvDsUJgwqCOYNwEHVBKsW\nWoIwpKarBCPkDQNmL7KpgR38cvLpH1ac89rZzdute6AGUrgEByBE+C1AH16ABW\ngCDFNyDR/Bk3VkP1rP1MhstWPOdCvgD6/UHLduZfg=</latexit>Trcontact\n\u20181\u2019s\n\u20180\u2019s\nStep 3: Scale \n<latexit sha1_base64=\"PkVisEX4bL9EeqvEdAe+AFIsrc=\">AB6HicbZDJS\ngNBEIZr4hbjFpebl8YgeAozgsvNgAc9JpANkiH0dGqSNj0L3T1CHPIEXjwo4tUH8OSTePom9hZDhr9oeHj/6voqvJiwZW27U8rs7C4tLySXc2trW9sbuW3d+oqSiTDGo\ntEJseVSh4iDXNtcBmLJEGnsCGN7gc541blIpHYVUPY3QD2gu5zxnVxqpUO/mCXbQnIn/BmUHh4v3u6+ptLy138h/tbsSAEPNBFWq5dixdlMqNWcCR7l2ojCmbEB72DIY\n0gCVm04GHZFD43SJH0nzQk0m7s+OlAZKDQPVAZU9V8Njb/y1qJ9s/dlIdxojFk04/8RBAdkfHWpMslMi2GBiT3MxKWJ9KyrS5Tc4cwZlf+S/Uj4vOafGk4hRKNkyVh\nX04gCNw4AxKcA1lqAEDhHt4hCfrxnqwnq2XaWnGmvXswi9Zr9/RsJCf</latexit>T\n<latexit sha1_base64=\"PkVisEX4bL9EeqvEdAe+AFIsrc=\">AB6HicbZDJS\ngNBEIZr4hbjFpebl8YgeAozgsvNgAc9JpANkiH0dGqSNj0L3T1CHPIEXjwo4tUH8OSTePom9hZDhr9oeHj/6voqvJiwZW27U8rs7C4tLySXc2trW9sbuW3d+oqSiTDGo\ntEJseVSh4iDXNtcBmLJEGnsCGN7gc541blIpHYVUPY3QD2gu5zxnVxqpUO/mCXbQnIn/BmUHh4v3u6+ptLy138h/tbsSAEPNBFWq5dixdlMqNWcCR7l2ojCmbEB72DIY\n0gCVm04GHZFD43SJH0nzQk0m7s+OlAZKDQPVAZU9V8Njb/y1qJ9s/dlIdxojFk04/8RBAdkfHWpMslMi2GBiT3MxKWJ9KyrS5Tc4cwZlf+S/Uj4vOafGk4hRKNkyVh\nX04gCNw4AxKcA1lqAEDhHt4hCfrxnqwnq2XaWnGmvXswi9Zr9/RsJCf</latexit>T  or \n<latexit sha1_base64=\"tdhlaczG82s+8iMFemKaWn6xKmY=\">AB+3icbVDLS\ngMxFM1YH7W+xorN8EiuCozgo9lwY3LCvYBbRkyaYNzWNIMmIZ5lfcuFDErR/gL7gQXPkpm70NYDgcM593JPThgzqo3nfTpLheWV1bXiemljc2t7x90tN7VMFCYNLJ\nlU7RBpwqgDUMNI+1YEcRDRlrh6DL3W7dEaSrFjRnHpMfRQNCIYmSsFLhlFXQ5MkPFUyFQdhkgVvxqt4EcJH4M1KpFT6+3/a/SD1w37t9iRNOhMEMad3xvdj0UqQMxYxk\npW6iSYzwCA1Ix1KBONG9dJI9g0dW6cNIKvuEgRP190aKuNZjHtrJPKe93LxP6+TmOil1IRJ4YIPD0UJQwaCfMiYJ8qg0bW4KwojYrxEOkbAO2rpItwZ/8iJpnlT9s\n+rptV+peWCKIjgAh+AY+OAc1MAVqIMGwOAO3INH8ORkzoPz7LxMR5ec2c4e+APn9QeDb5kg</latexit>rcontact\n<latexit sha1_base64=\"tdhlaczG82s+8iMFemKaWn6xKmY=\">AB+3icbVDLS\ngMxFM1YH7W+xorN8EiuCozgo9lwY3LCvYBbRkyaYNzWNIMmIZ5lfcuFDErR/gL7gQXPkpm70NYDgcM593JPThgzqo3nfTpLheWV1bXiemljc2t7x90tN7VMFCYNLJ\nlU7RBpwqgDUMNI+1YEcRDRlrh6DL3W7dEaSrFjRnHpMfRQNCIYmSsFLhlFXQ5MkPFUyFQdhkgVvxqt4EcJH4M1KpFT6+3/a/SD1w37t9iRNOhMEMad3xvdj0UqQMxYxk\npW6iSYzwCA1Ix1KBONG9dJI9g0dW6cNIKvuEgRP190aKuNZjHtrJPKe93LxP6+TmOil1IRJ4YIPD0UJQwaCfMiYJ8qg0bW4KwojYrxEOkbAO2rpItwZ/8iJpnlT9s\n+rptV+peWCKIjgAh+AY+OAc1MAVqIMGwOAO3INH8ORkzoPz7LxMR5ec2c4e+APn9QeDb5kg</latexit>rcontact if necessary\nScaled \n<latexit sha1_base64=\"PkVisEX4bL9EeqvEdAe+AFIsrc=\">AB6HicbZDJS\ngNBEIZr4hbjFpebl8YgeAozgsvNgAc9JpANkiH0dGqSNj0L3T1CHPIEXjwo4tUH8OSTePom9hZDhr9oeHj/6voqvJiwZW27U8rs7C4tLySXc2trW9sbuW3d+oqSiTDGo\ntEJseVSh4iDXNtcBmLJEGnsCGN7gc541blIpHYVUPY3QD2gu5zxnVxqpUO/mCXbQnIn/BmUHh4v3u6+ptLy138h/tbsSAEPNBFWq5dixdlMqNWcCR7l2ojCmbEB72DIY\n0gCVm04GHZFD43SJH0nzQk0m7s+OlAZKDQPVAZU9V8Njb/y1qJ9s/dlIdxojFk04/8RBAdkfHWpMslMi2GBiT3MxKWJ9KyrS5Tc4cwZlf+S/Uj4vOafGk4hRKNkyVh\nX04gCNw4AxKcA1lqAEDhHt4hCfrxnqwnq2XaWnGmvXswi9Zr9/RsJCf</latexit>T\n<latexit sha1_base64=\"PkVisEX4bL9EeqvEdAe+AFIsrc=\">AB6HicbZDJS\ngNBEIZr4hbjFpebl8YgeAozgsvNgAc9JpANkiH0dGqSNj0L3T1CHPIEXjwo4tUH8OSTePom9hZDhr9oeHj/6voqvJiwZW27U8rs7C4tLySXc2trW9sbuW3d+oqSiTDGo\ntEJseVSh4iDXNtcBmLJEGnsCGN7gc541blIpHYVUPY3QD2gu5zxnVxqpUO/mCXbQnIn/BmUHh4v3u6+ptLy138h/tbsSAEPNBFWq5dixdlMqNWcCR7l2ojCmbEB72DIY\n0gCVm04GHZFD43SJH0nzQk0m7s+OlAZKDQPVAZU9V8Njb/y1qJ9s/dlIdxojFk04/8RBAdkfHWpMslMi2GBiT3MxKWJ9KyrS5Tc4cwZlf+S/Uj4vOafGk4hRKNkyVh\nX04gCNw4AxKcA1lqAEDhHt4hCfrxnqwnq2XaWnGmvXswi9Zr9/RsJCf</latexit>T\nStep 4: Shifts\n<latexit sha1_base64=\"zqE2OeSHl+17jGOGxRyMpYTs3Y=\">AB+nicbVDJS\ngNBEO2JW4zbRL15GQyCpzAjuFzESA7Ri0QwCyRD6Ol0kiY9PUN3jRrHfIoXD4oInvIl3jz6J3aWgyY+KHi8V0VPS/kTIFtfxmJufmFxaXkcmpldW19w0xvlUQSUJLJO\nCBrHpYUc4ELQEDTquhpNj3OK143fzQr9xSqVgbqAXUtfHbcFajGDQUsNMF07rQO9Bkbh4nr+8KvQbZsbO2iNYs8SZkMzZ4OG78LEdFxvmZ70ZkMinAgjHStUcOwQ3xhIY\n4bSfqkeKhph0cZvWNBXYp8qNR6f3rT2tNK1WIHUJsEbq74kY+0r1fE93+hg6atobiv95tQhaJ27MRBgBFWS8qBVxCwJrmIPVZJIS4D1NMJFM32qRDpaYgE4rpUNwpl+eJ\neWDrHOUPbx2MjkbjZFEO2gX7SMHaMcukBFVEIE3aEn9IJejUfj2Xgz3setCWMys4X+wBj8AJwVl0E=</latexit>G = PACING\n<latexit sha1_base64=\"zqE2OeSHl+17jGOGxRyMpYTs3Y=\">AB+nicbVDJS\ngNBEO2JW4zbRL15GQyCpzAjuFzESA7Ri0QwCyRD6Ol0kiY9PUN3jRrHfIoXD4oInvIl3jz6J3aWgyY+KHi8V0VPS/kTIFtfxmJufmFxaXkcmpldW19w0xvlUQSUJLJO\nCBrHpYUc4ELQEDTquhpNj3OK143fzQr9xSqVgbqAXUtfHbcFajGDQUsNMF07rQO9Bkbh4nr+8KvQbZsbO2iNYs8SZkMzZ4OG78LEdFxvmZ70ZkMinAgjHStUcOwQ3xhIY\n4bSfqkeKhph0cZvWNBXYp8qNR6f3rT2tNK1WIHUJsEbq74kY+0r1fE93+hg6atobiv95tQhaJ27MRBgBFWS8qBVxCwJrmIPVZJIS4D1NMJFM32qRDpaYgE4rpUNwpl+eJ\neWDrHOUPbx2MjkbjZFEO2gX7SMHaMcukBFVEIE3aEn9IJejUfj2Xgz3setCWMys4X+wBj8AJwVl0E=</latexit>G = PACING\nFigure 7: How the random pattern generator works.\nB\nReward Design\nOur reward design is based on those in legged gym [42]. The total reward consists of 8 weighted\nreward terms: \ud835\udc3d = \u00cd8\n\ud835\udc56=1 \ud835\udc64\ud835\udc56\ud835\udc5f\ud835\udc56, where \ud835\udc64\ud835\udc56\u2019s are the weights and \ud835\udc5f\ud835\udc56\u2019s are the rewards. The definition\nof each reward term and the value of the weights are in the following. We put the purpose of each\nreward term in the bracket at the beginning of the description.\n\u2022 [Task Reward] Linear velocity tracking reward. \ud835\udc5f1 = \ud835\udc52\u22124\u00d7((\ud835\udc63\ud835\udc65\u2212 \u02c6\ud835\udc63\ud835\udc65 )2+\ud835\udc632\n\ud835\udc66 ), where \ud835\udc63\ud835\udc65 and \u02c6\ud835\udc63\ud835\udc65\nare the current and desired linear velocities along the robot\u2019s heading direction, and \ud835\udc63\ud835\udc66 is\nthe current linear velocity along the lateral direction. All velocities are in the base frame,\nand \ud835\udc641 = 1.\n\u2022 [Task Reward] Angular velocity tracking reward. \ud835\udc5f2 = \ud835\udc52\u22124\u00d7\ud835\udf142\n\ud835\udc67, where \ud835\udf14\ud835\udc67 is the current\nangular yaw velocity in the base frame and \ud835\udc642 = \u22120.5.\n\u2022 [Task Reward] Penalty on foot contact pattern violation.\n\ud835\udc5f3 =\n1\n4\n\u00cd4\n\ud835\udc56=1 |\ud835\udc50\ud835\udc56 \u2212 \u02c6\ud835\udc50\ud835\udc56|, where\n\ud835\udc50\ud835\udc56, \u02c6\ud835\udc50\ud835\udc56 \u2208 {0, 1} are the realized and desired foot-ground contact indicators for the \ud835\udc56-th foot,\nand \ud835\udc643 = \u22121.\n\u2022 [Sim-to-Real] Regularization on action rate. \ud835\udc5f4 = \u00cd12\n\ud835\udc56=1(\ud835\udc4e\ud835\udc61 \u2212 \ud835\udc4e\ud835\udc61\u22121)2 where \ud835\udc4e\ud835\udc61 and \ud835\udc4e\ud835\udc61\u22121 are\nthe controller\u2019s output at the current and the previous time steps, and \ud835\udc644 = \u22120.005.\n13\n\u2022 [Sim-to-Real] Penalty on roll and pitch angular velocities. We encourage the robot\u2019s base\nto be stable during motion and hence \ud835\udc5f5 = \ud835\udf142\n\ud835\udc65 + \ud835\udf142\n\ud835\udc66, where \ud835\udf14\ud835\udc65 and \ud835\udf14\ud835\udc66 are the current roll\nand pitch angular velocities in the base frame. This penalty does not apply to \ud835\udc3a = BOUND\nand \ud835\udc645 = \u22120.05.\n\u2022 [Sim-to-Real] Penalty on linear velocity along the z-axis. Similar to the previous term,\nwe use this term to encourage the base stability during motion. \ud835\udc5f6 = \ud835\udc632\n\ud835\udc67 where \ud835\udc63\ud835\udc67 is the\ncurrent linear velocity along the z-axis in the base frame. This penalty does not apply to\n\ud835\udc3a = BOUND either and \ud835\udc646 = \u22122.\n\u2022 [Natural Motion] Penalty on body collision. \ud835\udc5f7 = \u00cd\ud835\udc3e\n\ud835\udc56=1 1{\ud835\udc39\ud835\udc56 > 0.1}, where \ud835\udc39\ud835\udc56 is the contact\nforce on the \ud835\udc56-th body. In our experiments \ud835\udc3e = 8 (i.e., 4 thighs and 4 calves) and \ud835\udc647 = \u22121.\n\u2022 [Natural Motion] Penalty on deviation from the default pose. \ud835\udc5f8 = \u00cd\n\ud835\udc4e\ud835\udc61 \u2208hip |\ud835\udc4e\ud835\udc61 |, where \ud835\udc4e\ud835\udc61\u2019s\nare the actions (i.e., deviation from the default joint position) applied to the hip joints, and\n\ud835\udc648 = \u22120.03.\nC\nTraining Configurations\nC.1\nControl\nWe use PD control to convert positions to torques in our system. The bases value for the 2 gains are\n\ud835\udc58 \ud835\udc5d = 20 and \ud835\udc58\ud835\udc51 = 0.5. Our control frequency is 50 Hz.\nC.2\nGait Sampling\nWe randomly assign a gait \ud835\udc3a to a robot at environment resets, and also samples it again every 150\nsteps in simulation. Of the 5 \ud835\udc3a\u2019s, some gaits are harder to learn than others. To avoid the case where\nthe hard-to-learn gaits die out, leaving the controller to learn only on the easier gaits, we restrict the\nsampling distribution such that the ratio of the 5 \ud835\udc3a\u2019s are always approximately the same.\nC.3\nReinforcement Learning\nWe use the Proximal policy optimization (PPO) [46] algorithm as our reinforcement learning method\nto train the controller. In our experiments, PPO trains an actor-critic policy. The architecture of the\nactor is introduced in Section 3.2.3, and the critic has the identical network architecture except that\n(1) its output size is 1 instead of 12, and (2) it also receives the base velocities in the local frame as its\ninput. We keep all the hyper-parameters the same as in [42] and train for 1000 iterations. For safety\nreasons, we end an episode early if the body height of the robot is lower than 0.25 meters. Training\ncan be done on a singe NVIDIA V100 GPU in approximately 15 minutes.\nC.4\nDomain Randomization\nDuring training, we sample noises \ud835\udf16 \u223c Unif, and add them to the controller\u2019s observations. We\nuse PD control to convert positions to torques in our system, and domain randomization is also\napplied to the 2 gains \ud835\udc58 \ud835\udc5d and \ud835\udc58\ud835\udc51. Table 3 gives the components where noises \ud835\udf16 were added and their\ncorresponding ranges.\nD\nObjective Evaluation on Generated Patterns\nWe implemented a domain knowledge based check program for each of the commands in Table 1,\nand evaluated the generated patterns with these checkers to produce Figure 5. By domain knowledge,\nwe mean knowledge about quadrupedal locomotion as to what each gait pattern should look like\n(e.g., the robot should move its diagonal legs together when trotting, while in pacing gait the robot\nshould move legs on the left/right side of the body together, etc).\n14\nTable 3: Domain randomization settings.\n#\nComponent\nNoise Range\n1\nBase linear velocities\n[\u22122, 2]\n2\nBase angular velocities\n[\u22120.25, 0.25]\n3\nGravity vector in the base frame\n[\u22121, 1]\n4\nJoint positions\n[\u22121, 1]\n5\nJoint velocities\n[\u22120.05, 0.05]\n6\n\ud835\udc58 \ud835\udc5d\n[\u22125, 0]\n7\n\ud835\udc58\ud835\udc51\n[0, 0.25]\n15\n"
  },
  {
    "title": "Neural Scene Chronology",
    "link": "https://arxiv.org/pdf/2306.07970.pdf",
    "upvote": "6",
    "text": "Neural Scene Chronology\nHaotong Lin1,2\u2217\nQianqian Wang2\nRuojin Cai2\nSida Peng1\nHadar Averbuch-Elor3\nXiaowei Zhou1\u2020\nNoah Snavely2\n1Zhejiang University\n2Cornell University\n3Tel Aviv University\nOctober 2013\nAugust 2013\n(a) Internet photos with timestamps\n(b) 4D reconstruction across time\n2011\n2012\n2013\nApril 2012\nMarch 2011\n(d) Same content under varying illumination\nMarch 2012\n2011\n2012\n2013\n(c) Detailed view of content changes over time\nFigure 1. Chronology reconstruction. Given timestamped Internet photos (a) of a landmark that has changed significantly over the years\n(e.g., 5Pointz, NYC, the collective graffiti art project shown above), our method can reconstruct a time-varying 3D model, and render\nphoto-realistic images (b) with independent control of viewpoint, time (c) and illumination (d). Photos by Flickr users Ryan Brown,\nDaniGMX, DJ Leekee, Diff Graff, Lee Smith, James Prochnik, Verity Rollins Photo under CC BY.\nAbstract\nIn this work, we aim to reconstruct a time-varying 3D\nmodel, capable of rendering photo-realistic renderings with\nindependent control of viewpoint, illumination, and time,\nfrom Internet photos of large-scale landmarks. The core\nchallenges are twofold. First, different types of temporal\nchanges, such as illumination and changes to the under-\nlying scene itself (such as replacing one graffiti artwork\nwith another) are entangled together in the imagery. Sec-\nond, scene-level temporal changes are often discrete and\nsporadic over time, rather than continuous. To tackle these\nproblems, we propose a new scene representation equipped\nwith a novel temporal step function encoding method that\ncan model discrete scene-level content changes as piece-\nwise constant functions over time. Specifically, we represent\nthe scene as a space-time radiance field with a per-image\nThe authors from Zhejiang University are affiliated with the State Key\nLab of CAD&CG. \u2217This work was done when Haotong Lin was in a remote\ninternship at Cornell University. \u2020Corresponding author: Xiaowei Zhou.\nillumination embedding, where temporally-varying scene\nchanges are encoded using a set of learned step functions.\nTo facilitate our task of chronology reconstruction from In-\nternet imagery, we also collect a new dataset of four scenes\nthat exhibit various changes over time. We demonstrate that\nour method exhibits state-of-the-art view synthesis results\non this dataset, while achieving independent control of view-\npoint, time, and illumination. Code and data are available\nat https://zju3dv.github.io/neusc/.\n1. Introduction\nIf we revisit a space we once knew during our childhood,\nit might not be as we remembered it. The buildings may\nhave weathered, or have been newly painted, or may have\nbeen replaced entirely. Accordingly, there is no such thing\nas a single, authoritative 3D model of a scene\u2014only a model\nof how it existed at a given instant in time. For a famous\nlandmark, Internet photos can serve as a kind of chronicle\nof that landmark\u2019s state over time, if we could organize the\narXiv:2306.07970v1  [cs.CV]  13 Jun 2023\ninformation in those photos in a coherent way. For instance,\nif we could reconstruct a time-varying 3D model, then we\ncould revisit the scene at any desired point in time.\nIn this work, we explore this problem of chronology re-\nconstruction, revisiting the work on Scene Chronology from\nnearly a decade ago [30]. As in that work, we seek to use In-\nternet photos to build a 4D model of a scene, from which we\ncan dial in any desired time (within the time interval where\nwe have photos). However, the original Scene Chronol-\nogy work was confined to reconstructing planar, rectangular\nscene elements, leading to limited photo-realism. We can\nnow revisit this problem with powerful neural scene represen-\ntations, inspired by methods such as NeRF in the Wild [29].\nHowever, recent neural reconstruction methods designed for\nInternet photos assume that the underlying scene is static,\nwhich works well for landmarks with a high degree of per-\nmanence, but fails for other scenes, like New York\u2019s Times\nSquare, that feature more ephemeral elements like billboards\nand advertisements.\nHowever, we find that adapting neural reconstruction\nmethods [29] to the chronology reconstruction problem has\nmany challenges, and that straightforward extensions do\nnot work well. For instance, augmenting a neural radiance\nfield (NeRF) model with an additional time input t, and fit-\nting the resulting 4D radiance field to a set of images with\ntimestamps yields temporally oversmoothed models, where\ndifferent scene appearances over time are blended together,\nforming ghosted content; such a model underfits the tempo-\nral signal. On the other hand, applying standard positional\nencoding [34] to the time input overfits the temporal sig-\nnal, conflating transient appearance changes due to factors\nlike illumination with longer-term, sporadic changes to the\nunderlying scene itself.\nInstead, we seek a model that can disentangle transient,\nper-image changes from longer-term, scene-level changes,\nand that allows for independent control of viewpoint, time,\nand illumination at render-time. Based on the observation\nthat scene-level content changes are often sudden, abrupt\n\u201cstep function\u201d-like changes (e.g., a billboard changing from\none advertisement to another), we introduce a novel encod-\ning method for time inputs that can effectively model piece-\nwise constant scene content over time, and pair this method\nwith a per-image illumination code that models transient\nappearance changes. Accordingly, we represent 4D scene\ncontent as a multi-layer perceptron (MLP) that stores density\nand radiance at each space-time (x, y, z, t) scene point, and\ntakes an illumination code as a side input. The time input\nt to this MLP is encoded with our proposed step function\nencoding that models piecewise constant temporal changes.\nWhen fit to a set of input images, we find that our representa-\ntion can effectively factor different kinds of temporal effects,\nand can produce high-quality renderings of scenes over time.\nTo evaluate our method, we collect a dataset of images\nfrom Flickr and calibrate them using COLMAP, resulting\nin 52K successfully registered images. These photos are\nsourced from four different scenes, including dense tourist ar-\neas, graffiti meccas, and museums, building upon the datasets\nused in Scene Chronology. These scenes feature a variety of\nelements that change over time, including billboards, graffiti\nart, and banners. Experiments on these scenes show that\nour method outperforms current state-of-the-art methods and\ntheir extensions to space-time view synthesis [6, 29]. We\nalso present a detailed ablation and analysis of our proposed\ntime encoding method.\nIn summary, our work makes the following contributions:\n\u2022 To the best of our knowledge, ours is the first work\nto achieve photo-realistic chronology reconstruction,\nallowing for high-quality renderings of scenes with\ncontrollable viewpoint, time, and illumination.\n\u2022 We propose a novel encoding method that can model\nabrupt content changes without overfitting to transient\nfactors. This leads to a fitting procedure that can ef-\nfectively disentangle illumination effects from content\nchanges in the underlying scene.\n\u2022 We benchmark the task of chronology reconstruction\nfrom Internet photos and make our dataset and code\navailable to the research community.\n2. Related Work\n3D/4D reconstruction from Internet photos. The typi-\ncal 3D reconstruction pipeline for Internet photos involves\nfirst recovering camera poses and a sparse point cloud using\nStructure from Motion (SfM) methods [1, 44, 46, 51, 52],\nthen computing a dense reconstruction using Multi-View\nStereo (MVS) algorithms [10, 11, 13, 47]. However, these\nmethods assume the scene to be largely static, and are un-\nable to produce coherent models for scenes with large-scale\nappearance changes over time. To extend these methods to\nachieve 4D reconstruction, Schindler and Dellaert developed\na method that takes photos of a city over time, and reasons\nprobabilistically about visibility and existence of objects like\nbuildings that may come and go across decades [45]. Most\nrelated to our work, Scene Chronology extends MVS meth-\nods [48] to 4D by clustering reconstructed 3D points into\nspace-time cuboids [30]. However, it can only reconstruct\nand render planar regions, leading to limited photo-realism.\nTo handle more complex geometry, Martin-Brualla et al.\nrepresent scene geometry using time-varying depth maps,\nallowing their method to generate high-quality time-lapse\nvideos [27, 28]. However, this depth map\u2013based represen-\ntation limits the range of camera viewpoints their method\ncan synthesize. In our work, we tackle these challenges and\ndevise a new method that can handle large-scale scenes with\ncomplex geometry, and can generate large camera motions.\nNovel view synthesis. Early methods achieve novel view\nsynthesis through light field interpolation [7,14,18] or image-\nbased rendering [4,9,17,60]. Recently, neural scene represen-\ntations [31,33,35,36,50,55,57] have shown unprecedented\nview synthesis quality. Of particular interest is NeRF [34],\nwhich represents radiance fields using a multi-layer percep-\ntron (MLP) and achieves impressive rendering results. Many\nworks [8, 12, 19, 20, 22, 24, 37\u201340, 43, 56, 58, 59] extend\nNeRF to model dynamic scenes with moving objects given\na monocular or multi-view video as input. In our work, we\nfocus on a different type of 4D view synthesis problem that\ninvolves modeling unstructured Internet photo collections\ncapturing scenes that exhibit substantial appearance changes\nover time.\nNeural rendering from Internet photos. One challenge of\nrendering from Internet photos is handling varying, unknown\nillumination present in the image collection. Recently, sev-\neral neural rendering methods demonstrate promising results\non rendering static landmarks while allowing for control of\nillumination effects [23, 32]. In particular, NeRF-W [29]\nconditions a reconstructed neural radiance field on a learn-\nable per-image illumination vector, thereby factoring out\nper-image illumination effects. Chen et al. [6] propose a\nCNN module for predicting an illumination vector from an\nimage, enabling transfer of illumination from unseen im-\nages to the model. Sun et al. [53] build on NeRF-W to\nreconstruct 3D meshes from a collection of Internet photos.\nFinally, Zhao and Yang et al. [2] and Rudnev et al. [42] en-\nable outdoor scene relighting based on neural radiance fields.\nHowever, these methods are limited to primarily static land-\nmarks like the Brandenburg Gate, and cannot handle scenes\nwith substantial changes over time like Times Square.\nModeling temporal signals. One useful type of data for\nmodeling temporal signals is time-lapse videos from station-\nary cameras, which provide organized visual information for\nscene understanding and factorization. Many previous meth-\nods [21,25,54] show how to factor temporally-varying fac-\ntors (e.g., illumination) from permanent scene factors (e.g.,\ngeometry and reflectance) from time-lapse videos. More re-\ncently, [16] introduces a method that disentangles time-lapse\nsequences in a way that allows separate, after-the-fact con-\ntrol of overall trends, cyclic effects, and random effects in the\nimages. In our work, we focus on a more challenging setup,\nwhere our input is unstructured Internet photos from differ-\nent viewpoints, and where we aim to synthesize novel views\nin addition to factorizing different temporal components.\n3. Method\nThe input to our method is a collection of Internet photos\nof a landmark (e.g., Times Square) with known timestamps\nand camera poses. Our goal is to recover a 4D scene repre-\nsentation that can be used to render photo-realistic images\nViewpoint\nTime\nIllumination\nInternet photos: sparse sampling of viewpoint, time and illumination\nSynthesized images: interpolating and independently controlling each component\nRendering with different viewpoints\nRendering with different time\nRendering with different illumination\nFigure 2. Problem illustration. Given an Internet photo collection\nof a scene, each image can be thought of as a sample in a high-\ndimensional space consisting of entangled information, including\nviewpoint, time, and lighting effects. The photo collection rep-\nresents a sparse sampling of this space. Our goal is to recover a\n4D scene representation from this sparse sampling, and to enable\ninterpolation in this high-dimensional space with disentanglement\nand independent control of viewpoint, time, and lighting.\nof that scene with independently controlled viewpoint, time,\nand lighting effects as illustrated in Fig. 2. This is a challeng-\ning problem because different kinds of temporal changes,\nincluding scene content changes (changes to the scene itself)\nand lighting variation (e.g., time of day) are entangled in each\nimage, but must be disentangled in the scene representation\nto enable independent control over each temporal component.\nFurthermore, content changes in our target scenes often hap-\npen suddenly, meaning that the scene representation must be\nable to model discrete, sporadic changes over time.\nTo tackle this problem, we propose a new 4D scene rep-\nresentation that can disentangle viewpoint, lighting effects,\nand time. Our key observation is that the scene content of-\nten changes less frequently over time and remains nearly\nconstant in-between changes, whereas illumination changes\nmuch more frequently and sometimes dramatically. For\nexample, the graffiti in 5Pointz (see Fig. 1) may only be\nreplaced every few months, but illumination can change over\nthe course of a few hours. Motivated by this observation,\nwe model illumination variation with a per-image illumina-\ntion embedding, and model the underlying 4D scene content\nusing an MLP with time as input. We introduce our scene\nrepresentation in Sec. 3.1. To model piece-wise constant\ntemporal content with abrupt transitions, we propose a novel\nencoding method in Sec. 3.2 that utilizes the behavior of the\nstep function, i.e., remaining consistent given a continuous\ninput, while allowing abrupt changes at transition points.\n2012-09-29\n2012-10-11\n2010-04-13\n2013-07-26\n2011-12-04\n0.0000\n0.0003\n0.0001\n0.0002\nWithout Encoding\n2012-10-23\n2010-04-13\n2013-07-26\n2011-12-04\n0.00\n0.16\n0.12\n0.08\n0.04\nPositional Encoding\n2012-09-29\n2012-10-11\n2012-10-23\n2010-04-13\n2013-07-26\n2011-12-04\n0.00\n0.06\n0.04\n0.02\nStep Function Encoding (Ours)\n2012-09-29\n2012-10-11\n2012-10-23\nFigure 3. Visual ablation of the proposed time encoding. Given a video rendered at a fixed viewpoint through several years, we compute\nthe Mean Squared Error (MSE) between every pair of consecutive frames. The vertical and horizontal axes represent MSE and time,\nrespectively. For a scene with sporadic changes, we expect to see infrequent \u201cdeltas\u201d in this MSE plot. We observe that our method indeed 1)\nrecovers the transition points of scene changes and 2) stays consistent (zero MSE) at other times. With no time encoding, 1) is violated,\nleading to blended, ghosted visual content (top left), and with positional encoding, 2) is violated, leading to temporal flicker (top middle).\nPlease see our supplemental video for animated results.\n3.1. 4D Reconstruction from Internet Photos\nGiven a posed image collection {Ii}N\ni=1 with timestamps\n{ti}N\ni=1, we represent the 4D scene as a time-varying neural\nfield. To disentangle changes to the underlying scene from\nthe varying and unknown per-image illumination, each im-\nage Ii is assigned a learnable illumination embedding vector\n\u2113i, which is meant to encode the illumination present in that\nview. Formally, given a space-time point (x, ti) with 3D\nspatial coordinate x and time ti, along with an illumination\ncode \u2113i and view direction d, we use an MLP denoted by F\nto encode its radiance c and volume density \u03c3 as follows:\nc, \u03c3 = F(x, ti, \u2113i, d).\n(1)\nFollowing NeRF [34], the input spatial coordinates x and\nray direction d are mapped to higher-dimensional vectors\nvia a fixed positional encoding function. For simplicity,\nwe assume that the scene geometry is mostly constant, and\nonly the appearance changes over time, but our method\ncould also be extended to handle time-varying geometry.\nTherefore the model in Eq. (1) can be divided into a static,\ntime-invariant geometric model and a time-aware appearance\nmodel, denoted by Fgeo and Fapp, respectively:\nv, \u03c3 = Fgeo(x),\n(2)\nc = Fapp(x, v, ti, \u2113i, d).\n(3)\nFgeo models static geometry, and is parameterized by just\nthe input 3D position x, while Fapp models time-dependent\nappearance, and depends on the space-time point (x, ti),\nillumination embedding \u2113i, view direction d, and the inter-\nmediate geometry feature vector v produced by Fgeo. Please\nrefer to the supplementary material for additional details\nabout the model architecture.\nFrom this scene representation, we can render images\nusing volume rendering, and optimize the scene representa-\ntion by comparing these rendered images to the known input\nviews via an image reconstruction loss. Specifically, given\nan input image Ii with timestamp ti, we compute the color\nof a ray r(s) = o + sd, emitted from the camera center o\nthrough a given pixel in direction d as follows: We use strat-\nified sampling to sample a set of quadrature points {sk}K\nk=1\nbetween sn and sf, the near and far bounds along the ray.\nThen, given the illumination embedding \u2113i and timestamp ti\nof image Ii, we can compute the color c(sk, ti, \u2113i) and den-\nsity \u03c3(sk) of each sample sk given our scene representation.\nWe then accumulate these points using volume rendering, as\nin NeRF [34], yielding the expected color \u02c6C(r, ti, \u2113i):\n\u02c6C(r, ti, \u2113i) =\nK\nX\nk=1\nT(sk)\u03b1(\u03c3(sk)\u03b4k)c(sk, ti, \u2113i),\n(4)\nwhere\nT(sk) = exp\n \n\u2212\nk\u22121\nX\nk\u2032=1\n\u03c3(sk\u2032)\u03b4k\u2032\n!\n,\n(5)\nwhere \u03b1(x) = 1 \u2212 exp(\u2212x), and \u03b4k = sk+1 \u2212 sk. We\nminimize the sum of squared error between the rendered and\nground truth pixels:\nL =\nX\n(r,i)\u2208\u2126\n\u2225Ci(r) \u2212 \u02c6C(r, ti, \u2113i)\u22252\n2,\n(6)\nwhere Ci(r) is the observed pixel color in image Ii with\ntimestamp ti, and \u2126 is the set of all the sampled pixels from\nthe image collection {Ii}N\ni=1.\nWithout Encoding\nPositional Encoding\nSIREN Activation\nStep Function Encoding\nInput Signal and Target\nGaussian Activation\nFigure 4. Fitting an MLP to a noisy piecewise constant 1D\nsignal. Given the noisy orange data points (uniform noise), we\naim to recover the clean blue curve shown in the upper left plot.\nWe present the results of fitting the input data using an MLP with\ndifferent encoding methods and activation functions. Our proposed\nstep function encoding achieves the best results, recovering the\ndiscrete, sporadic changes in the signal without over-fitting to noise.\n3.2. Step Function Encoding for Time Input\nThe method described above serves as a baseline to model\na 4D scene from Internet photos. However, we found that\nthis baseline cannot model temporal changes in the target\nscene well. Specifically, temporal appearance changes in\nman-made scenes are often abrupt, such as a new billboard\nor sign in Times Square, or a new graffiti artwork in an art\nmecca like 5Pointz. In contrast, the baseline above tends to\naverage over temporal content changes, resulting in a cross-\nfade transition in time between two appearance states, rather\nthan a sharp, sudden transition. Fig. 3 shows an example\nwhere this baseline (denoted \u201cwithout encoding\u201d in the fig-\nure) produces a ghosted blend of two temporally consecutive\ngraffiti artworks. This finding is consistent with NeRF\u2019s\nobservation that standard coordinate inputs cannot model\nhigh-frequency signals [34]. To address this issue, NeRF\nuses positional encoding to map input spatial coordinates to\na high-frequency signal. However, we found that applying\npositional encoding to the time input causes the network to\nnot only fit the underlying appearance changes in the scene,\nbut also overfit to per-image lighting effects. In other words,\nit fails to disentangle these two components and leads to\nsevere flickering artifacts over time, as shown in Fig. 3.\nTo address this problem, we present a novel encoding\nmethod based on a step function. The step function has\nthe desirable property that the output mostly stays constant\nwith respect to the input, except when it changes abruptly\nat a transition point. Therefore, we consider using the step\nfunction defined below as the encoding function for time t:\nh(t) =\n(\n0\nif t \u2264 u\n1\nif t > u ,\n(7)\nwhere u is a learnable parameter representing the transition\npoint. However, h(t) is discontinuous and the gradient for u\nis not well-defined. We therefore use a smooth approxima-\ntion to h(t) to make it differentiable, denoted as \u00afh(t):\n\u00afh(t) =\n(\n1\n2exp( t\u2212u\n\u03b2 )\nif t \u2264 u\n1 \u2212 1\n2exp( \u2212(t\u2212u))\n\u03b2\n)\nif t > u ,\n(8)\nwhere \u03b2 is a learnable parameter representing the steepness\nof the step function. In practice, u is randomly initialized\nfrom zero to one and \u03b2 is initialized to 0.3. Our encoding\nmethod uses a vector of step functions, denoted as H(t),\neach with its own learned transition point, to express mul-\ntiple transition points. u and \u03b2 are jointly learned during\ntraining. We experimentally show that we can simply set the\ndimension of this vector to a large number that exceeds the\nexpected number of scene transitions.\nTo illustrate the effectiveness of our proposed encoding\nfunction, we compare it with different encoding functions\non a toy 1-D fitting experiment in Fig. 4. Baseline meth-\nods either overfit the noise (positional encoding [34], Gaus-\nsian [41]) or underfit the discrete, sporadic changes (without\nencoding, SIREN [49]). In contrast, our step function en-\ncoding correctly recovers the sharp changes in the signal by\napproximating real step functions with small \u03b2 parameters.\nNote that Gaussian and SIREN are used as the activation\nlayer of a network, while our method and positional encod-\ning are used to modulate the input.\n3.3. Implementation Details\nLearning scene appearance with parametric encoding.\nUsing an implicit representation to reconstruct a large scene\nfeaturing content changes over time requires a large model\ncapacity. While we could simply increase the size of the\nMLP, this strategy incurs a linear increase in training and\nrendering time. Motivated by Neural Sparse Voxel Fields\n(NSVF) [26], we adopt a parametric encoding which adds\nadditional trainable parameters for scene appearance to ef-\nfectively increase model capacity without introducing as\nmuch overhead. Observing that our target man-made scenes\noften satisfy the Manhattan world assumption, we use a tri-\nplane structure to arrange additional trainable parameters [3],\nwhich we found to be compact and expressive in our ex-\nperiments. Specifically, we define three learnable feature\nplanes: Exy, Eyz, Exz. Each feature plane has a resolution\nof D \u00d7 D \u00d7 B, where D and B denote the spatial resolution\nand number of feature channels, respectively. Given a 3D\npoint p, we project it onto three axis-aligned orthogonal\nplanes to obtain pxy, pyz, pxz. We can fetch the feature\nfxy = interp(Exy, pxy), where interp is a linear interpola-\ntion operation. The same method can be applied to obtain\nfyz and fxz. The appearance parametric encoding for p is\ndefined as the concatenation of fxy, fyz and fxz.\nHandling transient objects. Learning a scene representa-\ntion using Internet photos with transient objects may intro-\nduce 3D inconsistencies. To solve this problem, we employ\nTimes Square Akihabara 5Pointz The Met\n# Retrieved images\n289,794\n105,445\n23,628 186,663\n# Calibrated images\n29,629\n13,671\n6,503\n2,184\n# Selected images\n5,965\n1,078\n3,521\n2,127\nTable 1. Dataset statistics. We collect four scenes for evaluation.\nFor each scene, we first retrieve photos from the Internet, then run\nCOLMAP to calibrate them and reconstruct a sparse point cloud.\nAfter calibration, we choose a region of interest from the point\ncloud and select the corresponding images as input to our method.\na pretrained semantic segmentation model [5] to identify pix-\nels of transient objects (e.g., pedestrians) and exclude these\npixels during training. However, a segmentation model may\nnot effectively filter out all transient pixels. To handle the re-\nmaining transient pixels, we use an MLP to predict whether\neach pixel in each image is a transient object. We learn it\nusing the uncertainty loss, reducing the effect of transient\npixels during training models as demonstrated in [6].\nOther details. Our model includes an 8-layer MLP with\n256 neurons for each layer as its backbone, and a 4-layer\nMLP as its appearance head. Our model is trained with an\ninitial learning rate of 5e-4, which is reduced to 5e-5 after\n800k iterations. At each iteration, we randomly sample 1024\nrays from an image. Following NeRF [34], we train our two\nmodels using the coarse-to-fine sampling strategy with 64\nand 128 points for each ray in the coarse and fine levels,\nrespectively. The model tends to converge after about 800k\niterations, which takes about 40 hours on an RTX 3090 GPU.\n4. Experiments\n4.1. Experimental Setup\nDatasets. We collect four scenes from Flickr that include\ntwo commercial tourist areas (Times Square and Akihabara),\na graffiti mecca (5Pointz), and a museum (the Metropolitan\nMuseum of Art aka the Met). The two commercial areas\nfeature an array of billboards and other elements that change\nover time. 5Pointz is an outdoor space where artists paint\ngraffiti art over time, each piece replacing (or augmenting)\na previous one. The Met has a varying array of banners and\nsigns advertising different exhibitions. For each scene, we\nrun COLMAP to recover camera parameters and a sparse\npoint cloud. Due to the large number of input images, run-\nning COLMAP on some of these scenes can take weeks on a\ncluster with multiple servers. We will release our processed\ndata and data processing scripts as a resource for the com-\nmunity. Note that whole calibrated scenes, such as Times\nSquare, can be excessively large for reconstruction using\nimplicit representations. Instead, we perform view synthesis\nexperiments on a region of the scene. Tab. 1 summarizes\nthese datasets. We include visualizations of the reconstructed\nmodels and the selected regions, along with data processing\ndetails, in the supplemental material.\nAct. func. /\nFreq.\n/ Dim.\nTemporal stability\nView synthesis quality\nMean \u2193 Entropy \u2193 PSNR \u2191 SSIM \u2191 LPIPS \u2193\nw/o. Time\n-\nN/A\nN/A\n18.18\n0.572\n0.461\nw/o. Encoding\n-\n0.116\n5.314\n20.54\n0.719\n0.296\nLearned Latent\n-\n11.04\n5.590\n20.95\n0.731\n0.291\nActivation\nSIREN [49]\n0.101\n5.552\n20.54\n0.696\n0.330\nGaussian [41]\n0.088\n5.512\n20.28\n0.704\n0.315\nPositional\nEncoding\n5\n0.247\n4.936\n20.71\n0.731\n0.288\n10\n5.657\n5.795\n20.57\n0.724\n0.294\n15\n9.995\n5.777\n20.64\n0.721\n0.301\nStep Func.\nEncoding\n8\n0.102\n1.602\n20.52\n0.728\n0.289\n16\n0.147\n2.213\n21.32\n0.745\n0.274\n24\n0.190\n2.625\n21.09\n0.734\n0.282\n32\n0.217\n2.806\n21.10\n0.738\n0.281\nTable 2. Quantitative ablation of the proposed time encoding\non 5Pointz. Act. func. represents the activation function while Freq.\nis the frequency of standard positional encoding [34]. Dim. is the\ndimension of the vector size for the learned step functions.\nMetrics. To measure view synthesis quality, we randomly\nselect a few dozen images per scene as a test set. To en-\nsure the validity of our evaluation, we visually inspect the\nimages in the test set and remove those that have excessive\nnoise or that do not align with the intended task, such as\nblack and white photos or images that are primarily por-\ntraits. For the remaining test images, there may still be some\ntransient objects present. We manually annotate masks for\nthese objects and ignore them during evaluation. Following\nNeRF-W [29], we use half of the valid pixels of each image\nto finetune the illumination embedding and the other half\nfor testing PNSR/SSIM/LPIPS metrics. Please refer to the\nsupplemental material for additional details.\n4.2. Ablations and Analysis\nQualitative ablation of the step function encoding. An ad-\nvantage of our method is that it can model abrupt scene-level\ncontent changes without overfitting per-image illumination\nnoise. We compare our method with two baselines: (1) or-\ndinary time input without any encoding, and (2) positional\nencoding of time [34] (with a frequency of 15). We seek\nto visualize the temporal stability of each method. To do\nso, given a 4D reconstruction, we first render a video of\nthe scene through time from a fixed viewpoint and with a\nfixed illumination code (i.e., only content changes). We then\ncompute the mean squared error (MSE) between every two\nconsecutive frames in this video. Plots of MSE over time\nfor each method are shown in Fig. 3. An ideal plot should\nhave large MSE values at sparse points due to abrupt content\nchanges, and zero MSE elsewhere. Our method yields results\nthat exhibit this desired behavior. In contrast, the baseline\nwith no temporal encoding (raw time input) produces smooth\ntransitions across content changes, while positional encoding\nof time leads to flickering videos. To further illustrate these\nbehaviors, we visualize images around a scene appearance\ntransition point in the first row of Fig. 3.\nQuantitative ablations and sensitivity analysis. In addi-\n\u001f\u001e\u001e\u001d\u001c\u001b\u001b\u001c\u001a\u001e\n\u001f\u001e\u001b\u001e\u001c\u001b\u001f\u001c\u001f\u001e\n\u001f\u001e\u001b\u001b\u001c\u001e\u001f\u001c\u001b\u0019\n\u001f\u001e\u001b\u001b\u001c\u001e\u001d\u001c\u001f\u0018\n\u001f\u001e\u001b\u001e\u001c\u001e\u0018\u001c\u001b\u0019\n\u001f\u001e\u001b\u001b\u001c\u001e\u0019\u001c\u001f\u0017\n\u001f\u001e\u001b\u001f\u001c\u001e\u0018\u001c\u001b\u0016\n\u001f\u001e\u001e\u001d\u001c\u001e\u0018\u001c\u001e\u0016\nIllumination control\nTime control\nFigure 5. Independent control of illumination effects and time. The top row shows the results of interpolating the illumination embeddings\nof two real images of 5Pointz, where the leftmost and rightmost images are real images, and all other images are rendered using our method.\nThe bottom rows show the results of rendering scenes across time with fixed illumination embeddings (for Times Square).\ntion to view synthesis quality, we also evaluate the temporal\nstability of synthesized views under a fixed illumination\nembedding, which indicates the degree of disentanglement\nbetween illumination effects and content changes. Similar to\nthe ablations discussed above, we measure temporal stability\nusing the statistics of MSE image differences between each\ntwo adjacent frames over time, for videos rendered at a fixed\nviewpoint with fixed illumination embedding. Specifically,\nwe choose the mean and entropy of these MSE values as\nour stability metrics. Higher mean values indicate signif-\nicant changes between adjacent frames, corresponding to\ntemporal flicker. High entropy is associated with high un-\ncertainty in the distribution. Considering image differences\nover time, high entropy values indicate that scene content\nchanges are distributed throughout time, and are not \u201cpeaky\u201d.\nSmaller mean and entropy values indicate that the changes\nare more concentrated, indicating better modeling of scene\ncontent with discrete and sporadic changes. Please refer to\nthe supplement for additional details on how these metrics\nare calculated.\nWe quantitatively ablate our encoding method in terms\nof view synthesis stability and quality in Tab. 2. We show\nseveral baselines and variants: (1) w/o Time which does\nnot take time as input, (2) w/o Encoding, which directly\ntakes raw, unencoded time as input, and (3) Learned Latent,\nmapping time to a set of learned latent codes. We also\nchange the activation function from ReLU to SIREN [49]\nand to Gaussian [41], which have been shown to have more\npowerful modeling abilities. The baselines of positional\nencoding with different frequencies are also included. While\nmany variants achieve reasonable reconstruction quality, our\nmethod can also achieve both low mean and entropy.\nWe also provide a sensitivity analysis on the dimension\nof the vector size of the learned step functions (Dim.) in\nTab. 2. The results show that our method can achieve high\nview synthesis quality once the dimension is \u2265 16. Larger\ndimensions lead to slightly lower temporal coherence, but\nnot a large degradation. This suggests that we can simply\nset the number of step functions to a number larger than the\nnumber of expected changes in that scene. We set Dim. to\n16 in our experiments for all scenes except Times Square,\nwhere we set Dim. to 32.\nFurther qualitative and quantitative ablations of the step\nfunction encoding can be found in the supp. material.\nApplication. We demonstrate the ability of our method to\nrender plausible and photo-realistic images with controlled\ntime and illumination effects in Fig. 5.\n4.3. Comparisons with the State of the Art\nWe compare to the state-of-the-art methods NeRF-W [29]\nand HaNeRF [6], which both reconstruct high-fidelity scene\nmodels via implicit representations [34]. NeRF-W models\nillumination using per-image embeddings, while HaNeRF\nmodels illumination using a CNN module. However, these\nmethods are designed for static landmarks and cannot handle\nour test scenes with substantial content changes. We there-\nfore extend these methods by adding time as a network input\nfor fairer comparisons.\nWe present quantitative and qualitative comparisons with\nthese methods in Tab. 3 and Fig. 6. Our method produces\n5Pointz\nAkihabara\nThe Met\nTimes Square\nOurs\nNeRFW-T\nHaNeRF-T\nImage (   )\nGT\nOurs\nNeRFW-T\nHaNeRF-T\nInput time:   \nRef. image (   )\nt0\nt1\nInput time: t1t1\nt0\nFigure 6. Qualitative comparison with the state of the art. The three column images under Input time: tx are rendered using timestamps tx\nand the viewpoints of the left image. Our method renders high-quality images and produces plausible images when changing the input time.\n4D view\nsynthesis\nTimes Square\nAkihabara\n5Pointz\nThe Met\nEntropy \u2193 PSNR \u2191 SSIM \u2191 LPIPS \u2193 Entropy \u2193 PSNR \u2191 SSIM \u2191 LPIPS \u2193 Entropy \u2193 PSNR \u2191 SSIM \u2191 LPIPS \u2193 Entropy \u2193 PSNR \u2191 SSIM \u2191 LPIPS \u2193\nNeRFW [29]\n%\nN/A\n16.59\n0.820\n0.211\nN/A\n17.41\n0.853\n0.164\nN/A\n17.52\n0.545\n0.500\nN/A\n23.21\n0.881\n0.159\nHaNeRF [6]\n%\nN/A\n15.39\n0.807\n0.218\nN/A\n17.50\n0.860\n0.160\nN/A\n16.82\n0.539\n0.508\nN/A\n22.32\n0.882\n0.158\nNeRFW-T\n!\n4.990\n18.71\n0.847\n0.190\n5.483\n19.16\n0.874\n0.140\n5.768\n19.41\n0.611\n0.418\n4.923\n23.83\n0.875\n0.164\nHaNeRF-T\n!\n4.929\n17.36\n0.844\n0.189\n5.565\n18.39\n0.873\n0.140\n5.881\n17.90\n0.585\n0.445\n4.943\n22.56\n0.881\n0.156\nOurs\n!\n3.122\n20.87\n0.894\n0.132\n2.482\n20.31\n0.902\n0.101\n2.213\n21.32\n0.745\n0.274\n2.399\n24.07\n0.895\n0.129\nTable 3. Quantitative comparison with the state of the art. We augment NeRF-W and HaNeRF to take time as input (*-T). Our method\noutperforms prior methods across all metrics, demonstrating that our method can better handle such time-varying Internet collections.\nlower entropy across all the scenes. In addition to better\ntemporal stability, our method also has better view synthesis\nquality. We attribute this to the use of a well-designed ap-\npearance parametric encoding. We include ablations of the\nparametric appearance encoding in the supplemental mate-\nrial. To compare the ability of view synthesis through time,\nwe synthesize photos of the same viewpoint at another time\nas shown in Fig. 6. The step function encoding helps avoid\nblending artifacts. In contrast, the other methods often ex-\nhibit such artifacts when content changes occur, as is evident\nin the supplemental video.\n5. Conclusion\nWe explored the problem of chronology reconstruction,\naiming to reconstruct and render temporally complex scenes\nwith controlled viewpoint, time, and illumination effects\nfrom Internet photos. We proposed a new neural scene rep-\nresentation equipped with a novel step function encoding to\naddress several challenges, including the entanglement of\nillumination variation and scene content changes, as well\nas abrupt scene content changes. We also collected a new\ndataset to benchmark this problem. Experiments show that\nour method exhibits state-of-the-art performance and is ca-\npable of producing plausible, stable view synthesis results\nacross time. Detailed ablations and analysis were conducted\nto validate our proposed components.\nLimitations and future work. Our method takes as input\nInternet photos with timestamps. Inaccurate timestamps may\nhinder the training process, and Internet photos that do not\nhave timestamps cannot be utilized for training. Exploring\nhow to simultaneously predict timestamps is an interesting\navenue for future work. In addition, some urban scenes such\nas Times Square have billboards that display videos (not still\nimages), which are difficult for our method to reconstruct,\nas their content has high temporal frequency is not well\nsupported by other images in the collection.\nAcknowledgements. The authors would like to acknowl-\nedge support from Information Technology Center and State\nKey Lab of CAD&CG, Zhejiang University.\nReferences\n[1] Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian\nSimon, Brian Curless, Steven M Seitz, and Richard Szeliski.\nBuilding rome in a day. ACM Communications, 2011. 2\n[2] Boming Zhao and Bangbang Yang, Zhenyang Li, Zuoyue Li,\nGuofeng Zhang, Jiashu Zhao, Dawei Yin, Zhaopeng Cui, and\nHujun Bao. Factorized and controllable neural re-rendering\nof outdoor scene for photo extrapolation. In ACM MM, 2022.\n3\n[3] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano,\nBoxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas\nGuibas, Jonathan Tremblay, Sameh Khamis, Tero Karras, and\nGordon Wetzstein. Efficient geometry-aware 3D generative\nadversarial networks. In CVPR, 2021. 5\n[4] Gaurav Chaurasia, Sylvain Duchene, Olga Sorkine-Hornung,\nand George Drettakis. Depth synthesis and local warps for\nplausible image-based navigation. ACM TOG, 2013. 3\n[5] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian\nSchroff, and Hartwig Adam. Encoder-decoder with atrous\nseparable convolution for semantic image segmentation. In\nECCV, 2018. 6\n[6] Xingyu Chen, Qi Zhang, Xiaoyu Li, Yue Chen, Ying Feng,\nXuan Wang, and Jue Wang. Hallucinated neural radiance\nfields in the wild. In CVPR, 2022. 2, 3, 6, 7, 8\n[7] Abe Davis, Marc Levoy, and Fredo Durand. Unstructured\nlight fields. In Eurographics, 2012. 3\n[8] Jiemin Fang, Taoran Yi, Xinggang Wang, Lingxi Xie, Xi-\naopeng Zhang, Wenyu Liu, Matthias Nie\u00dfner, and Qi Tian.\nFast dynamic radiance fields with time-aware neural voxels.\nSIGGRAPH Asia, 2022. 3\n[9] John Flynn, Ivan Neulander, James Philbin, and Noah Snavely.\nDeepstereo: Learning to predict new views from the world\u2019s\nimagery. In CVPR, June 2016. 3\n[10] Jan-Michael Frahm, Pierre Fite-Georgel, David Gallup, Tim\nJohnson, Rahul Raguram, Changchang Wu, Yi-Hung Jen,\nEnrique Dunn, Brian Clipp, Svetlana Lazebnik, et al. Building\nrome on a cloudless day. In ECCV, 2010. 2\n[11] Yasutaka Furukawa, Brian Curless, Steven M Seitz, and\nRichard Szeliski. Towards internet-scale multi-view stereo.\nIn CVPR, 2010. 2\n[12] Chen Geng, Sida Peng, Zhen Xu, Hujun Bao, and Xiaowei\nZhou. Learning neural volumetric representations of dynamic\nhumans in minutes. In CVPR, 2023. 3\n[13] Michael Goesele, Noah Snavely, Brian Curless, Hugues\nHoppe, and Steven M Seitz. Multi-view stereo for community\nphoto collections. In ICCV, 2007. 2\n[14] Steven J Gortler, Radek Grzeszczuk, Richard Szeliski, and\nMichael F Cohen. The lumigraph. In SIGGRAPH, 1996. 3\n[15] Zekun Hao, Arun Mallya, Serge Belongie, and Ming-Yu Liu.\nGancraft: Unsupervised 3d neural rendering of minecraft\nworlds. In ICCV, 2021. 11\n[16] Erik H\u00a8ark\u00a8onen, Miika Aittala, Tuomas Kynk\u00a8a\u00a8anniemi, Samuli\nLaine, Timo Aila, and Jaakko Lehtinen. Disentangling ran-\ndom and cyclic effects in time-lapse sequences. ACM TOG,\n2022. 3\n[17] Nima Khademi Kalantari, Ting-Chun Wang, and Ravi Ra-\nmamoorthi. Learning-based view synthesis for light field\ncameras. ACM TOG, 2016. 3\n[18] Marc Levoy and Pat Hanrahan. Light field rendering. In\nSIGGRAPH, 1996. 3\n[19] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green,\nChristoph Lassner, Changil Kim, Tanner Schmidt, Steven\nLovegrove, Michael Goesele, Richard Newcombe, et al. Neu-\nral 3d video synthesis from multi-view video. In CVPR, 2022.\n3\n[20] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang.\nNeural scene flow fields for space-time view synthesis of\ndynamic scenes. In CVPR, 2021. 3\n[21] Zhengqi Li and Noah Snavely.\nLearning intrinsic image\ndecomposition from watching the world. In CVPR, 2018. 3\n[22] Zhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker,\nand Noah Snavely. Dynibar: Neural dynamic image-based\nrendering. In CVPR, 2023. 3\n[23] Zhengqi Li, Wenqi Xian, Abe Davis, and Noah Snavely.\nCrowdsampling the plenoptic function. In ECCV, 2020. 3\n[24] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai,\nHujun Bao, and Xiaowei Zhou. Efficient neural radiance\nfields for interactive free-viewpoint video. In SIGGRAPH\nAsia, 2022. 3\n[25] Andrew Liu, Shiry Ginosar, Tinghui Zhou, Alexei A Efros,\nand Noah Snavely. Learning to factorize and relight a city. In\nEuropean Conference on Computer Vision, pages 544\u2013561.\nSpringer, 2020. 3\n[26] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and\nChristian Theobalt. Neural sparse voxel fields. NeurIPS,\n2020. 5\n[27] Ricardo Martin-Brualla, David Gallup, and Steven M Seitz.\n3d time-lapse reconstruction from internet photos. In ICCV,\n2015. 2\n[28] Ricardo Martin-Brualla, David Gallup, and Steven M Seitz.\nTime-lapse mining from internet photos. ACM TOG, 2015. 2\n[29] Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Saj-\njadi, Jonathan T. Barron, Alexey Dosovitskiy, and Daniel\nDuckworth. NeRF in the Wild: Neural Radiance Fields for\nUnconstrained Photo Collections. In CVPR, 2021. 2, 3, 6, 7,\n8\n[30] Kevin Matzen and Noah Snavely. Scene chronology. In\nECCV, 2014. 2\n[31] Lars M. Mescheder, Michael Oechsle, Michael Niemeyer, Se-\nbastian Nowozin, and Andreas Geiger. Occupancy networks:\nLearning 3d reconstruction in function space. In CVPR, 2019.\n3\n[32] Moustafa Meshry, Dan B Goldman, Sameh Khamis, Hugues\nHoppe, Rohit Pandey, Noah Snavely, and Ricardo Martin-\nBrualla. Neural rerendering in the wild. In CVPR, 2019.\n3\n[33] Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon,\nNima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and\nAbhishek Kar. Local light field fusion: Practical view synthe-\nsis with prescriptive sampling guidelines. ACM TOG, 2019.\n3\n[34] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view synthe-\nsis. In ECCV, 2020. 2, 3, 4, 5, 6, 7\n[35] Michael Niemeyer, Lars M. Mescheder, Michael Oechsle,\nand Andreas Geiger. Differentiable volumetric rendering:\nLearning implicit 3d representations without 3d supervision.\nIn CVPR, 2020. 3\n[36] Jeong Joon Park, Peter Florence, Julian Straub, Richard A.\nNewcombe, and Steven Lovegrove. Deepsdf: Learning con-\ntinuous signed distance functions for shape representation. In\nCVPR, 2019. 3\n[37] Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien\nBouaziz, Dan B Goldman, Steven M. Seitz, and Ricardo\nMartin-Brualla. Nerfies: Deformable neural radiance fields.\nIn ICCV, 2021. 3\n[38] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T\nBarron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-\nBrualla, and Steven M Seitz.\nHypernerf:\nA higher-\ndimensional representation for topologically varying neural\nradiance fields. arXiv, 2021. 3\n[39] Sida Peng, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei\nZhou. Representing volumetric videos as dynamic mlp maps.\nIn CVPR, 2023. 3\n[40] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and\nFrancesc Moreno-Noguer. D-nerf: Neural radiance fields\nfor dynamic scenes. In CVPR, 2021. 3\n[41] Sameera Ramasinghe and Simon Lucey. Beyond periodicity:\nTowards a unifying framework for activations in coordinate-\nmlps. In ECCV, 2022. 5, 6, 7\n[42] Viktor Rudnev, Mohamed Elgharib, William Smith, Lingjie\nLiu, Vladislav Golyanik, and Christian Theobalt. Nerf for\noutdoor scene relighting. In ECCV, 2022. 3\n[43] Sara Fridovich-Keil and Giacomo Meanti, Frederik Rahb\u00e6k\nWarburg, Benjamin Recht, and Angjoo Kanazawa. K-planes:\nExplicit radiance fields in space, time, and appearance. In\nCVPR, 2023. 3\n[44] Frederik Schaffalitzky and Andrew Zisserman. Multi-view\nmatching for unordered image sets, or \u201chow do i organize my\nholiday snaps?\u201d. In ECCV, 2002. 2\n[45] Grant Schindler and Frank Dellaert. Probabilistic temporal\ninference on reconstructed 3d scenes. In CVPR, 2010. 2\n[46] Johannes L Schonberger and Jan-Michael Frahm. Structure-\nfrom-motion revisited. In CVPR, 2016. 2\n[47] Johannes L Sch\u00a8onberger, Enliang Zheng, Jan-Michael Frahm,\nand Marc Pollefeys. Pixelwise view selection for unstructured\nmulti-view stereo. In ECCV, 2016. 2\n[48] Shuhan Shen. Accurate multiple view 3d reconstruction using\npatch-based stereo for large-scale scenes. TIP, 2013. 2\n[49] Vincent Sitzmann, Julien Martel, Alexander Bergman, David\nLindell, and Gordon Wetzstein. Implicit neural representa-\ntions with periodic activation functions. NeurIPS, 2020. 5, 6,\n7\n[50] Vincent Sitzmann, Justus Thies, Felix Heide, Matthias\nNie\u00dfner, Gordon Wetzstein, and Michael Zollh\u00a8ofer. Deep-\nvoxels: Learning persistent 3d feature embeddings. In CVPR,\n2019. 3\n[51] Noah Snavely, Steven M Seitz, and Richard Szeliski. Photo\ntourism: exploring photo collections in 3d. In SIGGRAPH,\n2006. 2\n[52] Noah Snavely, Steven M Seitz, and Richard Szeliski. Mod-\neling the world from internet photo collections. IJCV, 2008.\n2\n[53] Jiaming Sun, Xi Chen, Qianqian Wang, Zhengqi Li, Hadar\nAverbuch-Elor, Xiaowei Zhou, and Noah Snavely. Neural 3D\nreconstruction in the wild. In SIGGRAPH, 2022. 3\n[54] Kalyan Sunkavalli, Wojciech Matusik, Hanspeter Pfister, and\nSzymon Rusinkiewicz. Factored time-lapse video. In ACM\nSIGGRAPH, pages 101\u2013es. 2007. 3\n[55] Richard Tucker and Noah Snavely. Single-view view synthe-\nsis with multiplane images. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 551\u2013560, 2020. 3\n[56] Chaoyang Wang, Ben Eckart, Simon Lucey, and Orazio Gallo.\nNeural trajectory fields for dynamic novel view synthesis.\narXiv, 2021. 3\n[57] Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon\nYenphraphai, and Supasorn Suwajanakorn. Nex: Real-time\nview synthesis with neural basis expansion. In CVPR, 2021.\n3\n[58] Wenqi Xian, Jia-Bin Huang, Johannes Kopf, and Changil\nKim. Space-time neural irradiance fields for free-viewpoint\nvideo. In CVPR, 2021. 3\n[59] Jiakai Zhang, Xinhang Liu, Xinyi Ye, Fuqiang Zhao, Yanshun\nZhang, Minye Wu, Yingliang Zhang, Lan Xu, and Jingyi\nYu. Editable free-viewpoint video using a layered neural\nrepresentation. ACM TOG, 2021. 3\n[60] C Lawrence Zitnick, Sing Bing Kang, Matthew Uyttendaele,\nSimon Winder, and Richard Szeliski. High-quality video view\ninterpolation using a layered representation. ACM TOG, 2004.\n3\nIn the supplementary material, we present more imple-\nmentation details, results, and discussion.\nA. Method Details\nA.1. Data Processing Details\nGiven Internet photos of a landmark, one way to obtain\ncamera calibration and scene structure is to run COLMAP\nwith built-in acceleration techniques such as Vocabulary\nTree Matching and Hierarchical Mapper. However, given the\nhuge number of Internet photos (up to 300K) in our problem,\nthis process requires very high computing resources and\nup to several TB of memory. To efficiently calibrate 300K\nimages, we design a scheme that can utilize multiple GPU\nmachines in parallel, while solves the problem of computing\nresources and memory requirements. Specifically, we first\nrandomly divide the pictures into several parts that partially\noverlap, and then run COLMAP for each part on a GPU\nseparately and in parallel. We then merge these models\nusing the overlapping registered images.\nThe whole calibrated scene may be too big to reconstruct\nfor the implicit representation. Instead of reconstructing the\nwhole scene, we attempt to reconstruct a piece of the scene.\nSpecifically, given a SFM sparse point cloud of the scene, we\ncan select a interested region from it using MeshLab. Then\nwe can obtain the images, which are registered to the model\nusing the points in the selected interesting region.\nPlease see Fig. 11 for SfM results and our selected re-\ngions.\nA.2. Sky Modeling\nGiven an outdoor scene, we can use the SFM sparse point\ncloud to define the near and far planes for sampling points\nto optimize our representation. For the sky part, this kind\nof sampling tends to recover a cloud above the building,\ncausing artifacts when we conduct view synthesis over a\nlarge angle range due to wrong geometry. To address the\nissue, we use a spherical radiance map proposed in [15] to\nmodel the sky. Specifically, we map the view direction and\nthe image illumination embedding to the sky color using an\nMLP and then use alpha blending to obtain the final image\ncolor.\nA.3. Model Architecture Details\nFig. 12 shows our model architecture. Given a 3D point\ncoordinate xyz and timestamp, along with an illumination\nembedding and view direction, we use an MLP to predict\nits radiance and density. For simplicity, we assume that the\nscene geometry is mostly constant, and only the appearance\nchanges over time. Therefore, we use an MLP (colored\nin orange in Fig. 12) to model the geometry and use an\nMLP (colored in blue in Fig. 12) to model time-varying\nappearance.\nGT\nImage\nw/o. Appearance\nParametric Encoding\nw. Appearance\nParametric Encoding\nFigure 7. Ablation on the appearance parametric encoding. As\nillustrated above, using a parametric encoding significantly affects\nthe rendering quality.\nFigure 8. Our method cannot reconstruct blind spots well. Peo-\nple cannot see the top of this small platform when standing on the\nground to take pictures.\nA.4. Entropy Details.\nOur technical contribution lies in the step function encod-\ning, which enables us to model abrupt scene-level content\nchange without overfitting per-image illumination variation.\nWe use the view synthesis stability (Entropy) along the time\nto quantify it. Specifically, we first render a video with a\nfixed viewpoint and a fixed illumination embedding. Then\nwe compute the image difference (MSE) over two consecu-\ntive frames of this video to obtain the MSE over time. The\ndistribution of MSE along the time can be regarded as the\ndistribution of content changes over time. The ideal distri-\nbution should be concentrated into several points. Given a\ndistribution P, which takes values in the alphabet P and is\ndistributed by p : P \u2192 [0, 1] (the normalized MSE over\ntime), its entropy E can be computed as:\nE(P) =\nX\nxi\u2208P\n\u2212p(xi)log p(xi).\n(9)\nWhen x is equal to zero or one, the function y = \u2212xlog(x)\nis zero, while this function is positive when x is between\nzero and one. An ideal concentrated distribution has zero\nentropy.\nB. Additional Results\nB.1. Ablation on the Step Func. Encoding\nTo further demonstrate the advantages of step function\nencoding (in addition to the ablation on the step function\nencoding we present in the main paper), we carefully se-\nlected a train/test split for 5Pointz and evaluated it by vi-\nsual inspection. This test split contains a large number\nof images captured at the time of scene content changes.\nWithout using the step function encoding, there would be\nnoticeable blending artifacts in such a test split. Specif-\nically, the results of using step function encoding versus\nnot using it are 19.19/0.715/0.307 and 18.07/0.678/0.345\n(PSNR/SSIM/LPIPS), respectively. We present qualitative\nresults on Fig. 10.\nB.2. Ablation on the Parametric Encoding\nWe find that the parametric encoding for appearance mod-\neling significantly affects the rendering quality. Specifically,\nmodels with and without proposed parameter encoding pro-\nduce 19.48/0.612/0.423 and 21.32/0.745/0.274 on 5Pointz\nin terms of PSNR/SSIM/LPIPS metrics, respectively. We\nprovide a visual ablation on Fig. 7.\nB.3. Ablation on the Static-geometry Assumption\nWe run an ablation study on 5Pointz testing the effect\nof allowing dynamic geometry, and the results indicate a\nslight decrease in performance (20.82/0.729 vs. 21.32/0.744\nin terms of PSNR/SSIM). Note that all baselines we evaluate\nassume static geometry, to ensure a fair comparison.\nB.4. Scalability of Step Function Encoding\nFig. 9 shows that our method works for synthetic signals\nwith nearly 100 transitions, indicating the scalability of our\napproach. Specifically, the synthetic signals have multiple\ndimensions, with each dimension consisting of a randomly\nsegmented and discretized value. Such signals are similar to\nreal-world scenarios, where the value in each dimension can\nbe analogized to a billboard in a real-world scene.\nC. Discussion\nC.1. Time Period Selection\nWe reconstructed 4 scenes in the period 2009\u20142013. We\nchose this period because we found that Flickr was actively\nused during this period, which provides us with lots of pho-\ntos. After this period, Flickr became less popular, perhaps\ndue to the emerging social media platforms. For example,\nFlickr has nearly 80,000 pictures about Akihabara in 2010-\n2013, but there are less than 10,000 pictures about Akihabara\nin the last 4 years. Future datasets should also consider these\nsocial platforms, if interested in representing more recent\nyears.\nStep Func. Encoding\nPositional Encoding\n//\n//\n//\n//\nFigure 9. An extension of the experiment in Fig. 3 of main paper.\nThe input noisy signal has 96 transitions, and the dimension of the\nstep function encoding is set to 128. This figure shows fragments of\nselected transitions. Compared to positional encoding, our method\nexhibits better temporal stability.\nC.2. Additional discussion on transient object de-\ntection\nWhy does transient object detection not interfere with\nour goal to disentangle per-image changes from scene-level\nchanges? The appearance of a scene at a certain stage is\nusually observed by multiple images, while transient objects\nare not. Uncertainty loss tends to increase the uncertainty\nof regions where the model cannot learn (e.g., regions of\ntransient objects), while minimizing uncertainty in other re-\ngions. The model can more easily learn scene appearance\nchanges in a 3D coherent way to fit regions with scene ap-\npearance changes, rather than predicting these regions with\nhigh uncertainty.\nC.3. Additional Discussion on Artifacts\nIn addition to limitations in Sec.5 of the main paper, our\nmethod also has the limitation of not being able to recon-\nstruct regions with very few observations. This problem is\nrelatively common for landmark reconstruction from Internet\nphotos. We include some visualization results in Fig. 8.\nGround Truth\nValid Pixels\nw/o. Step Func.\nw. Step Func. (Ours)\nFigure 10. Qualitative ablation on the step function encoding. We focus on the evaluation of manually annotated valid pixels for\nreasonable metric computation. It can be observed that blending artifacts appear when step function encoding (Step Func.) is not used. This\ndemonstrates the effectiveness of Step Function Encoding.\nSelected Region\nCalibrated Model\n5Pointz\nTimes Sqaure\nAkihabara\nThe Met.\nFigure 11. Scene scale visualization. The left column images are the sparse point cloud of the COLMAP model. We select an interesting\nregion from the sparse point cloud using MeshLab.\nDensity\nTimestamp\n(Step Func. Encoding)\nxyz\n(Appearance \nParametric Encoding)\nRadiance\nxyz\n(Pos. Encoding)\nView Direction (Pos. Encoding)\nand Illumination Embedding\nSkip Connection\nIntermediate\nFeature Vector\nFigure 12. Model Architecture. Step Func. Encoding and Pos. Encoding represent Step Function Encoding and Positional Encoding,\nrespectively. Each layer of the MLP has 256 neurons. We use ReLU as our activation function.\n"
  },
  {
    "title": "arXiVeri: Automatic table verification with GPT",
    "link": "https://arxiv.org/pdf/2306.07968.pdf",
    "upvote": "6",
    "text": "arXiVeri: Automatic table verification with GPT\nGyungin Shin1,4\nWeidi Xie2,3\nSamuel Albanie4,5\n1Visual Geometry Group, University of Oxford, UK\n2Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, China\n3Shanghai AI Laboratory, China\n4Cambridge Applied Machine Learning Lab, University of Cambridge, UK\n5Filtir, UK\nhttps://www.robots.ox.ac.uk/vgg/research/arxiveri\nAbstract\nWithout accurate transcription of numerical data in scientific documents, a scientist\ncannot draw accurate conclusions. Unfortunately, the process of copying numerical\ndata from one paper to another is prone to human error. In this paper, we propose to\nmeet this challenge through the novel task of automatic table verification (AutoTV),\nin which the objective is to verify the accuracy of numerical data in tables by\ncross-referencing cited sources. To support this task, we propose a new benchmark,\narXiVeri, which comprises tabular data drawn from open-access academic papers\non arXiv. We introduce metrics to evaluate the performance of a table verifier in\ntwo key areas: (i) table matching, which aims to identify the source table in a cited\ndocument that corresponds to a target table, and (ii) cell matching, which aims to\nlocate shared cells between a target and source table and identify their row and\ncolumn indices accurately. By leveraging the flexible capabilities of modern large\nlanguage models (LLMs), we propose simple baselines for table verification. Our\nfindings highlight the complexity of this task, even for state-of-the-art LLMs like\nOpenAI\u2019s GPT-4. The code and benchmark will be made publicly available.1\n1\nIntroduction\nMany areas of scientific research employ numerical data to analyse, summarise and communicate\nfindings. When a researcher proposes a new framework, model or algorithm, it is often informative to\ncompare their contribution with prior work by comparing performance metrics. These performance\nmetrics are typically collated in tables that are interleaved with the body of text contained within\nscientific manuscripts. In practice, to enable the comparison, it is common for the researcher to\nmanually copy performance metrics from the original manuscript into their own manuscript. While\npragmatic, this copying process is susceptible to human error. When errors are introduced, the\nconclusions drawn from the comparisons are also affected. Given the importance of transferring\nsuch data correctly, there is a need for mechanisms that ensure its fidelity, but such tooling is not yet\navailable. In short, we lack a \u201cspell checker\u201d for manually copied scientific data.\nOn first sight, the problem appears simple\u2014after all, verifying that two numbers are equal is not a\nmathematically complicated task. However, in practice, it is beset with technical difficulties. Tables in\nthe scientific literature are designed to be readable for a human audience rather than machine parsers.\nAs such, they can vary significantly in layout, design, naming convention and manuscript location.\nThe same numerical data may itself be reported at different levels of precision, using percentages,\nfractions or decimals and in absolute or relative metrics.\nTo meet these challenges we propose the task of automatic table verification\u2014authenticating the\nnumerical data encapsulated in tables by cross-verifying the referred sources. Specifically, we tackle\n1https://github.com/caml-lab/research/tree/main/arxiveri\nPreprint. Under review.\narXiv:2306.07968v1  [cs.CL]  13 Jun 2023\nFigure 1: (Left) Table matching: given a target table from one paper and a list of source tables\nfrom another paper cited in the target table, the verifier needs to identify the source table containing\nnumeric data, specifically floating point numbers, that supports the data presented in the target table.\n(Right) Cell matching: given a target table and a source table, the verifier needs to identify and locate\ncells that hold the same semantic content in both tables, subsequently outputting the respective row\nand column indices of these matching cells in each table. The cells that are emphasised in red depict\nthe instances of hard negative cases. Best viewed in colour.\nthis task with Large Language Models (LLMs) inspired by their strong performance in many text-\nbased processing tasks [33, 31, 28, 18, 6, 19, 30, 21, 4, 22]. To facilitate evaluation of this task and\naddress the incumbent challenges, we introduce arXiVeri, a succinct benchmark composed of tabular\ndata extracted from open-access academic papers on arXiv. We further propose evaluation metrics\nfor gauging the efficacy of the verification system in two key dimensions: table matching and cell\nmatching. The former involves identifying the equivalent source table in a cited document for a given\ntarget table, while the latter aims to pair shared cells between a target and source table, and accurately\nidentify their respective indices (see Fig. 1). Our experimental findings underscore the complexity\ninherent to the task (with frontier models such as GPT-4 [18] struggling in many cases), indicating\nthat there is considerable room for further research progress.\nOur contributions can be summarised as follows: (i) We introduce a new and challenging task\ncalled Automatic Table Verification (AutoTV), paving the way for advancements in automatic data\nverification in scientific documents; (ii) To stimulate further research in the AutoTV field, we\nintroduce a benchmark dataset named arXiVeri, comprised of 3.8K target-source cell pairs and 158\ntarget-source table pairs, sourced from publicly accessible papers on arXiv; (iii) To facilitate the\nassessment of AutoTV, we define a set of evaluation metrics for table matching and cell matching\nsub-tasks. In addition, we provide baselines to underpin future comparisons. (iv) Finally, we conduct\na range of ablation studies to evaluate the key components of our approach which bring noticeable\nperformance gains.\n2\nRelated work\nOur work is connected to large language models (LLMs) for scientific research, table detection and\ntable structure recognition, and automating human labour with LLMs, which we describe next.\nLarge language models for scientific research. LLMs have been adapted for scientific research\nthrough various avenues, such as utilising models pretrained on scientific text to enhance performance\nin scientific NLP tasks [2, 29]. There are also notable advances in the biomedical sector, where\nspecialised LLMs pretrained on biomedical text have demonstrated considerable improvements [25].\nAdditionally, the compilation of extensive academic paper corpora equipped with metadata and\nstructured full text is proving to be an invaluable resource for academic research and text mining [16].\nAlongside these advancements, there are investigations into the consequences of model scaling in\nscientific applications, evaluating the relationship between model size and performance [11]. Our\nresearch further develops this field by presenting a new challenge: automatic table verification\nin scientific documents, highlighting the essential role of data accuracy and integrity via cross-\nreferencing cited sources.\nTasks for tables in a single document. Recent advancements in table-related tasks have pri-\nmarily focused on detecting tables within documents and understanding their structure within a\nsingle document. Early efforts developed practical algorithms for detecting tables in heterogeneous\n2\ndocuments [24], which later evolved with the incorporation of deep learning, specifically using\nConvolutional Neural Networks (CNNs), to enhance detection in PDF documents by combining\nvisual features with non-visual information [10]. Subsequent research introduced end-to-end deep\nlearning systems capable of not only detecting tables but also recognising their structure in document\nimages, without the need for metadata or heuristics [23]. Later work tackled both table detection\nand structure recognition simultaneously [20]. Prior work has also explored dataset construction for\ntable extraction from unstructured documents [26]. However, the central theme of these works has\nremained the detection and structural understanding of tables within a single document. In contrast,\nwe focus on table verification across documents.\nTask automation with LLMs. LLMs have significantly impacted various automation tasks. For\ninstance, Codex [5], an LLM fine-tuned on code from GitHub, exhibits proficient Python code\ngeneration capabilities, automating a task typically requiring human expertise. In the domain of data\nannotation, traditionally a labour-intensive task, LLMs like ChatGPT have demonstrated the potential\nto outperform human crowd-workers in speed, accuracy, and cost-effectiveness [8]. More recently,\nexperiments have been conducted with GPT-4 to assess its ability to assist with Neural Architecture\nSearch (NAS) [34] and interpreting neurons [3]. Our work also targets automation, offering a novel\napplication of LLMs to automate the intricate task of table verification in scientific documents.\n3\nAutomatic Table Verification\nIn this section, we define the proposed task of automatic table verification (Sec. 3.1) and metrics to\nevaluate the performance of a verifier on this task (Sec. 3.2). Then we describe our approach to tackle\nAutoTV (Sec. 3.3).\n3.1\nTask definition\nThe high-level objective of Automatic Table Verification (AutoTV) is to confirm that a document,\nreferenced in a table (termed the target table) within a separate document, contains a corroborative\ntable (termed the source table) which supports the cited information. When such a source table exists,\nAutoTV aims to identify matching cells between the source and target tables.\nOur focus is particularly on instances within academic papers, where precise referencing of numeric\ndata (e.g., floating point numbers) in tables is vital for comparative analysis. We observe that such\nin-table citations in academic literature occur for various reasons, including: attributing a specific\napproach to its original paper and quoting numerical data from an experimental result. The primary\nfocus of table verification is the latter case, where a verifier is tasked with solving two sub-tasks\n(see Fig. 1): (i) Table matching: detecting a table in the cited document that matches a table in the\nreferring document and if no such table exists, stating that there is no match; (ii) Cell matching:\nidentifying correspondences between cells with a floating point number in the source and target\ntables that share the same semantic meaning. This implies not only identical numeric values, but also\nsimilar meanings as suggested by their respective table headers. The process includes pinpointing the\nlocation of such cells by providing their respective row and column indices in each table.\nWe note that these sub-tasks pose distinct challenges. First, there may not be a source table that\nmatches the target table (e.g., the table citation may simply attribute to another document rather\nthan quoting numbers). Second, multiple cells within a table (e.g., source table) can share the same\nnumeric value, making it ambiguous how to pair those cells with ones in another table (e.g., target\ntable). Third, a table can have a complex structure, with a single cell spanning across multiple rows\nand/or columns or featuring multiple headers, making it difficult to identify cell locations.\n3.2\nEvaluation metrics\nTo quantitatively measure performance of a verifier on AutoTV, we define four metrics including\ntable matching accuracy for table matching, cell matching recall, cell matching precision, and F-1\nscore for cell matching as follows.\nTable matching accuracy evaluates the verifier\u2019s ability to accurately identify a source table that\nmatches a given target table, or to determine that no such source table exists in the cited document.\nFormally, given a set of all target tables Tt, a target table t \u2208 Tt with a set of Nt in-table references\nRt = {ri|1 \u2264 i \u2264 Nt}, a set of candidate source tables Ts;ri from a cited document ri, and a\n3\nverifier \u03a6(\u00b7, prompt; \u03b8), the table detection accuracy (Acc.) is defined as:\nAcc. =\nP\nt\u2208Tt\nP\nri\u2208Rt\n\u03b4[sri;t = \u02c6sri;t]\n|Tt|\n,\n\u02c6sri;t = \u03a6(t, Ts;ri, prompt; \u03b8)\n(1)\nwhere \u03b4[\u00b7], \u02c6sri;t and sri;t denote the Kronecker delta function, the detected source table and the\nground-truth source table in the cited document which matches the given target table t, respectively.\nCell matching recall quantifies the percentage of target-source cell matches that are accurately\nidentified (i.e., true positives) among a ground-truth set of cell matches across a source table and a\ntarget table. Let us denote the ground-truth set of Nri;t paired cells between a target table t and a\nsource table sri;t in a cited document ri as Cri;t = {(ct, cri;t)j|1 \u2264 j \u2264 Nri;t} and a set of \u02c6Nri;t\ndetected cell matches as \u02c6Cri;t = {(\u02c6ct, \u02c6cri;t)j|1 \u2264 j \u2264 \u02c6Nri;t} where ct and cri;t represent the row and\ncolumn indices of a cell in the target and source tables, resp. Then, the cell matching recall (Recall)\nis defined as:\nRecall =\nP\nt\u2208Tt\nP\nri\u2208Rt\n|Cri;t \u2229 \u02c6Cri;t|\nP\nt\u2208Tt\nP\nri\u2208Rt\n|Cri;t|\n, \u02c6Cri;t = \u03a6(t, sri;t, prompt; \u03b8)\n(2)\nCell matching precision measures how many target-source cell pairs are true positives among all\nthe detected target-source cell pairs. Using the same notation as above, the cell matching precision\n(Prec.) is defined as:\nPrec. =\nP\nt\u2208Tt\nP\nri\u2208Rt\n|Cri;t \u2229 \u02c6Cri;t|\nP\nt\u2208Tt\nP\nri\u2208Rt\n| \u02c6Cri;t|\n, \u02c6Cri;t = \u03a6(t, sri;t, prompt; \u03b8)\n(3)\nF1 score is a harmonic mean of the cell matching recall and precision to encapsulate both the\nmeasures in a single metric:\nF1 score = 2Prec. \u00d7 Recall\nPrec. + Recall\n(4)\nRemark. All four metrics have a fixed range of [0, 1], with higher values being better. The text\nprompt, denoted by prompt, provided to the verifier may vary with the task, i.e., table matching and\ncell matching.\n3.3\nBaseline methods\nTo tackle AutoTV, we propose baseline approaches for table matching and cell matching as follows.\nTable matching. We utilise a text embedding model (e.g., OpenAI\u2019s text-embedding-ada-\n002) to embed a target table alongside a set of candidate source tables from a document cited in\nthe target table, including their respective captions. It is worth mentioning that the tables are in\nHTML format by default which we extract during the data collection process (detailed in Sec. 4.1).\nSubsequently, we rank the candidate tables based on their cosine similarities with the target table in\nthe embedding space, selecting the one with the highest similarity score that also shares at least one\nfloating point number with the target table. As the final step, we label the prediction as \u201cno match\nfound\u201d if the chosen table\u2019s similarity score falls below a specified threshold.\nIn addition, we consider weighting each candidate table based on the number of floating point\nnumbers that it shares with the target table before ranking them with their cosine similarity. In\nessence, we multiply the similarity score for a candidate table by a weight, which is determined by\nthe number of floating-point numbers shared with the target table. Specifically, we sort the candidate\ntables based on the number of shared floats and assign each table with a weight between 0 and 1\naccording to their rank such that the table with the most shared floats is assigned with 1. These\nweights are evenly distributed with intervals of 1 divided by the count of candidate tables that share at\n4\nTarget-source cell matching\nInput\na target table (target_table), a source table (source_table)\nSystem\nYou are a helpful assistant.\nUser\nCompare the following target and source tables and identify cells\nthat contain floating point numbers with the same meaning present in\nboth tables. Return the matched cells in a Python dictionary with the\nfollowing format:\n{\n(target_table_row_index, target_table_column_index):\n(source_table_row_index, source_table_column_index),\n...\n}\nUse 0-based indexing, including headers, rowspan, and colspan at-\ntributes. Locate as many matching cell pairs as possible. If no matches\nare found, return an empty dictionary ({}).\nThe target table and its caption: {target_table}\nThe source table and its caption: {source_table}\nGPT-4\nAnswer\nTable 1: Text prompt used for the cell matching task. We apply a regular expression to the answer\nstring of the model to ensure the final result follows the specified Python dictionary format.\nleast one floating point number with the target table. For tables that do not share any floating points,\nwe assign a weight of 0. We show the effect of the weighting in Sec. 5.2.\nImportantly, in all cases, floating point numbers are normalized to account for potential unit differ-\nences between the source and target tables.\nCell matching. We employ GPT-4 to extract matches between target-source cells containing a\nfloating point number present in both target and source tables. As detailed in Sec. 3.2, each match\nis depicted as a pair of row and column indices for a cell in the target table and its corresponding\ncell in the source table. To facilitate this, we direct GPT-4 to generate a string representing a Python\ndictionary where keys denote cell indices in the target table and values represent indices in the source\ntable as shown in Tab. 1. We then extract this dictionary using a regular expression that conforms to\na specified dictionary pattern. For cell matching, we experiment with different types of commonly\nused table formats including HTML, CSV, or Markdown and show the effect of the table format\nin Sec. 5.2.\n4\narXiVeri benchmark\nHere, we introduce a benchmark composed of academic papers from arXiv, termed arXiVeri, for\nmeasuring performance of a verifier on the proposed AutoTV task. We first detail the data collection\nprocess (Sec. 4.1) and provide statistics of the arXiVeri benchmark (Sec. 4.2).\n4.1\nData collection\nAs shown in Fig. 2, our data collection process is composed of three steps: (i) target and source paper\nretrieval, (ii) target-source table matching, and (iii) manual cell paring, as detailed next.\nTarget and source paper retrieval. We begin by collecting recent arXiv papers published in\n2022 under the CC-BY license, using the open-source arXiv API.2 We specifically focus on papers\ncategorised as cs.CV, which had the highest number of submissions on arXiv in 2022. We extract\ntables along with their captions from each paper\u2019s HTML5 format using ar5iv,3 which enables us\nto isolate tables from other elements such as main text by accessing the appropriate HTML tags\n(e.g., <table>). Subsequently, we employ an Elasticsearch-based arXiv search system to retrieve\npapers cited in each table (termed the source papers) with their title available in the \"References\"\nsection of the referring paper (termed the target paper). As the title of a cited paper in the References\n2https://github.com/lukasschwab/arxiv.py\n3https://ar5iv.labs.arxiv.org\n5\nFigure 2: Data collection pipeline for the arXiVeri benchmark. Top: We randomly select open-\naccess papers under the CC-BY license from arXiv and extract tables with in-table references (i.e.\ntarget tables) from an HTML5 version of the selected papers. Then, we repeat the process to retrieve\nthe cited papers and their tables. Bottom left: To identify a candidate source table, which supports a\ntarget table, we pick one which has the most cells which are shared with the target table. Bottom\nright: Given the target and the candidate source table, we manually pair the common cells between\nthem. If no paired cells are identified, we conclude that the candidate source table is a false positive\nand the source paper does not contain any matching source table for the target table. See the text for\nthe details. Best viewed in colour.\nsection is often presented with irrelevant information (e.g., a url to a code) for the search system, we\nutilise GPT-3.5-turbo to extract the title from the whole reference information of a cited paper.\nImportantly, to increase the benchmark\u2019s complexity, we omit the cited paper if it does not contain a\ntable sharing at least one cell value with the target table, or if it does not contain more than one table.\nTarget-source table matching. Given a table in a target paper (i.e., the target table) and a source\npaper which is cited in the target table, we select a table among the set of tables extracted from the\nsource paper (using the same method described above) that supports the target table (i.e., the source\ntable). Specifically, we choose a table that has the highest number of shared floating-point numbers\nwith the target table to be the candidate source table. By iterating through all the references in a\ntarget table, we identify a corresponding candidate source table in each cited paper.\nIt is important to note that a target paper can have multiple tables referring to the same source paper,\nresulting in several potential table matchings between the target and source papers. In such cases,\nwe choose the matching with the highest number of overlapping floating-point numbers per one\ntarget-source paper pair to increase the diversity of papers in the arXiVeri benchmark.\nManual cell pairing. In the final step of the collection process, we manually match cells that\nare commonly found in both target and candidate source tables. To determine a correct cell pair,\nwe compare two cells from the tables and mark them as a match if they meet all of the following\nconditions:\n(i) Both cells must represent the same value with an identical meaning, as indicated by their\nrespective row and column headers.\n(ii) Each cell must not contain more than one floating-point number for different metrics,\navoiding the use of delimiters such as a comma (\u2018,\u2019) or a slash (\u2018/\u2019).\n(iii) If both cells have the same number of significant digits and the same unit, they must be\nexactly identical; for example, \u201812.3\u2019 and \u201812.4\u2019 would be treated as an incorrect pair.\nThe first condition ensures that matched cells have the same meaning as well as value, as determined\nby their row and column headers. The second condition aims to remove ambiguity during the\n6\n5\n10\n15\n20\n# tables per source paper\n0\n10\n20\n30\n40\nCount\n20\n40\n60\n80\n# target-source cell pairs\n10\n20\nCount\n0\n10\n20\n30\n40\n# columns\n0\n10\n20\n30\n40\n# rows\nAvg.\ny = x\n100\n200\n300\nFigure 3: Data statistics of the arXiVeri dataset. Left: A histogram illustrating the count distribution\nof tables within source papers. Middle: A histogram representing 3.8K of shared cells between target\nand source tables. Right: A distribution plot of table dimensions (rows and columns), with colour\nindicating table size, and the average dimensions marked in red. Best viewed in colour.\nevaluation step by avoiding cases where a single cell with multiple values is mapped to several cells\nin another table. The third condition accounts for potential discrepancies in rounding methods or\nmistakes, requiring matched cells to have the exact values given the same significant digits. If no\nsuch cell pairs are found between the target and candidate source tables, we regard the table pair does\nnot have a source table from the source paper for the target table. On the other hand, if there is at\nleast one cell pair, we treat the candidate source table as the source table (for the target table).\nPost-processing. To ensure that the models used in our experiments can process each table and its\ncaption as input, we filter out tables whose token length, including their captions, exceeds 3,072, as\nestimated by a tokeniser (i.e., tiktoken4)\n4.2\nStatistics\nWe annotate a total of 3.8K cell pairs from 158 target-source table pairs, involving 110 different\ntarget papers and 158 distinct source papers. As illustrated in Fig. 3, we make three observations: (i)\nsource papers contain an average of 4.6 tables, with three being the most frequent number of tables in\na source paper; (ii) on average, there are 19.5 cell pairs between a target and a source table with the\nminimum and maximum number of cell pairs being 1 and 84, resp.; (iii) the dimensions of tables\nin the dataset exhibit a considerable range, with the smallest table measuring 4 by 5 and the largest\nreaching 20 by 19. On average, tables tend to fall around the size of 15.9 by 8.0.\n5\nExperiments\nIn this section, we first provide implementation details in Sec. 5.1 and conduct ablation studies\nin Sec. 5.2 to investigate each component of our approach for the proposed AutoTV task.\n5.1\nImplementation details\nFor the task of table matching, we employ four different text embedding models. These include\nOpenAI\u2019s text-embedding-ada-002 which has an output dimension of 1536, as well as three\nmodels from Cohere: embed-multilingual-v2.0, embed-english-v2.0, and embed-\nenglish-light-v2.0, with respective output dimensions of 768, 4096, and 1024. For the cell\nmatching task, we employ the gpt-4-0314 model with a maximum length of 8,192 tokens and set\nthe temperature parameter \u03c4 to 0, unless specified otherwise. To further minimize variability in the\nmodel\u2019s performance, we report the average score obtained by running each model three times across\nour experiments.\n5.2\nAblation study\nEffect of text embedding models on table matching. To investigate the influence of selecting\ndifferent text embedding models, we evaluate four different models, as depicted in Tab. 2 (left).\nAlongside, we measure the performance of two baseline strategies: (i) \u201crandom\u201d, which selects a\ntable from a candidate set of source tables, including \u201cno match\u201d, and (ii) \u201coverlap\u201d, which chooses a\ntable that shares the most floating point numbers with the target table. As can be seen, each of the\nfour embedding models significantly outperforms the baseline strategies by a margin of 12.7-15.8%.\nAmong them, the embed-english-light-v2.0 model from Cohere demonstrates the best\nperformance.\n4https://github.com/openai/tiktoken\n7\nmethod\nInc.\ndim. Acc.\nrandom\n-\n-\n13.7\noverlap\n-\n-\n27.2\ntext-embedding-ada-002\nOpenAI 1536 41.1\nembed-multilingual-v2.0\nCohere\n768\n39.9\nembed-english-v2.0\nCohere 4096 42.4\nembed-english-light-v2.0 Cohere 1024 43.0\nmethod\nweighting Acc.\ntext-embedding-ada-002\n\u2717\n36.1\ntext-embedding-ada-002\n\u2713\n41.1 (+5.0)\nembed-multilingual-v2.0\n\u2717\n34.2\nembed-multilingual-v2.0\n\u2713\n39.9 (+5.7)\nembed-english-v2.0\n\u2717\n39.9\nembed-english-v2.0\n\u2713\n42.4 (+2.5)\nembed-english-light-v2.0\n\u2717\n38.6\nembed-english-light-v2.0\n\u2713\n43.0 (+4.4)\nTable 2: Ablation studies on the table matching task. Left: we examine the impact of employing\ndifferent text embedding models. We also offer random and overlap baselines denoted in grey (see\nthe text for details). Right: we investigate the influence of implementing our proposed weighting\nfunction on the predictions generated by each embedding model.\nformat\ncell index Recall\nPrec.\nF1 score\nHTML\n-\n30.8\n22.8\n26.2\nCSV\n\u2717\n21.2\n16.2\n18.4\nMarkdown\n\u2717\n18.1\n13.5\n15.4\nCSV\n\u2713\n48.1 (+26.9) 45.8 (+29.6) 46.9 (+28.5)\nMarkdown\n\u2713\n49.4 (+31.3) 47.1 (+33.6) 48.2 (+32.8)\n\u03c4\nRecall Prec. F1 score\n0.0\n49.4\n47.1\n48.2\n0.25\n50.7\n46.8\n48.7\n0.50\n49.0\n46.8\n47.9\n0.75\n46.0\n44.2\n45.0\n1.0\n48.5\n52.3\n50.3\nTable 3: Ablation studies on the cell matching task. Left: the impact of various input table formats,\nnamely HTML, CSV, and Markdown and the influence of supplying cell indices for each table in the\nCSV and Markdown formats. Right: the effect of the temperature parameter for GPT-4 (using the\nMarkdown format tables).\nEffect of weighting on table matching. As described in Sec. 3.3, we further refine our approach by\nweighting each candidate table based on the number of shared floating point numbers with the target\ntable. Tab. 2 (right) illustrates the impact of this weighting mechanism on the performance of each\nembedding model. Notably, implementing this weighting strategy improves performance across all\nfour embedding models, underscoring its effectiveness.\nEffect of table format and providing cell indices. In the cell matching task, we explore three\ndifferent table formats\u2014HTML, CSV, and Markdown\u2014for feeding tables to GPT-4. We posit that to\nenable the model to accurately identify a cell\u2019s location, providing explicit row and column indices\nfor each cell could be beneficial. To verify this hypothesis, we also assess performance of the model\nwhen row and column indices are explicitly specified on the left and top of a table, resp.\nFrom our results in Tab. 3 (left), we can see that the choice of format significantly influences the\nmodel\u2019s performance with the HTML format yielding the best performance in the absence of cell\nindices. We conjecture that this is because the HTML format contains more distinctive delimiters\nsuch as <tr> and <td> for table rows and table columns compared to CSV or Markdown where the\nmodel has to infer a cell location by counting a line break character and a comma (\u2018,\u2019), which can\nappear in other parts of the input than the actual table (e.g., text prompt and caption). Indeed, when\ncell indices are provided with an input table, we can observe that both of the CSV and Markdown\nformats have significant boost in all of the Recall, Prec., and F1 metrics, outperforming the HTML\nformat. Examples of each format are provided in the supplementary materials.\nEffect of temperature. In addition, we experiment with the temperature parameter in GPT-4, which\nmodulates the randomness of the model\u2019s output. High values (nearing 1) introduce diversity, while\nlow values (tending towards 0) enhance deterministic behavior. As shown in Tab. 3 (right), we\ntest temperatures of {0, 0.25, 0.5, 0.75, 1.0}, and find the optimal performance at a setting of 1.0,\nfavouring diversity.\nQualitative example. In Fig. 4, we visualise GPT-4\u2019s predictions for the cell matching task. Green\nand orange arrows represent correct and incorrect cell matches, respectively. As can be noted, the\nmodel can map semantically identical cells despite unit differences in the target and source tables\n8\nFigure 4: A qualitative example of our approach for the cell matching task. Cells marked in\ngreen denote accurate correspondences, while those highlighted in orange indicate mismatches.\nwhereas it also predicts incorrect mappings between cells that represent different meanings (i.e.,\ndifferent metrics and methods). More qualitative examples are shown in the supplementary materials.\n6\nLimitations\nWe acknowledge several limitations to our approach. First, our task of automatic table verification\ncurrently only processes tables in text formats like HTML, CSV, or Markdown. This means our\napproach may not be suitable for table data embedded within images or PDF files, which are common\nformats in many documents. Second, the data collection pipeline for our arXiVeri benchmark is\nspecifically designed to operate with arXiv papers that can be successfully transformed from PDF to\nHTML format via ar5iv. While this conversion allows us to cleanly extract tables with appropriate\ntags (e.g., <table>), this process may exclude certain papers if the conversion is unsuccessful,\nwhich could limit the diversity of table types included in the benchmark and potentially introduce a\nselection bias. Third, while the benchmark includes data from academic papers on arXiv, it may not\nfully encapsulate the variety of tables encountered across different domains. This could restrict the\ngenerality of our dataset. Future work should aim to extend our data collection pipeline to cater to\na broader range of table sources. Lastly, we have limited insight into the GPT-4 inference process\nvia OpenAI\u2019s API, and it is unclear if our encoded text is pre-processed or the model\u2019s output is\npost-processed. This can potentially affect the reproducibility of the experiments.\n7\nBroader impacts\nThe automatic table verification proposed in this work has potential utility for many domains. In\nscientific research, it can reduce human error in data transcription and thus prevent such errors\nfrom influencing the interpretation of empirical data. In industries such as finance, healthcare, and\nengineering, it can ensure data accuracy, preventing costly mistakes.\nTurning to negative impacts, the deployment of table verification (particularly while it remains far\nfrom perfect) may produce an elevated risk of over-reliance on the technology (with fewer \u201csanity\nchecks\u201d performed by researchers). More broadly, task automation may contribute to potential job\ndisplacement.\nFinally, we note that care must be taken to mitigate privacy risks when deploying table verification\nacross sensitive documents.\n8\nConclusion\nIn this paper we address the critical task of ensuring numerical data accuracy in academic documents\nby introducing a novel task\u2014automatic table verification\u2014leveraging the capabilities of large\nlanguage models. For this, we presented arXiVeri, a benchmark comprising tabular data from arXiv\npapers, and proposed metrics for evaluating verification performance. Despite the sophistication of\nadvanced models like GPT-4, our findings underline the inherent complexity of the task, underscoring\nthe necessity for further research in this field.\n9\nAcknowledgements. GS would like to thank Vishaal Udandarao for proof-reading and Zheng Fang\nfor the enormous support. SA would like to acknowledge the support of Z. Novak and N. Novak in\nenabling his contribution.\nReferences\n[1] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image\nencoder for end-to-end retrieval. arXiv:2104.00650, 2021.\n[2] Iz Beltagy, Kyle Lo, and Arman Cohan. Scibert: Pretrained language model for scientific text. In EMNLP,\n2019.\n[3] Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan\nLeike, Jeff Wu, and William Saunders. Language models can explain neurons in language models, 2023.\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,\nChristopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models\nare few-shot learners. In NeurIPS, 2020.\n[5] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger,\nMichael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder,\nMikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,\nFelipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-\nVoss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir\nBalaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam,\nVedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,\nPeter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba.\nEvaluating large language models trained on code. arXiv:2107.03374, 2021.\n[6] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha\nTsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar\nPrabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael\nIsard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk\nMichalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito,\nDavid Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani\nAgrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor\nLewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi\nWang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern,\nDouglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways.\narXiv:2204.02311, 2022.\n[7] Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, and Zicheng Liu. Violet:\nEnd-to-end video-language transformers with masked visual-token modeling. arXiv:2111.1268, 2021.\n[8] Fabrizio Gilardi, Meysam Alizadeh, and Ma\u00ebl Kubli. Chatgpt outperforms crowd-workers for text-\nannotation tasks. arXiv:2303.15056, 2023.\n[9] Akash Gokul, Konstantinos Kallidromitis, Shufan Li, Yusuke Kato, Kazuki Kozuka, Trevor Darrell, and\nColorado J Reed. Refine and represent: Region-to-object representation learning. arXiv:2208.11821, 2022.\n[10] Leipeng Hao, Liangcai Gao, Xiaohan Yi, and Zhi Tang. A table detection method for pdf documents based\non convolutional neural networks. In DASW, 2016.\n[11] Zhi Hong, Aswathy Ajith, Gregory Pauloski, Eamon Duede, Kyle Chard, and Ian Foster. The diminishing\nreturns of masked language models to science. arXiv:2205.11342, 2023.\n[12] Olivier J. H\u00e9naff, Skanda Koppula, Jean-Baptiste Alayrac, Aaron van den Oord, Oriol Vinyals, and Jo\u00e3o\nCarreira. Efficient visual pretraining with contrastive detection. arXiv:2103.10957, 2021.\n[13] Sana Khamekhem Jemni, Mohamed Ali Souibgui, Yousri Kessentini, and Alicia Forn\u00e9s. Enhance to\nread better: An improved generative adversarial network for handwritten document image enhancement.\narXiv:2105.12710, 2021.\n[14] Yuyan Li and Ye Duan. Multi-scale network with attentional multi-resolution fusion for point cloud\nsemantic segmentation. arXiv:2206.13628, 2022.\n[15] Yuyan Li, Chuanmao Fan, Xu Wang, and Ye Duan. Spnet: Multi-shell kernel convolution for point cloud\nsemantic segmentation. arXiv:2109.11610, 2021.\n[16] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. S2ORC: The semantic\nscholar open research corpus. In ACL, 2020.\n[17] Muhammad Akhtar Munir, Muhammad Haris Khan, M. Saquib Sarfraz, and Mohsen Ali. Synergizing\nbetween self-training and adversarial learning for domain adaptive object detection. arXiv:2110.00249,\n2021.\n[18] OpenAI. Gpt-4 technical report. arXiv:2303.08774, 2023.\n10\n[19] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training\nlanguage models to follow instructions with human feedback. In NeurIPS, 2022.\n[20] Devashish Prasad, Ayan Gadpal, Kshitij Kapadni, Manish Visave, and Kavita Sultanpure. Cascadetabnet:\nAn approach for end to end table detection and structure recognition from image-based documents. In\nCVPRW, 2020.\n[21] Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob\nMenick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth\nRauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan\nUesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant\nJayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini,\nLaurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya,\nDomenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai\nGrigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama,\nCyprien de Masson d\u2019Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark,\nDiego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman,\nLaura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer,\nOriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu,\nand Geoffrey Irving. Scaling language models: Methods, analysis & insights from training gopher.\narXiv:2112.11446, 2022.\n[22] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.\nJMLR, 2020.\n[23] Sebastian Schreiber, Stefan Agne, Ivo Wolf, Andreas Dengel, and Sheraz Ahmed. Deepdesrt: Deep\nlearning for detection and structure recognition of tables in document images. In ICDAR, 2017.\n[24] Faisal Shafait and Ray Smith. Table detection in heterogeneous documents. In Table Detection in\nHeterogeneous Documents, 2010.\n[25] Hoo-Chang Shin, Yang Zhang, Evelina Bakhturina, Raul Puri, Mostofa Patwary, Mohammad Shoeybi, and\nRaghav Mani. BioMegatron: Larger biomedical domain language model. In EMNLP, 2020.\n[26] Brandon Smock, Rohith Pesala, and Robin Abraham. Pubtables-1m: Towards comprehensive table\nextraction from unstructured documents. In CVPR, 2022.\n[27] Mohamed Ali Souibgui, Sanket Biswas, Sana Khamekhem Jemni, Yousri Kessentini, Alicia Forn\u00e9s,\nJosep Llad\u00f3s, and Umapada Pal. Docentr: An end-to-end document image enhancement transformer.\narXiv:2201.10252, 2022.\n[28] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia,\nAndrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science.\narXiv:2211.09085, 2022.\n[29] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia,\nAndrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv\npreprint arXiv:2211.09085, 2022.\n[30] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin\nGhafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen,\nYuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching\nChang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-\nHellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben\nZevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina,\nErin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya\nKuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui,\nMarian Croak, Ed Chi, and Quoc Le. Lamda: Language models for dialog applications. arXiv:2201.08239,\n2022.\n[31] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv:2302.13971,\n2023.\n[32] Hongwei Xue, Yuchong Sun, Bei Liu, Jianlong Fu, Ruihua Song, Houqiang Li, and Jiebo Luo. Clip-vip:\nAdapting pre-trained image-text model to video-language representation alignment. arXiv:2209.06430,\n2023.\n[33] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng\nZhang, Yuxiao Dong, and Jie Tang. Glm-130b: An open bilingual pre-trained model. arXiv:2210.02414,\n2022.\n[34] Mingkai Zheng, Xiu Su, Shan You, Fei Wang, Chen Qian, Chang Xu, and Samuel Albanie. Can gpt-4\nperform neural architecture search? arXiv:2304.10970, 2023.\n11\n[35] Yangtao Zheng, Di Huang, Songtao Liu, and Yunhong Wang. Cross-domain object detection through\ncoarse-to-fine feature adaptation. arXiv:2003.10275, 2020.\n12\n1\n2\n3\n4\n# source papers per target table\n0\n20\n40\n60\n80\nCount\n5\n10\n15\n20\nColumn index\n10\n20\n30\nRow index\n0\n20\n40\n60\nFigure 5: Additional statistics for the arXiVeri bechmark.\nAppendices\nIn this supplementary material, we provide additional statistics for the arXiVeri dataset in Sec. A and\nshow examples of tables in different formats in Sec. B. Then we describe details of the prompts used\nfor our ablation study in Sec. C and provide additional examples of predictions made by GPT-4 on\nthe cell matching task in Sec. D. Lastly, we visualise an actual case where we find human errors in\nthe process of quoting numeric data between tables in Sec. E.\nA\nAdditional statistics\nHere, we provide further details of statistics for the proposed benchmark, arXiVeri. In Fig. 5, we\ndisplay the histogram of source paper counts per target table (left) and the distribution of matched\ncell locations in tables (right). We observe that (i) most target tables have a single source paper which\nis retrieved through the data collection pipeline (as decribed in Sec. 4 of the main paper) with the\nmaximum number of source papers being 4; (ii) the locations of paired cells range from 1 to 33 for\nrow indices, and from 1 to 20 for column indices, with an average cell location of approximately 7.3\nand 4.9, respectively. Note that this is due to the fact that the average dimensions of the tables in the\narXiVeri benchmark is 15.9 by 8.0, causing the average location of matching cells to fall short of the\naverage dimensions of the tables.\n13\nFigure 6: Examples of three different table formats used in our ablation study. From top to\nbottom, the original table from [7], and the HTML, CSV, and Markdown formats are shown.\nB\nExamples of table formats\nIn Fig. 6, we display examples of three table formats considered in our ablation study: HTML,\nCSV, and Markdown. In the case of CSV and Markdown formats, table headers are reformatted to\naccommodate cells spanning multiple columns in the original table by repeating the cell value (i.e.,\nTGIF, MSRVTT, and LSMDC).\n14\nTarget-source cell matching\nInput\na target table (target_table), a source table (source_table)\nSystem You are a helpful assistant.\nUser\nCompare the following target and source tables and identify cells\nthat contain floating point numbers with the same meaning present in\nboth tables. Return the matched cells in a Python dictionary with the\nfollowing format:\n{\n(target_table_row_index, target_table_column_index):\n(source_table_row_index, source_table_column_index),\n...\n}\nUse 0-based indexing. Locate as many matching cell pairs as possible.\nIf no matches are found, return an empty dictionary ({}).\nThe target table and its caption: {target_table}\nThe source table and its caption: {source_table}\nGPT-4 Answer\nTable 4: Text prompt used for CSV and Markdown format tables on the cell matching task.\nTarget-source cell matching\nInput\na target table (target_table), a source table (source_table)\nSystem You are a helpful assistant.\nUser\nCompare the following target and source tables and identify cells\nthat contain floating point numbers with the same meaning present in\nboth tables. Return the matched cells in a Python dictionary with the\nfollowing format:\n{\n(target_table_row_index, target_table_column_index):\n(source_table_row_index, source_table_column_index),\n...\n}\nUse the row and column indices provided on the leftmost column and\nthe topmost row of the tables, respectively. These indices are numerical\nand serve as identifiers to specify the location of each cell within the\ntable. The row index is listed vertically along the left side of the table,\nwhile the column index is listed horizontally at the top. If no matches\nare found, return an empty dictionary ({}).\nThe target table and its caption: {target_table}\nThe source table and its caption: {source_table}\nGPT-4 Answer\nTable 5: Text prompt used for CSV and Markdown format tables with cell indices (on the cell\nmatching task).\nC\nPrompts for cell matching\nWhile the prompt shown in Sec. 3.3 of the main paper is used for tables in the HTML format, a\nslightly different prompt shown in Tab. 4 is utilised for the CSV and Markdown formats due to the\nabsence of HTML attributes such as colspan (refer to Fig. 6 to see the differences between these\ntable formats). For the case where we provide cell indices along with the tables, we employ the\nprompt as shown in Tab. 5.\n15\nFigure 7: Two successful examples of cell matching predicted by GPT-4. Even though there exist\ncells in both the target and source tables with identical values but different meanings as indicated\nby their respective table headers (marked in red), the model appropriately pairs only those cells that\nshare both meaning and value. In both cases, the target tables are located on the left and the source\ntables on the right. The shaded area in the top left table denote cells that have been omitted for visual\nclarity. The tables are from [32, 1, 27, 13]. Best viewed in colour.\nD\nMore qualitative examples\nIn Fig. 7, we present examples of successful applications of GPT-4 for cell matching, while Fig. 8\nshowcases two typical instances where GPT-4 failed to correctly perform the task. As observed in the\nsuccessful cases, the model adeptly pairs cells from the target and source tables based on the shared\nmeaning and value of the cells, even in the presence of hard negatives (i.e., cells highlighted in red)\nthat exhibit the same value but differ in meaning.\nIn the failure cases, we observe that GPT-4 often matches cells based solely on their meanings, as\ndefined by their respective table headers, despite the fact that the actual cell values between the pair\ndiffer. Another frequent type of error involves the model inaccurately locating cell indices.\n16\nFigure 8: Two common failure cases for cell matching are shown. Green and orange arrows denote\ncorrect and incorrect cell matchings. Top: while GPT-4 paired the cells based on the meanings of\nthe cells defined by their table headers, it failed to verify whether the cells also share the same value.\nBottom: GPT-4 incorrectly positioned the cell indices in the target table (upper table) by shifting the\nmatched cells one position to the right. The tables are from [9, 12, 14, 15].\n17\nFigure 9: A practical example of a human error occurring during the process of transferring\nnumerical data from one table to another. Although the numbers from the source table [35] are\naccurately copied and pasted into the target table [17] while maintaining their original order (indicated\nin green), the sequence of the column headers in the target table has been altered (highlighted in red).\nE\nExamples of human errors in transferring numeric data\nHere, we visualise a practical example where we find human errors in the process of quoting numbers\nfrom a table to another. In Fig. 9, we note that the target table contains the numbers from the source\ntable while keeping their order whereas the sequence of column headers is changed. We emphasise\nthat our intention is not to cast blame on the authors of the target paper, but merely to highlight that\nsuch errors can inadvertently occur due to human oversight. This highlights the need for automatic\ntable verification tools.\n18\n"
  },
  {
    "title": "3D molecule generation by denoising voxel grids",
    "link": "https://arxiv.org/pdf/2306.07473.pdf",
    "upvote": "5",
    "text": "3D molecule generation by denoising voxel grids\nPedro O. Pinheiro, Joshua Rackers, Joseph Kleinhenz, Michael Maser, Omar Mahmood,\nAndrew Martin Watkins, Stephen Ra, Vishnu Sresht, Saeed Saremi\nPrescient Design, Genentech\nAbstract\nWe propose a new score-based approach to generate 3D molecules represented as\natomic densities on regular grids. First, we train a denoising neural network that\nlearns to map from a smooth distribution of noisy molecules to the distribution\nof real molecules. Then, we follow the neural empirical Bayes framework [1] and\ngenerate molecules in two steps: (i) sample noisy density grids from a smooth\ndistribution via underdamped Langevin Markov chain Monte Carlo, and (ii) recover\nthe \u201cclean\u201d molecule by denoising the noisy grid with a single step. Our method,\nVoxMol, generates molecules in a fundamentally different way than the current state\nof the art (i.e., diffusion models applied to atom point clouds). It differs in terms of\nthe data representation, the noise model, the network architecture and the generative\nmodeling algorithm. Our experiments show that VoxMol captures the distribution\nof drug-like molecules better than state of the art, while being faster to generate\nsamples. The code is available at https://github.com/genentech/voxmol.\n1\nIntroduction\nFinding novel molecules with desired properties is an important problem in chemistry with applications\nto many scientific domains. In drug discovery in particular, standard computational approaches\nperform some sort of local search\u2014by scoring and ranking molecules\u2014around a region of the\nmolecular space (chosen based on some prior domain knowledge). The space of possible drug-like\nmolecules is prohibitively large (it scales exponentially with the molecular size [2, 3], estimated to\nbe around 1060 [4]), therefore search in this space is very hard. Search-based approaches achieve some\nsuccesses in practice, but have some severe limitations: we can only explore very small portions of the\nmolecular space (on the order of billions to trillions molecules) and these approaches cannot propose\nnew molecules conditioned on some desiderata.\nGenerative models for molecules have been proposed to overcome these limitations and explore the\nmolecular space more efficiently [5]. These approaches often consider one of the following types of\nmolecule representations: (i) one-dimensional sequences such as SMILES [6] or SELFIES [7] (e.g., [8\u2013\n10]), (ii) two-dimensional molecular graphs, where nodes represent atoms or molecular substructures\nand edges represent bonds between them (e.g., [11\u201314]), or (iii) atoms as three-dimensional points\nin space. Molecules are entities laying on three-dimensional space, therefore 3D representations are\narguably the most complete ones\u2014they contain information about atom types, their bonds and the\nmolecular conformation.\nRecent generative models consider molecules as a set of points in 3D Euclidean space and apply\ndiffusion models on them [15\u201320]. Point-cloud representations allow us to use equivariant graph neural\nnetworks [21\u201325]\u2014known to be very effective in molecular discriminative tasks\u2014as the diffusion\nmodel\u2019s denoising network. However, point-based diffusion approaches have some limitations when it\ncomes to generative modeling. First, the number of atoms in the molecule (i.e., nodes on the 3D graph)\nto be diffused need to be known beforehand. Second, atom types and their coordinates have very differ-\nent distributions (categorical and continuous variables, respectively) and are treated separately. Because\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2306.07473v2  [cs.LG]  8 Mar 2024\nFigure 1: Voxelized molecules generated by our model and their corresponding molecular graphs. Left,\nsamples from a model trained on QM9 dataset (323 voxels). Right, samples from a model trained on\nGEOM-drugs (643 voxels). In both cases, each voxel is a cubic grid with side length of .25\u00c5. Each\ncolor represents a different atom (and a different channel on the voxel grid). Best seen in digital version.\nSee appendix for more generated samples.\na score function is undefined on discrete distributions, some workaround is necessary. Finally, graph\nnetworks operate only on nodes and edges (single and pairwise iterations, respectively). Therefore,\ncapturing long-range dependencies over multiple atoms (nodes) can become difficult as the number\nof atoms increases. This is related to the limitations of the message-passing formalism in graph neural\nnetworks [26]. Higher-order message passing can alleviate this problem to a degree [27, 28], but they\ncome at a significant computational cost and they have been limited to third-order models [29] (see next\nsection for more discussions on the tradeoffs between model expressivity and built-in equivariance).1\nIn this work we introduce VoxMol, a new score-based method to generate 3D molecules. Similar\nto [33], and unlike most recent approaches, we represent atoms as continuous (Gaussian-like) densities\nand molecules as a discretization of 3D space on voxel (i.e., a discrete unit of volume) grids. Voxelized\nrepresentations allow us to use the same type of denoising architectures used in computer vision. These\nneural networks\u2014the workhorse behind the success of score-based generative models on images,\ne.g. [34\u201336]\u2014are very effective and scale very well with data.\nWe start by training a neural network to denoise noisy voxelized molecules. Noisy samples are created\nsimply by adding Gaussian noise (with a fixed identity covariance matrix scaled by a large noise level)\nto each voxel in the molecular grid. This denoising network also parametrizes the score function of\nthe smooth/noisy distribution. Note that in contrast to diffusion models, the noise process we use here\ndoes not displace atoms. Then, we leverage the (learned) denoising network and generate molecules in\ntwo steps [1]: (i) (walk) sample noisy density grids from the smooth distribution via Langevin Markov\nchain Monte Carlo (MCMC), and (ii) (jump) recover \u201cclean\u201d molecules by denoising the noisy grid.\nThis sampling scheme, referred to as walk-jump sampling in [1], has been successfully applied before\nto 2D natural images [37, 38] and 1D amino acid sequences [39].\nCompared to point-cloud diffusion models, VoxMol is simpler to train, it does not require knowing\nthe number of atoms beforehand, and it does not treat features as different distributions (continuous,\ncategorical and ordinal for coordinates, atom types and formal charge)\u2014we only use the \u201craw\u201d\nvoxelized molecule. Moreover, due to its expressive network architecture, our method scales better\nto large, drug-sized molecules. Figure 1 (and Figures 8, 9 on appendix) illustrates voxelized molecules\nand their corresponding molecular graphs generated by our model, trained on two different datasets.\nThese samples show visually that our model learns valences of atoms and symmetries of molecules.\nThe main contributions of this work can be summarized as follows. We present VoxMol, a new\nscore-based method for 3D molecule generation.\nThe proposed method differs from current\napproaches\u2014usually diffusion models on point clouds\u2014in terms of the data representation, the noise\nmodel, the network architecture, and the generative modeling algorithm. We show in experiments that\nVoxMol performs slightly worse than state of the art on a small dataset (QM9 [40]), while outperforms\nit (by a large margin) on a challenging, more realistic drug-like molecules dataset (GEOM-drugs [41]).\n1On this topic and since our approach in 3D molecule generation is based in computer vision, we should also\nhighlight here the great success of vision transformer models [30], where in contrast to convolutional neural\nnetworks [31] no notion of equivariance whatsoever is built into the model architecture. Quite remarkably,\nafter training, vision transformers can achieve a higher degree of equivariance compared to convolutional\narchitectures [32].\n2\n2\nRelated Work\nVoxel-based unconditional 3D molecule generation.\nSkalic et al. [42] and Ragoza et al. [33] map\natomic densities on 3D regular grids and train VAEs [43] using 3D convolutional networks to generated\nvoxelized molecules. To recover atomic coordinates from the generated voxel grids2, [33] introduces\na simple optimization-based solution, while [42] trains another model that \u201ctranslates\u201d voxel structures\ninto SMILES strings. Voxel representations are flexible and can trivially be applied to related problems\nwith different data modalities. For instance, [44] proposes a GAN [45] on voxelized electron densities,\nwhile [46] leverages voxelized 3D pharmacophore features to train a pocket-conditional model. Similar\nto these works, our model also relies on discretization of 3D space. Like [33], we use a simple peak de-\ntection algorithm to extract atomiccoordinates fromthe generated voxelgrids. However, our methoddif-\nfers on the underlying generative modeling, architecture, datasets, input representations and evaluations.\nPoint cloud-based unconditional generation.\nMost recent models treat molecules as sets of\npoints, where each node is associated with a particular atom type, its coordinates and potentially extra\ninformation like formal charge. Different modeling approaches have been proposed, e.g., [47\u201349]\nutilize autoregressive models to iteratively sample atoms, and [50, 51] use normalizing flows [52].\nHoogeboom et al. [15] proposes E(3) Equivariant Diffusion Models (EDM), a diffusion [53]-based\napproach that performs considerably better than previous models on this task. EDMs learn to denoise\na diffusion process (operating on both continuous and categorical data) and generate molecules by\niteratively applying the denoising network on an initial noise. Several works have been proposed on\nthe top of EDM [54, 55, 20, 56]. For instance, Xu et al. [56] improves EDM by applying diffusion\non a latent space instead of the atomic coordinates, while MiDi [20] shows that EDM results can be\nimproved by jointly generating the 3D conformation and the connectivity graph of molecules (in this\nsetting, the model has access to both the 3D structure and the 2D connectivity graph).\nConditional 3D molecule generation.\nA related body of work is concerned with conditional\ngeneration. In many cases, conditional generation is built on the top of unconditional generation\nmethods. Some authors propose to predict the 3D structure of the molecules given a molecular graph\n(this is called the conformer generation task): VAEs [57, 58], normalizing flows [59], reinforcement\nlearning [60], optimal transport [61], autoregressive models [62] and diffusion models [63, 16, 64] have\nbeen proposed to this task. Some work [65, 66] condition 3D generation on shape while other works\ncondition molecule generation on other structures. For instance, [17\u201319, 67] adapt (unconditional)\ndiffusion models to condition on protein pockets, while [68] adapt their previous work [33] to condition\nvoxelized structures to protein targets. Finally, [46] proposes a hybrid conditional generation model\nby modeling fragments/scaffolds with point cloud representation and the 3D target structures and\npharmacophores features [69] with voxel grids.\nComparison between voxel and point-cloud representations.\nVoxels have some advantages\nand disadvantages compared to point cloud representations.\nFirst, voxels are straightforward\ngeneralizations of 2D pixels to 3D space, therefore we can leverage similar machinery used in\nscore-based generative modeling for images. These models are known to perform well and scale\nnicely with data. Second, message passing on graphs operate on single and pairwise interactions\nwhile convolution filters (and potentially transformer layers applied to regular grids) can capture\nmultiple local interactions by construction (see [70] for a discussion on the many-body representation\nhypothesis). Third, voxel representations have a higher memory footprint and lower random memory\naccesses than point cloud representations [71]. We note however, that developing models on drug-sized\nmolecules (that is, molecules with size close to those on GEOM-drugs [41]) with reasonable resolution\n(.1\u2013.2\u00c5) is possible on current GPU hardware. Fourth, recovering point coordinates from a discrete\ngrid has no analytical solution, therefore voxel-based models require an extra step to retrieve atomic\ncoordinates. We show empirically that this is not a problem in practice as we can achieve competitive\nresults, even with a very simple peak detection algorithm.\nFinally, graph networks are less expressive due to message passing formalism [26, 27], but are a better\nfit for built-in SE(3)-equivariance architectures (e.g. [21\u201325]). Rotation-equivariant 3D convolutional\n2This is necessary if we want to extract the molecular graph. However, raw voxelized generations could be\nuseful to other computational chemistry tasks.\n3\nnetwork have been proposed [72\u201374] 3, but current models do not scale as well as standard convnets,\nand it would be a challenge to apply them to drug-sized molecules. Built-in rotation equivariance is a\ngood property to have, however equivariance can also be learned with strong data augmentation/larger\ndatasets [75, 76, 32]. In fact, concurrently to this work, [77] also show that built-in SE(3)-equivariant\narchitecture is not necessary to generate molecules. Our experiments show that an expressive denoiser\nscales up better, allowing VoxMol to outperform current state of the art on GEOM-drugs. However,\nwe hope our results motivate exploration of more efficient SE(3)-equivariant convnet architectures.\n3\nMethod\nWe follow previous work (e.g., [78, 33, 70, 79]) and represent atoms as continuous Gaussian-like\natomic densities in 3D space, centered around their atomic coordinates. Molecules are generated by\ndiscretizing the 3D space around the atoms into voxel grids, where each atom type (element) is repre-\nsented by a different grid channel. See appendix for more information on how we discretize molecules.\nThis discretization process gives us a dataset with n voxelized molecules {xi}n\ni=1,xi \u2208Rd,d=c\u00d7l3,\nwhere l is the length of each grid edge and c is the number of atom channels in the dataset. Each voxel\nin the grid can take values between 0 (far from all atoms) and 1 (at the center of atoms). Throughout\nour experiments, we consider a fixed resolution of .25\u00c5 (we found it to be a good trade-off between\naccuracy and computation). Therefore, voxel grids occupy a volume of (l/4)3 cubic \u00c5ngstr\u00f6ms.\n3.1\nBackground: neural empirical Bayes\nLet p(x) be an unknown distribution of voxelized molecules and p(y) a smoother version of it obtained\nby convolving p(x) with an isotropic Gaussian kernel with a known covariance \u03c32Id4. Equivalently,\nY =X+N, where X \u223cp(x), N \u223cN(0,\u03c32Id). Therefore Y is sampled from:\np(y)=\nZ\nRd\n1\n(2\u03c0\u03c32)d/2 exp\n\u0010\n\u2212\u2225y\u2212x\u22252\n2\u03c32\n\u0011\np(x)dx.\nThis transformation will smooth the density of X while still preserving some of the structure\ninformation of the original voxel signals. Robbins [80] showed that if we observe Y = y, then the\nleast-square estimator of X is the Bayes estimator, i.e., \u02c6x(y) = E[X|Y = y]. Built on this result,\nMiyasawa [81] showed that, if the noising process is Gaussian (as in our case), then the least-square\nestimator \u02c6x(y) can be obtained purely from the (unnormalized) smoothed density p(y):\n\u02c6x(y) = y+\u03c32g(y),\n(1)\nwhere g(y) = \u2207y log p(y) is the score function [82] of p(y). This interesting equation tells us that,\nif we know p(y) up to a normalizing constant (and therefore the score function associated with it),\nwe can estimate the original signal x only by observing its noisy version y. Equivalently, if we have\naccess to the estimator \u02c6x(y), we can compute the score function of p(y) via (1).\nOur generative model is based on the neural empirical Bayes (NEB) formalism [1]: we are interested\nin learning the score function of the smoothed density p(y) and the least-square estimator \u02c6x(y) from\na dataset of voxelized molecules {xi}n\ni=1, sampled from unknown p(x). We leverage the (learned)\nestimator and score function to generate voxelized molecules in two steps: (i) sample yk \u223cp(y) with\nLangevin MCMC [83], and (ii) generate clean samples with the least-square estimator. The intuition\nis that it is much easier to sample from the smooth density than the original distribution. See Saremi\nand Hyv\u00e4rinen [1] for more details.\n3.2\nDenoising voxelized molecules\nWe parametrize the Bayes estimator of X using a neural network with parameters \u03b8 denoted by\n\u02c6x\u03b8 : Rd \u2192 Rd. Since the Bayes estimator is the least-squares estimator, the learning becomes a\nleast-squares denoising objective as follows:\nL(\u03b8) = Ex\u223cp(x),y\u223cN(x,\u03c32Id) ||x\u2212\u02c6x\u03b8(y)||2 .\n(2)\n3We made an attempt at using an equivariant 3D convnets for denoising, but initial experiments were not\nsuccessful. See appendix for details.\n4We use the convention where we drop random variable subscripts from probability density function when the\narguments are present: p(x):=pX(x) and p(y):=pY (y).\n4\n<latexit sha1_base64=\"4/xRJSM3P1QXVghlRDQHFg9l\noEo=\">AB8XicbVDJSgNBEK2JW4xb1KOXwSAIQpgRt4sQ9OIxglkwGUJPpyZp0tM9dPcIYchfePGgiFf/xpt/Y2c5aO\nKDgsd7VTVCxPOtPG8bye3tLyupZfL2xsbm3vFHf36lqmimKNSi5VMyQaORNYM8xwbCYKSRxybISD27HfeEKlmRQPZ\nphgEJOeYBGjxFjp8aSNiWZciutOseSVvQncReLPSAlmqHaKX+2upGmMwlBOtG75XmKCjCjDKMdRoZ1qTAgdkB62LBUkR\nh1k4tH7pFVum4klS1h3In6eyIjsdbDOLSdMTF9Pe+Nxf+8VmqiqyBjIkNCjpdFKXcNdIdv+92mUJq+NASQhWzt7q0\nTxShxoZUsCH48y8vkvp2b8on9+flSo3szjycACHcAw+XEIF7qAKNaAg4Ble4c3Rzovz7nxMW3PObGYf/sD5/AE8f5C\nm</latexit>+\u270f =\n<latexit sha1_base64=\"sjv37chHCFu4ba15eA8/hD\n9IU=\">AB73icbVDLSgNBEOyNrxhfqx69DAZBEMKu+LoIQS8eI5gHJEuYncwmQ2Zm15lZISz5CS8eFPHq73jzb5wke9\nDEgoaiqpvurjDhTBvP+3YKS8srq2vF9dLG5tb2jru719Bxqgitk5jHqhViTmTtG6Y4bSVKIpFyGkzHN5O/OYTVZrF8\nsGMEhoI3JcsYgQbK7VOpr1Bb7umWv4k2BFomfkzLkqHXdr04vJqmg0hCOtW7XmKCDCvDCKfjUifVNMFkiPu0banEg\nuogm947RkdW6aEoVrakQVP190SGhdYjEdpOgc1Az3sT8T+vnZroKsiYTFJDJZktilKOTIwmz6MeU5QYPrIE8XsrYgM\nsMLE2IhKNgR/uVF0jit+BeV8/uzcvUmj6MIB3AIx+DJVThDmpQBwIcnuEV3pxH58V5dz5mrQUn9mHP3A+fwCKxo+\np</latexit>+\u03c3 =\n<latexit sha1_base64=\"7+\nEtT7R/9tratiXjOvNcTOIGY=\">AB6nicbVDLSgNBEOyNrx\nhfUY9eBoPgKeyKr2PQi8eI5gHJEmYnvcmQ2dlZlYMSz7Biwd\nFvPpF3vwbJ8keNFrQUFR1090VJIJr47pfTmFpeWV1rbhe2tjc\n2t4p7+41dZwqhg0Wi1i1A6pRcIkNw43AdqKQRoHAVjC6nvqtB\n1Sax/LejBP0IzqQPOSMGivdPfZ4r1xq+4M5C/xclKBHPVe+bP\nbj1kaoTRMUK07npsYP6PKcCZwUuqmGhPKRnSAHUsljVD72ezU\nCTmySp+EsbIlDZmpPycyGmk9jgLbGVEz1IveVPzP6QmvPQzL\npPUoGTzRWEqiInJ9G/S5wqZEWNLKFPc3krYkCrKjE2nZEPwFl\n/+S5onVe+8enZ7Wqld5XEU4QAO4Rg8uIAa3EAdGsBgAE/wAq+\nOcJ6dN+d93lpw8pl9+AXn4xtj0I3i</latexit>xi\n<latexit sha1_base64=\"jXEr5I8cR9vsWQhrB2RPIi6K\nhbc=\">AB6nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PQi8eI5gHJEmYns8mQ2dlplcISz7BiwdFvPpF3vwbJ8keNL\nGgoajqprsrSKQw6LrfTmFldW19o7hZ2tre2d0r7x80TZxqxhslrFuB9RwKRvoEDJ24nmNAokbwWj26nfeuLaiFg94\njhfkQHSoSCUbTSw7gneuWKW3VnIMvEy0kFctR75a9uP2ZpxBUySY3peG6CfkY1Cib5pNRNDU8oG9EB71iqaMSNn81On\nZATq/RJGtbCslM/T2R0ciYcRTYzoji0Cx6U/E/r5NieO1nQiUpcsXmi8JUEozJ9G/SF5ozlGNLKNPC3krYkGrK0KZT\nsiF4iy8vk+Z1busXtyfV2o3eRxFOIJjOAUPrqAGd1CHBjAYwDO8wpsjnRfn3fmYtxacfOYQ/sD5/AFlVo3j</latex\nit>yi\n(a)\n(b)\n<latexit sha1_base64=\"9MCp7Z7eG5/+kPDtJpIDw3IwQo=\">AB7nicbVDLSsNAFL1TX7W+qi7dDBbBjSURX8uiG5cV7APaUCbTSTtkMgkzEyGEfoQbF4q49Xvc\n+TdO2y09cCFwzn3cu89fiK4No7zjUorq2vrG+XNytb2zu5edf+greNUdaisYhV1yeaCS5Zy3AjWDdRjES+YB0/vJv6nSemNI/lo8kS5kVkJHnAKTFW6mSDPDxzJ4Nqzak7M+Bl4hakBgWag+pXfxjTNGLSUEG07rlOYrycKMOpYJNKP9UsITQkI9azVJKIaS+fnTvBJ1YZ4iBWtqTBM/X3RE4irbPIt50RMWO96E3F/7xeaoIbL+cySQ2TdL4\noSAU2MZ7+jodcMWpEZgmhitbMR0TRaixCVsCO7iy8ukfV53r+qXDxe1xm0RxmO4BhOwYVraMA9NKEFEJ4hld4Qwl6Qe/oY95aQsXMIfwB+vwBCaePYw=</latexit>yk\u22121\n<latexit sha1_base64=\"mU/3SKsgp3ClACjd5FP8ACepis=\">AB7HicbVBNS8NAEJ3Ur1q/qh69LBbBU0nEr2PRi8cKxhbaUDbTbt0swm7E6GE/gYvHhTx6g/y5r\n9x2+agrQ8GHu/NMDMvTKUw6LrfTmldW19o7xZ2dre2d2r7h8miTjPskYluh9RwKRT3UaDk7VRzGoeSt8LR7dRvPXFtRKIecJzyIKYDJSLBKFrJH/fy0aRXrbl1dwayTLyC1KBAs1f96vYTlsVcIZPUmI7nphjkVKNgk8q3czwlLIRHfCOpYrG3AT57NgJObFKn0SJtqWQzNTfEzmNjRnHoe2MKQ7NojcV/M6GUbXQS5UmiFXbL4oyiTBhEw/\nJ32hOUM5toQyLeythA2pgxtPhUbgrf48jJ5PKt7l/WL+/Na46aIowxHcAyn4MEVNOAOmuADAwHP8ApvjnJenHfnY95acoqZQ/gD5/MHLT+O8Q=</latexit>yk\n<latexit sha1_base64=\"X2X/z6dTSH57Pet6OsMIr+WFVFg=\">AB7nicbVDLSsNAFL1TX7W+qi7dDBZBEoivpZFNy4r2Ae0oUymk3bIZBJmJkI/Qg3LhRx6/e482+ctl\no64ELh3Pu5d57/ERwbRznG5VWVtfWN8qbla3tnd296v5BW8epoqxFYxGrk80E1yluFGsG6iGIl8wTp+eDf1O09MaR7LR5MlzIvISPKAU2Ks1MkGeXjmTgbVmlN3ZsDLxC1IDQo0B9Wv/jCmacSkoYJo3XOdxHg5UYZTwSaVfqpZQmhIRqxnqSQR014+O3eCT6wyxEGsbEmDZ+rviZxEWmeRbzsjYsZ60ZuK/3m91AQ3Xs5lkhom6XxRkApsYjz9HQ+5YtSIzBJC\nFbe3YjomilBjE6rYENzFl5dJ+7zuXtUvHy5qjdsijIcwTGcgvX0IB7aEILKITwDK/whL0gt7Rx7y1hIqZQ/gD9PkDBpuPYQ=</latexit>yk+1\n<latexit sha1_base64=\"b9xjFCb3AtC7nJPckW97CecJbOI=\">AB83icbVBNS8NAEJ3Ur1q/qh69BItQLyURv45FLx4r2A9oQtlsN+3SzSbsTsQS+je8eFDEq3/Gm/GbZuD\ntj4YeLw3w8y8IBFco+N8W4WV1bX1jeJmaWt7Z3evH/Q0nGqKGvSWMSqExDNBJesiRwF6ySKkSgQrB2Mbqd+5EpzWP5gOE+REZSB5yStBInjckmD1NquPe6LRXrjg1ZwZ7mbg5qUCORq/85fVjmkZMIhVE67rJOhnRCGngk1KXqpZQuiIDFjXUEkipv1sdvPEPjFK3w5jZUqiPVN/T2Qk0nocBaYzIjUi95U/M/rphe+xmXSYpM0vmiMBU2xvY0ALvPFaMoxoY\nQqri51aZDoghFE1PJhOAuvrxMWmc197J2cX9eqd/kcRThCI6hCi5cQR3uoAFNoJDAM7zCm5VaL9a79TFvLVj5zCH8gfX5A+MekZk=</latexit>\u02c6x(yk)\n<latexit sha1_base64=\"r1+l3qYVDHhYoCAB1Iz3mKSt1JA=\">AB83icbVBNS8NAEJ3Ur1q/qh69BItQLyURv45FLx4r2A9oQtlsN+3S\nzSbsTsQS+je8eFDEq3/Gm/GbZuDtj4YeLw3w8y8IBFco+N8W4WV1bX1jeJmaWt7Z3evH/Q0nGqKGvSWMSqExDNBJesiRwF6ySKkSgQrB2Mbqd+5EpzWP5gOE+REZSB5yStBInjckmD1NquMeP+2VK07NmcFeJm5OKpCj0St/ef2YphGTSAXRus6CfoZUcipYJOSl2qWEDoiA9Y1VJ\nKIaT+b3TyxT4zSt8NYmZJoz9TfExmJtB5HgemMCA71ojcV/O6KYbXfsZlkiKTdL4oTIWNsT0NwO5zxSiKsSGEKm5utemQKELRxFQyIbiLy+T1lnNvaxd3J9X6jd5HEU4gmOogtXUIc7aEATKCTwDK/wZqXWi/VufcxbC1Y+cwh/YH3+AOAUkZc=</latexit>\u02c6x(yi)\n<latexit sha1_base64=\"sjv37chHCFu4ba15eA8/hD9IU=\">A\nAB73icbVDLSgNBEOyNrxhfqx69DAZBEMKu+LoIQS8eI5gHJEuYncwmQ2Zm15lZISz5CS8eFPHq73jzb5wke9DEgoaiqpvurjDhTBvP+3Y\nKS8srq2vF9dLG5tb2jru719Bxqgitk5jHqhViTmTtG6Y4bSVKIpFyGkzHN5O/OYTVZrF8sGMEhoI3JcsYgQbK7VOpr1Bb7umWv4k2BF\nomfkzLkqHXdr04vJqmg0hCOtW7XmKCDCvDCKfjUifVNMFkiPu0banEguogm947RkdW6aEoVrakQVP190SGhdYjEdpOgc1Az3sT8T+vnZr\noKsiYTFJDJZktilKOTIwmz6MeU5QYPrIE8XsrYgMsMLE2IhKNgR/uVF0jit+BeV8/uzcvUmj6MIB3AIx+DJVThDmpQBwIcnuEV3pxH\n58V5dz5mrQUn9mHP3A+fwCKxo+p</latexit>+\u03c3 =\n<latexit sha1_base64=\"7+EtT\n7R/9tratiXjOvNcTOIGY=\">AB6nicbVDLSgNBEOyNrxhfUY9eBoPgKe\nyKr2PQi8eI5gHJEmYnvcmQ2dlZlYMSz7BiwdFvPpF3vwbJ8keNFrQUF\nR1090VJIJr47pfTmFpeWV1rbhe2tjc2t4p7+41dZwqhg0Wi1i1A6pRcIk\nNw43AdqKQRoHAVjC6nvqtB1Sax/LejBP0IzqQPOSMGivdPfZ4r1xq+4M\n5C/xclKBHPVe+bPbj1kaoTRMUK07npsYP6PKcCZwUuqmGhPKRnSAHUslj\nVD72ezUCTmySp+EsbIlDZmpPycyGmk9jgLbGVEz1IveVPzP6QmvPQzLp\nPUoGTzRWEqiInJ9G/S5wqZEWNLKFPc3krYkCrKjE2nZEPwFl/+S5onVe\n+8enZ7Wqld5XEU4QAO4Rg8uIAa3EAdGsBgAE/wAq+OcJ6dN+d93lpw8pl\n9+AXn4xtj0I3i</latexit>xi\n<latexit sha1_base64=\"jXEr5I8cR9vsWQhrB2RPIi6Khbc=\">A\nAB6nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PQi8eI5gHJEmYns8mQ2dlplcISz7BiwdFvPpF3vwbJ8keNLGgoajqprsrSKQw6LrfTmF\nldW19o7hZ2tre2d0r7x80TZxqxhslrFuB9RwKRvoEDJ24nmNAokbwWj26nfeuLaiFg94jhfkQHSoSCUbTSw7gneuWKW3VnIMvEy0kFc\ntR75a9uP2ZpxBUySY3peG6CfkY1Cib5pNRNDU8oG9EB71iqaMSNn81OnZATq/RJGtbCslM/T2R0ciYcRTYzoji0Cx6U/E/r5NieO1nQiU\npcsXmi8JUEozJ9G/SF5ozlGNLKNPC3krYkGrK0KZTsiF4iy8vk+Z1busXtyfV2o3eRxFOIJjOAUPrqAGd1CHBjAYwDO8wpsjnRfn3fmY\ntxacfOYQ/sD5/AFlVo3j</latexit>yi\n(a)\n(b)\n<latexit sha1_base64=\"9MCp7Z7eG5/+kPDtJpIDw3IwQo=\">AB7nicbVDLSsNAFL1TX7W+qi7dDBbBjSURX8uiG5cV7APaUCbTSTtkMgkzEyGEfoQbF4q49Xvc+TdO2y09cCFwzn3cu89\nfiK4No7zjUorq2vrG+XNytb2zu5edf+greNUdaisYhV1yeaCS5Zy3AjWDdRjES+YB0/vJv6nSemNI/lo8kS5kVkJHnAKTFW6mSDPDxzJ4Nqzak7M+Bl4hakBgWag+pXfxjTNGLSUEG07rlOYrycKMOpYJNKP9UsITQkI9azVJKIaS+fnTvBJ1YZ4iBWtqTBM/X3RE4irbPIt50RMWO96E3F/7xeaoIbL+cySQ2TdL4oSAU2MZ7+jodcMWpEZgmhitbMR0TRaixCVsCO7iy8ukfV53r+qXDxe1xm0\nRxmO4BhOwYVraMA9NKEFEJ4hld4Qwl6Qe/oY95aQsXMIfwB+vwBCaePYw=</latexit>yk\u22121\n<latexit sha1_base64=\"mU/3SKsgp3ClACjd5FP8ACepis=\">AB7HicbVBNS8NAEJ3Ur1q/qh69LBbBU0nEr2PRi8cKxhbaUDbTbt0swm7E6GE/gYvHhTx6g/y5r9x2+agrQ8GHu/NMDMvTK\nUw6LrfTmldW19o7xZ2dre2d2r7h8miTjPskYluh9RwKRT3UaDk7VRzGoeSt8LR7dRvPXFtRKIecJzyIKYDJSLBKFrJH/fy0aRXrbl1dwayTLyC1KBAs1f96vYTlsVcIZPUmI7nphjkVKNgk8q3czwlLIRHfCOpYrG3AT57NgJObFKn0SJtqWQzNTfEzmNjRnHoe2MKQ7NojcV/M6GUbXQS5UmiFXbL4oyiTBhEw/J32hOUM5toQyLeythA2pgxtPhUbgrf48jJ5PKt7l/WL+/Na46aIowxHcAyn\n4MEVNOAOmuADAwHP8ApvjnJenHfnY95acoqZQ/gD5/MHLT+O8Q=</latexit>yk\n<latexit sha1_base64=\"X2X/z6dTSH57Pet6OsMIr+WFVFg=\">AB7nicbVDLSsNAFL1TX7W+qi7dDBZBEoivpZFNy4r2Ae0oUymk3bIZBJmJkI/Qg3LhRx6/e482+ctlo64ELh3Pu5d57/ERwbRzn\nG5VWVtfWN8qbla3tnd296v5BW8epoqxFYxGrk80E1yluFGsG6iGIl8wTp+eDf1O09MaR7LR5MlzIvISPKAU2Ks1MkGeXjmTgbVmlN3ZsDLxC1IDQo0B9Wv/jCmacSkoYJo3XOdxHg5UYZTwSaVfqpZQmhIRqxnqSQR014+O3eCT6wyxEGsbEmDZ+rviZxEWmeRbzsjYsZ60ZuK/3m91AQ3Xs5lkhom6XxRkApsYjz9HQ+5YtSIzBJCFbe3YjomilBjE6rYENzFl5dJ+7zuXtUvHy5qjdsijIcwTGcgvX0IB7aEILKIT\nwDK/whL0gt7Rx7y1hIqZQ/gD9PkDBpuPYQ=</latexit>yk+1\n<latexit sha1_base64=\"b9xjFCb3AtC7nJPckW97CecJbOI=\">AB83icbVBNS8NAEJ3Ur1q/qh69BItQLyURv45FLx4r2A9oQtlsN+3SzSbsTsQS+je8eFDEq3/Gm/GbZuDtj4YeLw3w8y8IBFco+N8W\n4WV1bX1jeJmaWt7Z3evH/Q0nGqKGvSWMSqExDNBJesiRwF6ySKkSgQrB2Mbqd+5EpzWP5gOE+REZSB5yStBInjckmD1NquPe6LRXrjg1ZwZ7mbg5qUCORq/85fVjmkZMIhVE67rJOhnRCGngk1KXqpZQuiIDFjXUEkipv1sdvPEPjFK3w5jZUqiPVN/T2Qk0nocBaYzIjUi95U/M/rphe+xmXSYpM0vmiMBU2xvY0ALvPFaMoxoYQqri51aZDoghFE1PJhOAuvrxMWmc197J2cX9eqd/kcRThCI6hCi5cQR3uoAFNoJ\nDAM7zCm5VaL9a79TFvLVj5zCH8gfX5A+MekZk=</latexit>\u02c6x(yk)\n<latexit sha1_base64=\"r1+l3qYVDHhYoCAB1Iz3mKSt1JA=\">AB83icbVBNS8NAEJ3Ur1q/qh69BItQLyURv45FLx4r2A9oQtlsN+3SzSbsTsQS+je8eFDE\nq3/Gm/GbZuDtj4YeLw3w8y8IBFco+N8W4WV1bX1jeJmaWt7Z3evH/Q0nGqKGvSWMSqExDNBJesiRwF6ySKkSgQrB2Mbqd+5EpzWP5gOE+REZSB5yStBInjckmD1NquMeP+2VK07NmcFeJm5OKpCj0St/ef2YphGTSAXRus6CfoZUcipYJOSl2qWEDoiA9Y1VJKIaT+b3TyxT4zSt8NYmZJoz9TfExmJtB5HgemMCA71ojcV/O6\nKYbXfsZlkiKTdL4oTIWNsT0NwO5zxSiKsSGEKm5utemQKELRxFQyIbiLy+T1lnNvaxd3J9X6jd5HEU4gmOogtXUIc7aEATKCTwDK/wZqXWi/VufcxbC1Y+cwh/YH3+AOAUkZc=</latexit>\u02c6x(yi)\n<latexit sha1_base64=\"7+EtT\n7R/9tratiXjOvNcTOIGY=\">AB6nicbVDLSgNBEOyNrxhfUY9eBoPgKey\nKr2PQi8eI5gHJEmYnvcmQ2dlZlYMSz7BiwdFvPpF3vwbJ8keNFrQUFR1\n090VJIJr47pfTmFpeWV1rbhe2tjc2t4p7+41dZwqhg0Wi1i1A6pRcIkNw4\n3AdqKQRoHAVjC6nvqtB1Sax/LejBP0IzqQPOSMGivdPfZ4r1xq+4M5C/\nxclKBHPVe+bPbj1kaoTRMUK07npsYP6PKcCZwUuqmGhPKRnSAHUsljVD72\nezUCTmySp+EsbIlDZmpPycyGmk9jgLbGVEz1IveVPzP6QmvPQzLpPUoGT\nzRWEqiInJ9G/S5wqZEWNLKFPc3krYkCrKjE2nZEPwFl/+S5onVe+8enZ7\nWqld5XEU4QAO4Rg8uIAa3EAdGsBgAE/wAq+OcJ6dN+d93lpw8pl9+AXn4x\ntj0I3i</latexit>xi\n<latexit sha1_base64=\"jXEr5I8cR9vsWQhrB2RPIi6Khbc=\">AB6nicbVD\nLSgNBEOyNrxhfUY9eBoPgKeyKr2PQi8eI5gHJEmYns8mQ2dlplcISz7BiwdFvPpF3vwbJ8keNLGgoajqprsrSKQw6LrfTmFldW19o7hZ2tre2d0r7x80TZxqxhsl\nrFuB9RwKRvoEDJ24nmNAokbwWj26nfeuLaiFg94jhfkQHSoSCUbTSw7gneuWKW3VnIMvEy0kFctR75a9uP2ZpxBUySY3peG6CfkY1Cib5pNRNDU8oG9EB71iqaM\nSNn81OnZATq/RJGtbCslM/T2R0ciYcRTYzoji0Cx6U/E/r5NieO1nQiUpcsXmi8JUEozJ9G/SF5ozlGNLKNPC3krYkGrK0KZTsiF4iy8vk+Z1busXtyfV2o3eRxF\nOIJjOAUPrqAGd1CHBjAYwDO8wpsjnRfn3fmYtxacfOYQ/sD5/AFlVo3j</latexit>yi\n<latexit sha1_base64=\"r1+l3qYVDHhYoCAB1Iz3mKSt1JA=\">AB83icbVBNS8NAEJ3Ur1q/qh69BItQLyURv45FLx4r2A9oQtlsN+3SzSbsTsQS+je8eFDEq3/Gm/GbZuD\ntj4YeLw3w8y8IBFco+N8W4WV1bX1jeJmaWt7Z3evH/Q0nGqKGvSWMSqExDNBJesiRwF6ySKkSgQrB2Mbqd+5EpzWP5gOE+REZSB5yStBInjckmD1NquMeP+2VK07NmcFeJm5OKpCj0St/ef2YphGTSAXRus6CfoZUcipYJOSl2qWEDoiA9Y1VJKIaT+b3TyxT4zSt8NYmZJoz9TfExmJtB5HgemMCA71ojcV/O6KYbXfsZlkiKTdL4oTIWNsT0NwO5zxSiKsSG\nEKm5utemQKELRxFQyIbiLy+T1lnNvaxd3J9X6jd5HEU4gmOogtXUIc7aEATKCTwDK/wZqXWi/VufcxbC1Y+cwh/YH3+AOAUkZc=</latexit>\u02c6x(yi)\n(a)\n(b)\n<latexit sha1_base64=\"4/xRJSM3P1QXVghlRDQHFg9l\noEo=\">AB8XicbVDJSgNBEK2JW4xb1KOXwSAIQpgRt4sQ9OIxglkwGUJPpyZp0tM9dPcIYchfePGgiFf/xpt/Y2c5aO\nKDgsd7VTVCxPOtPG8bye3tLyupZfL2xsbm3vFHf36lqmimKNSi5VMyQaORNYM8xwbCYKSRxybISD27HfeEKlmRQPZ\nphgEJOeYBGjxFjp8aSNiWZciutOseSVvQncReLPSAlmqHaKX+2upGmMwlBOtG75XmKCjCjDKMdRoZ1qTAgdkB62LBUkR\nh1k4tH7pFVum4klS1h3In6eyIjsdbDOLSdMTF9Pe+Nxf+8VmqiqyBjIkNCjpdFKXcNdIdv+92mUJq+NASQhWzt7q0\nTxShxoZUsCH48y8vkvp2b8on9+flSo3szjycACHcAw+XEIF7qAKNaAg4Ble4c3Rzovz7nxMW3PObGYf/sD5/AE8f5C\nm</latexit>+\u270f =\n<latexit sha1_base64=\"sjv37chHCFu4ba15eA8/hD9IU=\">AB73icbVDLSgNB\nEOyNrxhfqx69DAZBEMKu+LoIQS8eI5gHJEuYncwmQ2Zm15lZISz5CS8eFPHq73jzb5wke9DEgoaiqpvurjDhTBvP+3YKS8srq2vF9dLG5tb2jru719Bxqgitk5jHqhViTmTtG\n6Y4bSVKIpFyGkzHN5O/OYTVZrF8sGMEhoI3JcsYgQbK7VOpr1Bb7umWv4k2BFomfkzLkqHXdr04vJqmg0hCOtW7XmKCDCvDCKfjUifVNMFkiPu0banEguogm947RkdW6aEoV\nrakQVP190SGhdYjEdpOgc1Az3sT8T+vnZroKsiYTFJDJZktilKOTIwmz6MeU5QYPrIE8XsrYgMsMLE2IhKNgR/uVF0jit+BeV8/uzcvUmj6MIB3AIx+DJVThDmpQBwIcnuE\nV3pxH58V5dz5mrQUn9mHP3A+fwCKxo+p</latexit>+\u03c3 =\n<latexit sha1_base64=\"7+EtT7R/9tratiXjOvNcTOIGY=\">AB6nicbVDLSgNBEOyNr\nxhfUY9eBoPgKeyKr2PQi8eI5gHJEmYnvcmQ2dlZlYMSz7BiwdFvPpF3vwbJ8keNFrQUFR1090VJIJr47pfTmFpeWV1rbhe2tjc2t4p7+41dZwqhg0Wi1i1A6pRcIkNw43AdqKQRoHAVjC6n\nvqtB1Sax/LejBP0IzqQPOSMGivdPfZ4r1xq+4M5C/xclKBHPVe+bPbj1kaoTRMUK07npsYP6PKcCZwUuqmGhPKRnSAHUsljVD72ezUCTmySp+EsbIlDZmpPycyGmk9jgLbGVEz1IveVPzP6\nQmvPQzLpPUoGTzRWEqiInJ9G/S5wqZEWNLKFPc3krYkCrKjE2nZEPwFl/+S5onVe+8enZ7Wqld5XEU4QAO4Rg8uIAa3EAdGsBgAE/wAq+OcJ6dN+d93lpw8pl9+AXn4xtj0I3i</latexit>xi\n<latexit sha1_base64=\"jXEr5I8cR9vsWQhrB2RPIi6Khbc=\">AB6nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PQi8eI5gHJEmYns8mQ2dlplcISz7BiwdFvPpF3vwbJ8k\neNLGgoajqprsrSKQw6LrfTmFldW19o7hZ2tre2d0r7x80TZxqxhslrFuB9RwKRvoEDJ24nmNAokbwWj26nfeuLaiFg94jhfkQHSoSCUbTSw7gneuWKW3VnIMvEy0kFctR75a9uP2ZpxBUySY3peG6CfkY1Cib5pNRNDU8oG9EB71iqaMSNn81OnZATq/RJGtbCslM/T2R0ciYcRTYzoji0Cx6U/E/r5NieO1nQiUpcsXmi8JUEozJ9G/SF5ozlGNLKNPC3krY\nkGrK0KZTsiF4iy8vk+Z1busXtyfV2o3eRxFOIJjOAUPrqAGd1CHBjAYwDO8wpsjnRfn3fmYtxacfOYQ/sD5/AFlVo3j</latexit>yi\n(a)\n(b)\n<latexit sha1_base64=\"9MCp7Z7eG5/+kPDtJpIDw3IwQo=\">AB7nicbVDLSsNAFL1TX7W+qi7dDBbBjSURX8uiG5cV7APaUCbTSTtkMgkzEyGEfoQbF4q49Xvc+TdO2y09cCFwzn3cu89fiK4No7zjUorq2vrG+XNytb2zu5edf+greNUdai\nsYhV1yeaCS5Zy3AjWDdRjES+YB0/vJv6nSemNI/lo8kS5kVkJHnAKTFW6mSDPDxzJ4Nqzak7M+Bl4hakBgWag+pXfxjTNGLSUEG07rlOYrycKMOpYJNKP9UsITQkI9azVJKIaS+fnTvBJ1YZ4iBWtqTBM/X3RE4irbPIt50RMWO96E3F/7xeaoIbL+cySQ2TdL4oSAU2MZ7+jodcMWpEZgmhitbMR0TRaixCVsCO7iy8ukfV53r+qXDxe1xm0RxmO4BhOwYVraMA9NKEFEJ4hld4Qwl6Qe/oY95aQsXMIfwB+vwBCaePYw=</latexit>yk\u22121\n<latexit sha1_base64=\"mU/3SKsgp3ClACjd5FP8ACepis=\">AB7HicbVBNS8NAEJ3Ur1q/qh69LBbBU0nEr2PRi8cKxhbaUDbTbt0swm7E6GE/gYvHhTx6g/y5r9x2+agrQ8GHu/NMDMvTKUw6LrfTmldW19o7xZ2dre2d2r7h8miTjPskY\nluh9RwKRT3UaDk7VRzGoeSt8LR7dRvPXFtRKIecJzyIKYDJSLBKFrJH/fy0aRXrbl1dwayTLyC1KBAs1f96vYTlsVcIZPUmI7nphjkVKNgk8q3czwlLIRHfCOpYrG3AT57NgJObFKn0SJtqWQzNTfEzmNjRnHoe2MKQ7NojcV/M6GUbXQS5UmiFXbL4oyiTBhEw/J32hOUM5toQyLeythA2pgxtPhUbgrf48jJ5PKt7l/WL+/Na46aIowxHcAyn4MEVNOAOmuADAwHP8ApvjnJenHfnY95acoqZQ/gD5/MHLT+O8Q=</latexit>yk\n<latexit sha1_base64=\"X2X/z6dTSH57Pet6OsMIr+WFVFg=\">AB7nicbVDLSsNAFL1TX7W+qi7dDBZBEoivpZFNy4r2Ae0oUymk3bIZBJmJkI/Qg3LhRx6/e482+ctlo64ELh3Pu5d57/ERwbRznG5VWVtfWN8qbla3tnd296v5BW8epoqxFYxGrk80E1\nyluFGsG6iGIl8wTp+eDf1O09MaR7LR5MlzIvISPKAU2Ks1MkGeXjmTgbVmlN3ZsDLxC1IDQo0B9Wv/jCmacSkoYJo3XOdxHg5UYZTwSaVfqpZQmhIRqxnqSQR014+O3eCT6wyxEGsbEmDZ+rviZxEWmeRbzsjYsZ60ZuK/3m91AQ3Xs5lkhom6XxRkApsYjz9HQ+5YtSIzBJCFbe3YjomilBjE6rYENzFl5dJ+7zuXtUvHy5qjdsijIcwTGcgvX0IB7aEILKITwDK/whL0gt7Rx7y1hIqZQ/gD9PkDBpuPYQ=</latexit>yk+1\n<latexit sha1_base64=\"b9xjFCb3AtC7nJPckW97CecJbOI=\">AB83icbVBNS8NAEJ3Ur1q/qh69BItQLyURv45FLx4r2A9oQtlsN+3SzSbsTsQS+je8eFDEq3/Gm/GbZuDtj4YeLw3w8y8IBFco+N8W4WV1bX1jeJmaWt7Z3evH/Q0nGqKGvSWMSqExDNBJes\niRwF6ySKkSgQrB2Mbqd+5EpzWP5gOE+REZSB5yStBInjckmD1NquPe6LRXrjg1ZwZ7mbg5qUCORq/85fVjmkZMIhVE67rJOhnRCGngk1KXqpZQuiIDFjXUEkipv1sdvPEPjFK3w5jZUqiPVN/T2Qk0nocBaYzIjUi95U/M/rphe+xmXSYpM0vmiMBU2xvY0ALvPFaMoxoYQqri51aZDoghFE1PJhOAuvrxMWmc197J2cX9eqd/kcRThCI6hCi5cQR3uoAFNoJDAM7zCm5VaL9a79TFvLVj5zCH8gfX5A+MekZk=</latexit>\u02c6x(yk)\n<latexit sha1_base64=\"r1+l3qYVDHhYoCAB1Iz3mKSt1JA=\">AB83icbVBNS8NAEJ3Ur1q/qh69BItQLyURv45FLx4r2A9oQtlsN+3SzSbsTsQS+je8eFDEq3/Gm/GbZuDtj4YeLw3w8y8IBFco+N8W4\nWV1bX1jeJmaWt7Z3evH/Q0nGqKGvSWMSqExDNBJesiRwF6ySKkSgQrB2Mbqd+5EpzWP5gOE+REZSB5yStBInjckmD1NquMeP+2VK07NmcFeJm5OKpCj0St/ef2YphGTSAXRus6CfoZUcipYJOSl2qWEDoiA9Y1VJKIaT+b3TyxT4zSt8NYmZJoz9TfExmJtB5HgemMCA71ojcV/O6KYbXfsZlkiKTdL4oTIWNsT0NwO5zxSiKsSGEKm5utemQKELRxFQyIbiLy+T1lnNvaxd3J9X6jd5HEU4gmOogtXUIc7aEATKCTwD\nK/wZqXWi/VufcxbC1Y+cwh/YH3+AOAUkZc=</latexit>\u02c6x(yi)\n<latexit sha1_base64=\"7+EtT7R/9tratiXjOvNcTOIGY=\">AB6n\nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PQi8eI5gHJEmYnvcmQ2dlZlYMSz7BiwdFvPpF3vwbJ8keNFrQUFR1090VJIJr47pfTmFpeWV1rbhe2tjc2\nt4p7+41dZwqhg0Wi1i1A6pRcIkNw43AdqKQRoHAVjC6nvqtB1Sax/LejBP0IzqQPOSMGivdPfZ4r1xq+4M5C/xclKBHPVe+bPbj1kaoTRMUK07npsY\nP6PKcCZwUuqmGhPKRnSAHUsljVD72ezUCTmySp+EsbIlDZmpPycyGmk9jgLbGVEz1IveVPzP6QmvPQzLpPUoGTzRWEqiInJ9G/S5wqZEWNLKFPc3kr\nYkCrKjE2nZEPwFl/+S5onVe+8enZ7Wqld5XEU4QAO4Rg8uIAa3EAdGsBgAE/wAq+OcJ6dN+d93lpw8pl9+AXn4xtj0I3i</latexit>xi\n<latexit sha1_base64=\"jXEr5I8cR9vsWQhrB2RPIi6Khbc=\">AB6nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PQi8eI5gHJEmYns8mQ2dlplcISz7BiwdFvP\npF3vwbJ8keNLGgoajqprsrSKQw6LrfTmFldW19o7hZ2tre2d0r7x80TZxqxhslrFuB9RwKRvoEDJ24nmNAokbwWj26nfeuLaiFg94jhfkQHSoSCUbTSw7gneuWKW3VnIMvEy0kFctR75a9uP2ZpxBUySY3peG6CfkY1Cib5pNRNDU8oG9EB71iqaMSNn81OnZATq/RJGtbCslM/T2R0ciYcRTYzoji0Cx6U/E/r5NieO1nQiUpcsXmi\n8JUEozJ9G/SF5ozlGNLKNPC3krYkGrK0KZTsiF4iy8vk+Z1busXtyfV2o3eRxFOIJjOAUPrqAGd1CHBjAYwDO8wpsjnRfn3fmYtxacfOYQ/sD5/AFlVo3j</latexit>yi\n<latexit sha1_base64=\"r1+l3qYVDHhYoCAB1Iz3mKSt1JA=\">AB83icbVBNS8NAEJ3Ur1q/qh69BItQLyURv45FLx4r2A9oQtlsN+3SzSbsTsQS+je8eFDEq3/Gm/GbZuD\ntj4YeLw3w8y8IBFco+N8W4WV1bX1jeJmaWt7Z3evH/Q0nGqKGvSWMSqExDNBJesiRwF6ySKkSgQrB2Mbqd+5EpzWP5gOE+REZSB5yStBInjckmD1NquMeP+2VK07NmcFeJm5OKpCj0St/ef2YphGTSAXRus6CfoZUcipYJOSl2qWEDoiA9Y1VJKIaT+b3TyxT4zSt8NYmZJoz9TfExmJtB5HgemMCA71ojcV/O6KYbXfsZlkiKTdL4oTIWNsT0NwO5zxSiKsSG\nEKm5utemQKELRxFQyIbiLy+T1lnNvaxd3J9X6jd5HEU4gmOogtXUIc7aEATKCTwDK/wZqXWi/VufcxbC1Y+cwh/YH3+AOAUkZc=</latexit>\u02c6x(yi)\n<latexit sha1_base64=\"sjv37chHCFu4ba15eA8/hD9IU=\">AB73icb\nVDLSgNBEOyNrxhfqx69DAZBEMKu+LoIQS8eI5gHJEuYncwmQ2Zm15lZISz5CS8eFPHq73jzb5wke9DEgoaiqpvurjDhTBvP+3YKS8srq2vF9dLG5tb2jru719\nBxqgitk5jHqhViTmTtG6Y4bSVKIpFyGkzHN5O/OYTVZrF8sGMEhoI3JcsYgQbK7VOpr1Bb7umWv4k2BFomfkzLkqHXdr04vJqmg0hCOtW7XmKCDCvDCKf\njUifVNMFkiPu0banEguogm947RkdW6aEoVrakQVP190SGhdYjEdpOgc1Az3sT8T+vnZroKsiYTFJDJZktilKOTIwmz6MeU5QYPrIE8XsrYgMsMLE2IhKNgR/\n/uVF0jit+BeV8/uzcvUmj6MIB3AIx+DJVThDmpQBwIcnuEV3pxH58V5dz5mrQUn9mHP3A+fwCKxo+p</latexit>+\u03c3 =\n<latexit sha1_base64=\"7+EtT7R/9tratiXjOvNcTOIGY=\">AB6nicbVDLS\ngNBEOyNrxhfUY9eBoPgKeyKr2PQi8eI5gHJEmYnvcmQ2dlZlYMSz7BiwdFvPpF3vwbJ8keNFrQUFR1090VJIJr47pfTmFpeWV1rbhe2tjc2t4p7+41dZwqhg0Wi1i1A6p\nRcIkNw43AdqKQRoHAVjC6nvqtB1Sax/LejBP0IzqQPOSMGivdPfZ4r1xq+4M5C/xclKBHPVe+bPbj1kaoTRMUK07npsYP6PKcCZwUuqmGhPKRnSAHUsljVD72ezUCTmyS\np+EsbIlDZmpPycyGmk9jgLbGVEz1IveVPzP6QmvPQzLpPUoGTzRWEqiInJ9G/S5wqZEWNLKFPc3krYkCrKjE2nZEPwFl/+S5onVe+8enZ7Wqld5XEU4QAO4Rg8uIAa3EA\ndGsBgAE/wAq+OcJ6dN+d93lpw8pl9+AXn4xtj0I3i</latexit>xi\n<latexit sha1_base64=\"jXEr5I8cR9vsWQhrB2RPIi6Khbc=\">AB6nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PQi8eI5gHJEmYns8mQ2dlplcISz7Bi\nwdFvPpF3vwbJ8keNLGgoajqprsrSKQw6LrfTmFldW19o7hZ2tre2d0r7x80TZxqxhslrFuB9RwKRvoEDJ24nmNAokbwWj26nfeuLaiFg94jhfkQHSoSCUbTSw7gneuWKW3VnIMvEy0kFctR75a9uP2ZpxBUySY3peG6CfkY1Cib5pNRNDU8oG9EB71iqaMSNn81OnZATq/RJGtbCslM/T2R0ciYcRTYzoji0Cx6U/E/r5N\nieO1nQiUpcsXmi8JUEozJ9G/SF5ozlGNLKNPC3krYkGrK0KZTsiF4iy8vk+Z1busXtyfV2o3eRxFOIJjOAUPrqAGd1CHBjAYwDO8wpsjnRfn3fmYtxacfOYQ/sD5/AFlVo3j</latexit>yi\n(a)\n(b)\n<latexit sha1_base64=\"9MCp7Z7eG5/+kPDtJpIDw3IwQo=\">AB7nicbVDLSsNAFL1TX7W+qi7dDBbBjSURX8uiG5cV7APaUCbTSTtkMgkzEyGEfoQbF4q49Xvc+TdO2y09cCFwzn3cu89fiK4No7zjUorq2vrG+XNy\ntb2zu5edf+greNUdaisYhV1yeaCS5Zy3AjWDdRjES+YB0/vJv6nSemNI/lo8kS5kVkJHnAKTFW6mSDPDxzJ4Nqzak7M+Bl4hakBgWag+pXfxjTNGLSUEG07rlOYrycKMOpYJNKP9UsITQkI9azVJKIaS+fnTvBJ1YZ4iBWtqTBM/X3RE4irbPIt50RMWO96E3F/7xeaoIbL+cySQ2TdL4oSAU2MZ7+jodcMWpEZgmhitbMR0TRaixCVsCO7iy8ukfV53r+qXDxe1xm0RxmO4BhOwYVraMA9NKEFEJ4hld4Qwl6Qe/oY95aQsXMIfwB+vwBCaePYw=</l\natexit>yk\u22121\n<latexit sha1_base64=\"mU/3SKsgp3ClACjd5FP8ACepis=\">AB7HicbVBNS8NAEJ3Ur1q/qh69LBbBU0nEr2PRi8cKxhbaUDbTbt0swm7E6GE/gYvHhTx6g/y5r9x2+agrQ8GHu/NMDMvTKUw6LrfTmldW19o7xZ2dr\ne2d2r7h8miTjPskYluh9RwKRT3UaDk7VRzGoeSt8LR7dRvPXFtRKIecJzyIKYDJSLBKFrJH/fy0aRXrbl1dwayTLyC1KBAs1f96vYTlsVcIZPUmI7nphjkVKNgk8q3czwlLIRHfCOpYrG3AT57NgJObFKn0SJtqWQzNTfEzmNjRnHoe2MKQ7NojcV/M6GUbXQS5UmiFXbL4oyiTBhEw/J32hOUM5toQyLeythA2pgxtPhUbgrf48jJ5PKt7l/WL+/Na46aIowxHcAyn4MEVNOAOmuADAwHP8ApvjnJenHfnY95acoqZQ/gD5/MHLT+O8Q=</latexit>yk\n<latexit sha1_base64=\"X2X/z6dTSH57Pet6OsMIr+WFVFg=\">AB7nicbVDLSsNAFL1TX7W+qi7dDBZBEoivpZFNy4r2Ae0oUymk3bIZBJmJkI/Qg3LhRx6/e482+ctlo64ELh3Pu5d57/ERwbRznG5VWVtfWN8qbla3tnd296v\n5BW8epoqxFYxGrk80E1yluFGsG6iGIl8wTp+eDf1O09MaR7LR5MlzIvISPKAU2Ks1MkGeXjmTgbVmlN3ZsDLxC1IDQo0B9Wv/jCmacSkoYJo3XOdxHg5UYZTwSaVfqpZQmhIRqxnqSQR014+O3eCT6wyxEGsbEmDZ+rviZxEWmeRbzsjYsZ60ZuK/3m91AQ3Xs5lkhom6XxRkApsYjz9HQ+5YtSIzBJCFbe3YjomilBjE6rYENzFl5dJ+7zuXtUvHy5qjdsijIcwTGcgvX0IB7aEILKITwDK/whL0gt7Rx7y1hIqZQ/gD9PkDBpuPYQ=</latexit>yk+1\n<latexit sha1_base64=\"b9xjFCb3AtC7nJPckW97CecJbOI=\">AB83icbVBNS8NAEJ3Ur1q/qh69BItQLyURv45FLx4r2A9oQtlsN+3SzSbsTsQS+je8eFDEq3/Gm/GbZuDtj4YeLw3w8y8IBFco+N8W4WV1bX1jeJmaWt7Z3evH/Q\n0nGqKGvSWMSqExDNBJesiRwF6ySKkSgQrB2Mbqd+5EpzWP5gOE+REZSB5yStBInjckmD1NquPe6LRXrjg1ZwZ7mbg5qUCORq/85fVjmkZMIhVE67rJOhnRCGngk1KXqpZQuiIDFjXUEkipv1sdvPEPjFK3w5jZUqiPVN/T2Qk0nocBaYzIjUi95U/M/rphe+xmXSYpM0vmiMBU2xvY0ALvPFaMoxoYQqri51aZDoghFE1PJhOAuvrxMWmc197J2cX9eqd/kcRThCI6hCi5cQR3uoAFNoJDAM7zCm5VaL9a79TFvLVj5zCH8gfX5A+MekZk=</latexit>\u02c6x(yk)\n<latexit sha1_base64=\"r1+l3qYVDHhYoCAB1Iz3mKSt1JA=\">AB83icbVBNS8NAEJ3Ur1q/qh69BItQLyURv45FLx4r2A9oQtlsN+3SzSbsTsQS+je8eFDEq3/Gm/GbZuDtj4YeL\nw3w8y8IBFco+N8W4WV1bX1jeJmaWt7Z3evH/Q0nGqKGvSWMSqExDNBJesiRwF6ySKkSgQrB2Mbqd+5EpzWP5gOE+REZSB5yStBInjckmD1NquMeP+2VK07NmcFeJm5OKpCj0St/ef2YphGTSAXRus6CfoZUcipYJOSl2qWEDoiA9Y1VJKIaT+b3TyxT4zSt8NYmZJoz9TfExmJtB5HgemMCA71ojcV/O6KYbXfsZlkiKTdL4oTIWNsT0NwO5zxSiKsSGEKm5utemQKELRxFQyI\nbiLy+T1lnNvaxd3J9X6jd5HEU4gmOogtXUIc7aEATKCTwDK/wZqXWi/VufcxbC1Y+cwh/YH3+AOAUkZc=</latexit>\u02c6x(yi)\n(a)\n(b)\n<latexit sha1_base64=\"c1ZMWAfoFGCJAWEuNwpKvOwZ3nA=\">AB6nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PQi8eI5gHJEmYns8mQ2dlplcISz7BiwdFvPpF3vwbJ8keNLGgoajqprsrSKQw6LrfTmFldW19o7hZ2tre2d0r7x80TZxqxhslrFuB9RwKRvoEDJ24nmNAo\nkbwWj26nfeuLaiFg94jhfkQHSoSCUbTSw7jn9soVt+rOQJaJl5MK5Kj3yl/dfszSiCtkhrT8dwE/YxqFEzySambGp5QNqID3rFU0YgbP5udOiEnVumTMNa2FJKZ+nsio5Ex4yiwnRHFoVn0puJ/XifF8NrPhEpS5IrNF4WpJBiT6d+kLzRnKMeWUKaFvZWwIdWUoU2nZEPwFl9eJs2zqndZvbg/r9Ru8jiKcATHcAoeXEN7qAODWAwgGd4hTdHOi/Ou/Mxby04+cwh/IHz+QMO8o2q</latexit>y0\n<latexit sha1_base64=\"c1ZMWAfoFGCJAWEuNwpKvOwZ3nA=\">AB6nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PQi8eI5gHJEmYns8mQ2dlplcISz7BiwdFvPpF3vwbJ8keNLGgoajqprsrSKQw6LrfTmFldW19o7hZ2tre2d0r7x80TZxqxhslrFuB9RwKRvoEDJ24nmNAo\nkbwWj26nfeuLaiFg94jhfkQHSoSCUbTSw7jn9soVt+rOQJaJl5MK5Kj3yl/dfszSiCtkhrT8dwE/YxqFEzySambGp5QNqID3rFU0YgbP5udOiEnVumTMNa2FJKZ+nsio5Ex4yiwnRHFoVn0puJ/XifF8NrPhEpS5IrNF4WpJBiT6d+kLzRnKMeWUKaFvZWwIdWUoU2nZEPwFl9eJs2zqndZvbg/r9Ru8jiKcATHcAoeXEN7qAODWAwgGd4hTdHOi/Ou/Mxby04+cwh/IHz+QMO8o2q</latexit>y0\nOLD:\n<latexit sha1_base64=\"4/xRJSM3P1QXVghlRDQHFg9l\noEo=\">AB8XicbVDJSgNBEK2JW4xb1KOXwSAIQpgRt4sQ9OIxglkwGUJPpyZp0tM9dPcIYchfePGgiFf/xpt/Y2c5aO\nKDgsd7VTVCxPOtPG8bye3tLyupZfL2xsbm3vFHf36lqmimKNSi5VMyQaORNYM8xwbCYKSRxybISD27HfeEKlmRQPZ\nphgEJOeYBGjxFjp8aSNiWZciutOseSVvQncReLPSAlmqHaKX+2upGmMwlBOtG75XmKCjCjDKMdRoZ1qTAgdkB62LBUkR\nh1k4tH7pFVum4klS1h3In6eyIjsdbDOLSdMTF9Pe+Nxf+8VmqiqyBjIkNCjpdFKXcNdIdv+92mUJq+NASQhWzt7q0\nTxShxoZUsCH48y8vkvp2b8on9+flSo3szjycACHcAw+XEIF7qAKNaAg4Ble4c3Rzovz7nxMW3PObGYf/sD5/AE8f5C\nm</latexit>+\u270f =\n<latexit sha1_base64=\"sjv37chHCFu4ba15eA8/hD\n9IU=\">AB73icbVDLSgNBEOyNrxhfqx69DAZBEMKu+LoIQS8eI5gHJEuYncwmQ2Zm15lZISz5CS8eFPHq73jzb5wke9\nDEgoaiqpvurjDhTBvP+3YKS8srq2vF9dLG5tb2jru719Bxqgitk5jHqhViTmTtG6Y4bSVKIpFyGkzHN5O/OYTVZrF8\nsGMEhoI3JcsYgQbK7VOpr1Bb7umWv4k2BFomfkzLkqHXdr04vJqmg0hCOtW7XmKCDCvDCKfjUifVNMFkiPu0banEg\nuogm947RkdW6aEoVrakQVP190SGhdYjEdpOgc1Az3sT8T+vnZroKsiYTFJDJZktilKOTIwmz6MeU5QYPrIE8XsrYgM\nsMLE2IhKNgR/uVF0jit+BeV8/uzcvUmj6MIB3AIx+DJVThDmpQBwIcnuEV3pxH58V5dz5mrQUn9mHP3A+fwCKxo+\np</latexit>+\u03c3 =\n<latexit sha1_base64=\"7+\nEtT7R/9tratiXjOvNcTOIGY=\">AB6nicbVDLSgNBEOyNrx\nhfUY9eBoPgKeyKr2PQi8eI5gHJEmYnvcmQ2dlZlYMSz7Biwd\nFvPpF3vwbJ8keNFrQUFR1090VJIJr47pfTmFpeWV1rbhe2tjc\n2t4p7+41dZwqhg0Wi1i1A6pRcIkNw43AdqKQRoHAVjC6nvqtB\n1Sax/LejBP0IzqQPOSMGivdPfZ4r1xq+4M5C/xclKBHPVe+bP\nbj1kaoTRMUK07npsYP6PKcCZwUuqmGhPKRnSAHUsljVD72ezU\nCTmySp+EsbIlDZmpPycyGmk9jgLbGVEz1IveVPzP6QmvPQzL\npPUoGTzRWEqiInJ9G/S5wqZEWNLKFPc3krYkCrKjE2nZEPwFl\n/+S5onVe+8enZ7Wqld5XEU4QAO4Rg8uIAa3EAdGsBgAE/wAq+\nOcJ6dN+d93lpw8pl9+AXn4xtj0I3i</latexit>xi\n<latexit sha1_base64=\"jXEr5I8cR9vsWQhrB2RPIi6K\nhbc=\">AB6nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PQi8eI5gHJEmYns8mQ2dlplcISz7BiwdFvPpF3vwbJ8keNL\nGgoajqprsrSKQw6LrfTmFldW19o7hZ2tre2d0r7x80TZxqxhslrFuB9RwKRvoEDJ24nmNAokbwWj26nfeuLaiFg94\njhfkQHSoSCUbTSw7gneuWKW3VnIMvEy0kFctR75a9uP2ZpxBUySY3peG6CfkY1Cib5pNRNDU8oG9EB71iqaMSNn81On\nZATq/RJGtbCslM/T2R0ciYcRTYzoji0Cx6U/E/r5NieO1nQiUpcsXmi8JUEozJ9G/SF5ozlGNLKNPC3krYkGrK0KZT\nsiF4iy8vk+Z1busXtyfV2o3eRxFOIJjOAUPrqAGd1CHBjAYwDO8wpsjnRfn3fmYtxacfOYQ/sD5/AFlVo3j</latex\nit>yi\n(a)\n(b)\n<latexit sha1_base64=\"9MCp7Z7eG5/+kPDtJpIDw3IwQo=\">AB7nicbVDLSsNAFL1TX7W+qi7dDBbBjSURX8uiG5cV7APaUCbTSTtkMgkzEyGEfoQbF4q49Xvc\n+TdO2y09cCFwzn3cu89fiK4No7zjUorq2vrG+XNytb2zu5edf+greNUdaisYhV1yeaCS5Zy3AjWDdRjES+YB0/vJv6nSemNI/lo8kS5kVkJHnAKTFW6mSDPDxzJ4Nqzak7M+Bl4hakBgWag+pXfxjTNGLSUEG07rlOYrycKMOpYJNKP9UsITQkI9azVJKIaS+fnTvBJ1YZ4iBWtqTBM/X3RE4irbPIt50RMWO96E3F/7xeaoIbL+cySQ2TdL4\noSAU2MZ7+jodcMWpEZgmhitbMR0TRaixCVsCO7iy8ukfV53r+qXDxe1xm0RxmO4BhOwYVraMA9NKEFEJ4hld4Qwl6Qe/oY95aQsXMIfwB+vwBCaePYw=</latexit>yk\u22121\n<latexit sha1_base64=\"mU/3SKsgp3ClACjd5FP8ACepis=\">AB7HicbVBNS8NAEJ3Ur1q/qh69LBbBU0nEr2PRi8cKxhbaUDbTbt0swm7E6GE/gYvHhTx6g/y5r\n9x2+agrQ8GHu/NMDMvTKUw6LrfTmldW19o7xZ2dre2d2r7h8miTjPskYluh9RwKRT3UaDk7VRzGoeSt8LR7dRvPXFtRKIecJzyIKYDJSLBKFrJH/fy0aRXrbl1dwayTLyC1KBAs1f96vYTlsVcIZPUmI7nphjkVKNgk8q3czwlLIRHfCOpYrG3AT57NgJObFKn0SJtqWQzNTfEzmNjRnHoe2MKQ7NojcV/M6GUbXQS5UmiFXbL4oyiTBhEw/\nJ32hOUM5toQyLeythA2pgxtPhUbgrf48jJ5PKt7l/WL+/Na46aIowxHcAyn4MEVNOAOmuADAwHP8ApvjnJenHfnY95acoqZQ/gD5/MHLT+O8Q=</latexit>yk\n<latexit sha1_base64=\"X2X/z6dTSH57Pet6OsMIr+WFVFg=\">AB7nicbVDLSsNAFL1TX7W+qi7dDBZBEoivpZFNy4r2Ae0oUymk3bIZBJmJkI/Qg3LhRx6/e482+ctl\no64ELh3Pu5d57/ERwbRznG5VWVtfWN8qbla3tnd296v5BW8epoqxFYxGrk80E1yluFGsG6iGIl8wTp+eDf1O09MaR7LR5MlzIvISPKAU2Ks1MkGeXjmTgbVmlN3ZsDLxC1IDQo0B9Wv/jCmacSkoYJo3XOdxHg5UYZTwSaVfqpZQmhIRqxnqSQR014+O3eCT6wyxEGsbEmDZ+rviZxEWmeRbzsjYsZ60ZuK/3m91AQ3Xs5lkhom6XxRkApsYjz9HQ+5YtSIzBJC\nFbe3YjomilBjE6rYENzFl5dJ+7zuXtUvHy5qjdsijIcwTGcgvX0IB7aEILKITwDK/whL0gt7Rx7y1hIqZQ/gD9PkDBpuPYQ=</latexit>yk+1\n<latexit sha1_base64=\"b9xjFCb3AtC7nJPckW97CecJbOI=\">AB83icbVBNS8NAEJ3Ur1q/qh69BItQLyURv45FLx4r2A9oQtlsN+3SzSbsTsQS+je8eFDEq3/Gm/GbZuD\ntj4YeLw3w8y8IBFco+N8W4WV1bX1jeJmaWt7Z3evH/Q0nGqKGvSWMSqExDNBJesiRwF6ySKkSgQrB2Mbqd+5EpzWP5gOE+REZSB5yStBInjckmD1NquPe6LRXrjg1ZwZ7mbg5qUCORq/85fVjmkZMIhVE67rJOhnRCGngk1KXqpZQuiIDFjXUEkipv1sdvPEPjFK3w5jZUqiPVN/T2Qk0nocBaYzIjUi95U/M/rphe+xmXSYpM0vmiMBU2xvY0ALvPFaMoxoY\nQqri51aZDoghFE1PJhOAuvrxMWmc197J2cX9eqd/kcRThCI6hCi5cQR3uoAFNoJDAM7zCm5VaL9a79TFvLVj5zCH8gfX5A+MekZk=</latexit>\u02c6x(yk)\n<latexit sha1_base64=\"r1+l3qYVDHhYoCAB1Iz3mKSt1JA=\">AB83icbVBNS8NAEJ3Ur1q/qh69BItQLyURv45FLx4r2A9oQtlsN+3S\nzSbsTsQS+je8eFDEq3/Gm/GbZuDtj4YeLw3w8y8IBFco+N8W4WV1bX1jeJmaWt7Z3evH/Q0nGqKGvSWMSqExDNBJesiRwF6ySKkSgQrB2Mbqd+5EpzWP5gOE+REZSB5yStBInjckmD1NquMeP+2VK07NmcFeJm5OKpCj0St/ef2YphGTSAXRus6CfoZUcipYJOSl2qWEDoiA9Y1VJ\nKIaT+b3TyxT4zSt8NYmZJoz9TfExmJtB5HgemMCA71ojcV/O6KYbXfsZlkiKTdL4oTIWNsT0NwO5zxSiKsSGEKm5utemQKELRxFQyIbiLy+T1lnNvaxd3J9X6jd5HEU4gmOogtXUIc7aEATKCTwDK/wZqXWi/VufcxbC1Y+cwh/YH3+AOAUkZc=</latexit>\u02c6x(yi)\n<latexit sha1_base64=\"7+EtT\n7R/9tratiXjOvNcTOIGY=\">AB6nicbVDLSgNBEOyNrxhfUY9eBoPgKey\nKr2PQi8eI5gHJEmYnvcmQ2dlZlYMSz7BiwdFvPpF3vwbJ8keNFrQUFR1\n090VJIJr47pfTmFpeWV1rbhe2tjc2t4p7+41dZwqhg0Wi1i1A6pRcIkNw4\n3AdqKQRoHAVjC6nvqtB1Sax/LejBP0IzqQPOSMGivdPfZ4r1xq+4M5C/\nxclKBHPVe+bPbj1kaoTRMUK07npsYP6PKcCZwUuqmGhPKRnSAHUsljVD72\nezUCTmySp+EsbIlDZmpPycyGmk9jgLbGVEz1IveVPzP6QmvPQzLpPUoGT\nzRWEqiInJ9G/S5wqZEWNLKFPc3krYkCrKjE2nZEPwFl/+S5onVe+8enZ7\nWqld5XEU4QAO4Rg8uIAa3EAdGsBgAE/wAq+OcJ6dN+d93lpw8pl9+AXn4x\ntj0I3i</latexit>xi\n<latexit sha1_base64=\"jXEr5I8cR9vsWQhrB2RPIi6Khbc=\">AB6nicbVD\nLSgNBEOyNrxhfUY9eBoPgKeyKr2PQi8eI5gHJEmYns8mQ2dlplcISz7BiwdFvPpF3vwbJ8keNLGgoajqprsrSKQw6LrfTmFldW19o7hZ2tre2d0r7x80TZxqxhsl\nrFuB9RwKRvoEDJ24nmNAokbwWj26nfeuLaiFg94jhfkQHSoSCUbTSw7gneuWKW3VnIMvEy0kFctR75a9uP2ZpxBUySY3peG6CfkY1Cib5pNRNDU8oG9EB71iqaM\nSNn81OnZATq/RJGtbCslM/T2R0ciYcRTYzoji0Cx6U/E/r5NieO1nQiUpcsXmi8JUEozJ9G/SF5ozlGNLKNPC3krYkGrK0KZTsiF4iy8vk+Z1busXtyfV2o3eRxF\nOIJjOAUPrqAGd1CHBjAYwDO8wpsjnRfn3fmYtxacfOYQ/sD5/AFlVo3j</latexit>yi\n<latexit sha1_base64=\"r1+l3qYVDHhYoCAB1Iz3mKSt1JA=\">AB83icbVBNS8NAEJ3Ur1q/qh69BItQLyURv45FLx4r2A9oQtlsN+3SzSbsTsQS+je8eFDEq3/Gm/GbZuD\ntj4YeLw3w8y8IBFco+N8W4WV1bX1jeJmaWt7Z3evH/Q0nGqKGvSWMSqExDNBJesiRwF6ySKkSgQrB2Mbqd+5EpzWP5gOE+REZSB5yStBInjckmD1NquMeP+2VK07NmcFeJm5OKpCj0St/ef2YphGTSAXRus6CfoZUcipYJOSl2qWEDoiA9Y1VJKIaT+b3TyxT4zSt8NYmZJoz9TfExmJtB5HgemMCA71ojcV/O6KYbXfsZlkiKTdL4oTIWNsT0NwO5zxSiKsSG\nEKm5utemQKELRxFQyIbiLy+T1lnNvaxd3J9X6jd5HEU4gmOogtXUIc7aEATKCTwDK/wZqXWi/VufcxbC1Y+cwh/YH3+AOAUkZc=</latexit>\u02c6x(yi)\n<latexit sha1_base64=\"sjv37chHCFu4ba15eA8/hD9IU=\">A\nAB73icbVDLSgNBEOyNrxhfqx69DAZBEMKu+LoIQS8eI5gHJEuYncwmQ2Zm15lZISz5CS8eFPHq73jzb5wke9DEgoaiqpvurjDhTBvP+3Y\nKS8srq2vF9dLG5tb2jru719Bxqgitk5jHqhViTmTtG6Y4bSVKIpFyGkzHN5O/OYTVZrF8sGMEhoI3JcsYgQbK7VOpr1Bb7umWv4k2BF\nomfkzLkqHXdr04vJqmg0hCOtW7XmKCDCvDCKfjUifVNMFkiPu0banEguogm947RkdW6aEoVrakQVP190SGhdYjEdpOgc1Az3sT8T+vnZr\noKsiYTFJDJZktilKOTIwmz6MeU5QYPrIE8XsrYgMsMLE2IhKNgR/uVF0jit+BeV8/uzcvUmj6MIB3AIx+DJVThDmpQBwIcnuEV3pxH\n58V5dz5mrQUn9mHP3A+fwCKxo+p</latexit>+\u03c3 =\n<latexit sha1_base64=\"7+EtT\n7R/9tratiXjOvNcTOIGY=\">AB6nicbVDLSgNBEOyNrxhfUY9eBoPgKe\nyKr2PQi8eI5gHJEmYnvcmQ2dlZlYMSz7BiwdFvPpF3vwbJ8keNFrQUF\nR1090VJIJr47pfTmFpeWV1rbhe2tjc2t4p7+41dZwqhg0Wi1i1A6pRcIk\nNw43AdqKQRoHAVjC6nvqtB1Sax/LejBP0IzqQPOSMGivdPfZ4r1xq+4M\n5C/xclKBHPVe+bPbj1kaoTRMUK07npsYP6PKcCZwUuqmGhPKRnSAHUslj\nVD72ezUCTmySp+EsbIlDZmpPycyGmk9jgLbGVEz1IveVPzP6QmvPQzLp\nPUoGTzRWEqiInJ9G/S5wqZEWNLKFPc3krYkCrKjE2nZEPwFl/+S5onVe\n+8enZ7Wqld5XEU4QAO4Rg8uIAa3EAdGsBgAE/wAq+OcJ6dN+d93lpw8pl\n9+AXn4xtj0I3i</latexit>xi\n<latexit sha1_base64=\"jXEr5I8cR9vsWQhrB2RPIi6Khbc=\">A\nAB6nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PQi8eI5gHJEmYns8mQ2dlplcISz7BiwdFvPpF3vwbJ8keNLGgoajqprsrSKQw6LrfTmF\nldW19o7hZ2tre2d0r7x80TZxqxhslrFuB9RwKRvoEDJ24nmNAokbwWj26nfeuLaiFg94jhfkQHSoSCUbTSw7gneuWKW3VnIMvEy0kFc\ntR75a9uP2ZpxBUySY3peG6CfkY1Cib5pNRNDU8oG9EB71iqaMSNn81OnZATq/RJGtbCslM/T2R0ciYcRTYzoji0Cx6U/E/r5NieO1nQiU\npcsXmi8JUEozJ9G/SF5ozlGNLKNPC3krYkGrK0KZTsiF4iy8vk+Z1busXtyfV2o3eRxFOIJjOAUPrqAGd1CHBjAYwDO8wpsjnRfn3fmY\ntxacfOYQ/sD5/AFlVo3j</latexit>yi\n(a)\n(b)\n<latexit sha1_base64=\"9MCp7Z7eG5/+kPDtJpIDw3IwQo=\">AB7nicbVDLSsNAFL1TX7W+qi7dDBbBjSURX8uiG5cV7APaUCbTSTtkMgkzEyGEfoQbF4q49Xvc+TdO2y09cCFwzn3cu89\nfiK4No7zjUorq2vrG+XNytb2zu5edf+greNUdaisYhV1yeaCS5Zy3AjWDdRjES+YB0/vJv6nSemNI/lo8kS5kVkJHnAKTFW6mSDPDxzJ4Nqzak7M+Bl4hakBgWag+pXfxjTNGLSUEG07rlOYrycKMOpYJNKP9UsITQkI9azVJKIaS+fnTvBJ1YZ4iBWtqTBM/X3RE4irbPIt50RMWO96E3F/7xeaoIbL+cySQ2TdL4oSAU2MZ7+jodcMWpEZgmhitbMR0TRaixCVsCO7iy8ukfV53r+qXDxe1xm0\nRxmO4BhOwYVraMA9NKEFEJ4hld4Qwl6Qe/oY95aQsXMIfwB+vwBCaePYw=</latexit>yk\u22121\n<latexit sha1_base64=\"mU/3SKsgp3ClACjd5FP8ACepis=\">AB7HicbVBNS8NAEJ3Ur1q/qh69LBbBU0nEr2PRi8cKxhbaUDbTbt0swm7E6GE/gYvHhTx6g/y5r9x2+agrQ8GHu/NMDMvTK\nUw6LrfTmldW19o7xZ2dre2d2r7h8miTjPskYluh9RwKRT3UaDk7VRzGoeSt8LR7dRvPXFtRKIecJzyIKYDJSLBKFrJH/fy0aRXrbl1dwayTLyC1KBAs1f96vYTlsVcIZPUmI7nphjkVKNgk8q3czwlLIRHfCOpYrG3AT57NgJObFKn0SJtqWQzNTfEzmNjRnHoe2MKQ7NojcV/M6GUbXQS5UmiFXbL4oyiTBhEw/J32hOUM5toQyLeythA2pgxtPhUbgrf48jJ5PKt7l/WL+/Na46aIowxHcAyn\n4MEVNOAOmuADAwHP8ApvjnJenHfnY95acoqZQ/gD5/MHLT+O8Q=</latexit>yk\n<latexit sha1_base64=\"X2X/z6dTSH57Pet6OsMIr+WFVFg=\">AB7nicbVDLSsNAFL1TX7W+qi7dDBZBEoivpZFNy4r2Ae0oUymk3bIZBJmJkI/Qg3LhRx6/e482+ctlo64ELh3Pu5d57/ERwbRzn\nG5VWVtfWN8qbla3tnd296v5BW8epoqxFYxGrk80E1yluFGsG6iGIl8wTp+eDf1O09MaR7LR5MlzIvISPKAU2Ks1MkGeXjmTgbVmlN3ZsDLxC1IDQo0B9Wv/jCmacSkoYJo3XOdxHg5UYZTwSaVfqpZQmhIRqxnqSQR014+O3eCT6wyxEGsbEmDZ+rviZxEWmeRbzsjYsZ60ZuK/3m91AQ3Xs5lkhom6XxRkApsYjz9HQ+5YtSIzBJCFbe3YjomilBjE6rYENzFl5dJ+7zuXtUvHy5qjdsijIcwTGcgvX0IB7aEILKIT\nwDK/whL0gt7Rx7y1hIqZQ/gD9PkDBpuPYQ=</latexit>yk+1\n<latexit sha1_base64=\"b9xjFCb3AtC7nJPckW97CecJbOI=\">AB83icbVBNS8NAEJ3Ur1q/qh69BItQLyURv45FLx4r2A9oQtlsN+3SzSbsTsQS+je8eFDEq3/Gm/GbZuDtj4YeLw3w8y8IBFco+N8W\n4WV1bX1jeJmaWt7Z3evH/Q0nGqKGvSWMSqExDNBJesiRwF6ySKkSgQrB2Mbqd+5EpzWP5gOE+REZSB5yStBInjckmD1NquPe6LRXrjg1ZwZ7mbg5qUCORq/85fVjmkZMIhVE67rJOhnRCGngk1KXqpZQuiIDFjXUEkipv1sdvPEPjFK3w5jZUqiPVN/T2Qk0nocBaYzIjUi95U/M/rphe+xmXSYpM0vmiMBU2xvY0ALvPFaMoxoYQqri51aZDoghFE1PJhOAuvrxMWmc197J2cX9eqd/kcRThCI6hCi5cQR3uoAFNoJ\nDAM7zCm5VaL9a79TFvLVj5zCH8gfX5A+MekZk=</latexit>\u02c6x(yk)\n<latexit sha1_base64=\"r1+l3qYVDHhYoCAB1Iz3mKSt1JA=\">AB83icbVBNS8NAEJ3Ur1q/qh69BItQLyURv45FLx4r2A9oQtlsN+3SzSbsTsQS+je8eFDE\nq3/Gm/GbZuDtj4YeLw3w8y8IBFco+N8W4WV1bX1jeJmaWt7Z3evH/Q0nGqKGvSWMSqExDNBJesiRwF6ySKkSgQrB2Mbqd+5EpzWP5gOE+REZSB5yStBInjckmD1NquMeP+2VK07NmcFeJm5OKpCj0St/ef2YphGTSAXRus6CfoZUcipYJOSl2qWEDoiA9Y1VJKIaT+b3TyxT4zSt8NYmZJoz9TfExmJtB5HgemMCA71ojcV/O6\nKYbXfsZlkiKTdL4oTIWNsT0NwO5zxSiKsSGEKm5utemQKELRxFQyIbiLy+T1lnNvaxd3J9X6jd5HEU4gmOogtXUIc7aEATKCTwDK/wZqXWi/VufcxbC1Y+cwh/YH3+AOAUkZc=</latexit>\u02c6x(yi)\n(a)\n(b)\n<latexit sha1_base64=\"JPMXk9X9dazmAr5WAOKoJEJ7vxE=\">AB6nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PQi8eI5gHJEmYnvcmQ2dlZlYISz7BiwdFvPpF3vwbJ8keNLGgoajqprsrSATXxnW/ncLK6tr6RnGztLW9s7tX3j9o6jhVDBsFrFqB1Sj4BIbhuB7UQhjQKBrWB0O/VbT6g0j+WjGSfoR3QgecgZNVZ6GPdGvXLFrbozkGXi5aQCOeq98le3H7M0QmYoFp3PDcxfkaV4UzgpNRNSaUjegAO5ZKGqH2s9mpE3JilT4JY2VLGjJTf09kNJ6HAW2M6JmqBe9qfif10lNeO1nXCapQcnmi8JUEBOT6d+kzxUyI8aWUKa4vZWwIVWUGZtOyYbgLb68TJpnV\ne+yenF/Xqnd5HEU4QiO4RQ8uIa3EdGsBgAM/wCm+OcF6cd+dj3lpw8plD+APn8wdoXo3l</latexit>yk\n<latexit sha1_base64=\"c1ZMWAfoFGCJAWEuNwpKvOwZ3nA=\">AB6nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PQi8eI5gHJEmYns8mQ2dlplcISz7BiwdFvPpF3vwbJ8keNLGgoajqprsrSKQw6LrfTmFldW19o7hZ2tre2d0r7x80TZxqxhslrFuB9RwKRvoEDJ24\nnmNAokbwWj26nfeuLaiFg94jhfkQHSoSCUbTSw7jn9soVt+rOQJaJl5MK5Kj3yl/dfszSiCtkhrT8dwE/YxqFEzySambGp5QNqID3rFU0YgbP5udOiEnVumTMNa2FJKZ+nsio5Ex4yiwnRHFoVn0puJ/XifF8NrPhEpS5IrNF4WpJBiT6d+kLzRnKMeWUKaFvZWwIdWUoU2nZEPwFl9eJs2zqndZvbg/r9Ru8jiKcATHcAoeXEN7qAODWAwgGd4hTdHOi/Ou/Mxby04+cwh/IHz+QMO8o2q</latexit>y0\n<latexit sha1_base64=\"JPMXk9X9dazmAr5WAOKoJEJ7vxE=\">AB6nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PQi8eI5gHJEmYnvcmQ2dlZlYISz7BiwdFvPpF3vwbJ8keNLGgoajqprsrSATXxnW/ncLK6tr6RnGztLW9s7tX3j9o6jhVDBsFrFqB1Sj4BIbhuB7UQhjQKBrWB0O/VbT6g0j+WjGSfoR3QgecgZNVZ6GPdGvXLFrbozkGXi5aQCOeq98le3H7M0QmYoFp3PDcxfkaV4UzgpNRNSaUjegAO5ZKGqH2s9mpE3JilT4JY2VLGjJTf09kNJ6HAW2M6JmqBe9qfif10lNeO1nXCapQcnmi8JUEBOT6d+kzxUyI8aWUKa4vZWwIVWUGZtOyYbgLb68TJpnV\ne+yenF/Xqnd5HEU4QiO4RQ8uIa3EdGsBgAM/wCm+OcF6cd+dj3lpw8plD+APn8wdoXo3l</latexit>y\n<latexit sha1_base64=\"c1ZMWAfoFGCJAWEuNwpKvOwZ3nA=\">AB6nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PQi8eI5gHJEmYns8mQ2dlplcISz7BiwdFvPpF3vwbJ8keNLGgoajqprsrSKQw6LrfTmFldW19o7hZ2tre2d0r7x80TZxqxhslrFuB9RwKRvoEDJ24\nnmNAokbwWj26nfeuLaiFg94jhfkQHSoSCUbTSw7jn9soVt+rOQJaJl5MK5Kj3yl/dfszSiCtkhrT8dwE/YxqFEzySambGp5QNqID3rFU0YgbP5udOiEnVumTMNa2FJKZ+nsio5Ex4yiwnRHFoVn0puJ/XifF8NrPhEpS5IrNF4WpJBiT6d+kLzRnKMeWUKaFvZWwIdWUoU2nZEPwFl9eJs2zqndZvbg/r9Ru8jiKcATHcAoeXEN7qAODWAwgGd4hTdHOi/Ou/Mxby04+cwh/IHz+QMO8o2q</latexit>y0\nF\nFigure 2: (a) A representation of our denoising training procedure. Each training sample (i.e., a\nvoxelized molecule) is corrupted with isotropic Gaussian noise with a fixed noise level \u03c3. The model is\ntrained to recover clean voxel grids from the noisy version. To facilitate visualization, we threshold the\ngrid values, \u02c6x=1\u2265.1(\u02c6x). (b) Graphical model representation of the walk-jump sampling scheme. The\ndashed arrows represent the walk, a MCMC chain to draw noisy samples from p(y). The solid arrow\nrepresents the jump. Both walks and jumps leverage the trained denoising network.\nUsing (1), we have the following expression for the smoothed score function in terms of the denoising\nnetwork5 :\ng\u03b8(y)= 1\n\u03c32 (\u02c6x\u03b8(y)\u2212y).\n(3)\nBy minimizing the learning objective (2) we learn the optimal \u02c6x\u03b8 and by using (3) we can compute\nthe score function g\u03b8(y)\u2248\u2207ylogp(y).\nWe model the denoising network \u02c6x\u03b8 with an encoder-decoder 3D convolutional network that maps\nevery noised voxel on the grid to a clean version of it. Figure 2(a) shows a general overview of the\ndenoising model. The noise level, \u03c3, is kept constant during training and is a key hyperparameter of\nthe model. Note that in the empirical Bayes formalism, \u03c3 can be any (large) value.\nCompared to diffusion models, this training scheme is simpler as the noise level is fixed during training.\nVoxMol does not require noise scheduling nor temporal embedding on the network layers. We observe\nempirically that single-step denoising is sufficient to reconstruct voxelized molecules (within the noise\nlevels considered in this paper). Our hypothesis is that this is due to the nature of the voxel signals,\nwhich contain much more \u201cstructure\u201d than \u201ctexture\u201d information, in comparison to natural images.\n3.3\nSampling voxelized molecules\nWe use the learned score function g\u03b8 and the estimator \u02c6x\u03b8 to sample. We follow the walk-jump\nsampling scheme [1, 37\u201339] to generate voxelized molecules xk:\n(i) (walk step) For sampling noisy voxels from p(y), we consider Langevin MCMC algorithms that\nare based on discretizing the underdamped Langevin diffusion [84]:\ndvt =\u2212\u03b3vtdt \u2212 ug\u03b8(yt)dt + (\np\n2\u03b3u)dBt\ndyt =vtdt,\n(4)\nwhere Bt is the standard Brownian motion in Rd, \u03b3 and u are hyperparameters to tune (friction\nand inverse mass, respectively). We use the discretization algorithm proposed by Sachs et al. [85]\nto generate samples yk, which requires a discretization step \u03b4. See appendix for a description\nof the algorithm.\n(ii) (jump step) At an arbitrary time step k, clean samples can be generated by estimating X from\nyk with the denoising network, i.e., computing xk = \u02c6x\u03b8(yk).\nThis approach allows us to approximately sample molecules from p(x) without the need to compute\n(or approximate) \u2207xlog p(x). In fact, we do MCMC on the smooth density p(y), which is known\nto be easier to sample and mixes faster than the original density p(x) [1, 38, 86]. Figure 2(b) shows\na schematic representation of the generation process. Following [37], we initialize the chains at by\nadding uniform noise to Gaussian noise (with the same \u03c3 used during training), i.e., y0 = N + U,\nN \u223cN(0,\u03c32Id), U \u223cUd(0,1) (this was observed to mix faster in practice).\n5Alternatively, one can also parameterize the score function: g\u03b8 :Rd \u2192Rd, in which case \u02c6x\u03b8(y)=y+\u03c32g\u03b8(y).\n5\n<latexit sha1_base64=\"d/RhcRSMbEv4ivpbAndL8To6sfA=\">AB6HicbVDLSgNBEOz1GeMr6tHLYBA8hV3xdQx68ZiAeUCyhNlJbzJmdnaZmRXCki/w4kERr36SN/GSbIHTSxoKq6e4KEsG\n1cd1vZ2V1bX1js7BV3N7Z3dsvHRw2dZwqhg0Wi1i1A6pRcIkNw43AdqKQRoHAVjC6m/qtJ1Sax/LBjBP0IzqQPOSMGivVR71S2a24M5Bl4uWkDlqvdJXtx+zNEJpmKBadzw3MX5GleFM4KTYTUmlI3oADuWShqh9rPZoRNyapU+CWNlSxoyU39PZDTSehwFtjOiZqgXvan4n9dJTXjZ1wmqUHJ5ovCVBATk+nXpM8VMiPGlCmuL2VsCFVlBmbTdG4C2+vEya5xXvqnJZvyhXb/M4CnAMJ3AGHlxDFe6hB\ng1gPAMr/DmPDovzrvzMW9dcfKZI/gD5/MH1cWM+Q=</latexit>k\n<latexit sha1_base64=\"JPMXk9X9dazmAr5WAOKoJEJ7vxE=\">AB6nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PQi8eI5gHJEmYnvcmQ2dlZlYISz7BiwdFvPpF3vwbJ8keNLGg\noajqprsrSATXxnW/ncLK6tr6RnGztLW9s7tX3j9o6jhVDBsFrFqB1Sj4BIbhuB7UQhjQKBrWB0O/VbT6g0j+WjGSfoR3QgecgZNVZ6GPdGvXLFrbozkGXi5aQCOeq98le3H7M0QmYoFp3PDcxfkaV4UzgpNRNSaUjegAO5ZKGqH2s9mpE3JilT4JY2VLGjJTf09kNJ6HAW2M6JmqBe9qfif10lNeO1nXCapQcnmi8JUEBOT6d+kzxUyI8aWUKa4vZWwIVWUGZtOyYbgLb\n68TJpnVe+yenF/Xqnd5HEU4QiO4RQ8uIa3EdGsBgAM/wCm+OcF6cd+dj3lpw8plD+APn8wdoXo3l</latexit>yk\n<latexit sha1_base64=\"b9xjFCb3AtC7nJPckW97CecJbOI=\">AB83icbVBNS8NAEJ3Ur1q/qh69BItQLyURv45FLx4r2A9oQtlsN+3SzSbsTsQS+je8eFDEq3/Gm/GbZuDtj4YeLw3w8y8IBFco+N8W4WV1bX1\njeJmaWt7Z3evH/Q0nGqKGvSWMSqExDNBJesiRwF6ySKkSgQrB2Mbqd+5EpzWP5gOE+REZSB5yStBInjckmD1NquPe6LRXrjg1ZwZ7mbg5qUCORq/85fVjmkZMIhVE67rJOhnRCGngk1KXqpZQuiIDFjXUEkipv1sdvPEPjFK3w5jZUqiPVN/T2Qk0nocBaYzIjUi95U/M/rphe+xmXSYpM0vmiMBU2xvY0ALvPFaMoxoYQqri51aZDoghFE1PJhOAuvrxMWmc197J2cX9eqd/kcRThCI6hCi5cQR3uoAFNoJDAM7zCm5VaL9a79TFvLVj5\nzCH8gfX5A+MekZk=</latexit>\u02c6x(yk)\n<latexit sha1_base64=\"c1ZMWAfoFGCJAWEuNwpKvOwZ3nA=\">AB6nicbVDLSgNB\nEOyNrxhfUY9eBoPgKeyKr2PQi8eI5gHJEmYns8mQ2dlplcISz7BiwdFvPpF3vwbJ8keNLGgoajqprsrSKQw6LrfTmFldW19o7hZ2tre2d0r7x80TZxqxhslrFuB9RwKRvoED\nJ24nmNAokbwWj26nfeuLaiFg94jhfkQHSoSCUbTSw7jn9soVt+rOQJaJl5MK5Kj3yl/dfszSiCtkhrT8dwE/YxqFEzySambGp5QNqID3rFU0YgbP5udOiEnVumTMNa2FJKZ+n\nsio5Ex4yiwnRHFoVn0puJ/XifF8NrPhEpS5IrNF4WpJBiT6d+kLzRnKMeWUKaFvZWwIdWUoU2nZEPwFl9eJs2zqndZvbg/r9Ru8jiKcATHcAoeXEN7qAODWAwgGd4hTdHOi/Ou\n/Mxby04+cwh/IHz+QMO8o2q</latexit>y0\n<latexit sha1_base64=\"d/RhcRSMbEv4ivpbAndL8To6sfA=\">AB6HicbVDLSgNBEOz1GeMr6tHLYBA8hV3xdQx68ZiAeUCyhNlJbzJmdnaZmRXCki/w4kERr36SN/GSbIHTSxoKq6e4KEsG\n1cd1vZ2V1bX1js7BV3N7Z3dsvHRw2dZwqhg0Wi1i1A6pRcIkNw43AdqKQRoHAVjC6m/qtJ1Sax/LBjBP0IzqQPOSMGivVR71S2a24M5Bl4uWkDlqvdJXtx+zNEJpmKBadzw3MX5GleFM4KTYTUmlI3oADuWShqh9rPZoRNyapU+CWNlSxoyU39PZDTSehwFtjOiZqgXvan4n9dJTXjZ1wmqUHJ5ovCVBATk+nXpM8VMiPGlCmuL2VsCFVlBmbTdG4C2+vEya5xXvqnJZvyhXb/M4CnAMJ3AGHlxDFe6hB\ng1gPAMr/DmPDovzrvzMW9dcfKZI/gD5/MH1cWM+Q=</latexit>k\n<latexit sha1_base64=\"JPMXk9X9dazmAr5WAOKoJEJ7vxE=\">AB6nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PQi8eI5gHJEmYnvcmQ2dlZlYISz7BiwdFvPpF3vwbJ8keNLGg\noajqprsrSATXxnW/ncLK6tr6RnGztLW9s7tX3j9o6jhVDBsFrFqB1Sj4BIbhuB7UQhjQKBrWB0O/VbT6g0j+WjGSfoR3QgecgZNVZ6GPdGvXLFrbozkGXi5aQCOeq98le3H7M0QmYoFp3PDcxfkaV4UzgpNRNSaUjegAO5ZKGqH2s9mpE3JilT4JY2VLGjJTf09kNJ6HAW2M6JmqBe9qfif10lNeO1nXCapQcnmi8JUEBOT6d+kzxUyI8aWUKa4vZWwIVWUGZtOyYbgLb\n68TJpnVe+yenF/Xqnd5HEU4QiO4RQ8uIa3EdGsBgAM/wCm+OcF6cd+dj3lpw8plD+APn8wdoXo3l</latexit>yk\n<latexit sha1_base64=\"b9xjFCb3AtC7nJPckW97CecJbOI=\">AB83icbVBNS8NAEJ3Ur1q/qh69BItQLyURv45FLx4r2A9oQtlsN+3SzSbsTsQS+je8eFDEq3/Gm/GbZuDtj4YeLw3w8y8IBFco+N8W4WV1bX1\njeJmaWt7Z3evH/Q0nGqKGvSWMSqExDNBJesiRwF6ySKkSgQrB2Mbqd+5EpzWP5gOE+REZSB5yStBInjckmD1NquPe6LRXrjg1ZwZ7mbg5qUCORq/85fVjmkZMIhVE67rJOhnRCGngk1KXqpZQuiIDFjXUEkipv1sdvPEPjFK3w5jZUqiPVN/T2Qk0nocBaYzIjUi95U/M/rphe+xmXSYpM0vmiMBU2xvY0ALvPFaMoxoYQqri51aZDoghFE1PJhOAuvrxMWmc197J2cX9eqd/kcRThCI6hCi5cQR3uoAFNoJDAM7zCm5VaL9a79TFvLVj5\nzCH8gfX5A+MekZk=</latexit>\u02c6x(yk)\n<latexit sha1_base64=\"c1ZMWAfoFGCJAWEuNwpKvOwZ3nA=\">AB6nicbVDLSgNB\nEOyNrxhfUY9eBoPgKeyKr2PQi8eI5gHJEmYns8mQ2dlplcISz7BiwdFvPpF3vwbJ8keNLGgoajqprsrSKQw6LrfTmFldW19o7hZ2tre2d0r7x80TZxqxhslrFuB9RwKRvoED\nJ24nmNAokbwWj26nfeuLaiFg94jhfkQHSoSCUbTSw7jn9soVt+rOQJaJl5MK5Kj3yl/dfszSiCtkhrT8dwE/YxqFEzySambGp5QNqID3rFU0YgbP5udOiEnVumTMNa2FJKZ+n\nsio5Ex4yiwnRHFoVn0puJ/XifF8NrPhEpS5IrNF4WpJBiT6d+kLzRnKMeWUKaFvZWwIdWUoU2nZEPwFl9eJs2zqndZvbg/r9Ru8jiKcATHcAoeXEN7qAODWAwgGd4hTdHOi/Ou\n/Mxby04+cwh/IHz+QMO8o2q</latexit>y0\n<latexit sha1_base64=\"b9xjFCb3AtC7nJPckW97CecJbOI=\">AB83icbVBNS8NAEJ3Ur1q/qh69BItQLyURv45FLx4r2A9oQtlsN+3SzSbsTsQS+je8eFDEq3/Gm/GbZuDtj4YeLw3w8y8IBFco+N8W4WV1bX1\njeJmaWt7Z3evH/Q0nGqKGvSWMSqExDNBJesiRwF6ySKkSgQrB2Mbqd+5EpzWP5gOE+REZSB5yStBInjckmD1NquPe6LRXrjg1ZwZ7mbg5qUCORq/85fVjmkZMIhVE67rJOhnRCGngk1KXqpZQuiIDFjXUEkipv1sdvPEPjFK3w5jZUqiPVN/T2Qk0nocBaYzIjUi95U/M/rphe+xmXSYpM0vmiMBU2xvY0ALvPFaMoxoYQqri51aZDoghFE1PJhOAuvrxMWmc197J2cX9eqd/kcRThCI6hCi5cQR3uoAFNoJDAM7zCm5VaL9a79TFvLVj5\nzCH8gfX5A+MekZk=</latexit>\u02c6x(yk)\nFigure 3: Illustration of walk-jump sampling chain. We do Langevin MCMC on the noisy distribution\n(walk) and estimate clean samples with the denoising network at arbitrary time (jump).\nThe noise level plays a key role in this sampling framework. If the noise is low, denoising (jump step)\nbecomes easier, with lower variance, while sampling a \u201cless smooth\u201d p(y) (walk step) becomes harder.\nIf the noise is high, the opposite is true.\nFigure 3 illustrates an example of a walk-jump sampling chain, where generated molecules change grad-\nually as we walk through the chain (the clean samples are shown every ten steps, \u2206k=10). This figure\nis a demonstration of the fast-mixing properties of our sampling scheme in generating 3D molecules.\nFor instance, some atoms (or other structures like rings) might appear/disappear/change as we move\nthrough the chain. Interestingly, this behavior happened on most chains we looked into explicitly.\n3.4\nRecovering atomic coordinates from voxelized molecules\nIt is often useful to extract atomic coordinates from generated voxelized molecules (e.g., to validate\natomic valences and bond types or compare with other models). We use a very simple algorithm (a\nsimplified version of the approach used in [33]) to recover the set of atomic coordinates from generated\nvoxel grids: first we set to 0 all voxels with value less than .1, i.e., xk =1\u2265.1(xk). Then we run a simple\npeak detection to locate the voxel on the center of each Gaussian blob (corresponding to the center\nof each atom). Finally we run a simple gradient descent coordinate optimization algorithm to find the\nset of points that best create the generated voxelized molecule. Once we have obtained the optimized\natomic coordinates, we follow previous work [33, 18, 17, 20] and use standard cheminformatics\nsoftware to determine the molecule\u2019s atomic bonds. Figure 4 shows our pipeline to recover atomic\ncoordinates and molecular graphs from generated voxelized molecules. See appendix for more details.\n4\nExperiments\nIn this section, we evaluate the performance of our model on the task of unconditional 3D molecule\ngeneration. Our approach is the first of its kind and therefore the objective of our experiments is to\nshow that (i) VoxMol is a feasible approach for unconditional generation (this is non-trivial) and\n(ii) it scales well with data, beating a established model on a large, drug-like dataset. In principle,\nVoxMol can be used for guided (or conditional) generation, an arguably more useful application for\nmolecular sciences (see appendix for a discussion on how guidance can be used on generation).\nWe start with a description of our experimental setup, followed by results on two popular datasets\nfor this problem. We then show ablation studies performed on different components of the model.\n4.1\nExperimental setup\nArchitecture.\nThe denoising network \u02c6x\u03b8 is used in both the walk and jump steps described above.\nTherefore, its parametrization is very important to the performance of this approach. We use a\n3D U-Net [87] architecture for our denoising network. We follow the same architecture recipe as\nDDPM [34], with two differences: we use 3D convnets instead of 2D and we use fewer channels on\nall layers. The model has 4 levels of resolution and we use self-attention on the two lowest resolutions.\nWe augment our dataset during training by applying random rotation and translation to every training\nsample. Our models are trained with noise level \u03c3=.9, unless stated otherwise. We train our models\nwith batch size of 128 and 64 (for QM9 and GEOM-drugs, respectively) and we use AdamW [88]\n6\npost-processing\n[H][N]=[C]1[C]([H])=[C]([H]) \n[C]2=[N]O[C](=[O])N12\n<latexit sha1_base64=\"9MCp7Z7eG5/+kPDtJpIDw3IwQo=\">AB7nicbVDLSsNAFL1TX7W+qi7dDBbBjSURX8uiG5cV7APaUCbTSTtkMgkzEyGEfoQbF4q49Xvc+TdO2y09cCFwzn3cu89fiK4No7zjUorq2vrG+XNytb2zu5edf+greNUdaisYhV1yea\nCS5Zy3AjWDdRjES+YB0/vJv6nSemNI/lo8kS5kVkJHnAKTFW6mSDPDxzJ4Nqzak7M+Bl4hakBgWag+pXfxjTNGLSUEG07rlOYrycKMOpYJNKP9UsITQkI9azVJKIaS+fnTvBJ1YZ4iBWtqTBM/X3RE4irbPIt50RMWO96E3F/7xeaoIbL+cySQ2TdL4oSAU2MZ7+jodcMWpEZgmhitbMR0TRaixCVsCO7iy8ukfV53r+qXDxe1xm0RxmO4BhOwYVraMA9NKEFEJ4hld4Qwl6Qe/oY95aQsXMIfwB+vwBCaePYw=</latexit>yk\u22121\n<latexit sha1_base64=\"mU/3SKsgp3ClACjd5FP8ACepis=\">AB7HicbVBNS8NAEJ3Ur1q/qh69LBbBU0nEr2PRi8cKxhbaUDbTbt0swm7E6GE/gYvHhTx6g/y5r9x2+agrQ8GHu/NMDMvTKUw6LrfTmldW19o7xZ2dre2d2r7h8miTjPskYluh9RwKRT\n3UaDk7VRzGoeSt8LR7dRvPXFtRKIecJzyIKYDJSLBKFrJH/fy0aRXrbl1dwayTLyC1KBAs1f96vYTlsVcIZPUmI7nphjkVKNgk8q3czwlLIRHfCOpYrG3AT57NgJObFKn0SJtqWQzNTfEzmNjRnHoe2MKQ7NojcV/M6GUbXQS5UmiFXbL4oyiTBhEw/J32hOUM5toQyLeythA2pgxtPhUbgrf48jJ5PKt7l/WL+/Na46aIowxHcAyn4MEVNOAOmuADAwHP8ApvjnJenHfnY95acoqZQ/gD5/MHLT+O8Q=</latexit>yk\n<latexit sha1_base64=\"X2X/z6dTSH57Pet6OsMIr+WFVFg=\">AB7nicbVDLSsNAFL1TX7W+qi7dDBZBEoivpZFNy4r2Ae0oUymk3bIZBJmJkI/Qg3LhRx6/e482+ctlo64ELh3Pu5d57/ERwbRznG5VWVtfWN8qbla3tnd296v5BW8epoqxFYxGrk80E1yluFGsG6iGIl8wTp+eDf1O09MaR7LR5MlzIvISPKAU2Ks1MkGeXjmTgbVmlN3ZsDLxC1IDQo0B9Wv/jCmacSkoYJo3XOdxHg5UYZTwSaVfqpZQmhIRqxnqSQR014+O3eCT6wyxEGsbEmDZ+rviZxEWmeRbzsjYsZ60ZuK/3m91AQ3Xs5lkhom6XxRkApsYjz9HQ+5YtSIzBJCFbe3Yjomi\nlBjE6rYENzFl5dJ+7zuXtUvHy5qjdsijIcwTGcgvX0IB7aEILKITwDK/whL0gt7Rx7y1hIqZQ/gD9PkDBpuPYQ=</latexit>yk+1\n<latexit sha1_base64=\"b9xjFCb3AtC7nJPckW97CecJbOI=\">AB83icbVBNS8NAEJ3Ur1q/qh69BItQLyURv45FLx4r2A9oQtlsN+3SzSbsTsQS+je8eFDEq3/Gm/GbZuDtj4YeLw3w8y8IBFco+N8W4WV1bX1jeJmaWt7Z3evH/Q0nGqKGvSWMSqExDNBJesiRwF6ySK\nkSgQrB2Mbqd+5EpzWP5gOE+REZSB5yStBInjckmD1NquPe6LRXrjg1ZwZ7mbg5qUCORq/85fVjmkZMIhVE67rJOhnRCGngk1KXqpZQuiIDFjXUEkipv1sdvPEPjFK3w5jZUqiPVN/T2Qk0nocBaYzIjUi95U/M/rphe+xmXSYpM0vmiMBU2xvY0ALvPFaMoxoYQqri51aZDoghFE1PJhOAuvrxMWmc197J2cX9eqd/kcRThCI6hCi5cQR3uoAFNoJDAM7zCm5VaL9a79TFvLVj5zCH8gfX5A+MekZk=</latexit>\u02c6x(yk)\n\ufb01nd  \npeaks\ndetermine connectivity\ndetermine bonds\nSMILES, graph, etc.\nRDKit post-processing\n[H][N]=[C]1[C]([H])=[C]([H])[C]2=[N]O[C](=[O])N12\nOLD:\n\ufb01nd  \npeaks\natomic  \nbonds\n<latexit sha1_base64=\"9MCp7Z7eG5/+kPDtJpIDw3IwQo=\">AB7nicbVDLSsNAFL1TX7W+qi7dDBbBjSURX8uiG5cV7APaUCbTSTtkMgkzEyG\nEfoQbF4q49Xvc+TdO2y09cCFwzn3cu89fiK4No7zjUorq2vrG+XNytb2zu5edf+greNUdaisYhV1yeaCS5Zy3AjWDdRjES+YB0/vJv6nSemNI/lo8kS5kVkJHnAKTFW6mSDPDxzJ4Nqzak7M+Bl4hakBgWag+pXfxjTNGLSUEG07rlOYrycKMOpYJNKP9UsITQkI9azVJKIaS+fnTvBJ1YZ4iBWtqTBM/X3\nRE4irbPIt50RMWO96E3F/7xeaoIbL+cySQ2TdL4oSAU2MZ7+jodcMWpEZgmhitbMR0TRaixCVsCO7iy8ukfV53r+qXDxe1xm0RxmO4BhOwYVraMA9NKEFEJ4hld4Qwl6Qe/oY95aQsXMIfwB+vwBCaePYw=</latexit>yk\u22121\n<latexit sha1_base64=\"X2X/z6dTSH57Pet6OsMIr+WFVFg=\">AB7nicbVDLSsNAFL1TX7W+qi7dDBZBEoivpZFNy4r2Ae0oUymk3bIZBJmJ\nkI/Qg3LhRx6/e482+ctlo64ELh3Pu5d57/ERwbRznG5VWVtfWN8qbla3tnd296v5BW8epoqxFYxGrk80E1yluFGsG6iGIl8wTp+eDf1O09MaR7LR5MlzIvISPKAU2Ks1MkGeXjmTgbVmlN3ZsDLxC1IDQo0B9Wv/jCmacSkoYJo3XOdxHg5UYZTwSaVfqpZQmhIRqxnqSQR014+O3eCT6wyxEGsbEm\nDZ+rviZxEWmeRbzsjYsZ60ZuK/3m91AQ3Xs5lkhom6XxRkApsYjz9HQ+5YtSIzBJCFbe3YjomilBjE6rYENzFl5dJ+7zuXtUvHy5qjdsijIcwTGcgvX0IB7aEILKITwDK/whL0gt7Rx7y1hIqZQ/gD9PkDBpuPYQ=</latexit>yk+1\n<latexit sha1_base64=\"mU/3SKsgp3ClACjd5FP8ACepis=\">AB7HicbVBNS8NAEJ3Ur1q/qh69LBbBU0nEr2PRi8cKxhbaUDbTbt0swm\n7E6GE/gYvHhTx6g/y5r9x2+agrQ8GHu/NMDMvTKUw6LrfTmldW19o7xZ2dre2d2r7h8miTjPskYluh9RwKRT3UaDk7VRzGoeSt8LR7dRvPXFtRKIecJzyIKYDJSLBKFrJH/fy0aRXrbl1dwayTLyC1KBAs1f96vYTlsVcIZPUmI7nphjkVKNgk8q3czwlLIRHfCOpYrG3AT57NgJObFKn0SJt\nqWQzNTfEzmNjRnHoe2MKQ7NojcV/M6GUbXQS5UmiFXbL4oyiTBhEw/J32hOUM5toQyLeythA2pgxtPhUbgrf48jJ5PKt7l/WL+/Na46aIowxHcAyn4MEVNOAOmuADAwHP8ApvjnJenHfnY95acoqZQ/gD5/MHLT+O8Q=</latexit>yk\n<latexit sha1_base64=\"YcWNgMQEmogTZQy1CzUNPB3tHxU=\">AB9XicbVDLSsNAFJ3UV62vqks3g0Wom5KIr2XRjcsK9gFtLJPpB06mYSZGzWE/IcbF4q49V/c+TdO2y09cCFwzn3cu89XiS4Btv+\ntgpLyura8X10sbm1vZOeXevpcNYUdakoQhVxyOaCS5ZEzgI1okUI4EnWNsbX0/89gNTmofyDpKIuQEZSu5zSsBI970RgfQpqyb9dJwd98sVu2ZPgReJk5MKytHol796g5DGAZNABdG69gRuClRwKlgWakXaxYROiZD1jVUkoBpN51eneEjowywHypTEvBU/T2RkDrJPBMZ0BgpOe9ifif143Bv3RTLqMYmKSzRX4sMIR4EgEecMUoiMQhU3t2I6IopQMEGVTAjO/MuLpHVSc85rZ7enlfpVHkcRHaBDVEUOukB1dI\nMaqIkoUugZvaI369F6sd6tj1lrwcpn9tEfWJ8/r32SpQ=</latexit>\u02c6x(yk)\nsample  \nvoxelized molecules\n[H][N]=[C]1[C]([H])=[C]([H])[C]2=[N]O[C](=[O])N12\n\u2026\nFigure 4: Pipeline for recovering atomic coordinates from voxel grids: (i) VoxMol generates voxelized\nmolecules, (ii) atomic coordinates are extracted from voxel grid with simple peak detection algorithm,\n(iii) we use cheminformatics software to add atomic bonds and extract SMILES strings, molecular\ngraphs, etc.\n(learning rate 2\u00d710\u22125, weight decay 10\u22122) to optimize the weights. The weights are updated with\nexponential moving average with a decay of .999. We use \u03b3 =1.0, u=1.0 and \u03b4=.5 for all our MCMC\nsamplings. See appendix for more details on the architecture, training and sampling.\nDatasets.\nWe consider two popular datasets for this task: QM9 [40] and GEOM-drugs [41]. QM9\ncontains small molecules with up to 9 heavy atoms (29 if we consider hydrogen atoms). GEOM-drugs\ncontains multiple conformations for 430k drug-sized molecules and its molecules have 44 atoms on\naverage (up to 181 atoms and over 99% are under 80 atoms). We use grids of dimension 323 and 643 for\nQM9 and GEOM-drugs respectively. These volumes are able to cover over 99.8% of all points on both\ndatasets. All our models model hydrogens explicitly. For QM9, we consider all 5 chemical elements (C,\nH, O, N and F) present on the dataset. For GEOM-drugs, we consider 8 elements (C, H, O, N, F, S, Cl and\nBr). We ignore P, I and B elements as they appear in less than .1% of the molecules in the dataset. Finally,\nthe input voxel grids are of dimension R5\u00d732\u00d732\u00d732 and R8\u00d764\u00d764\u00d764 for QM9 and GEOM-drugs, re-\nspectively. Weperformthesamepre-processinganddatasetsplitas[20]andendupwith100K/20K/13K\nmolecules for QM9 and 1.1M/146K/146K for GEOM-drugs (train, validation, test splits respectively).\nBaselines.\nWecompareourmethodwithtwostate-of-the-artapproaches: GSchNet [47], apoint-cloud\nautoregressive model and EDM [15], a point-cloud diffusion-based model. We note that both methods\nrely on equivariant networks, while ours does not. Our results could potentially be improved by success-\nfully exploiting equivariant 3D convolutional networks. We also show results of VoxMoloracle in our\nmain results, where we assume we have access to real samples from the noisy distribution. Instead of per-\nforming MCMC to sample yk, we sample molecules from the validation set and add noise to them. This\nbaseline assumes we would have perfect sampling of noisy samples (walk step) and let us assess the qual-\nity of our model to recover clean samples. It serves as an upper bound for our model and allows us to dis-\nentangle the quality of the walk (sampling noisy samples) and jump (estimating clean molecules) steps.\nAll methods generate molecules as a set of atom types and their coordinates (in the case of voxelized\nmolecules, we use the post-processing described above to get the atomic coordinates). We follow\nprevious work [33, 18, 17, 20] and use standard cheminformatics software to determine the molecule\u2019s\natomic bonds given the atomic coordinates 6. Using the same post-processing for all methods allows\na more apples-to-apples comparison of the models.\nMetrics.\nMost metrics we use to benchmark our model come from [20] 7. We draw 10,000 samples\nfrom each method and measure performance with the following metrics: stable mol and stable atom,\nthe percentage of stable molecules and atoms, respectively, as defined in [15]; validity, the percentage\nof generated molecules that passes RDKit [90]\u2019s sanitization filter; uniqueness, the proportion of\nvalid molecules that have different canonical SMILES; valency W1, the Wasserstein distance between\nthe distribution of valencies in the generated and test set; atoms TV and bonds TV, the total variation\n6We follow previous work and use OpenBabel [89] to get the bond orders.\n7We use the official implementation from https://github.com/cvignac/MiDi.\n7\nstable\nstable\nvalid\nunique\nvalency\natom\nbond\nbond\nbond\nmol %\u2191\natom %\u2191\n%\u2191\n%\u2191\nW1\u2193\nTV\u2193\nTV\u2193\nlen W1\u2193\nang W1\u2193\ndata\n98.7\n99.8\n98.9\n99.9\n.001\n.003\n.000\n.000\n.120\nGSchNet\n92.0\n98.7\n98.1\n94.5\n.049\n.042\n.041\n.005\n1.68\nEDM\n97.9\n99.8\n99.0\n98.5\n.011\n.021\n.002\n.001\n0.44\nVoxMolno rot\n84.2\n(\u00b11.6)\n98.2\n(\u00b1.3)\n98.1\n(\u00b1.4)\n77.2\n(\u00b11.7)\n.043\n(\u00b1.0)\n.171\n(\u00b1.200)\n.050\n(\u00b1.010)\n.007\n(\u00b1.0)\n3.80\n(\u00b1.7)\nVoxMol\n89.3\n(\u00b1.6)\n99.2\n(\u00b1.1)\n98.7\n(\u00b1.1)\n92.1\n(\u00b1.3)\n.023\n(\u00b1.002)\n.029\n(\u00b1.009)\n.009\n(\u00b1.002)\n.003\n(\u00b1.002)\n1.96\n(\u00b1.04)\nVoxMoloracle\n90.1\n99.3\n98.9\n99.9\n.024\n.009\n.002\n.001\n0.37\nTable 1: Results on QM9. We use 10,000 samples from each method. Our results are shown with\nmean/standard deviation across 3 runs.\nbetween the distribution of atom types and bond types, respectively; bond length W1 and bond angle\nW1, the Wasserstein distance between the distribution of bond and lengths, respectively. Finally, we\nalso report the strain energy metric proposed in [91]. This metric is defined as the difference between\nthe internal energy of the generated molecule\u2019s pose and a relaxed pose of the molecule. The relaxation\nand the energy are computed using the Universal Force Field (UFF) [92] within RDKit. See appendix\nfor more details about the metrics.\n4.2\nExperimental results\nTable 1 and Table 2 show results on QM9 and GEOM-drugs respectively. We report results for models\ntrained with and without data augmentation (VoxMol and VoxMolno rot, respectively) and generate\n10,000 samples with multiple MCMC chains. Each chain is initialized with 1,000 warm-up steps, as\nwe observed empirically that it slightly improves the quality of generated samples. Then, samples are\ngenerated after each 500 walk steps (each chain having a maximum of 1,000 steps after the warm-up\nsteps). Results for our models are shown with mean/standard deviation among three runs. The row\ndata on both tables are randomly sampled molecules from the training set.\nOn QM9, VoxMol performs similar to EDM in some metrics while performing worse in others\n(specially stable molecule, uniqueness and angle lengths). On GEOM-drugs, a more challenging and\nrealistic drug-like dataset, the results are very different: VoxMol outperforms EDM in eight out of\nnine metrics, often by a considerably large margin.\nFigure 5(a,b) shows the cumulative distribution function (CDF) of strain energies for the generated\nmolecules of different models on QM9 and GEOM-drugs, respectively. The closer the CDF of gener-\nated molecules from a model is to that of data (samples from training set), the lower is the strain energy\nof generated molecules. The ground truth data has median strain energy of 43.87 and 54.95 kcal/mol\nfor QM9 and GEOM-drugs, respectively. On QM9, all models have median strain energy around the\nsame ballpark: 52.58, 66.32 and 56.54 kcal/mol for EDM, VoxMolno rot and VoxMol, respectively.\nOn GEOM-drugs, the molecules generated by VoxMol have considerably lower median strain energy\nthan EDM: 951.23 kcal/mol for EDM versus 286.06 and 171.57 for VoxMolno rot and VoxMol.\nWe observe, as expected, that augmenting the training data with random rotations and translations\nimproves the performance of the model. The improvement is bigger on QM9 (smaller dataset) than\non GEOM-drugs. In particular, the augmentations help to capture the distribution of bonds and angles\nbetween atoms and to generate more unique molecules. We note that, unlike EDM, our model does\nnot require knowledge of the number of atoms beforehand (neither for training nor sampling). In fact,\nFigure 6 show that our model learns the approximate distribution of the number of atoms per molecule\non both datasets. Implicitly learning this distribution can be particularly useful in applications related\nto in-painting (e.g., pocket conditioning, linking, scaffold conditioning). Finally, our method generates\ndrug-like molecules in fewer iterations and is faster than EDM on average (see Table 3). EDM\nsampling time scales quadratically with the number of atoms, while ours has constant time (but scales\ncubically with grid dimensions).\nThese results clearly show one of the main advantages of our approach: a more expressive model scales\nbetter with data. Architecture inductive biases (such as built-in SE(3) equivariance) are helpful in the\nsetting of small dataset and small molecules. However, on the large-scale regime, a more expressive\nmodel is more advantageous in capturing the modes of the distribution we want to model. Compared\n8\n(a)\n(b)\n(a)\n(b)\n(c)\n(a)\n(b)\nOLD\nFigure 5: The cumulative distribution function of strain energy of generated molecules on (a) QM9 and\n(b) GEOM-drugs. For each method, we use 10,000 molecules.\nstable\nstable\nvalid\nunique\nvalency\natom\nbond\nbond\nbond\nmol %\u2191\natom %\u2191\n%\u2191\n%\u2191\nW1\u2193\nTV\u2193\nTV\u2193\nlen W1\u2193\nang W1\u2193\ndata\n99.9\n99.9\n99.8\n100.\n.001\n.001\n.025\n.000\n.050\nEDM\n40.3\n97.8\n87.8\n99.9\n.285\n.212\n.048\n.002\n6.42\nVoxMolno rot\n44.4\n(\u00b1.1)\n96.6\n(\u00b1.1)\n89.7\n(\u00b1.2)\n99.9\n(\u00b1.0)\n.238\n(\u00b1..001)\n.025\n(\u00b1.001)\n.024\n(\u00b1.001)\n.004\n(\u00b1.000)\n2.14\n(\u00b1.02)\nVoxMol\n75.0\n(\u00b11.)\n98.1\n(\u00b1.3)\n93.4\n(\u00b1.5)\n99.1\n(\u00b1.2)\n.254\n(\u00b1.003)\n.033\n(\u00b1.041)\n.036\n(\u00b1.006)\n.002\n(\u00b1.001)\n0.64\n(\u00b1.13)\nVoxMoloracle\n81.9\n99.0\n94.7\n97.4\n.253\n.002\n.024\n.001\n0.31\nTable 2: Results on GEOM-drugs. We use 10,000 samples from each method. Our results are shown\nwith mean/standard deviation across 3 runs.\n(a)\n(b)\n(a)\n(b)\n(c)\n(a)\n(b)\nFigure 6: Empirical distribution of number of atoms per molecule on QM9 (left) and GEOM-drugs\n(right). We sample 10,000 molecules from train set and generate the same number of VoxMol samples.\nto VoxMoloracle results, we see that VoxMol can still be vastly improved. We can potentially close\nthis gap by improving the quality of the denoising network (e.g., by improving the architecture, train\non more data, efficient built-in SE(3)-equivariance CNNs, etc).\n4.3\nAblation studies\nNoise level \u03c3.\nUnlike diffusion models, the noise level is considered fixed during training and\nsampling. It is an important hyperparameter as it poses a trade-off between the quality of the walk\nstep (Langevin MCMC) and the jump step (empirical Bayes). The ideal noise level is the highest\npossible value such that the network can still learn how to denoise. We train models on QM9 with \u03c3\nin {.6,.7,...,1.2}, while keeping all other hyperparameters the same. Figure 7(a,b,c) shows how noise\nlevel \u03c3 influences the performance on the validation set. While most metrics get better as the noise level\nincreases, others (like stable molecules and valency W1) get worse after a value. We observe empirically\nthat \u03c3=.9 is the sweet spot level that achieves better overall performance on the validation set of QM9.\nNumber of steps \u2206k.\nTable 3 shows how VoxMol\u2019s performance on GEOM-drugs change with\nthe number of walk steps \u2206k on the Langevin MCMC sampling. In this experiment, we use the same\ntrained model and only change the number of steps during sampling. Results of EDM are also shown\nfor comparison (it always requires 1,000 diffusion steps for generation). We see that some metrics\nbarely change, while others improve as \u2206k increases. The average time (in seconds) to generate a\n9\n(a)\n(b)\n(c)\n(a)\n(b)\n(a)\n(b)\n(c)\nFigure 7: Effect of noise level \u03c3 on generation quality. Models are trained on QM9 with a different\nnoise level. Each plot shows two metrics: (a) molecule stability and uniqueness, (b) atom and bond TV,\n(c) valency and angle lengths W1.\n\u2206k\nstable\nstable\nvalid\nunique\nvalency\natom\nbond\nbond\nbond\navg. t\n(n steps)\nmol %\u2191\natom %\u2191\n%\u2191\n%\u2191\nW1\u2193\nTV\u2193\nTV\u2193\nlen W1\u2193\nang W1\u2193\ns/mol.\u2193\n50\n78.9\n98.7\n96.3\n87.8\n.250\n.073\n.102\n.002\n1.18\n0.90\n100\n78.6\n98.6\n95.5\n94.3\n.256\n.050\n.101\n.002\n1.62\n1.64\n200\n77.9\n98.4\n94.4\n98.6\n.253\n.037\n.104\n.002\n1.02\n3.17\n500\n76.7\n98.2\n93.8\n99.2\n.252\n.043\n.042\n.002\n0.56\n7.55\n1,000\n75.5\n98.4\n93.4\n99.8\n.257\n.029\n.050\n.002\n0.79\n14.9\nEDM\n40.3\n97.8\n87.8\n99.9\n.285\n.212\n.048\n.002\n6.42\n9.35\nTable 3: Effect of number of walk steps \u2206k on generation quality on GEOM-drugs (2,000 samples).\nEDM results are shown for comparison.\nmolecule increases linearly with the number of steps, as expected. We observe that even using 500\nsteps, our model is still faster than EDM on average, while achieving better performance in these\nmetrics. Remarkably, with only 50 steps, VoxMol already outperforms EDM in most metrics, while\nbeing an order of magnitude faster on average.\nAtomic density radii.\nWe also assess how the performance of the model changes with respect to\nthe size of atomic radii chosen during the voxelization step (while always keeping the resolution\nof the grid fixed at .25\u00c5). See appendix for how this is done. We tried four different values for the\nradii (same for all elements): .25, .5, .75 and 1.0. We observe\u2014throughout different versions of the\nmodel, with different hyperparameters\u2014that using a fixed radius of .5 consistently outperform other\nvalues. Training does not converge with radius .25 and quality of generated samples degrades as we\nincrease the radius. We also tried to use Van der Waals radii (where each atom type would have their\nown radius), but results were also not improved.\n5\nConclusion\nWe introduce VoxMol, a novel score-based method for 3D molecule generation. This method generates\nmolecules in a fundamentally different way than the current state of the art (i.e., diffusion models applied\ntoatoms). Thenoisemodelusedisalsonovelintheclassofscore-basedgenerativemodelsformolecules.\nWe represent molecules on regular voxel grids and VoxMol is trained to predict \u201cclean\u201d molecules from\nits noised counterpart. The denoising model (which approximates the score function of the smoothed\ndensity) is used to sample voxelized molecules with walk-jump sampling strategy. Finally atomic\ncoordinates are retrieved by extracting the peaks from the generated voxel grids. Our experiments show\nthat VoxMol scales better with data and outperforms (by a large margin) a representative state of the\nart point cloud-based diffusion model on GEOM-drugs, while being faster to generate samples.\nBroader impact.\nGenerating molecules conditioned on some desiderata can have huge impacts\nin many different domains, such as, drug discovery, biology, materials, agriculture, climate, etc. This\nwork deals with unconditional 3D molecule generation (in a pure algorithmic way): a problem that\ncan be seen as an initial stepping stone (out of many) to this long-term objective. We, as a society, need\nto find solutions to use these technologies in ways that are safe, ethical, accountable and exclusively\nbeneficial to society. These are important concerns and they need to be thought of at the same time\nwe design machine learning algorithms.\nAcknowledgements.\nThe authors would like to thank the whole Prescient Design team for helpful\ndiscussions and Genentech\u2019s HPC team for providing a reliable environment to train/analyse models.\n10\nReferences\n[1] Saeed Saremi and Aapo Hyvarinen. Neural empirical Bayes. JMLR, 2019.\n[2] Tobias Fink, Heinz Bruggesser, and Jean-Louis Reymond. Virtual exploration of the small-molecule\nchemical universe below 160 daltons. Angewandte Chemie International Edition, 2005.\n[3] Jiankun Lyu, Sheng Wang, Trent E Balius, Isha Singh, Anat Levit, Yurii S Moroz, Matthew J O\u2019Meara,\nTao Che, Enkhjargal Algaa, Kateryna Tolmachova, et al. Ultra-large library docking for discovering new\nchemotypes. Nature, 2019.\n[4] Regine S Bohacek, Colin McMartin, and Wayne C Guida. The art and practice of structure-based drug\ndesign: a molecular modeling perspective. Medicinal research reviews, 1996.\n[5] Camille Bilodeau, Wengong Jin, Tommi Jaakkola, Regina Barzilay, and Klavs F Jensen. Generative models\nfor molecular discovery: Recent advances and challenges. Computational Molecular Science, 2022.\n[6] David Weininger. Smiles, a chemical language and information system. 1. introduction to methodology\nand encoding rules. Journal of chemical information and computer sciences, 1988.\n[7] Mario Krenn, Florian H\u00e4se, AkshatKumar Nigam, Pascal Friederich, and Alan Aspuru-Guzik. Self-\nreferencing embedded strings (selfies): A 100% robust molecular string representation. Machine Learning:\nScience and Technology, 2020.\n[8] Marwin HS Segler, Thierry Kogej, Christian Tyrchan, and Mark P Waller. Generating focused molecule\nlibraries for drug discovery with recurrent neural networks. ACS central science, 2018.\n[9] Thomas Blaschke, Marcus Olivecrona, Ola Engkvist, J\u00fcrgen Bajorath, and Hongming Chen. Application\nof generative autoencoder in de novo molecular design. Molecular informatics, 2018.\n[10] Gabriel Lima Guimaraes, Benjamin Sanchez-Lengeling, Carlos Outeiral, Pedro Luis Cunha Farias, and\nAl\u00e1n Aspuru-Guzik. Objective-reinforced generative adversarial networks (organ) for sequence generation\nmodels. arXiv preprint arXiv:1705.10843, 2017.\n[11] Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for molecular\ngraph generation. In ICML, 2018.\n[12] Yujia Li, Oriol Vinyals, Chris Dyer, Razvan Pascanu, and Peter Battaglia. Learning deep generative models\nof graphs. arXiv preprint arXiv:1803.03324, 2018.\n[13] Jiaxuan You, Rex Ying, Xiang Ren, William L. Hamilton, and Jure Leskovec. Graphrnn: A deep generative\nmodel for graphs. In ICML, 2018.\n[14] Omar Mahmood, Elman Mansimov, Richard Bonneau, and Kyunghyun Cho. Masked graph modeling for\nmolecule generation. Nature Communications, 2021.\n[15] Emiel Hoogeboom, V\u00edctor Garcia Satorras, Cl\u00e9ment Vignac, and Max Welling. Equivariant diffusion for\nmolecule generation in 3D. In ICML, 2022.\n[16] Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. Geodiff: A geometric\ndiffusion model for molecular conformation generation. In ICLR, 2022.\n[17] Ilia Igashov, Hannes St\u00e4rk, Clement Vignac, Victor Garcia Satorras, Pascal Frossard, Max Welling,\nMichael M Bronstein, and Bruno Correia. Equivariant 3d-conditional diffusion models for molecular linker\ndesign. In NeurIPS, AI for ScienceWorkshop, 2022.\n[18] Arne Schneuing, Yuanqi Du, Charles Harris, Arian Jamasb, Ilia Igashov, Weitao Du, Tom Blundell, Pietro\nLi\u00f3, Carla Gomes, Max Welling, et al. Structure-based drug design with equivariant diffusion models. arXiv\npreprint arXiv:2210.13695, 2022.\n[19] Gabriele Corso, Hannes St\u00e4rk, Bowen Jing, Regina Barzilay, and Tommi S. Jaakkola. Diffdock: Diffusion\nsteps, twists, and turns for molecular docking. In ICLR, 2023.\n[20] Clement Vignac, Nagham Osman, Laura Toni, and Pascal Frossard. Midi: Mixed graph and 3d denoising\ndiffusion for molecule generation. arXiv preprint arXiv:2302.09048, 2023.\n[21] Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Riley. Tensor\nfield networks: Rotation-and translation-equivariant neural networks for 3d point clouds. arXiv preprint\narXiv:1802.08219, 2018.\n11\n[22] Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael JL Townshend, and Ron Dror. Learning from\nprotein structure with geometric vector perceptrons. ICLR, 2021.\n[23] V\u0131ctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E (n) equivariant graph neural networks.\nIn ICML, 2021.\n[24] Mario Geiger and Tess Smidt. e3nn: Euclidean neural networks. arXiv preprint arXiv:2207.09453, 2022.\n[25] Yi-Lun Liao and Tess Smidt. Equiformer: Equivariant graph attention transformer for 3d atomistic graphs.\nICLR, 2023.\n[26] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks?\nICLR, 2019.\n[27] Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav Rattan,\nand Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks. In AAAI, 2019.\n[28] Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph networks.\nNeurips, 32, 2019.\n[29] Ilyes Batatia, David P Kovacs, Gregor Simm, Christoph Ortner, and G\u00e1bor Cs\u00e1nyi. MACE: Higher order\nequivariant message passing neural networks for fast and accurate force fields. Neurips, 35:11423\u201311436,\n2022.\n[30] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[31] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional\nneural networks. Advances in neural information processing systems, 25, 2012.\n[32] Nate Gruver, Marc Finzi, Micah Goldblum, and Andrew Gordon Wilson. The Lie derivative for measuring\nlearned equivariance. arXiv preprint arXiv:2210.02984, 2022.\n[33] Matthew Ragoza, Tomohide Masuda, and David Ryan Koes. Learning a continuous representation of 3d\nmolecular structures with deep generative models. In Neurips, Structural Biology workshop, 2020.\n[34] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020.\n[35] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. NeurIPS, 2021.\n[36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution\nimage synthesis with latent diffusion models. In CVPR, 2022.\n[37] Saeed Saremi and Rupesh Kumar Srivastava. Multimeasurement generative models. ICLR, 2022.\n[38] Saeed Saremi, Rupesh Kumar Srivastava, and Francis Bach. Universal smoothed score functions for\ngenerative modeling. arXiv preprint arXiv:2303.11669, 2023.\n[39] Nathan C Frey, Dan Berenberg, Joseph Kleinhenz, Isidro Hotzel, Julien Lafrance-Vanasse, Ryan Lewis\nKelly, Yan Wu, Arvind Rajpal, Stephen Ra, Richard Bonneau, Kyunghyun Cho, Andreas Loukas, Vladimir\nGligorijevic, and Saeed Saremi. Learning protein family manifolds with smoothed energy-based models.\nIn ICLR, Workshop on Physics for Machine Learning, 2023.\n[40] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl\nLeswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. Chemical science,\n2018.\n[41] Simon Axelrod and Rafael Gomez-Bombarelli. Geom, energy-annotated molecular conformations for\nproperty prediction and molecular generation. Scientific Data, 2022.\n[42] Miha Skalic, Jos\u00e9 Jim\u00e9nez, Davide Sabbadin, and Gianni De Fabritiis. Shape-based generative modeling\nfor de novo drug design. Journal of chemical information and modeling, 2019.\n[43] Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. In ICLR, 2014.\n[44] Lvwei Wang, Rong Bai, Xiaoxuan Shi, Wei Zhang, Yinuo Cui, Xiaoman Wang, Cheng Wang, Haoyu Chang,\nYingsheng Zhang, Jielong Zhou, et al. A pocket-based 3d molecule generative model fueled by experimental\nelectron density. Scientific reports, 2022.\n12\n[45] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yosh Bengio. Generative adversarial nets. In NIPS, 2014.\n[46] Fergus Imrie, Thomas E Hadfield, Anthony R Bradley, and Charlotte M Deane. Deep generative design\nwith 3d pharmacophoric constraints. Chemical science, 2021.\n[47] Niklas Gebauer, Michael Gastegger, and Kristof Sch\u00fctt. Symmetry-adapted generation of 3d point sets\nfor the targeted discovery of molecules. In NeurIPS, 2019.\n[48] Niklas Gebauer, Michael Gastegger, and Kristof T Sch\u00fctt. Generating equilibrium molecules with deep\nneural networks. arXiv preprint arXiv:1810.11347, 2018.\n[49] Youzhi Luo and Shuiwang Ji. An autoregressive flow model for 3d molecular geometry generation from\nscratch. In ICLR, 2022.\n[50] Jonas K\u00f6hler, Leon Klein, and Frank No\u00e9. Equivariant flows: exact likelihood generative learning for\nsymmetric densities. In ICML, 2020.\n[51] Victor Garcia Satorras, Emiel Hoogeboom, Fabian Fuchs, Ingmar Posner, and Max Welling.\nE (n)\nequivariant normalizing flows. In NeurIPS, 2021.\n[52] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In ICML, 2015.\n[53] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning\nusing nonequilibrium thermodynamics. In ICML, 2015.\n[54] Lei Huang, Hengtong Zhang, Tingyang Xu, and Ka-Chun Wong. Mdm: Molecular diffusion model for\n3d molecule generation. arXiv preprint arXiv:2209.05710, 2022.\n[55] Lemeng Wu, Chengyue Gong, Xingchao Liu, Mao Ye, et al. Diffusion-based molecule generation with\ninformative prior bridges. In NeurIPS, 2022.\n[56] Minkai Xu, Alexander Powers, Ron Dror, Stefano Ermon, and Jure Leskovec. Geometric latent diffusion\nmodels for 3d molecule generation. In ICML, 2023.\n[57] Elman Mansimov, Omar Mahmood, Seokho Kang, and Kyunghyun Cho. Molecular geometry prediction\nusing a deep generative graph neural network. Scientific Reports, 2019.\n[58] Gregor NC Simm and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. A generative model for molecular distance geometry.\nICML, 2020.\n[59] Frank No\u00e9, Simon Olsson, Jonas K\u00f6hler, and Hao Wu. Boltzmann generators: Sampling equilibrium states\nof many-body systems with deep learning. Science, 2019.\n[60] Gregor NC Simm, Robert Pinsler, G\u00e1bor Cs\u00e1nyi, and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. Symmetry-aware\nactor-critic for 3d molecular design. arXiv preprint arXiv:2011.12747, 2020.\n[61] Octavian Ganea, Lagnajit Pattanaik, Connor Coley, Regina Barzilay, Klavs Jensen, William Green, and\nTommi Jaakkola. Geomol: Torsional geometric generation of molecular 3d conformer ensembles. In\nNeurIPS, 2021.\n[62] Xingang Peng, Shitong Luo, Jiaqi Guan, Qi Xie, Jian Peng, and Jianzhu Ma. Pocket2mol: Efficient\nmolecular sampling based on 3d protein pockets. In ICML, 2022.\n[63] Chence Shi, Shitong Luo, Minkai Xu, and Jian Tang. Learning gradient fields for molecular conformation\ngeneration. In ICML, 2021.\n[64] Bowen Jing, Gabriele Corso, Jeffrey Chang, Regina Barzilay, and Tommi Jaakkola. Torsional diffusion\nfor molecular conformer generation. arXiv preprint arXiv:2206.01729, 2022.\n[65] Siyu Long, Yi Zhou, Xinyu Dai, and Hao Zhou. Zero-shot 3d drug design by sketching and generating.\nIn NeurIPS, 2022.\n[66] Keir Adams and Connor W Coley.\nEquivariant shape-conditioned generation of 3d molecules for\nligand-based drug design. arXiv preprint arXiv:2210.04893, 2022.\n[67] Jiaqi Guan, Wesley Wei Qian, Xingang Peng, Yufeng Su, Jian Peng, and Jianzhu Ma. 3d equivariant\ndiffusion for target-aware molecule generation and affinity prediction. ICLR, 2023.\n13\n[68] Matthew Ragoza, Tomohide Masuda, and David Ryan Koes. Generating 3d molecules conditional on\nreceptor binding sites with deep generative models. Chemical science, 2022.\n[69] David Schaller, Dora \u0160ribar, Theresa Noonan, Lihua Deng, Trung Ngoc Nguyen, Szymon Pach, David\nMachalz, Marcel Bermudez, and Gerhard Wolber. Next generation 3d pharmacophore modeling. Wiley\nInterdisciplinary Reviews: Computational Molecular Science, 2020.\n[70] Raphael JL Townshend, Martin V\u00f6gele, Patricia Suriana, Alexander Derry, Alexander Powers, Yianni\nLaloudakis, Sidhika Balachandar, Bowen Jing, Brandon Anderson, Stephan Eismann, et al. Atom3d: Tasks\non molecules in three dimensions. NeurIPS, 2020.\n[71] Zhijian Liu, Haotian Tang, Yujun Lin, and Song Han. Point-voxel cnn for efficient 3d deep learning. In\nNeurIPS, 2019.\n[72] Maurice Weiler, Mario Geiger, Max Welling, Wouter Boomsma, and Taco S Cohen. 3d steerable cnns:\nLearning rotationally equivariant features in volumetric data. NeurIPS, 2018.\n[73] Ivan Diaz, Mario Geiger, and Richard Iain McKinley. An end-to-end se (3)-equivariant segmentation\nnetwork. arXiv preprint arXiv:2303.00351, 2023.\n[74] Jiehong Lin, Hongyang Li, Ke Chen, Jiangbo Lu, and Kui Jia. Sparse steerable convolutions: An efficient\nlearning of se (3)-equivariant features for estimation and tracking of object poses in 3d space. NeurIPS, 2021.\n[75] Karel Lenc and Andrea Vedaldi. Understanding image representations by measuring their equivariance\nand equivalence. In CVPR, 2015.\n[76] Diane Bouchacourt, Mark Ibrahim, and Ari Morcos.\nGrounding inductive biases in natural images:\ninvariance stems from variations in data. In NeurIPS, 2021.\n[77] Daniel Flam-Shepherd and Al\u00e1n Aspuru-Guzik. Language models can generate molecules, materials, and\nprotein binding sites directly in three dimensions as xyz, cif, and pdb files. arXiv preprint arXiv:2305.05708,\n2023.\n[78] Matthew Ragoza, Joshua Hochuli, Elisa Idrobo, Jocelyn Sunseri, and David Ryan Koes. Protein\u2013ligand\nscoring with convolutional neural networks. Journal of chemical information and modeling, 2017.\n[79] Michael Maser and SE Reisman. 3d computer vision models predict dft-level homo-lumo gap energies\nfrom force-field-optimized geometries. ChemRvix, 2021.\n[80] Herbert Ellis Robbins. An empirical Bayes approach to statistics. In Proc. 3rd Berkeley Symp. Math. Statist.\nProbab., 1956, 1956.\n[81] Koichi Miyasawa. An empirical bayes estimator of the mean of a normal population. Bull. Inst. Internat.\nStatist, 1961.\n[82] Aapo Hyv\u00e4rinen. Estimation of non-normalized statistical models by score matching. JMLR, 2005.\n[83] Giorgio Parisi. Correlation functions and computer simulations. Nuclear Physics B, 1981.\n[84] Xiang Cheng, Niladri S. Chatterji, Peter L. Bartlett, and Michael I. Jordan. Underdamped Langevin MCMC:\nA non-asymptotic analysis. In COLT, 2018.\n[85] Matthias Sachs, Benedict Leimkuhler, and Vincent Danos. Langevin dynamics with variable coefficients\nand nonconservative forces: from stationary states to numerical methods. Entropy, 2017.\n[86] Saeed Saremi, Ji Won Park, and Francis Bach. Chain of log-concave Markov chains. arXiv preprint\narXiv:2305.19473, 2023.\n[87] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image\nsegmentation. In MICCAI, 2015.\n[88] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019.\n[89] Noel M O\u2019Boyle, Michael Banck, Craig A James, Chris Morley, Tim Vandermeersch, and Geoffrey R\nHutchison. Open babel: An open chemical toolbox. Journal of cheminformatics, 2011.\n[90] Greg Landrum. Rdkit: Open-source cheminformatics software, 2016.\n14\n[91] Charles Harris, Kieran Didi, Arian R Jamasb, Chaitanya K Joshi, Simon V Mathis, Pietro Lio, and Tom\nBlundell. Benchmarking generated poses: How rational is structure-based drug design with generative\nmodels? arXiv preprint arXiv:2308.07413, 2023.\n[92] Anthony K Rapp\u00e9, Carla J Casewit, KS Colwell, William A Goddard III, and W Mason Skiff. Uff, a full\nperiodic table force field for molecular mechanics and molecular dynamics simulations. Journal of the\nAmerican chemical society, 1992.\n[93] Lin Li, Chuan Li, and Emil Alexov. On the modeling of polar component of solvation energy using smooth\ngaussian-based dielectric function. Journal of Theoretical and Computational Chemistry, 2014.\n[94] Gabriele Orlando, Daniele Raimondi, Ramon Duran-Roma\u00f1a, Yves Moreau, Joost Schymkowitz, and\nFrederic Rousseau. Pyuul provides an interface between biological structures and deep learning algorithms.\nNature communications, 2022.\n[95] Yuxin Wu and Kaiming He. Group normalization. In ECCV, 2018.\n[96] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network function\napproximation in reinforcement learning. Neural Networks, 2018.\n[97] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,\nZeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep\nlearning library. In NeurIPS, 2019.\n15\nA\nExtra implementation details\nA.1\nVoxel representation\nMolecules in our datasets are converted into voxelized atomic densities. For each molecule, we\nconsider a box around its center and divide it into discrete volume elements. We follow [93, 94] and\nfirst convert each atom (of each molecule) into 3D Gaussian-like densities:\nVa(d,ra)=exp\n\u0010\n\u2212\nd2\n(.93\u00b7ra)2\n\u0011\n,\n(5)\nwhere Va is defined as the fraction of occupied volume by atom a of radius ra at distance d from\nits center. Although we could consider a different radius for each element, in this work we consider\nall atoms to have the same radius ra =.5\u00c5. The occupancy of each voxel in the grid is computed by\nintegrating the occupancy generated by every atom in a molecule:\nOcci,j,k =1\u2212\nNa\nY\nn=1\n(1\u2212Van(||Ci,j,k\u2212xn||,ran)),\n(6)\nwhere Na is the number of atoms in the molecule, an is the nth atom, Ci,j,k are the coordinates (i,j,k)\nin the grid and xn is the coordinates of the center of atom n [93]. The occupancy takes the maximum\nvalue of 1 at the center of the atom and goes to 0 as it moves away from it. Every channel is considered\nindependent of one another and they do not interact nor share volumetric contributions. We use the\npython package PyUUL [94] to generate the voxel grids from the raw molecules (.xyz or .sdf format).\nWe use grids with 323 voxels on QM9 and 643 on GEOM-drugs and place the molecules on the\ncenter of the grid. These volumes are able to cover over 99% of all points in the datasets. We use all\n5 chemical elements present on the dataset (C, H, O, N and F), while for GEOM-drugs, we use 8 (C,\nH, O, N, F, S, Cl and Br). We model hydrogen explicitly in all our experiments. Finally, the input\nvoxel grids are of dimension R5\u00d732\u00d732\u00d732 and R8\u00d764\u00d764\u00d764 for QM9 and GEOM-drugs, respectively.\nWe augment the dataset during training by applying random rotation and translation to the molecules.\nFor rotation, we sample three Euler angles uniformly (between [0,2\u03c0)) and rotate each training sample.\nFor translation, we randomly shift the center of the molecules on each of the three dimensions by\nsampling an uniform shift between [0,.25]\u00c5.\nA.2\nArchitecture\nOur neural network architecture follows standard encoder-decoder convnet architecture. We use a\nvery similar architecture recipe to DDPM [34]. The model uses four levels of resolution: 323 to 43\nfor the QM9 dataset and 643 to 83 for the GEOM-drugs dataset. The input voxel is embedded into\na 32 dimensional space with a grid projection layer (3D convnet with kernel size 3\u00d73\u00d73). Each\nresolution (on both encoder and decoder) has two convolutional residual blocks. Each block contains\na group normalization [95] layer, followed by SiLU [96] non-linearity and a 3D convnet (with kernel\nsize 3\u00d73\u00d73). All convolutions have stride 1 and we pad the feature maps with 1 on each side. We\nuse self-attention layers between the convolutional layers in the two lowest resolutions. We reduce\n(increase, respectively) the resolution of the encoder (decoder) with 2\u00d72\u00d72 (stride 1) max-poolings\n(bilinear-upsampling). The model has skip connections at each resolution to concatenate the encoder\nfeature map with the decoder feature map. We double the number of feature maps at each resolution,\nexcept the last resolution where we quadruple. VoxMol has approximately 111M parameters. We\nalso implemented a smaller version (with reduced number of channels per layer) with around 30M.\nThese models achieve performance close to the base model and are faster to train and sample.\nA.3\nBuilt-in SE(3) equivariance experiments\nIn early experiments, we made an attempt at using a SE(3)-equivariant 3D U-Net using steerable\nconvnets [72] for denoising, but initial experiments were not successful. The hypothesis is that\na built-in SE(3) equivariant version of our model, VoxMolequi, would be advantageous over the\nnon-equivariant version for the task of molecule generation. We start with the official implementation\nof [73] and tune several network hyperparameters (related to architecture, optimization and training) so\nthat the network is able to achieve good denoising metrics on QM9. We then use the same procedure to\n16\ngenerate samples as described in the main paper (only switching the network from the non-equivariant\nto the equivariant version). We tried different sampling hyperparameters, but we were never able to\nachieve the same performance as non-equivariant VoxMol. Table 4 compares the results of the model\nwith and without built-in SE(3) equivariance.\nQM9\nstable\nstable\nvalid\nunique\nvalency\natom\nbond\nbond\nbond\nmol %\u2191\natom %\u2191\n%\u2191\n%\u2191\nW1\u2193\nTV\u2193\nTV\u2193\nlen W1\u2193\nang W1\u2193\nVoxMol\n89.3\n99.2\n98.7\n92.1\n.023\n.029\n.009\n.003\n1.96\nVoxMolequi\n25.1\n81.8\n95.9\n92.9\n13.2\n.104\n.015\n.015\n5.31\nTable 4: Results on QM9 of our model without (VoxMol) and with (VoxMolequi) built-in SE(3)\nequivariance.\nThere might be many reasons why this is the case: (i) the best reconstruction loss we found with the\nequivariant model is higher than the non-equivariant (approx. 9.4\u00d710\u22125 vs. 5.4\u00d710\u22125 MSE on val.\nset), (ii) the equivariant model needs more capacity to be competitive with the non-equivariant one\n(currently it has over 90\u00d7 fewer parameters), (iii) something in the sampling procedure needs to be\ndifferent on the equivariant version (unlikely).\nWe hypothesize that if an equivariant version of VoxMol achieves similar (or lower) reconstruction\nloss as the vanilla version, it will probably achieve competitive/better results in the task of molecule\ngeneration. Finally, our equivariant implementation is less efficient (around 50-60% slower) and\nconsumes more memory than the original version. This poses, therefore, an extra challenge to scale\nup the size of the dataset and the size of the molecules (e.g., GEOM-drugs requires 643 voxel grid).\nA.4\nTraining and sampling\nThe weights are optimized with batch size 128 and 64 (for QM9 and GEOM-drugs, respectively),\nAdamW optimizer (\u03b21 =.9, \u03b22 =.999), learning rate of 10\u22125 and weight decay of 10\u22122. The models\nare trained for 500 epochs on QM9 and around 24 epochs on GEOM-drugs. We discretize the\nunderdamped Langevin MCMC (Equation 4) with the algorithm proposed by Sachs et al. [85] (this\nhas been applied on images before [37]). Algorithm 1 describes this process.\nAlgorithm 1: Walk-jump sampling [1] using the discretization of Langevin diffusion by [85]. Lines\n6-13 correspond to the walk step and line 14 to the jump step.\n1: Input \u03b4 (step size), u (inverse mass), \u03b3 (friction), K (steps taken)\n2: Input Learned score function g\u03b8(y)\u2248\u2207ylogp(y) and noise level \u03c3\n3: Output bxK\n4: y0 \u223cN(0,\u03c32Id)+Ud(0,1)\n5: v0 \u21900\n6: for k=0,...,K\u22121 do\n7:\nyk+1 \u2190yk+ \u03b4\n2vk\n8:\ng\u2190g\u03b8(yk+1)\n9:\nvk+1 \u2190vk+ u\u03b4\n2 g\n10:\n\u03b5\u223cN(0,Id)\n11:\nvk+1 \u2190exp(\u2212\u03b3\u03b4)vk+1+ u\u03b4\n2 g+\np\nu(1\u2212exp(\u22122\u03b3\u03b4))\u03b5\n12:\nyk+1 \u2190yk+1+ \u03b4\n2vk+1\n13: end for\n14: \u02c6xK \u2190yK +\u03c32g\u03b8(yK)\nWe use \u03b3 =1.0, u=1.0, \u03b4=.5 for all samplings and we generate multiple chains in parallel (200 chains\nfor QM9 and 100 for GEOM-drugs). We follow [37] and initialize the chains by adding uniform noise\nto the initial Gaussian noise (with the same \u03c3 used during training), i.e., y0 = N(0,\u03c32Id)+Ud(0,1)\n(this was observed to mix faster in practice).\nAll experiments and analysis on this paper were done on A100 GPUs and with PyTorch [97]. The\nmodels on QM9 were trained with 2 GPUs and the models on GEOM-drugs on 4 GPUs.\n17\ndset\ncoords\nstable\nstable\nvalid\nunique\nvalency\natom\nbond\nbond\nbond\nref.\nmol %\u2191\natom %\u2191\n%\u2191\n%\u2191\nW1\u2193\nTV\u2193\nTV\u2193\nlen W1\u2193\nang W1\u2193\nQM9\n-\n80.5\n98.5\n98.1\n93.3\n.051\n.028\n.005\n.008\n2.94\n\u2713\n89.3\n99.2\n98.7\n92.1\n.023\n.029\n.009\n.003\n1.96\nGEOM\n-\n73.9\n99.0\n94.7\n98.6\n.236\n.030\n.038\n.008\n2.92\n\u2713\n74.9\n98.1\n93.4\n99.2\n.254\n.033\n.036\n.002\n.63\nTable 5: Effect of coordinate refinement on QM9 and GEOM-drugs. We use 10,000 samples from each\nmethod.\nA.5\nRecovering atomic coordinates from voxel grid\nFigure 4 shows our pipeline to recover atomic coordinates and molecular graphs from generated\nvoxelized molecules. In the first step, we use the model to \u201cjump\u201d to the data manifold generating\na sample in the voxelized representation, xk. We set to 0 all voxels with value less than .1, i.e.,\nxk = 1\u2265.1(xk). We then apply a simple peak finding algorithm to find the voxel coordinates\ncorresponding to the peaks in the generated sample. Our peak finding algorithm uses a maximum\nfilter with a 3\u00d73\u00d73 stencil to find local maxima. Note that this algorithm always returns points on\nthe voxel grid and is therefore limited by the resolution of the discretization.\nIn order to further refine the atomic coordinates we take advantage of the fact that our voxelization\nprocedure is differentiable to perform gradient based optimization of the coordinates. Specifically we\nuse L-BFGS to optimize the atomic coordinates based on the L2 norm of the reconstruction error in the\nvoxel representation. Note, unlike some previous work [33] we perform peak detection and refinement\nin a single step and do not perform search over multiple possible numbers of atoms or atom identities.\nTable 5 shows the effect of coordinate refinement on molecule generation. We generate molecules\non the same setting in the experimental section.\nOnce we have obtained the optimized atomic coordinates, we follow previous work [33, 18, 17, 20]\nand use standard cheminformatics software to determine the molecule\u2019s atomic bonds.\nA.6\nMetrics\nMost of the metrics used to benchmark models come from [20]8. Below we describe the metrics:\n\u2022 Atom stability: the percentage of generated atoms with the right valency. This metric is\ncomputed on the raw 3D sample (before any postprocessing), therefore it is more stringent\nthan validity.\n\u2022 Molecule stability: the percentage of generated molecules where all its atoms are stable.\n\u2022 Validity: The percentage of generated molecules that passes RDKit\u2019s sanitization filter.\n\u2022 Uniqueness:. The proportion of valid molecules (defined above) that has a unique canonical\nSMILES (generated with RDKit) representation.\n\u2022 Atoms TV: The total variation between the distribution of bond types in the generated and test\nset. We consider 5 atom types on QM9 and 8 atom types on GEOM-drugs. The histograms\n\u02c6hatm and hatm are generated by counting the number of each atom type on all molecules\non both the generated and real sample set. The total variation is computed as:\nAtoms TV(\u02c6hatm,hatm)=\nX\nx\u2208atom types\n|\u02c6hatm(x)\u2212hatm(x)|\n\u2022 Bonds TV: Similar to above, the histograms for real and generated samples are created by\ncounting all bond types on all molecules. The total variation is computed as:\nBonds TV(\u02c6hbond,hbond)=\nX\nx\u2208bond types\n|\u02c6hbond(x)\u2212hbond(x)|\n8We do not compare directly with [20], since this model is an extension of EDM and leverages more information\n(connectivity graph and formal charges) during training.\n18\n\u2022 Valency W1: This is the weighted sum of the Wasserstein distance between the distribution\nof valencies for each atom type:\nValency W1(generated,target)=\nX\nx\u2208atom types\np(x)W1(\u02c6hval(x),hval(x)),\nwhere \u02c6hval(x) and hval(x) are the histogram of valencies for atom type x for generated and\nholdout set samples, respectively.\n\u2022 Bond length W1: The weighted sum of Wasserstein distance between the distribution of bond\nlengths for each bond type:\nBond Len W1(generated,target)=\nX\nb\u2208bond types\np(b)W1(\u02c6hdist(b),hdist(b)),\nwhere \u02c6hdist(b) and hdist(b) are the histogram of bond lengths for bond type b, for generated\nand holdout set samples, respectively.\n\u2022 Bond angles W1: The weighted sum of Wasserstein distance between the distribution of bond\nangles (in degrees) for each atom type in the dataset:\nBond Ang W1(generated,target)=\nX\nx\u2208atom types\np(x)W1(\u02c6hang(x),hang(x)),\nwhere \u02c6hang(x) and hang(x) are the histogram of angles for atom type x for generated and\nholdout set samples, respectively. See [20] for how angles are measured.\n\u2022 Strain energy: The strain energy for a generated molecule is computed as the difference\nbetween the energy on the generated pose and the energy of a relaxed position. The relaxation\nand the energy are computed using UFF provided by RDKit. We use [91]\u2019s implementation9.\nA.7\nGuiding the generation process\nLike diffusion models, our method also leverages (learned) score functions and relies on Langevin\nMCMC for sampling. Therefore, in theory we can condition VoxMol similarly to how it is done in\ndiffusion models: by constraining the score function as we walk through the MCMC chain. In the\ncase of diffusion models, the score function of all steps is constrained to guide the transition steps\nfrom noise to a (conditioned) sample. In VoxMol, the constrained score function would affect the\n\u201cwalk steps\u201d (the Langevin MCMC steps): it would restrict the region where the chain samples noisy\nmolecules y to p(y|c) (instead of p(y)), c is the condition (e.g., gradient of a classifier). The \u201cjump step\u201d\n(a forward pass of the denoising network over the noised molecules) is independent of the condition\nand remains unchangeable.\nManyoftheinnovationsonconditioningdiffusionmodelscomefromcomputervision, whereU-netsare\nusually used. Since VoxMol has the same architecture (albeit 3D instead of 2D), many of the condition-\ningtechniques/tricksusedinimagesmaybemoreeasilytransferable. Forexample, wecouldinprinciple\nuse the gradient of a classifier (trained jointly) to guide the sampling (using the same trick as in Dhariwal\nand Nichol [35]) or adapt gradient-free guidance ([34]). Pocket conditioning could also be possible, as\nin e.g., [18, 67], but utilizing voxel representations instead of point clouds and neural empirical Bayes in-\nstead of diffusion models. In-painting has also proven to work very well in 2D U-Nets, so it could poten-\ntially work with 3D U-Nets as well. These in-painting techniques could also be leveraged in the context\nof molecule generation on voxel grids, e.g., for linker generation, scaffold/fragment conditioning.\n9Code\ntaken\nfrom\nhttps://github.com/cch1999/posecheck/blob/main/posecheck/utils/\nstrain.py\n19\nB\nGenerated samples\nFigure 8: Random generated samples from VoxMol trained on QM9 (passing RDKit\u2019s sanitization).\nMolecular graphs are generated with RDKit.\n20\nFigure 9: Random generated samples from VoxMol trained on GEOM-drugs (passing RDKit\u2019s\nsanitization). Molecular graphs are generated with RDKit.\n21\n"
  },
  {
    "title": "Speech-to-Text Adapter and Speech-to-Entity Retriever Augmented LLMs for Speech Understanding",
    "link": "https://arxiv.org/pdf/2306.07944.pdf",
    "upvote": "5",
    "text": "Speech-to-Text Adapter and Speech-to-Entity Retriever\nAugmented LLMs for Speech Understanding\nMingqiu Wang, Izhak Shafran, Hagen Soltau, Wei Han, Yuan Cao, Dian Yu, Laurent El Shafey\nGoogle DeepMind\nmingqiuwang,izhak,soltau,weihan,yuancao,dianyu,shafey@google.com\nAbstract\nLarge Language Models (LLMs) have been applied in the\nspeech domain, often incurring a performance drop due to\nmisaligned between speech and language representations. To\nbridge this gap, we propose a joint speech and language model\n(SLM) using a Speech2Text adapter, which maps speech into\ntext token embedding space without speech information loss.\nAdditionally, using a CTC-based blank-filtering, we can reduce\nthe speech sequence length to that of text. In speech MultiWoz\ndataset (DSTC11 challenge), SLM largely improves the dialog\nstate tracking (DST) performance (24.7% to 28.4% accuracy).\nFurther to address errors on rare entities, we augment SLM with\na Speech2Entity retriever, which uses speech to retrieve rele-\nvant entities, and then adds them to the original SLM input as a\nprefix. With this retrieval-augmented SLM (ReSLM), the DST\nperformance jumps to 34.6% accuracy. Moreover, augmenting\nthe ASR task with the dialog understanding task improves the\nASR performance from 9.4% to 8.5% WER.\n1. Introduction\nThere has been considerable interest in extending the capability\nof the large language models (LLMs) from text to other modal-\nities including speech. One thread of work attempts to map\nspeech and text to the same latent representations [1, 2, 3]. A\nshared encoder is employed for both speech and text, in one\ncase with an explicit loss term promoting the same embedding\nspace [3] and in other without the explicit term [1].\nIn most practical spoken language systems, speech input is\nrecognized using an automatic speech recognition (ASR) and\nthe recognized transcripts are fed into a back-end NLU system,\nwhere the back-ends are increasing powered by LLMs [4]. This\ncascaded approach does not offer an opportunity to correct po-\ntential ASR misrecognitions. Besides, both the LLMs and the\nASR systems have a common weakness in processing entities\nthat are not well-represented in their training data.\nIn this paper, we examine these challenges in the context\nof a speech understanding system using the DSTC-11 dialog\ntracking task [5]. The task is a based on the popular Multi-\nWoz, a fully-labeled collection of human-human conversations\nspanning multiple domains and topics such as train and hotel\nreservations [6]. In this particular challenge, the written user\nresponses were replaced with spoken version collected from\ncrowd-sourced workers. The model is expected to infer the dia-\nlog states corresponding to the current user utterance and given\ndialog context. The context could be the acoustic or the rec-\nognized version of the dialog history along with the previously\ninferred states. This task is particularly interesting because of\nhigh occurrence of rare entities such as restaurants, tourist at-\ntractions, cities and train stations.\nFigure 1: Direct speech to dialog state prediction in multi-turn\ndialogs. Given speech of the current user turn i and a text tran-\nscript of the dialog history, the SLM / ReSLM models generate\na single output sequence for both the corresponding transcript\n[ASR] and dialog state [DST]. The ASR transcript predicted\nfrom turn i is used as history turn i + 1 in an auto-regressive\nmanner.\nThe key contributions of this paper are:\n1. We propose a Speech2Text adapter which maps speech en-\ncodings into text token embeddings with seemingly minimal\nloss of information.\n2. We propose a joint speech and language model (SLM) with\nboth speech and text modalities in input. With the adapter,\nSLM can leverage the rich knowledge encoded in LLMs (pre-\ntrained T5 XXL in our case) directly for speech tasks.\n3. We introduce a Speech2Entity retriever to handle rare task-\nspecific entities, which uses speech to retrieve entities.\n4. We propose a novel retrieval-augmented speech language\nmodel (ReSLM) which augments SLM with the retriever. We\nshow that ReSLM achieves strong performance in ASR and\nspeech understanding tasks.\nUnlike cascaded systems, both SLM and ReSLM operate di-\nrectly on speech input and as such are not stuck with misrecog-\nnized words from the first stage ASR. We demonstrate the bene-\nfits of the different components of the model using the DSTC11\nchallenge task. While this work reports results on a dialog track-\ning task, the model is applicable more widely for speech under-\nstanding tasks.\narXiv:2306.07944v1  [eess.AS]  8 Jun 2023\n2. Related work\nA closely related line of work injects text inputs into speech\nmodels [7, 1, 3, 8, 2] and align the learned representation of\ntext and speech to the same space. This is done by TTS or\nmore recently up-sampled text and minimizing an L2 loss of\naligned speech and text frames.\nThis is in contrast to our\nwork, where we do the opposite and reduce the frame rate of\nthe audio sequence to bring it closer to text. This is done via\na CTC model [9] where we use the predictions to filter out\nblank frames. This results in a highly compressed speech en-\ncodings that preserve semantic information and makes down-\nstream NLU tasks much easier. There also other use cases of\nfiltering CTC blank frames. For example, the work in [10] used\nit to speed up training of RNN-T [11] models.\nThe compression of speech signal when combinining\nspeech and text modalitiess has analogies in tasks where vision\nand text modalities are combined. For example, a Perceiver [12]\narchitecture is used to compress images before interleaved with\ntext tokens [13]. However, the cross-attentions between the Per-\nceiver outputs and the frozen LM layers makes the model sub-\nstantially different from a standard LM and hence their model\ncannot share a standard LM at serving time, unlike our work.\nIn an alternative approach, the speech input is tokenized and\nthen fed into the LLMs [14, 15]. These approaches suffer from\nthe same issue as cascaded systems where LLMs cannot utilize\nacoustic encodings to correct errors in tokenization.\nRetrieval-augmented language models have demonstrated\nsuperior performance on various natural language tasks [16,\n17, 18], especially the knowledge-intensive ones [19, 20,\n21]. In particular, retrieval-augmentation disentangles encoding\ndomain-specific knowledge from training parameters, thereby\nbeing a desirable property for task-oriented dialog where inte-\ngrating domain schema is a critical requirement [22, 23]. Fur-\nthermore, in dialog understanding, unseen domains and tasks\nmay demand adaptation to a new set of schema [24]. To deal\nwith these challenges, previous work propose to either retrieve\nsimilar training examples [25, 26] or corresponding intents and\nslots for dialog state tracking [27].\nInspired by these work, we extend retrieval-augmented\nmethods to speech understanding. As mentioned before, unlike\nwritten domain, speech understanding poses an additional chal-\nlenge that rare entities are not easily recognizable [5]. Therefore\nwe introduce an audio retrieval method to alleviate these dif-\nficulties and achieve better performance on end-to-end speech\ndialog understanding.\n3. Model\n3.1. Joint speech and language model (SLM)\nThe speech understanding task requires a model that can simul-\ntaneously recognize speech inputs and understand the semantic\nmeaning of the corresponding language. In previous research,\nlarge pre-trained language models such as BERT, T5, and GPT\nhave demonstrated impressive capabilities for understanding se-\nmantics in NLU tasks [28, 29, 30]. Leveraging this capability,\nwe combine a speech encoder with a T5 model for this speech\nunderstanding task as shown in Figure 2.\nThe speech encoder is based on a CTC model trained sep-\narately, described further in Section 4.2. We only utilize non-\nblank predicted frames of the CTC model. This CTC-based\nblank-filtering approach has two advantages: First, only seman-\ntic relevant information of the audio signal is being \u2019forwarded\u2019\nto the down-stream task, making fine-tuning for NLU much\neasier. Secondly, the effective sequence length of the encoded\nspeech sequence is reduced by approximately 4x. This helps\nwith joint modeling of speech and text sequences, where other-\nwise the audio sequence is much larger than the text sequence\nand makes processing much harder. Note, this is in contrast to\nthe opposite approach employed in other works where text was\nupsampled to match the acoustic frame rate [7] which cannot\ntake advantage of pre-trained LLMs.\n3.2. Speech2Text Adapter\nThe Speech2Text adapter consists of a few self-attention lay-\ners to map the CTC-filtered speech encodings to the text token\nembeddings of the pre-trained LLMs. The resulting outputs are\nconcatenated with the text embeddings and then fed into the\npre-trained LLMs. Note that for the adapter to be effective, it\nis crucial that it undergoes pre-training to ensure a successful\nmapping to the text embedding space. This can be done by sim-\nply training SLM with any ASR task, where the input is the\nspeech and the prediction is the corresponding transcript. The\ntext input part of SLM is unused while training the adapter. It\u2019s\nworth noting that both the speech and language model weights\nare frozen during this pre-training process.\nTherefore, our Speech2Text adapter refers to two folds of\nmeanings: 1) a few self-attention layers between speech en-\ncoder and language model; 2) pretraining with both speech and\nlanguage models frozen.\n3.3. Speech2Entity Retriever\nThe main task of the retriever is to extract a subset of entities\nfrom a list of given task-specific entities that are relevant for the\ncurrent speech input. We adopt a dual encoder architecture for\nthe retriever whose keys are acoustic encodings of the speech\ninput and the values are the entities mentioned in the input [31].\nThe model is trained using entities mentioned in the reference\ntranscript of the input speech. The keys and values (candidate\nentities) are encoded separately and cosine distance between\ntheir representations is used to measure similarity. The in-batch\nnegatives are used as negative targets to optimize the contrastive\nloss. In our case, we use the multimodal SLM encoder since it\ncan encode both audio and text. During inference, we compute\nthe nearest neighbors efficiently using cosine distance with the\nSCAM library and retrieve the top-K candidates [32].\n3.4. Retrieval-augmented SLM model (ReSLM)\nIn the retriever-based SLM, we integrate the top-K candidates\nfrom the audio retriever into the previously described SLM.\nSpecifically, with acoustic encodings of the current speech in-\nput as queries we retieve the top-K entities from the large pool\nof task-specific entities. The retrieved entities are pre-prended\nto the original text inputs before being fed into the encoder.\n4. Experiments and results\n4.1. Evaluation Task\nThe DSTC11 Challenge Task is based on MutliWoz 2.1 and has\nthe same distribution in terms of dialogs and turns [5]. The\nmain difference is that the written user responses are replaced\nwith spoken versions. The responses were generated using TTS\nin the training set and by human voices from crowd-sourced\nworkers in the test set. Additionally, previously researchers had\ndiscovered that the slot values in the training and test sets had\nsubstantial overlap, which led to misleading and overly opti-\nFigure 2: Model architecture for ReSLM, and SLM (without\nthe Speech2Entity retriever component). The SLM and ReSLM\nmodels take both speech and text as inputs. The speech frame\nsequence is shortened by CTC-based blank-filtering, trans-\nformed by Speech2Text adapter, then concatenated with text em-\nbeddings before being fed into a T5 encoder-decoder model. In\nReSLM, a few entities are selected using the speech input by\nSpeech2Entity retriever and prepended to the text input.\nmistic performance reports. To alleviate this issue, the organiz-\ners of the challenge modified the test set by replacing the slot\nvalues (city, restuarant names, times, etc). As such, the perfor-\nmance of systems on DSTC11 test set are expected to be lower\nthan the written version.\nThe main focus of the task was dialog state tracking where\nthe performance was measured using Joint Goal Accuracy\n(JGA) and Slot Error Rate (SER). For details, see [5]. Addition-\nally, we also measure word error rate (WER) of the recognized\ninput speech for ablation experiments to tease apart the impact\nof misrecognitions.\n4.2. Experiment Setup\nThe speech encoder is derived from a CTC [9] model trained on\nthe PeopleSpeech public corpus [33] of approx. 32,000 hours of\nspeech data. The encoder consists of 16 Transformer layers, al-\ntogether a 220m parameter model. The model\u2019s input frame rate\nis 25ms and produces outputs every 75ms obtained via a down-\nsampling layer sandwiched between the transformer layers. We\nuse the activations (1024-dim) from the last transformer layer\nas speech encodings. Additionally, we remove blank frames\n(e.g. frames where the highest scoring token is blank). The\nmodel emits a non-blank frame on average every 305ms and\neach word is encoded on average with 1.48 frames. Filtering\nCTC blank frames results in a very strong compression of the\nspeech signal and makes down-stream NLU tasks substantially\neasier while preserving the semantic information.\nWe reused the previously trained unimodal checkpoints:\nspecifically the T5 XXL checkpoints for the text encoder-\ndecoder, and the CTC speech encoder checkpoints for the\nspeech encoder.\nThroughout the training process, we main-\ntained the speech encoder in a frozen state for all experiments\nand exclusively trained the text encoder-decoder along with the\nSpeech2Text adaptation layer. We also show ablation studies of\nonly partially finetuning T5 models.\n4.3. Auto-Regressive Inference\nDialogs have multiple turns and the dialog state values are in-\nferred turn-by-turn auto-regressively. The task of dialog state\ntracking requires predicting all dialog states up to the current\nturn i, therefore the entire dialog history is required as input.\nAs shown in Figure 1, we feed the speech of turn i as speech\ninput and the dialog history from turn 1 to i \u2212 1 as text input.\nThe dialog history can be long and is best represented in the text\nform, not speech. For this reason, we trained the SLM model\nto simultaneously recognize the words spoken in turn i along\nwith the dialog states in one output string. The transcript from\neach turn is incrementally collated to create the dialog history\nfor subsequent turns.\nDuring the training process, the input consists of speech of\nthe current turn and the dialog history based on the ASR tran-\nscripts from the previously described CTC model. The loss is\ncomputed with target consisting of the reference transcript of\nthe current turn and the associated reference dialog state values.\n4.4. Speech2Entity Retriever Results\nThe Speech2Entity retriever, describe in Section 3.3, was\ntrained on three categories of the entities: hotel names, restau-\nrant names, and city names [5]. The retriever was trained on\na pool of 2.5k entities and a separate pool of 14k entities were\nused for evaluation.\nIn principle, the two-tower retriever model can utilize any\nspeech and text encoders. In our experiments, we use the SLM\nspeech and text encoders both query and candidates. The check-\npoints from previously trained SLM was used to initialize the\ntwo-tower encoders before training the retriever.\nThe performance of the Speech2Entity retriever is shown\nin Table 1. The subset of retrieved entities were selected us-\ning a distance threshold of -0.78, which resulted in top-10 en-\ntities per utterance. This threshold was chosen to balance re-\ncall and precision, with a focus on optimizing recall so that the\nresulting ReSLM model could access entities with the highest\npossible coverage. As a consequence of this emphasis on re-\ncall, precision was sacrificed. However, we anticipated that the\nReSLM model would learn to discard incorrectly retrieved en-\ntities. Clearly, the retriever can be improved further and this is\nmostly a demonstration of the proof-of-the-concept and in spite\nof the poor precision we obtain substantial gains in ReSLM as\ndescribed later.\nRecall (%)\nPrecision (%)\nR@1\n40.2\nP@1\n13.3\nR@3\n51.9\nP@3\n6.7\nR@5\n57.0\nP@5\n5.0\nR@10\n62.2\nP@10\n3.6\nR@20\n66.5\nP@20\n2.8\nR@100\n70.4\nP@100\n2.0\nTable 1: Performance of the Speech2Entity retriever. Top-k re-\ncall and precision filtered by -0.78 similarity threshold.\n4.5. Dialog State Tracking Results\nThe results on dialog state tracking are reported in the Table 2\nwhere the left half corresponds to the SLM model and the right\nto the ReSLM. In the upper half of the table, the adapter lay-\ners were trained from scratch and in lower half of the table, the\nSpeech2Text adapter was trained with ASR task. The different\nrows shows the impact of training different groups of parame-\nters including the embedding layer, the encoder and the decoder\nof the T5 model.\nThe results show that the Speech2Text adapter improves\nperformance for both SLM and ReSLM, with gains ranging\nfrom 3-5 JGA. Interestingly, when Speech2Text adapter is em-\nployed just training the encoder and/or embedding gives the best\nresult (29.8% and 35.1% JGA), suggesting that adapter is effec-\ntive is bringing the speech modality close to the text modality.\nOn top of the gains from Speech2Text adapter, the Speech2Text\nretriever gives a further boost of 5% JGA in all conditions.\n%\nJGA \u2191\nWER \u2193\nJGA \u2191\nWER \u2193\nCascaded [5]\n31.8\n13.0\nTrainable params\nSLM\nReSLM\nwithout Speech2Text Adapter\nWhole T5\n24.7\n11.5\n31.3\n9.5\nT5 encoder+emb\n27.3\n10.1\n32.0\n8.9\nT5 encoder only\n27.1\n11.6\n31.6\n9.3\nwith Speech2Text Adapter\nWhole T5\n28.4\n9.2\n34.6\n8.5\nT5 encoder+emb\n29.5\n9.2\n35.1\n8.5\nT5 encoder only\n29.8\n9.2\n34.5\n8.6\nTable 2: Dialog state tracking performance evaluated using\njoint goal accuracy (JGA). We compare model performances\nwith and without pretrained (see section 3.2) Speech2Text\nadapters, with and without retrieved entities. Note that the SLM\n/ ReSLM models predict both speech recognized transcript and\ndialog state in the same output sequence. So we can also report\nword error rate (WER) here. All numbers are on test set.\nZooming into the improvements for categories of dialog\nstate variables, using Slot Error Rate (SER), show that the fol-\nlowing categories of dialog state variables benefited from the\nSpeech2Text retriever: hotel names (26% gain), train destina-\ntion station (35% gain), and and restaurant name (14% gain).\nThis is in spite of poor precision of the Speech2Text retriever\n(see section below), which makes this a remarkable gain.\n4.6. ASR Results\nSince we trained the model using multi-task objective to in-\nclude ASR, we can evaluate the performance of the model\non the recognition task.\nThere are two clear trends, the\nSpeech2Text adapter improves the ASR performance across all\nconditions for SLM. It also compares favorably with a general\npurpose baseline RNN-T ASR model (13.0% WER) [5]. When\nSpeech2Entity retriever is also used the gain is further boosted\nin all cases, mirroring the results in dialog state tracking.\nOne useful ablation study would be to understand the ASR\ngains without the DST loss. We tested this by feeding speech in-\nput and training the model for ASR task alone and report the re-\nsults in Table 4. The ASR performance matches the in-domain\nASR system (10.4% vs 10.7%). Since we were able to achieve\nthis performance while keeping the LLM frozen, we hypothe-\nsize that the Speech2Text adapter is able to map from the acous-\ntic encoding space to the textual encoding space. Interestingly,\nwhile the Speech2Text retriever does not bring additional gains\nwhen trained on ASR task alone, it brings gains when trained\nwith DST task (Table 2). This can be attributed to the fact that\nthe DST loss places additional focus on improving the recogni-\ntion of entities and semantics of their context.\n5. Conclusions\nWe proposed a joint speech and language model (SLM) with\nboth speech and text inputs. The speech input is encoded us-\ning a separately trained CTC encoder where the input is down-\n%\nWER \u2193\nRNN-T [5]\n13.0\nRNN-T in-domain finetuned [5]\n10.4\nAdapter only\n10.7\nTrainable params\nSLM\nReSLM\nwithout Speech2Text Adapter\nWhole T5\n12.0\n11.0\nT5 encoder+emb\n11.2\n11.2\nT5 encoder only\n11.3\n10.1\nwith Speech2Text Adapter\nWhole T5\n9.7\n9.4\nT5 encoder+emb\n9.4\n9.5\nT5 encoder only\n9.5\n9.4\nTable 3: Speech recognition performance. We compare model\nperformances with and without pretrained (see section 3.2)\nSpeech2Text adapters, with and without retrieved entities. The\nWER values here were calculated from an ASR-only setup (dif-\nferent from joint DST-ASR setup in Table 2).\n%\nWER \u2193\nRNN-T [5]\n13.0\nRNN-T in-domain finetuned [5]\n10.4\nAdapter only\n10.7\nTrainable params\nSLM\nwithout Speech2Text Adapter\nWhole T5\n12.0\nT5 encoder+emb\n11.2\nT5 encoder only\n11.3\nwith Speech2Text Adapter\nWhole T5\n9.7\nT5 encoder+emb\n9.4\nT5 encoder only\n9.5\nTable 4: Speech recognition performance. We compare model\nperformances with and without pretrained (see section 3.2)\nSpeech2Text adapters, with and without retrieved entities. The\nWER values here were calculated from an ASR-only setup (dif-\nferent from joint DST-ASR setup in Table 2).\nsampled by filtering out the blank symbols from the CTC de-\ncoder.\nThe CTC-based blank filtering reduces the number\nof speech frames to roughly match the textual units, unlike\nprevious work where text was up-sampled to speech [7]. A\nSpeech2Text adapter, trained with seq-to-seq ASR task, trans-\nforms the blank-filtered speech encodings to the textual encod-\ning space. This allows us to readily use pre-trained large lan-\nguage models for understanding the content of both speech and\ntext inputs. The model can be trained to perform both recogni-\ntion and downstream speech understanding task simultaneously.\nOur results, on both DST and ASR tasks, comparing training\ndifferent groups of parameters show that the LLM decoder does\nnot need to be trained to obtain most of the performance gains\nfrom the Speech2Text adapter. This suggests that the adapter is\neffective in bringing the speech to the text encoder space.\nFurther, we introduce an Speech2Entity retriever to select\nentities relevant to the speech input using a two-tower model\nwith the SLM encoder. In our retrieval-based SLM (ReSLM),\nby pre-pending the retrieved entities to the text input, we show\nthat the performance of inferring the dialog states related to\ntask-specific entities can be improved. This also translates to\nsignificant improvement in the downstream speech understand-\ning task, in our case, prediction of dialog states (34.5% JGA).\nThus, the combined system with the Speech2Text adapter and\nthe Speech2Text retriever outperforms a strong cascade base-\nline system (31.8% JGA) where the DST was trained on error-\nprone ASR transcripts.\nSimilarly, the ReSLM model (8.6%\nWER) with the adapter and the retriever outperforms a strong\nin-domain ASR baseline (10.4% WER).\nWhile the experiments are performed on DST task, the\nmodel is more widely applicable and its performance can be\nfurther improved with better retriever.\n6. Acknowledgements\nWe would like to acknowledge Jeffrey Zhao, Abhinav Rastogi\nand Aramys Miranda for their invaluable help.\n7. References\n[1] A. Bapna, Y. Chung, N. Wu, A. Gulati, Y. Jia, J. H. Clark,\nM. Johnson, J. Riesa, A. Conneau, and Y. Zhang, \u201cSLAM: A\nunified encoder for speech and language modeling via speech-text\njoint pre-training,\u201d CoRR, vol. abs/2110.10329, 2021. [Online].\nAvailable: https://arxiv.org/abs/2110.10329\n[2] S. Thomas, B. Kingsbury, G. Saon, and H.-K. J. Kuo, \u201cIntegrating\ntext inputs for training and adapting rnn transducer asr models,\u201d\nin Proc. ICASSP, 2022.\n[3] Z. Chen, Y. Zhang, A. Rosenberg, B. Ramabhadran, P. J. Moreno,\nA. Bapna, and H. Zen, \u201cMAESTRO: Matched Speech Text Rep-\nresentations through Modality Matching,\u201d in Proc. Interspeech,\n2022.\n[4] J. Zhao, R. Gupta, Y. Cao, D. Yu, M. Wang, H. Lee, A. Rastogi,\nI. Shafran, and Y. Wu, \u201cDescription-driven task-oriented dialog\nmodeling,\u201d arXiv preprint arXiv:2201.08904, 2022.\n[5] H. Soltau, I. Shafran, M. Wang, A. Rastogi, J. Zhao, Y. Jia,\nW. Han, Y. Cao, and A. Miranda, \u201cSpeech aware dialog system\ntechnology challenge (dstc11),\u201d arXiv preprint arXiv:2212.08704,\n2022.\n[6] M. Eric, R. Goel, S. Paul, A. Sethi, S. Agarwal, S. Gao,\nand D. Hakkani-T\u00a8ur, \u201cMultiwoz 2.1:\nMulti-domain dialogue\nstate corrections and state tracking baselines,\u201d CoRR, vol.\nabs/1907.01669, 2019. [Online]. Available: http://arxiv.org/abs/\n1907.01669\n[7] A. Rosenberg, Y. Zhang, B. Ramabhadran, Y. Jia, P. J. Moreno,\nY. Wu, and Z. Wu, \u201cSpeech recognition with augmented\nsynthesized speech,\u201d CoRR, vol. abs/1909.11699, 2019. [Online].\nAvailable: http://arxiv.org/abs/1909.11699\n[8] Z. Chen, Y. Zhang, A. Rosenberg, B. Ramabhadran, P. Moreno,\nand G. Wang, \u201cTts4pretrain 2.0: Advancing the use of text and\nspeech in asr pretraining with consistency and contrastive losses,\u201d\nin Proc. ICASSP, 2022.\n[9] A. Graves, S. Fern\u00b4andez, F. Gomez, and J. Schmidhuber, \u201cCon-\nnectionist temporal classification:\nLabelling unsegmented se-\nquence data with recurrent neural networks,\u201d in Proc. ICML. As-\nsociation for Computing Machinery, 2006.\n[10] Y. Wang, Z. Chen, C. Zheng, Y. Zhang, W. Han, and P. Haghani,\n\u201cAccelerating rnn-t training and inference using ctc guidance,\u201d\n2022. [Online]. Available: https://arxiv.org/abs/2210.16481\n[11] A. Graves, \u201cSequence transduction with recurrent neural net-\nworks,\u201d CoRR, 2012.\n[12] A. Jaegle, S. Borgeaud, J. Alayrac, C. Doersch, C. Ionescu,\nD. Ding, S. Koppula, D. Zoran, A. Brock, E. Shelhamer, O. J.\nH\u00b4enaff, M. M. Botvinick, A. Zisserman, O. Vinyals, and J. Car-\nreira, \u201cPerceiver io: A general architecture for structured inputs &\noutputs.\u201d\narXiv, 2021.\n[13] J. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Has-\nson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, R. Ring,\nE. Rutherford, S. Cabi, T. Han, Z. Gong, S. Samangooei, M. Mon-\nteiro, J. Menick, S. Borgeaud, A. Brock, A. Nematzadeh, S. Shar-\nifzadeh, M. Binkowski, R. Barreira, O. Vinyals, A. Zisserman,\nand K. Simonyan, \u201cFlamingo: a visual language model for few-\nshot learning,\u201d CoRR, vol. abs/2204.14198, 2022.\n[14] Z. Borsos, R. Marinier, D. Vincent, E. Kharitonov, O. Pietquin,\nM. Sharifi, O. Teboul, D. Grangier, M. Tagliasacchi, and N. Zeghi-\ndour, \u201cAudiolm: a language modeling approach to audio genera-\ntion,\u201d arXiv preprint arXiv:2209.03143, 2022.\n[15] F. Wu, K. Kim, S. Watanabe, K. Han, R. McDonald, K. Q.\nWeinberger, and Y. Artzi, \u201cWav2seq:\nPre-training speech-to-\ntext encoder-decoder models using pseudo languages,\u201d 2022.\n[Online]. Available: https://arxiv.org/abs/2205.01086\n[16] U. Khandelwal, A. Fan, D. Jurafsky, and L. Z. amd M. Lewis,\n\u201cNearest neighbor machine translation,\u201d in Proc. ICLR, 2021.\n[17] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford,\nK. Millican, G. v. d. Driessche, J.-B. Lespiau, B. Damoc, A. Clark,\nD. d. L. Casas, A. Guy, J. Menick, R. Ring, T. Hennigan,\nS. Huang, L. Maggiore, C. Jones, A. Cassirer, A. Brock, M. Pa-\nganini, G. Irving, O. Vinyals, S. Osindero, K. Simonyan, J. W.\nRae, E. Elsen, and L. Sifre, \u201cImproving language models by re-\ntrieving from trillions of tokens,\u201d CoRR, vol. abs/2112.04426,\n2021.\n[18] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni,\nT. Schick, J. Dwivedi-Yu, A. Joulin, S. Riedel, and E. Grave,\n\u201cAtlas:\nFew-shot learning with retrieval augmented language\nmodels,\u201d 2022. [Online]. Available: https://arxiv.org/abs/2208.\n03299\n[19] K. Guu, K. Lee, Z. Tung, P. Pasupat, and M.-W. Chang, \u201cRe-\ntrieval augmented language model pre-training,\u201d in Proc. ICML,\nvol. 119.\nPMLR, 2020, pp. 3929\u20133938.\n[20] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal,\nH. K\u00a8uttler, M. Lewis, W.-t. Yih, T. Rockt\u00a8aschel, S. Riedel,\nand D. Kiela, \u201cRetrieval-augmented generation for knowledge-\nintensive nlp tasks,\u201d in Proc. NIPS, vol. 33, 2020, pp. 9459\u20139474.\n[21] G. Izacard and E. Grave, \u201cDistilling knowledge from reader to\nretriever for question answering,\u201d in Proc. ICLR, 2021. [Online].\nAvailable: https://openreview.net/forum?id=NTEz-6wysdb\n[22] C.-S. Wu, A. Madotto, E. Hosseini-Asl, C. Xiong, R. Socher,\nand P. Fung, \u201cTransferable multi-domain state generator for task-\noriented dialogue systems,\u201d in Proc. ACL, Jul. 2019, pp. 808\u2013819.\n[23] L. Zhou and K. Small, \u201cMulti-domain dialogue state tracking\nas dynamic knowledge graph enhanced question answering,\u201d\nCoRR, vol. abs/1911.06192, 2019. [Online]. Available:\nhttp:\n//arxiv.org/abs/1911.06192\n[24] A. Rastogi, X. Zang, S. Sunkara, R. Gupta, and P. Khaitan, \u201cTo-\nwards scalable multi-domain conversational agents: The schema-\nguided dialogue dataset,\u201d Proc. AAAI Conference on Artificial In-\ntelligence, 2020.\n[25] P. Pasupat, Y. Zhang, and K. Guu, \u201cControllable semantic pars-\ning via retrieval augmentation,\u201d in Proc. EMNLP, Nov. 2021, pp.\n7683\u20137698.\n[26] R. Gupta, H. Lee, J. Zhao, Y. Cao, A. Rastogi, and Y. Wu, \u201cShow,\ndon\u2019t tell: Demonstrations outperform descriptions for schema-\nguided task-oriented dialogue,\u201d in Proc. NACCL. ACL, Jul. 2022.\n[27] D. Yu, M. Wang, Y. Cao, L. El Shafey, I. Shafran, and H. Soltau,\n\u201cKnowledge-grounded dialog state tracking,\u201d in Proc. EMNLP,\n2022.\n[28] J. Devlin, M. Chang, K.Lee, and K. Toutanova, \u201cBert: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding,\u201d arXiv preprint arXiv:1810.04805, 2018.\n[29] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\nY. Zhou, W. Li, and P. J. Liu, \u201cExploring the limits of transfer\nlearning with a unified text-to-text transformer,\u201d The Journal of\nMachine Learning Research, 2020.\n[30] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever,\n\u201cImproving language understanding by generative pre-training,\u201d\n2018.\n[31] J. Ni, C. Qu, J. Lu, Z. Dai, G. H. \u00b4Abrego, J. Ma, V. Y.\nZhao, Y. Luan, K. B. Hall, M.-W. Chang, and Y. Yang,\n\u201cLarge dual encoders are generalizable retrievers,\u201d arXiv preprint\narXiv:2112.07899, 2021.\n[32] R. Guo, P. Sun, E. Lindgren, Q. Geng, D. Simcha, F. Chern, and\nS. Kumar, \u201cAccelerating large-scale inference with anisotropic\nvector quantization,\u201d in International Conference on Machine\nLearning.\nPMLR, 2020, pp. 3887\u20133896.\n[33] D. Galvez, G. Diamos, J. Ciro, J. F. Cer\u00b4on, K. Achorn, A. Gopi,\nD. Kanter, M. Lam, M. Mazumder, and V. J. Reddi, \u201cThe people\u2019s\nspeech: A large-scale diverse english speech recognition dataset\nfor commercial usage,\u201d CoRR, vol. abs/2111.09344, 2021.\n"
  },
  {
    "title": "GeneCIS: A Benchmark for General Conditional Image Similarity",
    "link": "https://arxiv.org/pdf/2306.07969.pdf",
    "upvote": "4",
    "text": "GeneCIS: A Benchmark for General Conditional Image Similarity\nSagar Vaze1,2*\nNicolas Carion1\nIshan Misra1\n1 FAIR, Meta AI\n2 VGG, University of Oxford\nProject Page: sgvaze.github.io/genecis\nAbstract\nWe argue that there are many notions of \u2018similarity\u2019 and\nthat models, like humans, should be able to adapt to these\ndynamically. This contrasts with most representation learn-\ning methods, supervised or self-supervised, which learn a\nfixed embedding function and hence implicitly assume a sin-\ngle notion of similarity. For instance, models trained on Im-\nageNet are biased towards object categories, while a user\nmight prefer the model to focus on colors, textures or spe-\ncific elements in the scene. In this paper, we propose the\nGeneCIS (\u2018genesis\u2019) benchmark, which measures models\u2019\nability to adapt to a range of similarity conditions. Ex-\ntending prior work, our benchmark is designed for zero-\nshot evaluation only, and hence considers an open-set of\nsimilarity conditions. We find that baselines from powerful\nCLIP models struggle on GeneCIS and that performance on\nthe benchmark is only weakly correlated with ImageNet ac-\ncuracy, suggesting that simply scaling existing methods is\nnot fruitful. We further propose a simple, scalable solution\nbased on automatically mining information from existing\nimage-caption datasets. We find our method offers a sub-\nstantial boost over the baselines on GeneCIS, and further\nimproves zero-shot performance on related image retrieval\nbenchmarks. In fact, though evaluated zero-shot, our model\nsurpasses state-of-the-art supervised models on MIT-States.\nWe, the architects of the machine, must decide a-priori\nwhat constitutes its \u2018world\u2019; what things are to be taken\nas \u2018similar\u2019 or \u2018equal\u2019 \u2014 Karl Popper, 1963\n1. Introduction\nHumans understand many notions of similarity and\nchoose specific ones depending on the task at hand [21,58].\nConsider the task of finding \u2018similar\u2019 images illustrated\nin Figure 1. Which of the rightmost images should be con-\nsidered \u2018most similar\u2019 to the reference? Given different con-\nditions, each image could be a valid answer. For instance,\nwe may be interested in a specific object in the scene, focus-\ning on either the \u2018car\u2019 or \u2018bridge\u2019. One could even indicate\n*Work done during an internship at Meta AI Research.\nWith the same bridge\nWith a black car\nWith the same car\nFigure 1. Given different conditions (shown as blue text), differ-\nent images on the right can be considered most \u2018similar\u2019 to the\nreference on the left. We present a general way to train and evalu-\nate models which can adapt to different notions of similarity.\na \u2018negative\u2019 similarity condition, specifying a change in the\nimage to identify the bottom image as most similar.\nLearning such similarity functions is a central goal in\ndiscriminative deep learning [11\u201313, 34, 63, 68, 75]. Dis-\ncriminative models, either supervised [30, 75] or self-\nsupervised [9, 10], learn embedding functions such that\n\u2018similar\u2019 images are closer in feature space than \u2018dissimilar\u2019\nimages. However, since there are infinitely many notions of\nimage similarity, how do we allow our models to choose?\nAlmost all current approaches assume a single notion of\nsimilarity, either by explicitly training on a specific concept\n[68,75] or through an implicit assumption in the underlying\ndata distribution [9, 12]. Meanwhile, prior works tackling\nthe conditional problem have focused on constrained do-\nmains such as fashion [69,73] or birds [46], with a restricted\nset of similarity conditions. This is because developing and\nevaluating models that can adapt to generic notions of sim-\nilarity is extremely challenging. Specifically, curating data\nto train and evaluate such models is difficult, as collecting\nannotations for all concepts of similarity is impossible.\n1\narXiv:2306.07969v1  [cs.CV]  13 Jun 2023\nIn this work we study the problem of general conditional\nimage similarity, training on an open-set of similarity con-\nditions, and evaluating on diverse similarity notions in a\n\u2018zero-shot\u2019 manner. We first design a benchmark compris-\ning of four evaluation datasets for conditional image simi-\nlarity, setting up conditional retrieval tasks. We define these\ntasks under a unified framework which spans practical use\ncases, and propose the benchmark as a sparse but broad\ncoverage of the conditional similarity space. We propose\nthese datasets for zero-shot evaluation only, and suggest\nthat models which can perform well without fine-tuning can\nflexibly adapt to general notions of similarity, as desired.\nWe name this benchmark GeneCIS (\u2018genesis\u2019) for General\nConditional Image Similarity. On GeneCIS, we find that\nbaselines built from powerful CLIP backbones struggle and,\nmoreover, that performance on it is only weakly correlated\nwith the backbones\u2019 ImageNet accuracy [17]. This is in\ncontrast to popular vision tasks such as segmentation [39]\nand detection [45], underlining the benchmark\u2019s utility.\nWe also propose a solution to training general condi-\ntional similarity models, based on parsing large-scale cap-\ntion datasets [64,66]. Rather than requiring exhaustive sim-\nilarity annotations, we find that we can automatically mine\nthis information from already abundant image-caption data.\nWe show that training in this way offers substantial gains\nover the baselines, approaching (and in some cases sur-\npassing) carefully designed specific solutions for each of\nthe GeneCIS tasks. In addition, we demonstrate that our\nmethod scales with increasing amounts of caption data, sug-\ngesting promising directions for future work. Finally, on\nrelated benchmarks from the \u2018Composed Image Retrieval\u2019\n(CIR) field [44,74], we find our method provides gains over\nzero-shot baselines. In fact, our model outperforms state-\nof-the-art on the MIT-States benchmark [28], despite being\nevaluated zero-shot and never seeing the training data.\nContributions. (i) We present a framework for considering\nconditional image similarity, an important but understudied\nproblem; (ii) We propose the GeneCIS benchmark to test\nmodels\u2019 abilities to dynamically adapt to different notions\nof similarity; (iii) We show that current vision-language\nmodels like CLIP struggle on GeneCIS, and that perfor-\nmance on it is only weakly correlated with ImageNet accu-\nracy; (iv) We design a scalable solution to the conditional\nsimilarity problem based on automatically parsing large-\nscale image-caption data; (v) We show our models provide\nsubstantial gains over zero-shot CLIP baselines; (vi) We\nvalidate our models on related CIR benchmarks, surpassing\nstate-of-the-art on MIT-States despite zero-shot evaluation.\n2. Related Work\nOur thesis that the similarity between two images should\nbe conditional is generally relevant to the representation\nlearning literature, which aims to learn embedding func-\ntions based on a single (often implicit) notion of similarity.\nFor instance, deep metric learning [30, 34, 63] aims to\nlearn visual representations such that images from the same\ncategory are projected nearby in feature space. This idea is\nused in practical domains such as image retrieval [7,59,61],\nface verification [11, 67, 68] and vehicle re-identification\n[25, 31, 42]. The key limitation here is that networks are\ntrained to encode a single notion of similarity, namely\ncategory-level similarity. While some work considered no-\ntions of similarity at different visual granularities [4,15,70],\nwe posit that there exist concepts of similarity (e.g. shape\nand color) which are orthogonal to categories.\nMeanwhile, contrastive learning [9, 10, 12, 13] defines\nnotions of similarity by specifying a set of transforma-\ntions to which the representation should be invariant (e.g.\ncolor jitter or random cropping), encouraging augmenta-\ntions of the same instance to be embedded together. Sim-\nilarly, vision-language contrastive training [29, 60] learns\njoint embedding spaces, where images\u2019 representations are\naligned with their paired captions. Though the precise no-\ntions of similarity are difficult to define in this case, we note\nthat the embeddings are fundamentally unconditional, with\na single deterministic embedding of a given image.\nFinally, we highlight three relevant sub-fields in the lit-\nerature: conditional similarity networks (CSNs); composi-\ntional learning (CL); and composed image retrieval (CIR).\nCSNs are networks with multiple subspaces for different\nnotions of similarity [73]. Though their motivation is highly\nrelated to our work, CSNs are trained in a supervised\nmanner with pre-defined similarity conditions [41, 46, 73],\nand/or are evaluated in constrained domains such as fash-\nion [32, 69]. In contrast, we aim to train on an open-set\nof similarity conditions and evaluate zero-shot on natural\nimages. Meanwhile, our work is related to CL research in\nthat we seek to compose information from images and con-\nditions to establish similarities. However, again, CL mod-\nels are often assessed on their ability to recognize unseen\ncombinations of a finite set of visual primitives [47,54,56].\nLastly, the most similar setup to GeneCIS is proposed in\nthe recent CIR [74]. It tackles the problem of composing\nan image and text prompt to retrieve relevant images from\na gallery [1,3,16]. This is typically posed in the context of\nfashion [23,76], with the text prompt acting as an image edit\ninstruction (e.g. \u2018the same dress but in white\u2019 [1]). As such,\nCIR tackles a subset of the conditional similarity problem,\nby presenting models with a \u2018negative\u2019 similarity condition.\nKey similarities and differences with prior work: In this\nwork, we leverage CIRR [44] and MIT-States [28] (natu-\nral image CIR datasets) for additional evaluations, and fur-\nther leverage the \u2018Combiner\u2019 architecture [3] to compose\ntext conditions and image features. Broadly speaking, our\nwork differs from CSNs, CL and CIR in that we do not\n2\nSame Object \nSame Attribute\nSame Object \nWrong Attribute\nSame Object \nWrong Attribute\nFocus on an Attribute\nSame Object \nCondition Attribute\nWrong Object \nCondition Attribute\nSame Object \nWrong Attribute\n\u201colive green\u201d\nChange an Attribute\n\u201ccolor\u201d\nSame Scene \nCondition Object\nSame Scene \nNo Condition Object\nWrong Scene \nCondition Object\nFocus on an Object\n\u201crefrigerator\u201d\nSame Scene \nCondition Object\nSame Scene \nNo Condition Object\nWrong Scene \nCondition Object\nChange an Object\n\u201cceiling\u201d\nFigure 2. The GeneCIS benchmark contains four evaluation tasks for conditional similarity, where the goal is to retrieve the most similar\nimage from a gallery (right, green squares), given a reference (left, yellow squares), and condition (blue ovals). Each task explores one\ncombination of \u2018focus\u2019/\u2018change\u2019 an \u2018attribute\u2019/\u2018object\u2019. All galleries contain \u2018distractors\u2019 (dashed, dark-red squares) which are implicitly\nsimilar to the reference or condition. Thus, given a reference and explicit condition, GeneCIS evaluates models\u2019 ability to select the most\nconditionally similar gallery image. Note: We show three gallery images for clarity, though all GeneCIS galleries have 10-15 images.\ntrain on a finite, closed-set of similarity conditions or vi-\nsual primitives. Instead, we train models on open-world\nimage-caption data, and demonstrate a flexible understand-\ning of conditional similarity through zero-shot evaluation\non a range of similarity conditions in natural images.\n3. Conditional Similarity\nWe now describe our setup for the conditional similarity\nproblem and its associated challenges \u2013 both with bench-\nmarking models and acquiring data to train them. In \u00a7 4 we\nintroduce the GeneCIS benchmark which measures impor-\ntant aspects of the problem. In \u00a7 5, we present a scalable\nsolution to automatically acquire training data from widely\navailable image-caption datasets.\nProblem Definition: We define the problem of conditional\nsimilarity as learning a similarity function between two im-\nages given an explicit condition: f(IT ; IR, c) yields the\nscalar similarity between a target image, IT , and a refer-\nence image, IR, given some external condition, c. We use\nthe scalar f(\u00b7) to find the most conditionally similar image\nfrom a target set, i.e., to solve a retrieval task. In this work\nwe consider the condition to be a user-specified text prompt,\nalthough other types of condition are possible. We highlight\nthat standard image similarity, framed as f(IT , IR), implic-\nitly assumes a similarity condition, often incorporated into\nthe model or dataset (see \u00a7 2). We refer to the case where\nimages are similar under an unspecified condition as the im-\nages being implicitly similar.\n3.1. Challenges in training and evaluation\nChallenges in evaluation: The key difficulty in evaluating\nconditional similarity is that there are infinitely many pos-\nsible conditions: from \u2018images with the same top-left pixel\nvalue are similar\u2019 to \u2018the same image but upside down is\nsimilar\u2019. Thus, it is impossible to evaluate models\u2019 ability\nto adapt to every similarity condition. Instead, in \u00a7 4, we\nintroduce the GeneCIS benchmark which consists of a sub-\nset of such conditions, and covers a broad range of practical\nuse cases. We suggest that models which produce zero-shot\ngains across GeneCIS, without finetuning, are more capable\nof flexibly adapting to different notions of similarity.\nChallenges in acquiring training data: Since the space\nand diversity of similarity conditions is huge, acquiring hu-\nman annotations to train for every type of conditional simi-\nlarity is not feasible. For instance, to train a function which\nis sensitive to object category given some conditions (e.g.,\n\u2018car\u2019 or \u2018bridge\u2019 objects in Figure 1), and \u2018color\u2019 given oth-\ners (e.g. \u2018blue\u2019 or \u2018black\u2019 car in Figure 1), we need training\ndata containing both features. Prior work addresses this by\ndramatically restricting the space of conditions and training\non human annotations for pre-defined notions of similar-\nity [46,73]. In \u00a7 5, we describe an automatic method which\nleverages existing large-scale image-text datasets to learn an\nopen-set of similarity conditions. The resulting model can\nbe evaluated in a zero-shot manner across different types of\nconditional similarity task.\n4. The GeneCIS Benchmark\nGeneCIS considers two important dimensions of the\nconditional similarity problem. Firstly, a user may be in-\nterested in an object in the scene (\u2018with the same car\u2019) or\nan attribute of a given object (\u2018the same color as the car\u2019).\nSecondly, the condition could either focus on a particular\naspect of the image (\u2018the same color as the car\u2019) or specify\nthe \u2018negative\u2019 space of a similarity condition, by defining a\nchange in the image (\u2018this car but in black\u2019).\n3\nWe propose four evaluation tasks in GeneCIS, that\ncovers the combination of the above dimensions and hence\na diverse range of conditional similarities. For each of the\ntasks, we construct retrieval problems with: a reference im-\nage, IR; a text condition, c; and a retrieval gallery of M\ntarget images, {IT\ni }M\ni=1, of which only one is \u2018correct\u2019 or\n\u2018positive\u2019. The task is to identify which of the target images\nis most similar to the reference, given the condition. The\nretrieval tasks, illustrated in Figure 2 with more examples\nin Appendix G.1, are:\n\u2022 Focus on an Attribute: This task evaluates a model\u2019s\nability to focus on a specific attribute type (e.g \u2018color\u2019 or\n\u2018material\u2019). For instance, in Figure 2, we see a white\nlaptop and the condition \u2018color\u2019, with the task being to\nselect the laptop with the same color from the gallery.\n\u2022 Change an Attribute: This task contains \u2018negative\u2019 sim-\nilarity conditions, considering target images with a spe-\ncific attribute changed to be most similar. In Figure 2,\nthe aim is to retrieve the same object (\u2018train\u2019) but with the\ncolor changed from \u2018green\u2019 to \u2018olive green\u2019.\n\u2022 Focus on an Object: This task considers reference im-\nages with many objects, and we refer to the set of objects\ntogether as a proxy for the image \u2018scene\u2019. The condition\nselects a single object from the reference as the most im-\nportant (e.g. \u2018refrigerator\u2019 in Figure 2) and the \u2018positive\u2019\ntarget contains the condition object as well as the same\n\u2018scene\u2019 (e.g. also contains \u2018sky\u2019, \u2018chair\u2019 etc. in Figure 2).\n\u2022 Change an Object: This task considers \u2018negative\u2019 sim-\nilarity through conditions which specify an object to be\nadded to a scene. For instance, in Figure 2, \u2018ceiling\u2019 is\nspecified, with the aim being to retrieve the same scene (a\ntrain station) but with a ceiling also present.\nThe tasks in GeneCIS are designed to be diverse and\nchallenging for a single model while remaining well-posed.\nIn Figure 2, given only the reference image, IR, and text\ncondition, c, a human can readily identify which of the tar-\nget images is most \u2018similar\u2019. We wish to benchmark vision\nmodels\u2019 competency at the same task.\nFor the benchmark to be challenging, we would want the\nmodel to need both the image content and the text condition\nto solve the problem. Thus, we include different forms of\n\u2018distractor\u2019 images in the galleries. For instance, for tasks\nwith objects in the condition, we include distractors which\nhave a similar \u2018scene\u2019 to the reference but do not contain\nthe condition object. Such distractors are likely to affect\nmodels which are over-reliant on information from the ref-\nerence image, without considering the condition. Similarly,\nwe include distractors which contain the object specified in\nthe condition, but not the reference scene, confusing mod-\nels which solely rely on the condition. Meanwhile, for the\nattribute-based tasks, we include distractors which contain\nthe reference object category, but not the correct attribute,\nTable 1. Statistics of the four tasks in the GeneCIS benchmark.\nName\nBase Dataset\n# Templates\n# Gallery Images\nFocus on an Attribute\nVAW [56]\n2000\n10\nChange an Attribute\nVAW [56]\n2112\n15\nFocus on an Object\nCOCO [36]\n1960\n15\nChange an Object\nCOCO [36]\n1960\n15\nSorted Conditions \nOccurrences\nAttributes\nObjects\n\u201cfrizzy\u201d\n\u201cmonochromatic\u201d\n\u201cdark\u201d\n\u201c\ufb02oor\u201d\n\u201cbridge\u201d\n\u201celephant\u201d\nFigure 3. Distribution of conditions for attribute- and object-\nbased conditions. For \u2018Focus on an Attribute\u2019, we show the distri-\nbution of the common attribute between the reference and positive\ntarget image (the condition itself is an attribute type, e.g. \u2018color\u2019).\nand vice-versa. As such, many targets are implicitly similar\nto the reference (similar given some condition), but the pos-\nitive image is the most similar given the provided condition.\nBenchmark Details:\nWe construct all tasks by re-\npurposing existing public datasets. For the two tasks which\n\u2018focus on\u2019 and \u2018change\u2019 attributes, we leverage the VAW\ndataset [56], which inherits from Visual Genome [37].\nFrom VAW, we extract crops for individual objects and, for\neach object, use annotations for: object category; positively\nlabelled attributes (which the object definitely possesses);\nand negatively labelled attributes (which the object defi-\nnitely does not possess). For the two tasks which \u2018focus\non\u2019 or \u2018change\u2019 objects, we use COCO Panoptic Segmenta-\ntion data [36,40] containing dense category annotations for\nevery pixel in the image. We give full details of the template\nconstruction process for each task in Appendix A.1.\nWe show statistics of the evaluations in Table 1, includ-\ning the number of retrieval templates and number of gallery\nimages. We note that we carefully construct the benchmarks\nsuch that there is only one \u2018positive\u2019 image among the tar-\ngets, with gallery sizes of between 10 and 15 images. This\nis different to many \u2018text-to-image\u2019 or \u2018image-to-text\u2019 re-\ntrieval benchmarks [40, 57], which contain galleries with\nthousands of targets. Though larger galleries increase the\ntasks\u2019 difficulty, the galleries inevitably contain some valid\ntargets which are treated as negative. We further show the\ndistribution of objects and attributes specified in the condi-\ntions in Figure 3, noting that our space of conditions spans\na long-tail of over 400 attributes and 100 objects.\n4\n2. Extract relationships\npainting of a brown horse on a canvas, with \na black tail and upright posture  \nRelationships: (\u2018Subject\u2019 \n 'Predicate\u2019 \n 'Object\u2019) \n1: painting (subj.) \u2192 of (pred.) \u2192 horse (obj.) \n2: horse (subj.) \u2192 with (pred.) \u2192 posture (obj.) \n\u2026 \nK: horse (subj.) \u2192 on (pred.) \u2192 canvas (obj.) \n\u2192\n\u2192\nFiltered Relationships: \n1: painting (subj.) \u2192 of (pred.) \u2192 horse (obj.) \n2: horse (subj.) \u2192 on (pred.) \u2192 canvas (obj.) \nEntities: \npainting, horse, \ncanvas, tail, posture \nText-scene-graph \nparser\nVisual Concreteness \nDatabase\nhorse (subj.) \u2192 on (pred.) \u2192 meadow (obj.)\nhorse (subj.) \u2192 on (pred.) \u2192 canvas (obj.)\nIR\nIT\n\u201con canvas\u201d\nc\n3. Construct triplets: (IR, IT, c)\nShared subject \nDi\ufb00erent objects \nCondition: \nTarget Pred.  \n+ \nTarget Obj.\nyoung swimmer in a  \nswimming pool \npainting of a brown horse on a canvas, with \na black tail and upright posture  \na golden crown on the fence\nhorses grazing on a meadow \n1. Image-Caption Data\n\u2026\n\u2026\nFigure 4. Method overview. Our method for training general conditional similarity functions extracts information from large-scale image-\ncaption datasets (left). We extract \u2018Subject\u2019 \u2192 \u2018Predicate\u2019 \u2192 \u2018Object\u2019 relationships from the caption data (middle), before using them to\nconstruct training triplets where a reference and target image are related by a condition (right).\nNoise and human verification: Though, in principle, our\nbenchmark should be error free, manual inspection of the\ntemplates shows that noise is introduced through underly-\ning inconsistencies in Visual Genome [37], VAW [56] and\nCOCO [36]. We are currently in the process of collect-\ning manual annotations and human verification of the tem-\nplates, and present the current version as \u2018GeneCIS v0\u2019.\n5. Method\nIn \u00a7 5.1, we briefly describe preliminaries for our ap-\nproach to learning general conditional similarity functions.\nThis includes the model architecture and optimization ob-\njective which we inherit from prior work [3]. In \u00a7 5.2,\nwe describe our main methodological contribution: an au-\ntomatic and scalable way of mining conditional similarity\ntraining data from widely available image-caption datasets.\n5.1. Preliminaries\nTraining data. To learn a conditional similarity function\nf(\u00b7), we train with triplets (IR, IT , c), where IRand IT are\ntermed reference and target images, and c is the condition\ndefining a relationship between them.\nModel Architecture We parametrize the conditional simi-\nlarity function f(\u00b7) with deep networks, first encoding fea-\ntures for (IR, IT , c) as (xR, xT , e) \u2208 RD. We learn sepa-\nrate encoders, \u03a6(I) and \u03a8(c), for the images and text con-\ndition. Next, we train a \u2018Combiner\u2019 network [3], which\ncomposes the reference image features with the condition\ntext features as g(xR, e) \u2208 RD.\nFinally, we consider\nthe scalar conditional similarity to be the dot product be-\ntween the combined feature, and target image feature, as:\nf(IT ; IR, c) = g(xR, e) \u00b7 xT . Details of the Combiner ar-\nchitecture can be found in Appendix D and [3].\nWe initialize our image and text backbones, \u03a6(\u00b7) and\n\u03a8(\u00b7), with CLIP [60].\nCLIP models are pre-trained on\n400M image-text pairs containing a range of visual con-\ncepts. Furthermore, the visual and text embeddings from\nCLIP are aligned, making it easier to learn the composition\nbetween reference image and conditioning text features.\nOptimisation Objective Given a batch of triplets, B =\n{(IR\ni , IT\ni , ci)}|B|\ni=1, we get features as {(xR\ni , xT\ni , ei)}|B|\ni=1.\nThen, given a temperature \u03c4, we optimise (\u03a6, \u03a8, g) with\na contrastive loss [50], as:\nL = \u2212 1\n|B|\nX\ni\u2208B\nlog\nexp\n\u0000g(xR\ni , ei)\u00b7xT\ni /\u03c4\n\u0001\nP\nj\u2208B exp\n\u0000g(xR\ni , ei)\u00b7xT\nj /\u03c4\n\u0001\n(1)\n5.2. Scalable training for conditional similarity\nTo train for general conditional similarity, we wish to cu-\nrate triplets for training, Dtrain = {(IR\ni , IT\ni , ci)}N\ni=1, with\ndiverse conditions and concepts of similarity. However, as\nthe space of conditions increases, the burden for exhaus-\ntively annotating such a dataset increases exponentially. In-\nstead, our method (illustrated in Figure 4) automatically\nmines training triplets from existing data sources:\nImage-caption Data: We begin with large-scale image-\ncaption data scraped from the internet, containing images\npaired with descriptive captions [51, 66].\nWe hope that\nthe captions contain information about the objects and at-\ntributes in the image, which we can utilize for the condi-\ntional similarity task. We also hope that such a method can\nscale with increasing data in the same way that conventional\nrepresentation learning algorithms do.\nExtract relationships: We use an off-the-shelf text-to-\nscene-graph parser [65, 77] to identify \u2018Subject\u2019 \u2192 \u2018Pred-\nicate\u2019 \u2192 \u2018Object\u2019 relationships within the caption [55]. For\ninstance, from the central image in Figure 4, we extract the\nhighlighted relationship \u2018Horse\u2019 \u2192 \u2018on\u2019 \u2192 \u2018Canvas\u2019. Note\nthat one caption may contain many such relationships.\nWe find that many of the entities (\u2018Subjects\u2019 or \u2018Ob-\njects\u2019) extracted by the parser are not visually grounded in\n5\nthe image, e.g., pronouns (\u2018I\u2019, \u2018you\u2019) or time-based nouns\n(\u2018today\u2019, \u2018yesterday\u2019). To address this, we introduce an ad-\nditional filtering step, where every entity is scored for \u2018vi-\nsual concreteness\u2019 based on a pre-existing database [8]. The\ndatabase contains human ratings between 1 and 5 for how\nvisually apparent a noun is. For each extracted relationship,\nwe average its \u2018Subject\u2019 and \u2018Object\u2019 concreteness scores,\ndiscarding relationships if their value is below a threshold.\nConstruct triplets: We first randomly select a relationship,\ntaking the image it comes from as the \u2018reference\u2019, IR. Hav-\ning identified the subject of the relationship (e.g. \u2018Horse\u2019 in\nthe rightmost column of Figure 4) we identify all other re-\nlationships in the dataset containing the same subject. From\nthis restricted pool of relationships, we randomly sample a\n\u2018target\u2019 relationship and image, IT , with the same subject\nbut a different object (e.g. a horse on a \u2018canvas\u2019 instead of\nin a \u2018meadow\u2019 in Figure 4). Finally, we define the condition\nof the triplet, c, as the concatenated \u2018Predicate\u2019 and \u2018Object\u2019\nfrom the target relationship (\u2018on canvas\u2019 in Figure 4).\nDiscussion: We note that our mined triplets exhibit a bias\ntowards the \u2018Change an Object\u2019 GeneCIS task. However,\nthe triplets often involve abstract relationships between ref-\nerence and target images (e.g. \u2018Horse on canvas\u2019 in Fig-\nure 4). As such, solving the training task requires the model\nto use the condition to extract and modify diverse forms of\ninformation from the reference, which is the central require-\nment of the broader conditional similarity problem.\n6. Main Experiments\nWe evaluate baselines, task-specific solutions, and our\nmethod on the proposed GeneCIS benchmark.\n\u00a7 6.1 de-\nscribes the baselines as well as specific solutions which we\ndesign for each of the GeneCIS tasks. \u00a7 6.3 shows results\non GeneCIS and, in \u00a7 6.4, we evaluate on related bench-\nmarks from the Composed Image Retrieval (CIR) literature.\n6.1. Baselines and Specific Solutions for GeneCIS\nCLIP-Only Baselines: We provide three simple CLIP-only\n[60] baselines for GeneCIS. Our Image Only baseline em-\nbeds all images with the CLIP image encoder and retrieves\nthe closest gallery image to the reference. The Text Only\nbaseline embeds the text condition with the CLIP text en-\ncoder, and the gallery images with the image encoder, and\nfinds the closest gallery image to the text embedding. Fi-\nnally, our Image + Text baseline averages the reference im-\nage with the condition text feature, before using the com-\nbined vector to find the closest gallery image.\nCIRR Combiner baseline:\nCIRR is a natural image\ndataset [44] containing 28K curated retrieval templates. All\ntemplates contain a human-specified text condition defining\nthe relationship between the reference and \u2018positive\u2019 target\nimage. Unlike our automatic and scalable triplet mining\nmethod, CIRR is manually constructed with a lengthy anno-\ntation process. We include a baseline from [3], which trains\na Combiner model with a CLIP backbone on CIRR. For fair\ncomparison with our method, we fine-tune both the image\nand text backbones on CIRR before evaluating the model\nzero-shot on GeneCIS, terming it Combiner (CIRR).\nSpecific Solutions: We also design specific solutions for\neach of the proposed tasks in GeneCIS. These solutions\ntake into account the construction mechanisms of each task\nand represent sensible approaches to tackling the tasks inde-\npendently. We design all solutions to respect the zero-shot\nnature of the evaluations and hence they are all based on\n\u2018open-vocabulary\u2019 models; we use CLIP for the attribute-\nbased tasks and Detic [81] for the object-based ones. For\nthe attribute-based tasks, we use CLIP to predict attributes\nor categories in the reference image, before using text em-\nbeddings of these predictions to search the gallery. For the\nobject-based tasks, we use Detic to detect the object cate-\ngories present in all images, treating the detected categories\nas bag-of-word descriptors of the target images. We give\nfull details of the specific solutions in Appendix B.\n6.2. Implementation Details\nWe train our strongest model on 1.6M triplets mined\nfrom Conceptual Captions 3 Million (CC3M) [66] which\ncontains 3M image-caption pairs. Each triplet has a visual\nconcreteness of at least 4.8 averaged over the \u2018Subject\u2019 and\n\u2018Object\u2019 entities in both the reference and target image. We\ntrain the contrastive loss with temperature \u03c4 = 0.01 and\nbatch size of 256, training for 28K gradient steps. We use\nearly stopping based on the Recall@1 on the CIRR valida-\ntion set and, for fair comparison with [3], initialize the im-\nage and text backbones with the ResNet50\u00d74 CLIP model.\nFurther details are in Appendix E.\n6.3. Analysis on GeneCIS\nWe report results for all methods on the GeneCIS bench-\nmark in\nTable 2.\nOur evaluation metric is Recall@K:\nthe frequency with which the model ranks the \u2018correct\u2019\ngallery image in its top-K predictions. We report results\nat K = {1, 2, 3} to evaluate under different constraints, and\nto account for any noise in the benchmark. We also report\nthe Average R@1 over all tasks to measure the overall per-\nformance across different forms of conditional similarity.\nTakeaways: From the baselines we find that both the \u2018Im-\nage Only\u2019 and \u2018Text Only\u2019 models perform poorly as ex-\npected, since they only rely on either the reference image\ncontent or the text condition. The \u2018Image + Text\u2019 and \u2018Com-\nbiner (CIRR)\u2019 models perform better, validating our claim\nthat both the reference and text condition are required to\nsolve the task. Phrased differently, this suggests the bench-\nmark evaluates conditional similarity, as implicit similarity\nfunctions (e.g. the \u2018Image Only\u2019 baseline) perform poorly\n6\nTable 2. Evaluation on GeneCIS. We evaluate baselines and our method. We also evaluate specific solutions for each task (shown gray,\nthese are not general conditional similarity functions and hence cannot be evaluated on all tasks). Both across ten random seeds, and with\nten cross-validation splits, we find a standard deviation of \u2248 0.2% in our model\u2019s R@1 on each task, as well as on average over all tasks.\nFocus Attribute\nChange Attribute\nFocus Object\nChange Object\nR@1\nR@ 2\nR@3\nR@1\nR@ 2\nR@3\nR@1\nR@ 2\nR@3\nR@1\nR@ 2\nR@3\nAverage R@1\nSpecific Solution (Focus Attribute)\n20.8\n32.6\n41.1\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nSpecific Solution (Change Attribute)\n-\n-\n-\n15.2\n25.8\n35.6\n-\n-\n-\n-\n-\n-\n-\nSpecific Solution (Object)\n-\n-\n-\n-\n-\n-\n18.7\n30.3\n37.4\n18.1\n28.7\n34.5\n-\nImage Only\n17.7\n30.9\n41.9\n11.9\n20.8\n28.8\n9.3\n18.2\n26.2\n7.2\n16.7\n24.9\n11.5\nText Only\n10.2\n20.5\n29.6\n9.5\n17.6\n26.4\n6.5\n16.8\n22.4\n6.2\n13.9\n21.4\n8.1\nImage + Text\n15.6\n26.3\n37.1\n12.6\n22.9\n32.0\n10.8\n21.0\n31.2\n11.3\n21.5\n30.3\n12.6\nCombiner (CIRR)\n15.1\n27.7\n39.8\n12.1\n22.8\n31.8\n13.5\n25.4\n36.7\n15.4\n28.0\n39.6\n14.0\nCombiner (CC3M, Ours)\n19.0\n31.0\n41.5\n16.6\n27.5\n36.5\n14.7\n25.9\n36.1\n16.8\n29.1\n39.7\n16.8\nTable 3. Results on MIT-States [28]. Zero-shot evaluation of our\nmodel outperforms SoTA supervised methods on this dataset.\nZero-shot\nRecall @ 1\nRecall @ 5\nRecall @ 10\nTIRG [74]\n\u2717\n12.2\n31.9\n43.1\nComposeAE [1]\n\u2717\n13.9\n35.3\n47.9\nLBF [27]\n\u2717\n14.7\n35.3\n46.6\nHCL [79]\n\u2717\n15.2\n36.0\n46.7\nMAN [19]\n\u2717\n15.6\n36.7\n47.7\nImage Only\n\u2713\n3.7\n14.1\n22.9\nText Only\n\u2713\n9.5\n22.5\n31.4\nImage + Text\n\u2713\n13.3\n31.7\n42.6\nCombiner (CC3M, Ours)\n\u2713\n15.8\n37.5\n49.4\non average. We further find that our method, using automat-\nically mined data, substantially outperforms all baselines on\naverage across the tasks, as well as at Recall@1 on all tasks\nindividually. Notably, it outperforms the model trained on\nmanually collected data from CIRR.\nAs expected, most per-task specific solutions perform\nbetter than our general method. However, the broad zero-\nshot nature of GeneCIS makes all tasks independently chal-\nlenging and the specific solutions do not work for all of\nthem. Broadly speaking, we found that CLIP [60] struggles\nto predict object attributes, and that Detic [81] struggles on\nthe \u2018stuff\u2019 categories in COCO Panoptic [36].\nFinally, caveats can be found in \u2018Image Only\u2019 results\non \u2018Focus Attribute\u2019, where the baseline performs slightly\nbetter than our method at higher recalls. This is because\nthere are some similarity conditions (e.g. \u2018color\u2019) for which\nstandard image embeddings are well suited. We also find\nthat \u2018Combiner (CIRR)\u2019 performs better on tasks with ob-\nject conditions, as the multi-object image distribution of\nCIRR is more closely aligned with these tasks, than with\nthe single-object images in the attribute-based tasks. We\nnote that good performance on all tasks collectively indi-\ncates strong general conditional similarity models.\n6.4. Comparisons to Prior Work\nGeneCIS uses natural images with general conditions,\nrather than being specialized to domains such as bird\nspecies [46], faces [80] or fashion compatability [23, 24,\n71, 76]. As such, to find comparable existing benchmarks,\nwe turn to the Composed Image Retrieval (CIR) literature.\nThe CIR task is to retrieve images which best match a com-\nTable 4. Results on CIRR [44]. Our model substantially outper-\nforms the comparable zero-shot baselines.\nZero-shot\nRecall @ 1\nRecall @ 5\nRecall @ 10\nARTEMIS [16]\n\u2717\n17.0\n46.1\n61.3\nCIRPLANT [44]\n\u2717\n19.6\n52.6\n68.4\nCombiner (CIRR, [3])\n\u2717\n38.5\n70.0\n81.9\nCombiner (CIRR, improved)\n\u2717\n40.9\n73.4\n84.8\nImage Only\n\u2713\n7.5\n23.9\n34.7\nText Only\n\u2713\n20.7\n43.9\n56.1\nImage + Text\n\u2713\n21.8\n50.9\n63.7\nCombiner (CC3M, Ours)\n\u2713\n27.3\n57.0\n71.1\nposed reference image and editing text condition. This task\naligns with the \u2018Change\u2019 dimension of GeneCIS. We eval-\nuate on both the MIT-States benchmark [28] as well as on\nCIRR [44], with the former precisely reflecting the \u2018Change\nAttribute\u2019 GeneCIS task.\nMetrics: On both benchmarks, we evaluate our model zero-\nshot on the test-sets and compare with prior work trained on\nthe datasets. These datasets are partially labeled and eval-\nuate using a global retrieval setting, i.e., the entire test-set\nis used as a gallery for each query. Thus, we follow prior\nwork and report Recall@K at multiple K = {1, 5, 10} to\nfully capture the model\u2019s performance. 1\nResults: We show results on MIT-States in Table 3. Prior\nwork on this benchmark trains models on the dataset from\nscratch and thus is not zero-shot. Nonetheless, zero-shot\nevaluation of our model surpasses state-of-the-art on this\ntask. However, we note that prior methods use smaller mod-\nels compared to our pre-trained CLIP backbone.\nWe report on CIRR in Table 4, evaluating through the of-\nficial test server and again comparing to methods that train\nfor this setting. We report results for the Combiner method\nfrom the paper [3] as well as our improved implementation\n(see \u00a7 6.1), which are both trained on CIRR. Our improved\nimplementation is a strong upper bound, surpassing previ-\nous fully supervised models. On zero-shot evaluation, our\nmethod surpasses the comparable baselines by a signficant\nmargin across all the recall metrics. Compared to super-\nvised methods, our model outperforms [16] and [44] zero-\nshot, though we note [16] trains from scratch. Finally, our\n1CIRR also has an evaluation on curated galleries, akin to GeneCIS. We\ndo not report on this as we found that the \u2018Text Only\u2019 baseline performed\ncomparably with SoTA models on this task, achieving over 60% Recall@1.\n7\nTable 5. Ablations of key design choices of our full model with\nresults reported on our GeneCIS benchmark.\nAverage Recall @ 1\nFull Model\n16.8\nNo filtering for visual concreteness\n15.0\nFreezing CLIP image backbone\n14.7\nFreezing CLIP text backbone\n15.8\nFreezing entire backbone\n15.1\nTraining on SBU [51] instead of CC3M [66] caption data\n16.5\nmodel reduces the gap between the baselines and specialist\nCombiner models trained on CIRR.\n7. Analysis\nAblations: Table 5 shows the effect of our design choices\non the performance on GeneCIS. We find that filtering out\nrelationships which are not visually concrete, and finetuning\nthe entire backbone, both strongly affect the performance.\nWe verify the robustness of our triplet mining procedure\nby training with SBU Captions [51], a smaller but differ-\nent source of image-caption data. We find that though the\nlarger CC3M [66] produces slightly better results, different\nimage-caption datasets are also suitable.\nComparing pretrained backbones: In Figure 6, we study\nthe effect of changing the CLIP initialization.\nWe train\nCombiner models with ResNet [26] and ViT [18] backbones\non CC3M, showing their performance as well as the \u2018Image\n+ Text\u2019 baseline from \u00a7 6.1. 2\nWe plot the performance on GeneCIS against the CLIP\nbackbone\u2019s zero-shot ImageNet accuracy [17]. We observe\nthat the performance on GeneCIS is weakly correlated\nwith the ImageNet performance of the backbone: a Top-1\ngain of 10% on ImageNet leads to only 1% improvement on\nGeneCIS. This suggests that improvements on ImageNet do\nnot directly transfer to GeneCIS and that GeneCIS measures\na different yet important capability of vision models. In ad-\ndition, our method offers a substantial boost over the \u2018Image\n+ Text\u2019 baseline, and a greater boost than scaling the under-\nlying CLIP model. Both of these results are in stark contrast\nto trends on popular vision tasks such as segmentation [39]\nand detection [45], where gains on ImageNet directly trans-\nfer to large gains on the downstream task, and often more\nsignificantly so than gains from the underlying method.\nScaling the number of triplets: In Figure 5, we investigate\nthe effect of scaling the conditional similarity training data.\nWe successively decrease the number of mined triplets by\nfactors of four (from the 1.6M used to train our strongest\nmodels) both with and without concreteness filtering. We\nfind results improve with increasing numbers of triplets and\nthat while our models are trained on a dataset of 3M image-\ncaption pairs [66], open-source caption datasets exist with\nup to five billion images [64]. We emphasize the utility of\n2For fair comparison with [3], we report with a ResNet50\u00d74 backbone\nin Table 2, and report on our strongest ViT-B/16 model in Appendix C.\n2.5\u00d7104\n1.0\u00d7105\n4.0\u00d7105\n1.6\u00d7106\nNumber of Unique Triplets\n14.0\n14.5\n15.0\n15.5\n16.0\n16.5\nAverage Performance on GeneCIS\nFiltered\nUnfiltered\nFigure 5. Scaling the number of mined triplets used for train-\ning our model improves the performance. This suggests that our\nautomatic mining strategy is a promising and scalable approach to\nlearning general similarity functions.\nRN50\nRN101\nViT-B/32\nViT-B/16\nRN50x4\nRN50x16\nRN50\nRN101\nViT-B/32\nViT-B/16\nRN50x4\nRN50x16\nFigure 6. Impact of different CLIP backbones on the perfor-\nmance of our model and the \u2018Image + Text\u2019 baseline. We show the\nAverage Recall@1 on GeneCIS against the backbones\u2019 zero-shot\nImageNet accuracy, showing the two have a weak correlation.\nthis finding, suggesting it is possible to train stronger condi-\ntional similarity models by further scaling the training data.\n8. Conclusion\nIn this paper we have proposed the GeneCIS benchmark\nfor General Conditional Image Similarity, an important but\nunderstudied problem in computer vision. The benchmark\nextends prior work and evaluates an open-set of similar-\nity conditions, by being designed for zero-shot testing only.\nFurthermore, we propose a way forward for scalably train-\ning conditional similarity models, which mines information\nfrom widely available image-caption datasets.\nOur method not only boosts performance over all base-\nlines on GeneCIS, but also provides substantial zero-shot\ngains on related image retrieval tasks. Moreover, we find\nthat unlike for many popular vision tasks, the performance\nof our models on GeneCIS is roughly decorrelated from\nscaling the backbone network\u2019s ImageNet accuracy, moti-\nvating further study of the conditional similarity problem.\n8\nReferences\n[1] Muhammad Umer Anwaar, Egor Labintcev, and Martin Kle-\ninsteuber. Compositional learning of image-text query for\nimage retrieval. In WACV, 2021. 2, 7\n[2] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended\ndiffusion for text-driven editing of natural images. In CVPR,\n2022. 15\n[3] Alberto Baldrati, Marco Bertini, Tiberio Uricchio, and Al-\nberto Del Bimbo. Conditioned and composed image retrieval\ncombining and partially fine-tuning clip-based features. In\nCVPRW, 2022. 2, 5, 6, 7, 8, 14\n[4] Maxim Berman, Herv\u00b4e J\u00b4egou, Andrea Vedaldi, Iasonas\nKokkinos, and Matthijs Douze. Multigrain: a unified image\nembedding for classes and instances. arXiv, 2019. 2\n[5] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nECCV, 2022. 15\n[6] Andrew Brown, Cheng-Yang Fu, Omkar Parkhi, Tamara L\nBerg, and Andrea Vedaldi. End-to-end visual editing with a\ngeneratively pre-trained artist. ECCV, 2022. 15\n[7] Andrew Brown, Weidi Xie, Vicky Kalogeiton, and Andrew\nZisserman. Smooth-ap: Smoothing the path towards large-\nscale image retrieval. In ECCV, 2020. 2\n[8] Marc Brysbaert, Amy Beth Warriner, and Victor Kuperman.\nConcreteness ratings for 40 thousand generally known en-\nglish word lemmas. In Behavior research methods, 2014.\n6\n[9] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Pi-\notr Bojanowski, and Armand Joulin. Unsupervised learn-\ning of visual features by contrasting cluster assignments.\nNeurIPS, 2020. 1, 2\n[10] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers.\nIn\nICCV, 2021. 1, 2\n[11] Jun-Cheng Chen, Vishal M. Patel, and Rama Chellappa. Un-\nconstrained face verification using deep cnn features.\nIn\nWACV, 2016. 1, 2\n[12] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learning\nof visual representations. In ICML, 2020. 1, 2\n[13] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.\nImproved baselines with momentum contrastive learning.\narXiv, 2020. 1, 2\n[14] Katherine Crowson,\nStella\nBiderman,\nDaniel Kornis,\nDashiell Stander, Eric Hallahan, Louis Castricato, and Ed-\nward Raff. Vqgan-clip: Open domain image generation and\nediting with natural language guidance. In ECCV. Springer,\n2022. 15\n[15] Yin Cui, Zeqi Gu, Dhruv Mahajan, Laurens Van Der Maaten,\nSerge Belongie, and Ser-Nam Lim. Measuring dataset gran-\nularity. arXiv, 2019. 2\n[16] Ginger Delmas, Rafael S Rezende, Gabriela Csurka, and Di-\nane Larlus.\nArtemis: Attention-based retrieval with text-\nexplicit matching and implicit similarity. In ICLR, 2022. 2,\n7\n[17] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR, 2009. 2, 8\n[18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. ICLR, 2020. 8, 14\n[19] Zhixiao Fu, Xinyuan Chen, Jianfeng Dong, and Shouling\nJi. Multi-order adversarial representation learning for com-\nposed query image retrieval. In ICASSP, 2021. 7\n[20] Rinon Gal, Or Patashnik, Haggai Maron, Amit H Bermano,\nGal Chechik, and Daniel Cohen-Or. Stylegan-nada: Clip-\nguided domain adaptation of image generators. ACM Trans-\nactions on Graphics (TOG), 2022. 15\n[21] Robert L. Goldstone and Ji Yun Son. 155 Similarity. Oxford\nUniversity Press, 2012. 1\n[22] Agrim Gupta, Piotr Dollar, and Ross Girshick.\nLVIS: A\ndataset for large vocabulary instance segmentation. In CVPR,\n2019. 15\n[23] Xintong Han, Zuxuan Wu, Phoenix X. Huang, Xiao Zhang,\nMenglong Zhu, Yuan Li, Yang Zhao, and Larry S. Davis. Au-\ntomatic spatially-aware fashion concept discovery. In ICCV,\n2017. 2, 7\n[24] Xintong Han, Zuxuan Wu, Yu-Gang Jiang, and Larry S\nDavis.\nLearning fashion compatibility with bidirectional\nlstms. In ACM, 2017. 7\n[25] Bing He, Jia Li, Yifan Zhao, and Yonghong Tian.\nPart-\nregularized near-duplicate vehicle re-identification.\nIn\nCVPR, June 2019. 2\n[26] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition.\nIn CVPR,\n2016. 8\n[27] Mehrdad Hosseinzadeh and Yang Wang. Composed query\nimage retrieval using locally bounded features. In CVPR,\n2020. 7\n[28] Phillip Isola, Joseph J. Lim, and Edward H. Adelson. Dis-\ncovering states and transformations in image collections. In\nCVPR, 2015. 2, 7\n[29] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,\nHieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom\nDuerig. Scaling up visual and vision-language representation\nlearning with noisy text supervision. In ICML, 2021. 2\n[30] Mahmut Kaya and Hasan S\u00b8akir Bilge. Deep metric learning:\nA survey. Symmetry, 11, 2019. 1, 2\n[31] Sultan Daud Khan and Habib Ullah. A survey of advances in\nvision-based vehicle re-identification. Computer Vision and\nImage Understanding, 2019. 2\n[32] Donghyun Kim, Kuniaki Saito, Samarth Mishra, Stan\nSclaroff, Kate Saenko, and Bryan A. Plummer.\nSelf-\nsupervised visual attribute learning for fashion compatibility.\nICCV Workshops, 2021. 2\n[33] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Dif-\nfusionclip: Text-guided diffusion models for robust image\nmanipulation. In CVPR, 2022. 15\n[34] Sungyeon Kim, Dongwon Kim, Minsu Cho, and Suha Kwak.\nProxy anchor loss for deep metric learning. In CVPR, 2020.\n1, 2\n9\n[35] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014. 14\n[36] Alex Kirillov, Tsung-Yi Lin, Holger Caesar, Ross Girshick,\nand Piotr Doll\u00b4ar.\nMicrosoft coco: Panoptic segmentation\nchallenge, 2017. 4, 5, 7, 12, 13, 14\n[37] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A Shamma, Michael Bernstein, and\nLi Fei-Fei. Visual genome: Connecting language and vision\nusing crowdsourced dense image annotations, 2016. 4, 5, 13\n[38] Gihyun Kwon and Jong Chul Ye. Clipstyler: Image style\ntransfer with a single text condition. In CVPR, 2022. 15\n[39] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen\nKoltun, and Ren\u00b4e Ranftl.\nLanguage-driven semantic seg-\nmentation. ICLR, 2022. 2, 8\n[40] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir\nBourdev, Ross Girshick, James Hays, Pietro Perona, Deva\nRamanan, C. Lawrence Zitnick, and Piotr Doll\u00b4ar. Microsoft\ncoco: Common objects in context. In ECCV, 2014. 4, 12, 13\n[41] Yen-Liang Lin, Son Tran, and Larry S. Davis. Fashion outfit\ncomplementary item retrieval. In CVPR, June 2020. 2\n[42] Xinchen Liu, Wu Liu, Huadong Ma, and Huiyuan Fu. Large-\nscale vehicle re-identification in urban surveillance videos.\nIn ICME, 2016. 2\n[43] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nICCV, 2021. 15\n[44] Zheyuan Liu, Cristian Rodriguez, Damien Teney, and\nStephen Gould. Image retrieval on real-life images with pre-\ntrained vision-and-language models. In ICCV, 2021. 2, 6, 7,\n14\n[45] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim\nNeumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh\nMahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran\nShen, et al. Simple open-vocabulary object detection with\nvision transformers. ECCV, 2022. 2, 8\n[46] Samarth Mishra, Zhongping Zhang, Yuan Shen, Ranjitha\nKumar, Venkatesh Saligrama, and Bryan A. Plummer. Ef-\nfectively leveraging attributes for visual similarity. In ICCV,\n2021. 1, 2, 3, 7\n[47] Ishan Misra, Abhinav Gupta, and Martial Hebert. From red\nwine to red tomato: Composition with context. In CVPR,\n2017. 2\n[48] Andrei Neculai, Yanbei Chen, and Zeynep Akata. Proba-\nbilistic compositional embeddings for multimodal image re-\ntrieval. In CVPR, 2022. 15\n[49] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. arXiv, 2021.\n15\n[50] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-\nsentation learning with contrastive predictive coding. arXiv,\n2018. 5\n[51] Vicente Ordonez,\nGirish Kulkarni,\nand Tamara Berg.\nIm2text: Describing images using 1 million captioned pho-\ntographs. In NeurIPS, 2011. 5, 8\n[52] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zem-\ning Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch:\nAn imperative style, high-performance deep learning library.\nNeurIPS, 2019. 14\n[53] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or,\nand Dani Lischinski. Styleclip: Text-driven manipulation of\nstylegan imagery. In ICCV, 2021. 15\n[54] Ethan Perez, Florian Strub, Harm De Vries, Vincent Du-\nmoulin, and Aaron Courville. Film: Visual reasoning with a\ngeneral conditioning layer. In AAAI, 2018. 2\n[55] Julia Peyre, Ivan Laptev, Cordelia Schmid, and Josef Sivic.\nDetecting unseen visual relations using analogies. In ICCV,\n2019. 5\n[56] Khoi Pham, Kushal Kafle, Zhe Lin, Zhihong Ding, Scott Co-\nhen, Quan Tran, and Abhinav Shrivastava. Learning to pre-\ndict visual attributes in the wild. In CVPR, 2021. 2, 4, 5, 12,\n13, 14\n[57] Bryan A. Plummer, Liwei Wang, Christopher M. Cervantes,\nJuan C. Caicedo, Julia Hockenmaier, and Svetlana Lazebnik.\nFlickr30k entities: Collecting region-to-phrase correspon-\ndences for richer image-to-sentence models. IJCV, 2017. 4\n[58] Karl Popper. Conjectures and Refutations: The Growth of\nScientific Knowledge. Routledge, 1963. 1\n[59] Filip Radenovi\u00b4c, Ahmet Iscen, Giorgos Tolias, Yannis\nAvrithis, and Ond\u02c7rej Chum.\nRevisiting oxford and paris:\nLarge-scale image retrieval benchmarking. In CVPR, June\n2018. 2\n[60] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable visual\nmodels from natural language supervision. In ICML, 2021.\n2, 5, 6, 7, 14, 15\n[61] Jerome Revaud, Jon Almazan, Rafael S. Rezende, and Cesar\nRoberto de Souza. Learning with average precision: Train-\ning image retrieval with a listwise loss. In ICCV, October\n2019. 2\n[62] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, 2022. 15\n[63] Karsten Roth, Timo Milbich, Samarth Sinha, Prateek Gupta,\nBjorn Ommer, and Joseph Paul Cohen.\nRevisiting train-\ning strategies and generalization performance in deep metric\nlearning. In ICML, 2020. 1, 2\n[64] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade W Gordon, Ross Wightman, Mehdi Cherti, Theo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, Patrick Schramowski, Srivatsa R Kundurthy, Katherine\nCrowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia\nJitsev. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. In NeurIPS Datasets and\nBenchmarks, 2022. 2, 8\n[65] Sebastian Schuster, Ranjay Krishna, Angel Chang, Li Fei-\nFei, and Christopher D Manning. Generating semantically\n10\nprecise scene graphs from textual descriptions for improved\nimage retrieval. In Proceedings of the fourth workshop on\nvision and language, 2015. 5\n[66] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\nSoricut. Conceptual captions: A cleaned, hypernymed, im-\nage alt-text dataset for automatic image captioning. In ACL,\n2018. 2, 5, 6, 8, 15, 22\n[67] Yi Sun, Yuheng Chen, Xiaogang Wang, and Xiaoou Tang.\nDeep learning face representation by joint identification-\nverification. In NeurIPS, 2014. 2\n[68] Yaniv Taigman, Ming Yang, Marc\u2019Aurelio Ranzato, and Lior\nWolf.\nDeepface: Closing the gap to human-level perfor-\nmance in face verification. In CVPR, June 2014. 1, 2\n[69] Reuben Tan, Mariya I. Vasileva, Kate Saenko, and Bryan A.\nPlummer. Learning similarity conditions without explicit su-\npervision. In ICCV, 2019. 1, 2\n[70] Hugo Touvron, Alexandre Sablayrolles, Matthijs Douze,\nMatthieu Cord, and Herv\u00b4e J\u00b4egou.\nGrafit: Learning fine-\ngrained image representations with coarse labels. In ICCV,\n2021. 2\n[71] Mariya I Vasileva, Bryan A Plummer, Krishna Dusad,\nShreya Rajpal, Ranjitha Kumar, and David Forsyth. Learn-\ning type-aware embeddings for fashion compatibility.\nIn\nECCV, 2018. 7\n[72] Vijay Vasudevan, Benjamin Caine, Raphael Gontijo-Lopes,\nSara Fridovich-Keil, and Rebecca Roelofs.\nWhen does\ndough become a bagel? analyzing the remaining mistakes\non imagenet. In NeurIPS, 2022. 13\n[73] Andreas Veit, Serge Belongie, and Theofanis Karaletsos.\nConditional similarity networks. In CVPR, 2017. 1, 2, 3\n[74] Nam Vo, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li\nFei-Fei, and James Hays.\nComposing text and image for\nimage retrieval-an empirical odyssey. In CVPR, 2019. 2, 7\n[75] Jiang Wang, Yang Song, Thomas Leung, Chuck Rosenberg,\nJingbin Wang, James Philbin, Bo Chen, and Ying Wu. Learn-\ning fine-grained image similarity with deep ranking.\nIn\nCVPR, June 2014. 1\n[76] Hui Wu, Yupeng Gao, Xiaoxiao Guo, Ziad Al-Halah, Steven\nRennie, Kristen Grauman, and Rogerio Feris. The fashion\niq dataset: Retrieving images by combining side information\nand relative natural language feedback. CVPR, 2021. 2, 7\n[77] Hao Wu, Jiayuan Mao, Yufeng Zhang, Yuning Jiang, Lei\nLi, Weiwei Sun, and Wei-Ying Ma. Unified visual-semantic\nembeddings: Bridging vision and language with structured\nmeaning representations. In CVPR, 2019. 5\n[78] Tete Xiao, Xiaolong Wang, Alexei A Efros, and Trevor Dar-\nrell. What should not be contrastive in contrastive learning.\nICLR, 2021. 15\n[79] Yahui Xu, Yi Bin, Guoqing Wang, and Yang Yang. Hierar-\nchical composition learning for composed query image re-\ntrieval. In ACM Multimedia Asia, 2021. 7\n[80] Yujie Zhong, Relja Arandjelovi\u00b4c, and Andrew Zisserman.\nFaces in places: Compound query retrieval. In BMVC, 2016.\n7\n[81] Xingyi Zhou,\nRohit Girdhar,\nArmand Joulin,\nPhilipp\nKr\u00a8ahenb\u00a8uhl, and Ishan Misra.\nDetecting twenty-thousand\nclasses using image-level supervision. In ECCV, 2022. 6, 7,\n14\n11\nAppendix\nTable of Contents\nA. Further GeneCIS Details\n12\nA.1. Task construction . . . . . . . . . . .\n12\nA.2. Implementation Details . . . . . . . .\n13\nA.3. Dataset noise . . . . . . . . . . . . .\n13\nA.4. Discussion on symmetry\n. . . . . . .\n13\nB. Specific Solutions\n14\nC. Results with ViT-B/16 on GeneCIS\n14\nD. Combiner Architecture\n14\nE. Further Implementation Details\n14\nF. Extended Related Work\n15\nG. Qualitative Examples\n15\nG.1. GeneCIS examples . . . . . . . . . .\n15\nG.2. Model Predictions . . . . . . . . . . .\n15\nG.3. Mined Triplets from CC3M . . . . . .\n15\nH. Attributions\n15\nI. Acknowledgements\n15\nWe provide additional details and discussion of components\nof the main paper. We particularly highlight Appendix A\nfor details on GeneCIS construction, and Appendix G for\nqualitative examples.\nA. Further GeneCIS Details\nHere, we provide details on the construction process of\neach GeneCIS task, making reference to the examples from\nFigure 2 for clarity.\nA.1. Task construction\nFocus on an Attribute: VAW [56] contains bounding box\nannotations for various objects, as well as a list of posi-\ntively labelled attributes and negatively labelled attributes\nfor each object. Note that, as discussed in \u00a7 3.1, it is impos-\nsible to exhaustively label an object for all possible \u2018posi-\ntive\u2019 attributes. It is, however, possible to determine a set of\n\u2018negative\u2019 attributes. For instance, one cannot exhaustively\nlabel a \u2018thick\u2019 tree trunk as {\u2018wide\u2019, \u2018fat\u2019, \u2018large\u2019...etc. }, but\none can determine that it is not \u2018thin\u2019.\nFor this task, we construct templates by first sampling\na reference object (e.g \u2018laptop\u2019) and identifying all positive\nattributes of the object (e.g. \u2018white\u2019, \u2018plastic\u2019) and their cor-\nresponding attribute type (e.g. \u2018color\u2019, \u2018material\u2019). Given an\nattribute type, we select a \u2018correct\u2019 target image to have the\nsame object category and attribute within the attribute type\nas the reference (a \u2018laptop\u2019 with the same \u2018color\u2019). Distrac-\ntors are then mined to have the same object category but to\nbe explicitly negatively labelled for the reference attribute\n(e.g. laptops which are negatively labelled for \u2018white\u2019). The\ncondition in this case is the attribute type (\u2018color\u2019).\nChange an Attribute: We first select an anchor attribute\ntype (e.g \u2018color\u2019), before choosing a reference image and a\n\u2018correct\u2019 target image which share the same object category,\nbut have different attributes within the attribute type. In Fig-\nure 2, the reference and \u2018correct\u2019 target are both have the\nsame object category (\u2018train\u2019) but have different \u2018colors\u2019.\nThe attribute of the \u2018correct\u2019 target is given as the condition\n(\u2018olive green\u2019), and a model must understand the category\nof the reference image, as well as the attribute specified in\nthe condition, to solve the problem.\nWe include two forms of \u2018distractors\u2019 in the gallery.\nThe first form includes images with the conditioning at-\ntribute (\u2018olive green\u2019), but with a different object category\n(e.g. \u2018tent\u2019). These images behave as distractors for models\nwhich retrieve based only on the the condition (we include 9\nsuch images). We also include 5 images with the reference\nobject category but without the conditioning attribute (e.g\n\u2018trains\u2019 which are \u2018red\u2019), behaving as distractors for models\nwhich only use the reference image content.\nFocus on an Object: For tasks where the condition con-\ntains an object, we take images of cluttered scenes from the\nmulti-object COCO dataset [40]. We use COCO Panoptic\nSegmentation [36] data which contains dense category la-\nbels for every pixel in the image.\nWe first select a reference image and identify all of its\nconstituent object categories, ensuring at least 10 categories\nare present. Next, we construct a set of all images in the\ndataset with at least 6 objects in common with the reference\n\u2013 but do not contain all reference categories \u2013 and rank them\nbased on the extent of their category intersection (IClose).\nWe also construct a set of images with very few intersecting\nobjects as IF ar. We consider the set of object category IDs\nin an image as a \u2018bag-of-words\u2019 descriptor for the image\nscene, with images in IClose containing a \u2018similar scene\u2019 to\nthe reference, and IF ar representing a \u2018different scene\u2019.\nWe randomly select the \u2018correct\u2019 target image from\nIClose, and the conditioning object is selected as one of this\nimage\u2019s intersecting objects with the reference (e.g. \u2018refrig-\nerator\u2019 in Figure 2). The first form of distractors is mined by\ntaking images in IClose which do not have the conditioning\nobject. These examples confuse models which only use the\nreference image (there are 9 of these). Another type of dis-\ntractor is constructed by taking images from IF ar which do\nhave the conditioning object, confusing models which only\n12\nconsider the text condition (there are 5 of these). In this\nway, only the target image has both a similar scene and also\nthe conditioning object, and is thus conditionally the most\nsimilar image in the gallery. In Figure 2, only the target im-\nage contains a \u2018refrigerator\u2019 and is \u2018outside\u2019. We note that\nsolutions which only match \u2018bag-of-objects\u2019 descriptors fail\nhere (e.g. those which simply detect all objects in the im-\nages): the \u2018correct\u2019 gallery image is randomly selected from\nIClose and does not necessarily contain the highest object\ncategory overlap.\nChange an Object: This task is constructed in a similar\nform to \u2018Focus on an Object\u2019, in that we first select a ref-\nerence image and construct IClose and IF ar. Differently,\nin this case, we first select the \u2018correct\u2019 gallery image from\nIClose as the most similar image which does not have per-\nfect object overlap. Next, the conditioning object is selected\nrandomly from the objects which do appear in the \u2018correct\u2019\ngallery image, but not in the reference (\u2018ceiling\u2019 in the ex-\nample in Figure 2). Distractors are constructed from both\nIF ar and IClose, such that they do, and do not, contain\nthe conditioning object respectively. There are 5 distractors\nfrom IF ar and 9 distractors from IClose.\nA.2. Implementation Details\nAttribute-based Tasks: A taxonomy of attribute types is\nprovided in VAW [56], containing diverse attribute types\nfrom \u2018letter color\u2019 to \u2018texture\u2019. We manually clean and re-\nfine the taxonomy for our purposes, for instance reassign-\ning many attributes which were assigned to the \u2018other\u2019 at-\ntribute type. The resulting taxonomy contains 45 attribute\ntypes with 663 constituent attributes. We build the tasks\nsuch that they are roughly balanced with respect to attribute\ntype, noting that for some attributes it was not possible to\nconstruct a suitable retrieval template. For the \u2018Focus on an\nAttribute\u2019 task, we manually filter attribute types which do\nnot form clear and visually grounded attribute categories.\nSpecifically, we filter: \u2018opinion\u2019; \u2018other after\u2019; \u2018other phys-\nical quality\u2019; \u2018state\u2019; and \u2018type\u2019.\nFinally, when cropping an object with a bounding box,\nwe dilate the box by a factor of 0.7 in height and width,\nbefore padding the resultant image to square with zeroes.\nThis allows some context to identify the object (we often\nfound it difficult to categorize the image without this), and\nalso maintains the aspect ratio of the underlying object. We\nchose the dilation factor which maximized the discrepancy\nbetween the \u2018Image + Text\u2019 and \u2018Image Only\u2019 Recall@1.\nObject-based Tasks: The object-based datasets are derived\nfrom the validation set of COCO Panoptic [36], containing\n57K images with 133 categories. The categories include\n\u2018thing\u2019 classes like \u2018zebra\u2019 and \u2018bench\u2019, as well as \u2018stuff\u2019\ncategories like \u2018sand\u2019 and \u2018roof\u2019. We only consider an ob-\nject category to be present in an image if it occupies more\nthan 1% of the image pixels. After a conditioning object is\nselected, the COCO category name is given as a condition,\nand we strip miscellaneous identifiers such as \u2018-stuff\u2019 and\n\u2018-other\u2019 from the category names.\nA.3. Dataset noise\nOur tasks are built upon manual annotations in the VAW\n[56], COCO [40] and Visual Genome [37] datasets. These\nare widely used datasets in the vision community and, as\nsuch, our tasks should be error free in principle. However,\nwe find some templates provide ill-posed problems through\nnoise and ambiguities in the underlying annotations, as well\nas through bounding box dilation for attribute-based tasks.\nNoise in the datasets is easy to understand, constituting\ninstances where an object category or attribute is obviously\nmislabelled. However, the ambiguities are more subtle, and\nare artefacts of the underlying taxonomies of the datasets.\nFor example, in some COCO images, \u2018ceiling lights\u2019 are\nlabelled as \u2018ceiling\u2019 instead of \u2018light\u2019. This is not necessar-\nily wrong, but reflects the fact that labels are defined in a\none-hot manner and these pixels could refer to either object\ncategory. This is particularly difficult for attribute-based an-\nnotations, as interpretations of attributes are highly subjec-\ntive (e.g. the definitions of \u2018wide\u2019 and \u2018narrow\u2019 are open\nto interpretation). We highlight that such label ambiguity,\nthough underexplored, is present in almost all computer vi-\nsion datasets, including in ImageNet [72].\nWe address this in a number of ways. Firstly, we ran a\nversion of our method with 10 random seeds as well as on\n10 cross-validation splits, finding the standard deviation in\nRecall@1 on each task to be around 0.2%. Although this\ndoes not quantify noise in the dataset, it gives an indication\nof what can be considered \u2018signal\u2019 on the tasks. Secondly,\nwe find that the \u2018Image + Text\u2019 baseline outperforms the\n\u2018Image Only\u2019 and \u2018Text Only\u2019 baselines on most tasks, sug-\ngesting that the tasks measure conditional similarity. We\ndiscuss the exceptional case of \u2018Focus on an Attribute\u2019 in\nAppendix C. Thirdly, we evaluate at Recall@{1, 2, 3}, to\naccount for any templates in which a \u2018distractor\u2019 image (i.e.\n\u2018incorrect\u2019 target) in the gallery actually constitutes a valid\nsolution to the problem. Finally, we are in the process of\nmanually filtering and verifying the templates, presenting\nthe current version as \u2018GeneCIS v0\u2019.\nA.4. Discussion on symmetry\nWe highlight that \u2018similarity\u2019, as discussed in this paper,\ndoes not describe a symmetric mathematical property. In\nGeneCIS, while the reference image is considered \u2018similar\u2019\nto the correct target image given the condition, the reverse\nmay not be true. For instance, in the \u2018Change an Attribute\u2019\nexample in Figure 2, the \u2018green train\u2019 in the reference is\nconditionally similar to the \u2018olive green train\u2019 target image,\ngiven the condition \u2018olive green\u2019. However, this target im-\nage is not similar to the reference image given the same con-\n13\ndition. In general, we find that \u2018Focus\u2019 tasks are symmetric\ngiven the conditions, but \u2018Change\u2019 tasks are not.\nB. Specific Solutions\nWe design specific solutions for each of the proposed\ntasks in GeneCIS. These solutions take into account the spe-\ncific construction mechanisms of each task and represent\nsensible approaches to tackling each task independently.\nWe design all solutions to respect the \u2018zero-shot\u2019 nature of\nthe evaluations and hence they are all based on \u2018open-world\u2019\nmodels; we use CLIP [60] for the attribute-based tasks and\nDetic [81] for the object-based ones. All descriptions here\nrefer to Figure 2 for clarity.\nFocus on an Attribute: Given the attribute type in the con-\ndition (e.g \u2018color\u2019), we first task CLIP with predicting the\nattribute of the reference image. Specifically, we use the\ntaxonomy of attributes provided in VAW to construct a zero-\nshot classifier between attributes within that attribute type\n(e.g. {\u2018red\u2019, \u2018blue\u2019, \u2018white\u2019} within \u2018color\u2019). Given the pre-\ndicted attribute (e.g. \u2018white\u2019), we use its text embedding to\nfind the nearest neighbour from the image embeddings of\nthe gallery set.\nChange an Attribute: We first use CLIP to predict the cat-\negory of the the reference image, by constructing a zero-\nshot classifier from the categories in VAW. We then compute\nthe text embedding of the concatenated predicted object\nname and conditioning attribute (e.g. \u2018olive green train\u2019)\nand find the nearest neighbour in the gallery.\nFocus on an Object and Change an Object: We use the\nsame specific solution for both of these settings. We first\nuse Detic [81] to detect all object categories in the reference\nand gallery images, passing it the 2017 COCO Panoptic cat-\negories to construct the classifier. Next, we filter out any\ngallery images which do not contain the conditioning object\ncategory. Finally, using the detected object categories in a\ngiven image, we construct \u2018bag-of-words\u2019 descriptors of the\nreference image and the remaining gallery images. Specif-\nically, these descriptors are binary vectors for each image\n(with elements for every COCO Panoptic category) and are\nset to \u20181\u2019 if a given category is detected in the image. We\nuse these descriptors to find the most conditionally similar\nimage to the reference from the (filtered) gallery.\nDiscussion: Note that all of the solutions described here are\nspecialized in two senses. Firstly, they are designed with the\nspecific task construction method in mind, and hence are not\napplicable to all tasks as we desire for a general conditional\nsimilarity model. Secondly, all specific solutions leverage\nthe underlying taxonomy of the datasets (VAW [56] and\nCOCO Panoptic [36]) used in the benchmark.\nC. Results with ViT-B/16 on GeneCIS\nIn Table 2, we report results on GeneCIS with a\nResNet50\u00d74 backbone for fair comparison with [3]. How-\never, in Figure 6, we demonstrate that our model per-\nforms best when intialized with a ViT-B/16 CLIP back-\nbone [18,60]. We include results for this model in Table 6,\nalong with the CLIP-only baselines described in \u00a7 6.1.\nWe first note that, with the ViT-B/16 backbone, our\nmodel outperforms all CLIP-only baselines, on all tasks and\nat all recalls. Particularly, with the ResNet50\u00d74 backbone\nin Table 2, the \u2018Image Only\u2019 baseline outperformed ours on\n\u2018Focus Attribute\u2019 at higher recalls, which is no longer the\ncase here. We further note that the \u2018Focus Attribute\u2019 task\ngives anomalous results when comparing the \u2018Image Only\u2019\nbaseline with \u2018Image + Text\u2019. Specifically, this is the only\ntask for which the \u2018Image + Text\u2019 model does not outper-\nform the other baselines. On this task, the information given\nby the condition is an attribute type, rather than the attribute\nitself (e.g. \u2018color\u2019 rather than \u2018white\u2019 in Figure 2). As such,\nthe condition information likely only confuses existing vi-\nsion models, and reduces the performance over the \u2018Image\nOnly\u2019 baseline.\nD. Combiner Architecture\nThe Combiner architecture takes in reference image and\ncondition text features, composing them into a single vector\nas: g(xR, e) where g, xR, e \u2208 RD.\nThe architecture consists of four functions (hi, built from\nMLPs) which process features in parallel as:\ng(xR, e) = \u03bbh1(xR) + (1 \u2212 \u03bb)h2(e) + h3(xR, e)\n(2)\nwhere \u03bb = h4(xR, e) is a dynamic weighting of the fea-\ntures. We refer to [3] for full details.\nE. Further Implementation Details\nOur method: All models trained on CC3M were trained for\n28K gradient steps. We train our strongest models with an\ninitial learning rate of 1\u00d710\u22126 and a cosine decay schedule,\ntraining both the CLIP backbone and the Combiner head\nwith the same learning rate. We evaluate our model at each\nepoch, selecting the checkpoint with the best Recall@1 on\nthe CIRR validation set [44]. Note that this single model\nis then taken and evaluated zero-shot on all benchmarks re-\nported in this paper. Our optimizer is Adam [35] and we\nimplement our models using PyTorch [52]. To fine-tune\nboth the backbones and Combiner head with a batch size\nof 256, we train our models on 16 A100 GPUs, with a train-\ning time of approximately 12 hours. Finally, all features are\nnormalized before the contrastive loss is computed.\nSpecific Solutions: For the attribute-based specific solu-\ntions, we use the same ResNet50\u00d74 backbone as for our\n14\nTable 6. Evaluation on GeneCIS with a ViT-B/16 backbone where we evaluate our method and CLIP-only baselines. We find our\nmethod performs best with a ViT-B/16 backbone and that, with this architecture, our model outperforms the baselines at all recalls on all\nGeneCIS tasks.\nFocus Attribute\nChange Attribute\nFocus Object\nChange Object\nR@1\nR@ 2\nR@3\nR@1\nR@ 2\nR@3\nR@1\nR@ 2\nR@3\nR@1\nR@ 2\nR@3\nAverage R@1\nImage Only\n18.1\n30.1\n40.6\n11.5\n21.9\n30.9\n9.4\n17.0\n25.4\n7.6\n17.1\n25.5\n11.7\nText Only\n10.3\n20.9\n30.4\n10.2\n18.2\n26.1\n7.4\n14.0\n23.0\n8.1\n16.4\n24.7\n9.0\nImage + Text\n17.1\n29.5\n40.5\n13.1\n22.2\n31.9\n11.5\n20.1\n29.2\n9.8\n20.0\n28.9\n12.9\nCombiner (CC3M, Ours)\n19.7\n31.7\n42.1\n16.2\n27.3\n37.5\n16.6\n27.7\n37.2\n18.0\n32.2\n41.6\n17.6\nmethod in Table 2. Furthermore, when embedding an ob-\nject name, we ensemble over the 80 standard CLIP prompts\nfrom [60]. For the object-based baselines, we use a Detic\nmodel with the strongest Swin-B backbone [43], trained for\nopen-vocabulary detection on the \u2018base\u2019 classes of the LVIS\ndataset [22]. For the first detection stage, we select a confi-\ndence threshold which optimizes downstream Recall@1 on\nthe \u2018Focus Object\u2019 evaluation (a confidence of 0.2).\nF. Extended Related Work\nWe briefly describe work from the text-based image-\nediting domain and discuss how it relates to our work. Gen-\nerative image-editing [6, 49, 53] is a popular task in which,\ngiven a reference image and some condition, a sensible edit\nof the image is generated. Recently, with the advent of\nwidely available large-scale generative models [62], there\nhas been substantial work which considers the prompt as\na text-condition, and uses CLIP text embeddings to guide\nthe generation process [2, 5, 14, 20, 33, 38]. As such, the\ninputs and outputs of image-editing models are similar to\nthose considered in this work. However, we highlight our\nwork focuses on the representation and retrieval of existing\nimages, rather than the synthesis of new ones.\nWe also note recent work which considers a task similar\nto \u2018Change an Object\u2019 in GeneCIS, in the context of compo-\nsitional learning [48]. Similar to work detailed in \u00a7 2, [48]\ntrains on a finite set of categories and considers only one\nof the four areas of the conditional similarity space which\nwe propose here. Finally, we highlight [78], which consid-\ners contrastively training a single backbone with multiple\nheads, each of which is invariant to different data augmen-\ntations. This work shares similar motivations to ours \u2013 that\nthere are many notions of image similarity \u2013 though they\ntrain with a pre-determined and fixed set of data augmenta-\ntions (and hence fixed concepts of similarity).\nG. Qualitative Examples\nG.1. GeneCIS examples\nWe provide example templates from the GeneCIS bench-\nmark tasks in Figures 7 to 10. Differently to Figure 2, we\nshow the entire curated retrieval templates of 10-15 target\nimages, as well as the reference image (leftmost, yellow)\nand condition text (blue oval).\nAs discussed in \u00a7 4, all\ngallery images are implicitly similar to the reference image\nor condition. The \u2018positive\u2019 target image is the most similar\ngiven the condition.\nG.2. Model Predictions\nWe show qualitative results of our model and CLIP-only\nbaselines in Figure 11. We show instances where our model\nfails in Figure 12 along with the \u2018correct\u2019 target image and\nthe prediction of the Image-Only CLIP baseline.\nG.3. Mined Triplets from CC3M\nWe show examples of training triplets which we auto-\nmatically mine from CC3M [66] in Figure 13.\nH. Attributions\nFigure\n1:\nThomas\nHawk,\nCC\nBY-NC\n2.0\n(https://creativecommons.org/licenses/by-nc/2.0/),\nvia\nFlickr.\nFigure 4, image of horse on canvas: Simon Kozhin,\nCC BY-SA 3.0 (https://creativecommons.org/licenses/by-\nsa/3.0), via Wikimedia Commons.\nI. Acknowledgements\nWe would like to thank Weidi Xie, Liliane Momeni,\nMannat Singh and Kalyan Vasudev Alwala for valuable dis-\ncussions on this work. Sagar is supported by a Facebook AI\nResearch Scholarship.\n15\n\u201cheight\u201d\nSame Object \nSame Attribute\nSame Object \nWrong Attribute\nSame Object \nWrong Attribute\nSame Object \nWrong Attribute\nSame Object \nWrong Attribute\nSame Object \nWrong Attribute\nSame Object \nWrong Attribute\nSame Object \nWrong Attribute\nSame Object \nWrong Attribute\nSame Object \nWrong Attribute\n\u201cwearing color\u201d\nSame Object \nSame Attribute\nSame Object \nWrong Attribute\nSame Object \nWrong Attribute\nSame Object \nWrong Attribute\nSame Object \nWrong Attribute\nSame Object \nWrong Attribute\nSame Object \nWrong Attribute\nSame Object \nWrong Attribute\nSame Object \nWrong Attribute\nSame Object \nWrong Attribute\n\u201cweight\u201d\nSame Object \nSame Attribute\nSame Object \nWrong Attribute\nSame Object \nWrong Attribute\nSame Object \nWrong Attribute\nSame Object \nWrong Attribute\nSame Object \nWrong Attribute\nSame Object \nWrong Attribute\nSame Object \nWrong Attribute\nSame Object \nWrong Attribute\nSame Object \nWrong Attribute\nshort grass\ngirl wearing red\nheavy wave\nFigure 7. Focus on an Attribute example templates.\n16\nWrong Object \nCondition Attribute\nWrong Object \nCondition Attribute\nWrong Object \nCondition Attribute\nWrong Object \nCondition Attribute\nWrong Object \nCondition Attribute\nWrong Object \nCondition Attribute\nSame Object \nCondition Attribute\nWrong Object \nCondition Attribute\nWrong Object \nCondition Attribute\nWrong Object \nCondition Attribute\nSame Object \nWrong Attribute\nSame Object \nWrong Attribute\nSame Object \nWrong Attribute\nSame Object \nWrong Attribute\nSame Object \nWrong Attribute\n\u201cshining\u201d\nSame Object \nWrong Attribute\nSame Object \nWrong Attribute\nSame Object \nWrong Attribute\nSame Object \nWrong Attribute\nSame Object \nWrong Attribute\nWrong Object \nCondition Attribute\nWrong Object \nCondition Attribute\nWrong Object \nCondition Attribute\nWrong Object \nCondition Attribute\nWrong Object \nCondition Attribute\nWrong Object \nCondition Attribute\nSame Object \nCondition Attribute\nWrong Object \nCondition Attribute\nWrong Object \nCondition Attribute\nWrong Object \nCondition Attribute\n\u201clong\u201d\nmatte table\nshort carrots\nFigure 8. Change an Attribute example templates.\n17\n\u201ccurtain\u201d\nSame Scene \nNo Condition Object\nSame Scene \nNo Condition Object\nSame Scene \nNo Condition Object\nSame Scene \nNo Condition Object\nSame Scene \nNo Condition Object\nSame Scene \nNo Condition Object\nSame Scene \nCondition Object\nSame Scene \nNo Condition Object\nSame Scene \nNo Condition Object\nSame Scene \nNo Condition Object\nWrong Scene \nCondition Object\nWrong Scene \nCondition Object\nWrong Scene \nCondition Object\nWrong Scene \nCondition Object\nWrong Scene \nCondition Object\n\u201cwood \ufb02oor\u201d\nSame Scene \nNo Condition Object\nSame Scene \nNo Condition Object\nSame Scene \nNo Condition Object\nSame Scene \nNo Condition Object\nSame Scene \nNo Condition Object\nSame Scene \nNo Condition Object\nSame Scene \nCondition Object\nSame Scene \nNo Condition Object\nSame Scene \nNo Condition Object\nSame Scene \nNo Condition Object\nWrong Scene \nCondition Object\nWrong Scene \nCondition Object\nWrong Scene \nCondition Object\nWrong Scene \nCondition Object\nWrong Scene \nCondition Object\nliving room \nwith curtains\nkitchen with\nwooden \ufb02oor\nFigure 9. Focus on an Object example templates.\n18\nSame Scene \nNo Condition Object\nSame Scene \nNo Condition Object\nSame Scene \nNo Condition Object\nSame Scene \nNo Condition Object\nSame Scene \nNo Condition Object\nSame Scene \nNo Condition Object\nSame Scene \nCondition Object\nSame Scene \nNo Condition Object\nSame Scene \nNo Condition Object\nSame Scene \nNo Condition Object\nWrong Scene \nCondition Object\nWrong Scene \nCondition Object\nWrong Scene \nCondition Object\nWrong Scene \nCondition Object\nWrong Scene \nCondition Object\n\u201cmountain\u201d\nSame Scene \nNo Condition Object\nSame Scene \nNo Condition Object\nSame Scene \nNo Condition Object\nSame Scene \nNo Condition Object\nSame Scene \nNo Condition Object\nSame Scene \nNo Condition Object\nSame Scene \nCondition Object\nSame Scene \nNo Condition Object\nSame Scene \nNo Condition Object\nSame Scene \nNo Condition Object\nWrong Scene \nCondition Object\nWrong Scene \nCondition Object\nWrong Scene \nCondition Object\nWrong Scene \nCondition Object\nWrong Scene \nCondition Object\n\u201cceiling\u201d\nvillage\nkitchen\nFigure 10. Change an Object example templates.\n19\n\u201cshape\u201d\n\u201chand movement\u201d\n\u201clight yellow\u201d\n\u201csur\ufb01ng\u201d\n\u201cteddy bear\u201d\n\u201cclock\u201d\n\u201cbridge\u201d\n\u201clight\u201d\nReference\nOur\n Prediction\nImage Only\n Prediction\nText Only\n Prediction\nFocus on an Attribute\nChange an Attribute\nFocus on an Object\nChange an Object\nReference\nOur\n Prediction\nImage Only\n Prediction\nText Only\n Prediction\nFigure 11. Qualitative results from our method showing instances where our model predicts correctly.\n20\n\u201ccolor\u201d\n\u201cshape\u201d\n\u201cknee high\u201d\n\u201cwriting\u201d\n\u201cdining table\u201d\n\u201cmirror\u201d\n\u201cwine glass\u201d\n\u201cpizza\u201d\nReference\nCorrect\nTarget\nOur\n Prediction\nImage Only\n Prediction\nFocus on an Attribute\nChange an Attribute\nFocus on an Object\nChange an Object\nReference\nCorrect\nTarget\nOur\n Prediction\nImage Only\n Prediction\nFigure 12. Qualitative results from our method showing instances where our model fails.\n21\n\u201co\ufb00 rocks\u201d\n\u201cwith dress\u201d\n\u201cin bucket\u201d\n\u201cpumping balloon\u201d\n\u201cexamines toy\u201d\n\u201cof \ufb02oors\u201d\n\u201cthe red beams are structural  \nbraces for crown of upper floors\u201d\nFigure 13. Examples of mined triplets from CC3M [66] as described in \u00a7 5.2. In each triplet, the text condition (blue oval) links the\nreference image (left) to the target image (right). We show an instance where a noisy triplet is produced in the bottom right. The caption\n(show in a speech bubble) incurs a misleading parsed relationship of \u2018Crown\u2019 \u2192 \u2018of\u2019 \u2192 \u2018floors\u2019.\n22\n"
  },
  {
    "title": "Galactic: Scaling End-to-End Reinforcement Learning for Rearrangement at 100k Steps-Per-Second",
    "link": "https://arxiv.org/pdf/2306.07552.pdf",
    "upvote": "3",
    "text": "Galactic: Scaling End-to-End Reinforcement Learning for Rearrangement\nat 100k Steps-Per-Second\nVincent-Pierre Berges*\nMeta AI (FAIR)\nAndrew Szot*\nGeorgia Tech\nDevendra Singh Chaplot\nMeta AI (FAIR)\nAaron Gokaslan\nCornell University\nRoozbeh Mottaghi\nMeta AI (FAIR)\nDhruv Batra\nMeta AI (FAIR), Georgia Tech\nEric Undersander\nMeta AI (FAIR)\nAbstract\nWe present Galactic,\na large-scale simulation and\nreinforcement-learning (RL) framework for robotic mobile\nmanipulation in indoor environments. Specifically, a Fetch\nrobot (equipped with a mobile base, 7DoF arm, RGBD camera,\negomotion, and onboard sensing) is spawned in a home\nenvironment and asked to rearrange objects \u2013 by navigating\nto an object, picking it up, navigating to a target location, and\nthen placing the object at the target location.\nGalactic is fast. In terms of simulation speed (rendering\n+ physics), Galactic achieves over 421,000 steps-per-second\n(SPS) on an 8-GPU node, which is 54x faster than Habitat\n2.0 [55] (7699 SPS). More importantly, Galactic was designed\nto optimize the entire rendering+physics+RL interplay since\nany bottleneck in the interplay slows down training. In terms\nof simulation+RL speed (rendering + physics + inference +\nlearning), Galactic achieves over 108,000 SPS, which 88x\nfaster than Habitat 2.0 (1243 SPS).\nThese massive speed-ups not only drastically cut the\nwall-clock training time of existing experiments, but also unlock\nan unprecedented scale of new experiments. First, Galactic can\ntrain a mobile pick skill to >80% accuracy in under 16 minutes,\na 100x speedup compared to the over 24 hours it takes to\ntrain the same skill in Habitat 2.0. Second, we use Galactic to\nperform the largest-scale experiment to date for rearrangement\nusing 5B steps of experience in 46 hours, which is equivalent\nto 20 years of robot experience. This scaling results in a single\nneural network composed of task-agnostic components achiev-\ning 85% success in GeometricGoal rearrangement, compared\nto 0% success reported in Habitat 2.0 for the same approach.\nThe code is available at github.com/facebookresearch/galactic .\n1. Introduction\nThe scaling hypothesis posits that as general-purpose neural\narchitectures are scaled to larger model sizes and training\nexperience ever increasingly sophisticated intelligent behavior\n*Equal contribution\nemerges. These so-called \u2018scaling laws\u2019 appear to be driving\nmany recent advances in AI, leading to massive improvements\nin computer vision [12,42,44] and natural language process-\ning [3, 40]. But what about embodied AI? We contend that\nembodied AI experiments need to be scaled by several orders\nof magnitude to become comparable to the experiment scales\nof CV and NLP, and likely even further beyond given the multi-\nmodal interactive long-horizon nature of embodied AI problems.\nConsider one of the largest-scale experiments in vision-and-\nlanguage: the CLIP [39] model was trained on a dataset of\n400 million images (and captions) for 32 epochs, giving a total\nof approximately 12 Billion frames seen during training. In\ncontrast, most navigation experiments in embodied AI involve\nonly 100-500M frames of experience [5,30,68]. The value of\nlarge scale training in embodied AI was demonstrated by Wi-\njmans et al. [60] by achieving near-perfect performance on the\nPointNav task through scaling to 2.5 billion steps of experience.\nSince then, there have been several other examples of scaling\nexperiments to 1B steps in navigation tasks [41,65]. Curiously,\nas problems become more challenging, going from navigation\nto mobile manipulation object rearrangement, the scale of\nexperiments have become smaller. Examples of training scales\nin rearrangement policies include [19, 55, 59] training skills\nin Habitat 2.0 for 100-200M steps and [58] in Visual Room\nRearrangement for 75M steps. In this work, we demonstrate for\nthe first time scaling training to 5 billion frames of experience\nfor rearrangement in visually challenging environments.\nWhy is large-scale learning in embodied AI hard? Unlike\nin CV or NLP, data in embodied AI is collected through an\nagent acting in environments. This data collection process\ninvolves policy inference to compute actions, physics to update\nthe world state, rendering to compute agent observations, and\nreinforcement learning (RL) to learn from the collected data.\nThese separate systems for rendering, physics, and inference\nare largely absent in CV and NLP.\nWe present Galactic, a large-scale simulation+RL framework\nfor robotic mobile manipulation in indoor environments.\nSpecifically, we study and simulate the task of GeometricGoal\narXiv:2306.07552v1  [cs.LG]  13 Jun 2023\nArcade RL Sims\nDevice\nRes Sensors Train SPS\nSim SPS\nPhotoreal Physics\nVizDoom [26,37]\n1x RTX 3090\n28\u00d772\nRGB\n18,900\n38,100\n\u2716\n\u2713\nPhysics-only Sims\nDevice\nRes Sensors Train SPS\nSim SPS\nPhotoreal Physics\nIsaac Gym (Shadow Hand) [29]\n1x A100\nN/A\nN/A\n150,000\n\u2013\n\u2716\n\u2714\nBrax (Grasp) [16]\n4x2 TPU v3\nN/A\nN/A\n1,000,000\n10,000,000\n\u2716\n\u2714\nADPL Humanoid [64]\n1x TITAN X\nN/A\nN/A\n40,960\n144,035\n\u2716\n\u2714\nEAI Sims\nDevice\nRes. Sensors Train SPS\nSim SPS\nPhotoreal Physics\niGibson [28,51]\n1x GPU 128\u00d7128\nRGB\n\u2013\n100\n\u2714\n\u2714\nAI2-THOR [11]\n8x RTX 2080TI\n224x224\nRGB\n\u2248300\n2,860\n\u2714\n\u2714\nMegaverse [37]\n1x RTX 3090\n128\u00d772\nRGB\n42,700\n327,000\n\u2716\n\u2713\n8x RTX 2080TI\n128\u00d772\nRGB\n134,000\n1,148,000\nLBS [48]\n1x RTX 3090\n64\u00d764\nRGB\n13,300\n33,700\n\u2714\n\u2716\n1x Tesla V100\n64\u00d764\nRGB\n9,000\n\u2013\n8x Tesla V100\n64\u00d764\nRGB\n37,800\n\u2013\nHabitat 2.0 [55,59]\n1x RTX 2080 Ti 128\u00d7128\nRGBD\n367\n1,660\n\u2714\n\u2714\n8x RTX 2080 Ti 128\u00d7128\nRGBD\n1,243\n7,699\n1x Tesla V100 128\u00d7128\nRGBD\n128\n2,790\n8x Tesla V100 128\u00d7128\nRGBD\n945\n17,465\nGalactic (Ours)\n1x Tesla V100 128\u00d7128\nRGBD\n14,807\n54,966\n\u2714\n\u2714\n8x Tesla V100 128\u00d7128\nRGBD\n108,806\n421,194\nTable 1. High-level throughput comparison of different simulators. Steps-per-second (SPS) numbers are taken from source publications, and\nwe don\u2019t control for all performance-critical variables including scene complexity and policy architecture. Comparisons should focus on orders\nof magnitude. We show Sim SPS (physics and/or rendering) and training SPS (physics and/or rendering, inference and learning) for various\nphysics-only and Embodied AI simulators. We also describe VizDoom, an arcade simulator which has served as a classic benchmarks for RL\nalgorithms due to its speed. The \u2713 for Megaverse and VizDoom represent physics for abstract, non-realistic environments. Among EAI simulators\nthat support realistic environments (photorealism and realistic physics), Galactic is 80\u00d7 faster than the existing fastest simulator, Habitat 2.0\n(108,806 vs 1243 training SPS for 8 GPUs). Galactic\u2019s training speed is comparable to LBS, Megaverse, and VizDoom, even though LBS doesn\u2019t\nsimulate physics and neither Megaverse nor VizDoom support realistic environments. We also compare to GPU-based physics simulators: while\nthese are generally faster than Galactic, they entirely omit rendering, which significantly reduces their compute requirements. For Galactic, we\nobserve near-linear scaling from 1 to 8 GPUs, with a 7.3x speedup.\nRearrangement [2], where a Fetch robot [43] equipped\nwith a mobile base, 7DoF arm, RGBD camera, egomotion,\nand proprioceptive sensing must rearrange objects in the\nReplicaCAD [55] environments by navigating to an object,\npicking up the object, navigating to a target location, and then\nplacing the object at the target location.\nGalactic is fast. In terms of simulation speed (rendering\n+ physics), Galactic achieves over 421,000 steps-per-second\n(SPS) on an 8-GPU node, which is 54x faster than Habitat\n2.0 [55] (7699 SPS). More importantly, Galactic was designed\nto optimize the entire rendering+physics+RL interplay since\nany bottleneck in the interplay slows down training. In terms\nof simulation+RL speed (rendering + physics + inference +\nlearning), Galactic achieves over 108,000 SPS, which 88x faster\nthan Habitat 2.0 (1243 SPS).\nOur key technical innovations are:\n(1) integration of\nCPU-based batch physics with GPU-heavy batch rendering\nand inference, and (2) a new, approximate kinematic simulation\ntargeted at EAI rearrangement tasks. Compared to a \u201cone\nsimulator, one environment, one process\u201d paradigm, batching\nyields massive speedups due to memory savings, lower\ncommunication overhead, and greater parallelism. Meanwhile,\nwe leverage our physics approximations and the reduced\ncomplexity of kinematic simulation to reduce our CPU compute\nand yield further speedups.\nThese massive speed-ups not only drastically cut the wall-\nclock training time of existing experiments, but also unlock an\nunprecedented scale of new experiments. First, Galactic can\ntrain a mobile pick skill to >80% accuracy in under 16 minutes,\na 100x speedup compared to the over 24 hours it takes to train\nthe same skill in Habitat 2.0. Second, we use Galactic to perform\nthe largest-scale experiment to date for rearrangement using 5B\nsteps of experience in 46 hours, which is equivalent to 20 years\nof robot experience (assuming 8 actions per second). This scal-\ning results in a single neural network composed of task-agnostic\ncomponents (CNNs and LSTMs) achieving 85% success in\nGeometricGoal rearrangement. This is impressive performance\nbecause (1) the task is extremely long horizon (involving naviga-\ntion, picking, and placing), (2) the architecture is monolithic and\nhas no mapping modules, task-planning, or motion-planning.\nFor context, Habitat 2.0 reported 0% success with a monolithic\nRL baseline in GeometricGoal rearrangement. We find the\nlearned policies are able to efficiently navigate, avoid distractor\nobjects, and synchronize base and arm movement for greater\nefficiency. Finally, we also show that models trained in Galactic\nare somewhat robust to zero-shot sim2sim generalization, i.e.\ncan achieve 26% success when deployed in Habitat 2.0 despite\ndifferences in rendering, physics, and underlying controller.\n2. Related Work\nScaling Approaches in Embodied AI. There is a large\nbody of work in speeding up training embodied agents in\nsimulation. There are three general approaches for increasing\nefficiency of the overall system: distributing the policy training\nand inference, increasing the sample efficiency of the learning\nalgorithms, and batch simulation.\nDistribution and parallelization: The works in this do-\nmain [15,24,35,54,60,67] achieve efficiency by distributing\ntraining across multiple GPUs or nodes and parallelization of\nthe computation. Galactic\u2019s systems contributions are targeted\nat rollout computation (inference and environment-stepping) on\na single CPU process and single GPU, so it is complementary\nto many of these approaches [59,60,67].\nTraining algorithms: Various techniques have been devel-\noped for the efficiency of training algorithms in interactive\nsettings.\nUsing auxiliary losses [25, 31, 36, 50, 66, 68],\noffline training [32, 33, 46, 49, 69, 70], and model-based\ntraining [6,9,20\u201322,34] are some examples that lead to sample\nefficiency (and typically wall clock time) of the training\nalgorithms.\nSimilarly, since our experiments use a single\nDD-PPO [60] policy trained from scratch, Galactic can be\ncombined with any of these techniques for further efficiency.\nBatch simulation: Batch simulation refers to vectorized\nphysics or rendering to compute updates across multiple\nenvironments with one operation in a batched fashion. This\nyields large speedups compared to the \u201cone process, one\nsimulator, one environment\u201d paradigm. Our approach is closest\nto LBS [48] and Megaverse [37], but ours is the first work to\ncombine batch physics and batch rendering to simulate realistic\nenvironments and vision sensors. [48] does not support physics,\nand instead only supports photorealistic, non-interactive scenes\nand cylinder agents. Megaverse [37] does not support physics\nsimulation with articulated agents and realistic movable objects,\nand instead only supports \u201cblock worlds\u201d with movable blocks\nand cylinder agents.\nEmbodied AI Simulators. There are various Embodied AI\nsimulation platforms [10, 17, 27, 38, 45, 51, 55, 62, 63] for in-\ndoor environments that support tasks such as navigation [45,61],\nobject manipulation [14,53], instruction following [1,52], inter-\nactive question answering [8,18], and object rearrangement [55,\n58]. The efficiency of the simulators is important for these tasks\nsince they typically involve long task horizons, and the state-of-\nthe-art training algorithms require millions of iterations to con-\nverge. Galactic supports similar tasks in indoor environments.\nKinematic Simulation. Some recent work uses \u201dkinematic\nsimulation\u201d, in which the robot and objects are moved directly\nwithout simulating rigid body dynamics. [55] uses a \u201csticky\nmitten\u201d abstraction for grasping rather than simulating contact\nphysics. It also uses kinematic movement for the base rather\nthan simulating base momentum and wheel forces. [57]\nexplores training a navigation policy for a quadruped robot in\nsimulation. They show that training with kinematic movement\nfor the base transfers better to real compared to training with\nfull quadruped dynamics. Both of these works use the Habitat\n2.0 simulator with underlying CPU-based Bullet physics [7].\nFor Galactic, we introduce a new kinematic simulator optimized\nspecifically for EAI rearrangement tasks.\n3. Galactic System\nFrom an ML systems perspective, RL training can be\nbroken down into rollout computation (collecting experience\nby interacting with the EAI simulator) and learning (updating\nthe policy). Our systems contributions focus on speeding up the\nexperience collection. In particular, we optimize batch rollout\ncomputation for a single GPU and associated CPU process by\n(1) integrating CPU-based batch physics with GPU-based batch\nrendering and DNN inference (Section 3.1), and (2) introducing\na new approximate kinematic simulation optimized for EAI\nrearrangement tasks (Section 3.2). For our RL experiments,\nwe combine fast rollouts with DD-PPO [60] (Section 4.2), but\nother approaches to distributed RL are also compatible [59,67].\nWe discuss throughput and scaling in Section 5.1. Galactic also\nsupports adding assets from different sources (see Appendix H\nfor details).\n3.1. Batching\nConsider non-batched EAI simulators like [27,55]. They\nuse a \u201cone Python process, one simulator, one environment\u201d\nparadigm.\nThe simulator manages a single environment\n(including physics and rendering) and produces a single set\nof observations.\nAdditionally, some task-specific logic is\nimplemented directly in Python code, e.g. extracting a robot\u2019s\npose from the sim and computing reward and other metrics.\nPython\u2019s cooperative threading model and global interpreter\nlock make parallelism difficult. To scale this RL training to mul-\ntiple environments, these approaches typically spawn multiple\nPython \u201cenv\u201d processes, each hosting its own simulator instance.\nIn-memory assets here include CPU data, such as physics\ncollision geometry, and GPU data, such as meshes and textures,\nand unfortunately they must be duplicated across instances,\nnot shared. Coordinating rollouts and gathering results requires\nsignificant interprocess communication, e.g. sending observa-\ntions between the \u201cenv\u201d and \u201cmain\u201d processes. Observations\nare batched on the main process and fed to GPU-based batch\nDNN inference. The number of parallel environments is limited\nby total system memory, interprocess communication overhead,\nand other factors and is typically between 16 and 28 [27,55].\nPrior work has shown the benefit of batch simulation\nover this non-batched paradigm, both for CPU-based physics\nsimulation [37], GPU-based physics simulation [16, 29, 64],\nand GPU-based rendering [37,48]. For Galactic, we batch both\nCPU-based physics and GPU-based rendering. For physics,\nthe main Python process hosts a single C++ simulator instance.\nThis instance steps physics for a batch of environments, sharing\nin-memory CPU assets across environments. Unlike [48] which\nmoves nearly all task logic to C++, we retain the flexibility of\nPython for reward and other task-specific computation. This is\nimplemented as efficient Numpy tensor operations, in contrast\nto the non-tensor Python code in the non-batched paradigm.\nWe use the Python buffer protocol to ensure we have zero-copy\nconversion between C++ and Python.\nFor rendering, we use Bps3D [48]. A single Bps3D renderer\ninstance renders all environments, sharing GPU memory assets\nacross environments. Scene graph updates are communicated\nfrom our physics simulation to the Bps3D renderer efficiently\nin C++.\nThe renderer outputs a single batch observation\nper camera sensor, essentially a stack of images from each\nenvironment in GPU memory.\nThis is directly consumed\nby PyTorch GPU-based batch DNN inference, without ever\ncopying pixels to the CPU. We visualize example observations\nfrom Galactic in Appendix D.\nFigure 1 shows our integration of batch physics (orange),\nbatch rendering (purple), and PyTorch DNN inference (blue)\ninto the rollout computation loop. \u201cStep post-processing\u201d refers\nto reward calculation and other task-specific logic. CPU-based\nphysics is computed in parallel with GPU-heavy rendering\nand inference. Because of this interleaving, we must accept a\none-step-delay: as in [55], our policy\u2019s actions are computed\nnot from the current step\u2019s observations, ot, but rather those\nfrom the previous step, ot\u22121. Our approximate kinematic sim\nis fast enough such that we are \u201cGPU-bound\u201d: the bottleneck\nhere is primarily the GPU, with large gaps on both the main and\nphysics threads corresponding to idle CPU. This GPU-bound\nproperty is desirable and means Galactic will benefit greatly\nas faster GPUs become available.\nCompared to the non-batched paradigm, Galactic rollouts\nhave no interprocess communication overhead because all CPU\ncompute happens in a single process. Because of memory\nsavings from batching, our number of environments is larger\nthan the non-batched paradigm (128 versus 16 to 32).\nFigure 1.\nRollout timeline for a single batch step, showing our\nintegration of batch physics (orange), batch rendering (purple), and\nPyTorch DNN inference (blue). CPU-based physics is computed\nin parallel with GPU-heavy rendering and inference. Physics is fast\nenough for rollout computation to be primarily GPU-bound.\n3.2. Approximate Kinematic Simulation\nBatra et al. [2] reviews physics realism for EAI simulators,\nincluding agent embodiment and how it interacts with the\nscene. They describe a spectrum: at the most abstract end are\nsimple cylinder embodiments, \u201cmagic pointer\u201d grasping, and\n\u201cvirtual backpacks\u201d [27,45]. At the other end is full rigid-body\ndynamics simulation: [16,29,64].\nGalactic\u2019s approximate kinematic simulation lies somewhere\nin the middle. It is kinematic: the robot and objects are moved\ndirectly without simulating rigid body dynamics.\nIt uses\napproximations for detecting and resolving collisions and for\nsimulating object-dropping. Regarding these approximation\nchoices, an important design goal is feasible sim-to-real for\npolicies trained in Galactic:\nour policies should produce\ndetailed, physically-plausible trajectories for an articulated\nrobot and movable objects, requiring only the addition of\nsimple, low-level controllers on real hardware. As a proxy for\nsim-to-real in this work, we explore sim-to-sim transfer to the\nHabitat 2.0 dynamic simulator in Section 5.2.\nRecent work has employed similar kinematic simula-\ntion [55,57], but these are built on top of Habitat 2.0 and the\nexisting CPU-based Bullet physics engine [7]. Meanwhile, we\nbuild a new simulator from scratch, leveraging both our physics\napproximations and the reduced complexity of kinematic\nsimulation to reduce our compute.\nOur action space for the articulated robot includes offsets\nfor base forward/back, base rotation, and the other degrees of\nfreedom, e.g. arm joint rotation. Inside the simulator, these off-\nsets are applied directly to the robot state, then we use forward\nkinematics to compute the position of the robot\u2019s articulated\nlinks. This is kinematic movement: no velocities, forces, or\nmomentum are simulated. This new candidate pose for the robot\nis tested for overlap (penetration) with the environment. A \u201ccol-\nlision\u201d is resolved by either disallowing all robot movement for\nthat step or by sliding the robot base (\u201callow sliding\u201d is a training\nhyperparameter). Our sliding implementation is approximate\nand uses a heuristic search described in Appendix I.1.\nCollision geometry is generally used in a physics simulator\nto perform contact and overlap tests.\nCommon primitives\ninclude convex hulls and triangle meshes, which can accu-\nrately represent real-world shapes using sufficiently high\nvertex/triangle counts. For our rearrangement task, we assume\nsub-centimeter tolerances are not important, so we use alternate\nprimitives which are less accurate but faster to query.\nIn\nparticular, we approximate the robot (and the current grasped\nobject, if any) with a set of spheres (green and blue in Figure 3).\nWe approximate movable objects with oriented bounding boxes\n(orange). Finally, we approximate the static (non-movable)\nparts of the environment with a voxel-like structure called a\ncolumn grid (gray). It has limited precision in the lateral (XZ)\ndirection (3 cm) but full floating-point precision in the vertical\n(Y) direction, so surface heights are accurately represented.\nSpheres are authored for the robot manually while bounding\nFigure 2. Overview of the GeometricGoal rearrangement task. In this task, a Fetch robot must move an object from a start position, specified\nas a 3D coordinate, to a goal position, also specified as a 3D coordinate, all from egocentric sensing. Figure adapted from [56].\nboxes and column grids are generated automatically from\nhigh-fidelity source meshes in a preprocessing step.\nSphere-versus-environment overlap tests are optimized in a\ncouple ways: (1) we precompute a distinct column grid for each\nunique sphere radius, so sphere-versus-column-grid tests are\nsimple lookups. (2) Resting movable objects are inserted into\na regular-grid acceleration structure. Testing against the entire\nset of movable objects is fast because we need only iterate over\nthe precomputed set of nearby (50 cm) objects.\nOur simulated grasping approximates a real-world suction\ngripper kinematically without simulating contact or suction\nforces. Firstly, our action space includes a discrete grasp/release\naction. In the simulator, if the grasp action is active, we query\na small (3 cm) sphere at the tip of the end effector and check\nfor overlap (contact) with movable objects in the scene. If\nan overlapping object is found, the object is fixed to the end\neffector. The object moves with the end effector until the\nrelease action is performed.\nOn release, our \u201csnap-to-surface\u201d operation approximates\nthe entire drop sequence for a released object: falling, landing,\nand settling. We use a sphere-cast query to find the nearest\nsurface below the object. Instead of allowing movable objects\nto stack on top of each other, we use an additional heuristic\nsearch described in Appendix I.2 to find a collision-free resting\nposition nearby.\nWe choose to implement our simulator in C++; it runs\non the CPU, in contrast to recent GPU-based physics simula-\ntors [16,29,64]. While these offer greater throughput, CPU-\nbased C++ code has advantages over GPU code: direct access to\nhost memory/disk/network resources, a less restrictive memory\nmodel, and greater availability of third-party libraries. In addi-\ntion, as shown in Figure 1, our kinematic sim (\u201cstep physics\u201d) is\nfaster than GPU-based rendering and PyTorch inference. This\nmakes it \u201cfree\u201d within our GPU-bound rollout computation, as\nshown by the large idle gaps on the Physics thread. This is in\ncontrast to GPU-based physics simulators, which despite their\nhigh throughput, would not be free in this GPU-bound scenario,\nwhich would negatively impact overall rollout compute time.\nFigure 3. Visualization of an environment (top) and its collision\ngeometry (bottom). In the bottom, the robot is represented with green\nspheres, and the held object with blue spheres. The other movable\nobjects as oriented boxes (orange), and the static environment as a\nvoxel-like \u201ccolumn grid\u201d data structure (gray).\n4. Experiment Setup: Object Rearrangement\n4.1. Task Description\nWe study how Galactic can accelerate learning in Geometric-\nGoal object rearrangement [2] with end-to-end RL. We follow\nthe GeometricGoal rearrangement task setup from [55], where\na robot is tasked with moving an object from a start location\nto the desired goal entirely from onboard sensing consisting\nof an RGBD head camera and proprioceptive sensing. The\ntask is specified via the 3D center of mass for the target object\u2019s\nstart location and the 3D center of mass for the object\u2019s target\nlocation. To accomplish the task, the agent must navigate\nthrough an indoor environment, pick, place, and avoid distractor\nobjects and clutter, all while operating from egocentric sensors.\nThe agent is a simulated Fetch robot [43], with a wheeled base, a\n7-DoF arm manipulator, a suction gripper, and a head-mounted\nRGBD camera (128\u00d7128 pixels). The episode is successful if\nthe agent places the object within 15cm of the desired position\nwithin the episode horizon of 500 time steps and calls the stop\naction. We aim to transfer the policies learned in Galactic to\nthe Rearrange Easy benchmark from the NeurIPS 2022 Habitat\nRearrangement Challenge [55,56]. We implement the same\nrearrangement task in Galactic to facilitate this transfer.\nWe use an 11-dimension action space in Galactic consisting\nof control for the base, arm, gripper, and episode termination.\n2 actions control the linear and angular velocity of the robot\nbase. 7 actions are for delta joint angles for each of the 7 arm\njoints. 1 binary action is for the grasping and 1 binary action\nindicates episode termination.\nThe observation space consists of visual sensors, proprio-\nceptive sensing, and task specification. The visual sensor is a\n128\u00d7128 RGBD, 90\u25e6 FoV head-mounted camera. Propriocep-\ntive sensing provides the angles in radians of all 7 joints and\nif the robot is holding an object. From a base egomotion sensor,\nwe derive the relative position of the robot base and end-effector\nto the object start and goal position as observations. Additionally,\nwe include an episode step counter in the observation.\nWe import the datasets and assets from the Rearrange\nEasy benchmark in Habitat 2.0 into Galactic. The Rearrange\nEasy benchmark uses the ReplicaCAD [55] scene dataset\nconsisting of 105 interactive indoor home spaces. We load\nthe train dataset of 10k episodes which specify rearrangement\nepisodes in the train split of 63 room layouts in ReplicaCAD.\nWe pre-compute the robot\u2019s starting position in each episode\nfor greater efficiency. Each episode contains one target object\nto rearrange and 29 distractor objects.\n4.2. Approach: End-to-End RL\nOur approach relies on training a \u201csensors-to-actions\u201d\npolicy directly using RL with the task reward alone. Previous\nstate-of-the-art approaches for mobile manipulation in Habitat\n2.0 rely on decomposing the task into the separate skills of\nnavigation, picking, and placing [19, 59]. However, such\nhierarchical approaches suffer from requiring a hand-specified\ntask decomposition and \u201chand-off problems\u201d where errors\nbetween skills compound. Our approach avoids these issues\nby training a single end-to-end policy via RL. Previously, such\nend-to-end approaches achieved little success in the Habitat\n2.0 rearrangement tasks [55]. However, with the speed of the\nGalactic simulator, we can generate experience fast enough for\nthe end-to-end approach to become a viable approach to rear-\nrangement tasks since it allows gathering the billions of samples\nneeded in only a few days with an 8-GPU compute node.\nReward function. We define the reward for the rearrangement\ntask largely following the default reward in the Rearrange Easy\nbenchmark, but with some modifications for smooth robot\nmotion. The agent gets a sparse reward for completing the task\nand picking up the object. The robot is given a dense reward\nas the decrease in L2 distance between the end-effector and\nobject as well as between the object and goal. The robot is\ngiven a penalty for large differences in actions at subsequent\nsteps. Furthermore, to speed up training convergence, we do\nnot allow the robot to call the stop or drop action until the\nend-effector is within a cylinder of width 0.15m and height\n0.3m around the goal position. We analyze this decision in\ndetail in Section 5.3. Additional details of the rearrangement\ntask and reward function in Galactic are in Appendix A.\nArchitecture and Hyperparameters. Our primary experiments\nuse a ResNet18 [23] visual encoder, with a 2-layer 512 hidden\nunit LSTM, and then separate actor and critic network heads.\nWe then train this with DD-PPO [60], a distributed version of\nPPO [47]. We run 128 environments per GPU across 8 GPUs\ngiving 1024 environment instances in total. We use a policy\nrollout length of 64, 2 mini-batches per update, and 1 epoch\nover the rollout data per update. This setup runs at over 30,000\nsteps-per-second (SPS), where the policy contains 6 million\ntrainable parameters, We provide all training hyperparameters\nin Appendix B.\n5. Results\n5.1. Throughput and Scaling\nIn Table 1, we report sim steps per second (physics and/or\nrendering) and training SPS (physics and/or rendering, plus\ninference and learning) for various physics-only and Embodied\nAI simulators. We also describe VizDoom, an arcade simulator\nthat has served as a classic benchmark for RL algorithms due\nto its speed.\nAmong EAI simulators that support realistic environments\n(photorealism and realistic physics), Galactic is 80\u00d7 faster than\nthe existing fastest simulator, Habitat 2.0 (108,806 vs 1243\ntraining SPS for 8 GPUs). Galactic\u2019s training throughput is\ncomparable to LBS, Megaverse, and VizDoom, even though\nLBS doesn\u2019t simulate physics, and neither Megaverse nor\nVizDoom supports realistic environments. We also compare\nto GPU-based physics-only simulators:\nwhile these are\ngenerally faster than Galactic, they entirely omit rendering,\nwhich significantly reduces the compute requirements of\nenvironment-stepping, inference, and training compared to\nvisual simulators and vision-based policy training. As seen\nin the last two rows, for our distributed training, we observe\nnear-linear scaling from 1 to 8 GPUs, achieving a 7.3x speedup.\nIn Figure 4, we show how the training SPS scales as a func-\ntion of the number of environments (batch size) and visual en-\ncoder on a rearrangement task (described in detail in Section 4).\nSPS increases for simpler encoders and larger batch sizes.\n5.2. Sim-to-Sim Results\nIn this section, we show that we can scale RL training in\nGalactic, achieve a high success rate in Galactic, and then\nzero-shot transfer the policy to Habitat 2.0.\nTraining in Galactic. First, we train a policy in Galactic using\nthe end-to-end training setup and policy architecture described\nin Section 4.2. We leverage the fast simulation and easy scaling\n16\n32\n64\n128\n256\n512\nNumber of environments per GPU\n20000\n40000\n60000\n80000\n100000\nSPS\nResnet18\nResnet9\nSimpleCNN\nFigure 4.\nThe steps-per-second throughput when varying visual\nencoder architecture and the number of environments per GPUs, using\na single compute node and 8 GPUs. The rightmost point on each\ncurve corresponds to the maximum number of environments we can\nuse before running out of GPU memory.\nof Galactic to train a neural policy with RL for 5 billion steps\nof experience. With 1024 environments across 8 GPUs, training\nruns at 30,000 SPS which takes 46 hours to train the policy to\nconvergence at 5 billion steps.\nTransfer to Habitat 2.0. Next, we transfer the policy trained\nin Galactic zero-shot to Habitat 2.0. The physics simulation\nin Galactic is kinematic, while the physics in Habitat 2.0 is\ndynamic. In the kinematic simulation of Galactic, the actions\ndenote joint delta angles and base velocity, which are set by the\nsimulator on the next step. However, in Habitat 2.0, the arm is\ncontrolled via joint motor torques at every step to achieve a target\njoint state. Hence, we bridge the dynamics simulation gap by us-\ning the policy trained in Galactic as a higher-level policy that out-\nputs target joint offset angles in Habitat 2.0 at 10Hz. A controller\nthen outputs the joint motor torques to achieve the target joint\nstate set by the policy trained in Galactic operating at 120Hz.\nThe policy takes as input the observations from Habitat 2.0 to\noutput this target delta state. Observations between Galactic and\nHabitat 2.0 are similar since they use the same scenes and assets.\nOther small differences in the simulators affect the\nsim-to-sim performance. Since Galactic does not simulate\ndynamics, the dropping mechanism \u201csnaps\u201d the object to the\nsurface directly below the drop point of the object. This ignores\nthe effects of the object rolling or bouncing off from the point\nof contact after the drop. However, as described in Section 4.2,\nthe policy is trained to drop the object just above the surface.\nThis low drop point minimizes the potential effects of bouncing\nor rolling due to high drop speeds, helping the sim-to-sim\ntransfer. Furthermore, collisions are handled differently in the\ntwo simulators. In Galactic, we \u201callow sliding\u201d for training like\ndescribed in Section 3.2, meaning that the base of the robot will\nslide when the arm penetrates the environment. However, in\nHabitat 2.0, the arm can collide with the surrounding scene.\nWe compare to training the same policy architecture directly\nin Habitat 2.0 with RL. Specifically, we use an identical reward\nand policy architecture setup as described in Section 4.2 in\nHabitat 2.0. We train this policy for 50 hours which leads to\naround 200M steps of training in Habitat 2.0 using the VER\ntrainer [59]. Training in Habitat 2.0 for 5 billion steps like\nGalactic would take 46 days of training. Both the Galactic\npolicy and the Habitat 2.0 baseline policy utilize comparable\ncompute resources for training.\nGalactic\nGalactic \u2192 H2.0\nH2.0\nTrain\n95.30\u00b1 0.67\n36.70\u00b1 0.46\n0.00\u00b1 0.00\nEval\n86.70\u00b1 1.06\n26.40\u00b1 0.43\n0.00\u00b1 0.00\nTable 2. Success rates in the Rearrange task. The middle column\nshows the zero-shot success of the policy trained in Galactic on the\nfully dynamic Habitat 2.0 (H2.0). The rightmost column is the policy\ntrained purely in H2.0. The leftmost column shows success for the\nGalactic trained policy evaluated in Galactic (grayed since it reports\nsuccess in Galactic while other columns are in H2.0). Numbers\nrepresent mean and standard error across 1k episodes.\nResults. We evaluate the success rate of the trained policies\non unseen episodes in Table 2. The top row of Table 2 shows\nthe success rate for 1k of the train episodes, and the bottom\nrow shows the success rate over the evaluation dataset of 1k\nepisodes (the \u201cval\u201d dataset from the Rearrange Easy challenge).\nThe leftmost column (Galactic \u2192 Galactic) of Table 2 shows\nthe performance of the policy trained in Galactic on the unseen\nepisodes in Galactic, demonstrating that the policy can learn\ngeneralizable rearrangement behavior from 5B steps of training.\nIn Habitat 2.0 \u2192 Habitat 2.0, we zero-shot transfer the policy\ntrained in Galactic to Habitat 2.0. Despite differences in visuals\nand dynamics between the simulators, the policy can still\nachieve 26.40% success rate without any further training. This\nfar outperforms training directly in Habitat 2.0 with a similar\ncompute budget as demonstrated by the policy trained directly\nin Habitat 2.0 achieving no success. This result is consistent\nwith [55,56] where end-to-end policies also achieve no success.\nThe policy in Habitat 2.0 learns too slow, and even after 200M\nsteps, it only learns to pick the object up but struggles to place\nit. In Appendix C, we further analyze policy in Habitat 2.0 and\nthe errors in the zero-shot transfer.\n5.3. Analyzing Rearrangement Settings in Galactic\nIn this section, we analyze what properties of the rearrange-\nment task structure are important for RL training. Galactic\nallows us to answer these questions at scale by training policies\nfully to convergence, even if it takes billions of steps.\nWe analyze three variations of the rearrange task with\ndifferent conditions around the termination and drop action.\n\u2022 Rearrange : The \u201cdefault\u201d version of rearrangement\nwhere the policy may drop the object at any point or call\nthe stop action at any point in the episode.\n\u2022 Rearrange [Safe Drop/Stop] : Same as Rearrange except\nthe policy has a heuristic condition around when the\nstop and drop actions are executed. The drop action is\nignored except when the robot\u2019s end-effector is within a\ncylinder of radius 0.15m and height of 0.3m around the\ngoal location. The stop action is ignored before the robot\nhas dropped the object.\n\u2022 Rearrange [No Distractors] : A version of Rearrange\n[Safe Drop/Stop] where there are no other objects in the\nscene other than the object the robot needs to move.\nPolicies are trained for 5 billion steps in the same training setup\nas Section 5.2. Figure 5 compares the learning curves for each\nsetting where each point on the curve is the policy checkpoint\nevaluated for 100 validation episodes (unseen configurations\nof objects and scenes) from the Tidy House dataset. When\nevaluating the last checkpoint on 1000 evaluation episodes,\nwe find that Rearrange [Safe Drop/Stop] achieves 84.73%\nsuccess rate while Rearrange achieves 79.63% success rate,\ndespite using the same sensor inputs. This demonstrates that the\nchallenges of learning the stop and drop actions harm learning.\nFurthermore, the gap between Rearrange [Safe Drop/Stop] with\na success rate of 84.73% and Rearrange [No Distractors] with\na 99.5% success rate after only 500 million steps shows that the\npresence of distractor objects makes learning more difficult.\nFigure 5. Evaluation success rate of different Rearrange tasks on 100\nheld-out episodes with mean and std across 3 seeds.\n5.4. Speeding Up Mobile Pick\nWe also show that Galactic can rapidly train policies in an\nadditional mobile pick task. In the mobile pick task, the robot\nis spawned within a 2-meter radius of the object to pick up\nand is provided the start coordinates of the object. The agent\nthen controls the base and arm to navigate to and then pick up\nthe object. The episode is successful if the agent picks up the\ncorrect object within the episode horizon of 300 steps. We use\nthe policy architecture described in Section 4.2 with a ResNet18\nvisual encoder for policies in Galactic and Habitat 2.0.\nThe plot in Figure 6 shows that Galactic is capable of training\npolicies in Galactic 100\u00d7 faster than in Habitat 2.0. To reach\nan 80% success rate, training in Galactic requires 16 minutesof\nwall-clock time, while reaching the same success in Habitat 2.0\nrequires over 26 hours. This 100\u00d7 speed-up validates that Galac-\ntic\u2019s higher SPS translates directly to faster wall-clock time-to-\nconvergence. It also demonstrates that Galactic can be used to\naccelerate even tasks less complex than full rearrangement.\n100x Faster\n16 minutes\n>26 hours\nFigure 6. Wall-clock time versus success rate in the mobile pick task\nbetween training policies in Galactic and Habitat 2.0. Galactic takes\n16 minutesto train a policy to 80% accuracy while Habitat 2.0 requires\nover 26 hours to train a policy to 80% accuracy.\n6. Conclusion\nWe propose Galactic, a framework for rendering, physics,\nand RL training for embodied tasks at a massive scale. To\nachieve high throughput for experience collection, we develop\nan approximate kinematic simulator optimized for embodied\nrearrangement tasks and propose a batch processing approach\nto jointly render observations and simulate the physics of the\nworld. Galactic processes more than 100,000 steps per second\nin RL training. Learning a mobile picking skill takes less than\n16 minutes, while training for the same task in Habitat 2.0, one\nof the fastest embodied simulation frameworks, requires more\nthan 26 hours. We also show that policies trained in Galactic\ngeneralize to zero-shot to Habitat 2.0. We hope that Galactic\nopens up new avenues in embodied AI research by enabling\nlarge-scale training.\nAcknowledgements. The Georgia Tech effort was supported\nin part by NSF, ONR YIP, and ARO PECASE. The views\nand conclusions contained herein are those of the authors and\nshould not be interpreted as necessarily representing the official\npolicies or endorsements, either expressed or implied, of the\nU.S. Government, or any sponsor. We thank Brennan Shacklett\nfor his help with Bps3D and systems advice. We also thank\nErik Wijamns for his help with the training pipeline and general\nadvice on RL training.\nReferences\n[1] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark\nJohnson, Niko S\u00a8underhauf, Ian D. Reid, Stephen Gould, and\nAnton van den Hengel.\nVision-and-language navigation:\nInterpreting visually-grounded navigation instructions in real\nenvironments. In CVPR, 2018. 3\n[2] Dhruv Batra, Angel X Chang, Sonia Chernova, Andrew J\nDavison, Jia Deng, Vladlen Koltun, Sergey Levine, Jitendra\nMalik, Igor Mordatch, Roozbeh Mottaghi, et al. Rearrangement:\nA challenge for embodied ai. arXiv, 2020. 2, 4, 5\n[3] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,\nJared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav\nShyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,\nChristopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner,\nSam McCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. Language models are few-shot learners. arXiv, 2020. 1\n[4] Berk Calli, Arjun Singh, Aaron Walsman, Siddhartha Srinivasa,\nPieter Abbeel, and Aaron M Dollar. The ycb object and model\nset: Towards common benchmarks for manipulation research.\nIn ICAR, 2015. 14\n[5] Devendra Singh Chaplot, Dhiraj Prakashchand Gandhi, Abhinav\nGupta, and Russ R Salakhutdinov. Object goal navigation using\ngoal-oriented semantic exploration. In NeurIPS, 2020. 1\n[6] Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey\nLevine. Deep reinforcement learning in a handful of trials using\nprobabilistic dynamics models. In NeurIPS, 2018. 3\n[7] Erwin Coumans and Yunfei Bai. Pybullet, a python module for\nphysics simulation for games, robotics and machine learning.\nhttp://pybullet.org. 3, 4\n[8] Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee,\nDevi Parikh, and Dhruv Batra. Embodied question answering.\nIn CVPR, 2018. 3\n[9] Marc Peter Deisenroth and Carl Edward Rasmussen. Pilco: A\nmodel-based and data-efficient approach to policy search. In\nICML, 2011. 3\n[10] Matt Deitke, Winson Han, Alvaro Herrasti, Aniruddha Kembhavi,\nEric Kolve, Roozbeh Mottaghi, Jordi Salvador, Dustin Schwenk,\nEli VanderBilt, Matthew Wallingford, Luca Weihs, Mark Yatskar,\nand Ali Farhadi. Robothor: An open simulation-to-real embodied\nai platform. In CVPR, 2020. 3\n[11] Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs, Jordi\nSalvador, Kiana Ehsani, Winson Han, Eric Kolve, Ali Farhadi,\nAniruddha Kembhavi, et al. Procthor: Large-scale embodied ai\nusing procedural generation. In NeurIPS, 2022. 2\n[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\nFei-Fei. Imagenet: A large-scale hierarchical image database.\nIn CVPR, 2009. 1\n[13] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman,\nRyan Hickman, Krista Reymann, Thomas B McHugh, and\nVincent Vanhoucke. Google scanned objects: A high-quality\ndataset of 3d scanned household items. In ICRA, 2022. 15\n[14] Kiana Ehsani, Winson Han, Alvaro Herrasti, Eli VanderBilt,\nLuca Weihs, Eric Kolve, Aniruddha Kembhavi, and Roozbeh\nMottaghi. ManipulaTHOR: A Framework for Visual Object\nManipulation. In CVPR, 2021. 3\n[15] Lasse Espeholt, Hubert Soyer, R\u00b4emi Munos, Karen Simonyan,\nVolodymyr Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim\nHarley, Iain Dunning, Shane Legg, and Koray Kavukcuoglu.\nImpala: Scalable distributed deep-rl with importance weighted\nactor-learner architectures. In ICML, 2018. 3\n[16] C. Daniel Freeman, Erik Frey, Anton Raichuk, Sertan Girgin,\nIgor Mordatch, and Olivier Bachem. Brax - a differentiable\nphysics engine for large scale rigid body simulation. In NeurIPS\nDatasets and Benchmarks Track, 2021. 2, 3, 4, 5\n[17] Chuang Gan, Jeremy Schwartz, Seth Alter, Martin Schrimpf,\nJames Traer, Julian De Freitas, Jonas Kubilius, Abhishek Bhand-\nwaldar, Nick Haber, Megumi Sano, Kuno Kim, Elias Wang,\nDamian Mrowca, Michael Lingelbach, Aidan Curtis, Kevin T.\nFeigelis, Daniel Bear, Dan Gutfreund, David Cox, James J. Di-\nCarlo, Josh H. McDermott, Joshua B. Tenenbaum, and Daniel\nL. K. Yamins. Threedworld: A platform for interactive multi-\nmodal physical simulation. In NeurIPS (dataset track), 2021. 3\n[18] Daniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari,\nJoseph Redmon, Dieter Fox, and Ali Farhadi. Iqa: Visual question\nanswering in interactive environments. In CVPR, 2018. 3\n[19] Jiayuan Gu, Devendra Singh Chaplot, Hao Su, and Jitendra\nMalik. Multi-skill mobile manipulation for object rearrangement.\nIn ICLR, 2023. 1, 6\n[20] Shixiang Shane Gu, Timothy P. Lillicrap, Ilya Sutskever, and\nSergey Levine. Continuous deep q-learning with model-based\nacceleration. In ICML, 2016. 3\n[21] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and\nJimmy Ba. Mastering atari with discrete world models. In ICLR,\n2021. 3\n[22] Danijar Hafner, Timothy P. Lillicrap, Ian S. Fischer, Ruben Vil-\nlegas, David R Ha, Honglak Lee, and James Davidson. Learning\nlatent dynamics for planning from pixels. In ICML, 2019. 3\n[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep\nresidual learning for image recognition. In CVPR, 2016. 6\n[24] Dan Horgan, John Quan, David Budden, Gabriel Barth-Maron,\nMatteo Hessel, Hado van Hasselt, and David Silver. Distributed\nprioritized experience replay. In ICLR, 2018. 3\n[25] Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki,\nTom Schaul, Joel Z Leibo, David Silver, and Koray Kavukcuoglu.\nReinforcement learning with unsupervised auxiliary tasks. In\nICLR, 2017. 3\n[26] Micha\u0142 Kempka, Marek Wydmuch, Grzegorz Runc, Jakub\nToczek, and Wojciech Ja\u00b4skowski. ViZDoom: A Doom-based\nAI research platform for visual reinforcement learning. In IEEE\nConference on Computational Intelligence and Games, 2016. 2\n[27] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt,\nLuca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav\nGupta, and Ali Farhadi. Ai2-thor: An interactive 3d environment\nfor visual ai. arXiv, 2017. 3, 4\n[28] Chengshu Li, Fei Xia, Roberto Mart\u00b4\u0131n-Mart\u00b4\u0131n, Michael\nLingelbach, Sanjana Srivastava, Bokui Shen, Kent Vainio,\nCem Gokmen, Gokul Dharan, Tanish Jain, et al.\nigibson\n2.0: Object-centric simulation for robot learning of everyday\nhousehold tasks. arXiv, 2021. 2\n[29] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo,\nMichelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita\nRudin, Arthur Allshire, Ankur Handa, and Gavriel State. Isaac\ngym: High performance gpu-based physics simulation for robot\nlearning. arXiv, 2021. 2, 3, 4, 5\n[30] Oleksandr Maksymets, Vincent Cartillier, Aaron Gokaslan, Erik\nWijmans, Wojciech Galuba, Stefan Lee, and Dhruv Batra. Thda:\nTreasure hunt data augmentation for semantic navigation. In\nICCV, 2021. 1\n[31] Piotr Wojciech Mirowski, Razvan Pascanu, Fabio Viola, Hubert\nSoyer, Andy Ballard, Andrea Banino, Misha Denil, Ross\nGoroshin, L. Sifre, Koray Kavukcuoglu, Dharshan Kumaran, and\nRaia Hadsell. Learning to navigate in complex environments.\nIn ICLR, 2017. 3\n[32] R\u00b4emi Munos, Tom Stepleton, Anna Harutyunyan, and Marc G.\nBellemare. Safe and efficient off-policy reinforcement learning.\nIn NeurIPS, 2016. 3\n[33] Ofir Nachum, Shixiang Shane Gu, Honglak Lee, and Sergey\nLevine. Data-efficient hierarchical reinforcement learning. In\nNeurIPS, 2018. 3\n[34] Anusha Nagabandi, Gregory Kahn, Ronald S. Fearing, and\nSergey Levine. Neural network dynamics for model-based deep\nreinforcement learning with model-free fine-tuning. In ICRA,\n2018. 3\n[35] Arun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek,\nRory Fearon, Alessandro De Maria, Vedavyas Panneershelvam,\nMustafa Suleyman, Charlie Beattie, Stig Petersen, Shane\nLegg, Volodymyr Mnih, Koray Kavukcuoglu, and David Silver.\nMassively parallel methods for deep reinforcement learning.\narXiv, 2015. 3\n[36] Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor\nDarrell.\nCuriosity-driven exploration by self-supervised\nprediction. In ICML, 2017. 3\n[37] Aleksei Petrenko, Erik Wijmans, Brennan Shacklett, and Vladlen\nKoltun. Megaverse: Simulating embodied agents at one million\nexperiences per second. In ICML, 2021. 2, 3\n[38] Xavier Puig, Kevin Kyunghwan Ra, Marko Boben, Jiaman Li,\nTingwu Wang, Sanja Fidler, and Antonio Torralba. Virtualhome:\nSimulating household activities via programs. In CVPR, 2018. 3\n[39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,\nGabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell,\nPamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya\nSutskever. Learning transferable visual models from natural\nlanguage supervision. arXiv, 2021. 1\n[40] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario\nAmodei, and Ilya Sutskever. Language models are unsupervised\nmultitask learners. arXiv, 2019. 1\n[41] Santhosh K Ramakrishnan, Aaron Gokaslan, Erik Wijmans,\nOleksandr Maksymets, Alex Clegg, John Turner, Eric Under-\nsander, Wojciech Galuba, Andrew Westbury, Angel X Chang,\net al. Habitat-matterport 3d dataset (hm3d): 1000 large-scale\n3d environments for embodied ai. arXiv, 2021. Matterport\nlicense available at http://kaldir.vc.in.tum.de/\nmatterport/MP_TOS.pdf. 1, 15\n[42] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In ICML, 2021. 1\n[43] Fetch robotics. Fetch. http://fetchrobotics.com/,\n2020. 2, 5, 14\n[44] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser,\nand Bj\u00a8orn Ommer. High-resolution image synthesis with latent\ndiffusion models. In CVPR, 2022. 1\n[45] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili\nZhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu,\nVladlen Koltun, Jitendra Malik, et al. Habitat: A platform for\nembodied ai research. In ICCV, 2019. 3, 4\n[46] Alexander Sax, Bradley Emi, Amir R. Zamir, Leonidas J.\nGuibas, Silvio Savarese, and Jitendra Malik. Mid-level visual\nrepresentations improve generalization and sample efficiency for\nlearning visuomotor policies. arXiv, 2018. 3\n[47] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford,\nand Oleg Klimov.\nProximal policy optimization algorithms.\narXiv, 2017. 6\n[48] Brennan Shacklett, Erik Wijmans, Aleksei Petrenko, Manolis\nSavva, Dhruv Batra, Vladlen Koltun, and Kayvon Fatahalian.\nLarge batch simulation for deep reinforcement learning. In ICLR,\n2021. 2, 3, 4\n[49] Rutav M. Shah and Vikash Kumar. RRL: resnet as representation\nfor reinforcement learning. In ICML, 2021. 3\n[50] Evan Shelhamer,\nParsa Mahmoudieh,\nMax Argus,\nand\nTrevor Darrell. Loss is its own reward: Self-supervision for\nreinforcement learning. arXiv, 2017. 3\n[51] Bokui Shen, Fei Xia, Chengshu Li, Roberto Mart\u2019in-Mart\u2019in,\nLinxi (Jim) Fan, Guanzhi Wang, S. Buch, Claudia. P\u00b4erez\nD\u2019Arpino, Sanjana Srivastava, Lyne P. Tchapmi, Micael Edmond\nTchapmi, Kent Vainio, Li Fei-Fei, and Silvio Savarese. igibson,\na simulation environment for interactive tasks in large realistic\nscenes. In IROS, 2021. 2, 3\n[52] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk,\nWinson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter\nFox. Alfred: A benchmark for interpreting grounded instructions\nfor everyday tasks. In CVPR, 2020. 3\n[53] Sanjana Srivastava, Chengshu Li, Michael Lingelbach, Roberto\nMart\u2019in-Mart\u2019in, Fei Xia, Kent Vainio, Zheng Lian, Cem\nGokmen, S. Buch, C. Karen Liu, Silvio Savarese, Hyowon\nGweon, Jiajun Wu, and Li Fei-Fei.\nBehavior: Benchmark\nfor everyday household activities in virtual, interactive, and\necological environments. In CoRL, 2021. 3\n[54] Adam Stooke and Pieter Abbeel. Accelerated methods for deep\nreinforcement learning. arXiv, 2018. 3\n[55] Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wijmans,\nYili Zhao, John Turner, Noah Maestre, Mustafa Mukadam,\nDevendra Singh Chaplot,\nOleksandr Maksymets,\nAaron\nGokaslan, Vladimir Vondrus, Sameer Dharur, Franziska Meier,\nWojciech Galuba, Angel Xuan Chang, Zsolt Kira, Vladlen\nKoltun, Jitendra Malik, Manolis Savva, and Dhruv Batra. Habitat\n2.0: Training home assistants to rearrange their habitat.\nIn\nNeurIPS, 2021. 1, 2, 3, 4, 5, 6, 7, 14\n[56] Andrew Szot, Karmesh Yadav, Alex Clegg, Vincent-Pierre\nBerges, Aaron Gokaslan, Angel Chang, Manolis Savva, Zsolt\nKira, and Dhruv Batra. Habitat rearrangement challenge 2022.\nhttps://aihabitat.org/challenge/rearrange_\n2022, 2022. 5, 6, 7\n[57] Joanne Truong, Max Rudolph, Naoki Harrison Yokoyama, Sonia\nChernova, Dhruv Batra, and Akshara Rai. Rethinking sim2real:\nLower fidelity simulation leads to higher sim2real transfer in\nnavigation. In CoRL, 2022. 3, 4\n[58] Luca Weihs, Matt Deitke, Aniruddha Kembhavi, and Roozbeh\nMottaghi. Visual room rearrangement. In CVPR, 2021. 1, 3\n[59] Erik Wijmans, Irfan Essa, and Dhruv Batra.\nVer: Scaling\non-policy rl leads to the emergence of navigation in embodied\nrearrangement. arXiv, 2022. 1, 2, 3, 6, 7\n[60] Erik Wijmans, Abhishek Kadian, Ari S. Morcos, Stefan Lee,\nIrfan Essa, Devi Parikh, Manolis Savva, and Dhruv Batra.\nDd-ppo: Learning near-perfect pointgoal navigators from 2.5\nbillion frames. In ICLR, 2020. 1, 3, 6\n[61] Mitchell Wortsman, Kiana Ehsani, Mohammad Rastegari, Ali\nFarhadi, and Roozbeh Mottaghi. Learning to learn how to learn:\nSelf-adaptive visual navigation using meta-learning. In CVPR,\n2019. 3\n[62] Fei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra\nMalik, and Silvio Savarese. Gibson env: Real-world perception\nfor embodied agents. In CVPR, 2018. 3\n[63] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu,\nFangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He\nWang, Li Yi, Angel X.Chang, Leonidas Guibas, and Hao Su.\nSAPIEN: A SimulAted Part-based Interactive ENvironment. In\nCVPR, 2020. 3\n[64] Jie Xu, Viktor Makoviychuk, Yashraj Narang, Fabio Ramos, Wo-\njciech Matusik, Animesh Garg, and Miles Macklin. Accelerated\npolicy learning with parallel differentiable simulation. ICLR,\n2022. 2, 3, 4, 5\n[65] Karmesh Yadav, Ram Ramrakhya, Arjun Majumdar, Vincent-\nPierre Berges, Sachit Kuhar, Dhruv Batra, Alexei Baevski, and\nOleksandr Maksymets. Offline visual representation learning\nfor embodied navigation. arXiv, 2022. 1\n[66] Denis Yarats, Amy Zhang, Ilya Kostrikov, Brandon Amos,\nJoelle Pineau, and Rob Fergus. Improving sample efficiency in\nmodel-free reinforcement learning from images. In AAAI, 2021. 3\n[67] Amir Yazdanbakhsh, Junchao Chen, and Yu Zheng. Menger:\nMassively\nlarge-scale\ndistributed\nreinforcement\nlearning.\nNeurIPS, Beyond Backpropagation Workshop, 2020. 3\n[68] Joel Ye, Dhruv Batra, Erik Wijmans, and Abhishek Das.\nAuxiliary tasks speed up learning pointgoal navigation. In CoRL,\n2020. 1, 3\n[69] Lin Yen-Chen, Andy Zeng, Shuran Song, Phillip Isola, and\nTsung-Yi Lin. Learning to see before learning to act: Visual\npre-training for manipulation. In ICRA, 2020. 3\n[70] Albert Zhan, Philip Zhao, Lerrel Pinto, Pieter Abbeel, and\nMichael Laskin. Learning visual robotic control efficiently with\ncontrastive pre-training and data augmentation. In IROS, 2022. 3\nA. Additional Task Details\nThe full reward function of the Rearrange task is described\nin Equation (1).\nrt=10Isuccess+5Ipick+\u2206obj\narm+\u2206goal\nobj \u22120.001Ct\n(1)\nWhere:\n\u2022 Isuccess is the indicator for task success.\n\u2022 Ipick is the indicator if the agent just picked up the object.\n\u2022 \u2206obj\narm is the change in Euclidean distance between\nthe end-effector (arm) and the target object (obj). If\ndt is the distance between the two at timestep t, then\n\u2206obj\narm=dt\u22121\u2212dt).\n\u2022 \u2206goal\nobj is the change in Euclidean distance between the\nobject (obj) and the goal position (goal).\n\u2022 Ct Is the squared difference in joint action values between\nthe current and previous time step. If ak\nt is the action for\ntimestep t for moving joint k then Ct=P\nk(ak\nt \u2212ak\nt\u22121)2\nThe reward signal for the mobile pick is identical, but the\ntask ends in a success if the robot picks the correct object\n(Isuccess=Ipick).\nThe action space of the monolithic policy consist of 11\nactions:\n\u2022 7 continuous actions controlling the change to the joints\nangles. These actions are normalized between \u22121 and 1\nwith the minimum and maximum value corresponding to\nthe maximum change to the joint angles allowed in each\ndirection per step.\n\u2022 1 continuous action between \u22121 and 1 corresponding\nto the robot moving forward. An action with value of 1\ncorresponds to the robot moving forward by 10cm and \u22121\nto the robot moving backward by 10cm in the simulation.\n\u2022 1 continuous action between \u22121 and 1 corresponding to\nthe robot rotating. An action with value of 1 corresponds\nto the robot rotating in clockwise by 5\u25e6 and \u22121 to the\nrobot rotating counter-clockwise by 5\u25e6.\n\u2022 1 discrete action with 2 options corresponding to the robot\nattempting to grasp or release an object. If the value is 0,\nand the robot is holding an object, the robot will attempt\nto release it. If the value is 1 and the robot is not holding\nan object, the robot will attempt to grasp.\n\u2022 1 discrete action with 2 options corresponding to the robot\nattempting to terminate the episode. If the value is 0 the\nrobot will continue the task. If the value is 1, the robot\nwill signal that the task is completed.\nB. Method Details\nMore details about the method architecture here.\nOur Hyperparameters are described in Table 3.\nHyperparameter\nValue\nstart learning rate\n3.5\u00d710\u22124\nend learning rate\n0\nlearning rate schedule\nlinear\nentropy coefficient\n1\u00d710\u22123\nclip gradient norm\n2.0\ntime horizon\n64\nnumber of epochs per updates\n1\nnumber of mini batches per updates\n2\nRGB and Depth image resolution\n128\u00d7128\nimage encoder\nResNet18\nnormalized advantage\ntrue\nTable 3. Hyperparameters used for DD-PPO training in Galactic\nTo calculate the entropy of this action space for entropy reg-\nularization in DD-PPO, we add the entropy of the discrete and\ncontinuous actions distributions together without any scaling.\nThe SimpleCNN model we use consists of 3 convolution\nlayers followed by a fully connected layer. The kernel sizes for\nthe three convolution layers are 8\u00d78, 4\u00d74 and 3\u00d73, the strides\nare 4\u00d74, 2\u00d72 and 1\u00d71 and there is no dilation nor padding.\nThis is the same SimpleCNN visual encoder used in Habitat\n2.0. The size of the models used are described in Table 4.\nModel\nTotal number of parameters\nSimpleCNN\n4,046,999\nResNet9\n4,338,007\nResNet18\n5,906,647\nTable 4. Model sizes for the different visual encoders used. This\nincludes the visual encoder, the actor, and the critic.\nC. Further Habitat 2.0 Results\nFirst, we analyze the poor performance of the policy purely\ntrained in Habitat 2.0, which achieves no success in the Table 2.\nFigure 7a shows the reward learning curve during training.\nThis learning curve demonstrates that even after 200M steps of\ntraining, the reward is still increasing, which provides evidence\nfor the necessity of Galactic to scale training. In this training\ntime, the agent reliably learns to pick the object around 80% of\nthe time as shown by the training plot in Figure 7b comparing\nthe fraction of the time the robot picked the object within an\nepisode versus the number of training steps.\nNext, we analyze the source of errors in the zero-shot transfer\nfrom Galactic to Habitat 2.0. We show the drop in performance\nis not due to the dynamic arm control by comparing to\ntransferring to Habitat 2.0 with a kinematic arm controller\ninstead of a dynamics-based torque controller. The agent with\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nEnvironment Steps\n1e8\n2\n0\n2\n4\n6\n8\n10\nEpisode Return\n(a) Reward\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nEnvironment Steps\n1e8\n0.0\n0.2\n0.4\n0.6\n0.8\nDid Pick Object Ratio\n(b) Picked object ratio\nFigure 7. Learning curves for the policy trained purely in Habitat 2.0.\nFigure 7a shows the episode reward does not saturate even after 200M\ntraining steps. Figure 7b shows that even though the agent is never\nsuccessful, it still learns to pick the object.\nthe kinematic arm controller achieves a 29.7% success rate on\nthe \u201cEval\u201d dataset, barely any better than the 26.4% success\nrate the dynamics-based torque controller achieves.\nD. Additional Task visuals\nIn this section, we visualize observations rendered using the\nGalactic simulator. Figure 8 are examples of 128 \u00d7 128 RGB\nimages used for training. Figure 9 are examples of 128 \u00d7 128\ndepth images used for training. We also visualize observations\nrendered using the Habitat 2.0 simulator also at 128\u00d7128 in\nFigure 10 and Figure 11.\nE. Training for 15 Billion steps\nTo show the usefulness of training for several billions of\nsteps, we trained the Rearrange task defined in Section 4 for\n15 billion steps. Training and validation success rates are still\nFigure 8. Samples of RGB observations collected in Galactic.\nFigure 9. Samples of Depth observations collected in Galactic.\nFigure 10. Samples of RGB observations collected in Habitat 2.0.\nimproving, showing that training still hasn\u2019t converged, even\nafter 15 billion steps.\nFigure 11. Samples of Depth observations collected in Habitat 2.0.\nFigure 12. Training and evaluation curves for a 15 billion steps\ntraining run. Each checkpoint is evaluated on 100 training or validation\nepisodes.\nVisual Encoder\nResNet18\nSimpleCNN\nNumber of Envs\n128\n512\nPyTorch Inference\n7.52\n8.80\nRender Setup\n0.77\n1.47\nStep Post-processing*\n1.84\n2.05\nStep Physics*\n1.78\n3.76\nGPU Rendering\n2.27\n9.68\nAdditional CPU\n0.74\n1.00\nTotal\n11.30 ms\n20.95 ms\nTable 5. Timing breakdown of a single batch rollout step, for two\nconfigurations in milliseconds. * Post-processing and Physics are\ninterleaved with GPU rendering and PyTorch inference and don\u2019t\ncontribute to total rollout step time. See also Figure 1. 1x Tesla V100,\n10x Intel Xeon Gold 6230 CPU @ 2.10GHz, 128x128 RGBD sensors.\nF. Performance Timings\nG. Additional Collision-Detection Details\nIn this section, we\u2019ll expand on Section 3.2, in par-\nticular,\nwe\u2019ll discuss our collision representations and\ncollision-detection queries.\nVisual Encoder\nResNet18\nSimpleCNN\nNumber of Envs\n128\n512\nCompute Rollouts\n726\n1289\nUpdate Agent\n1381\n856\nTotal\n2204 ms\n2215 ms\nTraining SPS\n3716 SPS\n14791 SPS\nTable 6. Timing breakdown of a single train update for two config-\nurations in milliseconds. 1x Tesla V100, 10x Intel Xeon Gold 6230\nCPU @ 2.10GHz, 128x128 RGBD sensors, 64 batch rollout steps.\nGalactic scenes include a Fetch robot [43], movable\nYCB objects [4], and 105 static (non-movable) ReplicaCAD\nscenes [55]. Note that scenes in the ReplicaCAD dataset include\nsome interactive furniture (e.g. openable cabinet drawers and\ndoors), but we don\u2019t simulate these in Galactic as they aren\u2019t\nrequired for the Rearrange Easy benchmark.\nAs discussed in Section 3.2, our approximate kinematic sim\nmust perform collision queries between the robot (including\ngrasped object, if any) and the environment (resting movable\nobjects and the static scene). We represent each articulated link\nof the robot with a set of spheres (green in Figure 3). These\nare authored manually, with the goal to approximate the shape\nof the Fetch robot with a minimal number of spheres. We also\nrepresent each grasped object with a set of spheres (blue). These\nare generated offline using a space-filling heuristic. Rather\nthan supporting arbitrary sphere radius, we limit ourselves to\na sphere-radius \u201cworking set\u201d of{1.5 cm, 5 cm, 12 cm}. This\nlimitation is important as we\u2019ll see shortly.\nWe approximate a ReplicaCAD scene as a voxel-like\nstructure called a column grid (gray in Figure 3). A column grid\nis generated offline for a particular scene and a particular sphere\nradius from our working set, so we generate three column grids\nper scene. A column grid is a dense 2D array of columns in\nthe XZ (ground) plane, with 3-centimeter spacing. For each\ncolumn, we represent vertical free space as a list of layers. For\nexample, a column in an open area of the room would contain\njust one layer, storing two floating-point height values roughly\ncorresponding to the height of the floor and the height of the\nceiling. A column in the vicinity of a table, meanwhile, would\ncontain two layers: one spanning from the floor to the underside\nof the table, and another spanning from the table surface to the\nceiling. Finally, the stored height values don\u2019t actually represent\nthe surface heights themselves, but rather the height of the\nquery sphere (of known radius) in contact with the surface. For\nReplicaCAD scenes, the maximum number of layers for any\ncolumn is approximately 10 and corresponds to columns in the\nvicinity of a particular bookshelf with many shelves.\nA column grid is generated offline using the ReplicaCAD\nscene\u2019s source triangle mesh and Habitat 2.0\u2019s sphere-query\nfunctionality. We load the scene in Habitat 2.0 and use the\nscene extents to derive the column grid\u2019s XZ (ground-plane)\nextents. We iterate over this region using our chosen 3-cm\nspacing. For each column, we perform a brute-force search of\nthe vertical region at the column\u2019s XZ position, using a series of\nsphere-overlap and vertical sphere-casts to find the free spans.\nAt runtime, to detect collisions between the robot (including\ngrasped object, if any) and the static scene, we implement\na fast sphere-versus-column-grid query. First, we select the\nappropriate column grid corresponding to the query sphere\u2019s\nradius. Second, we retrieve the nearest column corresponding\nto the sphere\u2019s XZ position. Finally, we linearly search the\ncolumn\u2019s layers to determine whether the query sphere\u2019s Y\nposition is in free versus obstructed space. This linear search\nis accelerated using caching: we start the search from the same\nlayer index found in recent searches. This leverages spatial and\ntemporal coherency, for example, consider the robot reaching\nunder a table: if one sphere from the robot arm\u2019s link is found\nto be between the floor and the underside of a table, it\u2019s likely\nthat other spheres from that same link or other queries from\nsucceeding timesteps will also lie in that vertical layer.\nWhereas a grasped movable object is represented with a set\nof spheres (blue in Figure 3), a resting movable object is ap-\nproximated as an oriented box (orange). This is computed from\nthe YCB object\u2019s triangle mesh in a preprocess. At runtime, to\ndetect collisions between the robot (including grasped object, if\nany) and the resting movable objects, we perform sphere-versus-\nbox queries. There are generally 30 resting movable objects in\nthe environment (1 target object and 29 distractor objects) and\nwe need to avoid performing all 30 sphere-versus-box queries.\nSo, we use a \u201cregular grid\u201d acceleration structure to quickly\nretrieve a list of nearby resting objects.\nResting movable objects are inserted into a regular grid at\nepisode initialization. This is a dense 2D array spanning the\nXZ (ground) plane, with each cell storing a list of objects that\noverlap it. Objects will generally overlap multiple cells and thus\nbe present in the object lists of multiple cells. When an object is\ngrasped by the robot, it is removed from all relevant cells in the\nregular grid, and if the object is later dropped, it is re-inserted\ninto the regular grid at its new resting position.\nLet\u2019s consider how to find the list of nearby resting\nobjects for a given query sphere. The regular grid spacing is\nchosen such that cells are at least 4\u00d7 the largest radius in our\nsphere-radius working set (12 cm). A query sphere may overlap\nup to four adjacent cells in the regular grid, e.g. the sphere is\ncentered near the shared edge of two cells or the shared corner\nof four cells. A naive approach here would be to merge and de-\nduplicate the object lists of the four cells. We avoid this expense\nand instead maintain four separate regular grids, all spanning\nthe entire scene XZ extent, with carefully-chosen varying X and\nZ offsets for the cell boundaries. In this way, any query sphere\nis guaranteed to lie fully inside a single cell of one of these grids\n(not spanning a cell edge or corner). Thus, our list of nearby\nresting objects is simply the list stored in this cell; we don\u2019t\nhave to merge or de-duplicate multiple lists. Note this approach\nof four somewhat-redundant regular grids comes at the expense\nof extra memory and added insertion/removal compute time.\nH. Simulator Flexibility to new Assets\nGalactic can work with various assets (robots, scenes and\nobjects) from different sources. We use a mostly-automated\npipeline that includes optimizing assets for the batch renderer\nand generating collision geometry (see Appendix G). In Fig. 13\nwe added Stretch and Spot robots loaded in a scene from the\nMP3D dataset [41] with new objects.\nFigure 13. Galactic with an MP3D scene, Google Scanned Objects [13],\nStretch robot (left), Stretch debug viz (center), and Spot robot (right).\nI. Description of Heuristics\nI.1. Sliding Heuristic\nWe implement the robot sliding heuristic as the following\nsteps: (1) start from a candidate pose, (2) if the pose penetrates\nthe scene, compute a jitter direction in the ground plane (3)\njitter the robot base in the horizontal plane. Repeat from step\n1 until a penetration-free pose is found, up to 3 times. If this\nfails, the robot does not move on that step.\nI.2. Object Placing Heuristic\nThe object placement heuristic is as follows: (1) start from\na candidate pose, (2) if the pose penetrates the scene, compute\na jitter direction in the ground plane, with some randomness, (3)\njitter the dropped object and re-cast down to a support surface.\nRepeat from step 1 until a penetration-free pose is found, up to\n6 times. If this fails, we restore the dropped object to its resting\nposition prior to grasp. This approximates the dropped object\nbouncing or rolling away. Snap-to-surface is an instantaneous\noperation that resolves within one physics step; objects do not\nfall or settle over time.\n"
  },
  {
    "title": "Instant Multi-View Head Capture through Learnable Registration",
    "link": "https://arxiv.org/pdf/2306.07437.pdf",
    "upvote": "3",
    "text": "Instant Multi-View Head Capture through Learnable Registration\nTimo Bolkart1\nTianye Li2\nMichael J. Black1\n1MPI for Intelligent Systems, T\u00a8ubingen\n2University of Southern California\nFigure 1. Given calibrated multi-view images (top: 4 of 16 views; contrast enhanced for visualization), TEMPEH directly infers 3D head\nmeshes in dense semantic correspondence (bottom) in about 0.3 seconds. TEMPEH reconstructs heads with varying expressions (left)\nand head poses (right) for subjects unseen during training. Applied to multi-view video input, the frame-by-frame inferred meshes are\ntemporally coherent, making them directly applicable to full-head performance-capture applications. See Sup. Mat. for the video output.\nAbstract\nExisting methods for capturing datasets of 3D heads in\ndense semantic correspondence are slow and commonly ad-\ndress the problem in two separate steps; multi-view stereo\n(MVS) reconstruction followed by non-rigid registration. To\nsimplify this process, we introduce TEMPEH (Towards Es-\ntimation of 3D Meshes from Performances of Expressive\nHeads) to directly infer 3D heads in dense correspondence\nfrom calibrated multi-view images. Registering datasets of\n3D scans typically requires manual parameter tuning to find\nthe right balance between accurately fitting the scans\u2019 sur-\nfaces and being robust to scanning noise and outliers. In-\nstead, we propose to jointly register a 3D head dataset while\ntraining TEMPEH. Specifically, during training, we mini-\nmize a geometric loss commonly used for surface registra-\ntion, effectively leveraging TEMPEH as a regularizer. Our\nmulti-view head inference builds on a volumetric feature\nrepresentation that samples and fuses features from each\nview using camera calibration information. To account for\npartial occlusions and a large capture volume that enables\nhead movements, we use view- and surface-aware feature\nfusion, and a spatial transformer-based head localization\nmodule, respectively. We use raw MVS scans as supervision\nduring training, but, once trained, TEMPEH directly pre-\ndicts 3D heads in dense correspondence without requiring\nscans. Predicting one head takes about 0.3 seconds with a\nmedian reconstruction error of 0.26 mm, 64% lower than\nthe current state-of-the-art. This enables the efficient cap-\nture of large datasets containing multiple people and di-\nverse facial motions. Code, model, and data are publicly\navailable at https://tempeh.is.tue.mpg.de.\n1. Introduction\nCapturing large datasets containing 3D heads of multi-\nple people with varying facial expressions and head poses is\na key enabler for modeling and synthesizing realistic head\navatars. Typically, building such datasets is done in two\nsteps: unstructured 3D scans are captured with a calibrated\nmulti-view stereo (MVS) system, followed by a non-rigid\nregistration step to unify the mesh topology [23]. This two-\nstage process has major drawbacks. MVS reconstruction\nrequires cameras with strongly overlapping views and the\nresulting scans frequently contain holes and noise. Reg-\nistering a template mesh to these scans typically involves\nmanual parameter tuning to balance the trade-off between\naccurately fitting the scan\u2019s surface and being robust to scan\nartifacts. Both stages are computationally expensive, each\ntaking several minutes per scan. For professional captures,\nboth steps are augmented with manual clean-up to enhance\narXiv:2306.07437v1  [cs.CV]  12 Jun 2023\nthe quality of the output meshes [3, 67]. Such manual edit-\ning is infeasible for large-scale captures (\u226b 10K scans).\nInstead, we advocate for a more practical setting that di-\nrectly predicts 3D heads in dense correspondence from cal-\nibrated multi-view images, effectively bypassing the MVS\nstep. We achieve this with TEMPEH (Towards Estimation\nof 3D Meshes from Performances of Expressive Heads),\nwhich quickly (\u223c 0.3 seconds per head on a NVIDIA A100-\nSXM GPU) infers accurate 3D heads (\u223c 0.26 mm median\nerror) in correspondence, without manual user input.\nWhile several methods exist that directly recover 3D\nfaces in correspondence from calibrated multi-view im-\nages, they have high computational cost and require care-\nful selection of optimization parameters per capture subject\n[9, 14, 29, 62]. These remain major obstacles for large-\nscale data captures. A few learning-based methods directly\nregress parameters of a 3D morphable model (3DMM) [80]\nor iteratively refine 3DMM meshes from multi-view images\n[6]. As shown by Li et al. [50], this 3DMM dependency\nconstrains the quality and expressiveness of these methods.\nThe recent ToFu [50] method goes beyond these 3DMM-\nbased approaches with a volumetric feature sampling\nframework to infer face meshes from calibrated multi-\nview images.\nWhile demonstrating high-quality predic-\ntions, ToFu has several limitations. (a) The training is fully-\nsupervised with paired data of multi-view images and high-\nquality registered meshes; creating such data requires ex-\ntensive manual input. (b) Only the face region is predicted;\nears, neck, and the back of the head are manually completed\nin an additional fitting step. (c) Self-occlusions in scanner\nsetups designed to capture the entire head result in mediocre\npredictions due to the na\u00a8\u0131ve feature aggregation strategy that\nignores the surface visibility. (d) Only a small capture vol-\nume is supported and increasing the size of the capture vol-\nume to cover head movements reduces the accuracy.\nTEMPEH adapts ToFu\u2019s volumetric feature sampling\nframework but goes beyond it in several ways: (a) The train-\ning requires no manually curated data as we jointly optimize\nTEMPEH\u2019s weights and register the raw scans. Obtaining\nthe clean, registered meshes required by ToFu is a key prac-\ntical hurdle. TEMPEH learns from raw scans and is robust\nto their noise and missing data. This is done by directly\nminimizing the point-to-surface distance between scans and\npredicted meshes. (b) At run time the entire head is inferred\nfrom images alone and includes the ears, neck, and back of\nthe head. (c) The feature aggregation accounts for surface\nvisibility. (d) A spatial transformer module [42] localizes\nthe head in the feature volume to only sample regions rele-\nvant for prediction, improving the accuracy.\nIn summary, TEMPEH is the first framework to accu-\nrately capture the entire head from multi-view images at\nnear interactive rates. During training, TEMPEH jointly\nlearns to predict 3D heads from multi-view images, and reg-\nisters unstructured scans. Once trained, it only requires cal-\nibrated camera input and it generalizes to diverse extreme\nexpressions and head poses for subjects unseen during train-\ning (see Fig. 1). TEMPEH is trained and evaluated on a dy-\nnamic 3D head dataset of 95 subjects, each performing 28\nfacial motions, totalling about 600K 3D head meshes. The\nregistered dataset meshes, raw images, camera calibrations,\ntrained model, and training code are publicly available.\n2. Related work\nScan registration: Registering unstructured 3D scans to a\ncommon mesh topology has been extensively studied over\nthe last two decades since the work of Blanz and Vetter [11].\nFor a comprehensive overview, we refer the reader to the\nsurvey of Egger et al. [23]. Most prior methods non-rigidly\ndeform a template mesh [11, 13, 15, 43, 48, 49, 57, 65,\n79, 81] with a generalization of the rigid iterative closest\npoint (ICP) algorithm [10]. Existing methods mostly regis-\nter 3D scans of faces in a neutral expression [11, 13, 57],\na few static expressions [4, 65], or faces in motion [1].\nBolkart and Wuhrer [12] and Zhang et al. [83] jointly reg-\nister static datasets of 3D faces while building a 3DMM.\nOnly a few methods consider more than just the face by\nregistering scans of entire heads [16, 17, 49, 81]. All these\nmethods have in common that they are optimization-based,\nwhich makes them slow and sensitive to scans with noise or\nholes. Dealing with this to obtain accurate results requires\nmanual parameter tuning. Few learning-based methods ex-\nist to directly go from a scan to a registered mesh [5, 52, 84].\nWhile registering scans faster than previous optimization-\nbased methods, these methods only register tightly cropped\nfaces (i.e., no neck, ears, back of the head, etc.), they re-\nquire facial landmarks, and scan noise can negatively im-\npact the reconstructed meshes. TEMPEH takes inspiration\nfrom these scan registration methods by minimizing sim-\nilar objective functions (namely, point-to-surface distance\nand an edge-based surface regularization [49]). However,\nin contrast to these registration methods, we jointly register\nthe training scans and train TEMPEH. Once trained, TEM-\nPEH requires no scans as input, but, instead, directly infers\n3D heads in correspondence from multi-view images.\nImage-based reconstruction:\nUndoubtedly, monocular\nimages or videos have drawn the largest focus as input\nto 3D face reconstruction methods and we refer to recent\nsurveys for a thorough overview [56, 88].\nMost single-\nimage-based 3D face reconstruction methods either fit some\n3DMM to an image [2, 8, 11, 31, 59, 74, 78] in an analysis-\nby-synthesis fashion or regress the parameters of a 3DMM\n[20, 24, 47, 72, 73].\nReconstructing 3D from monoc-\nular images or videos is an ill-posed problem, as many\n3D reconstructions give the same image when projected\nto 2D. To improve reconstruction accuracy, existing meth-\nods leverage multi-image constraints (e.g., same identity\nacross images) [18, 25, 27, 66, 75] or multi-view con-\nstraints [68] during training. Others use paired image-3D\ndata for training, obtained by fitting a 3DMM to images\n[26, 37, 41, 55, 77] or by sampling a 3DMM to generate\nsynthetic data [22, 33, 61]. The state-of-the-art in single-\nimage face reconstruction leverages a face recognition net-\nwork trained on large amounts of image data, combined\nwith supervised training from paired 2D-3D data obtained\nby registering 3D scans [87].\nAt test time, these meth-\nods reconstruct 3D faces from a single image without prior\nknowledge about the camera, image resolution, lighting,\netc., making the problem ill-posed. The ambiguity between\n3D face shape, camera, and distance to the camera limits\nthe metric accuracy of their 3D reconstructions [7].\nInstead, several methods focus on a more constrained\nscenario by reconstructing 3D faces from collections of im-\nages instead of just a single image. Such methods optimize\nthe parameters of a parametric model [78] or non-rigidly\ndeform a template mesh [6, 30, 45, 46, 64, 70] to fit mul-\ntiple images of one subject. Learning-based methods in-\ndependently regress 3DMM parameters from multiple im-\nages of the same subject and fuse the identity shape param-\neters to obtain coherent identity shape parameters per sub-\nject [21, 60, 71, 80]. As camera intrinsics are unknown for\nthese image collections, the ambiguity between (unknown)\nfocal length and object scale means that the reconstructed\nfaces are not metrically accurate. Further, the approximate\nassumption about the camera (typically weak perspective\nprojection) and the ambiguity of identity and expression-\ndependent shape result in reconstruction errors for each im-\nage. This limits the overall reconstruction accuracy when\nintegrating the erroneous results across images (see Li et\nal. [50] for comparisons). TEMPEH instead leverages cam-\nera calibration information to reconstruct metrically accu-\nrate 3D heads, and it is designed for a multi-view setup\n(i.e., multiple time-synchronized images per expression) to\ndisambiguate identity and expression shape variations.\nFew methods directly reconstruct 3D faces from cal-\nibrated multi-view images.\nAmong these, optimization-\nbased approaches [9, 14, 29, 62] to date achieve the most\nimpressive results with fine-scale geometric details, but at\nthe cost of being computationally slow, and requiring care-\nfully tuned parameters per subject. As these methods are\ntailored towards specific custom capture setups, they cannot\nbe directly applied to our off-the-shelf active stereo system.\nThe recent ToFu method [50] directly predicts 3D faces in\ncorrespondence, parameter-free at test time, and at near in-\nteractive rates. However, ToFu is trained fully-supervised\nfrom high-quality registered 3D faces, which are difficult\nto obtain and limit the applicability of ToFu to new capture\nsetups. Further, its na\u00a8\u0131ve multi-view feature integration ne-\nglects surface visibility, limiting generalization to 360\u25e6 cap-\nture setups as, e.g., features from the back of the head con-\ntribute to reconstructing face vertices and vice versa. Addi-\ntionally, it assumes the capture volume tightly encapsulates\nthe face, leading to mediocre results for larger capture vol-\numes, required for covering moving heads. ToFu is there-\nfore only able to capture tightly cropped faces, while the\nrest of the head is completed in a manual post-processing\nstep. We build on top of ToFu\u2019s design but to overcome its\nlimitations, (1) we train our model directly from raw scans,\nwhich makes it easier to adapt to new capture setups, (2)\nour feature integration considers surface properties and vis-\nibility, and (3) we localize the head in the feature volume\nwith a spatial transformer. These changes improve the re-\nconstruction quality and enable us to infer entire heads.\nMulti-view stereo: MVS capture systems are commonly\nused to reconstruct 3D faces [9, 34, 35, 54]. While recon-\nstructing high-quality geometry, these methods are compu-\ntationally expensive due to the pairwise feature matching\nacross views. Recent learning-based methods [36, 39, 44,\n69, 82] reduce this computational cost but lose accuracy.\nFor an overview of learning-based MVS methods, see the\nsurvey of Wang et al. [76]. All these methods have in com-\nmon that the reconstructed geometry is unstructured, while\nour goal is to reconstruct meshes in correspondence. How-\never, we use a commercial MVS method to generate un-\nstructured 3D head scans for our training data, and use these\nscans as a supervision signal. Once trained, TEMPEH di-\nrectly predicts head meshes in correspondence from multi-\nview images without requiring MVS scans.\n3. Method\nGiven sets of images {Ii \u2208 Rw\u00d7h\u00d73}k\ni=1 and camera\ncalibrations {Ci}k\ni=1 (i.e., camera intrinsics, extrinsics, and\nradial distortion parameters) of a multi-view capture system\nwith k views, TEMPEH infers a head mesh Mr := (Vr, T)\nwith vertices Vr \u2208 Rnv\u00d73 and faces T \u2208 Rnf \u00d73. Here, nv\nand nf are the number of vertices and faces, respectively.\nNote that nv and T are fixed, as all meshes output for dif-\nferent sets of input images are in dense correspondence.\nAs outlined in Fig. 2, TEMPEH consists of two stages,\na coarse head inference stage, which outputs an intermedi-\nate 3D head Mc := (Vc, T) with vertices Vc \u2208 Rnv\u00d73,\nfollowed by a refinement stage that updates all vertex loca-\ntions and outputs the final mesh Mr. This two stage process\nallows us to leverage the surface properties of Mc (i.e., ver-\ntex position, visibility and normals) in the second stage for\nmulti-view feature aggregation and vertex refinement.\n3.1. Coarse head prediction\nFeature extraction:\nEach image Ii is processed by a\nshared convolutional U-Net Fimg(Ii) \u2192 Fi \u2208 Rw\u00d7h\u00d7df\nto extract 2D feature maps Fi with df feature channels.\nVolumetric feature sampling: Following Li et al. [50],\nall 2D feature maps are then unprojected and fused into\nFeature extraction\nHead localization\nHead inference\nHead re\ufb01nement\nCoarse mesh\nRe\ufb01ned mesh\nScan\nScale s  Rotation r\nTranslation t\n0\n2\n4\n6\n8\n10\n11\n12\n13\n14\nCoarse and re\ufb01nement\nfeature maps\nInput images\n<\nl\na\nt\ne\nx\ni\nt\n \ns\nh\na\n1\n_\nb\na\ns\ne\n6\n4\n=\n\"\nm\n3\nB\nA\n+\nF\nc\nd\nI\nt\nN\n2\nu\nt\n3\nQ\nb\nL\n1\nm\nD\nd\nZ\n1\nY\n9\n4\n=\n\"\n>\nA\nA\nA\nC\nB\nn\ni\nc\nb\nV\nD\nL\nS\ns\nN\nA\nF\nJ\n3\n4\nr\nP\nU\nV\nd\nS\nn\nC\nY\nB\nF\nc\nl\nU\nS\nK\nd\ni\nM\nU\n3\ne\ni\nu\ng\nn\n1\nA\nE\n8\nN\nk\nO\nm\nm\nH\nT\ni\nZ\nh\nZ\ni\nK\nU\nk\nJ\nU\nb\nf\n8\nW\nN\nC\n0\nX\nc\n+\ng\n3\nu\n/\nB\ns\nn\na\nQ\nt\na\nP\nT\nD\nD\n4\nZ\nx\n7\nu\nf\nc\ne\nP\n2\nZ\nU\nK\ns\nv\n6\nM\nh\nY\nW\nl\n5\nZ\nX\nV\nk\nt\nr\n5\nf\nW\nN\nz\na\n1\nt\nc\n2\ne\n3\nL\na\nN\nE\nY\nN\nL\nC\nE\nY\nt\nE\n1\n0\ne\nS\nM\nM\np\nJ\nS\n1\nH\nF\nS\nD\nc\nW\nB\nI\nU\n+\nI\nx\n1\n/\nd\nJ\nn\n7\nn\nX\ns\ni\nJ\nI\n3\n4\nr\nR\nr\nH\nx\nA\n3\nR\ng\nN\nO\nA\nY\nq\nS\n0\n5\nJ\nk\nH\nT\nu\nq\nE\nS\nA\n0\nx\nY\nu\nl\n1\n5\nq\nU\n0\nc\n/\nL\n/\n3\nM\n7\nu\n0\nl\nH\nm\nm\nR\nW\nr\na\nh\nW\nA\nf\n4\nk\n9\nJ\nR\nU\nw\nR\nd\nM\nz\nP\n5\n1\n+\nh\nJ\nO\nQ\nc\nI\nU\nZ\nk\nr\nJ\nn\nW\n7\nF\ny\nU\ny\nQ\nU\nx\nY\nx\nk\nZ\nS\ne\nR\nJ\nE\nZ\n4\nh\nA\na\nk\np\ny\nl\nH\nI\nZ\nF\nu\nW\np\ny\nR\nw\nS\nO\nt\n9\nG\nE\nQ\nC\nf\n2\n4\ng\no\nX\n6\ns\ny\nN\nF\no\nZ\nT\nj\n0\nN\ne\nV\n+\nc\np\ny\n3\ns\nv\nF\n/\n7\nx\ne\no\no\nK\n6\nm\n1\nI\ne\nJ\n4\np\nw\nP\nB\nk\nU\nJ\nA\ny\nq\nC\nO\na\nZ\nw\nD\n4\nV\nB\nC\ns\n2\n1\ng\nR\nh\nQ\nf\nW\nu\nE\nA\n+\nR\nQ\nF\nj\np\n5\nM\np\nF\nC\nP\nX\nZ\n7\nf\nN\nk\nF\nk\nL\n7\np\nG\nq\nf\nV\nm\ns\n3\nt\nU\nr\nj\nY\nh\np\nH\nC\ne\ny\nD\nQ\n3\nA\nM\nb\nH\nA\nG\nG\nu\nA\nK\nN\nE\nE\nL\nY\nP\nA\nA\nn\ns\nA\nL\ne\nD\nU\ne\nj\nW\nf\nj\nz\nX\ni\nf\nl\nC\n4\nY\n0\n5\n4\n9\n8\nA\nv\nG\nx\nz\nd\nZ\nH\n5\nn\nB\n<\n/\nl\na\nt\ne\nx\ni\nt\n>\n{Ii}k\ni=1\n<\nl\na\nt\ne\nx\ni\nt\n \ns\nh\na\n1\n_\nb\na\ns\ne\n6\n4\n=\n\"\nh\nV\n1\nl\n1\n5\nF\nj\nS\nB\nX\nB\nf\nq\n6\ne\nK\nJ\nP\nl\na\n/\na\n8\ne\n8\ng\n=\n\"\n>\nA\nA\nA\nC\nB\nn\ni\nc\nb\nV\nD\nL\nS\ns\nN\nA\nF\nJ\n3\n4\nr\nP\nU\nV\nd\nS\nn\nC\nY\nB\nF\nc\nl\nU\nS\nK\nd\ni\nM\nU\nB\nX\nF\nZ\nw\nT\n6\ng\ni\nW\nE\ny\nn\nb\nR\nD\nJ\n5\nM\nw\nM\nx\nF\nK\ny\nM\nq\nN\nv\n+\nL\nG\nh\nS\nJ\nu\n/\nQ\nZ\n3\n/\no\n2\nT\nt\nA\nW\nt\nH\np\nj\nh\nc\nM\n6\n9\n3\nH\nu\nP\nH\nz\nM\nq\nl\nW\nV\n9\nG\nQ\nu\nL\nS\n8\ns\nr\nq\n6\nW\n1\n8\nv\nr\nG\n5\nt\na\n2\nu\nb\nP\nb\nl\nl\nE\ni\nM\nG\nn\nh\ni\nE\nW\ni\n6\ny\nN\nJ\nG\nO\nW\nk\np\na\nh\ni\np\nB\ns\nL\ng\nk\nK\nf\nk\nY\n4\n/\nu\ns\nz\n9\nz\nj\n0\nR\nk\nk\nb\n8\nV\no\n1\nj\n4\no\nZ\no\nw\nG\nl\nA\nM\nV\nJ\na\n8\ns\nw\nD\nJ\n3\nV\nC\np\nI\nY\nY\ns\nf\nQ\nq\n8\n1\nK\na\nO\nf\nl\n/\nb\nm\nd\n3\n6\nS\nj\nz\nz\nI\np\nV\nt\nQ\nr\nA\nv\n8\nS\ne\nk\ng\nq\nY\no\nu\nm\nZ\nn\n0\n4\n/\nw\nk\nl\nI\nu\nM\nI\nM\nS\nd\nm\nz\nr\nV\ni\n5\nK\nR\nK\nK\nY\nk\na\ny\ns\np\nN\nI\nE\ni\nM\n8\nQ\ng\nP\nS\n0\n5\nS\nj\nk\nE\ng\n3\nL\nc\n7\nI\n4\nJ\nF\nW\n+\nj\nC\nI\nh\nH\n5\nc\nw\nU\nL\n9\n2\nZ\nG\ni\nU\nM\np\nx\n6\nO\nv\nK\nf\nG\nU\n5\n7\n+\nX\ni\nf\n1\n4\nv\nU\nU\nH\nd\nT\nS\nm\nP\nE\n0\nU\n4\nn\ng\nw\nK\nE\ng\nZ\nV\nB\nP\nN\nM\nY\nJ\n8\nK\ng\nh\nU\nb\na\n4\nK\nw\no\nH\np\nX\ni\nI\nd\nI\nI\nK\nx\n0\nc\nu\nU\ni\nh\nP\nr\ns\n9\nn\nk\ny\nC\n6\nF\n9\nU\nr\nV\nP\nq\n7\nW\nb\nW\nq\nV\nx\nM\nY\n2\nj\nB\nP\nb\nB\nI\nT\ng\nG\nN\nj\ng\nD\nD\nX\nA\nN\nm\nq\nA\nF\nM\nH\ng\nA\nT\n+\nA\nF\nv\nB\nq\nP\nx\nr\nP\nx\nZ\nr\nx\nP\nS\nh\ne\nM\na\nc\n8\ne\n+\nA\nX\nj\n4\nx\nt\nU\nY\nJ\nm\n+\n<\n/\nl\na\nt\ne\nx\ni\nt\n>\n{Fi}k\ni=1\n(2D)\nVolume feature\nGrid\n<\nl\na\nt\ne\nx\ni\nt\n \ns\nh\na\n1\n_\nb\na\ns\ne\n6\n4\n=\n\"\nr\nI\nP\nz\ni\nG\nq\nI\nf\ns\n2\nI\nB\n6\nT\nk\n7\nB\nz\nq\n0\nd\n2\n/\nw\nk\nI\n=\n\"\n>\nA\nA\nA\nB\n+\nH\ni\nc\nb\nV\nD\nL\nS\ns\nN\nA\nF\nL\n2\np\nr\n1\no\nf\nj\nb\np\n0\nE\ny\ny\nC\nq\n5\nJ\nI\n0\nS\n6\nL\nb\nl\ny\n2\nY\nB\n/\nQ\nh\nj\nC\nZ\nT\nt\nq\nh\nk\n0\nm\nY\nm\nQ\ng\n1\n9\nE\nv\nc\nu\nF\nD\nE\nr\nZ\n/\ni\nz\nr\n9\nx\nk\nq\na\ng\n1\nQ\nM\nD\nh\n3\nP\nu\n5\nZ\n4\n5\nf\ns\ny\no\nV\nL\nb\n9\nZ\nZ\nQ\n2\nN\nr\ne\n2\nd\n8\nq\n7\nl\nb\n3\n9\ng\n8\nO\nq\ne\nX\nT\nc\nk\n1\nE\ni\nM\nO\nn\ni\ni\nE\nV\ni\n4\nC\nN\nJ\nG\nO\nW\nk\nq\n6\nh\ni\nZ\nB\nA\nL\ng\nk\nK\nf\nk\nb\n4\n/\nu\n8\n3\n8\n/\ng\nM\nR\nk\nk\nb\n8\nX\ns\n1\nj\n4\no\nZ\no\nw\nm\nl\nA\nM\nV\nJ\na\n8\ns\nz\nq\nK\nE\nR\nq\ni\nh\nF\nL\nO\nw\ns\nv\nx\nQ\nv\nP\nr\nN\nl\n1\nO\n4\nf\n1\nl\nz\ng\nF\nq\nU\nG\nB\nt\nm\nd\n+\nj\ns\nY\nR\nT\nk\nL\nC\nF\nW\nZ\nI\ny\nq\nF\nj\nx\n8\np\nN\nk\nV\nA\nU\nM\n7\nK\no\nj\nB\nJ\nJ\nY\no\nR\nn\na\nE\nK\nG\nm\nn\nI\nU\nE\nu\nm\nm\ne\nf\nC\nF\nd\na\n6\nV\ns\nR\nV\nE\nQ\nj\n+\nu\nr\nF\nz\n9\nu\nZ\nG\ni\nU\nM\np\n5\n6\nO\nv\nJ\nL\nK\nZ\nc\n9\nz\nL\nx\nP\n2\n+\nY\nq\nK\nD\np\np\np\nT\nH\ni\nS\nI\nc\nL\nw\n8\nF\nC\nb\nN\nU\nZ\nG\nU\nt\nW\nG\nM\nq\nC\nF\nZ\ns\nr\ng\nn\nC\ng\nu\nq\ns\nF\np\n4\ni\ng\nb\nD\nS\nX\nV\nX\ny\nE\np\nq\nr\nv\n6\n+\nT\nV\nQ\nm\n9\ny\n7\np\nz\nV\nW\n9\n0\nG\nr\nX\nW\nT\nV\nF\nH\nG\nU\n7\nh\nD\nC\n7\nA\ng\nW\nt\no\nw\nR\n2\n0\no\nQ\ns\nY\nE\nn\ni\nC\nF\n3\ng\n1\nH\no\n1\nn\n4\n8\n1\n4\nX\n4\n6\nW\nj\nG\nL\nn\nB\nH\n7\nB\n+\nP\ng\nG\nU\nu\n2\nT\nm\nA\n=\n=\n<\n/\nl\na\nt\ne\nx\ni\nt\n>\nQc\n<\nl\na\nt\ne\nx\ni\nt\n \ns\nh\na\n1\n_\nb\na\ns\ne\n6\n4\n=\n\"\nF\nv\n7\nZ\n3\ni\nr\nh\n5\nq\nz\nx\n8\nn\na\nQ\nA\na\na\nr\nf\nh\nJ\nM\nX\n2\nI\n=\n\"\n>\nA\nA\nA\nB\n+\nH\ni\nc\nb\nV\nD\nL\nS\ns\nN\nA\nF\nL\n2\np\nr\n1\no\nf\nj\nb\np\n0\nM\n1\ng\nE\nV\ny\nW\nV\no\nl\n0\nW\nX\ne\ni\ny\ng\nn\n1\nA\nG\n8\nJ\nk\nO\nm\nm\nH\nT\ni\nZ\nh\nZ\ni\nL\nU\nk\nC\n9\nx\n4\n0\nI\nR\nt\n3\n6\nK\nO\n/\n/\nG\nJ\nE\n1\nB\nq\nw\nc\nG\nD\nu\nf\nc\ny\nz\n1\nz\n3\nJ\nA\nz\np\nS\n3\nr\ny\ny\ni\nt\nr\nW\n9\ns\nb\np\nW\n3\nK\nz\nu\n7\ne\n/\nt\nV\n8\n+\nC\nw\np\n4\nJ\nI\nE\nt\no\nl\nA\nQ\n/\nk\nw\nM\nW\nK\nc\ni\nZ\no\nV\nz\nP\nN\n6\nS\nC\nU\nF\nP\ns\nu\np\n3\n1\n3\nd\np\n3\n5\n/\nQ\nc\nq\nF\nQ\nv\nE\nv\nZ\n6\nH\n1\nP\nb\nx\nR\nD\nC\nP\nE\na\nx\nT\ny\nT\nG\nr\nI\nx\n/\nr\nK\nc\nE\n8\nv\nk\nm\nc\nm\nC\nS\nO\nW\nb\nP\nq\nV\ng\n7\n0\nl\nz\nQ\nK\nU\no\nM\nC\nH\nc\nf\n8\nH\nI\n0\nD\nE\nv\nl\nU\na\nM\nK\nx\nU\ns\nO\nG\nF\nW\no\n7\nx\nl\nI\nz\nw\nm\nl\nS\nG\nU\nW\nK\nh\np\nj\nM\n8\nI\nQ\nO\nU\ny\nq\nw\nT\n5\nU\nd\n5\n8\nE\nT\nd\nJ\no\nq\nY\n+\nQ\nF\nM\nn\n1\nC\no\n1\nz\n9\nu\nR\nF\nj\nX\n6\nm\n5\n7\n6\na\nT\nW\nU\ny\n1\n6\nm\nX\ni\nf\n9\n4\nw\n0\nl\n7\nL\nj\np\nk\nI\nI\n0\n0\nF\nW\nR\nz\ny\nI\no\n5\n0\ng\nL\nI\nW\n0\nJ\nh\nJ\nS\nj\nS\nf\np\nw\nQ\nT\ny\nd\nK\ns\ni\nE\ny\nx\nx\nE\nS\nn\nX\nV\nX\ny\nE\nl\nr\nL\nv\n6\n+\nS\nZ\nQ\nm\n9\n8\n3\nr\nj\no\nt\n6\n8\na\n9\nb\na\nV\n0\nU\nd\nZ\nT\ni\nG\nE\nz\ni\nD\nB\nl\nx\nC\nG\n2\n6\nh\nA\n1\n0\ng\nE\nM\nE\nT\nv\nM\nC\nr\n8\nW\ng\n8\nG\n2\n/\nG\n+\n2\nK\n0\nZ\nB\nQ\n7\nR\n/\nA\nL\nx\ns\nc\n3\nQ\n5\nO\nT\nj\ng\n=\n=\n<\n/\nl\na\nt\ne\nx\ni\nt\n>\nGc\nVolume feature\nGrid\nSpatial\ntransformer\n<\nl\na\nt\ne\nx\ni\nt\n \ns\nh\na\n1\n_\nb\na\ns\ne\n6\n4\n=\n\"\nm\n9\nH\nl\ng\n3\n7\ny\np\nG\ny\nf\nN\n2\nV\nG\nM\nN\nR\nS\nt\ni\nA\nf\n0\nz\ns\n=\n\"\n>\nA\nA\nA\nB\n+\nX\ni\nc\nb\nV\nD\nL\nS\ns\nN\nA\nF\nL\n2\np\nr\n1\np\nf\nU\nZ\nd\nu\ng\nk\nV\n0\nV\nV\nI\np\n2\nm\nX\nR\nh\nS\n4\nr\n2\nA\ne\n0\nI\nU\ny\nm\nk\n3\nb\no\nZ\nB\nJ\nm\nJ\no\nU\nS\n8\ni\nd\nu\nX\nC\nj\ni\n1\nj\n9\nx\n5\n9\n8\n4\nS\nV\nP\nQ\n6\no\nG\nB\nw\nz\nn\n3\nc\ns\n8\nc\nL\n2\nJ\nU\nK\nt\nv\n+\nM\nk\np\nr\n6\nx\nu\nb\nW\n+\nX\nt\ny\ns\n7\nu\n3\nv\n6\nB\ne\nX\nj\nU\nl\nW\nE\ns\nM\nO\nn\ng\nk\nI\nW\ni\n7\ny\nF\nJ\nG\nO\nW\nk\no\n6\nh\ni\np\nB\n8\nJ\ng\ng\nK\nP\nk\nZ\n4\n3\nv\nc\n3\n8\n3\no\nw\nI\nS\nU\nP\n+\nq\nO\nY\nR\nc\nQ\nI\n0\n5\nt\nS\nn\nG\nC\nk\nt\nu\na\nY\n5\nD\nJ\nC\na\nY\nM\nS\nS\nu\n9\nR\nN\nc\nH\nr\nu\nm\nl\nW\n7\nZ\nu\ne\nw\n/\np\nJ\n6\nQ\na\np\nQ\no\nO\n2\na\nn\n8\nN\nR\ni\nO\nO\nA\nc\nI\nU\nZ\nk\nn\nJ\nQ\nt\ny\nP\nl\nJ\nE\ng\no\ni\nh\nl\nJ\nK\n8\nN\nY\nk\ng\nj\nh\nK\nR\nq\nT\ng\na\nY\nc\nB\nU\nQ\n6\nS\nZ\n4\n8\nt\nc\n6\n0\nM\nr\nL\n8\nU\nO\nj\nH\nl\nZ\nW\nr\nP\nz\nc\nS\nF\nE\ng\n5\nD\nz\nw\n9\nm\ne\nW\nU\nq\n1\n4\nm\n/\nu\nc\nN\nY\nu\nU\n3\nn\nY\nT\ny\nK\nF\na\nE\n4\n8\nU\nh\nP\n2\na\nW\nC\nq\n2\ns\nB\nm\nt\nE\nB\nc\nG\nK\nz\nT\nV\nB\nW\nF\nC\nd\n1\nc\nI\nT\nJ\nB\nB\nW\nu\nq\nx\nK\nX\nk\nJ\nz\n+\nf\nd\nV\ns\ni\ny\nh\ne\n1\nm\nr\nX\n9\nU\na\nD\n4\n1\nq\n6\n6\na\no\no\nw\nw\nn\nc\nA\no\nX\nU\nI\nd\nr\na\nM\nE\n9\nt\nK\nE\nD\nG\nG\nb\nw\nB\nC\n/\nw\na\ni\nT\nG\ns\n/\nF\nm\nv\nC\n9\nG\nS\n0\na\nx\nc\nw\ny\n/\nY\nH\nx\n8\nA\n6\nn\n9\nk\n7\n8\n=\n<\n/\nl\na\nt\ne\nx\ni\nt\n>\nG0\nc\n<\nl\na\nt\ne\nx\ni\nt\n \ns\nh\na\n1\n_\nb\na\ns\ne\n6\n4\n=\n\"\nI\nZ\nb\nJ\nj\n7\nv\n4\nW\n7\nf\nA\n4\nY\nQ\nH\np\nV\nq\n/\nm\no\nu\n4\nZ\nX\nA\n=\n\"\n>\nA\nA\nA\nB\n+\nX\ni\nc\nb\nV\nD\nL\nS\ns\nN\nA\nF\nL\n2\np\nr\n1\np\nf\nU\nZ\nd\nu\nB\no\nv\no\nq\nq\nR\nS\nt\nM\nu\ni\nG\n5\nc\nt\n2\nA\ne\n0\nI\nU\ny\nm\nk\n3\nb\no\nZ\nB\nJ\nm\nJ\no\nU\nS\n8\ni\nd\nu\nX\nC\nj\ni\n1\nj\n9\nx\n5\n9\n+\nY\np\nC\nl\no\n9\nc\nD\nA\n4\nZ\nx\n7\nu\nW\ne\nO\nG\n3\nK\nm\nt\nG\nV\n9\nG\na\nW\nN\nz\na\n3\nt\nn\nf\nJ\nu\nZ\nW\n/\n/\n4\nP\nD\nI\nP\nD\n7\np\nq\nS\nC\nS\nh\nH\nZ\nJ\nw\nA\nM\n5\nc\nL\nG\ni\nn\nA\nn\na\n1\nU\nx\nz\nO\ng\ng\nl\nx\nb\n7\nL\na\nd\n+\nd\n3\nW\nd\n+\nf\n0\n6\nl\nY\no\nF\n4\n1\nI\nu\nQ\n2\nj\n6\ne\nC\nO\nY\nx\ng\nn\nU\nq\nO\na\nY\n5\n8\nr\nG\ne\nE\ns\nz\nj\nT\nu\nL\nE\nJ\nL\nl\n0\nz\nK\np\nV\ns\n3\nK\ng\nv\n6\nR\ne\nk\nC\no\nU\na\nD\nv\nm\n5\n2\ng\nc\nk\nM\ni\nn\nQ\nh\nO\nO\nl\nR\nr\nW\nr\nV\nD\nb\nM\nZ\na\na\nE\nU\n6\nT\ny\ni\nh\nS\nN\nM\nR\nk\nh\ni\nd\n0\nm\nF\nK\nB\nf\na\nr\ns\nO\nE\n+\ne\no\nI\nt\nU\nG\nS\nM\nv\nk\nO\nk\nT\nG\nu\nX\nq\nz\n4\n0\nY\n+\n0\no\nt\nf\nD\ne\nd\nz\nH\nK\nq\nd\nS\n8\nT\n/\n/\nO\nG\nk\nf\na\na\nd\ns\nx\nE\nG\nG\nk\nq\ny\nP\nK\nQ\nF\n3\nG\nk\nA\n5\nT\nV\ng\nM\nZ\nM\nU\nq\nL\n5\nI\ni\nW\nY\nS\nJ\nZ\nm\nR\nW\nS\nK\nJ\nS\nY\n6\nL\na\nu\nS\nl\n9\nB\nc\n/\nX\n2\nd\nr\nE\nr\no\nX\nd\nf\nq\nN\n7\nV\nG\np\n1\nF\nt\n3\nR\nV\n1\nl\nO\nE\nM\nz\nu\nE\nK\n6\nn\nA\nL\nL\nX\ni\nA\nN\nn\nS\nB\nw\nB\ny\ne\n4\nA\nV\ne\nj\nd\nh\n4\nN\nt\n6\nM\n9\n+\nV\no\ny\nS\nh\n2\nT\nu\nE\nX\nj\nI\n9\nv\nu\nW\nG\nT\ny\nQ\n=\n=\n<\n/\nl\na\nt\ne\nx\ni\nt\n>\nQ0\nc\nVolume feature\nLocal grid\n<\nl\na\nt\ne\nx\ni\nt\n \ns\nh\na\n1\n_\nb\na\ns\ne\n6\n4\n=\n\"\nI\nZ\nY\nS\no\nt\na\n6\ni\n0\n5\ng\nH\nr\nO\nk\nc\nB\ni\ns\ns\n2\nO\n0\nt\n8\n8\n=\n\"\n>\nA\nA\nA\nB\n+\nH\ni\nc\nb\nV\nD\nL\nS\ns\nN\nA\nF\nL\n2\np\nr\n1\no\nf\nj\nb\np\n0\nE\ny\ny\nC\nq\n5\nJ\nI\n0\nS\n6\nL\nb\nl\ny\n2\nY\nB\n/\nQ\nh\nj\nC\nZ\nT\nt\nq\nh\nk\n0\nm\nY\nm\nQ\ng\n1\n9\nE\nv\nc\nu\nF\nD\nE\nr\nZ\n/\ni\nz\nr\n9\nx\nk\nq\na\ng\n1\nQ\nM\nD\nh\n3\nP\nu\n5\nZ\n4\n5\nf\ns\ny\no\nV\nL\nb\n9\nZ\nZ\nQ\n2\nN\nr\ne\n2\nd\n8\nq\n7\nl\nb\n3\n9\ng\n8\nO\nq\ne\nX\nT\nc\nk\n1\nE\ni\nM\nO\nn\ni\ni\nE\nV\ni\n4\nC\nN\nJ\nG\nO\nW\nk\nq\n6\nh\ni\nZ\nB\nA\nL\ng\nk\nK\nf\nk\nb\n4\n/\nu\n8\n3\n8\n/\ng\nM\nR\nk\nk\nb\n8\nX\ns\n1\nj\n4\no\nZ\no\nw\nm\nl\nA\nM\nV\nJ\na\n8\ns\nz\nq\nK\nE\nR\nq\ni\nh\nF\nL\nO\nw\ns\nv\nF\nQ\nv\nP\nr\nN\nl\n1\nO\n4\nf\n1\nl\nz\ng\nF\nq\nU\nG\nB\nt\nm\nd\n+\nj\ns\nY\nR\nT\nk\nL\nC\nF\nW\nZ\nI\ny\nq\nF\nj\nx\n8\np\nN\nk\nV\nA\nU\nM\n7\nK\no\nj\nB\nJ\nJ\nY\no\nR\nn\na\nE\nK\nG\nm\nn\nI\nU\nE\nu\nm\nm\ne\nf\nC\nF\nd\na\n6\nV\ns\nR\nV\nE\nQ\nj\n+\nu\nr\nF\nz\n9\nu\nZ\nG\ni\nU\nM\np\n5\n6\nO\nv\nJ\nL\nK\nZ\nc\n9\nz\nL\nx\nP\n2\n+\nY\nq\nK\nD\np\np\np\nT\nH\ni\nS\nI\nc\nL\nw\n8\nF\nC\nb\nN\nU\nZ\nG\nU\nt\nW\nG\nM\nq\nC\nF\nZ\ns\nr\ng\nn\nC\ng\nu\nq\ns\nF\np\n4\ni\ng\nb\nD\nS\nX\nV\nX\ny\nE\np\nq\nr\nv\n6\n+\nT\nV\nQ\nm\n9\ny\n7\np\nz\nV\nW\n9\n0\nG\nr\nX\nW\nT\nV\nF\nH\nG\nU\n7\nh\nD\nC\n7\nA\ng\nW\nt\no\nw\nR\n2\n0\no\nQ\ns\nY\nE\nn\ni\nC\nF\n3\ng\n1\nH\no\n1\nn\n4\n8\n1\n4\nX\n4\n6\nW\nj\nG\nL\nn\nB\nH\n7\nB\n+\nP\ng\nG\na\nb\ni\nT\np\nw\n=\n=\n<\n/\nl\na\nt\ne\nx\ni\nt\n>\nQr\n<\nl\na\nt\ne\nx\ni\nt\n \ns\nh\na\n1\n_\nb\na\ns\ne\n6\n4\n=\n\"\nj\nG\nN\nM\n7\n5\ns\nX\nk\n9\ny\nY\ni\n0\nz\nY\nT\nW\nD\nC\nh\nL\nY\n4\n8\nB\nw\n=\n\"\n>\nA\nA\nA\nB\n+\nH\ni\nc\nb\nV\nD\nL\nS\ns\nN\nA\nF\nL\n2\np\nr\n1\no\nf\nj\nb\np\n0\nM\n1\ng\nE\nV\ny\nW\nV\no\nl\n0\nW\nX\ne\ni\ny\ng\nn\n1\nA\nG\n8\nJ\nk\nO\nm\nm\nH\nT\ni\nZ\nh\nZ\ni\nL\nU\nk\nC\n9\nx\n4\n0\nI\nR\nt\n3\n6\nK\nO\n/\n/\nG\nJ\nE\n1\nB\nq\nw\nc\nG\nD\nu\nf\nc\ny\nz\n1\nz\n3\nJ\nA\nz\np\nS\n3\nr\ny\ny\ni\nt\nr\nW\n9\ns\nb\np\nW\n3\nK\nz\nu\n7\ne\n/\nt\nV\n8\n+\nC\nw\np\n4\nJ\nI\nE\nt\no\nl\nA\nQ\n/\nk\nw\nM\nW\nK\nc\ni\nZ\no\nV\nz\nP\nN\n6\nS\nC\nU\nF\nP\ns\nu\np\n3\n1\n3\nd\np\n3\n5\n/\nQ\nc\nq\nF\nQ\nv\nE\nv\nZ\n6\nH\n1\nP\nb\nx\nR\nD\nC\nP\nE\na\nx\nT\ny\nT\nG\nr\nI\nx\n/\nr\nK\nc\nE\n8\nv\nk\nm\nc\nW\nC\na\nO\nW\nb\nP\nq\nV\ng\n7\n0\nl\nz\nQ\nK\nU\no\nM\nC\nH\nc\nf\n8\nH\nI\n0\nD\nE\nv\nl\nU\na\nM\nK\nx\nU\ns\nO\nG\nF\nW\no\n7\nx\nl\nI\nz\nw\nm\nl\nS\nG\nU\nW\nK\nh\np\nj\nM\n8\nI\nQ\nO\nU\ny\nq\nw\nT\n5\nU\nd\n5\n8\nE\nT\nd\nJ\no\nq\nY\n+\nQ\nF\nM\nn\n1\nC\no\n1\nz\n9\nu\nR\nF\nj\nX\n6\nm\n5\n7\n6\na\nT\nW\nU\ny\n1\n6\nm\nX\ni\nf\n9\n4\nw\n0\nl\n7\nL\nj\np\nk\nI\nI\n0\n0\nF\nW\nR\nz\ny\nI\no\n5\n0\ng\nL\nI\nW\n0\nJ\nh\nJ\nS\nj\nS\nf\np\nw\nQ\nT\ny\nd\nK\ns\ni\nE\ny\nx\nx\nE\nS\nn\nX\nV\nX\ny\nE\nl\nr\nL\nv\n6\n+\nS\nZ\nQ\nm\n9\n8\n3\nr\nj\no\nt\n6\n8\na\n9\nb\na\nV\n0\nU\nd\nZ\nT\ni\nG\nE\nz\ni\nD\nB\nl\nx\nC\nG\n2\n6\nh\nA\n1\n0\ng\nE\nM\nE\nT\nv\nM\nC\nr\n8\nW\ng\n8\nG\n2\n/\nG\n+\n2\nK\n0\nZ\nB\nQ\n7\nR\n/\nA\nL\nx\ns\nc\n3\nW\nl\n6\nT\nn\nQ\n=\n=\n<\n/\nl\na\nt\ne\nx\ni\nt\n>\nGr\n<\nl\na\nt\ne\nx\ni\nt\n \ns\nh\na\n1\n_\nb\na\ns\ne\n6\n4\n=\n\"\nw\nX\nB\n4\n4\n/\nS\n5\ni\nO\nk\n7\nm\na\nH\nt\nJ\nm\nq\nc\ni\nN\nA\nq\ne\nr\nA\n=\n\"\n>\nA\nA\nA\nB\n7\nH\ni\nc\nb\nV\nB\nN\nS\n8\nN\nA\nE\nJ\n3\nU\nr\n1\nq\n/\nq\nh\n6\n9\nL\nB\nb\nB\nU\n0\nm\nk\na\nI\n9\nF\nL\n1\n6\nE\nC\nq\nY\nt\nt\nK\nF\ns\nt\np\nt\n2\n6\nW\nY\nT\nd\nj\nd\nC\nC\nP\n0\nN\nX\nj\nw\no\n4\nt\nU\nf\n5\nM\n1\n/\n4\nz\nZ\nN\nQ\na\ns\nP\nB\nh\n7\nv\nz\nT\nA\nz\nz\n4\n8\n5\nU\n9\nq\n2\nv\n6\nz\nS\n2\nv\nr\nG\n5\nl\nZ\n5\nu\n7\nK\nz\nu\n7\nd\n/\nU\nD\n0\n8\n6\nq\ng\no\nk\nY\nS\n6\nJ\nO\nK\nR\n7\nP\nl\nY\nU\nc\n4\nE\nd\nT\nX\nT\nn\nP\nZ\ni\nS\nX\nH\no\nc\n9\nr\n1\np\nz\nd\nz\nv\n/\nt\nI\np\nW\nK\nR\ne\nN\nB\np\nT\nL\n0\nQ\nj\nw\nU\nL\nG\nM\nH\na\nS\nO\n7\nd\nM\nJ\nO\nz\nY\nb\nV\nm\n1\n+\n0\nc\n6\nC\n9\nx\nC\nl\nK\nD\nA\nu\n1\nh\n9\nX\nM\nw\ni\nk\ng\nS\nU\nq\nE\nJ\nx\n0\nr\n1\nH\nT\nv\nW\nX\no\na\nl\nZ\no\nT\nT\nW\nW\nW\nQ\nK\nB\np\nj\nM\ns\nV\nj\n2\nj\nd\nU\n4\nJ\nA\nq\nL\n8\nu\nP\nn\na\nE\nz\no\n4\nx\nQ\nE\nE\nl\nT\nQ\nq\nN\nc\n/\nT\nm\nR\n4\nV\nC\np\nN\nP\nR\nN\nZ\n4\nj\n1\nR\nK\n1\n6\nc\n/\nE\n/\nr\n5\n/\no\no\nO\nl\nl\nT\nM\nS\nJ\np\no\nI\ns\nF\ng\nU\nJ\nR\nz\np\nC\n8\n8\n/\nR\ni\nE\nl\nK\nN\nE\n8\nN\nw\nU\nQ\ny\nc\ny\ns\ni\nE\ny\nw\nx\n0\nS\na\nf\nS\nh\n5\nC\nc\n/\nn\n7\nK\nl\nm\nG\n0\nL\nm\no\nO\n5\nf\n1\nx\nn\n2\nj\n1\nr\no\nu\n4\ni\nj\nD\nC\nZ\nz\nC\nO\nT\nh\nw\nB\nS\n2\n4\nh\nT\na\n4\nQ\nI\nD\nB\nE\n7\nz\nA\nq\ny\nW\ns\nZ\n+\nv\nN\ne\nl\n+\n0\nl\nq\nx\ni\n5\nh\nh\n+\nw\nf\nr\n4\nB\nv\n1\nL\nj\nu\nA\n=\n<\n/\nl\na\nt\ne\nx\ni\nt\n>\nMr\nSupervise\nSupervise\nCoarse\nmesh\n<\nl\na\nt\ne\nx\ni\nt\n \ns\nh\na\n1\n_\nb\na\ns\ne\n6\n4\n=\n\"\nc\nP\nB\nq\np\n6\nH\nT\nn\nu\ns\n6\n8\n0\nJ\nv\nJ\nA\nb\nN\nb\n7\nP\nV\nE\n+\n8\n=\n\"\n>\nA\nA\nA\nB\n7\nH\ni\nc\nb\nV\nB\nN\nS\n8\nN\nA\nE\nJ\n3\nU\nr\n1\nq\n/\nq\nh\n6\n9\nL\nB\nb\nB\nU\n0\nm\nk\na\nI\n9\nF\nL\n1\n6\nE\nC\nq\nY\nt\nt\nK\nF\ns\nt\np\nt\n2\n6\nW\nY\nT\nd\nj\nd\nC\nC\nP\n0\nN\nX\nj\nw\no\n4\nt\nU\nf\n5\nM\n1\n/\n4\nz\nZ\nN\nQ\na\ns\nP\nB\nh\n7\nv\nz\nT\nA\nz\nz\n4\n8\n5\nU\n9\nq\n2\nv\n6\nz\nS\n2\nv\nr\nG\n5\nl\nZ\n5\nu\n7\nK\nz\nu\n7\nd\n/\nU\nD\n0\n8\n6\nq\ng\no\nk\nY\nS\n6\nJ\nO\nK\nR\n7\nP\nl\nY\nU\nc\n4\nE\nd\nT\nX\nT\nn\nP\nZ\ni\nS\nX\nH\no\nc\n9\nr\n1\np\nz\nd\nz\nv\n/\nt\nI\np\nW\nK\nR\ne\nN\nB\np\nT\nL\n0\nQ\nj\nw\nU\nL\nG\nM\nH\na\nS\nO\n7\nd\nM\nC\nO\nz\nY\nb\nV\nm\n1\n+\n0\nc\n6\nC\n9\nx\nC\nl\nK\nD\nA\nu\n1\nh\n9\nX\nM\nw\ni\nk\ng\nS\nU\nq\nE\nJ\nx\n0\nr\n1\nH\nT\nv\nW\nX\no\na\nl\nZ\no\nT\nT\nW\nW\nW\nQ\nK\nB\np\nj\nM\ns\nV\nj\n2\nj\nd\nU\n4\nJ\nA\nq\nL\n8\nu\nP\nn\na\nE\nz\no\n4\nx\nQ\nE\nE\nl\nT\nQ\nq\nN\nc\n/\nT\nm\nR\n4\nV\nC\np\nN\nP\nR\nN\nZ\n4\nj\n1\nR\nK\n1\n6\nc\n/\nE\n/\nr\n5\n/\no\no\nO\nl\nl\nT\nM\nS\nJ\np\no\nI\ns\nF\ng\nU\nJ\nR\nz\np\nC\n8\n8\n/\nR\ni\nE\nl\nK\nN\nE\n8\nN\nw\nU\nQ\ny\nc\ny\ns\ni\nE\ny\nw\nx\n0\nS\na\nf\nS\nh\n5\nC\nc\n/\nn\n7\nK\nl\nm\nG\n0\nL\nm\no\nO\n5\nf\n1\nx\nn\n2\nj\n1\nr\no\nu\n4\ni\nj\nD\nC\nZ\nz\nC\nO\nT\nh\nw\nB\nS\n2\n4\nh\nT\na\n4\nQ\nI\nD\nB\nE\n7\nz\nA\nq\ny\nW\ns\nZ\n+\nv\nN\ne\nl\n+\n0\nl\nq\nx\ni\n5\nh\nh\n+\nw\nf\nr\n4\nB\nu\na\nA\nj\nt\nE\n=\n<\n/\nl\na\nt\ne\nx\ni\nt\n>\nMc\n<\nl\na\nt\ne\nx\ni\nt\n \ns\nh\na\n1\n_\nb\na\ns\ne\n6\n4\n=\n\"\nc\nP\nB\nq\np\n6\nH\nT\nn\nu\ns\n6\n8\n0\nJ\nv\nJ\nA\nb\nN\nb\n7\nP\nV\nE\n+\n8\n=\n\"\n>\nA\nA\nA\nB\n7\nH\ni\nc\nb\nV\nB\nN\nS\n8\nN\nA\nE\nJ\n3\nU\nr\n1\nq\n/\nq\nh\n6\n9\nL\nB\nb\nB\nU\n0\nm\nk\na\nI\n9\nF\nL\n1\n6\nE\nC\nq\nY\nt\nt\nK\nF\ns\nt\np\nt\n2\n6\nW\nY\nT\nd\nj\nd\nC\nC\nP\n0\nN\nX\nj\nw\no\n4\nt\nU\nf\n5\nM\n1\n/\n4\nz\nZ\nN\nQ\na\ns\nP\nB\nh\n7\nv\nz\nT\nA\nz\nz\n4\n8\n5\nU\n9\nq\n2\nv\n6\nz\nS\n2\nv\nr\nG\n5\nl\nZ\n5\nu\n7\nK\nz\nu\n7\nd\n/\nU\nD\n0\n8\n6\nq\ng\no\nk\nY\nS\n6\nJ\nO\nK\nR\n7\nP\nl\nY\nU\nc\n4\nE\nd\nT\nX\nT\nn\nP\nZ\ni\nS\nX\nH\no\nc\n9\nr\n1\np\nz\nd\nz\nv\n/\nt\nI\np\nW\nK\nR\ne\nN\nB\np\nT\nL\n0\nQ\nj\nw\nU\nL\nG\nM\nH\na\nS\nO\n7\nd\nM\nC\nO\nz\nY\nb\nV\nm\n1\n+\n0\nc\n6\nC\n9\nx\nC\nl\nK\nD\nA\nu\n1\nh\n9\nX\nM\nw\ni\nk\ng\nS\nU\nq\nE\nJ\nx\n0\nr\n1\nH\nT\nv\nW\nX\no\na\nl\nZ\no\nT\nT\nW\nW\nW\nQ\nK\nB\np\nj\nM\ns\nV\nj\n2\nj\nd\nU\n4\nJ\nA\nq\nL\n8\nu\nP\nn\na\nE\nz\no\n4\nx\nQ\nE\nE\nl\nT\nQ\nq\nN\nc\n/\nT\nm\nR\n4\nV\nC\np\nN\nP\nR\nN\nZ\n4\nj\n1\nR\nK\n1\n6\nc\n/\nE\n/\nr\n5\n/\no\no\nO\nl\nl\nT\nM\nS\nJ\np\no\nI\ns\nF\ng\nU\nJ\nR\nz\np\nC\n8\n8\n/\nR\ni\nE\nl\nK\nN\nE\n8\nN\nw\nU\nQ\ny\nc\ny\ns\ni\nE\ny\nw\nx\n0\nS\na\nf\nS\nh\n5\nC\nc\n/\nn\n7\nK\nl\nm\nG\n0\nL\nm\no\nO\n5\nf\n1\nx\nn\n2\nj\n1\nr\no\nu\n4\ni\nj\nD\nC\nZ\nz\nC\nO\nT\nh\nw\nB\nS\n2\n4\nh\nT\na\n4\nQ\nI\nD\nB\nE\n7\nz\nA\nq\ny\nW\ns\nZ\n+\nv\nN\ne\nl\n+\n0\nl\nq\nx\ni\n5\nh\nh\n+\nw\nf\nr\n4\nB\nu\na\nA\nj\nt\nE\n=\n<\n/\nl\na\nt\ne\nx\ni\nt\n>\nMc\n(3D)\n(3D)\n(3D)\n<\nl\na\nt\ne\nx\ni\nt\n \ns\nh\na\n1\n_\nb\na\ns\ne\n6\n4\n=\n\"\nS\ne\nc\nC\nQ\n5\nX\nF\nj\n5\nR\ng\n0\nw\nl\nA\nv\n/\nU\nR\nV\nZ\nq\nl\nj\nO\nM\n=\n\"\n>\nA\nA\nA\nB\n8\n3\ni\nc\nb\nV\nD\nL\nS\ns\nN\nA\nF\nL\n2\np\nr\n1\np\nf\nV\nZ\nd\nu\nB\no\nv\ng\nq\ni\nR\nS\nt\nM\nu\ni\nG\n5\nc\nV\n7\nA\nO\na\nU\nC\nb\nT\nS\nT\nt\n0\nM\ng\nk\nz\nk\n0\nI\nJ\n/\nQ\n0\n3\nL\nh\nR\nx\n6\n8\n+\n4\n8\n2\n+\nc\np\nC\nl\no\n9\nc\nD\nA\n4\nZ\nx\n7\nu\nW\ne\nO\nH\n3\nO\nm\nt\nG\n1\n/\nW\na\nW\nN\nz\na\n3\nt\nn\nf\nJ\nu\nZ\nW\n/\n/\n4\nP\nC\no\ne\nn\nz\nS\nV\nV\nE\ni\nC\ne\n2\nQ\ni\nE\ne\ny\n7\n2\nN\nF\nO\nR\nO\n0\no\n5\nn\nm\nt\nB\n9\nL\ni\nk\nO\nf\n0\n5\n4\n/\nv\nc\nv\n8\n3\no\nx\nK\nx\nS\nL\nx\nq\nO\nc\nx\n9\nU\nI\n8\nF\ni\nx\ng\nB\nG\ns\nj\nu\nW\n6\nI\n9\nc\nQ\nP\n0\nt\nl\ni\nS\nI\nb\nV\nm\nl\n2\n3\nc\n6\nC\n/\nx\nC\nl\nI\nD\nQ\nq\n0\nh\n9\nV\nP\nd\nx\nS\nR\nJ\nK\nR\nC\nE\n4\n6\nV\nG\nj\nh\n2\nr\nL\n0\nU\nS\n8\n0\nI\np\n4\nu\nK\nm\ny\ng\na\nY\nz\nL\nF\nY\nz\no\nw\nV\nO\nC\nQ\nK\ni\n/\nN\nM\ny\n/\nQ\nh\nV\nF\nG\nK\nI\ni\nk\ne\nU\nK\nj\nX\nP\n2\n5\nk\ne\nJ\nQ\nq\nX\nn\no\nm\n8\nk\ns\no\n1\nr\n3\nM\nv\nE\n/\nb\n5\nD\no\no\nO\nm\nl\nT\nM\nS\nJ\np\no\nI\ns\nD\nw\nU\nJ\nR\nz\np\nC\nW\nQ\nF\no\nx\nC\nQ\nl\nm\ns\n8\nN\nw\nU\nQ\ny\nk\nx\nW\nR\nC\nZ\na\nY\na\nF\nN\nT\nJ\nS\n+\nh\nu\nf\nr\n7\nO\nl\nm\nV\n0\nL\n2\nq\nO\n9\nf\n1\nx\nk\nO\nj\n1\nr\no\nt\n6\ni\nj\nD\nG\nZ\nz\nD\nJ\nT\nh\nw\nA\ny\n2\n4\nh\nz\nZ\n0\ng\nE\nA\nM\nT\n/\nA\nC\nr\n1\nZ\ni\nP\nV\nt\nv\n1\nv\nt\ny\nt\nG\nQ\nV\nO\n6\nf\nw\nC\n9\nb\nH\nN\n3\n8\ny\nk\ng\n4\n=\n<\n/\nl\na\nt\ne\nx\ni\nt\n>\nvc\nVertex\nNormal\n<\nl\na\nt\ne\nx\ni\nt\n \ns\nh\na\n1\n_\nb\na\ns\ne\n6\n4\n=\n\"\n2\nB\nr\nd\n7\n9\n7\n6\ne\n+\n3\nV\n6\nJ\ng\n8\nu\nR\nX\n5\nI\nK\nP\n8\nZ\nV\nk\n=\n\"\n>\nA\nA\nA\nB\n8\n3\ni\nc\nb\nV\nD\nL\nS\ns\nN\nA\nF\nL\n2\np\nr\n1\np\nf\nV\nZ\nd\nu\nB\no\nv\ng\nq\ni\nR\nS\na\np\nd\nF\nN\ny\n4\nr\n2\nA\nc\n0\no\nU\ny\nm\nk\n3\nb\no\nZ\nB\nJ\nm\nJ\nk\nI\nJ\n/\nQ\n0\n3\nL\nh\nR\nx\n6\n8\n+\n4\n8\n2\n+\nc\np\nC\nl\no\n9\nc\nD\nA\n4\nZ\nx\n7\nu\nW\ne\nO\nH\n3\nO\nm\nt\nG\n1\n/\nW\na\nW\nN\nz\na\n3\nt\nn\nf\nJ\nu\nZ\nW\n/\n/\n4\nP\nC\no\ne\nn\nz\nS\nU\n1\nE\ni\nC\ne\n2\nS\ni\nE\nd\ny\n4\nG\nN\nF\nO\nR\nO\n0\nq\n5\nn\nm\nd\nB\nB\nL\ni\nk\nO\nf\n0\n7\n4\n/\nu\n8\n3\n8\n/\ni\nO\nV\ni\nk\nX\ni\nQ\nc\n9\nj\n6\no\nV\n4\nI\nl\nj\nA\nC\nN\nZ\nG\nc\nt\n0\nQ\n6\n6\nk\nf\np\nG\nI\nx\nI\nq\nN\nq\nz\na\n7\nb\nO\nd\nB\nf\n4\nh\nS\nk\nB\ng\nU\n6\no\n+\nq\nn\nO\n4\n5\nI\nE\nl\nK\nh\nC\nc\nd\nK\nD\nR\n0\n7\n1\nl\n6\nK\np\nW\na\nE\n0\n0\nX\nF\nT\nR\nS\nN\nM\nZ\nn\nh\nC\nR\n0\na\nK\nn\nB\nI\nl\nZ\nf\nm\nm\nR\nf\no\nw\ni\nh\nj\nF\nE\nT\nS\nP\nK\nF\nR\nr\nv\n7\nc\nS\nH\nG\no\n1\nD\nz\n0\nz\nW\nS\nW\nU\na\n1\n7\nm\nf\ni\nf\nN\n0\nx\n0\n0\nP\nJ\nS\nJ\nu\nJ\nE\nU\n0\nG\nW\nh\n4\nK\nE\nI\nx\n2\nh\nr\nA\nA\n0\nZ\np\nI\nS\nz\ne\ne\nG\nY\nC\nK\nZ\ny\nY\nr\nI\nF\nE\nt\nM\nt\nK\nm\np\nk\np\nf\nQ\nW\nv\n1\n9\nn\na\nx\nK\n6\nF\n3\nV\nn\nW\na\n9\nc\nd\n+\no\nt\nW\n+\nK\nO\ns\np\nw\nB\nu\nd\nw\nC\nQ\n5\nc\nQ\nx\nv\nu\no\nA\nN\nd\nI\nB\nD\nD\nE\n7\nz\nA\nq\n5\nV\nY\nz\n9\na\nb\n9\nb\n4\nc\nL\nV\nn\nF\nz\ni\nn\n8\ng\nv\nX\nx\nD\nX\nL\n6\nk\ng\nY\n=\n<\n/\nl\na\nt\ne\nx\ni\nt\n>\nnc\ninvisible\n<\nl\na\nt\ne\nx\ni\nt\n \ns\nh\na\n1\n_\nb\na\ns\ne\n6\n4\n=\n\"\nN\nA\nn\nm\np\nQ\nT\nu\ni\n8\nB\nA\n3\nq\nb\nA\nx\nh\nk\nV\nO\n8\nv\n/\n2\nF\nU\n=\n\"\n>\nA\nA\nA\nB\n6\nH\ni\nc\nb\nV\nD\nL\nS\ng\nN\nB\nE\nO\ny\nN\nr\nx\nh\nf\nU\nY\n9\ne\nB\no\nP\ng\nK\ne\nx\nK\n0\nB\ny\nD\nX\nj\nw\nm\na\nB\n6\nQ\nL\nG\nF\n2\n0\np\nu\nM\nm\nX\n0\nw\nM\ny\nu\nE\nk\nC\n/\nw\n4\nk\nE\nR\nr\n3\n6\nS\nN\n/\n/\nG\ny\nW\nY\nD\nG\ni\n1\no\nK\nK\nq\n6\n6\ne\n7\ny\nY\ns\nG\nV\nt\nu\n0\nv\nK\n7\ne\n2\nv\nr\nG\n5\nl\nd\n8\nu\n7\nO\nz\nu\n7\nR\n8\nU\nD\n4\n9\na\nK\nk\no\nk\nw\ny\na\nL\nR\nC\nQ\n7\nH\nl\nU\no\ne\nI\nh\nN\nz\nb\nX\nA\nT\ni\ny\nR\nB\np\n7\nA\nt\nj\ne\n+\nm\nf\nv\nt\nR\n5\nS\nK\nR\n+\nG\n9\nn\ns\nT\no\nB\nn\nQ\nY\nc\np\n8\nz\nq\no\n3\nU\nu\nO\ns\nX\nS\n3\nb\nZ\nT\nk\nH\n+\nE\ni\nc\nj\nJ\nc\nh\nQ\n7\nx\nc\n/\ne\n4\nO\nI\nJ\nQ\nG\nG\nm\ng\nm\nq\nV\nN\ne\nx\nY\n+\n1\nO\nq\nd\nS\nc\nC\nZ\nw\nV\ne\no\nn\nC\nm\nL\nI\nx\nH\nW\nL\nX\n0\nJ\nA\nG\nq\nN\nx\np\ne\nu\ni\nM\nn\nB\nl\nl\nQ\nP\nx\nI\nm\ng\no\n1\nS\nd\nW\nf\nE\n1\nM\na\nK\nD\nU\nJ\nP\nN\nM\nZ\nU\nD\n1\nS\nq\n9\n5\nc\n/\nM\n/\nr\nJ\nt\nq\nv\nu\nl\nM\ne\nx\no\nn\nG\nk\nC\n0\nW\n+\nY\nk\ng\nO\ni\nL\nz\nr\n8\nm\nA\nS\n2\nR\na\nT\nA\ny\nh\nT\nH\nJ\nz\nK\n2\nE\nj\nK\ni\nn\nT\nJ\np\nt\nC\nG\nk\nJ\n1\n+\nf\ns\nq\nW\nY\nb\nQ\nu\ni\ng\n7\nl\n+\nV\nK\no\n1\nK\nq\nX\nW\nd\nx\n5\nO\nE\nE\nT\nu\nE\nc\nH\nL\ni\nC\nG\nt\nx\nC\nH\nZ\nr\nA\nA\nO\nE\nJ\nX\nu\nD\nV\ne\nr\nC\ne\nr\nT\nf\nr\nf\nd\nG\na\ns\n7\nK\nZ\nY\n/\ng\nF\n6\n+\nM\nb\nu\nj\n2\nM\n9\nQ\n=\n=\n<\n/\nl\na\nt\ne\nx\ni\nt\n>\nS\nGrid transform\nFigure 2. Overview. TEMPEH predicts a high-quality registered head mesh in two stages. The coarse stage builds a feature volume from\nthe feature maps, localizes the moving head in this volume with a spatial transformer, and then infers an intermediate 3D head mesh from\nthe localized feature volume. The refinement stage updates each vertex location by sampling features locally, fusing these features view-\nand surface-aware, and predicting the updated vertex location from the local feature volume. During training, raw MVS scans are used as\nsupervision. Once trained, TEMPEH directly predicts head meshes from calibrated multi-view images without requiring scans as input.\nFigure 3. Volumetric feature sampling illustrated in 2D. While\nthe coarse stage (left) directly fuses the feature vectors of all views,\nthe refinement stage (right) performs a surface-aware feature fu-\nsion by leveraging the mesh estimated in the coarse stage. Specifi-\ncally, image features for occluded views (dashed lines), and views\nwith large angle between the surface normal and viewing direction\nare assigned a low importance weight, while non-occluded views\nwith low angle receive a higher weight.\na feature cube (see Fig. 3 left). Specifically, for a point\np \u2208 R3 and view i, a feature vector fi \u2208 Rdf is ob-\ntained by perspective projection \u03a0(.) and bilinear sam-\npling as Fi(\u03a0(p, Ci)) \u2192 fi. The view feature vectors are\nthen fused by computing the mean \u00b5c =\n1\nk\nPk\ni=1 fi and\nvariance \u03c32\nc =\n1\nk\nPk\ni=1 f2\ni \u2212 \u00b52\nc (with a slight abuse of\nnotation, where f2\ni and \u00b52\nc are the element-wise squares)\nacross all views, and concatenated to the feature vector\nf = (\u00b5c \u2295 \u03c32\nc) \u2208 R2df , where \u2295 denotes the vector con-\ncatenation. This procedure is performed for every point in a\n3D sampling grid Gc \u2208 Rdc\u00d7dc\u00d7dc\u00d73 that covers the entire\ncapture volume with dc samples per dimension, to obtain a\nfeature cube Qc \u2208 Rdc\u00d7dc\u00d7dc\u00d72df .\nHead localization: While the head to be captured only oc-\ncupies a small subspace of the capture volume, the feature\ncube Qc covers the entire space. This wastes learning capa-\nbility due to the lower resolution of the feature cube in the\nrelevant head region. Training a network to directly recover\na 3D head from Qc, as done in ToFu, therefore reduces the\nreconstruction accuracy. However, Qc contains information\nabout the location and scale of the head. We leverage this\nwith a trainable spatial transformer Floc(Qc) \u2192 {s, r, t}\nthat predicts scale s \u2208 R3, rotation r \u2208 R6 [86], and transla-\ntion t \u2208 R3 to localize the head in Qc. Transforming every\npoint of Gc with {s, r, t} then gives a head localized feature\ngrid G\n\u2032\nc. We then perform the volumetric feature sampling\nfor G\n\u2032\nc to obtain a localized feature cube Q\n\u2032\nc.\nHead inference: The nv vertices vc\ni of Mc are then pre-\ndicted from Q\n\u2032\nc following Li et al. [50]. For this, a convo-\nlutional 3D U-Net Frec(Q\n\u2032\nc) \u2192 P with a Softmax function\napplied to the output of the final layer\u2019s output across the\nfirst three dimensions predicts a probability volume P \u2208\nRdc\u00d7dc\u00d7dc\u00d7nv. In P, each of the nv channels encodes the\nprobability distribution Pi of the 3D location for one vertex\nvc\ni. Each vc\ni is then computed as the element-wise product\nof Pi and the grid G\n\u2032\nc, followed by a sum over the grid di-\nmensions. Formally, vc\ni = Pdc\nj,k,l=1(Pi \u2299 G\n\u2032\nc)jkl, where\n\u2299 denotes the Hadamard product, broadcasted across the\nfourth dimension of G\n\u2032\nc (i.e., the xyz-channels), and (.)jkl is\nthe jkl-th tensor element.\n3.2. Geometry refinement\nVolumetric surface-aware feature fusion: Following the\ncoarse feature sampling stage, for a point p \u2208 R3 and view\ni, a feature vector fi \u2208 Rdf is obtained from the i-th view\n2D feature map by perspective projection and bilinear sam-\npling. Different from the coarse stage, which uses one big\nsampling grid, the refinement stage defines, for every vertex\nvc, a small 3D sampling grid Gr \u2208 Rdr\u00d7dr\u00d7dr\u00d73, centered\nat vc. For simplicity, we omit the vertex index and describe\nthe feature sampling for one particular vertex.\nToFu [50] na\u00a8\u0131vely fuses all fi with equal importance\nacross views, regardless of the visibility of point p in the\ni-th view. Instead, we use a surface-aware feature aggrega-\ntion (see Fig. 3 right) in form of a weighted mean\n\u00b5r =\n1\nPk\ni=1 \u03b7i\nk\nX\ni=1\n\u03b7ifi,\n(1)\nand weighted variance\n\u03c32\nr =\n1\nPk\ni=1 \u03b7i\nk\nX\ni=1\n\u03b7if2\ni \u2212 \u00b52\nr,\n(2)\nwhere \u03b7i is a view- and surface-dependent weight that de-\npends on the coarse mesh vertex vc, its corresponding vertex\nnormal nc, and the camera location oi (in Ci) of view i. The\nweight is computed as\n\u03b7i = Softplus(\u03b4i \u00b7 cos \u03b8i),\n(3)\nwhere \u03b4i \u2208 {0, 1} is the visibility of vc from the camera\ncentered at oi (i.e., 0 invisible, 1 visible), and cos \u03b8i =\n< nc, di > measures the angle between the surface nor-\nmal nc and the negative viewing direction di, where di =\n(oi \u2212 p)/\u2225oi \u2212 p\u2225. The Softplus enforces positivity of the\nweight to get non-zero gradients w.r.t. features of all views.\nFinally, we compute the fused feature vector as f =\n(\u00b5r \u2295 \u03c32\nr \u2295 v) \u2208 R2df +3, where v \u2208 R3 is the vertex cor-\nresponding to vc of a fixed template mesh, and \u2295 denotes\nthe concatenation operation. Adding v acts as identifier of\nthe vertex to be processed to the refinement network. Per-\nforming the volumetric feature sampling for all points in Gr\ngives a local feature cube Qr for every vertex vc.\nMesh refinement: Similar to the coarse face reconstruc-\ntion, a convolutional 3D U-Net Fref(Qr) \u2192 P, with a Soft-\nmax function applied to the final layer\u2019s output, predicts a\nprobability volume P \u2208 Rdr\u00d7dr\u00d7dr. Note that different\nfrom the coarse stage, P encodes the probability distribu-\ntion of the 3D location of one vertex vc. The final vertex\nlocation vr is then reconstructed as Pdr\nj,k,l=1(P \u2299 Gr)jkl.\n3.3. Loss functions\nSurface distance: TEMPEH reconstructs meshes M \u2208\n{Mc, Mr} (i.e., either coarse mesh Mc or refined mesh Mr)\nin correspondence, which must closely resemble the raw\ntraining scans S. To enforce this, the training of TEMPEH\nminimizes the point-to-surface distance, given as\nEs2m = \u03bbs2m\n1\n|S|\nX\ns\u2208S\n\u03c1\n\u0012\nmin\nm\u2208M \u2225s \u2212 m\u22252\n2\n\u0013\n,\n(4)\nwhich measures the distances between points s \u2208 R3 on the\nsurface of S and their closest points m \u2208 R3 on the surface\nof M. The Geman-McClure [32] robust penalty function\n\u03c1(.) provides robustness to noise and outliers in the scans,\nthe weight \u03bbs2m \u2208 R+\n0 controls the impact of the loss.\nSurface regularization: Directly optimizing Eq. 4 results\nin poor registrations with overlapping and self-intersecting\nfaces. To prevent this, we add a relative edge regularization\nEreg = \u03bbreg\n1\nne\nne\nX\ni=1\n\u03b3i\n\r\rem\ni \u2212 et\ni\n\r\r2\n2 ,\n(5)\nwhich penalizes the difference between the 3D edge vectors\nem of M \u2208 {Mc, Mr} and the corresponding edge vectors\net of the reference registration T. To account for varying\nscan quality in different face regions (e.g., eyes, lips, and\nhair regions are often more noisy), pre-defined edge weights\n\u03b3 \u2208 R+\n0 control the amount of regularization per face re-\ngion, \u03bbreg \u2208 R+\n0 weights the overall regularization.\nRegistration error: The distance to the registration T is\nminimized as vertex-to-vertex distance, specifically as\nEv2v = 1\nnv\nnv\nX\ni=1\n\u03c9i\n\r\rvm\ni \u2212 vt\ni\n\r\r2\n2 ,\n(6)\nwhere vm and vt are vertices of M \u2208 {Mc, Mr} and T,\nrespectively. The binary weights \u03c9i \u2208 {0, 1} control the\nimpact of individual vertices. Note that Ev2v is only used\nto pre-train the coarse reconstruction stage for the full head\n(i.e., \u03c9i = 1 \u2200i). Afterwards, it only serves to regularize the\neyeball location, with \u03c9i = 1 for all eyeball vertices, and\n\u03c9i = 0 otherwise.\n4. Implementation details\nCapture setup: We use a multi-camera active stereo cap-\nture system (3dMD LLC, Atlanta) with eight pairs of gray-\nscale stereo cameras, and eight color cameras. The capture\nsystem is calibrated, providing parameters for camera ex-\ntrinsics, intrinsics, and radial distortions. Each camera cap-\ntures images of resolution 1600 \u00d7 1200 at 60 fps, where the\ncolor cameras are time synchronized with light LED pan-\nels, and the stereo cameras are synchronized with random\nspeckle pattern projectors. The system uses a MVS method\nto reconstruct unstructured 3D scans at 60 fps, each with\nabout 110K vertices, from the stereo images.\nData capture: We collect a multi-view 3D head dataset,\nreferred to as FaMoS (Facial Motion across Subjects), from\n95 subjects with our capture setup. Each subject performs\n28 motion sequences, containing six prototypical expres-\nsions (i.e., Anger, Disgust, Fear, Happiness, Sadness, and\nSurprise), two head rotations (left/right and up/down), and\ndiverse facial motions, including extreme and asymmetric\nexpressions. All subjects wear a hair net. In total, FaMoS\ncontains around 600K frames (i.e., \u223c 225 frames per se-\nquence) of calibrated multi-view images and corresponding\n3D head scans. See the Sup. Mat. for additional details.\nTraining data: We train TEMPEH on data of 78 FaMoS\nsubjects (70 subjects for training, 8 for validation).\nFor\ntraining, we randomly sample 40 frames per expression se-\nquence, and 120 frames per head rotation sequence (88K\nframes in total). For validation, we sample 5 frames per se-\nquence (1,118 frames in total). For each frame, we compute\na reference registration in FLAME mesh topology with the\nfully-automatic registration method of Li et al. [49].\nTest data:\nWe qualitatively and quantitatively evaluate\nTEMPEH on all 28 sequences of 15 FaMoS subjects, dis-\njoint from the training subjects. For each sequence, we ran-\ndomly sample 20 frames; in total 8, 350 frames.\nImplementation details: TEMPEH is implemented in Py-\nTorch [58], and optimized with AdamW [53] with a learn-\ning rate of 1e\u22124 for the parameters of the head localization\nnetworks, and 1e\u22123 for all other parameters. The volumet-\nric feature sampling is adapted from ToFu [50]. The differ-\nentiable distance between scans and reconstructed mesh is\nbased on Kaolin [28], random points are sampled in each\nscan\u2019s surface during training with Trimesh [19].\nTEMPEH takes the 16 gray-scale stereo images as input,\nas they are time synchronized with the 3D scans used as\ntraining supervision, and it outputs meshes in FLAME mesh\ntopology [49] with nv = 5023 vertices and nf = 9976\nfaces.\nThe feature network Fimg is a U-Net [63] with a\nResNet34 [38] backbone. The head localization network\nFloc consists of two 3D convolutional layers and a fully-\nconnected layer, each with ReLU activations, followed by a\nfinal linear layer. The reconstruction networks Frec and Fref\nare 3D U-Nets [40], with five and three down- and upsam-\npling layers, respectively. See the Sup. Mat. for details on\nthe model architecture and computational requirements.\nParameter settings: First, the coarse reconstruction stage\nis trained for 300K iterations on the reference registrations\nwith \u03c9i = 1 for all vertices, and without surface distance\nor regularization (\u03bbs2m = \u03bbreg = 0). Then, we train the\ncoarse reconstruction stage with surface distance weight\n\u03bbs2m = 10.0 and surface regularization weight \u03bbreg = 1.0\nfor 300K iterations. Directly optimizing Es2m and Ereg can\nresult in dislocated eyeballs during training, as the eyeballs\nare parts separated from the head, and Ereg is translation in-\nvariant. To prevent this, we regularize the eyeball locations\nwith \u03c9i = 1 for all eyeball vertices, for all other vertices,\nwe set \u03c9i = 0. We train the refinement stage for 150K itera-\ntions with \u03bbs2m = 10.0 and \u03bbreg = 0.3. Following ToFu, the\ndimensions of the feature grids are dc = 32 (coarse stage)\nand dr = 8 (refinement stage), and the number of features is\ndf = 8. To reduce the memory consumption, we downsam-\nple the input images by a factor of 8 for the coarse training\n(i.e., w = 200, h = 150), and by a factor of 4 for the refine-\nment step (i.e., w = 400, h = 300). We use a batch size of\n2 for the entire training.\n5. Evaluation\nQualitative evaluation: We evaluate the quality of the pre-\ndicted meshes on the FaMoS test data, and compare it with\nthe predictions of the current state-of-the-art, ToFu [50]. As\nthe publicly available ToFu model1 does not generalize to\nour scanner setup and it only predicts faces, we train ToFu\non our training data to predict complete heads. For this, we\nfirst predict a low-resolution head with 341 vertices in the\nglobal stage, followed by upsampling and refining the mesh\nto the final mesh resolution. Unlike ToFu, we use no hier-\narchical architecture. To factor out the potential impact of\nthe intermediate mesh resolution, we additionally compare\nto ToFu without mesh hierarchy, where the global stage di-\nrectly predicts meshes of the final resolution, followed by\nrefining the mesh in the local stage. We refer to this op-\ntion as ToFu+. To compare with a direct image-to-3DMM\nregressor, we extend the coarse model of DECA [25] to a\nmulti-view setting by regressing FLAME [49] parameters\nfrom the concatenated view feature vectors, independently\nreconstructed for each view.\nFigure 4 shows that TEMPEH reconstructs 3D heads\nwith the lowest error in the neck region for extreme head\nposes (Row 1), and with head shapes closer to the reference\nscans (Rows 2 & 3). See the Sup. Mat. for additional results,\nincluding 3DMM regressor details and comparisons.\nQuantitative evaluation: We quantify the accuracy of the\npredicted 3D heads on the FaMoS test data by computing\nthe point-to-surface distances between the vertices of each\nreference scan, and their closest points in the predicted 3D\nhead\u2019s surface. To analyze the accuracy in different head\nregions, we segment each scan into face, scalp, and neck\nregions (see Sup. Mat.), and report the reconstruction errors\n1https://github.com/tianyeli/tofu\n0 mm\n>3 mm \n0 mm\n>3 mm \n0 mm\n>3 mm \nInput (4 of 16 views)\nToFu\nToFu+\nOurs\nFigure 4. Qualitative evaluation. Comparison to ToFu [50] and ToFu+ on FaMoS test samples with varying expressions and head poses\nfor subjects not present during training. For each method, we show the predicted mesh (left) and the color coded point-to-surface distance\n(right) between the reference scan and the predicted mesh as a heatmap on the scan\u2019s surface (red means \u2265 3 millimeter).\nComplete head\nFace\nScalp\nNeck\nMethod\nMedian \u2193\nMean \u2193\nStd \u2193\nMedian \u2193\nMean \u2193\nStd \u2193\nMedian \u2193\nMean \u2193\nStd \u2193\nMedian \u2193\nMean \u2193\nStd \u2193\n3DMM regressor\n9.42\n12.06\n10.11\n8.74\n10.38\n7.91\n9.19\n11.80\n9.74\n13.02\n16.45\n13.29\nToFu\n0.72\n1.44\n2.72\n0.63\n0.93\n1.50\n0.62\n1.46\n3.27\n1.30\n2.39\n3.24\nToFu+\n0.82\n1.59\n2.84\n0.68\n1.00\n1.52\n0.73\n1.59\n3.16\n1.50\n2.77\n3.82\nOurs\n0.26\n0.51\n1.22\n0.21\n0.34\n1.22\n0.26\n0.41\n0.66\n0.38\n0.95\n1.91\nTable 1. Quantitative evaluation. Reconstruction error for varied head regions (FaMoS test set). We compare to a 3DMM regressor, ToFu\n[50] and ToFu without hierarchical architecture (ToFu+), all trained to predict entire heads on the TEMPEH training data. Errors in mm.\nfor the entire head and the individual segments. Table 1\nshows that TEMPEH outputs 3D heads with 64% lower re-\nconstruction error compared to ToFu, and 68% lower than\nToFu+. See the Sup. Mat. for cumulative error plots and\nadditional qualitative comparisons.\nTraining TEMPEH minimizes the distance between the\npredicted heads and MVS scans, hence it effectively regis-\nters the scans. TEMPEH closely fits the training scans, with\na median error of 0.17 mm (coarse stage: 0.80 mm).\nAblation experiments: To quantify the impact of individ-\nual design choices, we train following model variants: (1)\nCoarse w/o s2m loss: fully supervised training with v2v\nloss only (Eq. 6) (i.e., no scan supervision). (2) Coarse\nw/o head localization: direct prediction of the head mesh\nfrom the coarse feature volume without head localization.\n(3) Refinement w/ na\u00a8\u0131ve feature fusion: feature aggregation\nas mean and variance across views, without surface-aware\nfeature fusion. (4) Ours w/o s2m loss: training of coarse\nand refinement stage with v2v loss only. (5) Ours w/o head\nlocalization: coarse model w/o head localization with re-\nfinement stage. (6) Ours color images input: use of color\nimages as input, instead of gray-scale stereo images. (7)\nOurs hierarchical: predicting a mesh with 1000 vertices in\nthe coarse stage, followed by upsampling and refinement in\nthe second stage. Fig. 5 and Tab. 2 compare different model\nvariants qualitatively and quantitatively. We find that both,\nsurface distance loss and head localization are essential for\nthe coarse head inference, as ablating either of them leads to\nworse performance. Further, our model with surface-aware\nfeature fusion and surface distance loss predicts heads with\nlowest error. While the models without head localization (5)\nor with hierarchical architecture (7) infer heads with com-\nparably low errors, they reconstruct e.g., the lips region with\nlower fidelity due to the worse expression initialization from\nthe coarse stage (see Fig. 5 and Sup. Mat.).\n6. Discussion\nReference registrations: TEMPEH uses reference regis-\ntrations for pre-training and regularization. While these reg-\n0 mm\n>3 mm \nInput (4 of 16 views)\nCoarse w/o s2m\nCoarse w/o head localization\nOurs (coarse)\n0 mm\n>3 mm \nNa\u00a8\u0131ve feature fusion\nOurs w/o s2m\nOurs w/o head localization\nOurs\nFigure 5. Ablation experiments. For each model variant, we show the reconstructed mesh (left) and the color coded point-to-surface\ndistance (right) between the reference scan and the reconstructed mesh as a heatmap on the scan\u2019s surface (red means \u2265 3 millimeter).\nComplete head\nFace\nScalp\nNeck\nMethod\nMedian \u2193\nMean \u2193\nStd \u2193\nMedian \u2193\nMean \u2193\nStd \u2193\nMedian \u2193\nMean \u2193\nStd \u2193\nMedian \u2193\nMean \u2193\nStd \u2193\nCoarse w/o s2m loss\n1.15\n1.85\n2.76\n0.91\n1.21\n1.56\n1.22\n2.13\n3.44\n1.75\n2.60\n2.91\nCoarse w/o head localization\n1.16\n1.60\n1.79\n1.12\n1.43\n1.61\n1.00\n1.33\n1.26\n1.69\n2.48\n2.65\nOurs coarse\n0.71\n1.11\n1.56\n0.68\n0.93\n1.40\n0.61\n0.92\n1.18\n1.09\n1.81\n2.28\nRefinement w/ na\u00a8\u0131ve feature fusion\n0.35\n0.70\n1.36\n0.27\n0.45\n1.26\n0.36\n0.63\n0.90\n0.54\n1.23\n2.06\nOurs w/o s2m loss\n0.78\n1.44\n2.59\n0.64\n0.89\n1.43\n0.76\n1.62\n3.22\n1.27\n2.18\n2.81\nOurs w/o head localization\n0.28\n0.58\n1.32\n0.22\n0.39\n1.24\n0.28\n0.44\n0.62\n0.40\n1.09\n2.16\nOurs color images input\n0.44\n0.77\n1.33\n0.34\n0.54\n1.27\n0.46\n0.71\n0.93\n0.65\n1.28\n1.94\nOurs hierarchical\n0.27\n0.61\n1.55\n0.21\n0.36\n1.24\n0.27\n0.62\n1.54\n0.37\n0.97\n2.04\nOurs\n0.26\n0.51\n1.22\n0.21\n0.34\n1.22\n0.26\n0.41\n0.66\n0.38\n0.95\n1.91\nTable 2. Ablation experiments. Effects of training from registered meshes instead of scans (w/o s2m loss), reconstructing heads from the\nentire feature volume (w/o head localization), aggregating features without leveraging surface and visibility information (w/ na\u00a8\u0131ve feature\nfusion), using the capture system\u2019s 8 color images as input, and using a hierarchical architecture (ours hierarchical). Errors in mm.\nistrations are obtained fully automatically [49], their com-\nputation is slow and computationally expensive. Instead,\ndirectly regularizing to a statistical model [23] during train-\ning by jointly optimizing the statistical model\u2019s parameters\n(i.e., as done for coupled registrations [49]) could mitigate\nthe need for registrations. This, however, adds an additional\nlevel of complexity, which goes beyond our current scope.\nRegistration quality: While TEMPEH\u2019s reconstructions\nwell resemble the reference scans, expressions like eye\nblinks are not well captured. This is due to the poor quality\nof the scans in the eye region, the fast motion of the eyelids,\nand the absence of a clear signal in the optimized point-to-\nsurface distance (Eq. 4). We plan to add an additional eyelid\nlandmark error to improve the eyelid tracking in the future.\nRepresentation: Several methods exist to learn deep im-\nplicit functions with dense correspondence from scans [51,\n84, 85]. Replacing TEMPEH\u2019s mesh representation with\nsuch implicit functions is an interesting future direction.\nCamera calibrations: TEMPEH is designed for lab envi-\nronments with a carefully calibrated capture system. Adapt-\ning TEMPEH to less constrained scenarios with noisy or un-\nknown camera calibration goes beyond the current scope.\n7. Conclusion\nWe have presented TEMPEH, a framework to predict\nentire 3D heads in dense correspondence from calibrated\nmulti-view images.\nTEMPEH infers 3D heads with re-\nconstruction accuracy that is 64% lower than the previous\nstate-of-the-art. We achieve this by training TEMPEH di-\nrectly from scans, using a spatial transformer head localiza-\ntion module, and surface-aware feature fusion. Intuitively,\nthe training from scans overcomes ambiguous correspon-\ndence across subjects and imperfect correspondence across\nexpressions. The head localization enables the coarse stage\nto handle a large capture volume by focusing on the region\nof interest, and it provides a better initialization for the ge-\nometry refinement. The surface-aware feature fusion ac-\ncounts for self-occlusions. Due to the inferred geometry\naccuracy and inference speed, TEMPEH is useful for appli-\ncations like multi-view head performance capture.\nAcknowledgement: We thank T. Alexiadis, M. H\u00a8oschle, Y.\nFincan, B. Pellkofer for data capture and IT support, T. Mc-\nConnell for voice over, and R. Dan\u02c7e\u02c7cek, W. Zielonka, P. Pa-\ntel, and P. Kulits for proofreading. Disclosure: https://\nfiles.is.tue.mpg.de/tbolkart/disclosure.txt\nReferences\n[1] Victoria Fern\u00b4andez Abrevaya, Stefanie Wuhrer, and Edmond\nBoyer. Spatiotemporal modeling for efficient registration of\ndynamic 3D faces. In International Conference on 3D Vision\n(3DV), pages 371\u2013380, 2018. 2\n[2] Oswald Aldrian and William AP Smith. Inverse rendering of\nfaces with a 3D morphable model. Transactions on Pattern\nAnalysis and Machine Intelligence (TPAMI), 35(5):1080\u2013\n1093, 2013. 2\n[3] Oleg Alexander, Mike Rogers, William Lambeth, Matt Jen-\nYuan Chiang, and Paul E. Debevec.\nThe digital Emily\nproject: photoreal facial modeling and animation. In SIG-\nGRAPH Courses, pages 12:1\u201312:15, 2009. 2\n[4] Brian Amberg, Reinhard Knothe, and Thomas Vetter. Ex-\npression invariant 3D face recognition with a morphable\nmodel. In International Conference on Automatic Face &\nGesture Recognition (FG), pages 1\u20136, 2008. 2\n[5] Mehdi Bahri, Eimear O\u2019 Sullivan, Shunwang Gong, Feng\nLiu, Xiaoming Liu, Michael M. Bronstein, and Stefanos\nZafeiriou.\nShape my face: Registering 3D face scans by\nsurface-to-surface translation. International Journal of Com-\nputer Vision (IJCV), 129(9):2680\u20132713, 2021. 2\n[6] Ziqian Bai, Zhaopeng Cui, Jamal Ahmed Rahim, Xiaoming\nLiu, and Ping Tan. Deep facial non-rigid multi-view stereo.\nIn Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 5849\u20135859, 2020. 2, 3\n[7] Anil Bas and William A. P. Smith. What does 2D geomet-\nric information really tell us about 3D face shape?\nInter-\nnational Journal of Computer Vision (IJCV), 127(10):1455\u2013\n1473, 2019. 3\n[8] Anil Bas, William A. P. Smith, Timo Bolkart, and Stefanie\nWuhrer. Fitting a 3D morphable model to edges: A com-\nparison between hard and soft correspondences. In Asian\nConference on Computer Vision Workshops, pages 377\u2013391,\n2017. 2\n[9] Thabo Beeler, Fabian Hahn, Derek Bradley, Bernd Bickel,\nPaul A. Beardsley, Craig Gotsman, Robert W. Sumner, and\nMarkus H. Gross. High-quality passive facial performance\ncapture using anchor frames.\nTransactions on Graphics,\n(Proc. SIGGRAPH), 30(4):75, 2011. 2, 3\n[10] Paul J. Besl and Neil D. McKay. A method for registration of\n3-D shapes. Transactions on Pattern Analysis and Machine\nIntelligence (TPAMI), 14(2):239\u2013256, 1992. 2\n[11] Volker Blanz and Thomas Vetter. A morphable model for the\nsynthesis of 3D faces. In SIGGRAPH, pages 187\u2013194, 1999.\n2\n[12] Timo Bolkart and Stefanie Wuhrer. A groupwise multilinear\ncorrespondence optimization for 3D faces. In International\nConference on Computer Vision (ICCV), pages 3604\u20133612,\n2015. 2\n[13] James Booth, Anastasios Roussos, Stefanos Zafeiriou, Allan\nPonniah, and David J. Dunaway. A 3D morphable model\nlearnt from 10,000 faces. In Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 5543\u20135552, 2016. 2\n[14] Derek Bradley, Wolfgang Heidrich, Tiberiu Popa, and Alla\nSheffer. High resolution passive facial performance capture.\nTransactions on Graphics (TOG), 29(4), 2010. 2, 3\n[15] Chen Cao, Yanlin Weng, Shun Zhou, Yiying Tong, and Kun\nZhou. FaceWarehouse: A 3D facial expression database for\nvisual computing. Transactions on Visualization and Com-\nputer Graphics (TVCG), 20(3):413\u2013425, 2014. 2\n[16] Hang Dai, Nick E. Pears, William A. P. Smith, and Christian\nDuncan. A 3D morphable model of craniofacial shape and\ntexture variation. In International Conference on Computer\nVision (ICCV), pages 3104\u20133112, 2017. 2\n[17] Hang Dai, Nick E. Pears, William A. P. Smith, and Chris-\ntian Duncan. Statistical modeling of craniofacial shape and\ntexture. International Journal of Computer Vision (IJCV),\n128(2):547\u2013571, 2020. 2\n[18] Radek Dan\u02c7e\u02c7cek, Michael J. Black, and Timo Bolkart. Emo-\ntion driven monocular face capture and animation. In Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\npages 20311\u201320322, 2022. 3\n[19] Dawson-Haggerty et al. Trimesh. https://trimsh.\norg/, 2019. 6\n[20] Yu Deng, Jiaolong Yang, Sicheng Xu, Dong Chen, Yunde\nJia, and Xin Tong.\nAccurate 3D face reconstruction with\nweakly-supervised learning: From single image to image set.\nIn Conference on Computer Vision and Pattern Recognition\nWorkshops (CVPR-W), pages 285\u2013295, 2019. 2\n[21] Pengfei Dou and Ioannis A. Kakadiaris. Multi-view 3D face\nreconstruction with deep recurrent neural networks. Image\nand Vision Computing (IVC), 80:80\u201391, 2018. 3\n[22] Pengfei Dou, Shishir K Shah, and Ioannis A Kakadiaris.\nEnd-to-end 3D face reconstruction with deep neural net-\nworks.\nIn Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 5908\u20135917, 2017. 3\n[23] Bernhard Egger, William A. P. Smith, Ayush Tewari, Ste-\nfanie Wuhrer, Michael Zollhoefer, Thabo Beeler, Florian\nBernard, Timo Bolkart, Adam Kortylewski, Sami Romdhani,\nChristian Theobalt, Volker Blanz, and Thomas Vetter. 3D\nmorphable face models - past, present and future. Transac-\ntions on Graphics (TOG), 39(5), 2020. 1, 2, 8\n[24] Haiwen Feng, Timo Bolkart, Joachim Tesch, Michael J.\nBlack, and Victoria Fern\u00b4andez Abrevaya. Towards racially\nunbiased skin tone estimation via scene disambiguation. In\nEuropean Conference on Computer Vision (ECCV), volume\n13673, pages 72\u201390, 2022. 2\n[25] Yao Feng, Haiwen Feng, Michael J. Black, and Timo\nBolkart.\nLearning an animatable detailed 3D face model\nfrom in-the-wild images. Transactions on Graphics, (Proc.\nSIGGRAPH), 40(4):88:1\u201388:13, 2021. 3, 6, 13\n[26] Yao Feng, Fan Wu, Xiaohu Shao, Yanfeng Wang, and Xi\nZhou. Joint 3D face reconstruction and dense alignment with\nposition map regression network. In European Conference\non Computer Vision (ECCV), volume 11218, pages 557\u2013574,\n2018. 3\n[27] Panagiotis P. Filntisis, George Retsinas, Foivos Paraperas-\nPapantoniou, Athanasios Katsamanis, Anastasios Roussos,\nand Petros Maragos. Visual speech-aware perceptual 3D fa-\ncial expression reconstruction from videos, 2022. 3\n[28] Clement Fuji Tsang,\nMaria Shugrina,\nJean Francois\nLafleche, Towaki Takikawa, Jiehan Wang, Charles Loop,\nWenzheng Chen, Krishna Murthy Jatavallabhula, Edward\nSmith, Artem Rozantsev, Or Perel, Tianchang Shen, Jun\nGao, Sanja Fidler, Gavriel State, Jason Gorski, Tommy Xi-\nang, Jianing Li, Michael Li, and Rev Lebaredian. Kaolin:\nA pytorch library for accelerating 3d deep learning re-\nsearch. https://github.com/NVIDIAGameWorks/\nkaolin, 2022. 6\n[29] Graham Fyffe, Koki Nagano, Loc Huynh, Shunsuke Saito,\nJay Busch, Andrew Jones, Hao Li, and Paul E. Debevec.\nMulti-view stereo on consistent face topology.\nComputer\nGraphics Forum (CGF), 36(2):295\u2013309, 2017. 2, 3\n[30] Pablo Garrido, Michael Zollh\u00a8ofer, Dan Casas, Levi Val-\ngaerts,\nKiran Varanasi,\nPatrick P\u00b4erez,\nand Christian\nTheobalt.\nReconstruction of personalized 3D face rigs\nfrom monocular video. Transactions on Graphics (TOG),\n35(3):28, 2016. 3\n[31] Baris Gecer, Stylianos Ploumpis, Irene Kotsia, and Stefanos\nZafeiriou. GANFIT: generative adversarial network fitting\nfor high fidelity 3D face reconstruction. In Conference on\nComputer Vision and Pattern Recognition (CVPR), pages\n1155\u20131164, 2019. 2\n[32] S. Geman and D. E. McClure. Statistical methods for to-\nmographic image reconstruction. Proceedings of the 46th\nSession of the International Statistical Institute, Bulletin of\nthe ISI, 52, 1987. 5\n[33] Kyle Genova, Forrester Cole, Aaron Maschinot, Aaron\nSarna, Daniel Vlasic, and William T. Freeman. Unsupervised\ntraining for 3D morphable model regression. In Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n8377\u20138386, 2018. 3\n[34] Abhijeet Ghosh, Graham Fyffe, Borom Tunwattanapong,\nJay Busch, Xueming Yu, and Paul E. Debevec. Multiview\nface capture using polarized spherical gradient illumination.\nTransactions on Graphics (TOG), 30(6):129, 2011. 3\n[35] Michael Goesele, Brian Curless, and Steven M. Seitz. Multi-\nview stereo revisited. In Conference on Computer Vision and\nPattern Recognition (CVPR), pages 2402\u20132409, 2006. 3\n[36] Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong\nTan, and Ping Tan. Cascade cost volume for high-resolution\nmulti-view stereo and stereo matching.\nIn Conference on\nComputer Vision and Pattern Recognition (CVPR), pages\n2492\u20132501, 2020. 3\n[37] Riza Alp G\u00a8uler, George Trigeorgis, Epameinondas Anton-\nakos, Patrick Snape, Stefanos Zafeiriou, and Iasonas Kokki-\nnos. DenseReg: Fully convolutional dense shape regression\nin-the-wild. In Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 6799\u20136808, 2017. 3\n[38] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n770\u2013778, 2016. 6, 13, 14\n[39] Sunghoon Im, Hae-Gon Jeon, Stephen Lin, and In So\nKweon. DPSNet: End-to-end deep plane sweep stereo. In In-\nternational Conference on Learning Representations (ICLR),\n2019. 3\n[40] Karim Iskakov, Egor Burkov, Victor S. Lempitsky, and Yury\nMalkov. Learnable triangulation of human pose. In Interna-\ntional Conference on Computer Vision (ICCV), pages 7717\u2013\n7726, 2019. 6, 14\n[41] Aaron S Jackson, Adrian Bulat, Vasileios Argyriou, and\nGeorgios Tzimiropoulos. Large pose 3D face reconstruction\nfrom a single image via direct volumetric CNN regression. In\nInternational Conference on Computer Vision (ICCV), pages\n1031\u20131039, 2017. 3\n[42] Max Jaderberg, Karen Simonyan, Andrew Zisserman, and\nKoray Kavukcuoglu. Spatial transformer networks. In Ad-\nvances in Neural Information Processing Systems (NeurIPS),\npages 2017\u20132025, 2015. 2\n[43] Penglei Ji, Hanchao Li, Luyan Jiang, and Xinguo Liu. Light-\nweight multi-view topology consistent facial geometry and\nreflectance capture.\nIn Advances in Computer Graphics\n(CGI), volume 13002, pages 139\u2013150, 2021. 2\n[44] Abhishek Kar, Christian H\u00a8ane, and Jitendra Malik. Learn-\ning a multi-view stereo machine.\nIn Advances in Neural\nInformation Processing Systems (NeurIPS), pages 365\u2013376,\n2017. 3\n[45] Ira Kemelmacher-Shlizerman.\nInternet based morphable\nmodel.\nIn International Conference on Computer Vision\n(ICCV), pages 3256\u20133263, 2013. 3\n[46] Ira Kemelmacher-Shlizerman and Steven M. Seitz. Face re-\nconstruction in the wild.\nIn International Conference on\nComputer Vision (ICCV), pages 1746\u20131753, 2011. 3\n[47] Hyeongwoo Kim, Michael Zollh\u00a8ofer, Ayush Tewari, Justus\nThies, Christian Richardt, and Christian Theobalt. Inverse-\nFaceNet: Deep monocular inverse face rendering. In Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\npages 4625\u20134634, 2018. 2\n[48] Hao Li, Bart Adams, Leonidas J. Guibas, and Mark Pauly.\nRobust single-view geometry and motion reconstruction.\nTransactions on Graphics (TOG), 28(5):175, 2009. 2\n[49] Tianye Li, Timo Bolkart, Michael. J. Black, Hao Li, and\nJavier Romero. Learning a model of facial shape and ex-\npression from 4D scans. Transactions on Graphics, (Proc.\nSIGGRAPH Asia), 36(6):194:1\u2013194:17, 2017. 2, 6, 8\n[50] Tianye Li, Shichen Liu, Timo Bolkart, Jiayi Liu, Hao Li, and\nYajie Zhao. Topologically consistent multi-view face infer-\nence using volumetric sampling. In International Conference\non Computer Vision (ICCV), pages 3824\u20133834, 2021. 2, 3,\n4, 5, 6, 7, 13, 14\n[51] Feng Liu and Xiaoming Liu. Learning implicit functions for\ntopology-varying dense 3D shape correspondence. In Ad-\nvances in Neural Information Processing Systems (NeurIPS),\n2020. 8\n[52] Feng Liu, Luan Tran, and Xiaoming Liu. 3D face modeling\nfrom diverse raw scan data. In International Conference on\nComputer Vision (ICCV), pages 9407\u20139417, 2019. 2\n[53] Ilya Loshchilov and Frank Hutter.\nDecoupled weight de-\ncay regularization. In International Conference on Learning\nRepresentations (ICLR), 2019. 6\n[54] Wan-Chun Ma, Tim Hawkins, Pieter Peers, Charles-F\u00b4elix\nChabert, Malte Weiss, and Paul E. Debevec. Rapid acqui-\nsition of specular and diffuse normal maps from polarized\nspherical gradient illumination. In Proceedings of the Euro-\ngraphics Symposium on Rendering Techniques, pages 183\u2013\n194, 2007. 3\n[55] Tetiana Martyniuk, Orest Kupyn, Yana Kurlyak, Igor\nKrashenyi, Jiri Matas, and Viktoriia Sharmanska.\nDAD-\n3DHeads: A large-scale dense, accurate and diverse dataset\nfor 3D head alignment from a single image. In Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n20910\u201320920, 2022. 3\n[56] Araceli Morales, Gemma Piella, and Federico M. Sukno.\nSurvey on 3D face reconstruction from uncalibrated images.\nComputer Science Review, 40:1\u201335, 2021. 2\n[57] Georgios Passalis, Panagiotis Perakis, Theoharis Theoharis,\nand Ioannis A. Kakadiaris. Using facial symmetry to handle\npose variations in real-world 3D face recognition. Transac-\ntions on Pattern Analysis and Machine Intelligence (TPAMI),\n33(10):1938\u20131951, 2011. 2\n[58] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,\nAndreas K\u00a8opf, Edward Yang, Zach DeVito, Martin Raison,\nAlykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu\nFang, Junjie Bai, and Soumith Chintala. PyTorch: An imper-\native style, high-performance deep learning library. In Ad-\nvances in Neural Information Processing Systems (NeurIPS),\n2019. 6\n[59] Stylianos Ploumpis, Evangelos Ververas, Eimear O\u2019 Sulli-\nvan, Stylianos Moschoglou, Haoyang Wang, Nick E. Pears,\nWilliam A. P. Smith, Baris Gecer, and Stefanos Zafeiriou.\nTowards a complete 3D morphable model of the human head.\nTransactions on Pattern Analysis and Machine Intelligence\n(TPAMI), 43(11):4142\u20134160, 2021. 2\n[60] Eduard Ramon, Janna Escur, and Xavier Gir\u00b4o-i-Nieto.\nMulti-view 3D face reconstruction in the wild using siamese\nnetworks. In International Conference on Computer Vision\nWorkshops (ICCV-W), pages 3096\u20133100, 2019. 3\n[61] Elad Richardson, Matan Sela, and Ron Kimmel. 3D face\nreconstruction by learning from synthetic data.\nIn Inter-\nnational Conference on 3D Vision (3DV), pages 460\u2013469,\n2016. 3\n[62] J\u00b4er\u00b4emy Riviere, Paulo F. U. Gotardo, Derek Bradley, Abhi-\njeet Ghosh, and Thabo Beeler. Single-shot high-quality fa-\ncial geometry and skin appearance capture. Transactions on\nGraphics (TOG), 39(4):81, 2020. 2, 3\n[63] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nNet: Convolutional networks for biomedical image segmen-\ntation. In Medical Image Computing and Computer-Assisted\nIntervention (MICCAI), pages 234\u2013241, 2015. 6, 14\n[64] Joseph Roth, Yiying Tong, and Xiaoming Liu.\nUncon-\nstrained 3D face reconstruction. In Conference on Computer\nVision and Pattern Recognition (CVPR), pages 2606\u20132615,\n2015. 3\n[65] Augusto Salazar, Stefanie Wuhrer, Chang Shu, and Flavio\nPrieto. Fully automatic expression-invariant face correspon-\ndence.\nMachine Vision and Applications, 25(4):859\u2013879,\n2014. 2\n[66] Soubhik Sanyal, Timo Bolkart, Haiwen Feng, and Michael J.\nBlack. Learning to regress 3D face shape and expression\nfrom an image without 3D supervision. In Conference on\nComputer Vision and Pattern Recognition (CVPR), pages\n7763\u20137772, 2019. 3\n[67] Mike Seymour, Chris Evans, and Kim Libreri. Meet Mike:\nEpic avatars. In SIGGRAPH, 2017. 2\n[68] Jiaxiang Shang, Tianwei Shen, Shiwei Li, Lei Zhou, Ming-\nmin Zhen, Tian Fang, and Long Quan.\nSelf-supervised\nmonocular 3D face reconstruction by occlusion-aware multi-\nview geometry consistency.\nIn European Conference on\nComputer Vision (ECCV), volume 12360, pages 53\u201370,\n2020. 3\n[69] Vincent Sitzmann, Justus Thies, Felix Heide, Matthias\nNie\u00dfner, Gordon Wetzstein, and Michael Zollh\u00a8ofer. Deep-\nVoxels: Learning persistent 3D feature embeddings.\nIn\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 2437\u20132446, 2019. 3\n[70] Supasorn Suwajanakorn, Ira Kemelmacher-Shlizerman, and\nSteven M Seitz. Total moving face reconstruction. In Euro-\npean Conference on Computer Vision (ECCV), pages 796\u2013\n812, 2014. 3\n[71] Ayush Tewari, Florian Bernard, Pablo Garrido, Gaurav\nBharaj, Mohamed Elgharib, Hans-Peter Seidel, Patrick\nP\u00b4erez, Michael Zollh\u00a8ofer, and Christian Theobalt.\nFML:\nFace model learning from videos. In Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 10812\u2013\n10822, 2019. 3\n[72] Ayush Tewari, Michael Zollh\u00a8ofer, Pablo Garrido, Florian\nBernard, Hyeongwoo Kim, Patrick P\u00b4erez, and Christian\nTheobalt. Self-supervised multi-level face model learning\nfor monocular reconstruction at over 250 Hz. In Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n2549\u20132559, 2018. 2\n[73] Ayush Tewari, Michael Zollh\u00a8ofer, Hyeongwoo Kim, Pablo\nGarrido, Florian Bernard, Patrick Perez, and Christian\nTheobalt. MoFA: model-based deep convolutional face au-\ntoencoder for unsupervised monocular reconstruction. In In-\nternational Conference on Computer Vision (ICCV), pages\n1274\u20131283, 2017. 2\n[74] Justus Thies, Michael Zollh\u00a8ofer, Marc Stamminger, Chris-\ntian Theobalt, and Matthias Nie\u00dfner. Face2Face: Real-time\nface capture and reenactment of RGB videos. In Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n2387\u20132395, 2016. 2\n[75] Anh Tuan Tran, Tal Hassner, Iacopo Masi, and Gerard\nMedioni.\nRegressing robust and discriminative 3D mor-\nphable models with a very deep neural network. In Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\npages 1599\u20131608, 2017. 3\n[76] Xiang Wang, Chen Wang, Bing Liu, Xiaoqing Zhou, Liang\nZhang, Jin Zheng, and Xiao Bai.\nMulti-view stereo in\nthe deep learning era: A comprehensive review. Displays,\n70:102102, 2021. 3\n[77] Huawei Wei, Shuang Liang, and Yichen Wei. 3D dense face\nalignment via graph convolution networks. arXiv preprint\narXiv:1904.05562, 2019. 3\n[78] Erroll Wood, Tadas Baltrusaitis, Charlie Hewitt, Matthew\nJohnson, Jingjing Shen, Nikola Milosavljevic, Daniel Wilde,\nStephan Garbin, Chirag Raman, Jamie Shotton, Toby Sharp,\nIvan Stojiljkovic, Tom Cashman, and Julien Valentin. 3D\nface reconstruction with dense landmarks. In European Con-\nference on Computer Vision (ECCV), pages 160\u2013177, 2022.\n2, 3\n[79] Chenglei Wu, Takaaki Shiratori, and Yaser Sheikh. Deep\nincremental learning for efficient high-fidelity face tracking.\nTransactions on Graphics (TOG), 37(6):234, 2018. 2\n[80] Fanzi Wu, Linchao Bao, Yajing Chen, Yonggen Ling, Yibing\nSong, Songnan Li, King Ngi Ngan, and Wei Liu. MVF-Net:\nMulti-view 3D face morphable model regression. In Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\npages 959\u2013968, 2019. 2, 3\n[81] Haotian Yang, Hao Zhu, Yanru Wang, Mingkai Huang, Qiu\nShen, Ruigang Yang, and Xun Cao. FaceScape: A large-\nscale high quality 3D face dataset and detailed riggable 3d\nface prediction. In Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 598\u2013607, 2020. 2\n[82] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long\nQuan.\nMVSNet: Depth inference for unstructured multi-\nview stereo. In European Conference on Computer Vision\n(ECCV), volume 11212, pages 785\u2013801, 2018. 3\n[83] Chao Zhang, William A. P. Smith, Arnaud Dessein, Nick E.\nPears, and Hang Dai. Functional faces: Groupwise dense\ncorrespondence using functional maps.\nIn Conference on\nComputer Vision and Pattern Recognition (CVPR), pages\n5033\u20135041, 2016. 2\n[84] Mingwu Zheng, Hongyu Yang, Di Huang, and Liming Chen.\nImFace: A nonlinear 3D morphable face model with implicit\nneural representations. In Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 20311\u201320320, 2022.\n2, 8\n[85] Zerong Zheng, Tao Yu, Qionghai Dai, and Yebin Liu. Deep\nimplicit templates for 3D shape representation. In Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\npages 1429\u20131439, 2021. 8\n[86] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao\nLi. On the continuity of rotation representations in neural\nnetworks. In Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 5745\u20135753, 2019. 4\n[87] Wojciech Zielonka, Timo Bolkart, and Justus Thies. Towards\nmetrical reconstruction of human faces. In European Con-\nference on Computer Vision (ECCV), pages 250\u2013269, 2022.\n3\n[88] Michael Zollh\u00a8ofer, Justus Thies, Pablo Garrido, Derek\nBradley, Thabo Beeler, Patrick P\u00b4erez, Marc Stamminger,\nMatthias Nie\u00dfner, and Christian Theobalt. State of the art\non monocular 3D face reconstruction, tracking, and appli-\ncations. Computer Graphics Forum (CGF), 37(2):523\u2013550,\n2018. 2\nA. Appendix\nMulti-view setup: TEMPEH infers 3D head meshes in correspon-\ndence from calibrated multi-view images. Specifically, we use the\neight pairs of gray-scale stereo images of an active stereo camera\nsystem as input (see Sec. 4 of the paper for details). Figure 6 shows\nFigure 6. Multi-view setup. The 16 gray-scale stereo images\n(contrast enhanced for visualization) used as input to TEMPEH.\nthe 16 images for the first sample of the paper\u2019s teaser figure.\nHead localization: The coarse head prediction stage localizes the\nhead in the feature volume with a learnable spatial transformer.\nFigure 7 visualizes the spatial dimensions of the localized volume\nfor different predicted heads of the coarse stage. This shows that\nthe spatial transformer successfully localizes the head for different\nsubjects in varying head poses.\nFigure 7. Head localization. Spatial dimensions of the localized\nfeature volumes (blue box) for different predicted heads, visual-\nized in the same global coordinate system.\nFigure 8. Error masks. Head regions (red) for quantitative evalu-\nations. From left to right: complete head, face, scalp, and neck.\nError masks: To analyze the accuracy in different head regions,\nreconstruction errors are reported individually for the face, scalp,\nand neck regions.\nFor this, different regions are defined on a\nFLAME template mesh (see Figure 8), and each scan is then seg-\nmented based on the distance to the closest points in the surface of\nthe reference registrations.\nTest evaluation: Figure 9 provide the cumulative reconstruction\nerrors for the FaMoS test data. TEMPEH predicts heads for the\ntest images (subjects disjoint from the training subjects) with a\nlower error than previous state-of-the art, ToFu [50], and its variant\nwithout mesh hierarchy, ToFu+.\n0\n1\n2\n3\n4\n5\n6\n7\nError [mm]\n0\n20\n40\n60\n80\n100\nPercentage\nToFu\nToFu+\nOurs\n0\n1\n2\n3\n4\n5\n6\n7\nError [mm]\n0\n20\n40\n60\n80\n100\nPercentage\nToFu\nToFu+\nOurs\nComplete head\nFace\n0\n1\n2\n3\n4\n5\n6\n7\nError [mm]\n0\n20\n40\n60\n80\n100\nPercentage\nToFu\nToFu+\nOurs\n0\n1\n2\n3\n4\n5\n6\n7\nError [mm]\n0\n20\n40\n60\n80\n100\nPercentage\nToFu\nToFu+\nOurs\nScalp\nNeck\nFigure 9. Quantitative evaluation. Cumulative plots of the re-\nconstruction errors on the FaMoS test data.\n3DMM regressor comparisons: For the multi-view 3DMM re-\ngressor, each image is processed by a shared ResNet152 [38] to\ninfer a 2048-dimensional feature vector for each view. The fea-\nture vectors are then fused across all 16 views by concatenating\nthem in a fixed order. We experimented with other feature fusion\nvariants such as using the mean across views, or the concatenated\nmean and variance, but these variants produced inferior results.\nFollowing DECA [25], a fully-connected layer with ReLU activa-\ntions outputs a 1024-dimensional feature vector, followed by final\nlinear layer to output FLAME parameters. We train the 3DMM\nComplete head\nFace\nScalp\nNeck\nMethod\nMedian \u2193\nMean \u2193\nStd \u2193\nMedian \u2193\nMean \u2193\nStd \u2193\nMedian \u2193\nMean \u2193\nStd \u2193\nMedian \u2193\nMean \u2193\nStd \u2193\nOurs (coarse)\n0.80\n1.61\n3.86\n0.67\n0.85\n1.31\n0.84\n2.31\n5.59\n1.11\n1.68\n2.34\nOurs\n0.17\n0.30\n0.97\n0.14\n0.23\n1.10\n0.16\n0.24\n0.39\n0.24\n0.53\n1.46\nTable 3. Registration quality. Reconstruction errors on the FaMoS training data. All errors are in millimeter.\nregressor for 1 Million iterations with a vertex-to-vertex to the ref-\nerence registrations, with a learning rate of 1e-3. We found that\nthe 3DMM regressor is unable to reliably reconstruct 3D heads in\nour setting (see Fig. 10).\nInput\nRegressor\nOurs\nScan\nFigure 10. 3DMM regressor comparison. For multi-view in-\nput (left: 4 of 16 views), the 3DMM regressor regressor (second\ncolumn) is unable to faithfully reconstruct the identity face shape,\nwhile TEMPEH (third column) closely resembles the reference\nscan (right).\nRegistration quality: The training of TEMPEH minimizes the\ndistance to MVS scans, hence it effectively registers the scans. For\ncompleteness, we also report the registration errors in Table 3. For\nthis, we predict all heads from the training images, and compute\nthe point-to-surface distance for all MVS scan points.\nAblation experiments: Figure 11 shows additional ablation re-\nsults. While the model variants without head localization (Ours\nw/o head localization) or with a hierarchical architecture (Ours hi-\nerarchical) produce reconstructions with low distance to the refer-\nence scans, they reconstruct the lip region with lower fidelity than\nthe final model.\nFailures: TEMPEH\u2019s coarse stage reconstruction can fail under\nlarge occlusions due to extreme head poses (see Figure 12). We\nfound empirically that training the coarse stage for 250K more\niterations improves the quality of the reconstructed head meshes\nfor such extreme head poses.\nComputational requirements: TEMPEH and the baseline mod-\nels are trained/evaluated on a computing unit with a single\nNVIDIA A100-SXM 80 GB GPU and 16 CPU cores.\nTrain-\ning TEMPEH/ToFu/ToFu+ allocates 26/4/14 GB GPU memory\nfor the coarse stage, 37/34/37 GB for refinement, and up to 21\nGB RAM. GPU memory is mainly allocated in the volumetric\nfeature sampling and the probability volume prediction for the 2\n(batch) \u00d75023 local grids of size 83. Training TEMPEH takes 6\ndays (3.5/2.5 days for coarse / refinement). Inference for TEM-\nPEH/ToFu/ToFu+ allocates 6/4/6 GB GPU memory for the coarse\nstage and 10/6/8 GB for refinement.\nRunning time evaluation: TEMPEH targets the typical two-step\nprocess of reconstructing 3D meshes in correspondence, MVS,\nfollowed by non-rigid registration. This pipeline takes \u2265 10 min-\nutes per mesh (Tab. 1 [50]), while TEMPEH takes 0.27s. While\nToFu/ToFu+ is even faster with 0.16/0.18s, TEMPEH reconstructs\n3D heads with a 64% lower error. The time difference between\nToFu/ToFu+ and TEMPEH is mainly due to the visibility compu-\ntation in the surface-aware feature fusion. TEMPEH w/ na\u00a8\u0131ve fea-\nture fusion requires 0.17s, comparable to ToFu/ToFu+. The coarse\nmodel inference accounts for about 0.03s for all models. The fast\ninference speed is due to downsampling of the input images, and\ndue to parallelization. Specifically, the feature extraction is paral-\nlelized across images (stacked across the batch dimension), while\nfeature sampling & aggregation, head inference, and mesh refine-\nment are parallelized across all points.\nData diversity: FaMoS data are female/male: 52/41; age 18-34:\n65, 35-50: 14, 51-69: 13, 70+: 1; Middle-Eastern: 6, South Amer-\nican: 10, Asian: 24, Pacific Ocean: 1, African: 3, European: 49.\nWe provide self-identified ethnicity labels as provided by each par-\nticipant with the dataset.\nModel architecture: TEMPEH uses volumetric features to local-\nize, infer and then refine the output mesh. These features are ex-\ntracted from the input images with two separate 2D feature extrac-\ntion networks Fimg, one for coarse head prediction (Section 3.1)\nand one for head refinement (Section 3.2). Both networks use a\nfully-convolutional U-Net [63] architecture with a ResNet34 [38]\nbackbone. Both feature networks take downsampled images as in-\nput (i.e., images of size w = 200, h = 150 for the coarse stage,\nand w = 400, h = 300 for the refinement stage), and output a fea-\nture map F with the same spatial resolution as the image, with\na feature dimension of 8. We empirically found that adding two\nadditional skip connections for the feature networks compared to\nToFu\u2019s implementation improved the reconstruction performance\nof the refinement network. For a fair comparison to ToFu and\nToFu+, we use the same feature extractor networks with added\nskip connections for all models.\nThe reconstruction networks in coarse and refinement stages,\nFrec and Fref, respectively, are both 3D U-Nets [40]. Similar to\nToFu, the coarse stage reconstruction network Frec has five down-\nand upsampling blocks, with a slight modification of the third last\nand second last convolution blocks, which output 64 and 128 chan-\nnels (instead of 32 for ToFu). The refinement stage reconstruction\nnetwork Fref follows a similar structure, but with three down- and\nupsampling layers, same as ToFu.\n0 mm\n>3 mm \nInput (4 of 16 views)\nCoarse w/o s2m\nCoarse w/o head localization\nOurs (coarse)\n0 mm\n>3 mm \nNa\u00a8\u0131ve feature fusion\nOurs w/o s2m\nOurs w/o head localization\nOurs\n0 mm\n>3 mm \nOurs hierarchical\nColor input (4 of 8 views)\nOurs color\nFigure 11. Additional ablation experiments. For each model variant, we show the reconstructed mesh (left) and the color coded point-\nto-surface distance (right) between reference scan and reconstructed mesh as heatmap on the scan\u2019s surface (red means \u2265 3 millimeter).\nFigure 12. Failures. For input images (left: 10 of 16 views) where\nthe face is occluded in most views, the coarse stage reconstruc-\ntion can fail (middle), resulting in poorly estimated head meshes.\nLonger training of the coarse stage can improve the reconstruction\nperformance for such extreme cases (right).\n"
  },
  {
    "title": "GPT-Calls: Enhancing Call Segmentation and Tagging by Generating Synthetic Conversations via Large Language Models",
    "link": "https://arxiv.org/pdf/2306.07941.pdf",
    "upvote": "3",
    "text": "GPT-CALLS: ENHANCING CALL SEGMENTATION AND TAGGING BY GENERATING\nSYNTHETIC CONVERSATIONS VIA LARGE LANGUAGE MODELS\nItzik Malkiel1\u2217 Uri Alon1\u2217 Yakir Yehuda1,2 Shahar Keren1 Oren Barkan1,3 Royi Ronen1 Noam Koenigstein1,4\n1Microsoft, 2Technion, 3The Open University, 4Tel-Aviv University\nABSTRACT\nTranscriptions of phone calls are of significant value across\ndiverse fields, such as sales, customer service, healthcare, and\nlaw enforcement. Nevertheless, the analysis of these recorded\nconversations can be an arduous and time-intensive process,\nespecially when dealing with extended or multifaceted di-\nalogues.\nIn this work, we propose a novel method, GPT-\ndistilled Calls Segmentation and Tagging (GPT-Calls), for ef-\nficient and accurate call segmentation and topic extraction.\nGPT-Calls is composed of offline and online phases. The of-\nfline phase is applied once to a given list of topics and involves\ngenerating a distribution of synthetic sentences for each topic\nusing a GPT model and extracting anchor vectors. The on-\nline phase is applied to every call separately and scores the\nsimilarity between the transcripted conversation and the topic\nanchors found in the offline phase. Then, time domain analy-\nsis is applied to the similarity scores to group utterances into\nsegments and tag them with topics. The proposed paradigm\nprovides an accurate and efficient method for call segmenta-\ntion and topic extraction that does not require labeled data,\nthus making it a versatile approach applicable to various do-\nmains. Our algorithm operates in production under Dynamics\n365 Sales Conversation Intelligence, and our research is based\non real sales conversations gathered from various Dynamics\n365 Sales tenants.\n1. INTRODUCTION\nIn today\u2019s highly competitive market, sales agents play a crit-\nical role in driving sales and maintaining a strong customer\nbase. One of the primary ways they interact with customers\nis through phone calls. These phone calls are often recorded\nand transcribed for quality assurance, training and coaching\nother sellers, curating insights, and more. Therefore, sales\ndepartments often maintain an extensive database consisting\nof millions of call transcriptions, which serves as an important\nsource of information for numerous tasks. However, analyz-\ning recorded calls can be a challenging and time-consuming\ntask, particularly when the conversations are lengthy or cover\nmultiple topics.\n\u2217 Denotes equal contribution.\nA common approach is to segment calls by post-processing\nthe recorded text and assigning a topic to each segment. This\nsimplifies the process for sellers to locate and extract im-\nportant information from previous calls. Call segmentation\ninvolves breaking down conversations into smaller sections\nbased on specific topics or themes discussed. This segmen-\ntation and tag capability greatly facilitates the day-to-day\ntasks of sellers and their managers. It enables sales agents\nand managers to easily track and analyze past calls, catego-\nrize them based on conversation topics, navigate to relevant\nparts of a call to extract crucial information, improve search\nengines that work with transcribed calls, and more.\nMoreover, automated segmentation and tagging of recorded\ncalls can help businesses optimize their sales strategies and\nprocesses across several dimensions:\n\u2022 Providing personalized coaching to sellers. Managers\ncan listen to specific parts of the call to provide feed-\nback and guidance, rather than listening to the entire\ncall.\n\u2022 Providing insights into customer needs, preferences,\nand pain points. Segmented and tagged information\ncan be used to tailor sales strategies and improve cus-\ntomer experience.\n\u2022 Monitoring compliance with legal and ethical guide-\nlines in sales conversations. Ensuring that sellers are\nadhering to company policies and regulations.\n\u2022 Generating reports and dashboards that provide insights\ninto sales performance and customer behavior. This in-\nformation can be used to make data-driven decisions\nand improve overall business outcomes.\n\u2022 Identifying key moments in the call that may be related\nto successful outcomes. This information can be used\nto train sales agents and improve their performance.\nUltimately, all of the above lead to increased customer satis-\nfaction, loyalty, sales, and revenue generation. Unfortunately,\ndespite many advances in the field, current techniques for call\nsegmentation and tagging have limitations that hinder their\nimpact and penetration on the day-to-day work of sellers and\ntheir managers. Specifically, they often produce sub-optimal\narXiv:2306.07941v1  [cs.CL]  9 Jun 2023\nFig. 1. The Calls Summary dashboard. Executive summary (left), suggested notes (middle), and the transcripted conversation\n(right) are presented. The predicted segmentation and their associated topics are showcased at the bottom, integrated with a\ncorresponding timeline bar representing the duration of the recorded call.\naccuracy, struggle to represent various topics, and require sig-\nnificant labeled data and domain expertise to produce accurate\nground truth segmentation. As a result, businesses have been\nslow to adopt post-processing segmentation in recorded calls.\nIn this work, we propose a method that overcomes these\nlimitations and provides accurate call segmentation and topic\nextraction without the need for labeled data. Based on a GPT\nmodel, our method generates synthetic data that mimics the\nnatural language used in recorded calls, while focusing on a\npre-defined set of topics. The synthetic data is then used by a\nsmaller Transformer network to accurately segment calls and\ntag them with topics.\nOur method, which we refer to as GPT-Calls, is designed\nto efficiently identify the topics discussed during a phone con-\nversation by analyzing the similarity between the conversa-\ntion utterances and predefined topic anchors. The algorithm\nconsists of offline and online phases. In the offline phase,\nGPT-Calls generates a distribution of synthetic sentences for\neach topic using a GPT model [1, 2]. Then, it embeds and\nclusters the sentences separately to extract anchor vectors. In\nthe online phase, GPT-Calls processes a conversation tran-\nscript by embedding each utterance using a language model\nand scoring it for similarity with the topic anchors. Then, it\napplies time domain analysis over the scores of consecutive\nutterances and groups them into segments that are distinctly\nidentified with a particular topic. The remaining segments\nthat do not correlate with any of the topics are considered\n\u201cbackground\u201d.\nThe GPT-Calls method offers an effective and precise so-\nlution for call segmentation and topic extraction that can cater\nto any topic without the need for labeled data. In this way, or-\nganizations can select a list of customized topics that are rele-\nvant to their particular business needs. Additionally, the algo-\nrithm is generic and can be implemented in diverse domains,\nsuch as customer support, sales conversations, surveys, and\nmore. Finally, the online phase of the system can be applied in\nnear-real-time scenarios, enabling the segmentation and tag-\nging of an entire call within two-three seconds using standard\nhardware. The GPT-Calls was recently adopted in Dynamics\n365 Sales, and its predicted segmentations are apparent in the\nlower part of the Calls Summary dashboard (see Figure 1).\nOur contributions are as follows: (1) we introduce the\nGPT-Calls scheme, a general method designed for the analy-\nsis of recorded calls in any domain. (2) we evaluate and report\nthe performance of GPT-Calls on a diverse dataset of calls\nfrom multiple domains and tenants. (3) we compare our pro-\nposed method against other state-of-the-art alternatives. Our\nresults demonstrate that the GPT-Calls method outperforms\nother approaches by a sizeable margin, and across all datasets.\nPrompt:\n\"an agent and\na costumer\ndiscussing\npricing\"\nGPT-3\nOffline\n\"...We have\npackages\nstarting from\n$500 up to\n$5000...\"\nLanguage\nModel\nTopic embedding\nclustering\n\"Good\nmorning, this is\nJohn speaking\nfrom ABC. Do\nyou have a few\nminutes?\"\nOnline\nTime\nDomain\nanalysis\nLanguage\nModel\nVectorization\nTopic probabilities\nTime domain\nTime domain\nGreeting\nSchedule\nEnding\nFig. 2. An illustration of the offline and online phases of GPT-Calls.\n2. RELATED WORK\nIn recent years, there has been a growing interest in text\nsegmentation, which involves dividing text passages into co-\nherent segments based on their content. While traditional text\nsegmentation methods have relied on features such as punc-\ntuation, paragraph breaks, or rule-based approaches, these\nmethods may not always capture the underlying semantic\nstructure of the text.\nTo address this challenge, several recent studies have pro-\nposed using semantic word embeddings to identify segments\nbased on the coherence of the relationships between words.\nOne such study proposes a method for text segmentation\nbased on semantic word embeddings [3]. In this work, the\nauthors use a pretrained word embedding model to gener-\nate embeddings for each word in the text and then apply a\ngreedy algorithm to group the words into segments. The au-\nthors demonstrate that their approach outperforms traditional\nmethods in terms of segmentation accuracy and that the iden-\ntified segments correspond well to the topics discussed in the\ntext.\nIn the experiments section, we compare our method\nwith the previous method used in Dynamics 365 Sales for\ncall segmentation, which was based on the same approach of\n[3].\nA different direction has been to employ text summa-\nrization methods for topic tagging. This approach involves\nassigning one or more tags to a given text to represent its\nkey topics or themes.\nOne such approach, known as ex-\ntractive summarization, involves extracting key sentences\nfrom the text and using them to tag the text with pre-defined\ntopics. Another approach, known as abstractive summariza-\ntion, employs neural-based summarization models, such as\nsequence-to-sequence models or transformer models, to gen-\nerate concise summaries that can be used to predict relevant\ntags. The latter approach can utilize the PEGASUS summa-\nrization model [4], which generates summaries that can ease\nthe process of predicting tags (compared to predicting tags\nfor the original text). The previous method used in Dynam-\nics 365 sales built upon the PEGASUS model to summarize\nthe segment and infer a relevant topic.\nMore details and\nevaluations for this method can be found in Sec.4.\nTextTiling [5] is a prominent text segmentation algorithm\nthat efficiently divides long text into coherent topical sec-\ntions. It utilizes local lexical cohesion and focuses on identi-\nfying abrupt shifts in the thematic structure of a text, which\nare indicative of topic boundaries. By employing statistical\ntechniques such as tokenization, similarity measurement, and\nsmoothing, TextTiling extracts informative features from the\ntext and clusters them into distinct sections, allowing for bet-\nter understanding and organization of large text corpora. This\nversatile segmentation algorithm has been widely applied in\nvarious natural language processing tasks, including informa-\ntion retrieval, summarization, and text classification, making\nit a valuable tool for researchers in the field of computational\nlinguistics.\nTopic segmentation of meetings has gained significant\nattention as a means to automatically partition meeting tran-\nscripts into coherent segments, facilitating a better under-\nstanding of the discourse structure. In [6], the authors pro-\npose an unsupervised method for meetings segmentation by\nBERT embeddings (BERT-TSeg). Their method builds upon\nBERT to generate contextualized word representations for\neach utterance in the transcript. By leveraging these embed-\ndings, BERT-TSeg computes the similarity between adjacent\nFig. 3. Representative synthetic sentences generated by GPT-\n3 for the topic pricing.\nsegments and constructs a similarity matrix. Subsequently,\na hierarchical clustering algorithm is applied to this matrix\nto identify topic boundaries.\nIn this study, we conduct a\ncomprehensive evaluation and comparison between our pro-\nposed method and BERT-TSeg to assess the advancements\nand efficacy of our approach in meeting topic segmentation.\n3. METHOD\nThe GPT-Calls algorithm consists of two distinct phases: an\noffline phase and an online phase. The offline phase uses a\nGPT model to generate synthetic data. This phase is applied\nonce for a user-defined list of topics. After the offline phase,\nthe algorithm invokes an online phase that reinforces repre-\nsentations extracted during the offline phase to accurately pre-\ndict segmentation and topics for a given call.\n3.1. Offline Phase\nIn the offline phase, the algorithm utilizes a GPT model to\ngenerate synthetic data, leveraging a given list of desired top-\nics chosen by the user. This phase is executed only once for\na specific set of desired topics. Subsequently, in the online\nphase, the algorithm uses the representations extracted dur-\ning the offline phase to predict segmentation and topics for\nindividual calls.\nSpecifically, for each topic, we build a prompt1 and use\nGPT-3 to generate thousands of sentences that are semanti-\ncally correlated with the topic.\nA representative sample of sentences generated for the\n\u201cpricing\u201d topic is shown in Figure 3.\nAn example of our\nprompts can be found in the Appendix Section A.\nSentences produced for each subject are embedded via a\npretrained Sentence-BERT (SBERT) model [7]. This model,\n1The prompts contain one example of a few sentences and a topic which\nare followed by a request for the model to generate another example for the\nquery topic\nan adjustment of the standard BERT model [8], is devised\nto encode entire sentences into a 384-dimensional embedding\nvector, enabling the use of the cosine similarity metric to mea-\nsure semantic similarity.\nThen, the DBSCAN [9] algorithm is applied on the sen-\ntence embeddings of each topic, in order to extract a set of\nmultiple \u201canchors\u201d representing the distribution of the topic.\nDBSCAN is a density-based clustering algorithm that groups\ndata into clusters based on the density of samples.\nHigh-\ndensity regions are grouped into a cluster, and samples in\nlow-density areas are marked as outliers. For each topic, DB-\nSCAN is applied to retrieve a set of clusters. The center of\neach cluster is extracted and used as an anchor.\nThese anchors will be used during the online phase to in-\nfer the topic probabilities for each utterance in the call. The\npipeline of the offline phase is illustrated in Figure 2 (upper\npart).\nOverall, the offline phase involves defining topics, gen-\nerating synthetic sentences, embedding them using SBERT,\nclustering the embeddings using DBSCAN, and extracting\nanchors. The anchors will be used later during the online\nphase to infer the probabilities of the topics.\n3.2. Online Phase\nIn the online phase, GPT-Calls operates on the transcriptions\nof the recorded conversations and predicts the topic probabil-\nities for each utterance in a given conversation. An utterance\nis an atomic unit of speech, which is mostly converted to a\nsingle sentence or a sub-sentence by the transcription model.\nThe method employs the Azure Cognitive Service tran-\nscription model2 and embeds the resulting transcripted ut-\nterances through SBERT. GPT-Calls then iterates over the\nembedding of each transcripted utterance, scoring its similar-\nity with all anchors of the pre-defined topics. For each topic,\nan utterance-topic score is defined by the maximal cosine\nsimilarity between the transcripted utterance embedding and\nthe anchors associated with the topic (which are also vectors\nin the same latent space, as described in the offline phase).\nThe utterance-topic scores for each topic are transformed into\nprobabilities using the Softmax function. By performing this\nprocess, one obtains a sequence of vectors, where each vector\nrepresents the probability that the corresponding utterance\nrelates to each of the topics.\nTo improve the accuracy of the topic probabilities, GPT-\nCalls applies a time-domain analysis to the above sequence,\ntreating it as a multivariant time series. It identifies the peak\npoints in each dimension of the time series, referred to as\n\u201cheat sources\u201d. GPT-Calls then applies a heat diffusion strat-\negy to the neighboring samples surrounding each heat source.\nFor every sample in the sequence and across each dimension,\nGPT-Calls calculates the distance to the nearest right and left\n2https://azure.microsoft.com/en-us/products/cognitive-services/speech-\nto-text/\nheat sources, strengthens the probability of the current sam-\nple\u2019s topic in proportion to the value of the closest right and\nleft heat sources, and decays the probability by the distance.\nIn other words, the probabilities of samples that are close to\nother samples that highly correlate with a specific topic will\nbe slightly promoted toward the same topic.\nThis approach intends to counteract the presence of noisy\ndata samples which often manifest in particular topics such as\nidentification.\nAfter the heat diffusion process, a Softmax function is ap-\nplied again to each utterance to assure its scores across the\ntopics are valid probabilities that sum to 1. GPT-Calls then\napplies a sliding window technique to tag windows of con-\nsecutive utterances with topics. Different window widths are\nutilized for each topic, which are hyperparameters determined\nindividually for each topic using a validation set. The cumu-\nlative probability of a specific topic is computed for each win-\ndow by averaging the relevant probabilities of all utterances\nwithin that window. If the cumulative probability exceeds a\npredefined threshold (configurable for each topic), the win-\ndow is labeled with the corresponding topic. This process is\nrepeated for all topics.\nAt the end of the above process, the sequence is associated\nwith windows tagged with topics. GPT-Calls then iterates\nthrough the tagged windows and merges consecutive win-\ndows with the same topic. If a window or sub-window was\ntagged with more than one topic, the leading topic (with the\nhighest score) is chosen, and the other windows are updated\naccordingly. The predicted segmentation and tagging are re-\ntrieved as the final output. A visualization of the predicted\nsegmentation and tags along with the underlying probabilities\nof each utterance can be seen in Figure 4. The pipeline of the\nonline phase is illustrated in Figure 2 (bottom part).\n4. EXPERIMENTS\nIn our evaluation, we compare GPT-Calls with baseline meth-\nods mentioned in the related work section, including TextTil-\ning [5] and the unsupervised BERT-TSeg method [6]. Text-\nTiling employs a sliding window approach based on lexical\ncohesion to identify coherent segments within a document.\nBERT-TSeg, on the other hand, utilizes BERT embeddings\nand hierarchical clustering to identify topic boundaries in\nmeeting transcripts.\nWe also compare to the previous model used in Dynam-\nics 365 Sales, which employed a technique that utilizes a\nGreedy Segmentation approach followed by a GPT-based\ndistilled Summary for Tagging (GSGST). GSGST first ap-\nplies a segmentation procedure and then tags each segment\nwith a relevant topic. The segmentation begins by embedding\nall the utterances in a given call using a pre-trained SBERT\nmodel. Then, the segments are inferred by employing the\ngreedy method introduced in [3]3 over the utterances embed-\n3The\nimplementation\ncan\nbe\nfound\nhere\nFig. 4. Topic score (Y-axis) vs utterance index (X-axis) of a\nrepresentative call with 67 utterances. All topic scores of all\nutterances are shown. Each topic score is presented by a solid\nline with a different color. The final retrieved segments are\nmarked over the relevant utterances as a colored background\nby the corresponding topic color.\nding. Given a call with N embedded utterances (w1, ..., wN),\na segment V = (wb, ..., we) where b, e are the beginning and\nending indices4 of V (0 \u2264 b + 1 < e \u2264 N) , and a split index\nb < t < e, the gain of splitting V at position t is defined as:\nge\nb(t) :=\n\r\r\r\r\r\nt\u22121\nX\ni=b\nwi\n\r\r\r\r\r +\n\r\r\r\r\r\ne\nX\ni=t\nwi\n\r\r\r\r\r \u2212\n\r\r\r\r\r\ne\nX\ni=b\nwi\n\r\r\r\r\r\nThe greedy approach calculates the index that maximizes\nthis term\nt\u2217 := arg max\nt\nge\nb(t)\n(1)\nThen, if ge\nb(t\u2217) > \u03c4 where \u03c4 > 0 is a pre-defined threshold,\nthe segment V is splited into two segments by around t\u2217.\nThe method begins by initiating b and e as the first and last\nutterances of the conversation respectively. The process pro-\nceeds recursively to all segments and stops when there is no\ncandidate split whose gain is above the pre-defined threshold,\nor the current number of segments has reached the maximum\ndefined by the user. Finally, the last segmentation is retrieved\nas an output.\nGiven a predicted segmentation, two models are em-\nployed to predict a relevant topic, a zero-shot model intro-\nduced in [10] and a summarization model based on PEGA-\nSUS [4]. If the first model fails to predict a topic with high\nconfidence, the second model is used.\nThe second model is based on the PEGASUS model,\nwhich was fine-tuned on sales calls where the labels were\nsummaries generated by GPT [11].\nhttps://github.com/chschock/textsplit\n4The minimal size of a segment is 3\nSports\nIT\nDiverse\nPk\nWinDiff\nPk\nWinDiff\nPk\nWinDiff\nTextTiling\n0.66 \u00b1 0.07\n0.89 \u00b1 0.12\n0.65 \u00b1 0.08\n0.93 \u00b1 0.12\n0.66 \u00b1 0.10\n0.92 \u00b1 0.12\nBERT-TSeg\n0.36 \u00b1 0.08\n0.36 \u00b1 0.08\n0.34 \u00b1 0.10\n0.35 \u00b1 0.10\n0.34 \u00b1 0.10\n0.35 \u00b1 0.10\nGSGST\n0.33 \u00b1 0.09\n0.34 \u00b1 0.08\n0.31 \u00b1 0.12\n0.32 \u00b1 0.11\n0.32 \u00b1 0.11\n0.33 \u00b1 0.11\nGPT-Calls\n0.29 \u00b1 0.13\n0.31 \u00b1 0.13\n0.29 \u00b1 0.10\n0.30 \u00b1 0.10\n0.29 \u00b1 0.14\n0.31 \u00b1 0.11\nTable 1. Pk and WinDiff scores for each model and dataset, reported as the mean values with standard deviations (Mean \u00b1\nSD).\nThe method assumes that each topic is associated with\na small set of representative sentences, created by an expert\n(typically around 2-20 sentences). For example, for the topic\n\u201cpricing\u201d the corresponding set contains 9 sentences, two of\nthem are: \u201cthe agent and customer discussed the price of\nthe product\u201d, \u201cthe customer asked for a better price\u201d, and so\non. These sentences were separately embedded by an SBERT\nmodel and were mean pooled to extract a single anchor rep-\nresenting the topic.\nThis model produces a single-sentence summary for ev-\nery segment.\nThe summarized segment is then embedded\nusing SBERT and is compared to the single anchor of each\ntopic. The predicted topic is the one that maximizes the cosine\nsimilarity with the summarized sentence embedding. Finally,\npost-processing is performed to filter out extremely short seg-\nments, merge adjacent segments with identical topics, and so\non.\n4.1. Metrics\nTo evaluate the performance of the models, we calculate two\nmetrics, Pk score [12] and WindowDiff [13].\nIn order to measure the performance on each topic sepa-\nrately, the metrics are computed over the binary segmentation\nobtained by the predicted and ground truth segmentation of\neach of the topics. For example, when evaluating the topic\n\u201cpricing\u201d, all the segments that are not associated with pricing\nwere considered as background. The Pk score is computed us-\ning a sliding window-based method, where the window size\nwas set to half of the average true segment number. The met-\nric determines whether the two ends of the window were in\nthe same or different segments in the ground truth segmenta-\ntion, and increased a counter if there was a mismatch. The\nfinal score is calculated by scaling the penalty between 0 and\n1 and dividing the number of measurements.\nTo overcome the challenges of the Pk score (which penal-\nizes false negatives more than false positives and does not take\ninto account the number of boundaries), we also calculated\nthe WindowDiff metric. Similar to the Pk score, the Win-\ndowDiff is also calculated using a sliding window. For each\nposition of the window of size k, the number of boundaries in\nthe ground truth segmentation is compared to the number of\nboundaries predicted by the model.\n4.2. Test data\nWe employed a human expert with relevant domain expertise\nto annotate three test sets of real conversations taken from\nvarious Dynamics 365 sales tenants5. The human expert seg-\nmented each conversation and annotate each segment with\none of the following topics: greetings, closing, pricing, iden-\ntification, and scheduling. The annotations were used as the\nground truth segmentation for evaluating the performance of\nthe models.\nThe first set, referred to as the \u201cSports\u201d set, contains\n\u223c 200 sports-related conversation, where various sellers\nfrom sport companies discuss with their clients about sub-\nscription renewal, tickets ordering, seats changes and so\non. The second set, named the \u201cIT\u201d set, comprises \u223c 100\nrecorded conversations of IT sellers reaching out to customers\nto propose software services, negotiate contracts, and address\ncustomer inquiries within the IT domain. Lastly, the third\nset, labeled as the \u201cDiverse\u201d set, consists of \u223c 200 conver-\nsations of sale agents from various fields, including finance,\ntechnology, billing companies, medical marketing and more.\n4.3. Results\nTable 1 presents the segmentation performance of all base-\nlines across the three datasets mentioned earlier, evaluated us-\ning the ground truth human annotations. In this evaluation, we\nfocus solely on measuring the accuracy of the segmentation\nand do not consider the topics associated with the segments.\nAs can be seen in the table, GPT-Calls outperforms all other\nalternatives by a sizeable margin. Specifically, compared to\nthe second-best performing method, GSGST, we observe a\nrelative improvement of \u223c 12%, \u223c 8%, in the Pk and Win-\nDiff scores, respectively, for the Sports dataset. Furthermore,\nthe proposed method shows even larger gains compared to the\nremaining baselines and across all three datasets.\nIn Table 2, we present our quantitative evaluations for all\nfive topics: Identification, Pricing, Schedule, Greeting, and\nClosing. For these evaluations, we utilize the annotated test\nset and compare the performance of the GSGST model with\n5with the tenant\u2019s consent and after all personally identifiable information\nwas removed\nIdentification\nPricing\nSchedule\nGreetings\nClosing\nGSGST\n0.56/0.18\n0.49/0.26\n0.36/0.21\n0.08/0.04\n0.07/0.05\nGPT-Calls\n0.11/0.10\n0.32/0.25\n0.20/0.15\n0.07/0.04\n0.07/0.03\nImprov.\n+80.4%/+44.4%\n+34.6%/+3.8%\n+44.4%/+28.5%\n+12.5%/+0%\n+0%/+40.0%\nTable 2. A comparison of the performance of the proposed method and the GSGST baseline, evaluated on the test set, for each\ntopic separately. Pk score/WinDiff are reported. Lower is better for both.\nModel Name\nHit\nReasonable\nFailure\nGPT-Calls\n77.1%\n16.2%\n6.7%\nTable 3. Human evaluation for the quality of segments. We\nmeasure the percentage of segments that (1) matches with the\nground truth annotations (Hit), (2) mismatch with the ground\ntruth but are considered as reasonable predictions (Reason-\nable), (3) mismatch with the ground truth and are not reason-\nable w.r.t. the underlying utterances in the segment (Failure).\nour proposed model in terms of Pk score and WinDiff, sepa-\nrately for each topic. Both metrics aim to achieve lower val-\nues, indicating better performance.\nThe results demonstrate that our proposed model yields\nsimilar or better performance compared to the baseline model,\nacross all topics.\nSpecifically, for the Identification, Pric-\ning, Schedule topics, we observe improvements ranging from\n34.6% to 80.4% and 3.8% to 44.4% in in the Pk and Win-\nDiff scores, respectively. The largest improvement was ob-\nserved in the Identification topic, where our model achieved\nan 80.4% improvement in the Pk score and a 44.4% improve-\nment in WinDiff.\n4.4. Human evaluations results\nWe randomly selected 100 calls from the above three datasets,\nand evaluated the performance of the proposed model in an\nend-to-end manner, while focusing on the end-user experi-\nence. The model performance was evaluated using three cri-\nteria: (1) \u201cHit\u201d was assigned when the predicted segment is\nwell correlated with the ground truth segment. (2) \u201cReason-\nable\u201d was designated when there was a discrepancy between\nthe predicted segment and the ground truth, but the predicted\nsegment and its topic are fairly associated with the underly-\ning utterances. (3) \u201cFailure\u201d was determined when when the\npredicted segment did not match the ground truth, and the\nprediction did not align well with the underlying utterances.\nThe results, depicted in Table 3, indicate that 93.3% of\nthe model predicted segmentations are considered fairly good\n(i.e. either \u201cHit\u201d or \u201cReasonable\u201d), and only 6.7% were de-\ntected as failures.\n5. CONCLUSION\nWe propose a novel approach for call segmentation and tag-\nging that builds upon distilling knowledge from the GPT-3\nmodel and does not require labeled data.\nOur solution is\ngeneric and can be applied to various domains. The proposed\nmethod is deployed in Dynamics 365 Sales Conversation In-\ntelligence and was shown to significantly improve upon other\nalternatives.\n6. REFERENCES\n[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al., \u201cLanguage models are few-shot learners,\u201d\nAdvances in neural information processing systems, vol.\n33, pp. 1877\u20131901, 2020.\n[2] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida,\nCarroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.,\n\u201cTraining language models to follow instructions with\nhuman feedback,\u201d\narXiv preprint arXiv:2203.02155,\n2022.\n[3] Alexander A Alemi and Paul Ginsparg, \u201cText segmen-\ntation based on semantic word embeddings,\u201d\narXiv\npreprint arXiv:1503.05543, 2015.\n[4] Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-\nter J Liu,\n\u201cPegasus: Pre-training with extracted gap-\nsentences for abstractive summarization. arxiv e-prints,\u201d\n2019.\n[5] Marti A Hearst, \u201cText tiling: Segmenting text into multi-\nparagraph subtopic passages,\u201d Computational linguis-\ntics, vol. 23, no. 1, pp. 33\u201364, 1997.\n[6] Alessandro\nSolbiati,\nKevin\nHeffernan,\nGeorgios\nDamaskinos, Shivani Poddar, Shubham Modi, and\nJacques Cali,\n\u201cUnsupervised topic segmentation\nof meetings with bert embeddings,\u201d\narXiv preprint\narXiv:2106.12978, 2021.\n[7] Nils Reimers and Iryna Gurevych, \u201cSentence-bert: Sen-\ntence embeddings using siamese bert-networks,\u201d arXiv\npreprint arXiv:1908.10084, 2019.\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova, \u201cBert: Pre-training of deep bidirec-\ntional transformers for language understanding,\u201d arXiv\npreprint arXiv:1810.04805, 2018.\n[9] Martin Ester, Hans-Peter Kriegel, J\u00a8org Sander, Xiaowei\nXu, et al., \u201cA density-based algorithm for discovering\nclusters in large spatial databases with noise.,\u201d in kdd,\n1996, vol. 96, pp. 226\u2013231.\n[10] Wenpeng Yin, Jamaal Hay, and Dan Roth,\n\u201cBench-\nmarking zero-shot text classification: Datasets, eval-\nuation and entailment approach,\u201d\narXiv preprint\narXiv:1909.00161, 2019.\n[11] Abedelkadir Asi, Song Wang, Roy Eisenstadt, Dean\nGeckt, Yarin Kuper, Yi Mao, and Royi Ronen, \u201cAn end-\nto-end dialogue summarization system for sales calls,\u201d\narXiv preprint arXiv:2204.12951, 2022.\n[12] Doug Beeferman, Adam Berger, and John D. Lafferty,\n\u201cStatistical models for text segmentation,\u201d\nMachine\nlearning, vol. 34, no. 1-3, pp. 177\u2013210, 1999.\n[13] Lev Pevzner and Marti A Hearst, \u201cA critique and im-\nprovement of an evaluation metric for text segmenta-\ntion,\u201d\nComputational Linguistics, vol. 28, no. 1, pp.\n19\u201336, 2002.\nA. APPENDIX\nA single prompt was created for each topic and was used\nthousands times to generate a distribution of synthetic seg-\nments associated with the topic. The prompts contained one\nto four shots, enabling the model to focus on the task and\ngenerate high-quality synthetic segments. For instance, the\npricing topic prompt is as follows: \u201cThis is a prefix of a call\nbetween two people, where they greet and introduce each\nother: \u201cThank you for calling Spencer and Bryce. This is\nTracy. How can I help you? Hey Tracy, I\u2019m Jeremy King\nfrom sales looking to reach Paul Lana. Uh, you know what?\nGive me your name again. Jeremy king. Calling regarding\nwhat Jeremy? I\u2019m a salesperson working for\u201d Here is part\nof the middle of a different phone call between two different\npersons from different companies, where they are discussing\nthe pricing of a product:\u201d\n"
  },
  {
    "title": "STUDY: Socially Aware Temporally Casual Decoder Recommender Systems",
    "link": "https://arxiv.org/pdf/2306.07946.pdf",
    "upvote": "1",
    "text": "STUDY: Socially Aware Temporally Causal Decoder\nRecommender Systems\nEltayeb Ahmed\nGoogle Research\nekahmed@google.com\nDiana Mincu\nGoogle Research\ndmincu@google.com\nLauren Harrell\nGoogle Research\nlaurenharrell@google.com\nKatherine Heller\nGoogle Research\nkheller@google.com\nSubhrajit Roy\nGoogle Research\nsubhrajitroy@google.com\nAbstract\nRecommender systems are widely used to help people find items that are tailored\nto their interests. These interests are often influenced by social networks, mak-\ning it important to use social network information effectively in recommender\nsystems, especially for demographic groups with interests that differ from the\nmajority. This paper introduces STUDY, a Socially-aware Temporally caUsal\nDecoder recommender sYstem. The STUDY architecture is significantly more\nefficient to learn and train than existing methods and performs joint inference\nover socially-connected groups in a single forward pass of a modified transformer\ndecoder network. We demonstrate the benefits of STUDY in the recommendation\nof books for students who have dyslexia or are struggling readers. Students with\ndyslexia often have difficulty engaging with reading material, making it critical\nto recommend books that are tailored to their interests. We worked with our non-\nprofit partner Learning Ally to evaluate STUDY on a dataset of struggling readers.\nSTUDY was able to generate recommendations that more accurately predicted\nstudent engagement, when compared with existing methods.\n1\nIntroduction\nRecommender systems are one of the major applications of AI systems and are an essential driver\nof many of our online experiences today. With applications ranging from e-commerce [1] and\nadvertising platforms [2] to video platforms [3], we are relying on recommender systems to surface\nrelevant and interesting content to individual users. In this work, we focus on recommender systems\ndeployed in the educational setting [4] to suggest relevant literature for students in grades 1 through\n12.\nRecommender systems for educational content have been studied in the context of online learning pro-\ngrams/massive open online courses (MOOCs) [5] but are not as common for primary and secondary\nschool student applications. Experiments with recommender systems in education have generally\nbeen limited by the lack of publicly-available large data sources - one review found only 5 experi-\nmental studies with sample sizes of over 1000 participants [6]. However, versions of recommender\nsystems have been applied in educational measurement and assessment for over four decades through\ncomputerized adaptive testing [7], where the test items presented to the test-takers depend on the\ncurrent estimate of the student\u2019s ability.\nIn more recent literature, expansions on methods for computerized adaptive testing have been\nproposed for recommending new content in an adaptive learning framework [8] where content can be\nautomatically presented to students given their particular stage of learning. These adaptive learning\narXiv:2306.07946v3  [cs.SI]  5 Sep 2023\nsystems require access to some measures of student subject-matter performance and do not account\nfor the student\u2019s interest in the material nor the social dynamics that may lead to greater engagement.\nAlternative approaches are needed in content recommendation contexts where a student\u2019s reading\nlevel cannot be measured or measures are not included in the available data for recommendations.\nPrevious studies have shown that higher levels of student motivation predicts growth in reading\ncomprehension [9]; thus promoting content that is most likely to align with a student\u2019s interests is\nhypothesized to produce better reading and literacy outcomes, particularly for students with reading\ndifficulties. In the United States, often the reading content assigned in by teachers aligns to a state or\ndistrict-level curriculum for a particular grade level, but for assigning reading materials outside the\nrequired texts, other strategies are needed, and we hypothesize that incorporating social connections\ncan be an effective strategy for recommending content successfully.\nIn one study of a book recommendation system, using an app function that allowed users to view the\nreading histories of peers had a beneficial long-term effect on reading choices [10], indicating that\nincorporating social dynamics into recommendations may lead to more effective recommendations.\nIn another social network analysis of second and third graders in the US [11], the researchers found\nthat on average students were able to effectively identify peers with higher reading skills to ask these\npeers for help, thus even for younger learners, peer relationships may be relevant for content selected.\nIn rural areas which sometimes lack resources to help with struggling students, a study [12] found\nthat adolescent reading choices were often motivated by conversations and materials borrowed from\nfriends and family, suggesting that a recommender system that includes peer preferences could also\nbe effective for reaching the rural student population.\n1.1\nPossibilities for recommender systems\nIn the applied educational setting, systems can be targeted towards either teachers [13] or stu-\ndents [14] to suggest content, and in both cases the goal of these systems is to surface relevant and\nengaging educational material that is beneficial to the students\u2019 learning. Student-facing educational\nrecommender system is built from data relevant to students interaction. with the platform, which falls\ninto the following categories [15]:\n\u2022 Data about the students, or \"user data\"\n\u2022 Data about the content to be recommended, or \"item data\"\n\u2022 Data about the context of the current session (e.g. time of day, session device, etc.), or\n\"context data\".\nIn the case of our applied scenario to recommend books, we do not assume that student or \"user data\"\nincludes measures of student performance.\nTwo widely used types of recommender systems are \"Batch\" and \"Sequential\". Batch recommender\nsystems operate on representations of previous interactions, and don\u2019t model time or relative order.\nThey include collaborative filtering based methods [16] and Embarrassingly Shallow Autoencoders\n(EASE) [17]. Sequential recommender [18] systems operate on representations of historical user\ninteraction as sequences [19].\nThe classroom setting enables socially-structured recommendation because of the availability of\na clearly-defined hierarchical network, which groups students into classrooms, year cohorts, and\nschools. This makes the utilization of social recommendation systems [20] particularly attractive\nwhere the relationships between users are leveraged to make recommendations.\nIn this work we present Socially-aware Temporally caUsal Decoder recommender sYstems (STUDY),\na sequence-based recommender system for educational content that makes use of known student\nhierarchies to improve recommendations. Our method does joint inference over groups of students\nwho are adjacent in the social network. Our method utilizes a single transformer decoder network to\nmodel both within-user and cross-user interactions. This paper is organized as follows: we provide\na review of the related work in Section 2, we review related previous recommender systems in\nSection 3, we introduce our new socially-aware recommender system in Section 4, and we present\nour experimental results in Section 5.5 before concluding in section 6.\nIn summary, the contributions of this paper are:\n2\nInteraction\nhistory\nAttention \nmask\nStudent 1\nItems\nTransformer \nDecoder\nInteraction\nhistory\nAttention \nmask\nStudent 2\nItems\nTransformer \nDecoder\nInteraction\nhistory\nAttention \nmask\nStudent 3\nItems\nTransformer \nDecoder\n(a) Processing students individually\nItems\nTransformer\nDecoder\nInteraction\nhistory\nAttention \nmask\n(b) Equivalent reformulation of in-\ndividual processing\nItems\nTransformer\nDecoder\nInteraction\nhistory\nAttention \nmask\n(c) Processing students jointly\nAttention mask set to 0\nAttention mask set to 1\nValue depends on timestamps\nSeparator token\nFigure 1: (a) a sequential autoregressive transformer with causal attention that processes each user\nindividually, (b) an equivalent joint forward pass that results in the same computation as (a), c) shows\nthat introducing new nonzero values (shown in purple) to the attention mask allows information to\nflow across users. Predictions condition on all interactions with an earlier timestamp, irrespective of\nwhether the interaction came from the same user or not.\n\u2022 Proposing a novel architecture and data pipeline for performing socially-aware sequential\nrecommendation.\n\u2022 Comparing the new method to modern and classical recommendation baselines.\n\u2022 Performing ablations and performance breakdowns to better understand the new model.\n2\nRelated Work\n2.1\nClick-through Rate Prediction\nOne of the popular approaches for recommender systems is click-through rate prediction [21], where\nthe probability of a user clicking on a specific presented item is predicted. These probabilities\nare then used as a proxy for user preferences. Click-through Rate (CTR) models typically make\npredictions for a suggested next item for a user based on the user\u2019s sequence of previous interactions,\nuser data and context data. Model architectures used in this problem range from standard models\nlike Transformers used in Behavior Sequence Transformers (BST) [22] and Convolutional Neural\nNetworks used in [23] to more task specific architectures such as Wide & Deep models [24] and\nField-Leveraged Embedding Networks (FLEN) [25]. This approach contrasts with other approaches\nsuch as neural collaborative filtering [26] and K-Nearest Neighbors (KNN) recommenders [27] where\nthere is no attempt to explicitly model the likelihood of the user interacting with a specific item.\n2.2\nSocially-Aware Recommendation Systems\nWhen social connectivity information exists for users, there are many modeling approaches that\nleverage this information. Methods such as TrustMF [28] and Sorec [29] project user preference\nvectors into a latent space using matrix factorization approaches. The underlying assumption of these\nsystems is homophily i.e. that users who are more socially connected are more likely to have similar\npreferences.\nDeep-learning based methods have leveraged graph neural networks to learn using social connectivity\nstructure. Methods such as DiffNet [30] and KCGN [31] utilize graph convolutional neural networks\nwhereas methods such as GraphRec [32] and Social Attentional Memory Networks (SAMN) [33]\nemploy graph attention mechanisms. Other notable work includes Disentangled Graph Neural\n3\nNetworks (DGNN) which have the capability to model non-heterogeneous relationships and utilize a\nmemory augmented graph network [20].\nIn this work we take a different approach to that of previous work, which has used graph neural\nnetworks or other custom architectures with separate components to handle cross-user interactions.\nWe utilize a single transformer decoder with a specifically-designed attention mask to do joint\ninference over groups of users. With this approach we have developed a single consistent way to\nhandle both within-user and cross-user interactions in a computationally efficient manner.\n3\nReview of Baseline Work\nWe review related work, used as baselines for comparison in the experiments section 5.5.\n3.1\nItem-based KNN recommender\nKNN recommender systems [34] compute the cosine similarity between the user\u2019s current feature\nvector and each entry in the training dataset. They then recommend to the user the k distinct items\nwith highest cosine similarity to the user\u2019s current feature vector. When feature vectors are sparse\nmost entries in the training dataset will have a cosine similarity of exactly zero with the user\u2019s current\nfeature vector. An efficient implementation in this situation precomputes an inverse index [35] for\nthe training data that can be used to retrieve only the feature vectors which have a nonzero value at\na specific index. Utilizing this inverse index enables only having to compute the cosine similarity\nfor vectors that will have a nonzero similarity with current features. These are the vectors that have\noverlapping nonzero entries with the current user feature vector.\nIn this paper\u2019s KNN implementation, we iterate over every sequence in the training dataset and\nfeaturize each item by computing a feature vector from the h interactions previously preceding\nit. Each item in the sequence is represented by a vector of size v + 1, one component for each\nentry in the vocabulary as well as an entry for the out-of-vocabulary token). The ith component of\nthis vector is the number of times the ith item in the vocabulary was interacted with in the users\nprevious h interactions with the system. As h \u226a n these feature vectors are very sparse. To make\nrecommendations at inference time, we compute a feature vector from the users h most recent\ninteractions from the users history.\n3.2\nIndividual\nFollowing the click-through rate prediction [36] method of recommendation this methodology takes\nthe next-click prediction approach to recommendation, and hence treats making recommendations\nas a causal sequence-modeling problem. In particular, this modeling framework borrows from the\nlanguage modeling literature [37] due to a similar problem setup. Concretely, given a set of students\nsj \u2208 S, and a set of historical item interactions {ik\nj : \u2200j|sj \u2208 S, \u2200k < \u2225sj\u2225, k \u2208 N} we learn a\npropensity function\nP(ik\nj ) = f(ik\nj |ik\u2032<k\nj\n; \u03b8)\nwhere the propensity of an item at a point in time the likelihood the student will interact with that\nitem at that time. To this end we modeled f as a causal decoder-only transformer with a next-token\nprediction objective to maximize the following likelihood of our data D\nL(D) = P\nsi\nj\u2208D log f(si\nj|si\nj\u2032<j; \u03b8)\nThis is the formulation we used for the model referred to as Individual, since inference is carried out\nfor each individual student separately.\n4\nMethod\nWe present our new Socially-aware Temporally Causal Decoder Recommender System (STUDY),\nenabling the efficient use of the unique social structure inherent to students in schools.\n4\n4.1\nSTUDY\nWe motivate our model by observing that for students with few previous interactions, we can rely on\ndata from other students with similar preferences to seed the model to improve predictive performance.\nConcretely, we concatenate the interaction sequences of multiple students within the same classroom.\nThis precludes using a causal sequence modeling approach to model this problem, since some item-\ninteractions for students presented earlier in the sequence could have occurred at a later point in\ntime relative to item-interactions for the students presented later in the sequence. Modeling data\nrepresented in this format using causal sequence modeling would lead to anti-causal data leakage\nand the model would learn to make recommendations conditioned on information not available at\ninference time.\nHence we introduce temporally causal masking into our model: a change to our model\u2019s forward pass\nusing a training process similar to causal sequence modeling that respects the causal relationships in\nour data as shown in Figure 1. Conceptually we concatenate the user vectors of students in the same\nclassroom and allow predictions for a particular item to condition on all interactions that happened in\nthe past, both within-user and cross-user. In more detail, if there is a subset of users u1, u2, \u00b7 \u00b7 \u00b7 , un\nwho are all in the same classroom, with interaction sequences S1, S2, \u00b7 \u00b7 \u00b7 , Sn, and with timestamp\nvectors T 1, T 2, \u00b7 \u00b7 \u00b7 T 2 where ti\nj is the timestamp of the interaction described at si\nj - and each user\nvector Sn and timestamp vector T n is terminated with a separator token - we define the concatenated\nclassroom vectors generated by the procedure described in Section 5.2 as\n\u02c6S =\n\u0000S1S2 \u00b7 \u00b7 \u00b7 Sn\u0001\n\u02c6T =\n\u0000T 1T 2 \u00b7 \u00b7 \u00b7 T n\u0001\nWe define the matrix M\nmi,j = 1\u02c6ti<\u02c6tj\nas the temporally causal mask matrix. This matrix is used as the mask in our attention operator instead\nof the usual causal mask used in decoder-only transformers. Hence our we redefine the attention\noperator in the decoder-only transformer as follows.\nA = Softmax( QKT\n\u221adk ) \u2299 M\nAttention(Q, K, V ) = AV\nwhere Q is the query matrix, K is the key matrix and V is the value matrix. With this modification\nwe can use next-item prediction sequence modeling to train the model without anti-causal information\nleakage, utilizing a multihead generalization of this attention mechanism [38]. We call the model\ndefined by this procedure STUDY.\n5\nExperiments\n5.1\nData\nWe test STUDY on a dataset of historical interactions with an educational platform collected by our\nnonprofit partner, Learning Ally. This platform recommends and provides access to audiobooks\nwith the goal of promoting reading in students with dyslexia. The data offered was anonymized,\nwith each student, school and district identified only by a unique randomly generated identification\nnumber. Furthermore, all descriptive student data was only available as highly aggregated summaries.\nThere are historical records of interactions between students and audiobooks in the dataset. For\neach interaction recorded we have a timestamp, an item ID and an anonymized student ID, an\nanonymized school ID and a grade level. This data was collected over two consecutive school years\ncontaining over 5 million interactions per each school year totalling over 10 million interactions.\nThese interactions come from a cohort of over 390,000 students. We use the data from the first school\nyear as our training dataset and split the data from our second school year into a validation dataset\nand a test dataset. This split was done according to the temporal global splitting strategy [39]. This\nwas done to model the scenario of deployment as realistically as possible. To partition the data from\n5\nthe second school year into a test set and a validation set we split by student, following the user split\nstrategy [39]. If a data split does not contain at least a full academic year then the distributions would\nnot match due to seasonal trends in the data.\nOverall this dataset is well suited to studying social recommendation algorithms due to the existence\nof implied social connections through known proximity and also due to the large amount of interaction\ndata on record. The existing book selections were made through either student choice or teacher\nrecommendation, where often the teacher-assigned content aligned to materials assigned to the\nwhole class or required curriculum. Interactions with the assigned content, however, were still up to\nthe learner, and thus we believe the existing data is a good fit for modeling preferences and likely\nengagement with content. Further details on the data including summary statistics can be found in\nAppendix A\n5.2\nPreprocessing\nIn order to get the training data representation, we express the items as tokens. The top v most popular\nitems get a unique and sequential integer as their token, while the rest of the items get assigned to an\nout-of-vocabulary token. The student interaction history will therefore become a list of such tokens\nassociated with a time point.\nThe following additional processing steps are taken based on the model type used downstream:\n\u2022 For transformer models: we split the student history into slices based on a context window\nof length c.\n\u2022 For models that process students jointly: we split the sequence associated with each student\ninto segments of length s, s < c, then compose sequences of length c by joining segments\nfrom multiple students in the same classroom, taking care to use a separator token.\nAdditional information, including a diagram of our preprocessing steps, are presented in Appendix B.\n5.3\nEvaluating Models\nWe implement KNN and SAMN1 [22] as baseline models, a transformer-based model that does\ninference for each student separately, which we will call Individual, as well as a transformer that\noperates over groups of students called STUDY.\nWe compare results from the Individual model, STUDY model, the item-based KNN baseline, and\nSAMN [33] as a social baseline. We tuned the hyperparameters learning rate on the validation set\nand report final results on the test set. We took the both the context length c and the segment size\ns for our transformer models to be 65, enough to the full history of most students in our dataset.\nDetails about further hyperparameters and compute can be found in Appendix C. Hits@n scores was\nused as our evaluation metric, where hits@n is the percentage of interactions when the actual item\ninteracted with falls within the top n recommendations from the model under evaluation. Since we\nobserve that students tend to repeatedly interact with an item multiple times before completing it, we\nadditionally evaluate our models on the subset of the dataset where the student is interacting with\na different item to the item previously interacted with, referred to as non-continuation evaluation.\nWe also evaluate on the subset of the dataset where the students are interacting with an item for the\nfirst time, referred to as novel evaluation. This motivated by the fact that we are interested in the\npower of our recommendation systems to engage students with new items in order to maximize time\nspent on the educational platform. Aggregate statistics are computed per student then averaged over\nstudents to prevent students with large numbers of interactions from dominating the reported statistics.\nWe also examine the relevant performance of these models on different slices of data, looking at\nco-factors such as demographic characteristics and school performance. We present the results of this\nexperiment in section 5.5.\n1We used the author\u2019s repository https://github.com/chenchongthu/SAMN as a guideline. We found a\ndiscrepancy between this code and the method described in the paper, but it didn\u2019t affect final performance.\n6\nEvaluation Subset\nn\nKNN(%)\nSAMN(%)\nIndividual (%)\nSTUDY(%)\nAll\n1\n16.67 \u00b1 0.14\n0.32 \u00b1 0.02\n28.06 \u00b1 0.14\n31.86 \u00b1 0.14\n3\n31.97 \u00b1 0.17\n2.01 \u00b1 0.05\n35.74 \u00b1 0.16\n38.65 \u00b1 0.14\n5\n37.16 \u00b1 0.20\n3.64 \u00b1 0.09\n38.63 \u00b1 0.18\n41.17 \u00b1 0.18\n10\n43.17 \u00b1 0.20\n6.87 \u00b1 0.10\n42.56 \u00b1 0.20\n44.85 \u00b1 0.18\n20\n48.02 \u00b1 0.20\n11.56 \u00b1 0.15\n46.70 \u00b1 0.20\n48.90 \u00b1 0.18\nNon-continuation\n1\n5.15 \u00b1 0.08\n0.33 \u00b1 0.02\n2.05 \u00b1 0.04\n3.75 \u00b1 0.07\n3\n9.35 \u00b1 0.08\n1.95 \u00b1 0.04\n10.38 \u00b1 0.11\n13.76 \u00b1 0.11\n5\n11.47 \u00b1 0.13\n3.56 \u00b1 0.07\n14.35 \u00b1 0.10\n17.66 \u00b1 0.11\n10\n14.93 \u00b1 0.10\n6.68 \u00b1 0.09\n19.96 \u00b1 0.13\n23.05 \u00b1 0.13\n20\n19.42 \u00b1 0.15\n11.29 \u00b1 0.15\n26.27 \u00b1 0.14\n29.50 \u00b1 0.16\nNovel\n1\n0.58 \u00b1 0.03\n0.32 \u00b1 0.02\n1.06 \u00b1 0.04\n1.86 \u00b1 0.06\n3\n2.21 \u00b1 0.06\n1.87 \u00b1 0.05\n5.03 \u00b1 0.09\n6.60 \u00b1 0.10\n5\n3.73 \u00b1 0.05\n3.45 \u00b1 0.06\n8.02 \u00b1 0.12\n9.77 \u00b1 0.13\n10\n6.68 \u00b1 0.08\n6.47 \u00b1 0.12\n13.14 \u00b1 0.12\n15.06 \u00b1 0.15\n20\n11.12 \u00b1 0.10\n10.99 \u00b1 0.11\n19.56 \u00b1 0.14\n22.01 \u00b1 0.16\nTable 1: Hits@n percentage metrics for the different recommendation models evaluated on the\nhistorical data in the test split, across three subsets: all, non-continuation and novel. Both transformer\ndecoder approaches significantly outperform KNN and SAMN with STUDY having the best perfor-\nmance. Uncertainties are 95% confidence intervals computed over 50 bootstraps.\n5.4\nAblation Experiments\nForce Mix: In our model because we set segment size s equal to context size c we only do joint\ninference over groups of students when processing a student who does not have enough previous\ninteractions to fill the transformer\u2019s context. We experiment with shorter segment size s = 20 \u226a c\nas per the definitions in Section 5.2. Practically, this leads to the model always sharing its context\nbetween students in a classroom when possible, even for students have enough history to fill the\ntransformer context. We present results for this ablation in Section 5.5.1\nClassroom Grouping: In STUDY we do joint inference over students in the same classroom. We\nablate the importance of using this particular grouping. Concretely, we experiment with grouping\nstudents who are in the same district and school year as being in a single group. We also experiment\nwith grouping all students in the dataset into a single group, which results in completely random\ngroups of students being jointly processed together. We present results in Section 5.5.1.\nData Tapering:We compare the effect of using only a subset of the available data and compare\nthe performance of STUDY and Individual. We compare the use of 25%, 50%, 75% and the entire\ndataset, with the aim of discerning the effect of using social information on the data efficiency of the\nsystem. We present results in Section 5.5.1.\n5.5\nResults and Analysis\nTable 1 shows the performance of the models STUDY, Individual, KNN and SAMN on the test split\nof audiobook usage data. We observe that both transformer models, Individual and STUDY, largely\noutperform KNN and SAMN, with the STUDY model outperforming the Individual model. We\nsee that the social model SAMN, derived from the collaborative filtering family of models, fails\nto pick up on the sequential patterns in the dataset, such as users revisiting the exact same item or\nsimilar items. This is exhibited by SAMN having similar performance in the evaluation subsets all,\nnon-continuation and novel. The performance differences are most pronounced when evaluated on the\nentire test set as seen in the all section of the table, but also holds up when evaluated across the more\ndifficult non-continuation and novel test subsets. Crucially, with the STUDY model outperforming the\nindividual model, we can see that leveraging the social hierarchy of our users to do joint predictions\nleads to improved recommendations.\nIn Figure 3 we see the relative performance of the models under examination to be constant, with\nSTUDY outperforming Individual, which in turn outperforms KNN. SAMN trailed behind with\n7\n0\n25\n50\n75\n100\n0.0\n0.2\n0.4\n0.6\nHits @ 1\nIndividual\nSTUDY\nKNN\nSAMN\n0\n25\n50\n75\n100\nEngagement\n0\n10000\n20000\n30000\n# Students\n>100\n(a)\n0\n25\n50\n75\n100\n0.0\n0.2\n0.4\n0.6\nHits @ 1\nIndividual\nForce Mix\n0\n25\n50\n75\n100\nEngagement\n0\n10000\n20000\n30000\n# Students\n>100\n(b)\nFigure 2: Performance of the presented models broken down by student engagement, accompanied by\na histogram of student engagement in the lower chart. (a) Hits@1 across four models KNN, SAMN,\nIndividual and STUDY. The two transformer based approaches outperform KNN and SAMN. The\nSTUDY model significantly outperforms the Individual model for students with low engagement\nof up to 35 interactions. (b) A comparison of the Individual against the Force Mix ablation. The\nForce Mix ablation only outperforms Individual on students with engagement of up to 17, with\nmatched performance onwards. Uncertainties shown are 95% confidence intervals computed over 50\nbootstraps.\nalmost 0 hits@1, we attribute this to SAMN\u2019s non-sequential nature. This ordering is the same when\nslicing by demographic variables such as metro-code (which describes schools as being in urban,\nsuburban, rural or town areas), school socio-economic indicators which indicate the level of wealth of\nthe area the in the vicinity of a school. We also observe the same ordering of models by performance\nwhen slicing by academic variables such as classroom reading scores. In Figure 2 we slice model\nperformance by student engagement, which we measure by the number of interactions the student has\non record. Here we see similar relative performance order for students with less than 35 or so total\ninteractions, but for students with more engagement, we see convergence between the performance\nof STUDY and Individual. This is discussed in more detail in Section 5.5.1.\n5.5.1\nAblations Results\nForce Mix: In Figure 2 we compare the performance of the STUDY model to the individual model\nand observe that STUDY significantly outperforms the Individual model on students with up to 35\nprevious interactions. Compared to the model from the Force Mix ablation only, STUDY outperforms\nIndividual on students who have up to about 17 interactions on the platform. Given that our segment\nlength for the Force Mix model is 20, it is at students with 20 previous interactions where Force Mix\nstarts to forgo the available history for the student at hand in favor of conditioning on data from other\npeer students. From here we can conclude that conditioning on peer student history is beneficial if it\nis done in addition to conditioning on all available history for a student, but not if it comes at the cost\nof conditioning on less history than available for the particular student.\nClassroom Grouping: In Figure 4a we compare the performance of our model that uses classrooms\nto group students for joint inference compared to a model that uses intersection of district and school\nyear to group students, to a model that uses a single group as well as to a model that does inference\nfor each student individually. We can see that using classrooms for grouping results in the best\nperformance, that using the intersection of district and school year for grouping performs slightly\nworse, and that putting all students in the same group performs similarly to individual processing.\nFrom here we can conclude that using groups of users whom we expect to have correlated interests\nis necessary for the performance of our model and using poorly designed groups can harm model\nperformance.\nData Tapering: In Figure 4b we see that STUDY outperforms the Individual recommender across all\ndata subsets used to train the models, confirming the benefit of adopting social recommender systems\n8\nA\nB\nC\nD\nE\n0.0\n0.1\n0.2\n0.3\nHits @ 1\nIndividual\nSTUDY\nKNN\nSAMN\nA\nB\nC\nD\nE\nSocio-Economic Indicator\n0\n10000\n20000\n30000\n# Students\nUnk.\n(a)\n0\n25\n50\n75\n100\n0.0\n0.1\n0.2\n0.3\nHits @ 1\nIndividual\nSTUDY\nKNN\nSAMN\n0\n25\n50\n75\n100\nClassroom Reading Score\n0\n20000\n40000\n60000\n# Students\nunk.\n(b)\nUrban\nSuburban\nRural\nTown\n0.0\n0.1\n0.2\n0.3\nHits @ 1\nIndividual\nSTUDY\nKNN\nSAMN\nUrban\nSuburban\nRural\nTown\nMetro Code Indicator\n0\n20000\n40000\n60000\n# Students\nUnk.\n(c)\nFigure 3: Model performance (hits@1) and histograms of number of students across slices (a)\nsocio-economic indicator, (b) classroom reading score and (c) Metro code which describes schools\nas being Urban, Suburban, Rural or Town. We see the relative order of performance Temporally\nCausal > Individual \u226b KNN \u226b SAMN is maintained across all slices. Uncertainties shown are 95%\nconfidence intervals. Note: (a) and (c) uncertainties are shown as error bars but are very small.\nsuch as STUDY even when in a data-constrained environment. We also note that both models witness\na performance drop when the amount of data used increases from 50% to 75%, suggesting that not\nall additional data is beneficial. We leave a deeper interpretation of this phenomenon to future work.\n6\nConclusion\nIn this paper we present STUDY, a socially aware recommendation system that leverages cross-user\ninformation at inference time and we demonstrate its applicability to the practical problem of book\nrecommendation for children inside a classroom. This is an important problem, as engagement with\nreading materials from an early age can positively impact language acquisition, communication skills,\nsocial skills and literacy skills.\nOur novel STUDY method uses attention masks that are causal with respect to interaction timestamps\nand is able to process both within-user and across-user interactions using a single forward pass\nthrough a modified transformer decoder network. It avoids complex architectures and circumvents\nthe need for graph neural networks which are notoriously difficult to train; thus, STUDY is an\nefficient system that can be deployed by partners with limited computational bandwidth that doesn\u2019t\nsacrifice model performance. We also compare STUDY to a number of baselines, both sequential and\n9\n5\n10\n15\n20\nn\n0.3\n0.4\n0.5\nHits@n\nClassroom Groups\nDistrict Groups\nSingle Group\nIndividual\n(a)\n25\n50\n75\n100\nPercentage of Training Data \n0.26\n0.28\n0.30\n0.32\nHits @ 1\nSTUDY\nIndividual\n(b)\nFigure 4: Figure (a): the results of grouping the students for STUDY by classroom, grouping by\nthe intersection of district and school year, grouping randomly, and doing individual inference. We\nobserve that grouping by classroom performs best, and that grouping by district \u00d7 school year\nperforms slightly worse. Random grouping is on par with individual inference. Figure (b): the effect\nof training with subsets of the training data of various sizes on Individual and STUDY. STUDY\noutperforms Individual at all subset sizes. We note the drop in performance when increasing the\ndataset size from 50% to 75% across both models. Uncertainties shown are 95% confidence intervals\ncomputed over 50 bootstraps and are obscured by datapoint markers when small.\nnon-sequential, and social and non-social. We show that STUDY outperforms alternative sequential\nand social methods, in a variety of scenarios, as demonstrated in ablation studies.\nLimitations: Evaluations were limited to offline evaluations on historical data, inline with much of\nthe literature. However, these evaluations cannot account for the utility of recommended items that\nthe user has never interacted with in the past, but would have actually enjoyed. Furthermore, our\nmethod is limited to structures where all the known relationships between users are homogeneous -\neach student in the classroom is assumed to have the same relationship with each other. Given that\nsocial dynamics in reality are more complicated, in future work we wish to explore extending this\nmethod to social networks with richer heterogeneous relationships between users where proximity\nbetween users can vary within a classroom.\nReferences\n[1] Zhao Li, Shen Xin, Yuhang Jiao, Xuming Pan, Pengcheng Zou, Xianling Meng, Chengwei Yao,\nand Jiajun Bu. Hierarchical bipartite graph neural networks: Towards large-scale e-commerce\napplications. 2020 IEEE 36th International Conference on Data Engineering (ICDE), pages\n1677\u20131688, 2020.\n[2] Xiangyu Liu, Chuan Yu, Zhilin Zhang, Zhenzhe Zheng, Yu Rong, Hongtao Lv, Da Huo, Yiqing\nWang, Dagui Chen, Jian Xu, Fan Wu, Guihai Chen, and Xiaoqiang Zhu. Neural auction:\nEnd-to-end learning of auction mechanisms for e-commerce advertising, 2021.\n[3] Yashar Deldjoo, Markus Schedl, Paolo Cremonesi, and Gabriella Pasi. Recommender systems\nleveraging multimedia content. ACM Comput. Surv., 53(5), sep 2020.\n[4] Solmaz Abdi, Hassan Khosravi, Shazia Sadiq, and Dragan Gasevic. Complementing educational\nrecommender systems with open learner models. In Proceedings of the tenth international\nconference on learning analytics & knowledge, pages 360\u2013365, 2020.\n[5] Katrien Verbert, Nikos Manouselis, Xavier Ochoa, Martin Wolpers, Hendrik Drachsler, Ivana\nBosnic, and Erik Duval. Context-aware recommender systems for learning: a survey and future\nchallenges. IEEE transactions on learning technologies, 5(4):318\u2013335, 2012.\n10\n[6] Felipe Leite da Silva, Bruna Kin Slodkowski, Ketia Kellen Ara\u00fajo da Silva, and S\u00edlvio C\u00e9sar\nCazella. A systematic literature review on educational recommender systems for teaching\nand learning: research trends, limitations and opportunities.\nEducation and Information\nTechnologies, 28(3):3289\u20133328, 2023.\n[7] David J Weiss and G Gage Kingsbury. Application of computerized adaptive testing to educa-\ntional problems. Journal of educational measurement, 21(4):361\u2013375, 1984.\n[8] Yunxiao Chen, Xiaoou Li, Jingchen Liu, and Zhiliang Ying. Recommendation system for\nadaptive learning. Applied psychological measurement, 42(1):24\u201341, 2018.\n[9] John T Guthrie, A Laurel W Hoa, Allan Wigfield, Stephen M Tonks, Nicole M Humenick, and\nErin Littles. Reading motivation and reading comprehension growth in the later elementary\nyears. Contemporary Educational Psychology, 32(3):282\u2013313, 2007.\n[10] Tzu-Chao Chien, Zhi-Hong Chen, and Tak-Wai Chan. Exploring long-term behavior patterns\nin a book recommendation system for reading. Journal of Educational Technology & Society,\n20(2):27\u201336, 2017.\n[11] North Cooc and James S Kim. Peer influence on children\u2019s reading skills: A social network\nanalysis of elementary school classrooms. Journal of Educational Psychology, 109(5):727,\n2017.\n[12] Kim Becnel and Robin A Moeller. What, why, and how they read: Reading preferences and\npatterns of rural young adults. Journal of Adolescent & Adult Literacy, 59(3):299\u2013307, 2015.\n[13] Mbarek Dhahri and Mohamed Koutheair Khribi. A review of educational recommender systems\nfor teachers. International Association for Development of the Information Society, 2021.\n[14] Robert Bodily and Katrien Verbert. Review of research on student-facing learning analytics dash-\nboards and educational recommender systems. IEEE Transactions on Learning Technologies,\n10(4):405\u2013418, 2017.\n[15] Hui Fang, Danning Zhang, Yiheng Shu, and Guibing Guo. Deep learning for sequential\nrecommendation: Algorithms, influential factors, and evaluations. ACM Transactions on\nInformation Systems (TOIS), 39(1):1\u201342, 2020.\n[16] Xiaoyuan Su and Taghi M Khoshgoftaar. A survey of collaborative filtering techniques. Ad-\nvances in artificial intelligence, 2009, 2009.\n[17] Harald Steck. Embarrassingly shallow autoencoders for sparse data. In The World Wide Web\nConference, pages 3251\u20133257, 2019.\n[18] Massimo Quadrana, Paolo Cremonesi, and Dietmar Jannach. Sequence-aware recommender\nsystems. ACM Computing Surveys (CSUR), 51(4):1\u201336, 2018.\n[19] Shoujin Wang, Longbing Cao, Yan Wang, Quan Z Sheng, Mehmet A Orgun, and Defu Lian. A\nsurvey on session-based recommender systems. ACM Computing Surveys (CSUR), 54(7):1\u201338,\n2021.\n[20] Lianghao Xia, Yizhen Shao, Chao Huang, Yong Xu, Huance Xu, and Jian Pei. Disentangled\ngraph social recommendation. arXiv preprint arXiv:2303.07810, 2023.\n[21] Weinan Zhang, Jiarui Qin, Wei Guo, Ruiming Tang, and Xiuqiang He. Deep learning for\nclick-through rate estimation. arXiv preprint arXiv:2104.10584, 2021.\n[22] Qiwei Chen, Huan Zhao, Wei Li, Pipei Huang, and Wenwu Ou. Behavior sequence transformer\nfor e-commerce recommendation in alibaba. In Proceedings of the 1st International Workshop\non Deep Learning Practice for High-Dimensional Sparse Data, pages 1\u20134, 2019.\n[23] Qiang Liu, Feng Yu, Shu Wu, and Liang Wang. A convolutional click prediction model.\nIn Proceedings of the 24th ACM international on conference on information and knowledge\nmanagement, pages 1743\u20131746, 2015.\n11\n[24] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye,\nGlen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. Wide & deep learning for\nrecommender systems. In Proceedings of the 1st workshop on deep learning for recommender\nsystems, pages 7\u201310, 2016.\n[25] Wenqiang Chen, Lizhang Zhan, Yuanlong Ci, Minghua Yang, Chen Lin, and Dugang Liu. Flen:\nleveraging field for scalable ctr prediction. arXiv preprint arXiv:1911.04690, 2019.\n[26] Ting Bai, Ji-Rong Wen, Jun Zhang, and Wayne Xin Zhao. A neural collaborative filtering\nmodel with interaction-based neighborhood. In Proceedings of the 2017 ACM on Conference\non Information and Knowledge Management, pages 1979\u20131982, 2017.\n[27] V Subramaniyaswamy and R Logesh. Adaptive knn based recommender system through mining\nof user preferences. Wireless Personal Communications, 97:2229\u20132247, 2017.\n[28] Bo Yang, Yu Lei, Jiming Liu, and Wenjie Li. Social collaborative filtering by trust. IEEE\ntransactions on pattern analysis and machine intelligence, 39(8):1633\u20131647, 2016.\n[29] Hao Ma, Haixuan Yang, Michael R Lyu, and Irwin King. Sorec: social recommendation using\nprobabilistic matrix factorization. In Proceedings of the 17th ACM conference on Information\nand knowledge management, pages 931\u2013940, 2008.\n[30] Le Wu, Peijie Sun, Yanjie Fu, Richang Hong, Xiting Wang, and Meng Wang. A neural influence\ndiffusion model for social recommendation. In Proceedings of the 42nd international ACM\nSIGIR conference on research and development in information retrieval, pages 235\u2013244, 2019.\n[31] Chao Huang, Huance Xu, Yong Xu, Peng Dai, Lianghao Xia, Mengyin Lu, Liefeng Bo, Hao\nXing, Xiaoping Lai, and Yanfang Ye. Knowledge-aware coupled graph neural network for social\nrecommendation. In Proceedings of the AAAI conference on artificial intelligence, volume 35,\npages 4115\u20134122, 2021.\n[32] Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. Graph neural\nnetworks for social recommendation. In The world wide web conference, pages 417\u2013426, 2019.\n[33] Chong Chen, Min Zhang, Yiqun Liu, and Shaoping Ma. Social attentional memory network:\nModeling aspect-and friend-level differences in recommendation. In Proceedings of the twelfth\nACM international conference on web search and data mining, pages 177\u2013185, 2019.\n[34] Mojdeh Bahadorpour, Behzad Soleimani Neysiani, and Mohammad Nadimi Shahraki. Deter-\nmining optimal number of neighbors in item-based knn collaborative filtering algorithm for\nlearning preferences of new users. Journal of Telecommunication, Electronic and Computer\nEngineering (JTEC), 9(3):163\u2013167, 2017.\n[35] Jijie Wang, Lei Lin, Ting Huang, Jingjing Wang, and Zengyou He. Efficient k-nearest neighbor\njoin algorithms for high dimensional sparse data. arXiv preprint arXiv:1011.2807, 2010.\n[36] Xinfei Wang. A survey of online advertising click-through rate prediction models. In 2020\nIEEE International Conference on Information Technology, Big Data and Artificial Intelligence\n(ICIBA), volume 1, pages 516\u2013521. IEEE, 2020.\n[37] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language\nunderstanding by generative pre-training. 2018.\n[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\n[39] Zaiqiao Meng, Richard McCreadie, Craig Macdonald, and Iadh Ounis. Exploring data splitting\nstrategies for the evaluation of recommendation models. In Proceedings of the 14th ACM\nConference on Recommender Systems, RecSys \u201920, page 681\u2013686, New York, NY, USA, 2020.\nAssociation for Computing Machinery.\n12\nAppendix\nA\nData\nHere we provide some descriptive statistics about the data this research was conducted on. The data\nwas collected over two school years by our data partner Learning Ally. To protect the privacy of\nindividuals and entities all personally identifiable information was removed from the dataset by our\npartner before we had access to the data. Furthermore much of the available metadata was redacted or\nhad its granularity reduced so individuals and entities cannot be identified. All identification numbers\nin the dataset supplied were randomly generated and are not traceable back to individual students by\nus. Table 2 shows descriptive statistics about the platform interactions recorded as well as metadata\ndescribing the students under observation.\nGrade Level\nNumber of Students\nGrade 1\n1726\nGrade 2\n6386\nGrade 3\n16668\nGrade 4\n28762\nGrade 5\n41070\nGrade 6\n49805\nGrade 7\n59836\nGrade 8\n57413\nGrade 9\n51217\nGrade 10\n31611\nGrade 11\n18353\nGrade 12\n14717\nOther\n137671\n(a) A breakdown of the students in the\ndataset by grade level.\nWealth Indicator\nNumber of Students\nA\n120281\nB\n91325\nC\n74855\nD\n61685\nE\n39083\nUnknown\n12108\n(b) A breakdown of the students in the\ndataset by the wealth indicator associ-\nated with their school (A is highest).\nPercentile\nTime Spent on Platform\n5th\n3m\n25th\n35m\n50th\n2h 35m\n75th\n8h 12m\n95th\n33h 33m\n(c) Percentiles of the amount of interac-\ntion time logged by students in the first\ndata collection year.\nPercentile\nTime Spent on Platform\n5th\n2m\n25inth\n29m\n50th\n2h 14m\n75th\n7h 34m\n95th\n35h 25m\n(d) Percentiles of the amount of inter-\naction time logged by students in the\nsecond data collection year.\nPercentile\nInteraction Count\n5th\n1\n25th\n3\n50th\n10\n75th\n29\n95th\n105\n(e) Percentiles of the total number\nof interactions logged per student in\nthe dataset after aggregating over both\nschool years.\nTable 2: This table shows some descriptive statistics relating to the students in the dataset as well as\nthe amount data logged for each student\nTo generate train, validation and test splits the following procedure was followed. The data from the\nfirst school year was taken as our training dataset and we split the data from the second school year\ninto a validation and test datasets. This first split was done according to the temporal global splitting\nstrategy [39]. This was done to model the scenario of deployment as realistically as possible. To\npartition the data from the second school year into a test set and a validation set we split by student,\nwith all interactions recorded for a particular student collected in the second school year are either all\nthe testing dataset or all the validation dataset. This second split followed the user split strategy [39]\n13\nTrain\nValidation\nTest\nNumber of Interactions\n5,179,466\n2,752,671\n2,747,699\nNumber of Students\n237,253\n126,049\n126,050\nNumber of Classrooms\n40,522\n30,243\n30,400\nNumber of Districts\n2,510\n2,378\n2,387\nTable 3: This table provides descriptive statistics with regards to the number of interactions we\nhave on record for each student. Students, classrooms and districts whose interactions appear in the\ntraining set might also have interactions collected at a later date that appear in the validation and test\nsplits as the data split was partially temporal and not purely user based\nbecause if a data split does not contain at least a full academic year then the distributions would not\nmatch due to seasonal trends in the data. Table 3 shows some key statistics for the data splits\nB\nPreprocessing\nWe apply the following preprocessing steps to our data before using it to train our models. First, we\nrepresent the items under recommendation with tokens. The tokens are sequential integers for the v\nmost popular items. We take the vocabulary size v to be 2000. We then assign all the remainder of\nthe items to a single unique token used to represent out-of-vocabulary items.\nBreaking into\ncontexts\n(a) For Individual sequences relating to different students are split into contexts of length\nc. Each context is fed into the transformer as a separate datapoint.\nBreaking into\nsegments\nSampling and\njoining to form \ncontexts\n(b) For STUDY we split sequences relating to students into segments of length s, s \u2264 c.\nWe then sample multiple chunks from different students in the same classroom. The\nsampled segments are then concatenated with separator tokens (shown in gray) in between\nthem to form datapoints of at most length c.\nFigure 5: This figure details the preprocessing pipelines used for the two transformer models, with\nthe pipeline for Individual shown in (a) and the pipeline for STUDY shown in (b)\n14\nAdditional preprocessing is applied for Individual and STUDY. For the Individual model we set a\ncontext window length c and split students with interaction histories longer than c items into separate\ndata-points of at most length c. We took context length c = 65 for the transformer models. This\nis necessary as although the vast majority of sequences in each data split (over 92%) are under 65\nentries in length, there exists a long tail of sequences with very long length.\nFor STUDY slightly different processing is required. We first split the sequence associated with each\nstudent into segments of at most length s, s \u2264 c. Then we compose data-points of at most length c by\nconcatenating together multiple segments from multiple students in the same classroom separated\nwith separator tokens. To compose a single data-point we sample multiple segments from students in\nthe same classroom, while satisfying the constraints that overall length of each data-point is at most c\nand that we do not sample more than one segment from a single student. In our final model we took\ns = c = 65 Figure 5 shows a diagram explaining these procedures.\nC\nExperimental Details\nIn the following section we detail the choices of hyperparameters and the computational resources\nneeded. For the KNN recommender we took K to be equal to 2. There were no further hyperparame-\nters for the KNN recommender. The KNN recommender system had no training requirements, only\npreprocessing and inference, which we were able to run on the entire test split on a single machine\nwith no GPU or TPU accelerators within a few hours. For STUDY, Individual and SAMN we used\nthe Adam Optimizer with the following learning rate schedule\n\u03b1(s) =\n(\n\u03b1p \u2217 s/W\n0 < s \u2264 W\n\u03b1p\n\u221as\u2212W\ns > W\nIndividual\nPeak Learning Rate \u03b1p\n0.1024\nWarm up steps W\n1000\nTotal steps\n3500\nBatch Size\n131,072\nNumber of TPUs\n32\nRun time\n\u223c12 hours\nSTUDY\nPeak Learning Rate \u03b1p\n0.1024\nWarm up steps W\n1000\nTotal steps\n3500\nBatch Size\n131,072\nNumber of TPUs\n32\nRun time\n\u223c12 hours\nSAMN\nPeak Learning Rate \u03b1p\n0.0128\nWarm up steps W\n350\nTotal steps\n3500\nBatch Size\n524,288\nNumber of TPUs\n32\nRun time\n\u223c3 hours\nTable 4: This table shows hyperparameter values as well as computation resources for STUDY,\nIndividual and SAMN.\n15\nwhere \u03b1(s) is the learning rate at sth step, \u03b1p is the peak learning rate and W is the number of warm\nup steps. The \u03b1p and batch size were tuned for each model individually. We used Google Cloud\nTPUs with the number of TPUs used shown along with the hyperparameter values in Table 4 We\nnote that we were able to obtain good results with our models using smaller batch sizes but opted for\nlarger batch sizes for faster development iteration speed.\n16\n"
  }
]