[
  {
    "title": "Self-Rewarding Language Models",
    "link": "https://arxiv.org/pdf/2401.10020.pdf",
    "upvote": "133",
    "text": "Self-Rewarding Language Models\nWeizhe Yuan1,2\nRichard Yuanzhe Pang1,2\nKyunghyun Cho2\nXian Li1\nSainbayar Sukhbaatar1\nJing Xu1\nJason Weston1,2\n1 Meta\n2 NYU\nAbstract\nWe posit that to achieve superhuman agents, future models require super-\nhuman feedback in order to provide an adequate training signal. Current\napproaches commonly train reward models from human preferences, which\nmay then be bottlenecked by human performance level, and secondly these\nseparate frozen reward models cannot then learn to improve during LLM\ntraining. In this work, we study Self-Rewarding Language Models, where the\nlanguage model itself is used via LLM-as-a-Judge prompting to provide its\nown rewards during training. We show that during Iterative DPO training\nthat not only does instruction following ability improve, but also the ability\nto provide high-quality rewards to itself. Fine-tuning Llama 2 70B on three\niterations of our approach yields a model that outperforms many existing\nsystems on the AlpacaEval 2.0 leaderboard, including Claude 2, Gemini\nPro, and GPT-4 0613. While there is much left still to explore, this work\nopens the door to the possibility of models that can continually improve in\nboth axes.\n1\nIntroduction\nAligning Large Language Models (LLMs) using human preference data can vastly improve\nthe instruction following performance of pretrained models [Ouyang et al., 2022, Bai et al.,\n2022a]. The standard approach of Reinforcement Learning from Human Feedback (RLHF)\nlearns a reward model from these human preferences. The reward model is then frozen and\nused to train the LLM using RL, e.g., via PPO [Schulman et al., 2017]. A recent alternative\nis to avoid training the reward model at all, and directly use human preferences to train the\nLLM, as in Direct Preference Optimization [DPO; Rafailov et al., 2023]. In both cases, the\napproach is bottlenecked by the size and quality of the human preference data, and in the\ncase of RLHF the quality of the frozen reward model trained from them as well.\nIn this work, we instead propose to train a self-improving reward model that, rather than\nbeing frozen, is continually updating during LLM alignment, in order to avoid this bottleneck.\nThe key to such an approach is to develop an agent that possesses all the abilities desired\nduring training, rather than separating them out into distinct models such as a reward\nmodel and a language model. In the same way that pretraining and multitasking training of\ninstruction following tasks allow task transfer by training on many tasks at once [Collobert\nand Weston, 2008, Radford et al., 2019, Ouyang et al., 2022], incorporating the reward\nmodel into that same system allows task transfer between the reward modeling task and the\ninstruction following tasks.\nWe thus introduce Self-Rewarding Language Models, that both (i) act as instruction following\nmodels generating responses for given prompts; and (ii) can generate and evaluate new\ninstruction following examples to add to their own training set. We train these models\nusing an Iterative DPO framework similar to that recently introduced in Xu et al. [2023].\narXiv:2401.10020v2  [cs.CL]  8 Feb 2024\nGenerate\nresponses\nGenerate\nrewards\nPreference \npairs\nDPO \ntraining\n select\nGenerated \nnew prompts\nSelf-Instruction creation\nInstruction following training\nNext iteration model\nSeed model\n(for t=1)\nFigure 1: Self-Rewarding Language Models. Our self-alignment method consists of two\nsteps: (i) Self-Instruction creation: newly created prompts are used to generate candidate\nresponses from model Mt, which also predicts its own rewards via LLM-as-a-Judge prompting.\n(ii) Instruction following training: preference pairs are selected from the generated data,\nwhich are used for training via DPO, resulting in model Mt+1. This whole procedure can\nthen be iterated resulting in both improved instruction following and reward modeling ability.\nStarting from a seed model, in each iteration there is a process of Self-Instruction creation\nwhereby candidate responses are generated by the model for newly created prompts, and are\nthen assigned rewards by that same model. The latter is implemented via LLM-as-a-Judge\nprompting, which can also be seen as an instruction following task. A preference dataset is\nbuilt from the generated data, and the next iteration of the model is trained via DPO, see\nFigure 1.\nIn our experiments, we start with a Llama 2 70B [Touvron et al., 2023] seed model fine-tuned\non Open Assistant [K\u00f6pf et al., 2023], and then perform the above training scheme. We\nfind that not only does the instruction following performance improve from Self-Rewarding\nLLM alignment compared to the baseline seed model, but importantly the reward modeling\nability, which is no longer fixed, improves as well. This means that the model during iterative\ntraining is able, at a given iteration, to provide a higher quality preference dataset to itself\nthan in the previous iteration. While this effect likely saturates in real-world settings, it\nprovides the intriguing possibility of obtaining reward models (and hence LLMs) that are\nsuperior to ones that could have been trained from the original human-authored seed data\nalone.\n2\nSelf-Rewarding Language Models\nOur approach first assumes access to a base pretrained language model, and a small amount\nof human-annotated seed data. We then build a model that aims to possess two skills\nsimultaneously:\n1. Instruction following: given a prompt that describes a user request, the ability to\ngenerate a high quality, helpful (and harmless) response.\n2. Self-Instruction creation: the ability to generate and evaluate new instruction-\nfollowing examples to add to its own training set.\nThese skills are used so that the model can perform self-alignment, i.e., they are the\ncomponents used to iteratively train itself using AI Feedback (AIF).\nSelf-instruction creation consists of generating candidate responses and then the model itself\njudging their quality, i.e., it acts as its own reward model, replacing the need for an external\none. This is implemented via the LLM-as-a-Judge mechanism [Zheng et al., 2023b], i.e., by\nformulating the evaluation of responses as an instruction following task. This self-created\nAIF preference data is used as a training set.\nOur overall self-alignment procedure is an iterative one, which proceeds by building a series\nof such models, with the aim that each improves over the last. Importantly, because the\nmodel can both improve its generation ability, and act as its own reward model through the\nsame generation mechanism, this means the reward model itself can improve through these\niterations, deviating from standard practices where the reward model is fixed [Ouyang et al.,\n2\n2022]. We believe this can increase the ceiling of the potential for self-improvement of these\nlearning models going forward, removing a constraining bottleneck.\nWe describe these steps in more detail below. An overview of the approach is illustrated in\nFigure 1.\n2.1\nInitialization\nSeed instruction following data\nWe are given a seed set of human-authored (instruction\nprompt, response) general instruction following examples that we use for training in a\nsupervised fine-tuning (SFT) manner, starting from a pretrained base language model.\nSubsequently this will be referred to as Instruction Fine-Tuning (IFT) data.\nSeed LLM-as-a-Judge instruction following data\nWe also assume we are provided\na seed set of (evaluation instruction prompt, evaluation result response) examples which\ncan also be used for training. While this is not strictly necessary, as the model using IFT\ndata will already be capable of training an LLM-as-a-Judge, we show that such training\ndata can give improved performance (see Appendix A.3 for supporting results). In this data,\nthe input prompt asks the model to evaluate the quality of a given response to a particular\ninstruction. The provided evaluation result response consists of chain-of-thought reasoning\n(a justification), followed by a final score (in our experiments out of 5). The exact prompt\nformat we chose is given in Figure 2, which instructs the LLM to evaluate the response using\nfive additive criteria (relevance, coverage, usefulness, clarity and expertise), covering various\naspects of quality. Subsequently this will be referred to as Evaluation Fine-Tuning (EFT)\ndata.\nWe use both these seed sets together during training.\n2.2\nSelf-Instruction Creation\nUsing the model we have trained, we can make it self-modify its own training set. Specifically,\nwe generate additional training data for the next iteration of training.\nThis consists of the following steps:\n1. Generate a new prompt: We generate a new prompt xi using few-shot prompting,\nsampling prompts from the original seed IFT data, following the approach of Wang\net al. [2023] and Honovich et al. [2023].1\n2. Generate candidate responses: We then generate N diverse candidate responses\n{y1\ni , . . . , yN\ni } for the given prompt xi from our model using sampling.\n3. Evaluate candidate responses: Finally, we use the LLM-as-a-Judge ability of our\nsame model to evaluate its own candidate responses with scores rn\ni \u2208 [0, 5] (exact\nprompt given in Figure 2).\n2.3\nInstruction Following Training\nAs previously described, training is initially performed with the seed IFT and EFT data\n(Section 2.1). This is then augmented with additional data via AI (Self-)Feedback.\nAI Feedback Training\nAfter performing the self-instruction creation procedure, we\ncan augment the seed data with additional examples for training, which we refer to as AI\nFeedback Training (AIFT) data.\nTo do this, we construct preference pairs, which are training data of the form (instruction\nprompt xi, winning response yw\ni , losing response yl\ni). To form the winning and losing pair we\ntake the highest and lowest scoring responses from the N evaluated candidate responses (see\n1In our main experiments, responses and rewards, items (2) and (3), are generated by the model\nwe have trained, but generating prompts is actually done by a model fixed in advance. However,\nwe show that prompts can also be generated by the newly trained model in each iteration in\nAppendix A.5.\n3\nReview the user\u2019s question and the corresponding response using the additive 5-point\nscoring system described below. Points are accumulated based on the satisfaction of each\ncriterion:\n- Add 1 point if the response is relevant and provides some information related to\nthe user\u2019s inquiry, even if it is incomplete or contains some irrelevant content.\n- Add another point if the response addresses a substantial portion of the user\u2019s question,\nbut does not completely resolve the query or provide a direct answer.\n- Award a third point if the response answers the basic elements of the user\u2019s question in a\nuseful way, regardless of whether it seems to have been written by an AI Assistant or if it\nhas elements typically found in blogs or search results.\n- Grant a fourth point if the response is clearly written from an AI Assistant\u2019s perspective,\naddressing the user\u2019s question directly and comprehensively, and is well-organized and\nhelpful, even if there is slight room for improvement in clarity, conciseness or focus.\n- Bestow a fifth point for a response that is impeccably tailored to the user\u2019s question\nby an AI Assistant, without extraneous information, reflecting expert knowledge, and\ndemonstrating a high-quality, engaging, and insightful answer.\nUser: <INSTRUCTION_HERE>\n<response><RESPONSE_HERE></response>\nAfter examining the user\u2019s instruction and the response:\n- Briefly justify your total score, up to 100 words.\n- Conclude with the score using the format: \u201cScore: <total points>\u201d\nRemember to assess from the AI Assistant perspective, utilizing web search knowledge as\nnecessary. To evaluate the response in alignment with this additive scoring model, we\u2019ll\nsystematically attribute points based on the outlined criteria.\nFigure 2: LLM-as-a-Judge prompt for our LLM to act as a reward model and\nprovide self-rewards for its own model generations. The model is initially trained with seed\ntraining data of how to perform well at this task, and then improves at this task further\nthrough our self-rewarding training procedure.\nSection 2.2), following Xu et al. [2023], discarding the pair if their scores are the same. These\npairs can be used for training with a preference tuning algorithm. We use DPO [Rafailov\net al., 2023].\n2.4\nOverall Self-Alignment Algorithm\nIterative Training\nOur overall procedure trains a series of models M1, . . . , MT where\neach successive model t uses augmented training data created by the t \u2212 1th model. We thus\ndefine AIFT(Mt) to mean AI Feedback Training data created using model Mt.\nModel Sequence\nWe define the models, and the training data they use as follows:\nM0 : Base pretrained LLM with no fine-tuning.\nM1 : Initialized with M0, then fine-tuned on the IFT+EFT seed data using SFT.\nM2 : Initialized with M1, then trained with AIFT(M1) data using DPO.\nM3 : Initialized with M2, then trained with AIFT(M2) data using DPO.\nThis iterative training resembles the procedure used in Pairwise Cringe Optimization and\nspecifically is termed Iterative DPO, introduced in Xu et al. [2023]; however, an external\nfixed reward model was used in that work.\n4\n3\nExperiments\n3.1\nExperimental Setup\nBase Model\nIn our experiments we use Llama 2 70B [Touvron et al., 2023] as our base\npretrained model.\n3.1.1\nSeed Training Data\nIFT Seed Data\nWe use the human-authored examples provided in the Open Assistant\ndataset [K\u00f6pf et al., 2023] for instruction fine-tuning. Following Li et al. [2024] we use\n3200 examples, by sampling only first conversational turns in the English language that are\nhigh-quality, based on their human annotated rank (choosing only the highest rank 0). In\nour experiments, we compare to a model fine-tuned from the base model using only this data\nvia supervised fine-tuning, and refer to it as our SFT baseline.\nEFT Seed Data\nThe Open Assistant data also provides multiple ranked human responses\nper prompt from which we can construct evaluation fine-tuning data. We split this into train\nand evaluation sets, and use it to create LLM-as-a-Judge data. This is done by placing it in\nthe input format given in Figure 2, which consists of the scoring criteria description, and\nthe given instruction and response to be evaluated.2 For training targets, chain-of-thought\njustifications and final scores out of 5 are not directly provided, so we use the SFT baseline\nto generate such output evaluations for each input, and accept them into the training set if\nthe ranking of their scores agrees with the human rankings in the dataset. We resample the\ntraining set by discarding some of the data that receives the most common score so that the\nscores are not too skewed, as we observe many samples receive a score of 4. This results in\n1,630 train and 531 evaluation examples (which do not overlap with the IFT data).\n3.1.2\nEvaluation Metrics\nWe evaluate the performance of our self-rewarding models in two axes: their ability to follow\ninstructions, and their ability as a reward model (ability to evaluate responses).\nInstruction Following\nWe evaluate head-to-head performance between various models\nusing GPT-4 [Achiam et al., 2023] as an evaluator over 256 test prompts (which we refer to\nas IFT test data) derived from various sources following Li et al. [2024] using the AlpacaEval\nevaluation prompt [Li et al., 2023]. We try the prompt in both orders comparing pairwise,\nand if the GPT-4 evaluations disagree we count the result as a tie. We also perform a\nsimilar evaluation with humans (authors). We additionally report results in the AlpacaEval\n2.0 leaderboard format which is evaluated over 805 prompts, and compute the win rate\nagainst the baseline GPT-4 Turbo model based on GPT-4 judgments. Further, we report\nresults on MT-Bench [Zheng et al., 2023b] a set of challenging multi-turn questions in various\ncategories from math and coding to roleplay and writing, which uses GPT-4 to grade the\nmodel responses out of 10. Finally we also test the models on a set of 9 NLP benchmarks:\nARC-Easy [Clark et al., 2018], ARC-Challenge [Clark et al., 2018], HellaSwag [Zellers et al.,\n2019], SIQA [Sap et al., 2019], PIQA [Bisk et al., 2020], GSM8K [Cobbe et al., 2021], MMLU\n[Hendrycks et al., 2021], OBQA [Mihaylov et al., 2018] and NQ [Kwiatkowski et al., 2019].\nReward Modeling\nWe evaluate the correlation with human rankings on the evaluation set\nwe derived from the Open Assistant dataset, as described in Section 3.1.1. Each instruction\nhas on average 2.85 responses with given rankings. We can thus measure the pairwise\naccuracy, which is how many times the order of the ranking between any given pair agrees\nbetween the model\u2019s evaluation and the human ranking. We also measure the exact match\ncount, which is how often the total ordering is exactly the same for an instruction. We also\nreport the Spearman correlation and Kendall\u2019s \u03c4. Finally, we report how often the responses\nthat the model scores a perfect 5 out of 5 are rated as the highest ranked by humans.\n2Note, the prompt, derived from Li et al. [2024], mentions \u201cutilizing web search\u201d, but our model\nis not actually capable of this action.\n5\n3.1.3\nTraining Details\nInstruction following training\nThe training hyperparameters we use are as follows. For\nSFT we use learning rate 5.5e\u22126 which decays (cosine) to 1.1e\u22126 at the end of training,\nbatch size 16 and dropout 0.1. We only calculate the loss on target tokens instead of the full\nsequence. For DPO we use learning rate 1e\u22126 which decays to 1e\u22127, batch size 16, dropout\n0.1, and a \u03b2 value of 0.1. We perform early stopping by saving a checkpoint every 200 steps\nand evaluating generations using Claude 2 [Anthropic, 2023] on 253 validation examples\nderived from various sources following Li et al. [2024]. This is evaluated pairwise against the\nprevious step\u2019s generations using the AlpacaEval evaluation prompt format [Li et al., 2023].\nSelf-Instruction creation\nTo generate new prompts we use a fixed model, Llama 2-Chat\n70B with 8-shot prompting following Self-Instruct [Wang et al., 2023], where we sample six\ndemonstrations from the IFT data and two from the model generated data, and use decoding\nparameters T = 0.6, p = 0.9. We use their prompt template for non-classification tasks and\napply the same filtering techniques, including the ROUGE-L [Lin, 2004] similarity check,\nkeyword filtering, and length filtering. Except for the prompt generation part, the other parts\nof the creation pipeline (generating the response, and evaluating it) use the Self-Rewarding\nmodel being trained. For candidate response generation we sample N = 4 candidate responses\nwith temperature T = 0.7, p = 0.9. When evaluating candidate responses, as there is variance\nto these scores, in our experiments we also use sampled decoding (with the same parameters)\nand generate these evaluations multiple (3) times and take the average. We added 3,964\nsuch preference pairs to form the AIFT(M1) dataset used to train M2 via DPO, and 6,942\npairs to form AIFT(M2) used to train M3.\n3.2\nResults\n3.2.1\nInstruction Following Ability\nHead to head performance results are provided in Figure 3.\nEFT+IFT seed training performs similarly to IFT alone\nWe find that adding\nthe Evaluation Fine-Tuning (EFT) task to training does not impact instruction following\nperformance compared to using Instruction Fine-Tuning (IFT) data alone with an almost\nequal head to head (30.5% wins vs. 30.9% wins). This is a positive result because it means\nthe increased capability of a model to self-reward does not affect its other skills. We can\nthus use IFT+EFT training as Iteration 1 (M1) of our Self-Rewarding model, and then run\nfurther iterations.\nIteration 2 (M2) improves over Iteration 1 (M1) and SFT Baseline\nIteration 2\nof Self-Rewarding training (M2) provides superior instruction following to Iteration 1 (M1)\nwith 55.5% wins for M2 compared to only 11.7% for M1 in a head to head evaluation. It\nprovides similar gains over the SFT Baseline as well (49.2% wins vs. 14.5% wins). Clearly,\nthere is a large jump in performance from M1 to M2 by using the preference data AIFT(M1)\nprovided by the reward model from Iteration 1.\nIteration 3 (M3) improves over Iteration 2 (M2)\nWe see a further gain in Iteration 3\nover Iteration 2, with 47.7% wins for M3 compared to only 12.5% for M2 in a head to head\nevaluation. Similarly, the win rate over the SFT Baseline for M3 increases to 62.5% wins vs.\n9.8%, i.e., winning more often than the M2 model did. Overall, we see large gains from M2\nto M3 through training using the preference data AIFT(M2) provided by the reward model\nfrom Iteration 2.\nSelf-Rewarding models perform well on AlpacaEval 2 leaderboard\nWe evaluate\nour models on the AlpacaEval 2.0 leaderboard format, with results given in Table 1. We\nobserve the same findings as in the head-to-head evaluations, that training iterations yield\nimproved win rates, in this case over GPT4-Turbo, from 9.94% in Iteration 1, to 15.38%\nin Iteration 2, to 20.44% in Iteration 3. Our Iteration 3 model outperforms many existing\nmodels in this metric, including Claude 2, Gemini Pro, and GPT4 0613. We show some\n6\nSelf-Rewarding M3\nvs.\nSFT Baseline\nSelf-Rewarding M2\nvs.\nSFT Baseline\nSelf-Rewarding M1\nvs.\nSFT Baseline\n62.5\n49.2\n30.5\n27.7\n36.3\n38.7\n9.8\n14.5\n30.9\nSelf-Rewarding Wins\nTie\nSFT Baseline Wins\nSelf-Rewarding M3\nvs.\nM2\nSelf-Rewarding M2\nvs.\nM1\nSelf-Rewarding M3\nvs.\nM1\n47.7\n55.5\n68.8\n39.8\n32.8\n22.7\n12.5\n11.7\n8.6\nLeft Wins (in Left vs. Right)\nTie\nRight Wins\nFigure 3: Instruction following ability improves with Self-Training: We evaluate our\nmodels using head-to-head win rates on diverse prompts using GPT-4. The SFT Baseline is\non par with Self-Rewarding Iteration 1 (M1). However, Iteration 2 (M2) outperforms both\nIteration 1 (M1) and the SFT Baseline. Iteration 3 (M3) gives further gains over Iteration 2\n(M2), outperforming M1, M2 and the SFT Baseline by a large margin.\nselected models from the leaderboard in the table. We note that many of those competing\nmodels contain either proprietary alignment data (which is typically large, e.g., over 1M\nannotations in Touvron et al. [2023]) or use targets that are distilled from stronger models. In\ncontrast, our Self-Rewarding model starts from a small set of seed data from Open Assistant,\nand then generates targets and rewards from the model itself for further iterations of training.\nFine-grained analysis\nAs described earlier, the overall performance of the model in\nAlpacaEval improves with each iteration of training. It would be interesting to break down\nthe overall performance improvement to see exactly what type of tasks these improvements\ncome from. Therefore, we cluster the instructions in AlpacaEval test set into different groups\nbased on three perspectives: (1) instruction category (2) instruction complexity (3) expected\nresponse length. We achieve this by using GPT-4. The detailed statistical information of the\nbreakdown and the prompting techniques we used for getting this breakdown can be found in\nAppendix A.6. Results for the instruction category are given in Figure 4, and the other two\nin Appendix Figure 11. From the results we can conclude that (i) Self-Rewarding models can\nsubstantially improve the win rate in most categories, but there are some tasks for which this\napproach does not improve, such as mathematics and logical reasoning, indicating that our\ncurrent training approach mainly allows the models to better utilize their existing knowledge.\n(ii) Through Self-Rewarding model training, the model\u2019s win rate increases on almost all\ntasks of different complexity, and especially on slightly more difficult tasks (complexity of 5,\n6, 7 out of 10). (iii) The models also show a steady increase in the win rate on tasks with\ninstructions with different expected response lengths.\n7\nTable 1: AlpacaEval 2.0 results (win rate over GPT-4 Turbo evaluated by GPT-4).\nSelf-Rewarding iterations yield improving win rates. Iteration 3 (M3) outperforms many\nexisting models that use proprietary training data or targets distilled from stronger models.\nAlignment Targets\nModel\nWin Rate\nDistilled\nProprietary\nSelf-Rewarding 70B\nIteration 1 (M1)\n9.94%\nIteration 2 (M2)\n15.38%\nIteration 3 (M3)\n20.44%\nSelected models from the leaderboard\nGPT-4 0314\n22.07%\n\u2713\nMistral Medium\n21.86%\n\u2713\nClaude 2\n17.19%\n\u2713\nGemini Pro\n16.85%\n\u2713\nGPT-4 0613\n15.76%\n\u2713\nGPT 3.5 Turbo 0613\n14.13%\n\u2713\nLLaMA2 Chat 70B\n13.87%\n\u2713\nVicuna 33B v1.3\n12.71%\n\u2713\nHumpback LLaMa2 70B\n10.12%\nGuanaco 65B\n6.86%\nDavinci001\n2.76%\n\u2713\nAlpaca 7B\n2.59%\n\u2713\nHealth\nProfessional\nLinguistics\nOther\nEntertainment\nTechnology\nLiterature\nCoding\nScience\nGaming\nPhilosophy\nSocial Studies\nTravel\nArts\nSports\nMathematics\nSocial Interaction\nDIY Projects\nCooking\n0\n5\n10\n15\n20\n25\n30\n35\nWin rate (%)\nM0\nM1\nM2\nM3\nFigure 4: AlpacaEval win rate breakdown for instruction categories (full names given in\nAppendix). Self-Rewarding models give gains across several topics, but tend to e.g. give less\ngains on mathematics and reasoning tasks.\nData distribution analysis\nWe perform a t-SNE [Van der Maaten and Hinton, 2008]\nvisualization of the IFT, EFT and AIFT(M1) data, shown in Appendix A.1. We find good\noverlap between the IFT and AIFT(M1) examples, which is desired, while the EFT examples\nlie in a different part of the embedding space, which can help explain why they would not\naffect IFT performance. We observe that generations from M1 on AlpacaEval have an\naverage length of 1092, for M2 they are 1552, and for M3 they are 2552, so the model is\nlearning to generate longer responses, which we note may be a factor in relative performance.\n8\nSelf-Rewarding M3\nvs.\nSFT Baseline\nSelf-Rewarding M2\nvs.\nSFT Baseline\nSelf-Rewarding M1\nvs.\nSFT Baseline\n66.0\n56.0\n28.0\n16.0\n24.0\n26.0\n18.0\n20.0\n46.0\nSelf-Rewarding Wins\nTie\nSFT Baseline Wins\nFigure 5:\nHuman evaluation results.\nIterations of Self-Rewarding (M1, M2 and\nM3) provide progressively better head-to-head win rates compared to the SFT baseline, in\nagreement with the automatic evaluation results.\nTable 2: MT-Bench Results (on a scale of 10). Self-Rewarding iterations yield improving\nscores across various categories. Math, code & reasoning performance and iteration gains\nare smaller than for other categories, likely due to the makeup of the Open Assistant seed\ndata we use.\nOverall\nMath, Code\nHumanities, Extraction,\nScore\n& Reasoning\nSTEM, Roleplay & Writing\nSFT Baseline\n6.85\n3.93\n8.60\nM1\n6.78\n3.83\n8.55\nM2\n7.01\n4.05\n8.79\nM3\n7.25\n4.17\n9.10\nTable 3: NLP Benchmarks. Self-Rewarding models mostly tend to maintain performance\ncompared to the Llama 2 70B base model and the SFT Baseline, despite being fine-tuned on\nvery different instruction-following prompts.\nARC (\u2191)\nchallenge\nHellaSwag\n(\u2191)\nGSM8K\n(\u2191)\nMMLU\n(\u2191)\nNQ\n(\u2191)\nLlama 2\n57.40\n85.30\n56.80\n68.90\n25.30\nSFT Baseline\n55.97\n85.17\n50.72\n69.76\n34.35\nM1\n57.51\n84.99\n60.27\n69.34\n35.48\nM2\n54.51\n84.27\n59.29\n69.31\n33.07\nM3\n53.13\n83.29\n57.70\n69.37\n31.86\nHuman evaluation\nTo examine whether human judgments align with automatic evaluation\nresults, we conduct human evaluations that compare SFT baseline generations with the\ngenerations from each iteration of Self-Rewarding training, i.e., models M1, M2, and M3.\nSpecifically, we randomly select 50 instructions from the IFT test set. Each instruction\ncorresponds to three pairs of generations (i.e., baseline vs. M1, baseline vs. M2, baseline\nvs. M3). For each pair of generations, we assign them to three different annotators (blind\nevaluation performed by the authors) to make a pairwise judgment, and take a majority vote\nto decide which generation is better. The human evaluation results are shown in Figure 5.\nWe find that Self-Rewarding models from later iterations show a larger advantage over the\nSFT baseline model, which is consistent with GPT-4\u2019s judgments, and demonstrates the\neffectiveness of our iterative training procedure.\nMT-Bench performance further validates these results\nWe report performance on\nMT-Bench in Table 2 for the SFT baseline and iterations of the Self-Rewarding model. We\n9\nTable 4: Reward Modeling ability improves with Self-Training: We evaluate the LLM-\nas-a-Judge via various metrics which measure alignment with held-out human preference data.\nSelf-Rewarding Iteration 2 (Model M2), which is trained using the self-reward model derived\nfrom its previous iteration M1 outperforms Iteration 1 (M1), while M1 itself outperforms a\nstandard SFT baseline model trained on only Instruction Fine-Tuning (IFT) data. Iteration\n3 (Model M3) gives further improvements over Iteration 2.\nSelf-Rewarding Models\nModel\nSFT Baseline\nIter 1 (M1)\nIter 2 (M2)\nIter 3 (M3)\nTraining data\nIFT\nIFT+EFT\nIFT+EFT\nIFT+EFT+AIFT(M1)\n+AIFT(M1)\n+AIFT(M2)\nPairwise acc. (\u2191)\n65.1%\n78.7%\n80.4%\n81.7%\n5-best % (\u2191)\n39.6%\n41.5%\n44.3%\n43.2%\nExact Match % (\u2191)\n10.1%\n13.1%\n14.3%\n14.3%\nSpearman corr. (\u2191)\n0.253\n0.279\n0.331\n0.349\nKendall \u03c4 corr. (\u2191)\n0.233\n0.253\n0.315\n0.324\nagain see improvements across the iterations of training from M1 to M3, from 6.78 (out of\n10) up to 7.25, with larger relative gains in the humanities, STEM, roleplay, writing and\nextraction categories, and smaller gains in the math, code and reasoning categories. We\nexpect that the latter is due to the seed prompts we use from Open Assistant tending to\nunderemphasize the reasoning-based tasks. We note also that these improvements are in\nspite of our method using and constructing prompts that only involve a single turn, given\nthe MT-Bench benchmark itself is a multi-turn evaluation.\nSelf-rewarding models did not lose ability on NLP Benchmarks\nAs shown in\nTable 3, the performance of most NLP benchmark tasks evaluated are roughly similar to\nthe baselines, with further detailed results on more datasets given in Appendix Table 9\nthat follow the same pattern. We hypothesize that given that our training data (seed data\nand synthetically generated data) are based on the Open Assistant prompts which may\nnot be especially relevant to skills needed in the Table 3 tasks, it is expected that the task\nperformance stays roughly similar, or may even drop. For example, in InstructGPT training\n[Ouyang et al., 2022] they found that \u201cduring RLHF fine-tuning, we observe performance\nregressions compared to GPT-3 on certain public NLP datasets\u201d which they refer to as an\n\u201calignment tax.\u201d A clear future direction is to extend the self-rewarding paradigm to these\ntypes of tasks, by relying not only on seed prompts from Open Assistant, but also on seed\nprompts found in a larger variety of datasets.\n3.2.2\nReward Modeling Ability\nReward modeling evaluation results are provided in Table 4.\nEFT augmentation improves over SFT baseline\nFirstly, we find that adding Eval-\nuation Fine-Tuning (EFT) data into training, which gives examples to the model of how\nto act as an LLM-as-a-Judge, naturally improves its performance compared to training\nwith Instruction Fine-Tuning (IFT) data alone. IFT data covers a wide range of general\ninstruction tasks, and so does endow the SFT Baseline with the ability to evaluate responses;\nhowever, EFT data gives more examples of this specific task. We find improvements across\nall five metrics measured when using IFT+EFT vs. IFT alone, e.g., the pairwise accuracy\nagreement with humans increases from 65.1% to 78.7%.\nReward Modeling ability improves with Self-Training\nWe find that performing\na round of self-reward training improves the ability of the model at providing self-rewards\nfor the next iteration, in addition to its improved instruction following ability. Model M2\n(Iteration 2) is trained using the reward model from M1 (Iteration 1), but provides improved\nperformance on all five metrics compared to M1. For example, pairwise accuracy improves\n10\nfrom 78.7% to 80.4%. Iteration 3 (M3) improves several of these metrics further compared\nto M2, for example pairwise accuracy increases from 80.4% to 81.7%. This performance gain\nis achieved despite there being no additional EFT data provided, and the examples created\nduring the Self-Instruction creation loop do not tend to look like LLM-as-a-Judge training\nexamples. We hypothesize that because the model is becoming better at general instruction\nfollowing, it nevertheless also improves at the LLM-as-a-Judge task.\nImportance of the LLM-as-a-Judge Prompt\nIn these experiments we used the LLM-\nas-a-Judge prompt format shown in Figure 2. In preliminary experiments we also tried\nvarious other prompts to decide the most effective one to use. For example, we tried the\nprompt proposed in Li et al. [2024] which also proposes a 5-point scale, but describes the\noptions as multiple choice in a range of quality buckets, see Appendix Figure 7. In contrast,\nour prompt describes the points as additive, covering various aspects of quality. We find\na large difference between these two prompts when using the SFT Baseline, e.g. 65.1%\npairwise accuracy for ours, and only 26.6% pairwise accuracy for theirs. See Appendix A.2\nfor further details.\n4\nRelated Work\nAutomatically improving or self-correcting large language models is becoming a major focus\nof research. A recent survey from Pan et al. [2023] attempts to summarize the topic. However,\nthis is a rapidly moving area, and there are already promising new works not covered there.\nReinforcement Learning from Human Feedback (RLHF)\nPreference learning ap-\nproaches such as in Ziegler et al. [2019], Stiennon et al. [2020], Ouyang et al. [2022], Bai et al.\n[2022a] train a fixed reward model from human preference data, and then use the reward\nmodel to train via reinforcement learning (RL), e.g. via Proximal Policy Optimization (PPO)\n[Schulman et al., 2017]. Thus, the reward signal in a certain sense already comes from a model\neven in these works, but distilled from human data. Nevertheless, this is commonly referred\nto as RL from Human Feedback (RLHF). Methods such as Direct Preference Optimization\n(DPO) [Rafailov et al., 2023] avoid training the reward model entirely, and instead directly\ntrain the LLM using human preferences. Several other such competing methods exist as\nwell [Zhao et al., 2023, Zheng et al., 2023a, Yuan et al., 2023], including Pairwise Cringe\nOptimization (PCO) [Xu et al., 2023]. PCO uses an iterative training approach similar to\nthe one in our work, except with a fixed reward model, and that work also showed that\nIterative DPO improves over DPO using the same scheme. We note that other works have\ndeveloped iterative preference training schemes as well, e.g. Adolphs et al. [2023], Gulcehre\net al. [2023], Xiong et al. [2023].\nReinforcement Learning from AI Feedback (RLAIF)\nConstitutional AI [Bai et al.,\n2022b] uses an LLM to give feedback and refine responses, and uses this data to train a\nreward model. This fixed, separate reward model is then used to train the language model\nvia RL, called \u201cRL from AI Feedback\u201d (RLAIF). Lee et al. [2023] compare RLAIF and\nRLHF procedures and find the methods they compare perform roughly equally. They use\nan \u201coff-the-shelf\u201d LLM to perform LLM-as-a-Judge prompting to build a training set to\ntrain a fixed reward model, which is then used for RL training. They also experiment with\nusing the fixed but separate LLM-as-a-Judge model directly, which the authors report is\ncomputationally expensive due to using it within PPO training (rather than the offline step\nin the iterative approach we use in our work, which is relatively computationally cheap).\nFinally, SPIN [Chen et al., 2024b] recently showed they can avoid reward models entirely in\nan Iterative DPO-like framework by using human labels as the winning response in a pair,\nand the last iteration\u2019s generations as the losing response in the pair. The authors note\nthis has the limitation that once the model generations reach human performance, they are\nbottlenecked. Further, each input prompt is required to have a human annotated response,\nin contrast to our work.\nImproving LLMs via data augmentation (and curation)\nSeveral methods have\nimproved LLMs by (self-)creating training data to augment fine-tuning. Self-Instruct [Wang\n11\net al., 2023] is a method for self-instruction creation of prompts and responses, which can\nbe used to improve a base LLM. We make use of a similar technique in our work, and then\nuse our self-reward model to score them. Several approaches have also created training data\nby distilling from powerful LLMs, and shown a weaker LLM can then perform well. For\nexample, Alpaca [Taori et al., 2023] fine-tuned a Llama 7B model with text-davinci-003\ninstructions created in the style of self-instruct. Alpagasus [Chen et al., 2024a] employed a\nstrong LLM-as-a-Judge (ChatGPT) to curate the Alpaca dataset and filter to a smaller set,\nobtaining improved results. Instruction Backtranslation [Li et al., 2024] similarly augments\nand curates training data, but augmenting via backtranslating from web documents to predict\nprompts. The curation is done by the LLM(-as-a-Judge) itself, so can be seen as an instance\nof a self-rewarding model, but in a specialized setting. Reinforced Self-Training (ReST)\n[Gulcehre et al., 2023] uses a fixed, external reward to curate new high-quality examples to\niteratively add to the training set, improving performance. In our experiments, we found\nthat adding only positive examples in a related manner did not help, whereas preference\npairs did help (see Appendix Section A.4 for details).\nLLM-as-a-Judge\nUsing LLM-as-a-Judge prompting to evaluate language models has\nbecome a standard approach [Dubois et al., 2023, Li et al., 2023, Fernandes et al., 2023, Bai\net al., 2023, Saha et al., 2023], and is being used to train reward models or curate data as\nwell, as described above [Lee et al., 2023, Chen et al., 2024a, Li et al., 2024]. While some\nworks such as Kim et al. [2023] create training data to train an LLM to perform well as a\njudge, to our knowledge it is not common to combine this training with general instruction\nfollowing skills as in our work.\n5\nConclusion\nWe have introduced Self-Rewarding Language Models, models capable of self-alignment via\njudging and training on their own generations. The method learns in an iterative manner,\nwhere in each iteration the model creates its own preference-based instruction training data.\nThis is done by assigning rewards to its own generations via LLM-as-a-Judge prompting,\nand using Iterative DPO to train on the preferences. We showed that this training both\nimproves the instruction following capability of the model, as well as its reward-modeling\nability across the iterations. While there are many avenues left unexplored, we believe this\nis exciting because this means the model is better able to assign rewards in future iterations\nfor improving instruction following \u2013 a kind of virtuous circle. While this improvement likely\nsaturates in realistic scenarios, it still allows for the possibility of continual improvement\nbeyond the human preferences that are typically used to build reward models and instruction\nfollowing models today.\n6\nLimitations\nWhile we have obtained promising experimental results, we currently consider them pre-\nliminary because there are many avenues yet to explore, among them the topics of further\nevaluation, including safety evaluation, and understanding the limits of iterative training.\nWe showed that the iterations of training improve both instruction following and reward\nmodeling ability, but only ran three iterations in a single setting. A clear line of further\nresearch is to understand the \u201cscaling laws\u201d of this effect both for more iterations, and with\ndifferent language models with more or less capabilities in different settings.\nWe observed an increase in length in model generations, and there is a known correlation\nbetween length and estimated quality, which is a topic that should be understood more\ndeeply in general, and in our results in particular as well. It would also be good to understand\nif so-called \u201creward-hacking\u201d can happen within our framework, and in what circumstances.\nAs we are using both a language model as the training reward, and a language model for final\nevaluation (GPT-4) in some of our benchmarks, even if they are different models, this may\nrequire a deeper analysis than we have provided. While the human evaluation we conducted\ndid provide validation of the automatic results, further study could bring more insights.\n12\nAnother clear further avenue of study is to conduct safety evaluations \u2013 and to explore safety\ntraining within our framework. Reward models have been built exclusively for safety in\nexisting systems [Touvron et al., 2023], and a promising avenue here would be to use the\nLLM-as-a-Judge procedure to evaluate for safety specifically in our self-rewarding training\nprocess. Given that we have shown that reward modeling ability improves over training\niterations, this could mean that the safety of the model could potentially improve over\ntime as well, with later iterations being able to catch and mitigate more challenging safety\nsituations that earlier iterations cannot.\n13\nReferences\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni\nAleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.\nGPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\nLeonard Adolphs, Tianyu Gao, Jing Xu, Kurt Shuster, Sainbayar Sukhbaatar, and Jason\nWeston. The CRINGE loss: Learning what language not to model. In Anna Rogers, Jordan\nBoyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers), pages 8854\u20138874,\nToronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/\nv1/2023.acl-long.493. URL https://aclanthology.org/2023.acl-long.493.\nAnthropic. Claude 2. https://www.anthropic.com/index/claude-2, 2023.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,\nDawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful\nand harmless assistant with reinforcement learning from human feedback. arXiv preprint\narXiv:2204.05862, 2022a.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy\nJones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitu-\ntional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073, 2022b.\nYushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng\nZeng, Yijia Xiao, Haozhe Lyu, Jiayin Zhang, Juanzi Li, and Lei Hou. Benchmarking\nfoundation models with language-model-as-an-examiner. In Thirty-seventh Conference\non Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL\nhttps://openreview.net/forum?id=IiRHQ7gvnq.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning\nabout physical commonsense in natural language. In Thirty-Fourth AAAI Conference on\nArtificial Intelligence, 2020.\nLichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang,\nVijay Srinivasan, Tianyi Zhou, Heng Huang, et al. AlpaGasus: Training a better alpaca\nwith fewer data. In The Twelfth International Conference on Learning Representations,\n2024a. URL https://openreview.net/forum?id=FdVXgSJhvz.\nZixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning\nconverts weak language models to strong language models. arXiv preprint arXiv:2401.01335,\n2024b.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,\nand Oyvind Tafjord. Think you have solved question answering? Try ARC, the AI2\nreasoning challenge. arXiv preprint arXiv:1803.05457, 2018.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz\nKaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher\nHesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint\narXiv:2110.14168, 2021.\nRonan Collobert and Jason Weston. A unified architecture for natural language processing:\nDeep neural networks with multitask learning. In Proceedings of the 25th International\nConference on Machine Learning, pages 160\u2013167, 2008.\nYann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos\nGuestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework\nfor methods that learn from human feedback. arXiv preprint arXiv:2305.14387, 2023.\nPatrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, Andr\u00e9 Martins, Graham\nNeubig, Ankush Garg, Jonathan Clark, Markus Freitag, and Orhan Firat. The devil is in the\nerrors: Leveraging large language models for fine-grained machine translation evaluation.\n14\nIn Philipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz, editors, Proceedings of\nthe Eighth Conference on Machine Translation, pages 1066\u20131083, Singapore, December\n2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.wmt-1.100. URL\nhttps://aclanthology.org/2023.wmt-1.100.\nCaglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts,\nAbhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al.\nReinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998,\n2023.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Ja-\ncob Steinhardt. Measuring massive multitask language understanding. In 9th International\nConference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7,\n2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=d7KBjmI3GmQ.\nOr Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning\nlanguage models with (almost) no human labor. In Anna Rogers, Jordan Boyd-Graber,\nand Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages 14409\u201314428, Toronto, Canada,\nJuly 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.806.\nURL https://aclanthology.org/2023.acl-long.806.\nSeungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun,\nSeongjin Shin, Sungdong Kim, James Thorne, et al. Prometheus: Inducing fine-grained\nevaluation capability in language models. arXiv preprint arXiv:2310.08491, 2023.\nAndreas K\u00f6pf, Yannic Kilcher, Dimitri von R\u00fctte, Sotiris Anagnostidis, Zhi-Rui Tam,\nKeith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Rich\u00e1rd Nagyfi,\net al. OpenAssistant conversations\u2013democratizing large language model alignment. arXiv\npreprint arXiv:2304.07327, 2023.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh,\nChris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton\nLee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit,\nQuoc Le, and Slav Petrov. Natural questions: a benchmark for question answering research.\nTransactions of the Association of Computational Linguistics, 2019.\nHarrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop,\nVictor Carbune, and Abhinav Rastogi. RLAIF: Scaling reinforcement learning from human\nfeedback with ai feedback. arXiv preprint arXiv:2309.00267, 2023.\nXian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason\nWeston, and Mike Lewis. Self-alignment with instruction backtranslation. In The Twelfth\nInternational Conference on Learning Representations, 2024. URL https://openreview.\nnet/forum?id=1oijHJBRsT.\nXuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin,\nPercy Liang, and Tatsunori B. Hashimoto.\nAlpacaeval: An automatic evaluator of\ninstruction-following models. https://github.com/tatsu-lab/alpaca_eval, 2023.\nChin-Yew Lin.\nROUGE: A package for automatic evaluation of summaries.\nIn Text\nSummarization Branches Out, pages 74\u201381, Barcelona, Spain, July 2004. Association for\nComputational Linguistics. URL https://aclanthology.org/W04-1013.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor\nconduct electricity? a new dataset for open book question answering. In EMNLP, 2018.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language\nmodels to follow instructions with human feedback. Advances in Neural Information\nProcessing Systems, 35:27730\u201327744, 2022.\n15\nLiangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang\nWang. Automatically correcting large language models: Surveying the landscape of diverse\nself-correction strategies. arXiv preprint arXiv:2308.03188, 2023.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\nLanguage models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and\nChelsea Finn. Direct preference optimization: Your language model is secretly a reward\nmodel. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.\nURL https://openreview.net/forum?id=HPuSIXJaa9.\nSwarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, and Xian\nLi. Branch-solve-merge improves large language model evaluation and generation. arXiv\npreprint arXiv:2310.15123, 2023.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Socialiqa:\nCommonsense reasoning about social interactions. CoRR, abs/1904.09728, 2019. URL\nhttp://arxiv.org/abs/1904.09728.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec\nRadford, Dario Amodei, and Paul F Christiano. Learning to summarize with human\nfeedback. Advances in Neural Information Processing Systems, 33:3008\u20133021, 2020.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin,\nPercy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following\nllama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:\nOpen foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nLaurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of\nmachine learning research, 9(11), 2008.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel\nKhashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-\ngenerated instructions.\nIn Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki,\neditors, Proceedings of the 61st Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 13484\u201313508, Toronto, Canada, July 2023.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.754. URL\nhttps://aclanthology.org/2023.acl-long.754.\nWei Xiong, Hanze Dong, Chenlu Ye, Han Zhong, Nan Jiang, and Tong Zhang. Gibbs\nsampling from human feedback: A provable kl-constrained framework for rlhf. arXiv\npreprint arXiv:2312.11456, 2023.\nJing Xu, Andrew Lee, Sainbayar Sukhbaatar, and Jason Weston. Some things are more\ncringe than others: Preference optimization with the pairwise cringe loss. arXiv preprint\narXiv:2312.16682, 2023.\nHongyi Yuan, Zheng Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang.\nRRHF: Rank responses to align language models with human feedback. In Thirty-seventh\nConference on Neural Information Processing Systems, 2023. URL https://openreview.\nnet/forum?id=EdIGMCHk4l.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can\na machine really finish your sentence? In Anna Korhonen, David R. Traum, and Llu\u00eds\nM\u00e0rquez, editors, Proceedings of the 57th Conference of the Association for Computational\nLinguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers,\npages 4791\u20134800. Association for Computational Linguistics, 2019. doi: 10.18653/V1/\nP19-1472. URL https://doi.org/10.18653/v1/p19-1472.\n16\nYao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J\nLiu. SLiC-HF: Sequence likelihood calibration with human feedback. arXiv preprint\narXiv:2305.10425, 2023.\nChujie Zheng, Pei Ke, Zheng Zhang, and Minlie Huang. Click: Controllable text generation\nwith sequence likelihood contrastive learning. In Anna Rogers, Jordan Boyd-Graber, and\nNaoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL\n2023, pages 1022\u20131040, Toronto, Canada, July 2023a. Association for Computational\nLinguistics. doi: 10.18653/v1/2023.findings-acl.65. URL https://aclanthology.org/\n2023.findings-acl.65.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao\nZhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and\nIon Stoica. Judging LLM-as-a-judge with MT-bench and chatbot arena. In Thirty-seventh\nConference on Neural Information Processing Systems Datasets and Benchmarks Track,\n2023b. URL https://openreview.net/forum?id=uccHPGDlao.\nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei,\nPaul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences.\narXiv preprint arXiv:1909.08593, 2019.\n17\nA\nAppendix\nA.1\nDistributions of IFT, EFT and AIFT data\n100\n50\n0\n50\nDimension 1\n75\n50\n25\n0\n25\n50\n75\nDimension 2\nIFT data\nEFT data\nAIFT data\n(a) Instruction distribution of IFT, EFT and\nAIFT data.\n50\n0\n50\nDimension 1\n80\n60\n40\n20\n0\n20\n40\n60\nDimension 2\nIFT data\nEFT data\nAIFT data\n(b) Response distribution of IFT, EFT, and AIFT\ndata.\nFigure 6: Distributions of both instructions and responses for IFT, EFT and AIFT data.\nWe have plotted the distribution of instructions for IFT, EFT and AIFT(M1) data, and the\ndistribution of responses for IFT, EFT and AIFT(M1) data in Figure 6. It is clear that the\nIFT data and EFT data come from very different distributions while the IFT and AIFT(M1)\ndata come from similar distributions.\nA.2\nEFT Prompts\nThe EFT prompt which we use in our main experiments is shown in Figure 2.\nOther EFT prompts we have tried\nAt first, we took the EFT prompt from Li et al.\n[2024] as shown in Figure 7. However, we found that this prompt was not as effective\nas our additive score-counting prompt because the model needed to treat the task as a\nmultiple-choice problem, and it was difficult for the model to break down this multiple-choice\nproblem into sub-problems involving evaluating various aspects of the response. When using\nthe model trained on 3,200 IFT data only, its performance on the EFT test set using our\nadditive score-counting prompt and prompt from Li et al. [2024] is shown in Table 5.\nEFT Prompt\nMultiple Choice prompt\nOurs\nPairwise accuracy (\u2191)\n26.6%\n65.1%\n5-best % (\u2191)\n23.5%\n39.6%\nExact Match % (\u2191)\n1.1%\n10.1%\nSpearman corr. (\u2191)\n-0.18\n0.25\nKendall \u03c4 corr. (\u2191)\n-0.16\n0.23\nTable 5: We tried various LLM-as-Judge prompts using the model trained with 3,200 IFT\ndata only and found that our additive score-counting prompt worked best which demonstrates\nsignificant improvements in EFT performance comparing to the prompt used by Li et al.\n[2024].\nA.3\nSelf-rewarding Models Using IFT Data Only\nTo demonstrate the importance of the EFT data, we also trained a series of models starting\nwith the model trained only on the IFT data. The following is the model sequence.\nM0 : Base pretrained LLM with no fine-tuning.\n18\nBelow is a question from an user and a candidate response.\nPlease grade the\nresponse on a 5-point scale using the following criteria:\n1:\nIt means the answer is incomplete, vague, off-topic, controversial, or not\nexactly what the user asked for. For example, some content seems missing, numbered\nlist does not start from the beginning, the opening sentence repeats user\u2019s question.\nOr the response is from another person\u2019s perspective with their personal experience\n(e.g. taken from blog posts), or looks like an answer from a forum. Or it contains\npromotional text, navigation text, or other irrelevant information.\n2: It means the answer addresses most of the asks from the user. It does not directly\naddress the user\u2019s question. For example, it only provides a high-level methodology\ninstead of the exact solution to user\u2019s question.\n3: It means the answer is helpful but not written by an AI Assistant. It addresses\nall the basic asks from the user. It is complete and self contained with the drawback\nthat the response is not written from an AI assistant\u2019s perspective, but from other\npeople\u2019s perspective. The content looks like an excerpt from a blog post, web page,\nor web search results. For example, it contains personal experience or opinion,\nmentions comments section, or share on social media, etc.\n4: It means the answer is written from an AI assistant\u2019s perspective with a clear\nfocus of addressing the instruction. It provide a complete, clear, and comprehensive\nresponse to user\u2019s question or instruction without missing or irrelevant information.\nIt is well organized, self-contained, and written in a helpful tone. It has minor room\nfor improvement, e.g. more concise and focused.\n5: It means it is a perfect answer from an AI Assistant. It has a clear focus on\nbeing a helpful AI Assistant, where the response looks like intentionally written to\naddress the user\u2019s question or instruction without any irrelevant sentences. The\nanswer provides high quality content, demonstrating expert knowledge in the area,\nis very well written, logical, easy-to-follow, engaging and insightful.\nUser: <INSTRUCTION_HERE>\n<response><RESPONSE_HERE></response>\nPlease first briefly describe your reasoning (in less than 100 words), and\nthen write \u201cScore: <rating>\u201d in the last line. Answer in the style of an AI Assistant,\nwith knowledge from web search if needed. To derive the final score based on the\ncriteria, let\u2019s think step-by-step.\nFigure 7: LLM-as-a-Judge prompt taken from Li et al. [2024].\nM \u2032\n1 : Initialized with M0, then fine-tuned on the IFT seed data only using SFT.\nM \u2032\n2 : Initialized with M \u2032\n1, then trained with AIFT(M \u2032\n1) data using DPO.\nM \u2032\n3 : Initialized with M \u2032\n2, then trained with AIFT(M \u2032\n2) data using DPO.\nSince we did not use EFT data to train the series of models, they were not always able\nto score the responses according to the format and even when they did, the scores given\ntypically converged to 4. Therefore, even when starting from the same number of generated\nnew prompts, we could only collect a very small number of valid training samples for DPO.\nIn total, we collected 541 pairs to form the AIFT(M \u2032\n1) dataset used to train M \u2032\n2 via DPO,\nand 429 pairs to form AIFT(M \u2032\n2) used to train M \u2032\n3. The win rates are shown in Figure 8.\nFrom the figure we can conclude that EFT data helps to get better performance in the same\nnumber of iterations and the gap in performance between the model trained with EFT data\nand the model trained without EFT data widens in the later iterations.\n19\nSelf-Rewarding M\u2032\n3\nvs.\nSFT Baseline\nSelf-Rewarding M\u2032\n2\nvs.\nSFT Baseline\n50.4\n46.5\n32.8\n34.8\n16.8\n18.8\nSelf-Rewarding Wins\nTie\nSFT Baseline Wins\nSelf-Rewarding M3\nvs.\nM\u2032\n3\nSelf-Rewarding M2\nvs.\nM\u2032\n2\n38.7\n34.8\n44.5\n36.7\n16.8\n28.5\nLeft Wins (in Left vs. Right)\nTie\nRight Wins\nFigure 8: EFT data helps the self-rewarding loop: We evaluated the series of models\ntrained using self-reward loops starting from the model trained using only IFT data. We\nperformed head-to-head win rates comparisons on the IFT test set. While M \u2032\n2 can improve\nover the SFT baseline and M \u2032\n3 can improve even more over the SFT baseline, they lag far\nbehind the corresponding models (M2, M3) that started from a base model trained using\nboth IFT and EFT data, see Figure 3.\n<LIST ALL ALPACAEVAL INSTRUCTIONS>\nGiven the above list of possible instructions, define a maximum of 20 categories that would\ncover the types of intructions, for example recipes, reasoning tasks, general knowledge etc.\nTry to cover as many of the instructions as possible with the maximum 20 categories, while\nkeeping the categories high-level, simple and easy to understand.\nFigure 9: Prompt used to obtain instruction categories on the AlpacaEval test set.\nInstruction: <INSTRUCTION>\nGiven the above, categorize it into one of the following 20 categories:\n<LIST ALL CATEGORIES>\nSecondly, score the instruction in terms of complexity:\nhow complex you think it\nis to answer from 1-10 (where 10 is a complex question whereby first reasoning or breaking\ndown the question into multiple subquestions for example might help improve the answer).\nThirdly, indicate how long you think the response to the instruction should be, ei-\nther (a) 1 sentence, (b) 1-3 sentences, (c) 1 paragraph, (d) 2 paragraphs, or (e) 3 or more\nparagraphs.\nProvide your final response in the following format:\nCategory: <one of the 20 categories>\nComplexity: <score out of 10>\nLength: <length category>. Do not provide the actual response.\nFigure 10: Prompt for categorizing instructions based on their topics, complexities and\nexpected response lengths.\n20\nTable 6: Breakdown of AlpacaEval test set instructions by instruction category.\nCategory\nNumber\nPercentage\nScience / Technology / Engineering\n134\n16.65%\nProfessional / Business / Marketing\n77\n9.57%\nSocial Interaction / Relationships / Human Behavior\n68\n8.45%\nMiscellaneous / Other\n61\n7.58%\nMathematics / Logical Reasoning\n52\n6.46%\nCooking / Recipes\n48\n5.96%\nSoftware Development / Coding / Algorithms\n44\n5.47%\nTravel / Geography / Exploration\n41\n5.09%\nLiterature / Writing / Communication\n39\n4.84%\nHistory / Social Studies\n38\n4.72%\nEntertainment / Media Analysis\n34\n4.22%\nLanguage Learning / Linguistics\n32\n3.98%\nMusic / Audio / Arts\n30\n3.73%\nDIY Projects / Hobbies\n24\n2.98%\nTechnology / Gadgets / Consumer Products\n20\n2.48%\nGaming / Game Development\n18\n2.24%\nExercise / Health / Wellness\n16\n1.99%\nPhilosophy / Ethics / Ideology\n15\n1.86%\nSports / Athletics / Physical Activity\n12\n1.49%\nStrategy / Problem-Solving / Critical Thinking\n2\n0.24%\nTable 7:\nBreakdown of AlpacaEval test set instructions by instruction complexity. The\ninstructions increase in complexity from 1 to 9, where 10 is a complex question that requires\nfirst reasoning or breaking the problem into sub-problems before it can be solved.\nComplexity\nNumber\nPercentage\n3\n238\n29.57%\n2\n206\n25.59%\n4\n122\n15.16%\n6\n79\n9.81%\n5\n68\n8.45%\n7\n41\n5.09%\n1\n34\n4.22%\n8\n14\n1.74%\n9\n3\n0.37%\n1\n2\n3\n4\n5\n6\n7\n8\nInstruction Complexity\n5\n10\n15\n20\n25\nWin rate (%)\nM0\nM1\nM2\nM3\n1 sentence\n1-3 sentences\n1 paragraph\n2 paragraphs\nExpected response length\n5\n10\n15\n20\n25\nWin rate (%)\nM0\nM1\nM2\nM3\nFigure 11: AlpacaEval win rate breakdown for instruction complexities (left) and expected\nresponse lengths (right). Self-Rewarding models give gains across most complexities and all\nresponse length ranges.\n21\nTable 8: Breakdown of AlpacaEval test set instructions by expected response length.\nExpected Length\nNumber\nPercentage\n1-3 sentences\n361\n44.84%\n1 paragraph\n269\n33.42%\n1 sentence\n143\n17.76%\n2 paragraphs\n31\n3.85%\n3 or more paragraphs\n1\n0.13%\nA.4\nPreference optimization outperforms augmenting with positive examples\nonly\nWe also tried an alternative self-training procedure of adding high-quality self-instruction\ncreation examples to supervised fine-tuning (without preference optimization), rather than\nDPO. In this variant, we add additional examples of (instruction prompt, response) curated\nby the model to the seed set for supervised fine-tuning, following other approaches [Li et al.,\n2024, Adolphs et al., 2023, Gulcehre et al., 2023], rather than constructing preference data.\nIn this setup we only add examples where the candidate response was evaluated to give a\nperfect score of rn\ni = 5. Unfortunately we could not find a configuration where this approach\nhelped. For example, adding 11,254 such examples that scored 5 out of 5, and optimizing\nthe mixing weight in training, still yielded a head to head with the SFT Baseline of 29%\nwins vs 30% wins, i.e., no improvement.\nA.5\nAugmented Prompt Generation Using Newly Trained Models\nIn our experiments, for time efficiency, we have created a fixed pool of augmented prompts\nin advance using ChatLlama 70B. In a real interactive system, ideally, those prompts could\ncome from real users so that we can ensure the models are trained to align with real user\nrequirements. Here, we also examine whether our newly trained Self-Rewarding models\nin each iteration can generate new prompts through in-context learning, instead of using\nChatLlama 70B. To check this, we constructed 30 prompts with in-context examples using\nthe original seed IFT data as described in Section 2.2 and tested whether M1, M2 and M3\nstill possess in-context learning ability and can generate high quality instructions. According\nto manual inspection, all models can generate novel instructions given in-context examples\nin all 30 cases.\nHowever, for M2 and M3, the model is likely to first generate a few\ninstructions, then generate a separator, and then start responding to the instructions, so\nsome postprocessing might be necessary.\nA.6\nAlpacaEval Test Sample Clustering\nWe used the GPT-4 (gpt-4-1106-preview) model to categorize the instructions in the\nAlpacaEval test set into clusters from three perspectives: (1) instruction category, (2)\ninstruction complexity, and (3) expected response length. To obtain instruction categories\nfor the AlpaceEval test set, we used the prompt in Figure 9 and obtained 20 categories in\ntotal. Then, to cluster the instructions into different groups, we use the prompt in Figure 10\nfor each test example. The corresponding statistics are given in Table 6, Table 7, Table 8.\nThe fine-grained results on instruction complexity and expected response length are given in\nFigure 11.\n22\nTable 9: NLP Benchmarks. Self-Rewarding models mostly tend to maintain performance\ncompared to the Llama 2 base model and the SFT Baseline, despite being fine-tuned on very\ndifferent instruction-following prompts.\nCommonsense Reasoning\nMath\nWorld Knowledge\nARC\neasy\nARC\nchallenge\nHellaSwag\nSIQA\nPIQA\nGSM8K\n(em)\nMMLU\n(macro_avg/acc)\nOBQA\n(acc_comp)\nNQ\n(em)\nLlama 2\n80.20\n57.40\n85.30\n50.70\n82.80\n56.80\n68.90\n60.20\n25.30\nSFT Baseline\n76.49\n55.97\n85.17\n51.48\n82.59\n50.72\n69.76\n57.80\n34.35\nM1\n78.14\n57.51\n84.99\n53.02\n82.92\n60.27\n69.34\n57.60\n35.48\nM2\n74.84\n54.51\n84.27\n51.23\n81.94\n59.29\n69.31\n57.60\n33.07\nM3\n72.35\n53.13\n83.29\n49.28\n80.79\n57.70\n69.37\n58.40\n31.86\nTable 10: MT-Bench Fine-grained Results. We list our models\u2019 performance on each\nproblem category. Self-reward is especially effective in improving the model\u2019s ability in\nwriting, role-playing, extraction, and STEM tasks.\nWriting\nRoleplay\nReasoning\nMath\nCoding\nExtraction\nSTEM\nHumanities\nOverall\nSFT\n8.83\n8.15\n5.30\n3.00\n3.50\n6.90\n9.18\n9.95\n6.85\nM1\n9.10\n7.65\n4.35\n3.05\n4.10\n7.20\n8.93\n9.85\n6.78\nM2\n9.10\n8.00\n4.60\n3.30\n4.25\n7.65\n9.40\n9.80\n7.01\nM3\n9.58\n8.73\n4.80\n3.50\n4.20\n7.80\n9.45\n9.95\n7.25\nA.7\nNLP Benchmark Results and MT-Bench Results\nWe provide the detailed model performance on a number of NLP benchmarks in Table 9 and\non MT-Bench in Table 10. In particular, some NLP benchmarks including ARC-Challenge,\nHellaSwag, SIQA, PIQA, and OBQA are all text completion tasks. In these tasks, given the\nmultiple choice options, we choose the option corresponding to the highest log probability\nscored by the models as the final answer. As such, the objective of these particular tasks is\nquite different from what our algorithm tries to optimize, so the results on these tasks may\nnot reflect the true capability of our models.\n23\n"
  },
  {
    "title": "VMamba: Visual State Space Model",
    "link": "https://arxiv.org/pdf/2401.10166.pdf",
    "upvote": "35",
    "text": "VMamba: Visual State Space Model\nYue Liu\nUCAS\nliuyue171@mails.ucas.ac.cn\nYunjie Tian\nUCAS\ntianyunjie19@mails.ucas.ac.cn\nYuzhong Zhao\nUCAS\nzhaoyuzhong20@mails.ucas.ac.cn\nHongtian Yu\nUCAS\nyuhongtian17@mails.ucas.ac.cn\nLingxi Xie\nHuawei Inc.\n198808xc@gmail.com\nYaowei Wang\nPengcheng Lab.\nwangyw@pcl.ac.cn\nQixiang Ye\nUCAS\nqxye@ucas.ac.cn\nYunfan Liu\nUCAS\nyunfan.liu@cripac.ia.ac.cn\nAbstract\nConvolutional Neural Networks (CNNs) and Vision Transformers (ViTs) stand as\nthe two most popular foundation models for visual representation learning. While\nCNNs exhibit remarkable scalability with linear complexity w.r.t. image resolu-\ntion, ViTs surpass them in fitting capabilities despite contending with quadratic\ncomplexity. A closer inspection reveals that ViTs achieve superior visual modeling\nperformance through the incorporation of global receptive fields and dynamic\nweights. This observation motivates us to propose a novel architecture that inherits\nthese components while enhancing computational efficiency. To this end, we draw\ninspiration from the recently introduced state space model and propose the Visual\nState Space Model (VMamba), which achieves linear complexity without sacrific-\ning global receptive fields. To address the encountered direction-sensitive issue, we\nintroduce the Cross-Scan Module (CSM) to traverse the spatial domain and convert\nany non-causal visual image into order patch sequences. Extensive experimental\nresults substantiate that VMamba not only demonstrates promising capabilities\nacross various visual perception tasks, but also exhibits more pronounced advan-\ntages over established benchmarks as the image resolution increases. Source code\nhas been available at https://github.com/MzeroMiko/VMamba.\n1\nIntroduction\nVisual representation learning is one of the most fundamental research topics in computer vision,\nwhich has experienced significant breakthroughs since the onset of the deep learning era. Two primary\ncategories of deep foundation models, i.e., Convolution Neural Networks (CNNs) [38, 19, 22, 29, 42]\nand Vision Transformers (ViTs) [10, 28, 45, 56], have been extensively employed in a variety of visual\ntasks. While both have achieved remarkable success in computing expressive visual representations,\nViTs generally exhibit superior performance compared to CNNs, which could be attributed to global\nreceptive fields and dynamic weights facilitated by the attention mechanism.\nPreprint. Under review.\narXiv:2401.10166v1  [cs.CV]  18 Jan 2024\nHowever, the attention mechanism requires quadratic complexity in terms of image sizes, resulting\nin expensive computational overhead when addressing downstream dense prediction tasks, such as\nobject detection, semantic segmentation, etc. To tackle this issue, substantial effort has been dedicated\nto improving the efficiency of attention by constraining the size or stride of computing windows [43],\nalbeit at the cost of imposing restrictions on the scale of receptive fields. This motivates us to design a\nnovel visual foundation model with linear complexity, while still preserving the advantages associated\nwith global receptive fields and dynamic weights.\nDrawing inspiration from the recently proposed state space model [12, 34, 47], we introduce the\nVisual State Space Model (denoted as VMamba) for efficient visual representation learning. The\npivotal concept behind VMamba\u2019s success in effectively reducing attention complexity is inherited\nfrom the Selective Scan Space State Sequential Model (S6) [12], originally devised to address Natural\nLanguage Processing (NLP) tasks. In contrast to the conventional attention computation approach,\nS6 enables each element in a 1-D array (e.g., text sequence) to interact with any of the previously\nscanned samples through a compressed hidden state, effectively reducing the quadratic complexity to\nlinear.\nHowever, due to the non-causal nature of visual data, directly applying such a strategy to a patchified\nand flattened image would inevitably result in restricted receptive fields, as the relationships against\nunscanned patches could not be estimated. We term this issue as the \u2018direction-sensitive\u2019 problem\nand propose to address it through the newly introduced Cross-Scan Module (CSM). Instead of\ntraversing the spatial domain of image feature maps in a unidirectional pattern (either column-wise or\nrow-wise), CSM adopts a four-way scanning strategy, i.e., from four corners all across the feature\nmap to the opposite location (see Figure 2 (b)). This strategy ensures that each element in a feature\nmap integrates information from all other locations in different directions, which renders a global\nreceptive field without increasing the linear computational complexity.\nExtensive experiments on diverse visual tasks are conducted to verify the effectiveness of VMamba.\nAs shown in Figure 1, VMamba models show superior or at least competitive performance on\nImageNet-1K in comparison with benchmark vision models including Resnet [19], ViT [10], and\nSwin [28] 1. We also report the results on downstream dense prediction tasks. For example, VMamba-\nTiny/Small/Base (with 22/44/75 M parameters respectively) achieves 46.5%/48.2%/48.5% mAP\non COCO using the MaskRCNN detector (1\u00d7 training schedule) and 47.3%/49.5%/50.0% mIoU\non ADE20K using UperNet with 512 \u00d7 512 inputs, demonstrating its potential to serve as a powerful\nfoundation model. Furthermore, when larger images are used as input, the FLOPs of ViT increase\nsignificantly faster than those of CNN models, despite usually still exhibiting superior performance.\nHowever, it is intriguing that VMamba, being essentially a foundation model based on the Transformer\narchitecture, is able to attain performance comparable to ViT with a steady increase in FLOPs.\nWe summarize the contributions below:\n\u2022 We propose VMamba, a visual state space model with global receptive fields and dynamic\nweights for visual representation learning. VMamba presents a novel option for vision\nfoundation models, extending beyond the existing choices of CNNs and ViTs.\n\u2022 The Cross-Scan Module (CSM) is introduced to bridge the gap between 1-D array scanning\nand 2-D plain traversing, facilitating the extension of S6 to visual data without compromising\nthe field of reception.\n\u2022 Without bells and whistles, we show that VMamba achieves promising results across various\nvisual tasks including image classification, object detection, and semantic segmentation.\nThese findings underscore the potential of VMamba to serve as a robust vision foundation\nmodel.\n2\nRelated Work\nDeep neural networks have substantially advanced the research in machine visual perception. There\nare primarily two prevalent types of visual foundation models, i.e., CNNs [23, 38, 41, 19, 42] and\nViTs [10, 28, 48, 9, 6, 56]. Recently, the success of State Space Models (SSMs) [12, 34, 47] has\n1We encounter a bug during the training of VMamba-B, and we will update the latest result as soon as\npossible\n2\nParams (Millions)\nImageNet-1K top-1 acc.\nVMamba\nSwin Transformer\nRegNetY\nDeiT\nResNet\nLinear\ncomplexity\nGlobal\nERF\nDynamic\nweights\nResNet\n\u221a\n\u00d7\n\u00d7\nDeiT\n\u00d7\n\u221a\n\u221a\n\u00d7\n\u00d7\n\u221a\nVMamba\n\u221a\n\u221a\n\u221a\nModel\nAttr.\nSwin Transformer\nFigure 1: Performance comparison on ImageNet-1K. VMamba series achieves superior top-1\naccuracy compared to popular counterparts. We note that the proposed VMamba has the capability of\nshowing global effective reception field (ERF), dynamic weights with linear complexity.\nillustrated their efficacy in efficient long sequence modeling, which has attracted extensive attention\nin both the NLP and CV communities. Our study sticks to this line of work and proposes VMamba,\na SSM-based architecture for data modeling in the vision domain. VMamba contributes as an\nalternative foundation model to the community, alongside CNNs and ViTs.\nConvolution Neural Networks (CNNs) serve as the landmark models in the history of visual\nperception. Early CNN-based models [25, 23] are designed for basic tasks, such as recognizing\nhandwritten digits [24] and classifying character categories [55]. The distinctive characteristics\nof CNNs are encapsulated in the convolution kernels, which employ receptive fields to capture\nvisual information of interest from images. With the aid of powerful computing devices (GPU) and\nlarge-scale datasets [7], increasingly deeper [38, 41, 19, 22] and efficient models [20, 42, 52, 36]\nhave been proposed to enhance performance across a spectrum of visual tasks. In addition to these\nefforts, progress has been made to propose more advanced convolution operators [4, 21, 53, 5] or\nmore efficient network architectures [59, 3, 51, 20].\nVision Transformers (ViTs) are adapted from the NLP community, showcasing a potent perception\nmodel for visual tasks and swiftly evolving into one of the most promising visual foundation\nmodels. Early ViT-based models usually require large-scale datasest [10] and appear in a plain\nconfiguration [54, 58, 1, 31]. Later, DeiT [45] employs training techniques to address challenges\nencountered in the optimization process, and subsequent studies tend to incorporate inductive bias of\nvisual perception into network design. For example, the community proposes hierarchical ViTs [28,\n9, 48, 31, 56, 44, 6, 8, 57] to gradually decrease the feature resolution throughout the backbone.\nMoreover, other studies propose to harness the advantages of CNNs, such as introducing convolution\noperations [49, 6, 46], designing hybrid architectures by combining CNN and ViT modules [6, 40, 31],\netc.\nState Space Models (SSMs) are recently proposed models that are introduced into deep learning as\nstate space transforming [16, 15, 39]. Inspired by continuous state space models in control systems,\ncombined with HiPPO [13] initialization, LSSL [16] showcases the potential in handling long range\ndependency problems. However, due to the prohibitive computation and memory requirements\ninduced by the state representation, LSSL is infeasible to use in practice. To solve this problem,\nS4 [15] proposes to normalize the parameter into diagonal structure. Since then, many flavors of\nstructured state space models sprang up with different structures like complex-diagonal structure\n[17, 14], multiple-input multiple output supporting [39], decomposition of diagonal plus low-rank\noperations [18], selection mechanism [12]. These models are then integrated into large representation\nmodels [34, 33, 11].\nThose models are mainly focuses on the how state space models are applied on long-range and casual\ndata like language and speech, such as language understanding [33, 34], content-based reasoning [12],\n3\n+\nAttention\n! \"!  complexity\nCross-Scan\n! \"  complexity\n(a)\n(b)\nFigure 2: Comparison of information flow: Attention vs. Cross-Scan Module (CSM). (a)\nThe attention mechanism uniformly integrates all pixels for the center pixel, resulting in O(N 2)\ncomplexity. (b) CSM integrates pixels from top-left, bottom-right, top-right, and bottom-left with\nO(N) complexity.\npixel-level 1-D image classification [15], few of them pay attention in visual recognition. The most\nsimilar work to ours is S4ND [35]. S4ND is the first work applying state space mechanism into\nvisual tasks and showing the potential that its performance may compete with ViT [10]. However,\nS4ND expands the S4 model in a simple manner, fails on efficiently capturing image information\nin an input-dependent manner. We demonstrates that with selective scan mechanism introduced by\nmamba [12], the proposed VMamba is able to match existing popular vision foundation models like\nResNet [19], ViT [10], swin [27], and convnext [29], showcasing the potential of VMamba to be the\npowerful foundation model.\n3\nMethod\nIn this section, we start by introducing the preliminary concepts related to VMamba, including the\nstate space models, the discretization process, and the selective scan mechanism. We then provide\ndetailed specifications of the 2D state space model which serves as the core element of VMamba.\nFinally, we present a comprehensive discussion of the overall VMamba architecture.\n3.1\nPreliminaries\nState Space Models. State Space Models (SSMs) are commonly considered as linear time-invariant\nsystems that map stimulation x(t) \u2208 RL to response y(t) \u2208 RL. Mathematically, these models are\ntypically formulated as linear ordinary differential equations (ODEs) (Eq. 1), where the parameters\ninclude A \u2208 CN\u00d7N, B, C \u2208 CN for a state size N, and the skip connection D \u2208 C1.\nh\u2032(t) = Ah(t) + Bx(t)\ny(t) = Ch(t) + Dx(t)\n(1)\nDiscretization. State Space Models (SSMs), as continuous-time models, face great challenges\nwhen integrated into deep learning algorithms. To overcome this obstacle, the discretization process\nbecomes imperative.\nThe primary objective of discretization is to transform the ODE into a discrete function. This\ntransformation is crucial to align the model with the sample rate of the underlying signal embodied in\nthe input data, enabling computationally efficient operations [16]. Considering the input xk \u2208 RL\u00d7D,\n4\nscan expand\nscan expand\nscan merge\nSS2D: 2D-Selective-scan-SSM\n!\n\"\n!, #\n\u0394\n%, &\nLinear\nLinear\nEmbedding\n\u0305%, (! = exp \u0394% , \u0394!\n\u210e! =\n\u0305%\u210e!\"# + (! 0!\n1! = #\u210e! + &0!\n1 = [1#, 1$, \u2026 1%]\nS6 block\n!\n\"\n!, #\n\u0394\n%, &\n\u0305%, (! = exp \u0394% , \u0394!\n\u210e! =\n\u0305%\u210e!\"# + (! 0!\n1! = #\u210e! + &0!\n1 = [1#, 1$, \u2026 1%]\n\u2026\n\u2026\n\u2026\n\u2026\nS6 block\n\u2026\n\u2026\n\u2026\n\u2026\nLinear\nLinear\nEmbedding\nFigure 3: Illustration of the 2D-Selective-Scan on an image. We commence by scanning an image\nusing CSM (scan expand). The four resulting features are then individually processed through the S6\nblock, and the four output features are merged (scan merge) to construct the final 2D feature map.\na sampled vector within the signal flow of length L following [17], the ODE (Eq. 1) could be\ndiscretized as follows using the zeroth-order hold rule:\nhk = \u00afAhk\u22121 + \u00afBxk,\nyk = \u00afChk + \u00afDxk,\n\u00afA = e\u2206A,\n\u00afB = (e\u2206A \u2212 I)A\u22121B,\n\u00afC = C\n(2)\nwhere B, C \u2208 RD\u00d7N and \u2206 \u2208 RD. In practice, following [12], we refine the approximation of \u00afB\nusing the first-order Taylor series:\n\u00afB = (e\u2206A \u2212 I)A\u22121B \u2248 (\u2206A)(\u2206A)\u22121\u2206B = \u2206B\n(3)\nSelective Scan Mechanism. Diverging from the prevalent approach that predominantly focuses\non linear time-invariant (LTI) SSMs, the proposed VMamba sets itself apart by incorporating the\nselective scan mechanism (S6) [12] as the core SSM operator. In S6, the matrices B \u2208 RB\u00d7L\u00d7N,\nC \u2208 RB\u00d7L\u00d7N, and \u2206 \u2208 RB\u00d7L\u00d7D are derived from the input data x \u2208 RB\u00d7L\u00d7D. This implies that\nS6 is aware of the contextual information embedded in the input, ensuring the dynamism of weights\nwithin this mechanism.\n3.2\n2D Selective Scan\nDespite its distinctive characteristics, S6 causally processes the input data, and thus can only capture\ninformation within the scanned part of the data. This naturally aligns S6 with NLP tasks that involve\ntemporal data but poses significant challenges when adapting to non-causal data such as image,\ngraph, set, etc. A straightforward solution to this problem would be to scan data along two different\ndirections (i.e., forward and backward), allowing them to compensate for the receptive field of each\nother without increasing the computational complexity.\nDespite the non-causal nature, images differ from texts in that they contain 2D spatial information\n(e.g. local texture and global structure). To tackle this problem, S4ND [35] suggests reformulating\nSSM with convolution and straightforwardly expanding the kernel from 1-D to 2-D via outer-product.\nHowever, such modification prevents the weights from being dynamic (i.e., input independent),\nresulting in a loss of the context-based data modeling capability. Therefore, we choose to preserve\ndynamic weights by sticking to the selective scan approach [12], which unfortunately disallows us to\nfollow [35] and integrate convolution operations.\nTo address this problem, we propose the Cross-Scan Module (CSM) as shown in Figure 2. We choose\nto unfold image patches along rows and columns into sequences (scan expand), and then proceed with\nscanning along four different directions: top-left to bottom-right, bottom-right to top-left, top-right to\nbottom-left, and bottom-left to top-right. In this way, any pixel (such as the center pixel in Figure 2)\nintegrates information from all other pixels in different directions. We then reshape each sequence\ninto a single image, and all sequences are merged to from a new one as illustrated in Figure 3 (scan\nmerge).\n5\n!\n32 \u00d7 %\n32 \u00d7&!\nLN\nVSA\n+\ndrop\n!\"\"#$%&' | *2 \n\u00d7\n&\nSS2D\nLN\nProj\n4\n'\n56789:\n!\u00d7%\u00d73\n:;96\n<44\n=>?@A\n4;789 1\n!\n4 \u00d7 %\n4 \u00d748\n\u00d72\nF?GH:76I>JH8\n<44\n=>?@A\n4;789 2\n!\n8 \u00d7 %\n8 \u00d7&\"\n\u00d72\nF?GH:76I>JH8\n<44\n=>?@A\n4;789 3\n\u00d79\n!\n16 \u00d7 %\n16 \u00d7&#\nF?GH:76I>JH8\n<44\n=>?@A\n4;789 4\n\u00d72\n!\n4 \u00d7 %\n4 \u00d7&$\n,-&\u210e/01&02-1\nH\n32 \u00d7 W\n32 \u00d7C!\nH\u00d7W\u00d73\nStem\nVSS\nBlock\nStage 1\n\u00d72\nDown Sampling\nVSS\nBlock\nStage 2\nH\n8 \u00d7 W\n8 \u00d7C\"\n\u00d72\nDown Sampling\nVSS\nBlock\nStage 3\n\u00d79\nH\n16 \u00d7 W\n16 \u00d7C#\nDown Sampling\nVSS\nBlock\nStage 4\n\u00d72\nH\n4 \u00d7 W\n4 \u00d7C$\n(a) Architecture\n(b) VSS Block\nInput Image\nS\nLinear\nDWConv\nSS2D\nLN\nH\n4 \u00d7 W\n4 \u00d7C$\nLN\nLinear\nLinear\nFigure 4: (a) The overall architecture of a VMamba model (VMamba-T); (b) The fundamental\nbuilding block of VMamba, namely the VSS block.\nThe integration of S6 with CSM, referred to as the S6 block, serves as the core element to construct the\nVisual State Space (VSS) block, which constitutes the fundamental building block of VMamba (further\ndetailed in the next subsection). We emphasize that the S6 block inherits the linear complexity of the\nselective scan mechanism while retaining a global receptive field, which aligns with our motivation\nto construct such a vision model.\n3.3\nVMamba Model\n3.3.1\nOverall Architecture\nAn overview of the architecture of VMamba-Tiny is illustrated in Figure 4 (a). VMamba begins the\nprocess by partitioning the input image into patches using a stem module, similar to ViTs, but without\nfurther flattening the patches into a 1-D sequence. This modification preserves the 2D structure of\nimages, resulting in a feature map with dimensions of H\n4 \u00d7 W\n4 \u00d7 C1.\nVMamba then stacks several VSS blocks on the feature map, maintaining the same dimension,\nconstituting \u201cStage 1\u201d. Hierarchical representations in VMamba are built by down-sampling the\nfeature map in \u201cStage 1\u201d through a patch merge operation [27]. Subsequently, more VSS blocks are\ninvolved, resulting in an output resolution of H\n8 \u00d7 W\n8 and forming \u201cStage 2\u201d. This procedure is repeated\nto create \u201cStage 3\u201d and \u201cStage 4\u201d with resolutions of H\n16\u00d7 W\n16 and H\n32\u00d7 W\n32 , respectively. All these stages\ncollectively construct hierarchical representations akin to popular CNN models [19, 22, 41, 29, 42],\nand some ViTs [27, 48, 6, 56]. The resulting architecture can serve as a versatile replacement for\nother vision models in practical applications with similar requirements.\nWe develop VMamba in three distinct scales, i.e., VMamba-Tiny, VMamba-Small, and VMamba-\nBase (referred to as VMamba-T, VMamba-S, and VMamba-B, respectively). Detailed architectural\nspecifications are outlined in Table 1. The FLOPs for all models are assessed using a 224 \u00d7 224 input\nsize. Additional architectures, such as a large-scale model, will be introduced in future updates.\n3.3.2\nVSS Block\nThe structure of VSS block is illustrated in Figure 4 (b). The input undergoes an initial linear\nembedding layer, and the output splits into two information flows. One flow passes through a 3 \u00d7 3\ndepth-wise convolution layer, followed by a Silu activation function [37] before entering the core\nSS2D module. The output of SS2D goes through a layer normalization layer and is then added to\nthe output of the other information flow, which has undergone a Silu activation. This combination\nproduces the final output of the VSS block.\nUnlike vision transformers, we refrain from utilizing position embedding bias in VMamba due to\nits causal nature. Our design diverges from the typical vision transformer structure, which employs\nthe following sequence of operations: Norm \u2192 attention \u2192 Norm \u2192 MLP in a block, and discards\n6\nlayer name\noutput size\nTiny\nSmall\nBase\nstem\n112\u00d7112\nconv 4\u00d74, 96, stride 4\nconv 4\u00d74, 96, stride 4\nconv 4\u00d74, 128, stride 4\nstage 1\n56\u00d756\n\uf8ee\n\uf8ef\uf8f0\nlinear 96 \u2192 2\u00d796\nDWConv 3\u00d73, 2\u00d796\nSS2D, dim 2\u00d796\nlinear 2\u00d796 \u2192 96\n\uf8f9\n\uf8fa\uf8fb\u00d72\n\uf8ee\n\uf8ef\uf8f0\nlinear 96 \u2192 2\u00d796\nDWConv 3\u00d73, 2\u00d796\nSS2D, dim 2\u00d796\nlinear 2\u00d796 \u2192 96\n\uf8f9\n\uf8fa\uf8fb\u00d72\n\uf8ee\n\uf8ef\uf8f0\nlinear 128 \u2192 2\u00d7128\nDWConv 3\u00d73, 2\u00d7128\nSS2D, dim 2\u00d7128\nlinear 2\u00d7128 \u2192 128\n\uf8f9\n\uf8fa\uf8fb\u00d72\npatch merging \u2192 192\npatch merging \u2192 192\npatch merging \u2192 256\nstage 2\n28\u00d728\n\uf8ee\n\uf8ef\uf8f0\nlinear 192 \u2192 2\u00d7192\nDWConv 3\u00d73, 2\u00d7192\nSS2D, dim 2\u00d7192\nlinear 2\u00d7192 \u2192 192\n\uf8f9\n\uf8fa\uf8fb\u00d72\n\uf8ee\n\uf8ef\uf8f0\nlinear 192 \u2192 2\u00d7192\nDWConv 3\u00d73, 2\u00d7192\nSS2D, dim 2\u00d7192\nlinear 2\u00d7192 \u2192 192\n\uf8f9\n\uf8fa\uf8fb\u00d72\n\uf8ee\n\uf8ef\uf8f0\nlinear 256 \u2192 2\u00d7256\nDWConv 3\u00d73, 2\u00d7256\nSS2D, dim 2\u00d7256\nlinear 2\u00d7256 \u2192 256\n\uf8f9\n\uf8fa\uf8fb\u00d72\npatch merging \u2192 384\npatch merging \u2192 384\npatch merging \u2192 512\nstage 3\n14\u00d714\n\uf8ee\n\uf8ef\uf8f0\nlinear 384 \u2192 2\u00d7384\nDWConv 3\u00d73, 2\u00d7384\nSS2D, dim 2\u00d7384\nlinear 2\u00d7384 \u2192 384\n\uf8f9\n\uf8fa\uf8fb\u00d79\n\uf8ee\n\uf8ef\uf8f0\nlinear 384 \u2192 2\u00d7384\nDWConv 3\u00d73, 2\u00d7384\nSS2D, dim 2\u00d7384\nlinear 2\u00d7384 \u2192 384\n\uf8f9\n\uf8fa\uf8fb\u00d727\n\uf8ee\n\uf8ef\uf8f0\nlinear 512 \u2192 2\u00d7512\nDWConv 3\u00d73, 2\u00d7512\nSS2D, dim 2\u00d7512\nlinear 2\u00d7512 \u2192 512\n\uf8f9\n\uf8fa\uf8fb\u00d727\npatch merging \u2192 768\npatch merging \u2192 768\npatch merging \u2192 1024\nstage 4\n7\u00d77\n\uf8ee\n\uf8ef\uf8f0\nlinear 768 \u2192 2\u00d7768\nDWConv 3\u00d73, 2\u00d7768\nSS2D, dim 2\u00d7768\nlinear 2\u00d7768 \u2192 768\n\uf8f9\n\uf8fa\uf8fb\u00d72\n\uf8ee\n\uf8ef\uf8f0\nlinear 768 \u2192 2\u00d7768\nDWConv 3\u00d73, 2\u00d7768\nSS2D, dim 2\u00d7768\nlinear 2\u00d7768 \u2192 768\n\uf8f9\n\uf8fa\uf8fb\u00d72\n\uf8ee\n\uf8ef\uf8f0\nlinear 1024 \u2192 2\u00d71024\nDWConv 3\u00d73, 2\u00d71024\nSS2D, dim 2\u00d71024\nlinear 2\u00d71024 \u2192 1024\n\uf8f9\n\uf8fa\uf8fb\u00d72\n1\u00d71\naverage pool, 1000-d fc, softmax\nParam. (M)\n22\n44\n75\nFLOPs\n4.5\u00d7109\n9.1\u00d7109\n15.2\u00d7109\nTable 1: Architectural overview of the VMamba series. Down-sampling is executed through patch\nmerging operations in stages 1, 2, and 3. The term \u201clinear\u201d refers to a linear layer. The \u201cDWConv\u201d\ndenotes a depth-wise convolution operation. The proposed 2D-selective-scan is labeled as \u201cSS2D\u201d.\nthe MLP operation. Consequently, the VSS block is shallower than the ViT block, which allows us to\nstack more blocks with a similar budget of total model depth.\n4\nExperiment\nIn this section, we perform a series of experiments to assess and compare VMamba against popular\nmodels, including CNNs and vision transformers. Our evaluation spans diverse tasks, including image\nclassification on ImageNet-1K, object detection on COCO, and semantic segmentation on ADE20K.\nSubsequently, we delve into analysis experiments to gain deeper insights into the architecture of\nVMamba.\n4.1\nImage Classification on ImageNet-1K\nSettings We evaluate VMamba\u2019s classification performance on ImageNet-1K [7]. Following the\nconfiguration in [27], VMamba-T/S/B undergo training from scratch for 300 epochs (with the first\n20 epochs to warmup), utilizing a batch size of 1024. The training process incorporates the AdamW\noptimizer with betas set to (0.9, 0.999), a momentum of 0.9, a cosine decay learning rate scheduler,\nan initial learning rate of 1 \u00d7 10\u22123, and a weight decay of 0.05. Additional techniques such as label\nsmoothing (0.1) and exponential moving average (EMA) are also employed. Beyond these, no further\ntraining techniques are applied.\nResults Table 2 summarizes results on ImageNet-1K, comparing VMamba with popular CNN\nmodels and vision transformers. The comparison reveals that, with similar FLOPs, VMamba-T\nachieves a performance of 82.2%, surpassing RegNetY-4G by 2.2%, DeiT-S by 2.4%, and Swin-T\nby 0.9%. Notably, the performance advantages of VMamba persist across small and base scale\nmodels. For instance, at the small scale, VMamba-S attains a top-1 accuracy of 83.5%, outperforming\nRegNetY-8G by 1.8% and Swin-S by 0.5%. Meanwhile, VMamba-B achieves a top-1 accuracy of\n7\nmethod\nimage\nsize\n#param.\nFLOPs\nImageNet\ntop-1 acc.\nRegNetY-4G [36]\n2242\n21M\n4.0G\n80.0\nRegNetY-8G [36]\n2242\n39M\n8.0G\n81.7\nRegNetY-16G [36]\n2242\n84M\n16.0G\n82.9\nEffNet-B3 [42]\n3002\n12M\n1.8G\n81.6\nEffNet-B4 [42]\n3802\n19M\n4.2G\n82.9\nEffNet-B5 [42]\n4562\n30M\n9.9G\n83.6\nEffNet-B6 [42]\n5282\n43M\n19.0G\n84.0\nViT-B/16 [10]\n3842\n86M\n55.4G\n77.9\nViT-L/16 [10]\n3842\n307M\n190.7G\n76.5\nDeiT-S [45]\n2242\n22M\n4.6G\n79.8\nDeiT-B [45]\n2242\n86M\n17.5G\n81.8\nDeiT-B [45]\n3842\n86M\n55.4G\n83.1\nSwin-T [28]\n2242\n29M\n4.5G\n81.3\nSwin-S [28]\n2242\n50M\n8.7G\n83.0\nSwin-B [28]\n2242\n88M\n15.4G\n83.5\nS4ND-ViT-B [35]\n2242\n89M\n-\n80.4\nVMamba-T\n2242\n22M\n4.5G\n82.2\nVMamba-S\n2242\n44M\n9.1G\n83.5\nVMamba-B\n2242\n75M\n15.2G\n83.2\u2020\nTable 2: Accuracy comparison across various models on ImageNet-1K. The symbol \u2020 indicates\nthat a bug is encountered during the training of VMamba-B, and we will update the correct number\nin the near future.\n83.2%, surpassing RegNetY-16G by 0.3% and DeiT-B by 0.1%. These promising results underscore\nVMamba\u2019s potential as a robust foundational model, extending its superiority beyond traditional\nCNN models and vision transformers.\n4.2\nObject Detection on COCO\nSettings In this section, we assess the performance of the proposed VMamba on object detection\nusing the MSCOCO 2017 dataset [26]. Our training framework is built on the mmdetection library [2],\nand we adhere to the hyperparameters in Swin [27] with the Mask-RCNN detector. Specifically, we\nemploy the AdamW optimizer and fine-tune the pre-trained classification models (on ImageNet-1K)\nfor both 12 and 36 epochs. The drop path rates are set to 0.2%/0.2%/0.2% 2 for VMamba-T/S/B,\nrespectively. The learning rate is initialized at 1 \u00d7 10\u22124 and is reduced by a factor of 10\u00d7 at the 9th\nand 11th epochs. We implement multi-scale training and random flip with a batch size of 16. These\nchoices align with established practices for object detection evaluations.\nResults The results for COCO are summarized in Table 3. VMamba maintains superiority in\nbox/mask Average Precision (AP) on COCO, regardless of the training schedule employed (12 or\n36 epochs). Specifically, with a 12-epoch fine-tuning schedule, VMamba-T/S/B models achieve\nobject detection mAPs of 46.5%/48.2%/48.5%, surpassing Swin-T/S/B by 3.8%/3.6%/1.6% mAP,\nand ConvNeXt-T/S/B by 2.3%/2.8%/1.5% mAP. Using the same configuration, VMamba-T/S/B\nachieves instance segmentation mIoUs of 42.1%/43.0%/43.1%, outperforming Swin-T/S/B by\n2.8%/2.1%/0.8% mIoU, and ConvNeXt-T/S/B by 2.0%/1.2%/0.7% mIoU, respectively.\nFurthermore, the advantages of VMamba persist under the 36-epoch fine-tuning schedule with\nmulti-scale training, as indicated in Table 3. When compared to counterparts, including Swin [28],\nConvNeXt [29], PVTv2 [49], and ViT [10] (with Adapters), VMamba-T/S exhibit superior per-\n2All being 0.2 is due to our oversight, and we will update the latest experiments.\n8\nMask R-CNN 1\u00d7 schedule\nBackbone\nAPb\nAPb\n50\nAPb\n75\nAPm\nAPm\n50\nAPm\n75\n#param.\nFLOPs\nResNet-50\n38.2\n58.8\n41.4\n34.7\n55.7\n37.2\n44M\n260G\nSwin-T\n42.7\n65.2\n46.8\n39.3\n62.2\n42.2\n48M\n267G\nConvNeXt-T\n44.2\n66.6\n48.3\n40.1\n63.3\n42.8\n48M\n262G\nPVTv2-B2\n45.3\n67.1\n49.6\n41.2\n64.2\n44.4\n45M\n309G\nViT-Adapter-S\n44.7\n65.8\n48.3\n39.9\n62.5\n42.8\n48M\n403G\nVMamba-T\n46.5\n68.5\n50.7\n42.1\n65.5\n45.3\n42M\n262G\nResNet-101\n38.2\n58.8\n41.4\n34.7\n55.7\n37.2\n63M\n336G\nSwin-S\n44.8\n66.6\n48.9\n40.9\n63.2\n44.2\n69M\n354G\nConvNeXt-S\n45.4\n67.9\n50.0\n41.8\n65.2\n45.1\n70M\n348G\nPVTv2-B3\n47.0\n68.1\n51.7\n42.5\n65.7\n45.7\n65M\n397G\nVMamba-S\n48.2\n69.7\n52.5\n43.0\n66.6\n46.4\n64M\n357G\nSwin-B\n46.9\n-\n-\n42.3\n-\n-\n107M\n496G\nConvNeXt-B\n47.0\n69.4\n51.7\n42.7\n66.3\n46.0\n108M\n486G\nPVTv2-B5\n47.4\n68.6\n51.9\n42.5\n65.7\n46.0\n102M\n557G\nViT-Adapter-B\n47.0\n68.2\n51.4\n41.8\n65.1\n44.9\n102M\n557G\nVMamba-B\n48.5\n69.6\n53.0\n43.1\n67.0\n46.4\n96M\n482G\nMask R-CNN 3\u00d7 MS schedule\nSwin-T\n46.0\n68.1\n50.3\n41.6\n65.1\n44.9\n48M\n267G\nConvNeXt-T\n46.2\n67.9\n50.8\n41.7\n65.0\n44.9\n48M\n262G\nPVTv2-B2\n47.8\n69.7\n52.6\n43.1\n66.8\n46.7\n45M\n309G\nViT-Adapter-S\n48.2\n69.7\n52.5\n42.8\n66.4\n45.9\n48M\n403G\nVMamba-T\n48.5\n69.9\n52.9\n43.2\n66.8\n46.3\n42M\n262G\nSwin-S\n48.2\n69.8\n52.8\n43.2\n67.0\n46.1\n69M\n354G\nConvNeXt-S\n47.9\n70.0\n52.7\n42.9\n66.9\n46.2\n70M\n348G\nPVTv2-B3\n48.4\n69.8\n53.3\n43.2\n66.9\n46.7\n65M\n397G\nVMamba-S\n49.7\n70.4\n54.2\n44.0\n67.6\n47.3\n64M\n357G\nTable 3: Object detection and instance segmentation results on COCO dataset. The FLOPs\nare calculated using inputs of size 1280 \u00d7 800. Here, AP b and AP m denote box AP and mask AP,\nrespectively. \"1\u00d7\" indicates models fine-tuned for 12 epochs, while \"3\u00d7MS\" signifies the utilization\nof multi-scale training for 36 epochs.\nformance, achieving 48.5%/49.7% mAP on object detection and 43.2%/44.0% mIoU on instance\nsegmentation. These results underscore the potential of VMamba in downstream dense prediction\ntasks.\n4.3\nSemantic Segmentation on ADE20K\nSettings Following Swin [28], we construct a UperHead [50] on top of the pre-trained model.\nEmploying the AdamW optimizer [30], we set the learning rate as 6 \u00d7 10\u22125. The fine-tuning process\nspans a total of 160k iterations with a batch size of 16. The default input resolution is 512 \u00d7 512, and\nwe additionally present experimental results using 640 \u00d7 640 inputs and multi-scale (MS) testing.\nResults The results are presented in Table 4. Once again, VMamba exhibits superior accuracy,\nparticularly with the VMamba-T model achieving 47.3% mIoU with a resolution of 512 \u00d7 512\nand 48.3% mIoU using multi-scale (MS) input. These scores surpass all competitors, including\nResNet [19], DeiT [45], Swin [28], and ConvNeXt [29]. Notably, the advantages extend to VMamba-\nS/B models, even when using 640 \u00d7 640 inputs.\n9\nmethod\ncrop size\nmIoU (SS)\nmIoU (MS)\n#param.\nFLOPs\nResNet-50\n5122\n42.1\n42.8\n67M\n953G\nDeiT-S + MLN\n5122\n43.8\n45.1\n58M\n1217G\nSwin-T\n5122\n44.4\n45.8\n60M\n945G\nConvNeXt-T\n5122\n46.0\n46.7\n60M\n939G\nVMamba-T\n5122\n47.3\n48.3\n55M\n939G\nResNet-101\n5122\n42.9\n44.0\n85M\n1030G\nDeiT-B + MLN\n5122\n45.5\n47.2\n144M\n2007G\nSwin-S\n5122\n47.6\n49.5\n81M\n1039G\nConvNeXt-S\n5122\n48.7\n49.6\n82M\n1027G\nVMamba-S\n5122\n49.5\n50.5\n76M\n1037G\nSwin-B\n5122\n48.1\n49.7\n121M\n1188G\nConvNeXt-B\n5122\n49.1\n49.9\n122M\n1170G\nVMamba-B\n5122\n50.0\n51.3\n110M\n1167G\nSwin-S\n6402\n47.9\n48.8\n81M\n1614G\nConvNeXt-S\n6402\n48.8\n48.9\n82M\n1607G\nVMamba-S\n6402\n50.8\n50.8\n76M\n1620G\nTable 4: Semantic segmentation results on ADE20K using UperNet [50]. We evaluate the\nperformance of semantic segmentation on the ADE20K dataset with UperNet [50]. The FLOPs are\ncalculated with input sizes of 512 \u00d7 2048 or 640 \u00d7 2560 based on the crop size. \"SS\" and \"MS\"\ndenote single-scale and multi-scale testing, respectively.\nResNet50\nConvNeXt-T\nSwin-T\nDeiT-S\nVMamba-T\nBefore training\nAfter training\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\nFigure 5: The Effective Receptive Field (ERF) is visualized for ResNet50 [19], ConvNeXt-T [29],\nSwin-T [28], DeiT-S [45] (ViT), and the proposed VMamba-T. A larger ERF is indicated by a more\nextensively distributed dark area. Only DeiT [45] and the proposed VMamba exhibit a global\nERF. The inspiration for this visualization is drawn from [32].\n4.4\nAnalysis Experiments\nEffective Receptive Field To assess the effective receptive fields (ERFs) [32] across various models,\nwe present a comparative analysis in Figure 5. The ERT measures the significance of model input\nconcerning its output. Visualizing the ERF of the central pixel with an input size of 1024 \u00d7 1024, we\ncompare VMamba with four prominent visual foundation models: ResNet50 [19], ConvNeXt-T [29],\nSwin-T [28], and DeiT-S [45] (ViT) at both the Before training and After training stages.\nKey observations from Figure 5 include: 1) Only DeiT (ViT) and VMamba exhibit global ERFs, while\nother models demonstrate local ERFs, despite their theoretical global potential. It\u2019s important to note\nthat the DeiT (ViT) model incurs quadratic complexity costs (refer to Figure 6). 2). In contrast to DeiT\n10\nFLOPs\nImageNet-1K top-1 acc.\nImage resolution\n(a)\nImage resolution\n(b)\nFigure 6: A comparison of input scaling evaluation for popular models trained with 224 \u00d7 224\ninputs. We assess performance (a) and FLOPs (b) of various popular models trained with 224 \u00d7 224\ninputs across different input sizes, ranging from 64 \u00d7 64 to 1024 \u00d7 1024.\n(ViT), which evenly activates all pixels using the attention mechanism, VMamba activates all pixels\nand notably emphasizes cross-shaped activations. The Cross-Scan Module\u2019s scanning mechanism\nensures the central pixel is most influenced by pixels along the cross, prioritizing long-dependency\ncontext over local information for each pixel. 3) Intriguingly, VMamba initially exhibits only a local\nERF at Before training. However, After training transforms the ERF to global, signifying\nan adaptive process in the model\u2019s global capability. We believe this adaptive process contributes to\nthe model\u2019s enhanced perception of images. This stands in contrast to DeiT, which maintains nearly\nidentical ERFs at both Before training and After training.\nInput Scaling We proceed to perform experiments on input scaling, measuring top-1 accuracy\non ImageNet-1K and FLOPs, as illustrated in Figure 6. In Figure 6 (a), we assess the inference\nperformance of popular models (trained with a 224\u00d7224 input size) across various image resolutions\n(ranging from 64 \u00d7 64 to 1024 \u00d7 1024). In comparison to counterparts, VMamba demonstrates\nthe most stable performance across different input image sizes. Notably, as the input size increases\nfrom 224 \u00d7 224 to 384 \u00d7 384, only VMamba exhibits an upward trend in performance (VMamba-S\nachieving 84%), highlighting its robustness to changes in input image size. In Figure 6 (b), we\nevaluate FLOPs using different image resolutions (also ranging from 64 \u00d7 64 to 1024 \u00d7 1024). As\nanticipated, the VMamba series report a linear growth in complexity, aligning with CNN models.\nVMamba\u2019s complexity is consistent with carefully designed vision transformers like Swin [28].\nHowever, it\u2019s crucial to note that only VMamba achieves a global effective receptive field (ERF).\nDeiT, which also exhibits global ERF capability, experiences a quadratic growth in complexity.\n5\nConclusion\nConvolutional Neural Networks (CNNs) and Vision Transformers (ViTs) represent the predominant\nfoundation models for visual representation learning. While CNNs exhibit linear complexity with\nrespect to image resolution, ViTs excel in fitting capabilities despite quadratic complexity. Our\ninvestigation reveals that ViTs achieve superior visual modeling through global receptive fields\nand dynamic weights. Motivated by this, we propose the Visual State Space Model (VMamba),\ndrawing inspiration from the state space model to achieve linear complexity without sacrificing\nglobal receptive fields. To address direction sensitivity, we introduce the Cross-Scan Module (CSM)\nfor spatial traversal, converting non-causal visual images into ordered patch sequences. Extensive\nexperiments demonstrate VMamba\u2019s promising performance across visual tasks, with pronounced\nadvantages as image resolution increases, surpassing established benchmarks.\n11\nReferences\n[1] Chun-Fu Chen, Quanfu Fan, and Rameswar Panda. Crossvit: Cross-attention multi-scale vision transformer\nfor image classification. In IEEE ICCV, 2021.\n[2] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng,\nZiwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, Qijie Zhao, Buyu\nLi, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change\nLoy, and Dahua Lin. Mmdetection: Open mmlab detection toolbox and benchmark. arXiv preprint\narXiv:1906.07155, 2019.\n[3] Yunpeng Chen, Jianan Li, Huaxin Xiao, Xiaojie Jin, Shuicheng Yan, and Jiashi Feng. Dual path networks.\nAdvances in neural information processing systems, 30, 2017.\n[4] Fran\u00e7ois Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages 1251\u20131258, 2017.\n[5] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable\nconvolutional networks. In Proceedings of the IEEE international conference on computer vision, pages\n764\u2013773, 2017.\n[6] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan. Coatnet: Marrying convolution and attention for\nall data sizes. NeurIPS, 34:3965\u20133977, 2021.\n[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. Imagenet: A large-scale hierarchical\nimage database. In IEEE CVPR, pages 248\u2013255, 2009.\n[8] Mingyu Ding, Bin Xiao, Noel Codella, Ping Luo, Jingdong Wang, and Lu Yuan. Davit: Dual attention\nvision transformers. In European Conference on Computer Vision, pages 74\u201392. Springer, 2022.\n[9] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and\nBaining Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows. In\nIEEE CVPR, pages 12124\u201312134, 2022.\n[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and\nNeil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR,\n2021.\n[11] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry\nhungry hippos: Towards language modeling with state space models. In The Eleventh International\nConference on Learning Representations, 2022.\n[12] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint\narXiv:2312.00752, 2023.\n[13] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Hippo: Recurrent memory with\noptimal polynomial projections. Advances in neural information processing systems, 33:1474\u20131487, 2020.\n[14] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of\ndiagonal state space models. Advances in Neural Information Processing Systems, 35:35971\u201335983, 2022.\n[15] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state\nspaces. In International Conference on Learning Representations, 2021.\n[16] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining\nrecurrent, convolutional, and continuous-time models with linear state space layers. Advances in neural\ninformation processing systems, 34:572\u2013585, 2021.\n[17] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state\nspaces. Advances in Neural Information Processing Systems, 35:22982\u201322994, 2022.\n[18] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela\nRus. Liquid structural state-space models. In The Eleventh International Conference on Learning\nRepresentations, 2022.\n[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn IEEE CVPR, pages 770\u2013778, 2016.\n12\n[20] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco\nAndreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision\napplications. arXiv preprint arXiv:1704.04861, 2017.\n[21] Binh-Son Hua, Minh-Khoi Tran, and Sai-Kit Yeung. Pointwise convolutional neural networks. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 984\u2013993, 2018.\n[22] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger.\nDensely connected\nconvolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 4700\u20134708, 2017.\n[23] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional\nneural networks. In NeurIPS, pages 1106\u20131114, 2012.\n[24] Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and\nLawrence D Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation,\n1(4):541\u2013551, 1989.\n[25] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to\ndocument recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.\n[26] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r,\nand C. Lawrence Zitnick. Microsoft COCO: common objects in context. In ECCV, pages 740\u2013755, 2014.\n[27] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang,\nLi Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pages 12009\u201312019, 2022.\n[28] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer: Hierarchical vision transformer using shifted windows. In IEEE ICCV, pages 10012\u201310022,\n2021.\n[29] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A\nconvnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 11976\u201311986, 2022.\n[30] Ilya Loshchilov and Frank Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017.\n[31] Jiasen Lu, Roozbeh Mottaghi, Aniruddha Kembhavi, et al. Container: Context aggregation networks.\nNeurIPS, 34:19160\u201319171, 2021.\n[32] Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard Zemel. Understanding the effective receptive field in\ndeep convolutional neural networks. Advances in neural information processing systems, 29, 2016.\n[33] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May,\nand Luke Zettlemoyer. Mega: Moving average equipped gated attention. In The Eleventh International\nConference on Learning Representations, 2022.\n[34] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via\ngated state spaces. In International Conference on Learning Representations, 2023.\n[35] Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and Christo-\npher R\u00e9. S4nd: Modeling images and videos as multidimensional signals with state spaces. Advances in\nneural information processing systems, 35:2846\u20132861, 2022.\n[36] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Designing network\ndesign spaces. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,\npages 10428\u201310436, 2020.\n[37] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.\n[38] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recogni-\ntion. arXiv preprint arXiv:1409.1556, 2014.\n[39] Jimmy TH Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence\nmodeling. In The Eleventh International Conference on Learning Representations, 2022.\n13\n[40] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani.\nBottleneck transformers for visual recognition. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 16519\u201316529, 2021.\n[41] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru\nErhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition, pages 1\u20139, 2015.\n[42] Mingxing Tan and Quoc V. Le. Efficientnet: Rethinking model scaling for convolutional neural networks.\nIn ICML, pages 6105\u20136114, 2019.\n[43] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. volume 55,\n2022.\n[44] Yunjie Tian, Lingxi Xie, Zhaozhi Wang, Longhui Wei, Xiaopeng Zhang, Jianbin Jiao, Yaowei Wang,\nQi Tian, and Qixiang Ye. Integrally pre-trained transformer pyramid networks. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18610\u201318620, 2023.\n[45] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou.\nTraining data-efficient image transformers & distillation through attention. In ICML, pages 10347\u201310357,\n2021.\n[46] Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake Hechtman, and Jonathon\nShlens. Scaling local self-attention for parameter efficient visual backbones. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12894\u201312904, 2021.\n[47] Jue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu, Mohamed Omar, and Raffay Hamid. Selective\nstructured state-spaces for long-form video understanding. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 6387\u20136397, 2023.\n[48] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and\nLing Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions.\nIn ICCV, pages 568\u2013578, 2021.\n[49] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and\nLing Shao. Pvt v2: Improved baselines with pyramid vision transformer. Computational Visual Media,\n8(3):415\u2013424, 2022.\n[50] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene\nunderstanding. In ECCV, pages 418\u2013434, 2018.\n[51] Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transforma-\ntions for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 1492\u20131500, 2017.\n[52] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. Focal\nself-attention for local-global interactions in vision transformers. arXiv preprint arXiv:2107.00641, 2021.\n[53] Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. arXiv preprint\narXiv:1511.07122, 2015.\n[54] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng,\nand Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In ICCV,\npages 558\u2013567, 2021.\n[55] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification.\nAdvances in neural information processing systems, 28, 2015.\n[56] Xiaosong Zhang, Yunjie Tian, Lingxi Xie, Wei Huang, Qi Dai, Qixiang Ye, and Qi Tian. Hivit: A simpler\nand more efficient design of hierarchical vision transformer. In International Conference on Learning\nRepresentations, 2023.\n[57] Weixi Zhao, Weiqiang Wang, and Yunjie Tian. Graformer: Graph-oriented transformer for 3d pose\nestimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 20438\u201320447, 2022.\n[58] Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Zihang Jiang, Qibin Hou, and Jiashi\nFeng. Deepvit: Towards deeper vision transformer. arXiv preprint arXiv:2103.11886, 2021.\n[59] Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint\narXiv:1611.01578, 2016.\n14\n"
  },
  {
    "title": "DiffusionGPT: LLM-Driven Text-to-Image Generation System",
    "link": "https://arxiv.org/pdf/2401.10061.pdf",
    "upvote": "26",
    "text": "DiffusionGPT\n: LLM-Driven Text-to-Image Generation System\nJie Qin1\u22c6\nJie Wu1\u22c6\nWeifeng Chen1\u22c6\nYuxi Ren1\nHuixia Li1\nHefeng Wu2\nXuefeng Xiao1\nRui Wang1\nShilei Wen1\n1ByteDance Inc 2Sun Yat-Sen University\nProject page: https://DiffusionGPT.github.io\nPrompt\nModel\nText-to-Image\nGenera.on\nSystem\n\ud83d\udde3 : \u2705\n\ud83c\udfa8 : \u274c\n\ud83d\udde3 : \u2705\n\ud83c\udfa8 : \u2705\n\ud83d\udde3 : \u274c\n\ud83c\udfa8 : \u2705\n\ud83d\udde3 : \u274c\n \ud83c\udfa8 : \u274c\n\ud83c\udfa8 : Output - Pick Expert Model\n\ud83d\udde3 : Input - Parse Diverse Prompts\ni.e., Civitai, LibLibAI\ni.e., SD1.5, SDXL\ni.e., DALLE3 \ni.e., Di\ufb00usionGPT\n(Ours) \na wooden toy horse with \na mane made of rope\na stork playing a violin\nDogs playing poker\nIf I could talk to trees, \nthey would share their \nancient wisdom with me\nI want to explore a mystical \nforest \ufb01lled with ancient trees\nGenerate an image of a \nskilled carpenter crafting a \nbeautiful piece of furniture \nwith intricate detail\nCapture the excitement \nand energy of a music \nfestival with crowds \ndancing in sync.\nImagine a world \nwhere cities \ufb02oat in the \nsky, connected by \nintricate bridges.\nIf I had a magic \nwand, I would bring \ntoys to life and create \na magical toyland.\nI want to be a superman \nand soar into the sky\nRed: Prompt-based;    Green: Inspiration-based;      Purple: Instruction-based;     Orange: Hypothesis-based\nFigure 1. We propose a unified generation system DiffusionGPT, which leverages Large Language Models (LLM) to seamlessly accommo-\ndating various types of prompts input and integrating domain-expert models for output. Our system is capable of parsing diverse forms of\ninputs, including Prompt-based, Instruction-based, Inspiration-based, and Hypothesis-based input types. It exhibits the ability to generate\noutputs of superior quality.\nAbstract\nDiffusion models have opened up new avenues for the\nfield of image generation, resulting in the proliferation\nof high-quality models shared on open-source platforms.\nHowever, a major challenge persists in current text-to-\nimage systems are often unable to handle diverse inputs,\nor are limited to single model results. Current unified at-\ntempts often fall into two orthogonal aspects: i) parse Di-\nverse Prompts in input stage; ii) activate expert model to\noutput. To combine the best of both worlds, we propose\nDiffusionGPT, which leverages Large Language Models\n(LLM) to offer a unified generation system capable of seam-\nlessly accommodating various types of prompts and inte-\ngrating domain-expert models.\nDiffusionGPT constructs\ndomain-specific Trees for various generative models based\non prior knowledge.\nWhen provided with an input, the\nLLM parses the prompt and employs the Trees-of-Thought\nto guide the selection of an appropriate model, thereby re-\nlaxing input constraints and ensuring exceptional perfor-\nmance across diverse domains. Moreover, we introduce Ad-\n\u22c6Equal contribution.\nvantage Databases, where the Tree-of-Thought is enriched\nwith human feedback, aligning the model selection process\nwith human preferences.\nThrough extensive experiments\nand comparisons, we demonstrate the effectiveness of Dif-\nfusionGPT, showcasing its potential for pushing the bound-\naries of image synthesis in diverse domains.\n1. Introduction\nRecent years have witnessed the prevalence of diffusion\nmodels[6] in image generation tasks, revolutionizing im-\nage editing, stylization, and other related tasks.\nBoth\nDALLE-2[14] and Imagen[17] are extremely good at gen-\nerating images from text prompts.\nHowever, their non-\nopen source nature has hindered widespread popularization\nand corresponding ecological development. The first open-\nsource text-to-image diffusion model, known as Stable Dif-\nfusion (SD)[16], which has rapidly gained popularity and\nwidespread usage. Various techniques tailored for SD, such\nas Controlnet[27], Lora, further paved the way for the de-\nvelopment of SD and foster its integration into various ap-\nplications. SDXL[10] is the latest image generation model\n1\narXiv:2401.10061v1  [cs.CV]  18 Jan 2024\ntailored to deliver exceptional photorealistic outputs with\nintricate details and artistic composition.\nMoreover, the\nimpact of SD extends beyond technical aspects. Commu-\nnity platforms such as Civitai, WebUI, and LibLibAI have\nemerged as vibrant hubs for discussions and collaborations\namong designers and creators. The capability evolution of\nStable Diffusion, the advancements in SD-customized tech-\nnologies, and the thriving community platforms, has created\nan exciting and fertile ground for further advancements in\nimage generation.\nDespite making significant strides, current stable diffu-\nsion models face two key challenges when applied to real-\nistic scenarios:\n\u2022 Model Limitation:\nWhile stable diffusion (such as\nSD1.5) demonstrate adaptability to a wide range of\nprompts, they exhibit poor performance in specific do-\nmains.\nConversely, domain-specific models (such as\nSD1.5+Lora) excel in producing extreme generation per-\nformance within specific sub-fields but lack versatility.\n\u2022 Prompt Constraint: During the training of stable dif-\nfusion, text information typically consists of descriptive\nstatements, such as captions. However, when utilizing\nthese models, we encounter various prompt types, includ-\ning instructions and inspirations.\nThe current genera-\ntion models struggle to achieve optimal generation per-\nformance for these diverse prompt types.\nThe existing mix-matches between stable diffusion mod-\nels and realistic applications often manifest as limited re-\nsults, poor generalization, and increased difficulty in their\npractical implementation. A series of research works have\nexplored to address these challenges and bridge the gap.\nWhile SDXL has made notable advancements in improv-\ning specific-domain performance, achieving the ultimate\nperformance in this regard remains elusive.\nOther ap-\nproaches involve incorporating prompt engineering tech-\nniques or fixed prompt templates to enhance the quality of\ninput prompts and improve the overall generation output.\nWhile these approaches have shown varying degrees of suc-\ncess in mitigating the aforementioned challenges, they do\nnot provide a comprehensive solution. This leads us to pose\na fundamental question: Can we create a unified framework\nto unleash prompt constraint and activate corresponding\ndomain expert model ?\nIn order to address the aforementioned question, we\npropose DiffusionGPT, which leverages Large Language\nModel (LLM) to offer a one-for-all generation system that\nseamlessly integrates superior generative models and effec-\ntively parses diverse prompts. DiffusionGPT constructs a\nTree-of-Thought (ToT) structure, which encompasses var-\nious generative models based on prior knowledge and hu-\nman feedback. When presented with an input prompt, the\nLLM first parses the prompt and then guides the ToT to\nidentify the most suitable model for generating the desired\noutput. Furthermore, we introduce Advantage Databases,\nwhere the Tree-of-Thought is enriched with valuable hu-\nman feedback, aligning the LLM\u2019s model selection process\nwith human preferences.\nThe contributions of this work can be summarized as:\n\u2022 New Insight:\nDiffusionGPT employs Large Language\nModel (LLM) to drive the entire text-to-image generation\nsystem. The LLM acts as the cognitive engine, processing\ndiverse inputs and facilitating expert selection for output.\n\u2022 All-in-one System: DiffusionGPT provides a versatile and\nprofessional solution by being compatible with a wide\nrange of diffusion models. Unlike existing approaches\nthat are limited to descriptive prompts, our framework can\nhandle various prompt types, expanding its applicability.\n\u2022 Efficiency and Pioneering: DiffusionGPT stands out for\nits training-free nature, allowing for easy integration as a\nplug-and-play solution. Through the incorporation of the\nTree-of-Thought (ToT) and human feedback, our system\nachieves higher accuracy and pioneers a flexible process\nfor aggregation of more experts.\n\u2022 High Effectiveness:\nDiffusionGPT outperforms tradi-\ntional stable diffusion models, demonstrating significant\nadvancements. By providing an all-in-one system, we of-\nfer a more efficient and effective pathway for community\ndevelopment in the field of image generation.\n2. Related Work\n2.1. Text-based Image Generation\nInitially, Generative Adversarial Networks (GANs) [15, 26]\nwere widely used as the primary approach for Text-based\nimage generation. However, the landscape of image gener-\nation has evolved, and diffusion models [6] have emerged as\na dominant framework, especially when integrated with text\nencoders such as CLIP [12] and T5 [13], enabling precise\ntext-conditioned image generation. For instance, DAELL-\n2 [14] leverages CLIP\u2019s image embeddings, derived from\nCLIP\u2019s text embeddings through a prior model, to generate\nhigh-quality images. Similarly, Stable Diffusion [16] di-\nrectly generates images from CLIP\u2019s text embeddings. Im-\nagen [17], on the other hand, utilizes a powerful language\nmodel like T5 [13] to encode text prompts, resulting in ac-\ncurate image generation. Transformer-based architectures\nhave also demonstrated their efficacy in generating images\nfrom textual inputs. CogView2 [4] and Muse [2] are notable\nexamples of such models. To align text-to-image diffusion\nmodels with human preferences, recent methods [1, 8, 24]\npropose training diffusion models with reward signals. This\nensures that the generated images not only meet quality\nbenchmarks but also closely align with human intent and\npreferences. These cutting-edge approaches aim to improve\nthe fidelity and relevance of generated images to better sat-\nisfy user requirements.\n2\nPrompt Parse \nAgent\nUser Input\nTOT of Models \nBuilding Agent\nTOT of Models \nSearching Agent\nModel Selection \nAgent\nPrompt Extension \nAgent\nImage \nGeneration\nStep 1: Prompt Parse\nStep 2: Tree-of-though of models\nStep 3: Model Selection\nStep 4: Execution of Generation\nAll-in-one Prompt\n\u2022\nPrompt-Based\n\u2022\nInstruction-Based\n\u2022\nInspiration-Based\n\u2022\nHypothesis-Based\n\u2022\n\u2026\u2026\nHuman \nFeedback\nDiffusion Model\nExample \nPrompt\ngenerate an image \nof a laughing \nwoman, fashion \nmagazine cover\nan image of a \nlaughing woman, \nfashion \nmagazine cover\n->(photo)\n->(cinematic)\n->(FilmVelvia2,\n \nCineStyle5)\nFilmVelvia2\nBuilt Model Tree\nThe woman on the magazine cover is \nlaughing joyfully, her eyes twinkling with \ndelight. She is wearing a fashionable \noutfit that accentuates her curves, and \nher hair is styled in a way that \ncomplements her features.\nExample\nDiffusionGPT\nFigure 2. Overview of DiffusionGPT. The workflow of DiffusionGPT consists of four steps: Prompt Parse, Tree-of-thought of Models of\nBuilding and Searching, Model Selection, and Execution Generation. The four steps are shown from left to right and interact with LLM\ncontinuously. The upper side shows the detailed process of each step. The lower side shows the example of the whole workflow.\n2.2. Large Language Models (LLMs) for Vision-\nLanguage Tasks\nThe field of natural language processing (NLP) has wit-\nnessed a significant transformation with the emergence of\nlarge language models (LLMs) [3, 9, 21], which have\ndemonstrated remarkable proficiency in human interaction\nthrough conversational interfaces. To further enhance the\ncapabilities of LLMs, the Chain-of-Thought (CoT) frame-\nwork [7, 22, 28, 29] has been introduced. This framework\nguides LLMs to generate answers step-by-step, aiming for\nsuperior final answers. Recent research has explored in-\nnovative approaches by integrating external tools or mod-\nels with LLMs [11, 18\u201320, 23]. For example, Toolformer\n[18] empowers LLMs with the ability to access external\ntools through API tags. Visual ChatGPT [23] and Hugging-\nGPT [19] extend the capabilities of LLMs by enabling them\nto leverage other models to handle complex tasks that go\nbeyond language boundaries. Similarly, Visual Program-\nming [5] and ViperGPT [20] harness the potential of LLMs\nin processing visual objects by utilizing programming lan-\nguages to parse visual queries. Drawing inspiration from\nthese endeavors, we embrace the concept of LLMs as ver-\nsatile tools and leverage this paradigm to guide T2I models\nto generate high-quality images.\n3. Methodology\nDiffusionGPT is an all-in-one system specifically designed\nto generate high-quality images for diverse input prompts.\nIts primary objective is to parse the input prompt and iden-\ntify the generative model that produces the most optimal\nresults, which is high-generalization, high-utility, and con-\nvenient.\nDiffusionGPT composes of a large language model\n(LLM) and various domain-expert generative models from\nthe open-source communities (e.g.\nHugging Face, Civi-\ntai). The LLM assumes the role of the core controller and\nmaintains the whole workflow of the system, which con-\nsists of four steps: Prompt Parse, Tree-of-thought of Mod-\nels of Building and Searching, Model Selection with Hu-\nman Feedback, and Execution of Generation. The overall\npipeline of DiffusionGPT is shown in Figure 2.\n3.1. Prompt Parse\nThe Prompt Parse Agent plays a pivotal role in our method-\nology as it utilizes the large language model (LLM) to an-\nalyze and extract the salient textual information from the\ninput prompt. Accurate parsing of the prompt is crucial for\neffectively generating the desired content, given the inher-\nent complexity of user input. This agent is applicable to var-\nious types of prompts, including prompt-based, instruction-\nbased, inspiration-based, hypothesis-based, etc.\nPrompt-based: The entire input is used as the prompt for\ngeneration. For example, if the input is \u201ca dog\u201d the prompt\nused for generation would be \u201ca dog\u201d.\nInstruction-based: The core part of the instruction is ex-\ntracted as the prompt for generation. For instance, if the in-\nput is \u201cgenerate an image of a dog\u201d, the recognized prompt\nwould be \u201can image of a dog\u201d.\nInspiration-based: The target subject of the desire is ex-\ntracted and used as the prompt for generation (e.g., Input:\n\u201cI want to see a beach\u201d; Recognized: \u201ca beach\u201d).\nHypothesis-based: It involves extracting the hypothesis\ncondition (\u201cIf xxx, I will xxx\u201d) and the object of the forth-\ncoming action as the prompt for generation. For instance, if\nthe input is \u201cIf you give me a toy, I will laugh very happily\u201d,\nthe recognized prompt would be \u201ca toy and a laugh face\u201d.\nBy identifying these forms of prompts, Prompt Parse\n3\nChatGPT\nGiven the user input text.\nPlease judge the paradigm of the input text, and then recognize the main string \nof text prompts according to the corresponding form. \nPrompt\nParse\nIdentify and behave as five different experts that are appropriate to select one \nelement from the input category list that best matches the input prompt.  All \nexperts will write down the selection result, then share it with the group. \nYou then analyze all 5 analyses and output the consensus selected element or \nyour best guess matched element. \nTree-of-thought \nof Models Search\n\u2022 Please judge whether each name in this list {model_list_T} has highly similar \nname in the list {model_list_H}, if yes, output the similar model name \n\u201cintersection_model\u201d.\n\u2022 Please select one model name from the following model list \n{intersection_model} that has the highest frequency and top ranking \naccording to the list {model_list_H}.\nPlease follow the sentence pattern of the example to expand the description of the \ninput paragraph. The output MUST preserve the contents of the input paragraph.\nExample: {example_prompt}.\nModel \nSelection\nPrompt \nExtension\nUser Input\nPrompt\n\u2022\nYou are an information analyst who can analyze and summarize a set of \nwords to abstract some representation categories.\n\u2022\nYou are an information analyst who can create a Knowledge Tree according \nto the input categories.\n\u2022\nYou are an information analyst who can add some input models to an input \nknowledge tree according to the similarity of the model tags and the \ncategories of the knowledge tree.\nTree-of-thought \nof Models Build\nPrompt\nPrompt\nThe built tree-of-thought of models.\nThe selected model list \u201cmodel_list_T\u201d.\nPrompt\nThe obtained model list \u201cmodel_list_H\u201d by the HF.\nPrompt\nThe selected model.\nFigure 3. Details of prompts during interactions with the ChatGPT[9]. Before being inputted into the ChatGPT, the slots \u201c{}\u201d in figure are\nuniformly replaced with the corresponding text values.\nAgent allows DiffusionGPT to accurately recognize the\ncore content that the user wants to generate, while miti-\ngating the influence of noisy text. This process is crucial\nfor selecting appropriate generative models and achieving\nhigh-quality generation results.\n3.2. Tree-of-thought of Models\nFollowing the prompt parsing stage, the subsequent step in-\nvolves selecting appropriate generative models from an ex-\ntensive model library to generate the desired images. How-\never, considering the large number of models available, it\nis impractical to input all models simultaneously into the\nlarge language model (LLM) for selection. Additionally,\nas different models may exhibit similarities in their genera-\ntion spaces, accurately identifying the most suitable model\nthrough a single fuzzy match across the entire model library\nbecomes challenging. To address this issue and identify the\noptimal model, we propose the utilization of a model tree\nbased on the concept of Tree-of-Thought (TOT).By leverag-\ning the search capabilities of the model tree, we can narrow\ndown the candidate set of models and enhance the accuracy\nof the model selection process.\nConstructing the Model Tree using TOT. The Tree-of-\nThought (TOT) of Model Building Agent is employed to au-\ntomatically build the Model Tree based on the tag attributes\nof all models. By inputting the tag attributes of all models\n4\ninto the agent, it analyzes and summarizes potential cate-\ngories derived from the Subject Domain and Style Domain.\nThe Style categories are then incorporated as subcategories\nwithin the Subject category, establishing a two-layer hierar-\nchical tree structure. Subsequently, all models are assigned\nto suitable leaf nodes based on their attributes, thereby com-\npleting the comprehensive model tree structure. The figure\nbelow illustrates the visual representation of the model tree.\nAs the model tree is automatically constructed by the agent,\nthis approach ensures convenient extensibility for incorpo-\nrating new models. Whenever new models are added, agent\nseamlessly places them in the appropriate position within\nthe model tree based on their attributes.\nSearching the Model Tree using TOT. The search process\nwithin the Model Tree, based on the Tree-of-Thought (TOT)\nof Models Searching Agent, aims to identify a candidate set\nof models that closely align with the given prompt. This\nsearch method employs a breadth-first approach, systemat-\nically evaluating the best subcategory at each leaf node. At\neach level, the categories are compared against the input\nprompt to determine the category that exhibits the closest\nmatch. This iterative process continues to derive the can-\ndidate set for the subsequent leaf node, and the search pro-\ngresses until reaching the final node, where the candidate set\nof models is obtained. This candidate set of models serves\nas the basis for model selection in the subsequent stage.\n3.3. Model Selection\nThe model selection stage aims to identify the most suit-\nable model for generating the desired image from the can-\ndidate set obtained in the previous stage. This candidate\nset represents a subset of the entire model library, consist-\ning of models that exhibit relatively high matching degrees\nwith the input prompt. However, the limited attribute in-\nformation available from open-source communities poses\nchallenges in precisely determining the best model while\nproviding detailed model information to the large language\nmodel (LLM). To address this, we propose a Model Selec-\ntion Agent that leverages human feedback and utilizes ad-\nvantage database technology to align the model selection\nprocess with human preferences.\nFor the advantage database, we employ a reward model\nto calculate scores for all model-generated results based\non a corpus of 10,000 prompts, storing the score informa-\ntion. Upon receiving an input prompt, we calculate the se-\nmantic similarity between the input prompt and the 10,000\nprompts, identifying the top 5 prompts with the highest\nsimilarity.\nSubsequently, the Model Selection Agent re-\ntrieves the precomputed performance of each model for\nthese prompts from an offline database and selects the top 5\nmodels for each selected prompt. This process produces a\ncandidate set of 5x5 models.\nThe agent then intersects the model set with the model\ncandidate set obtained in the TOT of models stage, focus-\ning on models with higher occurrence probabilities and rela-\ntively higher rankings. These models are ultimately chosen\nas the final selection for model generation.\n3.4. Execution of Generation\nOnce the most suitable model has been selected, the chosen\ngenerative model is utilized to generate the desired images\nusing the obtained core prompts.\nPrompt Extension.\nTo enhance the quality of prompts\nduring the generation process, a Prompt Extension Agent\nis employed to augment the prompt. This agent leverages\nprompt examples from the selected model to automatically\nenrich the input prompt. The example prompts and the in-\nput prompts are both sent to LLM in the in-context learning\nparadigm. Specially, this agent incorporates rich descrip-\ntions and detailed vocabulary to the input prompts follow-\ning the sentence pattern of example prompts. For exam-\nple, if the input prompt is \u201can image of a laughing woman,\nfashion magazine cover\u201d and example prompt is \u201cfashion\nphotography portrait of woman avatar, 1girl in blue lush\nAlien Rainforest with flowers and birds, fantasy, octane ren-\nder, hdr, Dolby Vision, (intricate details, hyperdetailed:1.2),\n(natural skin texture, hyperrealism, soft light:1.2), fluffy\nshort hair, sharp focus, night, necklace, Chinese mythol-\nogy, cleavage, medium breasts, sci-fi headband, looking at\nviewer, best quality, perfect body\u201d, the Prompt Extension\nAgent enhances it to a more detailed and expressive form\nsuch as: \u201cThe woman on the magazine cover is laughing\njoyfully, her eyes twinkling with delight. She is wearing a\nfashionable outfit that accentuates her curves, and her hair\nis styled in a way that complements her features\u201d. This\naugmentation significantly improves the quality of the gen-\nerated outputs.\n4. Experiments\n4.1. Settings\nIn our experimental setup, the primary large language\nmodel (LLM) controller employed was ChatGPT[9], specif-\nically utilizing the text-davinci-003 version, which is acces-\nsible through the OpenAI API. To facilitate the guidance\nof LLM responses, we adopted the LangChain framework,\nwhich effectively controlled and directed the generated out-\nputs. For the generation models utilized in our experiments,\nwe selected a diverse range of models sourced from the Civ-\nitai and Hugging Face communities. The selection process\ninvolved choosing the most popular models across different\ntypes or styles available on these platforms.\n5\nPrompt\nInstruction\nInspiration\nHypothesis\nSD 15\nOurs\nAlignment\nAesthetics\nThe man who whistles tunes \npianos, watercolor\na girl examining an \nammonite fossil\nCreate an illustration of a romantic \ncouple sharing a tender moment \nunder a starry sky\nTransport us to a snowy wonderland \nwhere children build snowmen and \nhave snowball fights\nI dream of floating in the \nclouds with colorful hot air \nballoons\nIf I had a magic wand, I would bring \ntoys to life and create a magical \ntoyland\nIf I could visit outer space, I \nwould witness breathtaking \nviews of distant galaxies\nCreate an illustration of a chef \nexpertly preparing a mouthwatering \ngourmet dish\nSD 15\nOurs\nFigure 4. When comparing SD1.5-based DiffusionGPT with SD15[16], it is observed that DiffusionGPT excels in generating more realistic\nresults at a fine-grained level for categories such as humans and scenes. The generated images demonstrate improved visual fidelity,\ncapturing finer details and exhibiting a higher degree of realism compared to SD15.\nTable 1. Quantitative Results: We evaluate the aesthetic score\nwith image-reward and aesthetic score. Compare to \u201cSD15\u201d and\n\u201cRandom\u201d select expert models to output, both Tree-of-Thoughts\nand Human Feedback can improve the aesthetic of generated im-\nages and human alignment.\nMethod\nImage-reward\nAes score\nSD15\n0.28\n5.26\nRandom\n0.45\n5.50\nDiffusionGPT wo HF\n0.56\n5.62\nDiffusionGPT\n0.63\n5.70\n4.2. Qualitative Results\n4.2.1\nVisualization of SD1.5 Version\nTo assess the efficacy of our system, we performed a com-\nprehensive evaluation by comparing its generation perfor-\nmance against the baseline method, SD 1.5[16]. SD 1.5\nserves as the foundational model for various specialized\ncommunity models. The comparative results are presented\nin Figure 4. We conducted a detail analysis of four dis-\ntinct prompt types and compared them along two key di-\nmensions: semantic alignment and image aesthetics.\nUpon careful examination of the results, we have iden-\ntified two notable issues with the base model: i) Semantic\nLack: The base model\u2019s generated images exhibit a limited\nfocus on specific semantic classes derived from the input\nprompt, resulting in an incomplete capture of the overall se-\nmantic information. This limitation is particularly evident\nin all kinds of prompt types, where the base model strug-\ngles to effectively generate objects related to \u201cman, chef,\nchildren, and toyland\u201d. ii) Poor performance on human-\nrelated targets: The base model faces challenges in gen-\nerating accurate facial and body details for human-related\nobjects, leading to subpar image quality. This deficiency\nbecomes apparent when comparing the aesthetic qualities\nof images depicting \u201cgirl and couple\u201d.\nIn contrast, DiffusionGPT effectively addresses these\nlimitations. The images produced by our system demon-\nstrate relatively complete representations of the target re-\ngions, successfully capturing the semantic information en-\ncompassing the entire input prompt.\nExamples such as\n\u201cman who whistles tunes pianos\u201d and \u201ca snowy wonder-\nland where children build snowmen and have snowball\nfights\u201d showcase the capacity of our system to encompass\nthe broader context. Furthermore, our system excels in gen-\nerating more detailed and accurate image for human-related\nobjects. This is exemplified by the prompt \u201ca romantic cou-\nple sharing a tender moment under a starry sky\u201d.\n4.2.2\nVisualization of SDXL Version.\nWith the advancements in publicly available universal gen-\neration models, the newly improved method SD XL[10]\nhas emerged as a promising approach, demonstrating supe-\nrior generation results. To further enhance our system, we\nhave developed an upgraded version by integrating various\nopen-source community models based on SD XL. In order\nto evaluate the performance of our system, we compared it\nagainst SD XL, as depicted in Figure 5. It is important to\n6\nPrompt\nInstruction\nInspiration\nHypothesis\nAlignment\nAesthetics\nOn a real playground, a \ncartoon shark is pulling a 3D \ntiger while running\nCreate an image of a futuristic \ncityscape with flying cars and towering \nskyscrapers, illuminated by neon lights\nCreate an image of a lone traveler \nwalking an endless desert under a \nstarry sky\nIf I could talk to trees, they \nwould share their ancient \nwisdom with me.\na white towel with a \ncartoon of a cat on it\nSD XL\nOurs\nSD XL\nOurs\nTransport us to a snowy wonderland \nwhere children build snowmen and \nhave snowball fights\nIt would be amazing to engage in a \nvirtual reality game with an animated \nalien creature in a lush jungle setting.\nIf I could communicate with animals, I \nwill stand on the building and discuss \nwith the eagle\nFigure 5. Comparison of SDXL version of DiffusionGPT with baseline SDXL[10]. All generated iamges are 1024\u00d71024 pixels.\nFigure 6. Comparison of DiffusionGPT-Xl with base model.\nFigure 7. User Study: Comparing DiffusionGPT with SD1.5.\nUsers strongly prefer expert models selected by DiffusionGPT\nover the baseline in terms of prompts from all 10 categories.\nnote that all output images have a resolution of 1024x1024,\nand four distinct types of prompts were generated for the\npurpose of comparison. Upon careful analysis, it becomes\napparent that SD XL occasionally exhibits a partial loss of\nsemantic information in specific cases. For instance, the\ngenerated results for prompts involving \u201c3D tiger\u201d in the\nprompt-based category or \u201cflying cars\u201d in the instruction-\nbased category may lack accurate representation. In con-\ntrast, our system excels in producing more precise and vi-\nsually appealing representations. Noteworthy examples in-\nclude the generation of \u201ca white towel with a cartoon cat\u201d\nand the depiction of a \u201cstarry sky\u201d.\n4.3. Quantitative Results\nThe alignment between user preferences and the quantita-\ntive findings presented in Table 1 serves as strong evidence\nof the robustness and effectiveness of DiffusionGPT. To fur-\nther evaluate the different generated results, we employed\nthe aesthetic predictor and human feedback related reward\nmodel. By comparing the effects of our basic version with\nthe baseline model SD1.5, the results in Table 1 demonstrate\nthat our overall framework outperforms SD15 in terms of\nimage-reward and aesthetic score, achieving improvements\nof 0.35% and 0.44% respectively.\n4.4. Ablation Study\n4.4.1\nTree-of-Thought and Human Feedback\nTo validate the effectiveness of the designed components,\nwe conducted a visual analysis of the performance achieved\nby incorporating different modules, as illustrated in Figure\n8. In the figure, the variant labeled as \u201cRandom\u201d repre-\nsents the random sampling model. Notably, the random se-\nlection model generates a considerable number of images\nthat do not align with the input prompts and lack seman-\ntic coherence. However, as we gradually incorporate the\ntree-of-thought (TOT) and human feedback (HF) modules\ninto our system, a significant improvement in the quality of\nthe generated images becomes evident. The images gener-\nated with the inclusion of TOT and HF modules exhibit en-\n7\nA black dragon perched on top of a \ntall Egyptian obelisk and breathing \nflames at a knight on the gro\na cartoon of an angry \nshark.\na panda\na woman looking at a \nhouse\nRandom Select\nOurs - TOT\nOurs - TOT+HF\nFigure 8. Ablation study of DiffusionGPT. The random selection is the baseline method for generating images. The TOT or TOT+HF\nrepresent the performance of different agents.\nThe old man and the boat\na city intersection without cars\nOriginal\nExtended\nThe image depicts an old man in a small boat, surrounded \nby a peaceful lake. He is wearing a tattered hat and a \nweathered coat, and his hands are resting on the oars. The \nsun is setting in the background, casting a warm, orange \nglow over the scene. The boat is weathered and worn, but \nstill sturdy, and the old man looks content and at ease.\nThe city intersection is eerily quiet, with no cars in sight. The \nabsence of traffic creates a surreal atmosphere, illuminated by \ndramatic lighting that casts long shadows across the empty street.\nFigure 9. Ablation study of Prompt Extension. The extension aims\nto provide the riched prompts that produces higher quality images.\nhanced realism, semantic alignment with the input prompts,\nand higher aesthetic appeal. This visual analysis showcases\nthe advantage of our system in selecting superior models\nthrough the integration of TOT and HF components.\n4.4.2\nPrompt extension\nTo evaluate the effectiveness of the prompt extension agent,\nwe conducted a comparison between the generation results\nusing the original prompt and the extended prompt as in-\nputs, as depicted in Figure 9. The extended prompts were\ndesigned to provide a richer and more detailed description\nof the desired image. Upon analysis, we observed that the\nextended prompts yielded notable improvements in the aes-\nthetics and level of detail of the generated images. The in-\nclusion of additional information in the extended prompts\nallowed for the generation of more visually appealing and\nartistically enhanced images.\n4.5. User Study\nTo obtain real human preferences for the generated images,\nwe conducted a user study comparing our model against a\nbaseline model. We utilized image captions sourced from\nPartiPrompts[25], randomly selecting 100 prompts and gen-\nerating four images for each prompt. Subsequently, we col-\nlected feedback from 20 users, who were asked to rate the\nsuperiority or equality of the images. This process resulted\nin approximately 400 votes for each base model (SD15 and\nSD XL). As depicted in Figure 7 and Figure 6, the results\nof the user study consistently revealed a clear preference\nfor our model over the baseline. The users consistently ex-\npressed a distinct preference for the images generated by\nour model, indicating that they perceived them to be of\nhigher quality or superiority compared to the baseline.\n4.6. Limitations and Further works\nAlthough DiffusionGPT has demonstrated the ability to\ngenerate high-quality images, there are still several limita-\ntions and our future plans are in the following ways:\nFeedback-Driven Optimization We aim to incorporate\nfeedback directly into the optimization process of LLM, en-\nabling more refined pompt parse and model selection.\nExpansion of Model Candidates To further enrich the\nmodel generation space and achieve more impressive re-\nsults, we will expand the repertoire of available models.\nBeyond Text-to-Image Tasks We intend to apply our in-\nsight to a broader set of tasks, including controllable gener-\nation, style migration, attribute editing, etc.\n5. Conclusion\nWe propose Diffusion-GPT, a one-for-all framework that\nseamlessly integrates superior generative models and effi-\nciently parses diverse prompts. By leveraging Large Lan-\n8\nguage Models (LLMs), Diffusion-GPT gains insights into\nthe intent of input prompts and selects the most suitable\nmodel from a Tree-of-Thought (ToT) structure. This frame-\nwork offers versatility and exceptional performance across\ndifferent prompts and domains, while also incorporating\nhuman feedback through Advantage Databases.\nTo sum\nup, Diffusion-GPT is training-free and can be easily in-\ntegrated as a plug-and-play solution, offers an efficient\nand effective pathway for community development in this\nfield.\nReferences\n[1] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and\nSergey Levine.\nTraining diffusion models with reinforce-\nment learning. 2\n[2] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot,\nJos\u00b4e Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy,\nWilliamT Freeman, Michael Rubinstein, Yuanzhen Li, Dilip\nKrishnan, and Google Research. Muse: Text-to-image gen-\neration via masked generative transformers. 2\n[3] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham,\nHyungWon\nChung,\nCharles\nSutton,\nSebas-\ntian\nGehrmann,\nParker\nSchuh,\nKensen\nShi,\nSasha\nTsvyashchenko, Joshua Maynez, Abhishek Rao, Parker\nBarnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran,\nEmily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James\nBradbury, Jacob Austin, Michael Isard, Guy Gur-Ari,\nPengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay\nGhemawat,\nSunipa Dev,\nHenryk Michalewski,\nXavier\nGarcia, Vedant Misra, Kevin Robinson, and Liam Fe. Palm:\nScaling language modeling with pathways. 3\n[4] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang.\nCogview2: Faster and better text-to-image generation via hi-\nerarchical transformers. 2\n[5] Tanmay Gupta and Aniruddha Kembhavi.\nVisual pro-\ngramming: Compositional visual reasoning without training.\n2022. 3\n[6] Jonathan Ho, Ajay Jain, Pieter Abbeel, and UC Berkeley.\nDenoising diffusion probabilistic models. 1, 2\n[7] Takeshi Kojima, Shane Shixiang, Gu Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. Large language models\nare zero-shot reasoners. 3\n[8] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins,\nYuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad\nGhavamzadeh, and ShixiangShane Gu.\nAligning text-to-\nimage models using human feedback. 2023. 2\n[9] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, John Schulman, Jacob\nHilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda\nAskell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan\nLowe. Training language models to follow instructions with\nhuman feedback. 3, 4, 5\n[10] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach.\nSdxl: Improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023. 1, 6, 7\n[11] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning\nDing, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao,\nChi Han, YiRen Fung, Yusheng Su, Huadong Wang, Cheng\nQian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu\nShen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei\nTang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin\nCong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu\nHan, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tong-\nshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. Tool\nlearning with foundation models. 3\n[12] Alec Radford, JongWook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable visual\nmodels from natural language supervision. Cornell Univer-\nsity - arXiv,Cornell University - arXiv, 2021. 2\n[13] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeterJ. Liu. Exploring the limits of transfer learning with\na unified text-to-text transformer.\narXiv: Learning,arXiv:\nLearning, 2019. 2\n[14] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. 1, 2\n[15] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Lo-\ngeswaran, Bernt Schiele, and Honglak Lee.\nGenerative\nadversarial text to image synthesis.\nCornell University -\narXiv,Cornell University - arXiv, 2016. 2\n[16] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bjorn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models. In 2022 IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2022. 1, 2, 6\n[17] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar, Seyed\nGhasemipour, Burcu Karagol, SSara Mahdavi, RaphaGon-\ntijo Lopes, Tim Salimans, Jonathan Ho, DavidJ Fleet, and\nMohammad Norouzi. Photorealistic text-to-image diffusion\nmodels with deep language understanding. 1, 2\n[18] Timo Schick, Jane Dwivedi-Yu, Roberto Dess`\u0131, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Can-\ncedda, and Thomas Scialom. Toolformer: Language models\ncan teach themselves to use tools. 2023. 3\n[19] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, Yueting Zhuang, Zhejiang University, and Mi-\ncrosoft Research. Hugginggpt: Solving ai tasks with chatgpt\nand its friends in huggingface. 3\n[20] D\u00b4\u0131dac Sur\u00b4\u0131s, Sachit Menon, and Carl Vondrick. Vipergpt:\nVisual inference via python execution for reasoning. arXiv\npreprint arXiv:2303.08128, 2023. 3\n[21] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar-\ntinet, Marie-Anne Lachaux, Timoth\u2019ee Lacroix, Baptiste\nRozi\u2018ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aure-\nlien Rodriguez, Armand Joulin, Edouard Grave, and Guil-\n9\nlaume Lample. Llama: Open and efficient foundation lan-\nguage models. 3\n[22] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought\nprompting elicits reasoning in large language models. 3\n[23] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang,\nZecheng Tang, and Nan Duan.\nVisual chatgpt: Talking,\ndrawing and editing with visual foundation models. 3\n[24] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai\nLi, Ming Ding, Jie Tang, and Yuxiao Dong.\nImagere-\nward: Learning and evaluating human preferences for text-\nto-image generation. 2023. 2\n[25] Jiahui Yu, Yuanzhong Xu, JingYu Koh, Thang Luong, Gun-\njan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-\nfei Yang, BurcuKaragol Ayan, Ben Hutchinson, Wei Han,\nZarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and\nYonghui Wu. Scaling autoregressive models for content-rich\ntext-to-image generation. 2022. 8\n[26] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiao-\ngang Wang, Xiaolei Huang, and Dimitris Metaxas. Stackgan:\nText to photo-realistic image synthesis with stacked genera-\ntive adversarial networks. In 2017 IEEE International Con-\nference on Computer Vision (ICCV), 2017. 2\n[27] Lvmin Zhang and Maneesh Agrawala. Adding conditional\ncontrol to text-to-image diffusion models. 1\n[28] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola.\nAutomatic chain of thought prompting in large language\nmodels. 2022. 3\n[29] Denny Zhou, Nathanael Sch\u00a8arli, Le Hou, Jason Wei, Nathan\nScales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet,\nQuoc Le, and Ed Chi. Least-to-most prompting enables com-\nplex reasoning in large language models. 2022. 3\n10\n"
  },
  {
    "title": "ChatQA: Building GPT-4 Level Conversational QA Models",
    "link": "https://arxiv.org/pdf/2401.10225.pdf",
    "upvote": "25",
    "text": "ChatQA: Building GPT-4 Level Conversational QA Models\nZihan Liu 1 Wei Ping 1 Rajarshi Roy 1 Peng Xu 1 Chankyu Lee 1 Mohammad Shoeybi 1 Bryan Catanzaro 1\nAbstract\nIn this work, we introduce ChatQA, a family of\nconversational question answering (QA) models\nthat obtain GPT-4 level accuracies. Specifically,\nwe propose a two-stage instruction tuning method\nthat can significantly improve the zero-shot con-\nversational QA results from large language mod-\nels (LLMs). To handle retrieval-augmented gener-\nation in conversational QA, we fine-tune a dense\nretriever on a multi-turn QA dataset, which pro-\nvides comparable results to using the state-of-the-\nart query rewriting model while largely reducing\ndeployment cost. Notably, our ChatQA-70B can\noutperform GPT-4 in terms of average score on\n10 conversational QA datasets (54.14 vs. 53.90),\nwithout relying on any synthetic data from Ope-\nnAI GPT models.\n1. Introduction\nMost recently, ChatGPT (OpenAI, 2022) and its follow\nups (OpenAI, 2023; Anthropic, 2023b; Google, 2023) have\nled to the paradigm shift of building question answer-\ning (QA) models in production and research community.\nIn particular, the following aspects of the QA models are\npreferred in real-world applications: i) The users can in-\nteract with the QA models in a conversational way, thus\none can easily raise follow-up questions; ii) The generalist\nmodels can generate answers in zero-shot manner without\ndataset-specific fine-tuning, while matching the accuracies\nof fine-tuned expert models; iii) The QA models are capable\nof integrating retrieved chunks of evidence in both open-\ndomain or long document settings, where the provided con-\ntext is much longer than the context window of LLM (e.g.,\nAnthropic, 2023a; Xu et al., 2023b). To this end, we focus\non the conversational QA covering these three aspects.\nHowever, building a conversational QA model that can\nmatch the accuracy of the state-of-the-art black-box model,\ni.e., GPT-4 (OpenAI, 2023), is still a grand challenge for the\n1NVIDIA. Correspondence to:\nZihan Liu <zihanl@\nnvidia.com>, Wei Ping <wping@nvidia.com>.\nCopyright 2024 by the author(s).\nresearch community. In this work, we introduce ChatQA-\n70B, a white-box conversational QA model with a detailed\nspecification and GPT-4 level accuracy, through the pro-\nposed two-stage instruction tuning recipe, an enhanced re-\ntriever for retrieval-augmented generation (RAG) in conver-\nsational QA, and a detailed data curation process.\nSpecifically, we make the following contributions:\n1. We propose a two-stage instruction tuning method\nand a dataset curation recipe that can largely en-\nhance LLM\u2019s capability of integrating user provided\nor retrieved context for zero-shot conversational QA\ntasks. We demonstrate that our method significantly\noutperforms regular instruction tuning or RLHF-\nbased recipes (e.g., Llama2-Chat).\n2. For RAG in conversational QA, we show that fine-\ntuning the state-of-the-art single-turn query retriever\non both human-annotated or synthetic multi-turn QA\ndataset works as well as utilizing the state-of-the-art\nLLM-based query rewriting model, i.e., GPT-3.5-\nturbo (OpenAI, 2022).\n3. We build a family of ChatQA models based on\nLlama2-7B, Llama2-13B, Llama2-70B (Touvron\net al., 2023), and in-house GPT-8B, GPT-22B\nmodels. We conduct comprehensive study on 10\nconversational QA datasets, including 5 datasets\nwith long documents that need retrieval and 3\ndatasets with tables. In terms of average score, our\nChatQA-70B model (54.14) can outperform GPT-\n3.5-turbo (50.37) and GPT-4 (53.90) without utiliz-\ning any synthetic data from ChatGPT models.\n4. We study the \u201cunanswerable\u201d scenario, where the\ndesired answer is not included in the provided or\nretrieved context, thus the LLM needs to gener-\nate \u201ccannot answer\u201d to avoid hallucination. Our\nChatQA-70B outperforms GPT-3.5-turbo in this re-\ngard, while still has a slight gap compared to GPT-\n4 (around 3.5%).\nWe organize the rest of the paper as follows. We discuss\nrelated work in \u00a7 2. We introduce the two-stage instruction\ntuning method and data curation for ChatQA in \u00a7 3, and\nstudy retrieval in conversational QA in \u00a7 4. We present the\n1\narXiv:2401.10225v2  [cs.CL]  23 Jan 2024\nChatQA: Building GPT-4 Level Conversational QA Models\nexperimental setup in \u00a7 5, results in \u00a7 6, and conclude the\npaper in \u00a7 7.\n2. Related Work\n2.1. Conversational QA\nQuestion answering in a conversational way naturally im-\nproves user experiences by addressing follow-up questions.\nThe model can also raise clarification questions for users\nif necessary, which can reduce hallucination. Thus, it be-\ncomes the default format of deploying QA models in produc-\ntion (e.g. OpenAI, 2022; Google, 2023; Anthropic, 2023b).\nIn recent years, many conversational QA datasets have been\nintroduced, where the models are asked to answer questions\nbased on provided context or documents. The provided\ncontext or documents can be: i) text-only documents from\nvarious domains (Feng et al., 2020; Anantha et al., 2021;\nSaeidi et al., 2018; Adlakha et al., 2022; Aliannejadi et al.,\n2021; Reddy et al., 2019; Qu et al., 2020; Wu et al., 2023;\nDeng et al., 2022; Guo et al., 2021; Choi et al., 2018; Cam-\npos et al., 2020), or ii) documents comprising plain text\nalong with tables (Pasupat & Liang, 2015; Nakamura et al.,\n2022; Chen et al., 2022a).\nIn contrast to the latest LLM-based generalist solution (e.g.,\nOpenAI, 2022), most of the previous studies focus on fine-\ntuned expert models on specific domains or datasets (Feng\net al., 2020; Izacard & Grave, 2021; Chen et al., 2022a; Gao\net al., 2022; Nakamura et al., 2022; Adlakha et al., 2022;\nWu et al., 2023).\n2.2. Retrieval for Multi-Turn QA\nConversational QA involves retrieval-augmented genera-\ntion (RAG) in open-domain setting, or when the provided\ndocuments are longer than the context window of LLM. The\ndense retrievers are usually trained to retrieve the top-k rele-\nvant chunks given a single question (e.g., Lin et al., 2023a;\nWang et al., 2022a; Izacard et al., 2022). In conversational\nQA, the follow-up questions (e.g., with pronouns referring\nto entities mentioned in the previous conversation) may have\ninsufficient information for retrieval, while feeding them\nalong with all of the dialogue history can be redundant, thus\nleading to sub-optimal results.\n2.2.1. CONVERSATIONAL QUERY REWRITING\nMost of the previous solutions are query rewriting methods.\nThe latest turn of question is rewritten to be a standalone\nquery without additional information from previous dia-\nlogue history (Vakulenko et al., 2021a; Ye et al., 2023; Mo\net al., 2023), so it can be directly used by retrieval model\nto retrieve relevant context (Vakulenko et al., 2021b; Mele\net al., 2021; Raposo et al., 2022; Mo et al., 2023). Many\ndatasets have been collected to facilitate this line of re-\nsearch (Elgohary et al., 2019; Chu et al., 2020; Qu et al.,\n2020; Anantha et al., 2021; Brabant et al., 2022), along-\nside multiple proposed query rewriting methods (Ishii et al.,\n2022; Yu et al., 2020; Wu et al., 2022; Del Tredici et al.,\n2021; Chen et al., 2022b; Galimzhanova et al., 2023). For\nexample, Wu et al. (2022) and Chen et al. (2022b) pro-\nposed to use reinforcement learning methods for the query\nrewriting. Yu et al. (2020) investigated few-shot generative\nmodels like GPT-2 for query rewriting. Galimzhanova et al.\n(2023) studied instruction tuned GPT-3.5-turbo and showed\nthat it achieved state-of-the-art results for conversational\nquery rewriting.\n2.2.2. FINE-TUNING RETRIEVER FOR MULTI-TURN QA\nSome previous work fine-tune a single-turn query retriever\non in-domain conversational query and context pairs (Feng\net al., 2020; Gao et al., 2022; Adlakha et al., 2022; Wu\net al., 2023), so it can directly take a concatenation of dialog\nhistory and current query as input. In this work, we focus on\nthe zero-shot evaluation. We fine-tune a single-turn query\nretriever on a high-quality multi-turn dataset. Then, we\nevaluate zero-shot capability of the fine-tuned retriever on\nfive benchmark datasets. Surprisingly, we find this simple\napproach can obtain comparable zero-shot results as the\nstate-of-the-art query rewriting model, i.e., GPT-3.5-turbo.\n2.3. Instruction Tuning\nThe goal of instruction tuning is to equip LLMs with the\ncapability to follow natural language instructions (Wei et al.,\n2022a; Sanh et al., 2022; Mishra et al., 2022; Iyer et al.,\n2022; Du et al., 2022; Ouyang et al., 2022; Wang et al.,\n2023b; Zhang et al., 2023b; Gao et al., 2023; Chung et al.,\n2022; Muennighoff et al., 2022; Xu et al., 2023a; Wang\net al., 2022c; Zhou et al., 2023). There has been a surge in\nthe development of high-quality instruction tuning datasets,\nincluding FLAN (Chung et al., 2022), Self-Instruct (Wang\net al., 2022b), unnatural Instructions (Honovich et al., 2022),\nDolly (Conover et al., 2023b), and OpenAssistant (K\u00a8opf\net al., 2023).\nAlthough numerous research on instruction tuning has been\nconducted, a few works focused on improving RAG or\ncontext awareness generation for QA. Lin et al. (2023b)\nintroduced a retrieval-augmented instruction tuning method,\nwhich appends top-k retrieved chunks for LLM fine-tuning.\nWang et al. (2023a) applied instruction tuning after retrieval-\naugmented pretraining. In contrast, we propose a two-stage\ninstruction tuning method to improve generation with re-\ntrieval or provided context. We find that appending top-k\nretrieved chunks for LLM fine-tuning does not help for a\nwide range of conversation QA tasks (see \u00a76.3 for details).\nSimilar to the latest work (Zhang et al., 2023a), we demon-\n2\nChatQA: Building GPT-4 Level Conversational QA Models\n1/4/24, 8:10 PM\nmodel\nPage 1 of 2\nhttps://www.mathcha.io/editor#\n \n \n \nStage-1: Supervised Fine-tuning\nFoundation\nLLM\nSFT\nModel\nChatQA\nStage-2: Context-Enhanced Instruction Tuning\nNarrativeQA\nDROP, Quoref\nROPES, SQuAD\nNewsQA, TAT-QA\nConversationalQA\nSoda, ELI5\nFLAN, Dolly\nOpenAssistant\nFigure 1. Two-stage instruction tuning framework for ChatQA.\nstrate that adding a small amount of \u201cunanswerable\u201d sam-\nples in instruction tuning can steer the model to generate\n\u201ccannot answer\u201d output when it is necessary, thus signifi-\ncantly reduce hallucination.\nAfter the release of ChatGPT (OpenAI, 2022), instruction\ntuning became an indispensable ingredient to build the state-\nof-the-art dialogue agent which has astonishing zero-shot\ncapability to a wide range of tasks. The conversational QA\ncapability plays a crucial role in a dialogue agent, yet there\nhas been limited research dedicated to this vital aspect.\n3. ChatQA\nIn this section, we propose a two-stage instruction tuning\nmethod for ChatQA. See Figure 1 for an illustration. Our\nmethod starts with a pretrained LLM foundation model. At\nstage-1, we apply supervised fine-tuning (SFT) as in Ouyang\net al. (2022) on a blend of instruction-following and dialog\ndatasets. After that, our model exhibits good capability to\nfollow instructions as a conversational agent. However, its\ncapability for contextualized or RAG-based QA remains lim-\nited. Hence, we introduce a subsequent stage, called context-\nenhanced instruction tuning, which is designed specifically\nfor enhancing our model\u2019s capability for context-aware or\nretrieval-augmented generation in conversational QA.\n3.1. Stage-1: Supervised Fine-tuning\nTo construct a large and comprehensive supervised fine-\ntuning (SFT) dataset, we follow Xu et al. (2023b), Wang\net al. (2023a) and gather a combined set of 128K SFT sam-\nples from high-quality instruction tuning datasets. It con-\nsists of 1) a social dialogue dataset Soda (Kim et al., 2022),\n2) a long-form QA dataset ELI5 containing elaborate an-\nswers (Fan et al., 2019), 3) FLAN and chain-of-thought\ndatasets (Wei et al., 2022b; Chung et al., 2022; Longpre\net al., 2023), 4) LLM synthetic instruction tuning datasets,\nincluding Self-Instruct (Wang et al., 2022b) and Unnatu-\nral Instructions (Honovich et al., 2022), and 5) a private\ncrowd-sourced conversational dataset, as well as two public\nhuman-written conversation datasets: OpenAssistant (K\u00a8opf\net al., 2023), and Dolly (Conover et al., 2023a).\nWe unify the structure of all the SFT data in a conversational\nformat. We first add a \u201cSystem\u201d role at the beginning to\nset up a general instruction guiding LLM to provide polite\nand helpful answers. We also add \u201cUser\u201d and \u201cAssistant\u201d\nroles to incorporate instruction and response pairs from the\ninstruction tuning datasets. We apply fine-tuning using this\nunified format on an LLM foundation model.\n3.2. Stage-2: Context-Enhanced Instruction Tuning\nTo further enhance the model\u2019s conversational QA capability\nover a given context, we conduct a second stage instruction\ntuning, which integrates contextualized QA datasets into the\ninstruction tuning blend. Specifically, the stage-2 instruction\ntuning datasets consist of a blend of contextualized single-\nturn QA and conversational QA datasets. We present further\ndetails for the stage-2 instruction tuning datasets below.\n3.2.1. HUMAN ANNOTATED DATA\nIn addition to the public available datasets, one of the key\nelements for stage-2 is to obtain a high-quality document-\ngrounded conversational QA dataset. We create a human-\nannotated conversational QA (called HumanAnnotatedCon-\nvQA) dataset only consisting of 7k dialogues. To build this\ndataset, we first collected 7k documents covering diverse\ntopics from the Internet. Then, we instruct the annotator to\nact both as a curious user asking questions (and follow-up\nquestions) about the document and as an agent to give the\nanswers. We create a multi-turn conversation for each docu-\nment, resulting in a total of 7k conversational QA dialogues\nwith an average of 5 user-agent turns per dialogue. Details\nof data collection guideline can be found in Appendix G.2.\nTo reduce hallucinated answers in unanswerable cases, we\naim to empower our model to explicitly indicate it when the\nanswer cannot be found within the given context. To obtain\nthese unanswerable data samples, we requested annotators\nto identify all parts of the context locations that are relevant\nto the user\u2019s question. Hence, it enabled us to construct unan-\nswerable scenarios by deleting the text from the correspond-\ning locations in the context. After deleting the relevant text\nto the question, we use a sentence, \u201cSorry. I cannot\nfind the answer based on the context\u201d, as\n3\nChatQA: Building GPT-4 Level Conversational QA Models\nthe response for the unanswerable questions. Finally, we\nconstruct another 1.5k user-agent turns with unanswerable\nannotations, which provides a good trade-off of answerable\nand unanswerable cases (see \u00a76.5 for details).\n3.2.2. SYNTHETIC DATA GENERATION\nTo validate the quality of the HumanAnnotatedConvQA, we\nleverage GPT-3.5-turbo to generate a synthetic conversa-\ntional QA dataset given its powerful instruction-following\nand text generation capability. Note that large-scale syn-\nthetic data for conversational QA has also been explored\nin Dai et al. (2022). In this work, we focus on midsize\nhigh-quality synthetic data for LLM fine-tuning.\nThe instruction for GPT-3.5-turbo comprises three parts:\n1) system role guiding the model to provide helpful answers,\n2) examples of conversational QA indicating the required\ndata types, and 3) a document that directs the model to gen-\nerate conversational QA based on its content. We collect 7k\ndocuments (average \u223c1k words per document) from com-\nmon crawl, which cover a wide range of domains. Each\ndocument is used for generation of a single conversational\nQA sample, which leads to a total of 7k multi-turn QA dia-\nlogues with an average of 4.4 user-agent turns per dialogue\n(called SyntheticConvQA).\nSimilar to the HumanAnnotatedConvQA, we construct an-\nother 1.5k user-agent turns with unanswerable annotations\nin this synthetic dataset. Since there are no annotations of\nthe context location for the agent\u2019s answer, we construct\nsynthetic unanswerable samples from SyntheticConvQA.\nSpecifically, we first cut the document (for each dialogue)\ninto different chunks. Then, we consider it as a valid unan-\nswerable sample only when there are chunks that have \u201chigh\noverlaps\u201d with the agent\u2019s answer to be removed and the rest\nof the chunks show \u201clow overlaps\u201d with the agent\u2019s answer.\nWe use the 4-gram recall score between each chunk and the\nagent\u2019s answer (which measures the ratios of the answer\u2019s\n4-gram phrases are within each chunk) as the metric to mea-\nsure their overlaps, and consider it higher than 0.5 as \u201chigh\noverlaps\u201d and lower than 0.1 as \u201clow overlaps\u201d.\n3.2.3. TRAINING BLENDS\nIn this part, we introduce the details of training blends for\nstage-2 instruction tuning. To boost the QA capability in\nhandling tabular documents and arithmetic calculation, we\nadd the TAT-QA dataset (Zhu et al., 2021) which contains\nboth elements. In addition, we integrate contextualized\nsingle-turn QA datasets to further strengthen the QA capa-\nbility of our model. We also retain the stage-1 SFT dataset\nin the training blend to maintain the model\u2019s instruction-\nfollowing capability.\nFinally, the training blend for stage-2 consists of: 1) A\nPage 1 of 2\nhttps://www.mathcha.io/editor\n \nQuery\nEncoder\nUser: is brandy melville a person?\nAssistant: No. It is an Italian brand\nUser: I haven't heard of it. How do\nthey advertise it?\n... ... It does not employ traditional \nadvertising techniques. However, they\ndepend heavily on social media for \ntheir advertising ... ...\nContrastive\nFinetuning\nMulti-Turn Queries\nCorresponding Contexts\nq1\nq2\nq3\nc1\nc2\nc3\nContext\nEncoder\nEq1 Eq2 Eq3\nEc1\nEc2\nEc3\npos\npos\npos\nneg neg\nneg\nneg\nneg neg\nFigure 2. Illustration of fine-tuning retriever for multi-turn QA.\nconversational QA dataset:\nHumanAnnotatedConvQA\nor SyntheticConvQA,\n1 2) single-turn QA datasets:\nDROP (Dua et al., 2019), NarrativeQA (Ko\u02c7cisk`y et al.,\n2018), Quoref (Dasigi et al., 2019), ROPES (Lin et al.,\n2019), SQuAD1.1 (Rajpurkar et al., 2016), SQuAD2.0 (Ra-\njpurkar et al., 2018), NewsQA (Trischler et al., 2017), TAT-\nQA (Zhu et al., 2021), and 3) all of SFT datasets from\nstage-1.\nWe follow the similar template as in stage-1 to unify all\nsingle-turn QA and conversational QA datasets. The dif-\nferences are in two parts: 1) Following the system role, we\nappend relevant context for the single-turn question or the\nmulti-turn conversation, and 2) Just before the single-turn\nquestion or multi-turn conversation, we integrate further in-\nstruction based on the answer types of different QA datasets\n(e.g., short answer, long answer, arithmetic calculation). We\nuse the format for SFT dataset from stage-1. 2\n4. Retrieval for Multi-Turn QA\nIn conversational QA tasks, when a document becomes too\nlengthy to feed directly into LLMs, a retriever that can han-\ndle conversational queries becomes essential. This conver-\nsational retriever encodes the concatenation of the dialogue\nhistory and the current query, and then retrieve relevant\ncontext from documents. After that, only the relevant con-\ntext will be used as inputs for LLMs. The state-of-the-art\nretrievers, e.g., Dragon (Lin et al., 2023a), are optimized\nfor single-turn queries, resulting in a limited generalization\ncapability for multi-turn conversational queries. In Figure 2,\nwe depict our retriever fine-tuning method to alleviate this\nissue. We propose to use conversational query and context\npairs for further fine-tuning a single-turn retriever to better\ncope with conversational inputs.\nAn alternative solution is conversational query rewriting\n1Unless specified otherwise, the experiments use HumanAnno-\ntatedConvQA as the default setting.\n2Details of the templates for both stage-1 and stage-2 instruc-\ntion tuning as well as the synthetic data generation can be found in\nthe Appendix A.\n4\nChatQA: Building GPT-4 Level Conversational QA Models\nModels\nAverage\nDoc2Dial\nQuAC\nQReCC\nTopiOCQA\nINSCIT\ntop-1\ntop-5\ntop-1\ntop-5\ntop-1\ntop-5\ntop-1\ntop-5\ntop-5*\ntop-20*\ntop-5*\ntop-20*\nAdlakha et al. (2022)\n-\n-\n-\n-\n-\n-\n-\n-\n-\n70.40\u25b3\n-\n-\nWu et al. (2023)\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n71.10\u25b3\nE5-unsupervised\u2020 (Wang et al., 2022a)\n31.56\n59.22\n23.02\n55.33\n43.49\n77.68\n44.71\n84.99\n26.25\n37.67\n20.32\n40.44\nE5-unsupervised + Rewrite\u2021\n33.23\n61.02\n25.56\n58.00\n46.00\n80.01\n45.50\n85.89\n27.58\n39.15\n21.53\n42.04\nE5-unsupervised + Fine-tune\u2020\n47.79\n75.00\n45.28\n80.96\n46.52\n80.74\n53.37\n89.91\n41.01\n51.07\n52.79\n72.31\nDragon\u2020 (Lin et al., 2023a)\n46.29\n73.09\n43.33\n75.61\n56.80\n82.86\n46.17\n81.96\n57.68\n78.80\n27.49\n46.22\nDragon + Rewrite\u2021\n54.46\n80.13\n47.60\n80.60\n47.10\n77.15\n51.73\n85.78\n73.07\n88.19\n52.79\n68.92\nDragon + Fine-tune\u2020\n52.72\n80.67\n48.94\n83.01\n52.64\n81.95\n50.73\n87.17\n67.86\n86.28\n43.43\n64.94\n- SyntheticConvQA\u2662\n52.98\n81.15\n48.64\n83.47\n54.75\n83.23\n49.63\n86.70\n64.48\n85.24\n47.41\n67.13\nTable 1. Retrieval results across five multi-turn QA datasets with the average top-1 and top-5 recall scores. Compared to rewriting,\nfine-tuning performs much better on E5-unsupervised and is comparable on Dragon. *Since the average context length in TopiOCQA and\nINSCIT is smaller than in other datasets, we report top-5 and top-20 to roughly match the context lengths of top-1 and top-5, respectively,\nin those datasets. \u2020The inputs for these two models are a concatenation of the dialogue history and the current query. \u2021The input for this\nmodel is the rewritten query. \u2662denotes that the HumanAnnotatedConvQA dataset is replaced with the SyntheticConvQA for fine-tuning.\n\u25b3The numbers are not apple-to-apple comparison (e.g., they use the training set for fine-tuning).\nmethod which uses a query rewriter to rewrite the current\nquestion based on the conversational history. The rewritten\nquery is then directly used as the input to a single-turn\nquery retriever for retrieving relevant context. In addition to\nthe embedding and search cost, the query rewriting model\nintroduces a large amount of extra computational expense\nto generate the rewritten query.\n4.1. Fine-tuning Retriever for Multi-turn QA\nTo build a high-quality fine-tuning dataset, we leverage the\nconversational QA dataset from either the HumanAnnotat-\nedConvQA or the SyntheticConvQA to construct conversa-\ntional query and context pairs.\nFor the HumanAnnotatedConvQA, we directly take the an-\nnotations of the conversational query and context pairs, and\nuse them to further fine-tune a single-turn query retriever.\nFor the SyntheticConvQA, we first cut each document in\nthe conversational QA dataset into different chunks. Then,\nwe calculate the 4-gram recall score between agent\u2019s answer\nand each chunk. After that, we consider the chunk that\nhas the highest recall score as the gold chunk for the cur-\nrent user\u2019s question. Finally, the constructed conversational\nquery and context pairs are used to fine-tune a single-turn\nquery retriever.\n4.2. Conversational Query Rewriting\nTo build powerful conversational query rewriting model, we\ntake GPT-3.5-turbo as the rewriter given that Galimzhanova\net al. (2023) demonstrated the state-of-the-art query rewrit-\ning results using GPT-3.5-turbo. Similar to Galimzhanova\net al. (2023), we not only provide GPT-3.5-turbo with the\nrewriting task instruction, but also give it few-shot rewriting\nexamples to enhance the quality of rewriting results. More\ndetails can be found in Appendix B.1.\n4.3. Comparisons\nIn Table 1, we compare the query rewriting and fine-tuning\nmethods across five datasets in the zero-shot setting. More\ndetails about these datasets can be found in \u00a75.2.1. We con-\nduct experiments on a state-of-the-art retriever, Dragon (Lin\net al., 2023a), and a strong unsupervised retriever, E5-\nunsupervised (Wang et al., 2022a), which is not finetuned\non MS MACRO (Nguyen et al., 2016). In terms of the\nexperiments on Dragon, we find that fine-tuning performs\nmarginally worse than query rewriting in average top-1 re-\ncall by 1.74%, while it achieves better results on average\ntop-5 recall by 0.54%. It demonstrates the effectiveness of\nthe fine-tuning approach for the conversational retrieval.\nIn addition, we observe that the results are comparable\nbetween using HumanAnnotatedConvQA and Synthetic-\nConvQA for fine-tuning. This highlights that our human-\nannotated dataset is in high-quality, and we do not rely on\nChatGPT models for building the state-of-the-art multi-turn\nquery retriever.\nSurprisingly, fine-tuning performs significantly better than\nrewriting on E5-unsupervised. We conjecture that since\nE5-unsupervised does not use human-annotated query and\ncontext pairs in the pre-training stage, it leads to weak gen-\neralization for the high-quality rewritten query. In contrast,\nusing a high-quality dataset to fine-tune E5-unsupervised\nbrings a giant boost, with more than a 15% improvement on\nboth average top-1 and top-5 recall scores.\nTherefore, fine-tuning a good single-turn retriever on high-\nquality conversational query context pairs performs on\npar with leveraging the state-of-the-art rewriter. However,\nrewriting method requires extra computational time for au-\ntoregressive generation process and probably also API cost\n5\nChatQA: Building GPT-4 Level Conversational QA Models\nfor using powerful models like GPT-3.5-turbo. In contrast,\nour proposed multi-turn fine-tuning bypasses these issues.\nFor the QA evaluations across these five datasets, we con-\nsistently use the retrieved top-5 results from the fine-tuning\napproach for all the QA models. We put more results on\ncomparisons between rewriting and fine-tuning methods in\nthe Appendix B.2.\n5. Experimental Setup\nIn this section, we present the details of our experimental\nsetup for the conversational question answering task.\n5.1. Baselines\nWe conduct experiments on different model sizes. First,\nto show the effectiveness of stage-2 context-enhanced in-\nstruction tuning, we compare against the Llama2-SFT-{7B,\n13B, 70B}, which is the Llama2-7B/13B/70B foundation\nmodel after the stage-1 supervised fine-tuning (SFT). Sec-\nond, we compare against Llama2-Chat-{7B, 13B, 70B}\nsince Llama2-Chat models are shown to possess strong in-\nstruction following and conversational QA capabilities (Tou-\nvron et al., 2023). Aside from Llama2 models, we also\nconduct experiments on our in-house GPT-{8B, 22B} foun-\ndation models, which are pretrained with 3.5 trillion to-\nkens, and we compare against their stage-1 SFT baselines\nGPT-SFT-{8B, 22B}. Finally, we compare against two\nvery strong OpenAI models: GPT-3.5-turbo-0613 (4k) and\nGPT-4-0613 (8k). For fair comparison, when retrieval is\nneeded, we use the same top-k retrieved chunks from our\nbest retriever as the context for all baselines and our ChatQA\nmodels. Note that we have carefully tuned the instructions\nfor all the baselines to ensure they achieve as good as possi-\nble results.3\n5.2. Evaluation Benchmarks\n5.2.1. LONG DOCUMENT DATASETS\nWe collect five conversational QA datasets with long doc-\numents which cannot be directly fitted into LLMs with a\nsequence length of 4K tokens. Hence, we run our best multi-\nturn query retriever to get top-5 relevant chunks as the inputs\n(experiments can be found in \u00a74.3).\n\u2022 Doc2Dial (Feng et al., 2020) is a document-grounded\nconversational QA dataset covering four domains:\nDMV, SSA, VA, and Student Aid. Each sample com-\nprises a dialogue where a user poses queries regarding\nthe document, and an agent responds those questions.\nThe average document length is around 101K words.\n\u2022 QuAC (Choi et al., 2018) is based on Wikipedia docu-\n3The prompts for these baselines can be found in Appendix C.\nments. Originally, the document is short. Since each\ndialogue is linked to multiple Wikipedia URLs, we\nextract text from these links to increase the document\nsize to approximately an average of 15K words. It\ncontains unanswerable cases where answers cannot be\nfound within the given context.\n\u2022 QReCC (Anantha et al., 2021) is an open-domain con-\nversational QA dataset across multiple sources. Similar\nto QuAC, each dialogue also has corresponding URLs.\nWe extract text from those URLs to construct the docu-\nments. In the end, the average document size is around\n5K words, with a maximum document size of 20K\nwords.\n\u2022 TopiOCQA (Adlakha et al., 2022) is grounded on the\nwhole Wikipedia. It incorporates topic switching and\nrequires the agent to search the entire Wikipedia for\nanswers to user questions.\n\u2022 INSCIT (Wu et al., 2023) is also grounded on the whole\nWikipedia. It studies the case where user questions are\nunder-specified and require clarification.\nFor Doc2Dial, QuAC, and QReCC, we segment documents\ninto around 300-word chunks, and we retrieve top-5 relevant\nchunks as context for each user question. For TopioCQA\nand INSCIT, we follow their original segmentation, result-\ning in smaller chunks. Hence, we retrieved top-20 chunks\nto obtain similar context length to the first three datasets.\n5.2.2. SHORT DOCUMENT DATASETS\nTo increase the diversity of document lengths, we collect\nfive conversational QA datasets with short documents (less\nthan 1.5K words). On average, 1 word will be tokenized\ninto 1.5 tokens. Hence, the document can be directly fitted\ninto LLMs with a sequence length of 4K tokens.\n\u2022 CoQA (Reddy et al., 2019) is a conversational QA\ndataset with each dialogue grounded on a short pas-\nsage. The answers are generally short, and the passages\ncover a wide range of domains like children\u2019s stories,\nliterature, mid/high school exams, news, Wikipedia.\n\u2022 DoQA (Campos et al., 2020) covers three domains:\ncooking, travel, and movies collected from active Stack\nExchange4 forums. The dataset contains unanswerable\ncases where answers cannot be found within the given\ndocument.\n\u2022 ConvFinQA (Chen et al., 2022a) is based on the Finan-\ncial domain. Each document contains a single financial\nreport table along with relevant text surrounding the\n4https://stackexchange.com/\n6\nChatQA: Building GPT-4 Level Conversational QA Models\nModels\nAverage\nDoc2Dial\nQuAC\nQReCC\nCoQA\nDoQA\nConvFinQA\nSQA\nTopiOCQA\nHybridDial\nINSCIT\nLlama2-7B-SFT\n34.81\n30.26\n19.21\n37.55\n62.75\n21.76\n34.43\n32.18\n32.88\n48.96\n28.16\nLlama2-7B-Chat\n38.86\n33.27\n25.83\n46.02\n72.28\n33.15\n36.58\n26.14\n36.68\n47.02\n31.67\nChatQA-7B\n47.71\n37.88\n29.69\n46.97\n76.61\n41.57\n51.61\n61.87\n45.45\n54.51\n30.96\nGPT-8B-SFT\n34.46\n31.03\n20.07\n37.69\n59.24\n21.72\n15.44\n40.06\n38.17\n52.29\n28.86\nChatQA-8B\n49.36\n36.76\n33.95\n45.54\n77.90\n44.65\n61.68\n60.74\n47.03\n53.81\n31.50\nLlama2-13B-SFT\n37.69\n30.68\n21.59\n38.25\n69.52\n21.70\n41.14\n37.85\n35.26\n52.22\n28.73\nLlama2-13B-Chat\n40.34\n34.74\n27.89\n47.19\n72.50\n32.60\n41.54\n25.39\n39.25\n49.82\n32.52\nChatQA-13B\n50.86\n38.05\n34.28\n48.06\n77.23\n43.31\n65.44\n66.41\n48.88\n56.19\n30.79\nGPT-22B-SFT\n38.83\n32.98\n22.83\n39.92\n68.19\n22.83\n30.54\n47.01\n38.07\n54.22\n31.66\nChatQA-22B\n53.53\n39.71\n36.89\n50.47\n80.32\n44.88\n72.08\n69.08\n48.83\n58.20\n34.11\nLlama2-70B-SFT\n43.22\n34.42\n25.65\n41.88\n73.04\n28.21\n46.64\n58.90\n37.20\n55.52\n30.71\nLlama2-70B-Chat\n45.21\n36.87\n32.47\n49.40\n80.41\n38.97\n46.85\n37.62\n44.31\n50.35\n34.88\nChatQA-70B\n54.14\n38.90\n41.82\n48.05\n78.57\n51.94\n73.69\n69.14\n50.98\n56.44\n31.90\n- SyntheticConvQA\u2662\n54.08\n39.19\n38.33\n48.73\n79.83\n48.65\n76.44\n68.63\n51.30\n55.68\n33.98\n- w/o stage-1\u2020\n52.18\n38.43\n37.52\n46.08\n73.51\n49.42\n72.15\n72.08\n51.28\n50.74\n30.56\n- w/o single-turn*\n52.25\n38.30\n37.89\n47.08\n76.74\n46.43\n72.42\n67.41\n49.85\n53.16\n33.18\n- w/o ConvQAData\u25b3\n48.97\n35.47\n28.19\n41.68\n75.80\n31.68\n73.83\n68.05\n45.90\n58.40\n30.70\nGPT-3.5-turbo (4k)\n50.37\n34.83\n37.17\n50.46\n79.33\n41.11\n73.15\n60.63\n44.30\n47.42\n35.27\nGPT-4 (8k)\n53.90\n34.16\n40.29\n52.01\n77.42\n43.39\n81.28\n79.21\n45.09\n49.81\n36.34\nTable 2. Zero-shot conversational QA results across 10 datasets.\n\u2662 denotes that the HumanAnnotatedConvQA is replaced with the\nSyntheticConvQA. \u2020 denotes that the stage-1 (SFT) is removed and only ChatQA stage-2 tuning is applied. *denotes that the single-\nturn QA datasets are removed from the ChatQA stage-2 training blends. In both \u201cw/o stage-1\u201d and \u201cw/o single-turn\u201d settings, the\nSyntheticConvQA data is used. In terms of average scores, our ChatQA models greatly surpass SFT and Chat counterparts. Our best\nmodel, ChatQA-70B, slightly outperforms GPT-4. \u25b3 denotes that the HumanAnnotatedConvQA is removed from the ChatQA stage-2\ntraining blends.\ntable. This dataset involves arithmetic calculation and\ncomplex numerical reasoning.\n\u2022 SQA (Pasupat & Liang, 2015) is grounded on docu-\nments which contain only a single Table without any\nsurrounding text. The documents are collected from\nWikipedia, and the questions are highly compositional,\nwhich requires the model with robust table comprehen-\nsion ability to give correct answers.\n\u2022 HybridDial (Nakamura et al., 2022) is a conversational\nQA dataset grounded on documents containing both\nWikipedia tabular and textual data. The questions are\ncomplex which requires reasoning over the documents.\nOver all the 10 datasets, ConvFinQA, SQA, and Hybrid-\nDial datasets contain tabular data in the documents, while\ndocuments of the remaining datasets are text-only.5\n5.3. Evaluation Metrics\nGiven that F1 score is the most commonly used automatic\nmetric to assess QA models, we use it for all datasets ex-\ncept for ConvFinQA. In ConvFinQA, we follow Chen et al.\n(2022a) to use exact match metric since the answers in Con-\nvFinQA are about extracting numbers from documents as\n5Details of these benchmark datasets are in the Appendix C.\nwell as arithmetic calculations. Hence, the answer only\nmakes sense when it is exactly the same as the answer.\nWhen models generate the arithmetic formula, we will cal-\nculate its final result based on a calculator and compare it\nwith the gold answer.\nIn addition, we also conduct human evaluations to assess the\ncorrectness of generated answers between our best model\nand GPT-4.\n6. Results\n6.1. Main Results\n6.1.1. OVERVIEW\nIn Table 2, we compare different model variants and OpenAI\nmodels across 10 conversational QA datasets.\nWe find that our ChatQA method greatly improves the\nmodel\u2019s conversational QA capability. In terms of averaged\nscores, Llama2-Chat models surpass SFT model counter-\nparts by a small margin, while our ChatQA models achieve\naround or over 10 point of absolute improvement over SFT\nand Chat counterparts. For example, ChatQA-13B improves\non Llama2-13B-SFT and Llama2-13B-Chat by 13.17 (from\n37.69 to 50.86) and 10.52 (from 40.34 to 50.86), respec-\ntively. This is because context-enhanced instruction fine-\n7\nChatQA: Building GPT-4 Level Conversational QA Models\ntuning enables the model to learn how to effectively extract\nuseful information from retrieved or relevant context.\nCompared to OpenAI models, our best model ChatQA-70B\nsurpasses GPT-3.5-turbo by 3.77 average score and can\noutperform GPT-4 by 0.24 average score. In addition, a\nmuch smaller size of ChatQA-13B is able to marginally\noutperform GPT-3.5-turbo by an average score of 0.49.\nNotably, our proposed stage-2 instruction tuning brings a\nsignificant boost to GPT-22B-SFT by 14.70 average score.\nAs a result, ChatQA-22B is only behind GPT-4 by a small\ngap (53.53 vs. 53.90), despite having a considerably smaller\nmodel size than Llama2-70B. This highlights the efficiency\nof the proposed instruction tuning method.\n6.1.2. IMPORTANCE OF STAGE-1 SFT\nIn Table 2, we conduct an ablation study on the importance\nof stage-1 SFT, which enhances the instruction-following\ncapability of ChatQA. We remove the stage-1 SFT from the\nfine-tuning stages and only apply stage-2 context-enhanced\ninstruction tuning on top of foundation LLM. We find that\nthe average score drops 1.9 (from 54.08 to 52.18). Aside\nfrom the SQA, removing stage-1 makes the model consis-\ntently perform worse on other datasets. Results indicate that\nstage-1 still plays an important role, even through all of SFT\ndatasets of stage-1 are blended in stage-2 instruction tuning\nas well. We conclude that building instruction-following\ncapability first is beneficial for stage-2 tuning.\n6.1.3. EFFECTIVENESS OF SINGLE-TURN DATA\nTo investigate how single-turn QA datasets affect model\u2019s\nmulti-turn QA capability, we conduct an ablation study by\nremoving them from the ChatQA-70B training blends in\nstage-2. As shown in Table 2, incorporating single-turn QA\ndatasets in the stage-2 training blends (ChatQA-70B) gener-\nally make the scores increase across all benchmark datasets,\nleading to an average improvement of 1.83 score. Interest-\ningly, we observe improvement in ConvFinQA, SQA, and\nHybridDial (table-based datasets), despite the added single-\nturn QA datasets not having tabular data in the documents.\nThese results align with our intuitions. Adding single-turn\ndatasets improves the model\u2019s capability to extract answers\nfrom context, resulting in better scores in conversational QA\ndatasets.\n6.1.4. EFFECTIVENESS OF CONVERSATIONAL QA DATA\nWe further explore the how conversational QA data af-\nfect the model\u2019s multi-turn QA capability by removing Hu-\nmanAnnotatedConvQA data from the ChatQA stage-2 train-\ning blends. As illustrated in Table 2, \u201cw/o ConvQAData\u201d\nmakes the results significantly worse than ChatQA-70B (av-\nerage scores degrading from 54.08 to 48.97). We observe\nOurs Win\nTie\nGPT-4 Win\nAverage\n13.81%\n69.09%\n17.10%\nDoc2Dial\n14.29%\n68.00%\n17.71%\nQuAC\n11.67%\n73.33%\n15.00%\nQReCC\n11.11%\n77.22%\n11.67%\nCoQA\n7.78%\n80.00%\n12.22%\nDoQA\n22.78%\n57.78%\n19.44%\nConvFinQA\n16.67%\n67.78%\n15.55%\nSQA\n11.11%\n61.67%\n27.22%\nTopiOCQA\n19.31%\n60.69%\n20.00%\nHybridDial\n7.78%\n78.33%\n13.89%\nINSCIT\n15.56%\n66.11%\n18.33%\nTable 3. Human evaluation (A/B testing) comparing our ChatQA-\n70B to GPT-4 over 10 datasets. In terms of average scores, our\nmodel and GPT-4 are tie most of the time (69.09%), and GPT-4\nachieves slightly higher win rate (\u223c3.3%) than ours.\nlarge degradation in datasets with text-based documents,\nsuch as QuAC, QReCC, and DoQA. However, the degra-\ndation in datasets with table-based documents (e.g., Con-\nvFinQA, SQA) are small, thanks to having TAT-QA (Zhu\net al., 2021) in the training blends.\nInterestingly, \u201cw/o ConvQAData\u201d can still outperform\nLlama2-70B-SFT (48.97 vs. 43.22) and Llama2-70B-Chat\n(48.97 vs. 45.21). This further demonstrates the effective-\nness of our proposed stage-2 tuning, which enhances the\nmodel\u2019s capability to handle the contextualized QA task.\n6.1.5. HUMAN ANNOTATED DATA VS. GPT-3.5-TURBO\nSYNTHETIC DATA\nIn Table 2, we also compare our ChatQA models using the\n7k GPT-3.5-Turbo synthetic dataset (SyntheticConvQA) and\nour collected 7k human-annotated dataset (HumanAnnotat-\nedConvQA). First, we find that both achieve comparable\nresults in terms of average scores, which suggests that we\ndo not need to rely on synthetic data from OpenAI mod-\nels to build the state-of-the-art conversational QA models.\nSecond, we find that using human-annotated data achieved\nsignificant improvements on QuAC and DoQA datasets.\nThis can be attributed to the fact that the human-annotated\ndata have higher quality on unanswerable cases which exists\nin QuAC and DoQA datasets. Eventually, it leads to the\noverall improvements on these two datasets. Detail results\nand analyses on unanswerable cases can be found in \u00a76.5.\n6.1.6. HUMAN EVALUATION\nDespite F1 scores being the most commonly used metrics\nfor evaluating the quality of QA models, there are often mul-\ntiple ways to answer questions, which makes the automatic\nmetrics less than perfect. Therefore, we use human evalua-\ntions to further compare our ChatQA-70B with GPT-4. In\nthis human evaluation, we ask annotators to verify the facts\n8\nChatQA: Building GPT-4 Level Conversational QA Models\nModels\nAvg-text\nAvg-table\nAvg-ret\nAvg-nonret\nChatQA-13B\n45.80\n62.68\n40.01\n61.72\nChatQA-70B\n48.88\n66.42\n42.33\n65.96\nGPT-3.5-turbo (4k)\n46.07\n60.40\n40.41\n60.33\nGPT-4 (8k)\n46.96\n70.10\n41.58\n66.22\nTable 4. Fine-grained studies on average scores of different dataset\ntypes. Avg-text covers datasets where the documents only have\ntext, including Doc2Dial, QuAC, QReCC, CoQA, DoQA, Topi-\nOCQA, and INSCIT. Avg-table covers datasets with table in the\ndocuments, including ConvFinQA, SQA, and HybridDial. Avg-ret\ncovers datasets with long documents requiring retrieval, including\nDoc2Dial, QuAC, QReCC, TopiOCQA, and INSCIT. Avg-nonret\ncovers datasets with short documents which do not require retrieval,\nincluding CoQA, DoQA, ConvFinQA, SQA, and HybridDial.\nin ChatQA-70B and GPT-4\u2019s outputs and determine which\nmodel provides a more accurate response to the question 6.\nThe human evaluation results over the 10 datasets are shown\nin Table 3. We first find that our ChatQA-70B and GPT-\n4 are tie most of the time (69.09%), and GPT-4 achieves\nslightly higher win rate (\u223c3.3%) than ours. This further con-\nfirms our model has powerful capability to produce correct\nanswers. Second, we find that our model achieves slightly\nbetter win rate than GPT-4 in ConvFinQA, which indicates\nthe strong arithmetic calculation ability of our model. Third,\nwe find that GPT-4 achieves significantly better win rate in\nSQA task, which suggests that there is still a gap between\nour model and GPT-4 on tabular reasoning task.\n6.2. Fine-grained Analyses\nIn Table 4, we further compared our models and OpenAI\nmodels across different dataset types in our conversational\nQA benchmarks. In the comparison between ChatQA-70B\nand GPT-4, ChatQA-70B achieves better results in text-\nonly documents (avg-text), demonstrating its superior text\nunderstanding ability. Meanwhile, GPT-4 shows better QA\ncapability in tabular data given the comparisons in avg-\ntable. Regarding the datasets that require or do not require\nretrieval, ChatQA-70B and GPT-4 are comparable (gaps are\nwithin an average score of 1).\nIn the comparison between ChatQA-13B and GPT-3.5-turbo,\nChatQA-13B demonstrates better tabular QA capability,\ngiven the scores in Avg-table. ChatQA-13B also shows\nbetter scores in datasets where documents do not need re-\ntrieval, while being on par with GPT-3.5-turbo in text-only\ndocuments and documents that need retrieval.\n6.3. Top-k Chunks for Stage-2 Instruction Tuning\nFor all the datasets used in stage-2 tuning, the context is\nprovided as continuous paragraphs or documents containing\n6More human evaluation setup can be found in the Appendix D.\nModels\nAvg-CQA\nAvg-ret\nAvg-nonret\nChatQA-70B\n54.14\n42.33\n65.96\n- w/ \u201ctop-5\u201d chunks\n54.04\n42.91\n65.16\nTable 5. Ablation study on using \u201ctop-5\u201d retrieved chunk as the\ncontext for the stage-2 instruction tuning. We report average scores\non all 10 conversational QA datasets (Avg-CQA), five retrieval\ndatasets (Avg-ret) and five non-retrieval datasets (Avg-nonret).\nthe answer. In contrast, the model needs to handle the top-\nk retrieved chunks during inference for long documents.\nTo address this train/test mismatch, we investigate whether\nreplacing some continuous paragraphs with retrieved top-k\nchunks will enhance the model\u2019s robustness.\nWe use NarrativeQA for this study, since each question has a\ncorresponding long document. Initially, we use a summary\nof the long document as the context, which contains the\nanswer. To incorporate discontinuous contexts, we first cut\nthe long document into 300-word chunks. Then, we use\nthe Dragon retriever to retrieve the top-4 chunks related to\nthe question as additional context. Finally, we consider the\nretrieved four chunks along with the summary of the long\ndocument as the \u201ctop-5\u201d chunks. 7 We use this reconstructed\nNarrativeQA to replace the original one for the stage-2\ninstruction tuning.\nIn Table 5, we observe that using \u201ctop-5\u201d chunks as the\ncontext for training leads to improvements on the datasets\nthat require retrieval. But it degrades the performance on\nnon-retrieval datasets. Overall, these two models perform\ncomparable. It is because incorporating \u201ctop-5\u201d retrieved\nchunks in stage-2 tuning aligns with the inference stage\nwhere retrieval is needed, which improves the Avg-ret score.\nHowever, mixing continuous and discontinuous documents\ncould make the stage-2 tuning less stable, leading to sub-\noptimal results on non-retrieval datasets. We believe more\nfuture work can be conducted regarding the balance of in-\ncorporating continuous context and top-k retrieved chunks\nin stage-2 tuning.\n6.4. Ablation Studies for Inference Stage\nIn Table 7, we show ablation studies on how the number\nof retrieved context/chunks, context ordering, and different\nretrievers affect the conversational QA results.\nFirst, we find that using more contexts as inputs do not\nalways improve the results. Utilizing top-5 contexts as in-\nput yields better results compared to using either top-3 or\ntop-10 contexts. Intuitively, more contexts have higher prob-\n7Note that, we do not directly use top-5 retrieved chunks for\ntraining, because they may not contain the answer. In such cases,\nfine-tuning the model to generate answer could encourage halluci-\nnation.\n9\nChatQA: Building GPT-4 Level Conversational QA Models\nModels\nAvg-Both\nAvg-QuAC\nQuAC (no*)\nQuAC (yes*)\nAvg-DoQA\nDoQA (no*)\nDoQA (yes*)\nAvg-CQA\nChatQA-70B\n- 1k unanswerable\u2020\n76.88\n80.89\n75.10\n86.67\n72.88\n64.49\n81.26\n54.16\n- 1.5k unanswerable\u2020\n77.25\n80.76\n77.66\n83.85\n73.74\n68.81\n78.67\n54.14\n- 2k unanswerable\u2020\n77.10\n80.82\n77.59\n84.05\n73.38\n67.95\n78.80\n53.86\n- 2.5k unanswerable\u2020\n75.87\n78.81\n73.76\n83.85\n72.93\n66.54\n79.31\n53.78\n- SyntheticConvQA\u2662\n69.84\n72.92\n55.38\n90.42\n66.77\n45.09\n88.45\n54.08\nGPT-3.5-turbo (4k)\n73.27\n78.34\n61.91\n94.76\n68.21\n51.99\n84.43\n50.37\nGPT-4 (8k)\n80.73\n87.42\n83.45\n91.38\n74.05\n74.28\n73.82\n53.90\nTable 6. Accuracies on answerable and unanswerable samples across QuAC and DoQA datasets. Avg-Both is the averaged score between\nQuAC and DoQA, and Avg-CQA is the average score across the 10 conversational QA datasets. \u2662 denotes that the HumanAnnotated-\nConvQA is replaced with the SyntheticConvQA. *\u201cno\u201d denotes the unanswerable samples, while \u201cyes\u201d denotes the answerable samples.\n\u2020We conduct ablation studies in terms of the number of unanswerable samples in HumanAnnotatedConvQA. We use 1.5k unanswerable\nsamples for our final ChatQA-70B, as it produces both high-quality generation and less hallucination.\nModels\nAvg.\nD2D\nQuAC\nQReCC\nTopiO\nINSCIT\nChatQA-70B\n42.31\n39.19\n38.33\n48.73\n51.30\n33.98\n- # of ctx: top-3\n41.91\n37.20\n38.35\n48.94\n52.78\n32.27\n- # of ctx: top-10\n40.71\n37.06\n36.95\n47.61\n49.40\n32.53\n- ctx reverse ordering\n42.48\n39.08\n38.85\n49.63\n51.16\n33.69\n- ctx swing ordering\n42.30\n39.35\n38.09\n49.09\n50.98\n33.99\n- ctx random ordering\n42.01\n39.32\n38.28\n48.79\n50.13\n33.51\n- Dragon Retrieval\n40.50\n37.92\n38.44\n47.88\n50.39\n27.87\nTable 7. Ablation studies on input context across datasets that re-\nquire retrieval. All models use SyntheticConvQA. D2D denotes\nDoc2Dial, and TopiO denotes TopiOCQA. We study the number\nof contexts used in inputs (# of ctx), context ordering (reverse,\nswing, random), and the use of retrieved context from the origi-\nnal Dragon. In comparison, ChatQA-70B (default setting) uses\n\u201cDragon + Fine-tune\u201d to retrieve the top-5 contexts, and arranges\nthem sequentially from the first to the fifth context in top-5.\nability to contain correct answers (better recall scores). As\na result, using top-5 contexts achieves better results than\nusing top-3. However, as the number of contexts further\nincreases, the model may suffer from \u201clost in the middle\u201d\nphenomenon (Liu et al., 2023) and the difficulty of extract-\ning answers from the provided context could also increase,\nwhich leads to inferior results by using top-10 contexts.\nSecond, we study how using different orderings of top-5\ncontexts affects the results. We compare sequential ordering\n(from 1st context to 5th context) to reverse ordering (from\n5th to 1st context), swing ordering (given the \u201clost in the\nmiddle\u201d phenomenon, we arrange the most relevant context\nto appear at the beginning and the end of the input context.\nHence, the ordering becomes {1st, 3rd, 5th, 4th, 2nd}),\nand random ordering (random shuffle the top-5 contexts).\nWe find using sequential ordering is comparable to using\nreverse and swing orderings, and random shuffling is slightly\nworse. Results indicate that our model excels in extracting\nthe correct answer from lengthy contexts, regardless of the\nanswer\u2019s location. This is because, during the ChatQA fine-\ntuning, the answer\u2019s location is randomly distributed within\nthe context.\nThird, we observe that when we replace \u201cDragon + Fine-\ntune\u201d with the original non-finetuned Dragon retriever, the\naverage score drops by 1.81 (from 42.31 to 40.50). In addi-\ntion, the score drops significantly in INSCIT dataset (from\n33.98 to 27.87) due to the large performance gap between\nthe two retrievers (as shown in Table 1). Basically, when\nthe quality of retrieval improves, it directly enhances the\nperformance of question answering.\n6.5. Evaluation of Unanswerable Case\n6.5.1. EVALUATION SETUP\nIn this section, we study another aspect of the model\u2019s ca-\npability, which is to discern if a question can be answered\nwithin the provided context. Generating an answer in unan-\nswerable case will lead to hallucination. To allow this eval-\nuation, we require the model to indicate it when no answer\ncan be found in the given context.\nWe use QuAC and DoQA datasets which have such unan-\nswerable cases to evaluate such capability. Specifically, for\nunanswerable case, we consider the model indicating that\nthe question cannot be answered as correct8, and as for an-\nswerable cases, we consider the model not indicating the\nquestion is unanswerable as correct (i.e., the model giv-\ning an answer). Note that for answerable cases, we only\nselect the samples where correct context is retrieved. Our\nmodel checkpoints (ChatQA-70B w/ 1.5k unanswerable and\nChatQA-70B w/ SyntheticConvQA) are same ones reported\nin Table 2.\nIn the end, we calculate the average accuracy score of unan-\nswerable and answerable cases as the final metric. We\nconsider this average accuracy as a reliable metric since it is\n8We collect a set of heuristic matching patterns from all gener-\nated samples to determine if the model suggests that the question\nis unanswerable. More details can be found in the Appendix E.\n10\nChatQA: Building GPT-4 Level Conversational QA Models\nContext\nConversation\nAssistant Responses\nThe process for Board hearings has changed. If you\u2019re appealing a VA decision\nunder the new process, keep reading below to learn how to request and prepare\nfor a hearing. You\u2019ll follow the new process if you\u2019re : Appealing a VA decision\nyou received on or after February 19 , 2019. Participating in the Rapid Appeals\nModernization Program Opting into the new process using VA Form 10182 If\nyou re using VA Form 9 to appeal a VA decision you received before February\n19, 2019, go to the Board of Veterans\u2019 Appeals BVA Hearing page ... The Board\nof Veterans Appeals schedules hearings in docket order the order in which they\nreceive requests ...\nUser: What can I learn at my Board hearing?\nAssistant: Do you want to learn exactly what to expect?\n...\nUser: Is it necessary for me to consider requesting a Board hearing?\nAssistant: You can request a video conference hearing if you want\nto provide testimony to a judge about your appeal.\nUser: when will I follow the new process for a Board hearing?\nAssistant:\nChatQA-70B: If you\u2019re appealing a VA decision you received on or\nafter February 19, 2019, you\u2019ll follow the new process.\nGPT-4: You\u2019ll follow the new process for a Board hearing if you\u2019re\nappealing a VA decision you received on or after February 19, 2019,\nparticipating in the Rapid Appeals Modernization Program, or opting\ninto the new process using VA Form 10182.\nGround Truth: You will follow the new process if appealing a VA\ndecision if you received it on or after February 19, 2019.\ntitle: How did Arya survive this confrontation unscathed?\nThere was no time or intention to turn her into White Walker. Let us look at the\nscene from S04E04. A white walker brings a baby holding in his lap and puts it\non an ice table (or whatever it is) for the arrival of the Night King. ... When Arya\ndrops the dagger, Night King looks surprised. This shows he doesn\u2019t know what\nwould happen next. There was no time to think and turn her into a white walker\nby touch. There was no planning in this scene like S04E04. ...\nUser: How did Arya survive this confrontation unscathed?\nAssistant: Arya was quick enough to react, drop the dagger and\ndo the rest.\nUser: How old is the child that they lay on the altar of ice?\nAssistant:\nChatQA-70B: The child appeared to be a baby.\nGPT-4: The text does not provide information on the exact age of the\nchild that was laid on the ice altar.\nGround Truth: a baby\nMassif\nType of nature reserve\nPreserved area\nChornohora\nCarpathian Biosphere\n2476.8 ha\nUholka / Wide Meadow\nCarpathian Biosphere\n11860 ha\nSvydovets\nCarpathian Biosphere\n3030.5 ha\nMaramoros\nCarpathian Biosphere\n2243.6 ha\nRozok\nPresov Preserved\n67.1 ha\nJasmund\nJasmund National Park\n492.5 ha\n...\nUser: which of the forests is a carpathian biosphere reserve?\nAssistant: Chornohora, Uholka / Wide Meadow, Svydovets,\nMaramoros, Kuziy / Trybushany\nUser: which of those covers more than 3100 ha?\nAssistant:\nChatQA-70B: Uholka / Wide Meadow\nGPT-4: Chornohora, Uholka / Wide Meadow, Svydovets, Maramoros\nGround Truth: Uholka / Wide Meadow\ntitle: Summary of enemy aircraft damaged/destroyed (John B. England)\nDate\nLocation\nNumber\nType\nStatus\n24 April 1944\nMunich, Germany\n3\nMe-110\nDestroyed\n27 May 1944\nStrasbourg, France\n1\nMe-109\nDamaged\n18 August 1944\nParis, France\n0.5\nMe-109\nDestroyed\n13 September 1944\nNordhausen, Germany\n2.5\nMe-109\nDestroyed\n6 October 1944\nBerlin, Germany\n1\nMe-109\nDamaged\n6 October 1944\nBerlin, Germany\n2\nMe-109\nDestroyed\n27 November 1944\nMagdeburg, Germany\n4\nFW-190\nDestroyed\n14 January 1945\nBerlin, Germany\n1\nMe-109\nDestroyed\n...\nUser: what were all of the dates in which john b. england destroyed or\ndamaged enemy planes?\nAssistant: 24 April 1944, 27 May 1944, 18 August 1944, 13 September\n1944, 6 October 1944, 27 November 1944, 14 January 1945 ...\nUser: on which of those dates did john b. england destroy or damage\nmore than one enemy plane?\nAssistant:\nChatQA-70B: 6 October 1944, 13 September 1944, 27 November 1944\n(missing 24 April 1944)\nGPT-4: 24 April 1944, 13 September 1944, 6 October 1944,\n27 November 1944\nGround Truth: 24 April 1944, 13 September 1944, 6 October 1944,\n27 November 1944\nTable 8. Case studies on ChatQA-70B and GPT-4. We use blue color to highlight the relevant context for the last user question. And we\nuse green and red color to highlight the correct and incorrect parts in the assistant responses.\nin the same spirit of F1 metric which measures the harmonic\nmean of precision and recall scores.\n6.5.2. RESULTS\nIn Table 6, we compare our models with OpenAI mod-\nels across QuAC and DoQA datasets. First, we find that\ncompared to using SyntheticConvQA, using HumanAnno-\ntatedConvQA significantly increase the average accuracies\non both QuAC and DoQA. It is because the unanswerable\nannotations are in higher quality for human-annotated data,\nwhich significantly improve the accuracy for unanswerable\ncases. Second, OpenAI models show powerful capability in\nthis task, especially for GPT-4. Compared to them, our best\nmodel achieved a significantly better average accuracy than\nGPT-3.5-turbo, while we still has a slight gap compared to\nGPT-4 (around 3.5%). Third, we find that models achieving\nhigher accuracy on unanswerable samples tends to get lower\naccuracy on answerable samples, and vice versa. We spec-\nulate that when a model tends to be \u201caggressive\u201d and offer\nsomewhat relevant answers to those unanswerable questions,\nit will boost the accuracy for answerable cases, but reduces\naccuracy for unanswerable ones. Conversely, when a model\nis more \u201cconservative\u201d and strictly checks if the question\ncan be answered, it will result in the opposite effects.\nWe conduct ablation studies in terms of the number of unan-\nswerable samples in HumanAnnotatedConvQA. We find\nthat using a small amount of unanswerable samples (e.g.,\n1.5k) is able to achieve remarkable results on the unanswer-\nable evaluation, as well as the overall conversational QA\nscore (i.e., Avg-CQA). Interestingly, incorporating more\nunanswerable samples does not necessarily lead to higher\naccuracy scores in the unanswerable evaluation.\n6.6. Case Study\nIn Table 8, we show four examples of ChatQA-70B and\nGPT-4 outputs. More examples can be found in Appendix F.\nThe first example (about Board hearing) is a simple infor-\nmation seeking problem, and both ChatQA-70B and GPT-4\nanswer correctly. In the second example (about Arya), the\nmodel needs to find implicit information (highlighted in\nblue) for giving the answer. GPT-4 tends to be safe on\nproviding an answer, and it replies that the context does\nnot provide exact information about the age which is also\ncorrect.\nBoth the third and fourth examples require the model to have\ngood tabular understanding and reasoning ability. In the\nthird example (about Massif), ChatQA-70B gives correct\nanswer by comparing the size of preserved area against\n3100 ha, while GPT-4 fails to do so. In the fourth example\n(about John B. England), ChatQA-70B correctly lists three\ndates but misses one date, while GPT-4 correctly answers\nthe question.\n11\nChatQA: Building GPT-4 Level Conversational QA Models\n7. Conclusion\nIn this paper, we build a family of ChatQA models, vary-\ning in model sizes from 7B to 70B. Comprehensive eval-\nuations on 10 conversational QA datasets show that our\nbest ChatQA-70B model can remarkably outperform GPT-\n3.5-turbo and perform on par with GPT-4 without using\nany synthetic data from ChatGPT models. In addition, we\ndemonstrate that fine-tuning a single-turn query retriever\nusing our curated conversational QA data performs com-\nparably to the state-of-the-art LLM-based query rewriting\nmodel, without incurring extra computational time and po-\ntential API costs associated with rewriting. Furthermore, we\nshow that incorporating a small amount of \u201cunanswerable\u201d\nsamples can significantly enhance our model\u2019s capability\nto handle scenarios where answers are unavailable. The\nunanswerable case evaluation highlights that our best model\nChatQA-70B only has a slight gap compared to GPT-4.\nReferences\nAdlakha, V., Dhuliawala, S., Suleman, K., de Vries, H.,\nand Reddy, S. Topiocqa: Open-domain conversational\nquestion answering with topic switching. TACL, 2022.\nAliannejadi, M., Kiseleva, J., Chuklin, A., Dalton, J., and\nBurtsev, M. Building and evaluating open-domain di-\nalogue corpora with clarifying questions. In EMNLP,\n2021.\nAnantha, R., Vakulenko, S., Tu, Z., Longpre, S., Pulman, S.,\nand Chappidi, S. Open-domain question answering goes\nconversational via question rewriting. In NAACL, 2021.\nAnthropic. Introducing 100k context windows, 2023a.\nAnthropic. Introducing Claude, 2023b.\nBrabant, Q., Lecorv\u00b4e, G., and Barahona, L. M. R. Coqar:\nQuestion rewriting on coqa. In LREC, 2022.\nCampos, J. A., Otegi, A., Soroa, A., Deriu, J. M., Cieliebak,\nM., and Agirre, E. Doqa-accessing domain-specific faqs\nvia conversational qa. In ACL, 2020.\nChen, Z., Li, S., Smiley, C., Ma, Z., Shah, S., and Wang,\nW. Y. Convfinqa: Exploring the chain of numerical rea-\nsoning in conversational finance question answering. In\nEMNLP, 2022a.\nChen, Z., Zhao, J., Fang, A., Fetahu, B., Rokhlenko, O., and\nMalmasi, S. Reinforced question rewriting for conversa-\ntional question answering. In EMNLP, 2022b.\nChoi, E., He, H., Iyyer, M., Yatskar, M., Yih, W.-t., Choi, Y.,\nLiang, P., and Zettlemoyer, L. Quac: Question answering\nin context. In EMNLP, 2018.\nChu, Z., Chen, M., Chen, J., Wang, M., Gimpel, K., Faruqui,\nM., and Si, X. How to ask better questions? a large-scale\nmulti-domain dataset for rewriting ill-formed questions.\nIn AAAI, 2020.\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y.,\nFedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S.,\nWebson, A., Gu, S. S., Dai, Z., Suzgun, M., Chen, X.,\nChowdhery, A., Castro-Ros, A., Pellat, M., Robinson,\nK., Valter, D., Narang, S., Mishra, G., Yu, A., Zhao, V.,\nHuang, Y., Dai, A., Yu, H., Petrov, S., Chi, E. H., Dean,\nJ., Devlin, J., Roberts, A., Zhou, D., Le, Q. V., and Wei,\nJ. Scaling instruction-finetuned language models. arXiv\npreprint arXiv: 2210.11416, 2022.\nConover, M., Hayes, M., Mathur, A., Meng, X., Xie, J.,\nWan, J., Shah, S., Ghodsi, A., Wendell, P., Zaharia, M.,\net al. Free dolly: Introducing the world\u2019s first truly open\ninstruction-tuned llm, 2023a.\nConover, M., Hayes, M., Mathur, A., Xie, J., Wan, J., Shah,\nS., Ghodsi, A., Wendell, P., Zaharia, M., and Xin, R. Free\nDolly: Introducing the world\u2019s first truly open instruction-\ntuned llm, 2023b.\nDai, Z., Chaganty, A. T., Zhao, V., Amini, A., Green, M.,\nRashid, Q., and Guu, K. Dialog inpainting: Turning\ndocuments to dialogs. In ICML, 2022.\nDasigi, P., Liu, N. F., Marasovi\u00b4c, A., Smith, N. A., and Gard-\nner, M. Quoref: A reading comprehension dataset with\nquestions requiring coreferential reasoning. In EMNLP,\n2019.\nDel Tredici, M., Barlacchi, G., Shen, X., Cheng, W., and\nde Gispert, A. Question rewriting for open-domain con-\nversational qa: Best practices and limitations. In CIKM,\n2021.\nDeng, Y., Lei, W., Zhang, W., Lam, W., and Chua, T.-\nS. Pacific: Towards proactive conversational question\nanswering over tabular and textual data in finance. In\nEMNLP, 2022.\nDu, Z., Qian, Y., Liu, X., Ding, M., Qiu, J., Yang, Z., and\nTang, J. Glm: General language model pretraining with\nautoregressive blank infilling. In ACL, 2022.\nDua, D., Wang, Y., Dasigi, P., Stanovsky, G., Singh, S., and\nGardner, M. Drop: A reading comprehension benchmark\nrequiring discrete reasoning over paragraphs. In NAACL,\n2019.\nElgohary, A., Peskov, D., and Boyd-Graber, J. Can you\nunpack that? learning to rewrite questions-in-context. In\nEMNLP, 2019.\n12\nChatQA: Building GPT-4 Level Conversational QA Models\nFan, A., Jernite, Y., Perez, E., Grangier, D., Weston, J., and\nAuli, M. Eli5: Long form question answering. In ACL,\n2019.\nFeng, S., Wan, H., Gunasekara, C., Patel, S., Joshi, S., and\nLastras, L. doc2dial: A goal-oriented document-grounded\ndialogue dataset. In EMNLP, 2020.\nGalimzhanova, E., Muntean, C. I., Nardini, F. M., Perego,\nR., and Rocchietti, G.\nRewriting conversational ut-\nterances with instructed large language models.\nIn\nIEEE/WIC International Conference on Web Intelligence\nand Intelligent Agent Technology, 2023.\nGao, C., Zhang, W., and Lam, W. Unigdd: A unified gen-\nerative framework for goal-oriented document-grounded\ndialogue. In ACL, 2022.\nGao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y.,\nSun, J., and Wang, H. Retrieval-augmented generation\nfor large language models: A survey. arXiv preprint\narXiv:2312.10997, 2023.\nGoogle. Introducing bard, 2023.\nGuo, M., Zhang, M., Reddy, S., and Alikhani, M. Abg-\ncoqa: Clarifying ambiguity in conversational question\nanswering. In AKBC, 2021.\nHonovich, O., Scialom, T., Levy, O., and Schick, T. Unnat-\nural instructions: Tuning language models with (almost)\nno human labor. arXiv preprint arXiv:2212.09689, 2022.\nIshii, E., Xu, Y., Cahyawijaya, S., and Wilie, B. Can ques-\ntion rewriting help conversational question answering?\nIn Proceedings of the Third Workshop on Insights from\nNegative Results in NLP, 2022.\nIyer, S., Lin, X. V., Pasunuru, R., Mihaylov, T., Simig,\nD., Yu, P., Shuster, K., Wang, T., Liu, Q., Koura, P. S.,\net al. Opt-iml: Scaling language model instruction meta\nlearning through the lens of generalization. arXiv preprint\narXiv:2212.12017, 2022.\nIzacard, G. and Grave, \u00b4E. Leveraging passage retrieval with\ngenerative models for open domain question answering.\nIn Proceedings of the 16th Conference of the European\nChapter of the Association for Computational Linguistics,\n2021.\nIzacard, G., Caron, M., Hosseini, L., Riedel, S., Bojanowski,\nP., Joulin, A., and Grave, E. Unsupervised dense infor-\nmation retrieval with contrastive learning. Transactions\non Machine Learning Research, 2022.\nKim, H., Hessel, J., Jiang, L., Lu, X., Yu, Y., Zhou,\nP., Bras, R. L., Alikhani, M., Kim, G., Sap, M.,\net al.\nSoda: Million-scale dialogue distillation with\nsocial commonsense contextualization. arXiv preprint\narXiv:2212.10465, 2022.\nKo\u02c7cisk`y, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann,\nK. M., Melis, G., and Grefenstette, E. The narrativeqa\nreading comprehension challenge. TACL, 2018.\nK\u00a8opf, A., Kilcher, Y., von R\u00a8utte, D., Anagnostidis, S.,\nTam, Z.-R., Stevens, K., Barhoum, A., Duc, N. M., Stan-\nley, O., Nagyfi, R., et al. Openassistant conversations\u2013\ndemocratizing large language model alignment. arXiv\npreprint arXiv:2304.07327, 2023.\nK\u00a8opf, A., Kilcher, Y., von R\u00a8utte, D., Anagnostidis, S., Tam,\nZ.-R., Stevens, K., Barhoum, A., Duc, N. M., Stanley,\nO., Nagyfi, R., ES, S., Suri, S., Glushkov, D., Dantu-\nluri, A., Maguire, A., Schuhmann, C., Nguyen, H., and\nMattick, A. Openassistant conversations - democratizing\nlarge language model alignment. arXiv preprint arXiv:\n2304.07327, 2023.\nLin, K., Tafjord, O., Clark, P., and Gardner, M. Reason-\ning over paragraph effects in situations. In Proceedings\nof the 2nd Workshop on Machine Reading for Question\nAnswering, 2019.\nLin, S.-C., Asai, A., Li, M., Oguz, B., Lin, J., Mehdad, Y.,\nYih, W.-t., and Chen, X. How to train your dragon: Di-\nverse augmentation towards generalizable dense retrieval.\narXiv preprint arXiv:2302.07452, 2023a.\nLin, X. V., Chen, X., Chen, M., Shi, W., Lomeli, M., James,\nR., Rodriguez, P., Kahn, J., Szilvasy, G., Lewis, M.,\net al. Ra-dit: Retrieval-augmented dual instruction tuning.\narXiv preprint arXiv:2310.01352, 2023b.\nLiu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilac-\nqua, M., Petroni, F., and Liang, P. Lost in the middle:\nHow language models use long contexts. arXiv preprint\narXiv:2307.03172, 2023.\nLongpre, S., Hou, L., Vu, T., Webson, A., Chung, H. W.,\nTay, Y., Zhou, D., Le, Q. V., Zoph, B., Wei, J., et al. The\nflan collection: Designing data and methods for effec-\ntive instruction tuning. arXiv preprint arXiv:2301.13688,\n2023.\nMele, I., Muntean, C. I., Nardini, F. M., Perego, R., Tonel-\nlotto, N., and Frieder, O. Adaptive utterance rewriting for\nconversational search. Information Processing & Man-\nagement, 2021.\nMishra, S., Khashabi, D., Baral, C., and Hajishirzi, H. Cross-\ntask generalization via natural language crowdsourcing\ninstructions. In ACL, 2022.\nMo, F., Mao, K., Zhu, Y., Wu, Y., Huang, K., and Nie, J.-Y.\nConvgqr: Generative query reformulation for conversa-\ntional search. arXiv preprint arXiv:2305.15645, 2023.\n13\nChatQA: Building GPT-4 Level Conversational QA Models\nMuennighoff, N., Wang, T., Sutawika, L., Roberts, A., Bi-\nderman, S., Scao, T. L., Bari, M. S., Shen, S., Yong, Z.-X.,\nSchoelkopf, H., et al. Crosslingual generalization through\nmultitask finetuning. arXiv preprint arXiv:2211.01786,\n2022.\nNakamura, K., Levy, S., Tuan, Y.-L., Chen, W., and Wang,\nW. Y. Hybridialogue: An information-seeking dialogue\ndataset grounded on tabular and textual data. In Findings\nof ACL, 2022.\nNguyen, T., Rosenberg, M., Song, X., Gao, J., Tiwary, S.,\nMajumder, R., and Deng, L. Ms marco: A human gen-\nerated machine reading comprehension dataset. choice,\n2016.\nOpenAI. Introducing ChatGPT, 2022.\nOpenAI. GPT-4, 2023.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\net al. Training language models to follow instructions\nwith human feedback. NeurIPS, 2022.\nPasupat, P. and Liang, P. Compositional semantic parsing\non semi-structured tables. In ACL, 2015.\nQu, C., Yang, L., Chen, C., Qiu, M., Croft, W. B., and Iyyer,\nM. Open-retrieval conversational question answering. In\nSIGIR, 2020.\nRajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. Squad:\n100,000+ questions for machine comprehension of text.\nIn EMNLP, 2016.\nRajpurkar, P., Jia, R., and Liang, P. Know what you don\u2019t\nknow: Unanswerable questions for squad. In ACL, 2018.\nRaposo, G., Ribeiro, R., Martins, B., and Coheur, L. Ques-\ntion rewriting? assessing its importance for conversa-\ntional question answering. In ECIR, 2022.\nReddy, S., Chen, D., and Manning, C. D. Coqa: A conver-\nsational question answering challenge. TACL, 2019.\nSaeidi, M., Bartolo, M., Lewis, P., Singh, S., Rockt\u00a8aschel,\nT., Sheldon, M., Bouchard, G., and Riedel, S. Interpreta-\ntion of natural language rules in conversational machine\nreading. In EMNLP, 2018.\nSanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L.,\nAlyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja,\nA., et al. Multitask prompted training enables zero-shot\ntask generalization. In ICLR, 2022.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., et al. Llama 2: Open foundation and fine-\ntuned chat models. arXiv preprint arXiv:2307.09288,\n2023.\nTrischler, A., Wang, T., Yuan, X., Harris, J., Sordoni, A.,\nBachman, P., and Suleman, K. Newsqa: A machine com-\nprehension dataset. In Proceedings of the 2nd Workshop\non Representation Learning for NLP, 2017.\nVakulenko, S., Longpre, S., Tu, Z., and Anantha, R. Ques-\ntion rewriting for conversational question answering. In\nWSDM, 2021a.\nVakulenko, S., Voskarides, N., Tu, Z., and Longpre, S. A\ncomparison of question rewriting methods for conversa-\ntional passage retrieval. In ECIR, 2021b.\nWang, B., Ping, W., McAfee, L., Xu, P., Li, B., Shoeybi,\nM., and Catanzaro, B.\nInstructretro: Instruction tun-\ning post retrieval-augmented pretraining. arXiv preprint\narXiv:2310.07713, 2023a.\nWang, L., Yang, N., Huang, X., Jiao, B., Yang, L.,\nJiang, D., Majumder, R., and Wei, F. Text embeddings\nby weakly-supervised contrastive pre-training.\narXiv\npreprint arXiv:2212.03533, 2022a.\nWang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A.,\nKhashabi, D., and Hajishirzi, H. Self-instruct: Aligning\nlanguage model with self generated instructions. arXiv\npreprint arXiv:2212.10560, 2022b.\nWang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y.,\nMirzaei, A., Naik, A., Ashok, A., Dhanasekaran, A. S.,\nArunkumar, A., Stap, D., et al. Super-naturalinstructions:\nGeneralization via declarative instructions on 1600+ nlp\ntasks. In EMNLP, 2022c.\nWang, Y., Ivison, H., Dasigi, P., Hessel, J., Khot, T., Chandu,\nK. R., Wadden, D., MacMillan, K., Smith, N. A., Beltagy,\nI., et al. How far can camels go? exploring the state\nof instruction tuning on open resources. arXiv preprint\narXiv:2306.04751, 2023b.\nWei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester,\nB., Du, N., Dai, A. M., and Le, Q. V. Finetuned language\nmodels are zero-shot learners. In ICLR, 2022a.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F.,\nChi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought\nprompting elicits reasoning in large language models.\nNeurIPS, 2022b.\nWu, Z., Luan, Y., Rashkin, H., Reitter, D., Hajishirzi, H.,\nOstendorf, M., and Tomar, G. S. Conqrr: Conversational\nquery rewriting for retrieval with reinforcement learning.\nIn EMNLP, 2022.\n14\nChatQA: Building GPT-4 Level Conversational QA Models\nWu, Z., Parish, R., Cheng, H., Min, S., Ammanabrolu, P.,\nOstendorf, M., and Hajishirzi, H. Inscit: Information-\nseeking conversations with mixed-initiative interactions.\nTACL, 2023.\nXu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao,\nC., and Jiang, D. Wizardlm: Empowering large language\nmodels to follow complex instructions. arXiv preprint\narXiv:2304.12244, 2023a.\nXu, P., Ping, W., Wu, X., McAfee, L., Zhu, C., Liu, Z., Sub-\nramanian, S., Bakhturina, E., Shoeybi, M., and Catanzaro,\nB. Retrieval meets long context large language models.\narXiv preprint arXiv:2310.03025, 2023b.\nYe, F., Fang, M., Li, S., and Yilmaz, E. Enhancing conver-\nsational search: Large language model-aided informative\nquery rewriting. In EMNLP, pp. 5985\u20136006, 2023.\nYu, S., Liu, J., Yang, J., Xiong, C., Bennett, P., Gao, J.,\nand Liu, Z. Few-shot generative conversational query\nrewriting. In SIGIR, 2020.\nZhang, H., Diao, S., Lin, Y., Fung, Y. R., Lian, Q., Wang, X.,\nChen, Y., Ji, H., and Zhang, T. R-tuning: Teaching large\nlanguage models to refuse unknown questions. arXiv\npreprint arXiv:2311.09677, 2023a.\nZhang, S., Dong, L., Li, X., Zhang, S., Sun, X., Wang, S.,\nLi, J., Hu, R., Zhang, T., Wu, F., et al. Instruction tuning\nfor large language models: A survey. arXiv preprint\narXiv:2308.10792, 2023b.\nZhou, C., Liu, P., Xu, P., Iyer, S., Sun, J., Mao, Y., Ma, X.,\nEfrat, A., Yu, P., Yu, L., et al. Lima: Less is more for\nalignment. arXiv preprint arXiv:2305.11206, 2023.\nZhu, F., Lei, W., Huang, Y., Wang, C., Zhang, S., Lv, J.,\nFeng, F., and Chua, T.-S. Tat-qa: A question answering\nbenchmark on a hybrid of tabular and textual content in\nfinance. In ACL, 2021.\n15\nChatQA: Building GPT-4 Level Conversational QA Models\nA. ChatQA Instruction Tuning\nA.1. Stage-1: Supervised Fine-tuning\nThe format template of LLM inputs in stage-1 is as follows:\nSystem: This is a chat between a user and an artificial intelligence assistant.\nThe assistant gives helpful, detailed, and polite answers to the user\u2019s questions\nbased on the context. The assistant should also indicate when the answer cannot be\nfound in the context.\nUser: {Question 1}\nAssistant: {Answer 1}\n...\nUser: {Latest Question}\nAssistant:\nWe use the {Latest Answer} from Assistant as the supervision for model outputs.\nA.2. Stage-2: Context-Enhanced Instruction Tuning\nBased on the stage-1 format template, the LLM inputs in stage-2 adds {Context for Latest Question} and\n{Instruction} from User, as depicted below:\nSystem: This is a chat between a user and an artificial intelligence assistant.\nThe assistant gives helpful, detailed, and polite answers to the user\u2019s questions\nbased on the context. The assistant should also indicate when the answer cannot be\nfound in the context.\n{Context for Latest Question}\nUser: {Instruction} + {Question 1}\nAssistant: {Answer 1}\n...\nUser: {Latest Question}\nAssistant:\nSame as Stage-1, we use the {Latest Answer} from Assistant as the supervision for model outputs.\nAs for the {Instruction}, we use different instructions for different datasets based on the answer types. Details can be\nfound below:\n\u201cPlease give a full and complete answer for the question.\u201d This is for datasets with long an-\nswers. We use it for the HumanAnnotatedConvQA or SyntheticConvQA.\n\u201cAnswer the following question with a short span.\nThe answer needs to be just in\na few words.\u201d This is for datasets with short answers. We use it for SQuAD1.1, SQuAD2.0, NarrativeQA, DROP,\nROPES, NewsQA, and Quoref.\n\u201cAnswer the following question with a number from context or the math arithmetic\n16\nChatQA: Building GPT-4 Level Conversational QA Models\nusing +, -, *, or /.\u201d This is for datasets using arithmetic calculation or extracting numbers from the context. We\nuse it for the TAT-QA dataset where questions need the arithmetic calculation or extracting numbers from the context.\n\u201cAnswer the following question with a short span, or a full and complete answer.\u201d\nThis is for datasets with both short and long answers. The model needs to tell whether to generate short or long answers\nbased on the questions. We use it for the TAT-QA dataset where questions do not need the arithmetic calculation, since we\nobserve the lengths of the corresponding answers can be both short and long.\nFor the stage-1 SFT data used in stage-2 instruction tuning, we keep the same format template as in stage-1. In other words,\nthe {Context for Latest Question} and {Instruction} are empty.\nA.3. Prompts for Synthetic Data Generation\nWe use a two-step prompting on GPT-3.5-turbo for obtaining a high-quality multi-turn QA conversation. In step-1,\nwe prompt GPT-3.5-turbo to generate a multi-turn QA conversation based on a given document. However, we find that the\ngenerated conversation lacks pronouns or common nouns to refer back to words or phrases in dialogue history. Hence, in\nstep-2, we prompt GPT-3.5-turbo to revise the generated conversation by adding pronouns or common nouns. For both\nstages, we give GPT-3.5-turbo few-shot examples to ensure a high-quality generation.\nThe in-context few-shot prompt in step-1 is as follows (aims to generate multi-turn QA conversation):\nYou are a helpful, pattern-following assistant.\nrole-user:\nPersonA has access to information in the following passage:\n###There are certain important aspects of college preparation. The student preparing\nfor college should define their goals and interests. The student should also plan\nfinancially and academically. We have a checklist that helps students get ready\nfinancially and academically.###\nGenerate a conversation of around four turns between PersonA and PersonU where\nPersonU needs information from PersonA that is in the passage. Use only information\nfound in the passage.\nPersonU starts the conversation and only asks questions. Most of PersonU\u2019s questions\nare follow-up or continuations of previous questions.\nConversation:\nrole-assistant:\nPersonU: I need to know how to prepare for college.\nPersonA: You should first define your goals and interest and be sure to know the\ncosts and how to plan financially and academically for college.\nPersonU: Is there something I can use as a guide to get ready for it?\nPersonA: you can use our checklist to get ready financially and academically.\nPersonU: Any other tip?\nPersonA: Be sure to check out the reasons to attend a College or Career schools.\nPersonU: Ok, that sounds good. Can you give me more tips?\nPersonA: Another thing to take into consideration is to explore your career options.\nrole-user:\nPersonA has access to information in the following passage:\n###There are several disability compensation benefits. One of the benefits is the\nannual clothing allowance. This allowance covers the cost of purchasing new clothes\nup to a limit of 200 dollars annually.###\nGenerate a conversation of around two turns between PersonA and PersonU where\nPersonU needs information from PersonA that is in the passage. Use only information\nfound in the passage.\nPersonU starts the conversation and only asks questions. Most of PersonU\u2019s questions\n17\nChatQA: Building GPT-4 Level Conversational QA Models\nare follow-up or continuations of previous questions.\nConversation:\nrole-assistant:\nPersonU: I heard about a disability benefit for clothing. What is it about?\nPersonA: This is a disability compensation benefit known as an annual clothing\nallowance.\nPersonU: I am quite interested about such benefits.\nPersonA: Would you like to find out if you are eligible?\nrole-user:\nPersonA has access to information in the following passage:\n###Vascepa (icosapent ethyl) is a prescription fish oil medication approved for\ntreating adults with extremely high triglycerides, defined as a triglyceride level\nof 500 mg/dL or higher. While using this medicine, you may need frequent blood tests\nat your doctor\u2019s office. Vascepa is only part of a treatment program that may also\ninclude diet, exercise, and weight control. Follow your doctor\u2019s instructions very\nclosely. You should not use Vascepa if you are allergic to icosapent. To make sure\nVascepa is safe for you, tell your doctor if you have: 1 liver disease; 2 diabetes;\n3 a thyroid disorder; 4 problems with your pancreas; 5 a bleeding or blood-clotting\ndisorder; 6 if you are allergic to fish or shellfish; or. 7 if you drink large\namounts of alcohol, Vascepa (icosapent) works in the liver and bloodstream to reduce\nvery low-density triglycerides.###\nGenerate a conversation of around three turns between PersonA and PersonU where\nPersonU needs information from PersonA that is in the passage. Use only information\nfound in the passage.\nPersonU starts the conversation and only asks questions. Most of PersonU\u2019s questions\nare follow-up or continuations of previous questions.\nConversation:\nrole-assistant:\nPersonU: I heard about a medication called Vascepa. What is it used for?\nPersonA: Vascepa is a prescription fish oil medication used for treating adults with\nextremely high triglyceride levels.\nPersonU: Are there any side effects or precautions I should be aware of?\nPersonA: While using Vascepa, you may need frequent blood tests, and it\u2019s important\nto follow your doctor\u2019s instructions closely. It\u2019s important to talk to your doctor\nabout any allergies or existing conditions you have.\nPersonU: Is it effective in lowering cholesterol levels?\nPersonA: Vascepa is used together with a low-fat diet and other treatments to lower\ntotal cholesterol in people with severe high cholesterol.\nrole-user:\nPersonA has access to information in the following passage:\n{Document}\nGenerate a conversation of around three turns between PersonA and PersonU where\nPersonU needs information from PersonA that is in the passage. Use only information\nfound in the passage.\nPersonU starts the conversation and only asks questions. Most of PersonU\u2019s questions\nare follow-up or continuations of previous questions.\nConversation:\n18\nChatQA: Building GPT-4 Level Conversational QA Models\nrole-assistant:\nThe in-context few-shot prompt in step-2 is as follows (aims to revise the multi-turn QA conversation by adding pronouns\nand common nouns):\nYou are a helpful, pattern-following assistant.\nrole-user:\nGiven the following conversation between PersonU and PersonA:\n###PersonU: I\u2019ve heard about adobo sauce in Mexican cuisine. Can you tell me more\nabout it?\nPersonA: Adobo sauce is a dark red sauce made from ground chiles, herbs, and\nvinegar. It\u2019s used as a marinade and cooking sauce in Mexican and Southwestern\ncuisines.\nPersonU: What are the main ingredients in adobo sauce?\nPersonA: The main ingredients are typically chile peppers, garlic, and vinegar.\nHowever, there can be variations in the recipe.\nPersonU: How can I make adobo sauce at home?\nPersonA: One way to make adobo sauce at home is by using smoked jalapeno peppers\ncalled chipotles. You can stew the chipotles in a sauce with tomatoes, garlic,\nvinegar, salt, and spices. It\u2019s also possible to find canned adobo sauce in the\nHispanic foods section of many grocery stores.\nPersonU: Are there any other uses for adobo sauce?\nPersonA: Adobo sauce can be used as a condiment or as a marinade for various dishes.\nIt enhances the flavor of meats and can be used as a cooking and serving sauce.\nAdditionally, adobo is also used in a stew-like presentation of meat prepared in\nthis sauce.\nPersonU: Is adobo sauce easy to preserve?\nPersonA: Yes, you can preserve adobo sauce by freezing or canning it when you\u2019re\ndone making it. This allows you to store it for up to a year and enjoy its flavors\nwhenever you want.###\nModify PersonU\u2019s questions to use pronouns or common nouns to refer back to words\nor phrases in PersonU\u2019s previous questions or PersonA\u2019s previous responses.\nrole-assistant:\nPersonU: I\u2019ve heard about adobo sauce in Mexican cuisine. Can you provide more\ninformation about it?\nPersonA: Adobo sauce is a dark red sauce made from ground chiles, herbs, and vinegar.\nIt\u2019s used as a marinade and cooking sauce in Mexican and Southwestern cuisines.\nPersonU: What are the main ingredients in the sauce?\nPersonA: The main ingredients are typically chile peppers, garlic, and vinegar.\nHowever, there can be variations in the recipe.\nPersonU: How can I make the sauce at home?\nPersonA: One way to make it at home is by using smoked jalapeno peppers called\nchipotles. You can stew them in a sauce with tomatoes, garlic, vinegar, salt, and\nspices. It\u2019s also possible to find canned adobo sauce in the Hispanic foods section\nof many grocery stores.\nPersonU: Are there any other uses for it?\nPersonA: It can be used as a condiment or as a marinade for various dishes. It\nenhances the flavor of meats and can be used as a cooking and serving sauce.\nAdditionally, it is also used in a stew-like presentation of meat prepared in\nthis sauce.\nPersonU: Is it easy to preserve?\nPersonA: Yes, you can preserve it by freezing or canning it when you\u2019re done\n19\nChatQA: Building GPT-4 Level Conversational QA Models\nmaking it. This allows you to store it for up to a year and enjoy its flavors\nwhenever you want.\nrole-user:\nGiven the following conversation between PersonU and PersonA:\n###PersonU: I\u2019ve heard about a test called urinalysis. What is it used for?\nPersonA: Urinalysis is a test that evaluates a sample of your urine. It is used to\ndetect and assess a wide range of disorders, such as urinary tract infection,\nkidney disease, and diabetes. It examines the appearance, concentration, and content\nof urine.\nPersonU: Can it help diagnose bladder cancer?\nPersonA: Yes, urinalysis can be used as a screening and diagnostic tool for\nbladder cancer. One way to test for bladder cancer is to check for blood in the\nurine, which is called hematuria. This can be done during a urinalysis by examining\nthe urine cells under a special microscope.\nPersonU: What other conditions or diseases can be detected through urinalysis?\nPersonA: Urinalysis is commonly used to diagnose urinary tract or kidney infections,\nevaluate causes of kidney failure, screen for the progression of chronic conditions\nlike diabetes mellitus and high blood pressure, and it can be used in combination\nwith other tests to diagnose certain diseases. It is a versatile test that helps\ndetect abnormalities in the urinary tract.\nPersonU: Where can I get a urinalysis done?\nPersonA: Urinalysis can be performed in various healthcare settings, including\ndoctors\u2019 offices, urgent care facilities, laboratories, and hospitals. You can\nprovide a urine sample in a specimen cup, and the sample can be analyzed on-site\nor sent to a laboratory for testing.\nPersonU: How much urine is typically needed for a urinalysis?\nPersonA: Usually, only small amounts of urine, ranging from 30 to 60 ml, are required\nfor urinalysis testing. It\u2019s a non-invasive and relatively simple test that provides\nvaluable information about your health.###\nModify PersonU\u2019s questions to use pronouns or common nouns to refer back to words\nor phrases in PersonU\u2019s previous questions or PersonA\u2019s previous responses.\nrole-assistant:\nPersonU: I\u2019ve heard about a test called urinalysis. What is it used for?\nPersonA: Urinalysis is a test that evaluates a sample of your urine. It\u2019s used to\ndetect and assess a wide range of disorders, such as urinary tract infection,\nkidney disease, and diabetes. It examines the appearance, concentration, and\ncontent of urine.\nPersonU: Can it help diagnose bladder cancer?\nPersonA: Yes, urinalysis can be used as a screening and diagnostic tool for bladder\ncancer. One way to test for bladder cancer is to check for blood in the urine, which\nis called hematuria. This can be done during a urinalysis by examining the urine\ncells under a special microscope.\nPersonU: What other conditions or diseases can be detected through this test?\nPersonA: Urinalysis is commonly used to diagnose urinary tract or kidney infections,\nevaluate causes of kidney failure, screen for the progression of chronic conditions\nlike diabetes mellitus and high blood pressure, and it can be used in combination\nwith other tests to diagnose certain diseases. It is a versatile test that helps\ndetect abnormalities in the urinary tract.\nPersonU: Where can I go to get this test done?\nPersonA: Urinalysis can be performed in various healthcare settings, including\ndoctors\u2019 offices, urgent care facilities, laboratories, and hospitals. You can\nprovide a urine sample in a specimen cup, and the sample can be analyzed on-site\n20\nChatQA: Building GPT-4 Level Conversational QA Models\nor sent to a laboratory for testing.\nPersonU: How much urine is typically needed for the test?\nPersonA: Usually, only small amounts of urine, ranging from 30 to 60 ml, are\nrequired for urinalysis testing. It\u2019s a non-invasive and relatively simple test\nthat provides valuable information about your health.\nrole-user:\nGiven the following conversation between PersonU and PersonA:\n{multi-turn QA conversation}\nModify PersonU\u2019s questions to use pronouns or common nouns to refer back to words\nor phrases in PersonU\u2019s previous questions or PersonA\u2019s previous responses.\nrole-assistant:\nB. More Details and Results for Retrieval in Conversational QA\nB.1. Query Rewriting Prompts for GPT-3.5-turbo\nThe in-context few-shot query rewriting prompt we use for GPT-3.5-turbo is as follows:\nYou are a helpful, pattern-following assistant.\nrole-user:\nGiven the following conversation between PersonU and PersonA:\nPersonU: Hello, I would like to know what to do if I do not agree with any decision.\nPersonA: disagree with our decision about your monthly income adjustment amounts?\nPersonU: no. Where can I find my SHIP contact information?\nPersonA: You can find your local SHIP contact information in the back of your\nMedicare & You 2020 Handbook online.\nPersonU: and how do they calculate the adjustments?\nInstead of having this entire conversation, how can PersonU get what he or she is\nlooking for using a single question? Respond with that question.\nrole-assistant:\nHow is the calculation for adjustments made by SHIP determined?\nrole-user:\nGiven the following conversation between PersonU and PersonA:\nPersonU: I need to know how to prepare for college.\nPersonA: You should first define your goals and interest and be sure to know the\ncosts and how to plan financially and academically for college.\nPersonU: Is there something I can use as a guide to get ready for it?\nInstead of having this entire conversation, how can PersonU get what he or she is\nlooking for using a single question? Respond with that question.\nrole-assistant:\nWhat resources or guides can I use to help me prepare for college?\nrole-user:\nGiven the following conversation between PersonU and PersonA:\n21\nChatQA: Building GPT-4 Level Conversational QA Models\n{Dialogue History + Latest Question}\nInstead of having this entire conversation, how can PersonU get what he or she is\nlooking for using a single question? Respond with that question.\nrole-assistant:\nB.2. More Results for Retrieval in Conversational QA\nModels\nAverage\nDoc2Dial\nQuAC\nQReCC\nTopiOCQA\nINSCIT\ntop-1\ntop-5\ntop-1\ntop-5\ntop-1\ntop-5\ntop-1\ntop-5\ntop-5*\ntop-20*\ntop-5*\ntop-20*\nDragon (w/ dialog history)\n46.29\n73.09\n43.33\n75.61\n56.8\n82.86\n46.17\n81.96\n57.68\n78.80\n27.49\n46.22\nDragon + Rewrite (w/ dialog history)\n47.57\n74.12\n44.54\n76.98\n57.23\n83.04\n46.45\n82.60\n60.94\n81.74\n28.69\n46.22\nDragon + Rewrite (w/ single query only)\n54.46\n80.13\n47.60\n80.60\n47.10\n77.15\n51.73\n85.78\n73.07\n88.19\n52.79\n68.92\nDragon + Fine-tune (w/ dialog history)\n52.72\n80.67\n48.94\n83.01\n52.64\n81.95\n50.73\n87.17\n67.86\n86.28\n43.43\n64.94\nDragon + Fine-tune + Rewrite (w/ dialog hisotry)\n53.17\n80.84\n49.30\n84.64\n55.04\n83.23\n51.23\n87.99\n60.50\n81.03\n49.80\n67.33\nTable 9. Comprehensive multi-turn retrieval results across five datasets.\nIn Table 9, we show comprehensive comparisons between fine-tuning and rewriting methods.\nInterestingly, we find that compared to only using the rewritten query as input (Dragon + Rewrite (w/ single query only)),\ngiving additional dialog history (Dragon + Rewrite (w/ dialog history)) makes the average scores significantly drop. This\nis because Dragon is originally pretrained on single-turn queries, it will naturally have better generalization ability when\na single-turn rewritten query instead of a multi-turn conversation is provided. And the rewritten query already contains\nsufficient information from the dialog history.\nIn addition, we observe that \u201cDragon + Fine-tune\u201d performs on par with \u201cDragon + Fine-tune + Rewrite\u201d. In other words,\nfor the multi-turn fine-tuning method, replacing original query with rewritten query as inputs yields comparable results.\nThis is because rewritten query will not provide much additional information for the model since the dialog history has\nalready been provided, and it might even cause negative effects (e.g., results on the TopiOCQA dataset) since it makes the\nwhole conversation input not natural. This once again demonstrates how effective the fine-tuning method is in equipping the\nmodel with the capability to comprehend the multi-turn context.\nC. Conversational QA Benchmarks\nC.1. Data Statistics\nDoc2Dial\nWe use the test set of Doc2Dial for the evaluation. It consists of 719 dialogs with 3939 user-agent turns.\nQuAC\nWe use the validation set of QuAC for the evaluation since its test set cannot be directly obtained. Its validation\nset consists of 1000 dialogs with 7354 user-agent turns. Among these 7354 user-agent turns, there are 1486 (around\n20.2%) unanswerable questions. For the combined evaluation of answerable and unanswerable questions, we set the\nground truth response for the unanswerable question as \u201cSorry.\nI cannot find the answer based on\nthe context.\u201d, same as the one we set in our stage-2 tuning. For the fair comparison, we replace the unanswerable\nresponses in the baseline models (i.e., Llama2-SFT/Chat, GPT-3.5-turbo, GPT-4) with the same sentence (Details can be\nfound in Appendix E).\nQReCC\nWe use the test set of QReCC for the evaluation. Its test set includes some dialogue samples source from the\nQuAC dataset. To avoid overlap with other benchmark datasets, we remove those QuAC-sourced samples, resulting in 2805\nuser-agent turns.\nTopiOCQA\nWe use the validation set of TopiOCQA since its test set is not available yet. Its validation set consists of\n205 dialogs with 2514 user-agent turns. Each question has a total of four human-written answers. It has unanswerable\ncases. However, we do not include this dataset in unanswerable case evaluation because we only find 46 questions with no\n22\nChatQA: Building GPT-4 Level Conversational QA Models\nanswer being found, which is not sufficient enough for the unanswerable case evaluation. Given that this dataset use the\nwhole Wikipedia as the retrieval corpus, we leverage its topic information for multi-turn retrieval since we focused more on\nconversational QA over documents which typically narrow down to a specific topic or domain.\nINSCIT\nWe use the validation set of TopiOCQA since its test set is not available yet. Its validation set consists of 86\ndialogues with 502 user-agent turns. Each question has an average of 1.9 human-written answers. It has unanswerable\nresponse for a few questions. However, we do not include this dataset in unanswerable case evaluation because every\nquestion always has at least one answerable response, which means every question is answerable. INSCIT also use the\nwhole Wikipedia as the retrieval corpus. Hence, we also leverage its topic information for multi-turn retrieval for the same\nreason mentioned in TopiOCQA.\nCoQA\nWe use the validation set of CoQA since its test set cannot be directly obtained. Its validation set consists of 500\ndialogues with 7983 user-agent turns. Each question has a total of four human-written answers. It has unanswerable cases.\nHowever, we do not include this dataset in unanswerable case evaluation for the same reason as in the TopiOCQA dataset.\nWe only find 13 questions with no answer being found, which is not sufficient enough for the unanswerable case evaluation.\nDoQA\nWe use the test set of DoQA for the evaluation. Its test set consists of 1200 dialogues with 5394 user-agent turns\nacross Cooking, Travel and Movie domains. Among 5394 user-agent turns, there are 1479 (around 27.4%) unanswerable\nquestions. We use the same strategy as the one mentioned in QuAC dataset to incorporate unanswerable samples into the\nevaluation.\nConvFinQA\nWe use the validation set of ConvFinQA since its test set cannot be directly obtained. Its validation set\nconsists of 421 dialogues with 1490 user-agent turns.\nSQA\nWe use the test set of SQA for the evaluation. Its test set consists of 1025 dialogues with 3100 user-agent turns.\nHybridDial\nWe use the test set of HybridDial for the evaluation. Its test set consists of 243 dialogues with 1111 user-agent\nturns.\nC.2. Details of Incorporating Table as Context\nWe use the markdown format to process the table in the document (i.e., using \u201c|\u201d to separate each cell in the table). However,\nas the number of rows in a table increases, it is harder for the model to link the values of cells in the last few rows to their\ncorresponding headers in the table. To address this issue, we append the headers to the corresponding cell values in each\nrow, so as to make it easier for the model to capture the connections between the cells and their respective headers. For\nexample, a original markdown table\n| Boat | Crew | Nation | Date | Meet | Location |\n| M1x | Mahe | FR | 2009 | | Poznan |\n| M2- | Hamish | US | 2012 | Olympics | Lucerne |\n| M2+ | Igor | DE | 1994 | | Indianapolis |\nwill be converted to\n| M1x (Boat) | Crew: Mahe | Nation: FR | Date: 2009 | Meet: | Location: Poznan |\n| M2- (Boat) | Crew: Hamish | Nation: US | Date: 2012 | Meet: Olympics | Location: Paris |\n| M2+ (Boat) | Crew: Igor | Nation: DE | Date: 1994 | Meet: | Location: Indianapolis |\nWe apply this table processing strategy to all tabular datasets.\nC.3. Prompts for the Benchmarks\nWe use the prompt format mentioned in \u00a7A.2 for benchmarks in ChatQA, Llama2-Chat, GPT-3.5-turbo and GPT-4. We\nkeep the {Context for Latest Question} the same for all models, while we adjust the {Instruction} for the\nbaselines (i.e., Llama2-Chat, GPT-3.5-turbo and GPT-4) to ensure they do not get sub-optimal results. We use different\n23\nChatQA: Building GPT-4 Level Conversational QA Models\ninstructions for test benchmarks with different answer types (e.g., long answer, short answer, arithmetic calculation). The\n{Instruction} for all models on the benchmarks are as follows:\nC.3.1. CHATQA\nWe keep the {Instruction} consistent between the data blends in stage-2 and test benchmarks based on different answer\ntypes.\nWe use \u201cPlease give a full and complete answer for the question.\u201d for DoQA, INSCIT, Hybrid-\nDial, Doc2Dial, QuAC, and QReCC, since these datasets generally have long answers for the questions.\nWe\nuse\n\u201cAnswer the following question with a short span, or a full and complete\nanswer.\u201d for SQA and TopiOCQA, since these datasets have both short and long answers based on the questions.\nWe\nuse\n\u201cAnswer the following question with a short span.\nThe answer needs to be\njust in a few words.\u201d for CoQA, since it generally has short answers for the questions.\nWe\nuse\n\u201cAnswer the following question with a number from context or the math\narithmetic using +, -, *, or /.\u201d\nfor ConvFinQA, since this dataset requires the model to either ex-\ntract numbers from the context or do arithmetic calculation. We will calculate the number based on the arithmetic formula\nthe model generates and compare it with the gold answer.\nC.3.2. LLAMA2-CHAT\nNote that we tried original Llama2-Chat prompt template9, which gives slightly worse results compared to using the one in\n\u00a7A.2. We have tried several {Instruction} for Llama2-Chat. We find the ones below works the best.\nWe use \u201cPlease give an answer in just one sentence.\u201d for DoQA, INSCIT, HybridDial, Doc2Dial,\nQuAC, and QReCC, since these datasets generally have long answers but within one sentence. We notice that it is important\nto give Llama2-Chat specific instruction like \u201cone sentence\u201d instead of \u201cfull and complete answer\u201d to prevent the model\nfrom generating very long answers.\nWe use \u201cAnswer the following question with a short span, or one sentence.\u201d\nfor Topi-\nOCQA, since this dataset has both short and long answers based on the questions, and the long answers are generally within\none sentence.\nWe use \u201cAnswer the following questions with one or a list of entities.\u201d for SQA, since the\nanswer for this dataset always consists of one or a list of entities from the context.\nWe\nuse\n\u201cAnswer the following question with a short span.\nThe answer needs to be\njust in a few words.\u201d for CoQA, since it generally has short answers for the questions.\nWe\nuse\n\u201cAnswer the following question with just a number from context or just the\nmath arithmetic using +, -, *, or /.\u201d for ConvFinQA, since this dataset requires the model to either\nextract numbers from the context or do arithmetic calculation. We extract the arithmetic formula generated by the model and\nuse a calculator to get its final result.\nC.3.3. GPT-3.5-TURBO & GPT-4\nWe have tried several {Instruction} for GPT-3.5-turbo and GPT-4, we find the ones below works the best (applied for\nboth GPT-3.5-turbo and GPT-4).\nWe use \u201cPlease give an answer in just one sentence.\u201d for DoQA, INSCIT, HybridDial, Doc2Dial,\nQuAC, and QReCC, since these datasets generally have long answers but within one sentence. Similar to Llama2-Chat, we\nalso find OpenAI models tend to generate quite long answers given the instruction of \u201cfull and complete answer\u201d. Hence,\nwe make the instruction more specific (i.e., \u201cone sentence\u201d) to prevent the model from generating very long answers.\nWe use \u201cAnswer the following questions in JUST a few words or one sentence.\u201d for Topi-\nOCQA, since this dataset has both short and long answers based on the questions, and the long answers are generally within\n9https://gpus.llm-utils.org/llama-2-prompt-template/\n24\nChatQA: Building GPT-4 Level Conversational QA Models\n \n \n \nFigure 3. Human evaluation instructions and the interface for annotators.\none sentence.\nWe use \u201cAnswer the following questions with one or a list of entities.\nDo not give\na detailed explanation.\nAnswer needs to be as short as possible.\u201d for SQA. We find that\nOpenAI models often give detailed explanation for SQA dataset unless specifically instructed not to.\nWe\nuse\n\u201cAnswer the following question with a short span.\nThe answer needs to be\njust in a few words.\u201d for CoQA, since it generally has short answers for the questions.\nWe use \u201cAnswer the following questions with just a number from context or just the\nmath arithmetic using +, -, *, or /.\u201d for ConvFinQA, since this dataset requires the model to either\nextract numbers from the context or do arithmetic calculation. We extract the arithmetic formula generated by the model and\nuse a calculator to get its final result.\nD. Human Evaluation\nWe conduct human evaluation across the ten test benchmark datasets. We randomly select 60 samples for each dataset, and\neach sample is labelled by three annotators, which results in a total of 1800 annotations.\nWe ask the annotators to verify the facts in models\u2019 outputs and determine which model provides a more accurate response\nto the question. We use Amazon Mechanical Turk platform to conduct the human evaluation. The details of our human\nevaluation instructions and the interface shown to annotators are illustrated in Figure 3.\n25\nChatQA: Building GPT-4 Level Conversational QA Models\nE. Unanswerable Case Evaluation\nFor unanswerable case, we consider the model indicating that the question cannot be answered as correct. For answerable\ncases, we consider the model not indicating the question is unanswerable as correct (i.e., the model giving an answer). We\nfind that ChatQA always generates \u201cSorry.\nI cannot find the answer based on the context\u201d when\nit finds the question is unanswerable, since we use this sentence as a response for unanswerable questions in the stage-2\ntuning. As for Llama2-Chat, GPT-3.5-turbo, and GPT-4, their outputs for unanswerable questions usually follow some\nspecific patterns. Below, we list the heuristic matching patterns derived from all generated samples in DoQA and QuAC\ndatasets to determine if the model suggests that the question is unanswerable. When we find the generated output contains\nthe pattern listed below, we interpret it as an indication that the question is unanswerable, and we will replace its answer\nwith \u201cSorry.\nI cannot find the answer based on the context.\u201d for the F1 evaluation.\ni\u2019m not sure, cannot find, does not provide, cannot provide, cannot answer,\ncannot be found, cannot be determined, don\u2019t have information, do not have\ninformation, couldn\u2019t find, no information in the context, does not mention,\nnot explicitly mentioned, i don\u2019t have any, i do not have any, does not\nspecify, doesn\u2019t provide, not able to, unable to, doesn\u2019t specify, there is\nno information, there is no mention, not mentioned, i don\u2019t have enough\ninformation, there is no specific information, there is no specific mention,\nno information found, I don\u2019t have that information\nF. Case Study\nMore examples on ChatQA-13B, ChatQA-70B, GPT-3.5-turbo, and GPT-4 can be found in Table 10. We use blue color to\nhighlight the relevant context for the last user question. And we use green and red color to highlight the correct and incorrect\nparts in the assistant responses.\n26\nChatQA: Building GPT-4 Level Conversational QA Models\nContext\nConversation\nAssistant Responses\nPos\nRider\nTime/Retired\n1\nLoris Capirossi\n38:04.730\n2\nValentino Rossi\n+0.180\n3\nJeremy McWilliams\n+0.534\n4\nTohru Ukawa\n+0.537\n5\nShinya Nakano\n+0.742\n6\nRalf Waldmann\n+7.019\n7\nFranco Battaini\n+20.889\n8\nStefano Perugini\n+20.891\n9\nJason Vincent\n+21.310\n10\nAnthony West\n+26.816\n11\nAlex Hofmann\n+26.933\n...\n23\nArno Visscher\n+1:40.635\n24\nHenk Van De Lagemaat\n+1 Lap\nRet\nMaurice Bolwerk\nRetirement\nRet\nAndre Romein\nRetirement\nRet\nRoberto Rolfo\nRetirement\nRet\nMarcellino Lucchi\nRetirement\nUser: what racers are shown?\nAssistant: Loris Capirossi, Valentino Rossi,\nJeremy McWilliams, Tohru Ukawa, ... ,\nHenk Van De Lagemaat, Maurice Bolwerk,\nRoberto Rolfo, Marcellino Lucchi.\nUser: of those, what times were recorded?\nAssistant: 38:04.730, +0.180, +0.534,\n+0.537, ... , +1:40.635, +1 Lap, Retirement.\nUser: of those times, which were not\nactual times?\nAssistant: +1 Lap, Retirement\nUser: which of was an actual finish?\nAssistant:\nChatQA-13B: Loris Capirossi\nChatQA-70B: +1 Lap\nGPT-3.5-turbo: 38:04.730\nGPT-4: 38:04.730\nGround Truth: +1 Lap\n2014 compared to 2013 mst 2019s net sales\ndecreased $ 305 million, or 3% in 2014 as\ncompared to 2013 ... space systems 2019\noperating results included the following\n(in millions):\nnet sales\nbacklog at year-end\n2015\n$ 9105\n$ 17400\n2014\n$ 9202\n$ 20300\n2013\n$ 9288\n$ 21400\n2015 compared to 2014 space systems 2019\nnet sales in 2015 decreased $ 97 million, or\n1%, compared to 2014. these decreases were\npartially offset by higher net sales of\napproximately $ 235 million for businesses\nacquired in 2014; and approximately $ 75\nmillion for the orion program due to\nincreased volume.\nUser: what was the average backlog in 2015?\nAssistant: 17400\nUser: what was it in 2014?\nAssistant: 20300\nUser: what is the sum of those years?\nAssistant: 17400 + 20300\nUser: what was the average backlog in 2013?\nAssistant: 21400\nUser: what is the total sum for all 3 years?\nAssistant:\nChatQA-13B: 17400 + 20300 + 21400\nChatQA-70B: 17400 + 20300 + 21400\nGPT-3.5-turbo: 9288 + 9202 + 9105\nGPT-4: 17400 + 20300 + 21400\nGround Truth: 17400 + 20300 + 21400\nThe XVI World Rhythmic Gymnastics\nChampionships were held in Brussels,\nBelgium, on November 20-22, 1992.\nRank\nGymnast\nCountry\nPoint\n1\nOxana Kostina\nRussia\n9.775\n2\nMaria Petrova\nBulgaria\n9.700\n3\nDiana Popova\nBulgaria\n9.625\n4\nCarmen Acedo\nSpain\n9.625\n5\nIrina Deleanu\nRomania\n9.550\n6\nEkaterina S.\nUkraine\n9.525\n7\nViktoriya Yani\nUkraine\n9.475\n8\nMaria Sansaridou\nGreece\n9.425\nUser: What is the XVI World Rhythmic\nGymnastics Championships?\nAssistant: The XVI World Rhythmic\nGymnastics Championships were held in\nBrussels, Belgium, on November\n20-22, 1992\nUser: How many Gymnastics participated\nthis championship?\nAssistant:\nChatQA-13B: There are 8 Gymnastics\nparticipated in the championship.\nChatQA-70B: There are 8 gymnasts*\nparticipated in the championship.\nGPT-3.5-turbo: There were a total of 8\ngymnasts* who participated in\nthis championship.\nGPT-4: The context does not provide\nthe total number of gymnasts who\nparticipated in the XVI World\nRhythmic Gymnastics Championships.\nGround Truth: There are 8 gymnasts\nparticipated this championship.\n*Both models correct the User\u2019s typo of\n\u201cGymnastics\u201d to \u201cgymnasts\u201d\n27\nChatQA: Building GPT-4 Level Conversational QA Models\nContext\nConversation\nAssistant Responses\nThe 3000 metres steeplechase or 3000-meter\nsteeplechase is the most common distance for the\nsteeplechase in track and field. It is an obstacle\nrace over the distance of the 3000 metres, which\nderives its name from the horse racing steeplechase.\nYear\nAthlete\nLocation\n1968\nJouko Kuha (FIN)\nStockholm\n1969\nVladimiras Dudinas (URS)\nKiev\n1970\nKerry O\u2019Brien (AUS)\nBerlin\n1971\nKerry O\u2019Brien (AUS)\nAdelaide\n1972\nAnders G\u00a8arderud (SWE)\nHelsinki\n1973\nBen Jipcho (KEN)\nHelsinki\n1974\nAnders G\u00a8arderud (SWE)\nHelsinki\n1975\nAnders G\u00a8arderud (SWE)\nStockholm\n1976\nAnders G\u00a8arderud (SWE)\nMontr\u00b4eal\n1977\nMichael Karst (FRG)\nStockholm\n...\nUser: Hello, can you tell me about the 3000\nmetres steeplechase?\nAssistant: The 3000 metres steeplechase is the\nmost common distance for the steeplechase in\ntrack and field that goes over the distance of\nthe 3000 metres, which derives its name from\nthe horse racing steeplechase.\nUser: Can you tell me the men that are part of it?\nAssistant: Some of the men are Henry Rono and\nMichael Karst.\nUser: Which locations did Kerry O\u2019Brien ran for?\nAssistant:\nChatQA-13B: He ran in Berlin, Adelaide, and Stockholm.\nChatQA-70B: Kerry O\u2019Brien ran in Adelaide and Berlin.\nGPT-3.5-turbo: Kerry O\u2019Brien ran in Berlin, Adelaide,\nand Champaign.\nGPT-4: Kerry O\u2019Brien ran in Berlin and Adelaide.\nGround Truth: He ran for both Berlin and Adelaide.\ntitle: Non-EU family member traveling to Schengen\ncountries. source: You can travel visa-free only if you\nhave a \u201cresidence\u201d card of a family member of a union\ncitizen\u201d issued under article 10 of the free movement\ndirective (or if you are a citizen of a country whose\nnationals don\u2019t require visas in the Schengen area). A\nresidence card issued by Romania to the spouse of a\nRomanian citizen is not necessarily an article 10 card.\nIf you do not have an article 10 card, EU countries\nother than Romania must issue you a visa free of charge\nand on the basis of an expedited procedure. The grounds\nfor refusal of that visa are strictly limited. These\nprovisions are also dependent on your traveling with or\njoining your spouse. In case you are not, you\u2019ll need a\nregular Schengen visa. As Relaxed pointed out in a\ncomment, it appears that your concern stems in part\nfrom the fact that Romania is not yet a full participant\nin the Schengen agreement. These rules apply to all\nEU countries, not just Schengen countries, because\nthey are based on the EU right of freedom of movement.\nUser: Can I travel from Romania to the Schengen\narea visa-free?\nAssistant: You can travel visa-free only if you\nhave a \u201cresidence card of a family member of a\nunion citizen\u201d issued under article 10 of the free\nmovement directive\nUser: I have a temporary residence card also, can\nwe travel together visa-free?\nAssistant: If you do not have an article 10 card, EU\ncountries other than Romania must issue you a visa\nfree of charge and on the basis of an expedited\nprocedure.\nUser: How do I get a article 10 card?\nAssistant:\nChatQA-13B: The grounds for refusal of that visa are\nstrictly limited. These provisions are also dependent\non your traveling with or joining your spouse. In\ncase you are not, you\u2019ll need a regular Schengen visa.\nChatQA-70B: Sorry. I cannot find the answer based on\nthe context.\nGPT-3.5-turbo: To obtain an article 10 card, you would\nneed to meet the requirements set out in the free\nmovement directive and apply for it through the\nappropriate channels in Romania.\nGPT-4: The text does not provide information on how\nto obtain an article 10 card.*\nGround Truth: Sorry. I cannot find the answer based\non the context.\n*In the evaluation, we will detect this unanswerable\npattern and replace it with \u201cSorry. I cannot find the\nanswer based on the context\u201d to match with the\nground truth.\ntitle: Bengal fox. \u201cVulpes bengalensis\u201d is a relatively\nsmall fox with an elongated muzzle, long, pointed\nears, and a bushy tail about 50 to 60% of the length\nof the head and body. Its dorsal pelage is very variable,\nbut mostly grayish and paler ventrally; its legs tend\nto be brownish or rufous. It is more daintily built than\n\u201cVulpes vulpes\u201d. The tail is bushy with a prominent\nblack tip which distinguishes it from \u201dV. vulpes\u201d. Back\nof ears are dark brown with black margin. Its rhinarium\nis naked and the lips are black, with small black hair\npatches on upper part of nuzzle (shaft) in front of eyes.\ntitle: Red fox. The red fox is considered a more\nspecialised form of \u201dVulpes\u201d than the Afghan, corsac\nand Bengal foxes in the direction of size and adaptation\nto carnivory; the skull displays far fewer neotenous\ntraits than in other species ...\ntitle: Bengal fox. The ears have the same colour as the\nnape or maybe darker, but not having a dark patch as in\n\u201cV. vulpes\u201d. Extensive variation in coat colour exists\nacross populations and seasonally within populations,\nbut generally varies from grey to pale brown ...\nUser: what is red fox?\nAssistant: It is the largest of the true foxes and\none of the most widely distributed members of\nthe order Carnivora.\nUser: where did it originate?\nAssistant: The species is Eurasian in origin.\nUser: how did it evolve?\nAssistant: It is considered a more specialised form\nof \u201cVulpes\u201d than the Afghan, corsac and Bengal\nfoxes in the direction of size and adaptation\nto carnivory.\nUser: what is the appearance of this third type\nof fox?\nAssistant:\nChatQA-13B: The ears have the same colour as the\nnape or maybe darker, but not having a dark patch\nas in \u201cV. vulpes\u201d. Extensive variation in coat colour\nexists across populations and seasonally within\npopulations, but generally varies from grey to pale\nbrown.\nChatQA-70B: \u201cVulpes bengalensis\u201d is a relatively\nsmall fox with an elongated muzzle, long, pointed\nears, and a bushy tail about 50 to 60% of the length\nof the head and body.\nGPT-3.5-turbo: I\u2019m sorry, but I couldn\u2019t find\nany information about a third type of fox in the\ngiven context.\nGPT-4: The context does not provide information on\na third type of fox.\nGround Truth: It is a relatively small fox with an\nelongated muzzle, long, pointed ears, and a bushy tail\nabout 50 to 60% of the length of the head and body.\nTable 10. Case study for ChatQA-13B, ChatQA-70B, GPT-3.5-turbo, and GPT-4. We use blue color to highlight the relevant context for\nthe last user question. And we use green and red color to highlight the correct and incorrect parts in the assistant responses.\n28\nChatQA: Building GPT-4 Level Conversational QA Models\nG. Human Annotated Conversational QA Data\nG.1. Statistics of User-Agent Turns\nNumber of user-agent turns per dialogue\nRatio\n0.00%\n10.00%\n20.00%\n30.00%\n40.00%\n50.00%\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n>14\nFigure 4. Statistics regarding the number of user-agent turns for each dialogue\nFigure 4 illustrates the statistics regarding the number of user-agent turns per dialogue over the 7k human annotated\nconversational QA data.\nG.2. Guidelines for Conversational QA Data Collection\nIn this section, we demonstrate the guidelines we provide for the conversational QA data collection. We request the\nannotators to act as both user and agent to construct the conversational QA sample based on the given document, which\nmakes the annotation more efficient. The guidelines consists of three parts: 1) What does conversational QA samples look\nlike; 2) What kinds of conversational QA samples we need; and 3) What we need to annotate.\nG.2.1. WHAT DOES CONVERSATIONAL QA SAMPLES LOOK LIKE\nOne conversational QA sample is based on a document that we provide. The user and agent behaviors are as follows:\n\u2022 User behavior: 1) Ask questions to agent based on the given document; 2) Answer questions from agent when agent\nwants to clarify something.\n\u2022 Agent behavior: 1) Answer questions from user based on the document; 2) Ask questions to the user when the user\u2019s\nquestion is not clear, or too general/broad.\nG.2.2. WHAT KINDS OF MULTI-TURN QA SAMPLES WE NEED\nWe list the requirements for user\u2019s questions and agent\u2019s responses below.\nUser\u2019s Questions\n\u2022 User\u2019s questions can refer to his/her previous (or multiple turns before) question.\n\u2022 User\u2019s questions can also refer to the previous (or multiple turns before) answer from the agent.\n\u2022 Try to use pronouns or common nouns to replace the entities mentioned before.\n29\nChatQA: Building GPT-4 Level Conversational QA Models\n\u2022 Try to make the user\u2019s questions diverse. For the same type of questions, try to use different ways to express it in\ndifferent dialogue annotations.\n\u2022 Other user question types we need\n\u2013 Given the answer from agent, ask for extra information (e.g., What else ...; any other ...; is there more ...).\n\u2013 Switch the topic and starting a new thread in conversation.\n\u2013 Ask two questions at the same time.\nAgent\u2019s Response\n\u2022 Try to make the agent\u2019s answer within 1-2 sentences. If the answer has to be long, try to make it concise.\n\u2022 Try not to directly copy the whole relevant context from the document as the answer. Instead, try to construct the\nanswer by paraphrasing the chosen context.\n\u2022 Try to come up with a small proportion of cases where agent asks questions to user to clarify something. Specifically,\nwhen the user\u2019s question is too broad or not clear, the agent needs to narrow down the scope by asking clarification\nquestions to see which specific aspects that the user cares more about.\nG.2.3. WHAT WE NEED TO ANNOTATE\nWe list what we need to annotate for each conversation below.\n\u2022 For each document, we need to annotate the user\u2019s questions and corresponding agent\u2019s responses. The average number\nof user-agent turns per conversation needs to be around five.\n\u2022 For each user\u2019s question, we need to annotate all the relevant context within the document.\n30\n"
  },
  {
    "title": "Rethinking FID: Towards a Better Evaluation Metric for Image Generation",
    "link": "https://arxiv.org/pdf/2401.09603.pdf",
    "upvote": "13",
    "text": "Rethinking FID: Towards a Better Evaluation Metric for Image Generation\nSadeep Jayasumana\nSrikumar Ramalingam\nAndreas Veit\nDaniel Glasner\nAyan Chakrabarti\nSanjiv Kumar\nGoogle Research, New York\n{sadeep, rsrikumar, aveit, dglasner, ayanchakrab, sanjivk}@google.com\nAbstract\nAs with many machine learning problems, the progress\nof image generation methods hinges on good evaluation\nmetrics. One of the most popular is the Fr\u00b4echet Inception\nDistance (FID). FID estimates the distance between a dis-\ntribution of Inception-v3 features of real images, and those\nof images generated by the algorithm. We highlight impor-\ntant drawbacks of FID: Inception\u2019s poor representation of\nthe rich and varied content generated by modern text-to-\nimage models, incorrect normality assumptions, and poor\nsample complexity. We call for a reevaluation of FID\u2019s use\nas the primary quality metric for generated images. We em-\npirically demonstrate that FID contradicts human raters,\nit does not reflect gradual improvement of iterative text-to-\nimage models, it does not capture distortion levels, and that\nit produces inconsistent results when varying the sample\nsize. We also propose an alternative new metric, CMMD,\nbased on richer CLIP embeddings and the maximum mean\ndiscrepancy distance with the Gaussian RBF kernel. It is\nan unbiased estimator that does not make any assumptions\non the probability distribution of the embeddings and is\nsample efficient. Through extensive experiments and anal-\nysis, we demonstrate that FID-based evaluations of text-\nto-image models may be unreliable, and that CMMD of-\nfers a more robust and reliable assessment of image qual-\nity. A reference implementation of CMMD is available at:\nhttps://github.com/google-research/google-\nresearch/tree/master/cmmd.\n1. Introduction\nText-to-image models are progressing at breakneck speed.\nRecent models such as [18, 21\u201323, 28] have been incred-\nibly successful at generating realistic images that remain\nfaithful to text prompts. As with many problems in ma-\nchine learning, a reliable evaluation metric is key to driv-\ning progress. Unfortunately, we find that the most popular\nmetric used in the evaluation of text-to-image models, the\nLow Distortion\nHigh Distortion\nFigure 1.\nBehaviour of FID and CMMD under distortions.\nCMMD monotonically increases with the distortion level, cor-\nrectly identifying the degradation in image quality with increasing\ndistortions. FID is wrong. It improves (goes down) for the first\nfew distortion levels, suggesting that quality improves when these\nmore subtle distortions are applied. See Section 6.2 for details.\nFr\u00b4echet Inception Distance (FID) [13], may disagree with\nthe gold standard, human raters, in some important cases;\nand is thus ill-suited for this purpose. We identify some im-\nportant limitations of the FID through statistical tests and\nempirical evaluations. To address these shortcomings, we\npropose an alternative metric: CMMD, which uses CLIP\nembeddings and Maximum Mean Discrepancy (MMD) dis-\ntance. Figure 1 shows one of our experiments, the details\nof which are discussed in Section 6.2, in which FID does\nnot reflect progressive distortion applied to images while\nCMMD correctly ranks the image sets based on the severity\nof the distortion.\nEvaluating image generation models is a uniquely chal-\nlenging task. Unlike traditional vision tasks such as clas-\nsification or detection, we need to evaluate multiple dimen-\nsions of performance including quality, aesthetics and faith-\nfulness to the text prompt. Moreover, these are hard-to-\nquantify concepts which depend on human perception. As a\nresult, human evaluation remains the gold standard for text-\nto-image research. Since human evaluation is an expensive\n1\narXiv:2401.09603v2  [cs.CV]  25 Jan 2024\nFr\u00b4echet distance\nMMD distance\nInception embeddings\n\u2717 Weak image embeddings\n\u2717 Weak image embeddings\n\u2717 Normality assumption\n\u2713 Distribution-free\n\u2717 Sample inefficient\n\u2713 Sample efficient\n\u2717 Biased estimator\n\u2713 Unbiased estimator\nCLIP embeddings\n\u2713 Rich image embeddings\n\u2713 Rich image embeddings\n\u2717 Normality assumption\n\u2713 Distribution-free\n\u2717 Sample inefficient\n\u2713 Sample efficient\n\u2717 Biased estimator\n\u2713 Unbiased estimator\nTable 1. Comparison of options for comparing two image distributions. FID, the current de facto standard for text-to-image evaluation is\nin the upper-left corner. The proposed metric, CMMD, is in the lower-right corner and has many desirable properties over FID.\nsolution that does not scale well, researchers often rely on\nautomated evaluation. Specifically, recent works have used\nFID and CLIP distance to measure image quality and faith-\nfulness to the text prompts, respectively.\nIn this work, we call for a reevaluation of this approach,\nin particular, the use of FID as a measure of image quality.\nWe highlight drawbacks of FID, such as incorrectly model-\ning Inception embeddings of image sets as coming from a\nmultivariate normal distribution and its inconsistent results\nwhen varying the sample size (also noted in [5]). We em-\npirically show that, FID can contradict human raters, does\nnot reflect gradual improvement of iterative text-to-image\nmodels and does not capture complex image distortions.\nOur proposed metric uses CLIP embeddings and the\nMMD distance. Unlike Inception embeddings, which were\ntrained on about 1 million ImageNet images, restricted to\n1000 classes [25], CLIP is trained on 400 million images\nwith corresponding text descriptions [20], making it a much\nmore suitable option for the rich and diverse content gener-\nated by modern image generation models and the intricate\ntext prompts given to modern text-to-image models.\nMMD, is a distance between probability distributions\nthat offers some notable advantages over the Fr\u00b4echet dis-\ntance. When used with an appropriate kernel, MMD is a\nmetric that does not make any assumptions about the distri-\nbutions, unlike the Fr\u00b4echet distance which assumes multi-\nvariate normal distributions. As shown in [5], FID is a bi-\nased estimator, where the bias depends on the model being\nevaluated. MMD, on the other hand, is an unbiased estima-\ntor, and as we empirically demonstrate it does not exhibit\na strong dependency on sample size like the Fr\u00b4echet dis-\ntance. Finally, it admits a simple parallel implementation.\nThe ability to estimate from a smaller sample size and the\nfast computation make MMD fast and useful for practical\napplications. Different options for comparing two image\ndistributions are compared in Table 1. The existing FID\nmetric is in the upper-left corner and has many unfavorable\nproperties. Our proposed metric, CMMD, is in the lower-\nright corner and avoids the drawbacks of FID.\nWe summarize our contributions below:\n\u2022 We call for a reevaluation of FID as the evaluation metric\nfor modern image generation and text-to-image models.\nWe show that it does not agree with human raters in some\nimportant cases, that it does not reflect gradual improve-\nment of iterative text-to-image models and that it does not\ncapture obvious image distortions.\n\u2022 We identify and analyze some shortcomings of the\nFr\u00b4echet distance and of Inception features, in the context\nof evaluation of image generation models.\n\u2022 We propose CMMD, a distance that uses CLIP features\nwith the MMD distance as a more reliable and robust al-\nternative, and show that it alleviates some of FIDs major\nshortcomings.\n2. Related Works\nGenerated image quality has been assessed using a vari-\nety of metrics including log-likelihood [9], Inception Score\n(IS) [1, 24], Kernel Inception Distance (KID) [2, 27],\nF\u00b4rechet Inception Distance (FID)\n[13], perceptual path\nlength [14], Gaussian Parzen window [9], and HYPE [29].\nIS is calculated using the Inception-v3 model [25], which\nhas been trained on ImageNet, to measure the diversity and\nquality of generated images by leveraging the 1000 class\nprobabilities of the generated images. While IS does not re-\nquire the original real images, KID and FID are computed\nby determining the distance between the distributions of\nreal and generated images. KID utilizes the squared MMD\ndistance with the rational quadratic kernel. FID employs the\nsquared Fr\u00b4echet distance between two probability distribu-\ntions, which is also equal to the Wasserstein-2 distance, with\nthe assumption that both distributions are multivariate nor-\nmal. Both FID and KID suffer from the limitations of the\nunderlying Inception embeddings: they have been trained\non only 1 million images, limited to 1000 classes. Intu-\nitively, we expect this could limit their ability to represent\nthe rich and complex image content seen in modern gener-\nated images.\nPrevious work has pointed to the unreliability of evalu-\nation metrics in image generation [5, 19]. Chong et al. [5]\n2\nFD\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nFD\u221e\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nMMD\n0.0\n0.5875\n5.794\n17.21\n78.88\n202.8\n244.9\nTable 2. Behavior of estimated Fr\u00b4echet distances and MMD when normality assumption is violated. Going from left to right, the probability\ndistribution changes more and more from the leftmost distribution. However, the Fr\u00b4echet distances to the leftmost distribution calculated\nwith normality assumption remains misleadingly zero. MMD, on the other hand, is able to correctly capture the progressive departure.\nshow that FID is a biased estimator and that the bias de-\npends on the model being evaluated.\nThey propose an\nextrapolation approach to compute a bias-free estimator:\nFID\u221e. Parmar et al. [19] show that low-level image pro-\ncessing operations such as compression and resizing can\nlead to significant variations in FID, and advocate the use\nof anti-aliased resizing operations. In this work, we show\nthat FID\u2019s issues extend well beyond what is discussed in\nthose prior works and that FID\u221e and/or anti-aliased resiz-\ning do not solve those issues.\n3. Limitations of FID\nIn this section we highlight some key limitations of FID.\nWe start with a background discussion of the metric in or-\nder to better understand its limitations. Fr\u00b4echet Inception\nDistance (FID) is used to measure the discrepancy between\ntwo image sets: I and I\u2032. Usually one set of images are\nreal (for example, from the COCO dataset) and the other set\nis generated using the image generation model to be evalu-\nated. To calculate FID, Inception-v31 embeddings [25] are\nfirst extracted for both image sets using the Inception-v3\nmodel trained on the ImageNet classification task. The FID\nbetween I and I\u2032 is then defined as the Fr\u00b4echet distance\nbetween these two sets of Inception embeddings.\n3.1. The Fr\u00b4echet Distance\nFor any two probability distributions P and Q over Rd hav-\ning finite first and second moments, the Fr\u00b4echet distance is\ndefined by [6, 17]:\ndist2\nF (P, Q) :=\ninf\n\u03b3\u2208\u0393(P,Q) E(x,y)\u223c\u03b3\u2225x \u2212 y\u22252,\n(1)\nwhere \u0393(P, Q) is the set of all couplings of P and Q. This is\nalso equivalent to the Wasserstein-2 distance on Rd. In gen-\neral, obtaining a closed-form solution for the Fr\u00b4echet dis-\ntance is difficult. However, the authors of [6] showed that\n1Throughout the paper we use the terms Inception and Inception-v3\ninterchangeably.\na closed-form solution exists for multivariate normal distri-\nbutions in the form:\ndist2\nF (P, Q) = \u2225\u00b5P \u2212\u00b5Q\u22252\n2+Tr(\u03a3P +\u03a3Q\u22122(\u03a3P \u03a3Q)\n1\n2 ),\n(2)\nwhere \u00b5P , \u00b5Q are the means and \u03a3P , \u03a3Q are the covari-\nances of the two multivariate normal distributions P and Q.\nNote that this simplified formula is strictly valid only when\nboth P and Q are multivariate normal distributions [6].\nFor FID, we need to estimate the Fr\u00b4echet distance be-\ntween two distributions of Inception embeddings, using two\ncorresponding samples. This is challenging due to the high\ndimensionality of inception embeddings, d = 2048. As-\nsuming that the Inception embeddings are drawn from a\nnormal distribution simplifies the problem, allowing us to\nuse Eq. (2) with \u00b5P , \u00b5Q and \u03a3P , \u03a3Q estimated from the\ntwo samples I and I\u2032. There are two kinds of error in this\nprocedure:\n1. As we show in Section 3.3, Inception embeddings for\ntypical image sets are far from being normally dis-\ntributed. The implications of this inaccurate assumption\nwhen calculating the Fr\u00b4echet distance are discussed in\nSection 3.2.\n2. Estimating (2048 \u00d7 2048)-dimensional covariance ma-\ntrices from a small sample can lead to large errors, as\ndiscussed in Section 6.3.\n3.2. Implications of Wrong Normality Assumptions\nWhen calculating the Fr\u00b4echet distance between two distri-\nbutions, making an incorrect normality assumption can lead\nto disastrous results. We illustrate this using a 2D isotropic\nGaussian distribution at the origin as the reference distribu-\ntion and by measuring the distance between that and a series\nof mixture-of-Gaussian distributions generated as described\nbelow. The results are summarized in Table 2.\nTo generate the series of second distributions, we start\nwith a mixture of four Gaussians, each having the same\nmean and covariance as the reference Gaussian. Since this\nmixture has the same distribution as the reference distribu-\n3\ntion, we expect any reasonable distance to measure zero dis-\ntance between this and the reference distribution (first col-\numn of Table 2). We then let the second distribution\u2019s four\ncomponents get further and further away from each other\nwhile keeping the overall mean and the covariance fixed\n(first row of Table 2). When this happens the second dis-\ntribution obviously gets further and further away from the\nreference distribution. However, the Fr\u00b4echet distance calcu-\nlated with the normality assumption (note that this is not the\ntrue Fr\u00b4echet distance, which cannot be easily calculated) re-\nmains misleadingly zero. This happens because the second\ndistribution is normal only at the start, therefore the nor-\nmality assumption is reasonable only for the first column of\nthe table. Since the second distribution is not normal after\nthat, the Fr\u00b4echet distance calculated with normality assump-\ntion gives completely incorrect results. Note that, as shown\nin the third row of Table 2, FID\u221e, the unbiased version\nof FID proposed in [5], also suffers from this shortcoming,\nsince it also relies on the normality assumption. In contrast,\nthe MMD distance described in Section 4 (bottom row of\nTable 2) is able to capture the progressive departure of the\nsecond distribution from the reference distribution. More\ndetails of the experiment setup are in Appendix B.\n3.3. Incorrectness of the Normality Assumption\nWhen estimating the Fr\u00b4echet distance, it is assumed that\nthe Inception embeddings for each image set (real and gen-\nerated), come from a multivariate normal distribution. In\nthis section, we show that this assumption is wrong. As\ndiscussed in Section 3.2, making a wrong normality as-\nsumption about the underlying distribution can lead to com-\npletely wrong results.\nIt should not be surprising that Inception embeddings\nfor a typical image set do not have a multivariate normal\ndistribution with a single mode.\nInception embeddings\nare activations extracted from the penultimate layer of the\nInception-v3 network.\nDuring training, these activations\nare classified into one of 1000 classes using a linear classi-\nfier (the last fully-connected layer of the Inception-v3 net-\nwork). Therefore, since the Inception-v3 network obtains\ngood classification results on the ImageNet classification\ntask, one would expect Inception embeddings to have at\nleast 1, 000 clusters or modes. If this is the case, they cannot\nbe normally distributed.\nFigure 2 shows a 2-dimensional t-SNE [26] visualiza-\ntion of Inception embeddings of the COCO 30K dataset,\ncommonly used as the reference (real) image set in text-to-\nimage FID benchmarks. It is clear that the low dimensional\nvisualization has multiple modes, and therefore, it is also\nclear that the original, 2048-dimensional distribution is not\nclose to a multivariate normal distribution.\nFinally, we applied three different widely-accepted sta-\ntistical tests: Mardia\u2019s skewness test, Mardia\u2019s kurtosis test,\nFigure 2.\nt-SNE visualization of Inception embeddings of the\nCOCO 30K dataset. Note that even in the reduced-dimensional\n2D representation, it is easy to identify that embeddings have mul-\ntiple modes and do not follow a multivariate normal distribution.\nand Henze-Zirkler test to test normality of Inception em-\nbeddings of the COCO 30K dataset. All of them strongly\nrefute the hypothesis that Inception embeddings come from\na multivariate normal distribution, with p-values of virtu-\nally zero (indicating an overwhelming confidence in reject-\ning the null hypothesis of normality). The details of these\ntests can be found in Appendix A.\nTo be clear, we do not expect CLIP embeddings to\nbe normally distributed either. It is FID\u2019s application of\nFr\u00b4echet distance with its normality assumption to non-\nnormal Inception features, that we object to. In fact, CLIP\nembeddings of COCO 30K also fail the normality tests with\nvirtually zero p-values, indicating that it is not reasonable to\nassume normality on CLIP embeddings either.\n4. The CMMD Metric\nIn this section, we propose a new metric to evaluate image\ngeneration models, using CLIP embeddings and the Maxi-\nmum Mean Discrepancy (MMD) distance, with a Gaussian\nRBF kernel. The CMMD (stands for CLIP-MMD) metric\nis the squared MMD distance between CLIP embeddings of\nthe reference (real) image set and the generated image set.\nCLIP embeddings [20] have changed the way we think\nabout image and text representations by learning them in a\njoint space. CLIP trains an image encoder and a text en-\ncoder jointly using 400 million image-text pairs containing\n4\ncomplex scenes. In contrast, Inception-v3 is trained on Im-\nageNet, which has on the order of 1 million images which\nare limited to 1000-classes and only one prominent object\nper image. As a result, CLIP embeddings are better suited\nfor representing the diverse and complex content we see in\nimages generated by modern image generation algorithms\nand the virtually infinite variety of prompts given to text-to-\nimage models.\nTo compute the distance between two distributions we\nuse the MMD distance [10, 11]. MMD was originally de-\nveloped as a part of a two-sample statistical test to deter-\nmine whether two samples come from the same distribution.\nThe MMD statistic calculated in this test can also be used to\nmeasure the discrepancy between two distributions. For two\nprobability distributions P and Q over Rd, the MMD dis-\ntance with respect to a positive definite kernel k is defined\nby:\ndist2\nMMD(P, Q) := Ex,x\u2032[k(x, x\u2032)] + Ey,y\u2032[k(y, y\u2032)]\n\u2212 2Ex,y[k(x, y)], (3)\nwhere x and x\u2032 are independently distributed by P and y\nand y\u2032 are independently distributed by Q. It is known that\nthe MMD is a metric for characteristic kernels k [8, 11].\nGiven two sets of vectors , X = {x1, x2, . . . , xm} and\nY = {y1, y2, . . . , yn}, sampled from P and Q, respec-\ntively, an unbiased estimator for d2\nMMD(P, Q) is given by,\n\u02c6\ndist\n2\nMMD(X, Y ) =\n1\nm(m \u2212 1)\nm\nX\ni=1\nm\nX\nj\u0338=i\nk(xi, xj)\n+\n1\nn(n \u2212 1)\nn\nX\ni=1\nn\nX\nj\u0338=i\nk(yi, yj)\n\u2212\n2\nmn\nm\nX\ni=1\nn\nX\nj=1\nk(xi, yj).\n(4)\nSome advantages of MMD over the Fr\u00b4echet distance are:\n1. MMD metric, when used with a characteristic kernel [8],\nis distribution-free. That is, it does not make any as-\nsumptions about the distributions P and Q. In contrast,\nthe Fr\u00b4echet distance in Eq. (2) assumes normality and is\nliable to give erroneous results when this assumption is\nviolated.\n2. As shown in [5], the FID estimated from a finite sample\nhas a bias that depends on the model being evaluated,\nto the extent that the sample size can lead to different\nrankings of the models being evaluated. Removing this\nbias requires a computationally expensive procedure in-\nvolving computation of multiple FID estimates [5]. In\ncontrast, the MMD estimator in Eq. (4), is unbiased.\n3. When working with high-dimensional vectors such as\nimage embeddings, MMD is sample efficient. Fr\u00b4echet\ndistance, on the other hand, requires a large sample to\nreliably estimate the d \u00d7 d covariance matrix. This will\nbe further elaborated on in Section 6.3.\nAs the kernel in the MMD calculation, we use the Gaus-\nsian RBF kernel k(x, y) = exp(\u2212\u2225x \u2212 y\u22252/2\u03c32), which\nis a characteristic kernel, with the bandwidth parameter set\nto \u03c3 = 10. Empirically, we observed that the bandwidth\nparameter does not significantly affect the overall trends\nof the metric.\nHowever, we propose to keep it fixed at\n10 to obtain consistent values for the metric.\nSince the\nMMD metric with the Gaussian kernel is bounded above\nat 2 (when the two distributions are maximally different), it\ngives small values for general distributions. We therefore\nscale up the value in Eq. (4) by 1000 to obtain more read-\nable values. For the CLIP embedding model, we use the\npublicly-available ViT-L/14@336px model, which is the\nlargest and the best performing CLIP model [20]. Also note\nthat we have m = n in Eq. (4) for text-to-image evalua-\ntion since we evaluate generated images against real images\nsharing the same captions/prompts. Our code for computing\nCMMD is publicly available.\n5. Human Evaluation\nWe now present a human evaluation to show that FID does\nnot agree with human perception of image quality.\nTo\nthis end, we picked two models, Model-A: the full Muse\nmodel as described in [3] with 24 base-model iterations\nand 8 super-resolution model iterations. Model-B: an early-\nstopped Muse model with only 20 base-model iterations and\n3 super-resolution model iterations. This was done inten-\ntionally to reduce the quality of produced images. We use\na Muse model trained on the WebLI dataset [4], generously\nmade available to us by the Muse authors. The choice of\nearly-stopping iterations is arbitrary: as shown in Figure 4,\nFID is consistently better (lower) for all early-stopped mod-\nels when compared with the full model (Model-A).\nWe performed a side-by-side evaluation where human\nraters were presented with two images, one generated from\nModel-A and the other generated from Model-B. We used\nthe same random seeds to ensure that image content and\ndegree of alignment to the prompt are the same. This al-\nlowed the raters to focus on image quality. The raters were\nasked to evaluate which image looked better. Raters had\nModel\nModel-A\nModel-B\nFID\n21.40\n18.42\nFID\u221e\n20.16\n17.19\nCMMD\n0.721\n0.951\nHuman rater preference\n92.5%\n6.9%\nTable 3. Human evaluation of different models. FID contradicts\nhuman evaluation while CMMD agrees.\n5\n(a) Step 1\n(b) Step 3\n(c) Step 6\n(d) Step 8\nFigure 3. The quality of the generated image monotonically improves as we progress through Muse\u2019s refinement iterations. CMMD\ncorrectly identifies the improvements. FID, however, incorrectly indicates a quality degradation (see Figure 4). Prompt: \u201cThe Parthenon\u201d.\nthe option of choosing either image or that they are indiffer-\nent. All image pairs were rated by 3 independent raters,\nhired through a high-quality crowd computing platform.\nThe raters were not privy to the details of the image sets\nand rated images purely based on the visual quality. The\nauthors and the raters were anonymous to each other.\nWe used all PartiPrompts [28], which is a collection of\n1633 prompts designed for text-to-image model evaluation.\nThese prompts cover a wide range of categories (abstract,\nvehicles, illustrations, art, world knowledge, animals, out-\ndoor scenes, etc.) and challenge levels (basic, complex fine-\ngrained detail, imagination, etc.).\nEvaluation results are\nsummarized in Table 3. For each comparison, we consider\na model as the winner if 2 or more raters have preferred\nthe image produced by that model. If there is no consensus\namong the raters or if the majority of the raters selected are\nindifferent, no model wins. We observed that Model-A was\npreferred in 92.5% of the comparisons, while Model-B was\npreferred only 6.9% of the time. The raters were indifferent\n0.6% of the time. It is therefore clear that human raters over-\nwhelmingly prefer Model-A to Model-B. However, COCO\n30K FID and its unbiased variant FID\u221e, unfortunately say\notherwise. On the other hand, the proposed CMMD metric\ncorrectly aligns with the human preference.\n6. Performance Comparison\nWe now compare FID with the proposed CMMD metric\nunder various settings to point out the limitations of FID\nwhile highlighting the benefits of CMMD. In all our exper-\niments, we use the COCO 30K dataset [15] as the reference\n(real) image dataset. Zero-shot evaluation on this dataset is\ncurrently the de facto evaluation standard for text-to-image\ngeneration models [3, 22, 23]. Throughout our experiments,\nwhere applicable, we use high-quality bicubic resizing with\nanti-aliasing as suggested in [19]. This prevents any ad-\nverse effects of improperly-implemented low level image\nprocessing operations on FID as those reported in [19].\nFor Stable Diffusion [22], we use the publicly available\nRefining Iteration\nFID and FID\u221e\nCMMD\n16\n18\n20\n22\n0.600\n0.700\n0.800\n0.900\n1.000\n1.100\n1\n2\n3\n4\n5\n6\n7\n8\nCMMD\nFID\nFID\u221e\nFigure 4. Behavior of FID and CMMD for Muse steps. CMMD\nmonotonically goes down, correctly identifying the iterative im-\nprovements made to the images (see Figure 3).\nFID is com-\npletely wrong suggesting degradation in image quality as itera-\ntions progress. FID\u221e has the same behavior as FID.\nStable Diffusion 1.4 model. We evaluate all models without\nany additional bells and whistles such as CLIP sorting.\n6.1. Progressive Image Generation Models\nMost modern text-to-image generation models are itera-\ntive. For example, diffusion models [22, 23] require mul-\ntiple denoising steps to generate the final image, the Parti\nmodel [28] auto-regressively generates image tokens one at\na time. While the Muse model [3] generates multiple to-\nkens at a time, it still requires iterative sampling steps to\ngenerate the final image, as shown in Figure 3. Gradually\nimproving the quality of the generated images in each step,\nthese methods go from poor quality images or pure noise\nimages to unprecedented photo-realism. This progression\nin quality is obvious to a human observer and we would ex-\npect any reasonable metric to monotonically improve as we\nprogress through iterations of image generation.\nFigure 4 shows FID, FID\u221e, and CMMD values for pro-\ngressive Muse iterations. FID and FID\u221e incorrectly sug-\n6\nFigure 5. Behavior of FID and CMMD under distortions. Images in the first row (FID: 21.40, CMMD: 0.721) are undistorted. Images\nin the second (FID: 18.02, CMMD: 1.190) are distorted by randomly replacing each VQGAN token with probability p = 0.2. The image\nquality clearly degrades as a result of the distortion, but FID suggests otherwise, while CMMD correctly identifies the degradation.\ngest that the image quality degrades, when the quality im-\nprovements are obvious as illustrated in Figure 3. In con-\ntrast, CMMD correctly identifies the quality improvements\nmade during Muse\u2019s iterative refinements. As seen in Fig-\nure 4, we consistently observe in our experiments that FID\nand FID\u221e have the same behavior although absolute values\nare different. This is not surprising since FID\u221e is derived\nfrom FID and inherits many of its shortcomings.\nFigure 6 shows an evaluation of the last 5 iterations\nof a 100-iteration Stable Diffusion model. Our proposed\nCMMD metric monotonically improves (decreases) with\nthe progression of the iterations, whereas FID has unex-\npected behavior. We focus on the more subtle differences\nin the final iterations of Stable Diffusion, since both FID\nand CMMD showed monotonicity at the easily-detectable\nhigh noise levels in the initial iterations.\n6.2. Image Distortions\nHere, we provide additional evidence that FID does not ac-\ncurately reflect image quality under complex image distor-\ntions. It was shown in [13] that FID accurately captures\nimage distortions under low-level image processing distor-\ntions such as Gaussian noise and Gaussian blur. Since In-\nception embeddings are trained on ImageNet images with-\nout extreme data augmentation, it is not surprising that FID\nis able to identify these distortion. However, in this section,\nwe show that FID is unable to identify more complex noise\nadded in the latent space.\nTo this end, we take a set of images generated by Muse\nand progressively distort them by adding noise in the VQ-\nGAN latent space [7]. For each image, we obtain VQGAN\ntokens, replace them with random tokens with probability\np, and reconstruct the image with the VQGAN detokenizer.\nRefining Iteration\nFID\nCMMD\n21.0\n22.0\n23.0\n24.0\n25.0\n26.0\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n95\n96\n97\n98\n99\n100\nCMMD\nFID\nFigure 6. Behavior of FID and CMMD for StableDiffusion steps.\nCMMD monotonically improves (goes down), reflecting the im-\nprovements in the images. FID\u2019s behavior is not consistent, it mis-\ntakenly suggests a decrease in quality in the last two iterations.\nExample distortions are shown in Figure 5. The images get\nmore and more distorted with increasing p and the qual-\nity loss with increasing p is visibly obvious. However, as\nshown in Figure 7, FID fails to reflect the degradation in\nimage quality for increasing values of p. Our CMMD met-\nric, on the other hand, monotonically worsens (increases)\nwith the distortion level p, correctly identifying the quality\nregression. Figure 1 shows that FID behaves poorly also\nwhen we measure the distances between progressively dis-\ntorted versions (using the same procedure) of the COCO\n30K dataset and the reference clean version of that dataset.\n6.3. Sample Efficiency\nAs stated in Section 4, calculating FID requires estimating a\n2048 \u00d7 2048 covariance matrix with 4 million entries. This\nrequires a large number of images causing FID to have poor\n7\nLow Distortion\nHigh Distortion\nFigure 7. Behavior of FID and CMMD under latent space noise\nadded to generated images. CMMD monotonically goes up, re-\nflecting the quality degradation of the images. FID\u2019s behavior is\ninconsistent, it mistakenly suggests an increase of quality. Image\nsets highlighted in green and yellow are visualized in Figure 5\u2019s\ntop and bottom rows, respectively.\nsample efficiency. This has also been noted by the authors\nof [5]. The proposed CMMD metric does not suffer from\nthis problem thanks to its usage of MMD distance instead\nof the Fr\u00b4echet distance.\nIn Figure 8 we illustrate this by evaluating a Stable Dif-\nfusion model at different sample sizes (number of images)\nsampled randomly from the COCO 30K dataset. Note that\nwe need more than 20,000 images to reliably estimate FID,\nwhereas CMMD provides consistent estimates even with\nsmall image sets. This has important practical implications:\ndevelopment of image generation models requires fast on-\nline evaluation, e.g. as a metric tracked during training. An-\nother relevant scenario is comparing a large number of mod-\nels. Since reliable estimation of FID requires generating a\nlarge number of images, FID evaluation is costly and time\nconsuming. In contrast, CMMD can be evaluated fast by\ngenerating only a small number of images. CMMD eval-\nuation is faster than FID evaluation for two reasons: 1) it\nrequires only a small number of images to be generated. 2)\nonce the images are generated the computation of CMMD\nis faster than the FID computation as discussed in the next\nsection.\n6.4. Computational Cost\nLet n be the number of images, and let d be the embedding\nlength. The cost of computing the Fr\u00b4echet distance (FD) is\ndominated by the matrix square root operation on a d \u00d7 d\nmatrix, which is expensive and not easily parallelizable.\nThe cost of computing the unbiased version FD\u221e is even\nhigher, since it requires computing FD multiple times with\ndifferent sample sizes. The asymptotic complexity of com-\nputing MMD is O(n2d). However, in practice, MMD can\nbe computed very efficiently, since it only involves matrix\nmultiplications which are trivially parallelizable and highly\noptimized in any deep learning library such as Tensorflow,\nPyTorch, and JAX.\nSample Size\nFID\nCMMD\n20.0\n40.0\n60.0\n80.0\n0.50\n0.60\n0.70\n0.80\n5000\n10000\n15000\n20000\n25000\n30000\nCMMD\nFID\nSample Size\nDiscrepancy\n100%\n150%\n200%\n250%\n300%\n350%\n5000\n10000\n15000\n20000\n25000\n30000\nCMMD\nFID\nFigure 8. Behavior of FID and CMMD under different sample\nsizes. Top: absolute values of the metrics. Bottom: Values relative\nto the value at 30k sample size.\nOperation\nTime\nFr\u00b4echet distance\n7007.59 \u00b1 231 ms\nMMD distance\n71.42 \u00b1 0.67 ms\nInception model inference\n2.076 \u00b1 0.15 ms\nCLIP model inference\n1.955 \u00b1 0.14 ms\nTable 4. Comparing runtime for computing Fr\u00b4echet/MMD dis-\ntances and Inception/CLIP feature extractions.\nTable 4 shows an empirical runtime comparison of com-\nputing FD and MMD on a set of size n = 30, 000 with\nd = 2048 dimensional features on a TPUv4 platform with a\nJAX implementation. For FD calculations, we use our JAX\nimplementation and publicly available PyTorch/numpy im-\nplementations from [19] and [5] and report the best runtime.\nIn the same table, we also report the runtime for Inception\nand CLIP feature extraction for a batch of 32 images.\n7. Discussion\nWe encourage image generation researchers to rethink the\nuse of FID as a primary evaluation metric for image quality.\nOur findings that FID correlates poorly with human raters,\nthat it does not reflect gradual improvement of iterative text-\nto-image models and that it does not capture obvious dis-\ntortions add to a growing body of criticism [5, 19]. We are\nconcerned that reliance on FID could lead to flawed rank-\nings among the image generation methods, and that good\nideas could be rejected prematurely. To address these con-\ncerns we propose CMMD as a more robust metric, suitable\nfor evaluation of modern text-to-image models.\n8\nAcknowledgment\nWe would like to thank Wittawat Jitkrittum for the valuable\ndiscussions.\nReferences\n[1] Shane Barratt and Rishi Sharma. A note on the inception\nscore, 2018. 2\n[2] Miko\u0142aj Bi\u00b4nkowski, Danica J. Sutherland, Michael Arbel,\nand Arthur Gretton. Demystifying MMD GANs, 2021. 2\n[3] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot,\nJose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy,\nWilliam T. Freeman, Michael Rubinstein, Yuanzhen Li, and\nDilip Krishnan. Muse: Text-to-image generation via masked\ngenerative transformers. ICML, 2023. 5, 6\n[4] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni,\nPiotr Padlewski, Daniel Salz, Sebastian Goodman, Adam\nGrycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov,\nJoan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari,\nGaurav Mishra, Linting Xue, Ashish Thapliyal, James Brad-\nbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia,\nBurcu Karagol Ayan, Carlos Riquelme, Andreas Steiner,\nAnelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu\nSoricut. Pali: A jointly-scaled multilingual language-image\nmodel, 2022. 5\n[5] Min Jin Chong and David A. Forsyth. Effectively unbiased\nFID and inception score and where to find them.\nCoRR,\nabs/1911.07023, 2019. 2, 4, 5, 8\n[6] D.C Dowson and B.V Landau. The Fr\u00b4echet distance between\nmultivariate normal distributions.\nJournal of Multivariate\nAnalysis, 12(3):450\u2013455, 1982. 3\n[7] Patrick Esser, Robin Rombach, and Bj\u00a8orn Ommer. Taming\ntransformers for high-resolution image synthesis. In CVPR,\n2021. 7\n[8] Kenji Fukumizu, Arthur Gretton, Bernhard Sch\u00a8olkopf, and\nBharath K. Sriperumbudur. Characteristic kernels on groups\nand semigroups. In NeurIPS. Curran Associates, Inc., 2008.\n5\n[9] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative Adversarial Nets. In NeurIPS,\n2014. 2\n[10] Arthur Gretton, Karsten Borgwardt, Malte Rasch, Bernhard\nSch\u00a8olkopf, and Alex Smola. A kernel method for the two-\nsample-problem. In NeurIPS. MIT Press, 2006. 5\n[11] Arthur Gretton, Karsten M. Borgwardt, Malte J. Rasch,\nBernhard Sch\u00a8olkopf, and Alexander Smola. A kernel two-\nsample test. J. Mach. Learn. Res., 13(1):723\u2013773, 2012. 5\n[12] Norbert Henze and Bernd Zirkler. A class of invariant con-\nsistent tests for multivariate normality. Communications in\nstatistics-Theory and Methods, 1990. 10\n[13] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. GANs Trained by\na Two Time-Scale Update Rule Converge to a Local Nash\nEquilibrium, 2018. 1, 2, 7\n[14] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks.\nCoRR, abs/1812.04948, 2018. 2\n[15] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft COCO: Common Objects in Context. In\nECCV, pages 740\u2013755. Springer, 2014. 6\n[16] K. V. Mardia. Measures of Multivariate Skewness and Kur-\ntosis with Applications. Biometrika, 1970. 10\n[17] Maurice Fr\u00b4echet. Sur la distance de deux lois de probabilit\u00b4e.\nAnnales de l\u2019ISUP, 1957. 3\n[18] Midjourney, 2022. https:://www.midjourney.com. 1\n[19] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu.\nOn\nAliased Resizing and Surprising Subtleties in GAN Evalu-\nation. In CVPR, 2022. 2, 3, 6, 8\n[20] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable visual\nmodels from natural language supervision. In ICML, 2021.\n2, 4, 5\n[21] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. preprint, 2022. 1\n[22] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, 2022. 6\n[23] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour, Burcu Karagol Ayan, S. Sara Mahdavi,\nRapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J\nFleet, and Mohammad Norouzi.\nPhotorealistic text-to-\nimage diffusion models with deep language understanding.\npreprint, 2022. [arXiv:2205.11487]. 1, 6\n[24] Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki\nCheung, Alec Radford, and Xi Chen. Improved techniques\nfor training gans. CoRR, abs/1606.03498, 2016. 2\n[25] Christian Szegedy,\nVincent Vanhoucke,\nSergey Ioffe,\nJonathon Shlens,\nand Zbigniew Wojna.\nRethinking\nthe inception architecture for computer vision.\nCoRR,\nabs/1512.00567, 2015. 2, 3\n[26] Laurens van der Maaten and Geoffrey E. Hinton. Visualiz-\ning high-dimensional data using t-sne. Journal of Machine\nLearning Research, 9:2579\u20132605, 2008. 4\n[27] Qiantong Xu, Gao Huang, Yang Yuan, Chuan Guo, Yu Sun,\nFelix Wu, and Kilian Q. Weinberger. An empirical study on\nevaluation metrics of generative adversarial networks. CoRR,\nabs/1806.07755, 2018. 2\n[28] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-\njan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-\nfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han,\nZarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and\nYonghui Wu. Scaling autoregressive models for content-rich\ntext-to-image generation. In ICML, 2022. 1, 6\n[29] Sharon Zhou, Mitchell L. Gordon, Ranjay Krishna, Austin\nNarcomey, Durim Morina, and Michael S. Bernstein. HYPE:\nhuman eye perceptual evaluation of generative models.\nCoRR, abs/1904.01121, 2019. 2\n9\nAppendix\nA. Multivariate Normality Tests\nFr\u00b4echet Inception Distance (FID) hinges on the multivariate\nnormality assumption. Since there is no canonical test, we\nshow that the Inception features for a typical image dataset\nlike COCO 30K do not satisfy this assumption using three\ndifferent widely-accepted statistical tests: Mardia\u2019s skew-\nness test [16], Mardia\u2019s kurtosis test [16] and Henze-Zirkler\ntest [12].\nThe null hypothesis for all of the tests is that the sample\nis drawn from a multivariate normal distribution. Different\ntests use different statistics as described below.\nMardia\u2019s Skewness Test\nFor a random sample of x1, x2, . . . , xn \u2208 Rd, a measure of\nmultivariate skewness is,\nA = 1\n6n\nn\nX\ni=1\nn\nX\nj=1\nh\n(xi \u2212 \u00afx)T \u02c6\u03a3\u22121(xj \u2212 \u00afx)\ni3\n.\n(5)\nWhere \u02c6\u03a3 is the biased sample covariance matrix, and \u00afx is\nthe sample mean.\nMardia [16] showed that under the null hypothesis that\nxis are multivariate normally distributed, the statistic A will\nbe asymptotically chi-squared distributed with d(d+1)(d+\n2)/6 degrees of freedom.\nTherefore, the normality of a\ngiven sample can be tested by checking how extreme the\ncalculated A-statistic is under this assumption. For Incep-\ntion embeddings computed on the COCO 30K dataset, this\ntest rejects the normality assumption with a p-value of 0.0,\nup to machine precision.\nMardia\u2019s Kurtosis Test\nFor a random sample of x1, x2, . . . , xn \u2208 Rd, a measure of\nmultivariate kurtosis is,\nB =\nr\nn\n8d(d + 2)\n(\n1\nn\nn\nX\ni=1\nh\n(xi \u2212 \u00afx)T \u02c6\u03a3\u22121(xi \u2212 \u00afx)\ni2\n\u2212 d(d + 2)\n)\n.\n(6)\nIt was shown in [16] that, under the null hypothesis that\nxis are multivariate normally distributed, the statistic B will\nbe asymptotically standard normally distributed. For Incep-\ntion embeddings computed on the COCO 30K dataset, this\ntest also rejects the normality assumption with a p-value\nof 0.0, To intuitively understand the confidence of the out-\ncome: this Mardia\u2019s test places the test statistics 19, 023\nstandard deviations away from the mean in a normal dis-\ntribution. This indicates the test\u2019s extreme confidence in\nrejecting the normality of Inception embeddings.\nHenze-Zirkler Test\nThe Henze-Zirkler test [12] is based on a functional that\nmeasures the distance between two distributions and has\nthe property that, when one of the distributions is standard\nmultivariate normal, it is zero if and only if the second dis-\ntribution is also standard multivariate normal. The Henze-\nZirkler test has been shown to be affine invariant and to have\nbetter power performance compared to alternative multi-\nvariate normal tests.\nThe Henze-Zirkler test\u2019s p-value for Inception embed-\ndings of COCO 30K is again 0.0 up to the machine preci-\nsion. Therefore, the Henze-Zirkler test also rejects the nor-\nmal assumption on Inception embeddings with overwhelm-\ningly high confidence.\nB. Synthetic Experiment Details\nIn this section, we discuss the details of the experiment de-\nscribed in Section 3.2. As the reference distribution, we\nuse an isotropic Gaussian distribution centered at the ori-\ngin with a covariance matrix \u03c32I2, where I2 is the 2 \u00d7 2\nidentity matrix. The second distribution consists of four\ndifferent equally-likely Gaussians, centered at the coordi-\nnates (\u03bb, 0), (0, \u03bb), (\u2212\u03bb, 0), (0, \u2212\u03bb), and each with the co-\nvariance matrix \u03c4 2\n\u03bbI2. In Table 2, we show the distribution\nvisualizations (first row), and the behavior of different dis-\ntance metrics (remaining rows) with increasing values of \u03bb.\nAs \u03bb increases, \u03c4\u03bb is adjusted as described below so that the\noverall covariance matrix of the mixture-of-Gaussians dis-\ntribution remains equal to \u03c32I2. Trivially, the mean of the\nmixture-of-Gaussians is the origin. Therefore, as \u03bb varies,\nboth the mean and the covariance matrix of the mixture-of-\nGaussians distribution remain equal to the reference distri-\nbution. Therefore, both FD and FD\u221e estimated using Eq. 2\nremain zero as \u03bb increases. This is obviously misleading as\nthe mixture-of-Gaussians distribution gets further and fur-\nther away from the reference as \u03bb increases. This error is\na direct consequence of the incorrect normality assumption\nfor the mixture-of-Gaussians distribution.\nTo see the relationship between \u03c4\u03bb and \u03bb that keeps the\noverall covariance matrix equal to \u03c32I2, consider a mix-\nture distribution consisting of 1-D PDFs f1, f2, . . . , fn with\nweights p1, p2, . . . , pn, where each pi > 0 and P\ni pi = 1.\nThe PDF of the mixture distribution is then given by f(x) =\nP\ni pifi(x). It follows from the definition of the expected\nvalue that, \u00b5(k) = P\ni pi\u00b5(k)\ni\n, where \u00b5(k) and \u00b5(k)\ni\nare the\nkth raw moment of f and fi, respectively. Recall also that\nvariance is \u00b5(2) \u2212 {\u00b5(1)}2. By applying the above result\nto x and y coordinates individually, we see that the over-\nall covariance matrix of the above mixture of four Gaus-\nsians, when they are away from the mean by \u03bb, is given by\n(\u03c4 2\n\u03bb + \u03bb2/2)I2. Setting \u03c4 2\n\u03bb = \u03c32 \u2212 \u03bb2/2 therefore keeps the\noverall covariance matrix at \u03c32I2 as we vary \u03bb.\n10\n"
  },
  {
    "title": "Improving fine-grained understanding in image-text pre-training",
    "link": "https://arxiv.org/pdf/2401.09865.pdf",
    "upvote": "12",
    "text": "2024-1-19\nImproving fine-grained understanding in\nimage-text pre-training\nIoana Bica1, Anastasija Ili\u01071, Matthias Bauer1, Goker Erdogan1, Matko Bo\u0161njak1, Christos Kaplanis1,\nAlexey A. Gritsenko1, Matthias Minderer1, Charles Blundell1, Razvan Pas,canu1 and Jovana Mitrovi\u01071\n1Google DeepMind\nWe introduce SPARse Fine-grained Contrastive Alignment (SPARC), a simple method for pretraining\nmore fine-grained multimodal representations from image-text pairs. Given that multiple image patches\noften correspond to single words, we propose to learn a grouping of image patches for every token in\nthe caption. To achieve this, we use a sparse similarity metric between image patches and language\ntokens and compute for each token a language-grouped vision embedding as the weighted average of\npatches. The token and language-grouped vision embeddings are then contrasted through a fine-grained\nsequence-wise loss that only depends on individual samples and does not require other batch samples\nas negatives. This enables more detailed information to be learned in a computationally inexpensive\nmanner. SPARC combines this fine-grained loss with a contrastive loss between global image and text\nembeddings to learn representations that simultaneously encode global and local information. We\nthoroughly evaluate our proposed method and show improved performance over competing approaches\nboth on image-level tasks relying on coarse-grained information, e.g. classification, as well as region-level\ntasks relying on fine-grained information, e.g. retrieval, object detection, and segmentation. Moreover,\nSPARC improves model faithfulness and captioning in foundational vision-language models.\n1. Introduction\nContrastive pre-training from large-scale, noisy image-text datasets (Jia et al., 2021; Radford et al.,\n2021) has become a widely used paradigm for learning general vision representations useful for a\nwide range of downstream tasks as well as for learning vision encoders in multimodal foundation\nmodels (Alayrac et al., 2022; Chen et al., 2022; Li et al., 2022a). By aligning global image and text\nrepresentations in a shared latent space using similar and dissimilar image-text pairs, these models\nachieve impressive performance on image-level vision tasks like classification (Radford et al., 2021),\ncoarse-grained retrieval and visual question answering (Alayrac et al., 2022; Chen et al., 2022). On\nthe other hand, these models have been shown to discard fine-grained visual information (Krojer\net al., 2022) and work poorly on downstream tasks involving localization (Ranasinghe et al., 2022;\nZhong et al., 2022), counting (Paiss et al., 2023) and understanding spatial relationships between\nobjects (Parcalabescu et al., 2021) or object attributes (Yuksekgonul et al., 2022). These shortcomings\nare further exacerbated when these pretrained models are used in foundation models (Alayrac et al.,\n2022; Chen et al., 2022; Li et al., 2022a) or when they are used to initialize models for object\ndetection (Minderer et al., 2022) or segmentation (Zhou et al., 2022).\nA recent line of work has started to explore incorporating losses between image patch and text\ntoken embeddings (Huang et al., 2021; Mukhoti et al., 2023; Wang et al., 2022; Yao et al., 2021)\nto learn representations encoding more fine-grained details. Motivated by the idea of aligning\npatches corresponding to individual objects in the image to tokens corresponding to the words\ndescribing these objects, these local losses learn soft correspondences between image patches and\ntext tokens from image-text pairs. While these models have achieved improved performance on\nfine-grained retrieval (Yao et al., 2021), image classification (Yao et al., 2021), object detection\nand segmentation (Mukhoti et al., 2023; Wang et al., 2022), they are computationally and memory\nCorresponding author(s): [ioanab,anastasijailic,msbauer,gokererdogan,matko,kaplanis,agritsenko,mjlm,cblundell,razp,mitrovic]@google.com\n\u00a9 2024 Google DeepMind. All rights reserved\narXiv:2401.09865v1  [cs.CV]  18 Jan 2024\nImproving fine-grained understanding in image-text pre-training\nexpensive, unstable during training (Yao et al., 2021) and/or rely on pretrained models to kickstart\nlearning.\n\u201ccat\u201d\n\u201cand\u201d\n\u201cdog\u201d\n\u201cin\u201d\n\u201cbasket\u201d\n\u00b7\n\u00b7\n\u00b7\n\u00b7\n+\nSparse cross-modal alignment\nToken embeddings\nLanguage-grouped\nvision embeddings\n\u201ccat\u201d\n\u201cand\u201d\n\u201cdog\u201d\n\u201cin\u201d\n\u201cbasket\u201d\nFinegrained loss\nToken embeddings\nFigure 1 | For every text token, SPARC learns a correspond-\ning language-grouped vision embedding as the alignment-\nweighted combination of patches that are most similar to that\ntoken. We calculate a sparse similarity metric between tokens\nand patches of individual image-text pairs (left) and use it\nto compute the resulting alignment weights (middle). We\ncontrast the language-grouped vision embeddings with token\nembeddings in a fine-grained contrastive sequence-wise loss\n(right).\nIn this work, we propose SPARse\nFine-grained Contrastive Alignment\n(SPARC), a novel objective for multi-\nmodal pretraining which learns rep-\nresentations that encode both coarse-\ngrained/global and fine-grained/local\ninformation.\nWe propose to build\nlanguage-grouped vision embeddings\nby learning to aggregate (in an unsu-\npervised way) image patches corre-\nsponding to individual words in the\ncaption; this is motivated by the ob-\nservation that usually multiple image\npatches correspond to one word in\nthe caption. As a first step, SPARC\ncomputes the similarity between the\npatch and token embeddings of an in-\ndividual image-text pair and enforces\nsparsity in the resulting similarity ma-\ntrix. This sparsification enables only\nthe most relevant image patches to be\nattributed to individual tokens. Next,\nas illustrated in Figure 1, for every\ntext token, we compute the corre-\nsponding language-grouped vision embedding as the alignment-weighted sum of the patch em-\nbeddings, where the alignment weights are computed from the sparsified similarity matrix. The\nresulting language-grouped vision embeddings are contrasted with the token embeddings from\nthe same image-text pair by optimizing for the similarity between individual tokens and their cor-\nresponding language-grouped vision embedding and dissimilarity to all other language-grouped\nvision embeddings. SPARC combines the resulting fine-grained/local contrastive loss with a global\ncontrastive loss between image and text embeddings which enables it to simultaneously encode global\nand local information in the learned representations.\nThrough its design choices, SPARC addresses several shortcomings of existing methods for learning\nimage representations with more fine-grained information. Firstly, several of these methods (Huang\net al., 2021; Mukhoti et al., 2023; Yao et al., 2021) learn representations with fine-grained losses that\ncompute similarities between all image patch embeddings and all text token embeddings in a batch.\nThis approach is both computationally and memory intensive and does not scale to large batch sizes\n(which are needed for obtaining good performance for contrastive methods (Jia et al., 2021; Radford\net al., 2021; Zhai et al., 2023b)). On the other hand, SPARC contrasts patch and token embeddings\nat the level of individual image-text pairs and does not use other examples from the batch to compute\nthe similarity matrix which leads to more favourable computation and memory footprints and more\neasily scales to large batch sizes. Secondly, for learning soft correspondences between image patches\nand text tokens, prior work (Huang et al., 2021; Mukhoti et al., 2023; Wang et al., 2022) usually\nrelies on building cross-modal weighted representations with weights computed as a softmax over\npatch and token embedding similarities. The winner-takes-all dynamics of softmax (Elfadel and\nWyatt Jr, 1993; Peterson and S\u00f6derberg, 1989) strongly bias learning towards one-to-one mappings\n2\nImproving fine-grained understanding in image-text pre-training\nbetween individual text tokens and image patches which often does not correspond to underlying\ndata. For example, in an image of a dog, the token embedding for \u201cdog\u201d should be matched with all\npatch embeddings that correspond to the dog in the image and not just one/a few. Moreover, softmax\ncan be problematic from a gradient flow perspective (Hoffmann et al., 2023; Shen et al., 2023; Zhai\net al., 2023a) as it tends to lead to a low entropy distribution, where softmax saturates and therefore\nits Jacobian vanishes (Hoffmann et al., 2023). See Appendix A for a more detailed explanation.\nOn the flip side, SPARC does not use softmax for calculating the alignment weights which allows it\nto learn a flexible one-to-many matching between individual tokens and the corresponding image\npatches and to avoid the winner-take-all dynamics of softmax. Thirdly, several of these approaches\nstart from contrastively pre-trained vision-language models (Mukhoti et al., 2023) or from pre-trained\nlanguage models (Huang et al., 2021; Wang et al., 2022). Moreover, existing fine-grained objectives\nhave been developed in different communities (i.e. medical (Huang et al., 2021; Wang et al., 2022)\nvs. general vision (Mukhoti et al., 2023; Yao et al., 2021)) leveraging different types and sizes of\ndatasets, architectures and pretraining setups. This makes it difficult to compare different approaches\nand assess the benefits of using individual fine-grained objectives.\nTo summarize, our main contributions are as follows:\n\u2022 We propose SPARC, a novel method for pre-training multimodal models on large-scale noisy\nimage-text data which learns both coarse-grained and fine-grained information.\n\u2022 Through an extensive experimental evaluation, we show that SPARC significantly improves per-\nformance on both fine-grained and coarse-grained downstream tasks over competing methods.\n\u2022 For the first time in the literature, we perform a thorough like-for-like comparison on the benefits\nof different fine-grained objectives for large-scale pretraining of multimodal models.\n2. Sparse Fine-grained Contrastive Alignment\nLet B = {(\ud835\udc99\ud835\udc63\n1, \ud835\udc99\ud835\udc61\n1), (\ud835\udc99\ud835\udc63\n2, \ud835\udc99\ud835\udc61\n2), . . . , (\ud835\udc99\ud835\udc63\n\ud835\udc35, \ud835\udc99\ud835\udc61\n\ud835\udc35)} be a mini-batch of image-text pairs. Let \ud835\udc53\ud835\udc63(\u00b7) be the image\nencoder, \ud835\udc53\ud835\udc61(\u00b7) the text encoder and \ud835\udc54\ud835\udc63(\u00b7) and \ud835\udc54\ud835\udc61(\u00b7) linear adaptors. For an image \ud835\udc99\ud835\udc63\n\ud835\udc56 , we denote the\ncorresponding patches as (\ud835\udc99\ud835\udc63\n\ud835\udc56,1, \ud835\udc99\ud835\udc63\n\ud835\udc56,2, . . . , \ud835\udc99\ud835\udc63\n\ud835\udc56,\ud835\udc43) and the patch embeddings as (\ud835\udc97\ud835\udc56,1, \ud835\udc97\ud835\udc56,2, . . . , \ud835\udc97\ud835\udc56,\ud835\udc43) with\n\ud835\udc97\ud835\udc56,\ud835\udc5d = \ud835\udc54\ud835\udc63( \ud835\udc53\ud835\udc63(\ud835\udc99\ud835\udc63\n\ud835\udc56,\ud835\udc5d)) \u2208 \u211d\ud835\udc51; \ud835\udc43 denotes the number of patch embeddings. We calculate the global vision\nembedding as \ud835\udc97\ud835\udc56 = \ud835\udc54\ud835\udc63(\u210e\ud835\udc63(avg_pool({ \ud835\udc53\ud835\udc63(\ud835\udc99\ud835\udc63\n\ud835\udc56,\ud835\udc5d)}\ud835\udc43\n\ud835\udc5d=1))) with \u210e\ud835\udc63 being a single non-linear layer that facili-\ntates the encoding of different granularities of information. For the corresponding text \ud835\udc99\ud835\udc61\n\ud835\udc56, we denote\nthe tokens as (\ud835\udc99\ud835\udc61\n\ud835\udc56,1, \ud835\udc99\ud835\udc61\n\ud835\udc56,2, . . . , \ud835\udc99\ud835\udc61\n\ud835\udc56,\ud835\udc3f\ud835\udc56) with \ud835\udc3f\ud835\udc56 the number of tokens for sample \ud835\udc56. The token embeddings\n(\ud835\udc95\ud835\udc56,1, \ud835\udc95\ud835\udc56,2, . . . , \ud835\udc95\ud835\udc56,\ud835\udc3f\ud835\udc56) are computed as \ud835\udc95\ud835\udc56,\ud835\udc59 = \ud835\udc54\ud835\udc61( \ud835\udc53\ud835\udc61(\ud835\udc99\ud835\udc61\n\ud835\udc56,\ud835\udc59)) and the global text embedding \ud835\udc95\ud835\udc56 is computed by\naverage pooling { \ud835\udc53\ud835\udc61(\ud835\udc99\ud835\udc61\n\ud835\udc56,\ud835\udc59)}\ud835\udc3f\ud835\udc56\n\ud835\udc59=1 and applying the adaptor \ud835\udc54\ud835\udc61, i.e. \ud835\udc95\ud835\udc56 = \ud835\udc54\ud835\udc61(avg_pool({ \ud835\udc53\ud835\udc63(\ud835\udc99\ud835\udc61\n\ud835\udc56,\ud835\udc59)}\ud835\udc3f\ud835\udc56\n\ud835\udc59=1).\nGlobal alignment:\nIn order to learn global information, SPARC uses the global contrastive loss (Jia\net al., 2021; Radford et al., 2021) which operates at the level of global image (\ud835\udc97) and global text\nembeddings (\ud835\udc95). Specifically, we learn image and text embeddings by maximizing the similarity to\nthe corresponding text and image embeddings, while minimizing the similarity to other text and\nimage embeddings in the batch, i.e. we optimize\n\ud835\udc3f\ud835\udc54 = \u2212 1\n2\ud835\udc35\n\ud835\udc35\n\u2211\ufe01\n\ud835\udc56=1\n \nlog\nexp(\ud835\udf19(\ud835\udc97\ud835\udc56, \ud835\udc95\ud835\udc56)/\ud835\udf0f)\n\u00cd\ud835\udc35\n\ud835\udc57=1 exp(\ud835\udf19(\ud835\udc97\ud835\udc56, \ud835\udc95 \ud835\udc57)/\ud835\udf0f)\n+ log\nexp(\ud835\udf19(\ud835\udc95\ud835\udc56, \ud835\udc97\ud835\udc56)/\ud835\udf0f)\n\u00cd\ud835\udc35\n\ud835\udc57=1 exp(\ud835\udf19(\ud835\udc95\ud835\udc56, \ud835\udc97\ud835\udc57)/\ud835\udf0f)\n!\n,\n(1)\nwith \ud835\udf19(\ud835\udc97\ud835\udc56, \ud835\udc95 \ud835\udc57) =\n\u00af\ud835\udc97\ud835\udc56\n\u2225\u00af\ud835\udc97\ud835\udc56\u22252 \u00b7\n\u00af\ud835\udc95\ud835\udc57\n\u2225\u00af\ud835\udc95\ud835\udc57\u22252 and \ud835\udf0f as temperature.\n3\nImproving fine-grained understanding in image-text pre-training\nsparsify and\nnormalize\nSimilarity matrix sl,p\nAlignment weights al,p\nglobal text embedding\n0\n75\n93\n17\n\u201da picture of a\ncat and dog\u201d\nText encoder\nglobal vision embedding\nVision encoder\nFinegrained alignment\n.\nlanguage-grouped\nvision embeddings\nGlobal\nalignment\nFigure 2 | Overall architecture for SPARC. The global alignment loss maximizes the similarity between\nthe global vision and global text embeddings, while minimizing the similarity with the other global\nembeddings in the batch. To obtain the finegrained alignment, we compute the similarity between\nthe patch embeddings and the token embeddings and then sparsify and normalize the resulting\nsimilarity matrix to obtain alignment weights. These alignment weights are then used to group the\npatch embeddings. The resulting language-grouped vision embeddings are then contrasted to the\ntoken emebddings in a sequence-wise finegrained alignment loss.\nFinegrained alignment:\nMotivated by the observation that usually multiple image patches cor-\nrespond to one word in the caption, we propose to learn groupings of patches that correspond to\nindividual text tokens. Specifically, for every token embedding we learn a corresponding language-\ngrouped vision embedding as an alignment-weighted combination of patches that encode that token in\nthe visual domain. We propose to compute the alignment weights based on the similarity between\ntoken and patch embeddings of the corresponding image-text pair. To facilitate the grouping of\nappropriate patch embeddings given a text token we sparsify and min-max normalize the similarity\nmatrix to compute the alignment weights. To learn language-grouped vision embeddings, we propose\na fine-grained local loss that optimizes for the alignment between individual token embeddings and\ntheir corresponding language-grouped vision embeddings within a given image-text pair. Specifically,\nwe propose a sequence-wise contrastive loss to optimize this fine-grained alignment within SPARC.\nOptimizing this loss (in addition to the global contrastive loss above) biases the learned representation\nto preserve detailed information about the image (as described by the caption) instead of just the\nglobal information sufficient to minimize the global contrastive loss.\nFor an image-text pair, let \ud835\udc60\ud835\udc56,\ud835\udc59\ud835\udc5d represent the similarity between text token embedding \ud835\udc95\ud835\udc56\ud835\udc59 and image\npatch embedding \ud835\udc97\ud835\udc56\ud835\udc5d, i.e. \ud835\udc60\ud835\udc56,\ud835\udc59\ud835\udc5d = \ud835\udc95\ud835\udc56\ud835\udc59 \u00b7 \ud835\udc97\ud835\udc56\ud835\udc5d, where \ud835\udc60\ud835\udc56,\ud835\udc59\ud835\udc5d \u2208 \u211d\ud835\udc3f\u00d7\ud835\udc45 and \u00b7 is the inner product. Going forward\nwe drop the example index \ud835\udc56 for simplicity. To obtain alignment weights, for each token \ud835\udc57, we first\nnormalize \ud835\udc60\ud835\udc59\ud835\udc5d to [0, 1] using min-max normalization across columns (i.e. patches):\n\u02c6\ud835\udc60\ud835\udc59\ud835\udc5d =\n\ud835\udc60\ud835\udc59\ud835\udc5d \u2212 min\ud835\udc58 \ud835\udc60\ud835\udc59\ud835\udc58\nmax\ud835\udc58 \ud835\udc60\ud835\udc59\ud835\udc58 \u2212 min\ud835\udc58 \ud835\udc60\ud835\udc59\ud835\udc58\n(2)\nWe sparsify the similarity matrix \ud835\udc46 = (\u02c6\ud835\udc60\ud835\udc57\ud835\udc58)1\u2264 \ud835\udc57\u2264\ud835\udc3f,1\u2264\ud835\udc58\u2264\ud835\udc43 to facilitate learning and to encourage each\ntoken to be aligned to a few of the patches, i.e.\n\u02dc\ud835\udc60\ud835\udc57\ud835\udc58 =\n(\n\u02c6\ud835\udc60\ud835\udc57\ud835\udc58\nif \u02c6\ud835\udc60\ud835\udc57\ud835\udc58 \u2265 \ud835\udf0e\n0\notherwise\n(3)\n4\nImproving fine-grained understanding in image-text pre-training\nwith \ud835\udc43 the number of patch embeddings of an image and \ud835\udf0e the sparsity threshold. We compute\nalignment weights as\n\ud835\udc4e\ud835\udc57\ud835\udc58 =\n\u02dc\ud835\udc60\ud835\udc57\ud835\udc58\n\u00cd\ud835\udc45\n\ud835\udc5f=1 \u02dc\ud835\udc60\ud835\udc57\ud835\udc5f\n(4)\nwhere \ud835\udc4e\ud835\udc57\ud835\udc58 represents the weight of patch \ud835\udc58 for computing the language-grouped vision embedding\ncorresponding to token \ud835\udc57. Note that this approach enables a flexible mapping between a token and\narbitrarily many patch embeddings that encode that token in the visual domain, e.g. all of the image\npatches corresponding to \u201cdog\u201d can be matched to the token encoding \u201cdog\u201d. For every token \ud835\udc61\ud835\udc59 we\ncompute the corresponding language-grouped vision embedding \ud835\udc84\ud835\udc59 as\n\ud835\udc84\ud835\udc59 =\n\ud835\udc45\n\u2211\ufe01\n\ud835\udc5f=1\n\ud835\udc4e\ud835\udc59\ud835\udc5f\ud835\udc97\ud835\udc5f\n(5)\nas the alignment-weighted combination of patch embeddings with \ud835\udc45 the number of patches with\nnon-zero alignment weight.\nTo learn fine-grained information we propose to optimize the alignment between token embeddings\nand their corresponding language-grouped vision embeddings. Specifically we propose a fine-grained\ncontrastive loss that operates over sequences of tokens and patches at the level of each image-text pair\nand does not require negatives from other image-text pairs. This considerably reduced computation\nand memory costs over previous methods (Huang et al., 2021; Yao et al., 2021) that require samples\nfrom the whole batch in order to compute their fine-grained losses. SPARC optimizes the following\nfine-grained alignment contrastive loss\n\ud835\udc3f \ud835\udc53 = \u2212 1\n2\ud835\udc35\n\ud835\udc35\n\u2211\ufe01\n\ud835\udc56=1\n\uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n1\n\ud835\udc3f\ud835\udc56\n\ud835\udc3f\ud835\udc56\n\u2211\ufe01\n\ud835\udc57=1\n \nlog\nexp(\ud835\udf19(\ud835\udc84\ud835\udc56\ud835\udc57, \ud835\udc95\ud835\udc56\ud835\udc57)/\ud835\udf0f)\n\u00cd\ud835\udc3f\ud835\udc56\n\ud835\udc58=1 exp(\ud835\udf19(\ud835\udc84\ud835\udc56\ud835\udc57, \ud835\udc95\ud835\udc56\ud835\udc58)/\ud835\udf0f)\n+ log\nexp(\ud835\udf19(\ud835\udc95\ud835\udc56\ud835\udc57, \ud835\udc84\ud835\udc56\ud835\udc57)/\ud835\udf0f)\n\u00cd\ud835\udc3f\ud835\udc56\n\ud835\udc58=1 exp(\ud835\udf19(\ud835\udc95\ud835\udc56\ud835\udc57, \ud835\udc84\ud835\udc56\ud835\udc58)/\ud835\udf0f)\n!#\n,\n(6)\nwhich tries to maximize the similarity of every token embedding with its corresponding language-\ngrouped vision embedding and minimize the similarity to other language-grouped vision embeddings\nin the sequence and vice versa.\nOverall objective:\nThe overall SPARC objective is a weighted sum of the global contrastive loss\nand the finegrained alignment constrastive loss:\n\ud835\udc3fSPARC = \ud835\udf06\ud835\udc54\ud835\udc3f\ud835\udc54 + \ud835\udf06 \ud835\udc53 \ud835\udc3f \ud835\udc53\n(7)\nwhere \ud835\udf06\ud835\udc54 and \ud835\udf06 \ud835\udc53 are hyperparameters. We provide the pseudo-code for SPARC in Appendix C.\nSparsity threshold.\nWe choose the sparsity threshold \ud835\udf0e to be equal to 1/\ud835\udc43 with \ud835\udc43 the number of\nimage patches. This choice is motivated by the consideration that every text token should attend\nto at least to one image patch. Since we use the min-max normalization the smallest similarity of\n1/\ud835\udc43 is achieved when all patches are equally similar as the number of patches is constant. Note that\nthis threshold naturally allows for the number of patches corresponding to one token to considerably\nvary between tokens within an image as well as across images; this enables the same class of objects\n(e.g. \u201cdogs\u201d) to be appropriately represented irrespective of the difference in sizes, scales and shapes\nacross different instances within and across images. Note also that the threshold also allows for the\ndecoupling of similarities of individual patches to different tokens as it allows for different number\nof zero entries in different rows of the similarity matrix; thus, whether and how much a patch is\nsimilar to a token, has no bearing to how similar it is to a different token which is useful e.g. in\nsituations when we have more detailed captions (e.g. \u201clarge brown dog\u201d) and/or when a single word\nis represented by multiple tokens.\n5\nImproving fine-grained understanding in image-text pre-training\n3. Related work\nContrastive image-text pre-training\nCLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021)\npopularized learning general visual representations by leveraging textual supervision from noisy large-\nscale data scrapped from the internet. These methods learn representations through a contrastive\nobjective that maximises the similarity between the representation of the whole image and the\nrepresentation of the full text of matched image-text pairs and minimizes the similarity between\nthe remaining image-text pairs within the batch. However, learning visual representations through\nmatching the global image and text embeddings can result in a coarse visual representation that\ndiscards many fine-grained details (i.e all details that are not needed for differentiating the matching\nof global text embedding from the other text embeddings in the batch). To address this problem,\nFILIP (Yao et al., 2021) proposes a cross-modal late interaction mechanism, which optimizes the\ntoken-wise maximum similarity between image and text tokens through a contrastive objective.\nWhile this approach achieves a finer-grained alignment between image patches and words in the\ntext, computing the token-wise similarity between all image patches and text tokens in the batch\nbecomes memory inefficient for large batch sizes so they use several tricks during pre-training to\naddress this issue. A related approach PACL (Mukhoti et al., 2023) starts from CLIP-pretrained\nvision and text encoders and trains on top of the frozen representations an adapter to obtain better\nfine-grained understanding. The adapter is a two-layer MLP with a residual connection and is trained\nthrough a contrastive objective that compares the global text embedding and a weighted global image\nembedding with the weights calculated using the cosine similarity between individual image patches\nand the global text embedding.\nIn a parallel stream of work, several methods have been proposed in the medical literature to\nlearn visual representation using medical images - radiology report pairs from small scale datasets\n(consisting of up to 200k data points) (Dawidowicz et al., 2023; Huang et al., 2021; Wang et al., 2022).\nGLoRIA (Huang et al., 2021) builds localized visual representations by contrasting attention-weighted\npatch embeddings with the text tokens, where the attention weights are computed through softmax on\nthe similarity matrix between the patch and token embeddings. Similarly to FILIP, the local objective\nin GLoRIA requires computing the similarity between all patch and token embeddings within the batch\nwhich is computationally intensive and does not scale to large batch sizes. Alternatively, MGCA (Wang\net al., 2022) considers a token-wise fine-grained loss that employs a bidirectional multi-head attention\nstrategy to learn the matching between image patch and token embedding. While this is more efficient\nto compute, learning these matchings through a bidirectional multi-head cross-attention strategy adds\nmore parameters to the dual encoders, involves tuning several additional hyperparameters and suffers\nfrom the same problems with using softmax for computing the attention weights. MGCA also uses a\ndomain-specific disease-level alignment loss that enforce a cluster assignment consistency to leverage\ninter-subject semantic correspondences. More recent methods (Dawidowicz et al., 2023) consider\nincorporating into the pre-training objective not only fine-grained losses similar to the ones used in\nGLoRIA and MGCA, but also domain-specific features and image views. Note that these methods\nfrom the medical literature start from a text encoder pre-trained with medical texts (Alsentzer et al.,\n2019), while we consider the case of pre-training the image and text encoders jointly from scratch.\nFine-grained understanding in vision-language models\nAlternative approaches for improving the\nfine-grained capabilities of vision-language models require pre-trained modules, specialised networks\nand human annotations. One line of work, proposes matching image regions to textual descriptions\nthrough contrastive losses, where the image regions - text description pairs are obtained from human\nannotations (Li et al., 2022b) or by using region proposal networks (Ren et al., 2015) and various\ntext matching approaches (Varma et al., 2023; Zhong et al., 2022). A separate line of work adds\na cross-modal encoder (with significant extra parameters) on top of the dual image-text encoder\n6\nImproving fine-grained understanding in image-text pre-training\nand uses captioning (Li et al., 2022a; Yu et al., 2022), masked language modelling (Li et al., 2021;\nYang et al., 2022), image-text matching (Li et al., 2021; Yang et al., 2022; Zeng et al., 2021) and\nbounding box prediction losses (Zeng et al., 2021) (with bounding boxes obtained from human-\nannotations (Krishna et al., 2017; Kuznetsova et al., 2020; Shao et al., 2019)). For more related\nworks see Appendix B.\n4. Experiments\nWhile there has been significant interest in learning fine-grained representations, the breadth of\ntraining setups used in the literature have made it difficult to compare different fine-grained objectives.\nSpecifically the use of custom datasets (Yao et al., 2021) and pretrained language and/or vision\nmodels (Huang et al., 2021; Mukhoti et al., 2023; Wang et al., 2022) have made it difficult to\ndiscern the benefit of individual fine-grained losses on learning more detailed representations. In\nthis work we want to enable a like-for-like comparison and understand the impact of SPARC and\ncompeting fine-grained losses on downstream performance. For this purpose, we reimplement all\ncompeting baselines: CLIP (Radford et al., 2021), FILIP (Yao et al., 2021), PACL (Mukhoti et al.,\n2023), MGCA (Wang et al., 2022) and GLoRIA (Huang et al., 2021), and use the same pretraining\ndatasets, architecture and number of training steps when training with the different objectives; we\npretrain randomly initialized networks. We thoroughly evaluate the learned representations across a\nbroad range of tasks and datasets, ranging from coarse-grained image-level tasks like classification\nand retrieval to fine-grained tasks like object detection and semantic segmentation. Unlike some\ncompeting methods that improve fine-grained understanding at the cost of decreasing coarse-grained\ntask performance, SPARC simultaneously boosts performance over both coarse- and fine-grained tasks\nacross a number of different benchmarks.\n4.1. Experimental setup\nModel architectures Following the literature, we use Vision Transformers (ViTs) (Dosovitskiy et al.,\n2020) as image encoders and Transformers (Vaswani et al., 2017) as text encoders. We experiment\nwith ViT-B/32, ViT-B/16 and ViT-L/14 and pair them with corresponding language models. See\ndetails in Appendix D.\nDatasets\nWe train using large-scale datasets ALIGN (Jia et al., 2021), JFT (Sun et al., 2017; Zhai\net al., 2022) and LTIP (Long Text & Image Pairs) (Alayrac et al., 2022). ALIGN has 1.8 billion\nimages paired with noisy alt-text, JFT has of 4 billion images semi-automatically annotated with a\nclass-hierarchy of 30k labels, while LTIP has 312 million higher-quality images - text pairs with richer\nimage captions. See Appendix D for more details.\nPre-training details\nWe resize images to the 224 \u00d7 224 resolution and tokenize the text with a 32k\nvocabulary sentencepiece tokenizer (Kudo and Richardson, 2018) while keeping a maximum number\nof 55 tokens for each caption. We train all models using the AdamW (Loshchilov and Hutter, 2017)\noptimizer, a cosine learning rate schedule with linear warm-up and weight decay regularization.\nWe use a batch size of 16348 and we pre-train the ViT-B models for 200k steps (\u2248 3.2 billion data\npoints) and the ViT-L models for 250k steps (\u2248 4.1 billion data points). See Appendix D for more\nhyperparameter details.\n7\nImproving fine-grained understanding in image-text pre-training\nObjective\nIN\nIN-V2 Th\nIN-V2 MF\nIN-V2 TI\nIN-R\nIN-C\nIN-A\nIN-Sketch\nViT-B/32\nCLIP\n66.7\n66.2\n58.9\n71.5\n63.2\n42.6\n15.1\n51.7\nFILIP\n52.7\n50.7\n44.0\n55.8\n47.1\n28.7\n8.4\n38.2\nPACL\n58.9\n56.9\n50.0\n62.6\n54.0\n34.9\n9.3\n44.1\nGloRIA\n62.8\n61.5\n54.3\n66.7\n56.7\n38.4\n11.2\n47.5\nMGCA\n66.0\n64.5\n56.4\n69.5\n62.0\n41.1\n14.7\n51.7\nSPARC (ours)\n68.1\n67.0\n59.7\n72.0\n64.9\n44.5\n16.7\n53.2\nViT-B/16\nCLIP\n71.6\n70.9\n63.7\n74.8\n71.1\n48.5\n32.2\n56.8\nFILIP\n56.6\n55.6\n48.9\n59.7\n54.0\n33.2\n14.4\n43.1\nPACL\n61.1\n59.6\n52.6\n64.8\n56.3\n36.1\n12.8\n45.2\nGloRIA\n67.4\n66.9\n59.8\n71.7\n66.6\n43.8\n24.6\n54.2\nMGCA\n69.6\n69.3\n62.2\n73.6\n68.8\n46.1\n29.0\n55.0\nSPARC (ours)\n72.6\n71.1\n64.4\n75.0\n72.0\n48.5\n33.8\n57.3\nViT-L/4\nCLIP\n77.3\n75.9\n69.5\n79.1\n78.8\n59.6\n52.5\n64.5\nMGCA\n75.6\n73.9\n68.0\n77.9\n77.2\n56.0\n45.0\n63.1\nSPARC (ours)\n78.2\n76.9\n70.6\n80.0\n79.3\n59.7\n51.9\n65.4\nTable 1 | Top-1 accuracy (in %) of zero-shot classification on ImageNet (IN) and its variants ImageNet-\nV2 Threshold (IN-V2 Th), ImageNet-V2 Matched Frequency (In-V2 MF), ImageNet-V2 Top Images\n(IN-V2 TI), ImageNet-R (IN-R), ImageNet-C (IN-C), ImageNet-Sketch (IN-Sketch).\n4.2. Zero-shot image classification\nWe first evaluate SPARC on the coarse-grained task of zero-shot image classification. Specifically we\ntest zero-shot classification on ImageNet (Russakovsky et al., 2015) and a number of datasets testing\nfor specific capabilities like robustness to perturbations and various distribution shifts; we choose\nImageNetV2 (Recht et al., 2019), ImageNet-R (Hendrycks et al., 2021), ImageNet-C (Hendrycks and\nDietterich, 2019), ImageNet-A (Hendrycks et al., 2019) and ImageNet-Sketch (Wang et al., 2019) for\nthis purpose. We follow a similar protocol to (Radford et al., 2021) for the evaluation, and compute\nresults for both one prompt per example (i.e. the class label) in Table 1 and when using prompt\nensembling in Table 2. For more details on the evaluation protocol please see Appendix D. From both\nTable 1 and Table 2 we see that SPARC outperforms or matches competing methods in all settings and\nacross different ViT architectures. Specifically, SPARC shows very effective information encoding from\nlarger patches as exhibited by the significant improvements over baselines for ViT B/32, especially on\nImageNet-R, -C, -A and -Sketch showcasing the robustness to perturbations and adversarial examples.\nMoreover, we notice that while prompt ensembling improves performance of all methods on zero-shot\nimage classification (which is in line with the literature) the performance gain from SPARC are still\npreserved in this evaluation setting.\nNote that PACL (Mukhoti et al., 2023), GLoRIA (Huang et al., 2021) and MGCA (Wang et al., 2022)\nwere developed with the use of pretrained language and/or vision encoders in mind, whereas here\nthey are tested in a pretraining from scratch setting. From Table 1 and Table 2, we see that in the\npretraining setting PACL and GLoRIA underperform CLIP, whereas MGCA shows more competitive\nperformance to CLIP. On the other hand, FILIP (Yao et al., 2021), which was developed as a fine-\ngrained objective for pretraining from scratch, has proven highly unstable to train across a wide\nrange of learning rates and weight decay parameters which lead to decreased performance. This\ntraining difficulty has also been noted in the original paper (Yao et al., 2021) (cf. in the Appendix\n8\nImproving fine-grained understanding in image-text pre-training\nObjective\nIN\nIN-V2 Th\nIN-V2 MF\nIN-V2 TI\nIN-R\nIN-C\nIN-A\nIN-Sketch\nViT-B/32\nCLIP\n69.0\n68.8\n60.4\n73.4\n62.4\n44.6\n15.8\n52.4\nFILIP\n56.8\n54.8\n48.4\n60.0\n44.6\n30.8\n7.8\n39.6\nPACL\n61.2\n59.5\n51.9\n65.2\n52.9\n36.4\n9.3\n45.2\nGloRIA\n65.9\n64.8\n57.0\n69.6\n57.4\n40.7\n11.7\n48.7\nMGCA\n68.6\n67.4\n59.2\n72.6\n61.0\n43.5\n14.1\n50.9\nSPARC (ours)\n70.4\n69.6\n62.1\n74.5\n63.2\n46.5\n17.3\n52.7\nViT-B/16\nCLIP\n73.9\n73.6\n66.1\n77.1\n68.8\n50.4\n32.5\n57.3\nFILIP\n61.4\n61.0\n53.8\n65.6\n53.2\n35.9\n14.2\n45.1\nPACL\n63.3\n61.7\n54.4\n66.8\n54.1\n37.3\n12.9\n45.4\nGloRIA\n70.4\n70.0\n62.8\n74.7\n65.7\n46.4\n25.0\n54.8\nMGCA\n72.7\n72.7\n65.3\n76.3\n67.6\n48.4\n29.8\n55.5\nSPARC (ours)\n74.7\n74.0\n67.1\n77.8\n71.1\n51.31\n34.2\n57.9\nViT-L/4\nCLIP\n79.2\n78.5\n71.8\n81.6\n78.5\n61.3\n51.5\n65.1\nMGCA\n78.0\n77.4\n70.5\n80.6\n75.2\n57.9\n45.5\n63.1\nSPARC (ours)\n79.7\n78.9\n72.6\n81.9\n79.8\n61.3\n53.4\n65.9\nTable 2 | Top-1 accuracy (in %) of zero-shot classification using prompt ensembling on ImageNet\n(IN) and its variants ImageNet-V2 Threshold (IN-V2 Th), ImageNet-V2 Matched Frequency (In-V2\nMF), ImageNet-V2 Top Images (IN-V2 TI), ImageNet-R (IN-R), ImageNet-C (IN-C), ImageNet-Sketch\n(IN-Sketch).\nA.3. \"...training is extremely unstable and the Nan loss easily happens.\"). In addition to that FILIP uses\na number of additional tricks not present in a standard pretraining setup like image augmentations,\nbacktranslation of captions and custom prompt ensembling.\n4.3. Image-Text retrieval\nNext we evaluate SPARC on zero-shot cross-modal retrieval tasks, i.e image-to-text and text-to-image\nretrieval, on Flickr30k (Plummer et al., 2015) and MSCOCO (Lin et al., 2014). From Table 3, we see\nthat SPARC outperforms all competing baselines across all metrics. While using fine-grained losses\nPACL and GLoRIA significantly underperforms the global contrastive objective CLIP, MGCA shows\ncompetitive performance to CLIP in the pretraining setting. Unfortunately, FILIP (Yao et al., 2021)\nagain underperforms CLIP across all metrics. In an attempt to stabilize FILIP we combined it with\nCLIP and observed an improvement on image-to-text Flikr30k on ViT B/32 while being competitive\non other benchmarks to CLIP. We provide these results in Appendix D.\n4.4. Evaluating faithfulness\nWe further examine fine-grained performance of SPARC through faithfulness\u2014how consistent the\nmodel\u2019s highest scoring caption is with the ground truth caption(s) (Ji et al., 2023). This is different\nfrom top-1 retrieval (R@1) which measures exact match retrieval and does not evaluate the ability of\nthe models to faithfully describe the elements in the image. Faithfulness has been used in the LLM\nliterature to assess the propensity of the model to hallucinate (Adlakha et al., 2023; Razumovskaia\net al., 2023) as models with higher faithfulness more accurately capture the details of the ground\ntruth while not inserting additional information (possible hallucinations). The lexical overlap metric\n9\nImproving fine-grained understanding in image-text pre-training\nFlickr30k\nMSCOCO\nimage-to-text\ntext-to-image\nimage-to-text\ntext-to-image\nObjective\nR@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10\nViT-B/32\nCLIP\n79.2 95.1\n97.2\n66.5 88.0\n93.1\n53.5 78.2\n86.7\n38.4 64.8\n74.9\nPACL\n65.5 86.8\n92.2\n49.8 76.5\n84.7\n37.6 65.1\n75.7\n26.5 50.6\n61.8\nGLoRIA\n74.6 92.1\n96.2\n61.5 85.3\n90.7\n46.9 73.0\n82.7\n34.5 61.0\n71.7\nMGCA\n81.5 93.9\n96.8\n64.4 86.5\n92.0\n54.5 78.6\n86.8\n37.7 63.7\n74.0\nFILIP\n62.6 86.9\n92.9\n50.5 77.7\n84.9\n35.6 61.0\n73.1\n26.2 51.0\n62.4\nSPARC (ours) 82.5 96.2\n97.6\n67.7 88.2\n93.0\n55.0 79.1\n87.3\n39.7 65.9\n75.7\nViT-B/16\nCLIP\n84.0 96.1\n98.2\n71.6 90.3\n94.1\n56.2 80.6\n88.2\n42.4 68.6\n78.3\nPACL\n69.6 89.7\n94.2\n54.9 80.7\n87.3\n41.8 67.8\n77.6\n29.1 54.3\n65.5\nGLoRIA\n78.0 95.5\n98.0\n68.4 88.9\n93.2\n49.7 75.4\n84.6\n38.9 65.1\n75.2\nMGCA\n82.2 96.1\n98.1\n67.7 88.5\n93.2\n57.6 80.5\n87.8\n39.8 65.7\n75.3\nFILIP\n69.0 89.8\n94.0\n55.8 81.5\n87.9\n40.2 66.0\n76.3\n29.5 55.3\n66.3\nSPARC (ours) 84.4 97.6\n98.7\n72.0 91.2\n94.9\n57.6 81.2\n88.5\n43.0 68.6\n78.5\nViT-L/14\nCLIP\n84.7 96.9\n98.4\n73.7 91.8\n95.4\n58.6 82.6\n89.1\n44.8 70.5\n79.5\nMGCA\n85.9 96.9\n98.1\n73.2 91.6\n95.3\n59.7 83.2\n89.7\n44.3 69.6\n78.8\nSPARC (ours) 86.9 97.3\n98.6\n74.4 91.7\n95.4\n58.9 82.9\n89.7\n45.6 71.1\n80.1\nTable 3 | Results on zero-shot image-to-text and text-to-image retrieval on MSCOCO and Flickr30k\ndatasets. R@i denotes Recall at i.\nof K-Precision measuring the proportion of tokens in the top chosen caption that appear in the\nground truth tokens has been shown to correlate well with human judgement (Adlakha et al., 2023).\nIn Table 4 we report the K-Precision on the MSCOCO for all tokens (K-P), as well as K-Precision\nrestricted to nouns and adjectives only (K-Pna), as these better encode the objects observed in the\nimage. We evaluate all methods on two architectures and see that SPARC reduced hallucinations of\nobjects (higher K-Pna) while also showing competitive performance to related methods when taking\nall tokens into account (as measured by K-P).\nViT-B/32\nViT-B/16\nMethod\nK-Pna\nK-P\nK-Pna\nK-P\nCLIP\n76.03\n77.82\n77.56\n78.99\nFILIP\n63.3\n66.83\n66.05\n70.09\nPACL\n3.36\n26.26\n4.09\n27.31\nGLoRIA\n71.63\n73.54\n73.85\n75.3\nMGCA\n75.79\n77.98\n77.66\n80.03\nSPARC (ours)\n76.46\n78.44\n78.72\n79.77\nTable 4 | All-token K-Precision (K-P) and the K-Precision restricted to nouns and adjectives (K-Pna)\n(in %) on MSCOCO.\n10\nImproving fine-grained understanding in image-text pre-training\n4.5. Fine-grained localization\nWe further examine SPARC by evaluating it on fine-grained tasks requiring precise localization such\nas open-vocabulary object detection and zero-shot semantic segmentation. For these evaluations, we\nuse the ViT-B/16 architecture.\nOpen-vocabulary object detection.\nTo first evaluate whether the improved fine-grained under-\nstanding learned with SPARC translates to tasks requiring fine-grained localization, we use SPARC\nas a backbone for object detection. Specifically, we used the OWL-ViT open-vocabulary object detec-\ntor (Minderer et al., 2022) with a ViT-B/16 backbone. After SPARC pre-training, detection heads are\nadded to the backbone and fine-tuned on Objects365 (Shao et al., 2019) and Visual Genome (Krishna\net al., 2017) datasets following the approach in Minderer et al. (2022). We evaluate the resulting\nmodel on the large-vocabulary dataset LVIS (Gupta et al., 2019) which is well-suited for testing the\ntransfer of knowledge from image-level pretraining. LVIS contains 1203 categories of objects, of\nwhich 307 \u201crare\u201d categories are excluded from the training data to measure zero-shot transfer from\npretraining. Moreover, we also evaluate detection on the 80 MSCOCO classes. We run detection\ntraining three times and report mean and standard deviation in Table 5. SPARC improves over CLIP\n+0.9% on LVIS and MSCOCO as measured by mean average precision and +3.1% on LVIS \u201crare\u201d classes.\nSince LVIS \u201crare\u201d classes are never seen during detection training data, the model has to rely on\ninformation transfer from the pretrained representations for these classes. The large improvement of\nSPARC over the baseline on LVIS APrare suggests that SPARC has learned more informative fine-grained\nrepresentations.\nLVIS\nMSCOCO\nMethod\nAPall\nAPrare\nAPall\nCLIP\n26.9 \u00b1 0.12\n22.0 \u00b1 0.79\n38.5 \u00b1 0.19\nSPARC (ours)\n27.9 \u00b1 0.11\n25.1 \u00b1 0.95\n39.4 \u00b1 0.13\nTable 5 | Mean Average precision (as mean \u00b1 standard deviation) on all and rare classes on LVIS and\non all classes in MSCOCO.\nMethod\nPascal VOC\nPascal Context\nCLIP\n23.02\n20.45\nFILIP\n19.32\n9.31\nPACL\n1.23\n1.61\nGLoRIA\n22.64\n15.26\nMGCA\n21.91\n11.50\nSPARC (ours)\n27.36\n21.65\nTable 6 | Semantic Segmentation: mIoU of\npredicted and ground-truth segmentation on\nPascal VOC and PASCAL Context datasets.\nSemantic Segmentation.\nFollowing related work\n(Mukhoti et al., 2023), we also perform zero-shot\nsegmentation given a text label, i.e. we compute\npatch embeddings of a given image and calculate\nthe cosine similarity of the patch embedding with\nthe text embeddings of all the ground-truth classes\n(Mukhoti et al., 2023; Ranasinghe et al., 2022). We\nassign a matching class for each patch as the text\nthat corresponds to the maximum cosine similarity of\nthat patch. We then upsample the patches to match\nthe resolution of the ground-truth segmentation and\ncalculate for each class the Intersection over Union\n(IoU) between the predicted and ground-truth seg-\nmentations; we report the mean of the IoU scores\nover the classes present in the ground-truth image. More details about this evaluation can found in\nAppendix D. From Table 6 we see that SPARC strongly improves over other baselines, significantly\nsurpassing the next best model by +4.34 mIoU on the PASCAL VOC (Everingham et al., 2015) dataset\nand by +1.2 mIoU on the PASCAL Context (Mottaghi et al., 2014) dataset. We visualize the predicted\n11\nImproving fine-grained understanding in image-text pre-training\nFigure 3 | Qualitative results for zero-shot segmentation on Pascal VOC dataset. We illustrate the\noriginal image, pixel-level ground-truth labels and the the patch-level segmentation masks obtained\nfrom SPARC, GLoRIA and CLIP.\nsegmentation masks on the PASCAL VOC dataset in Figure 3. Whereas CLIP predicts the object to be\npresent in many different parts of the image, SPARC achieves better object localization and predicts\ntheir shapes more accurately.\n4.6. SPARC backbones in vision language models\nMethod\nMSCOCO\nFlickr30k\nCLIP\n24.3\n12.9\nSPARC (ours)\n25.3\n13.6\nTable 7 | CIDEr score evaluating captioning\nperformance of different vision backbones in\na Flamingo-style (Alayrac et al., 2022) model.\nVision backbones trained contrastively from image-\ntext paired data are often frozen and used in\nfoundational vision-language models (VLMs) such\nas Flamingo (Alayrac et al., 2022).\nTo under-\nstand whether the fine-grained performance im-\nprovements obtained from SPARC translate to better\ncaptioning performance in VLMs, we perform exper-\niments where we compare using a CLIP backbone\nvs. a SPARC backbone in a Flamingo-style architec-\nture (Alayrac et al., 2022). For this, we freeze the\nViT-B/16 vision models trained with CLIP and SPARC\nand pair them with a frozen 400M parameter (pre-\ntrained) language model. On top of the frozen vision and language backbones, we train Perceiver\nResampler cross-attention layers (Alayrac et al., 2022) to produce free-form text as output. More\ndetails about the training set-up can be found in Appendix D. We evaluate the models on captioning\ntasks on MSCOCO and Flickr30k datasets and we report results in Table 7.\n12\nImproving fine-grained understanding in image-text pre-training\n4.7. Ablations\nTo assess the benefits of the different components in SPARC on performance, we perform the following\ntwo ablations: removing the sparsity on the similarity matrix and using softmax instead to compute\nthe alignment weights for grouping the patch embeddings. From the results in Table 8 on both\nfine-grained (MSCOCO retrieval) and coarse-grained (ImageNet zero-shot classification) tasks we\nnotice that both components play a significant role in the model\u2019s performance. In particular, using\nsoftmax results in the highest decrease in performance. See Appendix A for a detailed discussion of\nthe problems with using softmax to compute the alignment weights.\nMSCOCO (i2t)\nMSCOCO (t2i)\nImageNet\nR@1\nR@5\nR@1\nR@5\nTop-1 acc.\nSPARC\n57.6\n81.2\n43.0\n68.6\n72.6\n- no sparsity\n56.1\n80.7\n42.4\n68.2\n72.1\n- softmax\n55.2\n79.8\n41.6\n67.5\n70.6\nTable 8 | Ablations for the ViT-B/16 SPARC model on the MSCOCO image-to-text (i2t) and text-to-\nimage (t2i) retrieval and zero-shot classification on ImageNet.\n4.8. Memory consumption and FLOPS\nTo understand the computational and memory efficiency of the different methods, we also compute\nthe FLOPS and peak memory usage for one update step for different batch size. Note that all methods\nare trained on 256 TPUs. In Figure 4 (a) we show the teraFLOPS (TFLOPS) and in Figure 4 (b) the\npeak memory usage (in MB) of the different methods for one update step when varying the batch size\n(B) from 2048 to 16384. Notice that GLoRIA (Huang et al., 2021) is as memory intensive at batch\nsize 4096 as the other methods (e.g. CLIP) at batch size 16384. Thus, due to device constraints, we\nwere only able to train GLoRIA with batch size 4096. Moreover, notice that for FILIP the TFLOPS\nused for one update step increases by more than 200% between B=8196 and B=16384, as opposed\nto the 100% increase for CLIP, SPARC and MGCA. In addition, for B=16384, both FILIP and PACL\nhave 2x peak memory compared to CLIP, SPARC and MGCA. On the other hand, note that CLIP,\nSPARC and MGCA use the same order of magnitude of FLOPS and memory. To further highlight the\ndifferences between them, we plot the relative increase in TFLOPS in Figure 4 (c) and the relative\nincrease in peak memory in Figure 4 (c) of SPARC and MGCA with respect to CLIP. Notice that for\nB=16384, i.e. the batch size we use for our experiments, the relative increase in TFLOPS and peak\nmemory for SPARC is almost half the one for MGCA. We provide detailed numbers for the FLOPS (in\nTFLOPS) and of the Peak Memory (in MB) in Appendix D.6.\n13\nImproving fine-grained understanding in image-text pre-training\n(a)\n(b)\n(c)\n(d)\nFigure 4 | TFLOPS (a) and Peak Memory (b) used by all methods. Relative increase in TFLOPS (c)\nand Peak memory (d) when comparing SPARC and MGCA to CLIP.\n5. Discussion\nIn this work we proposed a novel method Sparse Fine-grained Contrastive Alignment (SPARC) for\nfine-grained vision-language pretraining. SPARC simultaneously learns information at different levels\nof granularity by contrasting both image-level and caption-level embeddings and token and patch\nembeddings. SPARC learns to group patches based on similarity to tokens and contrast the resulting\nlanguage-grounded patch embeddings with token embeddings. Unlike previous work this comparison\nis done within individual image-text pairs and does not require the computationally and memory\nexpensive comparison of all patches and tokens within the full batch. Through extensive experimental\nevaluation we show that SPARC improves performance both on image-level tasks like classification and\nretrieval and more fine-grained tasks like object detection and segmentation that require localization.\nMoreover, SPARC improves model faithfulness and\nWhile the simple sparsification of the similarity matrix in SPARC already improves performance, we\nbelieve that exploring different approaches to sparsification and learning patch groupings could lead\nto even more informative representations. Moreover, given that SPARC learns patch groupings based\non the associated caption, exploring pretraining data with highly descriptive captions is another\ninteresting line of future work. Also, leveraging bounding boxes and segmentation masks (in addition\nto image-text pairs) would facilitate learning patch groupings and improve learning efficiency since\nthe similarity matrix could be pre-sparsified according to these signals. Another interesting avenue of\nfuture work is further exploring how SPARC encoders perform as part of multimodal foundational\nmodels like Flamingo (Alayrac et al., 2022), BLIP (Li et al., 2022a) and PALI (Chen et al., 2022).\n14\nImproving fine-grained understanding in image-text pre-training\nReferences\nV. Adlakha, P. BehnamGhader, X. H. Lu, N. Meade, and S. Reddy. Evaluating correctness and faith-\nfulness of instruction-following models for question answering. arXiv preprint arXiv:2307.16877,\n2023.\nJ.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican,\nM. Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural\nInformation Processing Systems, 35:23716\u201323736, 2022.\nE. Alsentzer, J. R. Murphy, W. Boag, W.-H. Weng, D. Jin, T. Naumann, and M. McDermott. Publicly\navailable clinical bert embeddings. arXiv preprint arXiv:1904.03323, 2019.\nX. Chen, X. Wang, S. Changpinyo, A. Piergiovanni, P. Padlewski, D. Salz, S. Goodman, A. Grycner,\nB. Mustafa, L. Beyer, et al. Pali: A jointly-scaled multilingual language-image model. arXiv preprint\narXiv:2209.06794, 2022.\nG. Dawidowicz, E. Hirsch, and A. Tal. Limitr: Leveraging local information for medical image-text\nrepresentation. ArXiv, abs/2303.11755, 2023. URL https://api.semanticscholar.org/\nCorpusID:257636659.\nA. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Min-\nderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition\nat scale. arXiv preprint arXiv:2010.11929, 2020.\nI. M. Elfadel and J. L. Wyatt Jr. The\" softmax\" nonlinearity: Derivation using statistical mechanics\nand useful properties as a multiterminal analog circuit element. Advances in neural information\nprocessing systems, 6, 1993.\nM. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal\nvisual object classes challenge: A retrospective. International Journal of Computer Vision, 111(1):\n98\u2013136, Jan. 2015.\nS. Geng, J. Yuan, Y. Tian, Y. Chen, and Y. Zhang. Hiclip: Contrastive language-image pretraining with\nhierarchy-aware attention. arXiv preprint arXiv:2303.02995, 2023.\nA. Gupta, P. Dollar, and R. Girshick. Lvis: A dataset for large vocabulary instance segmentation. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5356\u20135364,\n2019.\nD. Hendrycks and T. Dietterich. Benchmarking neural network robustness to common corruptions\nand perturbations. arXiv preprint arXiv:1903.12261, 2019.\nD. Hendrycks, K. Zhao, S. Basart, J. Steinhardt, and D. Song. Natural adversarial examples.(2019).\narXiv preprint cs.LG/1907.07174, 5(6), 2019.\nD. Hendrycks, S. Basart, N. Mu, S. Kadavath, F. Wang, E. Dorundo, R. Desai, T. Zhu, S. Parajuli,\nM. Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization.\nIn Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8340\u20138349,\n2021.\nD. T. Hoffmann, S. Schrodi, N. Behrmann, V. Fischer, and T. Brox. Eureka-moments in transformers:\nMulti-step tasks reveal softmax induced optimization problems. arXiv preprint arXiv:2310.12956,\n2023.\n15\nImproving fine-grained understanding in image-text pre-training\nS.-C. Huang, L. Shen, M. P. Lungren, and S. Yeung. Gloria: A multimodal global-local representation\nlearning framework for label-efficient medical image recognition. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 3942\u20133951, 2021.\nZ. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang, A. Madotto, and P. Fung. Survey of\nhallucination in natural language generation. ACM Computing Surveys, 55(12):1\u201338, 2023.\nC. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and T. Duerig. Scaling\nup visual and vision-language representation learning with noisy text supervision. In International\nconference on machine learning, pages 4904\u20134916. PMLR, 2021.\nR. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen, Y. Kalantidis, L.-J. Li, D. A.\nShamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image\nannotations. International journal of computer vision, 123:32\u201373, 2017.\nB. Krojer, V. Adlakha, V. Vineet, Y. Goyal, E. Ponti, and S. Reddy. Image retrieval from contextual\ndescriptions. arXiv preprint arXiv:2203.15867, 2022.\nT. Kudo and J. Richardson. Sentencepiece: A simple and language independent subword tokenizer\nand detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018.\nA. Kuznetsova, H. Rom, N. Alldrin, J. Uijlings, I. Krasin, J. Pont-Tuset, S. Kamali, S. Popov, M. Malloci,\nA. Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and\nvisual relationship detection at scale. International Journal of Computer Vision, 128(7):1956\u20131981,\n2020.\nJ. Li, R. Selvaraju, A. Gotmare, S. Joty, C. Xiong, and S. C. H. Hoi. Align before fuse: Vision and\nlanguage representation learning with momentum distillation. Advances in neural information\nprocessing systems, 34:9694\u20139705, 2021.\nJ. Li, D. Li, C. Xiong, and S. Hoi. Blip: Bootstrapping language-image pre-training for unified vision-\nlanguage understanding and generation. In International Conference on Machine Learning, pages\n12888\u201312900. PMLR, 2022a.\nL. H. Li, P. Zhang, H. Zhang, J. Yang, C. Li, Y. Zhong, L. Wang, L. Yuan, L. Zhang, J.-N. Hwang, et al.\nGrounded language-image pre-training. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 10965\u201310975, 2022b.\nT.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll\u00e1r, and C. L. Zitnick. Microsoft\ncoco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\u2013755. Springer, 2014.\nI. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101,\n2017.\nM. Minderer, A. Gritsenko, A. Stone, M. Neumann, D. Weissenborn, A. Dosovitskiy, A. Mahendran,\nA. Arnab, M. Dehghani, Z. Shen, et al. Simple open-vocabulary object detection. In European\nConference on Computer Vision, pages 728\u2013755. Springer, 2022.\nR. Mottaghi, X. Chen, X. Liu, N.-G. Cho, S.-W. Lee, S. Fidler, R. Urtasun, and A. Yuille. The role of\ncontext for object detection and semantic segmentation in the wild. In IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2014.\n16\nImproving fine-grained understanding in image-text pre-training\nJ. Mukhoti, T.-Y. Lin, O. Poursaeed, R. Wang, A. Shah, P. H. Torr, and S.-N. Lim. Open vocabulary\nsemantic segmentation with patch aligned contrastive learning. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 19413\u201319423, 2023.\nR. Paiss, A. Ephrat, O. Tov, S. Zada, I. Mosseri, M. Irani, and T. Dekel. Teaching clip to count to ten.\narXiv preprint arXiv:2302.12066, 2023.\nL. Parcalabescu, M. Cafagna, L. Muradjan, A. Frank, I. Calixto, and A. Gatt. Valse: A task-independent\nbenchmark for vision and language models centered on linguistic phenomena. arXiv preprint\narXiv:2112.07566, 2021.\nC. Peterson and B. S\u00f6derberg. A new method for mapping optimization problems onto neural\nnetworks. International Journal of Neural Systems, 01(01):3\u201322, 1989.\nB. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo, J. Hockenmaier, and S. Lazebnik. Flickr30k\nentities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In\nProceedings of the IEEE international conference on computer vision, pages 2641\u20132649, 2015.\nA. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin,\nJ. Clark, et al. Learning transferable visual models from natural language supervision. In Interna-\ntional conference on machine learning, pages 8748\u20138763. PMLR, 2021.\nK. Ranasinghe, B. McKinzie, S. Ravi, Y. Yang, A. Toshev, and J. Shlens. Perceptual grouping in\nvision-language models. arXiv preprint arXiv:2210.09996, 2022.\nK. Ranasinghe, B. McKinzie, S. Ravi, Y. Yang, A. Toshev, and J. Shlens. Perceptual grouping in\ncontrastive vision-language models. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 5571\u20135584, 2023.\nE. Razumovskaia, I. Vuli\u0107, P. Markovi\u0107, T. Cichy, Q. Zheng, T.-H. Wen, and P. Budzianowski.\nDial BeInfo for Faithfulness: Improving factuality of information-seeking dialogue via behavioural\nfine-tuning, 2023.\nB. Recht, R. Roelofs, L. Schmidt, and V. Shankar. Do imagenet classifiers generalize to imagenet? In\nInternational conference on machine learning, pages 5389\u20135400. PMLR, 2019.\nS. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region\nproposal networks. Advances in neural information processing systems, 28, 2015.\nO. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla,\nM. Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of\ncomputer vision, 115:211\u2013252, 2015.\nS. Shao, Z. Li, T. Zhang, C. Peng, G. Yu, X. Zhang, J. Li, and J. Sun. Objects365: A large-scale,\nhigh-quality dataset for object detection. In Proceedings of the IEEE/CVF international conference on\ncomputer vision, pages 8430\u20138439, 2019.\nK. Shen, J. Guo, X. Tan, S. Tang, R. Wang, and J. Bian. A study on relu and softmax in transformer.\narXiv preprint arXiv:2302.06461, 2023.\nC. Sun, A. Shrivastava, S. Singh, and A. Gupta. Revisiting unreasonable effectiveness of data in deep\nlearning era. In Proceedings of the IEEE international conference on computer vision, pages 843\u2013852,\n2017.\n17\nImproving fine-grained understanding in image-text pre-training\nM. Varma, J.-B. Delbrouck, S. Hooper, A. Chaudhari, and C. Langlotz. Villa: Fine-grained vision-\nlanguage representation learning from real-world data. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 22225\u201322235, 2023.\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin.\nAttention is all you need. Advances in neural information processing systems, 30, 2017.\nF. Wang, Y. Zhou, S. Wang, V. Vardhanabhuti, and L. Yu. Multi-granularity cross-modal alignment\nfor generalized medical visual representation learning. Advances in Neural Information Processing\nSystems, 35:33536\u201333549, 2022.\nH. Wang, S. Ge, Z. Lipton, and E. P. Xing. Learning robust global representations by penalizing local\npredictive power. In Advances in Neural Information Processing Systems, pages 10506\u201310518, 2019.\nJ. Xu, S. De Mello, S. Liu, W. Byeon, T. Breuel, J. Kautz, and X. Wang. Groupvit: Semantic segmentation\nemerges from text supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 18134\u201318144, 2022.\nJ. Xu, J. Hou, Y. Zhang, R. Feng, Y. Wang, Y. Qiao, and W. Xie. Learning open-vocabulary semantic\nsegmentation models from natural language supervision. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 2935\u20132944, 2023.\nJ. Yang, J. Duan, S. Tran, Y. Xu, S. Chanda, L. Chen, B. Zeng, T. Chilimbi, and J. Huang. Vision-\nlanguage pre-training with triple contrastive learning. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 15671\u201315680, 2022.\nL. Yao, R. Huang, L. Hou, G. Lu, M. Niu, H. Xu, X. Liang, Z. Li, X. Jiang, and C. Xu. Filip: Fine-grained\ninteractive language-image pre-training. arXiv preprint arXiv:2111.07783, 2021.\nJ. Yu, Z. Wang, V. Vasudevan, L. Yeung, M. Seyedhosseini, and Y. Wu. Coca: Contrastive captioners\nare image-text foundation models. arXiv preprint arXiv:2205.01917, 2022.\nM. Yuksekgonul, F. Bianchi, P. Kalluri, D. Jurafsky, and J. Zou. When and why vision-language models\nbehave like bag-of-words models, and what to do about it? arXiv preprint arXiv:2210.01936, 2022.\nY. Zeng, X. Zhang, and H. Li. Multi-grained vision language pre-training: Aligning texts with visual\nconcepts. arXiv preprint arXiv:2111.08276, 2021.\nS. Zhai, T. Likhomanenko, E. Littwin, D. Busbridge, J. Ramapuram, Y. Zhang, J. Gu, and J. Susskind.\nStabilizing transformer training by preventing attention entropy collapse. ICML, 2023a.\nX. Zhai, A. Kolesnikov, N. Houlsby, and L. Beyer. Scaling vision transformers. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12104\u201312113, 2022.\nX. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer. Sigmoid loss for language image pre-training.\nInternational Conference on Computer Vision, 2023b.\nY. Zhong, J. Yang, P. Zhang, C. Li, N. Codella, L. H. Li, L. Zhou, X. Dai, L. Yuan, Y. Li, et al. Regionclip:\nRegion-based language-image pretraining. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 16793\u201316803, 2022.\nC. Zhou, C. C. Loy, and B. Dai. Extract free dense labels from clip. In European Conference on Computer\nVision, pages 696\u2013712. Springer, 2022.\n18\nImproving fine-grained understanding in image-text pre-training\nA. Problems with using softmax for obtaining alignment weights\n\ud835\udc46\ud835\udc5c \ud835\udc53\ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65 is ubiquitously used to normalise activations that should or could be interpreted as probabil-\nities, as it is for example the case of attention/alignmnet weights. One potential reason behind this\nchoice is the dominating practice of using \ud835\udc60\ud835\udc5c \ud835\udc53\ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65 as the output activation function for classification\ntasks, being the canonical link function for multinomial outputs. Another appealing property is that it\nacts as a differentiable \ud835\udc5a\ud835\udc4e\ud835\udc65-operator, allowing for a natural interpretation of selecting one class out of\nmultiple.\nHowever, \ud835\udc60\ud835\udc5c \ud835\udc53\ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65 can be problematic from a gradient flow perspective (Hoffmann et al., 2023;\nShen et al., 2023; Zhai et al., 2023a), and in this section we will expand this observation and the\nimplications it might have on our specific use case. Also, intuitively from its role as a soften \ud835\udc5a\ud835\udc4e\ud835\udc65\noperator, softmax prefers to converge to peaky uni-modal distribution, selecting one out of \ud835\udc58, and\nis less likely to represent multi-modal distributions. This is due to how gradients flow through the\nactivation, leading to winner-takes-all dynamics (Elfadel and Wyatt Jr, 1993; Peterson and S\u00f6derberg,\n1989) that ensures the peakyness and unimodality of the distribution represented.\nIf we assume \ud835\udc4e(h) = \ud835\udc60\ud835\udc5c \ud835\udc53\ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65(h)1, for some h \u2208 R\ud835\udc58, then we can write the derivative as\n\ud835\udf15a\ud835\udc56\n\ud835\udf15h\ud835\udc57\n=\n\u001a a\ud835\udc56 \u2212 a2\n\ud835\udc56\niff \ud835\udc56 = \ud835\udc57\n\u2212a\ud835\udc56a\ud835\udc57\notherwise\n(8)\nAssume we have some loss \ud835\udc3f which is a function of \u00cd\n\ud835\udc56 a\ud835\udc56V\ud835\udc56, i.e. some values V\ud835\udc56 \u2208 R\ud835\udc5b that have been\nsummarised using attention weights a\ud835\udc56.\nSoftmax gradients vanish at initialisation.\nAssume we have a large number of patches or tokens\nwe want to attend over. In our notation, \ud835\udc58 \u226b 0. At initialisation, all preactivation entries h\ud835\udc56 will be\nsmall numbers of similar magnitude. The attention weights will be uniformally distributed over the \ud835\udc58\npatches, leading to a\ud835\udc56 \u2248 1\n\ud835\udc58 \u226a 1, \u2200\ud835\udc56. Due to the weights being almost uniformally distributed, different\nobservation will lead to randomly selecting a different patch. Therefore in expectation the gradient\nthrough the softmax on a particular token \ud835\udc56 will be scaled by 1\n\ud835\udc582 which will vanish very fast to 0 as \ud835\udc58\ngrows. Note that in the rare scenario that the system picks the \ud835\udc56-th element, the gradient becomes\n1\n\ud835\udc58 which also vanishes to 0 as \ud835\udc58 grows. If we consider a very large \ud835\udc58, this ensures that we have a\nplateau at initialization that might be hard to escape (or might take many updates to do so). See also\n(Hoffmann et al., 2023) for a similar observation.\nSoftmax exhibits winner-takes-all dynamics.\nThis has been understood and seen as a desirable\nproperty early on, see for example (Peterson and S\u00f6derberg, 1989) and (Elfadel and Wyatt Jr, 1993).\nOne way to intuitively justify this behaviour is to think of the effect of applying the softmax operation\nmultiple time (i.e. study the dynamics of a system whose transition function is just softmax). As\nshown in (Peterson and S\u00f6derberg, 1989) Fig. 5, the corners of the simplex act as attractors of this\ndynamical system, where from any initial condition, the system very quickly converges to one of the\ncorners. This is caused by the dynamics of the gradients. When a particular weight is pushed up,\nall other weights are pushed down due to the normalisation. The amount by which the weight is\npushed depends on its magnitude. So if a particular weight is larger and correlates positively with\nthe desired behaviour, it will be pushed up proportionally more than other weights that correlate\npositively. Note that the particular form of the function (including the exponentiation) play a role in\nthe form the gradients take, and removing the exponentiation will change the behaviour. These types\nof dynamics, have the downside of leading the distribution induced by the softmax to be unimodal.\n1By abuse of notation, we will use a \u2208 R\ud835\udc58, where a = \ud835\udc4e(h) and use a\ud835\udc56 for the \ud835\udc56-th dimension of vector a\n19\nImproving fine-grained understanding in image-text pre-training\nThat is, softmax will act, as the name of the activation indicates, as a max operator, preferring to\nlearn a behaviour where it picks one out of \ud835\udc58, rather than multiple equally relevant candidates.\nSoftmax saturates proportional to its certainty\nAssume \u2203\ud835\udc56 such that \u2200\ud835\udc57, \ud835\udc57 \u2260 \ud835\udc56 we have \ud835\udc4e\ud835\udc56 \u226b \ud835\udc4e\ud835\udc57.\nThis implies that 1 \u2212 \ud835\udc4e\ud835\udc56 \u2192 0 and \ud835\udc4e\ud835\udc57 < 1 \u2212 \ud835\udc4e\ud835\udc56. The gradient for the \ud835\udc56-th position, according to equation 8,\nwill be \ud835\udc4e\ud835\udc56(1 \u2212 \ud835\udc4e\ud835\udc56) and will go to zero as linearly as \ud835\udc4e\ud835\udc56 approaches 1. The gradient for any other position\n\ud835\udc57, will go to 0 at the same rate, as it will be roughly \ud835\udc4e\ud835\udc57 which is bounded from above from 1 \u2212 \ud835\udc4e\ud835\udc56. Note\nthat a step of size \u0394 on \u210e, due to the exponentiation and normalization of softmax, will make \ud835\udc4e\ud835\udc56 \u2192 1\nexponentially fast for constant change in \u210e.\nB. Additional related works\nWe further expand here the discussion on achieving fine-grained understanding in vision-language\nmodels (VLMs) through additional losses and modules.\nIn addition to the approaches described in Section 3, another line of work involves proposes modifying\nthe underlying vision transformer architecture to build modules that lead to a hierarchical grouping\nof image regions: e.g. GroupViT (Xu et al., 2022), OVSegmentor (Xu et al., 2023), HiCLIP (Geng\net al., 2023). While these methods propose architectural changes, the objective used for training\nstill involves having a global contrastive loss. Conversely, in our work, we use the standard vision\ntransformer architecture and propose instead changes to the training objective to achieve finegrained\nunderstanding.\nMoreover, note that several of these approaches (Xu et al., 2023) and the other methods who add a\ncross-modal encoder on top of the dual image-text encoder (Li et al., 2021; Yang et al., 2022) with\ncaptioning/masked language modelling losses start training from pre-trained text encoders and/or\nvision encoder.\nSimilarly, (Ranasinghe et al., 2023) improve the semantic and spatial information in dual encoders\ntrained contrastively by changing the patch embeddings aggregation methods from average pooling\nto max pooling and by starting training with both pre-trained vision and language encoders. In our\nwork, we focus specifically on the set-up of training the dual encoders from scratch.\n20\nImproving fine-grained understanding in image-text pre-training\nC. SPARC pseudo-code\nListing 1 provides JaX-alike pseudo-code for the SPARC objective detailing the construction of both\nthe global and the local losses.\n1\n# Models:\n2\n#\nvision_encoder\n3\n#\nlanguage_encoder\n4\n# Inputs:\n5\n#\nimage - [B, H, W, C]\n6\n#\ntext - [B, N]\n7\n# Hyperparameters :\n8\n#\nsimilarity_threshold\n9\n#\nglobal_loss_weight\n10\n#\nlocal_loss_weight\n11\n#\ninverse_temperature\n12\n13\ndef\npairwise_contrastive_loss (a, b, labels):\n14\nlabels = eye(a.shape[0])\n15\nlogits_ab = dot(a * b.T) * inverse_temperature\n16\nreturn\nsoftmax_cross_entropy (logits=logits_ab , labels=labels , reduction=\u2019mean \u2019)\n17\n18\ndef\nmasked_pairwise_contrastive_loss (a, b, mask):\n19\nbatch_size , seq_len , _ = a.shape [0]\n20\nmask_logits = einshape (\u2019bnm ->(bn)m\u2019, 1 .0\n- mask , n=seq_len )\n21\nlabels = einshape (\u2019ns ->(bn)s\u2019, eye(a.shape[1]), b=batch_size )\n22\nlogits = einsum(\u2019bmd ,bnd ->bmn \u2019, a, b) * inverse_temperature\n23\nlogits = einshape (\u2019bnm ->(bn)m\u2019, logits )\n24\nloss = softmax_cross_entropy (logits=logits - mask_logits *\nINF , labels=labels)\n25\nloss = sum(loss * mask) / sum(mask)\n26\nreturn\nloss\n27\n28\n# ---------- GLOBAL\nLOSS\n----------\n29\n30\n# encoders\ninclude\nadapters\n31\nv_patch_embed = vision_encoder (image)\n32\nl_token_embed , language_mask =\nlanguage_encoder (text)\n33\n34\nv_embed = l2_normalize (mean(v_patch_embed , axis=1), axis= -1 )\n35\nl_embed = l2_normalize (mean(l_token_embed , axis=1), axis= -1 )\n36\n37\nloss_vl =\npairwise_contrastive_loss (v_embed , l_embed )\n38\nloss_lv =\npairwise_contrastive_loss (l_embed , v_embed )\n39\n40\nglobal_loss = 0 .5\n* (loss_vl + loss_lv )\n# (eq 1)\n41\n42\n# ---------- LOCAL\nLOSS\n----------\n43\n44\n# similarity\ncalculation\n45\nsimilarity = einsum (\u2019btd ,bpd ->btp \u2019, l_token_embed , v_patch_embed )\n46\n47\n# min -max\nnormalisation\n48\nsimilarity = ( similarity\n- min(similarity , axis= -1 )) /\n49\n(max(similarity , axis= -1 ) - min(similarity , axis= -1 ))\n# (eq 2)\n50\n51\n# thresholding\n52\nsimilarity = where( similarity\n< similarity_threshold , 0 .0 , similarity )\n# (eq 3)\n53\n54\n# alignment -weighting\n55\nv_align_weights = similarity / sum(similarity , axis= -1 )\n# (eq 4)\n56\nl_grouped_v_patch_embed = einsum (\u2019btp ,bpd ->btd \u2019, v_align_weights , v_patch_embed )\n# (eq 5)\n57\n58\nl_grouped_v_patch_embed = l2_normalize ( l_grouped_v_patch_embed , axis= -1 )\n59\nl_token_embed = l2_normalize (l_token_embed , axis= -1 )\n60\n61\nloss_vl_local =\nmasked_pairwise_contrastive_loss ( l_grouped_v_patch_embed , l_token_embed , language_mask )\n62\nloss_lv_local =\nmasked_pairwise_contrastive_loss (l_token_embed , l_grouped_v_patch_embed , language_mask )\n63\n64\nlocal_loss = 0 .5\n* ( loss_vl_local + loss_lv_local )\n# (eq 6)\n65\n66\n# ---------- TOTAL (SPARC) LOSS\n----------\n67\n68\nloss = global_loss_weight * global_loss + local_loss_weight * local_loss\n# (eq 7)\nListing 1 | Pseudo-code for SPARC.\n21\nImproving fine-grained understanding in image-text pre-training\nD. Experiments details\nD.1. Model architectures\nFor the dual-encoder, we use the standard Vision Transformers (ViTs) (Dosovitskiy et al., 2020) as\nimage encoders and Transformers (Vaswani et al., 2017) as text encoders. We perform experiments\nwith ViT-B models with different patch sizes (ViT-B/32 and ViT-B/16) and a ViT-L model with patch\nsize 14 (ViT-L/14). Thus, for the ViT-B image encoder, we use a model with 12 layers, 768 width and\n12 attention heads, while for the ViT-L image encoder we use a model with 24 layers, 1024 width and\n16 attention heads. For the language encoder, we use an architecture with 12 layers, 768 width and\n12 attention heads. The linear adapters \ud835\udc54\ud835\udc63(\u00b7) and \ud835\udc54\ud835\udc61(\u00b7) project the vision and language embeddings\nrespectively to a shared embedding space of dimensionality 512.\nD.2. Datasets\nAs described in Section 4, we use the following datasets for pre-training: ALIGN (Jia et al., 2021),\nJFT (Sun et al., 2017; Zhai et al., 2022) and LTIP (Long Text & Image Pairs) (Alayrac et al., 2022).\nNote that for JFT, where the images were semi-automatically annotated with a class-hierarchy of 30k\nlabels, we flatten the hierarchical label structure and use all the assigned labels to describe the image.\nWe use a multi-step training strategy where we alternate sampling batches from each of the 3 large\ndatasets; the gradient updates are then performed by aggregating the gradients from computing the\nloss on one batch from each of the datasets.\nD.3. Baselines\nOur implementation of baselines follow the publicly available code (where available2) with a few\nminor differences we outline here.\nIn the original MGCA implementation, token-wise cross-modal alignment (see Eqn. 5 in the original\npaper) uses the last-layer attention weight from a visual token to the [CLS] token (averaged across\nmultiple heads) to weight the loss terms for different visual tokens (and vice versa for language\ntokens). In our implementation, since we do not use the [CLS] token but instead use average pooling\nto get the global language/vision embeddings, we omit this weighting operation.\nIn the original GLoRIA implementation, language tokens are aggregated for each word to ensure that\ncontrasted language embeddings refer to complete words (see Section 3.2.1 in the original paper);\nhowever, to ensure fair comparison, we do not have this additional aggregation operation, and instead\nuse language tokens directly in local losses. Additionally, in our experiments we found that it is\ncrucial to normalize the pairwise vision-language embedding similarities (see Eqn. 3 in the original\npaper) by\n\u221a\n\ud835\udc37 where \ud835\udc37 is the embedding size. Without this normalization, we found training with\nGLoRIA to be unstable. Moreover, recall that GLoRIA requires computing similarities between all\ntoken embeddings and all patch embeddings in the batch. This is memory expensive and it was not\npossible (due to device memory constraints) for batch sizes of 16348. Consequently, we used a batch\nsize of 4096 for Gloria and trained the models for 800k steps (to match the number of examples seen\nby the other baseline). See discussion in Section D.6 for detailed computation of FLOPs and memory\nusage of GLoRIA.\nFor FILIP [50] we follow the original paper and implement token dropping for FILIP which the\nauthors propose in order to reduce the large memory consumption of their method. In the original\npaper the authors comment on the training difficulty in the original paper (cf. in the Appendix\n2GLoRIA: https://github.com/marshuang80/gloria, MGCA: https://github.com/HKU-MedAI/MGCA\n22\nImproving fine-grained understanding in image-text pre-training\nObjective\nIN\nIN-V2 Th\nIN-V2 MF\nIN-V2 TI\nIN-R\nIN-C\nIN-A\nIN-Sketch\nViT-B/32\nCLIP\n66.7\n66.2\n58.9\n71.5\n63.2\n42.6\n15.1\n51.7\nFILIP\n52.7\n50.7\n44.0\n55.8\n47.1\n28.7\n8.4\n38.2\nCLIP + FILIP\n66.5\n65.8\n58.2\n71.1\n63.0\n42.3\n15.1\n51.3\nSPARC (ours)\n68.1\n67.0\n59.7\n72.0\n64.9\n44.5\n16.7\n53.2\nViT-B/16\nCLIP\n71.6\n70.9\n63.7\n74.8\n71.1\n48.5\n32.2\n56.8\nFILIP\n56.6\n55.6\n48.9\n59.7\n54.0\n33.2\n14.4\n43.1\nCLIP + FILIP\n71.8\n70.5\n63.4\n74.4\n70.6\n47.8\n32.0\n56.2\nSPARC (ours)\n72.6\n71.1\n64.4\n75.0\n72.0\n48.5\n33.8\n57.3\nTable 9 | Top-1 accuracy (in %) of zero-shot classification on ImageNet (IN) and its variants ImageNet-\nV2 Threshold (IN-V2 Th), ImageNet-V2 Matched Frequency (In-V2 MF), ImageNet-V2 Top Images\n(IN-V2 TI), ImageNet-R (IN-R), ImageNet-C (IN-C), ImageNet-Sketch (IN-Sketch). All methods have\nbeen trained on ALIGN, JFT, LTIP for the same number of training steps.\nA.3.\u201d...training is extremely unstable and the Nan loss easily happens.\u201d). We observed similar training\ninstability in our setup across a wide range of learning rates and weight decay parameters. This\ntraining instability leads to significant performance degradation compared to CLIP. We hypothesize\nthat the non-standard additional tricks that FILIP uses such as image augmentations, backtranslation\nof captions and custom prompt ensembling could potentially improve training stability; note that we\ndo not use these tricks in order to ensure a fair comparison across methods. Given FILIP\u2019s training\ninstability, we conducted a number of additional experiments combining CLIP and FILIP in order to\nbetter understand the training instability. Below in Tables 9 and 10 we present these results \u2013 as can\nbe seen combining these two methods leads to some improvements on some benchmarks while some\nperformance degradation on other benchmarks.\nFinally, all methods in our paper use learned temperature parameters (instead of fixed temperatures\nas is done in the original MGCA and GLoRIA implementations) as our experiments showed that this\nsignificantly improved performance for all methods.\nMSCOCO\nFlickr30k\nimage-to-text\ntext-to-image\nimage-to-text\ntext-to-image\nObjective\nR@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10 R@1 R@5 R@10\nViT-B/32\nCLIP\n53.5 78.2\n86.7\n38.4 64.8\n74.9\n79.2 95.1\n97.2\n66.5 88.0\n93.1\nFILIP\n35.6 61.0\n73.1\n26.2 51.0\n62.4\n62.6 86.9\n92.9\n50.5 77.7\n84.9\nCLIP + FILIP 52.0 77.0\n85.6\n37.8 64.4\n74.5\n81.2 95.4\n97.1\n66.8 87.7\n92.3\nSPARC (ours) 55.0 79.1\n87.3\n39.7 65.9\n75.7\n82.5 96.2\n97.6\n67.7 88.2\n93.0\nViT-B/16\nCLIP\n56.2 80.6\n88.2\n42.4 68.6\n78.3\n84.0 96.1\n98.2\n71.6 90.3\n94.1\nFILIP\n40.2 66.0\n76.3\n29.5 55.3\n66.3\n69.0 89.8\n94.0\n55.8 81.5\n87.9\nCLIP + FILIP 54.9 79.0\n87.4\n41.3 67.7\n77.5\n82.7 97.0\n98.4\n71.1 90.5\n94.7\nSPARC (ours) 57.6 81.2\n88.5\n43.0 68.6\n78.5\n84.4 97.6\n98.7\n72.0 91.2\n94.9\nTable 10 | Results on zero-shot image-to-text and text-to-image retrieval on MSCOCO and Flickr30k\ndatasets. R@i denotes Recall at i. All methods have been trained on ALIGN, JFT, LTIP for the same\nnumber of training steps.\n23\nImproving fine-grained understanding in image-text pre-training\nD.4. Hyperparameters details\nWe train all models using the AdamW (Loshchilov and Hutter, 2017) optimizer, a cosine learning rate\nschedule with linear warm-up of 2500 steps. For all methods, we sweep over learning rate and weight\ndecay values in the following ranges: learning rate in [7\ud835\udc52 \u2212 4, 9\ud835\udc52 \u2212 4, 1.1\ud835\udc52 \u2212 4] and weight decay in\n[0.1, 0.2, 0.3]. We use a batch size of 16348 (except for GLoRIA for which we use 4096 batch size)\nand we pre-train the ViT-B models for 200k steps (\u2248 3.2 billion data points).\nFor the other SPARC hyperparameters, we set the global loss weight \ud835\udf06\ud835\udc54 = 0.5 and we sweep the local\nloss weight in \ud835\udf06 \ud835\udc53 \u2208 [0.5, 1.0, 5.0, 10.0]. Moreover, we use a learned temperature parameter \ud835\udf0f.\nFor baseline specific hyperparameters, we follow the publicly available code (where available) and the\noriginal papers. For MGCA (Wang et al., 2022), as described in the paper, we set the weighing of the\ndifferent losses \ud835\udf061 = 1, \ud835\udf062 = 1, \ud835\udf063 = 1, the number of attention heads for computing the cross-modal\nembeddings to 1 with a 128 embedding dimension. For MGCA\u2019s crossmodal prototype alignment loss,\nwe use 500 prototypes with \ud835\udf16 = 0.05 and 3 iterations for the Sinkhorn-Knopp clustering algorithm.\nFor FILIP, we implemented the token dropping procedure described in the paper and use 20% token\ndropping in our experiments.\nFor PACL, we closely follow the original paper in terms of implementation up to one notable detail \u2013\nwe include a learnable temperature parameter in the loss as we found this to significantly improve\nperformance.\nD.5. Prompt ensembling for zero-shot classification\nFollowing Radford et al. (2021) and Yao et al. (2021) we use prompt templates to augment the label\nfor classification tasks. We use the prompt templates format from Yao et al. (2021):\n[prefix]{class label}, [suffix]\n(9)\nFor the [prefix], we use the templates from Radford et al. (2021). On the other hand, for the [suffix],\nwe use the templates from Yao et al. (2021), which shows that adding the reference word \u2018it\u2019 at the\nend of the prompt, e.g. \u2018I like it\u2019, further improves performance.\nD.6. Memory consumption and FLOPS for the different methods\nWe provide detailed numbers for the FLOPS (in TFLOPS) and of the Peak Memory (in MB) in Table\n11.\nFLOPS (TFLOPS)\nPeak memory (MB)\nObjective\nB = 2048\nB = 4096\nB = 8192\nB = 16384\nB = 2048\nB = 4096\nB = 8192\nB = 16384\nCLIP\n1.15\n2.29\n4.57\n9.14\n4394\n4452\n5889\n8578\nPACL\n1.2\n2.46\n5.24\n12.8\n4682\n6267\n9786\n14785\nGLoRIA\n3.34\n13.21\n\u2212\n\u2212\n8013\n13840\n\u2212\n\u2212\nMGCA\n1.16\n2.31\n4.62\n9.23\n4412\n4462\n5936\n8681\nFILIP\n1.37\n3.17\n8.09\n27.25\n4394\n5230\n8657\n15463\nSPARC (ours)\n1.15\n2.3\n4.6\n9.19\n4408\n4450\n5914\n8620\nTable 11 | TFLOPS and peak memory usage for one update step of each method for different batch\nsizes.\n24\nImproving fine-grained understanding in image-text pre-training\nD.7. Semantic segmentation\nFor zero-shot semantic segmentation, we pass the patch embeddings through the extra dense layer\nand the adapter to compute the cosine similarity with the text embeddings for the ground-truth\nclasses. Similarly to (Mukhoti et al., 2023) we compute the mean Intersection over Union (mIoU)\nonly for the foreground classes.\nD.8. SPARC backbones in vision language models\nWe train the Perceiver Resampler part of Flamingo (Alayrac et al., 2022) on the ALIGN (Jia et al.,\n2021), LTIP (Long Text & Image Pairs) (Alayrac et al., 2022) and VTP (Video & Text Pairs) (Alayrac\net al., 2022) datasets. VTP consists of 27 million short videos paired with text descriptions, where\neach video if 22s on average. We use the AdamW optimizer, a cosine learning rate schedule with\npeak learning rate of 1\ud835\udc52 \u2212 4, linear warmup with 5000 warm-up steps and 250k training steps in total.\nD.9. SPARC vs CLIP Faithfulness Examples\nTo further understand the ability of SPARC and CLIP models to faithfully describe the elements in\nthe image, we provide several qualitative examples. Thus, for MSCOCO, we chose examples where\nthe top-1 retrieved caption for both SPARC and CLIP is not part of the ground truth captions, but\nwhere where SPARC has higher all-token K-Precision (Figure 5) and higher K-Precision restricted\nto nouns and adjectives (6). From these figure, we notice that captions retrieved using the CLIP\nrepresentations describe objects that not present in the image (e.g. \u201cseveral signs for bars\u201d when there\nare none present) or get the number of objects wrong (e.g. \"two motorcycles\" when there is only one\nmotorcycle). Alternatively, captions retrieved using the SPARC representations are more faithful to\nthe image, but also provide more descriptive details (e.g. \"young boy in white shirt\", \"dinner table\nwith a place setting\").\n25\nImproving fine-grained understanding in image-text pre-training\nFigure 5 | SPARC vs CLIP vs Ground Truth for examples where SPARC has higher all-token K-Precision\n(K-P)\nFigure 6 | SPARC vs CLIP vs Ground Truth for examples where SPARC has higher K-Precision restricted\nto nouns and adjectives (K-Pna)\n26\n"
  },
  {
    "title": "WorldDreamer: Towards General World Models for Video Generation via Predicting Masked Tokens",
    "link": "https://arxiv.org/pdf/2401.09985.pdf",
    "upvote": "12",
    "text": "WorldDreamer: Towards General World Models for Video Generation\nvia Predicting Masked Tokens\nXiaofeng Wang*1 Zheng Zhu*1\f Guan Huang*1,2 Boyuan Wang1 Xinze Chen1 Jiwen Lu2\n1GigaAI\n2Tsinghua University\nProject Page: https://world-dreamer.github.io\nText to Video\nWorldDreamer\nImage to Video\nAction to Video\nVideo Inpainting\nVideo Stylization\n\u201cA polar bear is \n walking on ice.\u201d\n\u201cA monkey is reading \na book in the desert.\u201d\n\u201cAutumn style, a sailboat \nglides across the lake.\u201d\nFigure 1. WorldDreamer demonstrates a comprehensive understanding of visual dynamics in the general world. It excels in image-to-video\nsynthesis, text-to-video generation, video inpainting, video stylization and even action-to-video generation.\nAbstract\nWorld models play a crucial role in understanding and\npredicting the dynamics of the world, which is essential for\nvideo generation. However, existing world models are con-\nfined to specific scenarios such as gaming or driving, limit-\ning their ability to capture the complexity of general world\ndynamic environments.\nTherefore, we introduce World-\nDreamer, a pioneering world model to foster a compre-\nhensive comprehension of general world physics and mo-\ntions, which significantly enhances the capabilities of video\ngeneration. Drawing inspiration from the success of large\n*These authors contributed equally to this work. \fCorresponding au-\nthor: Zheng Zhu, zhengzhu@ieee.org\nlanguage models, WorldDreamer frames world modeling\nas an unsupervised visual sequence modeling challenge.\nThis is achieved by mapping visual inputs to discrete to-\nkens and predicting the masked ones.\nDuring this pro-\ncess, we incorporate multi-modal prompts to facilitate in-\nteraction within the world model. Our experiments show\nthat WorldDreamer excels in generating videos across dif-\nferent scenarios, including natural scenes and driving en-\nvironments.\nWorldDreamer showcases versatility in ex-\necuting tasks such as text-to-video conversion, image-to-\nvideo synthesis, and video editing.\nThese results under-\nscore WorldDreamer\u2019s effectiveness in capturing dynamic\nelements within diverse general world environments.\narXiv:2401.09985v1  [cs.CV]  18 Jan 2024\n1. Introduction\nThe next significant leap in artificial intelligence is ex-\npected to come from systems that possess a profound un-\nderstanding of the dynamic visual world. At the core of this\nadvancement are world models, crucial for comprehending\nand predicting the dynamic nature of our world.\nWorld\nmodels hold great promise for learning motion and physics\nin the general world, which is essential for video generation.\nThe early exploration of world models [19] primarily fo-\ncus on gaming scenarios, which proposes a generative neu-\nral network model capable of learning compressed repre-\nsentations of spatial and temporal dynamics within game\nenvironments.\nSubsequent research in the Dreamer se-\nries [21\u201323] further validated the efficacy of world mod-\nels across diverse gaming scenarios. Considering its struc-\ntured nature and paramount importance, autonomous driv-\ning has become a forefront domain for the practical appli-\ncation of world models. Various approaches [31,32,49,50]\nare introduced to explore the efficacy of world models in\nautonomous driving scenarios. Furthermore, DayDreamer\n[52] has extended the application of world models to en-\ncompass real-world robotic environments, However, cur-\nrent world models are predominantly confined to gaming,\nrobotics, and autonomous driving, lacking the capability to\ncapture the motion and physics of the general world. Be-\nsides, relevant research in world models mainly relies on\nRecurrent Neural Networks (RNNs) [20\u201324,35,42,52] and\ndiffusion-based methods [32,49,50] to model visual dynam-\nics. While these approaches have yielded some success in\nvideo generation, they encounter challenges in effectively\ncapturing the motion and physics in general world scenes.\nIn this paper, we introduce WorldDreamer, which pio-\nneers the construction of general world models for video\ngeneration. Drawing inspiration from the successes of large\nlanguage models (LLMs) [5, 12, 37, 38], we predict the\nmasked visual tokens to effectively model the intricate dy-\nnamics of motion and physics embedded in visual signals.\nSpecifically, WorldDreamer involves encoding images into\ndiscrete tokens using VQGAN [15].\nWe then randomly\nmask a portion of these tokens and utilize the unmasked\ntokens to predict the masked ones, a process integral to\ncapturing the underlying motion and physics in visual data.\nWorldDreamer is constructed on the Transformer architec-\nture [46]. Regarding the spatial-temporal priority inherent\nin video signals, we propose the Spatial Temporal Patch-\nwise Transformer (STPT), which enables attention to focus\non localized patches within a temporal-spatial window, fa-\ncilitating the learning of visual signal dynamics and accel-\nerating the convergence of the training process. Addition-\nally, WorldDreamer integrates language and action signals\nthrough cross-attention, to construct multi-modal prompts\nfor interaction within world model. Notably, compared to\ndiffusion-based methods, WorldDreamer capitalizes on the\nreuse of LLM infrastructure and benefits from optimiza-\ntions developed over years for LLMs, including model scal-\ning learning recipes. Besides, WorldDreamer exhibits a re-\nmarkable speed advantage, parallel decoding videos with\njust a few iterations, which is \u223c3\u00d7 faster than diffusion-\nbased methods [3,10,48]. Therefore, WorldDreamer holds\ngreat promise for constructing a general world model from\nvisual signals.\nThe main contributions of this paper can be summarized\nas follows: (1) We introduce WorldDreamer, the first gen-\neral world model for video generation, which learns general\nworld motion and physics. (2) We propose the Spatial Tem-\nporal Patchwise Transformer (STPT), which enhances the\nfocus of attention on localized patches within a temporal-\nspatial window. This facilitates easier learning of visual\nsignal dynamics and expedites the training process. (3) We\nconduct extensive experiments to verify that WorldDreamer\nexcels in generating videos across different scenarios, in-\ncluding natural scenes and driving environments. World-\nDreamer showcases versatility in executing tasks such as\ntext-to-video conversion, image-to-video synthesis, video\nediting, and action-to-video generation (see Fig. 1).\n2. Related Work\n2.1. Video Generation\nCurrently, state-of-the-art video generation models are\nprimarily classified into two categories: Transformer-based\nmethods and diffusion-based methods.\nTransformer-based methods. The Transformer-based\nvideo generation methods are derived from the general fam-\nily of LLMs [5, 12, 37, 38]. Typically, these methods em-\nploy autoregressive prediction of the next token or parallel\ndecoding of masked tokens to generate videos. Drawing in-\nspiration from image generation techniques [11,13,40,54],\nVideoGPT [53] integrates VQVAE [45] with Transformer-\nbased token prediction, enabling it to autoregressively pre-\ndict visual tokens for video generation.\nFurthermore,\nGAIA-1 [31] integrates various modalities, including text\ndescriptions, images, and driving actions, resulting in the\ngeneration of autonomous driving scenario videos. Unlike\nthese autoregressive methods, some Transformer-based ap-\nproaches [29, 47], draw inspiration from [8, 9, 14, 55], ac-\ncelerating video generation through parallel decoding. In\naddition to these methods, VideoPoet [33] adopts video tok-\nenizer [56] and generates exceptionally high-quality videos\nbased on parallel decoding. The incorporation of Trans-\nformer models into video language models showcases their\nformidable zero-shot capability in handling various tasks\nduring pretraining.\nTherefore, employing Transformer-\nbased mask image models as the foundation for general\nworld models emerges as a promising avenue.\nDiffusion based methods. Compared to Transformer-\nPredicted \nVisual Tokens\nText \nEmbeddings\nVisual \nTokenizer\nLLM\nEncoder\nMLP\nText:\n\u2018A lizard is reading \na book in the desert.\u2019\n\u2018A monkey is reading \na book in the desert.\u2019\nMask\nImage\nVideo\nAction:\nVelocity/steering\nSpatial Temporal\nPatchwise Transformer\nVisual\nDecoder\nText to Video\nImage to Video\nVideo Inpainting\nVideo stylization\nAction to Video\nAction\nEmbeddings\n\u00d7 \u01b6\nMaksed Visual \nEmbeddings\nFigure 2. Overall framework of WorldDreamer. WorldDreamer first converts images and videos into visual tokens, followed by a token-\nmasking operation. Text and action inputs are encoded separately into embeddings, acting as multimodal prompts. Subsequently, STPT\npredicts the masked visual tokens, which are processed by visual decoders to enable video generation and editing in various scenarios.\nbased models, there has been extensive research employing\ndiffusion-based models for video generation. VideoLDM\n[4] introduces a temporal dimension to the latent space of\nthe 2D diffusion model and fine-tuned it using videos, effec-\ntively transforming the image generator into a video genera-\ntor and enabling high-resolution video synthesis. Similarly,\nLVDM [26] explores lightweight video diffusion models,\nmaking use of a low-dimensional 3D latent space. Make-A-\nVideo [43] also employs a pre-trained text-to-image model,\neliminating the need for large-scale video training. More-\nover, in the Imagen Video [27], a cascading video diffusion\nmodel is built upon the pretrained 2D diffusion model [27].\nDiffT [25] and W.A.L.T [18] improve the video genera-\ntion by utilizing a Transformer-based Diffusion network.\nRecently, Emu Video [17] and PixelDance [57] propose\na two-step factorization approach for text-to-video gener-\nation, wherein the process is initially decomposed into text-\nto-image conversion, followed by image-to-video synthesis.\nThis methodology capitalizes on the effectiveness of con-\ntemporary text-to-image models, strategically directing the\nfocus of the video diffusion model training toward the learn-\ning of motion dynamics. However, diffusion-based methods\nhave difficulty integrating multiple modalities within a sin-\ngle model. Furthermore, these diffusion-based approaches\nstruggle to produce results that accurately capture dynamics\nand motion.\n2.2. World Models\nWorld models play a pivotal role in comprehending and\npredicting the dynamic nature of our environment, holding\nimmense potential for acquiring insights into motion and\nphysics on a global scale. Initially, the exploration of world\nmodel [19] focuses primarily on gaming scenarios, present-\ning a generative neural network model capable of learning\ncondensed representations of spatial and temporal dynamics\nwithin game environments. Subsequent research within the\nDreamer series [21\u201323] affirmed the effectiveness of world\nmodels across a diverse array of gaming scenarios. Given\nits structured nature and critical significance, the domain of\nautonomous driving has emerged as a forefront application\narea for world models. Numerous approaches [31,32,49,50]\nhave been introduced to assess the efficacy of world mod-\nels in autonomous driving scenarios. Additionally, Day-\nDreamer [52] has expanded the scope of world models to\nencompass real-world robotic environments. However, it\nis noteworthy that current world models primarily oper-\nate within the realms of gaming, robotics, and autonomous\ndriving, lacking the capability to comprehensively capture\nthe motion and physics of the general world.\n3. WorldDreamer\n3.1. Overall Framework\nThe overall framework of WorldDreamer is depicted in\nFig. 2. The initial phase involves encoding visual signals\n(i.e., images and videos) into discrete tokens using a visual\ntokenizer. These tokens undergo a carefully devised mask-\ning strategy before being processed by STPT. Meanwhile,\ntextual and action signals are separately encoded into em-\nbeddings, which serve as multimodal prompts. STPT en-\ngages in the pivotal task of predicting the masked visual\ntokens, which are then decoded by visual decoders, facili-\ntating video generation and editing in multiple contexts.\nTo train WorldDreamer, we construct triplets of Visual-\nText-Action data, where the training supervision solely in-\nvolves predicting masked visual tokens without any addi-\ntional supervision signals.\nWorldDreamer also supports\ntraining without text or action data, which not only re-\nduces the difficulty of data collection but also enables\nWorldDreamer to learn unconditional or single-condition\nvideo generation. At inference time, WorldDreamer can\naccomplish various video generation and video editing\ntasks: (1) For image-to-video, only a single image input\nis needed, considering the remaining frames as masked.\nWorldDreamer can also predict the future frames based on\nboth single image condition and text condition.\n(2) For\nvideo stylization, a video segment can be input, with a ran-\ndom masking of certain pixels. WorldDreamer can alter\nthe video style, such as creating an autumn-themed effect,\nbased on both the input language. (3) For text-to-video,\nproviding language input allows WorldDreamer to predict\nthe corresponding video, assuming that all visual tokens are\nmasked. (4) For video inpainting, a video segment can be\ninput, with a manually masked region of interest. World-\nDreamer can fill in the masked portion based on the input\nlanguage and unmasked visual signals. (5) For action-to-\nvideo, inputting the initial frame of a driving scene along\nwith future driving commands allows WorldDreamer to pre-\ndict future frames.\nThe subsequent subsections elaborate on the model ar-\nchitecture and the masking strategy.\n3.2. Model Architecture\nPreliminery WorldDreamer utilizes VQGAN [16] to tok-\nenize visual signals:\nV = Fv(I),\n(1)\nwhere I \u2208 RN\u00d7H\u00d7W \u00d73 are N frames of visual inputs.\nVQGAN Fv downsamples the resolution by 16\u00d7, which\nproduces visual tokens TV \u2208 RN\u00d7h\u00d7w (h = H\n4 , w = W\n4 ).\nThe VQGAN has a vocabulary size of 8192 and is trained\nwith billions of images [41]. For text inputs, we employ the\npretrained T5 [39] to map them into high-dimensional em-\nbeddings ET \u2208 RK\u00d7CT, where K is the sequence length\nand CT is the embedding channel. To be compatible with\nthe feature learning in STPT, the text embeddings are re-\npeated for N frames, and the embedding channel is mapped\ninto CV. Furthermore, Multi Layer Perception (MLP) is uti-\nlized to encode action inputs, which generates action em-\nbeddings EA \u2208 RN \u00d7CV. The text embeddings and action\nembeddings are concatenated, producing the multimodal\nprompt embeddings EM \u2208 RN\u00d7(K+1)\u00d7CV. Note that either\nVisual Embeddings\nMultimodal\nEmbeddings\n3D Conv. Layers\n\u00d7 \u01b6\nSpatial Temporal \nPatchwise Self-attention\nSpatial-wise\nCross-attention\nFigure 3. Overall architecture of STPT. STPT first utilizes 3D con-\nvolution to aggregate visual embeddings. Then these embeddings\nare partitioned into several patches for spatial-temporal patchwise\nself-attention.In the following, spatial-wise cross attention is ap-\nplied to facilitate feature interaction between visual embeddings\nand multimodal embeddings.\ntext or action embedding can be empty, enabling uncondi-\ntional learning.\nDuring training, and the optimizing objective is to pre-\ndict the masked visual token conditioned on unmasked to-\nkens and multimodal prompts:\nLWorldDreamer = \u2212 log p( \u02c6TV| eTV, EM),\n(2)\nwhere \u02c6TV are masked visual tokens, and eTV are unmasked\nvisual tokens.\nSTPT STPT leverages the foundation of U-ViT [30] while\nstrategically enhancing its architecture to better capture\nthe intricacies of spatial-temporal dynamics in video data.\nSpecifically, STPT confines the attention mechanism within\nspatial-temporal patches. Additionally, to seamlessly incor-\nporate multimodal information, spatial-wise cross-attention\nis employed to integrate multimodal embeddings. For in-\nput tokens \u02c6TV, STPT transforms them into visual embed-\ndings EV \u2208 RN\u00d7h\u00d7w\u00d7CV by referencing a learnable code-\nbook. The size of this codebook is set to 8193, exceeding\nthe codebook size of VQGAN by 1, which enables com-\npatibility with masked tokens. In each layer of STPT, as\nillustrated in Fig. 3, the visual embeddings are first pro-\ncessed through a 3D convolutional network. Then these\nembeddings are spatially partitioned into several patches\nEP \u2208 RN\u00d7h/s\u00d7w/s\u00d7CV, where we empirically set patch\nstride s as 2. Subsequently, each patch embeddings are flat-\ntened for spatial-temporal patchwise self-attention:\nEP = G\u22121\ns (Fst(Gs(EP))),\n(3)\nwhere Gs is the flatten operation that maps the embedding\ndimension to RNhw/s2\u00d7CV, and G\u22121\ns\nis the reverse opera-\ntion. Fs is the standard self-attention. These patches are\nDiffusion Model: Denoising Process\nAutoregressive Model: Autoregressively Predicting Next Token\nWorldDreamer: Parallel Predicting Masked Tokens\nFigure 4. Comparison between the inference schedule of diffusion-based methods, autoregressive methods and WorldDreamer. Diffusion-\nbased methods usually require \u223c30 steps to reduce noise, and autoregressive methods need \u223c200 steps to iteratively predict the next token.\nIn contrast, WorldDreamer parallel predicts masked tokens, achieving video generation in about 10 steps.\nthen concatenated and reshaped back to their original di-\nmensions. In the following, the spatial-wise cross attention\nis applied, which facilitates feature interaction between vi-\nsual embeddings and multimodal embeddings:\nEV = Fc(Ev, EM),\n(4)\nwhere Fc is the cross-attention operation that regards the\nframe number as batch size. After being processed through\nL layers of STPT, the feature dimensionality of EV is\nmapped to the codebook size of VQGAN. This enables the\nutilization of softmax to calculate the probability of each\ntoken, facilitating the prediction of masked visual tokens.\nFinally, cross-entropy loss is employed to optimize the pro-\nposed STPT:\nLce( \u02c6TV, PSTPT( eTV, EM)),\n(5)\nwhere PSTPT( eTV, EM) are visual token probabilities pre-\ndicted by the STPT.\nNotably, the proposed STPT can be trained jointly with\nvideos and images. For image inputs, we simply replace\nthe attention weight of Fs as a diagonal matrix [18]. Simul-\ntaneous training on both video and image datasets offers a\nsubstantial augmentation of the training samples, enabling\nmore efficient utilization of extensive image datasets. Be-\nsides, the joint training strategy has significantly enhanced\nWorldDreamer\u2019s capability to comprehend temporal and\nspatial aspects within visual signals.\n3.3. Mask Strategy\nMask strategy is crucial for training WorldDreamer, fol-\nlowing [8], we train WorldDreamer utilizing a dynamic\nmasking rate based on cosine scheduling. Specifically, we\nsample a random mask rate r \u2208 [0, 1] in each iteration, and\ntotally 2hw\n\u03c0 (1 \u2212 r2)\n\u22121\n2\ntokens are masked in each frame.\nNote that we employ the same token mask across different\nframes. This decision is grounded in the similarity of vi-\nsual signals between adjacent frames. Using different token\nmasks could potentially lead to information leakage during\nthe learning process. In comparison to an autoregressive\nmask scheduler, the dynamic mask schedule employed in\nour approach is crucial for parallel sampling at inference\ntime, which enables the prediction of multiple output tokens\nin a single forward pass. This strategy capitalizes on the as-\nsumption of a Markovian property, where many tokens be-\ncome conditionally independent given other tokens [8]. The\ninference process also follows a cosine mask schedule, se-\nlecting a fixed fraction of the highest-confidence masked to-\nkens for prediction at each step. Subsequently, these tokens\nare unmasked for the remaining steps, effectively reducing\nthe set of masked tokens. As shown in Fig. 4, diffusion-\nbased methods usually require \u223c30 steps to reduce noise,\nand autoregressive methods need \u223c200 steps to iteratively\npredict the next token. In contrast, WorldDreamer, paral-\nlel predicts masked tokens in about 10 steps, presenting a\n3\u00d7 \u223c 20\u00d7 acceleration compared to diffusion-based or au-\ntoregressive methods.\n4. Experiment\n4.1. Datasets\nWe employ a diverse set of images and videos to train\nWorldDreamer, enhancing its understanding of visual dy-\nnamics. The specific data utilized in this training includes:\nDeduplicated LAION-2B [34] The original LAION\ndataset [41] presented challenges such as data duplication\nand discrepancies between textual descriptions and accom-\npanying images. We follow [36] to address these issues.\nSpecifically, we opted to utilize the deduplicated LAION-\n2B dataset [34] for training WorldDreamer. This refined\ndataset excludes images with a watermark probability ex-\nceeding 50% or an NSFW probability surpassing 45%. The\ndeduplicated LAION dataset was made available by [41],\nfollowing the methodology introduced in [51].\nWebVid-10M [1] WebVid-10M comprises approxi-\nmately 10 million short videos, each lasting an average\nof 18 seconds and primarily presented in the resolution\nof 336 \u00d7 596. Each video is paired with associated text\ncorrelated with the visual content. A challenge posed by\nWebVid-10M is the presence of watermarks on all videos,\nresulting in the watermark being visible in all generated\nvideo content. Therefore, we opted to further refine World-\nDreamer leveraging high-quality self-collected video-text\npairs.\nSelf-collected video-text pairs We obtain publicly avail-\nable video data from the internet and apply the procedure\ndetailed in [3] to preprocess the obtained videos. Specifi-\ncally, we use PySceneDetect [7] to detect the moments of\nscene switching and obtain video clips of a single continu-\nous scene. Then, we filtered out clips with slow motion by\ncalculating optical flow. Consequently, 500K high-quality\nvideo clips are obtained for training. For video caption,\nwe extract the 10th, 50th, and 90th percentile frames of\nthe video as keyframes. These key frames are processed\nby Gemini [44] to generate captions for each keyframe.\nAdditionally, Gemini is instructed to aggregate these indi-\nvidual image captions into an overall caption for the entire\nvideo. Regarding that highly descriptive captions enhance\nthe training of generative models [2], we prompt Gemini\nto generate captions with as much detail as possible. The\ndetailed captions allow WorldDreamer to learn more fine-\ngrained text-visual correspondence.\nNuScenes [6]\nNuScenes is a popular dataset for au-\ntonomous driving, which comprises a total of 700 training\nvideos and 150 validation videos. Each video includes ap-\nproximately 20 seconds at a frame rate of 12Hz. World-\nDreamer utilizes the front-view videos in the training set,\nwith a frame interval of 6 frames. In total, there are approx-\nimately 28K driving scene videos for training. For video\ncaption, we prompt Gemini to generate a detailed descrip-\ntion of each frame, including weather, time of the day, road\nstructure, and important traffic elements. Then Gemini is\ninstructed to aggregate these image captions into an over-\nall caption for each video. Furthermore, we extract the yaw\nangle and velocity of the ego-car as the action metadata.\n4.2. Implementation Details\nTrain details WorldDreamer is first trained on a combi-\nnation of WebVid and LAION datasets. For WebVid videos,\nwe extract 16 frames as a training sample. For the LAION\ndataset, 16 independent images are selected as a training\nsample. Each sample is resized and cropped to an input res-\nolution of 256 \u00d7 256. WorldDreamer is trained over 2M\niterations with a batch size of 64. The training process in-\nvolves the optimization with AdamW and a learning rate of\n5 \u00d7 10\u22125, weight decay 0.01. To enhance the training and\nextend the data scope, WorldDreamer is further finetuned\non self-collected datasets and nuScenes data, where all (1B)\nparameters of STPT can be trained. During the finetuning\nstage, the input resolution is 192\u00d7320, and each sample has\n24 frames. WorldDreamer is finetuned over 20K iterations\nwith a batch size of 32, and the learning rate is 1 \u00d7 10\u22125.\nInference details At inference time, Classifier-Free\nGuidance (CFG) [28] is utilized to enhance the generation\nquality.\nSpecifically, we randomly eliminate multimodal\nembeddings for 10% of training samples. During inference,\nwe calculate a conditional logit c and an unconditional logit\nu for each masked token. The final logits g are then derived\nby adjusting away from the unconditional logits by a factor\nof \u03b2, referred to as the guidance scale:\ng = (1 + \u03b2)c \u2212 \u03b2u.\n(6)\nFor the predicted visual tokens, we employ the pretrained\nVQGAN decoder to directly output the video.\nNotably,\nWorldDreamer can generate a video consisting of 24 frames\nat a resolution of 192 \u00d7 320, which takes only 3 seconds on\na single A800.\n4.3. Visualizations\nWe have conducted comprehensive visual experiments\nto demonstrate that WorldDreamer has acquired a profound\nunderstanding of the general visual dynamics of the gen-\neral world. Through detailed visualizations and results, we\npresent compelling evidence showcasing Worlddreamer\u2019s\nability to achieve video generation and video editing across\ndiverse scenarios.\nImage to Video WorldDreamer excels in high-fidelity\nimage-to-video generation across various scenarios. As il-\nlustrated in Fig. 5, based on the initial image input, World-\ndreamer has the capability to generate high-quality, cine-\nmatic landscape videos. The resulting videos exhibit seam-\nless frame-to-frame motion, akin to the smooth camera\nmovements seen in real films. Moreover, these videos ad-\nhere meticulously to the constraints imposed by the original\nimage, ensuring a remarkable consistency in frame compo-\nsition. It generates subsequent frames adhering to the con-\nstraints of the initial image, ensuring remarkable frame con-\nsistency.\nFigure 5. WorldDreamer excels in producing high-fidelity image-to-video generation across various scenarios.\n\u201cA brown bear is surfing in the rainbow.\u201d\n\u201cA train passes through the valley, anime-style.\u201d\n\u201cEvening streets of Japan, zoom in.\u201d\n\u201cThe waves crash against the beach.\u201d\nFigure 6. WorldDreamer demonstrates proficiency in generating videos from text in diverse stylistic paradigms.\nText to Video Fig. 6 demonstrates WorldDreamer\u2019s re-\nmarkable proficiency in generating videos from text across\nvarious stylistic paradigms.\nThe produced videos seam-\nlessly align with the input language, where the language\nserves as a powerful control mechanism for shaping the\ncontent, style, and camera motion of the videos. This high-\n\u201cJellyfish swim in the sea.\u201d\n\u201cA Bear swim in the sea.\u201d\n\u201cA lizard is reading \na book in the desert.\u201d\n\u201cA monkey is reading a book in the desert.\u201d\nFigure 7. WorldDreamer possesses an exceptional ability to achieve high-quality video inpainting.\n\u201cA sailboat glides \nacross the lake.\u201d\n\u201cAutumn style, a sailboat glides across the lake.\u201d\n\u201cA small elephant is \ncrossing the forest.\u201d\n\u201cAutumn style, a small elephant is crossing the forest.\u201d\nFigure 8. WorldDreamer excels in delivering high-quality video stylization capabilities.\nlights WorldDreamer\u2019s effectiveness in translating textual\ndescriptions into visually faithful video content.\nVideo Inpainting As depicted in Fig. 7, WorldDreamer\nexhibits an exceptional ability for high-quality video in-\npainting. By providing a mask outlining the specific area of\ninterest and a text prompt specifying desired modifications,\nWorldDreamer intricately alters the original video, yielding\nremarkably realistic results in the inpainting process.\nVideo Stylization Fig. 8 shows that WorldDreamer ex-\ncels in delivering high-quality video stylization. By sup-\nplying a randomly generated visual token mask and a style\nprompt indicating desired modifications, WorldDreamer\nconvincingly transforms the original video, achieving a gen-\nuinely realistic outcome in the stylization process.\nAction to Video WorldDreamer shows the ability to gen-\nerate videos based on actions in the context of autonomous\ndriving. As shown in Fig. 9, given identical initial frames\nand different driving actions, WorldDreamer can produce\ndistinct future videos corresponding to different driving ac-\ntions (e.g., controlling the car to make a left-turn or a right-\nturn).\n5. Conclusion\nIn conclusion, WorldDreamer marks a notable advance-\nment in world modeling for video generation. Unlike tra-\nditional models constrained to specific scenarios, World-\nDreamer capture the complexity of general world dynamic\nenvironments. WorldDreamer frames world modeling as a\nvisual token prediction challenge, fostering a comprehen-\nsive comprehension of general world physics and motions,\nwhich significantly enhances the capabilities of video gen-\neration. In experiments, WorldDreamer shows exceptional\nperformance across scenarios like natural scenes and driv-\ning environments, showcasing its adaptability in tasks such\nas text-to-video conversion, image-to-video synthesis, and\nvideo editing.\nTurn  Left\nTurn  Right\nTurn  Left\nTurn  Right\nFigure 9. WorldDreamer excels in realizing the ability to generate videos based on actions in the context of autonomous driving.\nReferences\n[1] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisser-\nman. Frozen in time: A joint video and image encoder for\nend-to-end retrieval. In ICCV, 2021. 6\n[2] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng\nWang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce\nLee, Yufei Guo, et al.\nImproving image generation with\nbetter captions.\nComputer Science. https://cdn. openai.\ncom/papers/dall-e-3. pdf, 2023. 6\n[3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel\nMendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi,\nZion English, Vikram Voleti, Adam Letts, et al. Stable video\ndiffusion: Scaling latent video diffusion models to large\ndatasets. arXiv preprint arXiv:2311.15127, 2023. 2, 6\n[4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In CVPR, 2023. 3\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. NeurIPS, 2020. 2\n[6] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora,\nVenice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-\nancarlo Baldan, and Oscar Beijbom.\nnuscenes: A multi-\nmodal dataset for autonomous driving. In CVPR, 2020. 6\n[7] Brandon Castellano.\nPyscenedetect.\nGithub repository,\n2020. 6\n[8] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot,\nJose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Mur-\nphy, William T Freeman, Michael Rubinstein, et al. Muse:\nText-to-image generation via masked generative transform-\ners. arXiv preprint arXiv:2301.00704, 2023. 2, 5\n[9] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T\nFreeman. Maskgit: Masked generative image transformer. In\nCVPR, 2022. 2\n[10] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang,\nXiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu,\nQifeng Chen, Xintao Wang, et al.\nVideocrafter1: Open\ndiffusion models for high-quality video generation. arXiv\npreprint arXiv:2310.19512, 2023. 2\n[11] Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Hee-\nwoo Jun, Prafulla Dhariwal, David Luan, and Ilya Sutskever.\nGenerative pretraining from pixels. 2020. 2\n[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova.\nBert:\nPre-training of deep bidirectional\ntransformers for language understanding.\narXiv preprint\narXiv:1810.04805, 2018. 2\n[13] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng,\nChang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao,\nHongxia Yang, et al. Cogview: Mastering text-to-image gen-\neration via transformers. NeurIPS, 2021. 2\n[14] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang.\nCogview2: Faster and better text-to-image generation via hi-\nerarchical transformers. NIPS, 2022. 2\n[15] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\ntransformers for high-resolution image synthesis. In CVPR,\n2021. 2\n[16] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\ntransformers for high-resolution image synthesis. In CVPR,\n2021. 4\n[17] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Du-\nval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi\nYin, Devi Parikh, and Ishan Misra. Emu video: Factoriz-\ning text-to-video generation by explicit image conditioning.\narXiv preprint arXiv:2311.10709, 2023. 3\n[18] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera\nHahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and Jos\u00b4e Lezama.\nPhotorealistic video generation with diffusion models. arXiv\npreprint arXiv:2312.06662, 2023. 3, 5\n[19] David Ha and J\u00a8urgen Schmidhuber. Recurrent world models\nfacilitate policy evolution. NeurIPS, 2018. 2, 3\n[20] Danijar Hafner, Kuang-Huei Lee, Ian Fischer, and Pieter\nAbbeel. Deep hierarchical planning from pixels. NeurIPS,\n2022. 2\n[21] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Moham-\nmad Norouzi. Dream to control: Learning behaviors by la-\ntent imagination. arXiv preprint arXiv:1912.01603, 2019. 2,\n3\n[22] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and\nJimmy Ba. Mastering atari with discrete world models. arXiv\npreprint arXiv:2010.02193, 2020. 2, 3\n[23] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy\nLillicrap. Mastering diverse domains through world models.\narXiv preprint arXiv:2301.04104, 2023. 2, 3\n[24] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy\nLillicrap. Mastering diverse domains through world models.\narXiv preprint arXiv:2301.04104, 2023. 2\n[25] Ali Hatamizadeh, Jiaming Song, Guilin Liu, Jan Kautz, and\nArash Vahdat. Diffit: Diffusion vision transformers for im-\nage generation. arXiv preprint arXiv:2312.02139, 2023. 3\n[26] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and\nQifeng Chen. Latent video diffusion models for high-fidelity\nvideo generation with arbitrary lengths.\narXiv preprint\narXiv:2211.13221, 2022. 3\n[27] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022. 3\n[28] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 6\n[29] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu,\nand Jie Tang.\nCogvideo:\nLarge-scale pretraining for\ntext-to-video generation via transformers.\narXiv preprint\narXiv:2205.15868, 2022. 2\n[30] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. sim-\nple diffusion: End-to-end diffusion for high resolution im-\nages. arXiv preprint arXiv:2301.11093, 2023. 4\n[31] Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez,\nGeorge Fedoseev, Alex Kendall, Jamie Shotton, and Gian-\nluca Corrado.\nGaia-1: A generative world model for au-\ntonomous driving. arXiv preprint arXiv:2309.17080, 2023.\n2, 3\n[32] Fan Jia, Weixin Mao, Yingfei Liu, Yucheng Zhao, Yuqing\nWen, Chi Zhang, Xiangyu Zhang, and Tiancai Wang.\nAdriver-i: A general world model for autonomous driving.\narXiv preprint arXiv:2311.13549, 2023. 2, 3\n[33] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jos\u00b4e Lezama,\nJonathan Huang, Rachel Hornung, Hartwig Adam, Hassan\nAkbari, Yair Alon, Vighnesh Birodkar, et al. Videopoet: A\nlarge language model for zero-shot video generation. arXiv\npreprint arXiv:2312.14125, 2023. 2\n[34] Hugo Laurenc\u00b8on, Lucile Saulnier, L\u00b4eo Tronchon, Stas Bek-\nman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Sid-\ndharth Karamcheti, Alexander M Rush, Douwe Kiela, et al.\nObelisc: An open web-scale filtered dataset of interleaved\nimage-text documents.\narXiv preprint arXiv:2306.16527,\n2023. 6\n[35] Jessy Lin, Yuqing Du, Olivia Watkins, Danijar Hafner, Pieter\nAbbeel, Dan Klein, and Anca Dragan. Learning to model\nthe world with language. arXiv preprint arXiv:2308.01399,\n2023. 2\n[36] Suraj Patil, William Berman, Robin Rombach, and Patrick\nvon Platen. amused: An open muse reproduction. arXiv\npreprint arXiv:2401.01808, 2024. 6\n[37] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. Improving language understanding by gen-\nerative pre-training. OpenAI, 2018. 2\n[38] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, Ilya Sutskever, et al. Language models are unsu-\npervised multitask learners. OpenAI, 2019. 2\n[39] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J Liu. Exploring the limits of transfer learning with\na unified text-to-text transformer. The Journal of Machine\nLearning Research, 2020. 4\n[40] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In ICML, 2021. 2\n[41] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. NIPS, 2022. 4, 6\n[42] Younggyo Seo, Danijar Hafner, Hao Liu, Fangchen Liu,\nStephen James, Kimin Lee, and Pieter Abbeel.\nMasked\nworld models for visual control. In CoRL, 2023. 2\n[43] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint arXiv:2209.14792,\n2022. 3\n[44] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui\nWu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan\nSchalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a\nfamily of highly capable multimodal models. arXiv preprint\narXiv:2312.11805, 2023. 6\n[45] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete\nrepresentation learning. NeurIPS, 2017. 2\n[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. NIPS, 2017. 2\n[47] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kin-\ndermans, Hernan Moraldo, Han Zhang, Mohammad Taghi\nSaffar, Santiago Castro, Julius Kunze, and Dumitru Erhan.\nPhenaki: Variable length video generation from open domain\ntextual description. ICLR, 2023. 2\n[48] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,\nXiang Wang, and Shiwei Zhang. Modelscope text-to-video\ntechnical report. arXiv preprint arXiv:2308.06571, 2023. 2\n[49] Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen,\nand Jiwen Lu.\nDrivedreamer: Towards real-world-driven\nworld models for autonomous driving.\narXiv preprint\narXiv:2309.09777, 2023. 2, 3\n[50] Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen,\nand Zhaoxiang Zhang. Driving into the future: Multiview\nvisual forecasting and planning with world model for au-\ntonomous driving. arXiv preprint arXiv:2311.17918, 2023.\n2, 3\n[51] Ryan Webster, Julien Rabin, Loic Simon, and Frederic Ju-\nrie.\nOn the de-duplication of laion-2b.\narXiv preprint\narXiv:2303.12733, 2023. 6\n[52] Philipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter\nAbbeel, and Ken Goldberg. Daydreamer: World models for\nphysical robot learning. In CoRL, 2023. 2, 3\n[53] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind\nSrinivas. Videogpt: Video generation using vq-vae and trans-\nformers. arXiv preprint arXiv:2104.10157, 2021. 2\n[54] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-\njan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-\nfei Yang, Burcu Karagol Ayan, et al.\nScaling autoregres-\nsive models for content-rich text-to-image generation. arXiv\npreprint arXiv:2206.10789, 2022. 2\n[55] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jos\u00b4e Lezama, Han\nZhang, Huiwen Chang, Alexander G Hauptmann, Ming-\nHsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked\ngenerative video transformer. In CVPR, 2023. 2\n[56] Lijun Yu, Jos\u00b4e Lezama, Nitesh B Gundavarapu, Luca Ver-\nsari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim\nGupta, Xiuye Gu, Alexander G Hauptmann, et al. Language\nmodel beats diffusion\u2013tokenizer is key to visual generation.\narXiv preprint arXiv:2310.05737, 2023. 2\n[57] Yan Zeng,\nGuoqiang Wei,\nJiani Zheng,\nJiaxin Zou,\nYang Wei, Yuchen Zhang, and Hang Li.\nMake pixels\ndance:\nHigh-dynamic video generation.\narXiv preprint\narXiv:2311.10982, 2023. 3\n"
  },
  {
    "title": "FreGrad: Lightweight and Fast Frequency-aware Diffusion Vocoder",
    "link": "https://arxiv.org/pdf/2401.10032.pdf",
    "upvote": "11",
    "text": "FREGRAD: LIGHTWEIGHT AND FAST FREQUENCY-AWARE DIFFUSION VOCODER\nTan Dat Nguyen\u2217, Ji-Hoon Kim\u2217, Youngjoon Jang, Jaehun Kim, Joon Son Chung\nKorea Advanced Institute of Science and Technology, South Korea\nABSTRACT\nThe goal of this paper is to generate realistic audio with a lightweight\nand fast diffusion-based vocoder, named FreGrad. Our framework\nconsists of the following three key components: (1) We employ dis-\ncrete wavelet transform that decomposes a complicated waveform\ninto sub-band wavelets, which helps FreGrad to operate on a simple\nand concise feature space, (2) We design a frequency-aware dilated\nconvolution that elevates frequency awareness, resulting in gener-\nating speech with accurate frequency information, and (3) We intro-\nduce a bag of tricks that boosts the generation quality of the proposed\nmodel. In our experiments, FreGrad achieves 3.7 times faster train-\ning time and 2.2 times faster inference speed compared to our base-\nline while reducing the model size by 0.6 times (only 1.78M param-\neters) without sacrificing the output quality. Audio samples are avail-\nable at: https://mm.kaist.ac.kr/projects/FreGrad.\nIndex Terms\u2014 speech synthesis, vocoder, lightweight model,\ndiffusion, fast diffusion\n1. INTRODUCTION\nNeural vocoder aims to generate audible waveforms from interme-\ndiate acoustic features (e.g. mel-spectrogram). It becomes an es-\nsential building block of numerous speech-related tasks including\nsinging voice synthesis [1, 2], voice conversion [3, 4], and text-\nto-speech [5, 6, 7].\nEarlier neural vocoders [8, 9] are based on\nautoregressive (AR) architecture, demonstrating the ability to pro-\nduce highly natural speech.\nHowever, their intrinsic architecture\nrequires a substantial number of sequential operations, leading to\nan extremely slow inference speed. Numerous efforts in speeding\nup the inference process have been made on non-AR architecture\nbased on flow [10, 11], generative adversarial networks [12, 13, 14],\nand signal processing [15, 16]. While such approaches have accel-\nerated the inference speed, they frequently produce lower quality\nwaveforms compared to AR methods. Among non-AR vocoders,\ndiffusion-based vocoders have recently attracted increasing attention\ndue to its promising generation quality [17, 18, 19, 20, 21, 22, 23].\nDespite its high-quality synthetic speech, diffusion-based vocoder\nsuffers from slow training convergence speed, inefficient inference\nprocess, and high computation cost. These factors hinder the utiliza-\ntion of diffusion-based vocoders in low-resource devices and their\napplication in real-world scenarios. While many works [19, 21, 24]\nhave tried to minimize training and inference times, there still re-\nmains a limited exploration to reduce computational costs.\nTo address the aforementioned problems at once, in this paper,\nwe propose a novel diffusion-based vocoder called FreGrad, which\n\u2217These authors contributed equally to this work. This work was sup-\nported by the National Research Foundation of Korea grant funded by the\nKorean government (Ministry of Science and ICT, RS-2023-00212845) and\nthe ITRC (Information Technology Research Center) support program (IITP-\n2024-RS-2023-00259991) supervised by the IITP (Institute for Information\n& Communications Technology Planning & Evaluation).\n0.0\n0.2\n0.4\n0.6\n0.8\nReal-time factor\n3.0\n3.5\n4.0\nMean opinion score\nWaveGrad (15.81M)\nDiffWave (2.62M)\nPriorGrad (2.62M)\nFreGrad (1.78M)\nFig. 1. FreGrad successfully reduces both real-time factor and the\nnumber of parameters while maintaining the synthetic quality.\nachieves both low memory consumption and fast processing speed\nwhile maintaining the quality of the synthesized audio. The key to\nour idea is to decompose the complicated waveform into two simple\nfrequency sub-band sequences (i.e. wavelet features), which allow\nour model to avoid heavy computation. To this end, we utilize dis-\ncrete wavelet transform (DWT) that converts a complex waveform\ninto two frequency-sparse and dimension-reduced wavelet features\nwithout a loss of information [25, 26]. FreGrad successfully reduces\nboth the model parameters and denoise processing time by a signifi-\ncant margin. In addition, we introduce a new building block, named\nfrequency-aware dilated convolution (Freq-DConv), which enhances\nthe output quality. By incorporating DWT into the dilated convolu-\ntional layer, we provide the inductive bias of frequency information\nto the module, and thereby the model can learn accurate spectral\ndistributions which serves as a key to realistic audio synthesis. To\nfurther enhance the quality, we design a prior distribution for each\nwavelet feature, incorporate noise transformation that replaces the\nsub-optimal noise schedule, and leverage a multi-resolution magni-\ntude loss function that gives frequency-aware feedback.\nIn the experimental results, we demonstrate the effectiveness of\nFreGrad with extensive metrics. FreGrad demonstrates a notable en-\nhancement in boosting model efficiency while keeping the genera-\ntion quality. As shown in Table 1, FreGrad boosts inference time by\n2.2 times and reduces the model size by 0.6 times with mean opinion\nscore (MOS) comparable to existing works.\n2. BACKGROUNDS\nThe denoising diffusion probabilistic model is a latent variable\nmodel that learns a data distribution by denoising a noisy sig-\nnal [27]. The forward process q(\u00b7) diffuses data samples through\nGaussian transitions parameterized with a Markov process:\nq(xt|xt\u22121) = N(xt;\np\n1 \u2212 \u03b2txt\u22121, \u03b2tI),\n(1)\narXiv:2401.10032v1  [eess.AS]  18 Jan 2024\nskip\nskip\nDWT\nAdd noise\nCompute\nprior\nTimestep\nembedding\nConv1D\nskip\nFrequency-aware\nResBlock\nConv1D + ReLU\nData-dependent\nnoise sampling\nMel upsampler\n(x128)\n...\nConv1D\nConv1D\nConv1D\ntime\nembedding\nmel\ncondition\nFrequency-\naware Dilated\nConv1D\nskip\nConv1D\nConv1D\n...\n...\n...\n...\n||\n: Fixed block\n: Compute loss\n||\n: Multiply\n: Data \ufb02ow\n: Add\n: Concatenate\n: Learnable blocks\nFig. 2. Training procedure and model architecture of FreGrad. We compute wavelet features {xl, xh} and prior distributions {\u03c3l, \u03c3h} from\nwaveform x and mel-spectrogram X, respectively. At timestep t, noises {\u03f5l, \u03f5h} are added to each wavelet feature. Given mel-spectrogram\nand timestep embedding, FreGrad approximates the noises {\u02c6\u03f5l, \u02c6\u03f5h}. The training objective is a weighted sum of Ldiff and Lmag between\nground truth and the predicted noise.\nwhere \u03b2t \u2208 {\u03b21, . . . , \u03b2T } is the predefined noise schedule, T is the\ntotal number of timesteps, and x0 is the ground truth sample. This\nfunction allows sampling xt from x0, which can be formulated as:\nxt = \u221a\u03b3tx0 +\np\n1 \u2212 \u03b3t\u03f5,\n(2)\nwhere \u03b3t = Qt\ni=1(1 \u2212 \u03b2i) and \u03f5 \u223c N(0, I).\nWith a sufficiently large T, the distribution of xT approxi-\nmates an Isotropic Gaussian distribution.\nConsequently, we can\nobtain a sample in ground truth distribution by tracing the exact\nreverse process p(xt\u22121|xt) from an initial point xT \u223c N(0, I).\nSince p(xt\u22121|xt) depends on the entire data distribution, we ap-\nproximate it with a neural network p\u03b8(xt\u22121|xt) which is defined\nas N(xt\u22121; \u00b5\u03b8(xt, t), \u03c32\n\u03b8(xt, t)). As shown in [27], the variance\n\u03c32\n\u03b8(\u00b7) can be represented as\n1\u2212\u03b3t\u22121\n1\u2212\u03b3t \u03b2t, and mean \u00b5\u03b8(\u00b7) is given by:\n\u00b5\u03b8(xt, t) =\n1\n\u221a1 \u2212 \u03b2t\n\u0012\nxt \u2212\n\u03b2t\n\u221a1 \u2212 \u03b3t\n\u03f5\u03b8(xt, t)\n\u0013\n,\n(3)\nwhere \u03f5\u03b8(\u00b7) is a neural network that learns to predict the noise.\nIn practice, the training objective for \u03f5\u03b8(\u00b7) is simplified to mini-\nmize Et,xt,\u03f5\n\u0002\n\u2225\u03f5 \u2212 \u03f5\u03b8(xt, t)\u22252\n2\n\u0003\n. PriorGrad [20] extends the idea by\nstarting the sampling procedure from the prior distribution N(0, \u03a3).\nHere, \u03a3 is a diagonal matrix diag\n\u0002\n(\u03c32\n0, \u03c32\n1, . . . , \u03c32\nN)\n\u0003\n, where \u03c32\ni\nis the ith normalized frame-level energy of mel-spectrogram with\nlength N. Accordingly, the loss function for \u03f5\u03b8(\u00b7) is modified as:\nLdiff = Et,xt,\u03f5,c\n\u0002\n\u2225\u03f5 \u2212 \u03f5\u03b8(xt, t, X)\u22252\n\u03a3\u22121\n\u0003\n,\n(4)\nwhere \u2225x\u22252\n\u03a3\u22121 = x\u22a4\u03a3\u22121x and X is a mel-spectrogram.\n3. FREGRAD\nThe network architecture of FreGrad is rooted in DiffWave [17]\nwhich is a widely used backbone network for diffusion-based\nvocoders [20, 23]. However, our method is distinct in that it op-\nerates on a concise wavelet feature space and replaces the existing\ndilated convolution with the proposed Freq-DConv to reproduce\naccurate spectral distributions.\n3.1. Wavelet Features Denoising\nTo avoid complex computation, we employ DWT before forward\nprocess. DWT downsamples the target dimension audio x0 \u2208 RL\ninto two wavelet features {xl\n0, xh\n0} \u2282 R\nL\n2 , each of which represents\nlow- and high-frequency components. As demonstrated in the pre-\nvious works [26, 28], the function can deconstruct a non-stationary\nsignal without information loss due to its biorthogonal property.\nFreGrad operates on simple wavelet features. At each training\nstep, the wavelet features xl\n0 and xh\n0 are diffused into noisy fea-\ntures at timestep t with distinct noise \u03f5l and \u03f5h, and each noise is\nsimultaneously approximated by a neural network \u03f5\u03b8(\u00b7). In reverse\nprocess, FreGrad simply generates denoised wavelet features, {\u02c6xl\n0,\n\u02c6xh\n0} \u2282 R\nL\n2 , which are finally converted into the target dimensional\nwaveform \u02c6x0 \u2208 RL by inverse DWT (iDWT):\n\u02c6x0 = \u03a6\u22121(\u02c6xl\n0, \u02c6xh\n0),\n(5)\nwhere \u03a6\u22121(\u00b7) denotes the iDWT function.\nNote that FreGrad generates speech with smaller computations\ndue to the decomposition of complex waveforms. In addition, the\nmodel maintains its synthetic quality, as iDWT guarantees a lossless\nreconstruction of a waveform from wavelet features [28, 29]. In our\nexperiments, we adopt Haar wavelet [30].\n3.2. Frequency-aware Dilated Convolution\nSince audio is a complicated mixture of various frequencies [26], it\nis important to reconstruct accurate frequency distributions for nat-\nural audio synthesis. To enhance the synthetic quality, we propose\nFreq-DConv which deliberately guides the model to pay attention to\nthe frequency information. As illustrated in Fig. 3, we adopt DWT\nto decompose the hidden signal y \u2208 R\nL\n2 \u00d7D into two sub-bands\n{yl, yh} \u2282 R\nL\n4 \u00d7D with hidden dimension D. The sub-bands are\nchannel-wise concatenated, and the following dilated convolution\nf(\u00b7) extracts a frequency-aware feature yhidden \u2208 R\nL\n4 \u00d72D:\nyhidden = f(cat(yl, yh)),\n(6)\nDWT\nconcat\nDilated Conv1D\nsplit\niDWT\nFig. 3. Frequency-aware dilated convolution.\nwhere cat denotes concatenation operation. The extracted feature\nyhidden is then bisected into {y\u2032\nl, y\u2032\nh} \u2282 R\nL\n4 \u00d7D along channel di-\nmension, and finally iDWT converts the abstract features into single\nhidden representation to match the length with input feature y:\ny\u2032 = \u03a6\u22121(y\u2032\nl, y\u2032\nh),\n(7)\nwhere y\u2032 \u2208 R\nL\n2 \u00d7D represents the output of the Freq-DConv. As\ndepicted in Fig. 2, we embed the Freq-DConv into every ResBlock.\nThe purpose of decomposing the hidden signal before the di-\nlated convolution is to increase the receptive field along the time\naxis without changing the kernel size. As a result of DWT, each\nwavelet feature has a reduced temporal dimension while preserving\nall temporal correlations. This helps each convolution layer to pos-\nsess a larger receptive field along the time dimension even with the\nsame kernel size. Furthermore, low- and high-frequency sub-bands\nof each hidden feature can be explored separately. As a result, we\ncan provide an inductive bias of frequency information to the model,\nwhich facilitates the generation of frequency-consistent waveform.\nWe verify the effectiveness of Freq-DConv in Sec. 4.3.\n3.3. Bag of Tricks for Quality\nPrior distribution. As demonstrated in previous works [20, 22],\na spectrogram-based prior distribution can significantly enhance the\nwaveform denoising performance even with fewer sampling steps.\nBuilding upon this, we design a prior distribution for each wavelet\nsequence based on the mel-spectrogram. Since each sub-band se-\nquence contains specific low- or high-frequency information, we use\nseparate prior distribution for each wavelet feature. Specifically, we\ndivide the mel-spectrogram into two segments along the frequency\ndimension and adopt the technique proposed in [20] to obtain sepa-\nrate prior distributions {\u03c3l, \u03c3h} from each segment.\nNoise schedule transformation. As discussed in [31, 32], signal-\nto-noise ratio (SNR) should ideally be zero at the final timestep T\nof forward process. However, noise schedules adopted in previous\nworks [17, 18, 20] fail to reach SNR near zero at the final step, as\nshown in Fig. 4. To achieve a zero SNR at the final step, we adopt\nthe proposed algorithm in [32], which can be formulated as follows:\n\u221a\u03b3new =\n\u221a\u03b30\n\u221a\u03b30 \u2212 \u221a\u03b3T + \u03c4 (\u221a\u03b3 \u2212 \u221a\u03b3T + \u03c4),\n(8)\nwhere \u03c4 helps to avoid division by zero in sampling process.\nLoss function. A common training objective of diffusion vocoder is\nto minimize the L2 norm between predicted and ground truth noise,\nwhich lacks explicit feedbacks in the frequency aspect.\nTo give\na frequency-aware feedback to the model, we add multi-resolution\nshort-time Fourier transform (STFT) magnitude loss (Lmag). Differ-\nent from the previous works [14, 24], FreGrad only uses magnitude\n0\n10\n20\n30\n40\n50\nt\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nBaselines\nFreGrad\n(a) Noise level \u03b3\n30\n35\n40\n45\n50\nt\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\nSNR\nBaselines\nFreGrad\n(b) SNR\nFig. 4. Noise level and log SNR through timesteps. \u201cBaselines\u201d refer\nto the work of [17, 18, 20] which use the same linear beta schedule\n\u03b2 ranging from 0.0001 to 0.05 for 50 diffusion steps.\npart since we empirically find that integrating spectral convergence\nloss downgrades the output quality. Let M be the number of STFT\nlosses, then Lmag can be represented as:\nLmag = 1\nM\nM\nX\ni=1\nL(i)\nmag,\n(9)\nwhere L(i)\nmag is STFT magnitude loss from ith analysis settings [14].\nWe separately apply the diffusion loss to low- and high-frequency\nsub-bands, and the final training objective is defined as:\nLfinal =\nX\ni\u2208{l,h}\nh\nLdiff(\u03f5i, \u02c6\u03f5i) + \u03bbLmag(\u03f5i, \u02c6\u03f5i)\ni\n,\n(10)\nwhere \u02c6\u03f5 refers to an estimated noise.\n4. EXPERIMENTS\n4.1. Training Setup\nWe conduct experiments on a single English speaker LJSpeech1\nwhich contains 13,100 samples. We use 13,000 random samples for\ntraining and 100 remaining samples for testing. Mel-spectrograms\nare computed from the ground truth audio with 80 mel filterbanks,\n1,024 FFT points ranging from 80Hz to 8,000Hz, and hop length\nof 256. FreGrad is compared against the best performing publicly\navailable diffusion vocoders: WaveGrad2, DiffWave3, and Prior-\nGrad4. For fair comparison, all the models are trained until 1M\nsteps, and all the audios are generated through 50 diffusion steps\nwhich is the default setting in DiffWave [17] and PriorGrad [20].\nFreGrad consists of 30 frequency-aware residual blocks with a\ndilation cycle length of 7 and a hidden dimension of 32. We follow\nthe implementation of DiffWave [17] for timestep embedding and\nmel upsampler but reduce the upsampling rate by half because the\ntemporal length is halved by DWT. For Lmag, we set M = 3 with\nFFT size of [512, 1024, 2048] and window size of [240, 600, 1200].\nWe choose \u03c4 = 0.0001 and \u03bb = 0.1 for Eqn. (8) and Eqn. (10), re-\nspectively. We utilize Adam optimizer with \u03b21 = 0.9, \u03b22 = 0.999,\nfixed learning rate of 0.0002, and batch size of 16.\n1https://keithito.com/LJ-Speech-Dataset\n2https://github.com/lmnt-com/wavegrad\n3https://github.com/lmnt-com/diffwave\n4https://github.com/microsoft/NeuralSpeech\nTable 1. Evaluation results. The MOS results are presented with 95% confidence intervals. \u2191 means higher is better, \u2193 denotes lower is better.\nModel\nMOS \u2191\nMAE \u2193\nMR-STFT \u2193\nMCD13 \u2193\nRMSEf0 \u2193\n#params \u2193\nRTF on CPU \u2193\nRTF on GPU \u2193\nGround truth\n4.74 \u00b1 0.06\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nWaveGrad\n3.14 \u00b1 0.09\n0.59\n1.39\n3.06\n39.97\n15.81M\n11.58\n0.29\nDiffWave\n4.00 \u00b1 0.10\n0.56\n1.18\n3.20\n40.10\n2.62M\n29.99\n0.64\nPriorGrad\n4.19 \u00b1 0.10\n0.47\n1.14\n2.22\n40.42\n2.62M\n29.20\n0.65\nFreGrad\n4.12 \u00b1 0.11\n0.45\n1.12\n2.19\n38.73\n1.78M\n11.95\n0.29\n \n(a) Ground truth\n \n(b) FreGrad\n \n(c) PriorGrad\nFig. 5. Spectrogram analysis on FreGrad and PriorGrad. While PriorGrad suffers from over-smoothed results, FreGrad reproduces detailed\nspectral correlation, especially in red boxes.\n4.2. Audio Quality and Sampling Speed\nWe verify the effectiveness of FreGrad on various metrics. To eval-\nuate the audio quality, we obtain mel-cepstral distortion (MCD13)\nand 5-scale MOS where 25 subjects rate the naturalness of 50 audio\nsamples. In addition, we compute mean absolute error (MAE), f0\nroot mean square error (RMSEf0), and multi-resolution STFT error\n(MR-STFT) between generated and ground truth audio. To compare\nthe model efficiency, we calculate the number of model parameters\n(#params) and real-time factor (RTF) which is measured on AMD\nEPYC 7452 CPU and a single GeForce RTX 3080 GPU. Except for\nMOS, all the metrics are obtained from 100 audio samples.\nAs demonstrated in Table 1, FreGrad highly reduces not only the\nnumber of model parameters but also inference speed on both CPU\nand GPU. In addition, FreGrad achieves the best results in all the\nquality evaluation metrics except for MOS. Given humans\u2019 height-\nened sensitivity to low-frequency sounds, we hypothesize that the\nMOS degradation in FreGrad results from low-frequency distribu-\ntion. However, in perspective of the entire spectrum of frequencies,\nFreGrad consistently demonstrates superior performance compared\nto existing methods, as confirmed by the MAE, MR-STFT, MCD13,\nand RMSEf0. The mel-spectrogram visualization analysis (Fig. 5)\nalso demonstrates the effectiveness of FreGrad in reconstructing ac-\ncurate frequency distributions. In addition, FreGrad takes significant\nadvantage of fast training time. It requires 46 GPU hours to con-\nverge, 3.7 times faster than that of PriorGrad with 170 GPU hours.\n4.3. Ablation Study on Proposed Components\nTo verify the effectiveness of each FreGrad component, we conduct\nablation studies by using comparative MOS (CMOS), RMSEf0, and\nRTF. In CMOS test, raters are asked to compare the quality of au-\ndio samples from two systems from \u22123 to +3. As can be shown\nTable 2. Ablation study for FreGrad components.\nCMOS \u2191\nRMSEf0 \u2193\nRTF on GPU \u2193\nFreGrad\n0.00\n38.73\n0.29\nw/o Freq-DConv\n\u22121.34\n39.05\n0.18\nw/o separate prior\n\u22120.26\n38.91\n0.29\nw/o zero SNR\n\u22120.69\n39.17\n0.29\nw/o Lmag\n\u22120.68\n39.82\n0.29\nin Table 2, each component independently contributes to enhancing\nthe synthetic quality of FreGrad. Especially, the utilization of Freq-\nDConv substantially elevates the quality with a slight trade-off in\ninference speed, where the increased RTF still surpasses those of ex-\nisting approaches. The generation qualities show relatively small but\nnoticeable degradations when the proposed separate prior and zero\nSNR techniques are not applied. The absence of Lmag results in the\nworst performance in terms of RMSEf0, which indicates that Lmag\ngives effective frequency-aware feedback.\n5. CONCLUSION\nWe proposed FreGrad, a diffusion-based lightweight and fast\nvocoder. FreGrad operates on a simple and concise wavelet feature\nspace by adopting a lossless decomposition method. Despite the\nsmall computational overhead, FreGrad can preserve its synthetic\nquality with the aid of Freq-DConv and the bag of tricks, which\nis designed specifically for diffusion-based vocoders.\nExtensive\nexperiments demonstrate that FreGrad significantly improves model\nefficiency without degrading the output quality. Moreover, we verify\nthe effectiveness of each FreGrad component by ablation studies.\nThe efficacy of FreGrad enables the production of human-like audio\neven on edge devices with limited computational resources.\n6. REFERENCES\n[1] Jinglin Liu, Chengxi Li, Yi Ren, Feiyang Chen, and Zhou\nZhao, \u201cDiffSinger: Singing voice synthesis via shallow dif-\nfusion mechanism,\u201d in Proc. AAAI, 2022.\n[2] Yi Ren, Xu Tan, Tao Qin, Jian Luan, Zhou Zhao, and Tie-Yan\nLiu, \u201cDeepSinger: Singing voice synthesis with data mined\nfrom the web,\u201d in Proc. KDD, 2020.\n[3] Kaizhi Qian, Yang Zhang, Shiyu Chang, Xuesong Yang, and\nMark Hasegawa-Johnson,\n\u201cAutoVC: Zero-shot voice style\ntransfer with only autoencoder loss,\u201d in Proc. ICML, 2019.\n[4] Hyeong-Seok Choi, Juheon Lee, Wansoo Kim, Jie Lee, Hoon\nHeo, and Kyogu Lee,\n\u201cNeural analysis and synthesis: Re-\nconstructing speech from self-supervised representations,\u201d in\nNeurIPS, 2021.\n[5] Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schus-\nter, Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang,\nYuxuan Wang, RJ-Skerrv Ryan, Rif A. Saurous, Yannis\nAgiomyrgiannakis, and Yonghui Wu, \u201cNatural TTS synthe-\nsis by conditioning wavenet on mel spectrogram predictions,\u201d\nin Proc. ICASSP, 2018.\n[6] Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima\nSadekova, and Mikhail A. Kudinov, \u201cGrad-TTS: A diffusion\nprobabilistic model for text-to-speech,\u201d in Proc. ICML, 2021.\n[7] Jaehyeon Kim, Sungwon Kim, Jungil Kong, and Sungroh\nYoon, \u201cGlow-TTS: A generative flow for text-to-speech via\nmonotonic alignment search,\u201d in NeurIPS, 2020.\n[8] A\u00a8aron van den Oord, Sander Dieleman, Heiga Zen, Karen Si-\nmonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, An-\ndrew W. Senior, and Koray Kavukcuoglu, \u201cWaveNet: A gen-\nerative model for raw audio,\u201d in Proc. SSW, 2016.\n[9] Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Ku-\nmar, Shubham Jain, Jose Sotelo, Aaron C. Courville, and\nYoshua Bengio, \u201cSampleRNN: An unconditional end-to-end\nneural audio generation model,\u201d in Proc. ICLR, 2017.\n[10] Ryan Prenger, Rafael Valle, and Bryan Catanzaro, \u201cWaveG-\nlow: A flow-based generative network for speech synthesis,\u201d\nin Proc. ICASSP, 2019.\n[11] Wei Ping, Kainan Peng, Kexin Zhao, and Zhao Song, \u201cWave-\nFlow: A compact flow-based model for raw audio,\u201d in Proc.\nICML, 2020.\n[12] Kundan Kumar, Rithesh Kumar, Thibault de Boissiere, Lucas\nGestin, Wei Zhen Teoh, Jose Sotelo, Alexandre de Br\u00b4ebisson,\nYoshua Bengio, and Aaron C. Courville, \u201cMelGAN: Genera-\ntive adversarial networks for conditional waveform synthesis,\u201d\nin NeurIPS, 2019.\n[13] Jesse H. Engel, Kumar Krishna Agrawal, Shuo Chen, Ishaan\nGulrajani, Chris Donahue, and Adam Roberts, \u201cGANSynth:\nAdversarial neural audio synthesis,\u201d in Proc. ICLR, 2019.\n[14] Ryuichi Yamamoto, Eunwoo Song, and Jae-Min Kim, \u201cPar-\nallel Wavegan: A fast waveform generation model based on\ngenerative adversarial networks with multi-resolution spectro-\ngram,\u201d in Proc. ICASSP, 2020.\n[15] Lauri Juvela, Bajibabu Bollepalli, Vassilis Tsiaras, and Paavo\nAlku, \u201cGlotNet - A raw waveform model for the glottal exci-\ntation in statistical parametric speech synthesis,\u201d IEEE/ACM\nTrans. on Audio, Speech, and Language Processing, vol. 27,\nno. 6, pp. 1019\u20131030, 2019.\n[16] Takuhiro Kaneko, Kou Tanaka, Hirokazu Kameoka, and Shogo\nSeki,\n\u201ciSTFTNET: Fast and lightweight mel-spectrogram\nvocoder incorporating inverse short-time fourier transform,\u201d in\nProc. ICASSP, 2022.\n[17] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan\nCatanzaro, \u201cDiffWave: A versatile diffusion model for audio\nsynthesis,\u201d in Proc. ICLR, 2021.\n[18] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad\nNorouzi, and William Chan, \u201cWaveGrad: Estimating gradients\nfor waveform generation,\u201d in Proc. ICLR, 2021.\n[19] Rongjie Huang, Max W. Y. Lam, Jun Wang, Dan Su, Dong Yu,\nYi Ren, and Zhou Zhao, \u201cFastDiff: A fast conditional diffusion\nmodel for high-quality speech synthesis,\u201d in Proc. IJCAI, 2022.\n[20] Sang-gil Lee, Heeseung Kim, Chaehun Shin, Xu Tan, Chang\nLiu, Qi Meng, Tao Qin, Wei Chen, Sungroh Yoon, and Tie-Yan\nLiu, \u201cPriorGrad: Improving conditional denoising diffusion\nmodels with data-dependent adaptive prior,\u201d in Proc. ICLR,\n2022.\n[21] Max W. Y. Lam, Jun Wang, Dan Su, and Dong Yu, \u201cBDDM:\nBilateral denoising diffusion models for fast and high-quality\nspeech synthesis,\u201d in Proc. ICLR, 2022.\n[22] Yuma Koizumi, Heiga Zen, Kohei Yatabe, Nanxin Chen, and\nMichiel Bacchiani, \u201cSpecGrad: Diffusion probabilistic model\nbased neural vocoder with adaptive noise spectral shaping,\u201d in\nProc. Interspeech, 2022.\n[23] Naoya Takahashi, Mayank Kumar, Singh, and Yuki Mitsu-\nfuji, \u201cHierarchical diffusion models for singing voice neural\nvocoder,\u201d in Proc. ICASSP, 2023.\n[24] Zehua Chen, Xu Tan, Ke Wang, Shifeng Pan, Danilo P.\nMandic, Lei He, and Sheng Zhao, \u201cInferGrad: Improving dif-\nfusion models for vocoder by considering inference in train-\ning,\u201d in Proc. ICASSP, 2022.\n[25] Ingrid Daubechies, Ten Lectures on Wavelets, SIAM, 1992.\n[26] Ji-Hoon Kim, Sang-Hoon Lee, Ji-Hyun Lee, and Seong-Whan\nLee, \u201cFre-GAN: Adversarial frequency-consistent audio syn-\nthesis,\u201d in Proc. Interspeech, 2021.\n[27] Jonathan Ho, Ajay Jain, and Pieter Abbeel, \u201cDenoising diffu-\nsion probabilistic models,\u201d in NeurIPS, 2020.\n[28] Sang-Hoon Lee, Ji-Hoon Kim, Kangeun Lee, and Seong-Whan\nLee, \u201cFre-GAN 2: Fast and efficient frequency-consistent au-\ndio synthesis,\u201d in Proc. ICASSP, 2022.\n[29] Julien Reichel, Gloria Menegaz, Marcus J Nadenau, and Murat\nKunt, \u201cInteger wavelet transform for embedded lossy to loss-\nless image compression,\u201d IEEE Trans. on Image Processing,\nvol. 10, no. 3, pp. 383\u2013392, 2001.\n[30] Alfred Haar, Zur theorie der orthogonalen funktionensysteme,\nGeorg-August-Universitat, Gottingen., 1909.\n[31] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans, \u201cSim-\nple diffusion: End-to-end diffusion for high resolution im-\nages,\u201d in Proc. ICML, 2023.\n[32] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang,\n\u201cCommon diffusion noise schedules and sample steps are\nflawed,\u201d in Proc. WACV, 2024.\n"
  },
  {
    "title": "SHINOBI: Shape and Illumination using Neural Object Decomposition via BRDF Optimization In-the-wild",
    "link": "https://arxiv.org/pdf/2401.10171.pdf",
    "upvote": "10",
    "text": "SHINOBI: Shape and Illumination using Neural Object Decomposition via\nBRDF Optimization In-the-wild\nAndreas Engelhardt\u2020\nUniversity of T\u00a8ubingen\nAmit Raj\nGoogle Research\nMark Boss\nUnity\nYunzhi Zhang\u2020\nStanford University\nAbhishek Kar\nGoogle Research\nYuanzhen Li\nGoogle Research\nDeqing Sun\nGoogle Research\nRicardo Martin Brualla\nGoogle Research\nJonathan T. Barron\nGoogle Research\nHendrik P. A. Lensch\nUniversity of T\u00a8ubingen\nVarun Jampani\u2217\nGoogle Research\n\u2026\nIn-the-wild images\nNeural Volume \n& Camera Parameters\nNormals\nIllumination\nBasecolor\nMetallic  \nRoughness\nDecomposition\nFigure 1. Object reconstruction using SHINOBI. SHINOBI decomposes challenging in-the-wild image collections into shape, material\nand illumination using a neural field representation while also optimizing camera parameters.\nAbstract\nWe present SHINOBI, an end-to-end framework for the re-\nconstruction of shape, material, and illumination from object\nimages captured with varying lighting, pose, and background.\nInverse rendering of an object based on unconstrained image\ncollections is a long-standing challenge in computer vision\nand graphics and requires a joint optimization over shape,\nradiance, and pose. We show that an implicit shape repre-\nsentation based on a multi-resolution hash encoding enables\nfaster and robust shape reconstruction with joint camera\nalignment optimization that outperforms prior work. Further,\nto enable the editing of illumination and object reflectance\n(i.e. material) we jointly optimize BRDF and illumination to-\n*Current affiliation is Stability AI.\n\u2020Work done during a Student Researcher position at Google.\ngether with the object\u2019s shape. Our method is class-agnostic\nand works on in-the-wild image collections of objects to\nproduce relightable 3D assets for several use cases such as\nAR/VR, movies, games, etc.\nProject page: https://shinobi.aengelhardt.\ncom\n1. Introduction\nWe present a category-agnostic technique to jointly recon-\nstruct 3D shape and material properties of objects from un-\nconstrained in-the-wild image collections. This data regime\nposes multiple challenges as images are captured in different\nenvironments using a variety of devices resulting in vary-\ning backgrounds, illumination, camera poses, and intrinsics.\nIn addition, camera baselines tend to be large. Fig. 1 (left)\nshows examples from an input image set. Many graphics\n1\narXiv:2401.10171v1  [cs.CV]  18 Jan 2024\napplications in AR/VR, games, and movies depend on high-\nquality 3D assets of real-world objects. Physically based\nmaterials are essential to integrate objects into new environ-\nments. The conventional acquisition involves laborious tasks\nlike 3D modeling, texture painting, and light calibration or\nuse controlled setups [6, 53] that are hard to scale. It is eas-\nier to obtain casually captured images from smartphones or\nimage collections from the internet for a large number of\nobjects.\nConventional structure-from-motion techniques like\nCOLMAP [64, 65] fail to reconstruct image collections un-\nder these challenging circumstances [14, 33]. Despite con-\nstraining the correspondences to lie within object bounds,\nspecifically in the context of the NAVI [33] in-the-wild\nscenes, less than half of the views are registered on aver-\nage with half the scenes failing completely. Consequently,\nwe observe that camera pose optimization has the largest\nimpact on the reconstruction quality in this setting. Many\nexisting works on shape and material estimation [6, 13, 68,\n79, 81, 86, 89] assume constant camera intrinsics and initial-\nization of camera poses close to the true poses. We support\n360\u00b0 multiview data with a rough quadrant-based pose ini-\ntialization with poses potentially far from the ground truth, as\nin SAMURAI [14] and NeRS [83]. For challenging data this\ncan be annotated in only a few minutes per image collection.\nEven though in SAMURAI [14], camera poses can be ini-\ntialized from very coarse directions slight offsets often lead\nto overly smooth textures and shapes in the final reconstruc-\ntions. Further, existing methods for material decomposition\nwith camera pose optimization are slow, often running more\nthan 12 hours on a single object [14, 37]. In contrast, we\npropose a pipeline based on multiresolution hash grids [51]\nwhich allows us to process more rays in a shorter time during\noptimization. Using this advantage we are able to improve\nreconstruction quality compared to SAMURAI while still\nkeeping a competitive run-time (Tab. 1).\nNaive integration of multi-resolution hash grids is not\nwell suited to camera pose estimation due to discontinuities\nin the gradients with respect to the input positions. We pro-\npose several components that work together to stabilize the\ncamera pose optimization and encourage sharp features. The\nkey distinguishing features of SHINOBI include:\n\u2022 Hybrid Multiresolution Hash Encoding with level anneal-\ning. We combine the multiresolution hash-based encod-\ning [51] with regular Fourier feature transformation of the\ninput coordinates to regularize the low-frequency gradi-\nent propagation. This makes the optimization significantly\nmore robust while only adding a small overhead. A similar\napproach has been recently proposed by Zhu et al. [91]\nfor a different task. We show that it is also beneficial for\ncamera pose optimization.\n\u2022 Camera multiplex constraint. We modify the cam-\nera parameterization of SAMURAI to avoid over-\nparameterization of the camera rotations. Furthermore,\nwe constrain the camera optimization with a projection-\nbased loss to enforce consistency over the camera propos-\nals inside a multiplex which further helps to smooth the\noptimization in the initial phase.\n\u2022 Per-view importance weighting. We propose a per-view im-\nportance weighting to leverage the important observation\nthat some views are more useful for optimization than oth-\ners. Specifically, we use well-working cameras to anchor\nthe reconstruction during the optimization.\n\u2022 Patch-based alignment losses. SHINOBI proposes a novel\npatch level loss to aid in camera alignment and additionally\nintroduces a silhouette loss inspired by Lensch et al. [38]\nfor better image to 3D alignment.\nExperiments on NAVI [33] in-the-wild datasets demon-\nstrate better view synthesis and relighting results with SHI-\nNOBI compared to existing works with a reduced run-time.\nCompared to SAMURAI the results look sharper and the av-\nerage runtime is cut in half. Fig. 1 (right) shows some sample\napplication results with 3D assets generated by SHINOBI .\nOur representation enables editing of appearance parameters,\nillumination and based on the mesh extraction also shape,\nfacilitating various tasks in a downstream graphics pipeline.\n2. Related works\nNeural fields have emerged as a popular technique of late\nto encode spatial information in the network weights of e.g.\nan MLP, which can be retrieved by simply querying the co-\nordinates [16, 49, 56, 69]. Works like NeRF [50] leverage\nthis neural volume rendering to achieve photo-realistic view\nsynthesis results with view-dependent appearance variations.\nRapid research in neural fields followed, which alternated\nthe surface representations [55, 71, 74, 75, 77, 80], allowed\nreconstruction from sparse data [8, 32, 46, 54, 60, 70, 78],\nenabled extraction of 3D geometry and materials [12, 37, 52,\n83], or enabled relighting of scenes [5, 12\u201314, 43, 47, 81].\nHowever, most prior works rely on pose information ex-\ntracted from COLMAP [64, 65], which can be inaccurate or\ncompletely fail in complex settings or sparse data regimes.\nSHINOBI is independent of any pose reconstruction that\nrelies on feature matching and robust to very coarse initial-\nization.\nInstant Neural Graphics Primitives (I-NGP) [51] is a pop-\nular geometric representation that enables fast optimization\nwith improved memory utilization by using an encoding\nscheme based on multi-resolution hash tables. Despite the im-\nprovement in speed, I-NGP suffers from discontinuous and\noscillating gradient flow through the hash-based encoding,\nwhich complicates camera pose optimization [31, 91, 91].\nTo enable reconstruction with camera pose fine-tuning using\nhash grids, Heo et al. [31] propose a modification to the\ninterpolation weighting, BAA-NGP [45] dynamically repli-\ncates low-resolution features and CAMP [58] pairs a robust\nsampling scheme [4] with camera preconditioning. These\n2\nPositions xt\nCs\nj\nP s\nj\nDirection\nd\nIntrinsics \u02c6fj\nExtrinsics\npj\neye, dj\n\u03d5\u03b8, dj\nup\nH(x)\nMLP\n\u03b3(x)\nAnnealing\nAnnealed Hybrid Encoding\nMLP\nMLP\nNetwork\nBRDFs\nbt\nDensities\n\u03c3t\nIllumination\nzj\nR\nPIL-Renderer\nColor\n\u02c6cj\nPatch-based Losses\n: Optimizable\nPrameters\nFigure 2. The SHINOBI pipeline. Two resolution annealed encoding branches, the multiresolution hash grid H(x) and the Fourier\nembedding \u03b3(x) are used to learn a neural volume conditioned on the input coordinates. This enables robust optimization of camera\nparameters jointly with the shape, material and illumination.\nmethods however are sensitive to camera initialization and\nlighting conditions. In contrast to these works, SHINOBI\nis able to reconstruct consistent objects from images cap-\ntured under varying illuminations and backgrounds besides\nsupporting coarser poses.\nJoint camera and shape estimation is a highly ambiguous\ntask, traditionally relying on accurate poses for precise shape\nreconstruction and vice versa. Often techniques rely on corre-\nspondences across images to estimate camera poses [64, 65].\nRecent approaches integrate camera calibration with neural\nvolume training; SCNeRF [34] and NopeNeRF [8] use corre-\nspondences and monocular depth images, respectively. Other\nrecent methods rely on rough initialization of the camera,\nglobal alignment, or a template shape for joint optimiza-\ntion [15, 44, 76, 83]. Other methods use transformer-based\nmodels[23] to predict the initial pose from image collection\n[67, 84]. In comparison, SHINOBI works on unconstrained\nimage collections, including various camera parameters and\nobject environments, where existing methods struggle to\ngeneralize or require additional input data like depth.\nBRDF and illumination estimation is a challenging and am-\nbiguous problem. Casual BRDF estimation enables on-site\nmaterial acquisition with simple cameras and a co-located\ncamera flash. These techniques often constrain the problem\nto planar surfaces with either a single shot [2, 9, 20, 30, 40,\n61], few-shot [2] or multi-shot [3, 10, 21, 22, 26] captures.\nCasual capture can also be extended to a joint BRDF and\nshape reconstruction [5\u20137, 11, 35, 53, 61, 82], even on entire\nscenes [41, 66]. Most of these methods, however, require a\nknown active illumination. Recovering a BRDF under un-\nknown passive illumination is significantly more challenging\nas it requires disentangling the BRDF from the illumination.\nRecently, neural field-based decomposition achieved decom-\nposition of scenes under varying illumination [12, 13] or\nfixed illumination [43, 85, 86, 88, 89]. IntrinsicNeRF [81]\nextends decomposition to larger scenes at the cost of a simpli-\nfied reflectance model. However, all these approaches require\nknown, near-perfect camera poses, whereas SHINOBI can\nwork with unposed image collection to recover per-image\nillumination.\n3. Method\nThe aim of SHINOBI is to convert 2D image collections into\na 3D representation with minimal manual work. The repre-\nsentation includes shape, material parameters and per-view\nillumination, allowing for view synthesis with relighting.\nProblem setup. We define in-the-wild data as a collection\nof q images Cj \u2208 Rsj\u00d73; j \u2208 {1, . . . , q} that show the\nsame object captured with different backgrounds, illumina-\ntions and cameras with potentially varying resolutions sj.\nIn addition, we assume a rough camera initialization. For\nour experiments we annotate camera pose quadrants as in\nSAMURAI [14]. Foreground masks can be added if avail-\nable or automatically generated and might be imperfect at\nthis point. At each point x \u2208 R3 in the neural volume V,\nwe estimate the BRDF parameters for the Cook-Torrance\nmodel [19] b \u2208 R5 (basecolor bc \u2208 R3, metallic bm \u2208 R,\nroughness br \u2208 R), unit-length surface normal n \u2208 R3 and\nvolume density \u03c3 \u2208 R (Fig. 1). To enable the decomposition\nwe also estimate the latent per-image illumination vectors\nzl\nj \u2208 R128; j \u2208 {1, . . . , q} [13]. Furthermore, we estimate\nper-image camera poses and intrinsics. Next, we provide a\nbrief overview of prerequisites: NeRF [50], InstantNGP [51]\nand SAMURAI [14].\nCoordinate-based MLPs and NeRF [50] uses a dense neu-\nral network to model a continuous function that takes 3D\nlocation x \u2208 R3 and view direction d \u2208 R3 and outputs a\nview-dependent output color c \u2208 R3 and volume density\n\u03c3 \u2208 R. Mildenhall et al. [50] overcome the spectral bias of\n3\nthe MLPs by transforming the input coordinates by a second\nfunction; A frequency encoding \u03b3 that maps from R to R2L\n[50, 69]:\n\u03b3(x) = (sin(20\u03c0x), cos(20\u03c0x),\n. . . , sin(2L\u22121\u03c0x), cos(2L\u22121\u03c0x))\n(1)\nInstantNGP [51] speed up the NeRF optimization drasti-\ncally by replacing the MLP-based volume representation by\na multiresolution voxel hash grid that is tailored to current\nGPU hardware. For a hash-size T, grid vertices are indexed\nby a spatial hash function h(x) =\n\u0012 dL\ni=1\nxi\u03c0i\n\u0013\nmod T us-\ning large unique prime numbers \u03c0i [51]. At each voxel ver-\ntex a d-dimensional embedding is optimized. Instead of the\nFourier embedding, the 3D coordinates x are directly used\nto tri-linearly interpolate between neighboring vertices at\neach level. The results are concatenated and fed to a MLP\nto decode the representation. We denote the full encoding\nfunction including interpolation and concatenation as H(x).\nBrief overview of SAMURAI. SAMURAI is a method for\njoint optimization of 3D shape, BRDF, per-image camera\nparameters, and illuminations for a given in-the-wild image\ncollection. SAMURAI [14] follows the NeRF idea outlined\nabove but uses the Neural-PIL [13] method for physically-\nbased differentiable rendering. It takes 3D locations as input\nand outputs volume density and BRDF parameters. An ad-\nditional GLO (generative latent optimization) embedding\nmodels the changes in appearances (due to different illumi-\nnations) across images. Neural-PIL [13] introduced the use\nof per-image latent illumination embedding zl\nj and a spe-\ncialized illumination pre-integration (PIL) network for fast\nrendering, which we refer to as \u2018PIL rendering\u2019. Neural-PIL\noptimizes a per-image embedding to model image-specific\nillumination. The rendered output color \u02c6c is equivalent to\nNeRF\u2019s output c, but due to the explicit BRDF decomposi-\ntion and illumination modeling, it enables relighting and ma-\nterial editing. To address the unavailability of accurate cam-\nera parameters for in-the-wild images, SAMURAI jointly\noptimizes camera extrinsics and per-view intrinsics from\na very coarse initialization. In addition to a coarse-to-fine\nannealing [44], this is achieved with a multiplexed optimiza-\ntion scheme where multiple camera proposals per view are\nkept and weighted according to their performance on the\nloss over time.\n3.1. SHINOBI Optimization with Hash Encoding\nWe identify misaligned and inconsistent camera poses as the\nmain limiting factor for in-the-wild reconstructions. Joint\nshape and camera optimization is a severely underdeter-\nmined problem. Reconstruction is typically slow and often\nlacks high-frequency detail in textures and shape. Multireso-\nlution hash grids from Instant-NGP [51] have the potential to\nspeed up the reconstruction while simultaneously allowing\nfor larger ray counts to be processed and thereby improving\nvisual quality and alignment (see Tab. 1). However, the naive\nreplacement of the point encoding with Hash grids reduces\nthe reconstruction quality and robustness of the joint camera\nand shape optimization.\nHash grids adapt to individual views faster resulting in a\nnoisy shape in the presence of misaligned cameras. As re-\nported previously [31, 42, 45, 91] multi-resolution hash grids\nwith the default linear interpolation backpropagate noisy and\ndiscontinuous gradients with respect to the input position.\nAdditionally, the coarse-to-fine scheme from BARF [44] of-\nten used for camera fine-tuning cannot be directly transferred\nto hash grids. Therefore, we propose an approach that makes\nuse of a camera multiplex, adds additional geometrical con-\nstraints, and a new encoding scheme to be able to improve\nboth reconstruction speed and quality. Next, we explain each\nof the components in detail.\nArchitecture overview. A high-level overview of the SHI-\nNOBI architecture is shown in Fig. 2, which follows the\nskeleton of SAMURAI [14] with the \u2018PIL renderer\u2019 [13].\nHowever, we map the input coordinates x using a new hybrid\nencoding. The combined embedding is processed by a small\nMLP like in I-NGP [51] to predict the density \u03c3, and the\nview and appearance conditioned radiance for a given image\npatch. We also predict a regular direction-dependent radi-\nance \u02dcc to stabilize the early training stages as in [12, 14]. The\nBRDF decoder operates as in SAMURAI [14], expanding\nthe feature representation to the BRDF (base color, metallic,\nroughness). Per sample, we estimate normal direction from\nthe first order derivative of the density w.r.t. the input position\n\u2202\u03c3\n\u2202x. From there the volumetric rendering from NeRF [50]\nis performed and the shading for the given pixel coordinate\nis determined using BRDF, normals and the pre-integrated\nillumination estimated by the NeuralPIL network. See sup-\nplementary material for further details on the architecture.\nCamera pose initialization and parameterization. Camera\npose optimization is a highly non-convex problem and tends\nto quickly get stuck in local minima. Our initial camera poses\nare much noisier and feature larger distances between initial\nand true poses compared to many related works [37, 76]. To\ncombat this, we assume a rough initialization in the form\nof camera pose quadrants in line with SAMURAI [14] and\nNeRS [83]. We use a \u2018lookat + direction\u2019 representation for\nthe camera parameters, storing initial values and offsets for\nan eye position peye \u2208 R3, lookat direction \u2206d\u03d5\u03b8 \u2208 R2. and\nup rotation angle dup \u2208 R as well as the focal length f \u2208 R\nper camera. We notice that this removes the overparameteri-\nzation regarding the rotation component encoded in eye and\ncenter position of the regular \u2018lookat\u2019 parameterization. This\nformulation performs best in our setting also compared to\nother recently proposed representations [58, 90].\nHybrid positional encoding. We use a hash grid hybrid as\ncoordinate encoding to improve the gradient flow w.r.t. the in-\n4\nFigure 3. Constrained camera multiplex. We optimize multiple\ncamera proposals per image and weight the contribution to the\nreconstruction according to a camera\u2019s performance on the loss.\nBetween cameras of a multiplex we add a projection based regular-\nization: Points from all members are projected into the currently\nbest camera and then compared against a new render to enforce a\nconsistent geometry.\nput coordinates x. A Fourier-based coordinate mapping \u03b3(x)\nfollowed by a small MLP generates a base embedding that is\nconcatenated with the output of the multiresolution hash grid\nH(x) resulting in the following formulation of the neural\nvolume F\u2295 ((H(x), \u03b3(x))). On \u03b3, we apply BARF\u2019s [44]\nFourier annealing. Similarly, we progressively add resolu-\ntion levels to the hash grid encoding. Starting with only the\nfeatures from a low resolution dense grid we increase the\nweights of the higher resolution levels gradually over time\n(cf. [42, 45].\nCamera multiplexes. An effective way to reduce the chance\nof camera pose optimization to be stuck in local minima is\nthe camera multiplex [14, 27]. For each image, m cameras\nare jittered around the initial camera and simultaneously\noptimized. Over time the worst performing camera is repeat-\nedly faded out until m = 1. This process is visualized in\nFig. 3. Since we render multiple proposals for a given image\nanyway, we see an opportunity to further constrain the opti-\nmization using projective geometry. Specifically, we project\nthe point sets Xi rendered by the m \u2212 1 members into the\ncurrently highest ranking camera \u03980 of the multiplex using\nthe estimated depth Di from the volumetric rendering. Then\nwe render the projected coordinates using \u03980 and compare\nthe rendered color and alpha values to the ones originally\nrendered at \u03981...m\u22121.\nLmultiplex =\nm\u22121\nX\ni=1\nLimage(ci, F \u02c6cV (Pi,0(Xi, Di, \u0398i, \u03980)))\n+ Lmask(\u03b1i, F\u03b1V (Pi,0(Xi, Di, \u0398i, \u03980)))\n(2)\nwhere Pi,0 is the perspective warp from image coordinates in\ncamera i to the reference camera. FV is the rendering func-\ntion connected to the neural field outputting color \u02c6c and mask\nvalue \u03b1, respectively. This regularization comes roughly at\nthe cost of adding a camera to the multiplex. Subsampling\nof Xi can decrease the memory footprint if needed. Limage\nand Lmask are the optimization losses active at the time as\noutlined in Sec. 3.2. Naturally, this component is only active\nwhile there are multiple cameras rendered during the first\npart of the overall schedule. Used as an additional loss it\nturns out to be surprisingly effective in constraining the cam-\nera optimization and therefore increasing the robustness of\nthe overall optimization. Essentially, we are enforcing a con-\nsistent surface to be generated and smooth the optimization\nlandscape around an initial camera pose.\nView importance scaling of input images. Not every input\nmight contribute to the reconstruction in the same way and in-\ndividual views that are not aligned with the current 3D shape\nmight have a negative impact on the overall optimization\nprogress. To improve high-frequency detail in the reconstruc-\ntion we reduce the impact of potentially misaligned cameras\nwhile anchoring the optimization using cameras that work\nwell given the loss. We keep a circular buffer of around 1000\nelements with the recent per-image losses. Like in SAMU-\nRAI, this is used to re-weigh images in the given collection\naccording to: L(j)\nnetwork = spj L(j)\nnetwork, where\nspj = max\n \ntanh\n \n\u00b5l \u2212 (L(j)\nmask + L(j)\nimage)\n\u03c3l\n!\n+ 1, 1\n!\n,\n(3)\nwith the mean \u00b5l and standard deviation \u03c3l of the loss buffer.\nThis limits the influence of badly aligned camera poses on\nthe shape reconstruction. In addition, we also apply an impor-\ntance weighting on Lcamera that reduces the gradient magni-\ntude for views that are performing well given the loss history.\nSpecifically, at step t we compute: L(j)\ncamera = sqj,t L(j)\ncamera,\nwith\nsqj,t =sqj,t\u22121\u03bbp max\n \ntanh\n \n\u00b5l \u2212 (L(j)\nmask + L(j)\nimage)\n\u03c3l\n!\n+1, 1\n!\n+(1 \u2212 \u03bbp)sqj,t\u22121\n(4)\nIn practice, we set the hyperparameter \u03bbp to 0.05.\n3.2. Losses and Optimization\nMultiscale patch loss. After a short initial phase of random\nray sampling, we render randomly sampled patches of size\n16x16 to 32x32. The goal is to constrain the updates and\n5\n(a) Reference silhouette (b) Rendered silhouette\n(c) Loss map\nFigure 4. Our silhouette based alignment loss penalizes the un-\naligned pixels given a reference and the rendered gray scale masks.\nespecially the alignment to be consistent on local neighbor-\nhoods. Therefore, we add a multi-scale patch loss on the\nrendered color \u02c6c which computes a Charbonnier loss at four\ndifferent resolution levels, by simple bilinear resampling. We\nweigh each level to compensate for the different pixel counts\nand enforce the low-resolution version to align first.\nMask losses. We add a silhouette loss LSilhouette whenever\npatch-based sampling is active. Here, we penalize the area be-\ntween the two silhouettes which can be interpreted as the re-\nsult of an xor operation on the rendered and input mask [38].\nBoth masks are filtered using a Gaussian blur where the ra-\ndius is heuristically chosen based on the patch size. Fig. 4\nvisualizes how the loss helps with the alignment task. We\ncombine this loss with a regular binary-cross-entropy loss\non the mask value as well as a loss enforcing a transparent\nbackground.\nRegularization losses. To regularize the hash grid encoding\nwe apply a normalized weight decay as proposed in [4] to\nput a higher penalty on coarser grid levels compared to naive\nweight decay. Additionally, we apply regularization to the\ncamera poses and normal output. Refer to the supplements\nfor details and the hyperparameters used.\nOptimization. In total, we use three optimizers: One\nADAM [36] optimizer for the networks, hash grid embed-\ndings and cameras, respectively. The learning rate is decayed\nexponentially on all optimizers. In addition to the camera\nrepresentation and constraints mentioned above the config-\nuration of the camera optimizer turns out to be crucial for\nsuccess in connection with the hash grids. We use ADAM\nwith the \u03b21 value reduced to 0.2 to smooth out the noise in\nthe camera updates. The learning rate is tuned between 1e-3\nto 2e-3 depending on scene size. Furthermore, we use three\nfading \u03bb variables to transition the optimization schedule\nsmoothly. Render resolution is continuously increased over\nthe first half of the optimization while the number of active\nmultiplex cameras is reduced. The direct color optimization\nis faded to the BRDF optimization and the encoding an-\nnealing is performed over the first third of the optimization.\nFocal length updates and the view importance weighting\nare delayed until an initial shape has been formed. See the\nsupplementary material for a detailed description and visual-\nization of the optimization scheduling.\nImplementation. We implement the multi-resolution hash\nMethod\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nRuntime\nSC\n\u223c SC\nSC\n\u223c SC\nSC\n\u223c SC\nNeROIC [37]\n22.75 21.31 0.91\n0.90\n0.0984\n0.0845 18 hours (4 GPUs)\nNeRS [83]\n17.92 18.02\n0.92\n0.93\n0.114\n0.1098\n3 hours (1 GPU)\nSAMURAI [14] 25.34 24.61 0.92 0.91\n0.0958 0.1054\n12 hours (1 GPU)\nSHINOBI\n27.69 27.79 0.94\n0.94\n0.0607 0.0578\n4 hours (1 GPU)\nTable 1. Metrics for view synthesis on NAVI. View synthesis\nmetrics are computed over two subsets from all wild-sets depending\non the success of COLMAP (SC / \u223c SC). Rendering quality is\nevaluated on a holdout set of test views. We initialize with the GT\nposes provided by NAVI [33].\ngrid encoding as a custom CUDA extension for Tensor-\nflow [1]. The implementation roughly follows the official\nCUDA implementation [51]. We enable first- and second-\norder gradients for the encoding to allow for computing\nanalytical surface normals. The remaining components are\nimplemented in Tensorflow.\nMethod\nPSNR\u2191 SSIM\u2191 Transl.\u2193 Rot. \u00b0\u2193\nw/o Multiplex Consistency Loss\n25.80\n0.93\n0.29\n23.12\nw/o Per View Importance\n22.43\n0.90\n0.36\n35.10\nw/o Coarse-to-fine (annealing)\n21.47\n0.90\n0.37\n30.44\nw/o Hybrid Encoding\n25.31\n0.93\n0.30\n23.33\nw/o Patch-based Training\n20.60\n0.89\n0.45\n41.30\nFull\n25.87\n0.93\n0.30\n22.90\nTable 3. Ablation study. Ablating components of our framework\nresults in worse view synthesis and relighting results (averaged over\n\u201dKeywest\u201d and \u201dSchool Bus\u201d scenes from NAVI) demonstrating\ntheir importance.\n4. Experiments\nDataset For evaluations, we use the in-the-wild collections\nfrom the NAVI dataset [33] which feature objects captured in\ndiverse environments using multiple mobile devices. High-\nquality annotated camera poses allow us to ablate and per-\nform quantitative evaluation of our pose estimation.\nBaselines. The closest prior work that can tackle our task\noutline in Sec. 3 is SAMURAI [14] on which our method\nis based. We compare against SAMURAI as a baseline and\nalso conduct experiments using NeROIC [37], GNeRF [48],\nand a modified version of NeRS [83] (details in the supple-\nment). For experiments on joint shape and pose estimation,\nwe use the same quadrant-based pose initialization for NeRS,\nSAMURAI and SHINOBI (ours); and we use the the meth-\nods\u2019 default pose initializations for NeROIC (COLMAP)\nand GNeRF (Random).\nEvaluation. We use two strategies for evaluation. First, the\nstandard novel view synthesis metrics using the learned vol-\numes that measure PSNR, SSIM, and LPIPS [87] scores\non held-out test images. Second, to evaluate camera poses\nw.r.t GT poses, we use Procrustes analysis [28] to align the\n6\nMethod\nPose Init\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nTranslation\u2193\nRotation \u25e6 \u2193\nSC\n\u223c SC\nSC\n\u223c SC\nSC\n\u223c SC\nSC\n\u223c SC\nSC\n\u223c SC\nGNeRF [48]\nRandom\n8.30\n6.25\n0.64\n0.63\n0.52\n0.57\n1.02\u00b1 0.16\n1.04\u00b1 0.09\n93.15\u00b1 26.54\n80.22\u00b1 27.64\nNeROIC [37]\nCOLMAP\n19.77\n-\n0.88\n-\n0.150\n-\n0.09\u00b1 0.12\n-\n42.11\u00b1 17.19\n-\nNeRS [83]\nDirections\n18.67\n18.66\n0.92\n0.93\n0.108\n0.107\n0.49\u00b1 0.21\n0.52\u00b1 0.19\n122.41\u00b1 10.61\n123.63\u00b1\n8.80\nSAMURAI [14]\nDirections\n25.34\n24.61\n0.92\n0.91\n0.096\n0.105\n0.24\u00b1 0.17\n0.35\u00b1 0.24\n26.16\u00b1 22.72\n36.59\u00b1 29.98\nSHINOBI\nDirections\n25.15\n24.77\n0.92\n0.92\n0.090\n0.095\n0.250\u00b1 0.085 0.28\u00b1 0.09\n22.84\u00b1 16.19\n33.00\u00b1 19.97\nTable 2. Metrics for 3D shape and pose on NAVI. View synthesis and pose metrics over two subsets from all wild-sets depending\non the success of COLMAP (SC / \u223c SC). Rendering quality is evaluated on a holdout set of test views that are aligned as part of the\noptimization without contributing to the shape recovery. We include GNeRF as a separate baseline although this method is not designed for\nmulti-illumination data. We report metrics with the methods\u2019 default camera initialization and evaluate against the annotation provided in\nNAVI [33].\nInput\nBasecolor\nMetallic\nRoughness\nNormal\nIllumination\nRe-render\nOurs\nSAMURAI\nFigure 5. Comparison with SAMURAI decomposition. Due to the improved alignment and representation higher frequency details are\nreconstructed in shape and the BRDF components compared to SAMURAI. Notice the improved texture detail and silhouettes of ours.\ncameras and then compute the mean absolute rotation and\ntranslation differences in camera pose estimations for all\navailable views. For evaluation purposes, we optimize the\ncameras and illuminations on the test images but do not al-\nlow the test images to affect the other network parts or hash\ngrid embedding. For a fair comparison, we use the ground\ntruth masks as input to all methods although our method also\nincludes functionality to automatically generate segmenta-\ntion masks. We run experiments on a single Nvidia A100 or\nV100 GPU per scene.\nResults. Tab. 1 shows the performance of different methods\nfor in-the-wild reconstruction when using GT poses from\nNAVI. Following NAVI [33], we divide the scenes into two\nsubsets based on whether the COLMAP works (SC) or not\n(\u223c SC) as some techniques like NeROIC need COLMAP\nposes to work on unposed image collections. Using the pro-\nvided annotated poses SHINOBI clearly performs best on\nthe view synthesis task (Tab. 1). This shows the advantage\nof our hybrid encoding scheme and the patch-based losses\nover previous methods for in-the-wild scenes. Optimization\nruntimes of different techniques show that we are 3 times\nfaster than the next-best SAMURAI approach.\nTab. 2 shows results of joint shape and pose optimization\nfrom in-the-wild image collections when the GT camera\nposes are not given as input. SHINOBI outperforms both\nNeROIC and NeRS by a healthy margin while being on-par\nwith SAMURAI. While PSNR of SHINOBI is similar to\nSAMURAI, our method is able to reconstruct scenes consis-\ntently with lower translation and rotation pose errors (with\nalso lower standard deviation in pose metrics). This results\nin SHINOBI obtaining better LPIPS perceptual metrics com-\npared to SAMURAI. The on-par mean PSNR compared to\nSAMURAI mostly stems from individual test cameras not\nbeing aligned properly. This also happens for other meth-\nods but seems to be emphasized by the faster optimization\nscheduling in SHINOBI. NeROIC can also achieve good\nresults if camera poses are close to the ground truth but\nfails for many scenes where a COLMAP-based initializa-\ntion is not possible. NeRS also succeeds in reconstructing\nall scenes. However, it achieves lower-quality camera align-\nments. Fig. 7 visually compares view synthesis results from\ndifferent methods, which visually confirms that SHINOBI\ncan produce sharper results that are more faithful to the in-\nput images. Further results on the NAVI dataset are shown\nin Fig. 6, where we show novel views predicted by SHI-\nNOBI initialized with either GT poses or rough quadrants.\nVisual results clearly show that SHINOBI can recover the\npose and provide a consistent illumination w.r.t the ground-\ntruth target views in both settings.\nDecomposition results. Fig 5 compares the BRDF and illu-\nmination decomposition of SHINOBI to SAMURAI where\nthe same output modalities are available. Visual results show\n7\n(a) GT Novel View\n(b) GT pose init.\n(c) Direction pose init.\nFigure 6. View synthesis on NAVI. Renderings from SHINOBI us-\ning models initialized with camera pose quadrants only or the GT\nprovided by NAVI [33] compared to the input image.\nsignificantly more high-frequency detail and plausible mate-\nrial parameters with SHINOBI compared to SAMURAI.\nAblation study. We ablate different aspects of SHINOBI in\nterms of reconstruction metrics using the \u201cKeywest\u201d and\n\u201cSchool Bus\u201d, two in-the-wild sets from NAVI [33] of\nmedium complexity. Metrics in Tab. 3 show that the res-\nolution annealing coarse-to-fine scheme and the patch-based\nlosses contribute most significantly to the final quality. The\nlatter improves local details and registration accuracy com-\npared to a simple pixel-wise loss. The view importance\nweighting is another important factor for improved sharp-\nness. It helps to stabilize the optimization after the initial\nresolution annealing schedule has ended. While the hybrid\nencoding and camera multiplex consistency do not seem to\nhave a large impact quantitatively, they play a critical role in\nstabilizing the optimization over different scene types and\nscales. Without them, the optimization might take longer\nor not converge at all depending on the initialization. Vi-\nsual examples of the specific ablations are compared in the\nsupplementary material.\nApplications. In addition to novel view synthesis using the\nNeRF [50] representation, the parametric material model\nallows for controlled editing of the object\u2019s appearance. Also\nthe illumination can be adjusted, e.g. for realistic composites.\nA mesh extraction allows further editing and integration in\nthe standard graphics pipeline including real-time rendering.\nSHINOBI can help in obtaining relightable 3D assets for\ne-commerce applications as well as 3D AR and VR for\nentertainment and education. Refer to the supplementary\nmaterial for sample visual results on relighting, material\nediting etc.\nLimitations. Joint pose and shape reconstruction is an in-\nherently ill-posed problem. While SHINOBI improves over\nprevious work, especially symmetric objects and highly spec-\nSHINOBI\nGT Test View\nSAMURAI\nNeRS\nNeROIC\nFigure 7. Novel view synthesis compared to existing meth-\nods. Compared to other methods on an example view from the\nNAVI [33] in-the-wild test set, SHINOBI preserves fine detail and\nrecreates the lighting realistically.\n(a) Kitchen sink (rgb, normals)\n(b) Water gun (rgb, normals)\nFigure 8. Failure cases. Unconstrained image collections featuring\nhighly symmetric objects or homogenous surfaces still pose a chal-\nlenge and potentially require additional assistance.\nular materials can lead to failure cases as shown in Fig. 8.\nThe coarse-to-fine scheme is not able to resolve the disam-\nbiguities and the camera poses are stuck in a local mini-\nmum. All existing methods show these limitations to some\nextent. In some regions, high-frequency detail is still not\nreconstructed properly due to misaligned views and the lim-\nited high-frequency capabilities of the illumination repre-\nsentation [13]. Furthermore, our BRDF and illumination\ndecomposition is not capable of modeling shadowing and\ninter-reflections. As we are mainly concerned with single-\nobject decomposition, the shadows and inter-reflections are\nnot crucial. Extending this method to more complex light\ntransport modeling forms an important future work.\n5. Conclusion\nWe present SHINOBI, a framework for shape, pose, and\nillumination estimation of objects from unposed in-the-wild\nimage collections. Our novel hybrid hash grid encoding en-\nables easier camera pose optimization using a multiresolu-\ntion hash grid. Additionally, our choice of camera param-\neterization along with per-view importance weighting and\npatch-based alignment loss allows for a better image-to-\n3D alignment resulting in better reconstruction with high-\nfrequency details. Although SHINOBI is able to recover the\ngeometry of objects from any category, its performance is\nlimited on thin/transparent structures and fails to recover\nhigh-frequency details under extreme illumination changes,\nwhich we leave as exploration for future work.\n8\nAcknowledgements\nThis work has been partially funded by the Deutsche\nForschungsgemeinschaft (DFG, German Research Founda-\ntion) under Germany\u2019s Excellence Strategy \u2013 EXC number\n2064/1 \u2013 Project number 390727645 and SFB 1233, TP 02\n- Project number 276693517. It was supported by the Ger-\nman Federal Ministry of Education and Research (BMBF):\nT\u00a8ubingen AI Center, FKZ: 01IS18039A.\nReferences\n[1] Mart\u00b4\u0131n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo,\nZhifeng Chen, Craig Citro, Greg S. Corrado, and Andy Davis\net al. TensorFlow: Large-scale machine learning on heteroge-\nneous systems, 2015. Software available from tensorflow.org.\n6\n[2] Miika Aittala, Timo Aila, and Jaakko Lehtinen. Reflectance\nmodeling by neural texture synthesis. ACM TOG, 2018. 3\n[3] Rachel Albert, Dorian Yao Chan, Dan B. Goldman, and\nJames F. O\u2019Brian. Approximate svBRDF estimation from\nmobile phone video. Eurographics Symposium on Rendering,\n2018. 3\n[4] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P.\nSrinivasan, and Peter Hedman. Zip-NeRF: Anti-Aliased Grid-\nBased Neural Radiance Fields. ICCV, 2023. 2, 6, 13\n[5] Sai Bi, Zexiang Xu, Pratul Srinivasan, Ben Mildenhall,\nKalyan Sunkavalli, Milo\u02c7s Ha\u02c7san, Yannick Hold-Geoffroy,\nDavid Kriegman, and Ravi Ramamoorthi. Neural reflectance\nfields for appearance acquisition. arXiv, 2020. 2, 3\n[6] Sai Bi, Zexiang Xu, Kalyan Sunkavalli, Milo\u02c7s Ha\u02c7san, Yannick\nHold-Geoffroy, David Kriegman, and Ravi Ramamoorthi.\nDeep reflectance volumes: Relightable reconstructions from\nmulti-view photometric images. ECCV, 2020. 2\n[7] Sai Bi, Zexiang Xu, Kalyan Sunkavalli, David Kriegman,\nand Ravi Ramamoorthi. Deep 3d capture: Geometry and\nreflectance from sparse multi-view images. CVPR, 2020. 3\n[8] Wenjing Bian, Zirui Wang, Kejie Li, Jiawang Bian, and Vic-\ntor Adrian Prisacariu. Nope-nerf: Optimising neural radiance\nfield with no pose prior. CVPR, 2023. 2, 3\n[9] Mark Boss and Hendrik P.A. Lensch. Single image brdf\nparameter estimation with a conditional adversarial network.\narXiv, 2019. 3\n[10] Mark Boss, Fabian Groh, Sebastian Herholz, and Hendrik\nP. A. Lensch. Deep Dual Loss BRDF Parameter Estimation.\nWorkshop on Material Appearance Modeling, 2018. 3\n[11] Mark Boss, Varun Jampani, Kihwan Kim, Hendrik P.A.\nLensch, and Jan Kautz. Two-shot spatially-varying BRDF\nand shape estimation. CVPR, 2020. 3\n[12] Mark Boss, Raphael Braun, Varun Jampani, Jonathan T. Bar-\nron, Ce Liu, and Hendrik P.A. Lensch. NeRD: Neural re-\nflectance decomposition from image collections. ICCV, 2021.\n2, 3, 4, 13\n[13] Mark Boss, Varun Jampani, Raphael Braun, Ce Liu,\nJonathan T. Barron, and Hendrik P.A. Lensch. Neural-pil:\nNeural pre-integrated lighting for reflectance decomposition.\nNeurIPS, 2021. 2, 3, 4, 8, 12\n[14] Mark Boss, Andreas Engelhardt, Abhishek Kar, Yuanzhen Li,\nDeqing Sun, Jonathan T. Barron, Hendrik P.A. Lensch, and\nVarun Jampani. SAMURAI: Shape And Material from Un-\nconstrained Real-world Arbitrary Image collections. NeurIPS,\n2022. 2, 3, 4, 5, 6, 7, 12, 13, 16\n[15] Yue Chen, Xingyu Chen, Xuan Wang, Qi Zhang, Yu Guo,\nYing Shan, and Fei Wang. Local-to-global registration for\nbundle-adjusting neural radiance fields. CVPR, pages 8264\u2013\n8273, 2023. 3\n[16] Zhiqin Chen and Hao Zhang. Learning implicit fields for\ngenerative shape modeling. CVPR, 2019. 2\n[17] Weihao Cheng, Yan-Pei Cao, and Ying Shan. Id-pose: Sparse-\nview camera pose estimation by inverting diffusion models.\narXiv preprint arXiv:2306.17140, 2023. 15\n[18] Blender Online Community. Blender - a 3D modelling and\nrendering package. Blender Foundation, Stichting Blender\nFoundation, Amsterdam, 2018. 16\n[19] Robert L. Cook and Kenneth E. Torrance. A reflectance model\nfor computer graphics. ACM TOG, 1982. 3\n[20] Valentin Deschaintre, Miika Aitalla, Fredo Durand, George\nDrettakis, and Adrien Bousseau. Single-image SVBRDF\ncapture with a rendering-aware deep network. ACM TOG,\n2018. 3\n[21] Valentin Deschaintre, Miika Aitalla, Fredo Durand, George\nDrettakis, and Adrien Bousseau. Flexible SVBRDF capture\nwith a multi-image deep network. Eurographics Symposium\non Rendering, 2019. 3\n[22] Valentin Deschaintre, George Drettakis, and Adrien Bousseau.\nGuided fine-tuning for large-scale material transfer. Euro-\ngraphics Symposium on Rendering, 2020. 3\n[23] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. ICLR, 2021. 3\n[24] Charles Dugas, Yoshua Bengio, Franc\u00b8ois B\u00b4elisle, Claude\nNadeau, and Ren\u00b4e Garcia. Incorporating second-order func-\ntional knowledge for better option pricing. In NeurIPS. MIT\nPress, 2000. 12\n[25] Stefan Elfwing, Eiji Uchibe, and Kenji Doya.\nSigmoid-\nweighted linear units for neural network function approx-\nimation in reinforcement learning. Neural networks : the\nofficial journal of the International Neural Network Society,\n107:3\u201311, 2017. 12\n[26] Duan Gao, Xiao Li, Yue Dong, Pieter Peers, and Xin Tong.\nDeep inverse rendering for high-resolution SVBRDF estima-\ntion from an arbitrary number of images. ACM Transactions\non Graphics (SIGGRAPH), 2019. 3\n[27] Shubham Goel, Angjoo Kanazawa, and Jitendra Malik. Shape\nand viewpoint without keypoints. ECCV, 2020. 5\n[28] John C Gower and Garmt B Dijksterhuis. Procrustes prob-\nlems. OUP Oxford, 2004. 6\n[29] Richard Hahnloser, Rahul Sarpeshkar, Misha Mahowald, Rod-\nney Douglas, and H. Seung. Digital selection and analogue\namplification coexist in a cortex-inspired silicon circuit. Na-\nture, 405:947\u201351, 2000. 12\n9\n[30] Philipp Henzler, Valentin Deschaintre, Niloy J Mitra, and\nTobias Ritschel. Generative modelling of BRDF textures from\nflash images. ACM Transactions on Graphics (SIGGRAPH\nASIA), 2021. 3\n[31] Hwan Heo, Taekyung Kim, Jiyoung Lee, Jaewon Lee,\nSoohyun Kim, Hyunwoo J. Kim, and Jin-Hwa Kim. Robust\ncamera pose refinement for multi-resolution hash encoding.\nICML, 2023. 2, 4, 12\n[32] Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting NeRF\non a Diet: Semantically Consistent Few-Shot View Synthesis.\nICCV, 2021. 2\n[33] Varun Jampani, Kevis-Kokitsi Maninis, Andreas Engelhardt,\nArjun Karpur, Karen Truong, Kyle Sargent, Stefan Popov,\nAndre Araujo, Ricardo Martin-Brualla, Kaushal Patel, Daniel\nVlasic, Vittorio Ferrari, Ameesh Makadia, Ce Liu, Yuanzhen\nLi, and Howard Zhou. Navi: Category-agnostic image col-\nlections with high-quality 3d shape and pose annotations.\nNeurIPS, 2023. 2, 6, 7, 8, 12, 14, 15\n[34] Yoonwoo Jeong, Seokjun Ahn, Christopher Choy, Ani-\nmashree Anandkumar, Minsu Cho, and Jaesik Park. Self-\ncalibrating neural radiance fields. ICCV, 2021. 3\n[35] Berk Kaya, Suryansh Kumar, Carlos Oliveira, Vittorio Ferrari,\nand Luc Van Gool. Uncalibrated neural inverse rendering for\nphotometric stereo of general surfaces. ICCV, 2021. 3\n[36] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. arXiv, 2014. 6, 14\n[37] Zhengfei Kuang, Kyle Olszewski, Menglei Chai, Zeng Huang,\nPanos Achlioptas, and Sergey Tulyakov. NeROIC: Neural\nobject capture and rendering from online image collections.\narXiv, 2022. 2, 4, 6, 7, 12\n[38] Hendrik P. A. Lensch, Wolfgang Heidrich, and Hans-Peter\nSeidel. Automated texture registration and stitching for real\nworld models. Pacific Graphics, 2000. 2, 6\n[39] Axel Levy, Mark Matthews, Matan Sela, Gordon Wetzstein,\nand Dmitry Lagun. MELON: NeRF with Unposed Images\nUsing Equivalence Class Estimation. 13\n[40] Zhengqin Li, Kalyan Sunkavalli, and Manmohan Chandraker.\nMaterials for masses: SVBRDF acquisition with a single\nmobile phone image. ECCV, 2018. 3\n[41] Zhengqin Li, Mohammad Shafiei, Ravi Ramamoorthi, Kalyan\nSunkavalli, and Manmohan Chandraker. Inverse rendering\nfor complex indoor scenes: Shape, spatially-varying lighting\nand SVBRDF from a single image. CVPR, 2020. 3\n[42] Zhaoshuo Li, Thomas M\u00a8uller, Alex Evans, Russell H Taylor,\nMathias Unberath, Ming-Yu Liu, and Chen-Hsuan Lin. Neu-\nralangelo: High-fidelity neural surface reconstruction. CVPR,\n2023. 4, 5, 12\n[43] Ruofan Liang, Huiting Chen, Chunlin Li, Fan Chen, Selvaku-\nmar Panneer, and Nandita Vijaykumar. ENVIDR: Implicit\nDifferentiable Renderer with Neural Environment Lighting.\narXiv, 2023. 2, 3\n[44] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Simon\nLucey. BARF: Bundle-Adjusting Neural Radiance Fields.\nICCV, 2021. 3, 4, 5, 12\n[45] Sainan Liu, Shan Lin, Jingpei Lu, Shreya Saha, Alexey\nSupikov, and Michael Yip. BAA-NGP: Bundle-Adjusting\nAccelerated Neural Graphics Primitives. arXiv, 2023. 2, 4, 5,\n12\n[46] Xiaoxiao Long, Cheng Lin, Peng Wang, Taku Komura, and\nWenping Wang. Sparseneus: Fast generalizable neural surface\nreconstruction from sparse views. ECCV, 2022. 2\n[47] Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Saj-\njadi, Jonathan T. Barron, Alexey Dosovitskiy, and Daniel\nDuckworth. NeRF in the Wild: Neural Radiance Fields for\nUnconstrained Photo Collections. CVPR, 2021. 2\n[48] Quan Meng, Anpei Chen, Haimin Luo, Minye Wu, Hao Su,\nLan Xu, Xuming He, and Jingyi Yu. GNeRF: GAN-based\nNeural Radiance Field without Posed Camera. ICCV, 2021.\n6, 7\n[49] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-\nbastian Nowozin, and Andreas Geiger. Occupancy networks:\nLearning 3d reconstruction in function space. CVPR, 2019. 2\n[50] Ben\nMildenhall,\nPratul\nSrinivasan,\nMatthew\nTancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:\nRepresenting scenes as neural radiance fields for view synthe-\nsis. ECCV, 2020. 2, 3, 4, 8, 12\n[51] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexander\nKeller. Instant neural graphics primitives with a multiresolu-\ntion hash encoding. ACM TOG, 2022. 2, 3, 4, 6\n[52] Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao,\nWenzheng Chen, Alex Evans, Thomas Mueller, and Sanja\nFidler.\nExtracting Triangular 3D Models, Materials, and\nLighting From Images. CVPR, 2022. 2\n[53] Giljoo Nam, Diego Gutierrez, and Min H. Kim. Practical\nSVBRDF acquisition of 3d objects with unstructured flash\nphotography. ACM Transactions on Graphics (SIGGRAPH\nASIA), 2018. 2, 3\n[54] Michael Niemeyer, Jonathan T. Barron, Ben Mildenhall,\nMehdi S. M. Sajjadi, Andreas Geiger, and Noha Radwan.\nRegnerf: Regularizing neural radiance fields for view synthe-\nsis from sparse inputs. CVPR, 2022. 2\n[55] Michael Oechsle, Songyou Peng, and Andreas Geiger.\nUnisurf: Unifying neural implicit surfaces and radiance fields\nfor multi-view reconstruction. ICCV, 2021. 2, 13\n[56] Jeong Joon Park, Peter Florence, Julian Straub, Richard New-\ncombe, and Steven Lovegrove. Deepsdf: Learning continuous\nsigned distance functions for shape representation. CVPR,\n2019. 2\n[57] Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien\nBouaziz, Dan B Goldman, Steven M. Seitz, and Ricardo\nMartin-Brualla. Deformable neural radiance fields. ICCV,\n2021. 12\n[58] Keunhong Park, Philipp Henzler, Ben Mildenhall, Jonathan T.\nBarron, and Ricardo Martin-Brualla. Camp: Camera precon-\nditioning for neural radiance fields. ACM Trans. Graph., 2023.\n2, 4, 13\n[59] G. Ponimatkin, Y. Labbe, B. Russell, M. Aubry, and J. Sivic.\nFocal length and object pose estimation via render and com-\npare. In CVPR, 2022. 13\n[60] Daniel Rebain, Mark Matthews, Kwang Moo Yi, Dmitry La-\ngun, and Andrea Tagliasacchi. LOLNeRF: Learn from One\nLook. CVPR, 2022. 2\n[61] Shen Sang and Manmohan Chandraker. Single-shot neural\nrelighting and SVBRDF estimation. ECCV, 2020. 3\n10\n[62] Paul-Edouard Sarlin, Cesar Cadena, Roland Siegwart, and\nMarcin Dymczyk. From coarse to fine: Robust hierarchical\nlocalization at large scale. In CVPR, 2019. 15\n[63] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz,\nand Andrew Rabinovich. SuperGlue: Learning feature match-\ning with graph neural networks. In CVPR, 2020. 15\n[64] Johannes\nLutz\nSch\u00a8onberger\nand\nJan-Michael\nFrahm.\nStructure-from-motion revisited. CVPR, 2016. 2, 3, 15\n[65] Johannes Lutz Sch\u00a8onberger, Enliang Zheng, Marc Pollefeys,\nand Jan-Michael Frahm. Pixelwise view selection for unstruc-\ntured multi-view stereo. ECCV, 2016. 2, 3, 15\n[66] Soumyadip Sengupta, Jinwei Gu, Kihwan Kim, Guilin Liu,\nDavid W. Jacobs, and Jan Kautz. Neural inverse rendering of\nan indoor scene from a single image. ICCV, 2019. 3\n[67] S. Sinha, J. Y. Zhang, A. Tagliasacchi, I. Gilitschenski, and\nD. B. Lindell. Sparsepose: Sparse-view camera pose regres-\nsion and refinement. CVPR, 2023. 3\n[68] Pratul P. Srinivasan, Boyang Deng, Xiuming Zhang, Matthew\nTancik, Ben Mildenhall, and Jonathan T. Barron. NeRV:\nNeural reflectance and visibility fields for relighting and view\nsynthesis. CVPR, 2021. 2\n[69] Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara\nFridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra-\nmamoorthi, Jonathan T. Barron, and Ren Ng. Fourier features\nlet networks learn high frequency functions in low dimen-\nsional domains. NeurIPS, 2020. 2, 4, 12\n[70] Prune Truong, Marie-Julie Rakotosaona, Fabian Manhardt,\nand Federico Tombari. Sparf: Neural radiance fields from\nsparse and noisy poses. CVPR, 2023. 2\n[71] Itsuki Ueda, Yoshihiro Fukuhara, Hirokatsu Kataoka, Hi-\nroaki Aizawa, Hidehiko Shishido, and Itaru Kitahara. Neural\ndensity-distance fields. ECCV, 2022. 2\n[72] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler,\nJonathan T. Barron, and Pratul P. Srinivasan. Ref-neRF: Struc-\ntured view-dependent appearance for neural radiance fields.\nCVPR, 2022. 13\n[73] Jianyuan Wang, Christian Rupprecht, and David Novotny.\nPoseDiffusion: Solving pose estimation via diffusion-aided\nbundle adjustment. ICCV, 2023. 15\n[74] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\nKomura, and Wenping Wang. NeuS: Learning neural implicit\nsurfaces by volume rendering for multi-view reconstruction.\nNeurIPS, 2021. 2\n[75] Yiming Wang, Qin Han, Marc Habermann, Kostas Daniilidis,\nChristian Theobalt, and Lingjie Liu. Neus2: Fast learning of\nneural implicit surfaces for multi-view reconstruction. ICCV,\n2023. 2\n[76] Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, and Vic-\ntor Adrian Prisacariu. NeRF\u2212\u2212: Neural radiance fields with-\nout known camera parameters. arXiv, 2021. 3, 4\n[77] Jiamin Xu, Zihan Zhu, Hujun Bao, and Weiwei Xu. A Hy-\nbrid Mesh-neural Representation for 3D Transparent Object\nReconstruction. cvmj, 2022. 2\n[78] Jiawei Yang, Marco Pavone, and Yue Wang. FreeNeRF: Im-\nproving Few-shot Neural Rendering with Free Frequency\nRegularization. CVPR, 2023. 2\n[79] Yao Yao, Jingyang Zhang, Jingbo Liu, Yihang Qu, Tian Fang,\nDavid McKinnon, Yanghai Tsin, and Long Quan. NeILF:\nNeural Incident Light Field for Physically-based Material\nEstimation. ECCV, 2022. 2\n[80] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Vol-\nume rendering of neural implicit surfaces. NeurIPS, 2021.\n2\n[81] Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Polle-\nfeys, Zhaopeng Cui, and Guofeng Zhang. IntrinsicNeRF:\nLearning Intrinsic Neural Radiance Fields for Editable Novel\nView Synthesis. ICCV, 2023. 2, 3\n[82] Jianzhao Zhang, Guojun Chen, Yue Dong, Jian Shi, Bob\nZhang, and Enhua Wu. Deep inverse rendering for practical\nobject appearance scan with uncalibrated illumination. ACG,\n2020. 3\n[83] Jason Zhang, Gengshan Yang, Shubham Tulsiani, and Deva\nRamanan. NeRS: Neural reflectance surfaces for sparse-view\n3d reconstruction in the wild. NeurIPS, 2021. 2, 3, 4, 6, 7, 14\n[84] Jiahui Zhang, Fangneng Zhan, Rongliang Wu, Yingchen Yu,\nWenqing Zhang, Bai Song, Xiaoqin Zhang, and Shijian Lu.\nVMRF: View Matching Neural Radiance Fields. ACM MM,\n2022. 3\n[85] Jingyang Zhang, Yao Yao, Shiwei Li, Jingbo Liu, Tian Fang,\nDavid McKinnon, Yanghai Tsin, and Long Quan. Neilf++:\nInter-reflectable light fields for geometry and material estima-\ntion. ICCV, 2023. 3\n[86] Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, and\nNoah Snavely. PhySG: Inverse rendering with spherical Gaus-\nsians for physics-based material editing and relighting. CVPR,\n2021. 2, 3\n[87] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. CVPR, 2018. 6\n[88] Xiuming Zhang, Pratul P. Srinivasan, Boyang Deng, Paul\nDebevec, William T. Freeman, and Jonathan T. Barron. Ner-\nfactor: Neural factorization of shape and reflectance under an\nunknown illumination. ACM Trans. Graph., 40(6), 2021. 3\n[89] Yuanqing Zhang, Jiaming Sun, Xingyi He, Huan Fu, Rongfei\nJia, and Xiaowei Zhou. Modeling indirect illumination for\ninverse rendering. CVPR, 2022. 2, 3\n[90] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao\nLi. On the continuity of rotation representations in neural\nnetworks. CVPR, 2019. 4, 13\n[91] Hao Zhu, Fengyi Liu, Qi Zhang, Xun Cao, and Zhan Ma.\nRhino: Regularizing the hash-based implicit neural represen-\ntation. arXiv, 2023. 2, 4, 12\n11\nSHINOBI\nGT Novel View\nSAMURAI\nNeRS\nNeROIC\nSHINOBI\nGT Novel View\nSAMURAI\nNeRS\nNeROIC\nN/A\nSHINOBI\nGT Novel View\nSAMURAI\nNeRS\nNeROIC\nFigure 9. Novel view synthesis compared to existing methods. Additional example objects from NAVI [33] in-the-wild image collections.\nSHINOBI robustly reconstructs even when initialized with exteremely coarse poses while e.g. NeROIC [37] does not succeed on some\nscenes.\nOverview\nIn the supplement to SHINOBI, a method for 3D joint re-\nconstruction of shape, illumination and materials from in-\nthe-wild image sequences, we first present additional details\non the method\u2019s architecture (Sec.A.1) and the optimization\n(Sec. A.4). In Sec. B we introduce additional qualitative re-\nsults from object reconstructions of the NAVI dataset [33]\nand add visual examples to our ablation study. Finally, ap-\nplications of our reconstructed data are shown in Sec. B.5.\nPlease also visit our project page for an overview of this\nwork and further visual results video.\nA. Additional Method Details\nA.1. Architecture\nHybrid hash encoding configuration. The hybrid encoding\nfeatures two branches. For the base encoding we use 10\nrandom offset annealed Fourier frequencies for the positional\nencoding followed by a small MLP featuring a single hidden\nlayer with 64 dimensions and silu activation [25]. The output\nequals the input dimension (3), again as it is done by Zhu et\nal. [91]. We apply BARF\u2019s [44] Fourier annealing and add\nrandom frequencies as offsets to the logarithmically spaced\nfrequencies [14, 69] to prevent artifacts from axis-aligned\nfrequencies. The multiresolution hash grid is configured with\n16 levels with a base resolution of 8 and a maximum target\nresolution of 2048. The embedding dimensions are 2 or 4.\nThe experiments reported in Sec. 4 of our paper are generated\nusing 2 dimensions. A slightly better decomposition quality\ncan be achieved by increasing the dimensionality at the cost\nof increased memory consumption and runtime. Hence, the\nfinal feature dimension after encoding and concatenation is\n35 or 67. See Sec. A.1 for an explanation of the annealing\nstrategy applied to the hash grids.\nNetworks. The main network taking in the encoded features\nconsists of 3 ReLU [29] activated layers with 64 channels.\nAn additional linear layer generates the output for the \u03c3\ndensity parameter from the 64 channel activation. Softplus\nsoftplus(x) = ln(1 + ex) [24] is applied to the raw \u03c3. The\ndirections are encoded using 4 non-annealed regular Fourier\ncomponents as in Mildenhall et al. [50] and then, concate-\nnated with the main network output, fed to a secondary MLP\nto predict the view direction-dependent radiance \u02dcc used in\nthe beginning of the optimization. The secondary conditional\nnetwork has a hidden dimension of 32 in our case. For the\nBRDF prediction a single linear layer compresses the main\nnetwork output to 16 channels. From there the BRDF de-\ncoder is applied which consists of another two layers with\n64 channels and ReLU activation each. Each BRDF output;\nbasecolor, metallic and roughness has its own output layer\nfollowed by a sigmoid activation [13]. An additional diffuse\nembedding is added as conditioning to the basecolor branch\nbefore output. The illumination network decoding the per\nview latent vector is conditioned by the same configuration\nof mapping layers as outlined in Neural-PIL [13].\nMultiresolution hash grid level annealing. Inspired by\nBARF [44] and Nerfies [57] we apply a coarse-to-fine an-\nnealing to the hash grid encoding by weighting the different\ngrid levels. Starting with only the features from the low\nresolution dense grid and all other features set to zero we\nincrease the weights of the higher resolution levels gradually\nover time (cf. [42, 45]). Similar to the implementation by\nLin et al. we formulate it as a truncated Hann window:\n\u0393k(x; \u03b1) = wk(\u03b1)\n\u0002\nsin(2kx), cos(2kx)\n\u0003\n(5)\nwk(\u03b1) = 1 \u2212 cos (\u03c0 clamp(\u03b1 \u2212 k, 0, 1))\n2\n(6)\nwhere \u03b1 \u2208 [0, L] with L being the number of resolution\nlevels of the hash grid encoding.\nWe also tested the idea of BAA-NGP [45] replicating em-\nbeddings from low-resolution levels but observed reduced\nperformance in our optimization setting. Similarly, we had\nno success with adding a straight-through operator to the\ninterpolation on the hash grid as proposed in [31].\nA.2. Camera Parameterization.\nWe label initial poses based on 3 simple binary questions:\nLeft vs. Right, Above vs. Below, and Front vs. Back. This\nonly takes about 4-5 minutes for a typical 80 image col-\nlection. Alternatively, our framework allows to extend the\ninitialization to a camera multiplex spanning more than one\nquadrant. This can enable fully random initialization for\nfront-facing scenes and image sets featuring rotating cam-\n12\neras with a fixed object distance as shown by Levy et al. [39].\nAs these constrained settings are uncommon for in-the-wild\ncollections we discard it here. We use a perspective pinhole\ncamera model and an initial field of view of 53.13 degrees.\nWe optimize offsets to the original camera parameters of our\n\u2018lookat + direction\u2019 parameterization as outlined in the main\npaper. Here, we encode the trainable lookat parameter \u2206d\ndirectly as two direction components, \u03d5, \u03b8, which are used\nto offset the viewing direction d to obtain the updated \u02c6d as\nfollows:\nd = (peye + \u2206peye) \u2212 pcenter\n(7)\n\u03b8 = arcsin(dy) + \u2206d\u03b8\n(8)\n\u03d5 = arctan2(dx, dz) + \u2206d\u03d5\n(9)\n\u02c6d = \u27e8cos \u03d5 sin \u03b8, sin \u03d5, cos \u03d5 cos \u03b8\u27e9\n(10)\nWe limit \u2206d to the range [\u22120.5\u03c0, 0.5\u03c0].\nWe also tried other camera parameterizations like the pop-\nular 6D rotation representation by Zhou et al. [90] or Fo-\ncalPose [59] that has recently been applied to NeRF with\ncamera fine-tuning [58]. Interestingly, our lookat + direction\nparameterization performs the best in our setting as it seems\nto work well with the regularizations on camera poses.\nA.3. Regularization and Losses.\nMultiresolution hash grid regularization. To regularize\nthe hash grid encoding we use the following normalized\nweight decay as proposed by Barron et al. [4]: LGrid =\nP\nl mean(Vl) with Vl referring to the grid embeddings at\nresolution level l. Computing the sum of the mean per-level\nputs a higher penalty on coarser grid levels compared to\nnaive weight decay over all parameters at once. We find a\nweighting of 0.02 to 0.05 work well in our setting and settle\nfor 0.02 as the final value. We apply gradient scaling to the\ngradients for the network by the norm of 0.1. Furthermore,\ngradient norm clipping with a clip value of 2.5 is applied to\nthe camera gradients before the parameter update.\nSurface normals regularization. We use the normal direc-\ntion loss Lndir from [72] to constrain the normals to face\nthe camera until the ray reaches the surface. This helps in\nproviding sharper surfaces without floater artifacts. Addi-\ntionally, we observe that the explicit rendering step helps to\nconstrain the surface normals as noise is reduced compared\nto optimization using only the predicted radiance.\nCamera regularization. The camera regularization losses\nfrom SAMURAI are kept, particularly one to force the\nlookat-direction to point towards the origin (LLookat) and\none to prevent the cameras from moving too far away from\nthe bounding volume (LBounds) [14]. An additional term on\nthe magnitude of the camera offset parameters helps to keep\ncameras from moving too far too fast with respect to the\ninitial position due to strong updates in the beginning of the\noptimization.\nBRDF losses. Joint estimation of BRDF and illumination is\na delicate endeavor. For example, the illumination can easily\nfall into a local minimum. The object is then tinted in a bluish\ncolor, and the illumination is an orange color to express a\nmore neutral color tone, for example. As our image collec-\ntions have multiple illuminations, we can force the base color\nbc to replicate the pixel color from the input images. This\nway, a mean color over the dataset is learned and it becomes\nless likely to be trapped in local minima. We evaluate the\nMean Squared Error (MSE) for this: LInit = LMSE(Cs, bc).\nAdditionally, we add a smoothness loss LSmooth for the nor-\nmal, roughness, and metallic parameters similar to the one\nused in UNISURF [55] to further regularize BRDF estima-\ntion [14].\nImage\nreconstruction\nloss\nis\na\nCharbonnier\nloss:\nLImage(g, p) =\np\n(g \u2212 p)2 + 0.0012 between the input\ncolor from C for pixel s and the corresponding predicted\ncolor of the networks \u02dcc. We also calculate the loss with the\nrendered color \u02c6c which becomes the main loss over time.\nThis loss is computed over multiple resolution levels as\noutlined in Sec. 3 of the main paper whenever patches are\nrendered.\nMask losses. In total we use three mask loss terms. The\nLsilhouette as described in Sec 3.2 as well as the binary cross-\nentropy loss LBCE between the volume-rendered mask and\nestimated foreground object mask and the background loss\nLBackground from NeRD [12]. The latter enforces all rays\ncast to the background to return 0. Consequently, the total\nmask loss is defined as: LMask = \u03bbxorLsilhouette + LBCE +\nLBackground where \u03bbxor is set to 50 and Lsilhouette is nor-\nmalized by the number of elements in the reference mask.\nFinal loss ensemble. Overall we compute two loss terms\nLNetwork and LCamera which consist of differently weighted\nversions of the photometric rendering loss and alignment\nlosses plus the respective regularizations. The loss to\noptimize the decomposition network can be written as\nLNetwork = \u03bbbLImage(Cs, \u02dcc) + (1 \u2212 \u03bbb)LImage(Cs, \u02c6c) +\nLMask + \u03bbaLInit + \u03bbndirLndir + \u03bbSmoothLSmooth +\n\u03bbDecSmoothLDecSmooth+\u03bbDecSparsityLDecSparsity. Here, \u03bbb\nand \u03bba are the optimization scheduling weights described\nbelow in more detail. As long as the camera multiplex has\nsize m > 1 the camera multiplex consistency loss is added\nas follows: LNetwork = LNetwork+0.1(Lmultiplex). To these\nlosses the camera posterior scaling is applied as in SAMU-\nRAI [14]. The camera loss is weighted according to our view\nimportance scaling instead. Badly initialized camera poses\ncan still recover over the training duration as they get poten-\ntially large updates while cameras that perform well in terms\nof the losses are gradually faded out of the optimization.\nAdditionally, the regularizations from above, LBounds and\nLLookat are added.\n13\n0\n1 0k\n20k\n30k\n40k\n50k\n60k\n0\n0.2\n0.4\n0.6\n0.8\n1\nimage  re solution factor\nOptimization Steps\nScaler Value\nAnnealing End\nFocal Length Start\nView Importance Start \nFigure 10. Optimization schedule. We use three \u03bb parameter to\nscale losses to enable a smooth flow of the optimization parameters.\nAdditionally, we indicate at which points in time the view impor-\ntance weighting is introduced, the focal length parameters start to\nget updated and the encoding annealing ends.\nA.4. Optimization\nOptimization scheduling. We use three fading \u03bb variables\nto steer the optimization schedule smoothly as visualized in\nFig. 10. Render resolution is continuously increased over\nthe first half of the optimization while the number of active\nmultiplex cameras is reduced. This is controlled by \u03bbc. Input\nimage resolution is increased from 100 pixels to a resolution\nof 400 pixels on the longer image side over the first half of the\ntraining. For higher final output resolutions an even larger\ndownsample factor (> 4) might be needed. This strategy\nallows the image patches to include even larger structures\nof the objects and improves camera alignment. The direct\ncolor optimization is faded to the BRDF optimization and\nthe encoding annealing is performed over the first third of\nthe optimization. \u03bbb is used for the BRDF transition and\nan independent \u03b1 value is kept for the annealing. Finally,\n\u03bba is used to scale some losses in a non-linear way. Focal\nlength updates are delayed until a quarter of the optimization\ntime. We start with the view importance weighting at the\nhalf-way point of the annealing schedule. SHINOBI renders\nimage patches for most of the training time which adds more\ncontext to each update step, allowing us to add new losses\ntailored to camera alignment. The first 1000 steps are trained\nusing regular random ray sampling, though, to help initialize\na global shape quickly while both the render resolution as\nwell as the hash grid resolution are low.\nOptimizer settings. The ADAM [36] optimizer updates the\nnetwork weights based on LNetwork with a learning rate of\n1e-3 that is exponentially decayed by an order of magnitude\nover the training time. The same decay rate is applied to the\noptimizer concerned with the hash grid embeddings. The\ngradient are computed based on LNetwork with the hash grid\nspecific regularization LGrid added. The learning rate of\nthe camera optimizer is exponentially decayed by an order\nof magnitude every 40k steps. As mentioned before the \u03b21\nparameter is set to 0.2 for the camera optimizer to stabilize\nthe training in the presence of noisy gradients. It uses the\ngradients computed based on LCamera. The framework is\ntrained using float16 mixed precision. The coordinate input\nto the encoding is 32 bit as is the rendering and illumination\nevaluation. The other MLPs and specifically the interpolation\non the hash grids run at 16 bit precision, though.\nB. Additional Experiments\nB.1. Details on Compared Methods.\nIn addition to SAMURAI which has been introduced in\nSec. 3 of the main paper we compare against two more\nrecent methods for in-the-wild object reconstruction.\nNeRS stands for Neural Reflectance Surfaces [83] that con-\nstrain reconstructions using a mesh-based representation.\nStarting from manually annotated rough initial poses and\na template mesh the objects are decomposed into a surface\nmesh, illumination and surface reflectivity parameterized as\nalbedo and shininess. We define the dimensions of an initial\ncuboid that approximates the object\u2019s bounding box for each\nscene in line with [33, 83].\nNeROIC presents a multi-stage approach to reconstruct ge-\nometry and material properties of objects from online image\ncollections. Camera poses are initialized with a COLMAP-\nbased pipeline and fine-tuned during the first reconstruction\nstage. Following high-quality surface normals are estimated\nduring the second stage. Finally, material properties and il-\nlumination are optimized to enable relighting in addition to\nnovel view synthesis.\nB.2. Additional Visual Results\nFig. 9 shows additional qualitative results on objects from\nthe NAVI dataset compared to the baseline methods. Note,\nthat the methods work at different image resolutions and that\nwe show the original output. NeROIC is able to reconstruct\nhigh-frequency detail for scenes that have good initial poses\nbut shows artifacts or fails on others. NeRS suffers from\nits low resolution mesh representation and often inaccurate\ncamera alignment. SAMURAI and SHINOBI both reproduce\nappearance that is closer to the original illumination setting\ndue to superior decomposition capabilities while SHINOBI\nrecovers more high-frequency details.\nB.3. Qualitative Results of Ablations\nFig. 11 shows qualitative results corresponding to the nu-\nmerical results from the ablation study reported in the main\npaper. It can be observed that a robust reconstruction is only\npossible using the full configuration of our method. While\n14\nw/o Multiplex Consistency Loss\nw/o Per View Importance\nw/o Coarse-to-fine\nFull\nw/o Hybrid Encoding\nw/o Patch-based Training\nFigure 11. Qualitative ablation study. We show view synthesis\nresults from novel view synthesis on the \u2018School Bus\u2019 scene from\nNAVI where we ablate components of our method. The visual\nresults underline the importance of each part.\nthe multiplex consistency loss has only minimal impact on\nthis example the result still shows some visible artifacts and\noverall increased noise level.\nMethod\nTranslation\u2193\nRotation \u00b0\u2193\nSC\n\u223c SC\nSC\n\u223c SC\nPoseDiffusion [73] 0.51\u00b1 0.09 0.43\u00b1 0.11\n41.33\u00b1 15.15\n43.50\u00b1 13.67\nHLoc [62, 63]\n0.07\u00b1 0.13 0.06\u00b1 0.10\n9.10\u00b1 18.75\n9.72\u00b1 20.08\nSHINOBI\n0.250\u00b1 0.0850.28\u00b1 0.09\n22.84\u00b1 16.19\n33.00\u00b1 19.97\nTable 4. Pose estimation on in-the-wild data.. Evaluation of abso-\nlute rotation and translation errors after alignment on the NAVI [33]\nin-the-wild scenes. We compare SHINOBI against specialized cam-\nera pose estimation solutions. Note, that HLoc [62] fails completely\non 5 scenes and is only able to recover 55% of views on average.\nB.4. Comparison to other Camera Pose Estimation\nMethods\nTab. 4 compares methods for camera pose estimation on the\nNAVI in-the-wild scenes [33]. Traditional SfM methods like\nCOLMAP [64, 65] paired with a neural feature detection and\n(a) Reconstructed assets under novel illumination\n(b) Edited materials\nFigure 12. Integration and editing. Although objects are initially\ncaptured under diverse illumination settings we can integrate multi-\nple objects consistently into a scene in the end. BRDF parameters\ncan be modified independently from the illumination.\nmatching can recover poses with great accuracy but only suc-\nceed on a subset of scenes and images. PoseDiffusion [73]\nand ID-Pose [17], both fully neural models trained on large\ndatasets, struggle on these out-of-distribution examples. We\nonly report a full evaluation on PoseDiffusion as an exam-\nple here. We observe that these models take important pose\ncues also from the background of object-centric image sets.\nThis leads to poor results on in-the-wild image collections.\nA simple fine-tuning on masked images did not improve\nperformance. In our experiments, camera pose estimation\nusually regresses to a front-facing camera layout for in-the-\nwild examples featuring different illumination and object\nscales. Consequently, our approach appears to be a good\ntrade-off in-terms of camera pose quality.\nB.5. Downstream Applications\nThe object decomposition into BRDF, illumination and shape\nenables us to edit illumination and material independently\nof the shape representation to re-light the object, for exam-\nple. Furthermore, we can convert our neural representation\ninto a parametric model like a mesh and physically based\nmaterial suitable for easy integration into standard graph-\nics pipelines. Mesh extraction and asset generation. We\n15\nFigure 13. Relighting application. View synthesis under three\ndifferent illumination settings using the estimated decomposition\nfor a sample view from the \u201cTractor\u201d scene.\nuse a modified version of the mesh extraction component\nfrom SAMURAI [14] to extract triangle meshes from the\nlearnt volume and the corresponding material parameters.\nMarching cubes is used to create an initial mesh. We post-\nprocess the mesh and perform automatic UV unwrapping\nusing Blender [18]. Finally, textures are extracted by query-\ning our pipeline for the BRDF around the baked surface\nlocations. The extraction of a mesh takes around 3 minutes.\nRelighting and material editing. Our reconstructed assets\ncan then be easily integrated into existing graphics pipelines.\nIn Fig. 12 we show a SHINOBI themed scene featuring\nobjects from the NAVI dataset in a new consistent illumi-\nnation environment as it would be required for AR and VR\napplications. We can also modify the BRDF parameters inde-\npendently of the lighting. Fig. 13 compares renderings of the\nsame camera view but lit with different environment lights.\nPlease also consider watching the supplementary video in-\ncluding more examples for the given applications.\n16\n"
  },
  {
    "title": "CustomVideo: Customizing Text-to-Video Generation with Multiple Subjects",
    "link": "https://arxiv.org/pdf/2401.09962.pdf",
    "upvote": "6",
    "text": "CustomVideo: Customizing Text-to-Video Generation with Multiple Subjects\nZhao Wang1, Aoxue Li2\u2217, Enze Xie2, Lingting Zhu3, Yong Guo2, Qi Dou1, Zhenguo Li2\n1The Chinese University of Hong Kong, 2Huawei Noah Ark\u2019s Lab, 3The University of Hong Kong\n{zwang21@cse., qidou}@cuhk.edu.hk, lax@pku.edu.cn, {xie.enze, li.zhenguo}@huawei.com,\nltzhu99@connect.hku.hk, guoyongcs@gmail.com\nhttps://kyfafyd.wang/projects/customvideo\na cat* and a dog* walking in the Times Square\na man* wearing a sunglasses*, giving a speech at square, 4K, high resolution\na toy bear* riding a bike* on the road, with mountains faraway in the background\na front view of a woodenpot* with a flower* in it on beach with a view of seashore\ncat*\ndog*\nman*\nsunglasses*\ntoy bear*\nbike*\nflower*\nwoodenpot*\nSubjects\nGenerated Videos\nFigure 1: Customized text-to-video generation results of our proposed CustomVideo with given multiple subjects (left) and text prompts\n(below). Our approach can disentangle highly similar subjects, e.g., cat v.s. dog, preserving the fidelity of subjects and smooth motions.\nAbstract\nCustomized text-to-video generation aims to gener-\nate high-quality videos guided by text prompts and\nsubject references.\nCurrent approaches designed\nfor single subjects suffer from tackling multiple\nsubjects, which is a more challenging and practical\nscenario. In this work, we aim to promote multi-\nsubject guided text-to-video customization.\nWe\npropose CustomVideo, a novel framework that can\ngenerate identity-preserving videos with the guid-\nance of multiple subjects. To be specific, firstly,\nwe encourage the co-occurrence of multiple sub-\njects via composing them in a single image. Fur-\n*Corresponding author.\nther, upon a basic text-to-video diffusion model,\nwe design a simple yet effective attention control\nstrategy to disentangle different subjects in the la-\ntent space of diffusion model. Moreover, to help\nthe model focus on the specific object area, we seg-\nment the object from given reference images and\nprovide a corresponding object mask for attention\nlearning. Also, we collect a multi-subject text-to-\nvideo generation dataset as a comprehensive bench-\nmark, with 69 individual subjects and 57 meaning-\nful pairs. Extensive qualitative, quantitative, and\nuser study results demonstrate the superiority of our\nmethod, compared with the previous state-of-the-\nart approaches.\narXiv:2401.09962v1  [cs.CV]  18 Jan 2024\n1\nIntroduction\nText-to-video (T2V) generation [Wang et al., 2023a; Zeng et\nal., 2023; Blattmann et al., 2023a; Kondratyuk et al., 2023]\nhas achieved fantastic progress taking advantages of diffu-\nsion models [Ho et al., 2020; Song et al., 2020; Lu et al.,\n2022]. Recently, artists have dreamed of generating videos\nwith their own belongings, e.g., pets, which directs a new\nresearch direction named customized T2V generation. Al-\nthough existing methods [Zhao et al., 2023; Wei et al., 2023;\nJiang et al., 2023] have been proposed to generate videos\nfrom a single object, tackling multiple objects still remains\na difficult scenario. The key challenge is to ensure the co-\noccurrence of multiple objects in the generated video and re-\ntain their corresponding identities.\nA recent work, VideoDreamer [Chen et al., 2023b],\nproposes disen-mix finetuning and human-in-the-loop re-\nfinetuning strategies based on Stable Diffusion [Rombach\net al., 2022], aiming to generate videos from multiple sub-\njects. However, VideoDreamer falls short in guaranteeing the\nco-occurrence of multiple subjects and disentangling similar\nsubjects due to its inconsistent object mixing strategy. As\nshown in Figure 3, the cat can not be consistently generated\nacross all frames during inference. Moreover, VideoDreamer\ncan not tackle a scenario with a foreground object with a\nbackground scene, such as car and barn (see Figure 3).\nIn contrast, in our approach, we ensure the co-occurrence\nof multiple objects during model training, which encourages\nthe model to capture the presence of different subjects si-\nmultaneously, thereby facilitating co-occurrence during infer-\nence. Additionally, we propose an attention control mecha-\nnism to disentangle multiple subjects during training, effec-\ntively guiding the model to focus on the corresponding sub-\nject area while disregarding irrelevant parts of the image. To\nfacilitate this process, we incorporate a ground truth object\nmask, obtained through segmentation either from a model\nlike SAM [Kirillov et al., 2023] or provided by human an-\nnotators, as supervision during optimization. Our attention\nmechanism consists of two key designs that contribute to the\ndisentanglement of subjects. Firstly, we highlight the corre-\nsponding subject area using the ground truth object mask on\nthe cross-attention map, aligning learnable text tokens with\nsubject identity. Secondly, we optimize the cross-attention\nmap towards a slight negative value, excluding the desired\nsubject and mitigating the influence of irrelevant areas in the\ninput image. To comprehensively evaluate our proposed ap-\nproach, we have curated a diverse dataset covering a wide\nrange of categories including pet, person, toy, transportation,\nscene, plant, wearable item, and so on. Beyond the Multi-\nStudioBench [Chen et al., 2023b] which includes 12 subject\npairs, our dataset CustomStudio comprises 69 individual sub-\njects and 57 subject pairs, notably featuring some challeng-\ning scenarios involving visually similar objects. Through ex-\ntensive experiments on this benchmark dataset, we provide\nqualitative, quantitative, and user study results that demon-\nstrate the superiority of our method in generating high-quality\nvideos with customized subjects. In summary, our contribu-\ntions are as follows:\n\u2022 We propose CustomVideo, a novel multi-subject driven\nT2V generation framework, powered by a simple yet ef-\nfective co-occurrence and attention control mechanism.\n\u2022 We collect a multi-subject T2V dataset and build it as\na comprehensive benchmark.\nWe highlight that our\nbenchmark covers a wide range of subject categories and\ndiverse subject pairs over them.\n\u2022 Our method consistently outperforms previous state-of-\nthe-art approaches in terms of diverse metrics, includ-\ning Textual Alignment, Image Aligenment, and Temporal\nConsistency. More critically, in the user study, we ob-\ntain significant improvement with the scores 3\u00d7 higher\nthan these methods.\n2\nRelated Work\n2.1\nText-to-Video Generation\nText-to-video generation has made significant advancements\nin recent years [Chen et al., 2023a; Duan et al., 2023; Wang et\nal., 2023a]. Early approaches employed GANs [Balaji et al.,\n2019; Skorokhodov et al., 2022] and VQVAE [Hong et al.,\n2022; Villegas et al., 2023], while more recent works have\nexplored diffusion models to generate high-quality videos\n[He et al., 2022; Wang et al., 2023c; Zhang et al., 2023a;\nZhang et al., 2023b]. Make-A-Video [Singer et al., 2023]\nutilizes a pre-trained image diffusion model with additional\ntemporal attention finetuning. VideoLDM [Blattmann et al.,\n2023b] introduces a multi-stage alignment approach in the\nlatent space to generate high-resolution and temporally con-\nsistent videos.\nOther methods [Khachatryan et al., 2023;\nGuo et al., 2023] generate videos with an image as the first\nframe and randomly initialized subsequent frames. To en-\nhance controllability, VideoComposer [Wang et al., 2023b]\nincorporates additional guidance signals, such as depth maps,\nto produce desired videos alongside text inputs.\nTune-A-\nVideo [Wu et al., 2023] proposes a temporal self-attention\nmodule that fine-tunes a pre-trained image diffusion model,\nachieving successful generation of videos with specific text\nguidance. Furthermore, diffusion-based video-to-video edit-\ning approaches [Qi et al., 2023; Geyer et al., 2023] have also\nbeen proposed for practical usages.\n2.2\nSubject-driven Customization\nThere has been a growing interest in customizing pre-trained\nimage and video diffusion models for personalized genera-\ntion. Customization involves generating images and videos\nwith specific subjects, typically based on a few reference im-\nages. For image diffusion customization, Textual Inversion\n[Gal et al., 2023] represents a specific object as a learnable\ntext token using only a few reference images. This learned\ntext token can then be integrated into a sentence to generate\npersonalized images during the inference stage. Addition-\nally, DreamBooth [Ruiz et al., 2023] fine-tunes the weights\nof the diffusion model to improve fidelity in image genera-\ntion. Several works [Kumari et al., 2023; Liu et al., 2023a;\nLiu et al., 2023b] have explored personalized image diffu-\nsion with multiple subjects, focusing on parameter-efficient\nfinetuning and text embedding learning. While there have\nbeen initial attempts to customize video diffusion, such as\nSegmentor\nSelf Attn.\nCross Attn.\nSelf Attn.\nCross Attn.\nSelf Attn.\nCross Attn.\nSelf Attn.\nCross Attn.\nSelf Attn.\nCross Attn.\n\u2026\n\u2026\nCross Attention Map !\na <new1> cat and a <new2>dog\nKV\nQ\na\n<new1>\nand\ncat\na\ndog\n<new2>\nWord Tokens\nSubject \nMasks \"\n--\n-\n- - -\n-\n-\n-\nAug.\nConcat\nTrainable\nFrozen\nUNet !\na <new1> cat and a \n<new2> dog walking \nin the Times Square\nSelf Attn.\nCross Attn.\nSelf Attn.\nCross Attn.\n\u2026\nUNet !\nTraining Stage\nInference Stage\nText\nEncoder\nVideo\nOutput\n+\n+ +\n++ +\nAugmented Samples\nIndividual Subjects\nFigure 2: The overview of our proposed CustomVideo. We propose a simple yet effective co-occurrence and attention control mechanism\nwith mask guidance to preserve the the fidelity of subjects for multi-subject driven text-to-video generation. During the training stage, only\nthe key and value weights in the cross attention layers are fine-tuned. In the inference stage, given a text prompt integrated with learned text\ntoken, we can easily obtain high-quality videos with specific subjects.\nVideoAssembler [Zhao et al., 2023] and VideoBooth [Jiang\net al., 2023], which use reference images to personalize the\nvideo diffusion model while preserving subject identity, and\nDreamVideo [Wei et al., 2023], which decouples the learning\nprocess for subject and motion customization, these methods\nare designed for single objects and cannot handle multiple\nsubjects when given. A recent work, VideoDreamer [Chen et\nal., 2023b], proposes multi-subject driven video customiza-\ntion through disen-mix finetuning strategy with LoRA [Hu et\nal., 2022]. However, the generated videos do not guarantee\nthe co-occurrence of multiple subjects, or disentanglement of\ndifferent subjects. In this work, we propose a simple yet ef-\nfective co-occurrence and attention mechanism that disentan-\ngles multiple subjects using masks as guidance while preserv-\ning the co-occurrence of subjects in the generated videos.\n3\nMethod\n3.1\nPreliminary: Text-to-Video\nVideo diffusion models (VDMs) [Wang et al., 2023a; Duan\net al., 2023; Chen et al., 2023a] generate videos by gradually\ndenoising a randomly sampled Gaussian noise \u03f5, following an\niterative denoising process that resembles a reverse procedure\nof a fixed-length Markov Chain. This iterative denoising al-\nlows VDMs to capture the temporal dependencies presented\nin video data. To be specific, a video diffusion model \u03b8 pre-\ndicts the added noise at each timestep t given a text condition\nc, where t \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , T}. Thus, the training objective for\nthis process can be expressed as a reconstruction loss:\nLrecon = E\u03f5,z,c,t\nh\n\u2225\u03f5 \u2212 \u03f5\u03b8 (zt, T (c), t)\u22252\n2\ni\n,\n(1)\nwhere z \u2208 RB\u00d7L\u00d7H\u00d7W \u00d7D is the latent code of the input\nvideo with batch size B, video length L, height H, width\nW, and latent dimension D. \u03f5\u03b8 is the noise prediction from\nthe model. T is a pre-trained text encoder. zt is obtained\nby adding noise to the ground truth z0 with zt = \u03b1tz0 +\np\n1 \u2212 \u03b12\nt \u03f5, where \u03b1t is a diffusion hyperparameter. In this\nwork, we utilize zeroscope1 T2V model as our base model,\nwhich is built upon a 3D UNet, with spatial and temporal\nmodeling for generating high-quality videos.\n3.2\nCustomVideo with Multiple Subjects\nFigure 2 provides an overview of our proposed CustomVideo\nframework, which enables multi-subject driven T2V cus-\ntomization.\nDuring the training stage, we employ a con-\ncatenation technique to ensure the co-occurrence of multi-\nple subjects. Specifically, we combine these subjects into a\nsingle image, facilitating the learning of multi-subject pat-\nterns by the model. To address the challenge of entanglement\namong highly similar subjects, we propose an attention con-\ntrol mechanism. This mechanism ensures that the learnable\nword tokens align with their corresponding regions on the\ncross-attention map. By achieving this alignment, we enable\nthe model to disentangle different subjects and enhance the\nquality of the generated videos. The training process focuses\non training the subject-related learnable word tokens, as well\nas the key and value weights within the cross-attention lay-\ners of the UNet architecture, adopting a parameter-efficient\nfinetuning approach. During the inference stage, users only\nneed to provide a text prompt description integrated with the\ncorresponding learned word tokens to generate high-quality\nvideos aligned with their preferences. In the following, we\nwill delve into the detailed workings of CustomVideo.\nCo-occurrence Control.\nIn the context of multi-subject\ndriven T2V generation, ensuring that the model consistently\n1https://huggingface.co/cerspense/zeroscope v2 576w\nlearns to generate videos with multiple subjects is crucial. In\na previous work, VideoDreamer [Chen et al., 2023b], the au-\nthors proposed a disen-mix strategy for multi-subject driven\ngeneration by fine-tuning the model using both single subject\nimages and concatenated images of multiple subjects. How-\never, we have identified that this mixing strategy can confuse\nthe model due to the inconsistent number of subjects present\nin a single image. Consequently, it leads to unstable gener-\nation of videos with multiple subjects (see line \u2018both single\nand concat\u2019 in Figure 5). In our approach, we have found\nthat fine-tuning the model using only concatenated images\nof multiple subjects is sufficient to ensure the co-occurrence\nof multiple subjects in the generated videos. Additionally,\nwe have observed that providing clear subjects without back-\nground aids the model in learning the specific characteristics\nof the subjects more effectively. To achieve this, we perform\nbackground removal on the subject images. This can be ac-\ncomplished manually or by employing an automatic tool such\nas the SAM model [Kirillov et al., 2023].\nAttention Control.\nEnsuring the co-occurrence of multiple\nsubjects in generated videos is achieved through image con-\ncatenation. However, a more challenging task is preserving\nthe distinct characteristics of each subject without interfer-\nence. Simply fine-tuning the model with concatenated im-\nages can lead to confusion among subject characteristics. As\nshown in Figure 5 (line \u2018w/o pos. attn.\u2019), the generated cat\npredominantly resembles the shape of the provided dog sam-\nple, despite having a similar color texture to the provided\ncat. Thus, disentangling the multiple subjects becomes cru-\ncial to generate high-quality videos that faithfully represent\neach subject\u2019s characteristics.\nIn our approach, we utilize a learnable text token to rep-\nresent each specific subject, which operates in the cross-\nattention layer of the diffusion model. To effectively regu-\nlate the subject learning process, we can directly leverage the\ncross-attention map. As illustrated in Figure 2, we employ an\nautomatic segmentor to obtain the ground truth mask Mp for\neach subject, which indicates the spatial position of the sub-\njects. During the training stage, we extract the cross-attention\nmap A from the cross-attention layer using the activations of\neach word in the given text prompt. For each learnable word\ntoken, such as \u2018<new1>\u2019, we enhance the corresponding area\non the cross-attention map A by encouraging the alignment\nbetween the subject area and the ground truth subject mask\nwith a loss function as the following:\nLattn = 1\nB\nB\nX\ni=1\n1\nN\nN\nX\nj=1\n\r\rAi,j \u2212 Mp\ni,j\n\r\r2\n2\n(2)\nwhere N is the number of subjects, the corresponding area of\nsubject in the mask Mp is filled by value 1 while the remain-\ning area is filled by 0. By employing this positive attention\nmechanism, the model is compelled to allocate more attention\nto the correct subject area, leading to an effective learning of\nthe corresponding subject characteristics.\nThe positive-style attention mechanism mentioned above\neffectively enhances the learning of specific characteristics\nfor each subject.\nHowever, there may still be issues with\nthe generated subjects, particularly when irrelevant areas are\npresent. For example, in Figure 5 (line \u2018w/o neg. attn.\u2019),\nwe can observe that the generated dog\u2019s legs are affected by\ncolor information from the given cat, which is not desirable.\nTo address this problem, we introduce negative guidance by\nconsidering the areas outside the subjects. In addition to the\npositive guidance within the ground truth subject mask, we\nincorporate a small negative value, denoted as \u03b7, into the re-\ngions outside the subject within the mask Mp. This mod-\nified mask, denoted as M[p,n], is then used in Eq. (2) to\nregulate the subject learning process. By integrating negative\nguidance, we can alleviate the issue of irrelevant areas and\nimprove the fidelity of the generated subjects.\n3.3\nModel Training and Inference\nTraining Strategy.\nDuring training, we employ a parame-\nter efficient fine-tuning strategy by only making the weights\nof key and value in all of the cross attention layers train-\nable.\nInspired by previous text-to-image personalization\n[Ruiz et al., 2023; Kumari et al., 2023], we conduct class-\nspecific prior preservation to improve the diversity of gener-\nated videos and alleviate the issue of language drift. The loss\nfor prior preservation is formulated as the following:\nLpr\nrecon = E\u03f5\u2032,z\u2032,c\u2032,t\u2032\nh\n\u2225\u03f5\u2032 \u2212 \u03f5\u2032\n\u03b8 (z\u2032\nt\u2032, T (c\u2032), t\u2032)\u22252\n2\ni\n,\n(3)\nwhere z\u2032 is the latent code of the input class image. \u03f5\u2032\n\u03b8 is\nthe noise prediction from the model \u03b8 and \u03f5\u2032 is the randomly\nsampled Gaussian noise. c\u2032 is the text condition for class im-\nage and t\u2032 is the sampling timestep. To this end, we train our\nCustomVideo via an end-to-end manner, with the following\noverall training objective:\nL = Lrecon + \u03b1 \u00b7 Lattn + \u03b2 \u00b7 Lpr\nrecon,\n(4)\nwhere \u03b1 and \u03b2 are two hyper-parameters to control the weight\nof attention control and prior preservation, respectively.\nInference.\nDuring inference, CustomVideo only requires\na specific text prompt with corresponding learned word to-\nken integrated to generate a required video. To be note that\nground truth masks of subjects are not required in inference.\nWith only text prompt as input, our proposed CustomVideo\ncan generate videos with high diversity while not losing the\nthe fidelity of subjects and motion smoothness.\n4\nExperiment\n4.1\nExperimental Setup\nDataset.\nWe collect a dataset CustomStudio with 69 indi-\nvidual subjects and construct 57 meaningful pairs for multi-\nsubject driven T2V generation. These subjects are adapted\nfrom DreamBooth [Ruiz et al., 2023], CustomDiffusion [Ku-\nmari et al., 2023] and internet. The objects cover a wide\nrange of categories, including pet, person, toy, transportation,\nscene, plant, wearable item, and so on. Each pair of subjects\nhas 10 different text prompts, which are designed with differ-\nent contexts, actions, and so on.\nSubjects\nDreamBooth\nCustomDiffusion\nVideoDreamer\ncar*\nbarn*\nCustomVideo\na front view of a car* stopping in front of a barn*\ncat*\ndog*\na cat* and a dog* sitting on the beach with a view of seashore\nFigure 3: Qualitative results of our CustomVideo with comparison to SOTA methods, including DreamBooth, CustomDiffusion, and\nVideoDreamer. The first line indicates the given subjects, while each line indicates the frames generated by the corresponding method. The\nbottom line is the text prompt used during inference. We can observe that our CustomVideo can generate videos with much better fidelity of\nsubjects compared with previous SOTA methods.\nImplementation Details.\nWe train CustomVideo for 500\nsteps with AdamW [Loshchilov and Hutter, 2017] optimizer\n(batch size 2, learning rate 4e-5, and weight decay 1e-2). For\nclass-specific prior preservation, we collect 200 class images\nfrom LAION-400M [Schuhmann et al., 2021] for each sub-\nject. Note that the corresponding class images are also con-\ncatenated during training phase. The negative value \u03b7 in the\nmask M is set as -1e-8. The weight parameters \u03b1 and \u03b2\nin Eq. (4) are set as 0.1 and 1.0, respectively. During in-\nference, we perform 50 steps denoising with DDIM [Song et\nal., 2020] sampler and classifier-free guidance [Ho and Sali-\nmans, 2022]. The resolution of generated 24-frame videos are\n576 \u00d7 320 with 8 fps. Interestingly, we find that the trained\nweights and word tokens learned from low resolution zero-\nscope T2V model can be directly loaded for high resolution\none2 to generate personalized videos with 1024 \u00d7 576 reso-\nlution, in which no additional training computation cost is re-\nquired. Our CustomVideo is implemented based on Diffusers\n[von Platen et al., 2022]. The training phase takes about 8\nminutes for a subject pair on 1 RTX 3090 GPU. Meanwhile,\nit takes about 1 minute and 3 minutes to generate a low and\nhigh resolution video on 1 RTX 3090 GPU, respectively.\nComparison Methods.\nExcept for VideoDreamer [Chen et\nal., 2023b], we also consider adapting previous SOTA image-\nbased multi-subject driven methods to a video scenario for\ncomparison, including DreamBooth [Ruiz et al., 2023] and\nCustomDiffusion [Kumari et al., 2023]. DreamBooth trains\nall of the weights in a diffusion model to personalize the gen-\n2https://huggingface.co/cerspense/zeroscope v2 XL\nTable 1: Quantitative results of our CustomVideo with compar-\nison to SOTA methods, including DreamBooth, CustomDiffusion,\nand VideoDreamer. Our proposed CustomVideo consistently out-\nperforms previous SOTA methods for all 4 evaluation metrics.\nMethod\nCLIP-T \u2191\nCLIP-I \u2191\nDINO-I \u2191\nT. Cons. \u2191\nDreamBooth\n0.6476\n0.6015\n0.3098\n0.7209\nCustomDiffusion\n0.6581\n0.6218\n0.3186\n0.7548\nVideoDreamer\n0.6649\n0.6314\n0.3471\n0.7362\nCustomVideo (ours)\n0.7051\n0.6749\n0.3955\n0.8142\neration from multiple subjects. CustomDiffusion fine-tune\nthe diffusion model with joint training on multiple subjects.\nVideoDreamer proposes a disen-mix and human-in-the-loop\nfine-tuning strategy for multi-subject customization of T2V\nmodel. For fair comparison, we implement all the compari-\nson method and our CustomVideo based on zeroscope model.\nEvaluation Metrics.\nFollowing previous works [Wei et al.,\n2023; Chen et al., 2023b], we quantitatively evaluate our\nCustomVideo with the following 4 metrics: 1) CLIP Textual\nAlignment computes the average cosine similarity between\nthe generated frames and text prompt with CLIP [Radford\net al., 2021] ViT-B/32 [Dosovitskiy et al., 2021] image and\ntext models; 2) CLIP Image Alignment calculates the average\ncosine similarity between the generated frames and subject\nimages with CLIP ViT-B/32 image model; 3) DINO Image\nAlignment measures the average visual similarity between\ngenerated frames and reference images with DINO [Caron et\nal., 2021] ViT-S/16 model; 4) Temporal Consistency [Esser et\nal., 2023] evaluates the average cosine similarity of between\nFigure 4: User study. Our CustomVideo achieves the best human\npreference compared with 3 SOTA comparison methods in terms of\nTextual Alignment, Image Alignment, and Overall Quality.\nTable 2: Quantitative results for component analysis of our pro-\nposed CustomVideo. We observe a significant performance drop\nwhen removing subjects concatenation or background removal. In\npractice, we suggest simultaneously using positive and negative at-\ntention mechanisms to obtain the best results.\nMethod\nCLIP-T \u2191\nCLIP-I \u2191\nDINO-I \u2191\nT. Cons. \u2191\nw/o remove bg\n0.6676\n0.6248\n0.3498\n0.7891\nw/o concat\n0.6189\n0.6371\n0.3572\n0.7936\nboth single and concat\n0.6148\n0.6319\n0.3208\n0.7983\nw/o pos. attn.\n0.6858\n0.6402\n0.3569\n0.8015\nw/o neg. attn.\n0.6926\n0.6392\n0.3694\n0.8075\nCustomVideo (ours)\n0.7051\n0.6749\n0.3955\n0.8142\nall consecutive frame pairs in the generated videos.\n4.2\nMain Results\nQuantitative Results.\nWe conduct quantitative experi-\nments on our collected dataset. To ensure a thorough anal-\nysis, we generate videos using 10 individual prompts and 4\nrandom seeds for each pair of subjects, resulting in a total\nof 2,280 generated videos for each method. We then eval-\nuate the quality of the generated videos using four metrics,\nand the results are presented in Table 1. The table clearly\ndemonstrates that our proposed method is capable of gener-\nating videos that are better aligned with the given subjects.\nThis improvement can be attributed to our specially designed\nco-occurrence and attention control mechanisms, which ef-\nfectively disentangle and preserve the fidelity of the sub-\njects. Notably, our CustomVideo outperforms the most re-\ncent method, VideoDreamer, by 6.05%, 6.89%, and 13.94%\nin terms of CLIP Textual Alignment, CLIP Image Alignment,\nand DINO Image Alignment, respectively. Furthermore, our\nCustomVideo generates videos with significantly higher tem-\nporal consistency compared to state-of-the-art methods, as\nindicated in Table 1. For instance, CustomVideo surpasses\nVideoDreamer by 10.59% in terms of Temporal Consistency.\nQualitative Results.\nWe present the qualitative comparison\nresults in Figure 3. From these results, we observe that our\nSubjects\nw/o remove bg\nw/o concat\nw/o pos. attn.\nw/o neg. attn.\ncat*\ndog*\nCustomVideo\na cat* and a dog* walking in the Times Square\nboth single \nand concat\nFigure 5: Qualitative results for component analysis of our pro-\nposed CustomVideo. We find that ensuring concatenating subjects\nduring training is effective for guaranteeing the co-occurrence in the\ngenerated video. Moreover, our attention mechanism could disen-\ntangle different subjects.\nCustomVideo approach effectively ensures the co-occurrence\nof multiple subjects and successfully disentangles different\nsubjects. However, both DreamBooth and CustomDiffusion\nfail to capture the structural color information of the car pro-\nvided in our experiments. The dominant black color from\nthe car window obscures the entire car, resulting in low fi-\ndelity. Moreover, the generated frames from VideoDreamer\nlack consistency, as some frames depict two cars while others\nonly show one car. Additionally, VideoDreamer fails to cap-\nture accurate color information, such as the color of the car\ndoor. In contrast, our CustomVideo method excels in han-\ndling such challenging scenes and foreground subject scenar-\nios, effectively capturing the intricate structural details of the\nprovided car for video generation. Similarly, when consid-\nering the case of \u2018cat\u2019 and \u2018dog\u2019, our approach also demon-\nstrates superior capability in generating high-quality videos.\nHuman Preference Study.\nTo further validate our method,\nwe conduct human evaluations on our CustomVideo with\ncomparison to 3 SOTA methods. In this study, we collect\n900 answers from 25 independent human raters with the fol-\nlowing questions: 1) which one is aligned to the text prompt\nbest? 2) which one is aligned to the subject images best? 3)\nwhich one has the best overall quality? The results are shown\nin Figure 4. It can be found that our proposed CustomVideo\nis the most preferred one from the human side from all these\nTable 3: Quantitative results of the weight of attention loss. In prac-\ntice, \u03b1=0.1 yields the best result and is adopted in all experiments.\n\u03b1\nCLIP-T \u2191\nCLIP-I \u2191\nDINO-I \u2191\nT. Cons. \u2191\n1.0\n0.6784\n0.6651\n0.3792\n0.8015\n0.1\n0.7051\n0.6749\n0.3955\n0.8142\n0.01\n0.6892\n0.6584\n0.3817\n0.8092\nTable 4: Quantitative results of the negative value. Interestingly, \u03b7=-\n1e-8 achieves the best result in terms of all the considered metrics.\nPractically, we adopt this setting in all the experiments.\n\u03b7\nCLIP-T \u2191\nCLIP-I \u2191\nDINO-I \u2191\nT. Cons. \u2191\n-1e-5\n0.6891\n0.6582\n0.3917\n0.7996\n-1e-8\n0.7051\n0.6749\n0.3955\n0.8142\n-1e-11\n0.6918\n0.6627\n0.3895\n0.8079\nthree dimensional evaluation.\n4.3\nAblation Studies\nWe conduct ablation analysis of our method from 2 aspects:\n1) the effect of each component in our method; 2) the effect\nof hyper-parameters in attention control mechanism.\nComponent Analysis.\nWe conduct a thorough analysis of\neach component in our CustomVideo, presenting both quali-\ntative and quantitative results in Figure 5 and Table 2. These\nfindings provide valuable insights into the importance and ef-\nfectiveness of each component. One crucial observation is\nthe significance of ensuring co-occurrence during the fine-\ntuning process. We notice a significant drop in performance\nwhen multiple subjects are not concatenated into a single im-\nage (lines \u2018w/o concat\u2019 and \u2018CustomVideo\u2019 in Table 2). The\nabsence of subject concatenation leads to the domination of\none single subject during fine-tuning, which is deemed unac-\nceptable (line \u2018w/o concat\u2019 in Figure 5). Moreover, we con-\nduct an ablation study on fine-tuning the diffusion model with\nboth single subjects and concatenated subjects to examine if\nthe single subject could aid in learning corresponding charac-\nteristics. Surprisingly, the results reveal that adding a single\nsubject to the training process proved detrimental, resulting\nin inconsistent generation during inference (line \u2018both single\nand concat\u2019 in Figure 5). Furthermore, we observe that re-\nmoving the background from the given subject images signif-\nicantly improves the generation of high-quality videos. By\neliminating the background, the diffusion model can focus\nsolely on learning the characteristics of the given subjects.\nThe comparison between videos generated with and without\nbackground removal clearly illustrates the monotonous na-\nture of videos lacking this component (line \u2018w/o remove bg\u2019\nin Figure 5). Our attention control mechanism plays a cru-\ncial role in aligning the generated videos with the given sub-\njects. The positive attention guidance in particular improves\nthe CLIP Image Alignment metric by 5.42%, as demonstrated\nin Table 2. The negative attention guidance also proved ben-\neficial in promoting better image alignment. Moreover, the\nattention control mechanism significantly enhances the tem-\nporal consistency of the generated videos, a crucial factor\nin video generation. The comparison between videos gen-\na sloth* and a teddybear* sitting in the snow\na man* riding a bike* on the beach\nsloth*\nteddybear*\nman*\nbike*\nSubjects\nGenerated Videos\nFigure 6: Failure cases of our CustomVideo. Our CustomVideo\nmay fail to capture some nuanced details of subjects when meeting\nextremely similar pairs of subjects, such as \u2018sloth\u2019 and \u2018teddybear\u2019\n(first line). Also, our approach fails to generate vivid facial contents\nwith a global view, such as \u2018man\u2019 and \u2018bike\u2019 (second line)\nerated without positive attention guidance and videos gener-\nated without negative attention guidance clearly showcases\nthe positive impact of both components (lines \u2018w/o pos. attn.\u2019\nand \u2018w/o neg. attn.\u2019 in Figure 5). The positive guidance\nspecifically preserves the unique characteristics of a subject,\nsuch as distinguishing between a cat and a dog, while the neg-\native guidance weakens the influence of other subjects on a\nspecific subject. By incorporating both positive and negative\nguidance, our CustomVideo excels in generating videos with\nhigh subject fidelity and remarkable temporal consistency.\nEffect of Hyper-parameters in Attention Control.\nIn ad-\ndition to component analysis, we also conduct investigations\ninto the effects of two important parameters: the weight of\nattention loss (\u03b1) and the negative value (\u03b7) used in the guid-\nance mask (M). The quantitative results of these investi-\ngations are presented in Table 3 and Table 4. Regarding the\nweight of attention loss, we observe that the best performance\nis achieved when \u03b1 is set to 0.1. This value results in optimal\nalignment between the generated videos and the given sub-\njects, indicating the importance of appropriately balancing\nthe attention guidance during training. For the negative value\nin the guidance mask, our findings reveal that even a slight\nnegative value is sufficient to enhance the quality of T2V\ngeneration. This implies that incorporating negative attention\nguidance can effectively suppress the influence of other sub-\njects on a specific subject, improving the generation quality.\n4.4\nLimitations\nIn Figure 6, we present some failure cases encountered during\nour experiments. One challenge arises when generating sub-\njects that are extremely similar, such as a sloth and a teddy\nbear, where capturing fine-grained details becomes difficult.\nAdditionally, since our method relies on the ability of base\nmodel, it would fail if the base model could not generate, such\nas small faces. To address the failure cases, we can improve\nCustomVideo by using a stronger T2V base model, like Sta-\nble Video Diffusion [Blattmann et al., 2023a]. Additionally,\nwe can implement mechanisms to prioritize and emphasize\nnuanced characteristics in the given images during training.\n5\nConclusions\nThis paper provides a novel framework CustomVideo for\nmulti-subject driven T2V generation, powered by a simple\nyet effective co-occurrence and attention control mechanism.\nTo evaluate the proposed framework, we collect a comprehen-\nsive dataset consisting of 69 individual subjects and 57 mean-\ningful pairs. Extensive experiments and human preference\nstudy on collected dataset demonstrate the effectiveness of\nCustomVideo, outperforming existing methods significantly.\nA\nAdditional Ablation Studies\nLayers of Cross Attention Maps.\nWe investigates the ef-\nfect of how we extract the cross attention maps from the cross\nattention layers. There are four levels of cross attention lay-\ners in the UNet, in which the sizes cross attention maps are\ndifferent. Taking the resolution 576 \u00d7 320 of our generated\nlow resolution video as an example, the sizes of cross atten-\ntion maps are 72 \u00d7 40 (\u21131), 36 \u00d7 20 (\u21132), 18 \u00d7 10 (\u21133), and\n9 \u00d7 5 (\u21134). Here, we study the effect of different levels of\ncross attention maps, the results are shown in Table 5. We\nobserve a significant performance drop with too large (\u21131) or\nsmall (\u21134) cross attention maps. Regarding the cross attention\nmaps with middle size from \u21132 and \u21133, \u21133 works better. Thus,\nin our experiments, we extract cross attention maps from \u21133\nfor attention control.\nB\nCustomStudio Dataset\nWe collect a dataset CustomStudio for multi-subject driven\nvideo generation. This dataset is composed of 69 individual\nobjects and 57 meaningful pairs. The objects cover a wide\nrange of categories, including decor item, food, furniture, in-\nstrument, jewelry, luggage, person, pet, plant, plushie, scene,\nthing, toy, transportation, and wearable item. The images are\nadopted from DreamBooth [Ruiz et al., 2023], CustomDiffu-\nsion [Kumari et al., 2023] and internet. An overview of the\nsamples in CustomStudio dataset is shown in Figure 7.\nC\nMore Experimental Details\nAll of the training process is under fp16 mixed precision\nwith accelerate package [Gugger et al., 2022]. We use data\naugmentations during training, including randomly horizon-\ntal flip, randomly crop and resize, together with correspond-\ning prompt change (\u2018very small\u2019 or \u2018close up\u2019 appends before\nthe prompt).\nDreamBooth.\nFor efficient fine-tuning, we utilize LoRA\n[Hu et al., 2022] to adapt the UNet under DreamBooth [Ruiz\net al., 2023]. The rank of LoRA module is set as 4. We use\nAdamW [Loshchilov and Hutter, 2017] optimizer, with learn-\ning rate 4e-4, weight decay 1e-2, batch size 2, and training\nsteps 1000.\nCustomDiffusion.\nThe weights of query and value in all of\nthe cross attention layers of UNet are fine-tuned in CustomD-\niffusion [Kumari et al., 2023]. We use AdamW [Loshchilov\nand Hutter, 2017] optimizer, with learning rate 4e-5, weight\ndecay 1e-2, batch size 2, and training steps 500.\nVideoDreamer.\nLoRA [Hu et al., 2022] is utilized for fine-\ntuning the UNet and text encoder in VideoDreamer [Chen et\nal., 2023b]. The AdamW [Loshchilov and Hutter, 2017] op-\ntimizer is used with learning rate 5e-5, weight decay 1e-2,\nbatch size 2, and training steps 500.\nTable 5: Quantitative results of cross attention levels. We find\nthat only using cross attention map from \u21133 works best.\n\u21131\n\u21132\n\u21133\n\u21134\nCLIP-T \u2191\nCLIP-I \u2191\nDINO-I \u2191\nT. Cons. \u2191\n!\n%\n%\n%\n0.6134\n0.5759\n0.3023\n0.8125\n%\n!\n%\n%\n0.6694\n0.6239\n0.3450\n0.7346\n%\n%\n!\n%\n0.7051\n0.6749\n0.3955\n0.8142\n%\n%\n%\n!\n0.6144\n0.6545\n0.3426\n0.7992\n!\n!\n%\n%\n0.6882\n0.6177\n0.3010\n0.8076\n%\n%\n!\n!\n0.7047\n0.5801\n0.3515\n0.7930\n%\n!\n!\n%\n0.6502\n0.6407\n0.3651\n0.7875\n!\n%\n%\n!\n0.6267\n0.6734\n0.3746\n0.7594\n!\n!\n!\n!\n0.6869\n0.5885\n0.3894\n0.7605\nFigure 7: The overview of our CustomStudio dataset. Samples in\nour CustomStudio dataset cover a wide range of object categories.\nCustomVideo.\nThe weights of query and value in all of the\ncross attention layers of UNet are fine-tuned. The learnable\nword tokens are initialized with the tokens of the correspond-\ning class names. The sampling step for high resolution video\nis 30.\nReferences\n[Balaji et al., 2019] Yogesh Balaji, Martin Renqiang Min,\nBing Bai, Rama Chellappa, and Hans Peter Graf. Con-\nditional gan with discriminative filter generation for text-\nto-video synthesis. In IJCAI, volume 1, page 2, 2019.\n[Blattmann et al., 2023a] Andreas Blattmann, Tim Dock-\nhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian,\nDominik Lorenz, Yam Levi, Zion English, Vikram Voleti,\nAdam Letts, et al. Stable video diffusion: Scaling latent\nvideo diffusion models to large datasets. arXiv, 2023.\n[Blattmann et al., 2023b] Andreas Blattmann, Robin Rom-\nbach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja\nFidler, and Karsten Kreis.\nAlign your latents: High-\nresolution video synthesis with latent diffusion models. In\nCVPR, pages 22563\u201322575, 2023.\n[Caron et al., 2021] Mathilde Caron, Hugo Touvron, Ishan\nMisra, Herv\u00b4e J\u00b4egou, Julien Mairal, Piotr Bojanowski, and\nArmand Joulin. Emerging properties in self-supervised vi-\nsion transformers. In ICCV, pages 9650\u20139660, 2021.\n[Chen et al., 2023a] Haoxin Chen, Menghan Xia, Yingqing\nHe, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo\nXing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al.\nVideocrafter1: Open diffusion models for high-quality\nvideo generation. arXiv, 2023.\n[Chen et al., 2023b] Hong Chen, Xin Wang, Guanning Zeng,\nYipeng Zhang, Yuwei Zhou, Feilin Han, and Wenwu\nZhu.\nVideodreamer: Customized multi-subject text-to-\nvideo generation with disen-mix finetuning. arXiv, 2023.\n[Dosovitskiy et al., 2021] Alexey Dosovitskiy, Lucas Beyer,\nAlexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Min-\nderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and\nNeil Houlsby. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In ICLR, 2021.\n[Duan et al., 2023] Zhongjie Duan, Lizhou You, Chengyu\nWang, Cen Chen, Ziheng Wu, Weining Qian, Jun Huang,\nFei Chao, and Rongrong Ji. Diffsynth: Latent in-iteration\ndeflickering for realistic video synthesis. arXiv, 2023.\n[Esser et al., 2023] Patrick Esser, Johnathan Chiu, Parmida\nAtighehchian, Jonathan Granskog, and Anastasis Ger-\nmanidis.\nStructure and content-guided video synthesis\nwith diffusion models. In ICCV, pages 7346\u20137356, 2023.\n[Gal et al., 2023] Rinon Gal, Yuval Alaluf, Yuval Atzmon,\nOr Patashnik, Amit Haim Bermano, Gal Chechik, and\nDaniel Cohen-or. An image is worth one word: Personal-\nizing text-to-image generation using textual inversion. In\nICLR, 2023.\n[Geyer et al., 2023] Michal Geyer,\nOmer Bar-Tal,\nShai\nBagon, and Tali Dekel. Tokenflow: Consistent diffusion\nfeatures for consistent video editing. arXiv, 2023.\n[Gugger et al., 2022] Sylvain\nGugger,\nLysandre\nDebut,\nThomas Wolf, Philipp Schmid, Zachary Mueller, Sourab\nMangrulkar, Marc Sun, and Benjamin Bossan. Accelerate:\nTraining and inference at scale made simple, efficient\nand adaptable. https://github.com/huggingface/accelerate,\n2022.\n[Guo et al., 2023] Yuwei Guo, Ceyuan Yang, Anyi Rao,\nYaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai.\nAni-\nmatediff: Animate your personalized text-to-image diffu-\nsion models without specific tuning. arXiv, 2023.\n[He et al., 2022] Yingqing He, Tianyu Yang, Yong Zhang,\nYing Shan, and Qifeng Chen.\nLatent video diffusion\nmodels for high-fidelity video generation with arbitrary\nlengths. arXiv, 2022.\n[Ho and Salimans, 2022] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion guidance. arXiv, 2022.\n[Ho et al., 2020] Jonathan Ho, Ajay Jain, and Pieter Abbeel.\nDenoising diffusion probabilistic models.\nNeurIPS,\n33:6840\u20136851, 2020.\n[Hong et al., 2022] Wenyi Hong, Ming Ding, Wendi Zheng,\nXinghan Liu, and Jie Tang.\nCogvideo:\nLarge-scale\npretraining for text-to-video generation via transformers.\narXiv, 2022.\n[Hu et al., 2022] Edward J Hu, yelong shen, Phillip Wallis,\nZeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. LoRA: Low-rank adaptation of large\nlanguage models. In ICLR, 2022.\n[Jiang et al., 2023] Yuming Jiang, Tianxing Wu, Shuai Yang,\nChenyang Si, Dahua Lin, Yu Qiao, Chen Change Loy, and\nZiwei Liu. Videobooth: Diffusion-based video generation\nwith image prompts. arXiv, 2023.\n[Khachatryan et al., 2023] Levon Khachatryan,\nAndranik\nMovsisyan,\nVahram Tadevosyan,\nRoberto Henschel,\nZhangyang Wang, Shant Navasardyan, and Humphrey\nShi. Text2video-zero: Text-to-image diffusion models are\nzero-shot video generators. ICCV, 2023.\n[Kirillov et al., 2023] Alexander\nKirillov,\nEric\nMintun,\nNikhila\nRavi,\nHanzi\nMao,\nChloe\nRolland,\nLaura\nGustafson, Tete Xiao, Spencer Whitehead, Alexander C.\nBerg, Wan-Yen Lo, Piotr Doll\u00b4ar, and Ross Girshick.\nSegment anything. ICCV, 2023.\n[Kondratyuk et al., 2023] Dan Kondratyuk, Lijun Yu, Xi-\nuye Gu, Jos\u00b4e Lezama, Jonathan Huang, Rachel Hor-\nnung, Hartwig Adam, Hassan Akbari, Yair Alon, Vigh-\nnesh Birodkar, et al. Videopoet: A large language model\nfor zero-shot video generation. arXiv, 2023.\n[Kumari et al., 2023] Nupur\nKumari,\nBingliang\nZhang,\nRichard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-\nconcept customization of text-to-image diffusion.\nIn\nCVPR, pages 1931\u20131941, 2023.\n[Liu et al., 2023a] Zhiheng Liu, Ruili Feng, Kai Zhu, Yifei\nZhang, Kecheng Zheng, Yu Liu, Deli Zhao, Jingren Zhou,\nand Yang Cao. Cones: Concept neurons in diffusion mod-\nels for customized generation. ICML, 2023.\n[Liu et al., 2023b] Zhiheng Liu, Yifei Zhang, Yujun Shen,\nKecheng Zheng, Kai Zhu, Ruili Feng, Yu Liu, Deli Zhao,\nJingren Zhou, and Yang Cao. Cones 2: Customizable im-\nage synthesis with multiple subjects. NeurIPS, 2023.\n[Loshchilov and Hutter, 2017] Ilya Loshchilov and Frank\nHutter.\nDecoupled weight decay regularization.\narXiv,\n2017.\n[Lu et al., 2022] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei\nChen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast\node solver for diffusion probabilistic model sampling in\naround 10 steps. NeurIPS, 35:5775\u20135787, 2022.\n[Qi et al., 2023] Chenyang Qi, Xiaodong Cun, Yong Zhang,\nChenyang Lei, Xintao Wang, Ying Shan, and Qifeng\nChen. Fatezero: Fusing attentions for zero-shot text-based\nvideo editing. ICCV, 2023.\n[Radford et al., 2021] Alec Radford, Jong Wook Kim, Chris\nHallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-\nwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack\nClark, et al. Learning transferable visual models from nat-\nural language supervision. In ICML, pages 8748\u20138763,\n2021.\n[Rombach et al., 2022] Robin\nRombach,\nAndreas\nBlattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn\nOmmer.\nHigh-resolution image synthesis with latent\ndiffusion models. In CVPR, pages 10684\u201310695, 2022.\n[Ruiz et al., 2023] Nataniel Ruiz, Yuanzhen Li, Varun Jam-\npani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\nDreambooth: Fine tuning text-to-image diffusion models\nfor subject-driven generation.\nIn CVPR, pages 22500\u2013\n22510, 2023.\n[Schuhmann et al., 2021] Christoph Schuhmann,\nRichard\nVencu, Romain Beaumont, Robert Kaczmarczyk, Clayton\nMullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and\nAran Komatsuzaki. Laion-400m: Open dataset of clip-\nfiltered 400 million image-text pairs. arXiv, 2021.\n[Singer et al., 2023] Uriel Singer, Adam Polyak, Thomas\nHayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu,\nHarry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal\nGupta, and Yaniv Taigman. Make-a-video: Text-to-video\ngeneration without text-video data. In ICLR, 2023.\n[Skorokhodov et al., 2022] Ivan\nSkorokhodov,\nSergey\nTulyakov, and Mohamed Elhoseiny.\nStylegan-v:\nA\ncontinuous video generator with the price, image quality\nand perks of stylegan2. In CVPR, pages 3626\u20133636, 2022.\n[Song et al., 2020] Jiaming Song, Chenlin Meng, and Ste-\nfano Ermon. Denoising diffusion implicit models. arXiv,\n2020.\n[Villegas et al., 2023] Ruben\nVillegas,\nMohammad\nBabaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo,\nHan Zhang, Mohammad Taghi Saffar, Santiago Castro,\nJulius Kunze, and Dumitru Erhan.\nPhenaki: Variable\nlength video generation from open domain textual\ndescriptions. In ICLR, 2023.\n[von Platen et al., 2022] Patrick von Platen, Suraj Patil, An-\nton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Ra-\nsul, Mishig Davaadorj, and Thomas Wolf.\nDiffusers:\nState-of-the-art diffusion models.\nhttps://github.com/\nhuggingface/diffusers, 2022.\n[Wang et al., 2023a] Jiuniu Wang, Hangjie Yuan, Dayou\nChen, Yingya Zhang, Xiang Wang, and Shiwei Zhang.\nModelscope text-to-video technical report. arXiv, 2023.\n[Wang et al., 2023b] Xiang Wang, Hangjie Yuan, Shiwei\nZhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yu-\njun Shen, Deli Zhao, and Jingren Zhou. Videocomposer:\nCompositional video synthesis with motion controllability.\narXiv, 2023.\n[Wang et al., 2023c] Yaohui Wang, Xinyuan Chen, Xin Ma,\nShangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yi-\nnan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-\nquality video generation with cascaded latent diffusion\nmodels. arXiv, 2023.\n[Wei et al., 2023] Yujie Wei, Shiwei Zhang, Zhiwu Qing,\nHangjie Yuan, Zhiheng Liu, Yu Liu, Yingya Zhang, Jin-\ngren Zhou, and Hongming Shan. Dreamvideo: Compos-\ning your dream videos with customized subject and mo-\ntion. arXiv, 2023.\n[Wu et al., 2023] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang,\nStan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu,\nYing Shan, Xiaohu Qie, and Mike Zheng Shou.\nTune-\na-video: One-shot tuning of image diffusion models for\ntext-to-video generation. In ICCV, 2023.\n[Zeng et al., 2023] Yan Zeng, Guoqiang Wei, Jiani Zheng,\nJiaxin Zou, Yang Wei, Yuchen Zhang, and Hang Li. Make\npixels dance: High-dynamic video generation.\narXiv,\n2023.\n[Zhang et al., 2023a] David Junhao Zhang, Jay Zhangjie\nWu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu,\nDifei Gao, and Mike Zheng Shou. Show-1: Marrying pixel\nand latent diffusion models for text-to-video generation.\narXiv, 2023.\n[Zhang et al., 2023b] Shiwei Zhang, Jiayu Wang, Yingya\nZhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang\nWang, Deli Zhao, and Jingren Zhou.\nI2vgen-xl: High-\nquality image-to-video synthesis via cascaded diffusion\nmodels. arXiv, 2023.\n[Zhao et al., 2023] Haoyu Zhao, Tianyi Lu, Jiaxi Gu, Xing\nZhang, Zuxuan Wu, Hang Xu, and Yu-Gang Jiang.\nVideoassembler: Identity-consistent video generation with\nreference entities using diffusion model. arXiv, 2023.\n"
  }
]