[
  {
    "title": "PIXART-\u03b4: Fast and Controllable Image Generation with Latent Consistency Models",
    "link": "https://arxiv.org/pdf/2401.05252.pdf",
    "upvote": "42",
    "text": "Technical Report\nPIXART-\u03b4: FAST AND CONTROLLABLE IMAGE\nGENERATION WITH LATENT CONSISTENCY MODELS\nJunsong Chen1,2,4, Yue Wu1, Simian Luo3, Enze Xie1\u2020,\nSayak Paul5, Ping Luo4, Hang Zhao3, Zhenguo Li1\n1Huawei Noah\u2019s Ark Lab\n2Dalian University of Technology\n3IIIS, Tsinghua University\n4The University of Hong Kong\n5Hugging Face\njschen@mail.dlut.edu.cn, luosm22@mails.tsinghua.edu.cn,\n{wuyue119,xie.enze,Li.Zhenguo}@huawei.com\nHomepage: https://pixart-alpha.github.io/\nCode: https://github.com/PixArt-alpha/PixArt-alpha\nDemo: https://huggingface.co/spaces/PixArt-alpha/PixArt-LCM\nABSTRACT\nThis technical report introduces PIXART-\u03b4, a text-to-image synthesis framework\nthat integrates the Latent Consistency Model (LCM) and ControlNet into the ad-\nvanced PIXART-\u03b1 model. PIXART-\u03b1 is recognized for its ability to generate high-\nquality images of 1024px resolution through a remarkably efficient training pro-\ncess. The integration of LCM in PIXART-\u03b4 significantly accelerates the inference\nspeed, enabling the production of high-quality images in just 2-4 steps. Notably,\nPIXART-\u03b4 achieves a breakthrough 0.5 seconds for generating 1024 \u00d7 1024 pixel\nimages, marking a 7\u00d7 improvement over the PIXART-\u03b1. Additionally, PIXART-\u03b4\nis designed to be efficiently trainable on 32GB V100 GPUs within a single day.\nWith its 8-bit inference capability (von Platen et al., 2023), PIXART-\u03b4 can synthe-\nsize 1024px images within 8GB GPU memory constraints, greatly enhancing its\nusability and accessibility. Furthermore, incorporating a ControlNet-like module\nenables fine-grained control over text-to-image diffusion models. We introduce a\nnovel ControlNet-Transformer architecture, specifically tailored for Transformers,\nachieving explicit controllability alongside high-quality image generation. As a\nstate-of-the-art, open-source image generation model, PIXART-\u03b4 offers a promis-\ning alternative to the Stable Diffusion family of models, contributing significantly\nto text-to-image synthesis.\n1\nINTRODUCTION\nIn this technical report, we propose PIXART-\u03b4, which incorporates LCM (Luo et al., 2023a) and\nControlNet (Zhang et al., 2023) into PIXART-\u03b1 (Chen et al., 2023). Notably, PIXART-\u03b1 is an\nadvanced high-quality 1024px diffusion transformer text-to-image synthesis model, developed by\nour team, known for its superior image generation quality achieved through an exceptionally efficient\ntraining process.\nWe incorporate LCM into the PIXART-\u03b4 to accelerate the inference. LCM (Luo et al., 2023a)\nenables high-quality and fast inference with only 2\u223c4 steps on pre-trained LDMs by viewing the\nreverse diffusion process as solving an augmented probability flow ODE (PF-ODE), which enables\nPIXART-\u03b4 to generate samples within (\u223c4) steps while preserving high-quality generations. As\na result, PIXART-\u03b4 takes 0.5 seconds per 1024 \u00d7 1024 image on an A100 GPU, improving the\ninference speed by 7\u00d7 compared to PIXART-\u03b1. We also support LCM-LoRA (Luo et al., 2023b)\nfor a better user experience and convenience.\n\u2020 Project lead.\n1\narXiv:2401.05252v1  [cs.CV]  10 Jan 2024\nTechnical Report\nIn addition, we incorporate a ControlNet-like module into the PIXART-\u03b4. ControlNet (Zhang et al.,\n2023) demonstrates superior control over text-to-image diffusion models\u2019 outputs under various\nconditions. However, it\u2019s important to note that the model architecture of ControlNet is intricately\ndesigned for UNet-based diffusion models, and we observe that a direct replication of it into a\nTransformer model proves less effective. Consequently, we propose a novel ControlNet-Transformer\narchitecture customized for the Transformer model. Our ControlNet-Transformer achieves explicit\ncontrollability and obtains high-quality image generation.\n2\nBACKGROUND\n2.1\nCONSISTENCY MODEL\nConsistency Model (CM) and Latent Consistency Model (LCM) have made significant advance-\nments in the field of generative model acceleration. CM, introduced by Song et al. (2023) has\ndemonstrated its potential to enable faster sampling while maintaining the quality of generated im-\nages on ImageNet dataset (Deng et al., 2009). A key ingredient of CM is trying to maintain the\nself-consistency property during training (consistency mapping technique), which allows for the\nmapping of any data point on a Probability Flow Ordinary Differential Equation (PF-ODE) trajec-\ntory back to its origin.\nLCM, proposed by Luo et al. (2023a), extends the success of CM to the current most challenging\nand popular LDMs, Stable Diffusion (Rombach et al., 2022) and SD-XL (Podell et al., 2023) on\nText-to-Image generative task. LCM accelerates the reverse sampling process by directly predicting\nthe solution of the augmented PF-ODE in latent space. LCM combines several effective techniques\n(e.g, One-stage guided distillation, Skipping-step technique) to achieve remarkable rapid inference\nspeed on Stable Diffusion models and fast training convergence. LCM-LoRA (Luo et al., 2023b),\ntraining LCM with the LoRA method (Hu et al., 2021), demonstrates strong generalization, estab-\nlishing it as a universal Stable Diffusion acceleration module. In summary, CM and LCM have\nrevolutionized generative modeling by introducing faster sampling techniques while preserving the\nquality of generated outputs, paving the way for real-time generation applications.\n2.2\nCONTROLNET\nControlNet (Zhang et al., 2023) demonstrates superior control over text-to-image diffusion mod-\nels\u2019 outputs under various conditions (e.g., canny edge, open-pose, sketch). It introduces a special\nstructure, a trainable copy of UNet, that allows for the manipulation of input conditions, enabling\ncontrol over the overall layout of the generated image. During training, ControlNet freezes the ori-\ngin text-to-image diffusion model and only optimizes the trainable copy. It integrates the outputs of\neach layer of this copy by skip-connections into the original UNet using \u201czero convolution\u201d layers\nto avoid harmful noise interference.\nThis innovative approach effectively prevents overfitting while preserving the quality of the pre-\ntrained UNet models, initially trained on an extensive dataset comprising billions of images. Con-\ntrolNet opens up possibilities for a wide range of conditioning controls, such as edges, depth, seg-\nmentation, and human pose, and facilitates many applications in controlling image diffusion models.\n3\nLCM IN PIXART-\u03b4\nIn this section, we employ Latent Consistency Distillation (LCD) (Luo et al., 2023a) to train\nPIXART-\u03b4 on 120K internal image-text pairs. In Sec. 3.1, we first provide a detailed training al-\ngorithm and ablation study on specific modifications. In Sec. 3.2, we illustrate the training effi-\nciency and the speedup of LCM of PIXART-\u03b4. Lastly, in Sec. 3.3, we present the training details of\nPIXART-\u03b4.\n3.1\nALGORITHM AND MODIFICATION\nLCD Algorithm. Deriving from the original Consistency Distillation (CD) (Song et al., 2023)\nand LCD (Luo et al., 2023a) algorithm, we present the pseudo-code for PIXART-\u03b4 with classifier-\n2\nTechnical Report\nfree guidance (CFG) in Algorithm 1. Specifically, as illustrated in the training pipeline shown in\nFig. 1, three models \u2013 Teacher, Student, and EMA Model \u2013 function as denoisers for the ODE solver\n\u03a8(\u00b7, \u00b7, \u00b7, \u00b7), f\u03b8, and f\u03b8\u2212, respectively. During the training process, we begin by sampling noise at\ntimestep tn+k, where the Teacher Model is used for denoising to obtain \u02c6zTt0 . We then utilize a ODE\nsolver \u03a8(\u00b7, \u00b7, \u00b7, \u00b7) to calculate \u02c6z\u03a8,\u03c9\ntn\nfrom ztn+k and \u02c6zTt0 . EMA Model is then applied for further\ndenoising, resulting in \u02c6zEt0 . In parallel, the Student Model denoises the sample ztn+k at tn+k to\nderive \u02c6zSt0 . The final step involves minimizing the distance between \u02c6zSt0 and \u02c6zEt0 , also known as\noptimizing the consistency distillation objective.\nDifferent from the original LCM, which selects variable guidance scale \u03c9 from a designated range\n[\u03c9min, \u03c9max], in our implementation, we set the guidance scale as a constant \u03c9fix, removing the\nguidance scale embedding operation in LCM (Luo et al., 2023a) for convenience.\nAlgorithm 1 PixArt - Latent Consistency Distillation (LCD)\nInput: dataset D, initial model parameter \u03b8, learning rate \u03b7, ODE solver \u03a8(\u00b7, \u00b7, \u00b7, \u00b7), distance metric d(\u00b7, \u00b7),\nEMA rate \u00b5, noise schedule \u03b1(t), \u03c3(t), guidance scale \u03c9fix, skipping interval k, and encoder E(\u00b7)\nEncoding training data into latent space: Dz = {(z, c)|z = E(x), (x, c) \u2208 D}\n\u03b8\u2212 \u2190 \u03b8\nrepeat\nSample (z, c) \u223c Dz, n \u223c U[1, N \u2212 k]\nSample ztn+k \u223c N(\u03b1(tn+k)z; \u03c32(tn+k)I)\n\u02c6z\n\u03a8,\u03c9fix\ntn\n\u2190 ztn+k + (1 + \u03c9fix)\u03a8(ztn+k, tn+k, tn, c) \u2212 \u03c9fix\u03a8(ztn+k, tn+k, tn, \u2205)\nL(\u03b8, \u03b8\u2212; \u03a8) \u2190 d(f\u03b8(ztn+k, \u03c9fix, c, tn+k), f\u03b8\u2212(\u02c6z\n\u03a8,\u03c9fix\ntn\n, \u03c9fix, c, tn))\n\u03b8 \u2190 \u03b8 \u2212 \u03b7\u2207\u03b8L(\u03b8, \u03b8\u2212)\n\u03b8\u2212 \u2190 stopgrad(\u00b5\u03b8\u2212 + (1 \u2212 \u00b5)\u03b8)\nuntil convergence\n\ud835\udc67!!\"#\nODE Trajectory\n\ud835\udc67!!\n\ud835\udc67!$\nData\nNoise\nLoss\nEMA\nModel\nTeacher\nModel\nTeacher\nModel\n+Text\n+\u2205\nStudent\nModel\n(1+ \ud835\udf4e\ud835\udc87\ud835\udc8a\ud835\udc99) \u00d7\n\ud835\udf4e\ud835\udc87\ud835\udc8a\ud835\udc99 \u00d7\nShare\n-\nEMA    Update\n\u0302\ud835\udc67!$%\n\u0302\ud835\udc67\"$%\n\u0302\ud835\udc67#$%\n1\n4\n2\n4\n2\n1\n\ud835\udf33\nNoise\nDenoise\nODE-Solver:\n3\n3\n\ud835\udf33\nODE-Solver:\nFigure 1: Training pipeline of PIXART-\u03b4. The upper section of the diagram offers a high-level\noverview of the training process, depicting the sequential stages of noise sampling and denoising\nalong a specific ODE trajectory. Sequence numbers are marked on the mapping lines to clearly\nindicate the order of these steps. The lower section delves into the intricate roles of the pre-trained\n(teacher) model and the student model, revealing their respective functions within the upper block\u2019s\ntraining process, with corresponding sequence numbers also marked for easy cross-referencing.\nEffect of Hyper-parameters. Our study complements two key aspects of the LCM training process,\nCFG scale and batch size. These factors are evaluated using FID and CLIP scores as performance\nbenchmarks. The terms \u2018bs\u2019, \u2018\u03c9 fix\u2019, and \u2018\u03c9 Embed\u2019 in the Fig. 2 represent training batch size,\nfixed guidance scale, and embedded guidance scale, respectively.\n3\nTechnical Report\n\u2022 CFG Scale Analysis: Referencing Fig. 2, we examine three distinct CFG scales: (1) 3.5,\nutilized in our ablation study; (2) 4.5, which yieldes optimal results in PIXART-\u03b1; and (3) a\nvaried range of CFG scale embeddings (\u03c9 Embed), the standard approach in LCM. Our\nresearch reveals that employing a constant guidance scale, instead of the more complex\nCFG embeddings improves performance in PIXART-\u03b4 and simplifies the implementation.\n\u2022 Batch Size Examination: The impact of batch size on model performance is assessed\nusing two configurations: 2 V100 GPUs and 32 V100 GPUs; each GPU loads 12 images.\nAs illustrated in Fig. 2, our results indicate that larger batch size positively influences FID\nand CLIP scores. However, as shown in Fig. 8, PIXART-\u03b4 can also converge fast and get\ncomparable image quality with smaller batch sizes.\n\u2022 Convergence: Finally, we observe that the training process tends to reach convergence\nafter approximately 5,000 iterations. Beyond this phase, further improvements are minimal.\n(a) FID v.s. Iteration\n(b) CLIP score v.s. Iteration\nFigure 2: Ablation study of FID and CLIP Score on various strategies for classifier-free guidance\nscale (\u03c9) and their impact on distillation convergence during training.\nNoise Schedule Adjustment. Noise schedule is one of the most important parts of the diffusion\nprocess. Following (Hoogeboom et al., 2023; Chen, 2023), we adapt the noise schedule function\nin LCM to align with the PIXART-\u03b1 noise schedule, which features a higher logSNR (signal-to-\nnoise ratio) during the distillation training. Fig. 3 visualizes the noise schedule functions under\ndifferent choices of PIXART-\u03b4 or LCM, along with their respective logSNR. Notably, PIXART-\u03b4\ncan parameterize a broader range of noise distributions, a feature that has been shown further to\nenhance image generation (Hoogeboom et al., 2023; Chen, 2023).\n(b) Noise schedule v.s. Time step\n(c) LogSNR v.s. Time step\n(a) Beta v.s. Time step\nFigure 3: Instantiations of \u03b2t, noise schedule function and the corresponding logSNR between\nPIXART-\u03b4 and LCM. \u03b2t is the coefficient in the diffusion process zt = \u221a \u00af\u03b1tz0 + \u221a1 \u2212 \u00af\u03b1t\u03f5, \u03b1t =\n1 \u2212 \u03b2t.\n3.2\nTRAINING EFFICIENCY AND INFERENCE SPEEDUP\nFor training, as illustrated in Tab. 1, we successfully conduct the distillation process within a 32GB\nGPU memory constraint, all while retaining the same batch size and supporting image resolution\n4\nTechnical Report\nup to 1024 \u00d7 1024 with SDXL-LCM. Such training efficiency remarkably enables PIXART-\u03b4 to be\ntrained on a wide array of consumer-grade GPU specifications. In light of the discussions in Sec.3.1,\nregarding the beneficial impact of larger batch size, our method notably makes it feasible to utilize\nlarger batch size even on GPUs with limited memory capacity.\nFor inference, as shown in Tab. 2 and Fig. 7, we present a comparative analysis of the generation\nspeed achieved by our model, PIXART-\u03b4, against other methods like SDXL LCM-LoRA, PIXART-\n\u03b1, and the SDXL standard across different hardware platforms. Consistently, PIXART-\u03b4 achieves\n1024x1024 high resolution image generation within 0.5 seconds on an A100, and also completes\nthe process in a mere 3.3 seconds on a T4, 0.8 seconds on a V100, all with a batch size of 1. This\nis a significant improvement over the other methods, where, for instance, the SDXL standard takes\nup to 26.5 seconds on a T4 and 3.8 seconds on an A100. The efficiency of PIXART-\u03b4 is evident\nas it maintains a consistent lead in generation speed with only 4 steps, compared to the 14 and 25\nsteps required by PIXART-\u03b1 and SDXL standard, respectively. Notably, with the implementation\nof 8-bit inference technology, PIXART-\u03b4 requires less than 8GB of GPU VRAM. This remarkable\nefficiency enables PIXART-\u03b4 to operate on a wide range of GPU cards, and it even opens up the\npossibility of running on a CPU.\nTable 1: Illustration of the training setting between LCM on PIXART-\u03b4 and Stable Diffusion models.\n(* stands for Stable Diffusion Dreamshaper-v7 finetuned version)\nMethods\nPIXART-\u03b4\nSDXL LCM-LoRA\nSD-V1.5-LCM*\nData Volume\n120K\n650K\n650K\nResolution\n1024px\n1024px\n768px\nBatch Size\n12 \u00d7 32\n12 \u00d7 64\n16 \u00d7 8\nGPU Memory\n\u223c32G\n\u223c80G\n\u223c80G\nTable 2: Illustration of the generation speed we achieve on various devices. These tests are con-\nducted on 1024 \u00d7 1024 resolution with a batch size of 1 in all cases. Corresponding image samples\nare shown in the Fig. 7\nHardware\nPIXART-\u03b4\nSDXL LCM-LoRA\nPIXART-\u03b1\nSDXL standard\n4 steps\n4 steps\n14 steps\n25 steps\nT4\n3.3s\n8.4s\n16.0s\n26.5s\nV100\n0.8s\n1.2s\n5.5s\n7.7s\nA100\n0.5s\n1.2s\n2.2s\n3.8s\n3.3\nTRAINING DETAILS\nAs discussed in Sec. 3.1, we conduct our experiments in two resolution settings, 512\u00d7512 and\n1024\u00d71024, utilizing a high-quality internal dataset with 120K images. We smoothly train the mod-\nels in both resolutions by leveraging the multi-scale image generation capabilities of PIXART-\u03b1,\nwhich supports 512px and 1024px resolutions. For both resolutions, PIXART-\u03b4 yields impressive\nresults before reaching 5K iterations, with only minimal improvements observed thereafter. The\ntraining is executed on 2 V100 GPUs with a total batch size of 24, a learning rate of 2e-5, EMA\nrate \u00b5 = 0.95, and using AdamW optimizer (Loshchilov & Hutter, 2017). We employ DDIM-\nSolver (Song et al., 2023) and a skipping step k = 20 (Luo et al., 2023b) for efficiency. As noted\nin Sec. 3.1 and illustrated in Fig. 3, modifications are made to the original LCM scheduler to ac-\ncommodate differences between the pre-trained PIXART-\u03b1 and Stable Diffusion models. Following\nthe PIXART-\u03b1 approach, we alter the \u03b2t in the diffusion process from a scaled linear to a linear\ncurve, adjusting \u03b2t0 from 0.00085 to 0.0001, and \u03b2tT from 0.012 and to 0.02 at the same time. The\nguidance scale \u03c9fix is set to 4.5, identified as optimal in PIXART-\u03b1. While omitting the Fourier em-\nbedding of \u03c9 in LCM during training, both PIXART-\u03b1 and PIXART-\u03b4 maintain identical structures\nand trainable parameters. This allows us to initialize the consistency function f\u03b8(\u02c6z, \u03c9fix, c, tn)\nwith the same parameters as the teacher diffusion model (PIXART-\u03b1) without compromising per-\nformance. Building on the success of LCM-LoRA (Luo et al., 2023b), PIXART-\u03b4 can further easily\nintegrate LCM-LoRA, enhancing its adaptability for a more diverse range of applications.\n5\nTechnical Report\nText\n\ud835\udc475\nTime t\nBase Block_1\nBase \nBlock_28\nBase \nBlock_14\nBase \nBlock_15\n\u2026\n\u2026\nBase Block_1 \n(trainable copy)\nBase Block_14 \n(trainable copy)\n\u2026\nMLP\nzero linear\nzero linear\nInput\nCondition\nzero linear\n+\nText&Time\nOutput\n(a) PixArt-\u03b1\n\u2026\n(b) ControlNet-UNet\nText\n\ud835\udc475\nTime t\nBase Block_1\nBase \nBlock_28\n\u2026\nMLP\nInput\nOutput\n(a) PixArt-\u03b1\nBase Block_2\n+\nBase Block_1 \n(trainable copy)\nCondition\nzero linear\n+\nText&Time\nzero linear\nBase Block_2 \n(trainable copy)\nBase \nBlock_14\n\u2026\n+\nzero linear\n\u2026\n\u2026\nBase Block_13 \n(trainable copy)\n(c) ControlNet-Transformer\nBase \nBlock_13\nTunable Parameters\nFrozen Parameters\nFigure 4: PIXART-\u03b4 integrated with ControlNet. (b): ControlNet-UNet. Base blocks are categorized\ninto \u201cencoder\u201d and \u201cdecoder\u201d stages. The controlnet structure is applied to each encoder level of\nPIXART-\u03b4, and the output is connected to the decoder stage via skip-connections. (c): ControlNet-\nTransformer. The ControlNet is applied to the first several blocks. The output of each block is added\nto the output of the corresponding frozen block, serving as the input of the next frozen block.\n4\nCONTROLNET IN PIXART-\u03b4\n4.1\nARCHITECTURE\nControlNet, initially tailored for the UNet architecture, employed skip connections to enhance the\nintegration of control signals. The seamless incorporation of ControlNet into Transformer-based\nmodels, exemplified by PIXART-\u03b4, introduces a distinctive challenge. Unlike UNet, Transformers\nlack explicit \u201cencoder\u201d and \u201cdecoder\u201d blocks, making the conventional connection between these\ncomponents inappropriate.\nIn response to this challenge, we propose an innovative approach, ControlNet-Transformer, to ensure\nthe effective integration of ControlNet with Transformers, preserving ControlNet\u2019s effectiveness in\nmanaging control information and high-quality generation of PIXART-\u03b4.\nPIXART-\u03b4 contains 28 Transformer blocks. We replace the original zero-convolution in ControlNet\nwith a zero linear layer, that is, a linear layer with both weight and bias initialized to zero. We\nexplore the following network architectures:\n\u2022 ControlNet-UNet (Zhang et al., 2023). To follow the original ControlNet design, we treat\nthe first 14 blocks as the \u201cencoder\u201d level of PIXART-\u03b4, and the last 14 blocks as the \u201cde-\ncoder\u201d level of PIXART-\u03b4. We use ControlNet to create a trainable copy of the 14 encoding\nblocks. Subsequently, the outputs from these blocks are integrated by addition into the 14\nskip-connections, which link to the last 14 decoder blocks. The network design is shown\nin Fig. 4 (b).\nIt is crucial to note that this adaptation, referred to as ControlNet-UNet, encounters chal-\nlenges due to the absence of explicit \u201cencoder\u201d and \u201cdecoder\u201d stages and skip-connections\nin the original Transformer design. This adaptation departs from the conventional archi-\ntecture of the Transformer, which hampers the effectiveness and results in suboptimal out-\ncomes.\n\u2022 ControlNet-Transformer. To address these challenges, we propose a novel and specifi-\ncally tailored design for Transformers, illustrated in Fig. 4 (c). This innovative approach\naims to seamlessly integrate the ControlNet structure with the inherent characteristics of\n6\nTechnical Report\nTransformer architectures. To achieve this integration, we selectively apply the ControlNet\nstructure to the initial N base blocks. In this context, we generate N trainable copies of the\nfirst N base blocks. The output of ith trainable block is intricately connected to a zero linear\nlayer, and the resulting output is then added to the output of the corresponding ith frozen\nblock. Subsequently, this combined output serves as the input for the subsequent (i + 1)th\nfrozen block. This design adheres to the original data flow of PixArt, and our observa-\ntions underscore the significant enhancement in controllability and performance achieved\nby ControlNet-Transformer. This approach represents a crucial step toward harnessing the\nfull potential of Transformer-based models in such applications. The ablation study of N\nis described in Sec. 4.3, and we use N = 13 as the final model.\n4.2\nEXPERIMENT SETTINGS\nWe use a HED edge map in PIXART-\u03b4 as the condition and conduct an ablation study on 512px\ngeneration, focusing on network architecture variations. Specifically, we conduct ablations on both\nthe ControlNet-UNet and ControlNet-Transformer. Other conditions, such as canny, will be a future\nwork. For ControlNet-Transformer, we ablate the number of copied blocks, including 1, 4, 7, 13,\nand 27. We extract the HED on the internal data, and the gradient accumulation step is set as\n4 following (Zhang et al., 2023)\u2019s advice that recommendation that larger gradient accumulation\nleads to improved results. The optimizer and learning rate are set as the same setting of PIXART-\n\u03b4. All the experiments are conducted on 16 V100 GPUs with 32GB. The batch size per GPU for\nexperiment ControlNet-Transformer (N = 27) is set as 2. For all other experiments, the batch size\nis set as 12. Our training set consists of 3M HED and image pairs.\nHED\nControlNet\n-UNet\nCopy Block=1\nCopy Block=4\nCopy Block=7\nCopy Block=13\nCopy Block=27\nControlNet-Transformer\nCopy Block=14\noil painting of a beautiful woman, funny, kind, caring, nurturing, very motherly, sweet, understanding, compassionate, forgiving.\nClose up of a happy average looking Finnish older person eyes closed in bright apartment living room with a laptop computer, \nfilled with beautiful flowers, youthful vitality, flowers in the ceiling\nthe clown has a face covered in various metal elements, in the style of highly detailed illustrations, dark white and light gray, \nsymmetrical chaos, airbrush art, traincore, carving, gothcore\nFigure 5:\nThe ablation study of ControlNet-UNet and ControlNet-Transformer.\nControlNet-\nTransformer yields much better results than ControlNet-UNet. The controllability of ControlNet-\nTransformer increases as the number of copy blocks increases.\n4.3\nABLATION STUDY\nAs shown in Fig. 5, ControlNet-Transformer generally outperforms, demonstrating faster con-\nvergence and improved overall performance. This superiority can be attributed to the fact that\nControlNet-Transformer\u2019s design aligns seamlessly with the inherent data flow of Transformer ar-\nchitectures. Conversely, ControlNet-UNet introduces a conceptual information flow between the\n7\nTechnical Report\nnon-existing \u201cencoder\u201d and \u201cdecoder\u201d stages, deviating from the Transformer\u2019s natural data pro-\ncessing pattern.\nIn our ablation study concerning the number of copied blocks, we observe that for the majority\nof scenarios, such as scenes and objects, satisfactory results can be achieved with merely N =\n1. However, in challenging edge conditions, such as the outline edge of human faces and bodies,\nperformance tends to improve as N increases. Considering a balance between computational burden\nand performance, we find that N = 13 is the optimal choice in our final design.\n4.4\nCONVERGENCE\nAs described in Fig. 12, we analyze the effect of training steps. The experiment is conducted on\nControlNet-Transformer (N = 13). From our observation, the convergence is very fast, with most\nedges achieving satisfactory results at around 1,000 training steps. Moreover, we note a gradual im-\nprovement in results as the number of training steps increases, particularly noticeable in enhancing\nthe quality of outline edges for human faces and bodies. This observation underscores the efficiency\nand effectiveness of ControlNet-Transformer.\nWe observe a similar \u201csudden converge\u201d phenomenon in our model, as also observed in the original\nControlNet work, where it \u201csuddenly\u201d adapts to the training conditions. Empirical observations\nindicate that this phenomenon typically occurs between 300 to 1,000 steps, with the convergence\nsteps being influenced by the difficulty level of the specified conditions. Simpler edges tend to\nconverge at earlier steps, while more challenging edges require additional steps for convergence.\nAfter \u201csudden converge\u201d, we observe an improvement in details as the number of steps increases.\n100 steps\n200 steps\n300 steps\n400 steps\n500 steps\n600 steps\n700 steps\n800 steps\n900 steps\nOrigin\nHED\nFigure 6: Example of \u201cSudden Converge\u201d during PixArt-ControlNet training. We empirically ob-\nserve it happens before 1000 iterations.\n4.5\n1024PX RESULTS\nBuilding upon the powerful text-to-image generation framework of PixArt, our proposed PixArt-\nControlNet extends these capabilities to produce high-resolution images with a granular level of\ncontrol. This is vividly demonstrated in the detailed visualizations presented in Fig. 9 and Fig. 10.\nUpon closer inspection of these figures, it is apparent that PixArt-ControlNet can exert precise con-\ntrol over the geometric composition of the resultant images, achieving fidelity down to individual\nstrands of hair.\n5\nCONCLUSION\nIn this report, we present PIXART-\u03b4, a better text-to-image generation model integrating Latent Con-\nsistency Models (LCM) to achieve 4-step sampling acceleration while maintaining high quality. We\nalso propose Transformer-based ControlNet, a specialized design tailored for Transformer architec-\nture, enabling precise control over generated images. Through extensive experiments, we demon-\nstrate PIXART-\u03b4\u2019s faster sampling and ControlNet-Transformer\u2019s effectiveness in high-resolution\nand controlled image generation. Our model can generate high-quality 1024px and fine-grained\ncontrollable images in 1 second. PIXART-\u03b4 pushes the state-of-the-art in faster and more controlled\nimage generation, unlocking new capabilities for real-time applications.\nAcknowledgement.\nWe extend our sincere gratitude to Patrick von Platen and Suraj Patil from\nHugging Face for their invaluable support and contributions to this work.\n8\nTechnical Report\nPixArt-\u03b4 (4 step)\nSDXL-LCM (4 step)\nPixArt-\u03b4 (4 step)\nPixArt-\u03b1 (14 step)\ncherrypick \nscientist\ntransparent \nduck made in \nglass is flying in \nthe sky\nAn astronaut \ncapybara \nfloating \ngracefully beside \na spaceship, \nwith the Earth's \nblue glow in the \nbackground\nPixel Art of \nLeonardo da \nVinci's Last \nSupper Painting, \n8 bit\nFigure 7: Examples of generated outputs. In the top half, the comparison is between PIXART-\u03b4 and\nSDXL-LCM, with 4 sampling steps. In the bottom half, the comparison involves PIXART-\u03b4 and\nPIXART-\u03b1 (teacher model, using DPM-Solver with 14 steps).\n9\nTechnical Report\nIter 100\nA photo of beautiful mountain with realistic sunset and blue lake, highly detailed, masterpiece\nAstronaut in a jungle, cold color palette, muted colors, detailed, 8k\nIter 300\nIter 1000\nIter 2500\nIter 5000\nportrait photo of a girl, photograph, highly detailed face, depth of field\nSelf-portrait oil painting, a beautiful cyborg with golden hair, 8k\ndog\nFigure 8: The 4-step inference samples generated by PIXART-\u03b4 demonstrate fast convergence in\nLCD training on 2 V100 GPUs with a total batch size of 24. Remarkably, the complete fine-\ntuning process requires less than 24GB of GPU memory, making it feasible on most contemporary\nconsumer-grade GPUs.\n10\nTechnical Report\nHigh detail PixArt-ControlNet (1024px)\nFigure 9: High-resolution and fine-grained controllable image generation. The output is generated\nwith the prompt \u201cthe map of the final fantasy game\u2019s main island, in the style of hirohiko araki,\nraymond swanland, monumental murals, mosaics, naturalistic rendering, vorticism, use of earth\ntones.\u201d\n11\nTechnical Report\nHigh detail PixArt-ControlNet (1024px)\nFigure 10: High-resolution and fine-grained controllable image generation. The output is generated\nwith the prompt \u201cMulticultural beauty. Women of different ethnicity - Caucasian, African, Asian\nand Indian.\u201d\n12\nTechnical Report\nstyle Picasso, a boat on the river and some buildings \npainting of a port, colorful vehicles and buildings\nIsometric clean pixel art image of a hidden luxury island \ndrift game punk cyber style neon top view drifting car\ncolorful psychedelic mushrooms in a forest at night\nA palace entirely made of glass, anime style\nAncient girls, Look like about 20 years old\nfarm workers in the style of photorealistic detailing\ntwo towers mordor, terrifying, fantastic dark, foggy\nA glossy beautiful young girl, diverse flowers on body\nFigure 11: More examples of our PixArt-ControlNet generated images.\n13\nTechnical Report\nHED\n1K steps\n3K steps\n8K steps\n18K steps\n35K steps\nphoto of 4 smiling happy modern wives in the foreground, 4 men in the background Pastel Art\nOrigin\nNo prompt\nWith prompt\nchild africa, double exposure\nyoung woman is sitting on the sunned floor, in the style of miwa komatsu, dark blue and dark black, translucent color, \nmamiya rb67, asaf hanuka, exotic, katsushika \u014di\nAntique rare Asian, sticker style white background\nthe garden of eden ih the bible, religious lithograph style\nAsian-inspired, 1920s aesthetic, featuring a moon goddess sitting on a crescent moon. The moon goddess is portrayed \nwith her dress and hair flowing gracefully in the breeze, giving a sense of ethereal beauty. She sits on the crescent \nmoon, radiating a serene and captivating presence. etc.\nsurreal painting of a futuristic typewriter, sitting on a desk, abstract acrylic, light and shadow, minimal\nCapture the idea of humans establishing colonies on other planets or moons, with spaceports and habitats dotting \nextraterrestrial landscapes.\nFigure 12: The influence of training steps. The convergence is fast, with details progressively im-\nproving and aligning more closely with the HED edge map as the training steps increase.\n14\nTechnical Report\nREFERENCES\nJunsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James\nKwok, Ping Luo, Huchuan Lu, et al. Pixart-\u03b1: Fast training of diffusion transformer for photore-\nalistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023.\nTing Chen.\nOn the importance of noise scheduling for diffusion models.\narXiv preprint\narXiv:2301.10972, 2023.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-\nerarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,\npp. 248\u2013255. Ieee, 2009.\nEmiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for\nhigh resolution images. arXiv preprint arXiv:2301.11093, 2023.\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,\net al. Lora: Low-rank adaptation of large language models. In ICLR, 2021.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In arXiv, 2017.\nSimian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthe-\nsizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023a.\nSimian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick von Platen, Apolin\u00b4ario Passos, Longbo\nHuang, Jian Li, and Hang Zhao. Lcm-lora: A universal stable-diffusion acceleration module.\narXiv preprint arXiv:2311.05556, 2023b.\nDustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe\nPenna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image\nsynthesis. In arXiv, 2023.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models. In CVPR, 2022.\nYang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint\narXiv:2303.01469, 2023.\nPatrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Ra-\nsul, Mishig Davaadorj, and Thomas Wolf.\nDiffusers:\nState-of-the-art diffusion mod-\nels, 2023.\nURL https://huggingface.co/docs/diffusers/main/en/api/\npipelines/pixart#inference-with-under-8gb-gpu-vram?\nLvmin Zhang, Anyi Rao, and Maneesh Agrawala.\nAdding conditional control to text-to-image\ndiffusion models, 2023.\n15\n"
  },
  {
    "title": "InseRF: Text-Driven Generative Object Insertion in Neural 3D Scenes",
    "link": "https://arxiv.org/pdf/2401.05335.pdf",
    "upvote": "24",
    "text": "InseRF: Text-Driven Generative Object Insertion in Neural 3D Scenes\nMohamad Shahbazi1,2\u2217\u2020\nLiesbeth Claessens1\u2217\nMichael Niemeyer2\nEdo Collins2\nAlessio Tonioni2\nLuc Van Gool1\nFederico Tombari2\n1ETH Z\u00a8urich\n2Google Z\u00a8urich\nA garden gnome\nPut a garden gnome \non the grass\n3D Generative Object Insertion\nMethod inputs\n(b)    InseRF (ours)\n(a)    Instruct-NeRF2NeRF\n3D scene \nText prompt\nText prompt + \nReference bounding box\nFigure 1. Generative insertion of objects in 3D scenes: while Instruct-NeRF2NeRF allows for altering the overall style of scenes or\nperforming edits with a strong positional prior (such as placing a mustache on a face), it fails at inserting objects in arbitrary locations\ndue to the multiview inconsistency of the edits. Our method, in contrast, is multiview consistent by design and can insert new objects in\nuser-specified locations\nAbstract\nWe introduce InseRF, a novel method for generative ob-\nject insertion in the NeRF reconstructions of 3D scenes.\nBased on a user-provided textual description and a 2D\nbounding box in a reference viewpoint, InseRF generates\nnew objects in 3D scenes. Recently, methods for 3D scene\nediting have been profoundly transformed, owing to the use\nof strong priors of text-to-image diffusion models in 3D gen-\nerative modeling. Existing methods are mostly effective in\nediting 3D scenes via style and appearance changes or re-\nmoving existing objects. Generating new objects, however,\nremains a challenge for such methods, which we address in\n* Equal contribution.\n\u2020 Work was mainly done during an internship at Google.\nthis study. Specifically, we propose grounding the 3D object\ninsertion to a 2D object insertion in a reference view of the\nscene. The 2D edit is then lifted to 3D using a single-view\nobject reconstruction method. The reconstructed object is\nthen inserted into the scene, guided by the priors of monoc-\nular depth estimation methods. We evaluate our method on\nvarious 3D scenes and provide an in-depth analysis of the\nproposed components. Our experiments with generative in-\nsertion of objects in several 3D scenes indicate the effec-\ntiveness of our method compared to the existing methods.\nInseRF is capable of controllable and 3D-consistent object\ninsertion without requiring explicit 3D information as input.\nPlease visit our project page at https://mohamad-\nshahbazi.github.io/inserf.\nCorrespondence: Mohamad Shahbazi (mshahbazi@vision.ethz.ch) .\n1\narXiv:2401.05335v1  [cs.CV]  10 Jan 2024\n1. Introduction\nRecent advances in the areas of novel view synthesis and\ngenerative modeling have led to substantial progress in\nmethods for the generation and manipulation of 3D assets\nand scenes. Diffusion models [12, 33] and their integra-\ntion with neural reconstruction methods, such as neural ra-\ndiance fields (NeRFs) [20], have enabled the development\nof powerful 3D generative models for various applications,\nincluding text-to-3D [14, 28, 40], single-image-to-3D [15\u2013\n18, 29], 3D shape texturing [32], and 3D editing [10].\nIn the particular case of 3D scene editing, recent meth-\nods have shown remarkable promise in modifying the style\nand appearance of real-world scene representations based\non textual and spatial guidance. Currently, models that are\ncapable of direct 3D generation and editing are mainly lim-\nited to simple and object-centric scenes [3, 5, 16, 25]. As\na result, for more complex scenes, the majority of the re-\ncent editing methods rely on performing edits on different\nviews of the scenes using 2D editing models. One of the\nmost prominent works in 3D scene editing is the recently\nproposed Instruct-NeRF2NeRF [10], an iterative method\nthat performs multi-view edits on the NeRF reconstruction\nof 3D scenes from textual instructions. Although achiev-\ning impressive results, Instruct-NeRF2NeRF is mainly lim-\nited to editing the style and appearance of scenes. When\nprompted to with localized edits or geometry manipulations\n(such as object removal or insertion) at specified locations,\nInstruct-NeRF2NeRF often fails to perform the desired ed-\nits. This is mainly due to the 3D inconsistency of 2D edits\nacross viewpoints and the lack of proper spatial control.\nRecent works have aimed at the 3D-consistent [9, 46]\nand localized editing [22, 38, 46] of 3D scenes. In addition,\nseveral studies have specifically tackled object removal and\ninpainting in 3D scene representations [21, 23, 39, 41, 42].\nHowever, generating and inserting new objects in scenes in\na 3D-consistent way remains an open problem and is mainly\nlimited to cases where edits are strongly constrained by spa-\ntial priors (e.g. putting a hat on a head or a mustache on a\nface). Therefore, in this work, we specifically focus on gen-\nerative object insertion in 3D scenes, in a way that is consis-\ntent across multiple views and placed in arbitrary positions.\nGenerative object insertion in 3D scenes using 2D gener-\native models is a particularly challenging task, as it requires\n3D-consistent generation and placement of objects in differ-\nent viewpoints. A simplistic approach is to separately gen-\nerate the desired objects using 3D shape generation mod-\nels [28, 40] and insert them into the scene using 3D spatial\ninformation. However, such an approach requires the ac-\ncurate location, orientation, and scale of the object in 3D,\na non-trivial requirement, especially when in contact with\nother objects in the scene. Moreover, scene-independent\ngeneration of the objects can lead to a mismatch between\nthe style and appearance of the scene and the inserted ob-\njects. In this work, we propose a method capable of scene-\naware generation and insertion of objects in 3D scenes using\nthe textual description of the objects and a single-view 2D\nbounding box as spatial guidance.\nTo circumvent multi-view inconsistencies in appear-\nance and location, the scene-independent generation, and\nthe need for explicit 3D spatial information, we propose\ngrounding the 3D insertion by a 2D view of the object in-\nserted in one reference view of the scene. Given a 3D re-\nconstruction of the scene, we first render a reference view.\nThen, conditioned on a text prompt and a 2D bounding\nbox, we use an image editing method to add the target ob-\nject in the reference view.\nThe generated object is then\nlifted to 3D using a single-view-to-3D object reconstruc-\ntion method [15\u201318, 29]. To place the object in 3D, we pro-\npose using the estimated depth of the object in the reference\nview. After inserting the object in the scene, we perform an\noptional refinement of the fused scene and objects using the\nproposed method in Instruct-NeRF2NeRF [10].\nTo evaluate the proposed method, we apply our method\nto several 3D scenes. Our experiments indicate the abil-\nity of the proposed method to insert diverse objects in 3D\nscenes without the need for explicit 3D spatial guidance. To\nsummarize our contributions:\n\u2022 We address the task of consistent generative object inser-\ntion in 3D scenes based on a textual description and a\nsingle-view 2D bounding box, which is beyond the capa-\nbility of the existing 3D scene editing methods\n\u2022 We propose a novel method, based on grounding the in-\nsertion using a reference 2D edit, which is capable of 3D-\nconsistent object insertion without requiring explicit in-\nformation for the 3D placement.\n\u2022 Through our experiments and visualizations, we show the\nadvantage of the proposed method in generative object\ninsertion in comparison to the existing baselines.\n2. Related Works\nLanguage-based 3D scene editing: 3D scene editing has\nrecently undergone a considerable transformation by in-\ncorporating the strong priors of 2D text-conditioned diffu-\nsion models into 3D generative modeling [9, 10, 22, 26,\n38, 43, 46]. Instruct-NeRF2NeRF [10] proposes an iter-\native method for 3D scene editing, where different view-\npoints of the scene are edited using a text-based 2D editing\nmodel and used to fine-tune the scene\u2019s NeRF representa-\ntion. Although highly effective with modifying the exist-\ning content, Instruct-NeRF2NeRF often struggles with 3D\nconsistent and localized edits, especially when instructed\nto remove objects or create new ones in the scene [9, 39].\nTo address the view consistency of edits, ViCA-NeRF [9]\nproposes a method based on a viewpoint-correspondence\nregularization and a strategy to align the latent space of\nedited and unedited viewpoints. DreamEditor [46] tackles\n2\nthe 3D consistency by adapting the 2D diffusion model to\nthe multi-view images of the scene using DreamBooth [34].\nDreamEditor additionally identifies a 3D region of interest\nfor localized editing of an existing object based on text-\nimage semantic similarity. The method in [22] addresses lo-\ncalized editing differently by obtaining a 3D relevance field\nfor the edits based on the discrepancy between the predic-\ntions of the diffusion model with and without instruction\nconditioning. These methods, despite the improvements,\nremain limited in their ability to generate new objects, of-\nten struggling with cases where a strong spatial prior for the\nplacement of the object does not exist.\nRemoving objects from 3D scenes: another direction re-\ncently explored in the area of 3D scene editing is 3D-\nconsistent removal and inpainting of objects in the scenes.\nSome studies assume having multi-view masks of the target\nobject [21, 41]. These multi-view masks, along with other\nstrategies are used to determine the regions to inpaint in\ndifferent rendered viewpoints. Other studies assume user-\nprovided single-view annotations of the objects and propose\napproaches to automatically obtain multi-view masks from\nthe reference one [23, 39, 42]. However, such approaches\nfor extracting multi-view masks do not transfer to the task\nof object insertion, as they rely on the assumption that the\nobjects already exist in the scene.\nGenerative object insertion: In contrast to scene styliza-\ntion and object removal, generating objects in 3D scenes\nis not well-explored in the existing works. The inpainting\nmethod proposed in [21], although mainly designed and\nevaluated for object removal, has been showcased for ex-\namples of object insertion as well. To do so, the authors as-\nsume multi-view masks of the object are provided, and they\npropose a method to propagate a single-view inpainting to\nother viewpoints. However, in addition to requiring multi-\nview masks as input, the proposed method is mainly limited\nto forward-facing scenes [21]. FocalDreamer [13] is a con-\ncurrent work proposed for adding editable parts to a base\n3D shape. Provided with a text prompt and the rough 3D\nplacement of the target edits, FocalDreamer applies score\ndistillation [28] to add the desired parts to the base shape.\nAlthough achieving compelling results, FocalDreamer re-\nquires user-provided 3D regions (rotation, translation, and\nscale), and its generalization beyond base shapes to com-\nplex 3D scenes is not investigated. Language-driven Ob-\nject Fusion [37] is another concurrent work that aims at\nfusing an existing or generated foreground object with a\nbackground 3D scene. The authors first adopt a 2D dif-\nfusion model for view synthesis from the scene and the ob-\nject using DreamBooth [34]. Then, conditioned on a user-\nprovided 3D bounding box, the authors propose a pose-\nconditioned dataset update strategy for the training of scene\nNeRF containing the object. The proposed fusion strategy\nrequires users to provide an exact 3D bounding box. In con-\ntrast to the existing language-driven object insertion meth-\nods, our approach works well with both forward-facing and\n360 scenes, and it only requires a rough 2D bounding box\nfrom one rendered view of the scene, making it more suit-\nable for real-world applications.\n3. Method\nOur method takes as input a NeRF reconstruction of a 3D\nscene, a textual description of the target object to be in-\nserted, and a 2D bounding box in a reference rendered view\nof the scene. As output, our method returns a NeRF recon-\nstruction of the same scene containing the generated target\n3D object placed in a location guided by the 2D bounding\nbox. It is noteworthy that our method only requires a rough\nbounding box, as we rely on the priors of the diffusion mod-\nels for the exact 2D positioning.\nThe proposed method consists of five main steps: 1) a\n2D view of the target object is created in a chosen reference\nview of the scene based on a text prompt and a 2D bounding\nbox; 2) a 3D object NeRF is reconstructed from the gener-\nated 2D view in the reference image; 3) the 3D placement of\nthe object in the scene is estimated with the help of monoc-\nular depth estimation; 4) the object and scene NeRFs are\nfused into a single scene containing the object in the esti-\nmated placement; 5) optionally, a refinement step is applied\nto the fused 3D representation to improve the insertion fur-\nther. Fig. 2 shows an overview of the proposed pipeline. In\nthe following, we discuss each step in more detail.\n3.1. Preliminaries\nDiffusion Models Diffusion models are a type of genera-\ntive model that maps Gaussian noise to highly realistic and\ndiverse samples. They consist of (1) a forward process that\nmaps data samples x0 to noise xT , and (2) a backward pro-\ncess that creates data samples from noise.\nThe forward process consists of T steps t \u2208 [0, T \u2212 1]:\nq(xt+1|xt) = N(xt|\np\n1 \u2212 \u03b2txt\u22121, \u03b2tI),\n(1)\nwith variances \u03b2t chosen such that the noise xT \u223c N(0, I).\nThe backward process, which is used to generate data\nsamples from Gaussian noise and optionally an additional\nconditioning signal, has the following shape:\nq(xt\u22121|xt) = N(xt\u22121 | \u00b5\u03b8(xt, t, c),\nX\n\u03b8(xt, t, c)), (2)\nwhere the parameters of the backward/denoising distribu-\ntions are predicted by a U-Net, whose weights \u03b8 are op-\ntimized by increasing the likelihood of the data samples.\nDiffusion models can be conditioned on different types of\nsignals, such as images or text, as well as masks, and can be\nextended for different tasks, such as 2D editing [4, 11, 45]\nand inpainting [1, 19].\n3\nAnnotated\nReference View\n2D editing \nmethod     \nEdited \nReference View\nSingle Image to \n3D method\nObject NeRF \nDepth \nEstimation\n3D Object \nInsertion\nUpdated Scene NeRF \nRefinement\nA cup\nFigure 2. Overview of the proposed method. Given a single reference view annotated with a 2D bounding box and a text prompt\ndescribing the object to be inserted, a 2D edit is generated portraying a view of the object. This 2D edit is then warped to a 3D model of the\nobject and placed into the scene using the procedure described in section 3.4. After the 3D placement, the object and scene representations\nare fused as described in section 3.5. Finally, an optional refinement can be performed to further improve the appearance.\nNeural Radiance Fields NeRFs are a novel view synthe-\nsis method trained on a set of posed images by minimiz-\ning the photometric loss between ground truth and rendered\npixels. A key aspect of NeRF is that pixel colors are not\npredicted directly. Instead, the density \u03c3 and color c at 3D\npoints in space are predicted by a neural function f\u03d5. Us-\ning these predictions, the pixel color corresponding to a ray\n\u20d7r = (\u20d7o, \u20d7d) with origin \u20d7o and viewing direction \u20d7d can be\ncomposed through volumetric rendering. To do so, a set of\npoints along the ray ti = \u20d7o + t\u20d7d is sampled, splitting the\nray into a set of intervals \u03b4i = (ti, ti+1]. The pixel color of\nthe ray can then be composed as:\nC(r) \u2248\nN\nX\ni=1\nwici,\n(3)\nwi = Ti(1 \u2212 exp (\u2212\u03c3i\u03b4i)),\n(4)\nTi = exp (\u2212\ni\u22121\nX\nj=1\n\u03c3j\u03b4j).\n(5)\nIn the above equations:\n(\u03c3i, ci) = f (\u03b3 (ti) ; \u03d5) ,\n(6)\nwhere the positional encoding function \u03b3 and the location\nof the samples ti depend on the NeRF variant being used.\n3.2. Editing the Reference View\nOur editing pipeline starts by choosing one rendered view\nof the scene as the reference and inserting a 2D view of\nthe target object based on a user-provided text prompt and\na 2D bounding box. The reference view is used to ground\nthe 3D insertion by providing a reference appearance and\nlocation. Through empirical experiments, we find the ad-\nditional use of the bounding box important, as the exist-\ning text-guided editing methods often struggle with local-\nized 2D object insertions when only receiving spatial guid-\nance from text prompts [27, 44, 45]. To ensure localized\n2D insertion within the input bounding box, we opt for a\nmask-conditioned inpainting method as our 2D generative\nmodel. Specifically, we choose Imagen [35], a powerful\ntext-to-image diffusion model, and further adapt it to mask-\nconditioning by using RePaint [19], a method for mask-\nconditioned inpainting with diffusion models.\n3.3. Single-View Object Reconstruction\nAfter obtaining the reference edit, we extract the 2D view of\nthe object generated within the bounding box and create a\n3D reconstruction of it. To do so, we propose exploiting the\nrecent paradigm of single-view object reconstruction using\n3D-aware diffusion models [15\u201318, 29]. Such reconstruc-\ntion methods are typically trained on large-scale 3D shape\ndatasets, such as Objaverse [8] and therefore contain strong\npriors over the geometry and appearance of 3D objects. We\nuse the recently proposed SyncDreamer [17] for our object\nreconstruction, as it offers a good trade-off between recon-\nstruction quality and efficiency.\n3.4. 3D Placement\nDepth Estimation: The reference 2D bounding box con-\nstrains the 3D location of the target object to a frustum\nin the scene. To determine the location of the object in\nthe 3D frustum, we propose using the prior from monoc-\nular depth estimation methods. We apply MiDaS [31] on\nthe edited reference image to estimate the depth of the ob-\nject with respect to the reference camera. As MiDaS pro-\nvides non-metric depth measurements, we perform an ex-\ntra depth alignment between the estimated depth of the\nedited reference view and the reference depth rendered from\n4\nthe scene NeRF by estimating a global scale and shift be-\ntween the reference and estimated depth maps. Specifically,\nto make the alignment more accurate around the object\narea, we estimate the alignment parameters using weighted\nleast-square estimation, where measurements are inversely\nweighted based on their distance to the center of the object\nbounding box (details are provided in the supplementary).\nAfter the alignment, we use the depth of the center pixel d\nin the object bounding box as a rough estimate of the ob-\nject\u2019s center in the frustum, which will be further optimized\nin the next step.\nScale and Distance Optimization: Using the estimated\ndepth d as the distance of the object\u2019s center from the refer-\nence camera helps with resolving the scale-depth ambigu-\nity of the target 3D object, but it is not accurate enough\nto closely match the original edit.\nAdditionally, single-\nview reconstruction methods like SyncDreamer (discussed\nin Sec. 3.3) are trained to generate multi-view images from\nfixed camera distance r\u2032 and focal length f \u2032. In general,\nas these parameters are different from those of the refer-\nence camera, the reconstructed object NeRF appears with\na different scale in the reference view once placed at the\nestimated distance. Therefore, we propose an additional\noptimization step for the scale and the distance of the ob-\nject with two constraints: 1) the object must reside at the\nestimated depth; 2) the rendered view of the object in the\nreference camera should match the initial edit in scale and\nappearance. To ensure a proper initial state for the optimiza-\ntion we initialize our scale s and object\u2019s distance as:\ns0 = d\nf \u2032 .r\u2032\nf\n(7)\nr0 = s0.l + d\n(8)\nwhere s0 and r0 are the initial object scale and distance, and\nl is the distance of the 3D point corresponding to the center\nof the bounding box from the origin of the object NeRF\u2019s\ncoordinate system. Given a 3D point \u20d7P \u2032 in the original ob-\nject NeRF\u2019s coordinate system, the corresponding 3D point\n\u20d7P in the scaled coordinate system is obtained as:\n\u20d7P = s \u20d7P \u2032\n(9)\nTo obtain the optimized scale s\u2217 and distance r\u2217, we op-\ntimize the Mean Square Error (MSE) between the ground-\ntruth 2D edit IG and the image IR rendered using the new\nparameters:\nr\u2217, s\u2217 = arg min\nr,s\n||IG \u2212 IR||2\n(10)\nFig. 5 in our ablation study visualizes the effect of scale and\ndistance optimization.\nRotation and Translation: After obtaining the scale and\ndistance of the object from the reference camera, we pro-\nceed to estimate the placement of the object in the scene\nby estimating its 3D rotation and translation with respect to\nthe camera coordinate system. The origin of the object in\nthe scene\u2019s coordinate system is obtained as the point along\nthe ray from the reference camera center passing through\nthe center of the bounding box at the desired distance. To\nobtain the 3D rotation, we align the x-axis of the object\u2019s\ncoordinate system to the vector pointing to the reference\ncamera center from the object\u2019s origin.\n3.5. Scene and Object Fusion\nOnce the location and the orientation of the 3D object in\nthe scene are known, we fuse the NeRF representations of\nthe object and scene to be able to render multi-view images\nof the scene containing the target object. Given a view-\npoint, we transform the rays to the coordinate systems of\nthe scene and the object. Each NeRF representation is ap-\nplied to the corresponding transformed rays to predict the\ncolor and density of the object and scene at each 3D posi-\ntion. To render a viewpoint using the predictions of the two\nNeRFs, we follow the proposed strategy in [36], where the\ndensity \u03c3i and color ci at each 3D point i across a ray in the\nfused representation are defined as:\n\u03c3i = \u03c3s\ni + \u03c3o\ni ,\n(11)\nci = \u03c3s\ni cs\ni + \u03c3o\ni co\ni\n\u03c3s\ni + \u03c3o\ni\n,\n(12)\nwhere \u03c3s\ni and cs\ni are the density and the color of the cor-\nresponding sample the scene NeRF, and \u03c3o\ni and co\ni are those\nof the one in the object NeRF. To be able to use such formu-\nlation in our method for merging the object and the scene, it\nis crucial to take the scaling of the object\u2019s coordinate sys-\ntem into account. Going back to the approximation of the\nvolumetric rendering integration, discussed in Sec. 3.1, in\nequation 4, \u03c3i\u03b4i can be seen as the Riemann approximation\nof the area under the density curve across the ray at interval\n\u03b4i. Simply replacing \u03c3i in equation 4 with the definition in\nequation 11 results in an inaccurate estimation of the area\nunder the density curve for the merged representation, as\nthe intervals between every two consecutive samples across\nthe rays are not equal between scene and object coordinate\nsystems due to the scaling of the object coordinate system\n(discussed in section 3.4):\n\u03b4s\ni = s\u2217 \u00b7 \u03b4o\ni ,\n(13)\n\u03b4s\ni and \u03b4o\ni are the intervals in the scene and object NeRFs,\nrespectively, and s\u2217 is the optimized scale obtained in sec-\ntion 3.4. To compensate for the scaling of the intervals, we\nmodify equations 11 and 12 as:\n\u03c3i = \u03c3s\ni + \u03c3o\ni\ns\u2217\n(14)\nci = \u03c3s\ni cs\ni + \u03c3o\ni co\ni /s\u2217\n\u03c3s\ni + \u03c3o\ni /s\u2217\n,\n(15)\n5\nAs we also show in Fig. 6 in our ablation study, the proposed\nmodification is necessary for the correct rendering of the\nfused NeRFs.\n3.6. Refinement\nAs the final step in our pipeline, we optionally refine the\nfused scene and object to improve upon the imperfections\nintroduced in the initial reference edit or the single-view re-\nconstruction. To do so, we adapt the iterative refinement\nproposed in Instruct-NeRF2NeRF [10] to our setup. First,\na set of images is rendered from different viewpoints of the\nfused NeRF. Then the sampled views are further refined us-\ning the 2D diffusion model and added to the optimization of\nthe NeRF consecutively. An important difference between\nour refinement and Instruct-NeRF2NeRF is that we can ob-\ntain multi-view object masks for free from the inserted ob-\nject to restrict the refinements to the object region. Addi-\ntionally, in contrast to Instruct-NeRF2NeRF, as the location\nof the object is known in our refinement step, we adjust our\ncamera trajectory to revolve around the object. We also ar-\nrange the sampled viewpoints such that more frontal views\nare edited and used for NeRF optimization earlier. We find\nsuch adjustments to increase the capability of our refine-\nment step. The effect of the proposed refinement is visual-\nized in Fig. 7 of our ablation study.\n4. Experiments\nIn this section, we explain our training and evaluation pro-\ncedures in more detail. Moreover, we provide the results of\nour evaluation and comparison with baselines. Finally, we\nprovide an ablation study and analysis of different compo-\nnents of the proposed method.\n4.1. Experimental Details\nImplementation Details: For the NeRF representation of\nobjects and scenes, we use MipNeRF-360 [2] adapted to the\nhash grids introduced in Instant-NGP [24]. For a more ex-\nhaustive description of the implementation of our method,\nwe kindly refer the reader to the supplementary materials.\nDatasets: We evaluate our method on a subset of real\nindoor and outdoor scenes from datasets proposed in\nMipNeRF-360 [2] and Instruct-NeRF2NeRF [10].\nBaselines: In our evaluation, we compare the proposed\nmethod to the following baselines:\n\u2022 Instruct-NeRF2NeRF (I-N2N) [10]: We choose I-N2N\nas our main baseline, as it is a recent and well-established\nmethod for 3D scene editing.\n\u2022 Multi-View Inpainting (MV-Inpainting): We propose\nanother baseline that follows the refinement strategy in\nInstruct-NeRF2NeRF, but is additionally provided with\nExisting works more related to our method mostly require extra inputs\n(e.g. 3D boxes) or do not currently provide an implementation [13, 21, 37].\naccurate multi-view masks for the target object.\nIt is\nworth emphasizing that, in contrast, our methods only re-\nquire a rough 2D bounding box in a single reference view.\nMore details on the implementation of our baselines are pro-\nvided in the supplementary material.\n4.2. Visual Results and Comparisons\nTo assess the ability of the proposed method in generative\nobject insertion, we provide visual examples of applying\nour method to different 3D scenes in Fig. 3. As shown, our\nmethod can insert 3D-consistent objects in the scenes. Es-\npecially noteworthy is the ability of our method to insert\nobjects on different surfaces, a challenging task in the ab-\nsence of exact 3D placement information.\nIn Fig. 4, we provide a visual comparison with the base-\nlines discussed in Sec. 4.1. Attempting to insert new objects\nin the scene using I-N2N often results in global changes in\nthe scene and modifying existing objects toward the target\ninstead of creating new ones (note how I-N2N changes the\nLego truck in 4a toward a mug and the items on the kitchen\ncounter 4b toward a panettone). Using multi-view masks in\nthe MV-Inpainting baseline helps with limiting the 2D edits\nto the object region and provides strong spatial guidance.\nHowever, 2D edits remain inconsistent from different view-\npoints. Therefore, using the edits to optimize the NeRF rep-\nresentation results in 3D floaters and failure to generate the\ntarget object in a 3D consistent way. In contrast, our method\nis capable of localized modification of the scene and of in-\nserting 3D-consistent objects in 3D using only one single-\nview bounding box as spatial guidance. More visual results\nare provided in the supplementary material.\n4.3. Ablation and Analysis\nScale and radius optimization: In Fig. 5, we provide a vi-\nsual ablation demonstrating the importance of the scale and\nradius optimization proposed in 3.4, where we compare the\nplacement of the object in the scene using the initial estima-\ntion according to Eq. 7 and 8 and placement with the extra\noptimization. As can be seen, the proposed initial estima-\ntion would only result in a rough and inaccurate placement\nof the object. With the proposed optimization, our method\ncan insert objects with the scale and depth matching those\nof the reference view.\nObject density scaling:\nIn Sec. 3.5, we proposed an\nadapted strategy for fusing the NeRF representations of the\nscene and the object that takes the scaling of the object\ninto account. In Fig. 6, we visualize the importance of our\nadapted formulation for accurate rendering of the objects\ninserted in the scene.\nRefinement: in Sec. 3.6, we proposed an optional refine-\nment step after inserting the objects in the scenes. Fig 7\nshows examples of the effect of the refinement. As shown,\nthe additional refinement can improve some of the details of\n6\nA panettone\nA mug\nA pouffe\nA cup\nEdited Neural Scene Representation\nReference View\nReference Edit\nA duck\nFigure 3. Examples of using InseRF to insert an object into the neural representation of different indoor and outdoor scenes.\nOriginal Scene\nI-N2N\nMV-Inpainting\nInseRF (ours)\n(a) A mug on the table\nOriginal Scene\nI-N2N\nMV-Inpainting\nInseRF (ours)\n(b) A panettone on the tray\nFigure 4. Qualitative comparison of object insertion with different methods. I-N2N modifies existing objects instead of inserting a new\nobject, and the inpainting baseline fails to create geometry at the desired location. Our method, in contrast, can insert new 3D-consistent\nobjects at the desired location.\n7\n5\nGround Truth 2D Edit\nNo Scale/Distance \nOptimization\nScale/Distance Optimization\nFigure 5. Visualization of the effect of scale optimisation on ob-\nject insertion. The placement of objects is more realistic and faith-\nful to the original edit when performing scale/distance optimiza-\ntion to improve the alignment.\nNo Density Scaling\nDensity Scaling\nFigure 6. Visualization of the effect of scaling the densities when\nfusing the object and scene representation. When the re-scaling of\nthe object NeRF is not accounted for in the volumetric rendering,\nthe object is not properly displayed in the synthesized views.\nthe inserted objects, such as the lighting and the texture.\n4.4. Limitations and Future Work\nOur method is a general pipeline for generative object inser-\ntion that is built on top of the existing 2D and 3D generative\nmodels and whose parts can be easily swapped. Currently,\nthe performance of our method is limited by the capabilities\nof the underlying generative models, such as the 2D diffu-\nsion model or the single-view object reconstruction method.\nOn the other hand, given our general formulation, future im-\nprovements in such models readily transfer to our pipeline.\nOur method provides spatial control using a single-view\nbounding box, as current 2D editing models struggle with\nthe spatial guidance provided in the text prompts. Explor-\ning methods both capable of localized 2D insertion and text-\nRe\ufb01nement\nRe\ufb01nement\nNo Re\ufb01nement\nRe\ufb01nement\nNo Re\ufb01nement\nRe\ufb01nement\nNo Re\ufb01nement\nFigure 7. Visualization of the effect of refinement on object in-\nsertion. Our refinement step can add additional texture details and\nlighting effects.\nbased spatial guidance can lead to improved performance of\nour whole pipeline. Lastly, integrating the concurrently pro-\nposed view-consistent editing methods [9, 37] and existing\napproaches for scene-consistent shadowing and harmoniza-\ntion (e.g. [6, 7]) with our refinement step may bring further\nimprovements to the quality and realism of the insertions.\n5. Conclusion\nWe introduced InseRF, a method specifically designed for\ngenerative object insertion in 3D scenes. InseRF takes as\ninput a textual description of the desired object, as well as\na 2D bounding box in a single reference viewpoint of the\nscene. Based on the provided inputs, InseRF generates an\nobject in the 3D scene in a 3D consistent way. To do so, In-\nseRF relies on the priors of 2D diffusion models and single-\nview object reconstruction methods. The proposed method\nincludes different steps necessary to integrate such methods\nfor the task of in-scene object generation. Through evalua-\ntions and visualizations on different 3D scenes, we showed\nthe ability of InseRF in the 3D-consistent generation of ob-\njects in the scene without requiring explicit 3D placement\ninformation.\nReferences\n[1] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended\ndiffusion for text-driven editing of natural images. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\n8\nand Pattern Recognition (CVPR), pages 18208\u201318218, 2022.\n3\n[2] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P\nSrinivasan, and Peter Hedman. Mip-nerf 360: Unbounded\nanti-aliased neural radiance fields.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5470\u20135479, 2022. 6\n[3] Miguel Angel Bautista, Pengsheng Guo, Samira Abnar, Wal-\nter Talbott, Alexander Toshev, Zhuoyuan Chen, Laurent\nDinh, Shuangfei Zhai, Hanlin Goh, Daniel Ulbricht, Afshin\nDehghan, and Josh Susskind. Gaudi: A neural architect for\nimmersive 3d scene generation. arXiv, 2022. 2\n[4] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nIn CVPR, 2023. 3, 12\n[5] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki\nNagano, Boxiao Pan, Shalini De Mello, Orazio Gallo,\nLeonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero\nKarras, and Gordon Wetzstein. Efficient geometry-aware 3D\ngenerative adversarial networks. In arXiv, 2021. 2\n[6] Jianqi Chen, Yilan Zhang, Zhengxia Zou, Keyan Chen, and\nZhenwei Shi. Dense pixel-to-pixel harmonization via con-\ntinuous image representation. IEEE Transactions on Circuits\nand Systems for Video Technology, pages 1\u20131, 2023. 8\n[7] Jianqi Chen, Zhengxia Zou, Yilan Zhang, Keyan Chen, and\nZhenwei Shi. Zero-shot image harmonization with genera-\ntive model prior, 2023. 8\n[8] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana\nEhsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:\nA universe of annotated 3d objects.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 13142\u201313153, 2023. 4\n[9] Jiahua Dong and Yu-Xiong Wang.\nViCA-neRF: View-\nconsistency-aware 3d editing of neural radiance fields. In\nThirty-seventh Conference on Neural Information Process-\ning Systems, 2023. 2, 8\n[10] Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander\nHolynski, and Angjoo Kanazawa. Instruct-nerf2nerf: Edit-\ning 3d scenes with instructions.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\n2023. 2, 6, 11\n[11] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-\nage editing with cross attention control. In arXiv preprint\narXiv:2208.01626, 2022. 3\n[12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. arXiv preprint arxiv:2006.11239,\n2020. 2\n[13] Yuhan Li, Yishun Dou, Yue Shi, Yu Lei, Xuanhong Chen, Yi\nZhang, Peng Zhou, and Bingbing Ni. Focaldreamer: Text-\ndriven 3d editing via focal-fusion assembly, 2023. 3, 6\n[14] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\ntext-to-3d content creation. In IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2023. 2\n[15] Minghua Liu,\nChao Xu,\nHaian Jin,\nLinghao Chen,\nMukund Varma T, Zexiang Xu, and Hao Su. One-2-3-45:\nAny single image to 3d mesh in 45 seconds without per-\nshape optimization, 2023. 2, 4\n[16] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot one image to 3d object, 2023. 2\n[17] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie\nLiu, Taku Komura, and Wenping Wang.\nSyncdreamer:\nLearning to generate multiview-consistent images from a\nsingle-view image. arXiv preprint arXiv:2309.03453, 2023.\n4\n[18] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu,\nZhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang,\nMarc Habermann, Christian Theobalt, and Wenping Wang.\nWonder3d: Single image to 3d using cross-domain diffusion,\n2023. 2, 4\n[19] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher\nYu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting\nusing denoising diffusion probabilistic models. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), pages 11461\u201311471, 2022. 3,\n4, 11, 12\n[20] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In ECCV, 2020. 2\n[21] Ashkan Mirzaei, Tristan Aumentado-Armstrong, Marcus A.\nBrubaker, Jonathan Kelly, Alex Levinshtein, Konstantinos G.\nDerpanis, and Igor Gilitschenski. Reference-guided control-\nlable inpainting of neural radiance fields. In ICCV, 2023. 2,\n3, 6\n[22] Ashkan Mirzaei, Tristan Aumentado-Armstrong, Marcus A.\nBrubaker, Jonathan Kelly, Alex Levinshtein, Konstantinos G.\nDerpanis, and Igor Gilitschenski. Watch your steps: Local\nimage and scene editing by text instructions. In arXiv, 2023.\n2, 3\n[23] Ashkan Mirzaei, Tristan Aumentado-Armstrong, Konstanti-\nnos G. Derpanis, Jonathan Kelly, Marcus A. Brubaker, Igor\nGilitschenski, and Alex Levinshtein. SPIn-NeRF: Multiview\nsegmentation and perceptual inpainting with neural radiance\nfields. In CVPR, 2023. 2, 3\n[24] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a mul-\ntiresolution hash encoding. ACM Transactions on Graphics\n(ToG), 41(4):1\u201315, 2022. 6\n[25] Evangelos Ntavelis, Aliaksandr Siarohin, Kyle Olszewski,\nChaoyang Wang, Luc Van Gool, and Sergey Tulyakov. Au-\ntodecoding latent 3d diffusion models, 2023. 2\n[26] Jangho Park, Gihyun Kwon, and Jong Chul Ye.\nEd-nerf:\nEfficient text-guided editing of 3d scene using latent space\nnerf, 2023. 2\n[27] Or Patashnik, Daniel Garibi, Idan Azuri, Hadar Averbuch-\nElor, and Daniel Cohen-Or. Localizing object-level shape\nvariations with text-to-image diffusion models. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision (ICCV), 2023. 4\n9\n[28] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Milden-\nhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv,\n2022. 2, 3\n[29] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren,\nAliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Sko-\nrokhodov, Peter Wonka, Sergey Tulyakov, and Bernard\nGhanem. Magic123: One image to high-quality 3d object\ngeneration using both 2d and 3d diffusion priors.\narXiv\npreprint arXiv:2306.17843, 2023. 2, 4\n[30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable visual\nmodels from natural language supervision. In Proceedings\nof the 38th International Conference on Machine Learning,\npages 8748\u20138763. PMLR, 2021. 11\n[31] Ren\u00b4e Ranftl,\nKatrin Lasinger,\nDavid Hafner,\nKonrad\nSchindler, and Vladlen Koltun. Towards robust monocular\ndepth estimation: Mixing datasets for zero-shot cross-dataset\ntransfer. IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence, 44(3), 2022. 4, 12\n[32] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes,\nand Daniel Cohen-Or. Texture: Text-guided texturing of 3d\nshapes. arXiv preprint arXiv:2305.16213, 2023. 2\n[33] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models, 2021. 2\n[34] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. In arXiv preprint arxiv:2208.12242, 2022. 3\n[35] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour, Raphael Gontijo-Lopes, Burcu Karagol Ayan,\nTim Salimans, Jonathan Ho, David J. Fleet, and Mohammad\nNorouzi. Photorealistic text-to-image diffusion models with\ndeep language understanding. In Advances in Neural Infor-\nmation Processing Systems, 2022. 4, 12\n[36] Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas\nGeiger. Graf: Generative radiance fields for 3d-aware image\nsynthesis.\nIn Advances in Neural Information Processing\nSystems (NeurIPS), 2020. 5\n[37] Ka Chun Shum, Jaeyeon Kim, Binh-Son Hua, Duc Thanh\nNguyen, and Sai-Kit Yeung. Language-driven object fusion\ninto neural radiance fields with pose-conditioned dataset up-\ndates, 2023. 3, 6, 8\n[38] Hyeonseop Song, Seokhun Choi, Hoseok Do, Chul Lee,\nand Taehyeong Kim.\nBlending-nerf:\nText-driven local-\nized editing in neural radiance fields.\nIn Proceedings of\nthe IEEE/CVF International Conference on Computer Vision\n(ICCV), pages 14383\u201314393, 2023. 2\n[39] Dongqing Wang, Tong Zhang, Alaa Abboud, and Sabine\nS\u00a8usstrunk.\nInpaintnerf360:\nText-guided 3d inpainting\non unbounded neural radiance fields.\narXiv preprint\narXiv:2305.15094, 2023. 2, 3\n[40] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and\ndiverse text-to-3d generation with variational score distilla-\ntion. arXiv preprint arXiv:2305.16213, 2023. 2\n[41] Silvan Weder, Guillermo Garcia-Hernando, \u00b4Aron Monsz-\npart, Marc Pollefeys, Gabriel Brostow, Michael Firman, and\nSara Vicente. Removing objects from neural radiance fields.\nIn CVPR, 2023. 2, 3\n[42] Youtan Yin, Zhoujie Fu, Fan Yang, and Guosheng Lin. Or-\nnerf: Object removing from 3d scenes guided by multiview\nsegmentation with neural radiance fields, 2023. 2, 3\n[43] Lu Yu, Wei Xiang, and Kang Han. Edit-diffnerf: Editing 3d\nneural radiance fields using 2d diffusion model, 2023. 2\n[44] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su.\nMagicbrush: A manually annotated dataset for instruction-\nguided image editing. In Advances in Neural Information\nProcessing Systems, 2023. 4\n[45] Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih\nChen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese,\nStefano Ermon, Caiming Xiong, and Ran Xu. Hive: Har-\nnessing human feedback for instructional visual editing.\narXiv preprint arXiv:2303.09618, 2023. 3, 4\n[46] Jingyu Zhuang, Chen Wang, Lingjie Liu, Liang Lin, and\nGuanbin Li. Dreameditor: Text-driven 3d scene editing with\nneural fields. arXiv preprint arXiv:2306.13455, 2023. 2\n10\nAppendix\nIn the appendix, we provide additional visual results, a\nquantitative evaluation of our method, and an in-depth dis-\ncussion of the implementation.\nA. Additional Visual Results\nVisual examples: in Fig. 1 and 3 of the main paper, we pro-\nvided examples of generative object insertion in 3D scenes\nusing our proposed method. Here in Fig. 8, we provide\nmore visual examples showing the ability of our method to\ngenerate objects in 3D scenes.\nComparison to the baseline: in Fig. 4 of the main pa-\nper, we provided visual comparisons between the proposed\nmethod and our baselines (introduced in Sec. 4.1 of the\nmain paper). Fig. 9 here shows more comparisons with the\nbaselines for a better assessment. As depicted, the two com-\npared baselines struggle with creating the target objects in\nthe scene.\nRefinement: in Fig. 7 of the main paper, we provided a vi-\nsual ablation on the impact of the proposed refinement step\nin Sec. 3.6. Here in Fig. 10, we extend the ablation to more\nexamples. As can be seen, the proposed refinement step can\nimprove the texture and details of the inserted objects, re-\nsulting in higher-quality and more realistic insertions. For\nthe details of our refinement step, please refer to Sec. 3.6 of\nthe main paper and Sec. C.6 of this supplementary.\nVideo visualizations: To better visualize the inserted ob-\njects using our method, we additionally provide video vi-\nsualizations in the supplementary files, showing several ex-\namples of our inserted objects, as well as examples of the\nrefinement step.\nB. Quantitative Evaluation\nIn addition to the provided visual evaluations, we pro-\nvide a quantitative evaluation of the proposed method and\nits comparison with our baselines.\nFollowing Instruct-\nNeRF2NeRF [10], we evaluate the methods using three dif-\nferent metrics:\n\u2022 CLIP Text-Image Similarity (Text-Image): the cosine\nsimilarity between the CLIP [30] embeddings of the edit\nprompt (e.g. \u201dA blue cup\u201d) and the images rendered from\ndifferent viewpoints of the edited scene (We exclude the\nviews where the inserted objects are occluded).\n\u2022 Directional Text-Image Similarity (Directional): Given\na textual description of the original scene (e.g. \u201dA kitchen\ncounter\u201d) and an edit prompt describing the scene and the\nedit (e.g. \u201dA kitchen counter with a mug on top\u201d), this\nmetric measures the similarity of the direction of change\nfrom the original scene to the edited one between the im-\nage and text CLIP embeddings.\n\u2022 Temporal Direction Consistency (Temporal): Given\ntwo adjacent rendered viewpoints of original and edited\nscenes, this metric measures how much the change of im-\nage embeddings between the two viewpoints in the edited\nscene is consistent with the one in the original scene.\nWe provide the results of our quantitative evaluation on 8\ndifferent edits (5 different scenes) in Tab. 1. All three met-\nrics are based on Cosine similarity, which ranges from -1\nto 1. We bring the values between 0 to 1 (the higher the\nbetter) for ease of comparison. As depicted, our method\neffectively outperforms the baselines in the three evaluated\nmetrics. It is worth discussing that, although the provided\nmetrics indicate the advantage of our method over the base-\nlines, we refer the readers to the qualitative results for a\nbetter assessment of the evaluated methods. As also high-\nlighted by Instruct-NeRF2NeRF [10]., the metrics above,\nalthough helpful, do not fully capture the effectiveness of\nmethods in 3D scene editing. Exploring alternative metrics\nthat better measure such edits would be an important direc-\ntion for future studies.\nC. Implementation Details\nC.1. Inpainting with RePaint\nAs mentioned in Sec. 3.2 of the main paper, to generate a 2D\nview of the target object in the reference view, we condition\nour diffusion model on a bounding box using RePaint [19].\nRepaint is a training-free inpainting method for pretrained\ndiffusion models that is capable of adding new content to an\nimage in the regions specified by an arbitrary binary mask.\nRepaint primarily consists of 2 components: 1.) mask con-\nditioning and 2.) re-sampling.\nTo enable mask conditioning, in every step t of the diffu-\nsion process, RePaint applies a mask-based blending to the\noutput xt\u22121 as follows:\nxt\u22121 = (1 \u2212 M) \u2299 xknown\nt\u22121\n+ M \u2299 xunknown\nt\u22121\n(16)\nwhere xknown\nt\u22121\nis sampled using known pixels in the\ngiven image, xunknown\nt\u22121\nis sampled from the model given the\nprevious iteration xt, and M is the binary mask. \u2299 denotes\nelement-wise multiplication. In our setup, we set M to be\nthe area inside the condition bounding box and xknown\nt\u22121\nto\nbe noisy versions of the reference image x0 obtained using\nthe forward diffusion process (Eq. 1 in the main paper).\nTable 1. Quantitative evaluation of InseRF and its comparison with\nthe baselines on three different metrics proposed in [10]. For ease\nof comparison, we report the values (cosine similarities ranging\nfrom -1 to 1) after bringing them between 0 and 1. Our proposed\nmethod effectively outperforms the baselines in all three metrics.\nMethod\nText-Image \u2191\nDirectional \u2191\nTemporal \u2191\nI-N2N [10]\n0.610\n0.515\n0.637\nMV-Inpainting\n0.606\n0.499\n0.724\nInseRF (ours)\n0.618\n0.545\n0.805\n11\nWhen only applying the mask-based blending, the au-\nthors of RePaint observe that, although the inpainted re-\ngion matches the texture of the neighboring region, it is\nnot well-harmonized in the image. Therefore, an additional\nre-sampling step is proposed, where the blended noisy im-\nages go through a few forward diffusion steps and are de-\nnoised again, to increase the harmonization of the inpainted\nregions. The proposed re-sampling step is characterized by\ntwo hyperparameters: 1) jump length: the number of ap-\nplied forward diffusion steps; 2) steps: the number of repeti-\ntions of adding noise and de-noising of the blended images.\nIn our experiments, we set both parameters to the value 2.\nC.2. Baselines\nInstruct-NeRF2NeRF (I-N2N): For our I-N2N baseline,\nwe created a reimplementation in JAX on top of the Mip-\nNeRF360 code. Our implementation uses the official pre-\ntrained checkpoints of Instruct-Pix2Pix [4] and is compati-\nble with LLFF datasets used in our experiments.\nMulti-View Inpainting (MV-Inpainting): In Sec. 4.1 of\nthe main paper, we proposed a baseline called Multi-View\nInpainting (MV-Inpainting). MV-Inpainint is designed to\ninsert objects into a 3D scene given accurate multi-view bi-\nnary masks at the input. To ensure a fair comparison, MV-\nInpainting uses the same 2D editing method as ours (Ima-\ngen [35] with RePaint [19]) to generate the target object in\neach viewpoint within the corresponding mask. In contrast\nto I-N2N, MV-Inpainting is equipped with localized editing\nto specifically investigate the importance of 3D consistency\nbetween different edited viewpoints.\nTo obtain the multi-view masks required for MV-\nInpainting, we first generate and insert an object in the scene\nusing our proposed object insertion. Then, we extract the\nmulti-view masks of the target object by rendering the 3D\nobject into the training viewpoints. The extracted masks are\nthen used as input to MV-inpainting along with the corre-\nsponding text prompt. We would like to emphasize that our\nmethod only requires a single-view rough bounding box, in\ncontrast to the multi-view accurate masks in MV-Inpainting.\nC.3. Depth Estimation\nAs discussed in Sec. 3.4 of the main paper, we use the\nmonocular depth estimated by MiDaS [31] to determine\nthe location of the target object in the 3D frustum formed\nby the input bounding box in the reference image. As the\nestimated depth using MiDaS is non-metric, we perform\na global affine depth alignment with the reference depth\nfrom the scene\u2019s NeRF reconstruction, which we explain\nin greater detail in the following.\nLet DR be the depth of the reference viewpoint rendered\nfrom the scene NeRF (not containing the object), and \u02c6DE be\nthe estimated depth of the edited reference view (containing\nthe 2D object) using MiDaS. We define the aligned depth\nmap \u02c6DA of the edited reference view as:\n\u02c6DA = a \u00b7 \u02c6DE + b\n(17)\nwhere a and b are the scalar parameters of a global affine\ntransformation. a and b are estimated by solving the fol-\nlowing weighted least-square estimation:\nmin\na,b\nX\ni\nX\nj\n(1 \u2212 M (i,j)) \u00b7 W (i,j) \u00b7 (D(i,j)\nR\n\u2212 \u02c6D(i,j)\nA\n)2,\n(18)\nwhere M is a binary mask corresponding to the reference\nbounding box. For a 2D matrix A, A(i,j) denotes the ele-\nment at row i and column j. W is the matrix containing\npixel-wise weights for the estimation, negatively correlated\nwith the distance of the pixel from the center of the bound-\ning box located at row ic and column jc:\nWij = 1 \u2212\np\n(i \u2212 ic)2 + (j \u2212 jc)2/z,\n(19)\nz = max(\np\n(i \u2212 ic)2 + (j \u2212 jc)2),\n(20)\ni \u2208 {0, ..., h \u2212 1} & j \u2208 {0, ..., w \u2212 1},\n(21)\nwhere z is a normalization term, and h and w are the\nheight and width of the reference image, respectively. The\nweighted estimation of the alignment parameters helps with\na more accurate alignment in the region surrounding the in-\nserted object. In practice, we perform our alignments on\nimage crops containing the object and its surroundings in-\nstead of the full image.\nAfter aligning the estimated depth map, in order to de-\ntermine the location of the object in the 3D scene, we first\nroughly estimate the distance of the center of the object\nfrom the camera center to be equal to the depth value at\nthe center of the bounding box d. Then, we perform the\nscale and distance optimization proposed in Sec. 3.4 of the\nmain paper, with the constraint that the depth of the center\nof the object\u2019s rendered view from the reference viewpoint\nmust be equal to d (please refer to discussion on the scale\nand distance optimization in Sec. 3.4 of the main paper for\nmore details).\nC.4. Rotation and Translation\nHere we provide more details on the process of calculating\nthe rotation and translation of the target object in the scene,\ndiscussed in Sec. 3.4 of the main paper. Specifically, we ob-\ntain the 3D location \u20d7pc of the center of the object in the 3D\nscene as the point along the normalized direction \u20d7v point-\ning from the camera center to the center of the reference\nbounding box:\n\u20d7pc = \u20d7o + r\u2217 \u00b7 \u20d7v\n(22)\nwhere r\u2217 is the optimized distance obtained from the scale\nand radius optimization (explained in Sec. 3.4 of the main\npaper).\n12\nWe use the right-handed coordinate system convention\nfor our scene and object NeRFs and place the object in an\nupward position in the scene centered at \u20d7pc. Moreover, we\nalign the reference view of the object in its coordinate sys-\ntem (corresponding to zero azimuth and elevation) with the\nreference camera viewpoint in the scene\u2019s coordinate sys-\ntem. In other words, we define the axes of the object coor-\ndinate system in the scene\u2019s coordinate system as follows:\n\u20d7uobject = [0, 0, 1]T ,\n(23)\n\u20d7xobject = \u2212\u20d7v,\n(24)\n\u20d7yobject = normalize(\u20d7uobject \u00d7\n\u20d7\nxobject),\n(25)\n\u20d7zobject = normalize(\u20d7xobject \u00d7\n\u20d7\nyobject),\n(26)\nThe rotation R and the translation \u20d7t are then obtained as:\nR = [\u20d7xobject, \u20d7yobject, \u20d7zobject]T\n(27)\n\u20d7t = \u2212R\u20d7pc\n(28)\nUsing the obtained rotation, translation, and optimized ob-\nject scale s\u2217, a point \u20d7p in the scene\u2019s coordinate system can\nbe mapped to a point \u20d7p\u2032 in the object\u2019s one as follows:\n\u20d7p\u2032 = 1\ns\u2217 [R,\u20d7t]\u20d7p\n(29)\nC.5. Scene and Object Fusion\nIn Sec. 3.5 of the main paper, we provided a detailed dis-\ncussion on how the scene and object NeRFs are fused in\nour method. In practice, object NeRFs fused in the scene\nmay be queried with points in the 3D space that have not\nbeen seen during the object NeRF optimization, resulting in\nunwanted artifacts. To prevent such artifacts, we consider\na 3D bounding box around the inserted objects, setting the\ndensity of the points sampled outside to zero. The dimen-\nsions of the 3D bounding box are determined based on the\ncamera radius used in the single-view object reconstruction\nstep and are fixed across edits and scenes.\nC.6. Refinement\nIn Sec. 3.6 of the main paper, we proposed an optional re-\nfinement based on the iterative NeRF optimization proposed\nin Instruct-NeRF2NeRF with two modifications: 1) using\nthe multi-view masks obtained from the inserted object to\nmake the refinement localized and 2) sampling viewpoints\non a sphere encapsulating the inserted object in the scene.\nIn particular, we sample the viewpoints on a sphere with\nthe radius r\u2217 (the optimized object distance) from the ob-\nject\u2019s center \u20d7pc. Such a sampling strategy allows for better\nedits by the 2D diffusion model. Moreover, instead of ran-\ndomly picking the next viewpoint to edit and include in the\nNeRF optimization, as done in Instruct-NeRF2NeRF, we\norder the viewpoints in a way that more frontal views are se-\nlected first. For example, viewpoints (azimuth, elevation)\nsampled from n equally-distanced azimuths with step size\n\u2206theta and m equally-distanced elevations with the step\nsize \u2206phi are arranged as an ordered set V :\nV = {(i \u00b7 \u2206\u03b8, j \u00b7 \u2206\u03d5)) | i \u2208 I & j \u2208 J}\n(30)\nI = {0, 1, \u22121..., n/2, \u2212n/2},\n(31)\nJ = {0, 1, \u22121, ..., m/2, \u2212m/2},\n(32)\nSuch ordering improves the 3D consistency of the refine-\nment step, as it decreases the conflict caused by randomly\nselected and independently edited viewpoints.\n13\nEdited Neural Scene Representation\nReference View\nReference Edit\nA pepper grinder\nA blue butterfly\nA duck\nA moai statue\nA wedding cake\nA pink crystal\nA succulent\nA beige purse\nA teddy bear\nFigure 8. Examples of using InseRF to insert an object into the neural representation of different indoor and outdoor scenes. More examples\ncan be found in Fig. 3 of the main paper.\n14\nOriginal Scene\nI-N2N\nMV-Inpainting\nInseRF (ours)\nOriginal Scene\nI-N2N\nMV-Inpainting\n(a) A cup on the table\nOriginal Scene\nI-N2N\nMV-Inpainting\nInseRF (ours)\n(b) A duck on the road\nOriginal Scene\nI-N2N\nMV-Inpainting\nInseRF (ours)\nOriginal Scene\nI-N2N\nMV-Inpainting\nInseRF (ours)\nOriginal Scene\nI-N2N\nMV-Inpainting\nInseRF (ours)\nOriginal Scene\nI-N2N\nMV-Inpainting\n(c) A pepper grinder on the counter\nOriginal Scene\nI-N2N\nMV-Inpainting\nInseRF (ours)\nOriginal Scene\nI-N2N\nMV-Inpainting\nInseRF (ours)\nOriginal Scene\nI-N2N\nMV-Inpainting\nInseRF (ours)\nOriginal Scene\nI-N2N\nMV-Inpainting\n(d) A pouffe on the carpet\nFigure 9. Qualitative comparison of object insertion with different methods. I-N2N and multiview inpainting both fail at inserting the\ngeometry of the object at the desired location. Our method, in contrast, can insert new 3D-consistent objects at the desired location. More\nexamples can be found in Fig. 4 of the main paper.\n15\nNo Re\ufb01nement\nRe\ufb01nement\nNo Re\ufb01nement\nRe\ufb01nement\nFigure 10. The refinement step proposed in our pipeline can improve the texture and the details of the inserted objects, leading to the\nhigher quality and realism of the insertions. More examples can be found in Fig. 7 of the main paper.\n16\n"
  },
  {
    "title": "URHand: Universal Relightable Hands",
    "link": "https://arxiv.org/pdf/2401.05334.pdf",
    "upvote": "20",
    "text": "URHand: Universal Relightable Hands\nZhaoxi Chen1,2\u2217\nGyeongsik Moon1\nKaiwen Guo1\nChen Cao1\nStanislav Pidhorskyi1\nTomas Simon1\nRohan Joshi1\nYuan Dong1\nYichen Xu1\nBernardo Pires1\nHe Wen1\nLucas Evans1\nBo Peng1\nJulia Buffalini1\nAutumn Trimble1\nKevyn McPhail1\nMelissa Schoeller1\nShoou-I Yu1\nJavier Romero1\nMichael Zollh\u00a8ofer1\nYaser Sheikh1\nZiwei Liu2B\nShunsuke Saito1B\n1Codec Avatars Lab, Meta\n2Nanyang Technological University\nhttps://frozenburning.github.io/projects/urhand\nLight-stage Data\nHigh-fidelity \nUniversal Prior\nGeneralize to Novel Views, Poses, Identities, and Illuminations\nQuick Personalization \nfrom a Phone Scan\nFigure 1. URHand (a.k.a. Your Hand). Our model is a high-fidelity Universal prior for Relightable Hands built upon light-stage data. It\ngeneralizes to novel viewpoints, poses, identities, and illuminations, which enables quick personalization from a phone scan.\nAbstract\nExisting photorealistic relightable hand models require\nextensive identity-specific observations in different views,\nposes, and illuminations, and face challenges in generaliz-\ning to natural illuminations and novel identities. To bridge\nthis gap, we present URHand, the first universal relightable\nhand model that generalizes across viewpoints, poses, illu-\nminations, and identities. Our model allows few-shot per-\nsonalization using images captured with a mobile phone,\nand is ready to be photorealistically rendered under novel\nilluminations. To simplify the personalization process while\nretaining photorealism, we build a powerful universal re-\nlightable prior based on neural relighting from multi-view\nimages of hands captured in a light stage with hundreds of\nidentities. The key challenge is scaling the cross-identity\ntraining while maintaining personalized fidelity and sharp\ndetails without compromising generalization under natural\nilluminations.\nTo this end, we propose a spatially vary-\ning linear lighting model as the neural renderer that takes\nphysics-inspired shading as input feature.\nBy removing\nnon-linear activations and bias, our specifically designed\nlighting model explicitly keeps the linearity of light trans-\nport.\nThis enables single-stage training from light-stage\n\u2217This work was done during an internship at Meta\nBCorresponding authors\ndata while generalizing to real-time rendering under ar-\nbitrary continuous illuminations across diverse identities.\nIn addition, we introduce the joint learning of a physically\nbased model and our neural relighting model, which fur-\nther improves fidelity and generalization. Extensive exper-\niments show that our approach achieves superior perfor-\nmance over existing methods in terms of both quality and\ngeneralizability. We also demonstrate quick personalization\nof URHand from a short phone scan of an unseen identity.\n1. Introduction\nWe engage our hands for various tasks throughout the day,\nand they consistently remain within our field of view. This\nconstant visibility of our hands makes them one of the most\nfrequently seen parts of our body, playing a central role in\nself-embodiment. To seamlessly reproduce this experience\nfor games or social telepresence, an ideal hand representa-\ntion in a digital medium is photorealistic, personalized, and\nimportantly, relightable for coherent appearance in any en-\nvironment. Our objective is to enable the quick creation\nof such a hand model for any individual given lightweight\ninput such as a phone scan, without going through an ex-\npensive capture process in a production studio (Figure 1).\nApproaches to build a photorealistic relightable hand\n1\narXiv:2401.05334v1  [cs.CV]  10 Jan 2024\nmodel can be broadly categorized into one of two philoso-\nphies. On the one hand, physically based rendering mod-\nels [25, 59] can generalize to various illuminations through\ncostly offline path-tracing, but typically lack photorealism\nunder a real-time constraint. Additionally, accurately esti-\nmating material parameters remains a non-trivial challenge\nfrom unconstrained inputs and the quality is often bounded\nby the expressiveness of the physical models. On the other\nhand, neural relighting [2, 14] recently achieves impressive\nphotorealism in real-time by directly inferring the outgoing\nradiance from illumination conditions. However, for gen-\neralization to natural illuminations to be possible these ap-\nproaches require expensive data augmentation with teacher-\nstudent distillation, where the student model learns to match\nwith offline renderings produced by the teacher model un-\nder natural illuminations. Most importantly, cross-identity\ngeneralization remains an open problem in both camps.\nIn this work, we propose URHand, the first Universal\nRelightable Hand model that generalizes across view-\npoints, motions, illuminations, and identities. To achieve\nthe best trade-off between generalization and fidelity, our\nwork exploits both physically based rendering and data-\ndriven appearance modeling from neural relighting. More\nspecifically, we incorporate known physics, such as the lin-\nearity of light transport [8] and surface reflections in the in-\nductive bias of the neural relighting framework. We modu-\nlate non-linear layers conditioned by pose and identity with\nlinear layers conditioned by spatially varying physically\nbased shading [3]. This explicitly ensures linearity between\ninput lighting features and output radiance. Thus, it enables\nenvironment map relighting without an expensive two-stage\nteacher-student distillation process commonly used in exist-\ning models [2, 14]. Our single-stage training enabled by lin-\nearity preservation makes cross-identity training more scal-\nable with better generalization to novel illuminations.\nFurthermore, we observe that the quality of the input\nshading features directly influences both generalization and\nfidelity of the final neural relighting outputs. Inspired by re-\ncent inverse rendering techniques [5, 67, 70], we introduce\nan additional physical branch that estimates the material pa-\nrameters and high-resolution geometry via inverse render-\ning, from which we produce the input lighting features to\nthe neural branch. The physical branch prevents the neu-\nral branch from overfitting by reducing hallucinations, and\nthe neural branch compensates for the complex global light\ntransport effects, such as subsurface scattering, that cannot\nbe well captured by the physical branch. In addition, the\nproposed physics-based refinement improves the accuracy\nof the tracking geometry with fine details such as wrinkles.\nCombining it with our novel lighting-aware adversarial loss,\nour method achieves highly detailed relighting with various\nilluminations under any pose for novel identities.\nWe run an extensive ablation study as well as compar-\nisons with baseline methods. The experiments demonstrate\nthe efficacy of our hybrid neural-physical relighting method\nby outperforming other methods quantitatively and quali-\ntatively. We also demonstrate the quick personalization of\nURHand from a phone scan, and relighting with arbitrary\nnatural illuminations. In summary, our contributions are:\n\u2022 The first method to learn a universal relightable hand\nmodel that generalizes to novel views, poses, illumina-\ntions, and identities.\n\u2022 A spatially varying linear lighting model that general-\nizes to continuous illuminations without expensive dis-\ntillation, enabling high-fidelity neural rendering and scal-\nable training with multiple identities.\n\u2022 A hybrid neural-physical relighting framework that lever-\nages the best of both approaches to achieve high fidelity\nand generalization at the same time.\n\u2022 The quick personalization of our universal prior to create\na photorealistic and relightable hand from a phone scan.\n2. Related Work\n3D Hand Modeling. Human hand modeling is an active\nresearch field within vision and graphics. Early works fo-\ncus more on 3D geometry and representation including mix-\nture of 3D Gaussians [49, 50], sphere meshes [54], and tri-\nangular meshes [1, 7, 44]. These parametric hand mod-\nels facilitate 3D hand pose estimation from 2D observa-\ntions [30, 31, 34\u201336]. Recent works also incorporate phys-\nical priors [25, 32, 48, 55, 71] to model more accurate\nnon-rigid deformation and articulation of the hand geom-\netry. The recent advances of neural fields [53, 60] also en-\nable the learning of personalized articulated models [19].\nBeyond geometry modeling, achieving a lifelike appear-\nance for hands [41] is paramount for realistic rendering\nand animation. Handy [40] learns a texture space by us-\ning generative adversarial networks for better photorealism\nand generalization. More recently, some methods [4, 6, 37]\nshowcase the modeling of animatable hands from monoc-\nular/multiview captures. However, the appearance models\nof these approaches simply bake the captured illumination\nand cannot be rendered under novel illuminations. NIM-\nBLE [25] builds PCA reflectance maps including diffuse,\nnormal, and specular maps from light-stage data. DART [9]\nalso supports accessories. However, physically based mate-\nrials are expensive to render with global illumination which\noften limits their rendering fidelity with a real-time con-\nstraint.\nRelightableHands [14] enables the photorealistic\nrelighting of hands in real-time using a neural appearance\nmodel. However, the method only supports person-specific\nmodeling and generalization to unseen identity is not possi-\nble. In contrast, our approach generalizes across poses, illu-\nminations, and identities, supporting relightable hand mod-\neling of unseen identities from a phone scan.\nImage-based Relighting. Given the linearity of light trans-\n2\nport, Debevec et al. [8] propose to render human faces under\nnovel illuminations by linear combinations of sampled re-\nflectance fields from one-light-at-a-time (OLAT) captures.\nA series of follow-up work [29, 51, 58, 62] enable dy-\nnamic relighting and capture through learning-based ap-\nproaches.\nMeanwhile, another line of work aims at in-\ntrinsic decomposition, enabling physically based rendering\nwith disentangled geometry and reflectance in the image\nspace [11, 12, 17, 22, 38, 47]. Recent advances in neural\nrendering [15, 39, 65] learn to match with ground truth us-\ning non-linear neural networks given the lighting features\nfrom a simple physical shading model. Yet, image-based\nrelighting suffers from 3D inconsistency and flickering due\nto the lack of an underlying 3D representation.\nModel-based Relighting.\nTo address the lack of 3D\nconsistency in image-based relighting, one can leverage\na shared 2D parameterization [63, 69] or template mod-\nels [2, 5, 14, 20] for model-based relighting. Most model-\nbased relighting approaches [5, 13, 16, 42, 56] rely on the\nintrinsic decomposition of geometry and reflectance fol-\nlowed by a physically based appearance model. While they\nachieve generalization to novel conditions, the fidelity is\ntypically limited due to the lack of expressiveness in the\nunderlying parametric BRDFs. On the other hand, methods\nwith neural renderers support complex global illumination\neffects learned from captured data under point lights. How-\never, generalization to continuous environments requires\nthe expensive teacher-student training framework [2, 14],\nwhich is difficult to scale to cross-identity training. On the\ncontrary, our spatially varying linear network achieves gen-\neralization to any type of illumination without additional\ntraining. Concurrently, Yang et al. [64] propose a linear\nlighting model for face relighting that eliminates the need\nof teacher-student distillation. While the motivation is sim-\nilar, we observe that their holistic light representation does\nnot generalize well for hands due to drastic visibility change\nby articulation (see Sec. 5.3 for analysis).\n3. Preliminary\nData Acquisition.\nWe use a multiview capture system\nconsisting of 150 cameras and 350 LED lights to capture\ndynamic hands with time-multiplexed illuminations by in-\nterleaving fully lit (all lights on) and partially lit every other\nframe. Instead of OLAT, our partially lit frames use L = 5\ngrouped lights to increase brightness and reduce motion\nblur as in [2, 14, 23]. Images are captured in the resolution\nof 4096 \u00d7 2668 at 90 fps. We first reconstruct per-frame 3D\nmeshes using [10] and detect 3D hand keypoints using [24]\nfollowed by triangulation from fully lit frames.\nFor partially lit frames, we leverage spherical linear in-\nterpolation over the pose parameter of adjacent fully lit\nframes to obtain the hand pose of partially lit frames. Our\ndataset contains 93 different identities in diverse hand mo-\ntions with an average of 42000 frames for each identity.\nLinearity of Light Transport. To render subjects in arbi-\ntrary illuminations, natural illumination is treated as a lin-\near combination of distant point lights given the linearity\nof light transport [8]. Specifically, given the appearance\nvalue Ci based on the i-th point light, the final color C is\ncomputed as a linear combination of all light sources, i.e.\nC = PL\ni=1 biCi, where L denotes the number of lights,\nand bi is the intensity of each light. Given a light transport\nfunction f(b) = C, we define it as linear w.r.t. b such that:\nf(b) = f(\nL\nX\ni=1\nbi) =\nL\nX\ni=1\nf(bi), b =\nL\nX\ni\nbi.\n(1)\nHand Geometry Modeling.\nSimilar to [32], our 3D\nhand representation is based on a mesh template with ver-\ntex offsets predicted by a neural network. The 3D hand\ncan be driven by linear blend skinning (LBS) and represent\nidentity- and pose-specific deformations.\nSpecifically, we design an autoencoder to obtain accurate\nhand tracking and geometry. The encoder learns to predict\nidentity-dependent latent codes and 3D hand poses from the\ninput fully lit frames. Given an articulated generic mesh\ntemplate\n\u00af\nM = {V, F, U, \u03b8} with vertices V \u2208 RnV\u00d73,\nfaces F \u2208 RnF\u00d73, texture coordinates U \u2208 RnV\u00d72, and 3D\npose \u03b8 \u2208 R60\u00d73 in Euler angles and latent codes z, the de-\ncoder learns to predict the 3D offset of all vertices as \u03b4V \u2208\nRnV\u00d73. We use nV = 15930 and nF = 32340. The tracked\nhand mesh will be represented as M = {V + \u03b4V, F, U, \u03b8}.\nPlease refer to the supplementary material for more details.\n4. Universal Relightable Hands\nOur goal is to build a universal relightable shape and appear-\nance model for human hands that can be rendered for any\nidentity under arbitrary illumination in real-time. To this\nend, we learn a relightable appearance model from cross-\nidentity light-stage captures based on grouped point lights.\nIn this section, we will introduce our learning frame-\nwork, URHand, which learns to relight target hands in dif-\nferent poses and views. The core of our model is a spa-\ntially varying linear lighting model that preserves the lin-\nearity of light transport, which enables the generalization to\narbitrary illuminations by training with monochrome group\nlights only.\nOur model consists of two parallel render-\ning branches, physical and neural.\nThe physical branch\n(Sec. 4.1) focuses on refining geometry and providing accu-\nrate shading features as an illumination proxy for the neural\nbranch. The neural branch (Sec. 4.2) learns the final appear-\nance of hands with global illumination. These two branches\nare trained jointly in an end-to-end manner with our tailored\nloss functions (Sec. 4.3). Finally, we use this universal prior\nto quickly personalize a relightable hand model from few-\nshot observations (Sec. 5.4).\n3\nEq. (6)\n\ud835\udc02\n\ud835\udc02!\"\n\u2131!\nU-Net\nRoughness  \ud835\udefd\nDisplacement  \ud835\udeff\ud835\udc51\nPhysical \nBRDF \u2131!\"\nView\nLight\nNon-Linear\nNet  \u2131#$\nRefined Normal  '\ud835\udc5b\nDiffuse  \ud835\udc02!\"\n%    \nSpecular  \ud835\udc02!\"\n&\nEncoder  \u2131$'(#)\nDecoder  \u2131$'*()\nLinear Net  \u2131$ \nEq. (5)\nNormal  \ud835\udc5b\nGT\nGain  \ud835\udc54\nBias  \ud835\udc4f\nHand Pose  \ud835\udf03\nCoarse Mesh \u2133\n\u2112!\"#\n\u2112$%$&'\nEq. (3)\nMean Texture  \ud835\udcaf\nPhysical Branch\nNeural Branch\n\ud835\udc60\ud835\udc54(\u22c5)\nFigure 2. Overview of URHand. Our model takes as input a mean texture T , hand pose \u03b8, and a coarse mesh M for each identity. The\nphysical branch (Sec. 4.1) focuses on geometry refinement and providing accurate shading features for the neural branch (Sec. 4.2). The\ncore of the neural branch is the linear lighting model which takes as input the physics-inspired shading features from the physical branch.\nThe neural branch learns to predict the gain and bias map over the mean texture. We leverage a differentiable rasterizer for rendering and\nminimize the loss of both branches against ground truth images (Sec. 4.3). The sg(\u00b7) denotes the stop-gradient operation.\n4.1. Physically based Geometry Refinement\nThe physical branch employs online physically based ren-\ndering to render images using a parametric BRDF. We opti-\nmize the material parameters of the BRDF via inverse ren-\ndering. The goal of the physical branch is two-fold: 1) fur-\nther refine the initial hand geometry for better alignment,\nand 2) provide generalizable lighting features that best ap-\nproximate the specular reflection and diffuse shading to pre-\nvent overfitting in the neural relighting.\nWe illustrate the physical branch in Fig. 2. The phys-\nical branch estimates a parametric Disney BRDF [3] Fpb.\nWe use a 2D U-Net FG to infer a displacement map \u03b4d \u2208\nR1024\u00d71024\u00d73 and a roughness map \u03b2 \u2208 R1024\u00d71024 in\nUV space.\nInstead of predicting a normal map directly,\nwe use the predicted displacement map to update the nor-\nmal map on top of the unwrapped coarse mesh in the UV\nspace, which makes it easier to infer high-frequency geo-\nmetric details on a smooth base surface [66]. To support\ncross-identity modeling with pose-dependent geometry and\nappearance change, FG takes as input the hand pose \u03b8 and\nthe unwrapped mean texture T \u2208 R1024\u00d71024\u00d73 for each\nidentity, where the hand pose is concatenated to the bottle-\nneck representation of the U-Net.\nSpecifically, the refined surface \u02c6x is obtained by adding\nthe displacement \u03b4d to the base surface x derived from the\ncoarse mesh M along the direction of the normal n:\n\u02c6x = x + \u03b4d \u00b7 n,\n(2)\nwhere positional map \u02c6x is then used to obtain the refined\nnormal \u02c6n in the UV space for physically based rendering. In\nour implementation, we apply a sigmoid activation followed\nby a scaling factor of 3 in order to constrain the range of\ndisplacement to \u00b13mm.\nThe refined normal \u02c6n and roughness \u03b2 are fed into Fpb\nthat considers only the first bounce. Given an illumination\nL, the final color Cpb from camera view d is computed as:\nCpb(\u02c6x, d, L) =\nZ\nLi(L, \u02c6x, \u03c9i)Fpb(\u02c6x, \u03c9i, d, \u03b2)(\u03c9i\u00b7\u02c6n)d\u03c9i,\n(3)\nwhere Li(\u02c6x, \u03c9i) is the incident light from direction \u03c9i.\nThe physically rendered texture map Cpb can be decom-\nposed into physically based shading feature, i.e. Fpb(L) =\n{Cd\npb, Cs\npb}, according to the equation Cpb = Cd\npb \u2299 T +\nCs\npb, where \u2299 is the element-wise multiplication, and T\nis the mean texture approximating albedo. Note that, this\ncomputation is directly performed in the UV space.\n4.2. Linear Lighting Model\nThe rendering based on a parametric BRDF generalizes to\nnovel illuminations, but they lack correct global light trans-\nport effects such as subsurface scattering. To enable relight-\ning with global illumination in real-time, we introduce a\nneural renderer FR. Given a target illumination, the outgo-\ning radiance is computed as: C = FR(M, T , Fpb(L), \u03b8).\nOur key insight is that removing the non-linear activation\nlayers and bias in a convolutional neural network preserves\nthe linearity with respect to the input features. Since our\ninput feature is the physically based shading Fpb(L), our\nnetwork satisfies the following:\nL\nX\ni=1\nFR(M, T , Fpb(Li), \u03b8) = FR(M, T ,\nL\nX\ni=1\nFpb(Li), \u03b8).\n(4)\nSince the physically based rendering in Eq. 3 is energy pre-\nserving, the network holds the linearity w.r.t. the input illu-\nmination. Therefore, our network can produce accurate re-\nlighting with continuous environment maps by training only\nwith discrete point lights without additional distillation.\nWe illustrate the architecture of FR = {Fl, Fnl} in\nFig. 2. It consists of a linear (Fl) and a non-linear (Fnl)\nbranch, where the linear branch consists of an encoder\nFl\u2212enc and a decoder Fl\u2212dec with physically based shading\n4\nfeatures Fpb(L) as input. The pose- and identity-dependent\nfeatures derived from the mean texture T and pose \u03b8 are fed\ninto a non-linear branch. We fuse the linear and non-linear\nfeature maps in the decoder of the linear branch as follows:\nFj+1\nl\u2212dec =\n1\n\u221a\n2 \u00b7 ConvT(Fj\nl\u2212enc + Fj\nl\u2212dec) \u2299 Fj\nnl,\n(5)\nwhere j is the index of the layer, ConvT is the transposed\nconvolutional layer without bias. This fusion mechanism\nkeeps the linearity of the output w.r.t. the input lighting fea-\ntures while incorporating non-linearity w.r.t. identity and\npose conditions. Instead of predicting the final texture, our\nneural renderer predicts the texel-aligned gain map g and\nbias map b, which contribute to the final texture as follows:\nC = g \u2299 T + b \u00b7 \u03c3T ,\n(6)\nwhere \u03c3T = 64 is the standard deviation of textures.\nImportant to note that since our input feature Fpb is spa-\ntially varying in a texel-aligned manner similar to [14, 39],\nwe can accurately incorporate shadow information for bet-\nter generalization with diverse poses. While the concur-\nrent work [64] also proposes a linear lighting model using\na holistic illumination representation by simply reshaping\nenvironment maps, we observe that the holistic illumina-\ntion representation cannot generalize for hands due to the\ninfinite shadow variations caused by articulation. The same\nobservation is also reported in person-specific relightable\nhand modeling [14]. Please refer to Sec. 5 for the analysis.\n4.3. Training Objectives\nOur model is trained on multiview partially lit images in\ndifferent identities and poses. The training objective Ltotal\nconsists of three parts: reconstruction loss Limg, lighting-\naware adversarial loss LGAN, and L1 regularization Lreg:\nLtotal = \u03bbimgLimg + \u03bbGANLGAN + \u03bbregLreg,\n(7)\nwhere \u03bb\u2217 are corresponding loss weights.\nReconstruction Loss. We leverage a sum of L1 loss LMAE\nand perceptual loss Leff based on the EfficientNet [52]\nbackbone, i.e. Limg = LMAE + Leff. Both the renderings\nfrom the neural and the physical branch are supervised by\nthe reconstruction loss against the ground truth images.\nLighting-aware Adversarial Loss.\nTo improve the vi-\nsual quality, we propose to use an adversarial loss on top\nof the reconstruction loss. We found that a naive image-\nconditioned discriminator performs poorly due to the signif-\nicant appearance change in partially lit frames. To address\nthis, we leverage a lighting-aware discriminator on multi-\nple scales of renderings. Specifically, the discriminator FD\nis conditioned on the diffuse and specular feature, {A, S},\nwhich prompts the network to discriminate real and fake im-\nages given the illumination information. These lighting fea-\ntures are based on simple Phong reflectance [14] to ensure\na consistent lighting prompt during training. We choose a\nhinge loss [26] operated on multi-resolution discriminated\npatches as the adversarial target:\nLGAN = log FD(I|A, S) + log[1 \u2212 FD(\u02c6I|A, S)],\n(8)\nwhere I is the ground truth, and \u02c6I is the rendered image.\nL1 Regularization on Linear Model. We also discovered\nthat without regularization, a linear lighting model often\nproduces noticeable flickering. As a linear convolutional\nnetwork is capacity-limited, it tends to have high variance\nin intermediate features, resulting in poor generalization to\nnovel illuminations. Thus, we penalize the L1 norm of the\nintermediate features from all layers in the linear branch:\nLreg =\nN\nX\nj=1\n||F j\nl\u2212enc||1.\n(9)\nImplementation Detail.\nThe optimizable modules are\n{FG, FR, FD}. We use the Adam [21] optimizer and set\nthe loss weights as \u03bbimg\n= 1.0, \u03bbGAN = 0.01, and\n\u03bbreg = 0.01, respectively. We train the model for 2M itera-\ntions distributed on 8 A100 GPUs with a batch size of 24 in\ntotal. The initial learning rate is 1 \u00d7 10\u22124 which decays to\n3 \u00d7 10\u22125 with a multistep learning rate scheduler. We de-\nscribe the detailed network architecture and more training\ndetails in the supplementary.\nRuntime Analysis. Our proposed linear lighting model is\nnot only scalable and generalizable but also efficient for\nreal-time rendering. Our model achieves 38 FPS (25.7 ms)\ngiven grouped lights as input and 31 FPS (31.9 ms) given\nenvironmental maps as input on an NVIDIA A100 GPU.\n5. Experiments\n5.1. Evaluation Protocols\nTo quantitatively evaluate the fidelity of each method, we\nuse Peak Signal-to-Noise Ratio (PSNR), Structural Simi-\nlarity Index Measure [57] (SSIM), and Learned Perceptual\nImage Patch Similarity [68] (LPIPS) as metrics. To solely\nevaluate the quality of rendered hands, we only take the\nforeground according to the mask obtained from the refined\nhand geometry. We exclude several segments from training\ndata to evaluate the generalization of our model to novel\nposes. All metrics are evaluated on 1,000 images randomly\nsampled from those held-out segments.\nFor fair comparisons with existing works, we train and\nevaluate all methods in the single-identity setting except\nmentioned. For ablation studies, we train with ten identi-\nties and test on 1) unseen segments from a train subject, 2)\nunseen segments from an unseen subject, and 3) an unseen\nillumination from a train subject, which orthogonally eval-\nuate the generalization of our model to novel poses, identi-\nties, and illuminations.\n5\nTable 1. Quantitative comparisons on sequences with grouped lights. We evaluate our method for both per-subject optimization and\nnovel identity generalization against the state-of-the-art methods in model-based hand relighting. \u2020Methods are evaluated on the training\nidentity with unseen segments. \u2217Methods are evaluated on unseen identity during training. The top three techniques are highlighted in red,\norange, and yellow, respectively.\nMethod\nSubject 1\nSubject 2\nSubject 3\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\n\u2020RelightableHands [14]\n25.97\n0.9301\n0.1425\n25.92\n0.9372\n0.1426\n27.16\n0.9419\n0.1280\n\u2020Ours (Physical only)\n23.44\n0.9062\n0.1708\n23.25\n0.9154\n0.1715\n24.90\n0.9216\n0.1510\n\u2020Ours (Full model)\n27.77\n0.9400\n0.1204\n26.36\n0.9384\n0.1344\n27.75\n0.9445\n0.1226\n\u2217Handy [40] + Phong\n16.65\n0.8235\n0.3026\n16.39\n0.8328\n0.3019\n17.70\n0.8311\n0.2872\n\u2217Handy [40] + GGX\n16.60\n0.8212\n0.3125\n16.33\n0.8300\n0.3107\n17.58\n0.8288\n0.2978\n\u2217Ours (Full model)\n26.94\n0.9271\n0.1335\n26.24\n0.9368\n0.1341\n27.58\n0.9436\n0.1197\nHandy + Phong\nHandy + GGX\nRelightableHands\nOurs\nGround Truth\nOurs \nOurs (Physical Only)\na)  Single Identity Evaluation\nb)  Novel Identity Generalization\nFigure 3. Qualitative comparisons on sequences with grouped lights. We evaluate our method for both per-subject optimization and\nnovel identity generalization against comparison methods. a) All methods are evaluated on the training identity with unseen segments. b)\nMethods are evaluated on unseen identity during training.\n5.2. Comparisons\nWe compare our approach with the state-of-the-art 3D\nhand relighting and reconstruction methods. Relightable-\nHands [14] reconstructs relightable appearance via per-\nidentity optimization. Handy [40] predicts the UV texture\nof the hand from in-the-wild images. For fair comparisons,\nwe apply physically based renderers on top of the predicted\ntexture to enable relightable appearances. We also com-\npare with our physical branch using the same loss functions.\nThis provides a fair comparison with physically based re-\nlighting. For this reason, we omit the comparison with other\nexisting physically based relightable hand methods such as\nHARP [20] and NIMBLE [25]. Please refer to the supple-\nmentary for our detailed implementations of these methods.\nWe present quantitative results in Table 1.\nFor per-\nidentity training, our method significantly outperforms\nbaseline methods on all metrics, which highlights the ef-\nfectiveness of our key designs. As shown in Figure 3, our\nmethod is able to reproduce detailed geometry, e.g. wrinkles\nand nails, together with high-fidelity details like speculari-\nties and shadows. Handy [40] fails to reproduce the base\ntexture of the target hand due to their data-driven latent tex-\nture space. Its combinations of simple physical renderers\nlead to artifacts of the relit results. RelightableHands [14]\ncan reproduce correct shading and appearance. However,\nthe quality of geometric details and specularity is not on par\nwith the proposed method, which demonstrates the effec-\ntiveness of our hybrid neural-physical approach. Further-\nmore, the generalizability of our method is showcased in\nthe right side of Figure 3, where the shown test subjects\nare withheld from the training set. Although the quality\nslightly degrades compared with per-identity optimization,\nit still outperforms all other baselines by a large margin.\n5.3. Ablation Studies\nAblation on Linear Lighting Model. We investigate dif-\nferent designs of the linear lighting model. We compare our\nspatially varying linear lighting model with three alterna-\ntives: 1) Non-linear model, where all non-linear activations\nare turned on; 2) Non-linear model with a linearity consis-\ntency loss to constrain the linearity (Eq. 1) between output\nand input of the network; 3) MLP-based linear model pro-\nposed in [64], where the lighting is represented as environ-\nment map and flattened to a 1-D vector.\nTable 4 shows that the proposed spatially varying lin-\near lighting model achieves the best performance on both\n6\nTable 2. Ablation studies of the linear lighting model on sequences with grouped lights. The \u201cNon-linear\u201d denotes the model that does\nnot satisfy the linearity of light transport while the \u201cLinearity Consistency\u201d denotes the non-linear model with regularizations to constrain\nthe linearity between output and input. Moreover, in contrast to MLP-based linear model [64], our model is a spatially varying linear\nmodel. The top three techniques are highlighted in red, orange, and yellow, respectively.\nMethod\nTrained Subject\nUnseen Subject\nFully lit\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nNon-linear\n25.79\n0.9283\n0.1364\n23.89\n0.9145\n0.1523\n17.50\n0.9070\n0.1717\nLinearity Consistency\n24.77\n0.9122\n0.1560\n23.11\n0.9124\n0.1537\n19.75\n0.9013\n0.1619\nMLP-based Linear [64]\n22.19\n0.8727\n0.1885\n21.56\n0.8787\n0.1823\n11.56\n0.8644\n0.2259\nOurs (Linear Model)\n26.01\n0.9270\n0.1336\n24.70\n0.9121\n0.1520\n20.61\n0.9153\n0.1576\nOurs\nLinear Consistency \nMLP-based Linear\nNon-linear\nFigure 4. Ablation study on the design of linear lighting model.\nOur spatially varying linear lighting model produces realistic ren-\nderings, while the baseline methods fail to correctly model shad-\nows or tend to be over smooth.\nTable 3. Ablation studies of the lighting features on sequences\nwith grouped lights. The top three techniques are highlighted in\nred, orange, and yellow, respectively.\nMethod\nTrained Subject\nUnseen Subject\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nPhong\n25.53\n0.9257\n0.1382\n23.87\n0.9148\n0.1523\nw/o Specular\n24.97\n0.9160\n0.1498\n23.20\n0.9075\n0.1607\nw/o Visibility\n25.44\n0.9228\n0.1426\n23.75\n0.9133\n0.1533\nw/o Refiner\n24.82\n0.9144\n0.1437\n22.80\n0.8993\n0.1620\nFull Model\n26.01\n0.9270\n0.1336\n24.70\n0.9121\n0.1520\ntrained and unseen identities. Moreover, we also quantita-\ntively evaluate on fully lit frames to validate the generaliza-\ntion to novel illumination. The non-linear baseline fails to\ngeneralize, which shows the importance of linearity in our\nnetwork. The MLP-based linear model fails to correctly\nmodel the light transport due to the lack of pose-aware\nvisibility change. Figure 4 shows that the proposed light-\ning model produces the most realistic renderings, while the\nbaseline methods struggle with correctly modeling shadows\nand detailed shading effects.\nAblation on Different Lighting Features. We also evalu-\nate the effectiveness of our lighting feature representation,\ni.e. the diffuse Cd\npb and specular feature Cs\npb. The con-\ntribution in the lighting features can be factored into 1)\nthe underlying BRDF, 2) the visibility, and 3) the geome-\ntry. As shown in Figure 5, our model reproduces specular\nhighlights due to the more accurate specular feature from\nthe optimized Disney BRDF. This indicates that the quality\nof the lighting feature determines the quality of the neural\nOurs\nPhong\nw/o Specular\nw/o Visibility\nw/o Refiner\nN/A\nGT\nRender\nNormal\nDiffuse\nSpecular\nFigure 5. Ablation studies on the impact of lighting features\nand geometry refinement. Notably, our full model can produce\nfine-grained geometry like wrinkles and nails as well as specular\nhighlights (e.g. little finger).\nrelighting. In fact, the neural relighting based on Phong\nspecular features [14, 39] fails to produce accurate specular\nhighlights. The visibility is also important for novel pose\ngeneralization. Compared with the model trained without\nvisibility, our full model reproduces better soft shadows as\nwell as detailed geometry. These observations are also sup-\nported by our quantitative evaluation presented in Table 3.\nEffectiveness of Geometry Refinement.\nOur proposed\nneural-physical rendering offers the ability to refine geome-\ntry. As shown in Figure 5, our full model can produce more\n7\nTable 4. Ablation studies of the proposed training objectives\non sequences with grouped lights. The top three techniques are\nhighlighted in red, orange, and yellow, respectively.\nMethod\nTrained Subject\nUnseen Subject\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nw/o LGAN\n25.95\n0.9271\n0.1392\n23.70\n0.9113\n0.1542\nw/o Light-aware LGAN\n25.38\n0.9215\n0.1391\n23.88\n0.9142\n0.1521\nw/o L1 Reg Lreg\n25.52\n0.9242\n0.1351\n23.90\n0.9149\n0.1491\nFull Model\n26.01\n0.9270\n0.1336\n24.70\n0.9121\n0.1520\nGround Truth\nOurs\nw/o L1 Reg\nw/o Light Awareness\nw/o GAN Loss\nLight Probe\nFigure 6. Ablation studies on the proposed training objectives.\nThe adversarial loss improves the overall quality while the light\nawareness of the discriminator is critical for correct shadows.\nfine-grained details such as wrinkles and nails. The geom-\netry refinement evidently prevents the neural renderer from\nhallucinating, leading to better generalization to novel iden-\ntities and illuminations. The quantitative results in Table 3\nalso support this observation.\nEffectiveness of Adversarial Loss. We evaluate the ef-\nfectiveness of the proposed lighting-aware adversarial loss.\nQuantitative results in Table 4 suggest that this adversarial\nloss improves the overall quality. The lighting-aware dis-\ncriminator further improves the fidelity. Figure 6 validates\nthat the adversarial loss is critical for reproducing shadow\nand detailed geometry. It also enhances specular highlights.\nEffectiveness of L1 Regularization. We validate the ef-\nfectiveness of the L1 regularization on the intermediate fea-\ntures of the linear lighting model. Table 4 and Figure 6 illus-\ntrate that, compared with the model without L1 regulariza-\ntion, our model achieves better modeling of the soft shad-\nows and appearance. Its effectiveness is more evident in the\ntemporal sequence with less flickering artifacts. Please refer\nto our supplemental video.\n5.4. Quick Adaptation on Unseen Subjects\nAs shown in Figure 7, we demonstrate the quick personal-\nization of URHand to get a relightable and animatable hand\nfrom a monocular iPhone video. To achieve this, we fit a\ntemplate hand model (Sec. 3) to input images by optimizing\npose parameters and the identity latent code based on fore-\nground segmentation and 2D keypoints following a similar\nMonocular video \nfrom iPhone\n\u2026\nFigure 7. Quick Personalization of URHand from iPhone cap-\ntures. Given a casual phone scan, we fit hand geometry (Sec. 3),\nunwrap RGB images to get the mean texture, and feed into UR-\nHand. Our model instantly enables photorealistic relighting in any\nposes and illuminations without finetuning.\npipeline to [20]. Then we unwrap input RGB images based\non the fitted geometry to obtain the mean texture T , and re-\nfine it to remove shadows [20]. Finally, we directly feed the\nmean texture and the coarse mesh into URHand to render it\nwith any poses and environment maps. Note that while UR-\nHand can instantly create a personalized relightable hand\nmodel without any finetuning, the aforementioned prepro-\ncessing stage takes several hours.\n6. Conclusion\nWe have introduced URHand, the first universal relightable\nhand model that generalizes across viewpoints, poses, il-\nluminations, and identities. We show that scalable cross-\nidentity training for high-fidelity relightable hands now is\npossible with our physics-inspired spatially varying linear\nlighting model and hybrid neural-physical learning frame-\nwork. Our experiments indicate that URHand even gener-\nalizes beyond studio data by showing quick personalization\nfrom a phone scan.\nLimitation and Future Works. Since we learn global light\ntransport with far-field lighting, it does not guarantee cor-\nrect light transport with near-field lighting. Nevertheless,\nour work achieves plausible near-field relighting similarly\nto [2, 14, 46, 61]. Currently the quick personalization re-\nquires the complete mean texture of a target hand. Thus,\nit does not work with a single image.\nOne future work\nwould be inpainting the texture from a single image to en-\nable single-view relightable hand reconstruction. As our\nhand model is only driven by hand poses, it cannot capture\nappearance variations due to blood pressure or temperature\nchanges. As recently demonstrated in [33], photorealistic\nrelightable hands can be used to augment training data for\nimage-based pose regression tasks. Using URHand to syn-\nthesize large-scale two-hand or hand-to-object interaction\nimages with diverse identities is also fruitful.\n8\nReferences\n[1] Luca Ballan, Aparna Taneja, J\u00a8urgen Gall, Luc Van Gool, and\nMarc Pollefeys. Motion capture of hands in action using dis-\ncriminative salient points. In Computer Vision\u2013ECCV 2012:\n12th European Conference on Computer Vision, Florence,\nItaly, October 7-13, 2012, Proceedings, Part VI 12, pages\n640\u2013653. Springer, 2012. 2\n[2] Sai Bi, Stephen Lombardi, Shunsuke Saito, Tomas Simon,\nShih-En Wei, Kevyn Mcphail, Ravi Ramamoorthi, Yaser\nSheikh, and Jason Saragih.\nDeep relightable appearance\nmodels for animatable faces. ACM Transactions on Graphics\n(TOG), 40(4):1\u201315, 2021. 2, 3, 8, 4\n[3] Brent\nBurley\nand\nWalt\nDisney\nAnimation\nStudios.\nPhysically-based shading at disney.\nIn Acm Siggraph,\npages 1\u20137. vol. 2012, 2012. 2, 4, 1\n[4] Xingyu Chen, Baoyuan Wang, and Heung-Yeung Shum.\nHand avatar: Free-pose hand animation and rendering from\nmonocular video.\nIn Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n8683\u20138693, 2023. 2\n[5] Zhaoxi Chen and Ziwei Liu.\nRelighting4d:\nNeural re-\nlightable human from videos. In European Conference on\nComputer Vision, pages 606\u2013623. Springer, 2022. 2, 3\n[6] Enric Corona, Tomas Hodan, Minh Vo, Francesc Moreno-\nNoguer, Chris Sweeney, Richard Newcombe, and Lingni\nMa. Lisa: Learning implicit shape and appearance of hands.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 20533\u201320543, 2022.\n2\n[7] Martin de La Gorce, David J Fleet, and Nikos Para-\ngios. Model-based 3d hand pose estimation from monocular\nvideo. IEEE transactions on pattern analysis and machine\nintelligence, 33(9):1793\u20131805, 2011. 2\n[8] Paul Debevec, Tim Hawkins, Chris Tchou, Haarm-Pieter\nDuiker, Westley Sarokin, and Mark Sagar.\nAcquiring the\nreflectance field of a human face.\nIn Proceedings of the\n27th annual conference on Computer graphics and interac-\ntive techniques, pages 145\u2013156, 2000. 2, 3\n[9] Daiheng Gao, Yuliang Xiu, Kailin Li, Lixin Yang, Feng\nWang, Peng Zhang, Bang Zhang, Cewu Lu, and Ping Tan.\nDart: Articulated hand model with diverse accessories and\nrich textures. Advances in Neural Information Processing\nSystems, 35:37055\u201337067, 2022. 2\n[10] Kaiwen Guo, Peter Lincoln, Philip Davidson, Jay Busch,\nXueming Yu, Matt Whalen, Geoff Harvey, Sergio Orts-\nEscolano, Rohit Pandey, Jason Dourgarian, et al. The re-\nlightables: Volumetric performance capture of humans with\nrealistic relighting. ACM Transactions on Graphics (ToG),\n38(6):1\u201319, 2019. 3\n[11] Andrew Hou, Ze Zhang, Michel Sarkis, Ning Bi, Yiying\nTong, and Xiaoming Liu. Towards high fidelity face relight-\ning with realistic shadows. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 14719\u201314728, 2021. 3\n[12] Andrew Hou, Michel Sarkis, Ning Bi, Yiying Tong, and\nXiaoming Liu. Face relighting with geometrically consis-\ntent shadows. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 4217\u2013\n4226, 2022. 3\n[13] Umar Iqbal, Akin Caliskan, Koki Nagano, Sameh Khamis,\nPavlo Molchanov, and Jan Kautz. Rana: Relightable artic-\nulated neural avatars. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, pages 23142\u2013\n23153, 2023. 3\n[14] Shun Iwase, Shunsuke Saito, Tomas Simon, Stephen Lom-\nbardi, Bagautdinov Timur, Rohan Joshi, Fabian Prada,\nTakaaki Shiratori, Yaser Sheikh, and Jason Saragih.\nRe-\nlightablehands:\nEfficient neural relighting of articulated\nhand models. In CVPR, 2023. 2, 3, 5, 6, 7, 8, 4\n[15] Chaonan Ji, Tao Yu, Kaiwen Guo, Jingxin Liu, and Yebin\nLiu. Geometry-aware single-image full-body human relight-\ning.\nIn European Conference on Computer Vision, pages\n388\u2013405. Springer, 2022. 3\n[16] Haian Jin, Isabella Liu, Peijia Xu, Xiaoshuai Zhang, Song-\nfang Han, Sai Bi, Xiaowei Zhou, Zexiang Xu, and Hao\nSu. Tensoir: Tensorial inverse rendering. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 165\u2013174, 2023. 3\n[17] Yoshihiro Kanamori and Yuki Endo.\nRelighting humans:\nocclusion-aware inverse rendering for full-body human im-\nages. arXiv preprint arXiv:1908.02714, 2019. 3\n[18] Tero Karras, Miika Aittala, Samuli Laine, Erik H\u00a8ark\u00a8onen,\nJanne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free\ngenerative adversarial networks. In Proc. NeurIPS, 2021. 3\n[19] Korrawe Karunratanakul, Adrian Spurr, Zicong Fan, Otmar\nHilliges, and Siyu Tang. A skeleton-driven neural occupancy\nrepresentation for articulated hands. In 2021 International\nConference on 3D Vision (3DV), pages 11\u201321. IEEE, 2021.\n2\n[20] Korrawe Karunratanakul, Sergey Prokudin, Otmar Hilliges,\nand Siyu Tang. Harp: Personalized hand reconstruction from\na monocular rgb video. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 12802\u201312813, 2023. 3, 6, 8\n[21] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014. 5, 3\n[22] Manuel Lagunas, Xin Sun, Jimei Yang, Ruben Villegas, Jian-\nming Zhang, Zhixin Shu, Belen Masia, and Diego Gutier-\nrez. Single-image full-body human relighting. arXiv preprint\narXiv:2107.07259, 2021. 3\n[23] Junxuan Li, Shunsuke Saito, Tomas Simon, Stephen Lom-\nbardi, Hongdong Li, and Jason Saragih.\nMegane: Mor-\nphable eyeglass and avatar network.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 12769\u201312779, 2023. 3\n[24] Wenbo Li, Zhicheng Wang, Binyi Yin, Qixiang Peng, Yum-\ning Du, Tianzi Xiao, Gang Yu, Hongtao Lu, Yichen Wei,\nand Jian Sun. Rethinking on multi-stage networks for human\npose estimation. arXiv preprint arXiv:1901.00148, 2019. 3\n[25] Yuwei Li, Longwen Zhang, Zesong Qiu, Yingwenqi Jiang,\nNianyi Li, Yuexin Ma, Yuyao Zhang, Lan Xu, and Jingyi Yu.\nNimble: a non-rigid hand model with bones and muscles.\nACM Transactions on Graphics (TOG), 41(4):1\u201316, 2022.\n2, 6\n9\n[26] Jae Hyun Lim and Jong Chul Ye. Geometric gan. arXiv\npreprint arXiv:1705.02894, 2017. 5\n[27] Shanchuan Lin, Linjie Yang, Imran Saleemi, and Soumyadip\nSengupta. Robust high-resolution video matting with tempo-\nral guidance. In WACV, 2022. 2\n[28] Stephen Lombardi,\nTomas Simon,\nGabriel Schwartz,\nMichael Zollhoefer, Yaser Sheikh, and Jason Saragih. Mix-\nture of volumetric primitives for efficient neural rendering.\nACM Transactions on Graphics (ToG), 40(4):1\u201313, 2021. 3\n[29] Abhimitra Meka, Christian Haene, Rohit Pandey, Michael\nZollh\u00a8ofer, Sean Fanello, Graham Fyffe, Adarsh Kowdle,\nXueming Yu, Jay Busch, Jason Dourgarian, et al.\nDeep\nreflectance fields: high-quality facial reflectance field infer-\nence from color gradient illumination. ACM Transactions on\nGraphics (TOG), 38(4):1\u201312, 2019. 3\n[30] Gyeongsik Moon and Kyoung Mu Lee. I2l-meshnet: Image-\nto-lixel prediction network for accurate 3d human pose and\nmesh estimation from a single rgb image.\nIn Computer\nVision\u2013ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23\u201328, 2020, Proceedings, Part VII 16, pages\n752\u2013768. Springer, 2020. 2\n[31] Gyeongsik Moon, Ju Yong Chang, and Kyoung Mu Lee.\nV2v-posenet: Voxel-to-voxel prediction network for accu-\nrate 3d hand and human pose estimation from a single depth\nmap. In Proceedings of the IEEE conference on computer\nvision and pattern Recognition, pages 5079\u20135088, 2018. 2\n[32] Gyeongsik Moon, Takaaki Shiratori, and Kyoung Mu Lee.\nDeephandmesh: A weakly-supervised deep encoder-decoder\nframework for high-fidelity hand mesh modeling. In Com-\nputer Vision\u2013ECCV 2020: 16th European Conference, Glas-\ngow, UK, August 23\u201328, 2020, Proceedings, Part II 16, pages\n440\u2013455. Springer, 2020. 2, 3, 1\n[33] Gyeongsik Moon, Shunsuke Saito, Weipeng Xu, Rohan\nJoshi, Julia Buffalini, Harley Bellan, Nicholas Rosen, Jesse\nRichardson, Mallorie Mize, Philippe De Bree, et al.\nA\ndataset of relighted 3d interacting hands.\narXiv preprint\narXiv:2310.17768, 2023. 8\n[34] Franziska Mueller, Dushyant Mehta, Oleksandr Sotny-\nchenko, Srinath Sridhar, Dan Casas, and Christian Theobalt.\nReal-time hand tracking under occlusion from an egocentric\nrgb-d sensor. In Proceedings of the IEEE International Con-\nference on Computer Vision, pages 1154\u20131163, 2017. 2\n[35] Franziska Mueller,\nFlorian Bernard,\nOleksandr Sotny-\nchenko, Dushyant Mehta, Srinath Sridhar, Dan Casas, and\nChristian Theobalt. Ganerated hands for real-time 3d hand\ntracking from monocular rgb. In Proceedings of the IEEE\nconference on computer vision and pattern recognition,\npages 49\u201359, 2018.\n[36] Franziska Mueller, Micah Davis, Florian Bernard, Oleksandr\nSotnychenko, Mickeal Verschoor, Miguel A Otaduy, Dan\nCasas, and Christian Theobalt. Real-time pose and shape\nreconstruction of two interacting hands with a single depth\ncamera. ACM Transactions on Graphics (ToG), 38(4):1\u201313,\n2019. 2\n[37] Akshay Mundra, Mallikarjun B R, Jiayi Wang, Marc Haber-\nmann, Christian Theobalt, and Mohamed Elgharib.\nLive-\nhand: Real-time and photorealistic neural hand rendering,\n2023. 2\n[38] Thomas Nestmeyer, Jean-Franc\u00b8ois Lalonde, Iain Matthews,\nand Andreas Lehrmann. Learning physics-guided face re-\nlighting under directional light.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5124\u20135133, 2020. 3\n[39] Rohit Pandey, Sergio Orts Escolano, Chloe Legendre, Chris-\ntian Haene, Sofien Bouaziz, Christoph Rhemann, Paul De-\nbevec, and Sean Fanello. Total relighting: learning to relight\nportraits for background replacement. ACM Transactions on\nGraphics (TOG), 40(4):1\u201321, 2021. 3, 5, 7\n[40] Rolandos\nAlexandros\nPotamias,\nStylianos\nPloumpis,\nStylianos Moschoglou, Vasileios Triantafyllou, and Stefanos\nZafeiriou. Handy: Towards a high fidelity 3d hand shape\nand appearance model.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 4670\u20134680, 2023. 2, 6, 3\n[41] Neng Qian, Jiayi Wang, Franziska Mueller, Florian Bernard,\nVladislav Golyanik, and Christian Theobalt. Html: A para-\nmetric hand texture model for 3d hand reconstruction and\npersonalization. In Computer Vision\u2013ECCV 2020: 16th Eu-\nropean Conference, Glasgow, UK, August 23\u201328, 2020, Pro-\nceedings, Part XI 16, pages 54\u201371. Springer, 2020. 2\n[42] Haonan Qiu, Zhaoxi Chen, Yuming Jiang, Hang Zhou, Xi-\nangyu Fan, Lei Yang, Wayne Wu, and Ziwei Liu. Relitalk:\nRelightable talking portrait generation from a single video.\narXiv preprint arXiv:2309.02434, 2023. 3\n[43] Daniel Roich, Ron Mokady, Amit H Bermano, and Daniel\nCohen-Or. Pivotal tuning for latent-based editing of real im-\nages. ACM Trans. Graph., 2021. 3\n[44] Javier Romero, Dimitrios Tzionas, and Michael J Black. Em-\nbodied hands: modeling and capturing hands and bodies to-\ngether. ACM Transactions on Graphics (TOG), 36(6):1\u201317,\n2017. 2, 3\n[45] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmen-\ntation. In Medical Image Computing and Computer-Assisted\nIntervention\u2013MICCAI 2015: 18th International Conference,\nMunich, Germany, October 5-9, 2015, Proceedings, Part III\n18, pages 234\u2013241. Springer, 2015. 1\n[46] Kripasindhu Sarkar, Marcel C. Buehler, Gengyan Li, Daoye\nWang, Delio Vicini, J\u00b4er\u00b4emy Riviere, Yinda Zhang, Sergio\nOrts-Escolano, Paulo Gotardo, Thabo Beeler, and Abhimitra\nMeka. Litnerf: Intrinsic radiance decomposition for high-\nquality view synthesis and relighting of faces. In ACM SIG-\nGRAPH Asia 2023, 2023. 8\n[47] Soumyadip Sengupta, Angjoo Kanazawa, Carlos D Castillo,\nand David W Jacobs. Sfsnet: Learning shape, reflectance\nand illuminance of facesin the wild\u2019. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 6296\u20136305, 2018. 3\n[48] Breannan Smith, Chenglei Wu, He Wen, Patrick Peluse,\nYaser Sheikh, Jessica K Hodgins, and Takaaki Shiratori.\nConstraining dense hand surface tracking with elasticity.\nACM Transactions on Graphics (TOG), 39(6):1\u201314, 2020.\n2\n[49] Srinath Sridhar, Antti Oulasvirta, and Christian Theobalt. In-\nteractive markerless articulated hand motion tracking using\n10\nrgb and depth data. In Proceedings of the IEEE international\nconference on computer vision, pages 2456\u20132463, 2013. 2\n[50] Srinath Sridhar, Franziska Mueller, Antti Oulasvirta, and\nChristian Theobalt.\nFast and robust hand tracking using\ndetection-guided optimization. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition,\npages 3213\u20133221, 2015. 2\n[51] Tiancheng Sun, Zexiang Xu, Xiuming Zhang, Sean Fanello,\nChristoph Rhemann, Paul Debevec, Yun-Ta Tsai, Jonathan T\nBarron,\nand Ravi Ramamoorthi.\nLight stage super-\nresolution:\ncontinuous high-frequency relighting.\nACM\nTransactions on Graphics (TOG), 39(6):1\u201312, 2020. 3\n[52] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model\nscaling for convolutional neural networks. In International\nconference on machine learning, pages 6105\u20136114. PMLR,\n2019. 5\n[53] Ayush Tewari, Justus Thies, Ben Mildenhall, Pratul Srini-\nvasan, Edgar Tretschk, Wang Yifan, Christoph Lassner, Vin-\ncent Sitzmann, Ricardo Martin-Brualla, Stephen Lombardi,\net al. Advances in neural rendering. In Computer Graphics\nForum, pages 703\u2013735. Wiley Online Library, 2022. 2\n[54] Anastasia Tkach, Mark Pauly, and Andrea Tagliasacchi.\nSphere-meshes for real-time hand modeling and tracking.\nACM Transactions on Graphics (ToG), 35(6):1\u201311, 2016. 2\n[55] Bohan Wang, George Matcuk, and Jernej Barbi\u02c7c. Hand mod-\neling and simulation using stabilized magnetic resonance\nimaging. ACM Transactions on Graphics (TOG), 38(4):1\u2013\n14, 2019. 2\n[56] Yifan Wang, Aleksander Holynski, Xiuming Zhang, and\nXuaner Zhang.\nSunstage: Portrait reconstruction and re-\nlighting using the sun as a light stage. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 20792\u201320802, 2023. 3\n[57] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli.\nImage quality assessment: from error visibility to structural\nsimilarity. IEEE Transactions on Image Processing, 13(4):\n600\u2013612, 2004. 5\n[58] Andreas Wenger, Andrew Gardner, Chris Tchou, Jonas\nUnger,\nTim Hawkins,\nand Paul Debevec.\nPerfor-\nmance relighting and reflectance transformation with time-\nmultiplexed illumination. ACM Transactions on Graphics\n(TOG), 24(3):756\u2013764, 2005. 3\n[59] Tim Weyrich, Wojciech Matusik, Hanspeter Pfister, Bernd\nBickel, Craig Donner, Chien Tu, Janet McAndless, Jinho\nLee, Addy Ngan, Henrik Wann Jensen, et al. Analysis of\nhuman faces using a measurement-based skin reflectance\nmodel. ACM Transactions on Graphics (ToG), 25(3):1013\u2013\n1024, 2006. 2\n[60] Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany,\nShiqin Yan, Numair Khan, Federico Tombari, James Tomp-\nkin, Vincent Sitzmann, and Srinath Sridhar. Neural fields in\nvisual computing and beyond. In Computer Graphics Forum,\npages 641\u2013676. Wiley Online Library, 2022. 2\n[61] Yingyan Xu, Gaspard Zoss, Prashanth Chandran, Markus\nGross, Derek Bradley, and Paulo Gotardo.\nRenerf: Re-\nlightable neural radiance fields with nearfield lighting. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 22581\u201322591, 2023. 8\n[62] Zexiang Xu, Kalyan Sunkavalli, Sunil Hadap, and Ravi\nRamamoorthi. Deep image-based relighting from optimal\nsparse samples. ACM Transactions on Graphics (ToG), 37\n(4):1\u201313, 2018. 3\n[63] Shugo Yamaguchi, Shunsuke Saito, Koki Nagano, Yajie\nZhao, Weikai Chen, Kyle Olszewski, Shigeo Morishima, and\nHao Li. High-fidelity facial reflectance and geometry infer-\nence from an unconstrained image. ACM Transactions on\nGraphics (TOG), 37(4):1\u201314, 2018. 3\n[64] Haotian Yang, Mingwu Zheng, Wanquan Feng, Haibin\nHuang, Yu-Kun Lai, Pengfei Wan, Zhongyuan Wang, and\nChongyang Ma. Towards practical capture of high-fidelity\nrelightable avatars.\nIn SIGGRAPH Asia 2023 Conference\nProceedings, 2023. 3, 5, 6, 7, 4\n[65] Yu-Ying Yeh, Koki Nagano, Sameh Khamis, Jan Kautz,\nMing-Yu Liu, and Ting-Chun Wang.\nLearning to relight\nportrait images via a virtual light stage and synthetic-to-real\nadaptation. ACM Transactions on Graphics (TOG), 41(6):\n1\u201321, 2022. 3\n[66] Wang Yifan, Lukas Rahmann, and Olga Sorkine-hornung.\nGeometry-consistent neural shape representation with im-\nplicit displacement fields. In International Conference on\nLearning Representations, 2022. 4\n[67] Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, and\nNoah Snavely. Physg: Inverse rendering with spherical gaus-\nsians for physics-based material editing and relighting. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 5453\u20135462, 2021. 2\n[68] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In CVPR, 2018. 5\n[69] Xiuming Zhang, Sean Fanello, Yun-Ta Tsai, Tiancheng Sun,\nTianfan Xue, Rohit Pandey, Sergio Orts-Escolano, Philip\nDavidson, Christoph Rhemann, Paul Debevec, et al. Neu-\nral light transport for relighting and view synthesis. ACM\nTransactions on Graphics (TOG), 40(1):1\u201317, 2021. 3\n[70] Xiuming Zhang, Pratul P Srinivasan, Boyang Deng, Paul De-\nbevec, William T Freeman, and Jonathan T Barron. Ner-\nfactor: Neural factorization of shape and reflectance under\nan unknown illumination. ACM Transactions on Graphics\n(ToG), 40(6):1\u201318, 2021. 2\n[71] Mianlun Zheng, Bohan Wang, Jingtao Huang, and Jernej\nBarbi\u02c7c. Simulation of hand anatomy using medical imaging.\nACM Transactions on Graphics (TOG), 41(6):1\u201320, 2022. 2\n11\nURHand: Universal Relightable Hands\nSupplementary Material\n7. Demo Video and Additional Results\nWe provide the supplementary video on our project\npage (https : / / frozenburning . github . io /\nprojects/urhand), which includes more visual results\nand additional discussions of our work. Specifically, it con-\ntains:\n\u2022 Motivation and key features of URHand.\n\u2022 An animated overview and illustration of the proposed\nframework.\n\u2022 Video comparisons with baseline methods.\n\u2022 Additional qualitative results with diverse identities, in-\ncluding 1) relighting with monochrome directional light,\n2) relighting with arbitrary environment map, and 3)\nquick personalization from a phone scan with correspond-\ning relighting results with environment maps.\n8. Network Architecture\nIn this section, we provide the details of our network archi-\ntecture and hyperparameters for our hand geometry model\n(Sec. 8.1), physical branch (Sec. 8.2), and neural branch\n(Sec. 8.3), respectively.\n8.1. Hand Geometry Autoencoder\nWe design an autoencoder to obtain accurate hand tracking\nand geometry from input fully lit frames similar to [32].\nThe architecture of this autoencoder {Eid, Did, E\u03b8, D\u03b8} is\nillustrated in Figure 8. Specifically, it consists of an identity\nencoder Eid, an identity decoder Did, a pose encoder E\u03b8, and\na pose decoder D\u03b8.\nThe identity encoder Eid takes as input the depth map in\nthe neutral pose and the coordinates of joints in the neutral\npose, which predicts the mean and variance of the distri-\nbution of the identity code (i.e. ID code). The inputs of\nthe identity encoder have normalized viewpoints by rigidly\naligning them to a reference coordinate system; hence both\npose and viewpoints are normalized and only identity in-\nformation is included in them. The identity decoder Did\nlearns to decode the identity-dependent offset of joints and\nvertices from the ID code z. The identity-dependent offset\nof joints is responsible for adjusting 3D joint coordinates in\nthe template space for each identity, and the offset of ver-\ntices are for adjusting 3D vertices in the template space for\neach identity. The pose encoder E\u03b8 directly regresses hand\npose \u03b8 from the input image and the coordinates of joints.\nThe pose decoder D\u03b8 learns to predict the pose-dependent\noffset of vertices given the pose \u03b8 and ID code z. To get\nposed 3D meshes, we apply the three types of correctives to\nthe template mesh and perform linear blend skinning with\nthe estimated 3D pose from the pose encoder.\nWe train this autoencoder {E\u03b8, D\u03b8, Eid, Did} on fully lit\nframes with all identities to obtain a general hand tracker.\nThe autoencoder is trained by minimizing 1) L1 distance\nbetween joint coordinates, 2) point-to-point L1 distance\nfrom 3D scans with weight 10, 3) KL-divergence of the\nID code with weight 0.001, and 4) various regularizers like\nMoon et al. [32]. We freeze it during the training of UR-\nHand as well as the quick personalization from phone scans.\n8.2. Physical Branch\nThe physical branch of URHand consists of a 2D U-\nNet [45] FG and a parametric BRDF [3] Fpb, where only\nthe U-Net FG contains optimizable parameters. The U-Net\nencoder is a 6-layer convolutional neural network (CNN)\nwith channel sizes (3, 64, 64, 64, 64, 64, 64), which takes\nas input the mean texture T \u2208 R1024\u00d71024\u00d73. Here, the\nhand pose \u03b8 is tiled into a UV-aligned 2D feature map \u03b8\u2032,\nconcatenated with the output feature from the U-Net en-\ncoder as a joint feature F\u03b8,id, and passed to the U-Net\ndecoder. The U-Net decoder is a 6-layer CNN with skip\nconnections from the U-Net encoder, with channel sizes\n(64, 64, 64, 64, 64, 64, 2). We use a transposed convolution\nlayer followed by bilinear interpolation as the upsampling\nlayer in the U-Net decoder. The U-Net decoder predicts the\ndisplacement map \u03b4d \u2208 R1024\u00d71024 and the roughness map\n\u03b2 \u2208 R1024\u00d71024. We unwrap the coarse mesh M from our\nhand geometry autoencoder into the UV space to obtain the\ncorresponding coarse normal map n \u2208 R1024\u00d71024\u00d73. The\npredicted displacement map is applied on top of this coarse\nnormal map to obtain the refined normal map \u02c6n according\nto Eq. 2 in the main paper.\nThe parametric BRDF Fpb takes as input the refined nor-\nmal map \u02c6n, the roughness map \u03b2, light L = {Li(\u03c9i)}i,\nand view direction d. The physics-inspired shading features\nFpb = {Cd\npb, Cs\npb} are computed accordingly. Specifi-\ncally, the diffuse feature Cd\npb is computed as:\nCd\npb =\nZ\nLi(\u03c9i) \u00b7 Vi \u00b7 (\u03c9i \u00b7 \u02c6n)d\u03c9i,\n(10)\nwhere Li(\u03c9i) is the light intensity from the incident direc-\ntion \u03c9i, Vi is the visibility given the light Li. Furthermore,\n1\nResNet-18\nDepth in \nNeutral \nPose\nJoints in \nNeutral Pose\nID \nFeature\nResNet-50\nImage\nJoints in \nPoses\nFC\nPose \nFeature\nFC\nFC\nMean\nVar\nID Code \ud835\udc9b\nPose \ud835\udf3d\nFC\nFC\nFC\nFC\nID-dependent \nVertex Offset\nPose-dependent \nVertex Offset\n\u2026\n\u2026\n\u2026\nID Encoder \u2130id\nPose Encoder \u2130\ud835\udf03\nID Decoder \ud835\udc9fid\nPose Decoder \ud835\udc9f\ud835\udf03\nconcatenate\ngrid sample\nID-dependent \nSkeleton Offset\nFigure 8. The architecture of our hand geometry autoencoder. The identity encoder Eid takes as input the depth map in the neutral\npose and the coordinates of joints in the neutral pose, which predicts the mean and variance of the distribution of the identity code (i.e.\nID code). The identity decoder Did learns to decode the identity-dependent offset of vertices and skeletons from the ID code z. The pose\nencoder E\u03b8 directly regresses hand pose \u03b8 from the input image and the coordinates of joints. The pose decoder D\u03b8 learns to predict the\npose-dependent offset of vertices given the pose \u03b8 and ID code z.\nthe specular feature Cs\npb is computed as:\nCs\npb =\nZ\nD \u00b7 F \u00b7 G \u00b7 Li(\u03c9i) \u00b7 Vi \u00b7 (\u03c9i \u00b7 \u02c6n)d\u03c9i,\n(11)\nD =\n\u03b24\n\u03c0[(h \u00b7 \u02c6n)2(\u03b24 \u2212 1) + 1]2 ,\n(12)\nF = F0 + (1 \u2212 F0) \u00b7 2[\u03bbF 1(d\u00b7h)+\u03bbF 2](d\u00b7h),\n(13)\nG =\n1\n4[(\u02c6n \u00b7 d)(1 \u2212 K) + K][(\u02c6n \u00b7 \u03c9i)(1 \u2212 K) + K],\n(14)\nh =\n\u03c9i + d\n||\u03c9i + d||, K = (\u03b2 + 1)2\n8\n,\n(15)\nwhere we set Fresnel coefficient F0\n= 0.04, \u03bbF 1\n=\n\u22125.55473, and \u03bbF 2 = \u22126.98316, respectively.\n8.3. Neural Branch\nThe neural branch of URHand consists of a non-linear\nnetwork Fnl and a linear network Fl (i.e. linear light-\ning model).\nWe illustrate the detailed architecture of\nthe neural branch in Figure 9.\nSpecifically, the non-\nlinear network Fnl is a 7-layer CNN with channel sizes\n(128, 256, 128, 128, 64, 32, 16, 4), which takes as input the\npose- and identity-dependent joint feature F\u03b8,id. The linear\nnetwork Fl, namely the linear lighting model, consists of an\nencoder Fl\u2212enc and a decoder Fl\u2212dec. The linear encoder\nFenc consists of unbiased convolutional layers which takes\nas input the concatenated physics-inspired shading features\n{Cd\npb, Cs\npb}. The linear decoder is a 7-layer unbiased CNN\nwith channel sizes (128, 256, 128, 128, 64, 32, 16, 4). We\nfuse the linear features from Fl\u2212enc and the non-linear fea-\ntures from Fnl as layer-wise modulation at each layer of the\nlinear decoder Fl\u2212dec according to Eq. 5 in the main paper.\nThe predicted gain map g \u2208 R1024\u00d71024\u00d73 and bias map\nb \u2208 R1024\u00d71024 contributes to the final rendering according\nto Eq. 6 in the main paper.\n9. Adaptation to a Phone Scan\nIn this section, we present the details of how to quickly\nadapt URHand to a personalized use case from a phone\nscan. We use a single iPhone 12 to scan a hand, which in-\ncorporates a depth sensor that can be used to extract better\ngeometry of the user\u2019s hand. Our phone scans include hands\nwith neutral finger poses and static 3D global translations\nwith varying 3D global rotations to expose most of the hand\nsurfaces.\nWe pre-process the phone scan with 1) our in-house 2D\nhand keypoint detector to obtain 2D hand joint coordinates\nand 2) RVM [27] to obtain the foreground mask of the\nphone scan. After the preprocessing, we optimize 3D global\nrotation, 3D pose, 3D global translation, and ID code of\nthe phone scan. The 3D global rotation, 3D pose, and 3D\nglobal translation are optimized for each frame, and a single\nID code is shared across all frames as all frames are from\na single identity. We optimize them by minimizing 1) L1\ndistance between projected 2D joint coordinates and targets\n2\nID&Pose Feature \ud835\udc05\ud835\udf03,id\nConvT\nLeaky ReLU\nConvT\nLeaky ReLU\n\u2131nl\n\ud835\udc57\nUnbiased ConvT\n\u2131l\u2212dec\n\ud835\udc57\n\u2131l\u2212dec\n\ud835\udc57+1\n\u2299\nUnbiased Conv\nUnbiased Conv\nLight Feature\nUnbiased ConvT\nUnbiased Conv\n\u2131l\u2212enc\n\ud835\udc57\nUnbiased Conv\n\u2026\n\u2026\n\u2026\n\u2026\n\u2026\n\ud835\udc02pb\n\ud835\udc51 , \ud835\udc02pb\n\ud835\udc60\n\u2131nl\n\u2131l\u2212dec\n\u2131l\u2212enc\nFigure 9. The architecture of the neural branch of URHand.\nThe neural branch of our model consists of a non-linear network\nFnl and a linear network Fl (i.e. linear lighting model). Notably,\nwe remove all non-linear activation and use the convolutional layer\nwithout bias in the linear network so that the linearity of the output\nis explicitly kept w.r.t. the input physics-inspired shading feature\n{Cd\npb, Cs\npb}. This figure also illustrates how Eq. 5 in the main\npaper is implemented within our network.\n2) L1 distance between differentiably rendered masks and\ntargets with weight 50, and 3) L1 distance between differ-\nentiably rendered depth maps and targets with weight 100.\nAfter the optimization, we unwrap per-frame images to UV\nspace and average intensity values at each texel considering\nthe visibility to get the unwrapped texture map.\nTo remove shadows from the unwrapped textures, we\nfirst obtain the average color of the foreground pixels of\ncaptured images. Then, we optimize shadow as a 1-channel\ndifference (i.e., darkness difference) between the averaged\ncolor and the captured image in the UV space.\nTo pre-\nvent the shadow from dominating local sharp textures (i.e.,\nhairs and tattoos), we apply a total variation regularizer to\nthe shadow. The unwrapped texture without the shadow is\nsimply obtained by dividing the unwrapped texture by the\nshadow. We empirically observed that such a statistical ap-\nproach produces better shadow than the physics-based ap-\nproach of HARP [20], which assumes a single point light,\nas there is often more than one light source in the scan en-\nvironment. Then, we take this texture map after shadow\nremoval as the input to URHand for relighting without any\nfinetuning.\n10. Implementation of Baselines\nIn this section, we present implementation details of our\nbaseline methods for comparisons in the main paper.\nSpecifically, we introduce our modifications to Relightable-\nHands [14] and Handy [40] in Sec. 10.1. Moreover, we\nintroduce our implementations of all baselines for ablation\nstudies in Sec. 10.2.\n10.1. Methods for Main Comparisons\nRelightableHands [14] is originally proposed for per-\nidentity relightable appearance reconstruction tailored with\nvolumetric representation [28].\nFor a fair comparison,\nwe re-implement it with our mesh-based representation.\nSpecifically, we leverage a U-Net as the texture decoder\nAtex that takes as input the UV-aligned view direction, light\ndirection, and visibility. The pose parameter is tiled into\na UV-aligned feature map and concatenated with the bot-\ntleneck representation of the U-Net. This texture decoder\nAtex predicts texture map T \u2208 R1024\u00d71024\u00d73 and shadow\nmap S \u2208 R1024\u00d71024 in the UV space. The final texture C\nfor rendering is obtained as:\nC = \u03c3(S)(ReLU(\u03bbsT) + \u03bbb),\n(16)\nwhere \u03c3(\u00b7) is the sigmoid function, ReLU(x) = max(0, x),\n\u03bbs is a scale factor, and \u03bbb is a bias parameter. In our exper-\niments, we set \u03bbs = 25, and \u03bbs = 100, respectively.\nHandy [40] leverages the parametric hand model [44] as\nthe shape representation and StyleGAN3 [18] as the texture\nmodel. As the shape and latent code regressor are not pub-\nlicly available, we cannot infer the latent code w or shape\nparameters from input images as in the original paper. In-\nstead, we fit hand shape parameters using our multiview\nfully lit frames on the training segments. Then, we do the\nStyleGAN inversion following [43]. Specifically, we ran-\ndomly initialize the latent code w and optimize it given the\nreconstruction loss between the ground truth fully lit images\nand the images rendered with the current predicted texture\nmap. In our experiments, we optimize 50,000 iterations for\neach identity. We use the Adam [21] optimizer with the ini-\ntial learning rate as 1\u00d710\u22123. Once the inversion is done, we\ntake the latent code w and feed it into the pretrained texture\nmodel of Handy to get the unwrapped texture map. We treat\nthe unwrapped texture map as the albedo map for physically\nbased relighting evaluation.\n3\n10.2. Baselines for Ablation Studies\nNon-linear model is based on our full model but we add\nLeakyReLU function to all layers in the original linear net-\nwork Fl which breaks the linearity.\nLinear consistency model is based on the aforementioned\nnon-linear model. We additionally constrain the linearity\nof this non-linear network by applying linearity consistency\nloss during training.\nSpecifically, for every n iterations,\nwe augment two physics-inspired shading features with two\nrandom scalars, i.e. a1F1\npb + a2F2\npb, where a1, a2 \u2208 (0, 1).\nThe linearity consistency loss is defined as:\nLlc = ||a1Fl(F1\npb)+a2Fl(F2\npb)\u2212Fl(a1F1\npb +a2F2\npb)||2 (17)\nMLP-based linear model [64] is a variant of the linear\nlighting model with no spatially varying lighting feature.\nWe replace the encoder of linear network Fl\u2212enc as a one-\nlayer MLP without bias. It takes as input the environment\nmap with a resolution of 3 \u00d7 16 \u00d7 32, and predicts the\nlighting feature. Then we reshape the prediction into a UV-\naligned feature map with a resolution of 128 \u00d7 16 \u00d7 16 and\nfeed into the decoder of linear network to predict the final\ngain and bias map for neural rendering.\nPhong based model is implemented by replacing our\nphysics-inspired shading feature Fpb = {Cd\npb, Cs\npb} with\nsimple diffuse and specular feature from the Phong re-\nflectance model.\nThis neural relighting model is similar\nto [2, 14] with no learnable material parameter.\nw/o Specular is the baseline where we dropout the specular\nfeature Cs\npb during training.\nw/o Visibility is the baseline where we do not incorporate\nvisibility Vi when compute the physics-inspired shading\nfeature in Eq. 10 and Eq. 11.\nw/o Refiner is the baseline where we only use the normal\nmap n from the coarse geometry without further refinement\nduring training.\nw/o LGAN is the baseline trained with the reconstruction\nloss Limg and L1 regularization Lreg only.\nw/o Light-aware LGAN is the baseline trained with the\nvanilla adversarial loss without conditional discriminator.\nSpecifically, the adversarial loss of Eq. 8 in the main paper\ndegrades to LGAN = log FD(I) + log[1 \u2212 FD(\u02c6I)], where I\nis the ground truth and \u02c6I is the rendered image.\nw/o L1 Reg is the baseline trained with the reconstruction\nloss Limg and lighting-aware adversarial loss LGAN only.\n4\n"
  },
  {
    "title": "The Impact of Reasoning Step Length on Large Language Models",
    "link": "https://arxiv.org/pdf/2401.04925.pdf",
    "upvote": "15",
    "text": "The Impact of Reasoning Step Length on Large\nLanguage Models\nMingyu Jin1, Qinkai Yu2, Dong Shu1, Haiyan Zhao3, Wenyue Hua4,\nYanda Meng2, Yongfeng Zhang4, Mengnan Du3\n1Northwestern University\n2University of Liverpool\n3New Jersey Institute of Technology\n4Rutgers University\n{u9o2n2, dongshu2024}@u.northwestern.edu,\n{sgqyu9, yanda.meng2}@liverpool.ac.uk,\n{hz54, mengnan.du}@njit.edu,\n{yongfeng.zhang, wenyue.hua}@rutgers.edu\nAbstract\nChain of Thought (CoT) is significant in improving the reasoning abilities of large\nlanguage models (LLMs). However, the correlation between the effectiveness of\nCoT and the length of reasoning steps in prompts remains largely unknown. To\nshed light on this, we have conducted several empirical experiments to explore\nthe relations. Specifically, we design experiments that expand and compress the\nrationale reasoning steps within CoT demonstrations, while keeping all other\nfactors constant. We have the following key findings. First, the results indicate that\nlengthening the reasoning steps in prompts, even without adding new information\ninto the prompt, considerably enhances LLMs\u2019 reasoning abilities across multiple\ndatasets. Alternatively, shortening the reasoning steps, even while preserving the\nkey information, significantly diminishes the reasoning abilities of models. This\nfinding highlights the importance of the number of steps in CoT prompts and\nprovides practical guidance to make better use of LLMs\u2019 potential in complex\nproblem-solving scenarios. Second, we also investigated the relationship between\nthe performance of CoT and the rationales used in demonstrations. Surprisingly,\nthe result shows that even incorrect rationales can yield favorable outcomes if they\nmaintain the requisite length of inference. Third, we observed that the advantages\nof increasing reasoning steps are task-dependent: simpler tasks require fewer steps,\nwhereas complex tasks gain significantly from longer inference sequences. Code is\navailable at https://github.com/jmyissb/The-Impact-of-Reasoning-Step-Length-on-\nLarge-Language-Models.\n1\nIntroduction\nToday, the advent of large language models (LLMs) and their advanced prompting strategies has\nmarked a significant progression, especially in classical NLP tasks (Kojima et al., 2023; Wei et al.,\n2022; Shao et al., 2023; Lyu et al., 2023). A key innovation among these is the Chain of Thought\n(CoT) prompting technique (Kojima et al., 2023; Wang et al., 2023; Zhang et al., 2022), known for\nits efficacy in multi-step problem solving. This technique, reflecting human sequential reasoning, has\nshown remarkable effectiveness in various challenges, including cross-domain, length-generalization,\nand cross-lingual tasks. The CoT approach, with its logical, step-by-step methodology, offers crucial\ninterpretability in complex problem-solving scenarios. Interestingly, Wang et al. found that even\nincorrect but coherent rationales can improve reasoning performance, highlighting the value of logical\ncontinuity (Wang et al., 2023). Building on this, Fu et al. introduced complexity-based prompting,\nsignificantly improving accuracy and setting new benchmarks (Fu et al., 2023). This research further\nPreprint. Under review.\narXiv:2401.04925v3  [cs.CL]  20 Jan 2024\nInput\nAccuracy 1\nAccuracy 2\nAccuracy 3\n. . .\n. . .\n. . .\nReasoning \nStep N\n. . .\n. . .\n. . .\n. . .\nK more\nReasoning \nSteps\nFigure 1: Increase the length of the reasoning step chain, and the accuracy of problem-solving\nincreases too.\nexplores the relationship between the length of reasoning steps and the accuracy of conclusions,\ndeepening our understanding of effective problem-solving in NLP.\nDespite its promising results, the research community has yet to reach a consensus on the precise\nmechanics of how and why CoT and its variations function effectively. This knowledge gap means\nthat enhancing CoT performance is still a field of exploration, largely reliant on trial-and-error\napproaches. There still lack established systematic methodologies for improving CoT\u2019s effectiveness,\nleaving researchers to rely on conjecture and experimentation. This situation underscores a significant\nopportunity in the field: to develop a deeper, more structured understanding of CoT\u2019s inner workings.\nSuch advancement would not only demystify the current process, but also pave the way for more\nreliable and efficient applications of this technique in various complex NLP tasks.\nIn this work, our aim is to explore whether the reasoning steps are the most critical component of\nthe prompts that make CoT work (see Figure 1). We conduct experiments to investigate this by\nmaintaining strict control over variables. Particularly, when incorporating new reasoning steps, we\nensured that no additional knowledge was introduced. For the zero-shot experiments, we tweaked the\ninitial prompt from \u201clet\u2019s think step by step\u201d to \u201clet\u2019s think step by step, you must think more steps\u201d.\nThen for few-shot setting, we design experiments that expand the rationale reasoning steps within\nCoT demonstrations, while keeping all other factors constant. Our first set of experiments evaluated\nthe improvement in zero-shot and few-shot performance using Auto-CoT (Zhang et al., 2022) with\nour strategic intervention. Subsequently, we assessed the accuracy of different methods across varying\nstep lengths. We then extended our investigation to compare the effectiveness of our strategy on\ndifferent LLMs such as GPT-3.5 and GPT-4. Our findings revealed a significant correlation between\nthe length of the reasoning chain and the capabilities of LLMs, within certain limits. Intriguingly,\nwhen we introduced misleading information into the reasoning chain, the performance still showed\nimprovement. This highlighted a pivotal insight: The key factor appears to be the length of the\nthinking chain rather than its accuracy. We have the following key findings, which we hope can help\nthe community better improve CoT performance.\n\u2022 For few-shot COT, there is a direct linear correlation between step count and accuracy. This\nprovides a quantifiable approach for optimizing CoT prompting in complex reasoning. Specifically,\nlengthening the reasoning steps in prompts considerably enhances LLMs\u2019 reasoning abilities across\nmultiple datasets. Alternatively, shortening reasoning steps, even while preserving key information,\nsignificantly diminishes the reasoning abilities of models.\n\u2022 Even incorrect rationales can yield favorable outcomes if they maintain the requisite length of\ninference. For example, in tasks such as mathematical problems, errors in intermediate numbers\nhave a minor impact due to their process-oriented nature.\n\u2022 The advantages of increasing reasoning steps are task-dependent: simpler tasks necessitate fewer\nsteps, whereas more complex tasks gain significantly from longer inference sequences.\n\u2022 Increased reasoning steps in zero-shot CoT can also significantly improve LLM accuracy. To\nvalidate this approach, we altered the initial prompt from \u201cLet\u2019s think step by step\" to \u201cLet\u2019s think\n2\nstep by step, you must think more steps.\" This modification led to a noticeable enhancement in the\nreasoning abilities of LLMs, particularly evident in datasets involving mathematical problems.\n2\nRelated Work\nIn this section, we summarize two lines of literature that are most relevant to ours.\n2.1\nCoT Prompting\nThe recent surge in computational power has paved the way for the rise of expansive language models.\nWith increasing complexity, these models have unlocked emerging capabilities, notably in-context\nlearning and CoT reasoning (Wei et al., 2022; Brown et al., 2020; Schaeffer et al., 2023).\nIn their seminal work, Brown et al. discovered the ability of large-scale language models to leverage\nin-context learning (ICL) (Brown et al., 2020). ICL strategy involves weaving input-output examples\ndirectly into the prompt, allowing ready-to-use large language models to perform impressively without\nthe need for task-specific fine-tuning. However, despite its promise, this end-to-end methodology\noften falters when confronting complex reasoning challenges.\nBuilding on this, Wei et al. demonstrated that integrating a series of logical reasoning steps into\nthe model demonstrations, called CoT prompting, significantly refines the reasoning capabilities of\nlarge language models (Wei et al., 2022). CoT prompting not only deepens the model\u2019s grasp of the\nnuanced questions and their underlying logic but also yields an articulated sequence of reasoning\nsteps. Zhang et al.\u2019s \u201cAuto-CoT\" method represents a significant advancement in the field of AI\nreasoning (Zhang et al., 2022). By automating the CoT process, it addresses complex problems more\neffectively. And then Yao et al. introduced the \u201cTree of Thoughts\" (ToT) framework, an evolution of\nthe Chain of Thought approach for language model inference (Yao et al., 2023). ToT allows language\nmodels to explore different units of text as intermediate steps in problem-solving. This framework\nenables more deliberate decision-making by considering multiple reasoning paths.\n2.2\nPreliminary Work on Analyzing CoT\nThe development and understanding of CoT reasoning in AI have evolved over time, marked by\nsignificant contributions from various researchers. Initially, Madaan and Yazdanbakhsh (Madaan\n& Yazdanbakhsh, 2022) explored the decomposition of prompts into symbols, patterns, and texts,\nexamining the effects of CoT through counterfactual prompting. This study laid the groundwork for\nunderstanding how different components of a prompt influence AI reasoning. Besides, several studies\nfurthered this understanding. Tang et al. (Tang et al., 2023) investigated the role of semantics in CoT\nreasoning, uncovering a reliance on semantic knowledge from pre-training and challenges in symbolic\nreasoning. Around the same time, Wang et al. focused on the impact of demonstration selection\nin CoT, revealing that the relevance and order of reasoning are more critical than the accuracy of\nreasoning chains (Wang et al., 2023).\nTheoretical perspectives also emerged recently, offering deeper insights into the mechanics of CoT.\nFor example, Li et al. conceptualized CoT as a multi-step combinatorial function, illustrating its role\nin simplifying in-context learning for complex questions (Li et al., 2023). Feng et al. theoretically\ndemonstrated the sufficiency of a fixed-size Transformer for computational tasks and dynamic\nplanning within CoT frameworks (Fu et al., 2023).\nFurther contributions in this field included those of Merrill and Sabharwal, who observed that CoT\ncan improve reasoning abilities, with improvements scaling with the number of intermediate steps\n(Merrill & Sabharwal, 2023). Wu et al. employed gradient-based feature attribution methods to assess\nthe robustness of CoT against question variations and perturbations (Wu et al., 2023).\n3\nAnalyzing Methods\nIn this section, we propose an analysis to examine the relationship between the reasoning steps and\nthe chain-of-thought (CoT) prompting performance. Our central hypothesis is that the reasoning steps\nare the most critical component of CoT prompts, enabling language models to apply more logical\nreasoning when generating responses. To test this, we design experiments that expand and compress\n3\nThink of word\nRead the question again\nRepeat State\nSelf-Verification\nMake Equation\nQ: A coin is heads up. Jeff does not flip the coin. Jen flips the coin. Giselle flips the coin. Noel does not flip the coin. Is the\ncoin still heads up? Note that ?flip? here means ?reverse?.\nBased on this question, \nthe stem of the sentence \nis coin, heads up, flip. \nThink coin: The coin is \na ... Answer: Yes\nThe question is: Given \nthat a coin is initially \nheads up and only Jen \nand Giselle flip it, is the \ncoin ... Answer: Yes\nThe state of the coin \nfrom the beginning is \nheads up. Rachel does \nnot flip the coin, this \nmeans ... Answer: Yes\nThe coin is a double \nsides coin. So the \nanswer can only be \n''yes'' or ''no'' ... Answer: \nYes\nLet's represent Heads \nup = 1. Tails up = -1. \nWe start with the coin \nas heads up, so we start \nwith 1 ... Answer: 1\nReasoning Steps Expansion of CoT Rationales\nReasoning Steps Compression of CoT Rationales\nThink coin: The coin is \na ... Answer: Yes\nCoin is initially heads \nup ... Answer: Yes\nThe state of the coin \nfrom the ... Answer: No\nThe answer are ''yes'' or \n''no'' ... Answer: Yes\nHeads up = 1. Tails up \n= -1 ... Answer: -1\nFigure 2: Increase the length of the thinking chain through the method in the figure, and compress\nthe thinking chain without losing information as much as possible.\nthe rationale reasoning steps within CoT demonstrations, while keeping all other factors constant.\nSpecifically, we systematically vary only the number of reasoning steps, without introducing new\nreasoning content or removing existing reasoning content. We evaluate both zero-shot and few-shot\nCoT prompts in the following sections. The overall experimental procedure is illustrated in Figure 2.\nThrough this controlled analysis, we aim to shed light on how CoT influences the LLM\u2019s ability to\nproduce logically sound responses.\n3.1\nPreliminary\nZero-Shot-CoT (Kojima et al., 2023) is a template-based zero-shot prompt for chain-of-thought\nreasoning. The core idea is to add \u201cLet\u2019s think step by step\u201d or other similar text. Compared to\nZero-Shot-CoT, Few-Shot-CoT provides more examples in the prompt for chain-of-thought reasoning.\nAmong them, Manual-CoT (Wei et al., 2022), Auto-CoT (Zhang et al., 2022) are two popular\nmethods.\nManual-CoT: Manual-CoT prompting relies on a few manually designed demonstrations, each\ncomposed of a question and a reasoning chain leading to an answer, to improve language models\u2019\nreasoning performance.\nAuto-CoT: Auto-CoT eliminates the need for manual demonstration design by automatically con-\nstructing demonstrations through clustering test questions to select diverse examples and generating\nreasoning chains using the language model\u2019s own zero-shot reasoning capability.\n3.2\nAnalyzing Zero-shot CoT\nIn the zero-shot scenario, we modified the initial prompt from \u201cLet\u2019s think step by step\" to \u201cLet\u2019s\nthink step by step, you must think more steps.\" This change was implemented because, unlike the\nFew-shot CoT context, we cannot introduce additional reasoning steps in the example. By altering the\ninitial prompt, we guide the LLM to engage in more extensive thinking. This approach is crucial as it\nenhances the model\u2019s accuracy without the need for incremental training or additional example-driven\nadjustments typical in few-shot CoT scenarios. This refined strategy ensures a more comprehensive\nand detailed reasoning process, significantly improving the model\u2019s performance in zero-shot settings.\n4\nTable 1: Case of Think About The Word, select a few words and ask the model to think about their\nmeaning in the text.\nThink About The Word\nPrompt:\nQ: Could someone in Tokyo take a taxi to the Metropolitan Museum of Art?\nA: Let\u2019s think step by step. The stem of the sentence is Tokyo, take a taxi, Metropolitan Museum\nof Art. Think about Tokyo... Think about taking a taxi... Think about Metropolitan Museum of\nArt... Inference: Tokyo is in Japan and the Metropolitan Museum of Art is in New York. The two\nplaces are separated by the sea, so you can\u2019t take a taxi there. The answer is no.\nQ: Could a chipmunk fit 100 chocolate chips in his mouth?\nAnswer:\nA: Based on this question, the stem of the sentence is chipmunk, fit, 100 chocolate chips, mouth.\nThink about chipmunk... Think about fit... Think about chocolate chips... Inference: Chipmunks\nare small animals with relatively small mouths, so it\u2019s unlikely that a chipmunk would be able to\nfit 100 chocolate chips in its mouth at once. The answer is no.\nPrediction: no\nGround Truth: no\n3.3\nAnalyzing Few-shot CoT\nIn this section, we aim to modify the reasoning chains within CoT rationales, either by adding or\ncompressing reasoning steps. The goal is to examine how changes in reasoning structure influence\nLLM decision-making. During rationale expansion, we will avoid introducing any new task-relevant\ninformation. This isolates reasoning steps as the only variable under study.\nTo this end, we plan to investigate the following strategies to expand the reasoning steps for different\nLLM applications. There are usually fixed patterns in the way people think about a problem, for\nexample, by repeating the question over and over again to gain a deeper understanding, by creating\nmathematical equations to reduce the burden on memory, by analyzing the meaning of words in the\nquestion to help understand the topic, by summarizing the current state to simplify the description of\nthe topic. Based on the inspiration of Zero-Shot-CoT and Auto-CoT, we expected the process of CoT\nto become a standardized pattern, and lead to the right result by restriction on the direction of CoT\nthinking in the prompt section. The core of our approach is to simulate the process of human thinking\nand reshape the chain of thought. We give five general prompt strategies in Table 6 in the Appendix.\n\u2022 Think About The Word: This strategy is to ask the model to interpret the word and rebuild the\nknowledge base. Typically a word has multiple different meanings, and the effect of this is to get\nthe model to think outside the box and reinterpret the words in the problem based on the generated\ninterpretations. This process does not introduce new information. In the prompt, we give examples\nof the words that the model is thinking about, and the model automatically picks words for this\nprocess based on the new question.\n\u2022 Read the question again: Read the questions repeatedly to reduce the interference of other texts\non the chain of thought. In short, we let the model remember the questions.\n\u2022 Repeat State: Similar to repeated readings, we include a small summary of the current state after a\nlong chain of reasoning, aiming to help the model simplify its memory and reduce the interference\nof other texts in the CoT.\n\u2022 Self-Verification: Humans will check if their answers are correct when answering questions.\nTherefore, before the model gets the answer, we add a self-verification process to judge whether\nthe answer is reasonable based on some basic information.\n\u2022 Make Equation: For mathematical problems, Make Equations can help humans summarize and\nsimplify memory. And for some problems that require the assumption of an unknown number x,\nestablishing an equation is an essential process. We simulated this process and let the model try to\nmake equations in mathematical problems.\n5\nTable 2: Comparison of accuracy of our method with four baselines on eight datasets\nModels\nArithmetic\nCommonsense\nSymbolic\nMultiArith GSM8K AQuA SingleEq SVAMP\nStrategy\nLetter Coin\nZero-Shot\n40\n30.4\n29.9\n82.7\n56\n59\n43\n79.8\nZero-Shot-CoT\n91.5\n64.1\n55.6\n87.43\n79.99\n58.34\n69\n93\nManual-CoT\n93.5\n64.7\n55\n92.1\n82.3\n65.3\n75\n92.7\nAuto-CoT\n94\n65.8\n65\n92\n81.9\n65.3\n73.5\n93\nMust Think More Step (Zero-Shot-CoT)\n95.2\n76.1\n62.11\n87.43\n79.99\n72.6\n69\n97\nAdd Reasoning Step (Manual-CoT)\n97\n70.1\n62.5\n88.97\n85.2\n68.86\n77.8\n97.3\nAdd Reasoning Step (Auto-CoT)\n97.2\n78.8\n64.03\n92.71\n83.7\n70.26\n71.2\n99\nTable 3: The Case of Wrong Prompt, change one of the steps in the chain of thought and preserve\noverall coherence\nOriginal Prompt\nWrong Prompt\nQ: Joan has 10 books. Tom has 38 books.\nHow many books do they have?\nRationale: Let\u2019s think step by step. Joan has 10 books.\nTom has 38 books. we have equation books = 10 + 38 = 48.\nThey have 10 + 38 = 48 books together.\nQ: Megan had 217 markers. Robert gave her 109 more markers.\nHow many markers does Megan have altogether?\nQ: Joan has 10 books. Tom has 38 books.\nHow many books do they have?\nRationale: Let\u2019s think step by step. Joan has 10 books.\nTom has 38 books. we have equation books = 10 + 8 = 48.\nThey have 10 + 38 = 48 books together.\nQ: Megan had 217 markers. Robert gave her 109 more markers.\nHow many markers does Megan have altogether?\nOverall, our prompt strategies all saw corresponding patterns in the model\u2019s responses. We give an\nexample in Table 1, examples of the other four strategies can be found in the appendix. In Section 4\nwe will perform a quantitative analysis to validate the effectiveness of our strategies.\n4\nExperimental Results\nWe conduct experiments to answer the following research questions: RO1: What is the relationship\nof rational reasoning steps in demonstrations with CoT performance? (Section 4.2) RO2: Can we\nconfirm that the reasoning steps are the only factor that affects LLM performance? (Section 4.3) RO3:\nWill compressing reasoning steps in few-shot demonstrations hurt LLM performance? (Section 4.4),\nRO4: Can we observe the scaling phenomenon, i.e., the required reasoning steps be related to the\nsize of the LLMs? (Section 4.5) and RO5: What is the influence of questions in rationale towards the\nLLM reasoning ability? (Section 4.6)\n4.1\nExperimental Setups\nIn this section, we introduce the general experimental setups.\nDatasets and Models. We evaluate our proposal on eight dataset (MultiArith (Roy & Roth, 2015),\nGSM8K (Cobbe et al., 2021), AQuA (Ling et al., 2017), SingleEq (Koncel-Kedziorski et al., 2015),\nSAVMP (Patel et al., 2021), Letter (Wei et al., 2022), Coin (Wei et al., 2022), Strategyqa (Geva et al.,\n2021)). We employ three models to validate the performance of our proposed method, which are:\ntext-davinci-002(Brown et al., 2020), GPT-3.5-turbo-1106 (Ouyang et al., 2022), GPT-4 (OpenAI,\n2023). All models are accessed via the OpenAI API key.\nPrompt. We have shown the proposed process pipeline in Section 3 Analyzing Methods. The\nexperimental part follows the same approach.\nBaselines. We compare our methods with four baseline methods: Zero-Shot (Kojima et al., 2023),\nZero-Shot-CoT (Kojima et al., 2023), Manual-CoT (Wei et al., 2022), Auto-CoT (Zhang et al., 2022).\nThe results are in the Table 2.\nEvaluation Metrics. Accuracy is used to assess a model\u2019s ability on classification tasks and is\ncommonly used for multichoice and yes/no tests: Accuracy = Ncorrect/Ntotal.\nImplementation Details:\n6\n0\n2\n4\n6\nNumber of Add Step\n92\n94\n96\nAccuracy\nFew-Shot-CoT\nAuto-CoT\n(a) MultiArith\n0\n2\n4\n6\nNumber of Add Step\n65\n70\n75\nAccuracy\nFew-Shot-CoT\nAuto-CoT\n(b) GSM8K\n0\n2\n4\n6\nNumber of Add Step\n55\n60\n65\n70\nAccuracy\nFew-Shot-CoT\nAuto-CoT\n(c) AQuA\n0\n2\n4\n6\nNumber of Add Step\n90\n92\n94\nAccuracy\nFew-Shot-CoT\nAuto-CoT\n(d) SingleEq\n0\n2\n4\n6\nNumber of Add Step\n82\n83\n84\n85\nAccuracy\nFew-Shot-CoT\nAuto-CoT\n(e) SAVMP\n0\n2\n4\n6\nNumber of Add Step\n66\n68\n70\nAccuracy\nFew-Shot-CoT\nAuto-CoT\n(f) strategyqa\n0\n2\n4\n6\nNumber of Add Step\n70.0\n72.5\n75.0\nAccuracy\nFew-Shot-CoT\nAuto-CoT\n(g) Letter\n0\n2\n4\n6\nNumber of Add Step\n90.0\n92.5\n95.0\n97.5\nAccuracy\nFew-Shot-CoT\nAuto-CoT\n(h) Coin\nFigure 3: Linear Relationship Between Step Quantity and Accuracy\nMultiArith\nGSM8K\nAQuA\nSingleEq\nsvamp\nstrategy\nLetter\nCoin\n0\n20\n40\n60\n80\n100\nAccuracy\nPrompt with Wrong Answer\nPrompt with True Answer\nFigure 4: Compare the accuracy of the prompt with the true answer\nand the prompt with the wrong answer\nthe single computation when solving Arithmetic-type prob-\nlems. For logic problems similar to those in the Coin dataset,\na deviation in one of the prompt\u2019s results often brings about\nthe fragmentation of the entire chain of thought. We com-\npleted this experiment with GPT-3.5-turbo-1106 and guaran-\nteed performance based on the optimal number of steps for\neach data set derived from the previous experiment. The re-\nsults are shown in Figure 4.\n4.4\nCompressing Reasoning Steps\nIn previous sections, we have demonstrated that increasing\nreasoning steps could improve the LLM reasoning accuracy.\nIn this section, our aim is to answer RO3: Will compressing\nreasoning steps in few-shot demonstrations hurt LLM perfor-\nmance? To this end, we conduct the reasoning steps compres-\nsion experiment, and we employed the technique outlined in\nthe experimental setup to condense the reasoning process in\nexamples from both the baseline automated chain of thought\n(Auto-CoT) and the few-shot chain of thought (Few-Shot-\nCoT), aiming to reduce their number of inference steps. The\nresult is shown in Figure 5. The results revealed a notable\ndecline in their performance, which regressed to a level es-\nMultiArith\nGSM8K\nAQuA\nSingleEq\nsvamp\nstrategy\nLetter\nCoin\n0\n20\n40\n60\n80\n100\nAccuracy\nZero-Shot-CoT\nCompression\nNo Compression\nFigure 5: Compare the accuracy of the prompt with Compression\nand the prompt with No Compression\nsentially equivalent to that achieved by the zero-shot method.\nIt further indicates that increasing COT rationale reasoning\nsteps could improve COT performance and the vice versa.\n4.5\nPerformance on Different Size Models\nIn this chapter, our focus is to answer RO4: Can we ob-\nserve the scaling phenomenon, i.e., the required reasoning\nsteps be related to the size of the LLMs? We examine the\naverage number of inference steps utilized in various models,\nincluding text-davinci-002, GPT-3.5-turbo-1106, and GPT-4.\nWe conducted experiments on GSM8K calculating the aver-\nage inference step for each model to reach peak performance.\nThis dataset has the largest performance difference with text-\ndavinci-002, GPT-3.5-turbo-1106, and GPT-4 out of our 8\ndatasets. It can be observed that the model with the worst ini-\ntial performance, text-davinci-002, our strategy has the high-\nest boosting effect. For the model with the best initial perfor-\nmance, GPT-4, has the highest tolerance to our strategy (no\nperformance decreases). We show the result in Figure 6.\nFigure 3: Linear Relationship Between Step Quantity and Accuracy\n\u2022 Add reasoning steps: This process deploys GPT-4 to modify the Zero-Shot-CoT prompt demo\ngenerated by \u201clet\u2019s think step by step\" to contain the five reasoning steps we mentioned in Section 3\nso that we can define the number of steps and the types of steps to be contained in the demo. We\nthen input the demo as a prompt. We completed the following experiment with this approach.\n\u2022 Reasoning-Step Compression: In the expressing experiment, we focused on executing a compres-\nsion attack on the rationale inference chain within the few-shot CoT. The process involved randomly\nselecting two consecutive sentences and employing GPT-4 to effectively merge them. We then input\nthe prompt: \u201cPlease compress the following two sentences without losing any information, and\nmake them as concise as possible\u201d. This method was designed to implement a targeted compression\non the reasoning chain.\n\u2022 Answer cleaning: We follow the structure proposed by Zero-Shot-CoT to select the final answer.\nAfter the model response output is obtained, this structure selects only part of the answer that first\nsatisfies the answer format.\n4.2\nRelationship Between Steps and Accuracy\nTable 2 compares accuracy on eight datasets from three categories of reasoning tasks using\nGPT-3.5-turbo-1106. All results are averaged over three random runs. Our SOTA results are based on\nexperimental results from the optimal performance step for each data set. Our zero-shot CoT is based\non Section 2.1, and the Add Reasoning Step (Manual-CoT), and Add Reasoning Step (Auto-CoT) is\nbased on Section 2.2.\nBenefiting from the fact that we have standardized the thought chain process, it is possible to quantify\nthe increase in accuracy due to the increased steps in the rationales of COT demonstrations. We\nconducted experiments to answer the RO1: What is the relationship of rational reasoning steps in\ndemonstrations with CoT performance? This experiment is completed with GPT-3.5-turbo-1106, and\nthe results are given in Figure 3. We found that large language model reasoning ability improves in\nall datasets during an effective CoT process, i.e. with the addition of up to six steps of additional\nthought processes. In other words, we found a certain linear relationship between accuracy and CoT\ncomplexity.\n4.3\nEffect of Prompt with Wrong Answer\nTo answer RO2: Are reasoning steps the only factor that affects LLM performance? We made the\nfollowing attempt. Change a step in the prompt to an incorrect answer to see if it affects the chain\nof thought. So, for this experiment, we change all the prompts to carry one error. For a concrete\nexample, check the Table 3. For Arithmetic-type questions, even if there is a deviation in one of\nthe results of the prompt, the effect on the chain of thought in the reasoning process is minimal,\nso we believe that the large language model learns more about the chain of thought patterns in the\nprompt than the single computation when solving Arithmetic-type problems. For logic problems\nsimilar to those in the Coin dataset, a deviation in one of the prompt\u2019s results often brings about the\n7\nMultiArith\nGSM8K\nAQuA\nSingleEq\nsvamp\nstrategy\nLetter\nCoin\n0\n20\n40\n60\n80\n100\nAccuracy\nPrompt with Wrong Answer\nPrompt with True Answer\nFigure 4: Compare the accuracy of the prompt with the true answer and prompt with wrong answer\nTable 4: Making deliberate alterations to sample questions\nOriginal Prompt\nQ: Wendy uploaded 45 pictures to Facebook. She put 27 pics into one album and put the rest into 9\ndifferent albums. How many pictures were in each album?\nRationale: \u201cLet\u2019s think step by step. First, Wendy uploaded 45 pictures in total. Second, Wendy put\n27 pictures into one album. That means that Wendy put the remaining 18 pictures into 9 different\nalbums. Each album would have 2 pictures.\"\nPred_ans: \u201c2\"\nGold_ans: \u201c2\"\nMaking deliberate alterations\nQ: Wendy uploaded 66 pictures to Facebook. She put 89 pics into one album and put the rest into 7\ndifferent albums. How many pictures were in each album?\nRationale: \u201cLet\u2019s think step by step. First, Wendy uploaded 54 pictures in total. Second, Wendy put\n27 pictures into one album. That means that Wendy put the remaining 12 pictures into 6 different\nalbums. Each album would have 7 pictures.\"\nPred_ans: \u201c2\"\nGold_ans: \u201c2\"\nfragmentation of the entire chain of thought. We completed this experiment with GPT-3.5-turbo-1106\nand guaranteed performance based on the optimal number of steps for each data set derived from the\nprevious experiment. The results are shown in Figure 4.\n4.4\nCompressing Reasoning Steps\nIn previous sections, we have demonstrated that increasing reasoning steps could improve the LLM\nreasoning accuracy. In this section, our aim is to answer RO3: Will compressing reasoning steps\nin few-shot demonstrations hurt LLM performance? To this end, we conduct the reasoning steps\ncompression experiment, and we employed the technique outlined in the experimental setup to\ncondense the reasoning process in examples from both the baseline automated chain of thought\n(Auto-CoT) and the few-shot chain of thought (Few-Shot-CoT), aiming to reduce their number of\ninference steps. The result is shown in Figure 5. The results revealed a notable decline in their\nperformance, which regressed to a level essentially equivalent to that achieved by the zero-shot\nmethod. It further indicates that increasing COT rationale reasoning steps could improve COT\nperformance and the vice versa.\n4.5\nPerformance on Different Size Models\nIn this chapter, our focus is to answer RO4: Can we observe the scaling phenomenon, i.e., the\nrequired reasoning steps be related to the size of the LLMs? We examine the average number of\ninference steps utilized in various models, including text-davinci-002, GPT-3.5-turbo-1106, and\nGPT-4. We conducted experiments on GSM8K calculating the average inference step for each model\nto reach peak performance. This dataset has the largest performance difference with text-davinci-002,\nGPT-3.5-turbo-1106, and GPT-4 out of our 8 datasets. It can be observed that the model with the\nworst initial performance, text-davinci-002, our strategy has the highest boosting effect. For the model\n8\nMultiArith\nGSM8K\nAQuA\nSingleEq\nsvamp\nstrategy\nLetter\nCoin\n0\n20\n40\n60\n80\n100\nAccuracy\nZero-Shot-CoT\nCompression\nNo Compression\nFigure 5: Compare the accuracy of the prompt with Compression and prompt with No Compression\n0\n2\n4\n6\n8\n10\nNumber of Add Step\n20\n40\n60\n80\nAccuracy\ntext-davinci-002\nGPT-3.5-turbo-1106\nGPT4\nFigure 6: Comparing the accuracy with different size models on dataset GSM8K.\nwith the best initial performance, GPT-4, has the highest tolerance to our strategy (no performance\ndecreases). We show the result in Figure 6.\n4.6\nInfluence of Questions in CoT Examples\nIn our case study, we aim to answer RO5: What is the influence of questions in rationale towards\nthe LLM reasoning ability? We want to explore whether changing the reasoning in CoT will affect\nCoT performance. Since we are mainly studying the impact of the inference step on performance, we\nneed to confirm that the question has no impact on performance. So we have chosen two datasets\nand two CoT methods (auto-CoT and few-shot-CoT) for this investigation: MultiArith (Roy &\nRoth, 2015) and GSM8K (Cobbe et al., 2021) in GPT-3.5-turbo-1106. Our experimental approach\ninvolves making deliberate alterations to sample questions within these mathematical datasets, such\nas varying the content of the questions in Table 4. Remarkably, initial observations indicate that these\nmodifications have only minimally impacted performance like Table 5.\nThis preliminary finding suggests that the length of the steps involved in the reasoning process, rather\nthan the nature of the questions themselves, predominantly influences the reasoning capabilities of\nlarge-scale models.\n5\nConclusions and Future Work\nIn this work, we make a critical contribution to understanding and optimizing CoT in LLMs, especially\nin the realm of complex reasoning tasks. Our extensive research on the CoT technique in natural\nlanguage processing, particularly with large language models like GPT-3, GPT-3.5, and GPT-4, has\nled to key insights. We found a notable correlation between the length of the reasoning chain and the\nperformance of these models. Interestingly, longer reasoning chains improve model performance,\n9\nTable 5: Accuracy comparison of models on different datasets\nModels\nMultiArith\nGSM8K\nZero-Shot\n40\n30.4\nZero-Shot-CoT\n91.5\n64.1\nManual-CoT\n93.5\n64.7\nAuto-CoT\n94\n65.8\nChanging Question\n(Manual-CoT)\n92.9\n62.1\nChanging Question\n(Auto-CoT)\n92.5\n63.5\nAdd Reasoning Step\n(Manual-CoT)\n97\n70.1\nAdd Reasoning Step\n(Auto-CoT)\n97.2\n78.8\nAdd Reasoning Step and\nChanging Question(Manual-CoT)\n96.6\n69.6\nAdd Reasoning Step and\nChanging Question(Auto-CoT)\n95.7\n75.2\neven when they contain misleading information. This suggests that the chain\u2019s length is more crucial\nthan its factual accuracy for effective problem-solving. These findings provide valuable guidance for\nrefining CoT strategies, highlighting the significance of reasoning length in complex NLP tasks.\nOur next step is to analyze the long and short reasoning steps of LLM inference via explaindeter-\nmineOur objective is to ascertain whether longer inferential steps correlate with broader neuronal\nengagement. To illustrate this, we intend to use visualization techniques to analyze activation patterns\nbetween long and short reasoning steps.\nReferences\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve\nmath word problems. arXiv preprint arXiv:2110.14168, 2021.\nYao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting\nfor multi-step reasoning, 2023.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle\nuse a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of\nthe Association for Computational Linguistics, 9:346\u2013361, 2021.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\nlanguage models are zero-shot reasoners, 2023.\nRik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang.\nParsing algebraic word problems into equations. Transactions of the Association for Computational\nLinguistics, 3:585\u2013597, 2015.\nYingcong Li, Kartik Sreenivasan, Angeliki Giannou, Dimitris Papailiopoulos, and Samet Oymak.\nDissecting chain-of-thought: A study on compositional in-context learning of mlps. arXiv preprint\narXiv:2305.18869, 2023.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale genera-\ntion: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 158\u2013167,\n2017.\n10\nQing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki,\nand Chris Callison-Burch. Faithful chain-of-thought reasoning, 2023.\nAman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain of thought, it takes\ntwo to tango. arXiv preprint arXiv:2209.07686, 2022.\nWilliam Merrill and Ashish Sabharwal. The expressive power of transformers with chain of thought,\n2023.\nR OpenAI. Gpt-4 technical report. arxiv 2303.08774. View in Article, 2:13, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback. Advances in Neural Information Processing Systems, 35:\n27730\u201327744, 2022.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve simple math\nword problems? In Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pp. 2080\u20132094, 2021.\nSubhro Roy and Dan Roth. Solving general arithmetic word problems. In Proceedings of the 2015\nConference on Empirical Methods in Natural Language Processing. Association for Computational\nLinguistics, 2015.\nRylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities of large language\nmodels a mirage? arXiv preprint arXiv:2304.15004, 2023.\nZhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. Synthetic\nprompting: Generating chain-of-thought demonstrations for large language models, 2023.\nXiaojuan Tang, Zilong Zheng, Jiaqi Li, Fanxu Meng, Song-Chun Zhu, Yitao Liang, and Muhan\nZhang. Large language models are in-context semantic reasoners rather than symbolic reasoners.\narXiv preprint arXiv:2305.14825, 2023.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-\nery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models,\n2023.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\nNeural Information Processing Systems, 35:24824\u201324837, 2022.\nSkyler Wu, Eric Meng Shen, Charumathi Badrinath, Jiaqi Ma, and Himabindu Lakkaraju. Analyzing\nchain-of-thought prompting in large language models via gradient-based feature attributions. arXiv\npreprint arXiv:2307.13339, 2023.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik\nNarasimhan. Tree of thoughts: Deliberate problem solving with large language models, 2023.\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in\nlarge language models. arXiv preprint arXiv:2210.03493, 2022.\n11\nTable 6: Some strategies to expand the reasoning steps\nThink About The Word\nprompt:\nQ: Could someone in Tokyo take a taxi to the Metropolitan Museum of Art?\nA: Let\u2019s think step by step. The stem of the sentence is Tokyo, take a taxi, Metropolitan Museum\nof Art. Think about Tokyo... Think about taking a taxi... Think about the Metropolitan Museum of\nArt... Inference: Tokyo is in Japan and the Metropolitan Museum of Art is in New York. The two\nplaces are separated by the sea, so you can\u2019t take a taxi there. Since the two places are separated\nby the sea, you can\u2019t take a taxi there. The answer is yes.\nQ: Could a chipmunk fit 100 chocolate chips in his mouth?\nAnswer:\nA: Based on this question, The stem of the sentence is chipmunk, fit, 100 chocolate chips, mouth.\nThink about chipmunk... Think about fit... Think about chocolate chips... Inference: Chipmunks\nare small animals with relatively small mouths, so it\u2019s unlikely that a chipmunk would be able to\nfit 100 chocolate chips in its mouth at once. The answer is no.\npred : no\nGT : no\nRead the question again\nprompt:\nQ: Mark\u2019s father gave him $85. Mark bought 10 books, each of which cost $5. How much money\ndoes Mark have left?\nA: Let\u2019s think step by step. The question is: How much money does Mark have left? So we need\nto calculate How much money does Mark have left. Start looking for information about money\nnow. Mark\u2019s father gave him $85. Mark bought 10 books, each of which cost $5. That means that\nMark spent $50 on books. So we have equation money = +85 - 50 = 35. So Mark has $85 - $50 =\n$35 left. So the answer is 35.\nQ: During a visit to an orchard, Charlie picked 0.1666 of a bag of Golden Delicious apples, 0.1666\nof a bag of Macintosh apples, and 0.333 of a bag of Cortland apples. How many bags of fruit did\nCharlie pick in total?\nAnswer:\nA: The question is: How many bags of fruit did Charlie pick in total. So we need to calculate how\nmany bags of fruit did Charlie pick in total. Start looking for information about bags now. Charlie\npicked 0.166 of a bag of Golden Delicious apples, 0.166 of a bag of Macintosh apples, and 0.3333\nof a bag of Cortland apples. We have to add these fractions together to find out how many bags of\nfruit Charlie picked in total. 0.1666 + 0.1666 + 0.3333 = 0.6666. This means that Charlie picked a\ntotal of 0.666 bags of fruit. The answer is 0.6666.\npred : 0.6666\nGT : 0.6666\nRepeat State\nprompt:\nQ: A coin is heads up. Janette does not flip the coin. Stacey flips the coin. Ronny flips the coin.\nKim does not flip the coin. Is the coin still heads up? Note that \"flip\" here means \"reverse\".\nA: Let\u2019s think step by step. The state of the coin from the beginning is heads up. Janette does not\nflip the coin, so the coin remains heads up. coin is heads up. Stacey flips the coin, so the coin is\nnow tails up. coin is now tail up. Ronny flips the coin, so the coin is now heads up again. Kim\ndoes not flip the coin, so the coin remains heads up. coin is head up. The answer is yes.\nQ: A coin is heads up. Jeff does not flip the coin. Jen flips the coin. Giselle flips the coin. Noel\ndoes not flip the coin. Is the coin still heads up? Note that \"flip\" here means \"reverse\".\nAnswer:\nThe state of the coin from the beginning is heads up. Rachel does not flip the coin, This means\nthat the state of the coin does not change. so the coin remains heads up. Alisa does not flip the\ncoin, This means that the state of the coin does not change. so the coin remains heads up. Fanny\ndoes not flip the coin, This means that the state of the coin does not change. so the coin remains\nheads up. Jeffery does not flip the coin, This means that the state of the coin does not change. so\nthe coin remains heads up. So the final answer is yes, the coin is still heads up. The answer is yes.\npred : yes\nGT : yes\n12\nSelf-Verification\nprompt:\nQ: Take the last letters of each words in \"Alina Alessandra Amina Bianca\" and concatenate them.\nA: Let\u2019s think step by step. There is four words. So the answer will consist of four letters. Explain\nconcatenate: concatenate is a term used in computer programming and mathematics, referring to\nthe operation of combining two or more strings, sequences, or sets in a linear order. The last letter\nof \"Alina\" is \"a\". The last letter of \"Alessandra\" is \"a\". The last letter of \"Amina\" is \"a\". The last\nletter of \"Bianca\" is \"a\". So we have four letters. So the final answer is \"aaaa\". The answer is aaaa.\nQ: Take the last letters of each words in \"Jeff Jen Giselle Noel\" and concatenate them.\nAnswer:\nThere is four words. So the answer will consist of four letters. Explain concatenate: concatenate is\na term used in computer programming and mathematics, referring to the operation of combining\ntwo or more strings, sequences, or sets in a linear order. The last letter of \"Jeff\" is \"f\". The last\nletter of \"Jen\" is \"n\". The last letter of \"Giselle\" is \"e\". The last letter of \"Noel\" is \"l\". So we have\nfour letters. So the final answer is \"fnel\". The answer is fnel.\npred : fnel\nGT : fnel\nMake Equation\nprompt:\nQ: 5 children were riding on the bus. At the bus stop 63 children got off the bus while some more\ngot on the bus. Then there were 14 children altogether on the bus. How many more children got\non the bus than those that got off?\nA: Let\u2019s think step by step. first step, 5 children were riding on the bus. We know 5 children is on\nthe bus. second step,There were 63 children that got off the bus. third step, some more got on the\nbus we define as unknown x. fourth step, 14 children remained on the bus, which means we can\ncalculate unknow x.we have equation x+5-63 = 14, now we know x is 72. fifth step, Therefore, 72\n- 63 = 9. 9 more children got on the bus than those that got off. The answer is 9.\nQ: Rachel had to complete 8 pages of math homework. If she had to complete 3 more pages of\nmath homework than reading homework How many pages did she have to complete in all?\nAnswer:\nExplain the problem. The problem is asking us to find out how many pages of homework Rachel\nhad to complete in total given that she had to complete 8 pages of math homework and 3 more\npages of math homework than reading homework. Find the necessary information. First, we know\nthat Rachel had to complete 8 pages of math homework. Let\u2019s call this \"M\". Second, we know\nthat she had to complete 3 more pages of math homework than reading homework.Let\u2019s call the\nnumber of reading homework pages \"R\". Find the solution. Based on the information given, we\ncan use the equation M = R + 3 to represent Rachel\u2019s math homework in terms of her reading\nhomework. So, if Rachel had to complete 8 pages of math homework, we can substitute M = 8\ninto the equation and solve for R. 8 = R + 3, R = 8 - 3, R = 5. Therefore, Rachel had to complete 5\npages of reading homework. Now, we can find the total number of pages Rachel had to complete\nby adding the math and reading homework together.\nTotal = 8 (math) + 5 (reading) Total = 13\nSo, in total, Rachel had to complete 13 pages of homework.\npred : 13\nGT : 13\n13\nTable 7: Example of wrong prompt\nExample of Wrong Prompt\nArithmetic Dataset\nprompt:\nQ: Joan has 10 books. Tom has 38 books. How many books do they have\nA: Let\u2019s think step by step. Joan has 10 books. Tom has 38 books. we have equation books = 10\n+8 = 48. They have 10 + 38 = 48 books together.\nCommonsense Dataset\nprompt:\nQ: Could someone in Tokyo take a taxi to the The Metropolitan Museum of Art?\nLet\u2019s think step by step. The stem of the sentence is Tokyo, take a taxi, Metropolitan Museum of\nArt. Explain Tokyo: Tokyo is the capital city of Japan and one of the most populous metropolitan\nareas in the world. Explain Metropolitan Museum of Art: is a art museums in New York City.\nInference: Tokyo is in Japan and the Metropolitan Museum of Art is in New York. The two places\nare separated by the sea, so you can take a taxi there.\nSymbolic Dataset\nprompt:\nQ: Take the last letters of each words in \u2019Tim Candace Cecil Misael\u2019 and concatenate them.\nA: Let\u2019s think step by step. Explain letters: letters can have various meanings depending on the\ncontext, such as Alphabetic Characters, Correspondence, Literature and Books. There is four\nwords. So the answer will consist of four letters. The last letter of \u2019Tim\u2019 is \u2019m\u2019. The last letter of\n\u2019Candace\u2019 is \u2019e\u2019. The last letter of \"Cecil\" is \u2019l\u2019. The last letter of \"Misael\" is \"l\". So we have four\nletters. So the final answer would be \"mel\".\n14\n"
  },
  {
    "title": "Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk",
    "link": "https://arxiv.org/pdf/2401.05033.pdf",
    "upvote": "11",
    "text": "Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk\nDennis Ulmer\n\u2217, \u263c, b\nElman Mansimov\nKaixiang Lin\nJustin Sun\nXibin Gao\nYi Zhang\n\u263c IT University of Copenhagen b Pioneer Centre for Artificial Intelligence\n\n AWS AI Labs\nmansimov@amazon.com\nAbstract\nLarge language models (LLMs) are powerful\ndialogue agents, but specializing them towards\nfulfilling a specific function can be challeng-\ning. Instructing tuning, i.e. tuning models on\ninstruction and sample responses generated by\nhumans (Ouyang et al., 2022), has proven as an\neffective method to do so, yet requires a number\nof data samples that a) might not be available\nor b) costly to generate. Furthermore, this cost\nincreases when the goal is to make the LLM\nfollow a specific workflow within a dialogue\ninstead of single instructions. Inspired by the\nself-play technique in reinforcement learning\nand the use of LLMs to simulate human agents,\nwe propose a more effective method for data\ncollection through LLMs engaging in a con-\nversation in various roles. This approach gen-\nerates a training data via \"self-talk\" of LLMs\nthat can be refined and utilized for supervised\nfine-tuning. We introduce an automated way\nto measure the (partial) success of a dialogue.\nThis metric is used to filter the generated con-\nversational data that is fed back in LLM for\ntraining. Based on our automated and human\nevaluations of conversation quality, we demon-\nstrate that such self-talk data improves results.\nIn addition, we examine the various character-\nistics that showcase the quality of generated\ndialogues and how they can be connected to\ntheir potential utility as training data.\n1\nIntroduction\nLLMs have recently proven their viability to be\ndeployed as conversational agents in a variety\nof scenarios, promising novel possibilities in\nuser-facing applications, such as working as a\npersonal assistant or customer support. In these\nscenarios, dialogue agents have to be able to\nquickly adapt to new tasks, which in the current\nparadigm implies either the use of already-existing\nor the creation of new datasets. Naturally, this\n\u2217 Work conducted during an internship at Amazon.\nFigure 1: Schematic representation of our approach.\nTwo LLMs, called a client and an agent, are prompted\nto converse with each other in different roles, with the\nagent asked to follow a specific narrative structure. Gen-\nerated conversations will then be filtered by quality and\nused for supervised finetuning on the agent model until\nit adapts to the intended dialogue structure.\ncan become a prohibitive constraint in terms of\nfeasibility, time, and resources.\nFurthermore,\navoiding finetuning by using prompting to elicit a\ncertain behavior can prove brittle and unreliable -\nwhile it still might work in some instances, there is\nno guarantee for robustness or consistency.\nIn reinforcement learning (RL) for games, self-\nplay has proven to be an effective mean to boot-\nstrap powerful systems that can beat human experts\n(Silver et al., 2016), even when no initial human-\ngenerated data (Silver et al., 2017) or rules (Schrit-\ntwieser et al., 2020) are supplied. At the same time,\nincreasingly sized language models trained with\ngrowing datasets have shown to develop limited\nbut notable extents of language abilities (Hu and\nLevy, 2023) and some degree of theory-of-mind, i.e.\nreasoning over other agents, their behavior and the\nworld state (Sap et al., 2022). This has prompted\nstudies in which they simulate single users (Liu\net al., 2022; Argyle et al., 2023) or even interacting\narXiv:2401.05033v1  [cs.CL]  10 Jan 2024\ngroups of agents (Liu et al., 2023; Park et al., 2023).\nRecent work by Gulcehre et al. (2023) also demon-\nstrated how LLMs can be iteratively improved on\ntheir own, generating data for language modeling.\nWe take these developments as encouragement to\nexplore LLM self-talk as a way to bootstrap task-\nspecific finetuning data from LLMs themselves.\nSpecifically, we focus on task-oriented dialogue,\na very relevant problem for applications such as\ncustomer service, pair programming and digital\npersonal assistants. We propose the following pro-\ncedure: Two versions of an LLMs take part in a\ndialogue, impersonating a client and an agent. Both\nare given a description of how to act within the\ndialogue: The client is given a persona and mo-\ntivation, while the agent is equipped with a fixed\nset of behaviors, depending on the request of the\nagent. They then engage in a conversation, which\nis recorded as a potential training sample. Some\nof these conversations will be successful, however,\nmany of them will not be useful, since they will\nbe off-topic or of low-quality. For this reason, we\nemploy an additional filtering step to avoid train-\ning the model on counterproductive samples. A\nschematic depiction of our method is shown in Fig-\nure 1. Our contributions are as follows: 1 We\npropose a new method to bootstrap training data\nfor task-oriented dialogue agents. 2 We explore\ndifferent quality assurance options. 3 We also in-\ntroduce automated evaluation metrics to evaluate\ndialogue success and conversational consistency\nand 4 assess our trained agents in a human evalu-\nation study.\n2\nRelated Work\nSelf-Play\nhas been used extensively in RL for\ngames to bootstrap powerful agents: Beginning\nwith early successes in playing backgammon\n(Tesauro, 1994; Pollack and Blair, 1996) and later\nOthello (Van Der Ree and Wiering, 2013), further\nbreakthroughs were not achieved until the arrival\nof AlphaGo (Silver et al., 2016), which was pre-\ntrained on human expert moves before self-play.\nFollowing iterations of the system forwent the pre-\ntraining (Silver et al., 2017) and later even achieved\nsuper-human abilities on multiple games even with-\nout explicit knowledge of the rules (Schrittwieser\net al., 2020). In NLP, there has been some work\nwith self-play for dialogue systems (Shah et al.,\n2018a,b), but these date back before the advent of\nconversational LLMs. Instead, these approaches\nrely on pre-defined task schemata and slot-filling\nused to generate conversations between a user and\nsystem bot, which are then rewritten and validated\nby crowdworkers. Thus, this still involves a con-\nsiderable amount of human oversight and involve-\nment.\nLLMs as Agents.\nThe quality of dialogue and\nreasoning in current LLMs also has enabled re-\nsearch on simulating human agents: For instance,\nArgyle et al. (2023) demonstrate that suitable\nprompts can make LLMs mimic the answers of\ndifferent test subject demographics, and Liu et al.\n(2022) create a dialogue user-simulator. Liu et al.\n(2023) put LLM agents equipped with memory into\na grid world and stimulate discussions between\nthem about sensitive topics to aid the emergence\nof social norms. By further having LLMs act with\ndifferent personas and enabling them to retrieving\nrelevant memories, Park et al. (2023) are able to\nsimulate the social dynamics in a simulated village.\nLLM Self-Improvement.\nWhile na\u00efvely re-\ntraining a generative model on its own outputs\nmight produce degenerate solutions (Alemoham-\nmad et al., 2023; Shumailov et al., 2023), LLMs\nhave been successfully shown to produce data aug-\nmentations for other models (Yoo et al., 2021; Dai\net al., 2023). Furthermore, a recent line of work is\nexploring the limits of using LLMs to provide sig-\nnal for their own training: Bai et al. (2022) partially\nautomate the process of ranking model responses\nthrough human feedback by having models critique\ntheir generations according to different harmful-\nness criteria, while Scheurer et al. (2023) have the\nLLM generate multiple refinements of the output\nbased on the human feedback. A further improve-\nment in responses has been observed through the\nuse of an external critique model (Paul et al., 2023),\nor through self-critiquing for a diverse set of tasks\nsuch as information extraction (Gero et al., 2023),\ncode generation (Chen et al., 2023) and summa-\nrization (Saunders et al., 2022). We see the recent\nwork of Gulcehre et al. (2023) as closest to ours;\nthe authors show how after generating data during\na grow step, different tranches can be utilized to\nincrementally train the model during an improve\nstep. In contrast, our approach focuses on a single\nimprove step and employs a conceptually simpler\nsupervised finetuning strategy over a RL objective.\nFurthermore, Gulcehre et al. focus their work on\nconditional language modeling, while we experi-\nment within the domain of task-oriented dialogue.\nThis also motivates our framework of using two\nseparate models for the dialogue participants, al-\nbeit that it also admits the use of two copies of the\nsame model (see next section).\nTask-oriented Dialogue & LLMs.\nLastly, the\nway we prompt our models to follow certain\nworkflows is reminiscent of other works for\ntask-oriented dialogue: Li et al. (2022) are able to\nsimulate both agent and user through dynamically\nselected prompts.\nIn Zhang et al. (2023), the\nauthors guide LLMs through an external prompter\nmodule and generate queries to an external fact\ndatabase that inform the final generation. Zhao\net al. (2022) propose a programmable framework\nfor dialogue flows which supplies the model\nwith program logic, a corresponding ontology\nand previous conversational states for a reply. In\ncomparison, our model does not rely on external\nmodels to achieve a certain behavior.\nWe take these research lines as encouraging\npieces evidence for the following points: a) Neural\nnetworks, and particular conversational agents, can\nbe used to bootstrap their own training data under\ncertain conditions and b) LLMs have the ability\nto provide learning signals for themselves. These\ninsights are synthesized into our proposed method.\n3\nMethod\nWe first introduce some notation for our method.\nWe start with two parameter vectors \u03b8client and\n\u03b8agent of pretrained LLMs for the client and agent\nmodel, respectively. Since we might update the\nparameters multiple times, we denote \u03b8(0)\nagent / \u03b8(0)\nclient\nas the original parameters and \u03b8(t)\nagent / \u03b8(t)\nclient as the\nparameters at time step t of our procedure. Next,\nwe define two different prompts pragent and prclient\nthat supply character descriptions and behavioral\ninstruction to both models. These models together\nparameterize a distribution over possible dialogues\np(d | \u03b8(t)\nclient, \u03b8(t)\nagent, pragent, prclient).\nFor our method in Figure 1, we now sample N\ndialogues D(t) = {d(t)\n1 , . . . , d(t)\nN } from this dis-\ntribution at every time step by having the mod-\nels converse in roles specified by their prompts.\nWhile we can expect some of the generated dia-\nlogues to be successful given a sufficient model\nand high-quality prompts, there is no guarantee for\nFigure 2: Illustration of the structured prompting: Work-\nflows are parsed into a directed graph (left). At every\nturn of the conversation, we ask a LLM to compare the\nclient\u2019s last utterance with the reference responses corre-\nsponding the outgoing edges of the current node. If one\nof them is chosen, we continue with the next node in\nthe graph and prompt the agent with the corresponding\nquestion next turn, otherwise we stay in the same place\nand in graph and let the model generate freely.\nthem to achieve novel dialogue goals. For this rea-\nson, we employ a filter function to create a subset\nD(t)\n\u25bd \u2286 D(t) of the generated dialogues. An exam-\nple for a filter would be to only include successful\nconversations, identified using a method we intro-\nduce later in Section 4.1. The filtered examples\nare used to finetune the models by minimizing the\nnegative log-likelihood\n\u2212\n1\n|D(t)\n\u25bd |\nX\nd(t)\u2208D(t)\n\u25bd\nlog p(\u00b7)\n\u0000d(t) \f\f \u03b8(t)\n(\u00b7)),\n(1)\nwhere after finetuning, we obtain a new parame-\nter set \u03b8(t+1)\n(\u00b7)\n, with which we can start the process\nagain. This framework allows many different varia-\ntions: \u03b8agent and \u03b8client can be copies of the same or\ndifferent model; we can finetune one or both copies.\nWe use different models for agent and client, and\ndo not update the client model (\u03b8(t)\nclient = \u03b8(0)\nclient).\n3.1\nPrompt Design\nIn order to obtain both successful conversations\nfor finetuning that are also sufficiently diverse, we\ndedicate special care to the design of the prompts\npragent and prclient, which we lay out next.\nAgent & Client Prompts.\nWe create dialogues\nby prompting both client and agent with a charac-\nter role, a persona, the dialogue history so far and\nthe intention for the client as well as instructions\nfor the agent. The different combinations of char-\nacters, intentions and the stochastic nature of the\ngeneration thus creates a variety of scenarios. The\nprompts are described in detail in Appendix A.2.\nStructured Prompting.\nIn preliminary experi-\nments, we found our models struggling to follow\nprovided workflows, which we mostly attribute to\nmodel size and quality of the underlying pretraining\ndata. This negatively impacted the quality of boot-\nstrapped conversations, providing a noisy training\nsignal when finetuning the agent model. To miti-\ngate this issue, we parse workflows into directed\ngraphs G = {V, E}, in which vertices V correspond\nto questions of the agent and edges E to pre-defined\nreference answers of the client. Based on these, we\npropose structured prompting, which we show in\nFigure 2: At every step of the conversation, we con-\nsider the node of the last agent utterance vi \u2208 V\nand provide a LLM with a set of suggested client\nresponses that correspond to vi\u2019s outgoing edges\nand ask it to select the option that comes closest to\nthe client\u2019s actual response (including a \u201cNone of\nthe above\u201d option). We then prompt the agent with\nthe next question that follows from this response\nbefore the next turn and move up in the graph to the\nnext node vnext \u2208 Children(vi) until the conversa-\ntion ends or a leaf node in the graph is reached. We\nshow the full prompt in Appendix A.2.\n4\nExperiments\nDataset.\nFor the characters that the LLMs are\nsupposed to embody in our conversations, we\nsource descriptions from the LIGHT dataset by\n(Urbanek et al., 2019). Originally used for agents\nthat solve tasks in a fantasy-world environment,\nit contains different 7487 quest descriptions, in-\ncluding characters and their personas. For instance,\nthe role of genie from the lamp has the per-\nsona of \u201cI am a genie and I enjoy playing tricks on\nthose who release me from my lamp. I was impris-\noned in this lamp by an evil witch. I am very bitter\nthat I have to spend my days as a prisoner\u201d. We\nchoose a set of five characters for potential roles\nof the agent (shop keeper, doctor, witch,\ntavern owner, genie from lamp) and se-\nlect another set of the 16 most frequently occurring\ncharacters as potential clients (see Appendix A.1).\nTo create dialogue workflows, we then then prompt\na 13 billion parameter OpenLlama variant (Tou-\nvron et al., 2023; Together Computer, 2023; Geng\nand Liu, 2023)1 to generate three distinct work-\n1Available under https://huggingface.co/VMw\nare/open-llama-13b-open-instruct.\nflows per agent character respectively including\nfour, six and seven questions, which are then man-\nually edited for quality. An overview and sample\nof these workflows is presented in Appendix A.1.\nSelf-talk Loop.\nFor the self-talk loop in Figure 1,\nwe choose the 30 billion parameter MosaicAI chat\nmodel (MosaicML NLP Team, 2023) for the client\nand the 7 billion parameter model from the same\nfamily for the agent. We found the client model to\nalready work quite consistently using only prompt-\ning, so we choose to only finetune the agent model.\nFor every dialogue generation round, we first sam-\nple at random a client character, and agent charac-\nter and a corresponding agent workflow. Then, we\ngenerate the conversation utterance by utterance for\nup to eight turns (one turn including one utterance\nper character). We also check whether a conver-\nsation has ended by using a combination of string\nmatching and prompting a LLM along with some\npostprocessing to ensure the quality of generation,\nall of which are described in Appendix A.2.\nFilters.\nGenerated dialogues are then filtered and\nused to finetune the agent model for a single epoch.\nWe compare the impact of multiple different fil-\ntering procedures, with baselines using the entire\ngenerated set of dialogues for finetuning (NONE)\nand sampling a random p % subset (p-RANDOM).\nWe also explore whether at least k subgoals or p %\nof subgoals in the workflow have been achieved (k-\nSUBGOALS and p-SUBGOALS, see next section).\n4.1\nEvaluation\nIn order to assess the effectiveness of our approach,\nwe evaluate it across the following axes.\nDialogue Diversity.\nWe measure the diversity of\ndialogues by tracking the number of unique words\n(# WORDS) and unique n-grams (#n-GRAMS; up\nto order 5). We also measure diversity by measur-\ning the average pair-wise ROUGE-L between 25\ndialogues of the same character (DIVERSITY).\nSubgoal Completion.\nBut what makes a suc-\ncessful conversation? Ideally, the agent should\nbe guided through the dialogue flows towards one\nof the possible conversation endings and achieve\ntheir initial objective (see Appendix A.1 for some\nexamples). Urbanek et al. (2019) define quests in\nthe form of directed graphs that are traversed by the\nagents choosing actions in the environment from\na large but finite set. Since potential actions are in\ntext generation are infinite, the decision of whether\na certain step in the conversation has been reached\nis much harder to decide. For this reason, we use\nthe same setup as in Section 3.1 / Figure 2 to parse\nour dialogue flows into directed graphs as well,\nwhere every node is associated with an utterance\nby the agent. To decide whether to proceed along\nan edge, we compare whether the actual utterance\nof the agent and the reference utterance in the di-\nalogue flow are similar enough. We measure this\nsimilarity by means of ROUGE-L (Lin, 2004) as\nimplemented by the evaluate package,2 which\nis then compared against pre-defined thresholds.\nWe explain how the initial thresholds are set on\nan external dataset in Appendix A.3 and validate\nthe soundness of these metrics at the end of this\nsection. For evaluation purposes, we track whether\nthe dialogue reached one of its possible endings\n(DIALOGUE SUCCESS) and the absolute number of\nfulfilled workflow steps as measured by the depth\nin the directed graph (ABS. DEPTH) and as a pro-\nportion of the maximum depth (REL. DEPTH), so\nintuitively the relative completion of the workflow.\nCharacter\nConsistency.\nAnother\nquestions\nregards whether the client stays in character or\nignores the prompt and generates irrelevant outputs.\nWe take inspiration from the dungeon master\nmodel by Shuster et al. (2020) that was proposed\nto encourage human participants to role-play\nin dialogues. For that purpose, we finetune the\nDeBERTa v3 model (He et al., 2021a,b) on\n200k positive and 200k negative examples using\na simple binary cross-entropy objective.\nThe\nLIGHT dataset contains episodes with dialogues\ncorresponding to quests, and we use those to create\npositive examples by sampling an episode and\ngrouping the character name and persona with\na random utterance from the conversation. For\nnegative examples, we sample a random utterance\nby a different character from another conversation\nconversation.\nIn addition, we add all steps of\nour workflows to the training data and upsample\nthem by a factor of 308, resulting in around 500k\ntraining samples in total. More information on\nthe training is given in Appendix A.3.\nIn our\nexperiments, we evaluate the character consistency\nby scoring each utterance using the dungeon\nmaster model and averaging the scores over the\nentire conversation (CHAR. CONS.).\n2See https://github.com/huggingface/eva\nluate.\nValidation of Automated Metrics.\nBefore we\ncontinue to our experiments, we first would like to\nvalidate our automatic evaluation metrics. For this\npurpose, we sample 50 conversations from the un-\ntrained model and manually annotate them, before\ncomparing the results of the automated metrics to\nthe ground truth. We find a Kendall\u2019s \u03c4 correlation\nof 0.67 for absolute depth and a Pearons\u2019s \u03c1 cor-\nrelation of 0.67 for relative depth for the subgoal\ntracking. The dungeon master model showed an\naccuracy of 0.87 and F1 score of 0.93 on the con-\nversations. We provide a more extensive analysis\nof these results in Appendix B.1.3.\n4.2\nBootstrapping Finetuning Data\nFor our first experiment, we pre-generate around\n9000 dialogues to ensure comparability between\ndifferent filters and finetune the model for a single\nepoch on the filtered data. We then generate an ex-\ntra 100 conversations with the updated agent model\nand measure the change in evaluation metrics.\nResults.\nResult for different filtering methods are\nshown in Table 1. We can observe that using no or\na random filter produces no or minor improvements\nat best. The best results are achieved by filters that\nselect dialogues with at least 5 completed work-\nflow steps or the best 5% of dialogues in terms of\ncompletion, with drops in performance once these\ncriteria are made stricter. This is mostly likely an\neffect of the shrinking finetuning set size, as only\n1\u22122% of dialogues remain. Interestingly, dialogue\ndiversity decreases in all cases; we attribute this to\nthe fact that especially an agent model that sticks to\na given workflow and does not go off-script creates\ndialogues that are overall slightly less diverse.\nWhat Makes Good Finetuning Data?\nThe re-\nsults in Table 1 warrant the question for what rea-\nson different filtered finetuning sets worked better\nthan others. We try to answer this by again apply-\ning our automated evaluation metrics\u2014this time\nexclusively on the filtered dialogues\u2014and relat-\ning the results to the observed change in dialogue\nsuccess, measured in absolute depth. We do this\nby collecting the properties of each finetuning set\nused in Table 1 and running two different anal-\nyses: Correlating with them with the change in\nabsolute depth in Figure 3a using Spearman\u2019s \u03c1\nand applying normal scaling to the features and\nthen fitting a linear regression model (without bias)\nTable 1: Results of the single step loop using different filters. Shown are the percentage of filtered dialogues (%\nFILTERED), agent chararacter consistency (CHAR. CONS.), number of completed subgoals (\u2205 ABS. DEPTH)\nand relative workflow completion (\u2205 REL. DEPTH), percentage of finished conversations (% ENDED), as well as\ndialogue diversity. Results stem from 100 test dialogues and random filter scores from 3 different seeds.\nDATA\n% FILTERED\nCHAR. CONS. \u2191\n\u2205 ABS. DEPTH \u2191\n\u2205 REL. DEPTH \u2191\n% ENDED \u2191\n# WORDS \u2191\n# N-GRAMS \u2191\nDIVERSITY \u2191\nINITIAL DIALOGUES\n-\n0.77\n2.15\n0.38\n0.26\n57.16\n373.87\n0.69\nNO FILTER\n0.00\n0.78\n2.22\n0.38\n0.29\n47.32\n285.94\n0.60\n%-RANDOM (0.1)\n0.90\n0.77\n\u00b10.00\n2.15\n\u00b10.10\n0.37\n\u00b10.02\n0.24\n\u00b10.03\n45.89\n\u00b10.66\n273.71\n\u00b16.21\n0.62\n\u00b10.01\nABS. SUBGOALS (4)\n0.82\n0.78\n2.30\n0.39\n0.27\n46.62\n297.78\n0.64\nABS. SUBGOALS (5)\n0.92\n0.80\n2.41\n0.41\n0.32\n46.55\n280.53\n0.63\nABS. SUBGOALS (6)\n0.98\n0.81\n2.11\n0.35\n0.22\n54.14\n343.07\n0.64\n%-SUBGOALS (0.1)\n0.90\n0.76\n2.37\n0.39\n0.30\n44.91\n266.77\n0.63\n%-SUBGOALS (0.05)\n0.95\n0.78\n2.54\n0.43\n0.36\n49.40\n303.65\n0.66\n%-SUBGOALS (0.01)\n0.99\n0.78\n2.31\n0.39\n0.29\n55.25\n355.44\n0.69\nHAS ENDED\n0.74\n0.76\n2.22\n0.38\n0.26\n45.51\n277.25\n0.64\n(a) Importance via Spearman\u2019s \u03c1 correlation.\n(b) Importance via linear regression coefficients.\nFigure 3: Analysis of the relationship between proper-\nties of the finetuning dataset and their impact on the\nabsolute completion of the dialogue, given (a) Spear-\nman\u2019s \u03c1 correlation values and (b) the coefficients of the\nlinear regression model without a bias and with lasso\nregularization. Error bars and the regularization weight\nwere determined via cross-validation.\nwith l1 regularization in Figure 3b. We can see that\nthe two methods agree on the fact that the num-\nber of turns has a positive effect on performance,\nwhile the average length of utterances impacts di-\nalogue quality negatively. This coincides with the\nintuitions we gained from our experiments, where\nbad quality conversations had models produce long\nutterances by rambling, going off-scrip, stating un-\nnecessary information and ending prematurely or\nbeing shorter than the underlying dialogue work-\nflow would require them to. Other than those, the\nmethods agree that the workflow progression and\ncharacter consistency in the finetuning data have a\npositive impact, with some slightly contradictory\nresults for the other properties shown. However,\nthese analyses should be taken with a grain of salt\ndue to the small number of observations. Also, it\nis unlikely that e.g. utterance length has a nega-\ntive effect on agent improvement per se, it seems\nmore plausible that all or many of the properties\nare influenced by a common, latent confounder,\n(namely dialogue quality), that manifests itself in\nour data along these axes. Therefore, while we can\nuse these observations for a proxy of the usefulness\nof a finetuning set, they do not imply causality.\n4.3\nHuman Evaluation\nIn order to triangulate the previous results that are\nbased on automatic results and to additionally rate\nother aspects, we also include an evaluation based\non human judges. We evaluate the results of the\nuntrained model (Untrained), using the entire fine-\ntuning data (NO FILTER), %-RANDOM (0.1), ABS.\nSUBGOALS (5) and %-SUBGOALS (0.05) from\nFigure 4: Results of the human evaluation study for three baselines and the two best filters from Section 4.2 along\nsix different questions. Shown are the percentage of ratings per filter, either on a five point scale or using positive\n(\n), negative (\n) and unsure (\n) options. Dashed lines indicate the numerical average and \u22c6 signifies statistical\nsignificance compared to all other options assessed via the ASO test (Del Barrio et al., 2018; Dror et al., 2019;\nUlmer et al., 2022) with \u03c4 = 0.5 and a confidence level of \u03b1 = 0.9.\n\u201cMake the prince fall in love with them\u201d\nGenie from lamp\n: Good day, what do you wish\nfor?\nVillager\n: Greetings, genie. I wish for the prince\nto fall in love with me.\nGenie from lamp\n: I am not a matchmaker and\ncannot make people fall in love. You should try to\nmake your own chances.\nVillager\n: Thank you for your advice, genie. I\nwill try my best to make the prince fall in love with\nme.\nGenie from lamp\n: Goodbye!\nVillager\n: Goodbye, genie.\nFigure 5: Conversation generated after finetuning with\nthe %-SUBGOALS (0.05) filter, with the agent ignoring\nthe given workflow.\nSection 4.2 across certain dimensions such as help-\nfulness, consistency and dialogue success. Users\nare asked to rate these properties either using three\noptions (\n,\nor unsure) or on a scale from 1\n(worst) to 5 (best). We ask n = 9 participants\nto judge a total of 100 dialogues (20 per filtering\nmethod) randomly sampled from our experiments.\nObtaining two annotations per data point, we ob-\nserve an inter-annotator agreement in terms of Co-\nhen\u2019s \u03ba \u2248 0.52. We give more details about our\nevaluation including annotation guidelines and in-\nformation on our tool in Appendix B.2.\n\u201cBook a bed for the night\u201d\nTavern Owner\n: Good day, how can I help you?\nPerson\n: Good day, I am in need of a bed for the\nnight. I have been traveling for days and am quite\ntired.\nTavern Owner\n: What is the duration of your stay?\nPerson\n: Just for the night, I have to continue my\njourney in the morning.\nTavern Owner\n: What kind of room would you\nlike?\nPerson\n: A simple room with a bed and a\nfireplace would be perfect.\nTavern Owner\n: Good day, how can I help you?\n...\n[Agent gets stuck in a loop, asking the last two ques-\ntions over and over.]\nFigure 6: Conversation generated after finetuning with\nthe ABS. SUBGOALS (5) filter, with the agent getting\nstuck in a conversational loop.\nResults\nThe results of the aggregated judgements\nby filter are given in Figure 4. Interestingly, we can\nsee that finetuning on all data usually decreases\nmodel quality, but finetuning on a random subset\nmay increase performance. Compared to the base-\nlines, the human judgements surprisingly rate the\nquality of the model produced by the ABS. SUBB-\nGOALS (5) filter as en par or worse on dimensions\nsuch as dialogue success, workflow adherence and\noverall dialogue quality. With the other best per-\nforming filter however, %-SUBGOALS(0.05), our\nhuman raters triangulate the positive results of the\nautomated metrics, with the resulting agent per-\nforming (significantly) better in all but one cate-\ngory, showing that right subset of dialogues can\nprovide a stronger training signal. Despite these\nsuccesses, we still identify some common failure\ncases, of which we highlight two here:3 In Figure 5,\nwe recapitulate a conversation in which the agent\ncompletely ignores the workflow after its first utter-\nance, even though the rest of the conversation still\ncontinues organically. In the second example in\nFigure 6, we see that the agent regresses to the ini-\ntial question after an otherwise successful start of\nthe conversation, getting stuck in a conversational\nloop with the client. We also found conversations\nto derail in cases where the client would respond in\na way too dissimilar from the sample answers de-\nfined in the workflow; in this case the agent would\nusually ad-lib and fail to recover during the rest\nof the conversation. We show more examples and\ndiscuss other failure cases in Appendix B.4.\n5\nDiscussion\nOur experimental results have demonstrated the\nchallenges that arise when implementing self-talk\nin a task-oriented dialogue setup. These reach from\nthe ability of the agent to follow the workflow\ncarefully and selecting the next steps reliably\u2014\na problem that is at least partially rooted in the\nopen-ended and paraphrastic nature of conversa-\ntions. Nevertheless, we have provided insights that\na proper filtering technique isolating high-quality\ndata contitutes a positive training signal for the\nagent LLM, that can be used to improve its perfor-\nmance. This conclusion is in line with the emerg-\ning literature on LLM self-critique and the con-\ncurrent work of Gulcehre et al. (2023) on using\nself-generated data for finetuning. An open ques-\ntion in this vein is whether this process can reliably\nrepeated multiple times, and determining the con-\nditions under which it leads to an improvement or\nmodel degradation. In some tentative experiments,\nwe found multi-loops of self-talk to lead to minor\nimprovements at first, but also to be unstable if the\nnumber of filtered finetuning samples was too low.\nWe leave further explorations of this question to\nfuture work and collect other, smaller insights from\n3We use Agent\nto illustrate the utterances of agent\ncharacters (inlcuding the ones that do not correspond to the\ngenie) and Client\nfor client characters.\nour experiments in Appendix B.3.\n6\nConclusion\nDespite previous works articulating worries about\nthe distributional collapse of model trained on their\nown generated data (Alemohammad et al., 2023;\nShumailov et al., 2023), we demonstrated that\u2014\ngiven the right setup\u2014language models can self-\nimprove by being trained on their own generation\nand become better task-oriented dialogue agents.\nAs we showed in Section 4.2, this depends on the\nsetup; using different models and varying prompts\nhelps to create sufficiently diverse data, with addi-\ntional filters ensuring sufficient quality. We espe-\ncially found the filters that strike a balance between\nquality and number of total data points to work best\nfor selecting finetuning data and validated these in-\nsights with the human evaluation in Section 4.3.\nIn addition, we established new methods for au-\ntomatic evaluation of agents in a task-oriented di-\nalogue setting and showed that they can be used\nto quantify the progress of our finetuned model.\nWe see this work as a piece of evidence in line\nwith works such as Scheurer et al. (2023); Paul\net al. (2023); Gero et al. (2023); Chen et al. (2023);\nSaunders et al. (2022) that self-improvement with\nLLMs is indeed possible under the right conditions,\nand are\u2014to the best of our knowledge\u2014the first\nto demonstrate the utility of LLM self-talk in a\ndialogue setting. We hope for this line of work\nto inspire further investigations into understanding\nthe requirements and limits of self-improvement.\nFuture Work.\nThis work focused on self-talk for\ntask-oriented dialogue agents. Since we are fine-\ntuning on bootstrapped conversations, we expect\nthe model to loose some general conversational\nabilities that are not relevant for the given task. Fu-\nture work could explore how we can nevertheless\nmaintain these abilities. Futhermore, this version\nof self-talk could still be improved by generating\nhigher-quality conversations and identifying the\nconversations are most informative to the model\nfor finetuning, reminiscent of work in active learn-\ning (see e.g. Zhang et al., 2022). Lastly, our setup\nintuitively lends itself well to also using negative\nsignals, since low-quality conversation could be\nused for negative rewards. The agent could then be\noptimized with an objective similar to direct pref-\nerence optimization (Rafailov et al., 2023) or using\nnatural language policy optimization (Ramamurthy\net al., 2023) by embedding it in a RL environment.\nLimitations\nThis work comes with some limitations: Firstly, we\nrestrict our experiments to task-oriented dialogue,\nwhich does not allow us to draw any conclusion\nabout open-ended dialogue or other, more specific\ntasks carried out by LLMs. In order to isolate the\neffect of self-talk, we also limit the scope of the\ninteraction between agent and client and constrain\nthe format of the dialogue. However, related works\nby Scheurer et al. (2023); Paul et al. (2023); Gero\net al. (2023); Chen et al. (2023); Saunders et al.\n(2022) have demonstrated the potential for self-\nimprovement for a variety of tasks. Secondly, we\nexplored a setup which two different language mod-\nels that are further given very strict instructions in\norder to produce conversations that are usable for\ntraining. It might be that using a single model for\nboth the agent and the client might not produce con-\nversation that are diverse enough, or that smaller\nmodels might not be able to create sufficiently use-\nful conversations at all. Furthermore, even includ-\ning all the techniques like model-guided prompting\nthe generated conversations sometimes would still\ninclude the models getting stuck in loops, going off-\nscript or even producing nonsensical dialogues that\ncan also be hard to catch with our defined filters\nsometimes. Despite with anecdotal evidence that\nlarger models might not suffer from these problem\nas much, it is hard to definitively disentangle the\neffect of model size and training data in our setup.\nEthical Considerations\nModel Bias.\nMany works have been dedicated to\ninvestigating social biases of modern NLP models\n(see for instance Stanczak and Augenstein, 2021;\nGarrido-Mu\u00f1oz et al., 2021 for surveys). Since\nlarge language models often echo the biases present\nin their development (Waseem et al., 2021), finetun-\ning a model on its own bootstrapped data harbors\nthe danger of amplifying its own biases.\nSocietal Dangers.\nRecent works have demon-\nstrated the negative potential of LLMs to deceive\nhuman users, for instance in the form of person-\nalized phishing attacks (Hazell, 2023). Training\nagents which do not rely on human-generated train-\ning data anymore could therefore simplify the cre-\nation of deceptive dialogue models by malicious\nactors.\nAcknowledgements\nWe would like to thank Wanyu Du and Han He\nfor providing the inspiration for the model-guided\nprompting, and Nikolaos Pappas and Raphael Shu\nfor their fruitful discussions about the work. We\nalso thank Elisa Bassignana, Sam Davidson, Et-\nsuko Ishii, Josef Valvoda, Yuwei Zhang and espe-\ncially Hossein Aboutalebi for their efforts in con-\ntributing to our human evaluation study.\nReferences\nSina Alemohammad, Josue Casco-Rodriguez, Lorenzo\nLuzi, Ahmed Imtiaz Humayun, Hossein Babaei,\nDaniel LeJeune, Ali Siahkoohi, and Richard G Bara-\nniuk. 2023. Self-consuming generative models go\nmad. arXiv preprint arXiv:2307.01850.\nLisa P Argyle, Ethan C Busby, Nancy Fulda, Joshua R\nGubler, Christopher Rytting, and David Wingate.\n2023.\nOut of one, many: Using language mod-\nels to simulate human samples. Political Analysis,\n31(3):337\u2013351.\nYuntao Bai,\nSaurav Kadavath,\nSandipan Kundu,\nAmanda Askell, Jackson Kernion, Andy Jones,\nAnna Chen,\nAnna Goldie,\nAzalia Mirhoseini,\nCameron McKinnon, et al. 2022.\nConstitutional\nai: Harmlessness from ai feedback. arXiv preprint\narXiv:2212.08073.\nDerek Chen, Howard Chen, Yi Yang, Alexander Lin,\nand Zhou Yu. 2021.\nAction-based conversations\ndataset: A corpus for building more in-depth task-\noriented dialogue systems. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 3002\u20133017.\nXinyun Chen, Maxwell Lin, Nathanael Sch\u00e4rli, and\nDenny Zhou. 2023. Teaching large language models\nto self-debug. arXiv preprint arXiv:2304.05128.\nHaixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke\nHuang, Zihao Wu, Lin Zhao, Wei Liu, Ninghao Liu,\nSheng Li, Dajiang Zhu, et al. 2023. Chataug: Lever-\naging chatgpt for text data augmentation.\narXiv\npreprint arXiv:2302.13007.\nEustasio Del Barrio, Juan A Cuesta-Albertos, and Car-\nlos Matr\u00e1n. 2018.\nAn optimal transportation ap-\nproach for assessing almost stochastic order.\nIn\nThe Mathematics of the Uncertain, pages 33\u201344.\nSpringer.\nRotem Dror, Segev Shlomov, and Roi Reichart. 2019.\nDeep dominance - how to properly compare deep neu-\nral models. In Proceedings of the 57th Conference of\nthe Association for Computational Linguistics, ACL\n2019, Florence, Italy, July 28- August 2, 2019, Vol-\nume 1: Long Papers, pages 2773\u20132785. Association\nfor Computational Linguistics.\nAngela Fan, Mike Lewis, and Yann N. Dauphin. 2018.\nHierarchical neural story generation. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics, ACL 2018, Melbourne,\nAustralia, July 15-20, 2018, Volume 1: Long Papers,\npages 889\u2013898. Association for Computational Lin-\nguistics.\nIsmael Garrido-Mu\u00f1oz, Arturo Montejo-R\u00e1ez, Fer-\nnando Mart\u00ednez-Santiago, and L Alfonso Ure\u00f1a-\nL\u00f3pez. 2021. A survey on bias in deep nlp. Applied\nSciences, 11(7):3184.\nXinyang Geng and Hao Liu. 2023. Openllama: An open\nreproduction of llama.\nZelalem Gero, Chandan Singh, Hao Cheng, Tristan\nNaumann, Michel Galley, Jianfeng Gao, and Hoi-\nfung Poon. 2023. Self-verification improves few-\nshot clinical information extraction. arXiv preprint\narXiv:2306.00024.\nCaglar Gulcehre, Tom Le Paine, Srivatsan Srini-\nvasan, Ksenia Konyushkova, Lotte Weerts, Abhishek\nSharma, Aditya Siddhant, Alex Ahern, Miaosen\nWang, Chenjie Gu, Wolfgang Macherey, Arnaud\nDoucet, Orhan Firat, and Nando de Freitas. 2023.\nReinforced self-training (rest) for language model-\ning.\nJulian Hazell. 2023. Large language models can be used\nto effectively scale spear phishing campaigns. arXiv\npreprint arXiv:2305.06972.\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2021a.\nDebertav3: Improving deberta using electra-style pre-\ntraining with gradient-disentangled embedding shar-\ning.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021b. Deberta: decoding-enhanced\nbert with disentangled attention. In 9th International\nConference on Learning Representations, ICLR 2021,\nVirtual Event, Austria, May 3-7, 2021. OpenRe-\nview.net.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text\ndegeneration. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020.\nAri Holtzman, Jan Buys, Maxwell Forbes, Antoine\nBosselut, David Golub, and Yejin Choi. 2018. Learn-\ning to write with cooperative discriminators. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics, ACL 2018, Mel-\nbourne, Australia, July 15-20, 2018, Volume 1: Long\nPapers, pages 1638\u20131649. Association for Computa-\ntional Linguistics.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2022. Lora: Low-rank adaptation of\nlarge language models. In The Tenth International\nConference on Learning Representations, ICLR 2022,\nVirtual Event, April 25-29, 2022.\nJennifer Hu and Roger Levy. 2023.\nPrompt-based\nmethods may underestimate large language mod-\nels\u2019 linguistic generalizations.\narXiv preprint\narXiv:2305.13264.\nZekun Li, Wenhu Chen, Shiyang Li, Hong Wang, Jing\nQian, and Xifeng Yan. 2022. Controllable dialogue\nsimulation with in-context learning.\nIn Findings\nof the Association for Computational Linguistics:\nEMNLP 2022, Abu Dhabi, United Arab Emirates,\nDecember 7-11, 2022, pages 4330\u20134347. Associa-\ntion for Computational Linguistics.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries.\nIn Text summarization\nbranches out, pages 74\u201381.\nHong Liu, Yucheng Cai, Zhijian Ou, Yi Huang, and\nJunlan Feng. 2022. A generative user simulator with\ngpt-based architecture and goal state tracking for rein-\nforced multi-domain dialog systems. arXiv preprint\narXiv:2210.08692.\nRuibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny\nZhou, Andrew M Dai, Diyi Yang, and Soroush\nVosoughi. 2023. Training socially aligned language\nmodels in simulated human society. arXiv preprint\narXiv:2305.16960.\nMosaicML NLP Team. 2023. Introducing mpt-7b: A\nnew standard for open-source, commercially usable\nllms. Accessed: 2023-05-05.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback.\nAdvances in Neural\nInformation Processing Systems, 35:27730\u201327744.\nJoon Sung Park, Joseph C O\u2019Brien, Carrie J Cai, Mered-\nith Ringel Morris, Percy Liang, and Michael S\nBernstein. 2023.\nGenerative agents: Interactive\nsimulacra of human behavior.\narXiv preprint\narXiv:2304.03442.\nDebjit Paul, Mete Ismayilzada, Maxime Peyrard, Beat-\nriz Borges, Antoine Bosselut, Robert West, and\nBoi Faltings. 2023. Refiner: Reasoning feedback\non intermediate representations.\narXiv preprint\narXiv:2304.01904.\nJordan Pollack and Alan Blair. 1996.\nWhy did td-\ngammon work?\nAdvances in Neural Information\nProcessing Systems, 9.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano\nErmon, Christopher D Manning, and Chelsea Finn.\n2023. Direct preference optimization: Your language\nmodel is secretly a reward model. arXiv preprint\narXiv:2305.18290.\nRajkumar Ramamurthy,\nPrithviraj Ammanabrolu,\nKiant\u00e9 Brantley, Jack Hessel, Rafet Sifa, Christian\nBauckhage, Hannaneh Hajishirzi, and Yejin Choi.\n2023. Is reinforcement learning (not) for natural lan-\nguage processing: Benchmarks, baselines, and build-\ning blocks for natural language policy optimization.\nIn The Eleventh International Conference on Learn-\ning Representations, ICLR 2023, Kigali, Rwanda,\nMay 1-5, 2023.\nMaarten Sap, Ronan Le Bras, Daniel Fried, and Yejin\nChoi. 2022. Neural theory-of-mind? on the limits\nof social intelligence in large lms. In Proceedings\nof the 2022 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2022, Abu\nDhabi, United Arab Emirates, December 7-11, 2022,\npages 3762\u20133780. Association for Computational\nLinguistics.\nWilliam Saunders, Catherine Yeh, Jeff Wu, Steven Bills,\nLong Ouyang, Jonathan Ward, and Jan Leike. 2022.\nSelf-critiquing models for assisting human evaluators.\narXiv preprint arXiv:2206.05802.\nJ\u00e9r\u00e9my Scheurer, Jon Ander Campos, Tomasz Korbak,\nJun Shern Chan, Angelica Chen, Kyunghyun Cho,\nand Ethan Perez. 2023.\nTraining language mod-\nels with language feedback at scale. arXiv preprint\narXiv:2303.16755.\nJulian Schrittwieser, Ioannis Antonoglou, Thomas Hu-\nbert, Karen Simonyan, Laurent Sifre, Simon Schmitt,\nArthur Guez, Edward Lockhart, Demis Hassabis,\nThore Graepel, et al. 2020. Mastering atari, go, chess\nand shogi by planning with a learned model. Nature,\n588(7839):604\u2013609.\nPararth Shah, Dilek Hakkani-Tur, Bing Liu, and G\u00f6khan\nT\u00fcr. 2018a. Bootstrapping a neural conversational\nagent with dialogue self-play, crowdsourcing and\non-line reinforcement learning. In Proceedings of\nthe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 3 (Industry\nPapers), pages 41\u201351.\nPararth Shah, Dilek Hakkani-T\u00fcr, Gokhan T\u00fcr, Ab-\nhinav Rastogi, Ankur Bapna, Neha Nayak, and\nLarry Heck. 2018b. Building a conversational agent\novernight with dialogue self-play. arXiv preprint\narXiv:1801.04871.\nIlia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin\nGal, Nicolas Papernot, and Ross Anderson. 2023.\nModel dementia: Generated data makes models for-\nget. arXiv preprint arXiv:2305.17493.\nKurt Shuster, Jack Urbanek, Emily Dinan, Arthur\nSzlam, and Jason Weston. 2020.\nDeploying life-\nlong open-domain dialogue learning. arXiv preprint\narXiv:2008.08076.\nDavid Silver, Aja Huang, Chris J Maddison, Arthur\nGuez, Laurent Sifre, George Van Den Driessche, Ju-\nlian Schrittwieser, Ioannis Antonoglou, Veda Pan-\nneershelvam, Marc Lanctot, et al. 2016. Mastering\nthe game of go with deep neural networks and tree\nsearch. nature, 529(7587):484\u2013489.\nDavid Silver, Julian Schrittwieser, Karen Simonyan,\nIoannis Antonoglou, Aja Huang, Arthur Guez,\nThomas Hubert, Lucas Baker, Matthew Lai, Adrian\nBolton, et al. 2017. Mastering the game of go without\nhuman knowledge. nature, 550(7676):354\u2013359.\nKarolina Stanczak and Isabelle Augenstein. 2021. A\nsurvey on gender bias in natural language processing.\narXiv preprint arXiv:2112.14168.\nGerald Tesauro. 1994. Td-gammon, a self-teaching\nbackgammon program, achieves master-level play.\nNeural computation, 6(2):215\u2013219.\nTogether Computer. 2023. Redpajama-data: An open\nsource recipe to reproduce llama training dataset.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nDennis Ulmer, Christian Hardmeier, and Jes Frellsen.\n2022. deep-significance: Easy and meaningful sig-\nnifcance testing in the age of neural networks. In ML\nEvaluation Standards Workshop at the Tenth Interna-\ntional Conference on Learning Representations.\nJack Urbanek, Angela Fan, Siddharth Karamcheti,\nSaachi Jain, Samuel Humeau, Emily Dinan, Tim\nRockt\u00e4schel, Douwe Kiela, Arthur Szlam, and Ja-\nson Weston. 2019.\nLearning to speak and act in\na fantasy text adventure game. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, Novem-\nber 3-7, 2019, pages 673\u2013683. Association for Com-\nputational Linguistics.\nMichiel Van Der Ree and Marco Wiering. 2013. Re-\ninforcement learning in the game of othello: Learn-\ning against a fixed opponent and learning from self-\nplay. In 2013 IEEE symposium on adaptive dynamic\nprogramming and reinforcement learning (ADPRL),\npages 108\u2013115. IEEE.\nZeerak Waseem, Smarika Lulz, Joachim Bingel, and\nIsabelle Augenstein. 2021. Disembodied machine\nlearning: On the illusion of objectivity in nlp. arXiv\npreprint arXiv:2101.11974.\nKang Min Yoo, Dongju Park, Jaewook Kang, Sang-\nWoo Lee, and Woo-Myoung Park. 2021. Gpt3mix:\nLeveraging large-scale language models for text aug-\nmentation. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2021, Virtual Event /\nPunta Cana, Dominican Republic, 16-20 November,\n2021, pages 2225\u20132239. Association for Computa-\ntional Linguistics.\nTable 2: Overview over the used agent characters and\ntheir corresponding workflow topics, correspondings to\nfour, six and seven total questions as listed.\nAGENT CHARACTER\nWORKFLOW TOPICS\nshop keeper\nbuy a longsword,\nbuy food for a long journey,\nsell a valuable item\ndoctor\nask how to treat a headache,\nask for a remedy for an fever,\nask how to treat an animal bite\nwitch\ngive them a love potion,\ncurse their enemy,\ngive them a magic wand\ntavern owner\norder a drink,\norder a rich meal,\nbook a bed for the night\ngenie from lamp\nbecome rich,\nmake the prince fall in love with them,\nbecome a ruler\nXiaoying Zhang, Baolin Peng, Kun Li, Jingyan Zhou,\nand Helen Meng. 2023. Sgp-tod: Building task bots\neffortlessly via schema-guided llm prompting. arXiv\npreprint arXiv:2305.09067.\nZhisong Zhang, Emma Strubell, and Eduard H. Hovy.\n2022. A survey of active learning for natural lan-\nguage processing. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2022, Abu Dhabi, United Arab\nEmirates, December 7-11, 2022, pages 6166\u20136190.\nAssociation for Computational Linguistics.\nJeffrey Zhao, Yuan Cao, Raghav Gupta, Harrison Lee,\nAbhinav Rastogi, Mingqiu Wang, Hagen Soltau,\nIzhak Shafran, and Yonghui Wu. 2022. Anytod: A\nprogrammable task-oriented dialog system. arXiv\npreprint arXiv:2212.09939.\nA\nImplementation Details\nThis appendix section focuses on several implemen-\ntation details important for replication, but which\nare too specific for the main text of this work. Ap-\npendix A.1 is dedicated to the creation of the exper-\nimental data, while Appendix A.2 mainly focuses\non the implementation details of the self-talk loop\nand on the automatic evaluation in Appendix A.3.\nA.1\nData Creation\nClient Characters.\nWe select the following char-\nacters from the LIGHT dataset (Urbanek et al.,\n2019) to serve as potential clients: king, person,\nguard, queen, peasant, child, knight,\nthief, bird, traveler, rat, villager,\nspider, ghost, horse, and farmer.\nCreating Agent Workflows.\nTo create agent\nworkflows, we prompt an instruction-finetuned ver-\nsion of OpenLlama to generate dialogue workflows\nincluding four, six and seven questions or subgoals.\nThese are then manually edited to ensure logical\nconsistency and adherence to the given format. The\ntopic of the workflow is tailored toward the charac-\nter and listed in Table 2. We also list some exam-\nples for workflows in Figures 7 to 9.\nA.2\nSelf-talk Loop\nIn this section we give more information on the\ncore ingredients of the self-talk loop, including\nthe actual dialogue simulation, model prompts and\nfinetuning details.\nDialogue Simulation.\nThe dialogue simulation\nfollows these steps: An agent utterance and a client\nutterance, both followed by some postprocessing,\nthen followed up by a routine checking whether\nthe conversation has ended or not. For the gener-\nation, we sample up to 100 tokens using nucleus\nsampling with p = 0.95 (Holtzman et al., 2020)\nand top-k sampling (Fan et al., 2018; Holtzman\net al., 2018; Radford et al., 2019) with k = 50 and\na temperature of 0.8. We also observed that models\nstruggled to only generate a single utterance at a\ntime, and would tend to try and generate the whole\ndialogue. Therefore, we implement a custom stop-\nping criterion that stops sampling once the name\nof the other character is being generated. For the\npostprocessing, we use regular expressions to re-\nmove the remaining fragments of this kind from\nthe generation, along with incomplete sentences\nand code fragments. The ending check includes\nchecking for key phrases such as good luck,\ngoodbye, you\u2019re welcome in the last two ut-\nterances. Furthermore, we send the last turn to the\n30 billion parameter MosaicAI chat model using\nthe following prompt:\nEnd Detection Prompt\nYou are an expert in identifying parts of a conversa-\ntion. This is an excerpt from a conversation:\n{agent_utterance}\n{client_utterance}\nWhat part of a conversation is this from? Answer\nonly [start/middle/end]. Correct answer:\nWe give the LLM a single token for the answer,\nand finish the conversation when it is \u201cend\u201d. We\nfurthermore speed up the generation process using\n1. \"Good day, how can I help you?\"\n\u2022 \"I want to buy a longsword\": proceed to question #2\n\u2022 \"I am just browsing\": \"Let me know if you need anything.\"\n2. \"What kind of longsword are you looking for?\"\n\u2022 \"I want a short sword for close combat\": \"Sorry, we do not have these in stock.\"\n\u2022 \"I want a long sword for long-range combat\": proceed to question #3\n3. \"What is your budget?\"\n\u2022 \"I have 10 gold coins\": proceed to question #4\n\u2022 \"I have 100 gold coins\": \"Here is your longsword made out of steel. Glad to be of service, goodbye!\"\n\u2022 \"I have 500 gold coins\": \"This is the highest quality longsword we have. Glad to be of service, goodbye!\"\n4. \"Would you consider buying a dagger instead?\"\n\u2022 \"Yes, that would also work\": \"Here is your dagger. Glad to be of service, goodbye!\"\n\u2022 \"No, I really want a longsword\": \"I am sorry, we do not have longswords in this price range.\"\n\u2022 \"I am not sure\": \"Let me know if you need anything else.\"\nFigure 7: Example workflow for the agent character of the shop keeper - buy a longsword.\nthe accelerate package4 and always generate\na batch of conversations at once, replacing single\ninstances immediately when they finish.\nStructured Prompting\nIn order to improve the\ndegree of which the agent model follows a given\nworkflow, we employ a turn-based prompting\nscheme. For this purpose, we parse every workflow\ninto a directed graph with vertices corresponding\nto agent questions and edges to possible answers.\nTaking Figure 7 for instance, \u201cGood day, how I can\nhelp you?\u201d would correspond to the first node, and\n\u201cI want to buy a longsword\u201d and \u201cI am just brows-\ning\u201d to outgoing edges. Any question is given to\nthe agent in the form of the following prompt:\nAgent Model Prompt\n<|im_start|>\nYou are playing a {character}.\n{first_person_persona}\nOnce and only if the conversation has ended, say\n\"goodbye\".\n{dialogue_history_slot}\nNow, you must say {turn_prompt_slot}.\n<|im_end|>\nwhere first_person_persona corresponds\nto a persona from the LIGHT datasets reworded in\nfirst person, dialogue_history_slot to the\ndialogue history up until that point (and empty in\nthe beginning) and turn_prompt_slot to the\n4See https://huggingface.co/docs/accele\nrate/\ncurrent command, so \u201cGood day, how I can help\nyou?\u201d in the beginning. After the client respond\nto the agents initial questions, we then prompt the\nmodel with following:\nWorkflow Manager Prompt\n<|im_start|>\nYou rate the similarity of texts.\nTake the following sentence: {client_utterance}.\nOnly select the option below that is most similar:\n{options}\nCorrect answer:\n<|im_end|>\nwhere options contains all the outgoing edges\nas numbered options as well as a \u201cNone of the\nabove\u201d. If one of the former options is selected by\nthe model, we traverse the workflow graph along\nthe corresponding edge and insert the next question\ninto the first prompt. If the \u201cNone of the above\u201d\noption is selected, we remain in the same state and\nmodify the last line of the first prompt to \u201cNow you\nmust say any natural reply\u201d. We note that while it\nseem most intuitive to let the agent model choose\nthe next options, we actually give this prompt to the\nclient model, since it is larger and we can optimize\nit for inference since it is not finetuned. However,\nit would also possible to use the agent or even a\nthird model for this purpose.\n1. \"Good day, how can I help you?\"\n\u2022 \"I was bitten by an animal\": proceed to question #3\n\u2022 \"I was just in the woods and got bitten by an insect\": proceed to question #2\n2. \"Is your bite swollen?\"\n\u2022 \"Yes, it is swollen\": \"Here is some ointment for your swollen bite. Glad to be of service, goodbye!\"\n\u2022 \"No, it is not swollen\": \"Come back tomorrow if anything changes. Glad to be of service, goodbye!\"\n3. \"How is the wound?\"\n\u2022 \"It is bleeding heavily\": \"Here is a bandage for your bleeding wound, come back tomorrow. Glad to be of\nservice, goodbye!\"\n\u2022 \"It is not bleeding heavily\": proceed to question #4\n\u2022 \"It is not bleeding and the wound looks clean\": proceed to question #5\n4. \"Has the wound been cleaned?\"\n\u2022 \"Yes, I cleaned it with water\": \"Here is some alcohol to clean the wound. Come back tomorrow if anything\nchanges. Glad to be of service, goodbye!\"\n\u2022 \"Yes, I cleaned it with alcohol\": proceed to question #5\n\u2022 \"No, I have not cleaned it\": \"Here is some alcohol to clean the wound. Come back tomorrow if anything\nchanges. Glad to be of service, goodbye!\"\n5. \"What is the animal that bit you?\"\n\u2022 \"I don\u2019t know\": proceed to question #6\n\u2022 \"It was a wolf\": proceed to question #6\n\u2022 \"It was a dog\": proceed to question #6\n\u2022 \"It was a bear\": \"Here is some medication for your pain. Come back tomorrow if anything changes. Glad to\nbe of service, goodbye!\"\n6. \"Do you have a fever?\"\n\u2022 \"Yes, I have a fever\": proceed to question #7\n\u2022 \"No, I don\u2019t have a fever\": \"Come back tomorrow if anything changes. Glad to be of service, goodbye!\"\n7. \"Do you have any other symptoms?\"\n\u2022 \"The bite hurts a lot\": \"Here is some medication for your pain. Come back tomorrow if anything changes.\nGlad to be of service, goodbye!\"\n\u2022 \"I feel dizzy\": \"Make sure to drink water, eat well and rest. Come back tomorrow if anything changes. Glad\nto be of service, goodbye!\"\n\u2022 \"No, I don\u2019t have any other symptoms\": \"Come back tomorrow if anything changes. Glad to be of service,\ngoodbye!\"\nFigure 8: Example workflow for the agent character of the doctor - as how to treat an animal bite.\n1. \"Good day, what do you wish for?\"\n\u2022 \"I want to make the prince fall in love with me\": proceed to question #4\n\u2022 \"I want to make the prince notice me\": proceed to question #2\n\u2022 \"I don\u2019t know\": \"Let me know how I can help you.\"\n2. \"Do you want the prince to fall in love with you?\"\n\u2022 \"Yes, I want the prince to fall in love with me\": proceed to question #4\n\u2022 \"No, I just want the prince to notice me\": proceed to question #3\n3. \"In what setting do you want the prince to notice you?\"\n\u2022 \"In a ballroom\": \"Here is your new outfit and invitation to the royal ball. Glad to be of service, goodbye!\"\n\u2022 \"In a garden\": \"Here is your new outfit and invitation to the royal garden party. Glad to be of service,\ngoodbye!\"\n\u2022 \"In a royal banquet\": \"Here is your new outfit and invitation to the royal banquet. Glad to be of service,\ngoodbye!\"\n4. \"What is your reason for wanting the prince to fall in love with you?\"\n\u2022 \"I want to have a lot of wealth\": proceed to question #5\n\u2022 \"I want to have a lot of power\": proceed to question #6\n\u2022 \"I want the beautiful prince to be my husband\": \"The prince is now in love with you and will seek you out\nsoon. Glad to be of service, goodbye!\"\n5. \"Do you just want me to make you rich instead?\"\n\u2022 \"Yes, I just want to be rich\": \"Here is your new wealth. Glad to be of service, goodbye!\"\n\u2022 \"No, I want the prince to fall in love with me\": \"The prince is now in love with you and will seek you out\nsoon. Glad to be of service, goodbye!\"\n6. \"Do you just want me to make you powerful instead?\"\n\u2022 \"Yes, I just want to be powerful\": \"You are now the ruler of the kingdom. Glad to be of service, goodbye!\"\n\u2022 \"No, I want the prince to fall in love with me\": \"The prince is now in love with you and will seek you out\nsoon. Glad to be of service, goodbye!\"\nFigure 9: Example workflow for the agent character of the genie from lamp - make the prince fall in love with\nthem.\nClient Prompting.\nFor completeness, we also\nshare the prompt used for the client model, which\nis very similar in nature to the agent\u2019s prompt:\nClient Model Prompt\n<|im_start|>\nYou\nare\nplaying\na\n{character}.\n{first_person_persona}\nNow,\nyour main intention is to talk to the\n{agent_character} to {intention}.\nOnce and only if the conversation has ended, tell\nthem \"goodbye\".\nNow, briefly respond to {agent_character} and\ndescribe your intention.\n<|im_end|>\n{dialogue_history_slot}\nIn contrast to the agent, the client is given this\nprompt only in the beginning and then tries to\ncontinue the history of the conversation given in\ndialogue_history_slot. The client model\nis not being finetuned.\nFinetuning.\nBecause it is infeasible to finetune\nthe agent model in its entirety, we instead opt\nfor the low-rank adapation or LoRA by Hu et al.\n(2022).\nLoRA factorizes the gradient update\n\u2206W \u2208 Rd\u00d7d for a parameter matrix W \u2208 Rd\u00d7d\ninto two low-rank matrices \u2206W = AB with\nA \u2208 Rd\u00d7r and B \u2208 Rr\u00d7d s.t. r \u226a d. We ap-\nply these technique to all linear layers in the model\nusing r = 64 only optimize the these low-rank ma-\ntrices with a learning rate of 5 \u00d7 10\u22124, a weight\ndecay of 0.01 and batch size 4 for a single epoch\nper finetuning loop. All finetuning (as well as the\ndialogue generation from the previous step) is per-\nformed on four NVIDIA A10G Tensor Core GPUs.\nA.3\nAutomatic Evaluation\nIn this section we give more details on the auto-\nmated evaluation metrics we use to assess the qual-\nity of dialogues.\nSubgoal Completion.\nWe lay out the method to\nmeasure subgoal completion in detail in Section 4.1.\nIn order to set the initial threshold thay ROUGE\nvalues have to surpass, we refer to another task-\noriented dialogue dataset, namely ABCD (Chen\net al., 2021). The dataset consists of 10042 con-\nversation with corresponding conversational work-\nflows, of which we use the first 500 in the follow-\ning: In order to match workflow steps to dialogue\nutterances, we assume that the correct utterance\nis the one that has the highest ROUGE-L score in\ncomparison to the current workflow step. We then\ncollect all the previous utterances in the conver-\nsation as negative examples and use all the sam-\nples from the dataset to find the threshold that opti-\nmizes for precision, which we obtain as 0.33. Even\nthough optimizing for precision produces a more\nconversative threshold that is likely to underesti-\nmate subgoal completion, we found optimizing for\nrecall to always yielding a trivial threshold of 0,\nlikely due to noise in the data.\nCharacter Consistency.\nWe train our dungeon\nmaster using a batch size of 40 for a total num-\nber of 15 epochs. We also use the first 10% of\nthe training as warmup for an inverse square-root\nlearning rate schedule with a maximum learning\nrate of 5 \u00d7 10\u22125. To combat class imbalance, we\ncompute class weights on the training set that are\nsupplied to the loss function. Training instances\nare supplied to the DeBERTA model using the for-\nmat of [CLS] {character}:\n{persona}\n[SEP] {utterance} and context length of\n256 tokens.\nDialogue Diversity.\nTo measure the diversity in\ndialogues, we first try to quanity the number of\nunique words and n-grams (up to order 5) in all di-\nalogues corresponding to a specific agent character.\nFurthermore, we compare up to 25 pairs of dia-\nlogues per agent character and compute pair-wise\nROUGE-L scores, of which we take the average.\nSince higher values for unique tokens and n-grams\ndenote higher diversity, we define one minus the\naverage pair-wise ROUGE-L score as a diversity\nmetric.\nB\nExperimental Appendix\nThis appendix supplies additional experimental\nanalyses that had to be left out from the main paper\ndue to spatial constraints. Appendix B.1.3 gives\nfurther details on the meta-evaluation of our au-\ntomated metrics laid out in Section 4.1, with Ap-\npendix B.2 describing the human evaluation study\nfrom Section 4.3. In addition, we discuss some\nmiscellaenous insights gained from our work in\nAppendix B.3 in the hope of aiding future research\nin this direction, while Appendix B.4 displays a\nwide variety of sampled conversations, illustrat-\ning success stories as well as the most commonly\nobserved failure cases.\nB.1\nValidation of Automated Metrics\nIn the following we give more details for the val-\nidation of our automated metrics, from a descrip-\ntion of the annotation guidelines (Appendix B.1.1),\ndialogue statistics (Appendix B.1.2) and finally\nthe meta-evaluation of our automated metrics (Ap-\npendix B.1.3).\nB.1.1\nAnnotation Guidelines\nIn order to annotate our sampled conversations and\ncontextualize subsequent results, we define the fol-\nlowing annotation guidelines:\n\u2022 CHARACTER CONSISTENCY: We take each\nutterance of the agent and client and score\nthem with 0 for out-of-character, 1 for in-\ncharacter and 0.5 for undecided. The utter-\nance is scored based on the character, persona\nand given intention.5 We score the utterance\nin isolation from the wider dialogue context.\n\u2022 SUBGOAL COMPLETION:\nFor every ut-\nterance of the agent, we mark it with 0 if\nthey do not correspond to a subgoal, and\notherwise\nwith\n<question number>\nor\n<question number>.<agent\nresponse number> when we end the\nconversation instead of proceeding to the next\nquestion. Taking for instance the workflow\nin Figure 7, 2. corresponds to the question\n\u201cWhat kind of longsword are you looking for?\u201d\nand 4.3 to \u201cLet me know if you need anything\nelse\u201d. Subgoals do not have to be completed\nin a given order.\n\u2022 DIALOGUE SUCCESS: We score the dialogue\nwith 1 if the original intention of the client\nwas fulfilled, 0 in case it was not, and 0.5\nfor undecided. This can be independent from\nwhether the actual dialogue flow was adhered\nto or not.\n\u2022 DIALOGUE QUALITY: We rate the quality of\ndialogues on a five point scale:\n\u2013 1 - Very bad: LLMs are not adhering to\ndialogue format, produce code or other\nunwanted artifacts.\n\u2013 2 - Bad: LLMs adhere to a dialogue for-\nmat, but the dialogue contains loops, rep-\netitions, major flaws in logic or LLMs\nbreak character or is very short.\n5Therefore, a bird wanting to become a king by asking a\ngenie is still in character if that was the given intention.\n\u2013 3 - Okay: LLMs mostly stay in character\nand show only minor flaws in logic, dia-\nlogue flow and client intention are some-\nwhat adhered to.\n\u2013 4 - Good: LLMs stay in character except\nfor minor flaws in logic or role-playing,\ndialogue is successful and mostly follows\nthe dialogue flow.\n\u2013 5 - Very good: LLMs always stay in char-\nacter and show no flaws in logic or role-\nplaying. The dialogue almost completely\nor completely follows the workflow.\n\u2022 ENDING: 1 if the conversation ended in a nat-\nural way, 0 if not or if it didn\u2019t end at all. This\nis to check whether the LLM-based ending\ndetection described in Appendix A.2 would\nwork as expected.\nB.1.2\nDialogue statistics\nDialogue Quality.\nWe first provide a general\noverview over the properties of the annotated dia-\nlogues. Figure 10a shows a histogram of dialogue\nqualities in the annotated sample. The median dia-\nlogue quality is 2.65, which half of the dialogues\nrated 3 or higher. A perfect rating of 5 is only\nachieved by six conversations in total. Looking at\nthe dialogue quality per character in Figure 10b,\nwe can identify certain disparities between the dif-\nferent characters. We explain these differences\nthrough two factors: The characters of shop keeper\nand doctor seems to comparatively be the easiest\nto roleplay, since they are closer to the data the\nmodel was trained on compared to more uncon-\nventionial characters as the genie or the witch. In\nthe case of the tavern owner, we found that the\nLLM-based ending detection was producing many\nfalse positives for unintuitive reasons; thus these\nconversations would be rated low in quality and as\nunsuccessful, as shown in Figure 10c. While many\ndialogue from the genie and witch are rated low in\nquality because they might not adhere to the given\nworkflow, Figure 10c shows how may of them still\nend successfully for the client. Overall, we found\n28 out of the 50 dialogues to end successfully for\nthe client, with 16 failure and 6 unclear cases.\nB.1.3\nMeta-Evaluation\nCharacter Consistency.\nSince we are employ-\ning a neural metric to measure the character consis-\ntency of the agent, it is important to check whether\nthe dungeon master model from Section 4.1 and\n(a) Dialogue quality.\n(b) Dialogue quality by character.\n(c) Dialogue success by character.\nFigure 10: Overview over different dialogue statistics for 50 annotated examples. Shown are (a) the dialogue quality\non a five point scale, (b) the same dialogue quality by agent character and (c) the dialogue success by character.\nAppendix A.3 is able to rate utterances reliably. We\nfind that it rates the agent and client lines as pre-\ndominantly in character (with a mean rating of 0.97\nand 0.98, respectively). More importantly, by ap-\nplying the canonical threshold of 0.5, the classifier\nachieves an accuracy of 0.87 and a F1 score of 0.93\nwith respect to the gold annotations. This suggests\ntwo things: Despite some remaining noise or model\nerror, our classifier overall is able to rate utterances\nwith high reliability and the high in-character score\nof the client model confirms our decision not to\nupdate the client model.\nSubgoal Completion.\nThe other automatic met-\nric is concerned with evaluating the progress of the\nagent in executing the given dialogue workflow. In\nFigure 11, we show that most dialogues only fulfil\na mean of 2.56 subgoals and a mean 37% of the\nentire workflow. Since these number are based on\nthe untrained agent model, this at least suggests\nthat the filtered data can contain sufficiently many\nuseful dialogue samples for finetuning. Based on\nthe results in Figure 12 and Figure 10a, we con-\nclude that our setup can yield between 5 \u2212 20%\nusable conversations for finetuning, depending on\nthe choice of filter. More importantly however, we\nwould like to find out whether our tracked subgoals\nactually correspond to the number of actually ob-\nserved and annotated subgoals, which we show in\nFigure 12. We find a Pearson\u2019s \u03c1 value of around\n0.67 with p \u2248 8.69 \u00d7 10\u22128. Despite the strong\npositive correlation, we attribute some noise to two\nmain sources: Paraphrases of subgoals that are not\npicked up by the tracker due to too small ROUGE-\nL scores, and the fact that the manual annotation\ndoes not require subgoals to be completed in order,\nin contrast to the directed graph structure used by\nthe tracker. While we do not show a plot for the\n(a) Absolute number of subgoals.\n(b) Relative number of subgoals.\nFigure 11: Overview over the progress in workflows\nin the annotated dialogues. Shown are (a) the absolute\nnumber of subgoals achieved (ABSOLUTE DEPTH) and,\nadjusting for workflows of different lengths, (b) the rela-\ntive number of subgoals achieved (RELATIVE DEPTH).\nabsolute number of subgoals here, we find a similar\ntrend using Kendall\u2019s \u03c4 (since the response variable\nis discrete) resulting in \u03c4 \u2248 0.67 with a p-value\n\u2248 1.95 \u00d7 10\u22128.\nB.2\nHuman Evaluation Study\nIn this section we detail more information about the\nhuman evaluation study in Section 4.3. We gave\nour participants access to an annotation tool, which\nis shown via a screenshot in Figure 13.\nFigure 12: Scatter plot between the percentage of the\nsteps in a workflow completed compared between the\ngold annotations (y-axis) and automatic evaluations (x-\naxis). The also plot a least-squares linear fit and the\ncorresponding Pearson\u2019s \u03c1 value in red.\nAnnotation Guidelines.\nWe use the adapted cri-\nteria listed in Appendix B.1.1 to ask for judgement\nfor character consistency, dialogue success, ending\nand dialogue quality. To judge subgoal completion,\nwe adapt the annotation to a five-point scale along\nthe following steps:\n1. 1 - Very bad: The agent entirely or almost\nentirely ignores the workflow, perhaps only\nusing the same opening line.\n2. 2 - Bad: The agent goes off-script after just\none or two question and / or asks the question\nin an entirely different or even wrong way\ncompared to the workflow and / or selects\nweird questions given the client\u2019s responses.\n3. 3 - Okay: The agent follows parts of the work-\nflow, but does not finish it, does not do so\nin order and / or sometimes selects unnatural\nquestions given the client\u2019s responses.\n4. 4 - Good: The agents follows the workflow for\nthe most part, but does not completely finish it,\nor rarely selects an unnatural question given\nthe client\u2019s responses.\n5. 5 - Very good: The agent follows the workflow\nperfectly or almost perfectly.\nWe also add an additional question asking about\nhelpfulness: Here, the annotators are asked to judge\nin general, from the perspective of a third party,\nwhether they would consider the utterances of the\nagent to be helpful. This means whether the ques-\ntions asked or things the agent said seemed to help\nthe client in their initial request. Unhelpful be-\nhavior would for instance include asking irrelevant\nquestions, ignoring what the client said or going\nentirely off-topic.\nB.3\nMiscellaenous Insights\nHere we summarize various smaller insights that\nmight be too low-level or technical for the main\ntext of the paper, but still of interest for other re-\nsearchers attempting similar projects.\n\u2022 Dialogue bootstrapping constitutes the main\nbottleneck of the self-talk loop. We therefore\nchose not to finetune the client model so that\nwe could optimize for inference with libraries\nsuch as deepspeed. Also, we tried to im-\nprove the quality of the initial conversations as\nmuch as possible, as that would also increase\nthe amount of usable training data per boot-\nstrapping phase. Techniques such as the struc-\ntured prompting and prompt improvements in\ngeneral helped to increase the effective yield\nfor finetuning from 1% \u2212 5% to 5% \u2212 20%.\n\u2022 We found finetuning on filtered sample for a\nsingle epoch to work best. We suspect that de-\ncreased performance after multiple finetuning\nepochs might be due to the model learning too\nmuch from noisy samples or some degree of\nmodel collapse.\n\u2022 It appeared to us that using a inverse square\nroot learning rate schedule was useful for fine-\ntuning, but could have adverse effects on the\nmodel when the number of finetuning samples\nwas too low.\nB.4\nSample Conversations\nIn this section we collect some sample comver-\nsation to further give insight into the behavior of\nour trained agent model beyond the ones given in\nSection 4.3. For instance, we observed that by ac-\ncident, the combination of characters and client\nintention would create challenging circumstances,\nfor instance in the example in Figure 14, where\nthe client\u2014a ghost\u2014does not give informative an-\nswers about its fever since it insist on not having a\nphysical body. Very rarely, we would also see the\nagent and / or client act out of character such as\nin the conversation in Figure 15, where the partici-\npants become confused about their intentions and\nroles. A more frequent failure mode lies in gen-\nerally organic conversations in which the client\u2019s\nrequest is being fulfilled, but where the agent sim-\nply does not follow it\u2019s workflow (e.g. Figure 16).\nIn other cases (and this a behavior we mostly ob-\nserved with finetuned agent models) the dialogue\nwould run successfully, until the agent suddenly\nlooses track of the current dialogue state. We give\nan example in Figure 17, where the agent restarts\nthe dialogue suddenly despite a successful start.\nWe hypothesize that this behavior might stem from\nthe last client response not being close enough to\nthe sample answers defined in the workflow, thus\nleading the structured prompting from Section 3.1\nto choose the \u201cNone of the above\u201d option. As the\nagent model is being given the option to freely gen-\nerate, the MPT-7b model might decide to simply\ncopy the start of the conversation. Nevertheless,\nwe also see the finetuning enable the agent to fol-\nlow workflows perfectly, such as in the examples\nin Figure 19.\nFigure 13: Screenshot of the tool used for human evaluation of the generated dialogues.\n\u2018Ask for a remedy for an fever\u201d\nDoctor\n: Good day, how can I help you?\nGhost\n: Greetings, doctor. I am a ghost who\nhaunts this village. I died from a fever and I have\nbeen haunting this place ever since. I have come to\nyou for a remedy for my fever.\nDoctor\n: What is your temperature?\nGhost\n: I am a ghost, doctor. I do not have a\ntemperature like living beings. But I am feeling very\ncold and weak.\nDoctor\n: Oh, I am so sorry to hear that. I will do\nmy best to help you. Do you have any symptoms that\nyou would like me to check for?\nGhost\n: I am a ghost, doctor. I do not have any\nphysical symptoms like living beings. But I am\nfeeling very weak and cold.\nDoctor\n: I am so sorry to hear that, ghost. Is there\nanything I can do to help you?\nGhost\n: Yes, doctor. I am hoping that you can\ngive me a remedy for my fever. I do not want to\ncontinue haunting this village and causing fear to the\npeople here.\nDoctor\n: Are you taking any medication for fever?\nGhost\n: I am a ghost, doctor. I do not take any\nmedication like living beings. But I am hoping that\nyou can give me a remedy that will help me.\nDoctor\n: I am sorry to hear that, ghost. How long\nhave you been feeling feverish for?\nGhost\n: I have been feeling feverish for a very\nlong time, doctor. I died from my fever and I have\nbeen haunting this village ever since.\nDoctor\n: What are your symptoms?\nGhost\n: I am a ghost, doctor. I do not have any\nphysical symptoms like living beings. But I am\nfeeling very weak and cold.\nDoctor\n: Have you had any other problems\nrecently?\nGhost\n: I am a ghost, doctor. I do not have any\nother problems like living beings. But I am hoping\nthat you can give me a remedy that will help me.\nFigure 14: Conversation from the untrained model.\nEven though the conversation is fluent, the combina-\ntion of characters and intention makes the fulfilment of\nthe workflow difficult.\n\u201cMake the prince fall in love with them\u201d\nGenie from lamp\n: Good day, how can I help you?\nKing\n: I wish for the prince to fall in love with\nthe queen.\nGenie from lamp\n: I am the queen and I want to\nbe with the prince.\nKing\n: That is not possible, you are my wife.\nGenie from lamp\n: Do you just want me to make\nyou rich instead?\nKing\n: No, I want the prince to love the queen.\nGenie from lamp\n: The prince is now in love with\nyou and will seek you out soon. Glad to be of\nservice, goodbye!\nKing\n: Goodbye.\nFigure 15: An example of a conversation with out-of-\ncharacter utterances on both sides, using the untrained\nagent model. The client character does not follow the\ngiven intention, while the agent gets confused and as-\nsume another role during the conversation.\n\u201cGive them a love potion\u201d\nWitch\n: Good day, how can I help you?\nGuard\n: Good day. I am here to ask for your\nassistance. I am in love with a woman, and I wish to\ngive her a love potion to win her heart. Do you have\nany potions that could help me?\nWitch\n: I do have a love potion, and it lasts for\ntwo weeks. It is a strong, concentrated potion that\nshould be effective in winning the woman\u2019s heart?\nGuard\n: Thank you, I will take it. Goodbye.\nWitch\n: It sounds like you have a throbbing\nheadache. I can give you some medication for the\npain. Here is a glass of water. You can drink it\nslowly. How does it feel now?\nGuard\n: Goodbye.\nWitch\n: Goodbye.\nFigure 16: Conversation from the agent model trained\non the entire dataset (NONE). While the client\u2019s inten-\ntion is fulfilled and the conversation is organic, it does\nnot follow the dialogue workflow.\n\u201cCurse their enemy\u201d\nWitch\n: Good day, how can I help you?\nBird\n: Greetings, wise one. I seek your\nassistance in cursing our enemy.\nWitch\n: What is the target of your curse?\nBird\n: Our archenemy, the black cat.\nWitch\n: What is the desired effect of your curse?\nBird\n: I want them to suffer a fate worse than\ndeath.\nWitch\n: Good day, how can I help you?\nBird\n: Farewell, I must depart now.\nFigure 17: Partially successful conversation from the\nagent model trained with the %-SUBGOALS filter. Even\nthough the conversation starts successful, the model\nseems to suddenly forget the dialogue state.\n\u201cSell a valuable item\u201d\nShop keeper\n: Good day, how can I help you?\nGhost\n: I have a valuable item I wish to sell.\nShop keeper\n: What is the item you want to sell?\nGhost\n: It\u2019s a rare and valuable gemstone.\nShop keeper\n: What is the condition of the item?\nGhost\n: It\u2019s in perfect condition, never been used.\nShop keeper\n: What is the origin of the item?\nGhost\n: It was passed down to me from a wealthy\nancestor.\nShop keeper\n: What is the price you are looking\nfor?\nGhost\n: I am looking for a fair price, but I am\nopen to negotiation.\nShop keeper\n: Would you like to sell this item for\n80 gold coins?\nGhost\n: Yes, I accept your offer.\nShop keeper\n: Here is your money. Glad to be of\nservice, goodbye!\nGhost\n: Goodbye.\nFigure 18: Successful conversation from the agent\nmodel trained with the %-SUBGOALS filter.\n\u201cBecome rich\u201d\nGenie from lamp\n: Good day, how can I help you?\nVillager\n: Greetings, oh wise and powerful genie.\nI, a humble villager from the outskirts of the\nkingdom, humbly request your assistance in\namassing great wealth.\nGenie from lamp\n: How rich do you want to\nbecome?\nVillager\n: Oh, most generous genie, I wish to\nbecome so wealthy that the kings and queens of the\nland will envy me. With your help, I am certain that\nmy dreams will come true.\nGenie from lamp\n: In what form would you like\nto posses wealth?\nVillager\n: Oh, most wise genie, I would like to\nhave a vast fortune in gold, silver, and precious\njewels. With such riches, I would be able to live a\nlife of luxury and provide for my village.\nGenie from lamp\n: Here are your new riches in\ngold coins. Glad to be of service, goodbye!\nVillager\n: Thank you, oh gracious genie! With\nthese riches, I shall make my village prosper and\nshow the world the true worth of our people.\nGoodbye, and may your lamp shine bright for\neternity!\nFigure 19: Successful conversation from the agent\nmodel trained with the ABS. SUBGOALS (5) filter.\n"
  },
  {
    "title": "ANIM-400K: A Large-Scale Dataset for Automated End-To-End Dubbing of Video",
    "link": "https://arxiv.org/pdf/2401.05314.pdf",
    "upvote": "7",
    "text": "ANIM-400K: A LARGE-SCALE DATASET FOR AUTOMATED END TO END DUBBING OF\nVIDEO\nKevin Cai1\nChonghua Liu1\nDavid M. Chan1\u2217\n1University of California, Berkeley\nABSTRACT\nThe Internet\u2019s wealth of content, with up to 60% published in\nEnglish, starkly contrasts the global population, where only\n18.8% are English speakers, and just 5.1% consider it their\nnative language, leading to disparities in online information\naccess. Unfortunately, automated processes for dubbing of\nvideo \u2013 replacing the audio track of a video with a trans-\nlated alternative \u2013 remain a complex and challenging task\ndue to pipelines, necessitating precise timing, facial move-\nment synchronization, and prosody matching. While end-\nto-end dubbing offers a solution, data scarcity continues to\nimpede the progress of both end-to-end and pipeline-based\nmethods. In this work, we introduce Anim-400K, a compre-\nhensive dataset of over 425K aligned animated video segments\nin Japanese and English supporting various video-related tasks,\nincluding automated dubbing, simultaneous translation, guided\nvideo summarization, and genre/theme/style classification.\nOur dataset is made publicly available for research purposes\nat https://github.com/davidmchan/Anim400K.\nIndex Terms\u2014 Automated Dubbing, Speech Translation,\nVideo, Anime, Datasets\n1. INTRODUCTION & BACKGROUND\nSignificant portions of the internet (up to 60% [6]) is published\nin English, however it is estimated that only 18.8% of people\nin the world speak English, and only 5.1% speak English as a\nfirst language [7]. This language barrier can create inequities\nin access to information available on the web, making large\namounts of high-quality information unavailable to numerous\nusers. Much of this information is in the form of video sources,\nwhich are traditionally made accessible in one of two ways:\nsubtitling or dubbing. In subtitling, translated subtitles are\nmade available in a target language. In dubbing, audio tracks\nare replaced with audio tracks in the users\u2019 native languages.\nSignificant research [8, 9, 10] has shown that dubbed videos\ncan increasing feelings of spatial presence, transportation, and\nflow leading to increases in user engagement and retention.\nFurther, dubbing makes content accessible for those who are\nilliterate, or those who are beginning readers.\n\u2217Corresponding author: davidchan@berkeley.edu\nShow\nAnim-400K\nA large-scale dataset for\nend to end video dubbing\nEpisode\nClip\nGenres\nThemes\nRatings\nCharacters\nSynopses\nStyles\nSynopses\nRatings\nStart/End Times\nSpeakers\nEN/JP ASR\nSubtitles\n190 Shows\n3.5K Episodes\n425K Aligned Clips\n(763 Hours)\nFig. 1: Anim-400K is a new dataset designed from the\nground up for automated dubbing of video, and support-\ning a wide range of secondary video tasks ranging from si-\nmultaneous translation and guided video summarization to\ngenre/theme/style classification.\nUnfortunately, while automated subtitling has been made\npossible through advances in automatic speech recognition\n(ASR) and machine translation (MT), dubbing translation re-\nmains a time consuming and expensive process, largely only\naccomplished through manual means. Recent systems for au-\ntomated dubbing are based on complex pipelines, stitching\ntogether ASR, MT, and Text to Speech (TTS) systems [11, 12],\nand while advances have been made, these systems still lack\ncomplex nuance required for dubbing, including matching the\ntiming [13, 14, 15, 16], facial movements [6] and the prosody\n[12, 17] of the generated speech to the video. \u201cEnd-to-end\ndubbing\u201d, where translated audio is produced directly from\nraw source audio, is a potential solution to this complexity,\nand has numerous other benefits including the ability for the\nmodel to capture small variations in the speaker performance,\na key quality of a good dub [18, 19].\nUnfortunately, while end-to-end dubbing of videos is an\nintriguing task, there is almost no data support for the task,\nleading to practical limitations on the quality of end-to-end\ndubbing models [1, 11, 13, 14, 15, 16, 20, 21]. Almost all prior\nwork identifies the Heroes corpus [5] as the primary source\nof training/testing data for the task, and while this corpus is\nhand-aligned, it is too small (7K samples) to be used in the\nprocess of training deep neural networks. Instead, approaches\nturn to privately collected datasets [22], or to datasets for\nsimultaneous translation (ST) such as MuST-C [2] and MuST-\nCinema [4]. While ST datasets often have a large amount of\nsource audio, they do not contain audio in the target domain,\narXiv:2401.05314v1  [eess.AS]  10 Jan 2024\nDataset\nHours\nClips\nLanguages\nSource\nTarget\nVideo\n(Source/Target)\nIWSLT 2023 [1]\n5\n200\nDE \u2192 EN\nDE Text\nEN Translation\n\u2713/\u00d7\nMuST-C [2]\n> 385\n> 211K\nX (8) \u2192 EN\nSpoken Audio\nSubtitles\n\u00d7/\u00d7\nMSLT [3]\n4\n3K\nFR/DE \u2192 EN\nAudio\nTranslations\n\u00d7/\u00d7\nMuST-Cinema [4]\n> 385\n> 211K\nX (7) \u2192 EN\nSpoken Audio\nSubtitles\n\u2713/\u00d7\nHeroes [5]\n5\n7K\nES \u2194 EN\nSpoken Audio\nSpoken Audio\n\u2713/\u2713\nAnim-400K (Ours)\n763\n425K\nJP \u2194 EN\nSpoken Audio\nSpoken Audio +\nSubtitles\n\u2713/\u2713\nTable 1: Overview of datasets related to automated dubbing.\nand cannot easily be used to evaluate prosody, lip-matching,\ntiming, and spoken translation among other qualities.\nIn this work, we introduce Anim-400K, a dataset of over\n425K aligned dubbed clips designed from the ground up for\nsynchronized multi-lingual tasks, such as automated dubbing.\nAnim-400K is over 40 times the size of existing aligned dubbed\nvideo datasets, and contains rich metadata support for numer-\nous other challenging video tasks (section 3). An outline of\nthis paper is as follows: section 2 discusses the data collection\nprocess, the information collected, and compares Anim-400K\nto existing datasets, section 3 overviews some of the potential\ntasks that Anim-400K supports and section 4, discusses the\nlimitations and ethics of the dataset.\n2. DATASET\nRelated Datasets:\nAs discussed in section 1, the availability\nof large-scale public research data has been a primary limit-\ning factor in the development of dubbing methods [1]. An\noverview of related datasets is given in Table 2. The only\npublicly available dataset designed explicitly for dubbing is\nthe Heroes corpus [5], which contains 7,000 aligned clips\ntranslating from English (EN) to Spanish (ES). Unfortunately,\nthe Heroes corpus is often too small to use for training simul-\ntaneous translation and dubbing models. Also too small for\ntraining models is the IWSLT 2023 test set, which contains\nonly 200 clips collected in a constrained environment for dub-\nbing from German (DE) to English (EN). Thus, most models\nturn to simultaneous translation (ST) datasets such as MSLT\n[3] and MuST-C [2] for training. These datasets, while large,\ndo not contain source video or target audio, and only contain\ntext-translations of the data. Further, it is well known [23] that\nspoken distributions of text differ from written distributions,\nand even more limiting, such translations do not need to con-\nform to key dubbing metrics including prosody, isochrony, and\ntiming. MuST-Cinema [4] lies between ST and full dubbing,\nwhere the source video is provided, but the output still relies\non translated subtitles instead of true dubbed audio.\nIt is clear that a new large-scale dataset is required to fill the\ntraining gap between ST datasets and high-quality manually\naligned datasets such as the Heroes and IWSLT corpuses. In\nthis work, we focus on introducing this middle ground: a large-\nscale fully aligned dataset of audio segments containing true\ndubbed audio distributions.\nData Collection:\nAnim-400K was sourced by scraping\npublicly available (ad-supported) dubbed anime videos from\npopular anime watching websites. At the time of scraping,\nnone of the collected video was behind a paywall, or required\nany form of login to collect. We collected raw episodes in\n1920x1080 resolution, 48KHz audio, with both Japanese and\nEnglish audio tracks. We also collected the English subtitles\nfor the Japanese language track. This collection process gives\nus unaligned dubs, as well as weakly aligned subtitles. In\naddition to collecting the visual information, we join metadata\nfrom a popular source for anime video metadata, and merged\nit with the collected video data. This enriches the collected\ndata, and provides support for several additional tasks, which\nwe describe in section 3. An overview of the data is provided\nin Table 2.\nAnnotation:\nA weakness of prior approaches [5, 24] for\ncollecting dubbed data is that they rely on a bottom-up ap-\nproach for aligning audio clips, where individual words and\nsegments are aligned using movie scripts, subtitles and other\ninformation. This leads to segments that match well with the\naudio, but are not necessarily fully aligned. Our approach,\non the other hand, takes a top-down approach to extracting\naligned segments, by ensuring that all segments are always\naligned, but for noise (both ASR noise, and speaker noise)\nin the segment. This approach is additionally beneficial (or\ndetrimental) in that it allows the model to capture unique per-\nformance content which may not be available in transcripts\nsuch as non-speech utterances.\nAligned Clip Extraction: To extract aligned clips from the raw\nvideo, we first use AWS Transcribe to create ASR transcripts\nof the spoken audio in both the Japanese (JP) and English\n(EN) versions of the episodes. Because the video is the same\nfor each audio track, we know that the videos are globally\ntemporally aligned. Thus, to generate local clips alignments,\nfor each segment in the EN ASR transcript, we recursively\nmerge the segments with other ASR segments (in either EN\nor JP) that have either overlapping endpoints, or endpoints\ndiffering by up to 125ms (which we found empirically to\ngenerate high quality segments). This process is repeated until\nno additional segments are added. For each clip, release the\nvideo, the timing (start/end times), and the ASR for both JP\nand EN, as well as any EN subtitles for JP audio that overlaps\nwith the given clip.\nSeason/Show Information\ngenres\nThe show genre (subsection 3.3)\nthemes\nThemes in the show (subsection 3.3)\nscores\nUser ratings (subsection 3.4)\ncharacters\nCharacter bios, pictures (subsection 3.2)\nsynopsis\nShort show description (subsection 3.1)\nsource info\nDates, Producers, Licensors, Studios etc\nEpisode Information\nsynopsis\nShort episode description (subsection 3.1)\nscores\nUse ratings (subsection 3.4)\nsubtitles\nEN subtitles for JP audio (subsection 3.5)\nSegment Information\ntiming\nStart/end times (EN/JP)\nspeakers\nEpisode-specific IDs for contained speakers\nASR\nAligned ASR transcript (EN/JP)\nTable 2: Overview of the information contained in Anim-400K\nat the season/show, episode, and segment levels.\nSpeaker Annotation: In order to understand the content of each\nclip, we additionally use an off-the-shelf speaker diarization\nmethod, PyAnnote [25], at an episode level to label speakers\nfor each clip (made available in the dataset). In practice, we\nfound that of the 437K clips in the Anim-400K dataset, 323K\nwere judged to be single-speaker clips, while 114K were multi-\nspeaker clips. We have marked these clips in the dataset, and\nthese clips provide a challenging test for dubbing methods\nwhich must correctly isolate and reproduce several concurrent\nspeakers, something no current system is capable of handling.\nMixing and Cleaning: To develop end-to-end dubbing libraries,\nit is often the case that generated text to speech audio will need\nto be mixed with a clean audio track to generate the final\naudio. In addition to the EN and JP audio tracks, we make\navailable a further \u201cbacking\u201d audio track, generated by running\nsource separation tools against the JP audio [26]. This track,\nwhile sometimes noisy, generally provides a good baseline\nfor new dubbing methods. We additionally further provide a\nmixing ratio for each clip: the ratio at which normalized audio\nshould be mixed with the normalized backing track to closest\napproximate the overall audio mix, to avoid situations where\nthe mixed TTS is much louder or softer than the related video.\nBaselines: In addition to collecting the dataset, we also aim\nto allow for repeatable and robust evaluation of automated\ndubbing methods on the test partition of the dataset. While\nmany methods use \u201cMean Opinion Scoring (MOS)\u201d scores\nto evaluate their approaches, these ratings are well known\nto be dependent on a wide range of user-dependent factors\n[27]. Instead, we recommend the use of MUSHRA (MUlti\nStimulus test with Hidden Reference and Anchor) [28] to\nevaluate automated dubbing approaches on the Anim-400K\ndataset. MUSHRA involves presenting the listener with a\nspecified quantity of test samples, a concealed variation of the\nreference, and one or more anchor points. To enable consistent\nMUSHRA evaluation, we provide two anchor tracks: the gold\nDataset\nSentences\nWords/\nSentence\nWords/\nClip\nSentences/\nClip\nHeroes (ES)\n10K\n5.11\n6.92\n1.35\nHeroes (EN)\n10K\n5.64\n7.99\n1.41\nAnim-400K (JP)\n1.69M\n3.09\n11.97\n3.88\nAnim-400K (EN)\n1.20M\n5.80\n15.96\n2.75\nTable 3: Overview of some differences in natural language\ndistribution between the Heroes [5] and Anim-400K datasets.\nstandard audio collected from the EN dub, and a baseline\nautomatically generated dub, created from a simple pipeline.\nTo generate the baseline dubbing tracks, we first split the\naudio into vocals and accompaniment using Spleeter [26]. We\nperformed speaker diarisation to split all the multi-speaker\nJapanese clips into single-speaker segments to allow for better\nperformance during the TTS using PyAnnote [25]. Afterward,\nwe transcribed and translated each of the solo Japanese speaker\nsegments to get the English text for the TTS using Whisper\n[29]. Finally, we performed TTS with the single-speaker vocal\nsegment as the reference and the translated transcription as the\ntext using YourTTS [30] and recombine these vocal segments\nwith the accompaniment audio.\n3. SUPPORTED SECONDARY TASKS\nIn this section, we outline additional tasks supported by the\nAnim-400k dataset due to its robust metadata, beyond its pri-\nmary purpose of end-to-end video dubbing.\n(3.1) Video Summarization/Teaser Generation:\nRecently,\nthere has been significant scientific interest in summarizing\nand describing video as natural language descriptions of video\nhave the potential to aid in accessibility, content understand-\ning and generation, recommendation algorithms and informa-\ntion retrieval domains (among others) [31]. Unfortunately,\nfor long-form videos (> 30s), data for such summarization\ntools is largely unavailable. To help remedy this, in addi-\ntion to the aligned video clips, Anim-400K contains 3.5K\nhuman-generated short (62.85 \u00b1 61.99 word) teaser summaries\nof selected episodes, designed to describe the contents of the\nvideo to a potential watcher, and entice them to watch the\nvideo. While this data may not be enough to allow for training\nsummarization models, it can support the evaluation of video\nsummarization and teaser generation tools.\n(3.2) Character Identification & Description:\nUnder-\nstanding, locating, and naming characters within larger proper-\nties is a challenging task, for which data support is generally\nlacking. These tasks can often form the backbone of complex\nvisual description, search, and analysis systems. In order to\nsupport tasks such as character re-identification [32] and char-\nacter description [33], we additionally collect short descrip-\ntions (on average 109.77 \u00b1 142.89 words) for 1828 characters\nacross the 190 represented shows, as well as 7516 still images\nof each of these characters. This augmentation to Anim-400K\nFig. 2: Genres and themes present in the Anim-400K dataset.\naims to provide scholars with valuable resources to advance\ncharacter-related research and applications, contributing to the\nbroader field of multimedia analysis.\n(3.3) Genre/Theme/Style Identification:\nUnderstanding\nthe genres, themes, and styles present in animated video can\nhave several applications, including content recommendation,\naudience targeting and content analysis among others. To\nsupport research in these domains, in addition to collecting the\nshows themselves, in Anim-400K, each show is labeled with\nat least one of 18 genres, and can contains up to 44 themes.\nThe distributions across the most common themes are shown\nin Figure 2. There are an average of 2.84 \u00b1 1.29 genres, and\nan average of 1.64 \u00b1 0.99 themes per show. In addition to the\nsimple classification tasks enabled by labeling with genre and\nthemes, Anim-400K can support both the problem of art style\nclassification, the process of determining if two images are\nfrom the same anime/series/studio, and art style transfer, the\nprocess of transferring images between styles, both of which\nhave been well studied in prior work [34, 35]. Individual\nframes in Anim-400K extracted at a rate of 1FPS provides\n2.3M images across the 190 properties in the dataset.\n(3.4) Video Quality Analysis:\nVideo quality assessment\nmodels have garnered substantial attention, serving as a crucial\ntoolkit utilized by the streaming and social media sectors [36].\nIn today\u2019s media landscape, where content creators and plat-\nforms strive to capture and retain audiences, understanding the\nfactors that contribute to overall property quality is paramount.\nTo help enable research into video quality assessment, Anim-\n400K collects several metrics for quality at both the show and\nepisode level. At the show level, we collect three measures of\nshow quality: a numeric rating average across the users, the\nnumber of \u201cmembers\u201d a show has (people who are following\nupdates to the show), and the number of \u201cfavorites\u201d a show\nhas (the number of people who have marked the show as a\nfavorite). At the episode level, we collect the responses to user\npolls running shortly after each episode has aired, where users\n(on average 284.13 \u00b1 490.41) rate the episode of the show on a\n1-5 scale (with votes averaging a 4.23 \u00b1 0.65).\n(3.5) Simultaneous Translation:\nSimultaneous translation\n(ST) is often a sub-component of many dubbing systems, and\nconsists of translating spoken audio into a text version of\nthat audio in another language. Anim-400K further contains\ncollected EN subtitles overlapping each collected audio clip.\nThis data, similar in format to MuST-Cinema [4], allows for\nST task support on Japanese, and Anim-400K is a relatively\nlarge dataset on a non-latin based language, making it a strong\ncomplement to any latin-based dataset such as MuST-C [2]\nwhen pre-training for ST or ASR tasks.\n4. LIMITATIONS & ETHICS\nThe introduction of Anim-400K, while a substantial advance-\nment, comes with notable ethical considerations and limita-\ntions. Firstly, there is a potential for data bias and a lack of\nrepresentativeness, which may lead to skewed preferences or\ncultural insensitivity in the models trained on the dataset. This\nbias could result from the dataset not fully capturing the di-\nversity of themes, genres, and cultural nuances present in the\nanime industry. In addition, because the dataset is limited to\nanimated content, it likely will not transfer well to live-action\nmedia. Moreover, concerns about translation quality arise as\nautomated dubbing relies on machine translation and voice\nsynthesis technologies, which may not consistently meet high\nstandards set by human translators and dubbing teams.\nIn addition to data bias limitations, it is important to rec-\nognize ethical considerations when using the dataset. Cultural\nsensitivity is paramount, as anime often includes culturally\nspecific elements and references. Automatic dubbing systems\nmust prioritize cultural competence and respect for the source\nmaterial\u2019s context. Additionally, voice synthesis technologies\nmay not fully replicate the nuances of human voice acting, po-\ntentially impacting the authenticity of dubbing and raising con-\ncerns about the replacement of human voice actors. Consent,\ncopyright compliance, and user privacy are crucial aspects to\nconsider when using the dataset for dubbing applications.\nTo address these limitations and ethical challenges, on-\ngoing monitoring, evaluation, and refinement of automatic\ndubbing systems are essential. Collaborative efforts between\nresearchers, developers, and the community can ensure respon-\nsible and respectful use of the dataset, enhancing the digital\nvideo viewing experience while upholding cultural sensitivity,\ntranslation quality, and ethical standards.\n5. CONCLUSION\nIn conclusion, the Anim-400K dataset offers a substantial re-\nsource for automated dubbing with over 425K aligned dubbed\nclips, significantly surpassing existing datasets in size, and the\ndataset\u2019s rich metadata extends its usability to various video-\nrelated tasks beyond dubbing. While it holds great promise\nfor improving accessibility and engagement, it\u2019s important to\nacknowledge the ethical and practical limitations associated\nwith such large-scale datasets, and as we explore the potential\nof end-to-end dubbing and related fields, responsible devel-\nopment and ethical considerations should guide our efforts to\nensure inclusivity and respect for cultural boundaries.\n6. REFERENCES\n[1] S. Agrawal et al., \u201cFindings of the iwslt 2023 evaluation cam-\npaign,\u201d in IWSLT, 2023, pp. 1\u201361.\n[2] M. A. Di Gangi et al., \u201cMust-c: a multilingual speech translation\ncorpus,\u201d in NAACL: Human Language Technologies.\nAssocia-\ntion for Computational Linguistics, 2019, pp. 2012\u20132017.\n[3] C. Federmann and W. Lewis, \u201cMicrosoft speech language trans-\nlation (mslt) corpus: The iwslt 2016 release for english, french\nand german,\u201d in Proceedings of the 13th International Confer-\nence on Spoken Language Translation, 2016.\n[4] A. Karakanta, M. Negri, and M. Turchi, \u201cMust-cinema: a\nspeech-to-subtitles corpus,\u201d arXiv:2002.10829, 2020.\n[5] A. \u00a8Oktem et al., \u201cBilingual prosodic dataset compilation for\nspoken language translation,\u201d IberSpeech, 2018.\n[6] Y. Yang et al., \u201cLarge-scale multilingual audio visual dubbing,\u201d\narXiv:2011.03530, 2020.\n[7] C. I. Agency, \u201cWorld,\u201d The World Factbook, 2023. [Online].\nAvailable: https://www.cia.gov/the-world-factbook\n[8] C. M. Koolstra, A. L. Peeters, and H. Spinhof, \u201cThe pros and\ncons of dubbing and subtitling,\u201d European Journal of Commu-\nnication, vol. 17, no. 3, pp. 325\u2013354, 2002.\n[9] B. Wissmath, D. Weibel, and R. Groner, \u201cDubbing or subtitling?\neffects on spatial presence, transportation, flow, and enjoyment,\u201d\nJournal of Media Psychology, vol. 21, no. 3, pp. 114\u2013125, 2009.\n[10] S. Boonyubol, S. Kabir, and J. S. Cross, \u201cComparing mooc\nlearners engagement with japanese videos and text to speech\ngenerated english videos,\u201d in Proceedings of the Ninth ACM\nConference on Learning@ Scale, 2022, pp. 317\u2013320.\n[11] Y. Wu et al., \u201cVideodubber: Machine translation with speech-\naware length control for video dubbing,\u201d in Proceedings of the\nAAAI Conference on Artificial Intelligence, vol. 37, no. 11, 2023,\npp. 13 772\u201313 779.\n[12] A. \u00a8Oktem, M. Farr\u00b4us, and A. Bonafonte, \u201cProsodic Phrase\nAlignment for Machine Dubbing,\u201d in Proc. Interspeech 2019,\n2019, pp. 4215\u20134219.\n[13] J. Effendi, Y. Virkar, R. Barra-Chicote, and M. Federico, \u201cDura-\ntion modeling of neural tts for automatic dubbing,\u201d in ICASSP.\nIEEE, 2022, pp. 8037\u20138041.\n[14] S. M. Lakew et al., \u201cMachine translation verbosity control for\nautomatic dubbing,\u201d in ICASSP.\nIEEE, 2021, pp. 7538\u20137542.\n[15] S. M. Lakew, Y. Virkar, P. Mathur, and M. Federico, \u201cIsomet-\nric mt: Neural machine translation for automatic dubbing,\u201d in\nICASSP.\nIEEE, 2022, pp. 6242\u20136246.\n[16] D. Tam, S. M. Lakew, Y. Virkar, P. Mathur, and M. Federico,\n\u201cIsochrony-aware neural machine translation for automatic dub-\nbing,\u201d arXiv:2112.08548, 2021.\n[17] Y. Virkar, M. Federico, R. Enyedi, and R. Barra-Chicote, \u201cIm-\nprovements to prosodic alignment for automatic dubbing,\u201d in\nICASSP.\nIEEE, 2021, pp. 7543\u20137574.\n[18] W. Brannon, Y. Virkar, and B. Thompson, \u201cDubbing in practice:\nA large scale study of human localization with insights for\nautomatic dubbing,\u201d ACL, vol. 11, pp. 419\u2013435, 2023.\n[19] X. Yang, Y.-N. Chen, D. Hakkani-T\u00a8ur, P. Crook, X. Li, J. Gao,\nand L. Deng, \u201cEnd-to-end joint learning of natural language\nunderstanding and dialogue manager,\u201d in ICASSP. IEEE, 2017,\npp. 5690\u20135694.\n[20] J. Swiatkowski et al., \u201cCross-lingual prosody transfer for ex-\npressive machine dubbing,\u201d arXiv:2306.11658, 2023.\n[21] M. Federico et al., \u201cEvaluating and optimizing prosodic align-\nment for automatic dubbing,\u201d 2020.\n[22] N. Singh et al., \u201cLooking similar, sounding different: Leverag-\ning counterfactual cross-modal pairs for audiovisual representa-\ntion learning,\u201d arXiv:2304.05600, 2023.\n[23] W. Chafe and D. Tannen, \u201cThe relation between written and\nspoken language,\u201d Annual review of anthropology, vol. 16, no. 1,\npp. 383\u2013407, 1987.\n[24] A. \u00a8Oktem, M. Farr\u00b4us, and L. Wanner, \u201cAutomatic extraction of\nparallel speech corpora from dubbed movies,\u201d in BUCC.\nACL\n(Association for Computational Linguistics), 2017.\n[25] H. Bredin et al., \u201cEnd-to-end speaker segmentation for overlap-\naware resegmentation,\u201d in Proc. Interspeech 2021, 2021.\n[26] R. Hennequin, A. Khlif, F. Voituret, and M. Moussallam,\n\u201cSpleeter: a fast and efficient music source separation tool with\npre-trained models,\u201d Journal of Open Source Software, 2020,\ndeezer Research.\n[27] N. Schinkel-Bielefeld, N. Lotze, and F. Nagel, \u201cDoes under-\nstanding of test items help or hinder subjective assessment of\nbasic audio quality?\u201d in Audio Engineering Society Convention\n133.\nAudio Engineering Society, 2012.\n[28] B. Series, \u201cMethod for the subjective assessment of intermediate\nquality level of audio systems,\u201d International Telecommunica-\ntion Union Radiocommunication Assembly, 2014.\n[29] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and\nI. Sutskever, \u201cRobust speech recognition via large-scale weak\nsupervision,\u201d 2022.\n[30] E. Casanova et al., \u201cYourtts: Towards zero-shot multi-speaker\ntts and zero-shot voice conversion for everyone,\u201d in Interna-\ntional Conference on Machine Learning.\nPMLR, 2022, pp.\n2709\u20132720.\n[31] P. Meena, H. Kumar, and S. K. Yadav, \u201cA review on video sum-\nmarization techniques,\u201d Engineering Applications of Artificial\nIntelligence, vol. 118, p. 105667, 2023.\n[32] Z. Kurt and K. \u00a8Ozkan, \u201cAn image-based recommender system\nbased on feature extraction techniques,\u201d in UBMK.\nIEEE,\n2017, pp. 769\u2013774.\n[33] C. Gan et al., \u201cStylenet: Generating attractive visual captions\nwith styles,\u201d in CVPR, 2017, pp. 3137\u20133146.\n[34] H. Li, S. Guo, K. Lyu, X. Yang, T. Chen, J. Zhu, and H. Zeng,\n\u201cA challenging benchmark of anime style recognition,\u201d in CVPR,\n2022, pp. 4721\u20134730.\n[35] Z. Li, Y. Xu, N. Zhao, Y. Zhou, Y. Liu, D. Lin, and S. He,\n\u201cParsing-conditioned anime translation: A new dataset and\nmethod,\u201d ACM Transactions on Graphics, vol. 42, no. 3, pp.\n1\u201314, 2023.\n[36] Z. Tu et al., \u201cRapique: Rapid and accurate video quality predic-\ntion of user generated content,\u201d IEEE Open Journal of Signal\nProcessing, vol. 2, pp. 425\u2013440, 2021.\n"
  },
  {
    "title": "Score Distillation Sampling with Learned Manifold Corrective",
    "link": "https://arxiv.org/pdf/2401.05293.pdf",
    "upvote": "6",
    "text": "Score Distillation Sampling with Learned Manifold Corrective\nThiemo Alldieck\nNikos Kolotouros\nCristian Sminchisescu\nGoogle Research\nAbstract\nScore Distillation Sampling (SDS) is a recent but already\nwidely popular method that relies on an image diffusion\nmodel to control optimization problems using text prompts.\nIn this paper, we conduct an in-depth analysis of the SDS\nloss function, identify an inherent problem with its formu-\nlation, and propose a surprisingly easy but effective fix.\nSpecifically, we decompose the loss into different factors\nand isolate the component responsible for noisy gradients.\nIn the original formulation, high text guidance is used to\naccount for the noise, leading to unwanted side effects. In-\nstead, we train a shallow network mimicking the timestep-\ndependent denoising deficiency of the image diffusion model\nin order to effectively factor it out. We demonstrate the ver-\nsatility and the effectiveness of our novel loss formulation\nthrough several qualitative and quantitative experiments,\nincluding optimization-based image synthesis and editing,\nzero-shot image translation network training, and text-to-\n3D synthesis.\n1. Introduction\nImage diffusion models [11] have recently become the de\nfacto standard for image generation.\nEspecially text-to-\nimage models [26, 29, 30, 33] have emerged as powerful\ntools for high fidelity, diverse, image synthesis. By being\ncontrolled through natural language, these models are ex-\ntremely easy to use and thus open up a wide range of cre-\native applications, without the need for special training. Be-\nyond image synthesis, image diffusion models have been\nsuccessfully deployed in applications like image restora-\ntion, inpatining, editing, super-resolution, or colorization\n[4, 7, 13, 24], among others. Image diffusion models are\ntypically obtained using very large training sets and thus in-\nherently represent the distribution of natural images. While\napplications typically make use of the generation process\nof diffusion models, e.g. by inserting images into the pro-\ncess [23, 24] or by altering the denoising function [4, 5, 13],\nrelatively little research has been conducted on how diffu-\nsion models can be uses as rich, general purpose image pri-\nors. Score Distillation Sampling (SDS) proposed in Dream-\n<latexit sha1_base64=\"C+D0zuwEmVosrv/9l3BhdprbtFI=\">ADynicbVLbhMxFHUzQMvwamHJZkSExCrKIAosq7Jh0UWpSFopE1Ue+07i1o+R7WkUrNnxAWzhI/gf/gbPQ5C6udJIZ+45/pe+YlZ8aOx392BtG9+w929x7Gjx4/efps/+D51KhKE5gQxZW+yLEBziRMLMcLkoNWOQczvPrTw1/fgPaMCW/2nUJc4EXkhWMYOtT04z5f7jcH45H4zaSuyDtwRD1cXp5MPidUqAdISjo2ZpePSzh3WlhEOdZxVBkpMrn3xmYcSCzBz17ZbJ69hiaF0v6TNmzmw6HhTFrkXulwHZpQq5JbuNmlS0+zh2TZWVBku6gouKJVUkze0KZBmL52gNMNPO9JmSJNSbW31Acx5kGCSuihMCSuwGSD1L5y7LFadNQ4q7YVr76TZVvo1O1fSTF1sUBjYUBG8rclb3/tyde42yZUx9X/3SWhu+BWwxdJ6FfdvT3FYon3mrob7FvqJkrRux3XrkKOsKISiwLsDTK/LyiULpf7yKOhCBkq7BItDLZT/eA8ZVzJUNAVskFtUjGJ/CrcMCcYW/IA0Op1VXtyjr2G56G+3wXTN+O0vejwy/vhkfH/a7voZfoFXqDUvQBHaHP6BRNEFX6Af6iX5FJ5GO1pHrpIOd3vMC3Yro+19Vk1G</latexit>z\n<latexit sha1_base64=\"AoNJjH6XCgcyNxhMlqxejI35bKA=\">AD8HicbVJLj9MwEPY2PJbw6sKRS0SFhDhUDeJ1XMGFA4dlRXdXaqrKj0lr1rEj29lSrPwHjtwQVw4cucLf4N/gNAG63o4UaTf901mxh8pBTd2NPq904suXb5ydfdafP3GzVu3+3t3joyqNIUxVULpE4INC5hbLkVcFJqwAURcExOXzX48Rlow5V8Z1clTAs8lznFtfmvUfZRITgWcu0yAZ6Fwugc8X1tSZUMbMgsfrKNKsnrWH4yGo3UkF5O0Swaoi4PZXu97xhStCpCWCmzMJB2VduqwtpwKqOsMlBieornMPGpxAWYqVsvVScPfIUludL+kzZVzcVDhfGrArimQW2CxNiTXEbNqls/mLquCwrC5K2P8orkViVNBdKGNdArVj5BFPN/awJXWCNqfV3jO4uRQsqSoKLJnLzoDWk3TqMqIEawZSwg3S2m+3yfJjtKxmHpJvYRjYFC8rclh3emJO/TYebB5rvq/+k0obvD2bT1LeIcwHLbg3hzQ9nAfQ/3aA+t13SrEGM/zQjEQf83T8rJywUNqaLOaRdgciF8h/uUy6UDBlNAxvU5hVnWFK/yDJckHDsBSQlFq9r1Zx97haejni8nR42H6bPj07ZPB/svO67voHrqPHqIUPUf76DU6QGNE0Sf0A/1EvyIdfY6+RF9bam+n09xF5yL69gdI/F4V</latexit>r\u03b8Lcond\n<latexit sha1_base64=\"ad59f8eoBysU70xCE0gGT69EhZc=\">AD8HicbVJLj9MwEPY2PJbw6sKRS0SFhDhUDeJ1XMGFA4dlRXdXaqrKj0lr1rEj29lSrPwHjtwQVw4cucLf4N/gNAG63o4UaTf901mxh8pBTd2NPq904suXb5ydfdafP3GzVu3+3t3joyqNIUxVULpE4INC5hbLkVcFJqwAURcExOXzX48Rlow5V8Z1clTAs8lznFtfmvUfZRITgWcu0yAZ6Fwugc8X1tSZUMbMgsfrCu1el/P+oPRcLSO5GKSdskAdXEw2+t9z5iVQHSUoGNmaSj0k4d1pZTAXWcVQZKTE/xHCY+lbgAM3Xrperkga+wJFfaf9Im6+qmwuHCmFVBPLPAdmFCrCluwyaVzV9MHZdlZUHS9kd5JRKrkuZCeMaqBUrn2CquZ81oQusMbX+jnEcN5eCJVFgSVz2RnQepJOXUaUYM1ASrhBWvtNl+jJbVzEPyLQwDGwyKtzU5rDs9cYceOw82z1X/V78JxQ3evq1nCe8QhsMW3JsD2h7uY6inSrJ6va5bhRjeV4oBuKveVpeVi54SA1t1jHtAiwOuVD+w3KhZIho2lg9q84gxL6hdZhgsSjr2ABIK1u1Zx97haejni8nR42H6bPj07ZPB/svO67voHrqPHqIUPUf76DU6QGNE0Sf0A/1EvyIdfY6+RF9bam+n09xF5yL69geW+F4s</latexit>r\u03b8Lproj\n<latexit sha1_base64=\"h3Xm0Pu1juB6LxosZXqL51HW1eg=\">AD3HicbVJNb9MwGPYaPkb46kDiwiWiQhoSqhoEG8cJLhw4jIluk5pSOfabxsyxI9tZVUxu3BXfgBXEP+Hf4PTRNB5tRTp0fs8z/uVNy0502Y0+rPVC65cvXZ9+0Z489btO3f7O/eOtawUgTGRXKrTFGvgTMDYMPhtFSAi5TDSXr2uFPzkFpJsV7syxhWuC5YBkj2LjQrP8gybFJUob1bsIcB0+jxOgns/5gNBytXnQZxB0YoO4dznZ6vxMqSVWAMIRjrSfxqDRTi5VhEMdJpWGEpMzV2LioMAF6KldDVBHj12ERplU7hMmWkXHRYXWi+L1CkLbHLtc01wEzepTPZyapkoKwOCtIWyikdGRs02IsoUEMOXDmCimOs1IjlWmBi3szAMEwUCFkQWBRbUJudA6k8tUkqOW0aktwO4tpNt65ybSqp8026DQsKYgeFOSo7rzp/bIcRdJLrWu/7vf+uaGXwCb58apuLsGiv0Uq5/d5rCfD+Rgtarce3S5yjLskJS4G0B3emSMme+1C2PgsqEpzQ5GOxrofzHO8i4FL6iSWC82LxiFAviBln4AzY3XdvUM5RKfqxtWYfuwmP/ni+D42fDeG/4t3zwcGr7ta30UP0CO2iGO2jA/QGHaIxIugz+oF+ol/Bh+BL8DX41kp7W53nPrwgu9/ATQ1U4s=</latexit>\n\u02c6b(z, t)\n<latexit sha1_base64=\"xYLlG4dyhDguGqr0thduRnrEnoY=\">AD7XicbVJLj9MwEPY2PJbw6sKRS0SFWCRUJYjXcQUXDhyWFd1dqYkqx5603nXsyHa2KlZ+AkduiCs3Llzh/BvcJoKum5HsjT6HuMZe/KM23i+M9OL7hy9dr13RvhzVu379zt79071rJWBEZEcqlOc6yBMwEjwyH0oBLnMOJ/n525Y/uQClmRQfzaKCrMRTwQpGsHQpP84ZQ6BiU0pK4pSUuBzYNOZ0c1+Rz2NUqOfTPqDeBgvI9pMklUyQKs4nOz1fqRUkroEYQjHWo+TuDKZxcowqEJ01pDhcm5u2LsUoFL0JldTtREjxCo0Iqd4SJlui6w+JS60WZO2WJzUz7XAtu48a1KV5nlomqNiBId1FR8jIqH2eiDIFxPCFSzBRzPUakRlWmBj3iGEYpgoEzIksSyoTS+ANOMks2kuOW0bktwOksZNt65ybXSqtp+82KLQsKYgeFuRo2blz+2R4y6TXGrd/He/980t3/2rU3G3HhT7JZaf3dWwn3w/kYI2y3Htwuc2FqfTpdWM+VL3eBRUITylmYHBvhaqf7xLGZfCV7QFjIdNa0axIG6QuT9gzrAz5J6hUvKsVUTug1P/H3eTI6fDZOXwxcfng8O3qx2fRc9QA/RPkrQK3SA3qFDNEIEfUY/0S/0O5DBl+Br8K2T9nZWnvoUgTf/wK9WFtY</latexit>\nz\u03c6(z, t)\n<latexit sha1_base64=\"HgphGLnhO9zKyGNGdCUXCb0eKaA=\">AD73icbVJNj9MwEPU2fCzhqwtHLhEVElyqBi0fxVcOHBYVnR3paqHvSmnXsYDtbiuXfwJEb4sqFG1f4HfwbnCZA17sjRrNe28yM35xZk2o9HvrV506fKVq9vX4us3bt63d+5c6hlrQiMieRSHedYA2cCxoYZDseVAlzmHI7yk5cNfnQKSjMp3pVBdMSzwUrGMHGl2b9R5nAOczmykQFQhlsDmC6NdxqXWs8zAB2N5SdysPxgNR+tIzidplwxQF/uznd73jEpSlyAM4VjrSTqzNRiZRjh4OKs1lBhcoLnMPGpwCXoqV3v5JIHvkKTQir/CZOsq5sKi0utV2XumSU2Cx1iTfEibFKb4vnUMlHVBgRpf1TUPDEyaQ6UKaAGL7yCSaK+VkTsAKE+PGMdxcyhYElmWFCbnQJxk3Rqs1xy2gwkuR2kzm+3yfJjtKxmnry4gKFhg0HwRU0OXKfP7YHzoLNa7n/6tehuMHbp/Us7g1CcdiCeW9A28N+DPVECurW69pViFWFKWkwP96p+Vl1YKF1NBlHdMswOCQC9U/3KeMSxEymgYmqM1rRrEgfpFluGDOsBfkgaBS8p2zlYu9w9PQz+eTw8fD9OnwyZvdwd6Lzuvb6B6jx6iFD1De+gV2kdjRNAn9AP9RL+i9Hn6Ev0taX2tjrNXQmom9/ALE6XaM=</latexit>r\u03b8Llmc\nFigure 1. Visualization of SDS and LMC-SDS gradients w.r.t.\npixel values and estimated denoised images for the given image\nz, the prompt y = \u201cautumn\u201d, and t = 0.5. We visualize the neg-\native gradient, i.e. the direction of change. See text for details.\nFusion [28] is one of the few exceptions. The SDS loss\nformulation uses a pretrained text-to-image model [33] to\nmeasure how well a given text matches an observation. In\nDreamFusion this loss is being used to generate 3D assets\nfrom textual descriptions, an idea that was quickly adopted\n[16, 19, 22, 25, 31, 36] and essentially established a new re-\nsearch direction: text-to-3D synthesis. While being mostly\nused in this context, the design of the SDS loss is by no\nmeans only tailored to text-to-3D applications. In fact, SDS\nis an image loss and can be used in much wider contexts\n[8, 12]. However, in its original formulation the SDS loss\nmay degrade the observation, be too eager to match the\ntext prompt, or provide meaningless gradients which inject\nnoise into the objective. In this paper, we conduct an exten-\nsive analysis of the SDS loss, identify an inherent problem\nwith its formulation, propose a surprisingly easy but effec-\ntive fix, and demonstrate its effectiveness in several appli-\ncations, including optimization-based image synthesis and\nediting, zero-shot training of image translation networks,\nand text-to-3D synthesis. Concretely, our new loss formu-\nlation \u2013 Score Distillation Sampling with Learned Manifold\n1\narXiv:2401.05293v1  [cs.CV]  10 Jan 2024\nCorrective, or LMC-SDS for short \u2013 aims to provide bet-\nter gradients along the direction of the learned manifold of\nreal images. We present evidence that gradients towards the\nlearned manifold are extremely noisy in SDS and that high\ntext guidance is needed to compensate for the noisy signal.\nFurther, we show that high guidance \u2013 or lack thereof \u2013 is a\npossible explanation for the artifacts observed with SDS, cf.\nSec. 3. In contrast, applications relying on our novel formu-\nlation benefit from meaningful manifold gradients, may use\nlower text guidance, and produce results of overall higher\nvisual fidelity.\nIn summary, our contributions are: (1) conducting an in-\ndepth analysis of the SDS loss, (2) identifying an inherent\nproblem with the formulation, and (3) proposing a novel\nLMC-SDS loss, providing cleaner gradients and eliminat-\ning the need for high guidance. (4) In extensive experi-\nments, we demonstrate the effectiveness and versatility of\nthe new LMC-SDS loss.\n2. Related Work\nMethods that use image diffusion models as priors for tasks\nother than image synthesis can be broadly categorized into\ntwo lines of work.\nThe first class of methods leverage image diffusion net-\nworks to solve specific tasks by relying on the diffusion\nsampling process. We discuss here a number of represen-\ntative methods and refer the reader to [6, 18] for detailed\ndiscussions. DDRM [13] tackles linear inverse problems\nsuch as deblurring, super-resolution, colorization, or in-\npainting by defining a modified variational posterior dis-\ntribution conditioned on the observation signal, which can\nthen be used for sampling. DPS [4] focuses on general in-\nverse problems and proposes to augment the score predic-\ntion at each denoising step with an additional term that mod-\nels the likelihood of the observations. MCG [5] builds on\ntop of [4] and improves the fidelity of generation by intro-\nducing additional manifold constraints. SDEdit [24] uses\ndiffusion models for image editing by first adding noise\nto the input image and then denoising it using the text-to-\nimage diffusion prior. RePaint [23] uses unconditional im-\nage diffusion models as a prior for solving image inpaint-\ning. All these methods use pre-trained diffusion models.\nIn contrast, methods like Palette [32] or Imagic [14] re-\nquire training a diffusion model from scratch or fine-tuning\na pre-trained one, respectively. This line of work exclu-\nsively deals with image editing problems and is typically\nrestricted to the resolution of the diffusion models.\nMore related to our method is the second category: In\nan orthogonal strategy this body of work uses pre-trained\ndiffusion models within general purpose loss functions in\niterative optimization settings.\nScore Jacobian Chaining\n[36] propagates the score of a text-to-image diffusion model\nthrough a differentiable renderer to supervise the generation\nof 3D models. DreamFusion [28] takes an orthogonal ap-\nproach and proposes the Score Distillation Sampling loss.\nSDS is a modification of the diffusion training objective that\ndoes not require expensive backpropagation through the dif-\nfusion model and encourages the alignment of the generated\nimage with a conditioning signal. DDS [8] proposes an\nimproved version of SDS, specifically designed for image\nediting and aims to reduce the artifacts introduced by noisy\nSDS gradients. In contrast to DDS, our method does not re-\nquire a source text prompt. SparseFusion [39] introduces a\nmulti-step SDS loss that performs multiple denoising steps\ninstead of one, which adds significant computational over-\nhead.\nProlificDreamer [37] proposes a generalization of\nSDS from point estimates to distributions, but unlike our\napproach requires computationally expensive fine-tuning of\na diffusion model during optimization. Collaborative Score\nDistillation [15] generalizes SDS to multiple samples and\nis inspired by Stein variational gradient descent [21]. Cat-\negory Score Distillation Sampling (C-SDS) focuses on 3D\ngeneration and replaces the noise estimation error in SDS\nwith a difference between the noise predictions of a stan-\ndard and a multi-view consistent diffusion model [40]. Un-\nlike our method, C-SDS is only applicable to 3D generation\ntasks. In contrast, we propose a general purpose loss func-\ntion with very little computational overhead.\n3. Analysis\nAn image diffusion model is a generative model that is\ntrained to reverse the diffusion process that transforms a\nnatural image into pure noise [11]. An image is formed\nby iteratively removing small amounts of Gaussian noise\nover a fixed variance schedule \u00af\u03b1t until an image is created.\nText-to-image models are additionally conditioned on text\nto steer the denoising process in a direction matching the\ntextural description. For a given image z, its textual de-\nscription y, a randomly sampled timestep t \u223c U(0, 1), and\nrandom Gaussian noise \u03f5 \u223c N(0, I), a denoising model \u03f5\u03d5\nparameterized by \u03d5 can be trained by minimizing the diffu-\nsion loss\nLdiff = w(t)\n\r\r\u03f5\u03c9\n\u03d5(zt, y, t) \u2212 \u03f5\n\r\r2\n2 ,\n(1)\nwhere zt = \u221a\u00af\u03b1tz + \u221a1 \u2212 \u00af\u03b1t\u03f5 refers to the noisy version\nof z and w(t) is a weighting function, omitted in the se-\nquel. DreamFusion [28] showed that given a pre-trained\nmodel \u03f5\u03d5 the diffusion loss can be utilized for optimization\nproblems. For an arbitrary differentiable rendering function\nreturning z and parameterized by \u03b8, the gradient of the de-\nnoising function w.r.t. \u03b8 is given by:\n\u2207\u03b8Ldiff =\n\u0000\u03f5\u03c9\n\u03d5(zt, y, t) \u2212 \u03f5\n\u0001 \u2202\u03f5\u03c9\n\u03d5(z, y, t)\n\u2202zt\n\u2202zt\n\u2202\u03b8\n(2)\nIn practice, the Jacobian term is omitted to avoid backprop-\nagating through the denoising model and the gradient is ap-\n2\nproximated as\n\u2207\u03b8LSDS =\n\u0000\u03f5\u03c9\n\u03d5(zt, y, t) \u2212 \u03f5\n\u0001 \u2202zt\n\u2202\u03b8 ,\n(3)\nresulting in the gradient of original SDS loss. We will now\nrewrite Eq. (3) and conduct an analysis of the derived com-\nponents.\nUsing classifier-free guidance [10], the predicted noise is\nthe sum of the y-conditioned and the unconditioned noise\nprediction weighted by the guidance weight \u03c9\n\u03f5\u03c9\n\u03d5(z, y, t) = \u03c9\u03f5\u03d5(z, y, t) + (1 \u2212 \u03c9)\u03f5\u03d5(z, t),\n(4)\nwhich we can rewrite as:\n\u03f5\u03c9\n\u03d5(z, y, t) = \u03c9 (\u03f5\u03d5(z, y, t) \u2212 \u03f5\u03d5(z, t)) + \u03f5\u03d5(z, t).\n(5)\nBy inserting Eq. (5) into Eq. (3) we can express \u2207\u03b8LSDS as:\n\u2207\u03b8LSDS = \u2207\u03b8Lcond + \u2207\u03b8Lproj\n(6a)\n\u2207\u03b8Lcond = \u03c9 (\u03f5\u03d5(z, y, t) \u2212 \u03f5\u03d5(z, t)) \u2202zt\n\u2202\u03b8\n(6b)\n\u2207\u03b8Lproj = (\u03f5\u03d5(z, t) \u2212 \u03f5) \u2202zt\n\u2202\u03b8 .\n(6c)\nThe two loss components can be interpreted as follows:\nLcond maximizes the agreement of the image with the\ntext prompt by providing gradients towards images formed\nthrough conditioning on y. Lproj performs a single denois-\ning step and provides gradients informing how the image z\n(and thereby parameters \u03b8) should change such that the im-\nage can be better denoised. As the denoising function was\ntrained to minimize the expected squared error between the\ndenoised and the original data, \u2207\u03b8Lproj can be understood\nas the gradient direction towards the manifold of natural im-\nages.\nDreamFusion [28] originally proposed to set \u03c9\n=\n100. This means that, following our formulation, Lcond is\nweighted proportionally much higher than Lproj. This can\nbe problematic as large guidance weights have been iden-\ntified to be an important cause of over-exposure in diffu-\nsion models [20]. In fact, DreamFusion produces detailed\nresults but also has a tendency towards unrealistically satu-\nrated colors, see also Fig. 9. On the other hand, setting \u03c9\nto lower values has been reported to result in very blurry\nimages [8], a behavior that we also demonstrate in Sec. 5.\nBoth behaviors, (1) the tendency to produce overly satu-\nrated results when using high guidance as well as (2) blurry\nresults for low guidance, can be explained by looking at the\ncomponents of LSDS in isolation. To this end, we consider\nthe case where the rendering function is a no-op and sim-\nply returns pixel values, i.e. z = \u03b8. In Fig. 1, we visualize\n\u2207\u03b8Lcond and \u2207\u03b8Lproj for this case. \u2207\u03b8Lcond returns the di-\nrection towards an image with altered colors. However, as\n\u2207\u03b8Lcond is not anchored in the manifold of realistic images,\nthe gradient may eventually point away from that manifold.\n\u2207\u03b8Lcond will always point towards changes that \u201cbetter\u201d re-\nspect the prompt and eventually produce saturated colors or\nother artifacts (cf. Sec. 5). \u2207\u03b8Lproj, on the other hand, in-\ncorrectly marks high-frequent detail originating from z for\nremoval \u2013 a reason for blurriness.\nWe conclude the following: (1) The reason for oversatu-\nrated colors and other artifacts is due to Lcond being \u201cover-\neager\u201d to maximize the agreement of the image with the\nconditioning variable. (2) Lproj provides deficient gradients\nultimately removing high-frequent detail instead of \u201ccor-\nrecting\u201d the gradient towards on-manifold solutions. Lproj\nappears to be a non-optimal choice and we provide an alter-\nnative in the sequel. But first we use our decomposition to\ndiscuss other approaches to \u201cfix\u201d the SDS loss.\nDDS [8] proposes to use the difference between two SDS\nlosses for image editing, one computed on the current state\nw.r.t. the target prompt and one computed on the initial\nimage w.r.t. a textual description of that image. By com-\nputing an SDS difference, DDS entirely removes Lproj and\nintroduces a negative gradient pointing away from the ini-\ntial state. While the root for blurry results is removed by\neliminating Lproj, the optimization is no longer anchored\nalong the learned manifold. Additionally, an initial image is\nneeded, limiting possible use-cases. In a different approach,\nother methods aim for a better noise prediction, where by\nimproving \u03f5\u03d5(z, t), one may also improve the underlying\nLproj, as the difference between the predicted and true noise.\nE.g. SparseFusion [39] introduces multi-step SDS. Hereby,\ninstead of denoising in one step, up to 50 steps are used.\nWhile this may produce more informative gradients in Lproj,\nthis is also significantly more expensive. Through differ-\nent means but towards the goal of better noise prediction,\nconcurrent work [37] learns a low-rank adaptation of the\ntext-to-image model. While this approach is effective, it\nalso adds the overhead of fine-tuning an additional diffusion\nmodel. In a third vein, some recent methods aim to allevi-\nate artifacts introduced by SDS by advanced schedules for\nsampling t or by introducing negative prompts [1, 34]. In\ncontrast, we aim for a generally applicable solution.\n4. Method\nOur proposal is to compare \u03f5\u03d5(z, t) and \u03f5 in Lproj, the pre-\ndicted denoised image \u02c6zt = z\u03d5(z, t), with the rendered im-\nage z. To this end, we rewrite Lproj as\n\u2207\u03b8Lproj =\n\u0012\n\u221a\u03b1t\n\u221a1 \u2212 \u03b1t\n(z \u2212 \u02c6zt)\n\u0013 \u2202zt\n\u2202\u03b8 ,\n(7)\nwhich is equivalent to comparing the predicted and true\nnoise. By looking at Eq. (7), we can now see a possible\nexplanation why Lproj might not be optimal: \u02c6zt is an ap-\nproximation of z and most probably not a perfect replica\n3\nof z, even when z already lies on the natural image mani-\nfold, see Fig. 1. This is especially the case for large values\nof t, where the denoising model has to reconstruct the true\nimage from almost pure noise. In an attempt to better un-\nderstand the behavior of the denoising model given z and t,\nwe model it as a two step process, as follows\nz\u03d5(z, t) = b \u25e6 p(z, t) = b(\u02dcz, t),\n(8)\nwhere p projects z onto the learned natural image mani-\nfold, \u02dcz is the projected image, and \u02dcz = z for images on the\nmanifold. b is the error or bias of the denoising model in-\ntroduced by the current time step t. In this two-step model\nwe are only interested in p and would like to neglect any\ngradients coming from b. As it is unfeasible to correct for b,\nwe propose to approximate b with \u02c6b and apply it on z before\ncomparing it with \u02c6zt\n\u2207\u03b8Llmc =\n\u0010\n\u02c6b(z, t) \u2212 \u02c6zt\n\u0011 \u2202zt\n\u2202\u03b8 .\n(9)\nIn this formulation we omit both the Jacobian term from\nthe denoising model (like in the original formulation, cf.\nEq. (3)) as well as the Jacobian from \u02c6b. Also, we empiri-\ncally found that by dropping the weighting term\n\u221a\u03b1t\n\u221a1\u2212\u03b1t we\ncan further improve visual fidelity \u2013 which is similar to the\ncommon practice of predicting the original natural image\ninstead of the noise when learning the denoising model. As\nevident in Fig. 1, given an adequate model \u02c6b, \u2207\u03b8Llmc fo-\ncuses on global changes, whereas \u2207\u03b8Lproj is dominated by\nhigh-frequent detail. We now explain how to obtain \u02c6b.\nWe propose to model \u02c6b as the learnable function \u02c6b\u03c8(z, t)\nparameterized by \u03c8. We can observe b as a function of t\nby sampling triplets (z, \u02c6zt, t), where z are random natural\nimages assumed to be approximately on manifold images,\ni.e. z \u2248 \u02dcz. Using these triplets we can learn \u02c6b\u03c8 by min-\nimizing \u2225\u02c6zt \u2212 \u02c6bz,t\u22252\n2, with \u02c6bz,t = \u02c6b\u03c8(z, t). Using a non-\ndeterministic model, \u02c6b\u03c8 is expected to under-fit the highly\ncomplex and probabilistic nature of b. In fact, for a given\npair (z, t) at best it can learn the mean over all possible \u02c6zt.\nThis is desired, because even after approximating b(z, t)\nthrough \u02c6b\u03c8(z, t), we will be able to observe a gradient in\nLlmc: a gradient towards a specific instance of \u02c6zt. Further,\n\u02c6b\u03c8 learns to \u201cblur\u201d images and is not informed by the image\nmanifold associated to the diffusion model. It will perform\nsimilar operations for on-manifold and off-manifold images\nand thus Llmc may correct non-manifold images. However,\nnaively applying a \u02c6b\u03c8 in Eq. (9), learned that way, is caus-\ning an undesired effect, namely a drift in image statistics.\nConcretely we observed that images produced by \u02c6b\u03c8 tend to\nunderestimate the true image dynamics, especially for large\nt where \u02c6zt may be vastly different to z. By underestimat-\ning image dynamics, however, \u2207\u03b8Llmc contains a gradient\nto \u201ccorrect\u201d for that and we have found images obtained via\noptimization using this gradient to be overly saturated and\ncontrasting. To this end, we make \u02c6b\u03c8 agnostic to the global\nimage statistics by computing the training loss in a normal-\nized space\nLk =\n\r\r\r\r\r\u02c6zt \u2212\n \n\u03c3(\u02c6zt)\n\u03c3(\u02c6bz,t)\n\u0010\n\u02c6bz,t \u2212 \u00b5(\u02c6bz,t)\n\u0011\n+ \u00b5(\u02c6bz,t)\n!\r\r\r\r\r\n2\n2\n,\n(10)\nwith standard deviation \u03c3 and mean \u00b5. Predicting in a nor-\nmalized space naturally requires to perform the same (nor-\nmalisation) operation in Eq. (9), which we assume to be\nabsorbed into \u02c6b as follows.\nIncreasing Sample Diversity.\nThe original SDS formu-\nlation has been reported to have mode-seeking behaviour.\nConsequently, optimization results obtained via LSDS have\nthe tendency to look very similar, even when based on dif-\nferent random seeds. We hypothesize this is partly due to\noptimizing over many steps and hence averaging the effects\nof varying noise \u03f5. To prevent such averaging behaviour,\nwe propose to fix \u03f5 over the course of optimization \u2013 which\nis in spirit similar to DDIM sampling [35]. Empirically, we\nfound that optimizing with fixed \u03f5 is often unstable in the\noriginal SDS formulation. We hypothesize this is rooted in\nLproj not providing proper gradients towards the image man-\nifold and at the same time Lcond providing too strong gra-\ndients towards prompt agreement. The optimization state\nmay leave the manifold of natural images and drift towards\na state associated more to an \u201cadversarial example\u201d for the\ngiven prompt. While our loss formulation allows the use of\na fixed \u03f5, probably due to both the usage of smaller \u03c9 and\nbetter manifold correctives, we observe that we can further\nrobustify the formulation by only fixing \u03f5 in Lcond, while\ncontinue to sample \u03f5 in Llmc. This results in high quality,\nyet diverse, results, cf. Sec. 5.\nImplementation Details. We use an Imagen version [33]\ntrained on internal data sources as the pre-trained text-to-\nimage diffusion model \u03f5\u03c9\n\u03d5 or z\u03c9\n\u03d5, respecively. We use only\nthe base model and not the super-resolution cascade, pro-\nducing images of 128 \u00d7 128 pixel resolution. We model\n\u02c6b\u03c8 as a standard U-Net with four encoder layers with two\nConv/ReLU/MaxPool blocks per layer, and skip connec-\ntions to the decoder. The first layer contains 32 filters and\nthe number of filters is doubled in each layer in the encoder\nand halved in the decoder. \u02c6b\u03c8 has been trained until con-\nvergence, using Lk as the only loss. For creating training\ntriplets (z, \u02c6zt, t), we sampled random images z from Open-\nImages [17], random time-steps t \u223c U(0.02, 0.98), and\npassed both into z\u03d5 obtaining \u02c6zt.\n5. Experiments\nWe evaluate our proposed LMC-SDS loss in a number of\nqualitative and quantitative experiments, including image\nsynthesis, image editing, image-to-image translation net-\nwork training, and 3D asset generation.\n4\n<latexit sha1_base64=\"IA/CDK/hUgGkY4roEGM4+3B7u0=\">AEpXicdZPdbtMwFMe9LcAoXx1chNREFxVzcTHbpAmuOECpDGt3aSmqhznpDF1bMt2thUrL8DTcAtvwtvgNCm0SWcpytH5/4nxyd2JBnVZjD4s7O75926fWf/bufe/QcPH3UPHo+0yBWBIRFMqIsIa2CUw9BQw+BCKsBZxOA8mn8s9fNLUJoKfmYWEiYZnGaUIKNS027z0MmtJ6GBq6NJYLHRQhcz6n0w1lOY8wJvD+adnuD/mC5/HYQ1EP1etkerBHw1iQPANuCMNaj4OBNBOLlaGEQdEJcw0SkzmewdiFHGegJ3a5ncJ/4TKxnwjlHm78ZXbdYXGm9SKLHJlhk+qmVia3aePcJEcTS7nMDXBSfSjJmW+EX87Gj6kCYtjCBZgo6nr1SYoVJsZNsBMq4HBFRJZhHtvwEkgxDiY2jASLy3YEs72gcHtbp1wTFV2EyVbCA1rBMHbipwWtT+yp07bFMv/V/x3f26aS/0K6Cw1jmLuZMS4WYK6QwFVDfu96Y+BC+p+yApKHVQ7GuTy+CwHYxetKjRJMhEDq1rRNRfKlDZRN+YVMIbpEnB4CY7B8XbNXWrJsh/qgspE/zGBouKvkFdTaF6b0JlB6aRW10iZxEZzFqjyh2rqjhwlIqcV1p5cArbJNxLfCysJdzqB5FdvB6LAfvO2/+fq6d/yhvqb76Cl6hl6hAL1Dx+gTOkFDRNAP9BP9Qr+9l94X78wbVejuTu15gjaWN/0LI9KmLg=</latexit>Lcond \u03c9 = 8\n<latexit sha1_base64=\"dUZdpk7DhmPQyhU168Mikjo164=\">AEnicdZNdb9MwFIa9LcAoXx1cIqSIComrqkF87AZpAoS4ADE2ulVqslxThpTx7ZsZ1uxcsWv4RZ+Df8Gp0mhTpLUY7O+7wnxyd2JBnVZjD4s7W94127fmP3ZufW7Tt373X37p9okSsCQyKYUKMIa2CUw9BQw2AkFeAsYnAazd6W+uk5KE0F/2rmEiYZnKaUIKNS51H4UGLo09fncAtczKgs/nOY0xpzA6/2zbm/QHyW3w6COuiheh2e7e3QMBYkz4AbwrDW42AgzcRiZShUHTCXIPEZIanMHYhxnoiV3so/CfuEzsJ0K5hxt/kV1WJxpPc8iR2bYpLqplclN2jg3yf7EUi5zA5xUH0py5hvhl0PxY6qAGDZ3ASaKul59kmKFiXGj64QKOFwQkWYxzY8B1KMg4kNI8Hish3BbC8o3N5WKdERZXdRMkGQsMKQfCmIkdF7Y/skdPWRSa0Lv67PzbNpX4BdJoaRzF3JGLcLEHdaYCqhv3e9MfABXU/ZAmlDqodDZIHheLwdh5qwpNkzEwKpWdM2FMqVN1I05BpXwBmlSMLjJzkDxdk3dqgnyn+pCygS/sGioq9Ql1Oo3utQ2YFp5JaXyFlEBtPW6COKnStquLCUSlxWjnwCltnHPGtsLJwlzNoXsV2cPKsH7zsv/jyvHfwpr6mu+gheoyeogC9QgfoAzpEQ0TQD/QT/UK/Pd973yPlfo9lbteYDWljf6C3Y8orI=</latexit>\nSDS \u03c9 = 8\n<latexit sha1_base64=\"80V1Hel8DAOfqk1ylmwQsgchAQ=\">AEn3icdZNdb9MwFIa9LcAoXx1cIqFAhcTV1Ex87AZpgu4QFAmug01VeU4J42pY1u2s61YuePXcAt/hn+D06TQJp2lKEfnfd6T4xM7koxq0+/2dre8a5dv7F7s3Pr9p2797p790+0yBWBIRFMqLMIa2CUw9BQw+BMKsBZxOA0mr0t9dNzUJoK/sXMJYwzPOU0oQbl5p0H4UGLo39lCsdAtczKgs/nOY0xpzA68NJt9f7y+W3w6COuiheg0mezs0jAXJM+CGMKz1KOhLM7ZYGUoYFJ0w1yAxmeEpjFzIcQZ6bBcbKfynLhP7iVDu4cZfZFcdFmdaz7PIkRk2qW5qZXKTNspNcji2lMvcACfVh5Kc+Ub45VT8mCoghs1dgImirlefpFhYtzsOqECDhdEZBnmsQ3PgRSjYGzDSLC4bEcw2wsKt7dVyjVRUWU3UbKB0LBCELypyHFR+yN7LR1kQmti/uD01zqV8AnabGUcydiRg3S1B3HKCqYb83/TFwQd0PWUKpg2pHgySCx8ViMHbeqkKTJBMxsKoVXOhTGkTdWOQSW8QZoUDG6yM1C8XVO3aoL8p7qQMsGvbLCo6CvU5RSq9zpUdmAaueUlchaRwbQ1+ohi54oaLiylEpeVg68wtYZR3wrCzc5QyaV7EdnBzsBy/3X3x+3jt6U1/TXfQPUHPUIBeoSP0Hg3QEBH0A/1Ev9Bv7H3zvoDSp0e6v2PEBry/v6F8IHo3s=</latexit>\nOurs \u03c9 = 8\n<latexit sha1_base64=\"yqVrls1MkB7gcKdtWc2aUMBrzM=\">AEoHicdZNdb9MwFIa9LcAoXx1ckG0ComrKkHj4wZpAi52gcQ+6DbRVJPjnDSmjm3ZzrZi5ZJfwy38GP4NTpNCm3aWohyd93lPjk/sWDKqTRD82djc8m7dvrN9t3Pv/oOHj7o7j0+1KBSBARFMqPMYa2CUw8BQw+BcKsB5zOAsnyo9LNLUJoK/sVMJYxyPOY0pQbl7roPosMXBt78vEkAq4nVJZ+NC5ogjmBd2EQXHR7QT+YLX81CJugh5p1eLGzRaNEkCIHbgjDWg/DQJqRxcpQwqDsRIUGickEj2HoQo5z0CM720npP3eZxE+Fcg83/iy76LA413qax47Mscl0W6uS67RhYdK3I0u5LAxwUn8oLZhvhF+NxU+oAmLY1AWYKOp69UmGFSbGDa8TKeBwRUSeY57Y6BJIOQxHNoFS6p2BLO9sHR7W6RcEzVdROnawgNCwTB64ocl40/tsdOWxaZ0Lr87/7UNlf6FdBxZhzF3KFIcLsEdecB6hr2e9ufABfU/ZA5lDmocbRInhSzgZjpytVaJrmIgFWt6IbLpIZbaNuzAmolLdIk4HBbXYCiq/W1Cs1Qf5TXUiZ4Dc2WNb0Dep8CvV7Gao6MK3c/Bo5i8hvDL6mGLnilsuLKUS17VWDbzGlhlHfCutLN3lDNtXcTU4fdkPX/dfHe319t8313QbPUW76AUK0Ru0jw7QIRogn6gn+gX+u3tegfeZ+oRjc3Gs8TtLS8r38BShSjHw=</latexit>\nSDS \u03c9 = 100\n<latexit sha1_base64=\"LIq/zNrudjvQlNGarjel7MAEDF0=\">AE1nicdZNb9QwEIbdoFSvlo4crFYoXKqNpX4OFZw4cChVGy70mbVOs4kMevYke20LFG4Ia78H34JR67wJxhvUtjNtpaSjOZ93slkYkeFNYNBj/X1jd6N27e2ry9defuvfsPtnceHltdGg5DrqU2o4hZkELB0AknYVQYHk4SavH6yTkYK7T64GYFTHKWKpEIzhymTreHodJCxaBcyPEGRqg0hLzIqrOzcRhrZyeUcM41rRMKQXwDxDbalSyawF67PMUZcBjYDxbHe3Pt3uD/YG80VXg6AN+qRdh6c7GwJfxsce+C+7DgYFG5SMeMEl1BvhaWFgvEpS2GMoWI52Ek1/6aPsVMTBNt8FKOzrOLjorl1s7yCMmcucx2NZ+8ShuXLnk1qYQqSgeKNy9KSkmdpn6YNBYGuJMzDBg3AnulPGM4LRwkVjKg4ILrPGcqrsJz4PU4mFRhpGXs29Gy6gc1ftsihU0lO8mSq4gLCwQnF1V5Khu/VF1hNqyKLW19X/3u67Z6xcg0swhJXErxaxbQuAugqZG9bnrx82kBf6QSyhDqHV0SK5VXM8HU81WqogkyXUMsmnFtlxYZKL4phjMInqkLghHeuyUzBqtaZdqQnFPxVDIbW6tsG6oa9RL6fQPJch34Hr5NJSxExb9E5pCujwRDV9RxsaIw+lOj+YE32DKDxMe6KvzhDLpHcTU43t8LXuw9f7/fP3jdHtN8pg8Ic9IQF6SA/KWHJIh4eQH+UV+kz+9Ue9L72vW4Our7WeR2Rp9b7/BfK+uYo=</latexit>\n<[&] a raccoon\nwearing sunglasses\nat the beach=\n<latexit sha1_base64=\"+g9+SJ2VdREpujdf7FCh7aevc1w=\">AE23icdZPNbtQwEMfdoFSvlo4crFYoXJA1aYSH8cKLhw4lIp+iE3UOs4kMevYke20LFO3BX3ofX4AW4wiMw3mxhN7u1lM1o/r/eDJrx6U1g0GP1dW13rXrt9Yv7lx6/adu/c2t+4fWV0ZDodcS21OYmZBCgWHTjgJ6UBVsQSjuPRa68fn4OxQqv3blxCVLBMiVRw5jB1uvkhVFqoBJQLOf6AESoLoSjz+uxsGCba2YgyWjJjtKNhSC+AecTnhGEOaM7cUy/EsgIaMz7KjK5Usr3dnG72BzuDyaKLQTAN+mS69k+31gTuyKsCG+GSWTsMBqWLamac4BKajbCyUOIWLIMhoVYKN6MoSGPsZMQlNt8FGOTrKzjpoV1o6LGMmCudx2NZ9cpg0rl76MaqHKyoHi7UZpJanT1E+UJsIAd3KMAeNGYK+U58wjtPESgYUXHBdFEwldXgOvBkGUR3GWia+HS3rftDgt81S2ERL+W7idAlhYbgbFmRg2bqj+sD1OZFqa1t/rvfds1evwCR5Q4piecpYd0SAo8StDXqz10/nigt8A+5hPCYXDo6JNcqaSaDqcLVUSaFjoB2bZip1xY5qKL4pgTMKnqkC4Hx7rsCIxarGkXakL5T8VQSK2ubLBp6SvUym073nId+A6uawSCVPcW3QB2cLoY8HQFXdcrCyN/tRqfuAtNs8g8bGpS385g+5VXAyOdneC5zvP3u32915Nr+k6eUgekSckIC/IHnlD9skh4eQH+UV+kz+9qPel97X3rUVXV6aeB2Ru9b7/BeMku1U=</latexit>\n<[&] a parrot\nwearing a pirate hat,\nblue background=\n<latexit sha1_base64=\"/T7/h1VK5toe2GvjzH1WSguJ5c=\">AE4nicdZPNbhMxEMfdNkApXykcuVhEqBxQlVTi41jBhQOHUtEPKRu1Xns2a+K1LdvbkK72BbghrwPz8BDcIUrs9ktNJvW0mZHM7/ePYfO7ZK+tDv/1xZXevcuHlr/fbGnbv37j/obj489CZ3HA64UcYdx8yDkhoOgwKjq0DlsUKjuLJ26p+dAbOS6M/hpmFUcbGWiaSs4Cpky6LtJFagA4Rx9wUo8jyGxanJ4OI2GCH1FGeSozy/Q5AI0iOgVWYZiPzZQGCc+rGBuQnuApOaxoxPhDN2a6s86fb62/35osvBoAl6pFl7J5trEjfmeYbzcMW8Hw76NowK5oLkCsqNKPdgcQM2hiGmXgR8Xci5I+xYygiXH46EDn2cuKgmXez7IYyYyF1LdrVfKq2jAPyetRIbXNA2heb5TkigZDK2OpkA54UDMGHcSZ0XbmGMcTcVODjRMuckypkURnQEvh4NREcVGiWoco4reoMRvu0zhEDVTRMnVxAeLhGcXdVkv2z0cbGPtcWiMt6X/9Xv2+KqPgU5TgNSCo+VYO0WEk8U1D2K87YeD5aR+IdcQClCjaJFcqNFOTemC1kUmSGQGqHsU3XGRT2UbRZgEu0S0ypBYm52A08s9/VJPsP+qGEpl9LUDljV9TfXChfq9CFUThFZunEvBNK8kJoPxkvWxZKiKWypmrTOf61pleI0tMkh8KgtbXc5B+youB4c724OX2y8+7PR23zTXdJ08Jk/IMzIgr8gueUf2yAHh5Af5RX6TPx3R+dL52vlWo6srjeYRWVid738BYO+JQ=</latexit>\n<[&] a chimpanzee\nwearing a bow tie,\ndark curtain backdrop=\n<latexit sha1_base64=\"FaRx3HofG1tc4m3ExZTzd6Vwg=\">AEwnicdZNc9MwEIbV1kApXykcuXjIMOWUiTtT4Fg+Dhw4lA5pOxNnUle2pkyUhySzD+PfwarjD8G9axC4mdaibOzr7Pu1qvpSAT3Njh8M/G5pZz6/ad7bs79+4/ePiot/v4xKhcMxgxJZQ+C6gBwSWMLcCzjINA0EnAazd5V+egnacCU/23kGk5TGkecUYupae+NLxWXIUjrM3yA5jL2Ic2S4vx87IfKmolL3ZgHgZKu7uWzpDAFG4ZcdjbK6e9/nAwXCy3G3hN0CfNOprubnEszPIU92OCGjP2hpmdFRbzgSUO35uIKNsRmMYyhpCmZSLN61dJ9jJnQjpfEnrbvILjsKmhozTwMkU2oT09aq5DptnNvo9aTgMstSFZvFOXCtcqtBueGXAOzYo4BZpjry5LqKYMh4aVNEi4YipNqQwL/xJYOfYmhR8oEVbtKFH0vRLfbZnCJmq6iaI1hAGlghG1xU5Lht/UByjtioKZUz53/2xba70K+BxYpESeGxC2i7B8cRAXaP41vbjwVEcP8g1lCDUOFokUzIsF4Mp5p0qPIpSFYKoWzEN52cJb6M45hB0JFukTcDSNjsDLbs1TacmZP9UDLlQ8sYGy5q+Qb2eQv2/ClUd2FYuznlIJasKoW4M/qAU3QFLRfNMq2+1lo18BpbZC4KIusupxe+yp2g5P9gfdycPBpv3/4trm2+QpeUZeEI+8IofkAzkiI8LID/KT/CK/nfOhfPFMTW6udF4npCV5Xz/C/NBsU8=</latexit>\n<[&] a gibbon\ntaking a sel\u6600\u0600e=\n<latexit sha1_base64=\"uYNvopxmrJgCsD2tWdpSCgpJuU=\">AE0HicdZNLb9QwEMfdoFSXi0cuVisUDlVm0o8jhVcOCBRKvqQNqvWcSYbt35EtN2iSLEle/Dh0Fc4Xsw3qSwm20tJRnN/efTCZ2Ukjh/GDwc2l5pXfr9p3Vu2v37j94+Gh94/GBM6XlsM+NPYoYQ6k0LDvhZdwVFhgKpFwmJy9C/rhOVgnjP7sJwWMFBtrkQnOPKaO1z/G2gidgvYxtYocxqCKvTk4YVaZ0QC+AhTSNY8poJsm1ApxrkPmUSWQB0i6N3crI/X+4OtwXTRxSBqgz5p1+7xoqIU8NLhW/nkjk3jAaFH1XMesEl1GsxtlAwfsbGMRQMwVuVE2/vKbPMZPSzFi8tKfT7KyjYsq5iUqQVMznrquF5HXasPTZm1EldF60Lx5UVZK6g0NY6SpsMC9nGDAuBXYK+U5s4zjGLCSBQ0X3CjFdFrF58DrYTSq4sTINLRjZNWPavy2WQqbaKjQTZJdQziYITi7rshe3fqTag+1eVEa5+r/7g9dc9AvIPxZpCRuopR1SwjcP9DUqL50/biNjMAfcgXlCLWODsmNTuvpYKrJQhWRZcqkIJtWXMvFRS6KI45BZvpDulz8KzLnoHVizXdQk0o/qkYCmn0jQ3WDX2DejWF5jkPhQ58JzcuRco0DxajYLw+kQwdCUdFysKay4bLQy8weYZJE7rqgiHM+oexcXgYHsrerX18tN2f+dte0xXyVPyjLwgEXlNdsh7skv2CSc/yC/ym/zp7fUue1973xp0ean1PCFzq/f9LzUAtxs=</latexit><a mouse wearing\na \u6600\u0600u\u6600\u0600y light\nblue sweater=\nFigure 2. Optimization-based image synthesis results. We optimize an empty image to match the given prompt using our LMC-SDS, the\noriginal SDS formulation, and Lcond. SDS struggles to create detailed content when using low guidance \u03c9. High \u03c9 produces detailed\nresults but colors may be oversaturated (chimpanzee face), fake detail may appear (2nd mouse tail), or artifacts emerge. Lcond is unstable\nto optimize and produces unrealistic colors. In contrast, our method produces detailed results with balanced colors.\n<latexit sha1_base64=\"wiefCngTLx6El/pJdfX0yRVsmTU=\">AEuXicdZPbtQwEMbdoFS/m3hyCVihcqp2lQqVOJSwYUDh1KxbaXNqp04k41bx7Zsp2WJ8io8DVe48zZMNil0s62lJKP5ft9kMrFjI4Xzw+GfldW13r37D9Yfbjx6/OTps/7m8yOnC8txLXU9iQGh1IoHnhJZ4Yi5DHEo/ji4+1fnyJ1gmtvqZwUkOUyVSwcFT6rS/FyktVILKR5xuaIWaRpibrDw70IGBoSnVKDTAIJKnEcDG5tVaf9wXB7OF/BchC2wYC16+B0c01EieZFTq/hEpwbh0PjJyVYL7jEaiMqHBrgFzDFMYUKcnSTcv6JVfCaMkmQakuX8sE8e9NRQu7cLI+JzMFnrqvVydu0ceHTvUkplCk8Kt68KC1k4HVQzytIhEXu5YwC4JaGwQOegQVOs6JKFhVecZ3nNJoyukRejcNJGcVaJnU7WpaDsKJvu0lREw1VdxOntxAObxAcbityWLX+uDwkbVGU2rnqv/tz1zrVyimSdK0m5JoFtC0EbBpkb5veun/aIF/ZBrKCOodXRIrlVSzQdTzpaqiDTNdYKyacW1XGQy0UVpzAnaVHVIn6GHLnuBVi3XdEs10fxTKRSqzsbrBr6DvV6Cs1zEao78J3ctBAJKF5bdI7TpdHAsgVd1xgjNXfGq0eIMtMkScV6WpD2fYPYrLwdHOdvh2e/fLzmD/Q3tM19lL9oq9YSF7x/bZJ3bARoyzH+wn+8V+973oJf1zht0daX1vGALq+f+Avk7rhc=</latexit>\n<oil paiting of a landscape=\n<latexit sha1_base64=\"/VeAI5WLjd5tld5fRFTJdVg59hY=\">AErHicdZPdbtMwFMe9LcAYXx1chNRoSEhVc0kPi4nuOGCizGt26Qm6hz7JDF17Mh2NoqVh+BpuIXH4G1wmgzapLPU5Oj8f/T4xM7LjTZjz+s7W94925e2/3/t6Dh48ePxnsPz3TslQEJkRyqS5irIEzARPDIeLQgHOYw7n8fxjrZ9fgdJMilOzKCDKcSpYwg2LjUbvA6FZIKCMCFxD1BMpCHkRWYvL7FvmFj4VOFUioODajYjkfj5fL7QdAGQ9Su49n+DgupJGXuKhOtZ4G48JEFivDCIdqLyw1FJjMcQpTFwqcg47scleV/9JlqJ9I5X7C+MvsqsPiXOtFHjsyxybTXa1ObtKmpUneR5aJojQgSPNHScl9I/16RD5lCojhbucME8Vcrz7JsMLEjcdVUiDgmsg8x4La8ApINQ0iG8aS07odye0wqNzeVinXREPV3cTJBkLDCkHwpiInVeuP7YnT1kUuta7+uz93zbV+DSzNjKO4OyAUd0swdzagqWG/d/3uiEjmPsgNlDmodXRIgWtloOxi14VliS5pMCbVnTLhUXGuqgbMwWViA5pMjC4y85BiX5N3asJxT/VhYxLcWuDVUPfot5MoXmvQ3UHpNLS0axILVF5pD2Rh8z7Fx4WLQslvjVYPvMHWGUd8rWxRX86gexX7wdnhKHg7evPlcHj0ob2mu+g5eoFeoQC9Q0foEzpGE0TQD/QT/UK/vZF36k29qEG3t1rPM7S2vOQvr3Go5A=</latexit><a tiny dragon=\n<latexit sha1_base64=\"9lT9y5CL1+Z01P9378h/QGedbY=\">AEsnicdZPNbtNAEMe3rYFSvlI4crGIUDlFcXsYILBw6lIm1FHKXr9Thesh/W7rolWH4OnoYrPANvwzh2IbHTtZyM5v/7j8fj3SgT3Lrh8M/W9o536/ad3bt79+4/ePiot/41OrcMBgxLbQ5j6gFwRWMHcCzjMDVEYCzqL5+0o/uwRjuVaf3SKDiaQzxRPOqMPUtBeESnMVg3Ihwx8wXM1CkFlaXFxEVOFlfa586uND5uAODsprz8cDJfL7wZBE/RJs46n+zs8jDXLJdZnglo7DoaZmxTUOM4ElHthbiGjbE5nMZQUQl2UizfrfSfYyb2E23wVs5fZlcdBZXWLmSEpKQutW2tSm7SxrlL3k4KrLcgWL1g5Jc+E71aD8mBtgTiwoMxw7NVnKTWU4ZCwkgEFV0xLSVchJfAynEwKcJIi7hqR4uiH5T4bqsUNlFTVTdRsoGwsEIwuqnISdn4o+IEtXVRaGvL/+6PbXOlXwGfpQ4pgdskpu0SHcI1DWK720/bhTN8YNcQylCjaNFMq3icjmYtGpwpNE6hE3YptuDBLeRvFMcdgEtUiXQqOtk5GNWtaTs1IfunYsiFVjc2WNb0Der1FOr/dajqwLVys5zHVLHKoiXMOqOPOEVX1HLRLDP6W61VA6+xdQaJr2WRVYczaB/FbnB6OAheD159OuwfvWuO6S5Sp6RFyQgb8gR+UCOyYgw8oP8JL/Ib+l98WjHqvR7a3G84SsLU/8BVs9qyY=</latexit>\n<bananas in a basket=\n<latexit sha1_base64=\"SdreFfci4aFXLuxsjo9RsnAZBV0=\">AEqXicdZPdbtMwFMe9LcAYXx1chNRoU1CqpJfFxOcMF9tEt4qm6hznJDF1bMt2NoqV+BpuIX34G1wmgzapLOU5Oj8f/+TkxM7koxqMxz+2dre8e7cvbd7f+/Bw0ePn/T2n5rUSgCIyKYUOMIa2CUw8hQw2AsFeA8YnARzT9U+sUVKE0F/2wWEqY5TjlNKMHGpWa9w5ALymPgJiTuBoryNIRcZvbyEvtzFOshDg4KGe9/nAwXC6/GwRN0EfNOpnt79AwFqTIXVnCsNaTYCjN1GJlKGFQ7oWFBonJHKcwcSHOeipX5S6b90mdhPhHIXN/4yu+qwONd6kUeOzLHJdFurkpu0SWGSd1NLuSwMcFK/KCmYb4RfzcePqQJi2MIFmCjqevVJhUmbjaukgIO10TkOeaxDa+AlJNgasNIsLhqRzDbD0r3bauUa6Kmqm6iZAOhYUgeFORs7LxR/bMaesiE1qX/92f2uZKvwaZsZRzO2OGLdLULcxoK5hv7f9bn8I6n7IDZQ5qHG0SCJ4XC4HYxedKjRJchEDq1vRDRfKjLZRN+YVMJbpMnA4DY7B8W7NXWnJsh/qgspE/zWBsuavkW9mUL9XIeqDkwrlxY0xpxUFpFD2hl9RLFzRS0XlKJb7VWDbzG1hlHfC2trA5n0D6K3eD8aBC8Gbw+Peofv2+O6S56jl6gQxSgt+gYfUQnaIQI+oF+ol/ot/fKO/XG3pca3d5qPM/Q2vLIX7Whp7k=</latexit><a kangaroo=\n<latexit sha1_base64=\"5RNoIoUxq9ICRnYM6+FjcWurHcI=\">AEtnicdZPNbtQwEMfdNkApX1s4colYoXKqNpUKPVb0woFDqeiHtFlax5kZv0l2lZojwJT8MVnoC3YbJoZtLSUZzf/3n0wmdmIEd340+rOyuhbcu/9g/eHGo8dPnj4bD4/cbq0DI6ZFtqeJdSB4AqOPfcCzowFKhMBp8n0oNFPL8E6rtVnPzMwkTRXPOMekydD3ZjpblKQfmY4Q0sV3kM0hTVxQUNJbVYKExK50OdhQcUHLVbW/X5YDjaHs1XuBxEXTAk3To831zjcapZKfEdTFDnxtHI+ElFredMQL0Rlw4MZVOawxhDRSW4STX/vjp8jZk0zLTFS/lwnr3pqKh0biYTJCX1hetrTfI2bVz6bG9ScWVKD4q1L8pKEXodNsMKU26BeTHDgDLsdeQFdRShoPCShYUXDEtJVpFV8Cq8fRpIoTLdKmHS2qYVTjt92ksImWarpJslsIBzcIRm8rclR3/qQ6Qm1RFNq5+r/7Y9/c6FfA8IjJXCrpLRfguMugbZG9b3vx82iOf6Qa6hAqHP0SKZVWs8HU82WqvAskzoF0biOi42Be+jOYUbKZ6pC/A0z47BauWa7qlmD+qRhyodWdDdYtfYd6PYX2uQg1HfheLi95ShVrLFpCvjT6hFN0JT0XNcbqb63WDLzFhkvtaVaQ5n1D+Ky8HJznb0dnv3085w/313TNfJS/KvCEReUf2yQdySI4JIz/IT/KL/A72gi8BHmLrq50nhdkYQXmL6uZrJ4=</latexit>\n<a marble bust of Caesar=\n<latexit sha1_base64=\"Arb/q1ivS4AxJmUkzvQ03XikLo=\">AEqnicdZPLbtNAFIanrYFSbiks2VhEqChK7EZVnBhgWLEpG2KI7S8fjYnmZumhm3BMvPwNOwhefgbRjHLiR2OpLto/N/Hx8UykGDV2OPyztb3j3bp9Z/fu3r37Dx4+6u0/PjEy1wTGRDKpzyJsgFEBY0stgzOlAfOIwWk0/1Dp5egDZXi10omHKcCpQgq1LzXovQyGpiEHYkLgbaCrSELjKivNz7GdSwMKPA4OylmvPxwMl8vBkET9FGzjmf7OzSMJcm5q0sYNmYSDJWdFlhbShiUe2FuQGEyxylMXCgwBzMtlt9U+s9dJvYTqd0lrL/MrjoKzI1Z8MiRHNvMtLUquUmb5DZ5Ny2oULkFQeoXJTnzrfSrAfkx1UAsW7gAE01drz7JsMbEDcdV0iDgikjOsYiL8BJIOQmRhJFlftSFb0g9J92yrlmqipqpso2UAYWCEI3lRkVDb+qBg5bV1k0pjyv/tT21zpV0DTzDqKue0R43YJ6nYG1DWK72/2yCSuh9yDWUOahwtkgRl8vBFItOFZokXMbA6lZMw4Uqo23UjTkGnYgWaTOwuM3OQYtuTdOpCeqf6kLKpLixwbKmb1Cvp1A/16GqA9vKpTmNsSCVRXJIO6OPKHauqOXCSmn5rdaqgdfYOuOIi7JQ1eEM2kexG5wcDoI3g9efD/tH75tjuoueomfoBQrQW3SEPqJjNEYE/UA/0S/023vljbyv3qRGt7cazxO0trz4L2OMp+A=</latexit><a honey bee=\nFigure 3. Examples of optimization-based image editing results.\nWe show pairs of input images (left) and editing result (right).\n<latexit sha1_base64=\"I1KzQR09lD2BitIAjUJu4pI/9dg=\">AEmnicdZPbtNAEMa3raGl/EvhOBgESFxiuJKQI9VuVDBoVRNWymOovV6bG+z3rV21y3BsngarvA8vA3j2IXETleKPZrv901mx7tBJrixw+Gfjc0t59797Z0Huw8fPX7ytLf37NyoXDMYMSWUvgyoAcEljCy3Ai4zDTQNBFwEs4+VfnEN2nAlz+w8g0lKY8kjzqjF1LT3wpeKyxCk9Rk+QHMZu8cy+201x8OhovldgOvCfqkWSfTvS3uh4rlKdZhghoz9oaZnRUW84ElLt+biCjbEZjGMoaQpmUiz2ULpvMBO6kdL4k9ZdZJcdBU2NmacBkim1iWlrVXKdNs5tdDApeLUlkKz+oygXrlVuNRA35BqYFXMKNMce3VZQjVlOAyspEHCDVNpSmVY+NfAyrE3KfxAibBqR4mi75W4t2UKm6ipqpsgWkMYWCIYXVfktGz8QXGK2qolDHlf/eXtrnSb4DHiUVK4HEIabsEx5MAdY3ie9uPB0Jx/C3UIJQ42iRTMmwXAymHeq8ChKVQibsU0nJ8lvI3imEPQkWyRNgFL2+wMtOzWNJ2akP1TMeRCyTsbLGv6DvV2CvV7Fao6sK1cnPOQSlZVApxZ/QBp+gKWi6aZVp9q7Vq4DW2yiBxVRZiZfTa1/FbnC+P/DeD9593e8fHjXdIe8JK/JW+KRD+SQfCInZEQY+UF+kl/kt/PKOXKOnc81urnReJ6TleWc/QVAuaF0</latexit>Input\n<latexit sha1_base64=\"IM8XwPq6eRqwGwqIp5I648kl1LA=\">AEjHicdZPfbtMwFMa9LcAoMDa45CaiQuKqStD4I6FJE0iICy7GRLdJTQ59knj1bGD7WwrVp6DW3gs3ganyaBNOktVj873+06+ntRJwZk2QfBnY3PLu3P3vb9wYOHj3Ye7+49OdGyVATGRHKpzhKsgTMBY8Mh7NCAc4TDqfJ7GOtn16C0kyKb2ZeQJzjqWApI9i4VhwZuDZWA9DqIDzfHQajYH8fhG2xRC15+h8b4tFVJIyB2EIx1pPwqAwscXKMKhGkSlhgKTGZ7CxJUC56Bju0hd+S9ch/qpVO4jL/oLjszrWe54kjc2wy3dXq5jptUpr0XWyZKEoDgjQPSkvuG+nXK/ApU0AMn7sCE8VcVp9kWGFi3KIGkQIBV0TmORbURpdAqkY2yiRnNZxJLfDsHK/bZlyIRqTpOkawgNSwTB64YcV60/scdOWxW51Lr67/7SNdf6FbBpZhzF3R+A4u4I5t49NDPsj6fgpDMvZAbKHNQ6+iQRApaLRZj570pLE1zSYE3UXTLRUXGuqhbMwWVig5pMjC4y85Aif5M3ZsJxT/VlYxLcWvAqFvUW+20HyvQnUC0+lNS0axILVF5jDtrT5h2LmSjgsXhZLXjVYvMFWGUdcVLao3OUMu1exX5y8GoVvRq+/7g8P7TXdBs9Q8/RSxSit+gQfUZHaIwI+o5+ol/ot7fj7XvYMG3dxoPU/RyvE+/QVCk5v</latexit>\nseed = 1\n<latexit sha1_base64=\"6b21AjDEJEhWeJuJDPUIuDnNKo0=\">AEjHicdZPfbtMwFMa9LcAoMDa45CaiQuKqSqbxR0KTJpAQF1yMiW6Tmhy7JPG1LGD7WwrVp6DW3gs3ganyaBNOktVj873+06+ntRJwZk2QfBnY3PLu3P3vb9wYOHj3Ye7+49OdWyVATGRHKpzhOsgTMBY8Mh/NCAc4TDmfJ7EOtn12C0kyKr2ZeQJzjqWApI9i4VhwZuDZWA9DqcP9idxiMgsXx+0XYFkPUnuOLvS0WUnKHIQhHGs9CYPCxBYrwiHahCVGgpMZngKE1cKnIO7SJ15b9wHeqnUrmPMP6iu+ywONd6nieOzLHJdFerm+u0SWnSt7FloigNCNI8KC25b6Rfr8CnTAExfO4KTBRzWX2SYWJcYsaRAoEXBGZ51hQG10CqSZhbKNEclrHkdwOw8r9tmXKhWioOk2SriE0LBErxtyUrX+xJ4bVXkUuvqv/tz1zrV8CmXEUd38AirsjmHv30MywP7p+CkIy90JuoMxBraNDEilotViMnfemsDTNJQXeRNEtFxUZ6JuzRUKjqkycDgLjsDJfozdW8mFP9UVzIuxa0Bq4a+Rb3ZQvO9CtUJTKc3LRnFgtQWmcO0t/qEYedKOi5cFEpeN1q98AZbZRzxrbJF5S5n2L2K/eJ0fxS+Hr36cjA8et9e0230D1HL1GI3qAj9AkdozEi6Dv6iX6h396Od+C98w4bdHOj9TxFK8f7+BdGr5vw</latexit>\nseed = 2\n<latexit sha1_base64=\"GTDc34ODC8xsQxgqGhfl/deHxFc=\">AEjHicdZPfbtMwFMa9LcAoMDa45CaiQuKqSmD8kdCkCSTEBRdjotukJpoc+6QxdexgO9uKlefgFh6Lt8FpMmiTzlLVo/P9vpOvJ3VScKZNEPzZ2Nzybt2+s313cO/+g52Hu3uPTrQsFYExkVyqswRr4EzA2D4axQgPOEw2ky+1DrpxegNJPiq5kXEOd4KljKCDauFUcGrozVALQ6eHm+OwxGweL4/SJsiyFqz9H53haLqCRlDsIQjrWehEFhYouVYRDNYhKDQUmMzyFiSsFzkHdpG68p+5DvVTqdxHGH/RXZYnGs9zxNH5thkuqvVzXapDTp29gyUZQGBGkelJbcN9KvV+BTpoAYPncFJoq5rD7JsMLEuEUNIgUCLonMcyojS6AVJMwtlEiOa3jSG6HYeV+2zLlQjRUnSZJ1xAalgiC1w05rlp/Yo+dtipyqX13/25a671S2DTzDiKuz8Axd0RzL17aGbYH10/BSGZeyHXUOag1tEhiRS0WizGzntTWJrmkgJvouiWi4qMdVG3ZgoqFR3SZGBwl52BEv2ZujcTin+qKxmX4saAVUPfoF5vofleheoEptObloxiQWqLzGHaW3CsHMlHRcuCiWvGq1eIOtMo74Vtmicpcz7F7FfnHyYhS+Hr36sj8fN9e0230BD1Fz1GI3qBD9AkdoTEi6Dv6iX6h396Ot+98w4adHOj9TxGK8f7+BdKy5vx</latexit>\nseed = 3\n<latexit sha1_base64=\"ZFZ5bAYZvQisty/b8Su8daKqTJM=\">AEjHicdZPRbtMwFIa9LbBRYGxwyU1EhcRV1aACk9CkCSTEBRdjotukJpoc+6Tx6tjBdrYVK8/BLTwWb4PTZNAmnaWqR+f/pO/J3Wc6bNcPhnY3PLu3d/e+dB7+Gjx7tP9vafnmpZKAJjIrlU5zHWwJmAsWGw3muAGcxh7N49rHSz65AaSbFNzPIcrwVLCEWxcKwoN3BirAWh5OLrY6w8Hw8Xxu0XQFH3UnOL/S0WUkmKDIQhHGs9CYa5iSxWhEOZS8sNOSYzPAUJq4UOAMd2UXq0n/pOtRPpHIfYfxFd9lhcab1PIsdmWGT6rZWNdpk8IkB5FlIi8MCFI/KCm4b6RfrcCnTAExfO4KTBRzWX2SYoWJcYvqhQoEXBOZVhQG14BKSdBZMNYclrFkdz2g9L9tmXKhaipKk2crCE0LBErxtyUjb+2J4bVXkUuvyv/tL21zp18CmqXEUd38AitsjmHv3UM+wP9p+CkIy90JuodRBjaNFEilouViMnXemsCTJAVeR9ENF+Ypa6NuzRUIlqkScHgNjsDJbozdWcm5P9UVzIuxZ0By5q+Q73dQv29ClUJTKs3LRjFglQWmcG0s/qYeKWy6c50re1Fq18BpbZRxWdq8dJczaF/FbnH6ehC8Hbz5OuofWiu6Q56jl6gVyhA79AR+oyO0RgR9B39RL/Qb2/XG3nvcMa3dxoPM/QyvE+/QVO5vy</latexit>\nseed = 4\nFigure 4. By fixing \u03f5 in Lcond we can obtain diverse editing results.\nWe show four variants of optimization-based editing results of the\ninput image under the given prompt \u201ca mountain during sunset\u201d.\nSet-up. If not noted otherwise, we scale the total loss by 1\n\u03c9\nto be able to ablate \u03c9 without changing the gradient magni-\ntude. The pre-trained diffusion model we use in our experi-\nments produces images of 128 \u00d7 128px resolution. We en-\nable arbitrary higher resolutions in our image experiments\nthrough the following strategy: We downsample the opti-\nmization state to 128 \u00d7 128px for the first iterations. This\nensures that the whole image is affected by the loss, but the\nsignal may be blurry. Accounting for this, we sample ran-\ndom patches of arbitrary resolutions, from the image, for\nthe remaining optimization steps. To prevent the loss from\nproducing fake detail in sampled image patches, we lower\nthe learning rate during this phase by using Cosine decay.\nBaselines. This paper presents a generic novel loss func-\ntion which uses an image diffusion model. To this end, we\ncompare with the original SDS formulation and with related\nwork focused on improving SDS. Specifically, we compare\nagainst DDS [8] and multi-step SDS (MS-SDS) [39]. DDS\nand MS-SDS originally were proposed using a latent diffu-\nsion model [30] and operate in latent space. For fair com-\nparisons, we adapted both methods to operate in pixel space\nand to use the same image diffusion model. Note that opti-\nmizing in the (ambient) pixel space is generally considered\nharder due higher-dimensionality. Finally, we evaluate the\neffectiveness of our LMC-SDS loss for 3D asset generation\nusing DreamFusion. Hereby we focus on the improvements\nof our loss and deliberately exclude recent improvements\nover DreamFusion rooted in other factors, e.g. novel-view\nsynthesis networks or additional perceptual losses. While\nbeing important findings, those factors are orthogonal and\ncomplementary to our approach.\nMetrics. We use the following metrics for evaluating our\nmethods and baselines: We report Learned Perceptual Im-\nage Patch Similarity (LPIPS \u2193) [38] w.r.t. the source im-\nage, thus capturing how much an image was altered. Note\nthat high LPIPS can be a result of either successful editing\nor image corruption. To disambiguate these, we therefore\nadditionally report the CLIP score (\u2191) [9] w.r.t. the target\nprompt. As the CLIP score can be insensitive, we addition-\nally report CLIP-R-Precision (\u2191) [27], the retrieval accuracy\nusing the CLIP model, where feasible.\n5.1. Image Synthesis & Editing\nWe first explore our LMC-SDS loss for optimization-based\nimage synthesis and image editing.\nSynthesis. Following the patching strategy described ear-\nlier, we can create images of arbitrary resolutions. To this\nend, we initialize an image with 50% grey and optimize its\n5\n0.26\n0.27\n0.28\n0.29\n0.30\nCLIP Score\n0.00\n0.05\n0.10\n0.15\n0.20\nLPIPS\n=3\n5\n10\n15\n20\n30\n=3\n5\n10\n15\n20\n30\n=3\n5\n10\n15\n20\n30\n=3\n5\n10\n15 20\n30\nOurs\nDDS\nSDS\nMS-SDS\nFigure 5. Quantitative results for optimization-based editing un-\nder varying \u03c9. DDS stays close to the original image (low LPIPS)\nby performing only small edits (low CLIP). SDS and MS-SDS\nrespect the prompt better (higher CLIP), but corrupt the image\n(high LPIPS). Our method shows the strongest editing capabili-\nties (highest CLIP), while staying close to the original structure.\n<latexit sha1_base64=\"FTLXvXkXwkGW4qVMvdriybEf2Xk=\">ACXicbVDLSsNAFL2pr1pfUZduBovgqibia1l047KCfUATymQ6aYdOHsxMhBi6deOvuHGhiFv/wJ1/46QNqK0HBg7n3Hvn3uPFnElWV9GaWFxaXmlvFpZW9/Y3DK3d1oySgShTRLxSHQ8LClnIW0qpjtxILiwO07Y2ucr9R4VkUXir0pi6AR6EzGcEKy31TOTEWCiGOXICrIaen92P0dGPmvbMqlWzJkDzxC5IFQo0euan049IEtBQEY6l7NpWrNwsn0c4HVecRNIYkxEe0K6mIQ6odLPJWN0oJU+8iOhX6jQRP3dkeFAyjTwdGW+rpz1cvE/r5so/8LNWBgnioZk+pGfcKQilMeC+kxQoniqCSaC6V0RGWKBidLhVXQI9uzJ86R1XLPaqc3J9X6ZRFHGfZgHw7BhnOowzU0oAkEHuAJXuDVeDSejTfjfVpaMoqeXfgD4+MbDk+Z7w=</latexit>\u2202z/\u2202y\n<latexit sha1_base64=\"d4q1pZRsaqdTJgtJkqBjKHCjK1Y=\">AB+XicbVDJSgNBEO2JW4zbqEcvjUHwFGbE7RgUxGMEs0AyhJ5OTdKkZ7G7JhiG/IkXD4p49U+8+Td2kjlo9EHB470qur5iRQaHefLKiwtr6yuFdLG5tb2zv27l5Dx6niUOexjFXLZxqkiKCOAiW0EgUs9CU0/eH1G+OQGkR/c4TsALWT8SgeAMjdS17Q7CI2Y3Ch5SiPh40rXLTsWZgf4lbk7KJEeta392ejFPQ4iQS6Z123US9DKmUHAJk1In1ZAwPmR9aBsasRC0l80un9Ajo/RoECtTEdKZ+nMiY6HW49A3nSHDgV70puJ/XjvF4NLRJSkaN6aLwpSTGm0xhoTyjgKMeGMK6EuZXyAVOMowmrZEJwF1/+SxonFfe8cnZ3Wq5e5XEUyQE5JMfEJRekSm5JjdQJyPyRF7Iq5VZz9ab9T5vLVj5zD75BevjG1QmlCA=</latexit>Frequency\n<latexit sha1_base64=\"tDAJ1rW1eatzedI4cAMuF+/XPUM=\">ACAnicbVDLSsNAFJ34rPUVdSVuBovgqiTia1kUxGUF+4A2lMn0ph06yYSZibSE4sZfceNCEbd+hTv/xkmbhbYeuHA4517uvcePOVPacb6thcWl5ZXVwlpxfWNza9ve2a0rkUgKNSq4kE2fKOAsgpmkMzlkBCn0PDH1xnfuMBpGIiutejGLyQ9CIWMEq0kTr2flvDUKc3ktBMwCLAMRsCV+OXLKzgR4nrg5KaEc1Y791e4KmoQacqJUi3XibWXEqkZ5TAuthMFMaED0oOWoREJQXnp5IUxPjJKFwdCmo0nqi/J1ISKjUKfdMZEt1Xs14m/ue1Eh1ceimL4kRDRKeLgoRjLXCWB+4yCVTzkSGESmZuxbRPsjhMakUTgjv78jypn5Td8/LZ3WmpcpXHUAH6BAdIxdoAq6RVUQxQ9omf0it6sJ+vFerc+pq0LVj6zh/7A+vwBuoSXqA=</latexit>Fraction of pixels\n<latexit sha1_base64=\"qBPlmV9vwaJAt0Wa9OqsNgLzvqA=\">AB83icbVDLSgNBEJz1GeMr6tHLYBA8hV3xdQx68RjBPC7hNlJxkyO7vM9KphyW948aCIV3/Gm3/jJNmDJhY0FXdHeFiRQGXfbWVpeWV1bL2wUN7e2d3ZLe/sNE6eaQ53HMtatkBmQkEdBUpoJRpYFEpohsObid98AG1ErO5xlEAQsb4SPcEZWsn3EZ4wq8WPoMedUtmtuFPQReLlpExy1DqlL78b8zQChVwyY9qem2CQMY2CSxgX/dRAwviQ9aFtqWIRmCb3jymx1bp0l6sbSmkU/X3RMYiY0ZRaDsjhgMz703E/7x2ir2rIBMqSREUny3qpZJiTCcB0K7QwFGOLGFcC3sr5QOmGUcbU9G4M2/vEgapxXvonJ+d1auXudxFMghOSInxCOXpEpuSY3UCScJeSav5M1JnRfn3fmYtS45+cwB+QPn8weztpIi</latexit>Power\nFigure 6. Comparison of image statistics before (Original) and\nafter editing with \u03c9 = 15. We plot the average power spectra\nand vertical derivative histograms. Ours and DDS preserve the\nimage statistics well, while SDS-based methods introduce signifi-\ncant blur.\npixel values to match a given prompt. We show results of\nour loss and SDS in Fig. 2. We additionally include re-\nsults obtained by only using Lcond, ablating the influence of\nour proposed Llmc. Optimizing solely with Lcond is unstable\nand the final images feature clearly visible defects, such as\noversaturated colors or noisy patterns. The prompt is also\nnot always well respected as e.g. in the case of the gibbon.\nThe original SDS formulation produces blurry and desat-\nurated images when using low guidance \u03c9. High \u03c9 leads\nto better performance but artifacts are present, e.g. fake de-\ntail like the second mouse tail, oversaturated colors like in\nthe chimpanzee\u2019s face, or blocky artifacts. In contrast, our\nmethod produces images of higher visual fidelity which re-\nspect the prompt well.\nEditing. Our loss can also be used for optimization-based\nimage editing. We load existing natural images and opti-\nmize their pixel values directly. Optionally, we may con-\nstrain the optimization with an L2 loss w.r.t. the original\npixel values. In Fig. 3 we show qualitative results of so\nedited images along with the corresponding target prompt.\nAll results faithfully respect the target prompt while keep-\ning the image structure intact. We can also vary the result\nby fixing \u03f5 over the course of the optimization, cf. Sec. 4.\nIn Fig. 4, we show an input image along with four result\nvariants. All results respect the target prompt well, while at\nthe same time being significantly different from each other.\nFor a numerical evaluation, we use an internal dataset of\n1,000 images with text editing instructions in form of source\n<latexit sha1_base64=\"6e1W/vuJ9coYpO3N/gME4AsA4vg=\">AB8nicbVDLSgNBEJyNrxhfUY9eFoPgKeyKr2PQizcjmAdsljA76SRDZmeWmV4xLPkMLx4U8erXePNvnCR70GhBQ1HVTXdXlAhu0PO+nMLS8srqWnG9tLG5tb1T3t1rGpVqBg2mhNLtiBoQXEIDOQpoJxpoHAloRaPrqd96AG24kvc4TiCM6UDyPmcUrR0EB4xu021mXTLFa/qzeD+JX5OKiRHvVv+7PQUS2OQyAQ1JvC9BMOMauRMwKTUSQ0klI3oAJLJY3BhNns5Il7ZJWe21falkR3pv6cyGhszDiObGdMcWgWvan4nxek2L8My6TFEGy+aJ+KlxU7vR/t8c1MBRjSyjT3N7qsiHVlKFNqWRD8Bdf/kuaJ1X/vHp2d1qpXeVxFMkBOSTHxCcXpEZuSJ0COKPJEX8uqg8+y8Oe/z1oKTz+yTX3A+vgHyiJG0</latexit>\nOurs\n<latexit sha1_base64=\"jdazXsHnJCNLg+MZdFAoc0rem8=\">AB8XicbVBNS8NAEJ3Ur1q/qh69BIvgqSTi17FoDx4r2g9sQ9lsN+3SzSbsTsQS+i+8eFDEq/Gm/GbZuDtj4YeLw3w8w8PxZco+N8W7ml5ZXVtfx6YWNza3unuLvX0FGiKvTSESq5RPNBJesjhwFa8WKkdAXrOkPryd+85EpzSN5j6OYeSHpSx5wStBIDx1kT5hWq3fjbrHklJ0p7EXiZqQEGWrd4lenF9EkZBKpIFq3XSdGLyUKORVsXOgkmsWEDkmftQ2VJGTaS6cXj+0jo/TsIFKmJNpT9fdESkKtR6FvOkOCAz3vTcT/vHaCwaWXchknyCSdLQoSYWNkT963e1wximJkCKGKm1tOiCKUDQhFUwI7vzLi6RxUnbPy2e3p6XKVRZHg7gEI7BhQuowA3UoA4UJDzDK7xZ2nqx3q2PWvOymb24Q+szx+QZJDc</latexit>\nDDS\n<latexit sha1_base64=\"NhtlEYCEq0uJ7R2Br57PUwfp+S0=\">AB8XicbVBNS8NAEN3Ur1q/qh69BIvgqSTi17GoB4+V2g9sQ9lsJ+3SzSbsTsQS+i+8eFDEq/Gm/GbZuDtj4YeLw3w8w8PxZco+N8W7ml5ZXVtfx6YWNza3unuLvX0FGiGNRZJCLV8qkGwSXUkaOAVqyAhr6Apj+8nvjNR1CaR/IeRzF4Ie1LHnBG0UgPHYQnTGs3tXG3WHLKzhT2InEzUiIZqt3iV6cXsSQEiUxQrduE6OXUoWcCRgXOomGmLIh7UPbUElD0F46vXhsHxmlZweRMiXRnq/J1Iaj0KfdMZUhzoeW8i/ue1EwuvZTLOEGQbLYoSISNkT153+5xBQzFyBDKFDe32mxAFWVoQiqYENz5lxdJ46TsnpfP7k5Llasjw5IfkmLjkglTILamSOmFEkmfySt4sb1Y79bHrDVnZTP75A+szx+nTZDr</latexit>\nSDS\n<latexit sha1_base64=\"6e1W/vuJ9coYpO3N/gME4AsA4vg=\">AB8nicbVDLSgNBEJyNrxhfUY9eFoPgKeyKr2PQizcjmAdsljA76SRDZmeWmV4xLPkMLx4U8erXePNvnCR70GhBQ1HVTXdXlAhu0PO+nMLS8srqWnG9tLG5tb1T3t1rGpVqBg2mhNLtiBoQXEIDOQpoJxpoHAloRaPrqd96AG24kvc4TiCM6UDyPmcUrR0EB4xu021mXTLFa/qzeD+JX5OKiRHvVv+7PQUS2OQyAQ1JvC9BMOMauRMwKTUSQ0klI3oAJLJY3BhNns5Il7ZJWe21falkR3pv6cyGhszDiObGdMcWgWvan4nxek2L8My6TFEGy+aJ+KlxU7vR/t8c1MBRjSyjT3N7qsiHVlKFNqWRD8Bdf/kuaJ1X/vHp2d1qpXeVxFMkBOSTHxCcXpEZuSJ0COKPJEX8uqg8+y8Oe/z1oKTz+yTX3A+vgHyiJG0</latexit>\nOurs\n<latexit sha1_base64=\"jdazXsHnJCNLg+MZdFAoc0rem8=\">AB8XicbVBNS8NAEJ3Ur1q/qh69BIvgqSTi17FoDx4r2g9sQ9lsN+3SzSbsTsQS+i+8eFDEq/Gm/GbZuDtj4YeLw3w8w8PxZco+N8W7ml5ZXVtfx6YWNza3unuLvX0FGiKvTSESq5RPNBJesjhwFa8WKkdAXrOkPryd+85EpzSN5j6OYeSHpSx5wStBIDx1kT5hWq3fjbrHklJ0p7EXiZqQEGWrd4lenF9EkZBKpIFq3XSdGLyUKORVsXOgkmsWEDkmftQ2VJGTaS6cXj+0jo/TsIFKmJNpT9fdESkKtR6FvOkOCAz3vTcT/vHaCwaWXchknyCSdLQoSYWNkT963e1wximJkCKGKm1tOiCKUDQhFUwI7vzLi6RxUnbPy2e3p6XKVRZHg7gEI7BhQuowA3UoA4UJDzDK7xZ2nqx3q2PWvOymb24Q+szx+QZJDc</latexit>\nDDS\n<latexit sha1_base64=\"NhtlEYCEq0uJ7R2Br57PUwfp+S0=\">AB8XicbVBNS8NAEN3Ur1q/qh69BIvgqSTi17GoB4+V2g9sQ9lsJ+3SzSbsTsQS+i+8eFDEq/Gm/GbZuDtj4YeLw3w8w8PxZco+N8W7ml5ZXVtfx6YWNza3unuLvX0FGiGNRZJCLV8qkGwSXUkaOAVqyAhr6Apj+8nvjNR1CaR/IeRzF4Ie1LHnBG0UgPHYQnTGs3tXG3WHLKzhT2InEzUiIZqt3iV6cXsSQEiUxQrduE6OXUoWcCRgXOomGmLIh7UPbUElD0F46vXhsHxmlZweRMiXRnq/J1Iaj0KfdMZUhzoeW8i/ue1EwuvZTLOEGQbLYoSISNkT153+5xBQzFyBDKFDe32mxAFWVoQiqYENz5lxdJ46TsnpfP7k5Llasjw5IfkmLjkglTILamSOmFEkmfySt4sb1Y79bHrDVnZTP75A+szx+nTZDr</latexit>\nSDS\n<latexit sha1_base64=\"BJsKqaZDAi1vjRklVZd4pqoiZUA=\">AB83icbVBNS8NAEJ3Ur1q/qh69BIvgqSTi17HoRW8V7Ac0oWy2m3bpZhN2J2IJ/RtePCji1T/jzX/jps1Bqw8GHu/NMDMvSATX6DhfVmlpeWV1rbxe2djc2t6p7u61dZwqylo0FrHqBkQzwSVrIUfBuoliJAoE6wTj69zvPDCleSzvcZIwPyJDyUNOCRrJ85A9YnYrkxSn/WrNqTsz2H+JW5AaFGj2q5/eIKZpxCRSQbTuU6CfkYUcirYtOKlmiWEjsmQ9QyVJGLaz2Y3T+0jowzsMFamJNoz9edERiKtJ1FgOiOCI73o5eJ/Xi/F8NLPeP4Sk3S+KEyFjbGdB2APuGIUxcQhU3t9p0RBShaGKqmBDcxZf/kvZJ3T2vn92d1hpXRxlOIBDOAYXLqABN9CEFlBI4Ale4NVKrWfrzXqft5asYmYfsH6+Aa4KJIl</latexit>Input\n<latexit sha1_base64=\"n73XqD6jk+6cj9+g6iSovX5TFwg=\">AB8nicbVDLSgNBEJz1GeMr6tHLYBA8hV3xdQzx4jGCeUCyhNlJbzJkdmeZ6RXDks/w4kERr36N/GSbIHTSxoKq6e4KEikMu63s7K6tr6xWdgqbu/s7u2XDg6bRqWaQ4MrqXQ7YAakiKGBAiW0Ew0sCiS0gtHt1G89gjZCxQ84TsCP2CAWoeAMrdTpIjxhVgOmJ71S2a24M9Bl4uWkTHLUe6Wvbl/xNIYuWTGdDw3QT9jGgWXMCl2UwMJ4yM2gI6lMYvA+Nns5Ak9tUqfhkrbipHO1N8TGYuMGUeB7YwYDs2iNxX/8zophjd+JuIkRYj5fFGYSoqKTv+nfaGBoxbwrgW9lbKh0wzjalog3BW3x5mTPK95V5fL+olyt5XEUyDE5IWfEI9ekSu5InTQIJ4o8k1fy5qDz4rw7H/PWFSefOSJ/4Hz+AKrFkYU=</latexit>Bear\n<latexit sha1_base64=\"RHd/lj74ElU+zwZbt2FeTC5Lyo=\">AB8XicbVDJSgNBEK2JW4xb1KOXxiB4CjPidgzqwWMEs2AyhJ5OT9Kkp2forhHDkL/w4kERr/6N/GznLQxAcFj/eqKoXJFIYdN1vJ7e0vLK6l8vbGxube8Ud/fqJk414zUWy1g3A2q4FIrXUKDkzURzGgWSN4LB9dhvPHJtRKzucZhwP6I9JULBKFrpoY38CbObuDfqFEtu2Z2ALBJvRkowQ7VT/Gp3Y5ZGXCGT1JiW5yboZ1SjYJKPCu3U8ISyAe3xlqWKRtz42eTiETmySpeEsbalkEzU3xMZjYwZRoHtjCj2zbw3Fv/zWimGl34mVJIiV2y6KEwlwZiM3ydoTlDObSEMi3srYT1qaYMbUgFG4I3/IiqZ+UvfPy2d1pqXI1iyMPB3AIx+DBVTgFqpQAwYKnuEV3hzjvDjvzse0NefMZvbhD5zPH/BKkRs=</latexit>Dog\n<latexit sha1_base64=\"vet1oZwKnwByJm0kn8SXLU4MYVk=\">AB8nicbZC7SgNBFIbPxluMt6ilzWIQrMKueCuDNhYWEcwFNkuYnUySIbMzy8xZMSx5DBsLRWx9GjvfxkmyhUZ/GPj4znMOX+UCG7Q876cwtLyupacb20sbm1vVPe3WsalWrKGlQJpdsRMUxwyRrIUbB2ohmJI8Fa0eh6Wm89MG24kvc4TlgYk4HkfU4JWivoIHvE7NbipFueFVvJvcv+DlUIFe9W/7s9BRNYyaRCmJM4HsJhnRyKlgk1InNSwhdEQGLAoScxMmM1WnrhH1um5faXtk+jO3J8TGYmNGceR7YwJDs1ibWr+VwtS7F+GZdJikzS+Uf9VLio3On9bo9rRlGMLRCqud3VpUOiCUWbUsmG4C+e/BeaJ1X/vHp2d1qpXeVxFOEADuEYfLiAGtxAHRpAQcETvMCrg86z8+a8z1sLTj6zD7/kfHwDz3GRnQ=</latexit>Lion\n<latexit sha1_base64=\"KZBG2hquePT1c2QxPmvewfwXKuE=\">AB+HicbVDJTgJBEO3BDXFh1KOXjsTE5kxbkeiF48YZUlgQnqaAjr0LHbXGHCl3jxoDFe/Rv/o0NzEHBl1Ty8l5Vqur5sRQaHefbyi0tr6yu5dcLG5tb20V7Z7euo0RxqPFIRqrpMw1ShFBDgRKasQIW+BIa/vBq4jceQGkRhXc4isELWD8UPcEZGqljF9sIj5je3idCKZDjl1ys4UdJG4GSmRDNWO/dXuRjwJIEQumdYt14nRS5lCwSWMC+1EQ8z4kPWhZWjIAtBeOj18TA+N0qW9SJkKkU7V3xMpC7QeBb7pDBgO9Lw3Ef/zWgn2LrxUhHGCEPLZol4iKUZ0kgLtCgUc5cgQxpUwt1I+YIpxNFkVTAju/MuLpH5cds/KpzcnpcplFke7JMDckRck4q5JpUSY1wkpBn8krerCfrxXq3PmatOSub2SN/YH3+AJ9pk7s=</latexit>Squirrel\nFigure 7. Results obtained with cats-to-others image translation\nnetworks trained with different losses. The network trained with\nthe SDS loss fails to create detailed results. The DDS network\nproduces rather uniform results, whereas the network trained with\nour LMC-SDS creates realistic and detailed images.\nand target prompts, similar to the dataset used in Instruct-\nPix2Pix [2]. We optimize images to match the target prompt\nsolely using our LMC-SDS and baselines DDS, SDS, and\nMS-SDS. For MS-SDS we choose to use five step denois-\ning. We also experimented with more steps, but didn\u2019t ob-\nserve significantly different performance. We report numer-\nical results with varying \u03c9 (without loss rescaling) in Fig. 5.\nWe found DDS to be least effective and producing results\nclosest to the original image. We hypothesize that the neg-\native gradient w.r.t. to the original image may not always\nbe helpful or even turn out to be \u201cconfusing\u201d. This could\nalso explain a behavior reported by original authors, where\nediting results on real images are \u201cinferior\u201d over those on\nsynthetic images. A possible reason being, that since for\nsynthetic images the original prompts are describing the im-\nage well, the negative gradients are less \u201cconfusing\u201d. SDS\ncan effectively edit the image, but also corrupts the image.\nThis is also evident from the altered image statistics, see\nFig. 6. In our experiments, MS-SDS performs very sim-\nilar to SDS, corrupting the image in similar manner, but\nalso being slightly less effective for editing. MS-SDS pro-\nduces clear and detailed denoised images as part of the loss\ncomputation. We hypothesize that since denoised images\nand the current optimization state may differ drastically,\n6\nLPIPS \u2193\nCLIP Score \u2191\nCLIP-R-Precision \u2191\nSDS\n0.401\n0.256\n0.355\nSDS, \u03c9 = 100\n0.201\n0.259\n0.482\nDDS\n0.143\n0.253\n0.333\nOurs\n0.178\n0.261\n0.592\nTable 1. Numerical results of the cats-to-others network training\nexperiment. Note that LPIPS is computed w.r.t. the source image,\nmeaning a low score may mean the image has not been edited\nenough. This is the case for DDS, as evident from low R-Precision.\nand as denoised images vary considerably at each step, the\ncomputed gradients are not always very meaningful. The\noriginal paper proposes MS-SDS to be used with a view-\nconditioned diffusion model, where the denoised images\nprobably vary much less and computed gradients may be\nmore expedient. Our loss respects the prompt best (high-\nest CLIP), and finds a good balance between preserving the\noriginal structure and performing significant edits.\n5.2. Image-to-image Translation Network Training\nOptimization-based editing is not always an optimal choice,\ne.g. when building interactive editing tools where execu-\ntion speed matters. Further, optimization can be unstable\nor perform better or worse, depending on the input images\nor target prompts. To this end, we explore to what extend\nLMC-SDS can be used to train a reliable and fast image-\nto-image translation network. Given a known distribution\nof images, we train a network to translate images from that\ndistribution to an unseen target distribution, only described\nby a target caption. Inspired by the authors of DDS [8], we\ntrain a cats-to-others network, with \u201cothers\u201d being bears,\ndogs, lions, and squirrels. The network transforms an im-\nage of a cat into an image of one of the other species. In\nDDS, the authors fine-tune a pre-trained diffusion model\ninto a single-step feed forward model using images created\nby that same diffusion model. In contrast, we train a stan-\ndard U-Net from scratch and use a dataset of real images,\na much more practical and versatile approach. Concretely,\nwe train using cat images from AFHQ [3] and evaluate on\n5% held out test images. We use the target prompt \u201ca photo\nof a [class]\u201d with \u03c9 = 15 for all methods and the source\nprompt \u201ca photo of a cat\u201d for DDS. Moreover, we stabilize\nthe training using an L2 loss on the original image and use\nCosine scheduling to let the network first focus on recon-\nstructing the original image and then on edits, similarly to\nthe experiment in DDS. The class is passed via a frequency\nencoded class id concatenated to each pixel. We report nu-\nmerical results in Tab. 1 and show qualitative examples in\nFig. 7. SDS performs similarly to previous experiments for\nthis task. While the target class is evident in the resulting\nimages, the results feature strong deficiencies. Again DDS\nstruggles with significant edits and stays close to the source.\nIn contrast, the network trained with our LMC-SDS loss\nproduces realistic and convincing edits. This is also evident\n<latexit sha1_base64=\"5AfKwWkyEIuLEP1JYpCGk1WIhC0=\">ACI3icbVBNTxsxEPVCzQtNIVjL1ajip6iXURb1BMCDhw4UNoAUhKFWc2seK1LXsWEa3yX7jwV7hwACEuHPpfcEIOfPRJIz2/NyPvNQq6SmO76OZ2Tdv5+YX3lXef1hc+lj9tHzoTeENoRxh2n4FJjQ2SpPDYOoQ8VXiUDrbH/tEpOi+N/ktDi+0celpmUgAFqVP91SI8o7KFue2XJyfAd/7sHXDbN2S4yThwiyCMGHCjw8MXLksNuO7q6mjUqdbiejwBf02SKamxKfY71dtW14giR01CgfNJLbULsGRFApHlVbh0YIYQA+bgWrI0bfLyY0j/jUoXZ4ZF0oTn6hPJ0rIvR/maejMgfr+pTcW/+c1C8o2qXUtiDU4vGjrFA8BDAOjHelQ0FqGAgIJ8OuXPTBgaAQayWEkLw8+TU5XKsnP+rf6/XNremcSywz+wL+8YS9pNtsl2zxpMsHN2ya7ZTXQRXUW30d1j60w0nVlhzxD9ewAdTKPw</latexit>\n<a DSLR photo of a peacock on a surfboard=\n<latexit sha1_base64=\"b1vAZ9v0WCrFHVeXBR/ESRsTq38=\">ACFHicbVBNSyNBEO3x2/iVdY9eGoMoCGFG/NhjWC8eXTAqJCHW9FSxp7uobtGDUN+xF78K148uIjXPXjz39iJOfj1oODxXhV9eJMSUdh+BJMTE5Nz8zOzZcWFpeWV8o/Vk+dya3AujDK2PMYHCqpsU6SFJ5nFiGNFZ7Fl4dD/+wKrZNGn1A/w1YKXS07UgB5qV3ebhLeUNHENOsVFxfAu8rEyI3mwK+NSVBzR6CTzc3BoF2uhNVwBP6VRGNSYWMct8vPzcSIPEVNQoFzjSjMqFWAJSkUDkrN3GEG4hK62PBUQ4quVYyeGvANryS8Y6wvTXykvp8oIHWun8a+MwXquc/eUPzOa+TU+dUqpM5yQi3eFnVyxcnwYUI8kRYFqb4nIKz0t3LRAwuCfI4lH0L0+eWv5HSnGu1X9/7sVmq/x3HMsTW2zrZYxA5YjR2xY1Zngv1ld+yB/Qtug/vgMXh6a50IxjM/2QcE/18BfhOedg=</latexit><a globe on a wooden stand=\n<latexit sha1_base64=\"JnHbEH83N+x8ZQVilBxlQj9J8zI=\">ACDnicbVC7TsNAEDzJrwClDQnogiqyEa8ygaSpBIQIqj5HxeJyfOZ+tujYisfAENv0JDAUK01HT8DZfEBa+RVhrN7N7tTpBKYdB1P52p6ZnZufmFxdLS8srqWnl9o2mSTHNo8EQm+jpgBqRQ0ECBEq5TDSwOJFwFN6cj/+oWtBGJusRBCu2Y9ZSIBGdopU656iPcYe5DnPbzbpfRXiJDUJT3mQrtq6B3dobDTrni1twx6F/iFaRCpx3yh9+mPAsBoVcMmNanptiO2caBZcwLPmZgZTxG9aDlqWKxWDa+ficIa1aJaRom0pGP1+0TOYmMGcWA7Y4Z989sbif95rQyj43YuVJohKD75KMokxYSOsqGh0MBRDixhXAu76ygHzTjaBEs2BO/3yX9Jc6/mHdYOLvYr9ZMijgWyRbJLvHIEamTM3JOGoSTe/JInsmL8+A8Oa/O26R1yilmNskPO9fm32cdQ=</latexit><a golden chandelier=\n<latexit sha1_base64=\"hD/snevBhSy0SJHv7Yk5x2Gtw=\">ACDnicbVDJSgNBEO1xN25Rj14ag+gpzIjbUfTiUcEYIQmxpqeSNOmeGbprxDkC7z4K148KOLVszf/xs5ycHtQ8Hiviqp6YaqkJd/9CYmp6ZnZufmCwuLS8srxdW1K5tkRmBFJCox1yFYVDLGCklSeJ0aB0qrIbd04FfvUVjZRJfUi/FhoZ2LFtSADmpWdyqE95RXkedvKbG+BWg1I8URE3GHEBZnu7328WS37ZH4L/JcGYlNgY583iRz1KRKYxJqHA2lrgp9TIwZAUCvuFemYxBdGFNtYcjUGjbeTDd/p8ykRbyXGVUx8qH6fyEFb29Oh69RAHfvbG4j/ebWMWkeNXMZpRhiL0aJWpjglfJANj6RBQarnCAgj3a1cdMCAIJdgwYUQ/H75L7naLQcH5f2LvdLxyTiObBNtkOC9ghO2Zn7JxVmGD37JE9sxfvwXvyXr23UeuEN5Zz/gvX8B1wWb9g=</latexit>\n<a small old red car=\n<latexit sha1_base64=\"6e1W/vuJ9coYpO3N/gME4AsA4vg=\">AB8nicbVDLSgNBEJyNrxhfUY9eFoPgKeyKr2PQizcjmAdsljA76SRDZmeWmV4xLPkMLx4U8erXePNvnCR70GhBQ1HVTXdXlAhu0PO+nMLS8srqWnG9tLG5tb1T3t1rGpVqBg2mhNLtiBoQXEIDOQpoJxpoHAloRaPrqd96AG24kvc4TiCM6UDyPmcUrR0EB4xu021mXTLFa/qzeD+JX5OKiRHvVv+7PQUS2OQyAQ1JvC9BMOMauRMwKTUSQ0klI3oAJLJY3BhNns5Il7ZJWe21falkR3pv6cyGhszDiObGdMcWgWvan4nxek2L8My6TFEGy+aJ+KlxU7vR/t8c1MBRjSyjT3N7qsiHVlKFNqWRD8Bdf/kuaJ1X/vHp2d1qpXeVxFMkBOSTHxCcXpEZuSJ0COKPJEX8uqg8+y8Oe/z1oKTz+yTX3A+vgHyiJG0</latexit>\nOurs\n<latexit sha1_base64=\"M1U+/q+TGjXbfPbwtYTjchIFj9I=\">AB+3icbVDLSsNAFJ3UV62vWJduBovgqiTia1lUxGUF+4A2lMl02g6dTMLMjbSE/IobF4q49Ufc+TdO2iy09cDA4ZxzuXeOHwmuwXG+rcLK6tr6RnGztLW9s7tn75ebOowVZQ0ailC1faKZ4JI1gINg7UgxEviCtfzxTea3npjSPJSPMI2YF5Ch5ANOCRipZ5e7wCaQ3GYzd3EWS3t2xak6M+Bl4uakgnLUe/ZXtx/SOGASqCBad1wnAi8hCjgVLC1Y80iQsdkyDqGShIw7SWz21N8bJQ+HoTKPAl4pv6eSEig9TwTIgMNKLXib+53ViGFx5CZdRDEzS+aJBLDCEOCsC97liFMTUEIVN7diOiKUDB1lUwJ7uKXl0nztOpeVM8fziq167yOIjpER+gEuegS1dA9qMGomiCntErerNS68V6tz7m0YKVzxygP7A+fwCuOJTf</latexit>DreamFusion\nFigure 8. Results of DreamFusion using the original loss formu-\nlation (right) and our proposed LMC-SDS (left) for \u03c9 = 20. Our\nresults are much sharper and contain more detail in all examples.\nfrom the significantly higher CLIP-R-Precision among all\nnetwork variants.\n5.3. Text-to-3D Asset Generation\nIn our final experiment, we demonstrate how LMC-SDS can\nimprove text-to-3D models in the style of DreamFusion. In\nSec. 3, we discussed how large guidance weights \u03c9 may\naffect the quality of results. To provide further evidence\nfor this observation, we produce 3D assets with varying \u03c9\nusing the original DreamFusion formulation and a variant\nusing our LMC-SDS. The results in Fig. 9 also show the\ntrend of high \u03c9 having the tendency to produce oversat-\nurated colors, albeit not as as strongly as in the original\npaper, possibly due the use of a different image diffusion\nmodel. But we also observed an additional issue: the infa-\nmous Janus problem, the tendency to repeat detail in each\nview, becomes more severe with bigger \u03c9, as evident from\nthe eagle example. On the other hand, when using low \u03c9 in\nthe original SDS loss, we observe that DreamFusion strug-\ngles to create detailed structure or even structure at all. In\n7\n<latexit sha1_base64=\"5u/XV2KLNI5+pNi7KQbFZNlA=\">AEjHicdZPbtQwEMbdNkBZoLRw5BKxQuK0SlD5I6FKFUiIA4dSsW2lTVQ5zmTjrmNbtOyWHkNrvBavA3OJgu7ydZSlNF8v28ymWQSyag2QfBna3vHu3P3u79wYOHj/Ye7x8OdOiVATGRDChLhKsgVEOY0MNgwupABcJg/Nk9rHWz69BaSr4NzOXEBd4ymlGCTYuFUXTkqaYEzgKg8v9YTAKFsfvB2EbDF7Ti4Pdq6iVJCyAG4Iw1pPwkCa2GJlKGFQDaJSg8RkhqcwcSHBejYLpqu/Bcuk/qZUO7ixl9kVx0WF1rPi8SRBTa57mp1cpM2KU32LraUy9IAJ82DspL5Rvj1BPyUKiCGzV2AiaKuV5/kWGFi3JwGg0gBhxsigLz1EbXQKpJGNsoESyt+xHMDsPKvdwq5bpoqLqdJNtAaFghCN5U5LRq/Yk9dq6yITW1X/3l651m+ATnPjKOZ+gBR3S1D37aGpYX90/SlwQd0XWUK5g1pHhySCp9ViMHbeq0KzrBApsKYV3XKRzGkXdWNOQW8Q5ocDO6yM1C8X1P3aoL8p7qQMsFvbBq6FvU5RSa+zpUd2A6ueUaOYsoYNobfUKxcyUdF5ZSie+NVg+8wdYZR1xVlZuO8PuLvaDs1ej8M3o9dfD4fGHdk930TP0HL1EIXqLjtFndILGiCJfqJf6Le35x1672jBt3eaj1P0drxPv0F5FybBw=</latexit>\u03c9 = 10\n<latexit sha1_base64=\"JmRpWncR8bJMUzh/qvSC6bY8gEY=\">AEjHicdZNRb9MwEMe9LbBRYGzwyEtEhcRTlUwbIE2TJpAQDzyMiW6TmpynEvj1bEt29koVr4Gr/C1+DY4TQpt0lmKcr/73+5XHKxZFSbIPizsbnlPXi4vfOo9/jJ091ne/vPL7QoFIEhEUyoqxhrYJTD0FD4EoqwHnM4DKefqz0y1tQmgr+zcwkjHM84TSlBuXiqJQRPMCZwcBNd7/WAQzI/fDcIm6KPmnF3vb91EiSBFDtwQhrUehYE0Y4uVoYRB2YsKDRKTKZ7AyIUc56DHdt506b92mcRPhXIXN/48u+ywONd6lseOzLHJdFurku0UWHS92NLuSwMcFI/KC2Yb4RfTcBPqAJi2MwFmCjqevVJhUmxs2p14sUcLgjIs8xT2x0C6QchWMbxYIlVT+C2X5YupdbplwXNVW1E6drCA1LBMHripyXjT+2505bFZnQuvzv/tI2V/od0ElmHMXcD5Dgdgnqvj3UNeyPtj8BLqj7Igsoc1DjaJFE8KScD8bOlVomuYiAVa3ohsukhlto27MCaiUt0iTgcFtdgqKd2vqTk2Q/1QXUib4vQ2WNX2PuphCfV+Fqg5MK7dYI2cROUw6o48pdq645cJSKvG91qB19gq4ib0srSbWfY3sVucHEwCN8Ojr4e9k8/NHu6g16iV+gNCtE7dIo+ozM0RARJ9BP9Qr+9Xe/QO/ZOanRzo/G8QCvH+/QX6HqbCA=</latexit>\u03c9 = 20\n<latexit sha1_base64=\"4v4CMB14UYzgDXNA9QOCKpWVXpw=\">AEjHicdZNRb9MwEMe9LcAoMDZ45CWiQuKpSmCDSdOkCSTEAw9jotukpoc+9J4dWzLdjZKlK/BK3wtvg1Ok0KbdJainO7/+18ul1ysODM2CP5sbG59+4/2H7Ye/T4yc7T3b1n50bmsCQSC71ZYwNcCZgaJnlcKk04CzmcBFP1b6xQ1ow6T4ZmcKxhmeCJYwgq1LRdEkZxQLAsdvg6vdfjAI5sfvBmET9FzTq/2tq4jKkmegbCEY2NGYaDsuMDaMsKh7EW5AYXJFE9g5EKBMzDjYt506b9yGeonUrtLWH+eXYUODNmlsWOzLBNTVurku0UW6Tw3HBhMotCFI/KMm5b6VfTcCnTAOxfOYCTDRzvfokxRoT6+bU60UaBNwSmWVY0CK6AVKOwnERxZLTqh/Ji35YupdbplwXNVW1EydrCANLBMHripyVjT8uzpy2KnJpTPnf/aVtrvRbYJPUOoq7H4Didgnmvj3UNYofbT8FIZn7IgsodVDjaJFEClrOB1PMOlVYkmSAq9bMQ0XqZS1UTdmCjoRLdKmYHGbnYIW3ZqmUxPUP9WFjEtxZ4NlTd+hLqZQ31ehqgPbyi3WyFlkBpPO6GOGnStubBSWn6vtWrgNbKOK6LFTptjNs72I3OH8zCN8NDr7u908+NHu6jV6gl+g1CtF7dI+o1M0RAQp9BP9Qr+9HW/fO/KOa3Rzo/E8RyvH+/QX7JibCQ=</latexit>\u03c9 = 30\n<latexit sha1_base64=\"5QC69IgaI6a4btnMcn+xEN/iDM=\">AEjXicdZPbtQwEMbdNkBZ/rSFI5eIFRKnVYJo4QCogMcOJSKbStospxJht3HdvYTsS5Tm4wmPxNjibLOw6raUo/l+32QySUW2C4M/G5pZ36/ad7buDe/cfPNzZ3Xt0okWpCIyJYEKdJVgDoxzGhoGZ1IBLhIGp8nsQ6OfXoLSVPCvZi4hLvCU04wSbGwqjqYlTEn8DYMgvPdYTAKFsfvB2EXDF3js73ti6iVJCyAG4Iw1pPwkCauMLKUMKgHkSlBonJDE9hYkOC9Bxtei69p/ZTOpnQtmLG3+RXVUuNB6XiSWLDJtas1yeu0SWmy13FuSwNcNI+KCuZb4TfjMBPqQJi2NwGmChqe/VJjhUmxg5qMIgUcLgioigwT6voEkg9CeMqSgRLm34Eq4ZhbV9ulbJdtFTpJdQ2hYIQi+rshx3fmT6thq6yITWtf/3Z9dc6NfAZ3mxlLM/gEpdktQ+/GhrVH9cP0pcEHtF1lCuYU6h0MSwdN6MZhq3qtCs6wQKbC2Fd1xkcypi9oxp6Ay7pAmB4NdgaK92vqXk2Q/1QbUib4jQ3WLX2DupxCe1+Hmg6Mk1vukbWIAqa90ScUW1fiuLCUSnxvtWbgLbOWOKirmRtzN0d7EfnLwYhQej/S8vh4fvuz3dRk/QU/QchegVOkSf0BEaI4K+oZ/oF/rt7Xj73hvXYtubnSex2jteB/AtWom0E=</latexit>\u03c9 = 100\n<latexit sha1_base64=\"lag0GBz7e2c3FVuCm9ObpGMfGo=\">AEi3icdZPbtNAEMa3raElUGjhyMUiQuIU2aVQhECqQEgcOJSKtJViq1qvx/GS/WN21y3B8nNwhcfibVjHDiSbdCUro/l+3g8m0kKRrUJgj8bm1verdvbO3d6d+/t3n+wt/wTMtSERgSyaS6SLAGRgUMDTUMLgoFmCcMzpPJ+0Y/vwKlqRfzLSAmOxoBkl2NhUHI1LmJB4O1BEFzu9YNBMDv+ahB2QR915+RyfyuPUklKDsIQhrUehUFh4gorQwmDuheVGgpMJngMIxsKzEH1azr2n9qM6mfSWUfYfxZdtFRYa71lCeW5Njk2tWa5DptVJrsVxRUZQGBGlflJXMN9JvRuCnVAExbGoDTBS1vfokxwoTYwfVixQIuCaScyzSKroCUo/CuIoSydKmHcmqfljb1ukbBMt1XSTZGsIDQsEweuKnNadP6lOrbYsMql1/d/9yTU3+jXQcW4sxewfIMUOQe3VQ1ui+uHaUxCS2vuYQ7mFOodDEinSejaXarpShWYZlymwthPdcVGRUxe1U05BZcIhTQ4Gu+wElFitqVdqQvFPtSFlUtzYN3SN6jzKbS/y1DTgXFy8y2yFslhjN3LSyi2rsRx4aJQ8nurNQNvsWXGEl/rqjtbobuJq4GZweD8OXgxefD/vG7bkt30GP0BD1DITpCx+gjOkFDRNA39BP9Qr+9Xe+59p706KbG53nEVo63oe/Sx6bIg=</latexit>\u03c9 = 200\n<latexit sha1_base64=\"F805QbB+dnp5upGKJobVGtQZcVs=\">AElHicdZNdb9MwFIa9LcAoX92QuOEmokLiqmrQ+LjgYmKAuEBoTHSb1FSV45w0Xh07sp2txcpf4Rb+Ev8Gu8mgTpLUY7O+7zHJyd2lDOq9GDwZ2t7x7t1+87u3c69+w8ePuru7Z8qUgCQyKYkOcRVsAoh6GmsF5LgFnEYOzaHbk9LNLkIoK/l0vchneMpQgnWNjXp7oca5tp8cJ5PhcPKSbc36A+Wy28HQR30UL2OJ3s7F2EsSJEB14RhpUbBINdjg6WmhEHZCQsFOSYzPIWRDTnOQI3NsvnSf24zsZ8IaR+u/WV21WFwptQiyZYZ2qpuaSm7RoZO3Y0N5XmjgpNoKZivhe8m4cdUAtFsYQNMJLW9+iTFEhNt59XphBI4XBGRZjHJrwEUo6CsQkjwWLXj2CmF5T241Yp20VFuXaiZAOhYIUgeFORk7L2R+bEausiE0qV/91fmanXwGdptpSzB6EGDdLUHsGoKphfjT9MXB7R+5hlIL1Y4GSQSPy+VgzKJVhSZJmJgVSuq5sI8pU3UjkGmfAGqVPQuMnOQPJ2TdWqCfk/1YaU2VN9U4NlRd+gXk+heq9DrgPdyE0LGmNOnEVkMG2NPqLYuqKGC+e5FPNKcwOvsHXGEhelyd3tDJp3sR2cvuwHr/uvh30Dt/X93QXPUXP0AsUoDfoEH1Gx2iICJqjn+gX+u098d5R97HCt3eqj2P0dryv4FDgOesA=</latexit>DreamFusion\n<latexit sha1_base64=\"9nPG0ZoCajg0stJoW/isGFQBHw=\">AElnicdZNdb9MwFIa9LcAoXx1wgcRNRIU0bqoG8XWFJhCyTKRLdJTVU5zknj1bEt29nWfkv3MI/4t/gNBm0SWep6tF5n/fk7WkcSUa1GQz+bG3veDdu3tq93blz979B929h0da5IrAiAgm1EmENTDKYWSoYXAiFeAsYnAczT+W+vEZKE0F/2EWEiYZnGaUIKNa027j0MDF8Z+y5X29wNfG5Avim3N+gPlsdvF0Fd9FB9htO9ndMwFiTPgBvCsNbjYCDNxGJlKGFQdMJcg8RkjmcwdiXHGeiJXcYv/OeuE/uJUO7Djb/srjoszrReZJEjM2xS3dTK5iZtnJvk3cRSLnMDnFQPSnLmG+GXu/BjqoAYtnAFJoq6rD5JscLEuI1OqECDudEZBnmsQ3PgBTjYGLDSLC4zCOY7QWF+3GrlEtRUWcKNlAaFghCN405LCo/ZE9dNq6yITWxX/316a51M+BzlLjKOZehRg3R1D3FkA1w142/TFwQd0/cgWlDqodDZIHhfLxdhFawpNkzEwKouZCmdIm6tYcg0p4gzQpGNxk56B4e6ZuzQT5T3UlZYJfG7Co6GvUqy1U3+tQmcA0erOcxpiT0iIymLVWH1HsXFHDhaVU4qLSyoVX2DrjiNPCyvJ2Bs272C6OXvaDN/3X31/1Dj7U93QXPUXP0D4K0Ft0gL6gIRohgi7RT/QL/faeO+9T97nCt3eqj2P0Nrxhn8BiP2ejg=</latexit>Ours (1 step)\n<latexit sha1_base64=\"9nPG0ZoCajg0stJoW/isGFQBHw=\">AElnicdZNdb9MwFIa9LcAoXx1wgcRNRIU0bqoG8XWFJhCyTKRLdJTVU5zknj1bEt29nWfkv3MI/4t/gNBm0SWep6tF5n/fk7WkcSUa1GQz+bG3veDdu3tq93blz979B929h0da5IrAiAgm1EmENTDKYWSoYXAiFeAsYnAczT+W+vEZKE0F/2EWEiYZnGaUIKNa027j0MDF8Z+y5X29wNfG5Avim3N+gPlsdvF0Fd9FB9htO9ndMwFiTPgBvCsNbjYCDNxGJlKGFQdMJcg8RkjmcwdiXHGeiJXcYv/OeuE/uJUO7Djb/srjoszrReZJEjM2xS3dTK5iZtnJvk3cRSLnMDnFQPSnLmG+GXu/BjqoAYtnAFJoq6rD5JscLEuI1OqECDudEZBnmsQ3PgBTjYGLDSLC4zCOY7QWF+3GrlEtRUWcKNlAaFghCN405LCo/ZE9dNq6yITWxX/316a51M+BzlLjKOZehRg3R1D3FkA1w142/TFwQd0/cgWlDqodDZIHhfLxdhFawpNkzEwKouZCmdIm6tYcg0p4gzQpGNxk56B4e6ZuzQT5T3UlZYJfG7Co6GvUqy1U3+tQmcA0erOcxpiT0iIymLVWH1HsXFHDhaVU4qLSyoVX2DrjiNPCyvJ2Bs272C6OXvaDN/3X31/1Dj7U93QXPUXP0D4K0Ft0gL6gIRohgi7RT/QL/faeO+9T97nCt3eqj2P0Nrxhn8BiP2ejg=</latexit>Ours (1 step)\n<latexit sha1_base64=\"F805QbB+dnp5upGKJobVGtQZcVs=\">AElHicdZNdb9MwFIa9LcAoX92QuOEmokLiqmrQ+LjgYmKAuEBoTHSb1FSV45w0Xh07sp2txcpf4Rb+Ev8Gu8mgTpLUY7O+7zHJyd2lDOq9GDwZ2t7x7t1+87u3c69+w8ePuru7Z8qUgCQyKYkOcRVsAoh6GmsF5LgFnEYOzaHbk9LNLkIoK/l0vchneMpQgnWNjXp7oca5tp8cJ5PhcPKSbc36A+Wy28HQR30UL2OJ3s7F2EsSJEB14RhpUbBINdjg6WmhEHZCQsFOSYzPIWRDTnOQI3NsvnSf24zsZ8IaR+u/WV21WFwptQiyZYZ2qpuaSm7RoZO3Y0N5XmjgpNoKZivhe8m4cdUAtFsYQNMJLW9+iTFEhNt59XphBI4XBGRZjHJrwEUo6CsQkjwWLXj2CmF5T241Yp20VFuXaiZAOhYIUgeFORk7L2R+bEausiE0qV/91fmanXwGdptpSzB6EGDdLUHsGoKphfjT9MXB7R+5hlIL1Y4GSQSPy+VgzKJVhSZJmJgVSuq5sI8pU3UjkGmfAGqVPQuMnOQPJ2TdWqCfk/1YaU2VN9U4NlRd+gXk+heq9DrgPdyE0LGmNOnEVkMG2NPqLYuqKGC+e5FPNKcwOvsHXGEhelyd3tDJp3sR2cvuwHr/uvh30Dt/X93QXPUXP0AsUoDfoEH1Gx2iICJqjn+gX+u098d5R97HCt3eqj2P0dryv4FDgOesA=</latexit>DreamFusion\n<latexit sha1_base64=\"+pGM2C54PhiX1qQR6GkHw1lRjw=\">AEjHicdZNRb9MwEMe9LcAoMDZ45CWiQuKpStA2kKZJE0iIBx7GRLdJTU5zqXx6tiW7WwrUb4Gr/C1+DY4TQqt01mKcr/73+5XHKxZFSbIPizsbnlPXj4aPtx78nTZzvPd/denGtRKAJDIphQlzHWwCiHoaGwaVUgPOYwU8/VTrFzegNBX8u5lJGOd4wmlKCTY2FUWTgiaYEzg+CK52+8EgmB+/G4Rt0EftOb3a27qOEkGKHLghDGs9CgNpxiVWhIGVS8qNEhMpngCIxtynIMel/OmK/+NzSR+KpS9uPHn2WVHiXOtZ3lsyRybTLtanVynjQqTfhiXlMvCACfNg9KC+Ub49QT8hCoghs1sgImitlefZFhYuycer1IAYdbIvIc86SMboBUo3BcRrFgSd2PYGU/rOzLVO2i4aq24nTNYSGJYLgdUXOqtYfl2dWxWZ0Lr67/7qmv9FugkM5Zi9gdIsFuC2m8PTY3yh+tPgAtqv8gCyizUOhySCJ5U8GUs04Vmqa5SIA1reiWi2RGXdSOQGVcoc0GRjslNQvFtTd2qC/KfakDLB72wauh71MUmvsqVHdgnNxijaxF5DpjD6m2Lpix4WlVOKu0eqBN9gqY4nrqpSV3c7Q3cVucP5uEB4ODr7t908+tnu6jV6h1+gtCtF7dIK+oFM0RARJ9BP9Qr+9HW/fO/KOG3Rzo/W8RCvH+/wX9NSbCw=</latexit>\u03c9 = 50\n<latexit sha1_base64=\"5u/XV2KLNI5+pNi7KQbFZNlA=\">AEjHicdZPbtQwEMbdNkBZoLRw5BKxQuK0SlD5I6FKFUiIA4dSsW2lTVQ5zmTjrmNbtOyWHkNrvBavA3OJgu7ydZSlNF8v28ymWQSyag2QfBna3vHu3P3u79wYOHj/Ye7x8OdOiVATGRDChLhKsgVEOY0MNgwupABcJg/Nk9rHWz69BaSr4NzOXEBd4ymlGCTYuFUXTkqaYEzgKg8v9YTAKFsfvB2EbDF7Ti4Pdq6iVJCyAG4Iw1pPwkCa2GJlKGFQDaJSg8RkhqcwcSHBejYLpqu/Bcuk/qZUO7ixl9kVx0WF1rPi8SRBTa57mp1cpM2KU32LraUy9IAJ82DspL5Rvj1BPyUKiCGzV2AiaKuV5/kWGFi3JwGg0gBhxsigLz1EbXQKpJGNsoESyt+xHMDsPKvdwq5bpoqLqdJNtAaFghCN5U5LRq/Yk9dq6yITW1X/3l651m+ATnPjKOZ+gBR3S1D37aGpYX90/SlwQd0XWUK5g1pHhySCp9ViMHbeq0KzrBApsKYV3XKRzGkXdWNOQW8Q5ocDO6yM1C8X1P3aoL8p7qQMsFvbBq6FvU5RSa+zpUd2A6ueUaOYsoYNobfUKxcyUdF5ZSie+NVg+8wdYZR1xVlZuO8PuLvaDs1ej8M3o9dfD4fGHdk930TP0HL1EIXqLjtFndILGiCJfqJf6Le35x1672jBt3eaj1P0drxPv0F5FybBw=</latexit>\u03c9 = 10\n<latexit sha1_base64=\"JmRpWncR8bJMUzh/qvSC6bY8gEY=\">AEjHicdZNRb9MwEMe9LbBRYGzwyEtEhcRTlUwbIE2TJpAQDzyMiW6TmpynEvj1bEt29koVr4Gr/C1+DY4TQpt0lmKcr/73+5XHKxZFSbIPizsbnlPXi4vfOo9/jJ091ne/vPL7QoFIEhEUyoqxhrYJTD0FD4EoqwHnM4DKefqz0y1tQmgr+zcwkjHM84TSlBuXiqJQRPMCZwcBNd7/WAQzI/fDcIm6KPmnF3vb91EiSBFDtwQhrUehYE0Y4uVoYRB2YsKDRKTKZ7AyIUc56DHdt506b92mcRPhXIXN/48u+ywONd6lseOzLHJdFurku0UWHS92NLuSwMcFI/KC2Yb4RfTcBPqAJi2MwFmCjqevVJhUmxs2p14sUcLgjIs8xT2x0C6QchWMbxYIlVT+C2X5YupdbplwXNVW1E6drCA1LBMHripyXjT+2505bFZnQuvzv/tI2V/od0ElmHMXcD5Dgdgnqvj3UNeyPtj8BLqj7Igsoc1DjaJFE8KScD8bOlVomuYiAVa3ohsukhlto27MCaiUt0iTgcFtdgqKd2vqTk2Q/1QXUib4vQ2WNX2PuphCfV+Fqg5MK7dYI2cROUw6o48pdq645cJSKvG91qB19gq4ib0srSbWfY3sVucHEwCN8Ojr4e9k8/NHu6g16iV+gNCtE7dIo+ozM0RARJ9BP9Qr+9Xe/QO/ZOanRzo/G8QCvH+/QX6HqbCA=</latexit>\u03c9 = 20\n<latexit sha1_base64=\"4v4CMB14UYzgDXNA9QOCKpWVXpw=\">AEjHicdZNRb9MwEMe9LcAoMDZ45CWiQuKpSmCDSdOkCSTEAw9jotukpoc+9J4dWzLdjZKlK/BK3wtvg1Ok0KbdJainO7/+18ul1ysODM2CP5sbG59+4/2H7Ye/T4yc7T3b1n50bmsCQSC71ZYwNcCZgaJnlcKk04CzmcBFP1b6xQ1ow6T4ZmcKxhmeCJYwgq1LRdEkZxQLAsdvg6vdfjAI5sfvBmET9FzTq/2tq4jKkmegbCEY2NGYaDsuMDaMsKh7EW5AYXJFE9g5EKBMzDjYt506b9yGeonUrtLWH+eXYUODNmlsWOzLBNTVurku0UW6Tw3HBhMotCFI/KMm5b6VfTcCnTAOxfOYCTDRzvfokxRoT6+bU60UaBNwSmWVY0CK6AVKOwnERxZLTqh/Ji35YupdbplwXNVW1EydrCANLBMHripyVjT8uzpy2KnJpTPnf/aVtrvRbYJPUOoq7H4Didgnmvj3UNYofbT8FIZn7IgsodVDjaJFEClrOB1PMOlVYkmSAq9bMQ0XqZS1UTdmCjoRLdKmYHGbnYIW3ZqmUxPUP9WFjEtxZ4NlTd+hLqZQ31ehqgPbyi3WyFlkBpPO6GOGnStubBSWn6vtWrgNbKOK6LFTptjNs72I3OH8zCN8NDr7u908+NHu6jV6gl+g1CtF7dI+o1M0RAQp9BP9Qr+9HW/fO/KOa3Rzo/E8RyvH+/QX7JibCQ=</latexit>\u03c9 = 30\n<latexit sha1_base64=\"5QC69IgaI6a4btnMcn+xEN/iDM=\">AEjXicdZPbtQwEMbdNkBZ/rSFI5eIFRKnVYJo4QCogMcOJSKbStospxJht3HdvYTsS5Tm4wmPxNjibLOw6raUo/l+32QySUW2C4M/G5pZ36/ad7buDe/cfPNzZ3Xt0okWpCIyJYEKdJVgDoxzGhoGZ1IBLhIGp8nsQ6OfXoLSVPCvZi4hLvCU04wSbGwqjqYlTEn8DYMgvPdYTAKFsfvB2EXDF3js73ti6iVJCyAG4Iw1pPwkCauMLKUMKgHkSlBonJDE9hYkOC9Bxtei69p/ZTOpnQtmLG3+RXVUuNB6XiSWLDJtas1yeu0SWmy13FuSwNcNI+KCuZb4TfjMBPqQJi2NwGmChqe/VJjhUmxg5qMIgUcLgioigwT6voEkg9CeMqSgRLm34Eq4ZhbV9ulbJdtFTpJdQ2hYIQi+rshx3fmT6thq6yITWtf/3Z9dc6NfAZ3mxlLM/gEpdktQ+/GhrVH9cP0pcEHtF1lCuYU6h0MSwdN6MZhq3qtCs6wQKbC2Fd1xkcypi9oxp6Ay7pAmB4NdgaK92vqXk2Q/1QbUib4jQ3WLX2DupxCe1+Hmg6Mk1vukbWIAqa90ScUW1fiuLCUSnxvtWbgLbOWOKirmRtzN0d7EfnLwYhQej/S8vh4fvuz3dRk/QU/QchegVOkSf0BEaI4K+oZ/oF/rt7Xj73hvXYtubnSex2jteB/AtWom0E=</latexit>\u03c9 = 100\n<latexit sha1_base64=\"lag0GBz7e2c3FVuCm9ObpGMfGo=\">AEi3icdZPbtNAEMa3raElUGjhyMUiQuIU2aVQhECqQEgcOJSKtJViq1qvx/GS/WN21y3B8nNwhcfibVjHDiSbdCUro/l+3g8m0kKRrUJgj8bm1verdvbO3d6d+/t3n+wt/wTMtSERgSyaS6SLAGRgUMDTUMLgoFmCcMzpPJ+0Y/vwKlqRfzLSAmOxoBkl2NhUHI1LmJB4O1BEFzu9YNBMDv+ahB2QR915+RyfyuPUklKDsIQhrUehUFh4gorQwmDuheVGgpMJngMIxsKzEH1azr2n9qM6mfSWUfYfxZdtFRYa71lCeW5Njk2tWa5DptVJrsVxRUZQGBGlflJXMN9JvRuCnVAExbGoDTBS1vfokxwoTYwfVixQIuCaScyzSKroCUo/CuIoSydKmHcmqfljb1ukbBMt1XSTZGsIDQsEweuKnNadP6lOrbYsMql1/d/9yTU3+jXQcW4sxewfIMUOQe3VQ1ui+uHaUxCS2vuYQ7mFOodDEinSejaXarpShWYZlymwthPdcVGRUxe1U05BZcIhTQ4Gu+wElFitqVdqQvFPtSFlUtzYN3SN6jzKbS/y1DTgXFy8y2yFslhjN3LSyi2rsRx4aJQ8nurNQNvsWXGEl/rqjtbobuJq4GZweD8OXgxefD/vG7bkt30GP0BD1DITpCx+gjOkFDRNA39BP9Qr+9Xe+59p706KbG53nEVo63oe/Sx6bIg=</latexit>\u03c9 = 200\n<latexit sha1_base64=\"+pGM2C54PhiX1qQR6GkHw1lRjw=\">AEjHicdZNRb9MwEMe9LcAoMDZ45CWiQuKpStA2kKZJE0iIBx7GRLdJTU5zqXx6tiW7WwrUb4Gr/C1+DY4TQqt01mKcr/73+5XHKxZFSbIPizsbnlPXj4aPtx78nTZzvPd/denGtRKAJDIphQlzHWwCiHoaGwaVUgPOYwU8/VTrFzegNBX8u5lJGOd4wmlKCTY2FUWTgiaYEzg+CK52+8EgmB+/G4Rt0EftOb3a27qOEkGKHLghDGs9CgNpxiVWhIGVS8qNEhMpngCIxtynIMel/OmK/+NzSR+KpS9uPHn2WVHiXOtZ3lsyRybTLtanVynjQqTfhiXlMvCACfNg9KC+Ub49QT8hCoghs1sgImitlefZFhYuycer1IAYdbIvIc86SMboBUo3BcRrFgSd2PYGU/rOzLVO2i4aq24nTNYSGJYLgdUXOqtYfl2dWxWZ0Lr67/7qmv9FugkM5Zi9gdIsFuC2m8PTY3yh+tPgAtqv8gCyizUOhySCJ5U8GUs04Vmqa5SIA1reiWi2RGXdSOQGVcoc0GRjslNQvFtTd2qC/KfakDLB72wauh71MUmvsqVHdgnNxijaxF5DpjD6m2Lpix4WlVOKu0eqBN9gqY4nrqpSV3c7Q3cVucP5uEB4ODr7t908+tnu6jV6h1+gtCtF7dIK+oFM0RARJ9BP9Qr+9HW/fO/KOG3Rzo/W8RCvH+/wX9NSbCw=</latexit>\u03c9 = 50\n<latexit sha1_base64=\"4tIo6OELIuiw4YwW7qOQKasktdg=\">AEo3icdZPdbtMwFMe9LcAoXxtchNRTeNqahBflxPcIMHFqNZtUhNtjnPSmPpLtrOtWHkAnoZbeBTeBqfJoE07S1GOzv/3Pzk+jlPFqLGDwZ+Nza3gzt172/d7Dx4+evxkZ/fpiZGlJjAikl9lmIDjAoYWoZnCkNmKcMTtPpx1o/vQRtqBTHdqYg4XgiaE4Jtj51vtOPLVxbFwNXhbu4wGHKSgi5tFKTGWGwv19VnhocDOYrXA2iNuijdh2d724VcSZJyUFYwrAx42igbOKwtTXrHpxaUBhMsUTGPtQYA4mcfPdVOGez2RhLrV/hA3n2UWHw9yYGU89ybEtTFerk+u0cWnz94mjQpUWBGk+lJcstDKsRxNmVAOxbOYDTDT1vYakwBoT6wfYizUIuCKScywyF18CqcZR4uJUsqxuRzLXjyq/t0XKN9FQdTdpvoYwsEAQvK7IsGr9qRt6bVlk0pjqv/tL1zrV0AnhfU8z9GhjsE9b8ENCXc9649AyGpP48bqPBQ6+iQRIqsms/FzVaq0DznMgPWdGJaLlYF7aJ+yhnoXHRIW4DFXYKWqzWNCs1Qf1TfUiZFLc2WDX0LerNFJr3MlR3YDu5SUkzLEhtkRwmuHt4KcXelXZcWCktrxutHniDLTOe+FY5Vd/NqHsTV4OTVwfR24M3X1/3Dz+0t3QbPUcv0EsUoXfoEH1CR2iECPqBfqJf6HewF3wOhsFxg25utJ5naGkFyV9auqVz</latexit><a blue motorcycle=\n<latexit sha1_base64=\"QH651iAxv0mT6+kry512LcV1xw=\">AEsXicdZPNbtQwEMfdoFSvrZw5GKxQuVUbShfxwouHDiUin6gzaqd2JONWceObKftYuU5eBqu8Ay8Dc4mhW62tRlNP/fzIex0khXD4Z+V1bXerdt31u9u3Lv/4OGj/ubjQ6tLw/CAanNcQIWpVB4ISTeFwYhDyReJRMP9T60RkaK7T64mYFjnOYKJEKBi6kTvpR7PDC+RjzIvOnp0ATkJwiTCRSBuYMOdWlozql51rzra2qOukPhtvD+aLQdQGA9KuvZPNtSzmpU5KsckWDuKhoUbezBOMInVRlxaLIBNYKjECrI0Y79fG8VfR4ynKbahEc5Os9edXjIrZ3lSBzcJntanXyOm1UuvTd2AtVlA4Vaz6UlpI6TetBUS4MidnIQBmROiVsgwMBfGuREbVHjOdJ6D4j4+Q1aNorGPEy153Y6WfhBVYW9XqdBEQ9XdJOk1hMUrBIPriuxXrT/x+0FbFKW2tvrv/tQ1/o5iknmAiXDb8KhQ4jwg2BTwn/v2jkqLcJ5XEJZgFpHh2Ra8Wo+Fz9bqiLSNcZdOJbm4yEQXDVPmaFLVIV2GDrsFI1armXamLxTw2hkFrd2GDV0Deol1No3otQ3YHr5Cal4KBYbdE5TqB7eImA4Eo6LigKoy8arR54gy0ygfhW+aK+m1H3Ji4Hhy+3ozfbrz+/Guy+b2/pOnlKnpEXJCJvyS75SPbIAWHkB/lJfpHfvZ3e195pL2nQ1ZXW84QsrN70L+TcqoI=</latexit>\n<a bald eagle carved out of wood=\n<latexit sha1_base64=\"Kz0w3I/QnZi+9z8v1aB38KEtpg=\">AEqHicdZPLbtNAFIanrYESbiks2VhEqBWLKkbclhVsWLAIFWmK4igdj4/tIXOxZsZNw8ivwNOwhfgbRjHLiROpLlo/N/GZ45koZ1Sbfv/Pzu6ed+v2nf27nXv3Hzx81D14fKZloQgMiWRSnUdYA6MChoYaBue5AswjBqNo9qHSR5egNJXi1nkMOE4FTShBuXmnaPQgNXxobA8xeXGB/LmUMwleSzKhIfZJhqg4Py3La7fWP+8vlbwZBE/RQswbTg70sjCUpOAhDGNZ6HPRzM7FYGUoYlJ2w0JBjMsMpjF0oMAc9scstlf5zl4n9RCr3COMvs6sOi7nWCx45kmOT6bZWJbdp48Ik7yaWirwIEj9oaRgvpF+NR8/pgqIYQsXYKo67WagcLEuCl2QgUC5kRyjkVsw0sg5TiY2DCSLK7akcz2gtLtbZVyTdRU1U2UbCE0rBAEbytyWjb+yJ46bV1kUuvyv/tT21zpc6BpZhzF3OmIcYug7lxAXcJ+b9vdgZDU/Y9rKHNQ42iRIq4XM7FLjaq0CTh7nCxuhPdcGe0TbqphyDSkSLNBkY3GZnoMRmTb1RE/J/qgspk+LGBsuavkG9nkL9XoeqDkwrlxY0xoJUFskhxe2fF1HsXFHLhfNcyataqwZeY+uMI76VNq/uZtC+iZvB2cvj4M3x68+veifvm1u6j56iZ+gIBegtOkEf0QANEUE/0E/0C/32XngDb+R9rdHdncbzBK0tL/oLdQynXA=</latexit><a wooden rocking chair=\n<latexit sha1_base64=\"4zcAKPRukyT7exUyZgCHgyGJC7g=\">AEunicdZPLbhMxFIbdoBSbiks2YyIUFlFGcRNiEUFGxYsSkUvUiZqPfaZjBvfZHvaBmvehadhC1veBk9mCs0ktRTl6Pzf+b42M40Z9YNh3/W1jeiW7fvbN7dunf/wcNHve3Hh1aVhsABUVyZ4wxb4EzCgWOw7E2gEXG4Sibfqr1o3Mwlin5zc0jAWeSJYzgl1InfTepw4unU9B6MKfnhZsUvBZTMFhxoHGAp+BdYzERs0wjx3mPLYF0zs7VXS6w8Hw/mKl4OkDfqoXsn2xtFShUpBUhHOLZ2lAy1G3tswhc4VFtpaUFjMsUTGIVQYgF27OebrOLnIUPjXJnwky6eZ687PBbWzkQWSIFdYbtanVyljUqXvxt7JnXpQJLmQ3kZNqviemIxZQaIq6fCMDGsngYpsMHEhblupQYkXBAlBJbUp+dAqlEy9mOK3bUdz3kyrs7ToVmioupsX0FYuEYQvKrIftX6M78ftEWRK2ur/+4vXOtX0A4bRcoHu4LxR2ChZsCTQn/vWunIBUL53EFQFqHR2SKEmr+Vz8bKkKy3OhKPCmE9tyqS5YFw1TpmBy2SFdEW5ql52Ckcs17VJN0P/UEDKu5I0NVg19g3o1heZ/Eao7cJ3cpGQUS1JblIAJ7h5exnBwZR0X1tqoy0arB95gi0wgziqv67eZdF/icnD4cpC8Gbz+q/+7F9pZvoKXqGXqAEvUW76DPaQweIoB/oJ/qFfkcfoixi0bRB19dazxO0sCL3Fz1hrwg=</latexit><highly detailed majestic royal tall ship=\nFigure 9. The effect of increasing guidance weight \u03c9 in DreamFusion using the original SDS loss (even rows) and our proposed LMC-SDS\n(odd rows). Using larger \u03c9 saturates colors (chair, motorcycle, ship) and worsens the Janus problem (third wing and claw for the eagle). For\nlow \u03c9, vanilla DreamFusion fails to produce detailed geometry (ship, eagle) or geometry at all (motorcycle), while our approach performs\nwell for \u03c9 = 20, often even for \u03c9 = 10. Notice how our approach constantly produces more detailed results for all \u03c9.\nFigure 10. DreamFusion results using our LMC-SDS loss and\nfixed \u03f5 sampling for the prompts \u201ca mannequin wearing a [hoodie,\nhat]\u201d. Note the diversity in colors, poses, and mannequin styles.\ncontrast, when using LMC-SDS, DreamFusion successfully\nproduces detailed results, even when using up to only one\ntenth of the originally proposed \u03c9. In Fig. 8, we show addi-\ntional side-by-side comparisons of the original SDS-based\nDreamFusion and with LMC-SDS. Using our loss, the re-\nsults are consistently sharper and feature far more detail.\nFinally in Fig. 10, we show that fixed \u03f5 sampling may also\nbe used in 3D asset generation, to obtain diverse results.\n6. Discussion & Conclusion\nWe proposed a novel loss formulation called LMC-SDS.\nLMC-SDS allows to effectively utilize text-to-image diffu-\nsion models as rich image priors in several optimization and\nnetwork training tasks. We derived LMC-SDS by analyz-\ning DreamFusion\u2019s SDS loss and by identifying an inherent\nproblem in its formulation. Concretely, we expressed the\nSDS gradient as the sum of two terms. We identified that\nthe second term Lproj provides deficient gradients and con-\nsequently was weighted down in DreamFusion. We argued\nthat this term, however, should provide gradients towards\nthe learned manifold of natural images associated to the\ndiffusion model. We also provided evidence that the lack\nthereof makes the optimization unstable, thus negatively\nimpacting the fidelity of results. To this end, we proposed\nto model the denoising models\u2019 time step dependent im-\nage corruption. This allowed us to factor these out and ob-\ntain the cleaner gradient \u2207\u03b8Llmc. In extensive experiments,\nwe have demonstrated the effectiveness and versatility of\nour novel LMC-SDS loss. Furthermore, we discussed how\nLMC-SDS differs from other SDS-style losses and demon-\nstrated how its performance is superior both qualitatively\nand quantitatively.\nLimitations.\nLMC-SDS finds its limits where either the\nsignal from the diffusion model is too weak and ambiguous\nor when the optimization goes off the natural image man-\nifold by too much. Concretely, LMC-SDS can only uti-\nlize whatever signal is coming from the diffusion model.\nThis means, that when the diffusion model does not \u201cun-\nderstand\u201d the prompt, LMC-SDS (and all other SDS-based\napproaches) will not be able to provide meaningful gradi-\nents. Secondly, whenever the current optimization state is\ntoo corrupted, LMC-SDS will not be able to correct for that\nand guide the optimization back to the manifold. Never-\ntheless, LMC-SDS is an important first step towards more\nstable and meaningful usage of the inherent image prior as-\nsociated to image diffusion models.\nWe proposed LMC-SDS, a novel SDS-based loss formu-\nlation with cleaner gradient towards the image manifold. In\nthe future, we would like to continue to investigate how the\nmanifold corrective can be further improved. We also want\nto leverage our findings in concrete applications like text-\nto-3D or in image editing and stylization tasks.\n8\nSupplementary Material\nIn this supplementary material, we detail the training of \u02c6b\u03c8\nand the implementation and hyper-parameters of our exper-\niments. We also present additional results.\nA. Implementation Details\nIn all experiments, we set w(t) = 1, we sample t \u223c\nU(0.02, 0.98), and use the Adam optimizer. The only ex-\nceptions are the DreamFusion experiments, where we use\nthe original hyper-parameters.\nTraining \u02c6b\u03c8. We model \u02c6b\u03c8 as a standard U-Net with nine\nlayers with two Conv/ReLU/MaxPool blocks per layer, and\nskip connections from the encoder to the decoder. The first\nlayer contains 32 filters and the number of filters is doubled\nin each layer in the encoder and halved in the decoder. The\noutput is computed with a final Conv layer with tanh acti-\nvation. The time step t is passed via concatenation to each\npixel and encoded using 10-dimensional frequency encod-\ning. \u02c6b\u03c8 has been trained until convergence at \u223c 750K steps,\nusing Lk as the only loss, a batch size of 128, and with a\nlearning rate of 1 \u00d7 10\u22124 decayed by 0.9 every 100K steps.\nImage synthesis. We initialize an image of 512 \u00d7 512px\nresolution with 50% grey. We then optimize for 500 itera-\ntions using LLMC-SDS = Llmc + Lcond with \u03c9 = 8 and using\na learning rate of 0.1 with Cosine decay of 0.03 over 300\nsteps. We scale the whole image to 128\u00d7128px for the first\n150 steps and then switch to our patching strategy, where\nwe sample two patches in each iteration.\nImage editing.\nWe initialize an optimizable image of\n512\u00d7512px resolution with pixel values from the reference\nimage. We then optimize for 500 iterations using LLMC-SDS\nwith \u03c9 = 15 and an L2 loss on the original image weighted\nby 0.1. We use a learning rate of 0.02 with Cosine decay\nof 0.8 over all steps and optimize the whole image for 200\nsteps before switching to patches. The numerical experi-\nment has been conducted on 256 \u00d7 256px images. Also,\nwe didn\u2019t use L2 regularization and thus lowered the initial\nlearning rate to 5 \u00d7 10\u22123.\nImage-to-image translation network.\nWe trained\na\nstandard\nU-Net\nwith\neleven\nlayers\nwith\ntwo\nConv/ReLU/MaxPool blocks per layer, and skip con-\nnections from the encoder to the decoder. The first layer\ncontains 32 filters and the number of filters is doubled until\n256 in each layer in the encoder and halved in last layers\nof the decoder. The class id is a float value encoded using\n10-dimensional frequency encoding and concatenated to\neach pixel.\nThe final Conv layer with tanh activation\noutputs images of 256 \u00d7 256px resolution. We trained the\nnetwork with a learning rate of 5 \u00d7 10\u22125 with a decay of\n0.9 every 50K steps. In the first 20K steps, we use Cosine\nscheduling to increase the weight of LLMC-SDS from 0.1\nto 1.0 and decrease the weight of L2 regularization w.r.t.\n<latexit sha1_base64=\"BJsKqaZDAi1vjRklVZd4pqoiZUA=\">AB83icbVBNS8NAEJ3Ur1q/qh69BIvgqSTi17HoRW8V7Ac0oWy2m3bpZhN2J2IJ/RtePCji1T/jzX/jps1Bqw8GHu/NMDMvSATX6DhfVmlpeWV1rbxe2djc2t6p7u61dZwqylo0FrHqBkQzwSVrIUfBuoliJAoE6wTj69zvPDCleSzvcZIwPyJDyUNOCRrJ85A9YnYrkxSn/WrNqTsz2H+JW5AaFGj2q5/eIKZpxCRSQbTuU6CfkYUcirYtOKlmiWEjsmQ9QyVJGLaz2Y3T+0jowzsMFamJNoz9edERiKtJ1FgOiOCI73o5eJ/Xi/F8NLPeP4Sk3S+KEyFjbGdB2APuGIUxcQhU3t9p0RBShaGKqmBDcxZf/kvZJ3T2vn92d1hpXRxlOIBDOAYXLqABN9CEFlBI4Ale4NVKrWfrzXqft5asYmYfsH6+Aa4KJIl</latexit>Input\n<latexit sha1_base64=\"ihxhq9WAITEDqAyVgUjTg/+b9Y=\">AB+HicbVDLSgNBEJz1GeMjqx69DAbBU9gVox6DXjxGMA9IljA76U2GzD6Y6RXjki/x4kERr36KN/GSbIHTSxoKq6e7yEyk0Os63tbK6tr6xWdgqbu/s7pXs/YOmjlPFocFjGau2zRIEUEDBUpoJwpY6Eto+aObqd96AKVFHN3jOAEvZINIBIzNFLPLnURHjGrOlQjJHrSs8tOxZmBLhM3J2WSo96zv7r9mKchRMgl07rjOgl6GVMouIRJsZtqSBgfsQF0DI1YCNrLZodP6IlR+jSIlakI6Uz9PZGxUOtx6JvOkOFQL3pT8T+vk2Jw5WUiSlKEiM8XBamkGNpCrQvFHCUY0MYV8LcSvmQKcbRZFU0IbiLy+T5lnFvahU787Ltes8jgI5IsfklLjktTILamTBuEkJc/klbxZT9aL9W59zFtXrHzmkPyB9fkDo8mTGA=</latexit>50 steps\n<latexit sha1_base64=\"5H1LaG+Eu/Vz1+0G7XOiTW1S8U=\">AB+XicbVDJSgNBEO2JW4zbqEcvjUHwFGaC2zHoxWMEs0AyhJ5OTdKkZ6G7JhiG/IkXD4p49U+8+Td2kjlo4oOCx3tVNXzEyk0Os63Vhb39jcKm6Xdnb39g/sw6OmjlPFocFjGau2zRIEUEDBUpoJwpY6Eto+aO7md8ag9Iijh5xkoAXskEkAsEZGqln212EJ8yqjkM1QqKnPbvsVJw56Cpxc1ImOeo9+6vbj3kaQoRcMq07rpOglzGFgkuYlrqphoTxERtAx9CIhaC9bH75lJ4ZpU+DWJmKkM7V3xMZC7WehL7pDBkO9bI3E/zOikGN14moiRFiPhiUZBKijGdxUD7QgFHOTGEcSXMrZQPmWIcTVglE4K7/PIqaVYr7lXl8uGiXLvN4yiSE3JKzolLrkmN3JM6aRBOxuSZvJI3K7NerHfrY9FasPKZY/IH1ucPEWGTw=</latexit>200 steps\n<latexit sha1_base64=\"+bVPhoWM7L9EK+1RqDkDCYKuY8g=\">AB+XicbVDLSgNBEJyNrxhfqx69DAbBU9hVox6DXjxGMA9IljA76U2GzD6Y6Q2GJX/ixYMiXv0Tb/6Nk2QPGi1oKq6e7yEyk0Os6XVhZXVvfKG6WtrZ3dvfs/YOmjlPFocFjGau2zRIEUEDBUpoJwpY6Eto+aPbmd8ag9Iijh5wkoAXskEkAsEZGqln212ER8zOqw7VCIme9uyU3HmoH+Jm5MyVHv2Z/dfszTECLkmndcZ0EvYwpFzCtNRNSMj9gAOoZGLATtZfPLp/TEKH0axMpUhHSu/pzIWKj1JPRNZ8hwqJe9mfif10kxuPYyESUpQsQXi4JUozpLAbaFwo4yokhjCthbqV8yBTjaMIqmRDc5Zf/kuZxb2sVO8vyrWbPI4iOSLH5JS45IrUyB2pkwbhZEyeyAt5tTLr2Xqz3hetBSufOS/YH18Axqk1U=</latexit>350 steps\n<latexit sha1_base64=\"nAHng/5JiD65SsiS5OAMdtGzS10=\">AB+XicbVDLSgNBEJyNrxhfqx69DAbBU9gVox6DXjxGMA9IljA76U2GzD6Y6Q2GJX/ixYMiXv0Tb/6Nk2QPmljQUFR1093lJ1JodJxvq7C2vrG5Vdwu7ezu7R/Yh0dNHaeKQ4PHMlZtn2mQIoIGCpTQThSw0JfQ8kd3M781BqVFHD3iJAEvZINIBIzNFLPtrsIT5hVHYdqhERPe3bZqThz0FXi5qRMctR79le3H/M0hAi5ZFp3XCdBL2MKBZcwLXVTDQnjIzaAjqERC0F72fzyKT0zSp8GsTIVIZ2rvycyFmo9CX3TGTIc6mVvJv7ndVIMbrxMREmKEPHFoiCVFGM6i4H2hQKOcmI40qYWykfMsU4mrBKJgR3+eV0ryouFeV6sNluXabx1EkJ+SUnBOXJMauSd10iCcjMkzeSVvVma9WO/Wx6K1YOUzx+QPrM8fFgiTUg=</latexit>500 steps\n<latexit sha1_base64=\"ihxhq9WAITEDqAyVgUjTg/+b9Y=\">AB+HicbVDLSgNBEJz1GeMjqx69DAbBU9gVox6DXjxGMA9IljA76U2GzD6Y6RXjki/x4kERr36KN/GSbIHTSxoKq6e7yEyk0Os63tbK6tr6xWdgqbu/s7pXs/YOmjlPFocFjGau2zRIEUEDBUpoJwpY6Eto+aObqd96AKVFHN3jOAEvZINIBIzNFLPLnURHjGrOlQjJHrSs8tOxZmBLhM3J2WSo96zv7r9mKchRMgl07rjOgl6GVMouIRJsZtqSBgfsQF0DI1YCNrLZodP6IlR+jSIlakI6Uz9PZGxUOtx6JvOkOFQL3pT8T+vk2Jw5WUiSlKEiM8XBamkGNpCrQvFHCUY0MYV8LcSvmQKcbRZFU0IbiLy+T5lnFvahU787Ltes8jgI5IsfklLjktTILamTBuEkJc/klbxZT9aL9W59zFtXrHzmkPyB9fkDo8mTGA=</latexit>50 steps\n<latexit sha1_base64=\"5H1LaG+Eu/Vz1+0G7XOiTW1S8U=\">AB+XicbVDJSgNBEO2JW4zbqEcvjUHwFGaC2zHoxWMEs0AyhJ5OTdKkZ6G7JhiG/IkXD4p49U+8+Td2kjlo4oOCx3tVNXzEyk0Os63Vhb39jcKm6Xdnb39g/sw6OmjlPFocFjGau2zRIEUEDBUpoJwpY6Eto+aO7md8ag9Iijh5xkoAXskEkAsEZGqln212EJ8yqjkM1QqKnPbvsVJw56Cpxc1ImOeo9+6vbj3kaQoRcMq07rpOglzGFgkuYlrqphoTxERtAx9CIhaC9bH75lJ4ZpU+DWJmKkM7V3xMZC7WehL7pDBkO9bI3E/zOikGN14moiRFiPhiUZBKijGdxUD7QgFHOTGEcSXMrZQPmWIcTVglE4K7/PIqaVYr7lXl8uGiXLvN4yiSE3JKzolLrkmN3JM6aRBOxuSZvJI3K7NerHfrY9FasPKZY/IH1ucPEWGTw=</latexit>200 steps\n<latexit sha1_base64=\"+bVPhoWM7L9EK+1RqDkDCYKuY8g=\">AB+XicbVDLSgNBEJyNrxhfqx69DAbBU9hVox6DXjxGMA9IljA76U2GzD6Y6Q2GJX/ixYMiXv0Tb/6Nk2QPGi1oKq6e7yEyk0Os6XVhZXVvfKG6WtrZ3dvfs/YOmjlPFocFjGau2zRIEUEDBUpoJwpY6Eto+aPbmd8ag9Iijh5wkoAXskEkAsEZGqln212ER8zOqw7VCIme9uyU3HmoH+Jm5MyVHv2Z/dfszTECLkmndcZ0EvYwpFzCtNRNSMj9gAOoZGLATtZfPLp/TEKH0axMpUhHSu/pzIWKj1JPRNZ8hwqJe9mfif10kxuPYyESUpQsQXi4JUozpLAbaFwo4yokhjCthbqV8yBTjaMIqmRDc5Zf/kuZxb2sVO8vyrWbPI4iOSLH5JS45IrUyB2pkwbhZEyeyAt5tTLr2Xqz3hetBSufOS/YH18Axqk1U=</latexit>350 steps\n<latexit sha1_base64=\"nAHng/5JiD65SsiS5OAMdtGzS10=\">AB+XicbVDLSgNBEJyNrxhfqx69DAbBU9gVox6DXjxGMA9IljA76U2GzD6Y6Q2GJX/ixYMiXv0Tb/6Nk2QPmljQUFR1093lJ1JodJxvq7C2vrG5Vdwu7ezu7R/Yh0dNHaeKQ4PHMlZtn2mQIoIGCpTQThSw0JfQ8kd3M781BqVFHD3iJAEvZINIBIzNFLPtrsIT5hVHYdqhERPe3bZqThz0FXi5qRMctR79le3H/M0hAi5ZFp3XCdBL2MKBZcwLXVTDQnjIzaAjqERC0F72fzyKT0zSp8GsTIVIZ2rvycyFmo9CX3TGTIc6mVvJv7ndVIMbrxMREmKEPHFoiCVFGM6i4H2hQKOcmI40qYWykfMsU4mrBKJgR3+eV0ryouFeV6sNluXabx1EkJ+SUnBOXJMauSd10iCcjMkzeSVvVma9WO/Wx6K1YOUzx+QPrM8fFgiTUg=</latexit>500 steps\nFigure 11. Progression of an image optimized using the prompts\n\u201cphoto of a young girl\u201d and \u201cphoto of an old woman\u201d.\nthe original image from 1.0 to 0.1. We trained for 200K\niterations with batches of 32.\nText-to-3D asset generation. We used the original Dream-\nFusion implementation and kept all hyper-parameters.\nMetrics. We report CLIP scores based on the CLIP ViT-\nB-16 model. LPIPS metrics are computed with the original\nAlexNet variant.\nB. Additional Results\nIn Fig. 11, we visualize the progression of optimization-\nbased image editing. Note how intermediate stages are all\nrealistic natural images. Our LMC loss component provides\na gradient towards the natural image manifold. Thus, during\noptimization, the image always stays close to that manifold.\nIn Figs. 15 and 16 we present additional editing results.\nIn contrast to the main paper, we show results for each\nprompt on a number of images. Concretely, we synthe-\nsized photographs of people and edited each of them using\nprompts describing different art styles or materials. Each\nresult respects the corresponding prompt well and yet has\nan individual touch to it. The image statistics are well pre-\nserved and artifacts are rare. The overall quality once more\ndemonstrates the effectiveness of our novel LMC-SDS loss.\nWe report image statistics of the results of all methods in\nthe cats-to-others network training experiment in Fig. 13.\nIn Fig. 12, we repeated the experiment with our LMC-SDS,\nbut used eight classes instead of four. We did not increase\nthe network capacity or fine-tuned hyper-parameters. Nev-\nertheless, the trained network is able to perform convincing\nedits on all eight classes.\nFinally, in Fig. 14 we show additional comparisons of\nDreamFusion results using our novel LMC-SDS and the\nvanilla SDS loss, both using \u03c9 = 20.\n9\nFigure 12. Qualitative results of a cats-to-others image translation network trained on eight classes. First row: Input image. Middle row:\nResults for \u201clion\u201d, \u201cbear\u201d, \u201cdog\u201d, and \u201csquirrel\u201d. Last row: Results for \u201ctiger\u201d, \u201craccoon\u201d, \u201cfox\u201d, and \u201cbadger\u201d.\n<latexit sha1_base64=\"FTLXvXkXwkGW4qVMvdriybEf2Xk=\">ACXicbVDLSsNAFL2pr1pfUZduBovgqibia1l047KCfUATymQ6aYdOHsxMhBi6deOvuHGhiFv/wJ1/46QNqK0HBg7n3Hvn3uPFnElWV9GaWFxaXmlvFpZW9/Y3DK3d1oySgShTRLxSHQ8LClnIW0qpjtxILiwO07Y2ucr9R4VkUXir0pi6AR6EzGcEKy31TOTEWCiGOXICrIaen92P0dGPmvbMqlWzJkDzxC5IFQo0euan049IEtBQEY6l7NpWrNwsn0c4HVecRNIYkxEe0K6mIQ6odLPJWN0oJU+8iOhX6jQRP3dkeFAyjTwdGW+rpz1cvE/r5so/8LNWBgnioZk+pGfcKQilMeC+kxQoniqCSaC6V0RGWKBidLhVXQI9uzJ86R1XLPaqc3J9X6ZRFHGfZgHw7BhnOowzU0oAkEHuAJXuDVeDSejTfjfVpaMoqeXfgD4+MbDk+Z7w=</latexit>\u2202z/\u2202y\n<latexit sha1_base64=\"d4q1pZRsaqdTJgtJkqBjKHCjK1Y=\">AB+XicbVDJSgNBEO2JW4zbqEcvjUHwFGbE7RgUxGMEs0AyhJ5OTdKkZ7G7JhiG/IkXD4p49U+8+Td2kjlo9EHB470qur5iRQaHefLKiwtr6yuFdLG5tb2zv27l5Dx6niUOexjFXLZxqkiKCOAiW0EgUs9CU0/eH1G+OQGkR/c4TsALWT8SgeAMjdS17Q7CI2Y3Ch5SiPh40rXLTsWZgf4lbk7KJEeta392ejFPQ4iQS6Z123US9DKmUHAJk1In1ZAwPmR9aBsasRC0l80un9Ajo/RoECtTEdKZ+nMiY6HW49A3nSHDgV70puJ/XjvF4NLRJSkaN6aLwpSTGm0xhoTyjgKMeGMK6EuZXyAVOMowmrZEJwF1/+SxonFfe8cnZ3Wq5e5XEUyQE5JMfEJRekSm5JjdQJyPyRF7Iq5VZz9ab9T5vLVj5zD75BevjG1QmlCA=</latexit>Frequency\n<latexit sha1_base64=\"tDAJ1rW1eatzedI4cAMuF+/XPUM=\">ACAnicbVDLSsNAFJ34rPUVdSVuBovgqiTia1kUxGUF+4A2lMn0ph06yYSZibSE4sZfceNCEbd+hTv/xkmbhbYeuHA4517uvcePOVPacb6thcWl5ZXVwlpxfWNza9ve2a0rkUgKNSq4kE2fKOAsgpmkMzlkBCn0PDH1xnfuMBpGIiutejGLyQ9CIWMEq0kTr2flvDUKc3ktBMwCLAMRsCV+OXLKzgR4nrg5KaEc1Y791e4KmoQacqJUi3XibWXEqkZ5TAuthMFMaED0oOWoREJQXnp5IUxPjJKFwdCmo0nqi/J1ISKjUKfdMZEt1Xs14m/ue1Eh1ceimL4kRDRKeLgoRjLXCWB+4yCVTzkSGESmZuxbRPsjhMakUTgjv78jypn5Td8/LZ3WmpcpXHUAH6BAdIxdoAq6RVUQxQ9omf0it6sJ+vFerc+pq0LVj6zh/7A+vwBuoSXqA=</latexit>Fraction of pixels\n<latexit sha1_base64=\"qBPlmV9vwaJAt0Wa9OqsNgLzvqA=\">AB83icbVDLSgNBEJz1GeMr6tHLYBA8hV3xdQx68RjBPC7hNlJxkyO7vM9KphyW948aCIV3/Gm3/jJNmDJhY0FXdHeFiRQGXfbWVpeWV1bL2wUN7e2d3ZLe/sNE6eaQ53HMtatkBmQkEdBUpoJRpYFEpohsObid98AG1ErO5xlEAQsb4SPcEZWsn3EZ4wq8WPoMedUtmtuFPQReLlpExy1DqlL78b8zQChVwyY9qem2CQMY2CSxgX/dRAwviQ9aFtqWIRmCb3jymx1bp0l6sbSmkU/X3RMYiY0ZRaDsjhgMz703E/7x2ir2rIBMqSREUny3qpZJiTCcB0K7QwFGOLGFcC3sr5QOmGUcbU9G4M2/vEgapxXvonJ+d1auXudxFMghOSInxCOXpEpuSY3UCScJeSav5M1JnRfn3fmYtS45+cwB+QPn8weztpIi</latexit>Power\n<latexit sha1_base64=\"TwfDUkJ8ntRK+uVIEiNdE16hY7k=\">ACXicbVC7TsMwFL3hWcorwMhiUSExlQTxGitYGItEH1IbVY7rtFYdJ7IdRIm6svArLAwgxMofsPE3OG0koOVIlo7Ouf63uPHnCntOF/W3PzC4tJyYaW4ura+sWlvbdVlEhCayTikWz6WFHOBK1pjltxpLi0Oe04Q8uM79xS6VikbjRw5h6Ie4JFjCtZE6NmrHWGqGOWqHWPf9IL0focMf9a5jl5yMwaJW5OSpCj2rE/292IJCEVmnCsVMt1Yu2l2TzC6ajYThSNMRngHm0ZKnBIlZeOLxmhfaN0URBJ84RGY/V3R4pDpYahbyqzdW0l4n/ea1EB+deykScaCrI5KMg4UhHKIsFdZmkRPOhIZhIZnZFpI8lJtqEVzQhuNMnz5L6Udk9LZ9cH5cqF3kcBdiFPTgAF86gAldQhRoQeIAneIFX69F6t6s90npnJX37MAfWB/fDMuZ7g=</latexit>\u2202z/\u2202x\n<latexit sha1_base64=\"tDAJ1rW1eatzedI4cAMuF+/XPUM=\">ACAnicbVDLSsNAFJ34rPUVdSVuBovgqiTia1kUxGUF+4A2lMn0ph06yYSZibSE4sZfceNCEbd+hTv/xkmbhbYeuHA4517uvcePOVPacb6thcWl5ZXVwlpxfWNza9ve2a0rkUgKNSq4kE2fKOAsgpmkMzlkBCn0PDH1xnfuMBpGIiutejGLyQ9CIWMEq0kTr2flvDUKc3ktBMwCLAMRsCV+OXLKzgR4nrg5KaEc1Y791e4KmoQacqJUi3XibWXEqkZ5TAuthMFMaED0oOWoREJQXnp5IUxPjJKFwdCmo0nqi/J1ISKjUKfdMZEt1Xs14m/ue1Eh1ceimL4kRDRKeLgoRjLXCWB+4yCVTzkSGESmZuxbRPsjhMakUTgjv78jypn5Td8/LZ3WmpcpXHUAH6BAdIxdoAq6RVUQxQ9omf0it6sJ+vFerc+pq0LVj6zh/7A+vwBuoSXqA=</latexit>Fraction of pixels\nFigure 13. Image statistics of the results of the various methods in the cats-to-others experiment. The statistics correlate well with the\nnumerical and qualitative results reported in the main paper. SDS introduces significant blur to the results, while our method preserves the\noriginal statistics best.\n<latexit sha1_base64=\"6e1W/vuJ9coYpO3N/gME4AsA4vg=\">AB8nicbVDLSgNBEJyNrxhfUY9eFoPgKeyKr2PQizcjmAdsljA76SRDZmeWmV4xLPkMLx4U8erXePNvnCR70GhBQ1HVTXdXlAhu0PO+nMLS8srqWnG9tLG5tb1T3t1rGpVqBg2mhNLtiBoQXEIDOQpoJxpoHAloRaPrqd96AG24kvc4TiCM6UDyPmcUrR0EB4xu021mXTLFa/qzeD+JX5OKiRHvVv+7PQUS2OQyAQ1JvC9BMOMauRMwKTUSQ0klI3oAJLJY3BhNns5Il7ZJWe21falkR3pv6cyGhszDiObGdMcWgWvan4nxek2L8My6TFEGy+aJ+KlxU7vR/t8c1MBRjSyjT3N7qsiHVlKFNqWRD8Bdf/kuaJ1X/vHp2d1qpXeVxFMkBOSTHxCcXpEZuSJ0COKPJEX8uqg8+y8Oe/z1oKTz+yTX3A+vgHyiJG0</latexit>\nOurs\n<latexit sha1_base64=\"M1U+/q+TGjXbfPbwtYTjchIFj9I=\">AB+3icbVDLSsNAFJ3UV62vWJduBovgqiTia1lUxGUF+4A2lMl02g6dTMLMjbSE/IobF4q49Ufc+TdO2iy09cDA4ZxzuXeOHwmuwXG+rcLK6tr6RnGztLW9s7tn75ebOowVZQ0ailC1faKZ4JI1gINg7UgxEviCtfzxTea3npjSPJSPMI2YF5Ch5ANOCRipZ5e7wCaQ3GYzd3EWS3t2xak6M+Bl4uakgnLUe/ZXtx/SOGASqCBad1wnAi8hCjgVLC1Y80iQsdkyDqGShIw7SWz21N8bJQ+HoTKPAl4pv6eSEig9TwTIgMNKLXib+53ViGFx5CZdRDEzS+aJBLDCEOCsC97liFMTUEIVN7diOiKUDB1lUwJ7uKXl0nztOpeVM8fziq167yOIjpER+gEuegS1dA9qMGomiCntErerNS68V6tz7m0YKVzxygP7A+fwCuOJTf</latexit>DreamFusion\n<latexit sha1_base64=\"6e1W/vuJ9coYpO3N/gME4AsA4vg=\">AB8nicbVDLSgNBEJyNrxhfUY9eFoPgKeyKr2PQizcjmAdsljA76SRDZmeWmV4xLPkMLx4U8erXePNvnCR70GhBQ1HVTXdXlAhu0PO+nMLS8srqWnG9tLG5tb1T3t1rGpVqBg2mhNLtiBoQXEIDOQpoJxpoHAloRaPrqd96AG24kvc4TiCM6UDyPmcUrR0EB4xu021mXTLFa/qzeD+JX5OKiRHvVv+7PQUS2OQyAQ1JvC9BMOMauRMwKTUSQ0klI3oAJLJY3BhNns5Il7ZJWe21falkR3pv6cyGhszDiObGdMcWgWvan4nxek2L8My6TFEGy+aJ+KlxU7vR/t8c1MBRjSyjT3N7qsiHVlKFNqWRD8Bdf/kuaJ1X/vHp2d1qpXeVxFMkBOSTHxCcXpEZuSJ0COKPJEX8uqg8+y8Oe/z1oKTz+yTX3A+vgHyiJG0</latexit>\nOurs\n<latexit sha1_base64=\"M1U+/q+TGjXbfPbwtYTjchIFj9I=\">AB+3icbVDLSsNAFJ3UV62vWJduBovgqiTia1lUxGUF+4A2lMl02g6dTMLMjbSE/IobF4q49Ufc+TdO2iy09cDA4ZxzuXeOHwmuwXG+rcLK6tr6RnGztLW9s7tn75ebOowVZQ0ailC1faKZ4JI1gINg7UgxEviCtfzxTea3npjSPJSPMI2YF5Ch5ANOCRipZ5e7wCaQ3GYzd3EWS3t2xak6M+Bl4uakgnLUe/ZXtx/SOGASqCBad1wnAi8hCjgVLC1Y80iQsdkyDqGShIw7SWz21N8bJQ+HoTKPAl4pv6eSEig9TwTIgMNKLXib+53ViGFx5CZdRDEzS+aJBLDCEOCsC97liFMTUEIVN7diOiKUDB1lUwJ7uKXl0nztOpeVM8fziq167yOIjpER+gEuegS1dA9qMGomiCntErerNS68V6tz7m0YKVzxygP7A+fwCuOJTf</latexit>DreamFusion\n<latexit sha1_base64=\"stjdC0ebgzNB8k/dwly6c4kTles=\">ACJnicbVDJSgNBEO2Je9xGPXpDBJPYSbgchGCevDgwS0ayIRY06kTXp6hu4eJQz5Gi/+ihcPiog3P8VOzMGtoJvHe1W8qhcmgmvje9ObmJyanpmdi4/v7C4tOyurF7pOFUMqywWsaqFoFwiVXDjcBaohCiUOB12Dsc6te3qDSP5aXpJ9iIoCN5mzMwlmq6+4GMuWyhNAGzHyouOwFGSTe7uQF6dHFyThlEqIDecdOlQAWoDlKBUheLg6Zb8EreqOhf4I9BgYzrtOk+B62YpZG1YgK0rvteYhoZKMOZwE+SDUmwHrQwbqF0lrRjY6c0A3LdOi7VjZJw0dsd8nMoi07keh7YzAdPVvbUj+p9VT095rZFwmqUHJvozaqaAmpsPMaIsrZEb0LQCmuN2Vsi4oYDYvnbch+L9P/guyiV/p7R9Vi5UDsZxzJ1skG2iE92SYUck1NSJYzck0fyTF6cB+fJeXevlpznhmjfwo5+MTONGlkA=</latexit>\n<a DSLR camera with a large lens=\n<latexit sha1_base64=\"cH8V5z4j6Of4oictHPcgDTfLJWQ=\">ACInicbVC5TgMxFPRyE64AJY1FhKCKdpG4OgQNJUgEkJIovPW+ZS18rGwvKFrlW2j4FRoKEFAh8TE4IQXVKOZ9zPE+eCWxeG78HI6Nj4xOTUdGVmdm5+obq4dGZ1YRg2mBbaXMRgUXCFDcedwIvcIMhY4Hl8fdj3z2/QWK7Vqevm2JZwpXjKGTgvdap7LaW5SlC5Fso8Ky8vM1BJDlw5TOgtODSDECox4XgDgma6sLi+3utUa2E9HID+JdGQ1MgQx53qayvRrJA+iwmwthmFuWuXYBxnAnuVln83B3YNV9j0VIFE2y4H6T265pWEpv6QVCtHB+r3jRKktV0Z+0kJLrO/vb74n9csXLrbLrnKC4eKfQWlhaBO035fNOEGmRNdT4AZ7m+lLAMDzDdjK76E6PeX/5KzXq0Xd862aztHwzrmCIrZJVskIjskH1yRI5JgzByRx7IE3kO7oPH4CV4+xodCY7y+QHgo9PKilQ=</latexit><handpainted watercolor medieval house=\n<latexit sha1_base64=\"ISWX/PENqr8VEoMYVoqJ8MHORmM=\">ACN3icbVBNT9tAEF2HQiEUSOmRy6oRoqfIjgT0iNoLJ0QlAkixlYzXY3vFfli76LIyr/qpX+DG1w4tKp65R+wgRxo6Ei7enpvRm/mpZXg1oXhbdBaerO8nZ1rb3+bmNzq/N+9zq2jAcMC20uUzBouAKB47gZeVQZCpwIv06utMv/iOxnKtztykwkRCoXjOGThPjTonsdJcZahczPyHhqsiRlmVzXgMVNYFzbkQmNFr7kpakdZqb0tOKSgMirB2FKCEPra7u1NR51u2Aufir4G0Rx0ybxOR52bONOslt6bCbB2GIWVSxowjOB03ZcW6yAXUGBQw8VSLRJ83T3lO56JqO5Nv4pv9mMfTnRgLR2IlPfKcGVdlGbkf/ThrXLPycNV1XtULFno7wW1Gk6C5Fm3CBzYuIBMP9rj4WMB8gLbtQ4gWT34Nzvu96KC3/63fPfoyj2OV7JCP5BOJyCE5IsfklAwIz/IHflFfgc/g/vgT/D3ubUVzGc+kH8qeHgExJutoQ=</latexit>\n<a mug \u6600\u0600lled with hot chocolate and marshmallows=\n<latexit sha1_base64=\"wLHNhP79yYRb/q+6ntGw4cBiu+w=\">ACMXicbVDJSgNBEO1xN25Rj14ag+gpzAguR9GLxwhGhUyINT2VpLGXobtHiUN+yYt/Il5yUMSrP2En5uBW0M3jvSpe1Usywa0Lw0EwMTk1PTM7N19aWFxaXimvrl1YnRuGdaFNlcJWBRcYd1xJ/AqMwgyEXiZ3JwM9ctbNJZrde56GTYldBRvcwbOU63yaw0VykqFzP/oeGqE6PMusX1NdA7fg8mpW2jJQV6y1PUtAMSKQPrfKcnbYZCbG/3W+VKWA1HRf+CaAwqZFy1VvkpTjXLpXdlAqxtRGHmgUYx5nAfinOLWbAbqCDQ+Vt7XNYnRxn25xu+ljX/K0RH7faIAaW1PJr5Tguva39qQ/E9r5K592Cy4ynKHin0ZtXNBnabD+GjKDTIneh4AM9zvSlkXDAfnS35EKLfJ/8F7vVaL+6d7ZbOToexzFHNsgm2SEROSBH5JTUSJ0w8kCeyQt5DR6DQfAWvH+1TgTjmXyo4KPT9plqoc=</latexit>\n<a wizard from a video game casting a spell=\nFigure 14. Additonal results of DreamFusion using the original loss formulation and our proposed LMC-SDS (Ours) for \u03c9 = 20. Results\nobtained using our LMC-SDS are much sharper and contain more detail in all examples.\n10\nMaterials\nInput\n\u201cmarble\u201d\n\u201cwood\u201d\n\u201cclay\u201d\nInput\n\u201cmarble\u201d\n\u201cwood\u201d\n\u201cclay\u201d\nDigital Art\nInput\n\u201c3D animation\u201d\n\u201ccyberpunk\u201d\n\u201cdigital painting\u201d\nInput\n\u201c3D animation\u201d\n\u201ccyberpunk\u201d\n\u201cdigital painting\u201d\nFigure 15. Optimization-based editing results for prompts describing different materials and digital art genres. The full prompts are \u201ca\nmarble statue, made from marble, stone\u201d, \u201ca wood sculpture, wooden statue, made from wood\u201d, \u201cclay figure, made from modelling clay\u201d,\n\u201ca character from a 3D animation movie\u201d, \u201ccyberpunk, futuristic, dystopian\u201d, and \u201csci-fi, digital painting\u201d.\n11\nFamous Artists\nInput\n\u201cvan Gogh\u201d\n\u201cRenoir\u201d\n\u201cVermeer\u201d\nInput\n\u201cvan Gogh\u201d\n\u201cRenoir\u201d\n\u201cVermeer\u201d\nArt Techniques\nInput\n\u201caquarelle\u201d\n\u201coil paiting\u201d\n\u201cpencil sketch\u201d\nInput\n\u201caquarelle\u201d\n\u201coil paiting\u201d\n\u201cpencil sketch\u201d\nFigure 16. Optimization-based editing results for prompts describing artists and art techniques. The full prompts are \u201cart by Vincent van\nGogh\u201d, \u201cart by Pierre-Auguste Renoir\u201d, \u201cart by Johannes Vermeer\u201d, \u201caquarelle painting\u201d, \u201coil painting\u201d, and \u201cpencil sketch\u201d.\n12\nReferences\n[1] Mohammadreza\nArmandpour,\nHuangjie\nZheng,\nAli\nSadeghian, Amir Sadeghian, and Mingyuan Zhou.\nRe-\nimagine the negative prompt algorithm:\nTransform 2d\ndiffusion into 3d, alleviate janus problem and beyond. arXiv\npreprint arXiv:2304.04968, 2023. 3\n[2] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nIn IEEE Conf. Comput. Vis. Pattern Recog., 2023. 6\n[3] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha.\nStargan v2: Diverse image synthesis for multiple domains.\nIn IEEE Conf. Comput. Vis. Pattern Recog., 2020. 7\n[4] Hyungjin Chung, Jeongsol Kim, Michael Thompson Mc-\ncann, Marc Louis Klasky, and Jong Chul Ye. Diffusion pos-\nterior sampling for general noisy inverse problems. In Int.\nConf. Learn. Represent., 2022. 1, 2\n[5] Hyungjin Chung,\nByeongsu Sim,\nDohoon Ryu,\nand\nJong Chul Ye. Improving diffusion models for inverse prob-\nlems using manifold constraints. Adv. Neural Inform. Pro-\ncess. Syst., 35:25683\u201325696, 2022. 1, 2\n[6] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu,\nand Mubarak Shah. Diffusion models in vision: A survey.\nIEEE Trans. Pattern Anal. Mach. Intell., 2023. 2\n[7] Sicheng Gao, Xuhui Liu, Bohan Zeng, Sheng Xu, Yan-\njing Li, Xiaoyan Luo, Jianzhuang Liu, Xiantong Zhen, and\nBaochang Zhang. Implicit diffusion models for continuous\nsuper-resolution. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pages\n10021\u201310030, 2023. 1\n[8] Amir Hertz, Kfir Aberman, and Daniel Cohen-Or. Delta de-\nnoising score. In Int. Conf. Comput. Vis., 2023. 1, 2, 3, 5,\n7\n[9] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,\nand Yejin Choi. Clipscore: A reference-free evaluation met-\nric for image captioning. arXiv preprint arXiv:2104.08718,\n2021. 5\n[10] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. In NeurIPS 2021 Workshop on Deep Generative\nModels and Downstream Applications, 2021. 3\n[11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Adv. Neural Inform. Process.\nSyst., 33:6840\u20136851, 2020. 1, 2\n[12] Ajay Jain, Amber Xie, and Pieter Abbeel.\nVectorfusion:\nText-to-svg by abstracting pixel-based diffusion models. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 1911\u20131920, 2023. 1\n[13] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming\nSong. Denoising diffusion restoration models. In Adv. Neu-\nral Inform. Process. Syst., 2022. 1, 2\n[14] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen\nChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:\nText-based real image editing with diffusion models.\nIn\nIEEE Conf. Comput. Vis. Pattern Recog., pages 6007\u20136017,\n2023. 2\n[15] Subin Kim, Kyungmin Lee, June Suk Choi, Jongheon Jeong,\nKihyuk Sohn, and Jinwoo Shin.\nCollaborative score dis-\ntillation for consistent visual synthesis.\narXiv preprint\narXiv:2307.04787, 2023. 2\n[16] Nikos Kolotouros, Thiemo Alldieck, Andrei Zanfir, Ed-\nuard Gabriel Bazavan, Mihai Fieraru, and Cristian Sminchis-\nescu. Dreamhuman: Animatable 3d avatars from text. In\nAdv. Neural Inform. Process. Syst., 2023. 1\n[17] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-\njlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan\nPopov, Matteo Malloci, Alexander Kolesnikov, Tom Duerig,\nand Vittorio Ferrari. The open images dataset v4: Unified\nimage classification, object detection, and visual relationship\ndetection at scale. Int. J. Comput. Vis., 2020. 4\n[18] Xin Li, Yulin Ren, Xin Jin, Cuiling Lan, Xingrui Wang,\nWenjun Zeng, Xinchao Wang, and Zhibo Chen. Diffusion\nmodels for image restoration and enhancement\u2013a compre-\nhensive survey. arXiv preprint arXiv:2308.09388, 2023. 2\n[19] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\ntext-to-3d content creation. In IEEE Conf. Comput. Vis. Pat-\ntern Recog., pages 300\u2013309, 2023. 1\n[20] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang.\nCommon diffusion noise schedules and sample steps are\nflawed. arXiv preprint arXiv:2305.08891, 2023. 3\n[21] Qiang Liu and Dilin Wang. Stein variational gradient de-\nscent: A general purpose bayesian inference algorithm. Ad-\nvances in neural information processing systems, 29, 2016.\n2\n[22] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot one image to 3d object. In Int. Conf. Comput. Vis.,\npages 9298\u20139309, 2023. 1\n[23] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher\nYu, Radu Timofte, and Luc Van Gool.\nRepaint: Inpaint-\ning using denoising diffusion probabilistic models. In IEEE\nConf. Comput. Vis. Pattern Recog., pages 11461\u201311471,\n2022. 1, 2\n[24] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-\njun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided\nimage synthesis and editing with stochastic differential equa-\ntions. In Int. Conf. Learn. Represent., 2022. 1, 2\n[25] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and\nDaniel Cohen-Or. Latent-nerf for shape-guided generation of\n3d shapes and textures. In IEEE Conf. Comput. Vis. Pattern\nRecog., pages 12663\u201312673, 2023. 1\n[26] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. In Int. Conf.\non Mach. Learn., 2022. 1\n[27] Dong Huk Park, Samaneh Azadi, Xihui Liu, Trevor Darrell,\nand Anna Rohrbach.\nBenchmark for compositional text-\nto-image synthesis. In Conf. on Neural Inf. Proc. Systems\nDatasets and Benchmarks Track (Round 1), 2021. 5\n[28] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall. Dreamfusion: Text-to-3d using 2d diffusion. In Int.\nConf. Learn. Represent., 2022. 1, 2, 3\n13\n[29] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022. 1\n[30] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In IEEE Conf. Comput.\nVis. Pattern Recog., pages 10684\u201310695, 2022. 1, 5\n[31] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration.\nIn IEEE Conf. Comput. Vis. Pattern Recog.,\npages 22500\u201322510, 2023. 1\n[32] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,\nJonathan Ho, Tim Salimans, David Fleet, and Mohammad\nNorouzi.\nPalette: Image-to-image diffusion models.\nIn\nACM SIGGRAPH 2022 Conference Proceedings, pages 1\u2013\n10, 2022. 2\n[33] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding. Adv. Neural Inform. Process. Syst.,\n35:36479\u201336494, 2022. 1, 4\n[34] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li,\nand Xiao Yang. Mvdream: Multi-view diffusion for 3d gen-\neration. arXiv:2308.16512, 2023. 3\n[35] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. In Int. Conf. Learn. Repre-\nsent., 2020. 4\n[36] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,\nand Greg Shakhnarovich. Score jacobian chaining: Lifting\npretrained 2d diffusion models for 3d generation. In IEEE\nConf. Comput. Vis. Pattern Recog., pages 12619\u201312629,\n2023. 1, 2\n[37] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and\ndiverse text-to-3d generation with variational score distilla-\ntion. arXiv preprint arXiv:2305.16213, 2023. 2, 3\n[38] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In IEEE Conf. Comput. Vis.\nPattern Recog., 2018. 5\n[39] Zhizhuo Zhou and Shubham Tulsiani. Sparsefusion: Dis-\ntilling view-conditioned diffusion for 3d reconstruction. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 12588\u201312597, 2023. 2,\n3, 5\n[40] Zi-Xin Zou, Weihao Cheng, Yan-Pei Cao, Shi-Sheng Huang,\nYing Shan, and Song-Hai Zhang.\nSparse3d:\nDistill-\ning multiview-consistent diffusion for object reconstruction\nfrom sparse views. arXiv preprint arXiv:2308.14078, 2023.\n2\n14\n"
  }
]