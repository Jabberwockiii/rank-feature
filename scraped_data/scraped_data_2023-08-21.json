[
  {
    "title": "TeCH: Text-guided Reconstruction of Lifelike Clothed Humans",
    "link": "https://arxiv.org/pdf/2308.08545.pdf",
    "upvote": "30",
    "text": "TeCH: Text-guided Reconstruction of Lifelike Clothed Humans\nYangyi Huang1\u2217, Hongwei Yi2\u2217, Yuliang Xiu2\u2217, Tingting Liao3, Jiaxiang Tang4, Deng Cai1, Justus Thies2\n1State Key Lab of CAD & CG, Zhejiang University\n2Max Planck Institute for Intelligent Systems\n3Mohamed bin Zayed University of Artificial Intelligence\n4Peking University\nhuangyangyi@zju.edu.cn, {hongwei.yi, yuliang.xiu, justus.thies}@tuebingen.mpg.de\ntingting.liao@mbzuai.ac.ae, tjx@pku.edu.cn, dengcai@cad.zju.edu.cn\nInput image\nHigh-resolution Full-body textured meshes with detailed appearance \nThis is a\u00a0caucasian\u00a0man\u00a0with\u00a0short black hair\u00a0and\u00a0beard, wearing\u00a0a\u00a0blue T-shirt,\u00a0blue jeans\u00a0and\u00a0boots\u00a0\n\ud83e\udd16 \nFigure 1. Given a single image, TeCH reconstructs a lifelike 3D clothed human. \u201cLifelike\u201d refers to 1) a detailed full-body geometry,\nincluding facial features and clothing wrinkles, in both frontal and unseen regions, and 2) a high-quality texture with consistent color and\nintricate patterns. The key insight is to guide the reconstruction using a personalized Text-to-Image (T2I) diffusion model and textual infor-\nmation derived via visual questioning answering (VQA). Multi-view supervision is established through Score Distillation Sampling (SDS).\nAbstract\nDespite recent research advancements in reconstruct-\ning clothed humans from a single image, accurately restor-\ning the \u201cunseen regions\u201d with high-level details remains\nan unsolved challenge that lacks attention. Existing meth-\nods often generate overly smooth back-side surfaces with\na blurry texture. But how to effectively capture all visual\nattributes of an individual from a single image, which are\nsufficient to reconstruct unseen areas (e.g. the back view)?\nMotivated by the power of foundation models, TeCH re-\nconstructs the 3D human by leveraging 1) descriptive text\nprompts (e.g. garments, colors, hairstyles) which are auto-\nmatically generated via a garment parsing model and Vi-\nsual Question Answering (VQA), 2) a personalized fine-\ntuned Text-to-Image diffusion model (T2I) which learns the\n*These authors contributed equally to this work.\n\u201cindescribable\u201d appearance. To represent high-resolution\n3D clothed humans at an affordable cost, we propose a hy-\nbrid 3D representation based on DMTet, which consists of\nan explicit body shape grid and an implicit distance field.\nGuided by the descriptive prompts + personalized T2I diffu-\nsion model, the geometry and texture of the 3D humans are\noptimized through multi-view Score Distillation Sampling\n(SDS) and reconstruction losses based on the original ob-\nservation. TeCH produces high-fidelity 3D clothed humans\nwith consistent & delicate texture, and detailed full-body\ngeometry. Quantitative and qualitative experiments demon-\nstrate that TeCH outperforms the state-of-the-art methods\nin terms of reconstruction accuracy and rendering quality.\nThe code will be publicly available for research purposes at\nhuangyangyi.github.io/TeCH\n1\narXiv:2308.08545v2  [cs.CV]  19 Aug 2023\n1. Introduction\nHigh-fidelity 3D digital humans are crucial for various ap-\nplications in augmented and virtual reality, such as gam-\ning, social media, education, e-commerce, and immersive\ntelepresence. To facilitate the creation of digital humans\nfrom easily accessible in-the-wild photos, numerous ap-\nproaches focus on reconstructing a 3D clothed human shape\nfrom a single image [12, 38, 39, 46, 67, 72, 102\u2013104, 119\u2013\n121, 136]. However, despite the advancements made by\nprevious approaches, this specific problem can be consid-\nered ill-posed due to the lack of observations of non-visible\nareas. Efforts to predict invisible regions (e.g. back-side)\nbased on visible visual cues (e.g. colors [5, 46, 103], normal\nestimates [104, 120, 121]) have proven unsuccessful, result-\ning in blurry texture and smoothed-out geometry, see Fig. 8.\nAs a result, inconsistencies arise when observing these re-\nconstructions from different angles. To address this issue,\nintroducing multi-view supervision could be a potential so-\nlution. But is it feasible given only a single input image?\nHere, we propose TeCH to answer this question. Unlike\nprior research that primarily explores the connection be-\ntween visible frontal cues and non-visible regions, TeCH\nintegrates textual information derived from the input im-\nage with a personalized Text-to-Image diffusion model, i.e.,\nDreamBooth [101], to guide the reconstruction process.\nSpecifically, we divide the information from the single\ninput image into the semantic information that can be accu-\nrately described by texts and subject\u2019s distinctive and fine-\ndetailed appearance which is not easily describable by text:\n1) Describable semantic prompts, including the detailed\ndescriptions of colors, styles of garments, hairstyles, and\nfacial features, are explicitly parsed from the input image\nusing a garment parsing model (i.e. SegFormer [117]) and\na pre-trained visual-language VQA model (i.e. BLIP [65]).\n2) Indescribable appearance information, which implic-\nitly specifies the subject\u2019s distinctive appearance and fine-\ngrained details, is embedded into a unique token \u201c[V ]\u201d, by\na personalized Text-to-Image (T2I) diffusion model [101].\nBased on these information sources, we optimize the\n3D human using multi-view Score Distillation Sampling\n(SDS)[94], reconstruction losses based on the original ob-\nservations, and regularization obtained from off-the-shelf\nnormal estimators, to enhance the fidelity of the recon-\nstructed 3D human models while preserving their original\nidentity. To represent a high resolution geometry at an af-\nfordable cost, we propose a hybrid 3D representation based\non DMTet [32, 106]. This hybrid 3D representation com-\nbines an explicit tetrahedral grid to approximate the overall\nbody shape and implicit Signed Distance Function (SDF)\nand RGB fields to capture fine details in geometry and tex-\nture. In a two-stage optimization process, we first optimize\nthis tetrahedral grid, extract the geometry represented as a\nmesh, and then optimize the texture.\nTeCH enables the reconstruction of high-fidelity 3D\nclothed humans with detailed full-body geometry, and intri-\ncate textures with consistent color and patterns. As a result,\nit facilitates various downstream applications such as novel\nview rendering, character animation, and shape & texture\nediting. Quantitative evaluations performed on 3D clothed\nhuman datasets, covering various poses (CAPE [93]) and\noutfits (THuman2.0 [126]), have demonstrated TeCH\u2019s su-\nperiority in reconstructing geometric details.\nQualitative\ncomparisons conducted on in-the-wild images, accompa-\nnied by a perceptual study, further confirm that TeCH\nsurpasses SOTA methods in terms of rendering quality.\nThe code will be publicly avaiable for research purpose at\nhuangyangyi.github.io/TeCH\n2. Related Work\nTeCH reconstructs a high-fidelity clothed human from a\nsingle image, and imagine the missing parts through the aid\nof descriptive prompts and a personalized diffusion model.\nWe relate TeCH to both image-based human reconstructors\n(Sec. 2.1) and 3D human generators (Sec. 2.2). Human re-\nconstructors could be grouped as: 1) Explicit-shape-based,\n2) Implicit-function-based, and 3) NeRF-based methods.\nThe human generators are categorized w.r.t. their training\ndata: 1) directly learned from 3D real captures or 2) in-\ndirectly learned from large-scale 2D images. In addition,\nthere is a line of image-to-3D works focusing on general\nobjects, which will be discussed in Sec. 2.3.\n2.1. Image-based Clothed Human Reconstruction\nExplicit-shape-based Methods. Human Mesh Recovery\n(HMR) from a single RGB image is a long-standing prob-\nlem that has been thoroughly explored.\nA lot of meth-\nods [26, 53, 56\u201359, 64, 66, 68, 129, 131] use mesh-based\nparametric body models [51, 78, 92, 123] to regress the\nshape and pose of minimally-clothed 3d body meshes.\nTo account for the 3D garments, 3D clothing offsets [1\u2013\n4, 63, 116, 139] or deformable garment templates [9, 49]\nare used on top of a body model. Also, non-parametric ex-\nplicit representations, such as depth maps [29, 108], normal\nmaps [121], and point clouds [127] could be leveraged to\nreconstruct the clothed human. However, explicit shapes of-\nten suffer from restricted topological flexibility, particularly,\nwhen dealing with outfit variations in real-world scenarios,\ne.g., dress, skirt, and open jackets.\nImplicit-function-based Methods.\nImplicit represen-\ntations (occupancy/distance field) are topology-agnostic,\nthus, can represent 3D clothed humans, with arbitrary\ntopologies, such as open jackets and loose skirts. A line\nof works regresses the free-form implicit surface in an end-\nto-end manner [5, 103, 104], leverages a 3D geometric\nprior [12, 21, 38, 39, 46, 72, 120, 124, 136], or progressively\n2\n(c) SMPL-X initialized\nhybrid 3D representation\n\"a [V] man, brown short hair, caucasian, [V] blue shirt, [V] khaki pants, socks, standing up, goatee beard\"\nDreamBooth\nBLIP\ngarments styles,\n    colors, hairstyle, ...\n\"the photo of a [V] man\"\nText guidance\n(a) Parsing human image\n(b) Embedding subject details\nRGB \nnormal \n(d) High-quality textured meshes \nNormal\nEstimator\nSegFormer\nFigure 2. Method overview. TeCH takes an image I of a human as input. Text guidance is constructed through (a) using garment parsing\nmodel (SegFormer) and VQA model (BLIP) to parse the human attributes A with pre-defined problems Q, and (b) embedding with subject-\nspecific appearance into DreamBooth D\u2032 as unique token [V ]. Next, TeCH represents the 3D clothed human with (c) SMPL-X initialized\nhybrid DMTet, and optimize both geometry and texture using LSDS guided by prompt P = [V ] + PVQA(A). During the optimization,\nLrecon is introduced to ensure input view consistency, LCD is to enforce the color consistency between different views, and Lnormal serves as\nsurface regularizer. Finally, the extracted high-quality textured meshes (d) are ready to be used in various downstream applications.\nbuilds up the 3D human using a \u201csandwich-like\u201d structure\nand implicit shape completion [121]. Among these works,\nPIFu [103], ARCH(++) [39, 46], and PaMIR [136] infer\nthe full texture from the input image. PHORHUM [5] and\nS3F [21] additionally decompose the albedo and global il-\nlumination. However, the lack of multi-view supervision\noften results in depth ambiguities or inconsistent textures.\nNeRF-based Methods. There is a separate line of research\nthat focuses on optimizing neural radiance fields (NeRF)\nfrom a single image. SHERF [43] and ELICIT [45] op-\ntimize a generalized human NeRF, incorporating model-\nbased priors (SMPL-X). While SHERF complements miss-\ning information from partial 2D observations, ELICIT uti-\nlizes pre-trained CLIP [97] to provide an appearance prior.\n2.2. Generative Modeling of 3D Clothed Humans\n3D Human Generator Trained on 3D Data. Statistical\nbody models [51, 78, 92, 123] can be considered as 3D gen-\nerative models of the human body. These models are trained\non numerous 3D scans of minimally-clothed bodies, and\ncan generate posed bodies with varying shapes, but with-\nout clothing. To account for the outfits, CAPE [79] learns a\nclothing offset layer based on the SMPL-D model, from reg-\nistered human scans, Chupa [55] \u201ccarves\u201d the SMPL mesh\nby dual normal maps generated by pose-conditioned dif-\nfusion model; Alternatively, gDNA [17], NPMs [88], and\nSPAMs [89], learn the implicit clothed avatars from nor-\nmalized raw captures (i.e., scans, depth maps). Unfortu-\nnately, all the aforementioned methods to learn generative\n3D humans with diverse shapes and appearances require\n3D data, which is both limited and expensive to acquire.\nRodin [113] has recently employed large-scale 3D syn-\nthetic head avatars in combination with a diffusion model\nto develop a high-fidelity head avatar generator.\nHow-\never, the scarcity of datasets containing real 3D clothed hu-\nmans [11, 18, 47, 126, 135] limits the model\u2019s generaliza-\ntion ability and may lead to overfitting on small datasets.\n3D Human Generator from 2D Image Collections. In\ncontrast to 3D data, large-scale 2D human images are\nwidely avaible from DeepFashion [34, 77], SHHQ [28]\nand LAION-5B [105].\nRelated human generators repre-\nsent 3D humans using meshes [36, 40, 50], DMTet [33],\nTri-planes [8, 25, 85, 109, 132], implicit functions [118],\nor neural fields [13, 41, 60, 128].\nSome methods adapt\nGANs [54] by integrating diff-renderer [8, 25, 36, 85,\n109, 110, 118, 132], while others leverage diffusion mod-\nels [13, 40, 44, 60, 130]. Despite the demonstrated quality\nof these methods in generating textured avatars, a signifi-\ncant gap still exists in achieving \u201clifelike\u201d avatars with de-\ntailed geometry and texture, consistent with the input.\nIn contrast, TeCH excels at generating \u201clifelike\u201d 3D\ncharacters from a single image, incorporating consistent\ntexture with intricate patterns like checkered or overlapped\ndesigns. It relies on a pretrained diffusion model which is\ntrained on a billion-level data, LAION-5B [105], and offers\nthe ability to imagine the non-visible regions, guided by\ndescriptive prompts. Furthermore, it leverages the image-\nbased reconstruction approach to faithfully reconstruct the\nvisible regions from a single input image.\n2.3. Image-to-3D for General Objects\nLifting 2D to 3D for general objects is a longstanding\nproblem with valuable explorations. Here, we mainly fo-\ncus on diffusion-guided approaches. Initially, CLIP [97]\nsemantic consistency loss [48], Score Jacobian Chaining\n(SJC) [112] and Score Distillation Sampling (SDS) [94]\n3\nWhat is the color of the tank top?\nWhat is the style of the tank top?\ntank top\nWhat upper-clothes is the person wearing?\nblack and white\nsleeveless\n...\n... wearing a black and white sleeveless tank top, ...\n(a) Descriptive prompt \nBackground augmentation\nDreamBooth\n(b) Subject-specific generation from [V] \nSegmentation\nFigure 3. Prompt construction (P = PVQA + [V ]). (a) In-\nquire VQA model with predefined questions on individual ap-\npearance to construct describable prompts PVQA. (b) Fine-tuned\nDreamBooth with background-augmented images to embed inde-\nscribable subject-specific details into unique identifier [V ].\nare proposed to leverage pretrained 2D diffusion models\nfor 3D content generation. Subsequently, there is a line of\nworks [22, 81, 98, 111, 122] that address this problem, by\nincorporating textural inversion [30], DreamBooth [101],\nCLIP-guided diffusion prior, depth prior, and reconstruction\nloss. In addition to aforementioned \u201creconstruct via multi-\nview SDS\u201d scheme, recent attention has been drawn to the\n\u201creconstruct via direct view-conditional generation\u201d [14,\n75, 76, 95, 114, 138]. In contrast, TeCH aims to recover\npixel-aligned models with intricate texture, even in non-\nvisible regions, which is a challenging scenario where ex-\nisting solutions have not shown promising results.\n3. Method\nGiven a single image as input, TeCH aims at reconstruct-\ning a high-fidelity 3D clothed human. Here, \u201chigh-fidelity\u201d\nrefers to the inclusion of consistent texture with intricate\npatterns, as well as detailed full-body geometry. To achieve\nthis, TeCH follows a two-step procedure: Firstly, a text\nprompt that describes the human in the input image is\nobtained via the human parsing model SegFormer [117]\nand the VQA model BLIP [65] (Sec. 3.1). This descrip-\ntive prompt is used to guide the generation process in\nDreamBooth [101], a personalized Text-to-Image diffusion\nmodel fine-tuned on augmented input images. Secondly,\nthe 3D human, which is represented as hybrid DMTet and\ninitialized with SMPL-X (Sec. 3.2), is optimized with SDS\nlosses [94] computed from the personalized DreamBooth\n(Sec. 3.3). The Score Distillation Sampling (SDS) loss has\nbeen introduced in DreamFusion [94] for the task of Text-\nto-3D generation of general objects, by optimizing a neural\nradiance field (NeRF) with gradients from a frozen diffu-\nsion model. In our case, we utilize the SDS loss to guide\nthe reconstruction of a 3D human from a single input image,\nemploying a multi-stage optimization strategy (Sec. 3.3) to\nget a consistent alignment of geometry and texture.\nFigure 4. The effects of text guidance. We compare the effective-\nness of using only VQA descriptions (TeCHvqa), only DreamBooth\nidentity token (TeCHdb), and both of them (TeCH).\n3.1. Extracting Text-guidance from the Observation\nParsing human attributes. As depicted in Fig. 3, given\nthe input image of a human, SegFormer [117], which is\nfine-tuned on ATR dataset [70, 71], is applied to recognize\neach part of the garments (e.g. hat, skirt, pants, belt, shoes).\nTo obtain detailed descriptions (i.e. color and style) of\nthe parsed garments, we utilize the vision-language model\nBLIP [65] as VQA captioner. This model has been pre-\ntrained on a vast collection of image-text pairs, enabling it\nto automatically generate descriptive prompts. Rather than\nusing naive image captioning, we employ a series of fine-\ngrained VQA questions {Qi} (see Appx.\u2019s Sec. B) as input\nto BLIP. These questions cover garment styles, colors, facial\nfeatures, and hairstyles, with the corresponding answers de-\nnoted as {Ai}. The set of {Ai} will be inserted into a pre-\ndefined template to create text prompts PVQA, which will\nserve as text-guidance to condition the text-to-image diffu-\nsion model, recap the full method overview in Fig. 2.\nEmbedding subject-specific appearance. Does the text\nprompt PVQA comprehensively capture all the visual char-\nacteristics of the subject? No, a picture is worth a thousand\nwords. Thus, we utilize DreamBooth [101] to learn the in-\ndescribable visual appearance. DreamBooth is a method\nfor \u201cpersonalizing\u201d a diffusion model through few-shot tun-\ning (3\u223c5 images). We perform DreamBooth\u2019s fine-tuning\non a pre-trained Stable Diffusion (v1.5) as the base model.\nTo generate the needed inputs, we augment the single input\nimage with five different backgrounds, as shown in Fig. 3.\nTo prevent language drift, we assign the subject classes\n\u201cman\u201d or \u201cwoman\u201d based on the gender determined by the\nVQA. After fine-tuning DreamBooth, the subject-specific\ndistinctive appearance is encoded within a unique identi-\nfier token \u201c[V ]\u201d. We insert \u201c[V ]\u201d into the prompt PVQA, to\nconstruct the final text prompt P used by the personalized\nDreamBooth D\u2032. In Fig. 4, you can see how these individual\nprompts contribute to the final appearance.\n4\n\u0000\u0001\u0004\u0002\u0001\u0005\u0003\n\u0000\u001c\u0011\u0012\u0004\u0017\u0019\u0016\u0018\u0004\u0013\t\n\u000e\u000f\u0004\u0001\u0005\u0018\u0004\u0010\f\u0001\u001d\u000b\u0004\u001b\b\u0019\u0007\u001a\u0016\u0018\u0004\u001b\u0006\u0007\u0019\b\u0003\u0004\n\u0000\u001c\u0011\u0012\u0004\u0002\u0007\u0005\u0007\u0004\u0017\u001a\u0007\u0005\u000b\u0004&\u000e#\u000f\u0004\u001b\u000b\u0007\u0019\b\u0003\u0004\n\u0000\u001c\u0011\u0012\u0004\u0017B\u0006\u0007\b\u0016\u0004;&\u000e@\u000f\u0004\b8\u0019\b\f\u0016\u0005\u0016\u001d\u000b\u0004\u001bB\u0016\u0001\b\u0016\u0019\u0003\u0004\nX\u0004\u0000\u001b\u0006\u0007\u0019\b\u001b\u0004\u0001\u0005\u0018\u0004\u001b\u0006M\u0019\b\u001b\u0003\nX\u0004b\u0001\u0019\u0002\u0016\u0005\b\u0004\u001dM\fM\u0019\u001b\nX\u0004\u0006\u0001\u0007\u0019\u001b\bn\f\u0016f\u0004r\u0001\u001d\u0016\u0004\n\u0000\u0001\u0004\u001c\u0011\u0012\u0004\u0002\u0001\u0005\u0003\n\u0000\u0001\u0004\u001c\u0011\u0012\u0004\u0002\u0001\u0005f\u0004\u001c\u0011\u0012\u0004B\u0006\u0007\b\u0016\u0004\u001b\u0006\u0007\u0019\bv\nf\u0004\u001c\u0011\u0012\u0004\u0019\u0016\u0018\u0004\u0001\u0005\u0018\u0004B\u0006\u0007\b\u0016\u0004\u001b\u0006M\u0019\b\u001b\u0004\u007f\u007f\u007f\u007f\u0003\n\u009c\u009b\u009a\u0004\u0096\u0095\u0093\u009b\u0092\u0099\u0095\u0091\u0004\u009b\u0098\u0099\u009b\u0093\u0092\u008e\u0094\u0004\u008d\u0095\u008c\u008b\u0099\u0093\u008c\u0004\u008e\u008a\u0004\u0093\u0095\u0090\u0093\u008b\u009b\u0099\u0004\u008f\u008b\u0092\u0091\u009b\u0094\u0089\u0095\n\u009c\u0098\u009a\u0004\u00b6\u0091\u0092\u0093\u0092\u0094\u008f\u0004\u008f\u009b\u008d\u00ac\u0095\u0094\u0093\u0004\u0089\u008e\u0099\u008e\u008d\u008c\u0004\u00b1\u0092\u0093\u00a4\u0004\u008c\u008b\u0098\u00a1\u0095\u0089\u0093\u009f\u008c\u00b4\u0095\u0089\u0092\u008a\u0092\u0089\u0004\u0093\u008e\u009d\u0095\u0094\nFigure 5. (a) Top depicts the impact of specific elements within the textual guidance, such as garment styles & colors, hairstyle, facial\nfeatures, and the placement & inclusion of \u201c[V ]\u201d. (b) Bottom demonstrates that TeCH facilities text-guided garment color editing.\nDeeper analysis of description P. In Fig. 5 (a), we first\nshow the impact of individual elements within the text\nprompt, including garment styles & colors, hairstyle, and\nface, which guide the model to recover the appearance of\neach attribute of the clothed human. The first column shows\nthat a basic class description alone cannot effectively guide\nthe reconstruction process.\nHowever, in the subsequent\ncolumns, text guidance incorporating detailed descriptions\nof clothing proves successful in accurately reconstructing\nthe structure of clothed humans. Furthermore, with addi-\ntional information regarding colors and hairstyles, the char-\nacters reconstructed by TeCHvqa exhibit greater semantic\nconsistency with respect to the input view. However, merely\nrelying on VQA descriptions is insufficient for generating a\n\u201cconvincingly fake\u201d appearance.\nOnly using the DreamBooth guidance (TeCHdb), helps to\nrecover original garment patterns, which demonstrates that\nDreamBooth has a high-level understanding of texture pat-\nterns. However, it sometimes will diffuse the patterns to\nthe entire human. By combining \u201c[V ]\u201d with the VQA pars-\ning text prompts PVQA, TeCH produces remarkably realis-\ntic texture with consistent color and intricate patterns.\nIn Fig. 5 (b), we also demonstrate some text-guided\ngarment color editing examples based on a fine-tuned\nDreamBooth model D\u2032 and subject-specific token \u201c[V ]\u201d.\n3.2. Hybrid 3D Representation\nTo efficiently represent the 3D clothed human at a high res-\nolution, we embed DMTet [32, 106] around the SMPL-X\nbody mesh [86].\nSpecifically, we construct a compact\ntetrahedral grid (Vshell, Tshell) within an outer shell Mshell,\nshown in Fig. 2-(c). Compared to the DMTet cubic-based\ntetrahedral grid, the outer shell tetrahedral grid is more com-\nputationally efficient for high-resolution geometry model-\ning of a human. Using PIXIE [26], we estimate an initial\nbody Mbody. To create Mshell, a series of mesh dilation,\ndown-sampling, and up-sampling steps are applied to the\nbody mesh Mbody (see details Sec. C of Appx.).\nWe use two MLP networks \u03a8g, \u03a8c with hash encod-\ning [83], parameterized by \u03c8g and \u03c8c to learn the geometry\nand color separately. The geometry network \u03a8g predicts the\nSDF value \u03a8g(vi) = s(vi; \u03c8g) of each DMTet vertex vi. It\nis initialized by fitting it to the SDF of Mshell:\nLinit =\nX\npi\u2208P\n\u2225s(pi; \u03c8g) \u2212 SDF(pi)\u22252\n2 ,\n(1)\nwhere P = {pi \u2208 R3} is a point set randomly sampled near\nMshell, and SDF(pi) is the pre-computed pointwise SDF.\nTriangular meshes can be extracted from this efficient hy-\nbrid 3D representation by Marching Tetrahedra (MT) [24]:\nM = MT(Vshell, Tshell, s(Vshell; \u03c8g)).\n(2)\nGiven the camera parameters k, the generated mesh is ren-\ndered through differentiable rasterization R [62], to get\nthe back-projected 3D locations P(M, k), rendered mask\nM(M, k), and rendered normal image N(M, k)\nR(M, k) = (P(M, k), M(M, k), N(M, k))\n(3)\n5\nThe albedo of each back-projected pixel is predicted by the\ncolor network \u03a8c, where \u03c8c represents the parameters:\nI\u2032(M, \u03c8c, k) = \u03a8c(\u03c8c, P(M, k)).\n(4)\nAs detailed in Section 3.3, we optimize this 3D repre-\nsentation using a coarse-to-fine strategy by applying suc-\ncessive subdivisions on the tetrahedral grids. Specifically, a\nmore detailed surface Msubdiv(\u03c8g) can be obtained by ap-\nplying volume subdivision on the surface tetrahedral grids\n(Vsurface, Tsurface) that intersect with M(\u03c8g). Note that the\nSDF values of the refined vertices are still inferred by \u03a8g.\n3.3. Multi-stage Optimization\nWe adopt a multi-stage, coarse-to-fine optimization process\nto sequentially recover the subject\u2019s geometry and texture.\nIn the initial stage, we utilize the tetrahedral representation\nto model the subject\u2019s geometry (Sec. 3.3.1). Next, the ap-\npearance is recovered using the mesh that is extracted from\nthe tetrahedral grid (Sec. 3.3.2). Both stages are leverag-\ning SDS-based losses using the personalized DreamBooth\nmodel which provides multi-view supervision by sampling\nnew camera views as described in Sec. 3.3.3.\n3.3.1\nGeometry Stage\nWe optimize the geometry based on a silhouette loss Lsil us-\ning the orig. image, a text-guided SDS loss on rendered nor-\nmal images Lnorm\nSDS , and geometric regularization Lreg based\non pred. normals Lnorm and surface smoothness Llap:\nLgeometry = \u03bbsilLsil + \u03bbSDSLnorm\nSDS + Lreg\nLreg = \u03bbnormLnorm + \u03bblapLlap,\n(5)\nwhere \u03bb represents the weights to balance the losses. Dur-\ning optimization of this loss, we perform a coarse-to-\nfine subdivision on DMTet, to robustly produce a high-\nresolution mesh for the clothed body. Specifically, the op-\ntimization is first performed w/o subdivision for tcoarse =\n5000 iters, and then with subdivision for tfine = 5000 iters.\nPixel-aligned silhouette loss.\nThe silhouette loss [125,\n133] enforces pixel-alignment with the foreground mask S\nof the input image I under the input camera view k:\nLsil = \u2225S \u2212 M(M, k)\u22252\n2\n+\nX\nx\u2208Edge(M(M,k))\nmin\n\u02c6x\u2208Edge(S) \u2225x \u2212 \u02c6x\u22251 .\n(6)\nIt consists of (1) a pixel-wise L2 loss over the foreground\nmask S and the rendered silhouette M, and (2) an edge dis-\ntance loss, based on the distance of each silhouette bound-\nary pixel x \u2208 Edge(M(M, k)) to the nearest foreground\nmask boundary pixel \u02c6x \u2208 Edge(S).\nFigure 6. The effects of normal regularization. Lnorm regular-\nizes the surface with predicted normal images \u02c6\nNfront, \u02c6\nNback.\nSDS loss on normal images. Inspired by Fantasia3D [16],\nour approach integrates normal renderings with the SDS\nloss [94]. It enables TeCH to effectively capture intricate\ngeometric details without rendering the color image. Given\nthe surface normals n = N(M, k), Lnorm\nSDS is defined as:\nLnorm\nSDS = \u2207\u03c8gLnorm\nSDS (n, cPnorm)\n= Et,\u03f5\n\u0014\nwt\n\u0000\u02c6\u03f5\u03d5\u2032(zn\nt ; cPnorm, t) \u2212 \u03f5\n\u0001 \u2202n\n\u2202\u03c8g\n\u2202zn\nn\n\u0015\n,\n(7)\nwhere cPnorm is the text condition with an augmented\nprompt Pnorm. We construct Pnorm from P by adding an\nextra description \u201ca detailed sculpture of\u201d to better reflect\nthe intrinsic characteristics of normal maps.\nGeometric regularization. We found that relying solely\non silhouette and SDS losses may lead to the generation\nof noisy surfaces, which is particularly evident for subjects\nwearing complex clothing. To address this, we leverage nor-\nmal estimations as an additional constraint to regularize the\nreconstructed surface (see Fig. 6):\nLnorm( \u02c6\nNk, n) = \u03bbnorm\nMSE\n\r\r\r \u02c6\nNk \u2212 n\n\r\r\r\n2\n2+LPIPS( \u02c6\nNk, n)),\n(8)\nwhere \u02c6\nNk are the front and back normal maps estimated us-\ning ICON [120] indexed by the view k (k \u2208 {front, back}).\nn are the corresponding differentiably rendered normal im-\nages of the 3D shape \u03a8g.We use a combination of LPIPS\nand MSE loss to enhance the similarity between \u02c6\nNk and\nn. Furthermore, we utilize a regularization loss based on\nLaplacian smoothing [6], represented as Llap.\nMesh extraction. We use Marching Tetrahedra [24] to ex-\ntract the mesh from the tetrahedral grid. Like ECON [121],\nwe register SMPL-X to this mesh which allows us to trans-\nfer skinning weights for reposing (see Fig. 9). In addition,\nwe replace the hands with SMPL-X ones which effectively\nmitigates any potential artifacts introduced during reposing\nwhich is needed in the subsequent texture generation stage.\n6\n3.3.2\nTexture Stage\nGiven the triangular mesh from the geometry stage, we op-\ntimize the full texture. To recover the consistent details and\ncolor, even for self-occluded regions, we render both the in-\nput pose (Min) and the A-pose (MA) during optimization.\nThe textures of Min and MA are modeled by \u03a8color in the\n3D space of MA. We optimize the texture from scratch with\n\u03c8c randomly initialized. In Fig. 7, we show the effect of this\nmulti-pose training. We utilize an occlusion-aware recon-\nstruction loss Lrecon on the input view of Min, an SDS loss\nLcolor\nSDS with text guidance on rendered color images of both\nMin and MA, and a color consistency regularization LCD,\nwith respective weights \u03bb to balance the individual losses:\nLtexture = \u03bbreconLrecon + \u03bbSDSLcolor\nSDS + \u03bbCDLCD,\n(9)\nNote that LCD is only utilized after the full-body tex-\nture convergence (5000 iters), in an additional optimization\nphase of 2000 iterations for enforcing color consistency.\nOcclusion-aware reconstruction loss. To enforce pixel-\nalignment, we apply an input view reconstruction loss to\nminimize the difference between input image I and the\nalbedo-rendered image I\u2032(M, \u03c8c, kI).\nAdditionally, we\nhave observed that applying Lrecon to self-occluded areas\nmay lead to incorrect texture due to geometry misalignment.\nTherefore, an occlusion-aware mask mocc is introduced to\nselectively exclude the Lrecon in occluded regions.\nLrecon = mocc(\u03bbMSE \u2225I \u2212 I\u2032(M, \u03c8c, kI)\u22252\n2\n+ LPIPS(I, I\u2032(M, \u03c8c, kI))),\n(10)\nwhere kI denotes the input view camera, and \u03bbMSE is a\nweight to balance the two loss terms.\nSDS loss on color images. To recover the full-body texture,\nincluding unseen regions, we update \u03c8c via SDS loss Lcolor\nSDS\nwith text guidance. This loss is calculated based on random-\nview color renderings x = I\u2032(\u03c8g, \u03c8c, k), and DreamBooth\nD\u2032 parameterized by \u03d5\u2032 and guided by text prompt P.\nLcolor\nSDS = \u2207\u03c8cLcolor\nSDS (x, cP )\n= Et,\u03f5\n\u0014\nwt\n\u0000\u02c6\u03f5\u03d5\u2032(zx\nt ; cP , t) \u2212 \u03f5\n\u0001 \u2202x\n\u2202\u03c8c\n\u2202zx\nx\n\u0015\n,\n(11)\nwhere k is the camera pose, cP is the text embedding of P.\nChamfer-based color consistency loss.\nAs mentioned\nin DreamFusion [94], the SDS loss may result in over-\nsaturated colors which will cause a noticeable color dispar-\nity between visible and invisible regions. To mitigate this\nissue, we incorporate a color consistency loss to ensure that\nthe rendered novel views align closely with the color distri-\nbutions observed in the input view. We quantify the dispar-\nity between the color distributions using a chamfer Distance\nInput\nGT\nw/o \nw/ \nInput\nGT\nw/o \nw/ \nFigure 7. The effects of color consistency loss LCD and multi-\npose training (MA) for texture optimization. LCD corrects the\nover-saturated back-side color generated by SDS, while MA im-\nproves the texture quality under self-occlusion or extreme poses.\n(CD) by treating the pixels from both views as point clouds\nwithin the RGB color space:\nLCD =\nX\nx\u2208Fx\nmin\ny\u2208FI ||x \u2212 y||2\n2 +\nX\ny\u2208FI\nmin\nx\u2208Fx ||x \u2212 y||2\n2,\n(12)\nwhere Fx and FI respectively represent the foreground pix-\nels of the novel-view albedo rendering x, and the input view\nI. The improvement using LCD is shown in Fig. 7.\n3.3.3\nCamera sampling during optimization\nTo optimize the 3D shape and texture using multi-view ren-\nderings, cameras are randomly sampled in a way that en-\nsures comprehensive coverage of the entire body by adjust-\ning various parameters. To mitigate the occurrence of mir-\nrored appearance artifacts (i.e., Janus-head), we incorpo-\nrate view-aware prompts (\u201cfront/side/back/overhead\nview\u201d) w.r.t. the viewing angle in the diffusion-based gen-\neration process, whose effectiveness has been demonstrated\nin DreamBooth [94]. In order to improve facial details, we\nalso sample cameras positioned around the face, together\nwith the additional prompt \u201cface of\u201d. More details about\nthe camera sampling strategy are in Sec. D of Appx.\n4. Experiments\nWe compare TeCH with state-of-the-art image-based 3D\nclothed human reconstruction methods, including body-\nagnostic methods, such as PIFu [103], PIFuHD [104] and\nPHORHUM [5], as well as methods that utilize SMPL-\n(X) body prior, such as PaMIR [136], ICON [120] and\nECON [121]. For a fair comparison, all methods (i.e., PIFu,\nPaMIR, ICON, ECON) utilize the same normal estimator\nfrom ICON. Official PIFu, PaMIR and PHORHUM are\nused to evaluate the quality of texture. For ECON, we use\nECONEX, due to its superior performance on both \u201cOOD\nposes\u201d and \u201cOOD outfits\u201d cases, as reported in the original\npaper [121]. Note that PHORHUM uses a different cam-\nera model which is not compatible with our testing data,\nthus, we use PHORHUM only for qualitative comparisons.\n7\nMethod\n3D Metrics\n2D Image Quality Metrics\nCAPE\nTHuman2.0\nCAPE\nTHuman2.0\nChamfer \u2193\nP2S \u2193\nNormal \u2193\nChamfer \u2193\nP2S \u2193\nNormal \u2193\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nw/o SMPL-X body prior\nPIFu [103]\n1.9683\n1.6236\n0.0623\n1.9305\n1.8031\n0.0802\n27.0994\n0.9362\n0.0987\n23.5068\n0.9296\n0.1083\nPIFuHD [104]\n3.2018\n2.9930\n0.0758\n2.4613\n2.3605\n0.0924\n-\n-\n-\n-\n-\n-\nw/ SMPL-X body prior\nPaMIR [136]\n1.3756\n1.1852\n0.0526\n1.2979\n1.2188\n0.0676\n27.7279\n0.9456\n0.0904\n22.5466\n0.9266\n0.1082\nICON [120]\n0.8689\n0.8397\n0.0360\n1.1382\n1.2285\n0.0623\n-\n-\n-\n-\n-\n-\nECON [121]\n0.9186\n0.9227\n0.0330\n1.2585\n1.4184\n0.0612\n-\n-\n-\n-\n-\n-\nTeCH\n0.7416\n0.6962\n0.0306\n1.2364\n1.2715\n0.0642\n28.3601\n0.9490\n0.0639\n25.2107\n0.9363\n0.0835\nTable 1. Quantitative evaluation against SOTAs. TeCH surpasses SOTA baselines in terms of both 3D metrics and 2D image quality\nmetrics. This demonstrates its superior performance in accurately reconstructing clothed human geometry with intricate details, as well as\nproducing high-quality textures with consistent appearance.\nMore implementation details about network structure and\noptimization setting can be found at Sec. E of Appx.\n4.1. Models and Datasets\nOff-the-shelf models.\nTeCH relies on multiple off-the-\nshelf pre-trained models and does not need any addi-\ntional training data.\nSpecifically, we use officially re-\nleased stable-diffusion-v1.5* as T2I diffusion model, which\nis trained on LAION-5B, the VQA model BLIP [65] pre-\ntrained on 129M images from multiple datasets [15, 61,\n74, 84, 87, 105] and fine-tuned on VQA2.0 [35], Seg-\nFormer* [117] pretrained from [10, 20, 23, 137] and fine-\ntuned on ATR[69], PIXIE [26] trained on human images\nfrom multiple datasets [19, 74, 90, 115, 140], and the nor-\nmal predictor of ICON [120] trained on AGORA [91].\nDatasets for evaluation. Based on the high-fidelity 3D tex-\ntured scans from CAPE [79] and THuman2.0 [126], we per-\nform quantitative evaluations.We follow ICON [120] to an-\nalyze the robustness of reconstructions under both simple\nand complex poses (150 scans from CAPE). An additional\n150 THuman2.0 scans are included, which comprises 100\nsubjects that were manually selected to represent a diverse\nrange of clothing styles (e.g., open jackets, long coats, gar-\nments with intricate patterns, etc.), and 50 randomly sam-\npled subjects. The images are rendered at a resolution of\n512 \u00d7 512.\nFor qualitative comparison, we selected the\nSHHQ dataset [28] due to its wide range of textures, out-\nfits, and gestures. From this dataset, we randomly sampled\n90 images with official mask annotations.\n4.2. Quantitative Comparison\nWe quantitatively evaluate the reconstruction quality of ge-\nometry and appearance, using the Chamfer (bi-directional\npoint-to-surface) and P2S (1-directional point-to-surface)\ndistance, to measure the difference between the recon-\nstructed and ground-truth meshes.\nAdditionally, we re-\nport the L2 Normal error between normal images rendered\n*runwayml/stable-diffusion-v1-5\n*matei-dorian/segformer-b5-finetuned-human-parsing\nfrom both meshes, to measure the consistency and fine-\nness of local surface details, by rotating the camera by\n{0\u25e6, 90\u25e6, 180\u25e6, 270\u25e6} w.r.t. to the input view. To evaluate\nthe quality of the texture, we report 2D image quality met-\nrics, on the multi-view colored images rendered in the same\nway as the normal images, including PSNR (Peak Signal-\nto-Noise Ratio), SSIM (Structural Similarity) and LPIPS\n(learned perceptual image path similarity).\nAs shown in Tab. 1, TeCH demonstrates superior per-\nformance across all 2D metrics and 3D metrics on CAPE.\nThis reveals that TeCH can accurately reconstruct both\ngeometry and texture, even for subjects with challenging\nposes (CAPE) or loose clothing (THuman2.0). However,\non THuman2.0, it achieves comparable reconstruction ac-\ncuracy to prior-based methods. This can be attributed to\nthe fact that the hallucinated back-side may differ from\nthe ground truth while still appears realistic. A perceptual\nstudy Tab. 2 was conducted for additional clarification. See\nSec. 4.4 of Appx. for more results on these datasets.\n4.3. Perceptual Evaluation\nTo assess the generalization of TeCH on in-the-wild images\nand evaluate the perceptual quality of our results, we con-\nducted a perceptual study using 90 randomly sampled im-\nages from the SHHQ dataset [28]. Participants were shown\nvideos showcasing rotating 3D humans reconstructed by\nTeCH, as well as the baselines (PaMIR [136], PIFu [103],\nICON [120], ECON [121] and PHORHUM [5]).\nThey\nwere asked to choose the more realistic and consistent re-\nsult based on the input image. We gathered a total of 3,150\npairwise comparisons from 63 participants, uniformly cov-\nering 90 SHHQ subjects. The results in Tab. 2 show that\nTeCH is preferred, both, in terms of geometry and texture.\nAs illustrated in Fig. 8, unlike other methods that tend to re-\nconstruct overly smooth surfaces and blurry textures, TeCH\nshows remarkable generalizability when applied to in-the-\nwild images featuring diverse clothing styles and gestures.\nIt produces more realistic clothing, haircut, and facial de-\ntails, even for unseen back-side views.\n8\nPIFu\nInput\nPaMIR\nPHORHUM\nTeCH\nPIFu\nPaMIR\nTeCH\nPIFu\nPaMIR\nTeCH\nPIFu\nPaMIR\nPHORHUM\nTeCH\nPHORHUM\nPHORHUM\nFigure 8. Qualitative comparison on SHHQ images. TeCH generalizes well on in-the-wild images with diverse clothing styles and\ntextures. It successfully recovers the overall structure of the clothed body with text guidance, and generates realistic full-body texture\nwhich is consistent with the colored pattern and the material of the clothes. \u00fc Zoom in to see the geometric details.\nPreference (%, \u2191)\nPIFu PaMIR PHORHUM ICON ECON\nGeometry\n88.6\n87.0\n81.7\n97.94\n90.48\nColored Rendering\n95.1\n93.7\n93.0\n-\n-\nTable 2. Perceptual study. The percentages of user preference to\nTeCH compared to other baselines are reported. Most participants\npreferred TeCH in both geometry and colored rendering (texture).\n4.4. More Qualitative Results\nIn addition to Fig. 8, we show more qualitative compar-\nisons between TeCH and other baselines (PIFu [103],\nPIFuHD\n[104],\nPaMIR\n[136],\nPHORHUM\n[5],\nICON [120],\nECON [121]) on CAPE, THuman2.0,\nand SHHQ [28] images (Figs. 12 to 14 of Appx.), by\nvisualizing multi-view surface normals,\ncolor render-\nings, and zoomed-in details.\nFor subjects in CAPE and\nTHuman2.0, TeCH precisely recover the human shape\nand generate high-quality details of garments and facial\nfeatures, regardless of hard poses, complex texture, loose\nclothing, or self-occlusion. Also, Fig. 14 demonstrates the\nstrong generalizability of TeCH on in-the-wild images,\nmore rotating 3D humans are provided in video.\n4.5. Ablation Studies\nTo assess the effectiveness of key designs in TeCH, we per-\nform ablation studies on a 10% subset of the test set, con-\nsisting of 15 subjects from THuman2.0 and 15 from CAPE.\nThe detailed analysis on these results is as follows:\nText guidance.\nTable 3-A shows that either the \u201cVQA-\nonly\u201d or \u201cDreamBooth-only\u201d guidance exhibit a decrease\nin performance w.r.t.\nreconstruction accuracy (Chamfer,\nExperiment settings\n3D Metrics\n2D Image Quality Metrics\nVQA DreamBooth Lnorm LCD MA multi-stage Chamfer \u2193\nP2S \u2193\nNormal \u2193 PSNR \u2191 SSIM \u2191 LPIPS \u2193\nOurs\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n0.9794\n0.9779\n0.0466\n26.7565\n0.9428\n0.0741\n\u2713\n\u2717\n\u2713\n\u2713\n\u2713\n\u2713\n0.9959\n1.0192\n0.0454\n26.2078\n0.9405\n0.0813\nA.\n\u2717\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n1.0032\n1.0218\n0.0470\n26.9602\n0.9428\n0.0785\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2717\n0.9957\n0.9963\n0.0468\n26.0465\n0.9395\n0.0775\nB.\n\u2713\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n1.0882\n0.9203\n0.0870\n-\n-\n-\nC.\n\u2713\n\u2713\n\u2713\n\u2713\n\u2717\n\u2713\n-\n-\n-\n26.6500\n0.9427\n0.0746\n\u2713\n\u2713\n\u2713\n\u2717\n\u2713\n\u2713\n-\n-\n-\n26.6506\n0.9425\n0.0786\nTable 3.\nAblation study.\nWe quantitatively evaluate the ef-\nfectiveness of each component. Top two results are colored as\nfirst\nsecond . All the factors are grouped w.r.t. their influence:\nA. geometry+texture, B. geometry only, C. texture only.\nP2S) and texture quality (LPIPS). Figure 4 shows that VQA\nprompts help to recover the overall structure of clothing,\nwhile DreamBooth enhances the fine details of the texture\npattern. Combining both text guidance sources yields the\nbest results. A detailed analysis of individual descriptive\ntexts (e.g., garments, hairstyles, etc.) is in Fig. 5\nGeometric regularization. As shown in Fig. 6, using only\nLnorm\nSDS to optimize the geometry will produce noisy arti-\nfacts, particularity noticeable in loose clothes. The signif-\nicant increase in \u201cNormal\u201d error shown in Tab. 3-B echos\nthis. This issue can be mitigated by incorporating Lnorm at\nthe beginning of the optimization.\nConsistent texture recovery.\nThe results presented in\nFig. 7 demonstrate that LCD notably enhances color consis-\ntency between the frontal and back sides, and \u201dmulti-pose\u201d\ntraining (MA) improves texture quality when dealing with\nself-occlusion scenarios. This improvement is further sup-\nported by Tab. 3-C, across all 2D image quality metrics.\nMulti-stage optimization. As shown in Tab. 3-A, com-\npared to the decoupled two-stage optimization (Ours), the\njoint optimization results in a performance drop across both\n9\n3D and 2D metrics. This may be attributed to the entan-\nglement of the gradients from the geometry and texture\nbranches during optimization. Notably, in the separate tex-\nture stage, a colored image is rendered from the extracted\nmesh, saving 20% of the run time compared to joint opti-\nmization, which involves rendering from the DMTet mesh.\n5. Applications\n5.1. Avatar animation\nFollowing the geometry optimization phase, TeCH aligns\nthe clothed body mesh with the SMPL-X model, enabling\nus to animate the reconstructed avatar with SMPL-X mo-\ntions [80], as shown in Fig. 9 and video.\nInput\nAnimation video results\nFigure 9. Animate TeCH with SMPL-X motions.\n5.2. Avatar editing\nThe text-guided texture generation feature also allows us to\nedit the texture of the generated avatars. Here, we show\nstylization results with different painting styles, like \u201cpop\nart, pixel art, van gogh\u201d. The resulting texture not\nonly features the desired styles but also preserves the inher-\nent appearance traits of the original character.\nOriginal\n\u201cPop art\u201d\n\u201cPixel art\u201d\n\u201cVan Gogh\u201d\nFigure 10. Text-guided stylization.\n6. Discussion\nLimitations. Despite achieving impressive results on di-\nverse datasets, some failures cases still exist, see Fig. 11:\nA. TeCH occasionally fails for extremely loose clothing,\nthis may relate to the constraint from SMPL-X-based ini-\ntialization. B. mismatched pattern may occur as tattoo. C.\nTeCH relies on robust SMPL-X pose estimation, which is\nstill an unsolved problem, especially for challenging poses.\nA. Extremely loose clothes\nB. Mismatched patterns\nC. SMPL-X Failure\nFigure 11. The proposed method might exhibit noisy surfaces for\nextremely loose clothing, or mismatched patterns. If PIXIE [26] is\npredicting a wrong initial pose, the error propagates to TeCH.\nEfficiency. For each subject, training DreamBooth takes 20\nmin, DMTet SMPL-X initialization takes 20 min, geometry\nstage (coarse-50 min, fine-50 min), mesh post-processing\ntakes 10 min (remeshing, SMPL-X registration, hand re-\nplacement), texture stage takes 140 min, 270 min in total.\nThus, our per-subject optimization process remains time-\nconsuming, requiring approximately 4.5 hours per subject\non a V100 GPU. Addressing these limitations is crucial to\nfacilitate broader applications.\nFuture work. Leveraging controllable T2I models [52, 82,\n96, 134] may help to improve the controllability and sta-\nbility of generation process. Also, how to compositionally\ngenerate the separate components, such as haircut [107],\naccessories [31], and decoupled outfits [27], is still an un-\nsolved problem. We leave these for future research.\nBroader impact.\nTeCH has many potential applica-\ntions Sec. 5. However, as the technique advances, it has\nthe potential to facilitate deep-fake avatars and raise IP con-\ncerns. Regulations should be established to address these\nissues alongside its benefits in the entertainment industry.\n7. Conclusion\nWe have proposed TeCH to reconstruct a lifelike 3D clothed\nhuman from a single image, with detailed full-body geom-\netry and high-quality, consistent texture. The core insight\nis that we can leverage descriptive text prompts and person-\nalized Text-to-Image diffusion models to optimize the 3D\navatar including parts that are not visible in the input. Ex-\ntensive experiments validate the superiority of TeCH over\nexisting methods in terms of geometry and rendering qual-\nity. We believe that this paradigm of using image and tex-\ntual descriptions for 3D body reconstruction is a stepping\nstone also for reconstruction tasks beyond human bodies.\nAcknowledgments. Haven Feng contributes the core idea of \u201cchamfer distance in\nRGB space\u201d Eq. (12). We thank Vanessa Sklyarova for proofreading, Haofan Wang,\nHuaxia Li, and Xu Tang for their technical support, and Weiyang Liu\u2019s and Michael\nJ. Black\u2019s feedback. Yuliang Xiu is funded by the European Union\u2019s Horizon 2020\nresearch and innovation programme under the Marie Sk\u0142odowska-Curie grant agree-\nment No.860768 (CLIPE). Hongwei Yi is supported by the German Federal Ministry\nof Education and Research (BMBF): Tubingen AI Center, FKZ: 01IS18039B. Yangyi\nHuang is supported by the National Nature Science Foundation of China (Grant Nos:\n62273302, 62036009, 61936006). Jiaxiang Tang is supported by National Natural\nScience Foundation of China (Grant Nos: 61632003, 61375022, 61403005).\n10\nAppendices\nWe provide an additional introduction to the preliminar-\nies (Sec. A) of TeCH. We list the VQA questions PVQA\n(Sec. B). Additional implementation details to construct the\nouter shell around SMPL-X (Sec. C), as well as details\non the camera sampling strategy (Sec. D) are given. Im-\nplementation details of network structure and optimization\nsetting (Sec. E). Based on the benchmark datasets (CAPE,\nTHuman2.0) and in-the-wild photos used in the perceptual\nstudies, we present more qualitative results (Figs. 12 to 14).\nA. Preliminaries\nDreamBooth.\nPretrained text-to-Image diffusion mod-\nels [99, 100, 102] lack the ability to mimic the appearance of\nsubjects in a given reference set and synthesize novel rendi-\ntions of them in different contexts. To enable subject-driven\nimage generation, DreamBooth [101] personalizes the pre-\ntrained diffusion model through few-shot tuning.\nSpecifically, for a pre-trained image diffusion model \u02c6x\u03d5,\nthe model takes an initial noise \u03f5 \u223c N(0, 1), and a text\nembedding c = \u0393(P), generated by the text encoder \u0393 and\na text prompt P, to produce an image xgen = \u02c6x\u03d5(\u03f5, c).\nDreamBooth uses 3\u223c5 images of the same subject to fine-\ntune the diffusion model using MSE denoising losses:\nEx,c,\u03f5,\u03f5\u2032,t =\n\u0002\nwt \u2225\u02c6x\u03d5(\u03b1txgt + \u03c3t\u03f5, c) \u2212 xgt\u22252\n2\n+ \u03bbwt\u2032 \u2225\u02c6x\u03d5(\u03b1t\u2032xprior + \u03c3t\u2032\u03f5\u2032, cprior) \u2212 xprior\u22252\n2\n\u0003\n(13)\nWhere xgt represents ground-truth images, and c is the\nembedding of a text prompt with a rare token as the\nunique identifier, and \u03b1t, \u03c3t, wt controls the noise sched-\nule and sample quality of the diffusion process at time\nt \u223c U([0, 1]). The second term is the prior-preservation loss\nweighted by \u03bb, which is supervised by self-generated im-\nages xprior conditioned with the class-specific embedding\ncprior = \u0393(\u201ca man/woman\u201d). This loss mitigates the phe-\nnomenon of language drift, where the model collapses into\na single mode by associating the class name with a particu-\nlar instance, thus augmenting the output diversity.\nScore Distillation Sampling (SDS). DreamFusion [94] in-\ntroduces Score Distillation Sampling (SDS) loss, to perform\nText-to-3D synthesis by using pretrained 2D Text-to-Image\ndiffusion model \u03d5. Instead of sampling in pixel space, SDS\noptimizes over the 3D volume, which is parameterized with\n\u03b8, with the differential renderer g, so the generated image\nx = g(\u03b8) closely resembles a sample from the frozen diffu-\nsion model. Here is the gradient of LSDS:\n\u2207\u03b8LSDS(\u03d5, x = g(\u03b8))\n= Et,\u03f5\n\u0014\nwt (\u02c6\u03f5\u03d5(zx\nt ; c, t) \u2212 \u03f5) \u2202x\n\u2202\u03b8\n\u2202zx\n\u2202x\n\u0015\n(14)\nwhere \u02c6\u03f5\u03d5(zx\nt ; c, t) denotes the noise prediction of the dif-\nfusion model with condition c and latent zx\nt of the gener-\nated image x. Such SDS-guided optimization is performed\nwith random camera poses to improve the multi-view con-\nsistency. In contrast to DreamFusion, the 3D shape here is\nparameterized with an improved DMTet instead of NeRF.\nDeep Marching Tetrahedra (DMTet).\nDMTet [32, 106]\nis a hybrid 3D representation designed for high-resolution\n3D shape synthesis and reconstruction. It incorporates the\nadvantages of both explicit and implicit representations, by\nlearning Signed Distance Field (SDF) values on the ver-\ntices of a deformable tetrahedral grid. For a given DMTet,\nrepresented as (VT , T), where VT are the vertices in the\ntetrahedral grid T, comprising K tetrahedrons Tk \u2208 T,\nwith k \u2208 {1, . . . , K}.\nEach tetrahedron is defined by\nfour vertices {v1\nk, v2\nk, v3\nk, v4\nk}. The objective of the model\nis firstly to estimate the SDF value s(vi) for each ver-\ntex, then to iteratively refine the surface and subdivide the\ntetrahedral grid by predicting the position offsets \u2206vi and\nSDF residual values \u2206s(vi). A triangular mesh can be ex-\ntracted through Marching Tetrahedra [24].\nAs noted by\nMagic3D [73], DMTet offers two advantages over NeRF,\nfast-optimization and high-resolution. It achieves this by\nefficiently rasterizing a triangular mesh into high-resolution\nimage patches using a differentiable renderer [62], enabling\ninteraction with pre-trained high-resolution latent diffusion\nmodels, such as eDiff-I [7], and Stable Diffusion [100].\nB. VQA Questions Q\nTo construct the descriptive prompt PVQA, we designed\na series of questions to parse clothed human attributes.\nFirst, we use BLIP [65] and a series of general ques-\ntions Qgeneral to parse genders, facial appearance, hair\ncolors, hairstyles, facial hairs, and body poses.\nSec-\nondly, we use SegFormer [117] to parse human garments,\nconsisting of 10 categories {hat, sunglasses, upper-\nclothes, skirt, pants, dress, belt, shoes,\nbag, scarf}, denoted as G, and use another group of\nquestions Qgarments to parse the attribute of each garment\ng \u2208 G. All the questions are listed in Tab. 4.\nEmpirically, we found that the BLIP [65] VQA model\ntends to use 1 \u223c 3 words to answer these questions, so\nwe simply concatenate all the answers and remove repeated\nwords to construct PVQA. Note that for the CAPE dataset,\nwe add the dataset-specific description \u201chairnet\u201d to the\nguidance as it is hard to be recognized by BLIP.\nC. Construction of the Outer SMPL-X Shell\nTo construct a compact tetrahedral grid (Vshell, Tshell), we\ncalculate a coarse outer shell Mshell from SMPL-X esti-\nmated body mesh Mbody. Specifically, we dilate Mbody\nwith an offset of \u2206Mbody = 0.1 and simplify the mesh\n11\nGroups\nQuetions\nQgeneral\nIs this person a man or a woman?\nWhat is this person wearing?\nWhat is the hair color of this person?\nWhat is the hairstyle of this person?\nDescribe the facial appearance of this person.\nDoes this person have facial hair?\nHow is the facial hair of this person?\nDescribe the pose of this person.\nQgarments\nIs this person wearing g?\nWhat g is the person wearing? \u2192 d\nWhat is the color of the d + g?\nWhat is the style of the d + g?\nTable 4. Predefined questions for parsing clothed human at-\ntributes. g is the segmentation category of a part of the garments,\nand d is the recognized garment category from the answer to the\nsecond question in Qgarments.\nby reducing triangle numbers by rdecimate = 90% using\nquadric decimation [42]. The we generate the tetrahedral\ngrid (Vshell, Tshell) of this outer shell by TetGen [37] with a\nmaximum volume size of 5 \u00d7 10\u22128.\nD. Camera Sampling\nTo ensure full coverage of the entire body and the human\nface, during optimization process, we sample virtual camera\nposes into two groups: 1) Kbody cameras with a field of\nview (FOV) covering the full body or the main body parts,\nand 2) zoom-in cameras Kface focusing the face region.\nThe ratio Pbody determines the probability of sampling\nk \u2208 Kbody, while the height hbody, radius rbody, elevation\nangle \u03d5body, and azimuth ranges \u03b8body are adjusted relative\nto the SMPL-X body scale. Empirically, we set Pbody =\n0.7, hbody = (\u22120.4, 0.4), rbody = (0.7, 1.3), \u03b8body =\n[\u2212180\u25e6, 180\u25e6), \u03d5body = {0\u25e6}, with the Mbody propor-\ntionally scaled to a unit space with xyz coordinates in the\nrange [\u22120.5, 0.5]. To mitigate the occurrence of mirrored\nappearance artifacts (i.e., Janus-head), we incorporate view-\naware prompts, \u201cfront/side/back/overhead view\u201d,\nw.r.t. the viewing angle during generation process, whose\neffectiveness has been demonstrated in DreamBooth [94].\nIn order to enhance facial details, we sample additional\nvirtual cameras positioned around the face k \u2208 Kface, to-\ngether with the additional prompt \u201cface of\u201d. With a prob-\nability of Pface = 1 \u2212 Pbody = 0.3, the sampling param-\neters include the view target cface, radius range rface, ro-\ntation range \u03b8face, and azimuth range \u03d5face. Empirically,\nwe set cface to the 3D position of SMPL-X head keypoint,\nrface = [0.3, 0.4], \u03b8face = [\u221290\u25e6, 90\u25e6] and \u03d5face = {0\u25e6}.\nE. Implementation Details\nE.1. Network Structure\nWe use two networks \u03a8g and \u03a8c to predict the SDF for ge-\nometry modeling and to predict the RGB value for albedo\ntexture modeling, respectively. For \u03a8g, we use a 2-layer\nMLP network with a hidden dimension of 32 and a hash po-\nsitional encoding with a maximum resolution of 1028 and\n16 resolution levels. During the forward process, we use\ncoordinates of Vshell in the normalized unit space, the ver-\ntices of the tetrahedral grid as the input of \u03a8g to query SDF\nvalue for each vertex.\nFor \u03a8c, we use a similar network with 1-layer MLP and\na hash positional encoding with a maximum resolution of\n2048.\nWe model the albedo texture in the canonical A-\npose 3D space. Specifically, for the post-processed result\nmesh Min = (Vin, F), we register the model with SMPL-\nX, and repose it with the standard A-pose MA = (VA, F).\nDuring rendering, if a target pixel is projected onto a tri-\nangle (vi\nin, vj\nin, vj\nin), where(i, j, k) \u2208 F of the Min.\nWe\nquery the pixel color with its corresponding 3d position in\nthe A-pose space, calculated by interpolation of the trian-\ngle (vi\nA, vj\nA, vj\nA). Additionally, we use two 2-layer MLP\n\u03a8g\nbg, \u03a8c\nbg conditioned by camera k to learn adaptive 3D\nbackground colors for both normal map rendering N(M, k)\nand color rendering I\u2032(M, \u03c8c, k).\nE.2. Optimization Details\nIn both stages of our multi-stage optimization pipeline, we\nuse an Adam optimizer with a base learning rate of \u03b7 =\n1 \u00d7 10\u22123, and weight decay of \u03bbWD = 5 \u00d7 10\u22124\nGeometry-stage optimization.\nWe optimize \u03a8g in a\ncoarse-to-fine manner, with tcoarse = 5000 steps w/o mesh\nsubdivision and tfine = 5000 steps w/ mesh subdivision.\nWe use a loss weight setting of \u03bbsil = 1 \u00d7 104, \u03bbSDS = 1,\n\u03bblap = 1 \u00d7 104, and a base loss weight \u03bbbase\nnorm = 1 \u00d7 104.\nFor \u03bbnorm, to ensure robust convergence of the geometry,\nwe start with a higher value of \u03bbnorm during each stage and\ngradually decrease it using a two-round cosine annealing,\nwhere \u03bbnorm(t) is the weight of Lnorm at the t-th iteration:\n\u03bbnorm(t) =\n\uf8f1\n\uf8f2\n\uf8f3\n0.5\u03bbbase\nnorm\n\u0010\n1 + cos\n\u0010\nt\ntcoarse \u03c0\n\u0011\u0011\nif t < tcoarse\n0.5\u03bbbase\nnorm\n\u0010\n1 + cos\n\u0010\nt\u2212tcoarse\ntfine\n\u03c0\n\u0011\u0011\nif t \u2265 tcoarse\n,\n(15)\nTexture-stage optimization.\nWe optimize \u03a8c\nfor\nttexture = 7000 steps, with \u03bbrecon = 2\u00d7104 and \u03bbSDS = 1.\nBesides, we set \u03bbCD = 0 at the beginning of the training,\nand \u03bbSDS = 1 \u00d7 106 at the last tCD = 2000 iterations to\nenforce color consistency.\n12\nGT\nPIFu\nPaMIR\nPhorhum\nPIFuHD\nECON\nICON\nTeCH\nPIFu\nPaMIR\nPhorhum\nTeCH\nGT\nGT\nFigure 12. Qualitative comparison on CAPE. TeCH performs better on subjects with challenging poses.\n13\nGT\nPIFu\nPaMIR\nPhorhum\nPIFuHD\nECON\nICON\nTeCH\nPIFu\nPaMIR\nPhorhum\nTeCH\nGT\nGT\nFigure 13. Qualitative comparison on THuman2.0. TeCH performs better regardless of hard pose, complex texture, or loose clothing.\n14\nPIFu\nInput\nPaMIR\nPHORHUM\nTeCH\nPIFu\nPaMIR\nPHORHUM\nTeCH\nPIFu\nPaMIR\nTeCH\nPHORHUM\nPIFu\nPaMIR\nTeCH\nPHORHUM\nPIFu\nPaMIR\nPHORHUM\nTeCH\nPIFu\nPaMIR\nTeCH\nPHORHUM\nPIFu\nPaMIR\nTeCH\nPHORHUM\nPIFu\nPaMIR\nPHORHUM\nTeCH\nPIFu\nPaMIR\nTeCH\nPHORHUM\nFigure 14. Qualitative comparison on SHHQ images. TeCH generalizes well on in-the-wild images with diverse clothing styles and\ntextures. It successfully recovers the overall structure of the clothed body with text guidance, and generates realistic full-body texture\nwhich is consistent with the colored pattern and the material of the clothes. \u00fc Zoom in to see the geometric details.\n15\nReferences\n[1] Thiemo Alldieck, Marcus A. Magnor, Weipeng Xu, Chris-\ntian Theobalt, and Gerard Pons-Moll.\nDetailed human\navatars from monocular video. In International Conference\non 3D Vision (3DV), 2018. 2\n[2] Thiemo Alldieck, Marcus A. Magnor, Weipeng Xu, Chris-\ntian Theobalt, and Gerard Pons-Moll. Video based recon-\nstruction of 3D people models. In Computer Vision and\nPattern Recognition (CVPR), 2018.\n[3] Thiemo Alldieck, Marcus A. Magnor, Bharat Lal Bhatna-\ngar, Christian Theobalt, and Gerard Pons-Moll. Learning\nto reconstruct people in clothing from a single RGB cam-\nera. In Computer Vision and Pattern Recognition (CVPR),\n2019.\n[4] Thiemo Alldieck, Gerard Pons-Moll, Christian Theobalt,\nand Marcus Magnor.\nTex2Shape: Detailed Full Human\nBody Geometry From a Single Image.\nIn International\nConference on Computer Vision (ICCV), 2019. 2\n[5] Thiemo Alldieck, Mihai Zanfir, and Cristian Sminchis-\nescu.\nPhotorealistic monocular 3d reconstruction of hu-\nmans wearing clothing. In Computer Vision and Pattern\nRecognition (CVPR), 2022. 2, 3, 7, 8, 9\n[6] Rie Ando and Tong Zhang. Learning on graph with lapla-\ncian regularization. Conference on Neural Information Pro-\ncessing Systems (NeurIPS), 2006. 6\n[7] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vah-\ndat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika\nAittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero\nKarras, and Ming-Yu Liu.\neDiff-I: Text-to-Image Diffu-\nsion Models with Ensemble of Expert Denoisers.\narXiv\npreprint:2211.01324, 2022. 11\n[8] Alexander Bergman, Petr Kellnhofer, Wang Yifan, Eric\nChan, David Lindell, and Gordon Wetzstein. Generative\nneural articulated radiance fields.\nConference on Neural\nInformation Processing Systems (NeurIPS), 2022. 3\n[9] Bharat Lal Bhatnagar, Garvita Tiwari, Christian Theobalt,\nand Gerard Pons-Moll. Multi-Garment Net: Learning to\ndress 3D people from images. In International Conference\non Computer Vision (ICCV), 2019. 2\n[10] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-\nstuff: Thing and stuff classes in context. In Computer Vision\nand Pattern Recognition (CVPR), pages 1209\u20131218, 2018.\n8\n[11] Zhongang Cai, Daxuan Ren, Ailing Zeng, Zhengyu Lin,\nTao Yu, Wenjia Wang, Xiangyu Fan, Yang Gao, Yi-\nfan Yu, Liang Pan, Fangzhou Hong, Mingyuan Zhang,\nChen Change Loy, Lei Yang, and Ziwei Liu. HuMMan:\nMulti-modal 4d human dataset for versatile sensing and\nmodeling.\nIn European Conference on Computer Vision\n(ECCV), 2022. 3\n[12] Yukang Cao, Guanying Chen, Kai Han, Wenqi Yang, and\nKwan-Yee K. Wong. JIFF: Jointly-aligned Implicit Face\nFunction for High Quality Single View Clothed Human Re-\nconstruction. In Computer Vision and Pattern Recognition\n(CVPR), 2022. 2\n[13] Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, and Kwan-\nYee K Wong. DreamAvatar: Text-and-Shape Guided 3D\nHuman Avatar Generation via Diffusion Models.\narXiv\npreprint:2304.00916, 2023. 3\n[14] Eric R. Chan, Koki Nagano, Matthew A. Chan, Alexan-\nder W. Bergman, Jeong Joon Park, Axel Levy, Miika Ait-\ntala, Shalini De Mello, Tero Karras, and Gordon Wetzstein.\nGeNVS: Generative novel view synthesis with 3D-aware\ndiffusion models. In arXiv, 2023. 4\n[15] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu\nSoricut. Conceptual 12M: Pushing web-scale image-text\npre-training to recognize long-tail visual concepts. In Com-\nputer Vision and Pattern Recognition (CVPR), 2021. 8\n[16] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fanta-\nsia3D: Disentangling Geometry and Appearance for High-\nquality Text-to-3D Content Creation. In International Con-\nference on Computer Vision (ICCV), 2023. 6\n[17] Xu Chen, Tianjian Jiang, Jie Song, Jinlong Yang, Michael J\nBlack, Andreas Geiger, and Otmar Hilliges. gDNA: To-\nwards generative detailed neural avatars. In Computer Vi-\nsion and Pattern Recognition (CVPR), 2022. 3\n[18] Wei Cheng, Ruixiang Chen, Wanqi Yin, Siming Fan, Keyu\nChen, Honglin He, Huiwen Luo, Zhongang Cai, Jingbo\nWang, Yang Gao, Zhengming Yu, Zhengyu Lin, Daxuan\nRen, Lei Yang, Ziwei Liu, Chen Change Loy, Chen Qian,\nWayne Wu, Dahua Lin, Bo Dai, and Kwan-Yee Lin. DNA-\nRendering: A Diverse Neural Actor Repository for High-\nFidelity Human-centric Rendering. In International Con-\nference on Computer Vision (ICCV), 2023. 3\n[19] Vasileios Choutas, Georgios Pavlakos, Timo Bolkart, Dim-\nitrios Tzionas, and Michael J. Black. Monocular expressive\nbody regression through body-driven attention. In Euro-\npean Conference on Computer Vision (ECCV), pages 20\u2013\n40, 2020. 8\n[20] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo\nScharw\u00a8achter, Markus Enzweiler, Rodrigo Benenson, Uwe\nFranke, Stefan Roth, and Bernt Schiele.\nThe cityscapes\ndataset. In CVPR Workshop on the Future of Datasets in\nVision. sn, 2015. 8\n[21] Enric Corona, Mihai Zanfir, Thiemo Alldieck, Eduard\nGabriel Bazavan, Andrei Zanfir, and Cristian Sminchisescu.\nStructured 3d features for reconstructing relightable and an-\nimatable avatars. In Computer Vision and Pattern Recogni-\ntion (CVPR), 2023. 2, 3\n[22] Congyue Deng, Chiyu Jiang, Charles R Qi, Xinchen Yan,\nYin Zhou, Leonidas Guibas, Dragomir Anguelov, et al.\nNeRDi:\nSingle-View NeRF Synthesis with Language-\nGuided Diffusion as General Image Priors. In Computer\nVision and Pattern Recognition (CVPR), 2023. 4\n[23] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical im-\nage database. In Computer Vision and Pattern Recognition\n(CVPR), pages 248\u2013255. Ieee, 2009. 8\n[24] Akio Doi and Akio Koide. An efficient method of triangu-\nlating equi-valued surfaces by using tetrahedral cells. IE-\nICE TRANSACTIONS on Information and Systems, 74(1):\n214\u2013224, 1991. 5, 6, 11\n[25] Zijian Dong, Xu Chen, Jinlong Yang, Michael J Black, Ot-\nmar Hilliges, and Andreas Geiger. AG3D: Learning to Gen-\n16\nerate 3D Avatars from 2D Image Collections. In Interna-\ntional Conference on Computer Vision (ICCV), 2023. 3\n[26] Yao Feng, Vasileios Choutas, Timo Bolkart, Dimitrios\nTzionas, and Michael J. Black. Collaborative regression of\nexpressive bodies using moderation. In International Con-\nference on 3D Vision (3DV), pages 792\u2013804, 2021. 2, 5, 8,\n10\n[27] Yao Feng, Jinlong Yang, Marc Pollefeys, Michael J. Black,\nand Timo Bolkart. Capturing and animation of body and\nclothing from monocular video. In SIGGRAPH Asia 2022\nConference Papers, 2022. 10\n[28] Jianglin Fu, Shikai Li, Yuming Jiang, Kwan-Yee Lin,\nChen Qian, Chen-Change Loy, Wayne Wu, and Ziwei Liu.\nStyleGAN-Human: A Data-Centric Odyssey of Human\nGeneration.\nEuropean Conference on Computer Vision\n(ECCV), 2022. 3, 8, 9\n[29] Valentin Gabeur, Jean-S\u00b4ebastien Franco, Xavier Martin,\nCordelia Schmid, and Gregory Rogez. Moulding humans:\nNon-parametric 3D human shape estimation from single\nimages. In International Conference on Computer Vision\n(ICCV), 2019. 2\n[30] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\nAmit H Bermano, Gal Chechik, and Daniel Cohen-Or. An\nimage is worth one word: Personalizing text-to-image gen-\neration using textual inversion. In International Conference\non Learning Representations (ICLR), 2023. 4\n[31] Daiheng Gao, Yuliang Xiu, Kailin Li, Lixin Yang, Feng\nWang, Peng Zhang, Bang Zhang, Cewu Lu, and Ping Tan.\nDART: Articulated Hand Model with Diverse Accessories\nand Rich Textures. In Thirty-sixth Conference on Neural\nInformation Processing Systems Datasets and Benchmarks\nTrack, 2022. 10\n[32] Jun Gao, Wenzheng Chen, Tommy Xiang, Alec Jacob-\nson, Morgan McGuire, and Sanja Fidler.\nLearning de-\nformable tetrahedral meshes for 3d reconstruction. Confer-\nence on Neural Information Processing Systems (NeurIPS),\n33:9936\u20139947, 2020. 2, 5, 11\n[33] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen,\nKangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja\nFidler. GET3D: A Generative Model of High Quality 3D\nTextured Shapes Learned from Images. In Conference on\nNeural Information Processing Systems (NeurIPS), 2022. 3\n[34] Yuying Ge, Ruimao Zhang, Lingyun Wu, Xiaogang Wang,\nXiaoou Tang, and Ping Luo. A versatile benchmark for de-\ntection, pose estimation, segmentation and re-identification\nof clothing images. In Computer Vision and Pattern Recog-\nnition (CVPR), 2019. 3\n[35] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\nBatra, and Devi Parikh. Making the V in VQA matter: El-\nevating the role of image understanding in Visual Question\nAnswering. In Conference on Computer Vision and Pattern\nRecognition (CVPR), 2017. 8\n[36] Artur Grigorev, Karim Iskakov, Anastasia Ianina, Renat\nBashirov, Ilya Zakharkin, Alexander Vakhitov, and Victor\nLempitsky. Stylepeople: A generative model of fullbody\nhuman avatars. In Computer Vision and Pattern Recogni-\ntion (CVPR), pages 5151\u20135160, 2021. 3\n[37] Si Hang. Tetgen, a delaunay-based quality tetrahedral mesh\ngenerator. ACM Trans. Math. Softw, 41(2):11, 2015. 12\n[38] Tong He, John P. Collomosse, Hailin Jin, and Stefano\nSoatto.\nGeo-PIFu: Geometry and pixel aligned implicit\nfunctions for single-view human reconstruction. In Confer-\nence on Neural Information Processing Systems (NeurIPS),\n2020. 2\n[39] Tong He, Yuanlu Xu, Shunsuke Saito, Stefano Soatto, and\nTony Tung. ARCH++: Animation-Ready Clothed Human\nReconstruction Revisited. In International Conference on\nComputer Vision (ICCV), pages 11046\u201311056, 2021. 2, 3\n[40] Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang\nCai, Lei Yang, and Ziwei Liu. Avatarclip: Zero-shot text-\ndriven generation and animation of 3d avatars. Transactions\non Graphics (TOG), 2022. 3\n[41] Fangzhou Hong, Zhaoxi Chen, Yushi Lan, Liang Pan, and\nZiwei Liu. EVA3D: Compositional 3D Human Generation\nfrom 2D Image Collections. In International Conference\non Learning Representations (ICLR), 2023. 3\n[42] Hugues Hoppe. New quadric metric for simplifying meshes\nwith appearance attributes. In Proceedings Visualization\u201999\n(Cat. No. 99CB37067), pages 59\u2013510. IEEE, 1999. 12\n[43] Shoukang Hu, Fangzhou Hong, Liang Pan, Haiyi Mei, Lei\nYang, and Ziwei Liu.\nSherf: Generalizable human nerf\nfrom a single image. In International Conference on Com-\nputer Vision (ICCV), 2023. 3\n[44] Yukun Huang, Jianan Wang, Ailing Zeng, He Cao, Xi-\nanbiao Qi, Yukai Shi, Zheng-Jun Zha, and Lei Zhang.\nDreamWaltz: Make a Scene with Complex 3D Animatable\nAvatars. arXiv preprint:2305.12529, 2023. 3\n[45] Yangyi Huang, Hongwei Yi, Weiyang Liu, Haofan Wang,\nBoxi Wu, Wenxiao Wang, Binbin Lin, Debing Zhang,\nand Deng Cai. One-shot implicit animatable avatars with\nmodel-based priors. In International Conference on Com-\nputer Vision (ICCV), 2023. 3\n[46] Zeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, and\nTony Tung. ARCH: Animatable Reconstruction of Clothed\nHumans.\nIn Computer Vision and Pattern Recognition\n(CVPR), pages 3093\u20133102, 2020. 2, 3\n[47] Mustafa Is\u00b8\u0131k, Martin R\u00a8unz, Markos Georgopoulos, Taras\nKhakhulin,\nJonathan\nStarck,\nLourdes\nAgapito,\nand\nMatthias Nie\u00dfner. HumanRF: High-Fidelity Neural Radi-\nance Fields for Humans in Motion. Transactions on Graph-\nics (TOG), 2023. 3\n[48] Ajay Jain, Matthew Tancik, and Pieter Abbeel.\nPutting\nNeRF on a Diet: Semantically Consistent Few-Shot View\nSynthesis. In International Conference on Computer Vision\n(ICCV), pages 5885\u20135894, 2021. 3\n[49] Boyi Jiang, Juyong Zhang, Yang Hong, Jinhao Luo, Lig-\nang Liu, and Hujun Bao. BCNet: Learning body and cloth\nshape from a single image.\nIn European Conference on\nComputer Vision (ECCV), 2020. 2\n[50] Ruixiang Jiang, Can Wang, Jingbo Zhang, Menglei Chai,\nMingming He, Dongdong Chen, and Jing Liao. Avatarcraft:\nTransforming text into neural human avatars with parame-\nterized shape and pose control. In International Conference\non Computer Vision (ICCV), 2023. 3\n17\n[51] Hanbyul Joo, Tomas Simon, and Yaser Sheikh. Total cap-\nture: A 3d deformation model for tracking faces, hands,\nand bodies. In Computer Vision and Pattern Recognition\n(CVPR), 2018. 2, 3\n[52] Xuan Ju, Ailing Zeng, Chenchen Zhao, Jianan Wang, Lei\nZhang, and Qiang Xu.\nHumanSD: A Native Skeleton-\nGuided Diffusion Model for Human Image Generation.\nIn International Conference on Computer Vision (ICCV),\n2023. 10\n[53] Angjoo Kanazawa, Michael J. Black, David W. Jacobs, and\nJitendra Malik. End-to-end recovery of human shape and\npose. In Computer Vision and Pattern Recognition (CVPR),\npages 7122\u20137131, 2018. 2\n[54] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila. Analyzing and improving\nthe image quality of StyleGAN. In Computer Vision and\nPattern Recognition (CVPR), 2020. 3\n[55] Byungjun Kim, Patrick Kwon, Kwangho Lee, Myunggi\nLee, Sookwan Han, Daesik Kim, and Hanbyul Joo. Chupa:\nCarving 3D Clothed Humans from Skinned Shape Priors\nusing 2D Diffusion Probabilistic Models. In International\nConference on Computer Vision (ICCV), 2023. 3\n[56] Muhammed Kocabas, Nikos Athanasiou, and Michael J.\nBlack. VIBE: Video inference for human body pose and\nshape estimation. In Computer Vision and Pattern Recog-\nnition (CVPR), pages 5252\u20135262, 2020. 2\n[57] Muhammed Kocabas, Chun-Hao P. Huang, Otmar Hilliges,\nand Michael J. Black. PARE: Part attention regressor for\n3D human body estimation. In International Conference\non Computer Vision (ICCV), pages 11127\u201311137, 2021.\n[58] Muhammed Kocabas, Chun-Hao P. Huang, Joachim Tesch,\nLea M\u00a8uller, Otmar Hilliges, and Michael J. Black. SPEC:\nSeeing people in the wild with an estimated camera. In In-\nternational Conference on Computer Vision (ICCV), pages\n11035\u201311045, 2021.\n[59] Nikos Kolotouros, Georgios Pavlakos, Michael J. Black,\nand Kostas Daniilidis.\nLearning to reconstruct 3D hu-\nman pose and shape via model-fitting in the loop. In In-\nternational Conference on Computer Vision (ICCV), pages\n2252\u20132261, 2019. 2\n[60] Nikos Kolotouros, Thiemo Alldieck, Andrei Zanfir, Ed-\nuard Gabriel Bazavan, Mihai Fieraru, and Cristian Smin-\nchisescu.\nDreamHuman: Animatable 3D Avatars from\nText. arXiv preprint:2306.09329, 2023. 3\n[61] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A Shamma, et al. Visual genome:\nConnecting language and vision using crowdsourced dense\nimage annotations. International Journal of Computer Vi-\nsion (IJCV), 123:32\u201373, 2017. 8\n[62] Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol,\nJaakko Lehtinen, and Timo Aila. Modular primitives for\nhigh-performance differentiable rendering. Transactions on\nGraphics (TOG), 39(6), 2020. 5, 11\n[63] Verica Lazova, Eldar Insafutdinov, and Gerard Pons-Moll.\n360-Degree textures of people in clothing from a single im-\nage. In International Conference on 3D Vision (3DV), 2019.\n2\n[64] Jiefeng Li, Chao Xu, Zhicun Chen, Siyuan Bian, Lixin\nYang, and Cewu Lu. HybrIK: A hybrid analytical-neural\ninverse kinematics solution for 3D human pose and shape\nestimation. In Computer Vision and Pattern Recognition\n(CVPR), pages 3383\u20133393, 2021. 2\n[65] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-image pre-training for uni-\nfied vision-language understanding and generation.\nIn\nInternational Conference on Machine Learning (ICML),\npages 12888\u201312900. PMLR, 2022. 2, 4, 8, 11\n[66] Jiefeng Li, Siyuan Bian, Qi Liu, Jiasheng Tang, Fan Wang,\nand Cewu Lu. NIKI: Neural inverse kinematics with invert-\nible neural networks for 3d human pose and shape estima-\ntion. In Computer Vision and Pattern Recognition (CVPR),\n2023. 2\n[67] Ruilong Li, Kyle Olszewski, Yuliang Xiu, Shunsuke Saito,\nZeng Huang, and Hao Li. Volumetric human teleportation.\nIn ACM SIGGRAPH 2020 Real-Time Live, 2020. 2\n[68] Zhihao Li, Jianzhuang Liu, Zhensong Zhang, Songcen Xu,\nand Youliang Yan. CLIFF: Carrying Location Information\nin Full Frames into Human Pose and Shape Estimation. In\nEuropean Conference on Computer Vision (ECCV), pages\n590\u2013606. Springer, 2022. 2\n[69] Xiaodan Liang, Si Liu, Xiaohui Shen, Jianchao Yang, Lu-\noqi Liu, Jian Dong, Liang Lin, and Shuicheng Yan. Deep\nhuman parsing with active template regression.\nTrans-\nactions on Pattern Analysis and Machine Intelligence\n(TPAMI), 37(12):2402\u20132414, 2015. 8\n[70] Xiaodan Liang, Si Liu, Xiaohui Shen, Jianchao Yang, Lu-\noqi Liu, Jian Dong, Liang Lin, and Shuicheng Yan. Deep\nhuman parsing with active template regression.\nTrans-\nactions on Pattern Analysis and Machine Intelligence\n(TPAMI), 37(12):2402\u20132414, 2015. 4\n[71] Xiaodan Liang, Chunyan Xu, Xiaohui Shen, Jianchao\nYang, Si Liu, Jinhui Tang, Liang Lin, and Shuicheng Yan.\nHuman parsing with contextualized convolutional neural\nnetwork. In International Conference on Computer Vision\n(ICCV), pages 1386\u20131394, 2015. 4\n[72] Tingting Liao, Xiaomei Zhang, Yuliang Xiu, Hongwei Yi,\nXudong Liu, Guo-Jun Qi, Yong Zhang, Xuan Wang, Xi-\nangyu Zhu, and Zhen Lei. High-Fidelity Clothed Avatar\nReconstruction from a Single Image. In Computer Vision\nand Pattern Recognition (CVPR), 2023. 2\n[73] Chen-Hsuan\nLin,\nJun\nGao,\nLuming\nTang,\nTowaki\nTakikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja\nFidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3D: High-\nResolution Text-to-3D Content Creation. In Computer Vi-\nsion and Pattern Recognition (CVPR), 2023. 11\n[74] Tsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and\nC Lawrence Zitnick. Microsoft COCO: common objects\nin context. In European Conference on Computer Vision\n(ECCV), pages 740\u2013755, 2014. 8\n[75] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund\nT, Zexiang Xu, and Hao Su. One-2-3-45: Any Single Image\nto 3D Mesh in 45 Seconds without Per-Shape Optimization.\narXiv preprint, 2023. 4\n18\n[76] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot One Image to 3D Object. In International Con-\nference on Computer Vision (ICCV), 2023. 4\n[77] Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou\nTang. Deepfashion: Powering robust clothes recognition\nand retrieval with rich annotations. In Proceedings of IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2016. 3\n[78] Matthew Loper, Naureen Mahmood, Javier Romero, Ger-\nard Pons-Moll, and Michael J. Black. SMPL: A skinned\nmulti-person linear model.\nTransactions on Graphics\n(TOG), 34(6):248:1\u2013248:16, 2015. 2, 3\n[79] Qianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades,\nGerard Pons-Moll, Siyu Tang, and Michael J. Black. Learn-\ning to Dress 3D People in Generative Clothing. In Com-\nputer Vision and Pattern Recognition (CVPR), 2020. 3, 8\n[80] Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje,\nGerard Pons-Moll, and Michael J. Black. AMASS: Archive\nof Motion Capture as Surface Shapes.\nIn International\nConference on Computer Vision (ICCV), pages 5442\u20135451,\n2019. 10\n[81] Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and\nAndrea Vedaldi. RealFusion: 360 Reconstruction of Any\nObject from a Single Image. In Computer Vision and Pat-\ntern Recognition (CVPR), 2023. 4\n[82] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang,\nZhongang Qi,\nYing Shan,\nand Xiaohu Qie.\nT2i-\nadapter:\nLearning adapters to dig out more control-\nlable ability for text-to-image diffusion models.\narXiv\npreprint:2302.08453, 2023. 10\n[83] Thomas M\u00a8uller,\nAlex Evans,\nChristoph Schied,\nand\nAlexander Keller. Instant neural graphics primitives with\na multiresolution hash encoding.\nACM Transactions on\nGraphics (ToG), 41(4):1\u201315, 2022. 5\n[84] Edwin G. Ng, Bo Pang, Piyush Kumar Sharma, and Radu\nSoricut.\nUnderstanding guided image captioning perfor-\nmance across domains. In Conference on Computational\nNatural Language Learning, 2020. 8\n[85] Atsuhiro Noguchi, Xiao Sun, Stephen Lin, and Tatsuya\nHarada. Unsupervised learning of efficient geometry-aware\nneural articulated representations. In European Conference\non Computer Vision (ECCV), pages 597\u2013614. Springer,\n2022. 3\n[86] Hayato Onizuka, Zehra Haiyrci, Diego Thomas, Akihiro\nSugimoto, Hideaki Uchiyama, and Rin-Ichiro Taniguchi.\nTetraTSDF: 3D human reconstruction from a single image\nwith a tetrahedral outer shell. In Computer Vision and Pat-\ntern Recognition (CVPR), 2020. 5\n[87] Vicente Ordonez, Girish Kulkarni, and Tamara Berg.\nIm2text: Describing images using 1 million captioned pho-\ntographs. In Conference on Neural Information Processing\nSystems (NeurIPS), 2011. 8\n[88] Pablo Palafox, Alja\u02c7z Bo\u02c7zi\u02c7c, Justus Thies, Matthias Nie\u00dfner,\nand Angela Dai. NPMs: Neural Parametric Models for 3D\nDeformable Shapes. In International Conference on Com-\nputer Vision (ICCV), 2021. 3\n[89] Pablo Palafox, Nikolaos Sarafianos, Tony Tung, and An-\ngela Dai. Spams: Structured implicit parametric models.\nComputer Vision and Pattern Recognition (CVPR), 2022. 3\n[90] Omkar M. Parkhi, Andrea Vedaldi, and Andrew Zisserman.\nDeep face recognition. In British Machine Vision Confer-\nence (BMVC), 2015. 8\n[91] Priyanka Patel, Chun-Hao Paul Huang, Joachim Tesch,\nDavid Hoffmann, Shashank Tripathi, and Michael J. Black.\nAGORA: Avatars in geography optimized for regression\nanalysis.\nIn Computer Vision and Pattern Recognition\n(CVPR), pages 13468\u201313478, 2021. 8\n[92] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,\nTimo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and\nMichael J Black. Expressive body capture: 3d hands, face,\nand body from a single image. In Computer Vision and\nPattern Recognition (CVPR), pages 10975\u201310985, 2019. 2,\n3\n[93] Gerard Pons-Moll, Sergi Pujades, Sonny Hu, and Michael\nBlack. ClothCap: Seamless 4D Clothing Capture and Re-\ntargeting. International Conference on Computer Graphics\nand Interactive Techniques (SIGGRAPH), 36(4), 2017. Two\nfirst authors contributed equally. 2\n[94] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall. DreamFusion: Text-to-3d using 2d diffusion. In Inter-\nnational Conference on Learning Representations (ICLR),\n2023. 2, 3, 4, 6, 7, 11, 12\n[95] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren,\nAliaksandr Siarohin,\nBing Li,\nHsin-Ying Lee,\nIvan\nSkorokhodov,\nPeter Wonka,\nSergey Tulyakov,\net al.\nMagic123: One Image to High-Quality 3D Object Gen-\neration Using Both 2D and 3D Diffusion Priors.\narXiv\npreprint:2306.17843, 2023. 4\n[96] Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao\nFeng, Zhen Liu, Dan Zhang, Adrian Weller, and Bernhard\nSch\u00a8olkopf. Controlling Text-to-Image Diffusion by Orthog-\nonal Finetuning. arXiv preprint:2306.07280, 2023. 10\n[97] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning Transferable Vi-\nsual Models From Natural Language Supervision. In Inter-\nnational Conference on Machine Learning (ICML), pages\n8748\u20138763. PMLR, 2021. 3\n[98] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer,\nBen Mildenhall, Nataniel Ruiz, Shiran Zada, Kfir Aberman,\nMichael Rubenstein, Jonathan Barron, Yuanzhen Li, and\nVarun Jampani. DreamBooth3D: Subject-Driven Text-to-\n3D Generation. arXiv preprint: 2303.13508, 2023. 4\n[99] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott\nGray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya\nSutskever. Zero-Shot Text-to-Image Generation. In Inter-\nnational Conference on Machine Learning (ICML), 2021.\n11\n[100] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image\nsynthesis with latent diffusion models.\nIn Computer Vi-\nsion and Pattern Recognition (CVPR), pages 10684\u201310695,\n2022. 11\n19\n[101] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. DreamBooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. In Computer Vision and Pattern Recognition\n(CVPR), 2023. 2, 4, 11\n[102] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour, Burcu Karagol Ayan, S Sara Mahdavi,\nRapha Gontijo Lopes, et al. Photorealistic text-to-image\ndiffusion models with deep language understanding.\nIn\nConference on Neural Information Processing Systems\n(NeurIPS), 2022. 2, 11\n[103] Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Mor-\nishima, Hao Li, and Angjoo Kanazawa. PIFu: Pixel-aligned\nimplicit function for high-resolution clothed human digiti-\nzation.\nIn International Conference on Computer Vision\n(ICCV), pages 2304\u20132314, 2019. 2, 3, 7, 8, 9\n[104] Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul\nJoo.\nPIFuHD: Multi-Level Pixel-Aligned Implicit Func-\ntion for High-Resolution 3D Human Digitization. In Com-\nputer Vision and Pattern Recognition (CVPR), pages 81\u201390,\n2020. 2, 7, 8, 9\n[105] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade W Gordon, Ross Wightman, Mehdi Cherti, Theo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, Patrick Schramowski, Srivatsa R Kundurthy, Kather-\nine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and\nJenia Jitsev. LAION-5B: An open large-scale dataset for\ntraining next generation image-text models.\nIn Thirty-\nsixth Conference on Neural Information Processing Sys-\ntems Datasets and Benchmarks Track, 2022. 3, 8\n[106] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and\nSanja Fidler. Deep marching tetrahedra: a hybrid represen-\ntation for high-resolution 3d shape synthesis. Conference\non Neural Information Processing Systems (NeurIPS), 34:\n6087\u20136101, 2021. 2, 5, 11\n[107] Vanessa Sklyarova, Jenya Chelishev, Andreea Dogaru, Igor\nMedvedev, Victor Lempitsky, and Egor Zakharov. Neural\nHaircut: Prior-Guided Strand-Based Hair Reconstruction.\nIn International Conference on Computer Vision (ICCV),\n2023. 10\n[108] David Smith,\nMatthew Loper,\nXiaochen Hu,\nParis\nMavroidis, and Javier Romero. FACSIMILE: Fast and ac-\ncurate scans from an image in less than a second. In In-\nternational Conference on Computer Vision (ICCV), 2019.\n2\n[109] Jiang Suyi, Jiang Haoran, Wang Ziyu, Luo Haimin, Chen\nWenzheng, and Xu Lan. HumanGen: Generating Human\nRadiance Fields with Explicit Priors. In Computer Vision\nand Pattern Recognition (CVPR), 2023. 3\n[110] David Svitov, Dmitrii Gudkov, Renat Bashirov, and Victor\nLemptisky. Dinar: Diffusion inpainting of neural textures\nfor one-shot human avatars. In International Conference on\nComputer Vision (ICCV), 2023. 3\n[111] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran\nYi, Lizhuang Ma, and Dong Chen.\nMake-It-3D: High-\nFidelity 3D Creation from A Single Image with Diffusion\nPrior.\nIn International Conference on Computer Vision\n(ICCV), 2023. 4\n[112] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,\nand Greg Shakhnarovich. Score Jacobian Chaining: Lift-\ning Pretrained 2D Diffusion Models for 3D Generation. In\nComputer Vision and Pattern Recognition (CVPR), 2023. 3\n[113] Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jian-\nmin Bao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen,\nFang Wen, Qifeng Chen, et al. Rodin: A Generative Model\nfor Sculpting 3D Digital Avatars Using Diffusion. In Com-\nputer Vision and Pattern Recognition (CVPR), 2023. 3\n[114] Daniel Watson, William Chan, Ricardo Martin-Brualla,\nJonathan Ho,\nAndrea Tagliasacchi,\nand Mohammad\nNorouzi.\nNovel View Synthesis with Diffusion Models\n(3DiM). In International Conference on Learning Repre-\nsentations (ICLR), 2023. 4\n[115] Donglai Xiang, Hanbyul Joo, and Yaser Sheikh. Monocular\ntotal capture: Posing face, body, and hands in the wild. In\nComputer Vision and Pattern Recognition (CVPR), pages\n10957\u201310966, 2019. 8\n[116] Donglai Xiang, Fabian Prada, Chenglei Wu, and Jessica K.\nHodgins.\nMonoClothCap: Towards temporally coherent\nclothing capture from monocular RGB video. In Interna-\ntional Conference on 3D Vision (3DV), 2020. 2\n[117] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar,\nJose M Alvarez, and Ping Luo. SegFormer: Simple and\nefficient design for semantic segmentation with transform-\ners. Conference on Neural Information Processing Systems\n(NeurIPS), 34:12077\u201312090, 2021. 2, 4, 8, 11\n[118] Zhangyang Xiong, Di Kang, Derong Jin, Weikai Chen, Lin-\nchao Bao, and Xiaoguang Han.\nGet3DHuman: Lifting\nStyleGAN-Human into a 3D Generative Model using Pixel-\naligned Reconstruction Priors. In International Conference\non Computer Vision (ICCV), 2023. 3\n[119] Yuliang Xiu, Ruilong Li, Shunsuke Saito, Zeng Huang,\nKyle Olszewski, and Hao Li. Monocular real-time volu-\nmetric performance capture. In European Conference on\nComputer Vision (ECCV), pages 49\u201367, 2020. 2\n[120] Yuliang Xiu,\nJinlong Yang,\nDimitrios Tzionas,\nand\nMichael J. Black.\nICON: Implicit Clothed humans Ob-\ntained from Normals.\nIn Computer Vision and Pattern\nRecognition (CVPR), 2022. 2, 6, 7, 8, 9\n[121] Yuliang Xiu, Jinlong Yang, Xu Cao, Dimitrios Tzionas, and\nMichael J. Black.\nECON: Explicit Clothed humans Op-\ntimized via Normal integration. In Computer Vision and\nPattern Recognition (CVPR), 2023. 2, 3, 6, 7, 8, 9\n[122] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang,\nand Zhangyang Wang. NeuralLift-360: Lifting An In-the-\nwild 2D Photo to A 3D Object with 360\u00b0 Views. Computer\nVision and Pattern Recognition (CVPR), 2023. 4\n[123] Hongyi Xu, Eduard Gabriel Bazavan, Andrei Zanfir,\nWilliam T. Freeman, Rahul Sukthankar, and Cristian Smin-\nchisescu. GHUM & GHUML: Generative 3D human shape\nand articulated pose models. In Computer Vision and Pat-\ntern Recognition (CVPR), pages 6183\u20136192, 2020. 2, 3\n[124] Xueting Yang, Yihao Luo, Yuliang Xiu, Wei Wang, Hao\nXu, and Zhaoxin Fan.\nD-IF: Uncertainty-aware Human\n20\nDigitization via Implicit Distribution Field. In International\nConference on Computer Vision (ICCV), 2023. 2\n[125] Hongwei Yi, Chun-Hao P. Huang, Dimitrios Tzionas,\nMuhammed Kocabas, Mohamed Hassan, Siyu Tang, Justus\nThies, and Michael J. Black. Human-Aware Object Place-\nment for Visual Environment Reconstruction. In Computer\nVision and Pattern Recognition (CVPR), 2022. 6\n[126] Tao Yu, Zerong Zheng, Kaiwen Guo, Pengpeng Liu, Qiong-\nhai Dai, and Yebin Liu. Function4D: Real-time Human Vol-\numetric Capture from Very Sparse Consumer RGBD Sen-\nsors. In Computer Vision and Pattern Recognition (CVPR),\n2021. 2, 3, 8\n[127] Ilya Zakharkin, Kirill Mazur, Artur Grigorev, and Victor\nLempitsky. Point-based modeling of human clothing. In In-\nternational Conference on Computer Vision (ICCV), 2021.\n2\n[128] Yifei Zeng, Yuanxun Lu, Xinya Ji, Yao Yao, Hao Zhu, and\nXun Cao. AvatarBooth: High-Quality and Customizable\n3D Human Avatar Generation. arXiv preprint:2306.09864,\n2023. 3\n[129] Hongwen Zhang, Yating Tian, Xinchi Zhou, Wanli Ouyang,\nYebin Liu, Limin Wang, and Zhenan Sun.\nPyMAF: 3D\nHuman Pose and Shape Regression with Pyramidal Mesh\nAlignment Feedback Loop. In International Conference on\nComputer Vision (ICCV), 2021. 2\n[130] Huichao Zhang, Bowen Chen, Hao Yang, Liao Qu, Xu\nWang, Li Chen, Chao Long, Feida Zhu, Kang Du, and\nMin Zheng. AvatarVerse: High-quality & Stable 3D Avatar\nCreation from Text and Pose. arXiv preprint:2308.03610,\n2023. 3\n[131] Hongwen Zhang, Yating Tian, Yuxiang Zhang, Mengcheng\nLi, Liang An, Zhenan Sun, and Yebin Liu.\nPyMAF-X:\nTowards Well-aligned Full-body Model Regression from\nMonocular Images. Transactions on Pattern Analysis and\nMachine Intelligence (TPAMI), 2023. 2\n[132] Jianfeng Zhang, Zihang Jiang, Dingdong Yang, Hongyi\nXu, Yichun Shi, Guoxian Song, Zhongcong Xu, Xinchao\nWang, and Jiashi Feng. Avatargen: a 3d generative model\nfor animatable human avatars.\nIn European Conference\non Computer Vision Workshops (ECCVw), pages 668\u2013685.\nSpringer, 2023. 3\n[133] Jason Y. Zhang, Sam Pepose, Hanbyul Joo, Deva Ramanan,\nJitendra Malik, and Angjoo Kanazawa.\nPerceiving 3D\nHuman-Object Spatial Arrangements from a Single Image\nin the Wild. In European Conference on Computer Vision\n(ECCV), pages 34\u201351, Cham, 2020. Springer International\nPublishing. 6\n[134] Lvmin Zhang and Maneesh Agrawala.\nAdding condi-\ntional control to text-to-image diffusion models.\narXiv\npreprint:2302.05543, 2023. 10\n[135] Zerong Zheng, Tao Yu, Yixuan Wei, Qionghai Dai, and\nYebin Liu. DeepHuman: 3D Human Reconstruction From\na Single Image. In International Conference on Computer\nVision (ICCV), pages 7738\u20137748, 2019. 3\n[136] Zerong Zheng, Tao Yu, Yebin Liu, and Qionghai Dai.\nPaMIR: Parametric Model-conditioned Implicit Represen-\ntation for image-based human reconstruction. Transactions\non Pattern Analysis and Machine Intelligence (TPAMI), 44\n(6):3170\u20133184, 2021. 2, 3, 7, 8, 9\n[137] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela\nBarriuso, and Antonio Torralba.\nScene parsing through\nade20k dataset.\nIn Computer Vision and Pattern Recog-\nnition (CVPR), pages 633\u2013641, 2017. 8\n[138] Zhizhuo Zhou and Shubham Tulsiani. Sparsefusion: Dis-\ntilling view-conditioned diffusion for 3d reconstruction. In\nComputer Vision and Pattern Recognition (CVPR), 2023. 4\n[139] Hao Zhu, Xinxin Zuo, Sen Wang, Xun Cao, and Ruigang\nYang. Detailed human shape estimation from a single im-\nage by hierarchical mesh deformation. In Computer Vision\nand Pattern Recognition (CVPR), 2019. 2\n[140] Christian Zimmermann, Duygu Ceylan, Jimei Yang, Bryan\nRussell, Max Argus, and Thomas Brox. Freihand: A dataset\nfor markerless capture of hand pose and shape from single\nrgb images. In International Conference on Computer Vi-\nsion (ICCV), 2019. 8\n21\n"
  },
  {
    "title": "Teach LLMs to Personalize -- An Approach inspired by Writing Education",
    "link": "https://arxiv.org/pdf/2308.07968.pdf",
    "upvote": "24",
    "text": "Teach LLMs to Personalize \u2013 An Approach inspired by Writing\nEducation\nCheng Li\nGoogle\nUSA\nchgli@google.com\nMingyang Zhang\nGoogle\nUSA\nmingyang@google.com\nQiaozhu Mei\u2217\nUniversity of Michigan\nUSA\nqmei@umich.edu\nYaqing Wang\nGoogle\nUSA\nyaqingwang@google.com\nSpurthi Amba Hombaiah\nGoogle\nUSA\nspurthiah@google.com\nYi Liang\nGoogle\nUSA\nyiliang@google.com\nMichael Bendersky\nGoogle\nUSA\nbemike@google.com\nABSTRACT\nPersonalized text generation is an emerging research area that has\nattracted much attention in recent years. Most studies in this direc-\ntion focus on a particular domain by designing bespoke features or\nmodels. In this work, we propose a general approach for personal-\nized text generation using large language models (LLMs). Inspired\nby the practice of writing education, we develop a multistage and\nmultitask framework to teach LLMs for personalized generation.\nIn writing instruction, the task of writing from sources is often\ndecomposed into multiple steps that involve finding, evaluating,\nsummarizing, synthesizing, and integrating information. Analo-\ngously, our approach to personalized text generation consists of\nmultiple stages: retrieval, ranking, summarization, synthesis, and\ngeneration. In addition, we introduce a multitask setting that helps\nthe model improve its generation ability further, which is inspired\nby the observation in education that a student\u2019s reading proficiency\nand writing ability are often correlated. We evaluate our approach\non three public datasets, each of which covers a different and repre-\nsentative domain. Our results show significant improvements over\na variety of baselines.\nKEYWORDS\npersonalized generation, large language models\n\u2217Work done as a visiting researcher at Google.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nXXX, XXX, XXX\n\u00a9 2023 Association for Computing Machinery.\nACM ISBN 978-1-4503-XXXX-X/18/06...$15.00\nhttps://doi.org/XXXXXXX.XXXXXXX\nACM Reference Format:\nCheng Li, Mingyang Zhang, Qiaozhu Mei, Yaqing Wang, Spurthi Amba\nHombaiah, Yi Liang, and Michael Bendersky. 2023. Teach LLMs to Personal-\nize \u2013 An Approach inspired by Writing Education. In Proceedings of XXX.\nACM, New York, NY, USA, 10 pages. https://doi.org/XXXXXXX.XXXXXXX\n1\nINTRODUCTION\nAs artificial intelligence (AI) based systems are increasingly used\nto assist content creation, there has been a tremendous amount of\ninterest in personalized text generation. Producing a customized re-\nsponse that takes into account auxiliary context, such as documents\npreviously written by the user, is crucial for the development of gen-\nerative systems that support specific audiences, creation contexts,\nand information needs. Example applications include AI-assisted\nwriting of various types of content from tweets and news stories\nto scientific articles and fictions, corporate and personal commu-\nnications (emails, chats, forums), and transformations of a given\npiece of written content into other styles, e.g., summarization, or\nconversely, elaboration.\nResearchers have investigated the generation of personalized\ntext on various domains, including but not limited to reviews [13,\n14], dialogue agents [17, 38, 41] and social networks [8]. Previous\nwork mostly relies on domain-specific features or knowledge and\nproposes models that address a particular task. How to design a\ngeneral approach that works for all scenarios is a less studied area.\nOn the other hand, along with the ascendance of generative\nAI, through chatbots like ChatGPT1 and Bard2 in particular, large\nlanguage models (LLMs) are playing an increasingly prominent role\nin many text generation tasks. However, few studies have explored\nhow to equip LLMs with the ability to personalize.\nIn this work, we propose a general approach for personalized text\ngeneration using large language models. Our work is inspired by\nthe widely-used practice in writing education, which decomposes\nthe task of writing from sources by a procedure of finding, evaluat-\ning, summarizing, synthesizing, and integrating information [3, 31].\n1https://chat.openai.com\n2https://bard.google.com\narXiv:2308.07968v1  [cs.CL]  15 Aug 2023\nXXX, XXX, XXX\nLi et al.\nAnalogously, we adopt a multistage multitask framework to teach\nLLMs for personalized text generation, with similar stages being\nretrieval, ranking, summarization, synthesis, and generation. Specif-\nically, given the immediate context, such as the title and the starting\nsentence of a document a user is writing, we formulate a query and\nretrieve relevant information from an auxiliary repository of per-\nsonal contexts, such as documents the user has authored in the past.\nWe then rank the retrieved results based on their relevance and\nimportance, followed by summarizing the ranked results. We also\nsynthesize the retrieved information into key elements, and finally\nfeed the retrieved results, summary and synthesized information\ninto the large language model for generating the new document.\nIn language education, it is often observed that the proficiency\nof one\u2019s writing skills is highly correlated with that of their reading\nskills [4]. Furthermore, studies show that author recognition tasks\ncan be used to measure the amount and level of reading by an indi-\nvidual [18], which correlates with their reading proficiency. Inspired\nby these two observations, we create a multitask setting that aims\nto improve the reading ability of the large language model, where\nwe introduce an auxiliary task charging the model to attribute the\nauthorship of a given text. We anticipate that this task will help\nthe model better understand (i.e., read) the given text and in turn\ngenerate (i.e., write) better and more personalized content.\nWe evaluate the proposed models on three public datasets, which\ncover personal email communications, social media discussions,\nand product reviews. By employing our multistage and multitask\nframework, we demonstrate significant improvements over a vari-\nety of baselines on all three datasets.\n2\nRELATED WORK\nWe present a literature review on personalized text generation and\ntwo related tasks, controlled text generation and text style transfer.\nPersonalized text generation. Some studies focus on improving\npersonalized generation for a particular domain by utilizing domain-\nspecific features or knowledge. Li and Tuzhilin [14] design a model\nbased on self-attentive recursive autoencoders to generate person-\nalized user reviews given product description, sentiment labels, and\nhistorical reviews of the user. A knowledge enhanced personalized\nreview generation model based on a capsule graph neural network\n(CapsGNN) is proposed in [13] to utilize product attributes. Gao\net al. [8] focus on personalized social text generation, where per-\nsonalized features are fed to the encoder to guide the generation of\nthe decoder. There are extensive studies on personalization for dia-\nlogue agents [17, 38, 41]. Due to limited real conversational data, re-\nsearchers have explored constructing data by asking crowd-workers\nto write dialogues for specific personas [41] and by extracting user\nattributes and utterances from Reddit [17, 38] and Weibo [25, 43].\nThere are investigations on using predefined attributes and topics\nfor personalization. A personalized sentence generation method is\nproposed [40] based on generative adversarial networks (GANs).\nFrequently used function words and content words are used as\ninput and as sentence structure constraints for model training.\nA less explored area is how to utilize large language models for\npersonalized generation across different domains without relying\non domain-specific or user-defined features. LaMP [29] is the work\nclosest to ours. It provides a benchmark for training and evaluat-\ning personalized language models on three classification and four\ntext generation tasks. They deploy an approach that retrieves text\nfrom user profiles. The generation tasks provided in LaMP are at\nthe sentence-level. We instead consider generating longer text of\npassage-length, which is more challenging. Method-wise, the re-\ntrieval based approach in LaMP can be viewed as an instantiation\nof a single component of the multi-stage framework we proposed.\nSkopyk et al. [34] propose to train transformer layer adapters to\nachieve the effect of personalization. The paper only proposes the\nmethod without including any experimental analysis.\nControlled text generation. Controlled text generation aims to\ngenerate text with a predefined list of attributes, which could be\nstylistic or semantic. To reduce the cost of finetuning, recent work\nof controlled text generation resorts to decoding-time methods,\ndirectly making pre-trained models generate texts towards de-\nsired attributes during inference. These methods include PPLM [5],\nGeDi [11], FUDGE [39], and DEXPERTS [16].\nControlled text generation is different from personalized genera-\ntion in that it requires a predefined set of attributes (constraints).\nText style transfer. A task related to controlled text generation is\ntext style transfer. Its goal is to transform a piece of text by control-\nling certain attributes of the generated text while preserving the\ncontent. There are two paradigms: supervised learning using paral-\nlel corpora, and unsupervised methods using non-parallel corpora.\nWith parallel data, standard sequence-to-sequence models can be\ndirectly employed [27]. There are three approaches when only non-\nparallel corpora are available. The first approach is to disentangle\ntext into content and attributes for generative modeling [33]. The\nsecond approach, called prototype editing [12], extracts a sentence\ntemplate and attribute markers for generation. The third approach\nconstructs pseudo-parallel corpora to train the model [42].\nUnlike text style transfer, personalized generation does not as-\nsume that the original text is already given. Like controlled text\ngeneration, most methods for text style transfer expect a given set\nof predefined attributes, which are not available in our setting.\nOur work is also aligned with the paradigm of teaching LLMs to\nreason through chain of thoughts [36, 37], with our decomposition\nof tasks deeply inspired by writing education and our model training\nfor each task going beyond prompt engineering.\n3\nPROBLEM FORMULATION\nWe consider the setting where a user is writing a document, which\nwe call the current document. Given the immediate context and\nthe user\u2019s personal context, the goal of the personalized model is\nto complete the document so that the generated document is close\nto the real current document as if the user finishes writing.\nThere might be different ways to define the immediate context\nand the personal context. For simplicity and generality, we use the\ntitle and a short start of the current document as the immediate\ncontext. The user\u2019s personal context is defined as the documents\nthey have written in the past at the time of writing the current\ndocument. These contexts are analogous to the sources a student is\ninstructed to write from [3].\nOur training task is formulated as follows. Given a list of ex-\namples {(\ud835\udc65\ud835\udc62\ud835\udc61, D\ud835\udc62\ud835\udc61,\ud835\udc51\ud835\udc62\ud835\udc61)}, where \ud835\udc65\ud835\udc62\ud835\udc61 is the immediate context of\nTeach LLMs to Personalize \u2013 An Approach inspired by Writing Education\nXXX, XXX, XXX\nImmediate context of \nthe current document\n1. Retrieve\nKey elements\nTop entries\n2. Rank\nSummary\nPersonal context\n3. Summarize\n4. Synthesize\nPersonalized \ndocument\nSame Author?\nMultitasking\nA document pair\nPredict\nGenerate\nLLM\nFigure 1: The overview of the multistage multitask frame-\nwork for personalized text generation.\nuser \ud835\udc62 for the current document \ud835\udc51\ud835\udc62\ud835\udc61 at time step \ud835\udc61, and D\ud835\udc62\ud835\udc61 =\n{\ud835\udc51\ud835\udc621,\ud835\udc51\ud835\udc622, ...,\ud835\udc51\ud835\udc62,\ud835\udc61\u22121} is the personal context of past documents, we\nwant to train a personalized generation model G that generates\n\ud835\udc51\u2032\n\ud835\udc62\ud835\udc61 based on (\ud835\udc65\ud835\udc62\ud835\udc61, D\ud835\udc62\ud835\udc61) so that we can maximize the similarity\nbetween \ud835\udc51\u2032\n\ud835\udc62\ud835\udc61 and \ud835\udc51\ud835\udc62\ud835\udc61. Whenever it is clear from the context, we will\nomit the subscript \ud835\udc62 and directly use \ud835\udc65\ud835\udc61, D\ud835\udc61 and \ud835\udc51\ud835\udc61 instead. We will\nalso use \u201cuser\u201d and \u201cauthor\u201d interchangeably.\n4\nMETHOD OVERVIEW\nThe overview of our multistage multitask framework for personal-\nized text generation is presented in Figure 1.\nGiven the immediate context \ud835\udc65\ud835\udc61 of the current document written\nby a user \ud835\udc62, a retriever Re(\ud835\udc65\ud835\udc61, D\ud835\udc61) retrieves entries from past user\ndocument set D\ud835\udc61 using \ud835\udc65\ud835\udc61 as the query. The returned entries are\nfed to a ranker Ra to produce ranked entries E\ud835\udc61 = Ra(Re(\ud835\udc65\ud835\udc61, D\ud835\udc61)),\nwhich are consumed by: (1) a summarization model Su(\ud835\udc65\ud835\udc61, E\ud835\udc61)\nto produce a summary of the multiple documents retrieved; and\n(2) a synthesis model Sy(\ud835\udc65\ud835\udc61, E\ud835\udc61) to produce key elements in these\ndocuments. The personalized generation model G generates the cur-\nrent document \ud835\udc51\u2032\n\ud835\udc61 = G(\ud835\udc65\ud835\udc61, Su(\ud835\udc65\ud835\udc61, E\ud835\udc61), Sy(\ud835\udc65\ud835\udc61, E\ud835\udc61), E\ud835\udc61) and is trained\nagainst the ground-truth current document \ud835\udc51\ud835\udc61.\nWe additionally consider an auxiliary task, called author dis-\ntinction, to help the model better understand the user context and\ngenerate better personalized content. Given a document \ud835\udc51\ud835\udc62\ud835\udc56 writ-\nten by a user \ud835\udc62, we randomly sample another document \ud835\udc51\ud835\udc63\ud835\udc57 to\nform a document pair. The model G is then trained on a set of\ntuples {(\ud835\udc51\ud835\udc62\ud835\udc56,\ud835\udc51\ud835\udc63\ud835\udc57),\ud835\udc66}, where the label \ud835\udc66 = \ud835\udc61\ud835\udc5f\ud835\udc62\ud835\udc52 if \ud835\udc63 = \ud835\udc62, otherwise\n\ud835\udc66 = \ud835\udc53 \ud835\udc4e\ud835\udc59\ud835\udc60\ud835\udc52. Note that we use text {\ud835\udc61\ud835\udc5f\ud835\udc62\ud835\udc52, \ud835\udc53 \ud835\udc4e\ud835\udc59\ud835\udc60\ud835\udc52} instead of numerical\nlabels for \ud835\udc66 since G is a sequence-to-sequence model.\n5\nPERSONALIZED TEXT GENERATION\nWe discuss the detail of each stage as outlined in Section 4.\n5.1\nRetrieval\nIn the retrieval stage, given the immediate context \ud835\udc65\ud835\udc61, the retriever\nRe(\ud835\udc65\ud835\udc61, D\ud835\udc61) uses \ud835\udc65\ud835\udc61 as the query to retrieve relevant text entries\nfrom past document set D\ud835\udc61.\nTo define an immediate context that can be applied to any sce-\nnario, we simply use \ud835\udc39\ud835\udc56\ud835\udc5f\ud835\udc60\ud835\udc61\ud835\udc3e\ud835\udc36\u210e\ud835\udc4e\ud835\udc5f\ud835\udc4e\ud835\udc50\ud835\udc61\ud835\udc52\ud835\udc5f\ud835\udc60(\ud835\udc51\ud835\udc61), where\n\ud835\udc39\ud835\udc56\ud835\udc5f\ud835\udc60\ud835\udc61\ud835\udc3e\ud835\udc36\u210e\ud835\udc4e\ud835\udc5f\ud835\udc4e\ud835\udc50\ud835\udc61\ud835\udc52\ud835\udc5f\ud835\udc60(\u00b7) returns the first \ud835\udc3e characters of a piece of text.\nWe set \ud835\udc3e = 150 for all experiments. If a document has a title, we\nconcatenate the title and the body as the text.\nWe experiment with both sparse and dense retrievers to re-\ntrieve relevant entries from a user\u2019s past documents. We employ\nBM25 [28] as the sparse retriever. We use a T5X Retrieval model [19,\n21], GTR-Large, as our dense retriever. We do not choose models of\nlarger sizes since they demonstrate similar performance but much\nworse effectiveness-latency trade-offs on benchmark datasets [21].\nFor dense retrieval, we experiment with two levels of granularity\nwhen indexing personal document entries: a document level and\na snippet level. We do not choose a sentence level since many\nsentences are too short to offer enough context information. We\ncreate a snippet in this way: we keep appending sentences from\nthe same document until we reach 250 characters or we reach the\nend of the document.\nSince the snippets to retrieve are quite short, we only examine\nthe performance of sparse retrieval at the document level.\n5.2\nRanking\nSince we experiment with indexing entries at both document and\nsnippet level, we can rank entries accordingly:\n\u2022 RankDocBM25. For sparse retrieval, we retrieve and rank\ndocuments based on BM25 scores.\n\u2022 RankDocDense. For dense retrieval, when we retrieve en-\ntries at the document level, we rank retrieved documents\nbased on their embedding similarity with the embedding of\nthe immediate context \ud835\udc65\ud835\udc61.\n\u2022 RankSnippet. Similarly, for dense retrieval, when we re-\ntrieve entries at the snippet level, we rank retrieved snippets\nbased on embedding similarity.\nDuring analysis, we find that issues exist for both RankDoc-\nDense and RankSnippet. The retrieved results via RankDocDense\ncan be less relevant since embeddings are less effective when the\ndocuments to encode are long. While for RankSnippet, many sim-\nilar snippets are retrieved, providing insufficient information for\ngeneration. For example, if the immediate context is I really enjoyed\nreading the book, we might retrieve similar snippets like I enjoy the\nbook, The book is fun, or I love this book. They are all relevant but\ndo not provide enough details on why this user enjoys a particular\nbook, which is critical for passage-level generation.\nTo alleviate the two issues, we propose another dense retrieval\nstrategy, RankDocBySnpt, inspired by past work on using passage\nevidence in retrieval [2]. RankDocBySnpt retrieves relevant text at\nthe snippet level, which addresses the issue that document embed-\ndings are less effective. At the ranking stage, instead of directly rank-\ning snippets, we rank documents that contain the retrieved snippets,\nto mitigate lack of diversity in snippets retrieved via RankSnip-\npet. Specifically for each document \ud835\udc51\ud835\udc56, we compute the embedding\nsimilarity score between each retrieved snippet \ud835\udc60\ud835\udc56\ud835\udc57 \u2208 \ud835\udc51\ud835\udc56 and the\nimmediate context \ud835\udc65\ud835\udc61, and use the max score as the document score\nfor ranking. That is, \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52(\ud835\udc51\ud835\udc56,\ud835\udc65\ud835\udc61) = max\ud835\udc60\ud835\udc56\ud835\udc57 \u2208\ud835\udc51\ud835\udc56 (\ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52(\ud835\udc60\ud835\udc56\ud835\udc57,\ud835\udc65\ud835\udc61)).\nTo make all the ranking strategies comparable, we concatenate\nranked entries into a string and truncate it to 2, 500 characters. Thus\nthe subsequent modules are fed with input text of the same length.\n5.3\nSummarization\nThe summarization stage aims to extract important information\nfrom the retrieved entries so that the generation model can have\na better understanding of what is the most important information\nin the user\u2019s personal context, such as key points, topics, or useful\nXXX, XXX, XXX\nLi et al.\n\u2022\nThis book is amazing.\n\u2022\nIt had me read it out loud.\n\u2022\nI enjoyed reading this book.\n\u2022\nI love the characters.\n\u2022\nThe book is a page turner.\nSnippets in the \nranked entries\n\u2022\nI really enjoyed the book.\n\u2022\nThe characters are loving.\n\u2022\nIt's a page-turner.\nCompute \nsimilarity to \nall snippets\n\u2026\nPair each ground-truth snippet \nwith a past one\nSnippets in the \nground-truth \ncurrent document\nPast snippets of \nthe highest \nsimilarity\nI really enjoyed the \nbook.\nI enjoyed reading \nthis book.\nThe characters are \nloving.\nI love the \ncharacters.\nIt's a page-turner.\nThe book is a page \nturner.\nSnippets in the ground-\ntruth current document\nI enjoyed reading this \nbook. I love the \ncharacters. The book is \na page turner.\nJoin selected \npast snippets \ninto a string\nThe weak label for context \ndependent summarization\n\u2026\nFigure 2: Creation of weak labels for context dependent sum-\nmarization.\nphrases (so they can be reflected in the output). With the summary,\nthe generation model does not need to work on extracting the\nhigh-level aspects and generating the exact words at the same time,\nmaking the generation task easier.\nWe experiment with two strategies \u2013 context independent and\ncontext dependent summarization. By context dependent, we mean\nthat the summarization is conditioned on the immediate context.\nContext independent summarization. We choose a straightfor-\nward implementation of context independent summarization \u2013 we\nfinetune an independent LLM, T5-11B [26], on publicly available\nsummarization datasets, and directly use the finetuned model on\nour ranked entries for inference. The datasets we use are CNN/Daily\nMail [30], ForumSum [9], and Reddit TIFU-long [10].\nContext dependent summarization. The challenge to train a con-\ntext dependent summarization model is the lack of ground-truth\nlabels. We tackle this challenge by generating weak labels based\non ground-truth current documents. Our intuition is that we want\nto extract text from the retrieved results that are more likely to be\nused in the current document. To this end, we find text from the\nranked entries that is similar to the text of the current document,\nwhich can be formulated as an extractive summarization task. An\nexample to illustrate this idea can be found in Figure 2.\nSpecifically for each snippet \ud835\udc60\ud835\udc61\ud835\udc56 \u2208 \ud835\udc51\ud835\udc61 in the ground-truth current\ndocument, we compute its similarity to all snippets in the ranked\nentries E\ud835\udc61 = {\ud835\udc521,\ud835\udc522, ...,\ud835\udc52\ud835\udc45} retrieved from past documents, where\n\ud835\udc52\ud835\udc57 is the \ud835\udc57-th snippet in the entries E\ud835\udc61, and \ud835\udc45 is the number of\nsnippets we include in the ranking. For simplicity, we reuse the\nembeddings obtained by the T5X Retrieval model [19, 21] in the\nretrieval stage to compute similarity. The snippet with the highest\nsimilarity score \ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc65\ud835\udc56 = arg max\ud835\udc52\ud835\udc57 \u2208E\ud835\udc61 (\ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52(\ud835\udc52\ud835\udc57,\ud835\udc60\ud835\udc61\ud835\udc56)) will be added\nto the candidate snippet list L\ud835\udc61. If the selected snippet \ud835\udc52\ud835\udc5a\ud835\udc4e\ud835\udc65\ud835\udc56 is\nalready in the candidate list L\ud835\udc61, we look for the snippet with the\nnext highest score until we find a new snippet. We keep adding\nnewly selected snippets from the ranked entries to the candidate\nlist until we have iterated through all the snippet pairs.\nThe weak labels are created by \ud835\udc3d\ud835\udc5c\ud835\udc56\ud835\udc5b(L\ud835\udc61), which joins the snip-\npets in the candidate list into one string. Given the immediate\ncontext \ud835\udc65\ud835\udc61 and the ranked entries E\ud835\udc61, the context dependent summa-\nrization model Su(\ud835\udc65\ud835\udc61, E\ud835\udc61) is trained using the weak labels \ud835\udc3d\ud835\udc5c\ud835\udc56\ud835\udc5b(L\ud835\udc61)\nto minimize the cross-entropy loss. We still choose T5-11B [26] as\nthe model.\nOne important note is that we only use half of the training data\nfor the generation task to train the summarization model. In this\nway, the generated summary will be noisier on the unseen half of the\ntraining data (similar to the test set). Consequently the generation\nmodel will be aware of the noisy summary during training and\nlearn to cope with it by attending to other information.\nAnother note is that no validation or test data for the generation\ntask are used to train the summarization model.\n5.4\nSynthesis\nThe synthesis step aims to find key elements that are common\nin the top retrieved entries for an overall understanding of the\ncurrent writing task. We experiment with extracting keywords as\nthe synthesis step in this paper. More sophisticated approaches to\nsynthesis are worth exploration and are left as future work.\nSimilar to summarization, we investigate two strategies \u2013 context\nindependent and context dependent synthesis. The context also\nrefers to the immediate context.\nContext independent synthesis. We extract keywords by finding\nfrequent terms from the past documents D\ud835\udc61. We limit terms to\nunigrams as most users do not have enough documents to extract\nn-grams with larger n. We also remove stopwords, words with\nfrequency of one, and words with small inverse document frequency\n(IDF < 1.5). We then sort remaining words in descending order by\ntheir frequency and then by IDF, and keep up to 20 words.\nContext dependent synthesis. Our idea of creating weak labels\nfor context dependent synthesis is very similar to how we create\nweak labels for context dependent summarization \u2013 we aim to find\nimportant words from the retrieved results that are likely to be\nused in the generation of the current document.\nTo be more specific, for each source word \ud835\udc64\ud835\udc61\ud835\udc56 \u2208 \ud835\udc51\ud835\udc61 in the ground-\ntruth current document, we compute its similarity with each target\nword \ud835\udc63\ud835\udc61 \ud835\udc57 \u2208 E\ud835\udc61 from the ranked entries. For both source and target\nwords, we skip stopwords or words with IDF < 1.5. Two words\n(\ud835\udc64\ud835\udc61\ud835\udc56, \ud835\udc63\ud835\udc61 \ud835\udc57) are similar if at least one of the conditions are met:\n\u2022 The two words are identical.\n\u2022 The two words are synonyms as defined in WordNet [6].\n\u2022 The two words are close in the embedding space. We use\nthe uncased GloVe [24] embeddings pretrained on the Com-\nmon Crawl dataset. We define two words as similar if their\nEuclidean distance is less than 4.\nWe add the qualified target word \ud835\udc63\ud835\udc61 \ud835\udc57 to the candidate target word\nlist T\ud835\udc61. After going through all possible (source, target) word pairs,\nthe words in the candidate list T\ud835\udc61 are then sorted inversely by the\nnumber of times they are selected, and then by IDF. The candidate\nword list is then joined into a string to form the weak label \ud835\udc3d\ud835\udc5c\ud835\udc56\ud835\udc5b(T\ud835\udc61).\nWe still finetune a T5-11B [26] model for synthesis. Given the\nimmediate context \ud835\udc65\ud835\udc61 and the ranked entries E\ud835\udc61, the context de-\npendent synthesis model Sy(\ud835\udc65\ud835\udc61, E\ud835\udc61) is trained using the weak label\n\ud835\udc3d\ud835\udc5c\ud835\udc56\ud835\udc5b(T\ud835\udc61) to minimize the cross-entropy loss.\nWe use the same set of training examples that are used for sum-\nmarization. The only difference is that the label is changed from the\njoined snippet list \ud835\udc3d\ud835\udc5c\ud835\udc56\ud835\udc5b(L\ud835\udc61) to the joined target word list \ud835\udc3d\ud835\udc5c\ud835\udc56\ud835\udc5b(T\ud835\udc61).\nTeach LLMs to Personalize \u2013 An Approach inspired by Writing Education\nXXX, XXX, XXX\n5.5\nPersonalized Generation\nGiven the immediate context \ud835\udc65\ud835\udc61, the ranked entries E\ud835\udc61 retrieved\nfrom the past documents, the context independent/dependent sum-\nmary from the summarization model Su(\ud835\udc65\ud835\udc61, E\ud835\udc61), the context inde-\npendent/dependent synthesis from the synthesis model Sy(\ud835\udc65\ud835\udc61, E\ud835\udc61),\nthe personalized generation model G(\ud835\udc65\ud835\udc61, Su(\ud835\udc65\ud835\udc61, E\ud835\udc61), Sy(\ud835\udc65\ud835\udc61, E\ud835\udc61), E\ud835\udc61)\nis trained using the ground-truth current document \ud835\udc51\ud835\udc61 as the label\nto minimize the cross-entropy loss.\nTo help the model distinguish between various information\nsources, we add different prefixes when converting the sources\nto a single string input. Specifically, the immediate context is pre-\nfixed by passage start, the summary is prefixed by summary, the\nsynthesis is prefixed by important words, and the list of ranked\nentries is prefixed by past passages.\n5.6\nMultitask Learning\nStudies in language education show that writing and reading skills\nare highly correlated [4]. Additionally, researches have found that\nauthor recognition tasks can be used to assess how much an individ-\nual reads [18], which correlates with reading proficiency. Inspired\nby the above studies, in addition to the generation task, which corre-\nsponds to writing, we add a reading comprehension task that aims\nto improve the model\u2019s ability to better understand the style of an\nauthor. Specifically, we introduce the author distinction task, which\nrequires the model to decide whether a given pair of documents\nare written by the same author or not.\nGiven a document \ud835\udc51\ud835\udc62\ud835\udc56 written by a user \ud835\udc62, for half of the time,\nwe randomly sample another document \ud835\udc51\ud835\udc62\ud835\udc57 from the same user\ud835\udc62 to\nform a positive training example (\ud835\udc65,\ud835\udc66) = ((\ud835\udc51\ud835\udc62\ud835\udc56,\ud835\udc51\ud835\udc62\ud835\udc57),\ud835\udc61\ud835\udc5f\ud835\udc62\ud835\udc52). Other-\nwise, we randomly sample another document\ud835\udc51\ud835\udc63\ud835\udc58 from another user\n\ud835\udc63 (\ud835\udc63 \u2260 \ud835\udc62) to form a negative example (\ud835\udc65,\ud835\udc66) = ((\ud835\udc51\ud835\udc62\ud835\udc56,\ud835\udc51\ud835\udc63\ud835\udc58), \ud835\udc53 \ud835\udc4e\ud835\udc59\ud835\udc60\ud835\udc52).\nNote that we use text labels \ud835\udc66 \u2208 {\ud835\udc61\ud835\udc5f\ud835\udc62\ud835\udc52, \ud835\udc53 \ud835\udc4e\ud835\udc59\ud835\udc60\ud835\udc52} since the generation\nmodel G is a sequence-to-sequence model.\nSince the generation model is simultaneously trained on two\ntasks, personalized generation and author distinction, we prepend\na task-level instruction to the model input to help the model distin-\nguish which task to perform. For the personalized generation task,\nthe instruction is \u201cFinish the passage in the user voice\u201d. While for\nthe author distinction task, the instruction is \u201cPredict whether two\npassages are from the same author\u201d.\nNote that all the documents for the author distinction task are\nsampled from users that never appear in the validation or test data\nof the personalized generation task.\n6\nEXPERIMENT SETUP\nIn this section, we describe our experiment setup, including datasets,\ntraining details, competing methods, and metrics.\n6.1\nDatasets\nWe evaluate our models on three public datasets, each from a rep-\nresentative domain. A summary of data statistics can be found in\nTable 1. For all the three datasets, we give their respective definition\nof a document.\nThe Avocado Research Email Collection [22] consists of emails\nand attachments taken from 279 accounts of an information tech-\nnology company named \u201cAvocado\u201d. We follow the processing steps\nof LaMP [29] and group emails by sender addresses. Note that the\nnumber of senders/users can be more than 279 accounts as there are\nsenders outside of the company. We treat an email as a document\nand its subject as the title.\nThe Amazon review data [20] provides user reviews from Ama-\nzon on different product categories. Since the entire dataset is very\nlarge, we choose the biggest category, books, which has the largest\nnumber of reviews. We group reviews by users. We treat reviews\nas documents and review titles as document titles.\nThe Reddit comments dataset [35] consists of all the posts and\ncomments available on Reddit from 2007 to 2015. We treat both\nposts and comments as documents and group them by users.\nFor all the three datasets, we deduplicate identical documents\nfrom each user\u2019s personal context. A document is qualified to be\na current document, which is the document to be generated, if this\ndocument is longer than 300 characters and the document author\nhas written at least 2 documents before this one. For each qualified\ncurrent document, we generate an example. We only keep users\nwho have at least 5 examples. We include up to 50 examples per\nuser in case the datasets are dominated by certain active users.\nIn order to evaluate the model\u2019s ability to generalize, we partition\nthe datasets by users so that the validation and test sets only contain\ndocuments from users that are unseen in the training set. The\npartition ratio of users in train/validation/test sets are 85/5/10.\n6.2\nTraining details\nWe finetune the T5-11B [26] model for all the summarization, syn-\nthesis, and personalized generation tasks. The T5-11B models are\noptimized by the Adafactor algorithm [32] with a base learning\nrate of 0.001. We use the first 1,000 training steps for warmup with\na linear warmup scheduler. We additionally apply the square root\nnormalized decay of the learning rate. We train the models until\nthe performance converges on the validation set. Beam search [7]\nis used for decoding with a beam size of 4.\nFor multitask learning, we simply mix the examples for person-\nalized generation and author distinction in the ratio of 1:1.\n6.3\nCompeting methods\nAs mentioned in Section 2, most personalized generation models are\nproposed for a particular domain, which relies on domain specific\nknowledge or features. The work closest to ours is LaMP [29],\nwhich employs a retrieval based method for personalization using\nLLMs. Since LaMP focuses on sentence-length generation, it does\nnot explore the advantage of utilizing snippet- or document-based\nstrategies as we have done in this paper.\nWe consider the following competing methods for better under-\nstanding of credit assignment.\nBaselines. We consider these baselines.\n\u2022 ImmedCtx. This method only uses the immediate context \ud835\udc65\ud835\udc61\nas the model input, which is the title and a short start of the\ncurrent document.\n\u2022 UserID. We add User ID to the model input and train the\nmodel using the next snippet generation task. The user ID\nhelps the model memorize the personal style of each user.\nXXX, XXX, XXX\nLi et al.\nTable 1: Dataset statistics.\n#avg chars of\n#avg past docs\n#users\n#current docs (#examples)\ncurrent docs\nper current doc\nTrain\nVal.\nTest\nTrain\nVal.\nTest\nAvocado email\n3,648.8\n42.3\n501\n27\n53\n13,305\n764\n1,227\nAmazon review\n1,056.7\n45.1\n354,275\n20,789\n41,331\n5,349,661\n311,469\n621,438\nReddit\n658.5\n88.5\n240,626\n14,223\n28,306\n3,858,731\n231,307\n455,582\nSince this model performs better on users seen during train-\ning, we include all documents that are never used for pre-\ndiction to train the model, including documents from the\nvalidation and the test set. The next snippet generation task\nrequires the model to generate the next snippet given a snip-\npet from a document.\nNote that the immediate context includes a short start of the\ncurrent document, which is also the start of the ground-truth\noutput. Other generation models learn to copy the start to\ntheir output to minimize loss but this baseline model is not\ntrained to do so. To make the comparison fair, as postprocess\nwe prepend the start of the current document included in\nthe immediate context to the model output.\n\u2022 LLMZeroShot. We use the PaLM 2 model [1], which is a\nnew state-of-the-art LLM, for zero-shot generation. PaLM 2\nis fed with the input of the best configuration based on exper-\niments, including the task instruction to prompt the model to\nperform the personalized generation task, the immediate con-\ntext, the context dependent summary, the context dependent\nsynthesis, and the retrieved entries from the best ranking\nstrategy. Similar to the UserID baseline, we also prepend the\nstart of the current document to the model output.\nRetrieval augmented methods. We experiment with the ranking\nstrategies introduced in Section 5.2: one sparse retrieval based\nmethod RankDocBM25, and three dense retrieval based methods \u2013\nRankDocDense, RankSnippet, and RankDocBySnpt. In addition,\nwe add a recency based baseline, RecentDoc, which retrieves most\nrecent documents written in the past as ranked entries.\nFor all these methods, the personalized generation model G is\ntrained on the immediate context \ud835\udc65\ud835\udc61 and the ranked entries E\ud835\udc61.\nSummarization. We examine the summarization methods intro-\nduced in Section 5.3 by performing summarization on top of the\nbest configuration of ranking strategies. The personalized generation\nmodel G is trained on the immediate context \ud835\udc65\ud835\udc61, the output from\nthe summarization model Su(\ud835\udc65\ud835\udc61, E\ud835\udc61), and the ranked entries E\ud835\udc61. We\nrefer to context independent summarization as SumCtxInd, and\ncontext dependent summarization as SumCtx.\nSynthesis. We evaluate the synthesis methods introduced in Sec-\ntion 5.4 by applying synthesis on top of the best configuration of\nsummarization methods. The personalized generation model G is\ntrained on the immediate context \ud835\udc65\ud835\udc61, the summary from the summa-\nrization model Su(\ud835\udc65\ud835\udc61, E\ud835\udc61), the synthesis from the synthesis model\nSy(\ud835\udc65\ud835\udc61, E\ud835\udc61), and the ranked entries E\ud835\udc61. We refer to context indepen-\ndent synthesis as SynCtxInd, and context dependent synthesis as\nSynCtx.\nMultitask Training. We use AuthorPred to refer to the multitask\nsetting, where we add the author distinction task on top of the best\nconfiguration of the single (generation) task to jointly train the\ngeneration model.\n6.4\nMetrics\nFor each generated current document, we compute its overlap with\nthe ground-truth current document. We adopt Bleu [23], Rouge-1,\nRouge-2 and Rouge-L [15] as evaluation metrics, which have been\nwidely used in personalized generation tasks [14, 29].\nWe conduct statistical significance tests using the paired t-test.\n7\nEXPERIMENTAL RESULTS\n7.1\nOverall performance\nTable 2: Overall performance(%) on the Avocado email dataset.\n*, \u2020 indicate statistically significant improvement over Im-\nmedCtx, RankDocBM25 respectively at the level of 0.01.\nAvocado email\nBleu\nRouge-1\nRouge-2\nRouge-L\nBaselines\nImmedCtx\n17.27\n32.36\n21.45\n28.58\nUserID\n13.28\n32.33\n20.95\n27.86\nLLMZeroShot\n14.93\n35.06\u2217\n22.11\u2217\n28.52\nRetrieval augmented methods\nRecentDoc\n19.57\u2217\n35.64\u2217\n23.96\u2217\n31.25\u2217\nRankDocBM25\n21.19\u2217\n37.69\u2217\n25.99\u2217\n33.07\u2217\nRankDocDense\n19.43\u2217\n35.62\u2217\n23.71\u2217\n30.90\u2217\nRankSnippet\n18.69\u2217\n35.82\u2217\n23.26\u2217\n30.78\u2217\nRankDocBySnpt\n21.06\u2217\n37.42\u2217\n25.65\u2217\n32.90\u2217\n+Summarization\nSumCtxInd\n21.23\u2217\n37.58\u2217\n25.79\u2217\n33.15\u2217\nSumCtx\n23.17\u2217\u2020\n39.31\u2217\u2020\n26.64\u2217\u2020\n34.37\u2217\u2020\n+Synthesis\nSynCtxInd\n23.06\u2217\u2020\n39.24\u2217\u2020\n26.72\u2217\u2020\n34.52\u2217\u2020\nSynCtx\n23.44\u2217\u2020\n40.38\u2217\u2020\n26.93\u2217\u2020\n34.34\u2217\u2020\n+Multitask\nAuthorPred\n23.27\u2217\u2020\n41.02\u2217\u2020\n28.60\u2217\u2020\n35.70\u2217\u2020\nThe overall performance on the three datasets are listed in Ta-\nble 2, 3, and 4 respectively. In general, retrieval augmented methods\nperform better than baselines that do not utilize the retrieved infor-\nmation. Summarization and synthesis bring additional gains when\nthey are dependent on the immediate context. Multitask learning\nTeach LLMs to Personalize \u2013 An Approach inspired by Writing Education\nXXX, XXX, XXX\nTable 3: Overall performance(%) on the Amazon review\ndataset. *, \u2020 indicate statistically significant improvement\nover ImmedCtx, RankDocBySnpt respectively at the level\nof 0.01.\nAmazon review\nBleu\nRouge-1\nRouge-2\nRouge-L\nBaselines\nImmedCtx\n17.83\n36.22\n22.49\n31.77\nUserID\n17.61\n36.62\n22.85\n32.11\u2217\nLLMZeroShot\n16.29\n38.74\u2217\n22.52\n30.79\nRetrieval augmented methods\nRecentDoc\n19.09\u2217\n37.51\u2217\n23.37\u2217\n32.67\u2217\nRankDocBM25\n19.49\u2217\n37.79\u2217\n23.56\u2217\n32.85\u2217\nRankDocDense\n19.38\u2217\n37.49\u2217\n22.92\u2217\n32.48\u2217\nRankSnippet\n19.44\u2217\n37.45\u2217\n23.10\u2217\n32.50\u2217\nRankDocBySnpt\n19.35\u2217\n38.28\u2217\n23.87\u2217\n33.23\u2217\n+Summarization\nSumCtxInd\n19.17\u2217\n38.33\u2217\n23.66\u2217\n33.35\u2217\nSumCtx\n19.81\u2217\u2020\n38.66\u2217\n24.25\u2217\u2020\n33.57\u2217\n+Synthesis\nSynCtxInd\n19.84\u2217\u2020\n38.68\u2217\n24.31\u2217\u2020\n33.61\u2217\nSynCtx\n19.87\u2217\u2020\n39.46\u2217\u2020\n24.66\u2217\u2020\n33.97\u2217\u2020\n+Multitask\nAuthorPred\n19.78\u2217\u2020\n39.36\u2217\u2020\n24.56\u2217\u2020\n33.87\u2217\u2020\nfurther improves the model\u2019s generation ability in most datasets.\nWe compare important methods in detail below.\nComparison among baselines. Comparing with ImmedCtx, UserID\nperforms surprisingly well, especially on the Amazon review and\nthe Reddit data. This is because the UserID model is trained on\nthe next snippet prediction task, resulting in relatively shorter\ngenerated output. By memorizing the user IDs, the model has an\nadvantage on datasets with shorter document length, e.g., the Ama-\nzon review and the Reddit data. However, since this model requires\nuser IDs as input, it has problems in generalizing to new users or\nscaling to a large number of users.\nLLMZeroShot is provided with the same input as SynCtx. The\nreduced performance suggests that finetuning is critical to the\npersonalized generation task.\nRetrieval augmented methods. All the retrieval augmented meth-\nods outperform ImmedCtx, which excludes all documents from a\nuser\u2019s personal context. This indicates that past documents provide\nuseful information when the model generates the current document\nfor a user.\nRecentDoc unexpectedly performs on par with some similarity\nbased retrieval methods in many cases, especially on the Avocado\nemail dataset. But it still performs worse than RankDocBySnpt.\nOn one hand, RecentDoc is a decent strategy as a user\u2019s recent\ndocuments might share similar writing style, and even similar con-\ntent, e.g., a user is writing multiple emails concerning a particular\ntopic, or a user starts to take interest in a particular book genre\nand are thus writing similar book reviews. On the other hand, pro-\nviding information more relevant to the current topic based on the\nimmediate context using RankDocBySnpt instead of simply re-\ntrieving the most recent content can further improve the generation\nperformance.\nRankDocBM25 performs closely to RankDocBySnpt except for\nthe Amazon review dataset, where RankDocBM25 is less effective.\nThis suggests that BM25 is still a powerful retrieval strategy even\nwhen the query, which is the immediate context, is longer than\ncommon search queries and is written in natural language.\nRankDocBySnpt consistently performs better than or equally\nwell as other retrieval based methods by combining the strength of\nRankDocDense and RankSnippet. By retrieving on the snippet\nlevel, dense retrieval becomes more effective by encoding less infor-\nmation in a single embedding. By ranking documents that contain\nthe retrieved snippets, the generation model is able to extract more\ndiverse or detailed information for content generation. RankDoc-\nDense, RankSnippet and RankDocBySnpt all perform well on\nthe Reddit data, due to the relatively short document length of this\ndataset.\nSummarization. SumCtxInd performs similarly as retrieval aug-\nmented methods without the summarization step, while SumCtx\noutperforms retrieval augmented methods. This indicates that a\ngeneric summary does not provide additional information to the\ngeneration model. The summary is more useful when it consid-\ners the immediate context. It guides the generation model to the\nincorporation of possible topics or phrases when generating the\nTable 4: Overall performance(%) on the Reddit dataset. *, \u2020\nindicate statistically significant improvement over UserID,\nRankSnippet respectively at the level of 0.01.\nReddit\nBleu\nRouge-1\nRouge-2\nRouge-L\nBaselines\nImmedCtx\n26.33\n43.69\n33.52\n40.53\nUserID\n26.55\n44.71\n34.61\n41.53\nLLMZeroShot\n24.04\n43.67\n31.24\n37.85\nRetrieval augmented methods\nRecentDoc\n28.23\u2217\n44.83\n34.57\n41.72\nRankDocBM25\n28.97\u2217\n45.71\u2217\n35.13\u2217\n42.52\u2217\nRankDocDense\n28.82\u2217\n45.61\u2217\n35.22\u2217\n42.56\u2217\nRankSnippet\n29.08\u2217\n45.94\u2217\n35.39\u2217\n42.68\u2217\nRankDocBySnpt\n28.87\u2217\n45.67\u2217\n35.24\u2217\n42.54\u2217\n+Summarization\nSumCtxInd\n28.91\u2217\n45.71\u2217\n35.26\u2217\n42.57\u2217\nSumCtx\n28.92\u2217\n46.09\u2217\n35.82\u2217\u2020\n43.14\u2217\u2020\n+Synthesis\nSynCtxInd\n28.82\u2217\n45.91\u2217\n35.87\u2217\u2020\n43.09\u2217\u2020\nSynCtx\n29.19\u2217\n46.45\u2217\u2020\n36.13\u2217\u2020\n43.27\u2217\u2020\n+Multitask\nAuthorPred\n29.13\u2217\n47.11\u2217\u2020\n36.94\u2217\u2020\n43.95\u2217\u2020\nXXX, XXX, XXX\nLi et al.\ncurrent document. Without the context dependent summary, the\ngeneration model needs to consider the high-level topics and the\nexact words to generate at the same time, making the task harder.\nSynthesis. We observe patterns similar as summarization \u2013 SynC-\ntxInd performs on par with SumCtx, a method without synthesis,\nwhile SynCtx outperforms SumCtx. This means that synthesis is\nuseful only when it is dependent on the immediate context. We\nalso see that SynCtx improves the Rouge-1 metric more than other\nmetrics. This is due to our design choice of the synthesis stage\nby identifying important unigrams. More sophisticated synthesis\nmethods are worth exploring as future work to improve metrics\nother than Rouge-1.\nMultitask. By jointly learning to predict whether two docu-\nments are from the same author, AuthorPred performs better\nthan SynCtx, which has a single task setting, on the Avocado email\nand the Reddit datasets, and performs closely on the Amazon re-\nview dataset. This indicates that improving the model\u2019s reading\nability by differentiating the writing style of different authors does\nno harm to the generation performance, which often benefits.\n7.2\nFormulation of input and output\nSince Table 2, 3, and 4 have already included the ablation study by\nreporting the performance of adding one new component at a time,\nwe additionally investigate whether varying the formulation of the\ninput and output will lead to significant change in performance.\nWe study the following settings.\n\u2022 NoRankedEntries. We investigate whether the ranked en-\ntries are still necessary when the summary and the syn-\nthesized outcome are available. To this end, we train the\ngeneration model using the immediate context, the context\ndependent summary, and the context dependent synthesis.\n\u2022 RemoveDocStart. The immediate context includes a short\nstart of the current document. For all the previous exper-\niments, this short start is still present in the ground-truth\nlabel, which is the current document, when we train the\ngeneration model. We study the effect of removing the short\nstart from the ground-truth label. Note that to make the\ncomparison fair, we prepend the short start to the model\ngenerated text when computing metrics.\n\u2022 ImmedCtxAtEnd. In all the experiments above, the informa-\ntion sources are placed in the order of the immediate context,\nthe summary, the synthesis, and the ranked entries. We study\nwhether the order matters by placing the immediate context\nat the end.\nTable 5: Analysis of formulation of input and output on the\nAvocado email dataset based on absolute change of perfor-\nmance(%) over AuthorPred. Other datasets show similar\npatterns and are omitted to save space.\nAvocado email\nBleu\nRouge-1\nRouge-2\nRouge-L\nNoRankedEntries\n-2.53\n-2.6\n-3\n-2.75\nRemoveDocStart\n-0.85\n-0.74\n-2.25\n-1.78\nImmedCtxAtEnd\n-0.12\n0.05\n0.13\n-0.08\nTable 6: Examples of model output. Italic words are used in\nthe immediate context.\nGround-truth doc: I just did not enjoy this book. I discovered this author a\nfew weeks ago and have been making my way through her cocktail series\nbut this one was not good for me. Didnt particularly like the characters\n(and I get it, Lucas looks like Prince Harry) and the plot was slow and less\nthan engaging. I finished it, but only because Im stubborn.\nSumCtx output: I just did not enjoy this book. I discovered this author a\nfew weeks ago and have been making my way through her books. I really\nliked the first book in the series, but this one was just not my favorite. I\njust didn\u2019t find myself connecting with the characters. I found the heroine\nto be a bit too naive and I just didn\u2019t find myself falling for the hero.\nRankDocBySnpt output: I just did not enjoy this book. I discovered this\nauthor a few weeks ago and have been making my way through her books.\nI\u2019ve enjoyed the first two books in this series and I\u2019m not sure why I didn\u2019t\nlike this one. I\u2019m not sure if it\u2019s because I\u2019m not a fan of the whole \"friends\nto lovers\" thing, but I just didn\u2019t like the way this book was written.\nWhy SumCtx is better: the summarization model mentions I didn\u2019t fall\nfor the hero, which makes the generation model comment more on the\ncharacters.\nGround-truth doc: Thanks for your insight. I do websites, mobile applica-\ntions, desktop applications. I have a lot of success with odesk and have been\nusing it ever since. I have been thinking of developing stuff for my own\nthe way I do it for my clients. I just get too busy with side projects and my\nfull-time employment.\nSynCtx output: Thanks for your insight. I do websites, mobile applications,\ndesktop applications. I have a lot of success with odesk and have been using\nit ever since. I have a lot of clients and I have been using it for a long time.\nI am a professional developer though and I have been doing this for a long\ntime.\nSumCtx output: Thanks for your insight. I do websites, mobile applications,\ndesktop applications. I have a lot of success with odesk and have been using\nit ever since. I have a lot of projects. I have a lot of experience with php,\njavascript, html, css, javascript, html5, c#, javascript, c#\nWhy SynCtx is better: the synthesis model mentions developer and free-\nlancer so that the generation model can focus on the topic of development\nand clients.\nGround-truth doc: As another reviewer wrote, this book flows gently. There\nisn\u2019t a lot of action and yes, the timeline bounces around but it\u2019s not hard to\nfigure out what is happening. But fair warning - this is a sad story. I got\nchoked up once or twice. It\u2019s very well written but the story isn\u2019t really\ngroundbreaking.\nAuthorPred output: As another reviewer wrote, this book flows gently.\nThere isn\u2019t a lot of action and yes, the timeline bounces around but it\u2019s not\nconfusing. The characters are well-developed and the story is interesting.\nI did find the ending a little sad, but I guess that\u2019s what I was expecting.\nSynCtx output: As another reviewer wrote, this book flows gently. There\nisn\u2019t a lot of action and yes, the timeline bounces around but it\u2019s not confusing.\nThe story is about a group of friends who have been friends since childhood.\nThey are all in their late 30s and have been friends since childhood.\nWhy AuthorPred is better: The author distinction task helps the model\nunderstand that this user often gives a high-level review of the story instead\nof going to details.\nDue to the space limit, we only show the performance on the\nAvocado email data in Table 5. Other datasets show similar patterns.\nNoRankedEntries underperforms AuthorPred by a large mar-\ngin, meaning that the generation model still relies on the retrieved\nTeach LLMs to Personalize \u2013 An Approach inspired by Writing Education\nXXX, XXX, XXX\nentries for information, e.g., word usage and writing style, even\nwhen the summary and the synthesis are available. There is a per-\nformance degradation of RemoveDocStart. We suspect that the\npresence of the short start of the current document in both the input\nand the ground-truth label helps the model understand the task bet-\nter. This is supported by the observation that the model converges\nfaster during training when the short start is included in the ground-\ntruth label. The close performance between ImmedCtxAtEnd and\nAuthorPred indicates that reordering the information sources\ndoes not affect the performance at least in the case of finetuning.\n7.3\nCase studies\nWe provide some illustrative examples in Table 6 for a better under-\nstanding of why the proposed method could provide better guidance\non how to generate personalized content.\n8\nCONCLUSION\nWe propose a general approach for teaching large language mod-\nels for personalized text generation. Analogous to how students\nare instructed to write from sources in a sequence of steps, the\nproposed approach consists of multiple stages: retrieval, ranking,\nsummarization, synthesis, and generation. Additionally, inspired by\nthe observation that reading and writing skills are correlated, we\ncreate a multitask setting that improves the model\u2019s reading ability\nby distinguishing the authorship of given document pairs. This\nmultitask setting further improves the model\u2019s ability to generate\npersonalized text empirically. We evaluate our models on three\npublicly released datasets from representative domains. Our results\ndemonstrate the effectiveness of the multistage multitask frame-\nwork. Investigation into the incorporation of world knowledge, e.g.,\nproduct information, is a promising direction for future work.\nREFERENCES\n[1] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin,\nAlexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen,\net al. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403 (2023).\n[2] James P. Callan. 1994. Passage-Level Evidence in Document Retrieval. In SIGIR\n\u201994, Bruce W. Croft and C. J. van Rijsbergen (Eds.). Springer London, London,\n302\u2013310.\n[3] Adeline Cooney, Eamon Darcy, and Denis Casey. 2018. Integrating reading and\nwriting: supporting students\u2019 writing from source. Journal of University Teaching\n& Learning Practice 15, 5 (2018), 3.\n[4] Marion Crowhurst. 1990. Reading/writing relationships: An intervention study.\nCanadian Journal of Education/Revue canadienne de l\u2019\u00e9ducation 15, 2 (1990), 155\u2013\n172.\n[5] Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero\nMolino, Jason Yosinski, and Rosanne Liu. [n.d.]. Plug and Play Language Models:\nA Simple Approach to Controlled Text Generation. In International Conference\non Learning Representations.\n[6] Christiane Fellbaum. 1998. WordNet: An electronic lexical database. MIT press.\n[7] Markus Freitag and Yaser Al-Onaizan. 2017. Beam Search Strategies for Neural\nMachine Translation. ACL 2017 (2017), 56.\n[8] Yong-Bing GAO, Jun-Tian GAO, Rong MA, and Li-Dong YANG. [n.d.]. Research\non user granularity-level personalized social text generation technology. Journal\nof Computer Applications ([n. d.]), 0.\n[9] Misha Khalman, Yao Zhao, and Mohammad Saleh. 2021. ForumSum: A multi-\nspeaker conversation summarization dataset. In Findings of the Association for\nComputational Linguistics: EMNLP 2021. 4592\u20134599.\n[10] Byeongchang Kim, Hyunwoo Kim, and Gunhee Kim. 2019. Abstractive Summa-\nrization of Reddit Posts with Multi-level Memory Networks. In NAACL-HLT.\n[11] Ben Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar,\nShafiq Joty, Richard Socher, and Nazneen Fatema Rajani. 2021. GeDi: Generative\nDiscriminator Guided Sequence Generation. In Findings of the Association for\nComputational Linguistics: EMNLP 2021. 4929\u20134952.\n[12] Juncen Li, Robin Jia, He He, and Percy Liang. 2018. Delete, Retrieve, Generate:\na Simple Approach to Sentiment and Style Transfer. In Proceedings of the 2018\nConference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long Papers). 1865\u20131874.\n[13] Junyi Li, Siqing Li, Wayne Xin Zhao, Gaole He, Zhicheng Wei, Nicholas Jing Yuan,\nand Ji-Rong Wen. 2020. Knowledge-enhanced personalized review generation\nwith capsule graph neural network. In Proceedings of the 29th ACM International\nConference on Information & Knowledge Management. 735\u2013744.\n[14] Pan Li and Alexander Tuzhilin. 2019. Towards Controllable and Personalized\nReview Generation. In Proceedings of the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP). 3237\u20133245.\n[15] Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries.\nIn Text summarization branches out. 74\u201381.\n[16] Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula,\nNoah A Smith, and Yejin Choi. 2021. DExperts: Decoding-Time Controlled Text\nGeneration with Experts and Anti-Experts. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Linguistics and the 11th International\nJoint Conference on Natural Language Processing (Volume 1: Long Papers). 6691\u2013\n6706.\n[17] Pierre-Emmanuel Mazare, Samuel Humeau, Martin Raison, and Antoine Bordes.\n2018. Training Millions of Personalized Dialogue Agents. In Proceedings of the\n2018 Conference on Empirical Methods in Natural Language Processing. 2775\u20132779.\n[18] Sean Patrick McCarron and Victor Kuperman. 2021. Is the author recognition test\na useful metric for native and non-native English speakers? An item response\ntheory analysis. Behavior Research Methods 53, 5 (2021), 2226\u20132237.\n[19] Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith Hall, Daniel\nCer, and Yinfei Yang. 2022. Sentence-T5: Scalable Sentence Encoders from Pre-\ntrained Text-to-Text Models. In Findings of the Association for Computational\nLinguistics: ACL 2022. 1864\u20131874.\n[20] Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019. Justifying recommendations\nusing distantly-labeled reviews and fine-grained aspects. In Proceedings of the\n2019 conference on empirical methods in natural language processing and the 9th\ninternational joint conference on natural language processing (EMNLP-IJCNLP).\n188\u2013197.\n[21] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern\u00e1ndez \u00c1brego, Ji Ma,\nVincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, et al. 2021. Large dual\nencoders are generalizable retrievers. arXiv preprint arXiv:2112.07899 (2021).\n[22] Douglas Oard, William Webber, David Kirsch, and Sergey Golitsynskiy. 2015.\nAvocado research email collection. Philadelphia: Linguistic Data Consortium\n(2015).\n[23] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a\nmethod for automatic evaluation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Computational Linguistics. 311\u2013318.\nXXX, XXX, XXX\nLi et al.\n[24] Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove:\nGlobal vectors for word representation. In Proceedings of the 2014 conference on\nempirical methods in natural language processing (EMNLP). 1532\u20131543.\n[25] Hongjin Qian, Xiaohe Li, Hanxun Zhong, Yu Guo, Yueyuan Ma, Yutao Zhu,\nZhanliang Liu, Zhicheng Dou, and Ji-Rong Wen. 2021. Pchatbot: A large-scale\ndataset for personalized chatbot. In Proceedings of the 44th International ACM\nSIGIR Conference on Research and Development in Information Retrieval. 2470\u2013\n2477.\n[26] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of\ntransfer learning with a unified text-to-text transformer. The Journal of Machine\nLearning Research 21, 1 (2020), 5485\u20135551.\n[27] Sudha Rao and Joel Tetreault. 2018. Dear Sir or Madam, May I Introduce the\nGYAFC Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer. In\nProceedings of the 2018 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long\nPapers). 129\u2013140.\n[28] Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu,\nMike Gatford, et al. 1995. Okapi at TREC-3. Nist Special Publication Sp 109 (1995),\n109.\n[29] Alireza Salemi, Sheshera Mysore, Michael Bendersky, and Hamed Zamani. 2023.\nLaMP: When Large Language Models Meet Personalization. arXiv preprint\narXiv:2304.11406 (2023).\n[30] Abigail See, Peter J Liu, and Christopher D Manning. 2017. Get To The Point:\nSummarization with Pointer-Generator Networks. In Proceedings of the 55th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers). 1073\u20131083.\n[31] Timothy Shanahan. 2015. Common Core State Standards: A new role for writing.\nThe Elementary School Journal 115, 4 (2015), 464\u2013479.\n[32] Noam Shazeer and Mitchell Stern. 2018. Adafactor: Adaptive learning rates with\nsublinear memory cost. In International Conference on Machine Learning. PMLR,\n4596\u20134604.\n[33] Tianxiao Shen, Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2017. Style trans-\nfer from non-parallel text by cross-alignment. Advances in neural information\nprocessing systems 30 (2017).\n[34] Khrystyna Skopyk, Artem Chernodub, and Vipul Raheja. [n.d.]. Personalizing\nLarge Language Models. ([n. d.]).\n[35] Stuck_In_the_Matrix. 2015. Reddit Public Comments (2007-10 through 2015-\n05). (2015). https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_\npublicly_available_reddit_comment/\n[36] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang,\nAakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain\nof thought reasoning in language models. arXiv preprint arXiv:2203.11171 (2022).\n[37] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,\nQuoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning\nin large language models. Advances in Neural Information Processing Systems 35\n(2022), 24824\u201324837.\n[38] Yuwei Wu, Xuezhe Ma, and Diyi Yang. 2021. Personalized response generation\nvia generative split memory network. In Proceedings of the 2021 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies. 1956\u20131970.\n[39] Kevin Yang and Dan Klein. 2021. FUDGE: Controlled Text Generation With\nFuture Discriminators. In Proceedings of the 2021 Conference of the North Ameri-\ncan Chapter of the Association for Computational Linguistics: Human Language\nTechnologies. 3511\u20133535.\n[40] Chenhan Yuan and Yi-chin Huang. 2020. Personalized sentence generation using\ngenerative adversarial networks with author-specific word usage. Computaci\u00f3n\ny Sistemas 24, 1 (2020), 17\u201328.\n[41] Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and\nJason Weston. 2018. Personalizing Dialogue Agents: I have a dog, do you have pets\ntoo?. In Proceedings of the 56th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers). 2204\u20132213.\n[42] Zhirui Zhang, Shuo Ren, Shujie Liu, Jianyong Wang, Peng Chen, Mu Li, Ming\nZhou, and Enhong Chen. 2018. Style transfer as unsupervised machine translation.\narXiv preprint arXiv:1808.07894 (2018).\n[43] Hanxun Zhong, Zhicheng Dou, Yutao Zhu, Hongjin Qian, and Ji-Rong Wen. 2022.\nLess is More: Learning to Refine Dialogue History for Personalized Dialogue\nGeneration. In Proceedings of the 2022 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies.\n5808\u20135820.\n"
  },
  {
    "title": "Dual-Stream Diffusion Net for Text-to-Video Generation",
    "link": "https://arxiv.org/pdf/2308.08316.pdf",
    "upvote": "23",
    "text": "Dual-Stream Diffusion Net for Text-to-Video Generation\nBinhui Liu1, Xin Liu2, Anbo Dai2, Zhiyong Zeng1,\nDan Wang1, Zhen Cui1, Jian Yang3\n1Nanjing University of Science and Technology, Nanjing, China\n2SeetaCloud, Nanjing, China\n3Nankai University, Tianjin, China\n{lbhasura, zhiyong.zeng, zhen.cui}@njust.edu.cn,\n{xin.liu, anbo.dai}@seetacloud.com, csjyang@nankai.edu.cn\na dog is walking in the street, narrow streets, clear day, cartoon\na girl is skiing, black dress, blonde hair\nthe two are dancing, sideways, black background, modern dance, blue-black tone\nFigure 1: Samples generated by our method.\nAbstract\nWith the emerging diffusion models, recently, text-to-video\ngeneration has aroused increasing attention. But an impor-\ntant bottleneck therein is that generative videos often tend to\ncarry some flickers and artifacts. In this work, we propose\na dual-stream diffusion net (DSDN) to improve the consis-\ntency of content variations in generating videos. In particular,\nthe designed two diffusion streams, video content and motion\nbranches, could not only run separately in their private spaces\nfor producing personalized video variations as well as con-\ntent, but also be well-aligned between the content and motion\ndomains through leveraging our designed cross-transformer\ninteraction module, which would benefit the smoothness of\ngenerated videos. Besides, we also introduce motion decom-\nposer and combiner to faciliate the operation on video mo-\ntion. Qualitative and quantitative experiments demonstrate\nthat our method could produce amazing continuous videos\nwith fewer flickers (see Fig. 1)1.\n1Please see the videos in supplementary material, and more\nIntroduction\nIn the realm of artificial intelligence generated content, one\nof the most exciting and challenging tasks is the transforma-\ntion from text into visual content. This task not only ben-\nefits for our understanding of natural language processing\nbut also promotes computer vision techniques. Meantime,\nit will cause immense potential applications in entertain-\nment, advertising, education, and surveillance. Over the past\nfew years, there has been substantial progress in developing\nmodels that convert textual descriptions into images (Nichol\net al. 2021; Ramesh et al. 2022; Rombach et al. 2022). In\ncontrast to images, videos could carry/express richer con-\ntent. For textual descriptions, videos can capture and con-\nvey intricate narratives well therein (Bain et al. 2021; Smaira\net al. 2020). Recently, the text-to-video generation has been\ninfo including code could be found in the anonymous website:\nhttps://anonymous.4open.science/r/Private-C3E8\narXiv:2308.08316v3  [cs.CV]  30 Dec 2023\npaid an increasing attention, specifically accompanying with\nthe rise of diffusion models.\nOne critical challenge lies in reasonable and continuous\ncontent generation in spatial-temporal domain for videos,\nnot only spatial content learnt by image-based diffusion\nmodels. Presently, the existing methods (Blattmann et al.\n2023; Hong et al. 2022; Singer et al. 2022) of text-to-video\ngeneration primarily focused on reproducing visual content\nfrom text. Due to the insufficiency in modeling video dy-\nnamics, their generated videos often contain many flickers\nand are intermittent in visual effects. To address this issue,\nin this work, our goal is to increase the consistency of motion\nand content between these video frames, while augmenting\nthe motion diversity of generated videos, so as to generate\nbetter visually continuous videos.\nTo this end, here we propose a dual-stream diffusion net\n(DSDN) to boost the consistency of content variations in\ngenerating videos. To characterize the motion information of\nvideo, in particular, we introduce a motion branch to encode\nvideo content variations besides the branch of video content.\nHereby, we construct a two-branch diffusion network, video\nmotion stream as well as video content stream. To make full\nof large-scale image generation model, the content stream\nruns upon a pre-trained text-to-image conditional diffusion\nmodel, but meantime is updated incrementally with a paral-\nlel network for personalized video content generation. On\nthe parallel step, the variations of video frames, i.e. mo-\ntion, takes a separate probability diffusion process through\nemploying 3D-UNet, so that personalize motion informa-\ntion could be generated. To align the generated content and\nmotion, we design a dual-stream transformation interaction\nmodule by using cross-attention between the two streams.\nAccordingly, the motion stream is integrated with the con-\ntent stream during the denoising process, which allows each\nstream to serve as contextual information for the other. Be-\nsides, we also introduce motion decomposer and combiner\nto faciliate the operation on motion. We conducted qualita-\ntive and quantitative experimental verification, and the ex-\nperiments demonstrate that our method enable to produce\nbetter visually continuous videos, as shown in Fig. 1.\nIn the end, we briefly summarize the contributions to the\nrealm of text-to-video generation: i) propose a Dual-Stream\nDiffusion Net (DSDN) to enhance the consistency and di-\nversity of generated videos, where the motion is specifi-\ncally modeled as a single branch that distinguishes from\nmost existing video diffusion methods; ii) design some use-\nful modules, including personalized content/motion genera-\ntion, dual-stream transformation interaction, to align content\nand motion while preserving the diversity of generated sam-\nples; iii) qualitative and quantitative evaluations demonstrate\nthat DSDN could effectively generates videos with remark-\nable consistency and diversity.\nRelated Work\nThe development and evolution of models for converting\ntextual descriptions into visual content have been a con-\nsistent focus in the field of artificial intelligence. The re-\nsearch has gradually transitioned from text-to-image mod-\nels to more dynamic and complex text-to-video generation\nmodels.\nText-to-Image Generation\nEarly efforts were dedicated\nto developing techniques for text-to-image synthesis. The\nDenoising Diffusion Probabilistic Model (DDPM) (Ho,\nJain, and Abbeel 2020; Song, Meng, and Ermon 2022) has\ngarnered significant attention owing to its remarkable abil-\nity to generate high-quality images. This innovative model\nhas exceeded the performance of previous generative ad-\nversarial networks (GANs) (Goodfellow et al. 2020), set-\nting a new benchmark in the field. Furthermore, the DDPM\nhas a unique feature: it can be trained with text guidance,\nempowering users to generate images from textual inputs.\nSeveral notable advancements have been made in this area.\nFor instance, GLIDE (Nichol et al. 2021) adopts classifier-\nfree guidance and trains the diffusion model using large-\nscale text-image pairs. DALLE-2 (Ramesh et al. 2022) uses\nCLIP (Radford et al. 2021) latent space as a condition,\nwhich significantly enhances the performance. Imagen (Sa-\nharia et al. 2022) employs a T5 (Raffel et al. 2020) coupled\nwith cascaded diffusion models to generate high-resolution\nimages. The Latent Diffusion Model (LDM) (Ramesh et al.\n2022) proposes forwarding the diffusion process in latent\nspace, demonstrating higher efficiency than other diffusion\nmodels.\nText-to-Video Generation\nDespite these advancements in\ntext-to-image models, transitioning to text-to-video synthe-\nsis presented new challenges, mainly due to the temporal\ndependencies between video frames and the need to main-\ntain motion semantics throughout the video sequence. Early\nworks in this regard include GAN-based methods (Von-\ndrick et al. 2016; Clark et al. 2019) and auto-regressive\none (Kalchbrenner et al. 2017; Hong et al. 2022). In the\ncontext of unconditional video generation, Ho et al. (Ho\net al. 2022) successfully extended the DDPM models ini-\ntially designed for images into the video domain, leading\nto the development of a 3D U-Net architecture. Harvey et\nal. (Harvey et al. 2022) put forth an innovative approach\nwherein they modeled the distribution of subsequent video\nframes in an auto-regressive manner. Our primary focus,\nhowever, lies in synthesizing videos in a controllable man-\nner - more specifically, in text-conditional video generation.\nExploring this avenue, Hong et al. (Hong et al. 2022) pro-\nposed CogVideo, an autoregressive framework that models\nthe video sequence by conditioning it on the given text and\nthe previous frames. Similarly, Levon et al. (Khachatryan\net al. 2023) proposed the Text2Video-Zero, a text-to-video\ngeneration method based on the text-to-image model sta-\nble diffusion (Rombach et al. 2022), which can not only di-\nrectly generate text-to-video, but also directly complete im-\nage editing tasks. The current issue in the text-to-video do-\nmain is that generative videos often tend to carry some flick-\ners and artifacts. Few attempts made to capture both the vi-\nsual and dynamic aspects of videos include the latent stream\ndiffusion models proposed by Ni et al. (Ni et al. 2023), and\net al. (Yu et al. 2023) projected latent video diffusion model\nfor generating long video through the integration of spatial\nand temporal information flow. These have been success-\nfully used in tasks such as generating high-quality images\nfrom textual descriptions (Nichol et al. 2021; Ramesh et al.\n2022; Rombach et al. 2022), while their potential in gener-\nating dynamic videos from text remains largely untapped.\nOur work is inspired by these previous research efforts and\nseeks to address the pitfalls common in existing models. We\nintroduce a novel dual-stream diffusion net to improve the\nconsistency of content variations in generating videos.\nMethod\nIn this section, we first provide an overview on the network,\nand then illustrate the details therein.\nOverview\nThe proposed DSDN network architecture is\nshown in Fig. 2. Initially, the input video x1:L\n0\nis projected\ninto a latent space via a frame-wise encoder E, denoted as\nz1:L\n0\n= E(x1:L\n0\n), where L is the length of video. Due to\nonly the frame-wise encoding without temporal dynamics,\nwe call z1:L\n0\nas the content features. To mine those temporal\ncues for better video generation, we introduce a motion de-\ncomposer to extract the corresponding motion information,\ndenoted as \u02dcz1:L\n0\n. Taking the both latent features as input, z1:L\n0\nand \u02dcz1:L\n0\n, we use a dual-stream diffusion way to producing\npersonalized video content and motion variation, and subse-\nquently propose a transformed interaction way to integrate\nthe two components to generate a video.\nAt the stage of dual-stream diffusion, two types of latent\nfeatures are transformed to standard Gaussian priors through\nseparate Forward Diffusion Processes (FDP). Then, the con-\ntent feature prior undergoes denoising along Personalized\nContent Generation Stream (PCGS), which would result in\npure denoised content features z1:L\n0|T . Similarly, the motion\nfeature prior is denoised by Personalized Motion Generation\nStream, which would lead to pure denoised motion features\n\u02dcz1:L\n0|T . To further align the generated content and motion for\nsuppressing flickers, we design a Dual-Stream Transforma-\ntion Interaction module to bridge the two types of generation\nstreams. After the alignment learning, we use a motion com-\nbiner to compensate dynamic information to video content,\nand finally form the latent feature of the target video, fol-\nlowing by a decoder D to produce videos in the pixel space.\nForward Diffusion Process\nTo reduce resource con-\nsumption, we take the similar way to the latent diffusion\nmodel (Rombach et al. 2022), underpinned by Denois-\ning Diffusion Probabilistic Models (DDPM) (Ho, Jain, and\nAbbeel 2020). Before diffusion, we use a pre-trained Vec-\ntor Quantized Variational AutoEncoder (VQ-VAE) (van den\nOord, Vinyals, and Kavukcuoglu 2018) to project video\nframes into a latent feature space, i.e., z1:L\n0\n= E(x1:L\n0\n). For\nsimplification, we frozen the encoder E during training. The\ncontent features z1:L\n0\nare then processed through a motion\ndecomposer (please see the following part: Motion Decom-\nposition and Combination) to obtain the motion feature \u02dcz1:L\n0\n.\nThe two part of features suffer noise perturbation through a\npre-defined Markov process, formally,\nq(zt|zt\u22121) = N(zt;\np\n1 \u2212 \u03b2tzt\u22121, \u03b2tI),\nq\u2032( \u02dczt|\u02dczt\u22121) = N(\u02dczt;\np\n1 \u2212 \u03b2t\u02dczt\u22121, \u03b2tI),\n(1)\nwhere t = 1, ..., T and T is the number of diffusion steps,\n\u03b2t defines the strength of noise at each iterative step. It is\nworth noting that the shared noising schedule for the two\nstreams works well in our experience. According to DDPM,\nthe above recursion formula could be derived into the a con-\ndensed version,\nz1:L\nt\n= \u221a \u00af\u03b1tz1:L\n0\n+\n\u221a\n1 \u2212 \u00af\u03b1t\u03f51, \u03f51 \u223c N(0, I),\n\u02dcz1:L\nt\n= \u221a \u00af\u03b1t\u02dcz1:L\n0\n+\n\u221a\n1 \u2212 \u00af\u03b1t\u03f52, \u03f52 \u223c N(0, I),\n(2)\nwhere \u00af\u03b1t = Qt\ni=1 \u03b1t, \u03b1t = 1 \u2212 \u03b2t. Until now, we have suc-\ncessfully completed the forward diffusion process for both\ncontent and motion features. This provides us with the priors\nz1:L\nT\nand \u02dcz1:L\nT\nwhich are instrumental in driving the ensuing\ndenoising process.\nPersonalized Content Generation Stream\nIn order to\ntake advantage of well-trained image-based diffusion model,\nwe leverage the large-scale text-to-image model, Stable Dif-\nfusion (Rombach et al. 2022), as the fundamental model of\nvideo content generation. But to better support personalized\nvideo content generation, we design an incremental learn-\ning module to refine content generation by using a similar\nway to LoRA (Hu et al. 2021). As shown in Fig. 2, we refer\nto them as the content basic unit and the content increment\nunit, respectively. The model parameters of the two units are\nadaptively integrated to boost content features. Such a way\nnot only inherits the merit of large-scale image-based gener-\nation model but also endows the creation of unique and per-\nsonalized content, contributing to the overall improvement\nof our method.\nConcretely, the content basic unit uses a modified U-\nNet architecture, where each resolution level incorporates\n2D convolution layers with self-attention and cross-attention\nmechanisms. Concurrently, the content increment unit em-\nploys an extra network branch with a few tunable parame-\nters for fine tuning. Suppose the basic unit is with parame-\nters W, we have the post-tuned weight: W \u2032 = W + \u03bb\u2206W,\nwhere \u2206W is the update quantity and \u03bb is the step length.\nThe hyper-parameter \u03bb dictates the influence exerted by the\ntuning process, thereby offering users extensive control over\nthe generation outcome. To counter potential over-fitting and\nreduce computational overhead, \u2206W \u2208 Rm\u00d7n is decom-\nposed into two low-rank matrices, as used in LoRA (Hu\net al. 2021). Let\u2019s denote \u2206W = ABT , where A \u2208 Rm\u00d7r,\nB \u2208 Rn\u00d7r, and r \u226a m, n.\nTo improve the smoothness of generated content frames,\nwe also generate video content on the condition of motion\ninformation, which refers to cross-transformer introduced in\nthe part: Dual-Stream Transformation Interaction. Formally,\nwe give the optimized objective on content information,\nLcon = Eez,y,t[\n\r\r\u03f5 \u2212 \u03f5\u03b8(z1:L\nt\n, t | c(y), \u02dcz1:L\nt\n)\n\r\r2\n2],\n(3)\nwhere y is the corresponding textual description, \u03f5\u03b8(\u00b7) here\nrepresents the part of the personalized content generation\nstream with the network parameter \u03b8. Note that we employ\nthe text encoder of CLIP (Radford et al. 2021) to perform\nthe text feature extraction c(\u00b7).\nT\n \nMotion \nDecomposer\nMotion \nCombiner\nText\nConditions\n a girl is playing the guitar c\nL\n \nContent \nBasic Unit\nContent \nIncrement \nUnit\nMotion Unit\n \n \n  \n   \n   \n   \n   \n   \n  \n   \n   \n   \n  \n   \n   \n   \n    \n   \n     \n   \n    \n   \n     \n   \nCross-Trans.\nFigure 2: DSDN network framework. Initially, Content and motion features are added to noise during the diffusion process,\nfollowed by a denoising step via the dual-stream diffusion net. Lastly, the latent space features of the generated video are\nobtained through the motion combiner and decoded to render the final generated video.\nPersonalized Motion Generation Stream\nIn the person-\nalized motion generation stream, we employ a 3D U-Net\nbased diffusion model to generate a motion-coherent latent\nfeatures, wherein the network architecture of 3D U-Net is\nsimilar to that in (Ho et al. 2022). The reason why use 3D\nU-Net is that the global motion variation of the entire input\nvideo could be captured for subsequent motion generation.\nGiven an input sequence of motion priors ez1:L\nT , we can ob-\ntain a transformed representation vector after using the en-\ncoding stage of 3D U-Net, and the next denosing process\ntakes the vector as input for diffusion, similar to DDPM. Dif-\nferently, to make the generated motion matched with con-\ntent, we use the generated content feature z1:L\nt\nas well as the\ntext prompt c(y), as the conditions, in the denoising diffu-\nsion process. The use of content condition refers to cross-\ntransformer, which will be introduced in the next part: Dual-\nStream Transformation Interaction. Hereby, the training ob-\njective of the personalized motion generation stream can be\nformulated as:\nLmot = Ez,y,t[\n\r\r\u03f5 \u2212 \u03f5\u02dc\u03b8(\u02dcz1:L\nt\n, t | c(y), z1:L\nt\n)\n\r\r2\n2],\n(4)\nwhere \u03f5\u02dc\u03b8(\u00b7) represents the part of the personalized motion\ngeneration stream with the network parameter \u02dc\u03b8.\nDual-Stream Transformation Interaction\nTo well align\nthe generated content and motion information, we design a\ncross-transformer interaction way between the two denois-\ning streams. On the one hand, we infuse the denoising gen-\neration procedure of the motion stream with conditional fea-\nture information from the content, by taking the transformer\nfrom content to motion, which would enhance the continuity\nof the overall motion. On the other hand, the denoising gen-\neration process of the content stream also absorbs the con-\nditional feature information from the motion, by taking the\ntransformer from motion to content. This cross-transformer\nbased streams render the overall content smoother, creating\na synergistic effect that enhances the consistency and quality\nof the final output. In detail, after each convolutional layer of\nU-Net, we interpose a cross-attention layer to integrate the\nlatent features of both content stream and motion stream.\nTaking the case from motion to content as an example, for-\nmally, we have\nzcon=Att(Qmot, Kcon, Vcon)=Softmax(QmotKT\ncon\n\u221a\nd\n) \u00b7 Vcon,\n(5)\nwhere Qmot = W Qzmot, Kcon = W Kzcon, and Vcon =\nW V zcon denote three projections of cross-attention along\nthe content stream with the parameters W Q, W K, W V , and\nd is the feature dimensionality. The motion stream fea-\ntures can constrain the content stream generated to ensure\nsmoother transitions from frame to frame. Similarly, the\nsame principle applies for the motion stream as well. At this\ntime, the content stream features can supply an understand-\ning of image apparent information for the generation of mo-\ntion latent features during the denoising process. Hence, the\ncross-attention layer in this context facilitates mutual condi-\ntioning between the dual-stream features.\nIntuitively, such a cross-transformer strategy is explicitly\nMotion \nStream \nLayer\nContent \nPrior\nMotion \nPrior\nFrozen\nTrainable\nReshape\nMotion \nStream \nLayer\nContent\nBasic Unit\nContent \nIncrement\nContent\nBasic Unit\nContent \nIncrement\nFigure 3: Dual-stream transformation block.\nperformed on two branches of diffusion processes. This is\nvery different from those previous video diffusion meth-\nods (Blattmann et al. 2023; Guo et al. 2023; Hong et al.\n2022), which essentially use a single-stream diffusion pro-\ncess by either directly inserting a pseudo 3D layer to man-\nage temporal information, or intercalating a 3D layer be-\ntween two successive 2D convolution layers. Besides, the\ndual-stream diffusion network also take the corresponding\ntextual conditional embedding as input.\nIn the end, the total optimization objective of dual-stream\ndiffusion net is the joint in both Eq. 3 and Eq. 4. Throughout\nthe optimization process, only the content increment unit (in\nthe content stream) and the cross-attention layer (between\ntwo denoising streams) are trainable, whilst the content basic\nunit, i.e., the underlying text-to-image model in the content\nstream, remains static in order to preserve the consistency of\nits feature space. An illustration is shown in Fig. 3.\n1x1, 2D Conv\nB, L, C, H, W\nB, C/r, H, W\nB, C/r, H, W\n3x3, \n2D Conv\nconcate\n1x1, 2D Conv\nB, C/r, H, W\nB, L, C, H, W\n  \n   \n   \n   \n  \n   \n1x1, 2D Conv\nB, 2L, C, H, W\n3x3, \n2D Conv\n3x3, \n2D Conv\n  \n \n   \n \n1x1, 2D Conv\nB, C/r, H, W\nB, C/r, H, W\nB, L, C, H, W\nconcate\nB, C/r, H, W\n   \n   \n(a) Motion Decomposer\n(b) Motion Combiner\n    \n   \n     \n   \n&\n   \n   \nL\n  \n \nL\nFigure 4: Details of Motion Decomposer and Motion Com-\nbiner.\nMotion Decomposition and Combination\nIn order to\nseparate motion features and reduce computational cost, we\ndesign the lightweight motion decomposer and correspond-\ning motion combiner inspired by the work (Jiang et al.\n2019). In terms of the motion decomposer, given the input\ncontent feature z1:L\n0\n\u2208 RB\u00d7L\u00d7C\u00d7H\u00d7W , the motion decom-\nposer first utilizes a 1 \u00d7 1 convolution layer to reduce the\nchannel (C) by a factor of r, which would alleviate com-\nputing expense. We then compute motion features derived\nfrom every pair of sequential frames. For instance, given\ntransformed zl\n0 and zl+1\n0\n, we initially apply a 2D channel-\nwise convolution to zl+1\n0\n. Subsequently, we subtract this new\nvalue from zl\n0 to obtain the (l)-th and (l+1)-th frame motion\nrepresentation \u02dczl\n0, formally,\n\u02dczl\n0 = conv(zl+1\n0\n) \u2212 zl\n0,\n(6)\nwhere conv(\u00b7) denotes a 2D channel-wise convolution. As\ndepicted in Fig. 4(a), we apply the motion decomposer on\nevery two adjacent frames. As a result, the motion decom-\nposer generates L \u2212 1 frames of motion features. To ensure\ncompatibility with the original temporal length, we append\nthe last frame of video to reach the length L in our experi-\nment. Finally, another 1 \u00d7 1 2D convolution layer is utilized\nto restore the number of channels back to C.\nFor the motion combiner, given the denoised content and\nmotion features z1:L and ez1:L, we also first employ 1 \u00d7 1\nconvolution to reduce the channel number. As shown in\nFig. 4(b), the content feature and their adjacent motion fea-\ntures are fused after doing a 2D convolution on motion fea-\ntures. Formally, the l-th frame latent feature \u02c6zl\n0 of the gener-\nated video is defined as,\n\u02c6zl\n0 = conv(\u02dczl\u22121\n0\n) + zl\n0 + conv(\u02dczl\n0),\n(7)\nwhere conv represents the 2D channel-wise convolution.\nUpon acquiring the combined video latent features, it comes\nback to the original channel dimension via a 1 \u00d7 1 convolu-\ntional layer. This combined features are then input into the\nfinal decoder, which yields a video in the pixel space.\nExperiments\nImplementation Details\nIn our experimental setup, we generate L = 16 frames\nwith a resolution 512 for each video. We trained the Dual-\nStream Diffusion Net using a subset (comprising 5M videos)\nfrom the WebVid-10M (Bain et al. 2021) and HD-VILA-\n100M (Xue et al. 2022) datasets. Video clips within the\ndataset are first sampled at a stride of 4, then resized and\ncentrally cropped to a resolution of 256 \u00d7 256. For the con-\ntent basic unit, we employ stable diffusion v1.5 pre-trained\nweights which remain frozen throughout the training proce-\ndure. The content incremental unit does not commence train-\ning from scratch, but rather utilizes an existing model (Civ-\nitai 2022) as pre-trained weights, followed by fine-tuning\non our training data. For the motion unit, we initialize our\nPersonalized Motion Generation Stream with the weights\nof LDM (Rombach et al. 2022), which were pre-trained\non Laion-5B (Schuhmann et al. 2022). During inference, it\ntakes approximately 35 seconds to sample a single video us-\ning one NVIDIA RTX 4090 GPU.\na man is dancing in the rain\na horse is galloping on a street\na man is running in the snow\na panda is walking down the street\nFigure 5: Qualitative comparison between Text2Video-\nZero (Khachatryan et al. 2023) (frames 1-4 in each row) and\nour method (frames 5-8 in each row). Please see the videos\nin the website.\nTable 1: Comparison of CLIP score metric with baselines.\nMethods\nFrame Consistency\nTextual Alignment\nCogVideo\n88.32\n22.02\nText2Video-Zero\n90.21\n29.56\nOurs\n92.13\n32.23\nComparison with Baselines\nWe compare our method with two publicly available base-\nline: 1) CogVideo (Hong et al. 2022): a Text-to-Video model\ntrained on a dataset of 5.4 million captioned videos, and\nis capable of generating videos directly from text prompts\nin a zero-shot manner. 2) Text2Video-Zero (Khachatryan\net al. 2023): also a Text-to-Video generation method based\non the Stable Diffusion model. Since our method is a text-\nto-video method we compare with Text2Video-Zero in pure\ntext-guided video synthesis settings. Owing to space con-\nstraints, we present a quantitative comparison of our method\nwith the two aforementioned approaches. However, for qual-\nitative results, we limit our comparative analysis to the supe-\nrior performing text2video-zero method for a more focused\nand meaningful evaluation.\nFigure 6: The diversity of our method qualitative results.\nPrompt: a cat is walking on the grass.\nQuantitative Comparison\nWe evaluate our method in re-\nlation to baseline models by employing automated metrics,\ndetailing frame consistency and textual alignment outcomes\nin the accompanying Table. 1. For assessing frame consis-\ntency, we compute CLIP (Radford et al. 2021) image em-\nbeddings on all frames within the output videos, reporting\nthe average cosine similarity across all pairings of video\nframes. To evaluate textual alignment, we calculate the av-\nerage CLIP score between all frames in the output videos\nand their corresponding prompts. Our findings reveal that\nthe videos generated via our method surpass publicly ac-\ncessible alternatives such as CogVideo (Hong et al. 2022)\nand Text2Video-Zero (Khachatryan et al. 2023), particularly\nwith regard to frame consistency and text alignment. This\nsuggests that our method offers a more robust and coherent\napproach to video generation from textual prompts.\nQualitative Comparison\nWhen compared to text2video-\nzero, our method demonstrates superior consistency in both\ncontent and motion across generated videos, as shown in\nFig. 5. As illustrated in the first row, observable body swings\nare evident as the panda walks, while in the second row,\nwe see the limbs swing as the person runs, accompanied\nby gradual changes in the background. In the third row, the\nlower limbs of the horse are seen swinging as it gallops,\nset against a dynamic background. Furthermore, our method\noutperforms the other approach in terms of content qual-\nity and its conformity with the text. For instance, the gen-\nerated pandas in our model appear more realistically ren-\ndered, the snow in the second row exhibits imprints, and the\nstreet in the third row is more logically constructed. We fur-\nther include a fourth row as a comparative example in a less\nfavourable environment - rain conditions. Here, the content\ngenerated by the other method appears unrealistic, whereas\nour method not only captures the essence of a rainy day\nmore effectively but also establishes the logical connection\nbetween rain and umbrellas, thereby enhancing the realism\nand context-appropriateness of generated videos.\nBeyond this, we further show the qualitative results of our\nmethod on video diversity generation, as shown in Fig. 6.\nUsing \u201da cat is walking on the grass\u201d as the text input, we can\nsee various actions such as a cat walking forward, left, and\nright. Impressively, the generated videos also exhibit diver-\nsity in aspects like fur color, body shape, and pose, thereby\nencompassing a rich variety in content. Concurrently, the\ngenerated video preserves high continuity. As demonstrated\nin the first row, the flower at the lower right gradually comes\ninto view, while in the second row, subtle changes in the cat\u2019s\nshadow can be discerned. In the third row, a figure in the\nbackground is progressively moving, further enhancing the\nsense of dynamic realism. Furthermore, we acknowledge a\nminor failure case depicted at the end of the third row, where\nthe color of the cat appears slightly altered. This issue pri-\nmarily stems from the generated background inadvertently\ninfluencing the content creation process itself. However, as\nevident from the comprehensive results showcased, such in-\nstances are rare, thereby attesting to the robustness and reli-\nability of our proposed method in text-to-video generation.\nw/o Incremental \nUnit\nw/o Motion \nUnit\nDSDN\nMotion Unit \nVisualization\nFigure 7: Ablation study. Prompt: a girl is dancing among leaves, curly hair.\nAblation Study\nWe conduct a rigorous ablation study to evaluate the signifi-\ncance of both the content increment unit and the motion unit,\nas depicted in Fig 7. Each design component is selectively\nablated to determine its individual impact on the model\u2019s\noverall performance.\nMotion Unit\nThe outcomes from the first row indicate that\nwhile a model void of the motion unit can still synthesize\napparent content in accordance with text conditions, it fails\nto maintain the continuity between video frames. This re-\nsult stems from the absence of temporal dimension mod-\neling\u2014resulting in generated video frames being indepen-\ndently constrained by text conditions without inter-frame\nconnection. In terms of content congruity, the generated\nvideo frames exhibit a solid alignment with the narrative\nconveyed by the textual conditions. For instance, elements\nlike \u2019dancing\u2019, \u2019leaves\u2019, and \u2019curly hair\u2019 described in the text\nare accurately manifested within the generated imagery.\nIncremental Unit\nAs observed in the second row, the visi-\nble content quality suffers a significant reduction without the\nincremental unit model. This underscores the pivotal role of\nthe content increment unit in learning richer visual content\nbeyond what the content base unit alone can achieve. Upon\nanalyzing the results from the first three rows, we observe a\nfew issues: 1) Fine-tuning the incremental unit seems to sta-\nbilize the apparent content; for instance, the girls in both the\nfirst and third rows face forward, whereas without the incre-\nmental unit, as seen in the second row, the girl\u2019s perspective\ncan emerge from any direction. 2)The clothing color in the\nfirst and third rows leans towards green, mirroring the hue of\nthe background environment. These challenges might arise\ndue to limited parameter volume within the incremental unit,\nthereby restricting the scope of apparent content it can learn\neffectively. Such observations underscore areas for further\nexploration and improvement in the incremental unit of our\nmethod.\nMotion Unit Visualization\nFurthermore, we offer a de-\ntailed visualization of the motion unit\u2019s effects of the third\nrow in the last row. The visualizations highlight the efficacy\nof the motion unit in accurately capturing inter-frame mo-\ntion details such as arm swings, body movements, and hair\nfluttering, thereby underscoring its critical role in achieving\na coherent and dynamic video output.\nConclusion\nThis work presented a novel dual-stream diffusion net\n(DSDN) to improve the consistency of content variations\nin generating videos. Specifically, the designed two diffu-\nsion streams, video content and motion branches, could not\nonly run separately in their private spaces for producing\npersonalized video variations as well as content, but also\nbe well-aligned between the content and motion domains\nthrough leveraging our designed cross-transformer interac-\ntion module, which would benefit the smoothness of gen-\nerated videos and enhance the consistency and diversity of\ngenerated frames, where the motion is specifically mod-\neled as a single branch that distinguishes from most exist-\ning video diffusion methods. Besides, we also introduced\nmotion decomposer and combiner to faciliate the operation\non video motion. Qualitative and quantitative experiments\ndemonstrated that our method produces better continuous\nvideos with fewer flickers.\nReferences\nBain, M.; Nagrani, A.; Varol, G.; and Zisserman, A. 2021.\nFrozen in Time: A Joint Video and Image Encoder for End-\nto-End Retrieval.\nProceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision (ICCV).\nBlattmann, A.; Rombach, R.; Ling, H.; Dockhorn, T.; Kim,\nS. W.; Fidler, S.; and Kreis, K. 2023.\nAlign Your La-\ntents: High-Resolution Video Synthesis With Latent Diffu-\nsion Models. Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 22563\u2013\n22575.\nCivitai. 2022. Civitai. https://civitai.com/.\nClark, A.; Donahue, J.; ; and Simonyan, K. 2019. Adversar-\nial Video Generation on Complex Datasets. arXiv preprint\narXiv:1907.06571.\nGoodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.;\nWarde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y.\n2020. Generative adversarial networks. Communications of\nthe ACM.\nGuo, Y.; Yang, C.; Rao, A.; Wang, Y.; Qiao, Y.; Lin, D.;\nand Dai, B. 2023. AnimateDiff: Animate Your Personal-\nized Text-to-Image Diffusion Models without Specific Tun-\ning. arXiv preprint arXiv:2307.04725.\nHarvey, W.; Naderiparizi, S.; Masrani, V.; Weilbach, C.; ;\nand Wood, F. 2022. Flexible Diffusion Modeling of Long\nVideos. arXiv preprint arXiv:2205.11495.\nHo, J.; Jain, A.; and Abbeel, P. 2020. Denoising Diffusion\nProbabilistic Models. Neural Information Processing Sys-\ntems (NeurIPS).\nHo, J.; Salimans, T.; Gritsenko, A.; Chan, W.; Norouzi, M.;\nand Fleet, D. J. 2022.\nVideo Diffusion Models.\narXiv\npreprint arXiv:2204.03458.\nHong, W.; Ding, M.; Zheng, W.; Liu, X.; and Tang, J. 2022.\nCogVideo: Large-scale Pretraining for Text-to-Video Gen-\neration via Transformers. arXiv preprint arXiv:2205.15868.\nHu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang,\nS.; Wang, L.; and Chen, W. 2021.\nLoRA: Low-Rank\nAdaptation of Large Language Models.\narXiv preprint\narXiv:2106.09685.\nJiang, B.; Wang, M.; Gan, W.; Wu, W.; and Yan, J. 2019.\nSTM: SpatioTemporal and Motion Encoding for Action\nRecognition. Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV), 2000\u20132009.\nKalchbrenner, N.; Oord, A.; Simonyan, K.; Danihelka, I.;\nVinyals, O.; Graves, A.; ; and Kavukcuoglu, K. 2017. Video\nPixel Networks. In Proceedings of the 34th International\nConference on Machine Learning, 1771\u20131779.\nKhachatryan, L.; Movsisyan, A.; Tadevosyan, V.; Hen-\nschel, R.; Wang, Z.; Navasardyan, S.; and Shi, H. 2023.\nText2Video-Zero: Text-to-Image Diffusion Models are\nZero-Shot Video Generators. Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV).\nNi, H.; Shi, C.; Li, K.; Huang, S. X.; and Min, M. R. 2023.\nConditional Image-to-Video Generation With Latent Flow\nDiffusion Models. Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\n18444\u201318455.\nNichol, A.; Dhariwal, P.; Ramesh, A.; Shyam, P.; Mishkin,\nP.; McGrew, B.; Sutskever, I.; and Chen, M. 2021. Glide: To-\nwards photorealistic image generation and editing with text-\nguided diffusion models. arXiv preprint arXiv:2112.10741.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nand Agarwal, S. 2021. Learning Transferable Visual Mod-\nels From Natural Language Supervision.\nProceedings of\nthe 38th International Conference on Machine Learning and\nPMLR, 8748\u20138763.\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; and Narang, S.\n2020. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. The Journal of Machine Learning\nResearch, 5485\u20135551.\nRamesh, A.; Dhariwal, P.; Nichol, A.; Chu, C.; and Chen,\nM. 2022. Hierarchical Text-Conditional Image Generation\nwith CLIP Latents. arXiv preprint arXiv:2204.06125.\nRombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Om-\nmer., B. 2022.\nHigh-resolution image synthesis with la-\ntent diffusion models.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\n10684\u201310695.\nSaharia, C.; Chan, W.; Saxena, S.; Li, L.; Whang, J.; Den-\nton, E. L.; and Ghasemipour, K. 2022. Photorealistic text-\nto-image diffusion models with deep language understand-\ning. Advances in Neural Information Processing Systems,\n36479\u201336494.\nSchuhmann, C.; Vencu, R.; Beaumont, R.; Coombes, T.;\nGordon, C.; Katta, A.; Kaczmarczyk, R.; ; and Jitsev, J.\n2022.\nLAION-5B: A new era of open large-scale multi-\nmodal datasets. arXiv preprint arXiv:2307.04725.\nSinger, U.; Polyak, A.; Hayes, T.; Yin, X.; An, J.; Zhang, S.;\nHu, Q.; Yang, H.; Ashual, O.; and Gafni, O. 2022. Make-\na-video: Text-to-video generation without text-video data.\narXiv preprint arXiv:2209.14792.\nSmaira, L.; Carreira, J.; Noland, E.; Clancy, E.; Wu, A.; and\nZisserman, A. 2020. A Short Note on the Kinetics-700-2020\nHuman Action Dataset. arXiv:2010.10864.\nSong, J.; Meng, C.; and Ermon, S. 2022. Denoising Diffu-\nsion Implicit Models. arXiv preprint arXiv:2010.02502.\nvan den Oord, A.; Vinyals, O.; and Kavukcuoglu, K. 2018.\nNeural Discrete Representation Learning. Advances in Neu-\nral Information Processing Systems.\nVondrick, C.; Pirsiavash, H.; ; and Torralba, A. 2016.\nGenerating Videos with Scene Dynamics.\narXiv preprint\narXiv:1609.02612.\nXue, H.; Hang, T.; Zeng, Y.; Sun, Y.; Liu, B.; Yang, H.; Fu,\nJ.; ; and Guo, B. 2022. Advancing high-resolution videolan-\nguage representation with large-scale video transcriptions.\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), 5036\u20135045.\nYu, S.; Sohn, K.; Kim, S.; and Shin, J. 2023. Video Prob-\nabilistic Diffusion Models in Projected Latent Space. Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), 18456\u201318466.\n"
  },
  {
    "title": "DragNUWA: Fine-grained Control in Video Generation by Integrating Text, Image, and Trajectory",
    "link": "https://arxiv.org/pdf/2308.08089.pdf",
    "upvote": "20",
    "text": "*Ongoing work\nDRAGNUWA: FINE-GRAINED CONTROL\nIN VIDEO\nGENERATION BY INTEGRATING TEXT, IMAGE, AND\nTRAJECTORY\nShengming Yin1\u2217\nChenfei Wu2\u2217\nJian Liang3\nJie Shi3\nHouqiang Li1\nGong Ming2\nNan Duan2\u2020\n1University of Science and Technology of China\n2Microsoft Research Asia\n3Peking University\n{sheyin@mail.,lihq}@ustc.edu.cn, {chewu,migon,nanduan}@microsoft.com, {j.liang@stu.,jieshi@}pku.edu.cn\nInput Text\nInput Image\nInput Trajectory\nOutput Video\nA cat is \nrunning on \nthe lawn\nA boy is \nrunning on\nthe lawn\nFish \nswimming \nin the pond\nFish \nswimming \nin the pond\nA boy playing \nfootball on the \nplayground\nA boy playing \nfootball on the \nplayground\nFigure 1: DragNUWA integrates text, image, and trajectory controls to achieve controllable video\ngeneration from semantic, spatial, and temporal perspectives. The three groups of examples demon-\nstrate the impact of altering one control while keeping the other two fixed. The first group (Row\n1-2) displays the control of complex trajectories, including complex motions (red curved arrows)\nand camera movements (red rightward arrows). The second group (Row 3-4) illustrates the influ-\nence of language control, pairing different text with the same image and trajectory to achieve the\neffect of introducing new objects in the images. The third group (Row 5-6) demonstrates the impact\nof image control, showcasing the generation of both real-world and artistic videos.\nABSTRACT\nControllable video generation has gained significant attention in recent years.\nHowever, two main limitations persist: Firstly, most existing works focus on ei-\nther text, image, or trajectory-based control, leading to an inability to achieve\nfine-grained control in videos. Secondly, trajectory control research is still in its\nearly stages, with most experiments being conducted on simple datasets like Hu-\nman3.6M. This constraint limits the models\u2019 capability to process open-domain\nimages and effectively handle complex curved trajectories. In this paper, we pro-\npose DragNUWA, an open-domain diffusion-based video generation model. To\ntackle the issue of insufficient control granularity in existing works, we simulta-\nneously introduce text, image, and trajectory information to provide fine-grained\ncontrol over video content from semantic, spatial, and temporal perspectives.\nTo resolve the problem of limited open-domain trajectory control in current re-\nsearch, We propose trajectory modeling with three aspects: a Trajectory Sampler\n(TS) to enable open-domain control of arbitrary trajectories, a Multiscale Fusion\n(MF) to control trajectories in different granularities, and an Adaptive Training\n(AT) strategy to generate consistent videos following trajectories. Our experi-\nments validate the effectiveness of DragNUWA, demonstrating its superior perfor-\nmance in fine-grained control in video generation. The homepage link is https:\n//www.microsoft.com/en-us/research/project/dragnuwa/\n\u2217Both authors contributed equally to this research. Shengming, Jian, and Jie\u2019s internship work under the mentorship of Chenfei.\n\u2020Corresponding author.\n1\narXiv:2308.08089v1  [cs.CV]  16 Aug 2023\n*Ongoing work\nA boat sailing on the lake\nA girl is skiing in the forest\nA steam locomotive journeying through the verdant forest\nFew people walking on the street\nHerd of elephants walking in the forest\nAlong the river of Qingming Festival\nA group of people discussing at the table\nA dog is running in the path\nZebras are running\nFigure 2: Samples generated by DragNUWA are presented, with the first column showcasing three\ninput controls: text, image, and trajectory. The second, third, and fourth columns exhibit the 5th,\n10th, and 15th frames of the output video, respectively. There are 16 frames with a resolution of\n576 \u00d7 320 in total. DragNUWA is capable of concurrently controlling the movement of the camera,\nmultiple objects, and complex trajectories, enabling the generation of videos featuring both real-\nworld scenes and artistic paintings.\n2\n*Ongoing work\n1\nINTRODUCTION\nControllable video generation is a hot topic in research. Most of these studies focus on controllable\nvisual generation. Early research primarily emphasized image-to-video generation, using the initial\nframe image as a control to manipulate the generated video spatially Lotter et al. (2016); Srivastava\net al. (2015); Chiappa et al. (2016). However, relying solely on images as controls cannot determine\nthe subsequent frames of future videos. Consequently, there has been growing interest in text-to-\nvideo research, employing text to semantically constrain video generation Wu et al. (2021; 2022);\nHong et al. (2022); Singer et al. (2022); Ho et al. (2022). Some studies also utilize both text and\nimage conditions for more precise control over video generation Hu et al. (2022); Yin et al. (2023);\nEsser et al. (2023). Nonetheless, both language and image remain limited in expressing the temporal\ninformation of videos, such as camera movements and complex object trajectories.\nTo control temporal information of videos, trajectory-based control has emerged as a user-friendly\napproach increasingly gaining attention in research. CVG Hao et al. (2018) and C2M Ardino et al.\n(2021) encode images and trajectories, predicting optical flow maps and warp features as interme-\ndiate results for controllable video generation. However, warp operations often result in unnatural\ndistortions. To solve this issue, II2V Blattmann et al. (2021b) and iPOKE Blattmann et al. (2021a)\ncompress videos into a dense latent space and learn to manipulate these latent variables using RNN.\nSimilarly, MCDiff Chen et al. (2023) predicts future frames by diffusion latents in an auto-regressive\nway. While MCDiff has achieved promising results, it relies on HRNet Wang et al. (2021) to extract\n17 keypoints for each person to construct data, it can only control motion from humans. Addition-\nally, MCDiff and the aforementioned models neglect to consider the control of languages, which in\nturn limits their ability to control the videos effectively.\nThe aforementioned research inspired us with a two-fold vision for controllable video generation.\n1) Firstly, the current consideration of text, image, and trajectory-based controls in existing works\nis not comprehensive enough. We argue that these three types of controls are indispensable, as they\neach contribute to the regulation of video content from semantic, spatial, and temporal perspectives.\nAs depicted in Figure 1, the combination of text and images alone is insufficient to convey the\nintricate motion details present in a video, which can be supplemented by incorporating trajectory\ninformation. Furthermore, while images and trajectories may not adequately represent future objects\nin a video, language can compensate for this shortcoming. Lastly, relying solely on trajectories and\nlanguage can result in ambiguity when expressing abstract concepts, such as differentiating between\nreal-world fish and a painting of a fish, whereas images can provide the necessary distinction. 2)\nSecondly, current research on trajectory control is still in its early stages, with most experiments\nbeing conducted on simple datasets like Human3.6M. This constraint limits the models\u2019 capability\nto process open-domain images and effectively handle complex curved trajectories, multiple object\nmovements, and camera motion simultaneously.\nBased on these observations, we propose DragNUWA, an open-domain video generation model.\nTo address the issue of insufficient control granularity in existing works, we simultaneously intro-\nduce text, image, and trajectory information to provide fine-grained control over video content from\nsemantic, spatial, and temporal perspectives. To resolve the problem of limited open-domain tra-\njectory control in current research, We model trajectory with three aspects: a Trajectory Sampler\n(TS) to enable open-domain control of arbitrary trajectories, a Multiscale Fusion (MF) to control\ntrajectories in different granularities, and an Adaptive Training (AT) strategy to generate consistent\nvideos following trajectories.\nThe main contributions of our work are as follows:\n\u2022 We introduce DragNUWA, an end-to-end video generation model that seamlessly inte-\ngrates three essential controls\u2014Text, Image, and Trajectory\u2014providing strong and user-\nfriendly controllability.\n\u2022 We focus on trajectory modeling with three aspects: a Trajectory Sampler (TS) to enable\nopen-domain control of arbitrary trajectories, a Multiscale Fusion (MF) to control trajecto-\nries in different granularities, and an Adaptive Training (AT) strategy to generate consistent\nvideos following trajectories.\n\u2022 We conduct extensive experiments to validate the effectiveness of DragNUWA, demon-\nstrating its superior performance in fine-grained control in video synthesis.\n3\n*Ongoing work\n2\nRELATED WORKS\n2.1\nTEXT/IMAGE CONTROL IN VIDEO SYNTHESIS\nEarly research primarily emphasized image-to-video generation, with a common assumption that the\nenvironment is deterministic and has only one possible future Lotter et al. (2016); Srivastava et al.\n(2015); Chiappa et al. (2016). However, this assumption cannot satisfy the requirements of real-\nworld videos with unlimited possibilities. To address this issue, text-to-video generation has been\nwidely studied in recent years (GODIVA Wu et al. (2021), NUWA Wu et al. (2022), CogVideo Hong\net al. (2022), Make A Video Singer et al. (2022), Imagen Video Ho et al. (2022)), introducing text\ndescriptions to semantically control the content of video generation. However, text alone cannot ac-\ncurately describe the spatial information of visuals. Therefore, MAGE Hu et al. (2022) emphasizes\ntext-image-to-video, utilizing both semantic information from text and spatial information from im-\nages for precise video control. Similarly, GEN-1 Esser et al. (2023) integrates depth maps with texts\nusing cross-attention mechanisms for control. In the domain of long video generation, text-image-to-\nvideo has also been widely used. For example, Phenaki Villegas et al. (2022) generates subsequent\nframes by auto-regressively introducing previous frames and text, achieving long video generation.\nNUWA-XL Yin et al. (2023) employs a hierarchical diffusion architecture to continuously complete\nintermediate frames based on previous frames and text.\nWhile text and images can effectively convey semantics and appearance, they struggle to adequately\nrepresent complex motion information and camera movements. Unlike these approaches, Drag-\nNUWA adds trajectory control to text and image control, enabling fine-grained control of videos in\nterms of semantics, appearance, and motion.\n2.2\nTRAJECTORY CONTROL IN VIDEO SYNTHESIS\nTo better control motion in videos, future video prediction methods control subsequent frame gen-\neration based on given video frames Wu et al. (2020); Wichers et al. (2018); Walker et al. (2017);\nLiang et al. (2017). On the other hand, video-to-video generation transfers the style of a complete\nvideo or video sketch to a new domain, providing rich control information Chan et al. (2019); Wang\net al. (2019). However, this requires users to provide video input and restricts fine-grained control\nas the style transfer is based on the original video\u2019s skeleton. Consequently, image-trajectory-to-\nvideo methods have emerged, controlling video development through trajectories given in images.\nCVG Hao et al. (2018) and C2M Ardino et al. (2021) encode images and trajectories, predict-\ning optical flow maps and warp features as intermediate results for controllable video generation.\nHowever, warp operations often result in unnatural distortions. II2V Blattmann et al. (2021b) and\niPOKE Blattmann et al. (2021a) compress videos into a dense latent space and learn to manipulate\nthese latent variables using recurrent neural networks. However, since trajectory control operates\non the pixel level, it is sparse and prone to ambiguity. To address this issue, sparse strokes are\nfirst transformed into dense flows, and then future frames are predicted based on dense flow using\nautoregression. Nonetheless, since MCDiff Chen et al. (2023) relies on HRNet Wang et al. (2021)\nto extract 17 keypoints for each person to construct data, it can only control motion from humans.\nTo achieve control of open-domain objects, Video Composer Wang et al. (2023) very recently used\nMPEG-4 to extract motion vector information from videos as conditions for training, but due to\nthe lack of high-level semantic information in motion vectors, it could only control simple object\nmovements.\nIn comparison to previous research, which solely focused on managing human motion or rudi-\nmentary object movements, DragNUWA stands out as the pioneering approach in accomplishing\nfine-grained open-domain video generation by enabling the dragging of any objects in an image,\nfacilitating control over multiple objects, and accommodating their complex trajectories and camera\nmovements.\n3\nMETHOD\nUnlike previous works that only support either text-based Wu et al. (2021), image-based Zhang et al.\n(2020), or trajectory-based control Hu et al. (2022), DragNUWA is designed to incorporate all three\ntypes of control while emphasizing trajectory modeling from three aspects:\n4\n*Ongoing work\nDown\nDown\nDown\nDown\nMid\nUp\nUp\nUp\nUp\nPrompts: A speedy \ndrive on a curvy road \nnear lake tahoe. \nText Enc\nMultiscale Fusion (MF)\nTrajectory Sampler (TS)\nAdaptive Training (AT)\nTrajectory Enc\nImage Enc\nFigure 3: Overview of DragNUWA\u2019s Training Process. DragNUWA supports three optional inputs:\ntext p, image s, and trajectory g, and focuses on designing the trajectory from three aspects. First, the\nTrajectory Sampler (TS) dynamically samples trajectories from open-domain video flow. Second,\nMultiscale Fusion (MF) deeply integrates trajectory with text and image within each block of the\nUNet architecture. Lastly, Adaptive Training (AT) adapts the model from optical flow conditions to\nuser-friendly trajectories. Ultimately, DragNUWA is capable of handling open-domain videos with\nmultiple objects and their complex trajectories.\n\u2022 1) To enable open-domain control of arbitrary trajectories, a Trajectory Sampler (TS) (in-\ntroduced in Sec. 3.1) is employed to directly sample trajectories from open-domain video\nflows during training, as opposed to the specific domain, such as human pose trajectories\nused in MCDiff Chen et al. (2023).\n\u2022 2) To achieve control over different trajectory granularities, a Multiscale Fusion (MF) (in-\ntroduced in Sec. 3.2) is utilized to downsample the trajectory to various scales and deeply\nintegrate it with text and image within each block of the UNet architecture, rather than\ndirectly concatenating controls with diffusion noise as in Chen et al. (2023); Wang et al.\n(2023).\n\u2022 3) To generate stable and consistent videos, we adopt an Adaptive Training (AT) (intro-\nduced in Sec. 3.3) approach, initially conditioning on dense flow to stabilize video genera-\ntion and subsequently training on sparse trajectories to adapt the model.\nIn the following sections, we focus on introducing the training process from Sec. 3.1 to Sec. 3.3,\nspecifically discussing how the model calculates loss by merely using the input video and text pair,\ndenoted as < v, p >. In Sec. 3.4, we introduce the inference process, which demonstrates how the\nmodel processes the input text p, image s, and trajectory g to output the generated video v.\n3.1\nTRAJECTORY SAMPLER (TS)\nIn the training data, since it only contains video and text pairs < v, p >, it is essential to extract\ntrajectories from the videos. Previous studies primarily utilized key point tracking models to pre-\nextract video trajectories for training. However, this approach has two main drawbacks. Firstly, as\nthese models are trained on specific domains, such as human poses, their ability to handle open-\ndomain videos is limited. Secondly, in practical applications, it is difficult to ensure that users\ninput trajectories precisely at key points, resulting in a gap between training and inference. To\nfacilitate open-domain video trajectories and enable users to input arbitrary trajectories, We designed\n5\n*Ongoing work\na Trajectory Sampler (TS) that directly samples trajectories from video optical flow, allowing the\nmodel to learn various possible trajectories in an open-domain setting.\nGiven a video v \u2208 RL\u00d7C\u00d7H\u00d7W with L frames, C channels, H height, W width, we first utilize Uni-\nmatch Xu et al. (2023), an optical flow estimator, to extract dense optical flow f \u2208 R(L\u22121)\u00d7C\u00d7H\u00d7W .\nFor clarity, we represent the optical flow of the first and second frames as f0 \u2208 RC\u00d7H\u00d7W . A\nstraightforward approach is to directly sample trajectories from f0 according to the intensity of the\noptical flow. However, this would result in excessive sampling on objects with larger motions, while\nthose objects with smaller motions would have limited opportunities for learning. To handle this\nissue, we uniformly distribute anchor points with an interval of \u03bb. Moreover, to cover the entire\nimage region as much as possible, we add random perturbations \u03b4 ranging from \u2212\u03bb/2 to \u03bb/2 to the\nanchor points. Finally, we obtain a slightly sparser anchored optical flow f a\n0 in the following:\nf a\n0,i,j =\n\u001a0,\nelse\nf0,i,j,\n(i + \u03b4)%\u03bb = 0 & (j + \u03b4)%\u03bb = 0\n(1)\nTo support control over multiple trajectories, we define the maximum number of trajectories N and\nrandomly sample the number of trajectories n \u223c U[1, N]. To accommodate both large and small\nmotion objects while selecting trajectories based on flow intensity, we sample n anchor tracking\npoints from f a\n0,i,j according to the multinomial distribution M(n, ||f a\n0,i,j||2). This results in a sparser\nflow of f s\n0 containing n tracking points. Since f s\n0 only contains the tracking points from the first\nframe, to obtain the full trajectory f s, we proceed to iteratively track the trajectories by updating the\nposition of the tracking points according to the corresponding optical flow f.\nGiven that f s is highly sparse, it is not conducive for the model to learn from these trajecto-\nries.\nTherefore, we apply Gaussian Filter to f s to obtain an enhanced trajectory map f g \u2208\nR(L\u22121)\u00d7C\u00d7H\u00d7W . Compared with f s, f g improves the robustness and helps the model to better\ncapture trajectory information.\n3.2\nMULTISCALE FUSION (MF)\nEncoding of Video\nDuring training, we treat video v \u2208 RL\u00d7C\u00d7H\u00d7W as independent frames and\nencode it into x0 \u2208 RL\u00d7c\u00d7h\u00d7w using a pre-trained image autoencoder Rombach et al. (2022). It\nis important to note that the subscript 0 in x0 does not represent the first frame but indicates the\ninitial step in the diffusion process. We follow the pre-defined diffusion process q (xt|xt\u22121) =\nN\n\u0000xt; \u221a\u03b1t xt\u22121, (1 \u2212 \u03b1t) I\n\u0001\nand add noise to x0:\nxt = \u221a\u00af\u03b1t x0 +\np\n(1 \u2212 \u00af\u03b1t)\u03f5\n\u03f5 \u223c N(0, I)\n(2)\nwhere \u03f5 is noise, xt is the t-th intermediate state in diffusion process, \u03b1t, \u00af\u03b1t is hyperparameters in\ndiffusion model.\nEncoding of Text Control\nGiven the text prompts, we encode them with CLIP Radford et al.\n(2021) Text Encoder to get prompt embedding p \u2208 Rlp\u00d7cp where lp is token length, cp is prompt\nembedding dimension.\nEncoding of Image Control\nFor image control, we utilize the first frame of the video as a con-\ndition, providing general information such as appearance, style, and layout. To match the size for\nfusion, the first frame is repeated L times. Subsequently, the pre-trained image autoencoder Rom-\nbach et al. (2022) and a sequence of convolution layers are employed to independently encode each\nframe into a representation s \u2208 RL\u00d7cs\u00d7h\u00d7w.\nEncoding of Trajectory Control\nBy Trajectory Sampler introduced in Sec. 3.1, we obtain f g \u2208\nR(L\u22121)\u00d7C\u00d7H\u00d7W directly from open-domain videos v. To match the fusion size, we pad a full\nzero frame in front of f g, and encode it using a series of convolutional layers, resulting in g \u2208\nRL\u00d7cg\u00d7h\u00d7w.\nTo fuse multiple controls in different granularities, we propose Multiscale Fusion (MF), which can\nsimultaneously accept text p, image s, and trajectory g as conditions and merge them at different\nresolutions. The Multiscale Fusion will first downsample the trajectory g and image s to various\n6\n*Ongoing work\nscales g(l) and s(l), where the superscript l represents downsample depth. The trajectory and im-\nage are then integrated with text p in UNet architecture, composed of multiscale downblocks and\nupblocks with skip connection.\nFor the image condition s and trajectory condition g, they are fused into hidden state h via linear\nprojection. In the l-th block of UNet architecture, s(l), m(l) and g(l) are first transferred to scale\nw(l)\ns , w(l)\nm , w(l)\ng\nand shift b(l)\ns , b(l)\nm , b(l)\ng via zero-initialized convolution layers, where m(l) is a binary\nmask to indicate whether the frame is provided as a condition. Then, the scale w and shift b are\nfused into h via simple linear projection.\nh := w(l)\ns\n\u00b7 h + b(l)\ns + h\n(3)\nh := w(l)\nm \u00b7 h + b(l)\nm + h\n(4)\nh := w(l)\ng \u00b7 h + b(l)\ng + h\n(5)\nFor the text condition p, it is injected to hidden states h via Prompt Cross-Attention with hidden\nstates h treated as query, and text p treated as key and value.\nTo support various combinations of conditions, we introduce randomness into the training process\nby randomly omitting text, images, and trajectories before feeding them into Multiscale Fusion.\nFor the dropped text, we employ empty strings as replacements, whereas for dropped images and\ntrajectories, frames populated with all zeros are used. Through this training paradigm involving\nmixed conditions, our model is capable of generating consistent videos during inference across\ndifferent condition combinations.\n3.3\nADAPTIVE TRAINING (AT)\nSimultaneously conditioning the video generation process on both image and sparse trajectory while\nmaintaining visual consistency presents a significant challenge. To address this issue, we employ an\nAdaptive Training (AT) strategy to optimize DragNUWA.\nIn the first stage, to generate visually and dynamically consistent videos, we provide the model with\nprompt p, dense optical flow f, and the repeated first frame s as conditions, the model is optimized\nto minimize the distance between the output of the UNet \u03f5\u03b8 (xt, p, s, f) and the added noise \u03f5.\nConsidering the density of optical flow, we do not apply Gaussian filtering for enhancement.\nL\u03b8 = ||\u03f5 \u2212 \u03f5\u03b8 (xt, p, s, f)||2\n2\n(6)\nAs provided the complete optical flow f as a condition, it is much easier to generate dynamically\nconsistent videos while preserving the first frame. In the second stage, to adapt the model from com-\nplete optical flow to user-friendly trajectories, we continue training the model by sampling trajectory\nfg from the original optical flow f using Trajectory Sampler (TS).\nL\u03b8 = ||\u03f5 \u2212 \u03f5\u03b8 (xt, p, s, g)||2\n2\n(7)\nDespite the trajectory being considerably sparser than the optical flow, the model is capable of\ngenerating dynamics consistent with trajectories while maintaining stability and consistency learned\nfrom the previous training.\n3.4\nINFERENCE\nDuring inference, given the text, image, and trajectory, DragNUWA is capable of generating realistic\nand contextually consistent videos v.\nThe text is encoded by CLIP Radford et al. (2021) Text encoder to get text embedding p. The image\nis repeated L times and encoded to s. The input trajectory is first processed by Gaussian Filter and\nzero frame padding and then encoded to g. After that, x0 is iteratively sampled from a pure Gaussian\nnoise xT using the Unet \u03f5\u03b8 (xt, p, s, g). Finally, the sampled latent code x0 is decoded into video\npixels v by image autoencoder.\n7\n*Ongoing work\nVersion\nDragNUWA-LD\nDragNUWA-HD\nData\nDataset\nWebVid\nWebVid+VideoHD\nSamples\n10M\n10M+75K\nResolution (W \u00d7 H)\n320 \u00d7 192\n576 \u00d7 320\nMax Duration\n2s\n4s\nFrames (L)\n8f\n16f\nFramerate\n4 fps\nAugmentation\nRandomResizeCrop\nscale=(0.9, 1.), ratio=(5/3, 5/3)\nColorJitter\nbrightness=0.05, contrast=0.15, saturation=0.15\nRandomStartFrame\n[0, video duration-2]\n[0, video duration-4]\nTrajectory\nSampler(TS)\nMax Trajectories (N)\n8\nGaussian Kernel\nkernel size=99, sigma=10\nAnchor Interval (\u03bb)\n16\nMultiscale\nFusion(MF)\nText Control (p)\n77x1024\nImage Control (s)\n8 \u00d7 320 \u00d7 40 \u00d7 24\n8 \u00d7 320 \u00d7 20 \u00d7 12\n8 \u00d7 640 \u00d7 10 \u00d7 6\n8 \u00d7 1280 \u00d7 5 \u00d7 3\n16 \u00d7 320 \u00d7 72 \u00d7 40\n16 \u00d7 320 \u00d7 36 \u00d7 20\n16 \u00d7 640 \u00d7 18 \u00d7 10\n16 \u00d7 1280 \u00d7 9 \u00d7 5\nTrajectory Control (g)\n8 \u00d7 320 \u00d7 40 \u00d7 24\n8 \u00d7 320 \u00d7 20 \u00d7 12\n8 \u00d7 640 \u00d7 10 \u00d7 6\n8 \u00d7 1280 \u00d7 5 \u00d7 3\n16 \u00d7 320 \u00d7 72 \u00d7 40\n16 \u00d7 320 \u00d7 36 \u00d7 20\n16 \u00d7 640 \u00d7 18 \u00d7 10\n16 \u00d7 1280 \u00d7 9 \u00d7 5\nControl Drop Ratio\nText: 0.1, Image: 0.1, Trajectory: 0.1\nAdaptive\nTraining(AT)\nBatch Size\n128\nLearning Rate\n5 \u00d7 10\u22126\nScheduler\nWarmupLinear, warmup ratio=0.05\nOptimizer\nAdam\nParameters\n1.60B\nTable 1: Implementation details of DragNUWA.\n4\nEXPERIMENTS\n4.1\nDATASETS\nIn the training process, we utilize WebVid and VideoHD to optimize DragNUWA.\n\u2022 WebVid is a vast dataset Bain et al. (2021) comprising 10 million web videos encompass-\ning diverse real-world scenarios with corresponding caption. It covers a wide range of\nmotion patterns, making it suitable for open-domain trajectory-based video generation.\n\u2022 VideoHD We build VideoHD dataset based on web-crawled videos. We first collected 75K\nhigh-resolution, top-quality video clips from the internet. Subsequently, these clips are\nannotated using BLIP2 Li et al. (2023). Finally, we manually filter out some errors in the\ngenerated results.\n4.2\nIMPLEMENTATION DETAILS\nWe implement two versions of DragNUWA, namely DragNUWA-LD and DragNUWA-HD.\nDragNUWA-LD is trained on videos of 8 frames with a resolution of 320\u00d7192, while DragNUWA-\nHD is trained on 16 frames with a resolution of 576 \u00d7 320. For the Trajectory Sampler (TS), the\nmaximum number of trajectories N is 8, with anchor interval \u03bb of 16. The Gaussian kernel size is\n99, with sigma value set to 10. To support different condition combinations, we randomly omit text,\nimages, and trajectories with a probability of 0.1. We train the model using Adam optimizer Kingma\n& Ba (2014) with a batch size of 128, learning rate of 5 \u00d7 10\u22126. More implementation details can\nbe found in Tab. 1.\n8\n*Ongoing work\n4.3\nTRAJECTORY CONTROLLABILITY\nContrary to existing studies that focus on text or image control, DragNUWA primarily emphasizes\nmodeling trajectory control. In order to validate the effectiveness of trajectory control, we test\nDragNUWA from two aspects: camera movements and complex trajectories.\nCamera movements. In video production, camera movements play a significant role in creating\ndynamic and engaging visuals for the audience. Different types of camera movements can aid in\nnarrative storytelling, or emphasizing elements within a scene. Common camera movements include\nnot only horizontal and vertical movements but also zooming in and zooming out. As shown in\nFig. 4, we discovered that although DragNUWA does not explicitly model camera movements, it\nlearns various camera movements from the modeling of open-domain trajectories.\nThe great wall of China\nInput Image :\nInput Text :\nInput Trajectory\n:\nOutput Video :\nFigure 4: Various camera movement effects can be achieved by utilizing identical text and images\nwhile altering the dragging trajectories. For instance, zoom-in and zoom-out effects can be expressed\nby drawing the directional trajectories at the desired zoom locations.\n9\n*Ongoing work\nComplex Trajectories. Motion modeling in video generation presents challenges due to the pres-\nence of multiple moving objects, intricate motion trajectories, and varying motion amplitudes among\ndifferent objects. To evaluate the capability of DragNUWA in accurately modeling complex motion,\nwe conducted tests on various intricate drag trajectories using the same image and text, as depicted\nin Fig. 5. Our findings indicate that DragNUWA can reliably control complex motions. This en-\ncompasses several aspects: firstly, DragNUWA supports complex curved trajectories, enabling the\ngeneration of objects moving along the specific intricate trajectory (see Row 6). Secondly, Drag-\nNUWA allows for variable trajectory lengths, with longer trajectories resulting in larger motion\namplitudes (see Row 7-8). Lastly, DragNUWA has the capability to simultaneously control the tra-\njectories of multiple objects. To the best of our knowledge, no existing video generation model has\neffectively achieved such trajectory controllability, highlighting DragNUWA\u2019s substantial potential\nto advance controllable video generation in future applications.\nTwo boys skateboarding \non the ramp\nInput Image :\nInput Text :\nInput Trajectory\n:\nOutput Video :\nFigure 5: Various complex trajectory effects can be achieved by employing the same text and image\nwhile altering the dragging trajectory. DragNUWA supports complex curved trajectories, allows for\nvariable trajectory lengths, and supports concurrent control of trajectories for multiple objects.\n10\n*Ongoing work\n4.4\nESSENTIAL OF THREE CONTROLS\nA man \nsurfing \non snow\nInput \nImage :\nInput \nTrajectory\n:\nInput\nText :\n\u00d7\nA man \nsurfing \non snow\nA man \nsurfing \non snow\nOutput \nVideo :\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\nFigure 6: DragNUWA achieves fine-grained video generation by integrating three essential controls:\ntext, image, and trajectory, corresponding to semantic, spatial, and temporal aspects, respectively.\nWhile DragNUWA primarily emphasizes trajectory control modeling, it also incorporates the control\nof text and images. We believe that text, image, and trajectory each represent one of the three\nfundamental control aspects of videos: semantic, spatial, and temporal perspectives. Fig. 6 illustrates\nthe necessity of these conditions by showcasing different combinations of text (p), trajectory (g),\nand image (s), including s2v, p2v, gs2v, ps2v, and pgs2v. It is important to note that we did not\nmodel g2v and pg2v, as we believe that trajectories without images are meaningless.\nThe s2v and p2v exemplify the constraints of image and text control when utilized as an individual\ncondition. As shown in s2v, although an image alone provides some potential semantic and kinetic\ninformation, it does not allow for precise control over the background and the character\u2019s movement.\nAs illustrated in p2v, when only text is provided, the model successfully generates a video related to\nthe text, however, the appearance and dynamics remain entirely uncontrollable. The gs2v and ps2v\nemphasize the importance of text (p) and trajectory (g). In the absence of text, it is impossible to\ndetermine whether the ambiguous image (s) represents surfing on the sea or snow. In the absence\nof trajectory, the model automatically assumes that the character is moving to the left. The pgs2v\ndemonstrates the combined impact of all three essential conditions, enabling the control of surfing\non the snow and moving to the right.\nIt is worth mentioning that some studies incorporate video as a condition, which is beyond the\nscope of this research. We focus on the fundamental conditions, while the video condition provides\nexcessive information, significantly constraining the creation of videos and primarily serving for\nstyle transfer purposes. Moreover, the video condition necessitates users to provide specific video\nmaterials, consequently presenting significant challenges in practical application.\n5\nCONCLUSION\nWe present DragNUWA, an end-to-end video generation model that seamlessly incorporates text,\nimage, and trajectory input, enabling fine-grained and user-friendly control from semantic, spatial,\nand temporal perspectives. Additionally, our trajectory modeling framework, consisting of the Tra-\njectory Sampler (TS), Multiscale Fusion (MF), and Adaptive Training (AT), tackles challenges in\nopen-domain trajectory control, thereby enabling the generation of coherent videos in accordance\nwith complex trajectories. Experiments validate DragNUWA\u2019s superiority over existing approaches,\ndemonstrating its ability to generate fine-grained videos effectively.\n11\n*Ongoing work\nREFERENCES\nPierfrancesco Ardino, Marco De Nadai, Bruno Lepri, Elisa Ricci, and St\u00b4ephane Lathuili`ere. Click\nTo Move: Controlling Video Generation With Sparse Motion. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pp. 14749\u201314758, 2021.\nMax Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisserman. Frozen in time: A joint video and\nimage encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference\non Computer Vision, pp. 1728\u20131738, 2021.\nAndreas Blattmann, Timo Milbich, Michael Dorkenwald, and Bj\u00a8orn Ommer. Ipoke: Poking a still\nimage for controlled stochastic video synthesis. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pp. 14707\u201314717, 2021a.\nAndreas Blattmann, Timo Milbich, Michael Dorkenwald, and Bjorn Ommer. Understanding Object\nDynamics for Interactive Image-to-Video Synthesis. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 5171\u20135181, 2021b.\nCaroline Chan, Shiry Ginosar, Tinghui Zhou, and Alexei A. Efros. Everybody Dance Now. In\nProceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5933\u20135942,\n2019.\nTsai-Shien Chen, Chieh Hubert Lin, Hung-Yu Tseng, Tsung-Yi Lin, and Ming-Hsuan Yang.\nMotion-Conditioned Diffusion Model for Controllable Video Synthesis.\narXiv preprint\narXiv:2304.14404, 2023.\nSilvia Chiappa, S\u00b4ebastien Racaniere, Daan Wierstra, and Shakir Mohamed. Recurrent Environment\nSimulators. In International Conference on Learning Representations, November 2016.\nPatrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Ger-\nmanidis. Structure and content-guided video synthesis with diffusion models. arXiv preprint\narXiv:2302.03011, 2023.\nZekun Hao, Xun Huang, and Serge Belongie. Controllable Video Generation With Sparse Trajec-\ntories. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.\n7854\u20137863, 2018.\nJonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P.\nKingma, Ben Poole, Mohammad Norouzi, and David J. Fleet. Imagen video: High \u02dcvideo gener-\nation with diffusion models. arXiv preprint arXiv:2210.02303, 2022.\nWenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. CogVideo: Large-scale Pre-\ntraining for Text-to-Video Generation via Transformers. arXiv preprint arXiv:2205.15868, 2022.\nYaosi Hu, Chong Luo, and Zhenzhong Chen. Make It Move: Controllable Image-to-Video Gener-\nation With Text Descriptions. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp. 18219\u201318228, 2022.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBLIP-2: Bootstrapping Language-\nImage Pre-training with Frozen Image Encoders and Large Language Models. arXiv preprint\narXiv:2301.12597, 2023.\nXiaodan Liang, Lisa Lee, Wei Dai, and Eric P. Xing. Dual Motion GAN for Future-Flow Embedded\nVideo Prediction. In Proceedings of the IEEE International Conference on Computer Vision, pp.\n1744\u20131752, 2017.\nWilliam Lotter, Gabriel Kreiman, and David Cox. Deep Predictive Coding Networks for Video\nPrediction and Unsupervised Learning. In International Conference on Learning Representations,\nNovember 2016.\n12\n*Ongoing work\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, and Jack Clark. Learning transferable visual\nmodels from natural language supervision. In International Conference on Machine Learning,\npp. 8748\u20138763. PMLR, 2021.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nResolution Image Synthesis With Latent Diffusion Models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 10684\u201310695, 2022.\nUriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry\nYang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-A-Video:\nText-to-Video Generation without Text-Video Data, September 2022.\nNitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. Unsupervised Learning of Video\nRepresentations using LSTMs. In Proceedings of the 32nd International Conference on Machine\nLearning, pp. 843\u2013852. PMLR, June 2015.\nRuben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang,\nMohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable\nLength Video Generation from Open Domain Textual Descriptions. In ICLR, September 2022.\nJacob Walker, Kenneth Marino, Abhinav Gupta, and Martial Hebert.\nThe Pose Knows: Video\nForecasting by Generating Pose Futures. In Proceedings of the IEEE International Conference\non Computer Vision, pp. 3332\u20133341, 2017.\nJingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu,\nYadong Mu, Mingkui Tan, Xinggang Wang, Wenyu Liu, and Bin Xiao. Deep High-Resolution\nRepresentation Learning for Visual Recognition. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 43(10):3349\u20133364, October 2021. ISSN 1939-3539. doi: 10.1109/TPAMI.\n2020.2983686.\nTing-Chun Wang, Ming-Yu Liu, Andrew Tao, Guilin Liu, Jan Kautz, and Bryan Catanzaro. Few-\nshot video-to-video synthesis. In Proceedings of the 33rd International Conference on Neural\nInformation Processing Systems, pp. 5013\u20135024, Red Hook, NY, USA, December 2019. Curran\nAssociates Inc.\nXiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen,\nDeli Zhao, and Jingren Zhou. VideoComposer: Compositional Video Synthesis with Motion\nControllability, June 2023.\nNevan Wichers, Ruben Villegas, Dumitru Erhan, and Honglak Lee. Hierarchical Long-term Video\nPrediction without Supervision. In Proceedings of the 35th International Conference on Machine\nLearning, pp. 6038\u20136046. PMLR, July 2018.\nChenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, and Nan\nDuan. GODIVA: Generating Open-DomaIn Videos from nAtural Descriptions. arXiv:2104.14806\n[cs], April 2021.\nChenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, and Nan Duan. N\\\u201dUWA:\nVisual Synthesis Pre-training for Neural visUal World creAtion. In Proceedings of the European\nConference on Computer Vision (ECCV), 2022.\nYue Wu, Rongrong Gao, Jaesik Park, and Qifeng Chen. Future Video Synthesis With Object Mo-\ntion Prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 5539\u20135548, 2020.\nHaofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, Fisher Yu, Dacheng Tao, and Andreas\nGeiger. Unifying flow, stereo and depth estimation. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 2023.\nShengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, Zhengyuan\nYang, Linjie Li, Shuguang Liu, and Fan Yang. NUWA-XL: Diffusion over Diffusion for eX-\ntremely Long Video Generation. arXiv preprint arXiv:2303.12346, 2023.\n13\n*Ongoing work\nJiangning Zhang, Chao Xu, Liang Liu, Mengmeng Wang, Xia Wu, Yong Liu, and Yunliang Jiang.\nDTVNet: Dynamic Time-Lapse Video Generation via Single Still Image. In Andrea Vedaldi,\nHorst Bischof, Thomas Brox, and Jan-Michael Frahm (eds.), Computer Vision \u2013 ECCV 2020,\nLecture Notes in Computer Science, pp. 300\u2013315, Cham, 2020. Springer International Publishing.\nISBN 978-3-030-58558-7. doi: 10.1007/978-3-030-58558-7 18.\n14\n"
  },
  {
    "title": "Distilled Feature Fields Enable Few-Shot Language-Guided Manipulation",
    "link": "https://arxiv.org/pdf/2308.07931.pdf",
    "upvote": "5",
    "text": "Distilled Feature Fields Enable Few-Shot\nLanguage-Guided Manipulation\nWilliam Shen\u22171, Ge Yang\u22171,2, Alan Yu1, Jansen Wong1,\nLeslie Pack Kaelbling1, Phillip Isola1\n1MIT CSAIL, 2Institute for Artificial Intelligence and Fundamental Interactions\nAbstract: Self-supervised and language-supervised image models contain rich\nknowledge of the world that is important for generalization. Many robotic tasks,\nhowever, require a detailed understanding of 3D geometry, which is often lacking\nin 2D image features. This work bridges this 2D-to-3D gap for robotic manip-\nulation by leveraging distilled feature fields to combine accurate 3D geometry\nwith rich semantics from 2D foundation models. We present a few-shot learning\nmethod for 6-DOF grasping and placing that harnesses these strong spatial and\nsemantic priors to achieve in-the-wild generalization to unseen objects. Using fea-\ntures distilled from a vision-language model, CLIP, we present a way to designate\nnovel objects for manipulation via free-text natural language, and demonstrate its\nability to generalize to unseen expressions and novel categories of objects. Project\nwebsite: https://f3rm.csail.mit.edu\n1\nIntroduction\nWhat form of scene representation would facilitate open-set generalization for robotic manipulation\nsystems? Consider a warehouse robot trying to fulfill an order by picking up an item from cluttered\nstorage bins filled with other objects. The robot is given a product manifest, which contains the text\ndescription it needs to identify the correct item. In scenarios like this, geometry plays an equally\nimportant role as semantics, as the robot needs to comprehend which parts of the object geometry\nafford a stable grasp. Undertaking such tasks in unpredictable environments \u2014 where items from\na diverse set can deviate markedly from the training data, and can be hidden or jumbled amidst\nclutter \u2014 underscores the critical need for robust priors in both spatial and semantic understanding.\nIn this paper, we study few-shot and language-guided manipulation, where a robot is expected to pick\nup novel objects given a few grasping demonstrations or text descriptions without having previously\nseen a similar item. Toward this goal, we build our system around pre-trained image embeddings,\nwhich have emerged as a reliable way to learn commonsense priors from internet-scale data [1, 2, 3].\nFigure 1 illustrates how our system works. The robot first scans a tabletop scene by taking a sequence\nof photos using an RGB camera mounted on a selfie stick (Figure 1, left). These photos are used\nto construct a neural radiance field (NeRF) of the tabletop, which, crucially, is trained to render\nnot just RGB colors but also image features from a pre-trained vision foundation model [4, 1]. This\nproduces a scene representation, called a Distilled Feature Field (DFF), that embeds knowledge from\n2D feature maps into a 3D volume (Figure 1, middle). The robot then references demonstrations\nand language instructions to grasp objects specified by a user (Figure 1, right).\nDistilled Feature Fields (DFFs) were introduced in computer graphics for tasks such as decomposing\nand editing images [5, 6]. The main contribution of this work is to study the use of DFFs instead\nfor robotic manipulation. We evaluate the robot\u2019s ability to generalize using features sourced from\nself-supervised vision transformers (DINO ViT, see [4]). These features have been shown to be\neffective out-of-the-box visual descriptors for dense correspondence [7]. We also source features\n*Equal contribution. Correspondence to {willshen,geyang}@csail.mit.edu\n7th Conference on Robot Learning (CoRL 2023), Atlanta, USA.\narXiv:2308.07931v2  [cs.CV]  30 Dec 2023\n1.Scan Scene\n2. Distill Features\nExtract Dense\n2D Features\nCofee Mug\nBaymax\n3. Language-Guided Manipulation\n3D Feature Field\nFigure 1: Distilled Feature Fields Enable Open-Ended Manipulation. (1) Robot uses a selfie\nstick to scan RGB images of the scene (camera frustums shown). (2) Extract patch-level dense\nfeatures for the images from a 2D foundation model, and distill them into a feature field (PCA\nshown) along with modeling a NeRF. (3) We can query CLIP feature fields with language to generate\nheatmaps and infer 6-DOF grasps on novel objects given only ten demonstrations.\nfrom a vision-language model, CLIP [1], which is a strong zero-shot learner on various vision and\nvisual question-answering tasks.\nOne challenge that makes distilled feature fields unwieldy for robotics is the long time it takes to\nmodel each scene. To address this, we build upon the latest NeRF techniques, and employ hierarchi-\ncal hashgrids to significantly reduce the modeling time [8, 9, 10]. When it comes to vision-language\nfeatures, CLIP is trained to produce image-level features, whereas 3D feature distillation requires\ndense 2D descriptors. Our solution is to use the MaskCLIP [11] reparameterization trick, which\nextracts dense patch-level features from CLIP while preserving alignment with the language stream.\nWe demonstrate that Distilled Feature Fields enable open-ended scene understanding and can be\nleveraged by robots for 6-DOF object manipulation. We call this approach Feature Fields for Robotic\nManipulation (F3RM). We present few-shot learning experiments on grasping and placing tasks,\nwhere our robot is able to handle open-set generalization to objects that differ significantly in shape,\nappearance, materials, and poses. We also present language-guided manipulation experiments where\nour robot grasps or places objects in response to free-text natural language commands. By taking\nadvantage of the rich visual and language priors within 2D foundation models, our robot generalizes\nto new categories of objects that were not seen among the four categories used in the demonstrations.\n2\nProblem Formulation\nWe consider the class of manipulation problems that can be parameterized via a single rigid-body\ntransformation T \u2208 SE(3), and focus on grasping and placing tasks. We parameterize a 6-DOF\ngrasp or place pose as T = (R, t) in the world frame (see Figure 2), where R is the rotation matrix,\nand t is the translation vector. In each scene, the robot is given a set of RGB images {I} with their\ncorresponding camera poses.\nFew-Shot Manipulation.\nWe aim to build robots that can manipulate objects given only a few\ndemonstrations of a task, such as grasping a mug by its handle. During learning, each demonstration\nD consists of the tuple \u27e8{I}, T\u2217\u27e9, where {I}N\ni=1 are N RGB camera views of the scene and T\u2217 is a\npose that accomplishes the desired task. During testing, the robot is given multiple images {I\u2032} of a\nnew scene which may contain distractor objects and clutter. The robot\u2019s goal is to predict a pose T\nthat achieves the task. We want to test for open-ended generalization: the new scene contains related\nbut previously unseen objects that differ from the demo objects in shape, size, pose, and material.\n2\nOpen-Text Language-Guided Manipulation.\nWe extend few-shot manipulation to include open-\ntext conditioning via natural language. Given a text description of an object, the robot\u2019s objective\nis to grasp the objects that match this description. The robot has access to a few demonstrations\nfor multiple object categories. During testing, the user provides the robot with a text query L+ to\nspecify which object to manipulate and negative texts L\u2212 to reject distractors. In practice, L\u2212 can\nbe sampled automatically. The object category we care about at test time does not necessarily appear\nin the demonstrations. We explicitly seek open-ended generalization to new object categories, given\njust a few demonstrations limited to a small class of object categories.\n3\nFeature Fields for Robotic Manipulation (F3RM)\nWe present Feature Fields for Robotic Manipulation (F3RM), our approach for distilling pre-trained\nrepresentations from vision and vision-language models into 3D feature fields for open-ended\nrobotic manipulation. Doing so involves solving three separate problems. First, how to produce\nthe feature field of a scene automatically at a reasonable speed; second, how to represent and infer\n6-DOF grasping and placing poses; and finally, how to incorporate language guidance to enable\nopen-text commands. We include a formal summary of NeRFs [12] in Appendix A.1.\n3.1\nFeature Field Distillation\nDistilled Feature Fields (DFFs) [5, 6] extend NeRFs by including an additional output to reconstruct\ndense 2D features from a vision model fvis. The feature field f is a function that maps a 3D position x\nto a feature vector f(x). We assume that f does not depend on the viewing direction d.1 Supervision\nof this feature field f is provided through the rendered 2D feature maps, where each feature vector\nis given by the feature rendering integral between the near and far plane (tn and tf):\nF(r) =\nZ tf\ntn\nT(t)\u03c3(rt)f(rt) dt\nwith T(t) = exp\n\u0012\n\u2212\nZ t\ntn\n\u03c3(rs) ds\n\u0013\n,\n(1)\nwhere r is the camera ray corresponding to a particular pixel, t is the distance along the ray, \u03c3 is the\ndensity field from NeRF, and T(t) represents the accumulated transmission from the near plane tn\nto t. Observe this is similar to the volume rendering integral (Eq. 5).\nFeature Distillation.\nWe begin with a set of N 2D feature maps {If\ni }N\ni=1, where If = fvis(I) for\neach RGB image I. We optimize f by minimizing the quadratic loss Lfeat = P\nr\u2208R\n\r\r\u02c6F(r)\u2212If(r)\n\r\r2\n2,\nwhere If(r) is the target feature vector from If, and \u02c6F(r) is estimated by a discrete approximation\nof the feature rendering integral in Eq. 1.\nExtracting Dense Visual Features from CLIP.\nThe common practice for extracting dense fea-\ntures from ViTs is to use the key/value embeddings from the last layer before pooling [4]. While\nthis approach has yielded great results for vision-only models on image segmentation and image-\nto-image correspondence tasks [7, 15], it removes the transformations needed to project the visual\nfeatures into the shared feature space with the language stream in vision-language models such as\nCLIP [1, 16]. Re-aligning the dense features typically requires additional training, which negatively\naffects the model\u2019s open-text generalization.\nRather than using CLIP as an image-level feature (Alg. 1, App. A.2), we extract dense features from\nCLIP using the MaskCLIP reparameterization trick (Alg. 2) [11]. These features retain a sufficient\nalignment with the language embedding to support zero-shot language guidance in our experiments\n(see Fig.7). We present pseudocode for this technique in Appendix A.2. A second modification we\napply is to interpolate the position encoding (see 4) to accommodate larger images with arbitrary\naspect ratios. This is needed because CLIP uses a small, fixed number of input patches from a\nsquare crop. These two techniques combined enable us to extract dense, high-resolution patch-level\n2D features from RGB images at about 25 frames per second and does not require fine-tuning CLIP.\n1Prior work that uses the association of image pixels in 3D to supervise learning 2D image features makes\nthe same assumption [13, 14].\n3\nTask Embedding\nDemo Embeddings\n(c) Average Over n Demos\nFeatures at Query Points\n(b) Sample Feature Vectors\nQuery Points\n6 DOF Gripper Pose \n \n(a) Collect Demonstrations in VR\nconcat\n =\nExample Object\nFeature Field\nz1\nz2\nZM\nFigure 2: Representing 6-DOF Poses. (a) Recording the gripper pose T\u2217 in virtual reality (VR) on\nan example mug. (b) We approximate the continuous local field via a fixed set of query points in\nthe gripper\u2019s canonical frame. (c) We concatenate feature vectors at these query points, then average\nover n (we use n = 2) demonstrations. This gives a task embedding ZM for the task M.\n3.2\nRepresenting 6-DOF Poses with Feature Fields\nWe wish to represent the pose of the gripper in a demonstration by the local 3D feature field in the\ngripper\u2019s coordinate frame. We approximate this local context via a discrete set of query points and\nthe feature vectors measured at each point. We sample a fixed set of Nq query points X = {x \u2208\nR3}Nq in the canonical gripper frame for each task M from a 3D Gaussian. We adjust its mean and\nvariance manually to cover parts of the object we intend to target, as well as important context cues\n(e.g., body of the mug when grasping the handle) and free-space (Fig.2b). For a 6-DOF gripper pose\nT, we sample the feature field f at each point in the query point cloud, transformed by T (Fig.2b).\nTo account for the occupancy given by the local geometry, we weigh the features by their corre-\nsponding alpha values from the density field \u03c3 of the NeRF model, integrated over the voxel. At a\npoint x in the world frame, this produces the \u03b1-weighted features\nf\u03b1(x) = \u03b1(x) \u00b7 f(x), where \u03b1(x) = 1 \u2212 exp(\u2212\u03c3(x) \u00b7 \u03b4) \u2208 (0, 1),\n(2)\nand \u03b4 is the distance between adjacent samples. We sample a set of features {f\u03b1(x) | x \u2208 TX}\nusing the transformed query points TX, and concatenate along the feature-dimension into a vector,\nzT \u2208 RNq\u00b7|f|. The query points X and demo embedding zT thus jointly encode the demo pose T.\nWe specify each manipulation task M by a set of demonstrations {D}. We average zT over the\ndemos for the same task to obtain a task embedding ZM \u2208 RNq\u00b7|f| (Fig. 2c). This allows us to\nreject spurious features and focus on relevant parts of the feature space. This representation scheme\nis similar to the one used in Neural Descriptor Fields [17]. The main distinction is that NDF is\ntrained from scratch on object point clouds, whereas our feature field is sourced from 2D foundation\nmodels that are trained over internet-scale datasets. The capabilities that emerge at this scale hold the\npotential for open-ended generalization beyond the few examples that appear in the demonstrations.\nInferring 6-DOF Poses.\nOur inference procedure involves a coarse pre-filtering step for the trans-\nlational DOFs, and an optimization-based fine-tuning step for the rotational DOFs. First, we sample\na dense voxel grid over the workspace, where each voxel v has a grid-size \u03b4. We remove free space\nby rejecting voxels with alphas \u03b1(v) < \u03f5free. We then remove voxels that are irrelevant to the task,\nusing the cosine similarity between the voxel feature f\u03b1(v) and the task embedding ZM. To get the\ncomplete 6-DOF poses T = {T}, we uniformly sample Nr rotations for each remaining voxel v.\nPose Optimization.\nWe optimize the initial poses with the following cost function\nJpose(T) = \u2212 cos(zT, ZM)\n(3)\nusing the Adam optimizer [18] to search for poses that have the highest similarity to the task embed-\nding ZM. After each optimization step, we prune poses that have the highest costs. We also reject\nposes that are in collision by thresholding the number of overlapping voxels between a voxelized\ngripper model and the scene geometry. This leaves us with a ranked list of poses that we feed into\na motion planner in PyBullet [19, 20]. We execute the highest-ranked grasp or place pose that has a\n4\nPick up the Bowl\nUser\nText Features from \u001f\u001e\u001d\u001c\narg max\nd\n=\nSelected Demo\n(a) Retrieving Demonstrations\nFeatures at Query Points\nminimize\nSelected\nDemo\nText Features\nfrom CLIP\nPick up the Bowl\nUser\nAverage over\n(b) Language-Guided Pose Optimization\nFigure 3: Pipeline for Language-Guided Manipulation. (a) Encode the language query with CLIP,\nand compare its similarity to the average query point features over a set of demos. The mug lip\ndemos have the highest similarity to \u201cPick up the Bowl\u201d. (b) Generate and optimize grasp proposals\nusing the CLIP feature field by minimizing Jlang. We use the selected demo from (a) in Jpose, and\ncompute the language-guidance weight with the text features and average query point features.\nvalid motion plan. Observe that our scheme operates over the entire feature field, and does not rely\non assumptions about objectness such as segmentation masks or object poses.\n3.3\nOpen-Text Language-Guided Manipulation\nNatural language offers a way to extend robotic manipulation to an open-set of objects, serving\nas an attractive alternative when photos of the target object are inaccurate or unavailable. In our\nlanguage-guided few-shot manipulation pipeline, the learning procedure and the representation for\nthe demonstrations remain consistent with Section 3.2. At test time, the robot receives open-text\nlanguage queries from the user that specify the object of interest to manipulate. Our language-guided\npose inference procedure comprises three steps (see Fig.3): (i) retrieving relevant demonstrations,\n(ii) initializing coarse grasps, and (iii) language-guided grasp pose optimization.\nRetrieving Relevant Demonstrations.\nWe select the two demonstrations whose average feature\nFd (averaged among the query points of each demo pose T\u2217) is closest to the text embedding\nq = embCLIP(L+) (Fig.3a). We found that using the positive query text (L+) alone is sufficient.\nThis means finding the demonstration that maximizes the cosine similarity cos(q, Fd). Note that\nthe objects used in the demonstrations do not have to come from the same category as the target\nobject. For instance, asking the robot to pick up the \u201cmeasuring beaker\u201d or \u201cbowl\u201d leads to the robot\nchoosing the demonstration of picking up a mug by its lip (Fig.4).\nInitializing Grasp Proposals.\nWe speed up grasp pose inference by first running a coarse proposal\nstep where we filter out regions in the feature field that are irrelevant to the text query. We start by\nsampling a dense voxel grid among the occupied regions by masking out free space (see Sec.3.2).\nAfterward, we prune down the number of voxels by keeping those more similar to the positive query\nL+ than any one of the negative queries L\u2212. Formally, let q\u2212\ni = embCLIP(L\u2212\ni ) | i \u2208 {1, . . . , n}\nbe the text embeddings of the negative queries. We compute the softmax over the pair-wise cosine\nsimilarity between the voxel\u2019s feature f\u03b1(v) and the ensemble [q, q\u2212\n1 , q\u2212\n2 , . . . , q\u2212\nn ], and identify the\nclosest negative query q\u2212. We remove voxels that are closer to q\u2212 than the positive query q. The\ncosine similarity between the voxel embedding and [q, q\u2212] pair forms a binomial distribution that\nallows us to reject voxels that have \u2265 50% probability of being associated with the negative query.\nFinally, to get the set of initial poses T = {T}, we sample Nr rotations for each remaining voxel.\nLanguage-Guided Grasp Pose Optimization.\nTo incorporate language guidance, we first com-\npute Jpose from Eq.3 using the two demonstrations retrieved in the first step. We then assign a lower\ncost to regions that are more similar to the language query q by computing a language-guidance\nweight Cq = mean\nx\u2208TX[q \u2297 f\u03b1(x)], and multiply it with Jpose (Fig.3b)\nJlang(T) = mean\nx\u2208TX\nh\nq \u2297 f\u03b1(x)\ni\n\u00b7 Jpose(T).\n(4)\n5\n(a) Grasp mug (lip)\n(b) Grasp screwdriver\n(c) Grasp caterpillar\n(d) Place cup on rack\nFigure 4: Five Grasping and Place Tasks. (a)\ngrasping a mug by its lip or handle (Fig.2); (b) a\nscrewdriver by the handle; (c) the caterpillar by\nits ears; and (d) placing a cup onto a drying rack.\nGripper poses indicate one of two demonstrations.\n(a) Top 10 Grasps\n(b) Robot Execution\n(c) Top 10 Grasps\n(d) Robot Execution.\nFigure 5: Generalizing to Novel Objects.\n(Top Row) Mug is much bigger than the ones\nused for demonstration. (Bottom Row) This\nrack has shorter pegs with a square cross-\nsection. Demo rack is cylindrical (cf. Fig.4d).\nThe first term, Cq, is the normalized cosine similarity between the text embedding q and the average\n\u03b1-weighted query point feature for a pose T. We iteratively update the pose T via gradient descent\nwhile pruning using the procedure from Section 3.2 till convergence.\n4\nResults\n4.1\nLearning to Grasp from Demonstrations\nWe consider five 6-DOF grasping and placing tasks and provide two demonstrations per task (Fig.4).\nTo label the demonstrations, we load a NeRF-reconstructed point cloud into virtual reality, and use\na hand controller to move the gripper to the desired pose (Fig.2a). We compare the performance of\nthree types of distilled features: (1) DINO ViT, (2) CLIP ViT, and (3) CLIP ResNet. We consider\nthree baselines, including (1) using density \u03c3 from the NeRF, (2) the intermediate NeRF features,\nand (3) the RGB color value as features, and compare against MIRA [21], a recent work which uses\nNeRFs to render orthographic viewpoints for pixel-wise affordance predictions. For each task, we\nevaluate in ten scenes that contain novel objects in arbitrary poses and distractor objects. The novel\nobjects belong to the same or related object category as the demo objects, but differ in shape, size,\nmaterial and appearance. We reset the scenes to about the same configuration for each compared\nmethod. We include the full details on the experimental setup in Appendix A.4.\nWe present the success rates in Table 1 and examples of robot executions in Figure 5. While the\nbaselines using density, RGB color values, or intermediate features from NeRF achieve respectable\nperformance, they struggle to identify the semantic category of the objects we care about, especially\nin complex scenes with more distractors. We find that DINO and CLIP feature fields exhibit im-\npressive generalization capabilities and have complementary advantages. The DINO ViT has a good\npart-level understanding of object geometry with 7/19 failure cases caused by inaccuracies in the\ngrasp rotations and occasionally, the translations. In comparison, 21/27 failures for CLIP ViT and\nResNet combined may be attributed to this issue. We find that CLIP favors semantic and categorical\ninformation over geometric features, which are essential for grasping and placing objects. DINO, on\nthe other hand, struggles with distinguishing target objects from distractor objects that contain sim-\nilar visual appearance to the objects used in the demonstrations. CLIP struggles less in this regard.\nThe fusion between semantic features and detailed 3D geometry offers a way to model multiple\nobjects piled tightly together: in Figure 6b, for instance, a caterpillar toy is buried under other toys.\nFigure 6c shows our robot grasping the caterpillar, and dragging it from the bottom of the pile.\n6\n(a) Demonstration (1 of 2)\n(b) Feature Field of Cluttered Scene\n(c) Robot Execution\nFigure 6: Grasping in a Cluttered Scene. (a) Demonstration for grasping the caterpillar in its\nDINO feature field (color is PCA, red dots show query points). (b) A cluttered scene with several\ntoys on top of each other. Inset shows the top 10 inferred grasps. Observe the caterpillar\u2019s ears share\nthe same features with the demo. (c) Robot successfully grasps the caterpillar.\nMug\nMug\nCaterpillar\nScrewdriver\nCup\nTotal\nlip\nhandle\near\nhandle\non rack\nMIRA [21]\n1/10\n2/10\n6/10\n3/10\n3/10\n15/50\nDensity\n5/10\n5/10\n10/10\n2/10\n5/10\n27/50\nIntermediate\n2/10\n2/10\n1/10\n3/10\n1/10\n9/50\nRGB\n4/10\n3/10\n9/10\n1/10\n4/10\n21/50\nDINO ViT\n5/10\n4/10\n8/10\n6/10\n8/10\n31/50\nCLIP ViT\n7/10\n7/10\n8/10\n6/10\n6/10\n34/50\nCLIP ResNet\n9/10\n6/10\n9/10\n8/10\n7/10\n39/50\nTable 1: Success rates on grasping and placing tasks. We compare\nthe success rates over ten evaluation scenes given two demonstrations\nfor each task. We consider a run successful if the robot grasps or\nplaces the correct corresponding object part for the task.\nColor\n7/10\nMaterial\n7/10\nRelational\n4/10\nGeneral\n4/10\nOOD\n9/10\nTotal\n31/50\nTable 2:\nSuccess rates\nof Language-Guided Ma-\nnipulation.\nLanguage\nquery success rates across\nsemantic categories.\n4.2\nLanguage-Guided Object Manipulation\nWe set up 13 table-top scenes to study the feasibility of using open-text language and CLIP feature\nfields for designating objects to manipulate. We reuse the ten demonstrations from the previous\nsection (Sec. 4.1), which span four object categories (see Fig.4). We include three types of objects\nin our test scenes: (1) novel objects from the same categories as the demonstrations, (2) out-of-\ndistribution (OOD) objects from new categories that share similar geometry as the demonstrated\nitems (e.g., bowls, measuring beakers, utensils), and (3) distractor items that we desire the system\nignore. Success metric: we consider a language query successful if the robot stably grasps the target\nobject and places it in a plastic bin at a known location. We include more details in Appendix A.7.\nWe break down the success rates by category in Table 2, and show the robot\u2019s execution sequence for\nan example scene in Figure 7 (video). This scene contained eleven objects, four were sourced from\nthe YCB object dataset (the apple, black marker, mango, and a can of SPAM), the rest collected from\nthe lab and bought online. We present five successful grasps (Figure 7). The robot failed to grasp\nthe stainless steel jug by its handle due to a small error in the grasp rotation. This is a typical failure\ncase \u2014 six out of 19 failures stem from these poor grasp predictions with rotational or translational\nerrors. The remaining 13/19 failed grasps are due to CLIP features behaving like a bag-of-words\nand struggling to capture relationships, attributes, and ordinal information within sentences [22].\nFor instance, in queries such as \u201cblack screwdriver\u201d and \u201cmug on a can of spam,\u201d CLIP paid more\nattention to other black objects and the can of spam, respectively. We found that retrieving demos\nvia text occasionally (particularly in the rack scene) benefits from prompt engineering.\nIn total, our robot succeeds in 31 out of 50 language queries, including both fairly general queries\n(e.g., mug, drying rack) and ones that specify properties, such as color, material, and spatial rela-\ntions (e.g., screwdriver on the block). Notably, our robot generalizes to out-of-distribution object\ncategories including bowls, rolls of tape, whiteboard markers and utensils using demonstrations\nonly on mugs and screwdrivers. Although this success rate is far from practical for industrial use,\nour overall strategy of using 2D visual priors for 3D scene understanding can leverage the rapid\nadvancements in VLMs, which hold significant potential for improving performance.\n7\nFigure 7: Language-Guided Manipulation Execution. (Top Row) Heatmaps given the language\nqueries. (Bottom Row) Robot executing grasps sequentially without rescanning. CLIP can behave\nlike a bag-of-words, as shown by the bleed to the blue bowl for \u201cblue screwdriver.\u201d\n5\nRelated Work\nOpen-Ended Generalization via Language.\nA number of prior work use natural language for\ntask-specification and long-horizon planning in applications such as tabletop and mobile manipula-\ntion [23, 24, 25, 26] and navigation [27, 28]. A recent line of work seeks to replicate the success of\nvision and language models in robotics, by jointly pre-training large foundation models on behavior\ndata [29, 30]. The goal of our work is different. F3RM seeks to incorporate geometric information\nwith any pre-trained features by lifting imagery and language prior knowledge into 3D.\nDense 2D Visual Descriptors.\n[31, 32] use dynamic 3D reconstruction from RGB-D videos to\nprovide association labels between pixels from different video frames. Dense Object Nets eliminate\nthe need for dynamic reconstruction, using multi-view RGB-D images of static scenes in the context\nof robotics [13, 33]. NeRF-Supervision extends descriptor learning to thin structures and reflective\nmaterials via NeRFs for 3D reconstruction [14]. In contrast, recent work indicates that excellent\ndense correspondence can emerge at a larger scale without the need for explicit supervision [4, 34].\nGeometric Aware Representations for Robotics.\nGeometric understanding is an essential part\nof mapping [35, 36, 37], grasping [38, 39, 17, 40], and legged locomotion [41]. These work either\nrequire direct supervision from 3D data such as point clouds, or try to learn representations from\nposed 2D images or videos [13, 14]. [21, 42, 43] leverage neural scene representations to take\nadvantage of their ability to handle reflective or transparent objects and fine geometry. Our work\nincorporates pre-trained vision foundation models to augment geometry with semantics.\n3D Feature Fields.\nA number of recent work integrate 2D foundation models with 3D neural fields\nin contexts other than robotic manipulation [44, 45, 46, 47, 48, 49, 50]. See Appendix A.8 for a de-\ntailed overview. Our work shares many similarities with LERF [51]. However, unlike the multi-scale\nimage-level features used in LERF, we extract dense patch-level features from CLIP. Additionally,\nwe take a step further by exploring the utilization of feature fields for robotic manipulation.\n6\nConclusion\nWe have illustrated a way to combine 2D visual priors with 3D geometry to achieve open-ended\nscene understanding for few-shot and language-guided robot manipulation. Without fine-tuning,\nDistilled Feature Fields enable out-of-the-box generalization over variations in object categories,\nmaterial, and poses. When the features are sourced from vision-language models, distilled feature\nfields offer language-guidance at various levels of semantic granularity.\nLimitations.\nOur system takes 1m 40s to collect 50 images of the scene, and 90s to model the\nNeRF and feature field. This highlights the need to develop generalizable NeRFs that can recover\ngeometry quickly with just a few views [9, 43], opening the possibility for closed-loop dynamic ma-\nnipulation. More generally, novel view synthesis is a generative process not too different from image\ngeneration with GANs [52] and diffusion models [53]. These alternatives, to which our philosophy\nequally applies, hold promise for solving general-purpose visual and geometric understanding.\n8\nAcknowledgement\nWe gratefully acknowledge support from Amazon.com Service LLC, Award #2D-06310236; from\nthe National Science Foundation under Cooperative Agreement PHY-2019786 (The NSF AI In-\nstitute for Artificial Intelligence and Fundamental Interactions, http://iaifi.org/); from NSF grant\n2214177; from AFOSR grant FA9550-22-1-0249; from ONR MURI grant N00014-22-1-2740; from\nARO grant W911NF-23-1-0034; from the MIT-IBM Watson AI Lab; and from the MIT Quest for\nIntelligence. The authors also thank Tom\u00b4as Lozano-P\u00b4erez, Lin Yen-Chen and Anthony Simeonov\nfor their advice; Boyuan Chen for initial discussions; Rachel Holladay for her extensive help with\nsetting up the robot; and Tom Silver for providing feedback on an earlier draft.\nReferences\n[1] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,\nP. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervi-\nsion. In International Conference on Machine Learning. PMLR, 2021.\n[2] G. Ilharco, M. Wortsman, R. Wightman, C. Gordon, N. Carlini, R. Taori, A. Dave, V. Shankar,\nH. Namkoong, J. Miller, H. Hajishirzi, A. Farhadi, and L. Schmidt. OpenCLIP, July 2021.\nURL https://doi.org/10.5281/zenodo.5143773.\n[3] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham, Q. Le, Y.-H. Sung, Z. Li, and T. Duerig.\nScaling Up Visual and Vision-Language Representation Learning with Noisy Text Supervision.\nIn International Conference on Machine Learning. PMLR, 2021.\n[4] M. Caron, H. Touvron, I. Misra, H. J\u00b4egou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging\nproperties in self-supervised vision transformers. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, 2021.\n[5] V. Tschernezki, I. Laina, D. Larlus, and A. Vedaldi. Neural Feature Fusion Fields: 3D Dis-\ntillation of Self-Supervised 2D Image Representations. In Proceedings of the International\nConference on 3D Vision (3DV), 2022.\n[6] S. Kobayashi, E. Matsumoto, and V. Sitzmann. Decomposing NeRF for Editing via Feature\nField Distillation. In Advances in Neural Information Processing Systems, volume 35, 2022.\n[7] S. Amir, Y. Gandelsman, S. Bagon, and T. Dekel. Deep ViT Features as Dense Visual Descrip-\ntors. ECCVW What is Motion For?, 2022.\n[8] T. M\u00a8uller, A. Evans, C. Schied, and A. Keller. Instant Neural Graphics Primitives with a\nMultiresolution Hash Encoding. ACM Transactions on Graphics (ToG), 2022.\n[9] A. Yu, V. Ye, M. Tancik, and A. Kanazawa. pixelNeRF: Neural radiance fields from one or\nfew images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2021.\n[10] M. Tancik, E. Weber, E. Ng, R. Li, B. Yi, J. Kerr, T. Wang, A. Kristoffersen, J. Austin,\nK. Salahi, A. Ahuja, D. McAllister, and A. Kanazawa. Nerfstudio: A Modular Framework\nfor Neural Radiance Field Development. In ACM SIGGRAPH 2023 Conference Proceedings,\nSIGGRAPH \u201923, 2023.\n[11] C. Zhou, C. C. Loy, and B. Dai. Extract free dense labels from clip. In European Conference\non Computer Vision (ECCV), 2022.\n[12] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. NeRF:\nRepresenting Scenes as Neural Radiance Fields for View Synthesis. In European Conference\non Computer Vision (ECCV), 2020.\n9\n[13] P. R. Florence, L. Manuelli, and R. Tedrake. Dense Object Nets: Learning Dense Visual Object\nDescriptors By and For Robotic Manipulation. In Proceedings of the 2nd Conference on Robot\nLearning (CoRL), June 2018.\n[14] L. Yen-Chen, P. Florence, J. T. Barron, T.-Y. Lin, A. Rodriguez, and P. Isola.\nNeRF-\nSupervision: Learning dense object descriptors from neural radiance fields. In IEEE Con-\nference on Robotics and Automation (ICRA), 2022.\n[15] M. Hamilton, Z. Zhang, B. Hariharan, N. Snavely, and W. T. Freeman. Unsupervised Semantic\nSegmentation by Distilling Feature Correspondences. In International Conference on Learning\nRepresentations, 2022. URL https://openreview.net/forum?id=SaKO6z6Hl0c.\n[16] Y. Rao, W. Zhao, G. Chen, Y. Tang, Z. Zhu, G. Huang, J. Zhou, and J. Lu.\nDenseclip:\nLanguage-guided dense prediction with context-aware prompting.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.\n[17] A. Simeonov, Y. Du, A. Tagliasacchi, J. B. Tenenbaum, A. Rodriguez, P. Agrawal, and V. Sitz-\nmann. Neural Descriptor Fields: SE(3)-Equivariant Object Representations for Manipulation.\nIn IEEE Conference on Robotics and Automation (ICRA), 2022.\n[18] D. P. Kingma and J. Ba.\nAdam: A method for stochastic optimization.\nIn International\nConference on Learning Representations, 2015.\n[19] E. Coumans and Y. Bai. Pybullet, a python module for physics simulation for games, robotics\nand machine learning. http://pybullet.org, 2016.\n[20] C. Garrett. PyBullet Planning. https://github.com/caelan/pybullet-planning, 2018.\n[21] L. Yen-Chen, P. Florence, A. Zeng, J. T. Barron, Y. Du, W.-C. Ma, A. Simeonov, A. R. Garcia,\nand P. Isola. MIRA: Mental imagery for robotic affordances. In Conference on Robot Learning\n(CoRL), 2022.\n[22] M. Yuksekgonul, F. Bianchi, P. Kalluri, D. Jurafsky, and J. Zou. When and Why Vision-\nLanguage Models Behave like Bags-Of-Words, and What to Do About It? In International\nConference on Learning Representations, 2023. URL https://openreview.net/forum?\nid=KRLUvxh8uaX.\n[23] M. Shridhar, L. Manuelli, and D. Fox. Cliport: What and where pathways for robotic manipu-\nlation. In Proceedings of the 5th Conference on Robot Learning (CoRL), 2021.\n[24] M. Shridhar, L. Manuelli, and D. Fox. Perceiver-actor: A multi-task transformer for robotic\nmanipulation. In Proceedings of the 6th Conference on Robot Learning (CoRL), 2022.\n[25] S. Karamcheti, S. Nair, A. S. Chen, T. Kollar, C. Finn, D. Sadigh, and P. Liang. Language-\ndriven representation learning for robotics. In Robotics: Science and Systems (RSS), 2023.\n[26] S. Stepputtis, J. Campbell, M. Phielipp, S. Lee, C. Baral, and H. Ben Amor.\nLanguage-\nConditioned Imitation Learning for Robot Manipulation Tasks. Advances in Neural Infor-\nmation Processing Systems, 33, 2020.\n[27] A. Stone, T. Xiao, Y. Lu, K. Gopalakrishnan, K.-H. Lee, Q. Vuong, P. Wohlhart, B. Zitkovich,\nF. Xia, C. Finn, et al. Open-world object manipulation using pre-trained vision-language mod-\nels. arXiv preprint arXiv:2303.00905, 2023.\n[28] D. Shah, B. Osinski, B. Ichter, and S. Levine. LM-nav: Robotic navigation with large pre-\ntrained models of language, vision, and action. In 6th Annual Conference on Robot Learning,\n2022. URL https://openreview.net/forum?id=UW5A3SweAH.\n10\n[29] D. Driess, F. Xia, M. S. M. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson,\nQ. Vuong, T. Yu, W. Huang, Y. Chebotar, P. Sermanet, D. Duckworth, S. Levine, V. Vanhoucke,\nK. Hausman, M. Toussaint, K. Greff, A. Zeng, I. Mordatch, and P. Florence. PaLM-E: An\nEmbodied Multimodal Language Model. arXiv preprint arXiv:2303.03378, Mar. 2023.\n[30] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Haus-\nman, A. Herzog, J. Hsu, J. Ibarz, B. Ichter, A. Irpan, T. Jackson, S. Jesmonth, N. J. Joshi,\nR. Julian, D. Kalashnikov, Y. Kuang, I. Leal, K.-H. Lee, S. Levine, Y. Lu, U. Malla, D. Manju-\nnath, I. Mordatch, O. Nachum, C. Parada, J. Peralta, E. Perez, K. Pertsch, J. Quiambao, K. Rao,\nM. Ryoo, G. Salazar, P. Sanketi, K. Sayed, J. Singh, S. Sontakke, A. Stone, C. Tan, H. Tran,\nV. Vanhoucke, S. Vega, Q. Vuong, F. Xia, T. Xiao, P. Xu, S. Xu, T. Yu, and B. Zitkovich. RT-1:\nRobotics Transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, Dec.\n2022.\n[31] C. B. Choy, J. Gwak, S. Savarese, and M. Chandraker. Universal Correspondence Network. In\nAdvances in Neural Information Processing Systems, June 2016.\n[32] T. Schmidt, R. Newcombe, and D. Fox. Self-supervised visual descriptor learning for dense\ncorrespondence. IEEE Robotics and Automation Letters (RA-L), Apr. 2017.\n[33] P. Florence, L. Manuelli, and R. Tedrake. Self-Supervised Correspondence in Visuomotor\nPolicy Learning. IEEE Robotics and Automation Letters (RA-L), 5(2):492\u2013499, Apr. 2020.\n[34] M. Oquab, T. Darcet, T. Moutakanni, H. V. Vo, M. Szafraniec, V. Khalidov, P. Fernandez,\nD. Haziza, F. Massa, A. El-Nouby, R. Howes, P.-Y. Huang, H. Xu, V. Sharma, S.-W. Li,\nW. Galuba, M. Rabbat, M. Assran, N. Ballas, G. Synnaeve, I. Misra, H. Jegou, J. Mairal,\nP. Labatut, A. Joulin, and P. Bojanowski. DINOv2: Learning robust visual features without\nsupervision. arXiv:2304.07193, 2023.\n[35] J. J. Leonard and H. F. Durrant-Whyte. Simultaneous map building and localization for an\nautonomous mobile robot. In IEEE/RSJ International Conference on Intelligent Robots and\nSystems (IROS), volume 3, pages 1442\u20131447, 1991.\n[36] H. Durrant-Whyte, D. Rye, and E. Nebot. Localization of Autonomous Guided Vehicles. In\nRobotics Research, pages 613\u2013625. Springer London, 1996.\n[37] H. Durrant-Whyte and T. Bailey.\nSimultaneous localization and mapping: part i.\nIEEE\nRobotics and Automation Magazine, 13(2):99\u2013110, 2006. doi:10.1109/MRA.2006.1638022.\n[38] J. Mahler, J. Liang, S. Niyaz, M. Laskey, R. Doan, X. Liu, J. A. Ojea, and K. Goldberg. Dex-\nnet 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp\nmetrics. In Robotics: Science and Systems (RSS), 2017.\n[39] X. Yan, J. Hsu, M. Khansari, Y. Bai, A. Pathak, A. Gupta, J. Davidson, and H. Lee. Learning\n6-dof grasping interaction via deep geometry-aware 3d representations. In IEEE International\nConference on Robotics and Automation (ICRA), pages 3766\u20133773. IEEE, 2018.\n[40] J. Urain, N. Funk, J. Peters, and G. Chalvatzaki. SE(3)-DiffusionFields: Learning smooth\ncost functions for joint grasp and motion optimization through diffusion. IEEE International\nConference on Robotics and Automation (ICRA), 2023.\n[41] R. Yang, G. Yang, and X. Wang. Neural volumetric memory for visual locomotion control.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 2023.\n[42] J. Kerr, L. Fu, H. Huang, Y. Avigal, M. Tancik, J. Ichnowski, A. Kanazawa, and K. Goldberg.\nEvo-NeRF: Evolving NeRF for Sequential Robot Grasping of Transparent Objects. In 6th\nAnnual Conference on Robot Learning, 2022. URL https://openreview.net/forum?id=\nBxr45keYrf.\n11\n[43] Q. Dai, Y. Zhu, Y. Geng, C. Ruan, J. Zhang, and H. Wang. GraspNeRF: Multiview-based\n6-DoF Grasp Detection for Transparent and Specular Objects Using Generalizable NeRF. In\nIEEE International Conference on Robotics and Automation (ICRA), 2023.\n[44] H.-Y. F. Tung, R. Cheng, and K. Fragkiadaki. Learning spatial common sense with geometry-\naware recurrent networks. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 2595\u20132603, 2019.\n[45] H.-Y. F. Tung, Z. Xian, M. Prabhudesai, S. Lal, and K. Fragkiadaki. 3D-OES: Viewpoint-\ninvariant object-factorized environment simulators. In Proceedings of the 4th Conference on\nRobot Learning (2020), 2020.\n[46] S. Peng, K. Genova, C. M. Jiang, A. Tagliasacchi, M. Pollefeys, and T. Funkhouser. Open-\nScene: 3D Scene Understanding with Open Vocabularies. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), 2023.\n[47] N. M. M. Shafiullah, C. Paxton, L. Pinto, S. Chintala, and A. Szlam. Clip-fields: Weakly\nsupervised semantic fields for robotic memory. In Robotics: Science and Systems (RSS), 2023.\n[48] B. Bolte, A. S. Wang, J. Yang, M. Mukadam, M. Kalakrishnan, and C. Paxton. USA-Net:\nUnified Semantic and Affordance Representations for Robot Memory. ArXiv, abs/2304.12164,\n2023.\n[49] C. Huang, O. Mees, A. Zeng, and W. Burgard. Visual Language Maps for Robot Navigation.\nIn IEEE International Conference on Robotics and Automation (ICRA), London, UK, 2023.\n[50] K. Jatavallabhula, A. Kuwajerwala, Q. Gu, M. Omama, T. Chen, S. Li, G. Iyer, S. Saryazdi,\nN. Keetha, A. Tewari, J. Tenenbaum, C. de Melo, M. Krishna, L. Paull, F. Shkurti, and A. Tor-\nralba. Conceptfusion: Open-set multimodal 3d mapping. In Robotics: Science and Systems\n(RSS), 2023.\n[51] J. Kerr, C. M. Kim, K. Goldberg, A. Kanazawa, and M. Tancik. Lerf: Language embedded\nradiance fields. In International Conference on Computer Vision (ICCV), 2023.\n[52] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville,\nand Y. Bengio. Generative adversarial nets. In Advances in Neural Information Processing\nSystems, volume 27, 2014.\n[53] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole.\nScore-\nBased Generative Modeling through Stochastic Differential Equations. In International Con-\nference on Learning Representations, 2021. URL https://openreview.net/forum?id=\nPxTIG12RRHS.\n[54] J. T. Kajiya and B. P. Von Herzen. Ray tracing volume densities. ACM SIGGRAPH computer\ngraphics, 18(3):165\u2013174, 1984.\n[55] N. Max. Optical models for direct volume rendering. IEEE Trans. Vis. Comput. Graph., 1(2):\n99\u2013108, June 1995.\n[56] J. L. Sch\u00a8onberger and J.-M. Frahm. Structure-from-Motion Revisited. In Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2016.\n[57] J. L. Sch\u00a8onberger, E. Zheng, M. Pollefeys, and J.-M. Frahm. Pixelwise View Selection for\nUnstructured Multi-View Stereo. In European Conference on Computer Vision (ECCV), 2016.\n[58] C.-H. Lin, W.-C. Ma, A. Torralba, and S. Lucey. Barf: Bundle-adjusting neural radiance fields.\nIn Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5741\u2013\n5751, 2021.\n12\n[59] Z. Wang, S. Wu, W. Xie, M. Chen, and V. A. Prisacariu. NeRF\u2212\u2212: Neural Radiance Fields\nWithout Known Camera Parameters. arXiv preprint arXiv:2102.07064, 2021.\n[60] A. Simeonov, Y. Du, L. Yen-Chen, , A. Rodriguez, L. P. Kaelbling, T. L. Perez, and P. Agrawal.\nSE(3)-Equivariant Relational Rearrangement with Neural Descriptor Fields. In Proceedings of\nthe 6th Conference on Robot Learning (CoRL). PMLR, 2022.\n[61] V. Zhong, T. Rockt\u00a8aschel, and E. Grefenstette. RTFM: Generalising to New Environment\nDynamics via Reading. In International Conference on Learning Representations, 2020. URL\nhttps://arxiv.org/abs/1910.08210.\n[62] D. Bahdanau, F. Hill, J. Leike, E. Hughes, A. Hosseini, P. Kohli, and E. Grefenstette. Learning\nto understand goal specifications by modelling reward. In International Conference on Learn-\ning Representations, 2018. URL https://openreview.net/forum?id=H1xsSjC9Ym.\n[63] R. Hartley and A. Zisserman. Multiple View Geometry in Computer Vision. Cambridge Uni-\nversity Press, Mar. 2004.\n[64] A. W. Harley, S. K. Lakshmikanth, F. Li, X. Zhou, H.-Y. F. Tung, and K. Fragkiadaki. Learn-\ning from unlabelled videos using contrastive predictive neural 3d mapping. arXiv preprint\narXiv:1906.03764, 2019.\n[65] K. Mazur, E. Sucar, and A. Davison. Feature-Realistic Neural Fusion for Real-Time, Open Set\nScene Understanding. In IEEE Conference on Robotics and Automation (ICRA), 2023.\n[66] L. Li, Z. Shen, Z. Wang, L. Shen, and L. Bo. Compressing volumetric radiance fields to 1 mb.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 4222\u20134231, 2023.\n13\nAppendix\nA.1\nNeural Radiance Fields (NeRFs)\nNeural radiance fields [12] model a scene as a 6D, vector-valued continuous function that maps from\na position x = (x, y, z) and a normalized viewing direction d = (dx, dy, dz), to the differential\ndensity \u03c3 and emitted color (r, g, b). In practice, this is achieved via two neural networks which\npartially share parameters: 1) the density network \u03c3(x) which depends only on the position x; and\n2) the color network c(x, d) which depends on both the position x and viewing direction d.\nNovel-View Synthesis.\nNeRF synthesizes an image by casting a ray r from the camera origin o\nthrough the center of each pixel. Points along the ray are parameterized as rt = o + td, where t is\nthe distance of the point to the camera origin o. The color C(r) of the ray r between the near and\nfar scene bounds tn and tf is given by the volume rendering integral [54]\nC(r) =\nZ tf\ntn\nT(t)\u03c3(rt)c(rt, d) dt,\nT(t) = exp\n\u0012\n\u2212\nZ t\ntn\n\u03c3(rs) ds\n\u0013\n,\n(5)\nwhere T(t) is the accumulated transmittance along the ray from rtn to rt.\nModeling a Scene with NeRFs.\nFor a scene, we are given a dataset of N RGB images {I}N\ni=1\nwith camera poses {T}N\ni=1. At each iteration, we sample a batch of rays R \u223c {T}N\ni=1 and optimize\n\u03c3 and c by minimizing the photometric loss Lrgb = P\nr\u2208R \u2225 \u02c6C(r) \u2212 I(r)\u22252\n2, where I(r) is the RGB\nvalue of the pixel corresponding to ray r \u2208 R, and \u02c6C(r) is the color estimated by the model using a\ndiscrete approximation of Equation 5 [12, 55].\nA.2\nDense 2D Feature Extraction via MaskCLIP\nWe provide pseudo code for the MaskCLIP method [11] for extracting dense, patch-level features\nfrom the CLIP model [1] below. Algorithm 1 is the computation graph of the last layer of vanilla\nCLIP. Algorithm 2 is MaskCLIP\u2019s modified graph. Note that the two linear transformations via Wv\nand Wout can be fused into a single convolution operation. We provide our feature extraction code\nin our GitHub repository (https://github.com/f3rm/f3rm).\nAlgorithm 1 Image Feature (Original)\n1 def forward(x):\n2\nq, k, v = W_qkv @ self.ln_1(x)\n3\nv = (q[:1] * k).softmax(dim=-1) * v\n4\nx = x + W_out @ v\n5\nx = x + self.mlp(self.ln_2(x))\n6\nreturn x[:1]\n# the CLS token\nAlgorithm 2 Dense Features (MaskCLIP 11)\n1 def forward(x):\n2\nv = W_v @ self.ln_1(x)\n3\nz = W_out @ v\n4\nreturn z[1:] # all but the CLS token\nA.3\nFeature Fields\nImplementation Details.\nMemory for caching the 2D feature map is a significant system bottle-\nneck that does not appear with RGB reconstruction because high-dimensional features, up-scaled to\nthe RGB image resolution, can grow to more than 40 GB for a standard NeRF dataset. We solve\nthis issue by reconstructing patch-level feature maps without up-scaling them to pixel resolution.\nWe speed up our feature distillation by building off newer NeRF implementations using hierarchical\nhash grids [8] based on Nerfacto [10].\nFeature Field Quality.\nF3RM benefits from neural feature fields\u2019 ability to reconstruct detailed\n3D geometry. We offer such an example in Figure A8. Notice the difference in resolution, between\nthe source 2D feature map (middle), and the final feature field.\n14\n(a) RGB Image\n(b) Raw DINO ViT Feat.\n(c) Distilled Feat.\nFigure A8: Level of Detail. (a) Mesh strainer and whisk. (b) Raw feature map from DINO ViT,\nvery low in resolution. Colors correspond to PCA of the features. (c) 3D feature fields recover a\nhigher level of detail than the source 2D feature maps. Inset corresponds to (b) in its original size\nfor comparison.\n(a) CLIP Feature Mean Squared Error\n0\n2000\n4000\n6000\n8000\n10000\nStep\n0.24\n0.25\n0.26\n0.27\nMSE\nMLP Head\nHash Grid\n(b) DINO Feature Mean Squared Error\n0\n2000\n4000\n6000\n8000\n10000\nStep\n0.15\n0.16\n0.17\n0.18\nMSE\nMLP Head\nHash Grid\nFigure A9: Feature Error During Feature Distillation. The mean squared error on a held-out\nset of feature maps for (a) CLIP and (b) DINO using the MLP head and hash grid architectures\ndescribed in Section A.3.1. The hash grid architecture consistently achieves a lower error.\nA.3.1\nAblation on Feature Field Architecture\nWe implement our feature field as a hierarchical hash grid [8] that takes a 3D position x as input, and\noutputs the feature vector. We compare this against a MLP head that takes the intermediate features\noutput by NeRF as input, which is similar to the architectures in [5, 6]. We first train a NeRF on\nimages collected by the robot of a tabletop scene, then distill features for 10000 steps over 3 seeds.\nFigure A9 shows that the hash grid architecture achieves a lower mean squared error (MSE), because\nit is able to capture higher-frequency signals and finer details. While the difference in the MSE seems\nmarginal between these two architectures, the hash grid-based architecture qualitatively results in\nsignificantly more well-defined semantic boundaries between objects as shown in Figure A10.\nA.4\nExperimental Setup\nWe provide details about our experimental setup used across our experiments for learning to grasp\nfrom demonstrations and language-guided object manipulation.\nPhysical Setup.\nWe collect RGB images with a RealSense D415 camera (the depth sensor is not\nused) mounted on a selfie stick. The selfie stick is used to increase the coverage of the workspace, as\na wrist-mounted camera can only capture a small area of the workspace due to kinematic limitations.\nWe program a Franka Panda arm to pick up the selfie stick from a magnetic mount, scan 50\u00d71280\u00d7\n720 RGB images of the scene following a fixed trajectory of three helical passes at different heights,\nand place the selfie stick back on the mount.\n15\n(a) MLP Head\n(b) Hash Grid\nFigure A10: Comparing Feature Field Architectures. We show the similarity heatmaps for the\nlanguage query \u201cmetal mug\u201d on the scene shown in Fig.7. (a) When using the MLP head, regions\nunrelated to the metal mug exhibit high similarity, as shown by the red bleed onto objects including\nthe red screwdriver and tip of the whiteboard marker. (b) In contrast, the hash grid architecture\nresults in significantly less bleed and more well-defined semantic boundaries between objects.\nTo calibrate the camera poses, we run COLMAP [56, 57] on a scan of a dedicated calibration scene\nwith calibration markers placed by the robot at known poses. We use these objects to solve for the\ntransformation from the COLMAP coordinate system to the world coordinate system. These camera\nposes are reused on subsequent scans. Given that the true camera poses vary due to small differences\nin how the robot grasps the selfie-stick, we optimize them as part of NeRF modeling to minimize\nany errors and improve reconstruction quality [10, 58, 59].\nLabeling Demonstrations.\nWe label demonstrations in virtual reality (VR) using a web-based 3D\nviewer based on Three.js that we developed which supports the real-time rendering of NeRFs, point\nclouds, and meshes. Given a NeRF of the demonstration scene, we sample a point cloud and export\nit into the viewer. We open the viewer in a Meta Quest 2 headset to visualize the scene, and move a\ngripper to the desired 6-DOF pose using the hand controllers (see Fig.2a).\nFeature Type\nResolution\nDINO ViT\n98 \u00d7 55\nCLIP ViT\n42 \u00d7 24\nCLIP ResNet\n24 \u00d7 14\nTable 3: Feature Map Resolu-\ntions.\nResolutions of the fea-\ntures output by the vision models\ngiven a 1280 \u00d7 720 RGB image.\nNeRF and Feature Field Modeling.\nWe downscale the im-\nages to 640 \u00d7 480 to speed up modeling of the RGB NeRF, and\nuse the original 1280 \u00d7 720 images as input to the vision model\nfor dense feature extraction. We optimize the NeRF and feature\nfield sequentially for 2000 steps each, which takes at most 90s\n(average is 80s) on a NVIDIA RTX 3090, including the time to\nload the vision model into memory and extract features.\nIn our experiments, we distill the features at their original fea-\nture map resolution which is significantly smaller than the RGB\nimages (see Table 3). We achieve this by transforming the cam-\nera intrinsics to match the feature map resolutions, and sampling rays based on this updated camera\nmodel. The specific models we used were dino vits8 for DINO ViT, ViT-L/14@336px for CLIP\nViT, and RN50x64 for CLIP ResNet.\nA.5\nAblation on Number of Training Views\nAlthough our robot scans 50 images per scene in our experiments, we demonstrate that it is possi-\nble to use a significantly smaller number of views for NeRF and feature field modeling without a\nsignificant loss in quality. To investigate this, we ablate the number of training images by evenly\nsubsampling from the 50 scanned images and modeling a NeRF and feature field.\nFigure A11 qualitatively compares the RGB, depth, and segmentation heatmaps. We observe an\nincrease in floaters as we reduce the number of training images, with approximately 20 images\nbeing the lower bound before a drastic decline in quality.\n16\n49 Training Images\n30 Training Images \n20 Training Images\n18 Training Images\nFigure A11: Ablating Number of Training Views. We qualitatively compare feature fields trained\non different numbers of views. (Top Row) Segmentation heatmap for \u201cBaymax\u201d from a CLIP feature\nfield overlaid on the RGB image from NeRF. (Bottom Row) Depth map rendered from NeRF.\nA.6\nLearning to Grasp from Demonstrations\nSampling Query Points.\nWe use Nq = 100 query points across all the tasks shown in Fig.4.\nAs other works have observed [17, 60], the downstream performance can vary significantly across\ndifferent samples of the query points. To address this issue, we sample five sets of query points over\ndifferent seeds for each task, and run the grasp optimization procedure across a set of test scenes\nused for method development. We select the query points that achieved the highest success rate on\nthe test scenes. The covariance of the Gaussian is manually tuned to fit the task.\nGrasp Pose Optimization.\nWe first discuss how we initialize the grasp poses. We consider a\ntabletop workspace of size 0.7\u00d70.8\u00d70.35 meters, and sample a dense voxel grid over the workspace\nwith voxels of size \u03b4 = 0.0075m (we use 0.005m for the cup on racks experiment), where each voxel\nv = (x, y, z) represents the translation for a grasp pose.\nNext, we compute the alpha value \u03b1(v) for each voxel using the NeRF density network \u03c3, and filter\nout voxels with \u03b1(v) < \u03f5free = 0.1. This removes approximately 98% of voxels by ignoring free\nspace. The cosine similarity of the voxel features f\u03b1(v) is thresholded with the task embedding\nZM to further filter out voxels. This threshold is adjusted depending on the task and type of feature\ndistilled, and typically cuts down 80% of the remaining voxels. Finally, we uniformly sample Nr =\n8 rotations for each voxel to get the initial grasp proposals T .\nWe minimize Equation 3 to find the grasp pose that best matches the demonstrations using the Adam\noptimizer [18] for 50 steps with a learning rate of 5e-3. This entire procedure takes 15s on average,\nbut could easily be sped up.\nGrasp Execution.\nWe reject grasp poses which cause collisions by checking the overlap between\na voxelized mesh of the Panda gripper and NeRF geometry by querying the density field \u03c3. We\ninput the ranked list of grasp poses into an inverse kinematics solver and BiRRT motion planner in\nPyBullet [19, 20], and execute the highest-ranked grasp with a feasible motion plan.\nA.6.1\nBaselines\nWe provide implementation details of the four baselines used in our few-shot imitation learning\nexperiments. The first three baselines use NeRF-based outputs as features for the query point-based\npose optimization:\n1. Density: we use the alpha \u03b1 \u2208 (0, 1) values for NeRF density to ensure the values are scaled\nconsistently through different scenes, as the density values output by the density field \u03c3 are\nunbounded.\n17\n(a) Test Scene with a Screwdriver\n(b) Affordance Prediction\n(c) Predicted Grasp\nFigure A12: MIRA Failure Case. (a) Test scene with a screwdriver and other distractors for the\nscrewdriver task (Fig.4b). (b) The orthographic render of the view selected by MIRA, we show\nthe RGB (top) and depth (bottom) renders. The pixel circled in cyan indicates the action with the\nhighest pixel-wise affordance across all views. (c) The predicted 6-DOF grasp incorrectly targets\nthe silicone brush, as it shares resemblance to a screwdriver from a top-down perspective.\n2. Intermediate Features: we use the features output by the intermediate density embedding MLP\nin Nerfacto [10], which have a dimensionality of 15.\n3. RGB: we use [r, g, b, \u03b1] as the feature for this baseline. \u03b1 is used to ensure that this baseline\npays attention to both the color and geometry, as we found that using RGB only with the alpha-\nweighted feature field f\u03b1 (Eq.2) collapsed RGB values to (0, 0, 0) for free space, which corre-\nsponds to the color black.\nMIRA Baseline.\nThe fourth baseline we consider is Mental Imagery for Robotic Affordances\n(MIRA) [21], a NeRF-based framework for 6-DOF pick-and-place from demonstrations that renders\northographic views for pixel-wise affordance prediction. Orthographic rendering ensures that an\nobject has the same size regardless of its distance to the camera, and is used to complement the\ntranslation equivariance of the Fully Convolutional Network (FCN) for predicting affordances.\nMIRA formulates each pixel in a rendered orthographic view as a 6-DOF action T = (R, t), with\nthe orientation of the view defining the rotation R and the estimated depth from NeRF defining\nthe translation t. The FCN is trained to predict the pixels in the rendered views corresponding to\nthe demonstrated 6-DOF actions, and reject pixels sampled from a set of negative views. During\ninference, MIRA renders several orthographic views of the scene and selects the pixel that has the\nmaximum affordance across all views.\nIn our experiments, we train a separate FCN for 20000 steps for each task in Fig.4 specified by two\ndemonstrations, and sample negative pixels from datasets consisting solely of distractor objects. We\nuse data augmentation following Yen-Chen et al. [21]\u2019s provided implementation and apply random\nSE(2) transforms to the training views. Given a test scene, we scan 50 RGB images as described\nin Appendix A.4, render 360 orthographic viewpoints randomly sampled over an upper hemisphere\nlooking towards the center of the workspace, and infer a 6-DOF action.\nMIRA was designed for suction cup grippers and does not predict end-effector rotations. We at-\ntempted to learn this rotation, but found that the policy failed to generalize. To address this issue\nand give MIRA the best chance of success, we manually select the best end-effector rotation to\nachieve the task. We additionally find that MIRA often selects floater artifacts from NeRF, and\nmanually filter these predictions out along with other unreasonable grasps (e.g., grasping the table\nitself).\nGiven that MIRA is trained from scratch given just two demonstrations, we find that it struggles to\ngeneralize and is easily confused by floaters and distractors despite data augmentations and negative\nsamples. MIRA additionally reasons over 2.5D by using the rendered RGB and depth from NeRF\nas inputs to the FCN, while our query point-based formulation reasons explicitly over 3D. Because\nof this, we observe that MIRA can fail when there are occlusions or distractor objects that look\n18\n(a) Novel Scene\n(b) DINO ViT Heatmap\n(c) CLIP ViT Heatmap\nFigure A13: Comparing DINO and CLIP feature fields. We depict the cosine similarity for the\ntask of grasping a mug by the handle. Two demos are provided on a red and a white mug (cf. Fig.3b).\n(b) DINO overfits to the red color of the apple, while (c) CLIP captures higher-level semantics, and\nidentifies the metal mug.\nlike the demonstration objects from certain viewpoints. For example, one of the demonstrations for\nthe screwdriver task was a top-down grasp on a screwdriver standing vertically in a rack (Fig.4b).\nFigure A12 depicts a test scene for the screwdriver grasping task where MIRA incorrectly selects\nthe silicone brush as it looks similar to a screwdriver from a top-down 2.5D view. DINO and CLIP\nResNet feature fields successfully grasp the screwdriver in this scene, highlighting the benefits of\nusing pretrained features and reasoning explicitly over 3D geometry.\nA.6.2\nDINO Failure Cases\nOur experiments show that DINO struggles with distractor objects which have high feature similarity\nto the demonstrations, despite not representing the objects and their parts we care about (Fig.A13b).\nWe observe that DINO has the tendency to overfit to color. On the other hand, CLIP struggles far\nless with distractors due to its stronger semantic understanding (Fig.A13c).\nA.7\nLanguage-Guided Manipulation\nFor the language-guided experiments, we distilled CLIP ViT features from ViT-L/14@336px. We\nreuse the 10 demonstrations from the learning to grasp from demonstrations section and their asso-\nciated Nq = 100 query points, and sample Nr = 8 rotations for each voxel when initializing the\ngrasp proposals. We minimize the language-guided cost function in Equation 4 for 200 steps with\nAdam using a learning rate of 2 \u00d7 10\u22123.\nRetrieving Demonstrations via Text\nIn practice, we compare the user\u2019s embedded text query\nq with the task embedding ZM for each task M which is specified by two demonstrations. We\nrandomly sample object names as our negatives (L\u2212).\nInference Time.\nThe inference time to optimize for a grasp pose given a language query is 6.9\nseconds on average. We did not make any substantial attempts to speed this up, but note that reducing\nthe number of optimization steps (we use 200 steps but observe convergence usually within 50-\n100 steps), pruning more aggressively, and improving the implementation will significantly reduce\ninference time.\nA.8\nAdditional Related Work\nWe provide a more comprehensive overview of the related work discussed in Section 5.\nOpen-Ended Generalization via Language.\nA number of prior work use natural language for\ntask-specification and long-horizon planning in applications such as tabletop and mobile manipula-\ntion [23, 26], navigation [27, 28], and more generally, sequential decision-making in games [61, 62].\nA recent line of work seeks to replicate the success of vision and language models in robotics, by\njointly pre-training large foundation models on behavior data [29, 30]. One can refer to [25] for a\n19\nmore comprehensive coverage. The goal of our work is different. We seek to find a way to incorpo-\nrate geometric information with any pre-trained features. F3RM is a model-agnostic approach for\nlifting imagery and language prior knowledge into 3D. In this regard, we are more closely connected\nto CLIPort [23], which focuses on top-down grasping, and PerAct [24], which trains a voxel grid\nconditioned behavior transformer from scratch given waypoints in motion trajectories.\nSemantic Understanding and Dense 2D Visual Descriptors.\nLearning visual descriptors for\ndense correspondence estimation is a fundamental problem in computer vision. Among the earliest\nworks, Choy et al. [31] and Schmidt et al. [32] used dynamic 3D reconstruction from RGB-D videos\nto provide labels of association between pixels from different video frames. Dense Object Nets\ntackle this problem in the context of robotics [13], and eliminate the need for dynamic reconstruction\nusing multi-view RGB-D images of static scenes [33]. NeRF-Supervision [14] leverages NeRFs for\n3D reconstruction, extending descriptor learning to thin structures and reflective materials that pose\nchallenges for depth sensors. Unlike these prior works, recent work in vision foundation models\nshows that self-supervised vision transformers offer excellent dense correspondence [4]. When\nscaled to a large, curated dataset, such models offer phenomenal few-shot performance on various\ndense prediction tasks [34].\nGeometric Aware Representations for Robotics.\nGeometric understanding has been a long-\nstanding problem in computer vision [63] and is an essential and mission-critical part of mapping\nand navigation [35, 36, 37], grasping [38, 39, 17, 40], and legged locomotion [41]. These work\neither require direct supervision from 3D data such as Lidar point clouds, or try to learn represen-\ntations from posed 2D images or videos [14]. More recently, the robot grasping community has\nexperimented with neural scene representations to take advantage of their ability to handle reflective\nor transparent objects and fine geometry [21, 42, 43]. Our work incorporates pre-trained vision and\nvision-language foundation models to augment geometry with semantics.\n3D Feature Fields in Vision and Robotics\nA number of recent work integrate 2D foundation\nmodels with 3D neural fields in contexts other than robotic manipulation. For example, [44, 64]\ntrain geometry-aware recurrent neural networks to lift 2D features into 3D feature grids using sensed\nor predicted depth for improved 3D object detection and segmentation. 3D-OES [45] extend this\nto learn object dynamics models using graph neural networks over the 3D feature maps. Open-\nScene [46] and CLIP-Fields [47] distill 2D features into a neural field by sampling points from\nexisting 3D point clouds or meshes. USA-Net [48] and VLMap [49] build 3D feature maps from\nRGB-D images and VLMs for navigation. Our approach uses RGB images and integrates geomet-\nric modeling via NeRF with feature fusion in a single pipeline. ConceptFusion [50] uses surfels\nto represent the features, which requires 50 \u2212 100s GB per scene. Distilled feature fields offer a\nsignificant reduction in space [65]. Each of our scenes requires between 60 \u2212 120 MBs and could\nbe further compressed using lower-rank approximations [66]. Our work shares many similarities\nwith LERF [51]. However, unlike the image-level features used in LERF, which are derived from a\nhierarchy of image crops, we extract dense, patch-level features from CLIP. Additionally, we take a\nstep further by exploring the utilization of these feature fields for robotic manipulation.\n20\n"
  },
  {
    "title": "SceNeRFlow: Time-Consistent Reconstruction of General Dynamic Scenes",
    "link": "https://arxiv.org/pdf/2308.08258.pdf",
    "upvote": "4",
    "text": "SceNeRFlow: Time-Consistent Reconstruction of General Dynamic Scenes\nEdith Tretschk1 Vladislav Golyanik1 Michael Zollh\u00a8ofer2\nAlja\u02c7z Bo\u02c7zi\u02c7c2\nChristoph Lassner2\nChristian Theobalt1\n1Max Planck Institute for Informatics, Saarland Informatics Campus\n2Meta Reality Labs Research\nAbstract\nExisting methods for the 4D reconstruction of general,\nnon-rigidly deforming objects focus on novel-view synthe-\nsis and neglect correspondences.\nHowever, time consis-\ntency enables advanced downstream tasks like 3D edit-\ning, motion analysis, or virtual-asset creation. We propose\nSceNeRFlow to reconstruct a general, non-rigid scene in a\ntime-consistent manner. Our dynamic-NeRF method takes\nmulti-view RGB videos and background images from static\ncameras with known camera parameters as input. It then\nreconstructs the deformations of an estimated canonical\nmodel of the geometry and appearance in an online fash-\nion. Since this canonical model is time-invariant, we obtain\ncorrespondences even for long-term, long-range motions.\nWe employ neural scene representations to parametrize the\ncomponents of our method. Like prior dynamic-NeRF meth-\nods, we use a backwards deformation model. We find non-\ntrivial adaptations of this model necessary to handle larger\nmotions: We decompose the deformations into a strongly\nregularized coarse component and a weakly regularized\nfine component, where the coarse component also extends\nthe deformation field into the space surrounding the object,\nwhich enables tracking over time. We show experimentally\nthat, unlike prior work that only handles small motion, our\nmethod enables the reconstruction of studio-scale motions.\n1. Introduction\nOne criterion for the proper reconstruction of a deform-\ning scene is time consistency, i.e. correspondences across\ntime. Reconstructing general dynamic objects from RGB\ninput in a time-consistent manner is a little explored [9, 54],\nbut challenging and highly relevant research direction. Es-\ntablishing correspondences is equivalent to factorizing the\nreconstruction into time-varying deformations and a time-\ninvariant geometry model. High-level tasks that go beyond\nnovel-view synthesis benefit immensely from such a deeper\nanalysis of the scene, namely a reconstruction with long-\nrange, long-term dense 3D correspondences. For example,\nFigure 1. SceNeRFlow. Our NeRF-based method reconstructs a\ngeneral non-rigid scene from multi-view videos with time consis-\ntency. Here, a person with a plush dog rotates 180\u25e6 from time t=1\nuntil t=T. Novel view 1 at t=1 is consistent with novel view 2 at\nt=T (placed opposite of novel view 1) for our method, but not for\nNR-NeRF [78]. This enables, e.g., time- and view-consistent re-\ncoloring. We color the correspondences according to 3D positions\nin static canonical space, which differs between both methods.\nhaving access to such a virtual dynamic 3D object is a cru-\ncial step towards sophisticated 3D editing or estimating a\nmotion model for virtual-asset creation [90]. Almost all ex-\nisting work on reconstructing general dynamic objects ei-\nther only obtains time consistency for small motion or re-\nlaxes it to very small time windows. In this work, we ex-\nplore the setting of time consistency for large motions.\nPrior work [16, 18, 42, 59, 60, 63, 78, 85] for non-\n1\narXiv:2308.08258v1  [cs.CV]  16 Aug 2023\nrigid 3D reconstruction based on NeRF [50] focuses on\nnovel-view synthesis and does not aim for long-range corre-\nspondences. Almost all dynamic-NeRF papers (except for\n[44, 63, 78], which only handle small motions) weaken the\nlong-term consistency of the reconstruction by design by\nhaving a time-varying geometry and/or appearance model.\nThus, only rather simple tasks like replaying the input scene\nunder novel views are straightforward. Furthermore, as we\nwill show in our experiments, this time dependency im-\nproves the novel-view rendering quality but loosens long-\nterm correspondences. In contrast, we explore the other ex-\ntreme, where only deformations are time-variant.\nClassical, non-NeRF-based work on 4D reconstruction\nonly handles small motion [5, 33, 36, 66, 67] or is not time-\nconsistent [3, 15, 55, 88]. Similarly, scene flow [91] focuses\non the deformations and not a full reconstruction, and ex-\nhibits correspondence drift [29], similar to optical flow.\nIn this paper, we propose SceNeRFlow (Scene Flow +\nNeRF) to tackle time-consistent reconstruction of a gen-\neral, non-rigidly deforming scene; see Fig. 1. Our method\nis NeRF-based, trained per scene, and by design estimates\neach timestamp\u2019s deformation of a time-invariant canoni-\ncal model, thereby counteracting correspondence drift. As\nis common for dynamic NeRFs [59, 63, 78], we employ\na backward deformation model.\nStandard reconstruction\ntechniques like online optimization, coarse-and-fine defor-\nmation decomposition, and rigidity regularization turn out\nto be insufficient for large motion. While category-specific\npriors like an articulated human skeleton would help by nor-\nmalizing out large motion, this work tackles the general set-\nting, where such prior knowledge is not available. Instead,\nwe crucially find that carefully \u201cextending the deformation\nfield\u201d is the minimally necessary change to make backwards\ndeformation modeling work for large non-rigid motion.\nSince ours is the first NeRF-based work with time con-\nsistency for large motion, we want to focus on the core\nproblem of long-term correspondences. We therefore use\nmulti-view input to avoid the need for correctly modeling\nthe deformations of occluded geometry that arises in the\nmonocular setting. Importantly, our scenes exhibit signif-\nicantly larger motion than any prior time-consistent gen-\neral reconstruction method could handle. Our experiments\nshow how SceNeRFlow enables the time-consistent recon-\nstruction even of studio-scale motion without any category-\nspecific priors, e.g. without a human skeleton.\nIn summary, our contributions are (1) an end-to-end dif-\nferentiable, time-consistent 4D reconstruction method for\ngeneral dynamic scenes from multi-view RGB input from\nstatic cameras, which is built around (2) a general approach\nto make backward deformation models work for larger mo-\ntion via \u201cextending the deformation field\u201d (Sec. 3.2).\n2. Related Work\n3D Correspondence Estimation.\nFor temporal data,\nVedula et al.\u2019s seminal work [81] introduced scene flow as\nthe 3D equivalent of optical flow, which led to many ap-\nproaches tackling the problem [6, 28, 64, 75, 82, 87], as\na recent survey [91] summarizes. Lately, learning-based\nmethods [24, 41, 45, 51] focus on 3D point clouds as input.\nMore generally, non-rigid 3D point-cloud registration [14]\nestimates correspondences of provided 3D point clouds. We\ngo beyond just estimating 3D correspondences to create a\nfull reconstruction (including appearance) in 3D space from\nonly 2D input data, in an end-to-end differentiable pipeline.\n4D Reconstruction of General Non-Rigid Scenes. Multi-\nple lines of research target 4D reconstruction without rely-\ning on NeRF. As a recent survey [79] discusses, monoc-\nular RGB input has traditionally been tackled by Non-\nRigid Structure-from-Motion [20, 22, 36, 58, 66] and\nShape-from-Template methods [5, 12, 33, 56, 67], although\nlearning-based approaches [34, 35, 40, 84, 86, 88] have\ngrown in popularity over the last years.\nDue to the re-\nstrictive input setting, they either only handle small mo-\ntions or lack time consistency. Slightly larger motions are\npossible with a single RGB-D camera [98], although these\nmethods [7, 8, 10, 25, 26, 30, 43, 55, 69, 70, 97] all up-\ndate [13] both previously unseen and previously seen parts\nof the canonical model over time during their online op-\ntimization, thereby losing time consistency (except for the\ntemplate-based [97]). Multi-view input has seen compar-\natively less work over the years [21, 38, 80, 92].\nFu-\nsion4D [15, 57] operates in real time but also modifies\nthe canonical model over time. Mustafa et al. [54] track\na mesh through a temporal sequence according to optical\nflow and refine its topology for each timestamp, but do not\nreconstruct the appearance. Unlike such multi-view mesh-\ntracking approaches [9, 95], our method is end-to-end dif-\nferentiable and can easily integrate recent advances in neu-\nral scene representations [77]. Bansal et al. [3] obtain im-\npressive but time-inconsistent novel-view results.\nDynamic NeRFs. 2D neural rendering [76] and 3D neural\nscene representations [77] based on NeRF [50] have in re-\ncent years seen great success. Apart from early work [46],\ncurrent methods [16, 18, 42, 49, 59, 60, 63, 78, 85] for\ndynamic scene reconstruction with neural representations\nbuild on NeRF. Some approaches focus on surface [31],\nfast [17, 27], generalizable [83], or depth-supported [1] re-\nconstruction. Our method is most related to this field of\nwork but differs in its goal: we do not primarily aim for\nnovel-view synthesis but rather for a time-consistent re-\nconstruction.\nA few prior methods [44, 63, 78] do ob-\ntain time consistency but either only in synthetic [63] or\nsmall-motion [44, 78] settings. Furthermore, most works\nuse monocular input [19] and only a few [39, 44, 71, 83]\nexplore multi-view input, where the latter forego long-term\n2\ntime consistency or only handle small motion [44].\n3. Method\nSceNeRFlow takes as input multi-view RGB images of size\nh\u00d7w over T consecutive timestamps from C static cam-\neras with known extrinsics and intrinsics, i.e.\n{Ic,t \u2208\n[0, 1]h\u00d7w\u00d73}C,T\nc=1,t=1, and associated background images\n{Bc}c. In a time-consistent manner, it then reconstructs\nthe general dynamic scene as a time-invariant, NeRF-style\ncanonical model of the geometry and appearance, with\ntime-dependent deformations. The optimization proceeds\nin an online manner: we build a canonical model from the\nfirst timestamp (Sec. 3.1) and then track it frame-by-frame\nthrough the temporal input sequence (Sec. 3.2). Fig. 2 pro-\nvides an overview of the pipeline.\n3.1. Constructing the Canonical Model\nCanonical Model. The canonical model m encodes the\ngeometry and appearance in a time-independent manner.\nSpecifically, we use a HashMLP [53] to represent opac-\nity \u03c3 and RGB color c \u2208 [0, 1]3 for any 3D point x:\n(\u03c3, c) = m(x). A HashMLP combines a hash grid v with a\nsubsequent shallow MLP M: m(x) = M(v(x)), which is\nsignificantly faster than NeRF\u2019s [50] pure MLP. A hash grid\nconsists of about a dozen voxel grids of increasing resolu-\ntion, each containing learnable features. To evaluate, each\ngrid is queried via trilinear interpolation and the resulting\nfeatures from all grids are concatenated. Crucially, each\nvoxel grid is implemented via hashed indexing into an array\nof feature vectors rather than as a dense voxel grid.\nRendering. Since we take 2D images as input, we need\nto render m into 2D. To this end, we follow the quadrature\ndiscretization of volumetric rendering from NeRF [50] for a\nray r(s) = o+sd with origin o \u2208 R3 and direction d \u2208 R3:\nC(r) =\nS\nX\ni=1\nwici, with wi = exp\n \n\u2212\ni\u22121\nX\nj=1\n\u03c3j\u03b4j\n!\n(1 \u2212 exp(\u2212\u03c3i\u03b4i)),\n(1)\nwhere i indexes S discrete samples {r(si)}i along the ray;\n(\u03c3i,ci) = m(r(si)) are the opacity and color of the i-th\nsample, respectively; and \u03b4i = si+1 \u2212 si. Like NeRF [50],\nwe use stratified sampling of S evenly sized intervals be-\ntween the near plane and far plane to obtain {si}i.\nWhen a background color cback is provided (e.g. from\nBc), we composite it at the end of the ray:\nC(r, cback) = C(r) +\n\u0010\n1 \u2212\nX\ni\nwi\n\u0011\ncback.\n(2)\nLosses. To obtain the canonical model, we iteratively opti-\nmize it on images {Ic,1}c from t=1 for 20k iterations. Each\niteration uses a batch of R rays with an \u21131 reconstruction\nloss w.r.t. the ground-truth color Ic,1(rr):\nLrec = 1\nR\nR\nX\nr=1\n\u2225C(rr, cback,r) \u2212 Ic,1(rr)\u22251.\n(3)\nHowever, using only Lrec leads to a canonical model with\nfloating geometry artifacts. We remove them by steering the\nmodel to commit to foreground or background by encour-\naging P\ni wi to be 1 or 0 via the beta distribution [46]:\nLback = 1\nR\nX\nr\nlog\n\u0010 X\ni\nwr,i\n\u0011\n+ log\n\u0010\n1 \u2212\nX\ni\nwr,i\n\u0011\n, (4)\nwhere wr,i is wi of ray r. We also discourage transparent\ngeometry by encouraging each wi to be 0 or 1 via a mixture\nof two Laplacian distributions [65]:\nLhard = \u2212 1\nRS\nX\nr\nX\ni\nlog\n\u0010\ne\u2212wr,i + e\u2212(1\u2212wr,i)\u0011\n.\n(5)\nWe note that these losses do not use foreground masks; the\ncanonical model needs to learn on its own whether it can be\ntransparent and use the background image B or not.\nThe total loss for the canonical model is thus:\nLcanon = Lrec + \u03bbbackLback + \u03bbhardLhard,\n(6)\nwhere \u03bbback, \u03bbhard \u2208 R are loss weights. Since we want the\ncanonical model to be time-consistent, we keep its parame-\nters fixed after constructing it from the first timestamp.\n3.2. Optimizing per Timestamp\nGiven the canonical model from t=1, we next reconstruct\nits deformations for the remaining timestamps t>1.\nSpace Warping. We model the deformations at time t like\nprior dynamic-NeRF work: we use backwards space warp-\ning dc(x; \u03b8t\nc) = x + \u2206c(x; \u03b8t\nc) = x\u2032, where x is a 3D point\nin deformed world space, \u2206c outputs a coarse offset (fine\noffsets \u2206f will be introduced later), x\u2032 is in undeformed\ncanonical space, and \u03b8t\nc are the time-dependent parameters\nof dc/\u2206c. We use a HashMLP to parametrize \u2206c, i.e. there\nis one HashMLP per timestamp for coarse deformations. To\nquery the canonical model from world space at t>1, we first\nundo the deformation: (\u03c3(x, t), c(x, t)) = m(dc(x; \u03b8t\nc)).\nWhen rendering, this leads to view-consistent ray bending:\n{r(si)}i in world space becomes {dc(r(si); \u03b8t\nc)}i in canon-\nical space. We can then apply Eq. 1 or Eq. 2 to the bent ray\nto render, which in turn enables us to use Lrec to optimize\nfor the deformation parameters at time t using {Ic,t}c.\nFrame-Wise Tracking.\nHowever, na\u00a8\u0131vely reconstruct-\ning the entire scene by optimizing all timestamps at once\ndoes not converge to a recognizable canonical model when\nthe scene contains large motion.\nFurthermore, keeping\nall {Ic,t}c,t in memory at once is expensive in practice.\n3\nFigure 2. SceNeRFlow Overview. Input: We take as input a multi-view RGB video {Ic,t}c,t of a general dynamic scene from C cameras\nwith background images {Bc}c and known camera parameters. Output: From this, we build a NeRF-based [50], time-consistent 4D\nreconstruction. Model: To render a ray r from camera c, we first sample discrete points {r(si)}i along the straight ray (with background\ncolor cback at the end), then coarsely bend the ray with the coarse deformations dc, and then finely bend the resulting ray with the fine\ndeformations df. We then query the canonical model m at the resulting positions. m represents geometry (opacity \u03c3) and appearance\n(color c) volumetrically for any 3D point, in a time-invariant manner. Finally, we use NeRF-style volumetric rendering to obtain the ray\u2019s\ncolor C. We parametrize all three components with HashMLPs [53]. Training: We construct m from time t=1 using a reconstruction\nloss (Lrec w.r.t. the ground-truth color Ic,t(r)) and geometric regularizers (Lback and Lhard), and keep m\u2019s parameters \u03b8m fixed for future\ntimestamps, which leads to time consistency. For t>1, we split the deformations into a strongly regularized coarse component \u2206c and\na weakly regularized fine component \u2206f. Our online optimization performs frame-wise tracking to handle large motion and we thus\ninitialize the deformation parameters \u03b8t\nc and \u03b8t\nf at time t with \u03b8t\u22121\nc\nand \u03b8t\u22121\nf\n. To handle a peculiarity of backward deformation models in\nthis tracking setting, we regularize \u2206c to \u201cextend the deformation field\u201d (Lnorm,w).\nFigure 3. Extending the Deformation Field for Tracking. With-\nout smoothness, the estimated backwards deformations at time t\ngive a bad initialization for t+1. Smoothness initializes the defor-\nmations closer to the ground truth.\nWe thus propose online, timestamp-by-timestamp tracking.\nSince t=1 has, by construction, zero offsets, we set the last\nlayer of \u2206c(\u00b7; \u03b81\nc) to zeros [78] and do not optimize the de-\nformations at t=1. After reconstructing time t, we fix \u03b8t\nc\nand proceed with t+1, where we initialize \u03b8t+1\nc\nwith \u03b8t\nc.\nExtending the Deformation Field. Unfortunately, na\u00a8\u0131vely\nemploying a HashMLP for dc fails to reconstruct the scene\nfor any deformed state that significantly differs from the\ncanonical model.\nThis is because, at time t+1, the dy-\nnamic object resides in a slightly different place in world\nspace, part of which was empty space at time t and is hence\nnot well initialized by dc(\u00b7; \u03b8t\nc). This becomes especially\nprevalent when the object has undergone large motion. We\npropose to mitigate this failure of the backward model by\nextending the deformations into the area surrounding the\ndynamic object at time t, as Fig. 3 illustrates.\nTo this end, we employ two means: (1) We find that re-\nducing the resolution of the coarsest grid of the deforma-\ntion hash grid to 323 stabilizes tracking. We hypothesize\nthat when the object enters a previously empty voxel in a\nfine grid, the uninitialized latent codes of this voxel are too\nquickly too influential in the trilinear interpolation of the\nlatent codes. These uninitialized latent codes thus do not\nobtain the right values before being relevant but rather neg-\natively impact the deformations, leading to artifacts and a\nlack of large-scale smoothness. (2) However, this structural\nchange gives an unpredictable, badly-controlled smooth-\nness, similar to an MLP. We encourage well-behavedness\nvia a smoothness loss, which uses the inherent smoothness\nof the coarse grid to propagate its influence.\nSmoothness Loss. To stabilize the tracking, we propose to\nimpose a smoothness loss on the deformations. Because our\ndeformation model is continuous and fully differentiable,\nwe do not need to discretize the loss. Instead, we influ-\nence the local behavior directly via the (spatial) Jacobian\nJ \u2208 R3\u00d73 of the deformations: J = Jx = \u2202dc(x;\u03b8t)\n\u2202x\n. As\nis common for general reconstruction methods [79], we as-\nsume the object to deform locally in an as-rigid-as-possible\nmanner [72]. Specifically, we take inspiration from Ner-\n4\nfies\u2019s elastic loss on J [59]. However, their loss is computa-\ntionally expensive because it needs to compute all rows of\nJrr(si) for each sample rr(si) (which takes three backward\npasses during the forward pass of the loss computation) and\nthen performs an SVD of Jrr(si). Denoting the identity ma-\ntrix by I, we can avoid the SVD and allow for trivial com-\nputation of gradients by relaxing the constraint from the ro-\ntation group SO(3) = {A \u2208 R3\u00d73|A\u22a4A=I, det A=1}\nto the orthogonal group O(3) = {A \u2208 R3\u00d73|A\u22a4A=I},\nwhich additionally allows reflections since det A= \u00b1 1 for\nA \u2208 O(3). We can then encourage rigidity via:\nLrigid =\n1\n9RS\nX\nr\nX\ni\nX\nj,k\n\f\f(J\u22a4\nrr(si)Jrr(si) \u2212 I)j,k\n\f\f,\n(7)\nwhere Aj,k \u2208 R is the entry of matrix A at index (j, k).\nWe can compute this prior even faster by reducing the\nneed for three backward passes to just one, lowering overall\ntraining time by a factor of 2\u22123. To this end, we first note\nthat norm preservation is an equivalent definition of O(3):\nA \u2208 O(3) \u21d0\u21d2 \u2200e \u2208 R3 : \u2225Ae\u22252 = \u2225e\u22252.\n(8)\nWe next exploit the fact that automatic differentiation [23,\n61, 73] computes Jacobian-vector products J\u22a4e in a sin-\ngle backward pass, where e is an arbitrary vector. Since\nJ\u22a4 = J for J \u2208 O(3), we can compute the norm of Je\nas \u2225Je\u22252 = \u2225J\u22a4e\u22252. The following loss then encourages\nnorm preservation and only requires one backward pass:\nLnorm =\n1\nRS\nX\nr\nX\ni\nEe\nh\f\f\u2225J\u22a4\nrr(si)e\u22252 \u2212 1\n\f\f\ni\n,\n(9)\nwhere e is distributed uniformly on the unit sphere and\nhence \u2225e\u22252 = 1. We note that it is sufficient to only con-\nsider vectors on the unit sphere due to linearity. In practice,\nwe approximate this expectation with one random sample.\nWeighting the Smoothness Loss. To extend the deforma-\ntion field as discussed earlier, we could na\u00a8\u0131vely encourage\nsmoothness uniformly everywhere. However, this restricts\nempty space too much, leading it to push back against ob-\nject deformations. We thus focus on the object by weighting\nLnorm by \u02c6\u03c3r,i = exp(\u2212\u03c3r,i\u03b4), where \u03c3r,i is \u03c3i of the r-th\nray and we do not backpropagate into the opacity [78].\nWe now need to regularize the space around the object.\nHowever, this space is hard to locate efficiently and we ap-\nproximate it by max-pooling over {\u02c6\u03c3r,i}i along ray r, with\na window size of 1% of the ray length.\nThis still makes the space around the object too stiff.\nWe thus weaken the regularization by dividing the result-\ning weight by u if \u02c6\u03c3r,i and the weight after max-pooling\ndiffer by at least a factor of u. We empirically set u=10.\nFinally, regularizing very small offsets, which are\nwidespread during early timestamps, tends to collapse the\nnetwork. We hence only regularize deformations that are\nnot very small. We denote this weighted loss as Lnorm,w(\u03b8t\nc).\nThe supplement contains a full mathematical description.\nFine Deformations. In addition to enabling tracking by\nextending the deformation field, we also use the smooth-\nness loss to stabilize the surface by strongly regularizing its\ndeformations \u2206c. However, this leads to a loss of detail\nbecause \u2206c can now only represent coarse deformations.\nTo counteract this, we add fine deformations \u2206f(\u00b7; \u03b8t\nf) on\ntop, after normalizing out (i.e., applying) the coarse de-\nformations: x\u2032\u2032 = df(x\u2032; \u03b8t\nf) = x\u2032 + \u2206f(x\u2032; \u03b8t\nf), where\nx\u2032 = dc(x; \u03b8t\nc) for any 3D point x in world space and we\nquery m at x\u2032\u2032. Crucially, we allow for details by using a\nmuch weaker weight \u03bbfine for Lnorm,w(\u03b8t\nf), where the Jaco-\nbian is w.r.t. x\u2032. We parametrize \u2206f with a HashMLP.\nFrame-Wise Tracking Revisited. We apply tracking to the\ncoarse and fine deformations as follows: At t=1, we initial-\nize the last layers of \u2206c and \u2206f to zeros and do not optimize\nfor the deformations. At any t>1, we initialize \u03b8t\nc with the\nfinal \u03b8t\u22121\nc\n, and optimize for \u2206c(\u00b7; \u03b8t\nc) for 5k iterations while\nsetting \u2206f = 0. We then fix \u03b8t\nc, initialize \u03b8t\nf with the final\n\u03b8t\u22121\nf\n, and optimize for \u2206f(\u00b7; \u03b8t\nf) for 5k iterations.\nWith \u03bbcoarse, \u03bbfine \u2208 R, the total loss for any t>1 is:\nLtime = Lrec + \u03bbcoarseLnorm,w(\u03b8t\nc) + \u03bbfineLnorm,w(\u03b8t\nf). (10)\n3.3. Implementation Details\nWe next describe how we can speed up our method and\ncorrect vignetting effects, and provide optimization details.\nFig. 4 visualizes some of these methods.\nForeground-Focused Batches. To speed up training, we\nfocus the batches on the foreground. We first use back-\nground subtraction w.r.t. Bc to get a rough foreground mask\nFc,t for each Ic,t. We then pick 80% of the rays in the batch\nfrom the foreground and the rest from the background.\nPruning. To speed up rendering, we prune any sample r(si)\nin world space that does not contain any foreground. To de-\ntermine this for time t, we query a binary voxel grid gt that\nwe overlay over the scene. gt is 1 for voxels that are po-\ntentially in the foreground and 0 otherwise. We fill gt via\nspace carving [37] with {Fc,t}c. To be conservative and to\nlet the deformation field extend into the surrounding area,\nwe dilate both the foreground masks and the foreground in\nthe resulting voxel grid. To clear out geometry artifacts in\nempty space, we do not prune when constructing the canon-\nical model, only when optimizing for t>1. Pruning removes\n\u223c80% of samples, making our method four times faster.\nVignetting Correction. Cameras collect less light around\nthe image border, which leads to darkening in the input im-\nages. We model this vignetting with a radial model [2, 74]:\nCvig(r) = C(r)(1 + k1p(r) + k2p(r)2 + k3p(r)3), (11)\nwhere p(r) \u2208 R is the squared distance to the camera center\n5\n(a)\n(b)\n(c)\n(d)\nFigure 4. Implementation. (a) Foreground Fc,t. (b) Pruning grid\nfor (a). (c) W/o and (d) w/ vignetting correction.\nin pixel space (we divide the distance by w for resolution in-\nvariance), and k1, k2, k3 \u2208 R are correction parameters. We\ninitialize these parameters to zeros, optimize for them when\nconstructing the canonical model, and keep them fixed for\nt>1. We share the same set of parameters across all cam-\neras and timestamps. We use Cvig in place of C in Eq. (2).\nThe quality improves since the canonical model no longer\nneeds to use artifacts to account for vignetting effects.\nOptimizer. We use AdamW [47] with weight decay of 0.01\nto stabilize the training. Auto-decoded parameters like hash\ngrids get sparsely non-zero gradients in any given training\niteration, which degrades the momentum accumulation. We\nthus use a modified version of AdamW: rather than treating\nall parameters {\u03b8i \u2208 R}i in a tensor the same, we separately\nkeep track of AdamW\u2019s parameters \u03b8opt\ni\n(e.g. the number\nof iterations) for each individual parameter \u03b8i. In any given\niteration, only \u03b8i whose derivative is non-zero have their\nAdamW parameters \u03b8opt\ni\nupdated and AdamW applied. The\nsupplement describes the learning rates in detail.\nHyperparameters.\nAll scenes use the same settings: a\nbatch size of R=1024, S=3072 samples per ray, and loss\nweights \u03bbback=0.001, \u03bbhard=1, \u03bbcoarse=1000, and \u03bbfine=30.\nWe use LeakyReLUs [48] for the deformations and ReLUs\nfor the canonical model. The supplement has more details.\nCode. We use PyTorch [62] and tiny-cuda-nn [52] via its\nPython wrappers for its fast implementation of hash grids.\n4. Experiments\nWe evaluate the reconstruction quality and time consistency\nof our method, perform ablations, show simple scene edit-\ning, and discuss limitations and future work. The supple-\nmental video contains many additional results.\nPrior Work. We compare with all prior time-consistent\ngeneral NeRF methods: NR-NeRF [78], D-NeRF [63], and\nDeVRF [44]. To evaluate time consistency, we compare\nagainst PREF [71], a NeRF-based method to estimate cor-\nrespondences.\nVariants.\nWe evaluate reconstruction quality by using\nnovel-view synthesis as a proxy. Unlike most works, we tar-\nget time-consistent reconstruction and not novel-view syn-\nthesis, which leads to a trade-off with novel-view quality.\nMost dynamic-NeRF papers condition the canonical\nmodel on time. Therefore, we evaluate two variants of our\nmethod that do the same and, hence, synthesize better novel\nGround Truth\nOurs\nNR-NeRF\nSNF-A\nSNF-AG\nRendered RGB\nRendered Depth\n\u2014\nRendered RGB\nRendered Depth\n\u2014\nFigure 5. Novel-View Synthesis. (Top) Seq. 1 at t=T. (Bottom)\nSeq. 5 at t=38. The depth color differences between methods are\ndue to normalization.\nviews at the cost of correspondences. The first variant, SNF-\nA, only makes the appearance time-varying, while the sec-\nond variant, SNF-AG, makes the appearance and canonical\ngeometry time-varying. The supplement has further details.\nWe emphasize that we do not claim that these variants are\ncompetitive with state-of-the-art 4D reconstruction meth-\nods that neglect correspondences and focus solely on novel-\nview synthesis. Our design is not tailored to this setting.\nData.\nFor ease of recording, we evaluate our general\nmethod on real-world scenes of one or two people. Cru-\ncially, the recordings also contain a plush dog and loose\nclothing. Furthermore, our method reconstructs scenes with\nmultiple people without any knowledge that there are mul-\ntiple entities in the scene nor that they are human. These\naspects demonstrate its generality. We use a total of C=117\nstatic cameras in a studio to record six scenes of 4\u22125sec and\none of 12sec at 25fps. For the test sets, we pick the same\ntwo cameras for all scenes. We train at an input resolution\nof h=1504 and w=2056. In addition, we consider the short\nmulti-view scene from NR-NeRF [78], which contains 16\ncamera pairs.\nRuntime. On an Nvidia A100 GPU, the canonical model\ntrains at 8iter/sec, the coarse deformations at 16iter/sec, and\nthe fine deformations at 14iter/sec. For efficiency, we render\nall images at half the input resolution, at 10\u221215sec/image.\n4.1. Qualitative Results\nWe first qualitatively evaluate reconstructions at any one\npoint in time, and then examine their consistency over time.\n6\nt=1, view 1\nt=T, view 2\nt=1, view 2\nt=T, view 2\nOurs\nNR-NeRF\nFigure 6. Correspondences. (Left) Seq. 2, opposite views. NR-\nNeRF fails. (Right) Seq. 4, same view. Only our correspondences\nfollow the 90\u25e6 left turn; see orange at t=1 & T.\n4.1.1\nVolumetric 3D Scene Reconstruction\nFig. 5 shows novel views of the reconstruction and its geom-\netry via depth maps. While NR-NeRF gives blurry results,\nour method yields high-quality reconstructions. Although\nblurrier, SNF-A matches the pattern on the dress better than\nours. SNF-AG reconstructs the non-rigid geometry more\naccurately as it loosens the correspondences.\n4.1.2\nTime Consistency\nCorrespondence Visualization. Time consistency enables\n3D correspondences over time: Fig. 6 shows that our esti-\nmated correspondences are temporally stable. However, on\nscenes with large motion, NR-NeRF fails to converge to a\nrecognizable model since this requires simultaneous corre-\nspondence estimation in this highly challenging setting.\nComparisons with Prior Work. For further evaluation, we\nfollow PREF [71], which estimates world-space 3D human\njoint positions over time: {\u02dcpt\nj \u2208 R3}t,j, where j indexes the\nJ=23 joints. Off-the-shelf commercial systems [11] exploit\nstrong human-specific priors to reliably estimate 3D joints,\nwhich makes for excellent pseudo-ground-truth long-term\n3D correspondences {\u02c6pt\nj}t,j. (The joints are used only for\nevaluation; our method does not use human priors.)\nTo track the joints with our method, we first initialize\nthe estimated positions {\u02dcp1\nj}j with the ground-truth joints\n{\u02c6p1\nj}j at t=1, which are the positions in the canonical\nmodel. We then need to invert the backwards deformation\nfield for t>1. To this end, for each joint j at time t, we eval-\nuate df(dc(\u00b7; \u03b8t\nc); \u03b8t\nf) on a voxel grid (with a resolution of\n1283 and a side length of 40cm) centered on the estimated\nposition at t\u22121, \u02dcpt\u22121\nj\n, and pick the voxel center that lands\nclosest to the ground-truth joint \u02c6p1\nj in the canonical model\nas the estimated world-space joint position \u02dcpt\nj. The supple-\nOurs\nPREF\nNR-NeRF\nOurs\nPREF\nNR-NeRF\nFigure 7. Time Consistency. (Left) Seq. 4. (Right) Seq. 6. The\nsolid skeleton is the tracking estimate at t=T. The dotted skeleton\nis the pseudo-ground truth at t=T.\nOurs\nSNF-A\nSNF-A\nOurs\nSNF-AG\nSNF-AG\nat t1 & t2\nat t1\nat t2\nat t1 & t2\nat t1\nat t2\nFigure 8. Canonical Model. Ours uses a static canonical model,\nwhile those of SNF-A(G) vary over time.\nSequence\n1\n2\n3\n4\n5\n6\n7\nMean\nOurs\n0.9\n0.9\n1.8\n1.8\n2.8\n0.9\n1.4\n1.5\nPREF\n11.0\n13.9\n47.8\n8.9\n13.4\n12.1\n3.9\n15.9\nNR-NeRF\n2.2\n46.4\n110.9\n6.1\n8.5\n14.8\n1.2\n27.2\nD-NeRF\n40.7\n46.2\n110.1\n12.2\n81.9\n35.3\n70.0\n56.6\nTable 1.\nTime Consistency.\nWe report per-scene and mean\nMPJPE in cm. Lower is better.\nment contains tracking details for NR-NeRF and PREF.\nFig. 7 shows results at t=T. Our method yields stable\ncorrespondences, while NR-NeRF does not. PREF\u2019s corre-\nspondences drift strongly over time due to error accumula-\ntion from chaining frame-to-frame correspondences.\nVariants. We contrast the time consistency of our method\nwith its variants. To this end, Fig. 8 visualizes the canonical\nmodel at two different timestamps. Unlike SceNeRFlow,\nthe canonical model of the variants changes over time, i.e.,\nthey lack inherent correspondences. However, as results\nin the video show, these additional degrees of freedom im-\nprove the variants\u2019 novel-view synthesis, showing a trade-\noff between time consistency and novel-view synthesis.\n4.2. Quantitative Results\nTime Consistency. Like PREF [71], we measure time con-\nsistency via 3D joint tracking. We report the mean per-joint\nposition error (MPJPE) [68] over all timestamps t>1 and\nthe J joints: MPJPE =\n1\n(T \u22121)J\nPT\nt=2\nPJ\nj=1\u2225\u02c6pt\nj \u2212 \u02dcpt\nj\u22252.\nLower is better. For scenes with two people, we report the\naverage across both. We evaluate all scenes except for the 5-\nframe Seq. 8 used in NR-NeRF [78]. Tab. 1 contains the re-\nsults. SceNeRFlow has the lowest error with only marginal\ndrift. PREF\u2019s frame-wise approach leads to large drift. D-\nNeRF cannot handle large motion.\n7\nOurs\nNR-NeRF\nSNF-A\nSNF-AG\nBackground\nUnmasked\nPSNR\n\u2191\n30.32\n28.77\n30.88\n30.34\n21.69\nSSIM\n\u2191\n0.939\n0.922\n0.940\n0.929\n0.920\nLPIPS\n\u2193\n0.054\n0.099\n0.057\n0.089\n0.101\nMasked\nPSNR\n\u2191\n32.38\n30.80\n33.31\n33.22\n\u2014\nSSIM\n\u2191\n0.976\n0.965\n0.978\n0.977\n\u2014\nLPIPS\n\u2193\n0.018\n0.041\n0.016\n0.018\n\u2014\nTable 2. Novel-View Synthesis. Mean PSNR, SSIM, and LPIPS\nacross scenes. SNF-A(G) are variants of ours.\nNo Online\nOurs\nGT\nNo Extend\nOurs\nGT\nNo Coarse\nNo Fine\nOurs\nGT\nFigure 9. Ablations. (Top left) Online optimization, (top right) ex-\ntending the deformations, (bottom) coarse and fine deformations.\nReconstruction.\nWe quantify the reconstruction quality\nby using novel-view quality as a proxy. Like NeRF [50],\nwe measure the latter with PSNR and SSIM [96] (for\nboth, higher is better), and the neural perceptual metric\nLPIPS [32] (lower is better).\nWe focus on the dynamic\nscene part by (1) also providing these metrics w.r.t. the static\nbackground image, and (2) reporting masked scores where\nwe mask out the renderings with foreground masks esti-\nmated from the ground truth.\nThe results are in Tab. 2.\nOur method outperforms NR-NeRF. The variants have ar-\ntifacts in empty space and only outperform our method on\nthe masked scores. The supplement has per-scene results\nand further comparisons with D-NeRF and DeVRF.\n4.3. Ablations\nWe ablate the core components of our method. We investi-\ngate the relevance of (1) optimizing in an online manner (by\noptimizing all timestamps at once), (2) \u201cextending the de-\nformation field\u201d (by using a coarse resolution of 5123 and\nLnorm,w without max-pooling), (3) the coarse deformation\nmodel (by setting \u2206c=0 at training time), and (4) the fine\ndeformation model (by setting \u2206f=0 at test time). Fig. 9\nshows the results, the supplement confirms these quantita-\ntively: (1) Optimizing online simplifies implicit correspon-\ndence estimation, avoiding ghosting artifacts; (2) extending\nthe deformation field helps the online optimization by ini-\ntializing the deformations for the next timestamp well; (3)\nthe coarse deformations stabilize the reconstruction; and (4)\nthe fine deformations add details.\nView 1\nView 2\nt=1\nt=T\nt=1\nt=T\nFigure 10. Scene Editing. (1) Coloring the left knee of person 1\ngreen, (2) blending the right shoulder of person 2 with blue, and\n(3) making the right foot of person 2 transparent.\n4.4. Simple Editing\nThe time consistency enables simple editing of the geome-\ntry and appearance. As a proof of concept, Fig. 10 shows\nthat we can re-color scene parts (e.g., of Seq. 3) or make\nthem transparent in a straightforward manner\u2014by modify-\ning the static canonical model. The deformations then con-\nsistently propagate these changes to all timestamps.\n4.5. Limitations and Future Work\nWhile our method design makes a few assumptions, we ex-\nploit them as little as possible, e.g. we do not use accurate\nsegmentation masks, depth estimates, or 3D estimates. This\nmakes it easier to extend our work to other input settings.\nAdjacent work offers promising remedies for our as-\nsumptions. Most dynamic-NeRF methods are monocular\n(but not time-consistent for large motion) and some static-\nNeRF works [77, 89] use only few cameras. Adapting these\nideas might reduce the number of cameras required by our\nmethod. Since our core problem lies in the dynamic fore-\nground, we exclude the background in a soft manner. The\nbackground could be included via a static NeRF as some\nexisting dynamic-NeRF approaches do. Na\u00a8\u0131vely modeling\ntime-varying appearance loosens correspondences. Sophis-\nticated regularization from Shape-from-Shading [4, 93, 94]\nmight help. To preserve time consistency, we do not up-\ndate the canonical model with newly visible geometry from\nt>1. However, multi-view input mitigates the effect of oc-\nclusions at t=1. Finally, reconstructing topology changes\nin a time-consistent manner remains an open challenge in\nthe field and we do not currently aim to address it.\n5. Conclusion\nSceNeRFlow demonstrates the great potential of modern\nneural scene representations for time-consistent reconstruc-\ntion of general dynamic objects. In particular, we showed\nhow backwards deformation models can be adapted to\ntracking large motion. As a proof of concept, we demon-\nstrated how SceNeRFlow enables simple edits that are con-\nsistent over time. We also showed how conditioning the\n8\ncanonical model on time trades off novel-view and corre-\nspondence quality. As discussed in Sec. 4.5, we make some\nsimplifying assumptions but exploit them as little as possi-\nble, paving the way for future extensions to weaker inputs.\nAcknowledgements. All data capture and evaluation was done at\nMPII. Research conducted by Vladislav Golyanik and Christian\nTheobalt at MPII was supported in part by the ERC Consolidator\nGrant 4DReply (770784). This work was also supported by a Meta\nReality Labs research grant.\nReferences\n[1] Benjamin Attal, Eliot Laidlaw, Aaron Gokaslan, Changil\nKim, Christian Richardt, James Tompkin, and Matthew\nO\u2019Toole. T\u00a8orf: Time-of-flight radiance fields for dynamic\nscene view synthesis. Advances in Neural Information Pro-\ncessing Systems (NeurIPS), 2021. 2\n[2] Artur Bal and Henryk Palus. Image vignetting correction\nusing a deformable radial polynomial model. Sensors, 2023.\n5\n[3] Aayush Bansal, Minh Vo, Yaser Sheikh, Deva Ramanan, and\nSrinivasa Narasimhan. 4d visualization of dynamic events\nfrom unconstrained multi-view videos. In Computer Vision\nand Pattern Recognition (CVPR), 2020. 2\n[4] Jonathan T Barron and Jitendra Malik. Shape, illumination,\nand reflectance from shading. Transactions on Pattern Anal-\nysis and Machine Intelligence (TPAMI), 2014. 8\n[5] Adrien Bartoli, Yan G\u00b4erard, Franc\u00b8ois Chadebecq, Toby\nCollins, and Daniel Pizarro. Shape-from-template. Transac-\ntions on Pattern Analysis and Machine Intelligence (TPAMI),\n2015. 2\n[6] Tali Basha, Yael Moses, and Nahum Kiryati.\nMulti-view\nscene flow estimation: A view centered variational approach.\nInternational Journal of Computer Vision (IJCV), 2013. 2\n[7] Aljaz Bozic, Pablo Palafox, Michael Zollh\u00a8ofer, Angela Dai,\nJustus Thies, and Matthias Nie\u00dfner. Neural non-rigid track-\ning.\nAdvances in Neural Information Processing Systems\n(NeurIPS), 2020. 2\n[8] Aljaz Bozic, Michael Zollhofer, Christian Theobalt, and\nMatthias Nie\u00dfner. Deepdeform: Learning non-rigid rgb-d\nreconstruction with semi-supervised data. In Computer Vi-\nsion and Pattern Recognition (CVPR), 2020. 2\n[9] Cedric Cagniart, Edmond Boyer, and Slobodan Ilic. Proba-\nbilistic deformable surface tracking from multiple videos. In\nEuropean Conference on Computer Vision (ECCV), 2010. 1,\n2\n[10] Hongrui Cai, Wanquan Feng, Xuetao Feng, Yan Wang, and\nJuyong Zhang.\nNeural surface reconstruction of dynamic\nscenes with monocular rgb-d camera. In Advances in Neural\nInformation Processing Systems (NeurIPS), 2022. 2\n[11] The Captury.\nCaptury Studio.\nhttps://captury.\ncom/, 2023. 7\n[12] David Casillas-Perez,\nDaniel Pizarro,\nDavid Fuentes-\nJimenez, Manuel Mazo, and Adrien Bartoli. The isowarp:\nthe template-based visual geometry of isometric surfaces. In-\nternational Journal of Computer Vision (IJCV), 2021. 2\n[13] Brian Curless and Marc Levoy. A volumetric method for\nbuilding complex models from range images. In Proc. Con-\nference on Computer Graphics and Interactive Techniques,\n1996. 2\n[14] Bailin Deng, Yuxin Yao, Roberto M Dyke, and Juyong\nZhang. A survey of non-rigid 3d registration. In Computer\nGraphics Forum (Eurographics State of the Art Reports),\n2022. 2\n[15] Mingsong Dou, Sameh Khamis, Yury Degtyarev, Philip\nDavidson, Sean Ryan Fanello, Adarsh Kowdle, Sergio Orts\nEscolano, Christoph Rhemann, David Kim, Jonathan Taylor,\net al. Fusion4d: Real-time performance capture of challeng-\ning scenes. ACM Transactions on Graphics, 2016. 2\n[16] Yilun Du, Yinan Zhang, Hong-Xing Yu, Joshua B Tenen-\nbaum, and Jiajun Wu. Neural radiance flow for 4d view syn-\nthesis and video processing. In International Conference on\nComputer Vision (ICCV), 2021. 1, 2\n[17] Jiemin Fang, Taoran Yi, Xinggang Wang, Lingxi Xie, Xi-\naopeng Zhang, Wenyu Liu, Matthias Nie\u00dfner, and Qi Tian.\nFast dynamic radiance fields with time-aware neural voxels.\nACM Transactions on Graphics (Proceedings of SIGGRAPH\nAsia), 2022. 2\n[18] Chen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang.\nDynamic view synthesis from dynamic monocular video. In\nInternational Conference on Computer Vision (ICCV), 2021.\n1, 2\n[19] Hang Gao, Ruilong Li, Shubham Tulsiani, Bryan Russell,\nand Angjoo Kanazawa. Monocular dynamic view synthesis:\nA reality check. In Advances in Neural Information Process-\ning Systems (NeurIPS), 2022. 2\n[20] Ravi Garg, Anastasios Roussos, and Lourdes Agapito. Dense\nvariational reconstruction of non-rigid surfaces from monoc-\nular video.\nIn Computer Vision and Pattern Recognition\n(CVPR), 2013. 2\n[21] Bastian Goldluecke and Marcus Magnor. Space-time isosur-\nface evolution for temporally coherent 3d reconstruction. In\nComputer Vision and Pattern Recognition (CVPR), 2004. 2\n[22] Stella Gra\u00dfhof and Sami Sebastian Brandt.\nTensor-based\nnon-rigid structure from motion. In Winter Conference on\nApplications of Computer Vision (WACV), 2022. 2\n[23] Andreas Griewank and Andrea Walther. Evaluating deriva-\ntives:\nprinciples and techniques of algorithmic differen-\ntiation.\nSociety for Industrial and Applied Mathematics\n(SIAM), 2008. 5\n[24] Xiuye Gu, Yijie Wang, Chongruo Wu, Yong Jae Lee, and\nPanqu Wang. Hplflownet: Hierarchical permutohedral lattice\nflownet for scene flow estimation on large-scale point clouds.\nIn Computer Vision and Pattern Recognition (CVPR), 2019.\n2\n[25] Kaiwen Guo, Feng Xu, Yangang Wang, Yebin Liu, and\nQionghai Dai. Robust non-rigid motion tracking and sur-\nface reconstruction using l0 regularization. In International\nConference on Computer Vision (ICCV), 2015. 2\n[26] Kaiwen Guo, Feng Xu, Tao Yu, Xiaoyang Liu, Qionghai Dai,\nand Yebin Liu. Real-time geometry, albedo, and motion re-\nconstruction using a single rgb-d camera. ACM Transactions\non Graphics, 2017. 2\n9\n[27] Xiang Guo, Guanying Chen, Yuchao Dai, Xiaoqing Ye, Ji-\nadai Sun, Xiao Tan, and Errui Ding.\nNeural deformable\nvoxel grid for fast optimization of dynamic view synthesis.\nIn Asian Conference on Computer Vision (ACCV), 2022. 2\n[28] Fr\u00b4ed\u00b4eric Huguet and Fr\u00b4ed\u00b4eric Devernay.\nA variational\nmethod for scene flow estimation from stereo sequences. In\nInternational Conference on Computer Vision (ICCV), 2007.\n2\n[29] Chun Ho Hung, Li Xu, and Jiaya Jia. Consistent binocular\ndepth and scene flow with chained temporal profiles. Inter-\nnational Journal of Computer Vision (IJCV), 2013. 2\n[30] Matthias Innmann, Michael Zollh\u00a8ofer, Matthias Nie\u00dfner,\nChristian Theobalt, and Marc Stamminger. VolumeDeform:\nReal-time Volumetric Non-rigid Reconstruction. 2016. 2\n[31] Erik C.M. Johnson, Marc Habermann, Soshi Shimada,\nVladislav Golyanik, and Christian Theobalt.\nUnbiased\n4d: Monocular 4d reconstruction with a neural deformation\nmodel. arXiv:2206.08368, 2022. 2\n[32] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual\nlosses for real-time style transfer and super-resolution. In\nEuropean Conference on Computer Vision (ECCV), 2016. 8\n[33] Navami Kairanda, Edith Tretschk, Mohamed Elgharib,\nChristian Theobalt, and Vladislav Golyanik. \u03d5-sft: Shape-\nfrom-template with a physics-based deformation model. In\nComputer Vision and Pattern Recognition (CVPR), 2022. 2\n[34] Angjoo Kanazawa, Shubham Tulsiani, Alexei A. Efros, and\nJitendra Malik. Learning category-specific mesh reconstruc-\ntion from image collections.\nIn European Conference on\nComputer Vision (ECCV), 2018. 2\n[35] Filippos Kokkinos and Iasonas Kokkinos. Learning monoc-\nular 3d reconstruction of articulated categories from motion.\nIn Computer Vision and Pattern Recognition (CVPR), 2021.\n2\n[36] Suryansh Kumar, Anoop Cherian, Yuchao Dai, and Hong-\ndong Li. Scalable dense non-rigid structure-from-motion: A\ngrassmannian perspective. In Computer Vision and Pattern\nRecognition (CVPR), 2018. 2\n[37] Kiriakos N Kutulakos and Steven M Seitz. A theory of shape\nby space carving. International Journal of Computer Vision\n(IJCV), 2000. 5\n[38] E Scott Larsen, Philippos Mordohai, Marc Pollefeys, and\nHenry Fuchs.\nTemporally consistent reconstruction from\nmultiple video streams using enhanced belief propagation. In\nInternational Conference on Computer Vision (ICCV), 2007.\n2\n[39] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon\nGreen, Christoph Lassner, Changil Kim, Tanner Schmidt,\nSteven Lovegrove, Michael Goesele, Richard Newcombe,\net al. Neural 3d video synthesis from multi-view video. In\nComputer Vision and Pattern Recognition (CVPR), 2022. 2\n[40] Xueting Li, Sifei Liu, Shalini De Mello, Kihwan Kim, Xiao-\nlong Wang, Ming-Hsuan Yang, and Jan Kautz. Online adap-\ntation for consistent mesh reconstruction in the wild. In Ad-\nvances in Neural Information Processing Systems (NeurIPS),\n2020. 2\n[41] Xueqian Li, Jhony Kaesemodel Pontes, and Simon Lucey.\nNeural scene flow prior.\nAdvances in Neural Information\nProcessing Systems (NeurIPS), 2021. 2\n[42] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang.\nNeural scene flow fields for space-time view synthesis of dy-\nnamic scenes. In Computer Vision and Pattern Recognition\n(CVPR), 2021. 1, 2\n[43] Wenbin Lin, Chengwei Zheng, Jun-Hai Yong, and Feng\nXu. Occlusionfusion: Occlusion-aware motion estimation\nfor real-time dynamic 3d reconstruction. 2022. 2\n[44] Jia-Wei Liu, Yan-Pei Cao, Weijia Mao, Wenqiao Zhang,\nDavid Junhao Zhang, Jussi Keppo, Ying Shan, Xiaohu Qie,\nand Mike Zheng Shou. Devrf: Fast deformable voxel radi-\nance fields for dynamic scenes. Advances in Neural Infor-\nmation Processing Systems (NeurIPS), 2022. 2, 3, 6\n[45] Xingyu Liu,\nCharles R Qi,\nand Leonidas J Guibas.\nFlownet3d: Learning scene flow in 3d point clouds. Com-\nputer Vision and Pattern Recognition (CVPR), 2019. 2\n[46] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel\nSchwartz, Andreas Lehrmann, and Yaser Sheikh. Neural vol-\numes: Learning dynamic renderable volumes from images.\nACM Transactions on Graphics, 2019. 2, 3\n[47] Ilya Loshchilov and Frank Hutter.\nDecoupled weight de-\ncay regularization. In International Conference on Learning\nRepresentations (ICLR), 2019. 6\n[48] Andrew L Maas, Awni Y Hannun, Andrew Y Ng, et al. Rec-\ntifier nonlinearities improve neural network acoustic models.\nIn International Conference on Machine Learning (ICML),\n2013. 6\n[49] Willi Menapace, St\u00b4ephane Lathuili`ere, Aliaksandr Siarohin,\nChristian Theobalt, Sergey Tulyakov, Vladislav Golyanik,\nand Elisa Ricci. Playable environments: Video manipulation\nin space and time. In Computer Vision and Pattern Recogni-\ntion (CVPR), 2022. 2\n[50] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view\nsynthesis.\nIn European Conference on Computer Vision\n(ECCV), 2020. 2, 3, 4, 8\n[51] Himangi Mittal, Brian Okorn, and David Held. Just go with\nthe flow: Self-supervised scene flow estimation. In Com-\nputer Vision and Pattern Recognition (CVPR), 2020. 2\n[52] Thomas M\u00a8uller. tiny-cuda-nn, 2021. 6\n[53] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a mul-\ntiresolution hash encoding. ACM Transactions on Graphics,\n2022. 3, 4, 17\n[54] Armin Mustafa, Hansung Kim, Jean-Yves Guillemaut, and\nAdrian Hilton.\nTemporally coherent 4d reconstruction of\ncomplex dynamic scenes. In Computer Vision and Pattern\nRecognition (CVPR), 2016. 1, 2\n[55] Richard A Newcombe, Dieter Fox, and Steven M Seitz.\nDynamicfusion: Reconstruction and tracking of non-rigid\nscenes in real-time. In Computer Vision and Pattern Recog-\nnition (CVPR), 2015. 2\n[56] Dat Tien Ngo, Sanghyuk Park, Anne Jorstad, Alberto Criv-\nellaro, Chang D. Yoo, and Pascal Fua. Dense image regis-\ntration and deformable surface reconstruction in presence of\nocclusions and minimal texture. In International Conference\non Computer Vision (ICCV), 2015. 2\n10\n[57] Sergio Orts-Escolano, Christoph Rhemann, Sean Fanello,\nWayne Chang, Adarsh Kowdle, Yury Degtyarev, David Kim,\nPhilip L Davidson, Sameh Khamis, Mingsong Dou, et al.\nHoloportation: Virtual 3d teleportation in real-time. In An-\nnual Symposium on User Interface Software and Technology,\n2016. 2\n[58] Shaifali Parashar, Mathieu Salzmann, and Pascal Fua. Local\nnon-rigid structure-from-motion from diffeomorphic map-\npings. In Computer Vision and Pattern Recognition (CVPR),\n2020. 2\n[59] Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien\nBouaziz, Dan B Goldman, Steven M. Seitz, and Ricardo\nMartin-Brualla. Nerfies: Deformable neural radiance fields.\nInternational Conference on Computer Vision (ICCV), 2021.\n1, 2, 5, 13\n[60] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T\nBarron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-\nBrualla, and Steven M Seitz.\nHypernerf:\na higher-\ndimensional representation for topologically varying neural\nradiance fields. ACM Transactions on Graphics, 2021. 1, 2\n[61] Adam Paszke, Sam Gross, Soumith Chintala, Gregory\nChanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-\nban Desmaison, Luca Antiga, and Adam Lerer. Automatic\ndifferentiation in pytorch. Advances in Neural Information\nProcessing Systems (NeurIPS) Workshops, 2017. 5\n[62] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zem-\ning Lin, Natalia Gimelshein, Luca Antiga, Alban Desmai-\nson, Andreas Kopf, Edward Yang, Zachary DeVito, Mar-\ntin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit\nSteiner, Lu Fang, Junjie Bai, and Soumith Chintala.\nPy-\ntorch: An imperative style, high-performance deep learning\nlibrary. In Advances in Neural Information Processing Sys-\ntems (NeurIPS). 2019. 6\n[63] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and\nFrancesc Moreno-Noguer. D-nerf: Neural radiance fields for\ndynamic scenes. In Computer Vision and Pattern Recogni-\ntion (CVPR), 2021. 1, 2, 6\n[64] Julian Quiroga, Thomas Brox, Fr\u00b4ed\u00b4eric Devernay, and James\nCrowley.\nDense semi-rigid scene flow estimation from\nrgbd images. In European Conference on Computer Vision\n(ECCV), 2014. 2\n[65] Daniel Rebain, Mark Matthews, Kwang Moo Yi, Dmitry La-\ngun, and Andrea Tagliasacchi. Lolnerf: Learn from one look,\n2022. 3\n[66] Chris Russell, Jo\u02dcao Fayad, and Lourdes Agapito. Dense non-\nrigid structure from motion. In International Conference on\n3D Imaging, Modeling, Processing, Visualization and Trans-\nmission, 2012. 2\n[67] Mathieu Salzmann, Vincent Lepetit, and Pascal Fua. De-\nformable surface tracking ambiguities. In Computer Vision\nand Pattern Recognition (CVPR), 2007. 2\n[68] Leonid Sigal, Alexandru O Balan, and Michael J Black. Hu-\nmaneva: Synchronized video and motion capture dataset and\nbaseline algorithm for evaluation of articulated human mo-\ntion. International Journal of Computer Vision (IJCV), 2010.\n7\n[69] Miroslava Slavcheva, Maximilian Baust, Daniel Cremers,\nand Slobodan Ilic. Killingfusion: Non-rigid 3d reconstruc-\ntion without correspondences. In Computer Vision and Pat-\ntern Recognition (CVPR), 2017. 2\n[70] Miroslava Slavcheva, Maximilian Baust, and Slobodan Ilic.\nSobolevfusion: 3d reconstruction of scenes undergoing free\nnon-rigid motion. In Computer Vision and Pattern Recogni-\ntion (CVPR), 2018. 2\n[71] Liangchen Song, Xuan Gong, Benjamin Planche, Meng\nZheng, David Doermann, Junsong Yuan, Terrence Chen, and\nZiyan Wu.\nPref: Predictability regularized neural motion\nfields. In European Conference on Computer Vision (ECCV),\n2022. 2, 6, 7, 17\n[72] Olga Sorkine and Marc Alexa. As-rigid-as-possible surface\nmodeling. In Symposium on Geometry Processing (SGP),\n2007. 4\n[73] Bert Speelpenning.\nCompiling fast partial derivatives of\nfunctions given by algorithms.\nUniversity of Illinois at\nUrbana-Champaign, 1980. 5\n[74] Jessi Stumpfel, Andrew Jones, Andreas Wenger, Chris\nTchou, Tim Hawkins, and Paul Debevec. Direct hdr cap-\nture of the sun and sky. In ACM SIGGRAPH Courses. 2006.\n5\n[75] Zachary Teed and Jia Deng. Raft-3d: Scene flow using rigid-\nmotion embeddings. In Computer Vision and Pattern Recog-\nnition (CVPR), 2021. 2\n[76] Ayush Tewari, Ohad Fried, Justus Thies, Vincent Sitzmann,\nStephen Lombardi, Kalyan Sunkavalli, Ricardo Martin-\nBrualla, Tomas Simon, Jason Saragih, Matthias Nie\u00dfner,\nRohit Pandey, Sean Fanello, Gordon Wetzstein, Jun-Yan\nZhu, Christian Theobalt, Maneesh Agrawala, Eli Shechtman,\nDan B Goldman, and Michael Zollh\u00a8ofer. State of the art on\nneural rendering. Computer Graphics Forum (Eurographics\nState of the Art Reports), 2020. 2\n[77] A. Tewari, J. Thies, B. Mildenhall, P. Srinivasan, E. Tretschk,\nW. Yifan, C. Lassner, V. Sitzmann, R. Martin-Brualla, S.\nLombardi, T. Simon, C. Theobalt, M. Nie\u00dfner, J. T. Barron,\nG. Wetzstein, M. Zollh\u00a8ofer, and V. Golyanik. Advances in\nNeural Rendering. Computer Graphics Forum (Eurograph-\nics State of the Art Reports), 2022. 2, 8\n[78] Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael\nZollh\u00a8ofer, Christoph Lassner, and Christian Theobalt. Non-\nrigid neural radiance fields: Reconstruction and novel view\nsynthesis of a dynamic scene from monocular video. In In-\nternational Conference on Computer Vision (ICCV), 2021.\n1, 2, 4, 5, 6, 7, 13, 17\n[79] Edith Tretschk, Navami Kairanda, Mallikarjun B R, Rishabh\nDabral, Adam Kortylewski, Bernhard Egger, Marc Haber-\nmann,\nPascal Fua,\nChristian Theobalt,\nand Vladislav\nGolyanik. State of the art in dense monocular non-rigid 3d\nreconstruction, 2023. 2, 4\n[80] Tony Tung, Shohei Nobuhara, and Takashi Matsuyama.\nComplete multi-view reconstruction of dynamic scenes from\nprobabilistic fusion of narrow and wide baseline stereo. In\nInternational Conference on Computer Vision (ICCV), 2009.\n2\n[81] Sundar Vedula, Simon Baker, Peter Rander, Robert Collins,\n11\nand Takeo Kanade. Three-dimensional scene flow. In Inter-\nnational Conference on Computer Vision (ICCV), 1999. 2\n[82] Christoph Vogel, Konrad Schindler, and Stefan Roth.\n3d\nscene flow estimation with a piecewise rigid scene model.\nInternational Journal of Computer Vision (IJCV), 2015. 2\n[83] Liao Wang, Jiakai Zhang, Xinhang Liu, Fuqiang Zhao, Yan-\nshun Zhang, Yingliang Zhang, Minye Wu, Jingyi Yu, and\nLan Xu. Fourier plenoctrees for dynamic radiance field ren-\ndering in real-time. In Computer Vision and Pattern Recog-\nnition (CVPR), 2022. 2\n[84] Shangzhe Wu, Tomas Jakab, Christian Rupprecht, and An-\ndrea Vedaldi. DOVE: Learning deformable 3d objects by\nwatching videos. arXiv preprint arXiv:2107.10844, 2021. 2\n[85] Wenqi Xian, Jia-Bin Huang, Johannes Kopf, and Changil\nKim. Space-time neural irradiance fields for free-viewpoint\nvideo. In Computer Vision and Pattern Recognition (CVPR),\n2021. 1, 2\n[86] Gengshan Yang, Minh Vo, Neverova Natalia, Deva Ra-\nmanan, Vedaldi Andrea, and Joo Hanbyul. Banmo: Building\nanimatable 3d neural models from many casual videos. In\nComputer Vision and Pattern Recognition (CVPR), 2022. 2\n[87] Jae Shin Yoon, Ziwei Li, and Hyun Soo Park. 3d semantic\ntrajectory reconstruction from 3d pixel continuum. In Com-\nputer Vision and Pattern Recognition (CVPR), 2018. 2\n[88] Jae Shin Yoon, Kihwan Kim, Orazio Gallo, Hyun Soo Park,\nand Jan Kautz. Novel view synthesis of dynamic scenes with\nglobally coherent depths from a monocular camera. In Com-\nputer Vision and Pattern Recognition (CVPR), 2020. 2\n[89] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.\npixelNeRF: Neural radiance fields from one or few images.\nIn Computer Vision and Pattern Recognition (CVPR), 2021.\n8\n[90] Hsiao yu Chen, Edith Tretschk, Tuur Stuyck, Petr Kadlecek,\nLadislav Kavan, Etienne Vouga, and Christoph Lassner. Vir-\ntual elastic objects. In Computer Vision and Pattern Recog-\nnition (CVPR), 2022. 1\n[91] Mingliang Zhai, Xuezhi Xiang, Ning Lv, and Xiangdong\nKong. Optical flow and scene flow estimation: A survey.\nPattern Recognition, 2021. 2\n[92] Li Zhang, Brian Curless, and Steven M Seitz. Spacetime\nstereo: Shape recovery for dynamic scenes. In Computer\nVision and Pattern Recognition (CVPR), 2003. 2\n[93] Ruo Zhang and Ping-Sing Tsai. Shape-from-shading: a sur-\nvey. Transactions on Pattern Analysis and Machine Intelli-\ngence (TPAMI), 1999. 8\n[94] Xiuming Zhang, Pratul P Srinivasan, Boyang Deng, Paul De-\nbevec, William T Freeman, and Jonathan T Barron. Ner-\nfactor: Neural factorization of shape and reflectance under\nan unknown illumination. ACM Transactions on Graphics,\n2021. 8\n[95] Fuqiang Zhao, Yuheng Jiang, Kaixin Yao, Jiakai Zhang, Liao\nWang, Haizhao Dai, Yuhui Zhong, Yingliang Zhang, Minye\nWu, Lan Xu, et al. Human performance modeling and ren-\ndering via neural animated mesh.\nACM Transactions on\nGraphics (Proceedings of SIGGRAPH Asia), 2022. 2\n[96] Zhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P.\nSimoncelli. Image quality assessment: from error visibility\nto structural similarity. IEEE Transactions on Image Pro-\ncessing, 2004. 8\n[97] Michael Zollh\u00a8ofer,\nMatthias Nie\u00dfner,\nShahram Izadi,\nChristoph Rehmann, Christopher Zach, Matthew Fisher,\nChenglei Wu, Andrew Fitzgibbon, Charles Loop, Christian\nTheobalt, and Marc Stamminger.\nReal-time non-rigid re-\nconstruction using an rgb-d camera. ACM Transactions on\nGraphics, 2014. 2\n[98] Michael Zollh\u00a8ofer, Patrick Stotko, Andreas G\u00a8orlitz, Chris-\ntian Theobalt, Matthias Nie\u00dfner, Reinhard Klein, and An-\ndreas Kolb. State of the art on 3d reconstruction with rgb-d\ncameras. 2018. 2\n12\nSupplementary Material\nWe provide details on how we visualize correspondences\nin Sec. A, a description of each scene in the dataset in\nSec. B, per-scene quantitative novel-view-synthesis results\nin Sec. C, novel-view-synthesis comparisons with D-NeRF\nand DeVRF in Sec. D, more qualitative joint-tracking and\nnovel-view results in Sec. E, quantitative ablation results in\nSec. F, further architecture and training details in Sec. G,\ndetails on how we adapt the baselines to joint tracking in\nSec. H, and more details on the foreground masks used for\nevaluation in Sec. I.\nA. Correspondence Visualization Details\nWe follow NR-NeRF\u2019s [78] visualization and replace the\nappearance in the canonical model with a voxel grid of RGB\ncubes. Like prior work [59, 78], we pick that sample i\u2032 of\nthe ray as the surface point r(si\u2032) that is closest to an ac-\ncumulated transmittance Pi\u2032\ni=1 wi of 0.5. For better visual-\nization, we filter rays with a total accumulated transmittance\nbelow 0.4 (i.e. those hitting the background) and visualize\nthem as transparent. Fig. 11 shows an example.\nB. Data Details\nTab. 3 contains a description and the length in frames of\neach scene. We record at 25fps.\nC. Per-Scene Quantitative Results\nWe collect the quantitative results per scene for novel-view\nsynthesis in Tab. 4.\nD.\nNovel-View-Synthesis\nComparison\nwith\nPrior Work\nTab. 5 contains further comparisons, without Seq. 8 due\nto memory constraints. SceNeRFlow outperforms D-NeRF\nand DeVRF, which both fail on large motion.\nE. More Qualitative Joint-Tracking and Novel-\nView Results\nFig. 12 contains the qualitative joints estimated at t=T for\nall sequences not shown in the main paper. Fig. 13 and\nFig. 14 contain more novel-view results of all methods for\nfour scenes at t= T\n2 .\nF. Quantitative Ablation Results\nTab. 6 confirms the qualitative ablation results quantita-\ntively.\nWith Filtering\nWithout Filtering\nFigure 11. Filtering for Correspondence Visualization. For bet-\nter visualization, we filter out rays with an accumulated transmit-\ntance below 0.4.\nName\nDescription\nFrames\nSeq. 1\ntwo people playing with a plush dog\n125\nSeq. 2\ntwo people holdings hands\n125\nSeq. 3\none person rotating, one person walking\n300\nSeq. 4\na person dancing while wearing a dark dress\n100\nSeq. 5\na person walking like a zombie while wearing a light dress\n125\nSeq. 6\na person playing with a plush dog\n125\nSeq. 7\na person standing with a brown dress\n125\nSeq. 8\na person doing squads (from NR-NeRF [78])\n5\nTable 3. Dataset Description.\nG. Further Architecture and Training Details\nFor all methods, we normalize the scene into the unit cube\nby tightly fitting an axis-aligned bounding box to all near\nand far plane samples of all images.\nG.1. Ours\nArchitecture. Both the coarse and fine deformation fields\nuse the same architecture. The hash grid consists of 16 lev-\nels, with two feature dimensions per level. The coarsest\nlevel has a resolution of 323, and each subsequent level has\na 1.3819 times higher resolution. The hashmap has a size of\n220. The shallow MLP has one hidden layer with 64 hidden\ndimensions.\nThe canonical model uses a hash grid with 13 levels, two\nfeature dimensions per level, a coarsest resolution of 1283,\nand a scaling factor of 1.3819. The shallow MLP that out-\nputs the opacity has one hidden layer with 64 hidden dimen-\nsions. A second MLP outputs the appearance and takes as\ninput a 15-dimensional vector additionally regressed by the\nfirst shallow MLP. The second MLP has two hidden layers\nwith 64 hidden dimensions.\nWeighting Scheme for Smoothness Loss. We weigh sam-\nple i on ray r depending on its closeness to the object.\nMathematically, we start with \u02c6\u03c3r,i = exp(\u2212\u03c3r,i\u03b4), where\n\u03c3r,i is the opacity of the i-th sample on ray r. We next ap-\nply max-pooling with window size k = \u230af \u00b7 S\u230b, where we\nempirically set f=0.005:\n\u02c6\u03c3\u2032\nr,i =\nmax\ni\u2032\u2208[i\u2212k,...,i+k] \u02c6\u03c3r,i\u2032.\n(12)\nWe then weaken the regularization on empty space by\n13\nOurs\nNR-NeRF\nSNF-A\nSNF-AG\nBackground\nSeq. 1\nUnmasked\nPSNR\n\u2191\n27.49\n26.84\n27.84\n26.73\n18.64\nSSIM\n\u2191\n0.928\n0.915\n0.928\n0.906\n0.894\nLPIPS\n\u2193\n0.074\n0.117\n0.076\n0.138\n0.139\nMasked\nPSNR\n\u2191\n29.73\n29.38\n30.02\n29.89\n\u2014\nSSIM\n\u2191\n0.970\n0.966\n0.971\n0.970\n\u2014\nLPIPS\n\u2193\n0.021\n0.036\n0.017\n0.018\n\u2014\nSeq. 2\nUnmasked\nPSNR\n\u2191\n27.93\n21.64\n28.24\n26.99\n18.57\nSSIM\n\u2191\n0.929\n0.875\n0.929\n0.907\n0.898\nLPIPS\n\u2193\n0.069\n0.183\n0.074\n0.136\n0.130\nMasked\nPSNR\n\u2191\n29.65\n22.75\n30.23\n29.96\n\u2014\nSSIM\n\u2191\n0.970\n0.931\n0.972\n0.970\n\u2014\nLPIPS\n\u2193\n0.019\n0.081\n0.017\n0.019\n\u2014\nSeq. 3\nUnmasked\nPSNR\n\u2191\n27.72\n21.48\n27.50\n26.61\n18.94\nSSIM\n\u2191\n0.923\n0.874\n0.920\n0.904\n0.895\nLPIPS\n\u2193\n0.075\n0.181\n0.089\n0.136\n0.140\nMasked\nPSNR\n\u2191\n28.95\n22.25\n28.90\n28.47\n\u2014\nSSIM\n\u2191\n0.961\n0.925\n0.961\n0.958\n\u2014\nLPIPS\n\u2193\n0.029\n0.089\n0.029\n0.033\n\u2014\nSeq. 4\nUnmasked\nPSNR\n\u2191\n31.59\n31.79\n31.92\n32.08\n24.68\nSSIM\n\u2191\n0.950\n0.948\n0.951\n0.949\n0.943\nLPIPS\n\u2193\n0.034\n0.055\n0.036\n0.044\n0.070\nMasked\nPSNR\n\u2191\n33.68\n33.67\n34.15\n34.41\n\u2014\nSSIM\n\u2191\n0.980\n0.978\n0.981\n0.981\n\u2014\nLPIPS\n\u2193\n0.011\n0.031\n0.012\n0.014\n\u2014\nSeq. 5\nUnmasked\nPSNR\n\u2191\n32.25\n31.73\n32.58\n32.77\n23.33\nSSIM\n\u2191\n0.946\n0.944\n0.947\n0.945\n0.938\nLPIPS\n\u2193\n0.042\n0.057\n0.044\n0.049\n0.076\nMasked\nPSNR\n\u2191\n34.08\n33.29\n34.49\n34.95\n\u2014\nSSIM\n\u2191\n0.976\n0.973\n0.978\n0.978\n\u2014\nLPIPS\n\u2193\n0.017\n0.033\n0.017\n0.017\n\u2014\nSeq. 6\nUnmasked\nPSNR\n\u2191\n28.52\n26.69\n29.05\n27.70\n19.76\nSSIM\n\u2191\n0.938\n0.917\n0.937\n0.918\n0.916\nLPIPS\n\u2193\n0.060\n0.123\n0.069\n0.120\n0.105\nMasked\nPSNR\n\u2191\n30.24\n28.85\n31.19\n30.99\n\u2014\nSSIM\n\u2191\n0.978\n0.969\n0.980\n0.979\n\u2014\nLPIPS\n\u2193\n0.015\n0.035\n0.013\n0.013\n\u2014\nSeq. 7\nUnmasked\nPSNR\n\u2191\n34.38\n34.79\n35.32\n35.43\n26.08\nSSIM\n\u2191\n0.959\n0.958\n0.960\n0.956\n0.950\nLPIPS\n\u2193\n0.026\n0.032\n0.026\n0.036\n0.057\nMasked\nPSNR\n\u2191\n37.88\n38.01\n39.33\n39.83\n\u2014\nSSIM\n\u2191\n0.991\n0.990\n0.992\n0.992\n\u2014\nLPIPS\n\u2193\n0.005\n0.007\n0.003\n0.004\n\u2014\nSeq. 8\nUnmasked\nPSNR\n\u2191\n32.67\n35.18\n34.58\n34.38\n23.53\nSSIM\n\u2191\n0.937\n0.942\n0.944\n0.941\n0.926\nLPIPS\n\u2193\n0.046\n0.041\n0.042\n0.047\n0.088\nMasked\nPSNR\n\u2191\n34.82\n38.17\n38.13\n37.26\n\u2014\nSSIM\n\u2191\n0.980\n0.984\n0.986\n0.984\n\u2014\nLPIPS\n\u2193\n0.022\n0.017\n0.013\n0.018\n\u2014\nTable 4.\nNovel-View Synthesis.\nWe report per-scene PSNR,\nSSIM, and LPIPS.\nSceNeRFlow\nD-NeRF\nDeVRF\nUnmasked\nPSNR\n\u2191\n29.98\n24.89\n25.24\nSSIM\n\u2191\n0.939\n0.913\n0.905\nLPIPS\n\u2193\n0.054\n0.107\n0.125\nMasked\nPSNR\n\u2191\n32.03\n26.13\n27.66\nSSIM\n\u2191\n0.975\n0.951\n0.957\nLPIPS\n\u2193\n0.017\n0.060\n0.041\nTable 5. Novel-View Synthesis. Mean PSNR, SSIM, and LPIPS\nacross scenes, except for Seq. 8 (due to memory limitations from\nthe high resolution).\nSceNeRFlow\nNo Online\nNo Extend\nNo Coarse\nNo Fine\nUnmasked\nPSNR\n\u2191\n30.32\n30.36\n28.53\n30.40\n28.95\nSSIM\n\u2191\n0.939\n0.929\n0.927\n0.936\n0.933\nLPIPS\n\u2193\n0.054\n0.074\n0.067\n0.056\n0.056\nMasked\nPSNR\n\u2191\n32.38\n32.61\n30.20\n32.45\n30.45\nSSIM\n\u2191\n0.976\n0.974\n0.966\n0.974\n0.970\nLPIPS\n\u2193\n0.018\n0.020\n0.027\n0.020\n0.021\nMPJPE\n\u2193\n1.5\n33.4\n15.6\n10.4\n1.5\nTable 6.\nAblations.\nMean PSNR, SSIM, LPIPS, and MPJPE\nacross scenes.\nOurs\nPREF\nNR-NeRF\nSeq. 1\nSeq. 2\nSeq. 3\nSeq. 5\nSeq. 7\nFigure 12. Time Consistency. The solid skeleton is the tracking\nestimate at t=T. The dotted skeleton is the pseudo-ground truth\nat t=T.\nu=10:\n\u02c6\u03c3\u2032\u2032\nr,i =\n(\n1\nu \u02c6\u03c3\u2032\nr,i\nif \u02c6\u03c3\u2032\nr,i > u \u00b7 \u02c6\u03c3r,i\n\u02c6\u03c3\u2032\nr,i\nelse.\n(13)\n14\nGround Truth\nOurs\nNR-NeRF\nSNF-A\nSNF-AG\nView 1\nView 2\nView 1\nView 2\nFigure 13. Novel-View Synthesis. (First row) Seq. 3 at t= T\n2 . (Second row) Seq. 4 at t= T\n2 .\nFinally, we weaken the regularization on very small offsets\n\u2206 \u2208 R3 with a soft threshold of st=0.001:\n\u02c6\u03c3\u2032\u2032\u2032\nr,i(\u2206) = sig\n\u00104\u2225\u2206\u22252\nst\n\u2212 2\n\u0011\n\u02c6\u03c3\u2032\u2032\nr,i,\n(14)\nwhere sig(x)=\n1\n1+exp(\u2212x) is the sigmoid function. We thus\nhave:\nLnorm,w =\n1\nRS\nX\nr\nX\ni\n\u02c6\u03c3\u2032\u2032\u2032\nr,i(\u2206)Ee\nh\f\f\u2225J\u22a4\nrr(si)e\u22252 \u2212 1\n\f\f\ni\n,\n(15)\nwhere \u2206=\u2206c(rr(si)) when regularizing the coarse defor-\nmations, and \u2206=\u2206f(dc(rr(si))) when regularizing the fine\ndeformations.\nLearning Rates. We use exponential decay for the learning\nrates when constructing the canonical model and for each\ntimestamp t>1. For t=1, we decay by a factor of 0.01\nfor all parameters. For t>1, we decay the coarse defor-\nmations by 0.1 when they get optimized, and the fine defor-\nmations by 0.1 when they get optimized. All parameters of\nthe canonical model have an initial learning rate of 10\u22122.\nAll deformation parameters have an initial learning rate of\n10\u22123. The vignetting parameters have an initial learning\nrate of 10\u22122 and are the only parameters with no weight\ndecay.\nDuring the first 1000 iterations when building the canon-\nical model, we warm up the learning rates with an additional\nfactor that exponentially increases from 0.01 to 1.\nG.2. Variants\nWe compare against two variants of our method that use\ntime-varying canonical models and thus neglect correspon-\ndences.\n15\nGround Truth\nOurs\nNR-NeRF\nSNF-A\nSNF-AG\nView 1\nView 2\nView 1\nView 2\nFigure 14. Novel-View Synthesis. (First row) Seq. 7 at t= T\n2 . (Second row) Seq. 8 at t=T.\nThe first variant, SNF-A, allows the appearance but not\nthe geometry of the canonical model to change for t>1. We\nimplement this via a separate appearance model that has\nthe same HashMLP architecture as the standard canonical\nmodel. We can then keep the standard canonical model,\nwhich now only predicts the geometry, fixed for t>1 while\nallowing the appearance to vary.\nThe second variant, SNF-AG, has time-varying appear-\nance and geometry.\nWe thus use the standard canonical\nmodel. We apply the geometry regularization losses Lback\nand Lhard to the canonical model at all timestamps t\u22651.\nFor both variants, we seek to explain as much of the re-\nconstruction as possible via the deformations, such that the\ncanonical model needs to change as little as possible. We\nthus split the 10k iterations per timestamp into three equally\nlong phases: only coarse deformations (as before), then\n16\nonly fine deformations (as before), then only the canonical\nmodel. I.e., the canonical model gets optimized only during\nthe third phase, when the deformations are fixed.\nWe found these variants unstable to train, with frequent\ndivergence, and the following remedies helped: The canon-\nical hash grid(s) use the settings from Instant NGP [53] (a\ncoarsest resolution of 16, with the resolution of each finer\nlevel being 1.5 times finer than the previous level, with a\ntotal of 16 levels). During the third phase, we use Instant\nNGP\u2019s Huber loss with a threshold of 0.1 as reconstruction\nloss instead of our \u21131 reconstruction loss. We use an expo-\nnentially decaying learning rate for the third phase that goes\nfrom 10\u22122 to 10\u22123 (except for the 300-frame Seq. 3, where\nwe use a ten times lower learning rate for long-term training\nstability).\nG.3. NR-NeRF\nWe adapt NR-NeRF [78] to our settings as follows: Since\nwe provide background images to NR-NeRF, we do not use\nits rigidity network. Furthermore, due to our large-motion\nsetting, we remove its offsets loss that encourages the de-\nformations to remain small. To keep runtime reasonable,\nwe apply pruning. Doing so also enables us to always sam-\nple densely and we thus do not apply hierarchical sampling.\nWe also use foreground-focused batches and vignetting cor-\nrection. Since the recommended [78] training time of 200k\niterations is only shown for very short scenes, we instead\ntrain NR-NeRF for longer on our scenes. Specifically, we\nextend NR-NeRF\u2019s training time to that of our method and\nthus train NR-NeRF for one third of the number of itera-\ntions of our method. We train Seq. 8 for 200k iterations, as\nthat is longer.\nG.4. PREF\nLike for NR-NeRF, we also supply background images, ap-\nply pruning, use foreground-focused batches, and learn the\nvignetting correction. Since we follow the authors of PREF\nand split our long scenes into chunks of 25 frames (see\nSec. H), we keep the training time the same.\nH. Joint Evaluation Details\nWe provide details on how we adapt PREF and NR-NeRF\nto long-term 3D joint tracking.\nPREF. To train on a longer sequence, PREF [71] splits the\nsequence into chunks of 25 frames and trains on each chunk\nindependently. We do the same with our scenes. To obtain\nlong-term correspondences across chunks, we overlap the\nchunks for three frames.\nNR-NeRF. Unlike SceNeRFlow, NR-NeRF\u2019s canonical\nmodel does not coincide with the world space at t=1. We\ntherefore cannot use {\u02c6p1\nj}j directly as the target canoni-\ncal positions. Instead, we apply the backward deformation\nmodel at t=1 to {\u02c6p1\nj}j to obtain their positions in canonical\nspace. We then have the joint positions in canonical space\nand the world-space positions at t=1, which allows us to\napply the same tracking procedure as for SceNeRFlow.\nI. Foreground Masks for Evaluation\nSince we did not tune the variants beyond making their\ntraining stable, they exhibit some significant undesired arti-\nfacts in empty space that we did not try to remove. In addi-\ntion to scores on the full images, we therefore also compute\nscores that are focused on the actual dynamic object of in-\nterest. To this end, we use foreground masks.\nDuring training, we use very coarse and inaccurate fore-\nground masks. However, we use more accurate foreground\nmasks when computing masked scores during evaluation.\nTo also consider reconstructions that are slightly off, these\nmasks are dilated to include the areas surrounding the dy-\nnamic foreground in pixel space. Fig. 15 shows example\nmasks.\nWe use the following procedure to obtain these more\naccurate foreground masks.\nWe first compute two fore-\nground masks separately: (1) mb using background sub-\ntraction with a threshold \u2206t followed by a morphological\nopening (i.e., first erosion, then dilation) of the foreground,\nand (2) m\u03c3. m\u03c3 very roughly detects shadows by deter-\nmining whether the pixel in Ic,t is a scaled version of the\nbackground pixel in Bc (i.e., whether a na\u00a8\u0131ve brightness\nchange of the background has occurred). To this end, we\nfirst divide each of the three channels of the pixel in Ic,t\nby the respective channel of the background pixel. We then\ntake the standard deviation of the resulting three factors and\nthreshold it at \u03c3t. We finally apply an opening to obtain the\nfinal m\u03c3. Since we use a very generous opening for both\nmasks, we combine them into a single foreground mask by\nconsidering those pixels foreground that are foreground in\nboth masks. We manually tune \u2206t, \u03c3t, and the opening\nsizes for each sequence.\n17\nView 1, t=1\nView 2, t=T\nGround Truth\nMask\nGround Truth\nMask\nSeq. 1\nSeq. 2\nSeq. 3\nSeq. 4\nSeq. 5\nSeq. 6\nSeq. 7\nSeq. 8\nFigure 15. Foreground Masks for Evaluation.\n18\n"
  }
]