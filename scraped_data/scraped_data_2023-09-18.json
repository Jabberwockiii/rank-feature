[
  {
    "title": "Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers",
    "link": "https://arxiv.org/pdf/2309.08532.pdf",
    "upvote": "50",
    "text": "Published as a conference paper at ICLR 2024\nCONNECTING LARGE LANGUAGE MODELS WITH EVO-\nLUTIONARY ALGORITHMS YIELDS POWERFUL PROMPT\nOPTIMIZERS\nQingyan Guo12\u2020\u2217, Rui Wang2\u2020, Junliang Guo2, Bei Li23, Kaitao Song2, Xu Tan2\u2021,\nGuoqing Liu2, Jiang Bian2, Yujiu Yang1\u2021\n1Tsinghua University\n2Microsoft Research\n3Northeastern University\ngqy22@mails.tsinghua.edu.cn, libei_neu@outlook.com,\n{ruiwa,junliangguo,kaitaosong,xuta,guoqingliu,jiabia}@microsoft.com\nyang.yujiu@sz.tsinghua.edu.cn\nABSTRACT\nLarge Language Models (LLMs) excel in various tasks, but they rely on carefully\ncrafted prompts that often demand substantial human effort. To automate this\nprocess, in this paper, we propose a novel framework for discrete prompt opti-\nmization, called EVOPROMPT, which borrows the idea of evolutionary algorithms\n(EAs) as they exhibit good performance and fast convergence. To enable EAs\nto work on discrete prompts, which are natural language expressions that need\nto be coherent and human-readable, we connect LLMs with EAs. This approach\nallows us to simultaneously leverage the powerful language processing capabil-\nities of LLMs and the efficient optimization performance of EAs. Specifically,\nabstaining from any gradients or parameters, EVOPROMPT starts from a popula-\ntion of prompts and iteratively generates new prompts with LLMs based on the\nevolutionary operators, improving the population based on the development set.\nWe optimize prompts for both closed- and open-source LLMs including GPT-3.5\nand Alpaca, on 31 datasets covering language understanding, generation tasks, as\nwell as BIG-Bench Hard (BBH) tasks. EVOPROMPT significantly outperforms\nhuman-engineered prompts and existing methods for automatic prompt generation\n(e.g., up to 25% on BBH). Furthermore, EVOPROMPT demonstrates that connect-\ning LLMs with EAs creates synergies, which could inspire further research on\nthe combination of LLMs and conventional algorithms. Our code is available at\nhttps://github.com/beeevita/EvoPrompt.\n1\nINTRODUCTION\nLarge language models (LLMs) show remarkable performance on multiple natural language pro-\ncessing (NLP) tasks (Touvron et al., 2023; Ouyang et al., 2022). To adapt to downstream tasks,\nsimply adding an instruction to the input text, also called discrete prompt, steers LLMs to carry out\nthe desired task with negligible impact on computational cost (Liu et al., 2023). Such approach\nalso eliminates the need for all the parameters and gradients in LLMs, making it suitable for LLMs\nwith block-box APIs such as GPT-3 and GPT-4 (Brown et al., 2020; OpenAI, 2023). Despite the\nconvenience, the performance of the LLMs towards a certain task is significantly influenced by the\nprompt (Liu et al., 2023; Zhu et al., 2023). Accordingly, the key challenge of this approach lies in the\ndesign of the prompt, which has emerged as a crucial technique known as prompt engineering (Zhou\net al., 2022). Given the wide variation in prompts across language models and tasks, the prompt\ndesign typically requires substantial human effort and expertise with subjective and relatively limited\nguidelines (Mishra et al., 2022a;b; Liu et al., 2023; Zamfirescu-Pereira et al., 2023; Wang et al.,\n2023).\n\u2217Work done during an internship at Microsoft Research Asia.\n\u2020Equal Contribution.\n\u2021Corresponding Author.\n1\narXiv:2309.08532v2  [cs.CL]  27 Feb 2024\nPublished as a conference paper at ICLR 2024\nTo alleviate human effort on discrete prompt design, previous approaches usually rely on access to\nthe token probabilities from the output layer of LLMs, which may not always be accessible through\nAPIs (Deng et al., 2022; Zhang et al., 2023a). Some recent works consider enumerating diverse\nprompts and selecting the best ones (Zhou et al., 2022; Jiang et al., 2020), or modifying current\nprompts to improve them (Guo et al., 2023; Prasad et al., 2022; Pryzant et al., 2023). Such approaches\neither emphasize exploring diverse prompts, which may lead to indecisiveness and wasted resources,\nor focus on exploiting upon the current identified good prompts, which may result in stagnation and\nconfine the search to local optima. Several conventional derivative-free algorithms are well-designed\nand strike a good balance between exploration and exploitation (Conn et al., 2009; Rios & Sahinidis,\n2013). Among these, evolutionary algorithms (EAs) stand out as they are simple and efficient, as\nwell as suitable for discrete prompt optimization (Storn & Price, 1997; Brest et al., 2006; Zhang &\nSanderson, 2009; Vesterstrom & Thomsen, 2004). Sequences of phrases in prompts can be regarded\nas gene sequences in typical EAs, making them compatible with the natural evolutionary process.\nIn this paper, we borrow the idea of EAs and propose a discrete prompt tuning framework, EVO-\nPROMPT. While evolutionary operators in EAs are typically designed for sequences, they tend to\nindependently alter tokens to generate new candidate solutions. Unfortunately, this approach ignores\nthe connections among tokens, which is crucial for maintaining coherence and readability in prompts.\nTaking advantage of LLMs\u2019 expertise in NLP and the exceptional optimization capabilities of EAs, we\nconnect these two approaches, where LLMs generate new candidate prompts following evolutionary\noperators, and EAs guide the optimization process to retain the optimal prompts.\nSpecifically, based on several initial prompts, we utilize LLMs to act as evolutionary operators to\ngenerate new prompt candidates, and the prompt with better performance on the development set\nis preserved. The above operations upon the updating population are iteratively applied to improve\nthe quality. By elaborately designing the evolutionary operators and adjusting the update strategy,\nEVOPROMPT can be instantiated with various types of EAs. We optimize the prompts for two\ndifferent LLMs (i.e., Alpaca (Taori et al., 2023), and GPT-3.5 (Brown et al., 2020)) on a diverse\nrange of neural language understanding and generation tasks, as well as challenging BIG-Bench\ntasks, using a total of 31 datasets. EVOPROMPT consistently gets better prompts compared with both\nmanually designed ones and previous automatic prompt generation methods. The main contributions\nof this paper include:\n\u2022 We propose a novel framework for automatic discrete prompt optimization connecting LLMs and\nEAs, called EVOPROMPT, which enjoys the following advantages: 1) It does not require access to\nany parameters or gradients of LLMs; 2) It strikes a balance between exploration and exploitation\nleading to better results; 3) The generated prompts are human-readable.\n\u2022 Experiments conducted on 31 datasets demonstrate the effectiveness of EVOPROMPT compared\nwith crafted prompts, as well as existing methods. We release the optimal prompts obtained\nby EVOPROMPT for these common tasks such as sentiment classification, topic classification,\nsubjectivity classification, simplification, summarization and reasoning.\n\u2022 We demonstrate that LLMs are capable of implementing multiple types of EAs provided with\nappropriate instructions. We hope that our explorations will inspire further investigations on\nthe combination of LLMs and conventional algorithms, paving the way for new and innovative\napplications of LLMs.\n2\nRELATED WORKS\nPrompts in LLMs\nPrompting is an efficient method for employing LLMs in specialized tasks.\nHowever, the performance is heavily influenced by the choice of the prompt. Recently, automatic\nprompt optimization has obtained wide attention. Continuous prompt-based methods, which only\ntune parameters of some input tokens (Li & Liang, 2021; Liu et al., 2021b;a; Zhang et al., 2021)\nattract lots of attention. In spite of their effective performance, two drawbacks of such paradigms\ncan not be ignored: 1) The optimization of continuous prompts requires parameters of LLMs that\nare inaccessible for black-box APIs. 2) Soft prompts often fall short of interpretability (Lester\net al., 2021). Discrete prompts, simply adding several discrete tokens, such as \u201cIt was\u201d (Schick &\nSch\u00fctze, 2021), or task-specific descriptive instructions, such as \u201cClassify the comment into positive\nor negative.\u201d, to the input text, can offer an interactive interface to humans with better interpretability\nand show promising performance in various NLP tasks (Liu et al., 2023).\n2\nPublished as a conference paper at ICLR 2024\nDiscrete Prompts\nVarious approaches have been proposed for automatic discrete prompt searching\nand generation (Shin et al., 2020; Shi et al., 2022; Wallace et al., 2019; Deng et al., 2022; Zhang et al.,\n2023a), while these methods still rely on the gradients or the token probabilities from the output layer.\nMore recently, considering the high variance of different prompts for downstream tasks, some works\nfocus on exploration by enumerating and selecting the best prompt from a number of candidates,\nmainly augmented by re-sampling (Zhou et al., 2022; Jiang et al., 2020). Approaches based on\nprompt edit (Zhang et al., 2023a; Prasad et al., 2022) emphasize exploitation, which may potentially\nlead to local optima. Another approach collects the incorrectly predicted cases and analyzes the\ncorresponding root cause to improve existing prompts (Pryzant et al., 2023; Guo et al., 2023), which\nalso emphasizes exploitation. Additionally, such approaches are constrained to tasks with standard\nanswers and cannot be directly applied to generation tasks. Our proposed EVOPROMPT empowered\nwith evolutionary algorithms strikes a balance between exploration and exploitation without requiring\nany parameters or gradients.\nLLMs and Optimization Algorithms\nLLMs demonstrate the potential to serve as black-box\noptimizers (Zheng et al., 2023); however, this black-box approach lacks explainability. Some works\nhave revealed that LLMs have the capability to imitate specific operations in conventional algorithms.\nFor instance, LLMs can perform \u201cGradient Descent\u201d in discrete space by collecting incorrectly\npredicted samples (Pryzant et al., 2023; Guo et al., 2023). Meanwhile, it has been demonstrated\nthat LLMs can imitate the mutation (Lehman et al., 2022) or crossover (Meyerson et al., 2023)\noperator in the genetic algorithm (GA). Chen et al. (2023) further integrates LLMs and GA for\nneural architecture search, while Lanzi & Loiacono (2023) introduce a similar approach to game\ndesign. Our work has taken a significant step forward by proposing a general framework that connects\nLLMs with evolutionary algorithms, which can be instantiated to a diverse range of evolutionary\nalgorithms through customization of evolutionary and selection processes, thereby broadening its\napplicability and potential influence in the domain. We aspire this work to inspire broader applications\nof combining LLMs and conventional algorithms.\n3\nAUTOMATIC DISCRETE PROMPT OPTIMIZATION\nAlgorithm 1 Discrete prompt optimization: EVOPROMPT\nRequire: Initial prompts P0 = {p1, p2, . . . , pN}, size of population N, a dev set D, fD(\u00b7) denotes\nthe score of a prompt on the desired LLM evaluated on D, a pre-defined number of iterations T,\ncarefully designed evolutionary operators to generate a new prompt Evo(\u00b7)\n1: Initial evaluation scores: S0 \u2190 {si = fD(pi)|i \u2208 [1, N]}\n2: for t = 1 to T do\n3:\nSelection: select a certain number of prompts from current population as parent prompts\npr1, . . . , prk \u223c Pt\u22121\n4:\nEvolution: generate a new prompt based on the selected parent prompts by leveraging LLM\nto perform evolutionary operators p\u2032\ni \u2190 Evo(pr1, . . . , prk)\n5:\nEvaluation: s\u2032\ni \u2190 f(p\u2032\ni, D)\n6:\nUpdate: Pt \u2190 {Pt\u22121, p\u2032\ni} and St \u2190 {St\u22121, s\u2032\ni} based on the evaluation scores\n7: end for\n8: Return the best prompt, p\u2217, among the final population PT : p\u2217 \u2190 argmaxp\u2208PT f(p, D)\nCurrent advanced LLMs are typically interacted via black-box APIs, while the gradients and parame-\nters are inaccessible. Evolutionary algorithms (EAs) are derivative-free algorithms with exceptional\naccuracy and rapid convergence. Accordingly, we consider introducing EAs into discrete prompt\noptimization. However, to generate new candidate solutions, evolutionary operators typically edit the\nelements in current solutions independently, without considering the connections between them. This\nmakes it challenging to apply evolutionary operators on discrete prompts, which require coherence\nand readability. To address this challenge, we propose a synergistic approach that connects the\nnatural language processing expertise of LLMs with the optimization capabilities of EAs, called\nEVOPROMPT. Specifically, LLMs generate new candidate prompts based on evolutionary operators,\nwhile EAs guide the optimization process to find the optimal prompts.\n3\nPublished as a conference paper at ICLR 2024\nGenetic Algorithm (GA) Implemented by LLMs\nQuery: \nPlease follow the instruction step-by-step to generate a better prompt.\n1. Cross over the following prompts and generate a new prompt:\n2. Mutate the prompt generated in Step 1 and generate a final prompt bracketed with \n<prompt> and </prompt>.\nResponse:\nPrompt 2: Assign a sentiment label to the given sentence from ['negative', \n'positive'] and return only the label without any other text.\nPrompt 1: Now you are a categorizer, your mission is to ascertain the \nsentiment of the provided text, either favorable or unfavourable.\n\ud835\udc02\ud835\udc2b\ud835\udc28\ud835\udc2c\ud835\udc2c\ud835\udc28\ud835\udc2f\ud835\udc1e\ud835\udc2b\n1. Crossover Prompt:  Your mission is to ascertain the sentiment of the \nprovided text and assign a sentiment label from ['negative', 'positive\u2019]. \n2. <prompt>Determine the sentiment of the given sentence and assign a label \nfrom ['negative', 'positive'].</prompt>\n\ud835\udc0c\ud835\udc2e\ud835\udc2d\ud835\udc1a\ud835\udc2d\ud835\udc1e\nFigure 1: GA process implemented by LLMs (Evo(\u00b7) in Algorithm 1). In Step 1, LLMs perform\ncrossover on the given two prompts (words in orange and blue are inherited from Prompt 1 and\nPrompt 2, respectively). In Step 2, LLMs perform mutation on the prompt.\nIn order to implement EVOPROMPT in practice, it is necessary to instantiate it with a specific\nalgorithm of EAs. There are various types of EAs, and in this paper, we consider two widely\nused algorithms, including Genetic Algorithm (GA) (Holland, 1975) and Differential Evolution\n(DE) (Storn & Price, 1997). GA is among the most highly regarded evolutionary algorithms (Holland,\n1975; 1992; Mitchell, 1998; Mirjalili et al., 2020) and DE has emerged as one of the most widely\nutilized algorithms for complex optimization challenges since its inception (Storn & Price, 1997;\nPrice, 2013; Das & Suganthan, 2010; Pant et al., 2020). In the following, we will first outline the\nproposed EVOPROMPT, and then instantiate EVOPROMPT with GA and DE respectively.\n3.1\nFRAMEWORK OF EVOPROMPT\nEAs typically start with an initial population of N solutions (prompts in our setting), then iteratively\ngenerate new solutions using evolutionary operators (e.g., mutation and crossover) on the current\npopulation and update it based on a fitness function. Following typical EAs, EVOPROMPT mainly\ncontains three steps:\n\u2022 Initial population: Contrary to most existing automatic prompt methods that neglect priori human\nknowledge, we apply available manual prompts as the initial population to leverage the wisdom\nof humans. Besides, EAs typically start from random solutions, resulting in a diverse population\nand avoiding being trapped in a local optimum. Accordingly, we also introduce some prompts\ngenerated by LLMs (Zhou et al., 2022) into the initial population.\n\u2022 Evolution: In each iteration, EVOPROMPT uses LLMs as evolutionary operators to generate a new\nprompt based on several parent prompts selected from the current population. To accomplish this,\nwe design steps of the mutation and crossover operators for each specific type of EAs, along with\ncorresponding instructions to guide the LLMs in generating new prompts based on these steps.\n\u2022 Update: We evaluate the generated candidate prompts on a development set and retain those with\nsuperior performance, similar to the survival of the fittest in nature. The specific updating strategy\nmay vary depending on the type of EAs used.\nThe algorithm stops when the number of iterations reaches a predefined value. The details of\nEVOPROMPT are outlined in Algorithm 1. When instantiating EVOPROMPT with a specific algorithm\nof EAs, the evolutionary processes need to be adjusted, and the key challenge is to design the\nevolutionary operators on discrete prompts.\n4\nPublished as a conference paper at ICLR 2024\n\u201ctweet\u201d -> \u201creview\u201d\n\u201cCategorize\u201d-> \u201cAnalyze\u201d\n\u201cSentiment analysis\u201d -> \u201cSentiment identification\u201d\nDifferential Evolution (DE) Algorithm Implemented by LLMs\n\ud835\udc83 \u2212 \ud835\udc84\nQuery: \nPlease follow the instruction step-by-step to generate a better prompt.\n1. Identify the different parts between the Prompt 1 and Prompt 2:\nNew Prompt: In this task, you are given reviews about products. The task is to \nanalyze each review and identify if it is positive or negative.\nFinal Prompt: <prompt>Here, you'll be given reviews about products and you'll \nneed to analyze each review and identify if it is positive or negative.</prompt>\nPrompt 1: Categorize the tweet according to if it has a positive or negative sentiment.\nPrompt 2: Carry out sentiment analysis for every sentence to decide if it is positive or \nnegative.\nDifferent parts:\n\"tweet\" vs \"sentence\"\n''Categorize'' vs ''Carry out sentiment analysis''\nPrompt 3: In this task, you are given sentences from product reviews. The task is to \nclassify a sentence as  positive or as negative.\n\ud835\udc02\ud835\udc2b\ud835\udc28\ud835\udc2c\ud835\udc2c\ud835\udc28\ud835\udc2f\ud835\udc1e\ud835\udc2b\n\ud835\udc82 + \ud835\udc6d(\ud835\udc83 \u2212 \ud835\udc84)\n\ud835\udc6d(\ud835\udc83 \u2212 \ud835\udc84)\n2. Randomly mutate the different parts\n3. Combine the different parts with Prompt 3, selectively replace it with the different parts \nin Step 2 and generate a new prompt.\n4. Cross over the prompt in the Step 3 with the following basic prompt and generate a final \nprompt bracketed with <prompt> and </prompt>:\nBasic Prompt: Here, you'll be given sentences from reviews about products and you'll \nneed to decide if it's a positive or a negative review.\nResponse:\n1. \n2. \n3. \n4. \nFigure 2: DE process implemented by LLMs (Evo(\u00b7) in Algorithm 1). In Step 1, LLMs find the\ndifferent parts (words in \u25a0 and \u25a0) between Prompt 1 and Prompt 2 (b \u2212 c in typical DE). In Step 2,\nLLMs perform mutation (words in \u25a0 ) on them (imitation of F(b \u2212 c)). Next, LLMs incorporate\nthe current best prompt as Prompt 3 with the mutated results in Step 2, to generate a new prompt\n(counterpart of a + F(b \u2212 c) in DE). Finally, LLMs perform crossover upon the current basic prompt\npi and the generated prompt in Step 3. See Figure 5 in Appendix B.2 for the complete response.\n3.2\nINSTANTIATION WITH GENETIC ALGORITHM\nSelection\nIn GA, parent solutions are conventionally selected using the roulette wheel selection\nmethod, guided by their fitness values (Lipowski & Lipowska, 2012). Analogously, we employ the\nroulette wheel selection to choose two parent prompts from the current population, based on their\nperformance scores obtained on the development sets. Let si denote the performance score of the i-th\nprompt within a population containing N prompts. The probability of selecting the i-th prompt as a\nparent can be expressed as pi = si/ PN\nj=1 sj.\nEvolution\nConforming to the GA framework, we generate a new candidate prompt via two steps:\n1) Crossover is performed between the parent prompts to produce a new offspring prompt that inherits\ncharacteristics from both parents; 2) Mutation is applied to the offspring prompt, introducing random\nalterations to certain elements. We formalize this two-stage operation into algorithmic instructions\nfor guiding LLMs to implement Evo(\u00b7) in Algorithm 1. The entire process is illustrated in Figure 1.\nUpdate\nWe employ a straightforward selection strategy for updating the population: at each\niteration, EVOPROMPT produces N new prompts, which are merged with the existing population of\nN prompts. Subsequently, the top N prompts, based on their scores, are retained to form the updated\npopulation. Accordingly, the overall quality of the population undergoes continuous enhancement,\nculminating in the selection of the best one within the final population as the optimal prompt.\n5\nPublished as a conference paper at ICLR 2024\n3.3\nINSTANTIATION WITH DIFFERENTIAL EVOLUTION\nHere, we begin with some preliminary knowledge of DE. Unlike GA, the solutions of DE are\nrepresented by numerical vectors. Each vector within the population is sequentially selected as a\nbase vector, denoted as x, which subsequently undergoes mutation and crossover. During mutation, a\nmutated solution y is generated from a randomly selected solution a from the current population. The\nmutation is achieved by adding a scaled difference between two distinct, randomly selected solutions\nb and c to a, i.e., y = a + F(b \u2212 c), where F is the scaled parameter.\nCrossover is to generate a trial solution x\u2032 = [x\u2032\n1, ..., x\u2032\nn] by choosing each parameter in the vector\nfrom either the basic solution x or the mutated solution y. Then, x is replaced with x\u2032 if x\u2032 is better\nthan x. Within step-by-step evolution, DE ends with a population of high quality. A modified version\nof DE uses the current best solution as vector a to exploit information from the best one.\nEvolution\nThe evolutionary process of DE can be decoupled into three steps: 1) F(b \u2212 c); 2)\ny = a + F(b \u2212 c); 3) Crossover of x and y. In EVOPROMPT based on DE, we follow the three steps\nto design the evolutionary process, as well as the corresponding instructions for LLMs to generate a\nnew prompt based on these steps as illustrated in Figure 2:\n\u2022 Inspired by the differential vector in DE, we consider mutating only the different parts of two\nrandomly selected prompts in the current population (Step 1 and Step 2 in Figure 2). The prompts\nin the current population are considered the current best ones. Accordingly, the shared components\nof two prompts tend to have a positive impact on the performance, and thus need to be preserved.\n\u2022 A variant of DE employs the current best vector during the mutation process, where a mutated\nvector is generated by adding the scale of the differential vector to the current best vector. Building\nupon this idea, we generate a mutated prompt by selectively replacing parts of the current best one\nwith the mutated different parts for combination. (Step 3 in Figure 2).\n\u2022 Crossover replaces certain components of a basic prompt (i.e., a candidate of the current population)\nwith segments from the mutated prompt. This operation combines the features of two different\nprompts, potentially creating a new and improved solution (Step 4 in Figure 2).\nUpdate\nFollowing the standard DE, each prompt pi in the current population is chosen as a basic\nprompt in turn to generate a corresponding new prompt p\u2032\ni using the instruction in Figure 2. Then,\nthe prompt with a higher score, either pi or p\u2032\ni, is retained. Accordingly, the population size remains\nconstant while the overall quality of the population is enhanced.\n4\nEXPERIMENTS\n4.1\nIMPLEMENTATION DETAILS AND BASELINES\nWith GPT-3.5 performing evolutionary operators, we optimize prompts using EVOPROMPT for the\nopen-source Alpaca-7b (Taori et al., 2023) and closed-source GPT-3.5 (text-davinci-003) (Brown\net al., 2020). We pick the prompt with the highest score on the development set and report its score on\nthe test set. Results reported on Alpaca are averaged over 3 random seeds and the standard deviation is\nprovided, while for GPT-3.5, we report results of one seed due to budget limitation. In our evaluation,\nwe compare EVOPROMPT against three categories of prompt-based approaches, detailed as follows:\n\u2022 Manual Instructions (MI): These serve as task-specific guidelines and are crafted based on\nestablished works, specifically referenced from Zhang et al. (2023b) for language understanding,\nSanh et al. (2021) for summarization, and Zhang et al. (2023c) for text simplification.\n\u2022 PromptSource (Bach et al., 2022) and Natural Instructions (NI) (Mishra et al., 2022b): These\nrepositories aggregate human-composed prompts across a diverse range of datasets.\n\u2022 APE (Zhou et al., 2022) and APO (Pryzant et al., 2023): APE employs an iterative Monte\nCarlo Search strategy, emphasizing on exploration. We reproduce it and initialize populations\nof equivalent sizes to that of EVOPROMPT. APO harnesses incorrectly predicted instances as\n\u201cpseudo-gradient\u201d to iteratively refine the original prompt, which emphasizes exploitation. We\nreproduce APO on binary classification tasks with the optimal manual prompt as the initial one.\n6\nPublished as a conference paper at ICLR 2024\nMethod\nSST-2\nCR\nMR\nSST-5\nAG\u2019s News\nTREC\nSubj\nAvg.\nMI (Zhang et al., 2023b)\n93.68\n91.40\n88.75\n42.90\n70.63\n50.60\n49.75\n71.07\nNI (Mishra et al., 2022c)\n92.86\n90.90\n89.60\n48.64\n48.89\n55.00\n52.55\n68.21\nPromptSource (Bach et al., 2022)\n93.03\n-\n-\n-\n45.43\n36.20\n-\n-\nAPE (Zhou et al., 2022)\n93.45(0.14)\n91.13(0.45)\n89.98(0.29)\n46.32(0.49)\n71.76(2.81)\n58.73(1.37)\n64.18(0.59)\n73.80\nAPO (Pryzant et al., 2023)\n93.87(0.39)\n91.20(0.04)\n89.85(0.35)\n-\n-\n-\n70.55(1.02)\n-\nEVOPROMPT (GA)\n95.13(0.21)\n91.27(0.06)\n90.07(0.25)\n49.91(0.61)\n72.81(0.61)\n64.00(0.16)\n70.55(2.58)\n76.25\nEVOPROMPT (DE)\n94.75(0.21)\n91.40(0.04)\n90.22(0.09)\n49.89(1.73)\n73.82(0.35)\n63.73(1.54)\n75.55(2.26)\n77.05\nTable 1: Main results on language understanding (accuracy) on Alpaca-7b.\nMethod\nAlpaca\nGPT-3.5\nROUGE-1\nROUGE-2\nROUGE-L\nROUGE-1\nROUGE-2\nROUGE-L\nMI (Sanh et al., 2021)\n35.92\n11.16\n31.67\n43.95\n17.11\n39.09\nAPE (Zhou et al., 2022)\n35.44(0.79)\n10.60(0.38)\n31.80(0.50)\n43.43\n16.72\n38.25\nEVOPROMPT (GA)\n38.46(1.45)\n13.36(0.75)\n34.20(1.40)\n45.22\n18.52\n41.06\nEVOPROMPT (DE)\n39.46(0.51)\n13.93(0.33)\n35.49(0.56)\n46.49\n19.49\n41.96\nTable 2: Main results on SAMSum dataset (summarization task) for Alpaca-7b and GPT-3.5.\n4.2\nLANGUAGE UNDERSTANDING\nDatasets and Settings\nWe first conduct experiments on language understanding tasks across 7\ndatasets to validate our methods, including sentiment classification (SST-2 (Socher et al., 2013),\nMR (PANG, 2005), CR (Hu & Liu, 2004), SST-5 (Socher et al., 2013)), topic classification (AG\u2019s\nNews (Zhang et al., 2015), TREC (Voorhees & Tice, 2000)) and subjectivity classification (Subj (Pang\n& Lee, 2004)). To constrain the output label space, we prepend the demonstration consisting of one\nexample per class before the test case. See Appendix B for more details.\nMain Results\nTable 1, shows that: 1) Compared with previous works on prompt generation and\nhuman written instructions, EVOPROMPT based on both GA and DE delivers significantly better\nresults. 2) EVOPROMPT (GA) is slightly better than EVOPROMPT (DE) on sentiment classification\ndatasets. When it comes to topic classification datasets, EVOPROMPT (DE) performs better. Notably,\non the subjectivity classification task (Subj), EVOPROMPT (DE) exhibits a substantial improvement\nover its GA counterpart, achieving a 5% accuracy advantage. This may be contributed by the\nexceptional ability of DE to evade local optima when the initial prompts are not of high quality.\n4.3\nLANGUAGE GENERATION\nMethod\nAlpaca\nGPT-3.5\nMI (Zhang et al., 2023c)\n43.03\n43.80\nAPE (Zhou et al., 2022)\n45.90(0.09)\n46.71\nEVOPROMPT (GA)\n46.43(0.19)\n47.36\nEVOPROMPT (DE)\n46.21(0.27)\n47.40\nTable 3: Main results (SARI) on simplification\n(ASSET) for Alpaca-7b and GPT3.5.\nDatasets and Settings\nFor language genera-\ntion, we evaluate our EVOPROMPT on text sum-\nmarization and simplification tasks. For summa-\nrization, we adopt SAMSum (Gliwa et al., 2019),\na challenging and intricate dialogue summariza-\ntion dataset, and report ROUGE-1/2/L scores on\nAlpaca-7b and GPT-3.5. For text simplification,\nwhich aims to simplify the source text while\npreserving its original meaning, we employ the\nASSET dataset (Alva-Manchego et al., 2020), a\nbenchmark known for its multiple reference translations. We apply SARI score (Xu et al., 2016)\nas the evaluation metric, an n-gram-based scoring system extensively utilized for text editing tasks.\nAdditional details regarding our experimental setup can be found in Appendix B.\nMain Results\nThe summarization and simplification results are presented in Tables 2 and 3.\nEVOPROMPT achieves a substantial performance gain over manually designed prompts, exhibiting\nan improvement of over 3 points in SARI scores across both Alpaca and GPT-3.5 API. Furthermore,\nEVOPROMPT consistently outperforms the APE approach across the evaluated scenarios, indicating\n7\nPublished as a conference paper at ICLR 2024\n01\n02\nTask ID\n0\n5\n10\n15\n20\n25\nNormalized Score\n03\n04\n05\n06\n07\n08\n09\nTask ID\n0\n1\n2\n3\n4\n5\n6\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\nTask ID\n2\n1\n0\n1\n2\n3\nEvoPrompt (DE)\nEvoPrompt (GA)\nFigure 3: Normalized scores on BBH tasks for EVOPROMPT (GA) and EVOPROMPT (DE).\nthat the generated prompts effectively harness the capabilities of LLMs for superior performance.\nMoreover, EVOPROMPT (DE) notably outperforms EVOPROMPT (GA) in the summarization task,\nwhile demonstrating comparable performance in the text simplification task. This suggests that the\nDE variant is particularly effective for more complex language generation tasks like summarization.\n4.4\nBIG BENCH HARD (BBH)\nDatasets and Settings\nTo validate our methods on diverse tasks, we apply BBH (Suzgun et al.,\n2022) including a suite of 23 challenging BIG-Bench tasks requiring multi-step reasoning. Since\nthese tasks are challenging, we focus on optimizing the prompts for GPT-3.5. We sample a subset\nfrom the test set as the development set and report the normalized scores1 in comparison to the\nprompt \u201cLet\u2019s think step by step.\u201d (Kojima et al., 2022) with 3-shot Chain-of-Thought demonstrations\n(following Fu et al. (2023)) on the test set. We use task IDs to simplify the denotation of each\ntask and remove one since the accuracy already reaches 100% with the manual prompt. Please see\nAppendix C.2 and Table 17 for details, as well as further comparisons with previous works.\nMain Results\nEVOPROMPT obtains better prompts for all 22 tasks (Figure 3). Specifically, EVO-\nPROMPT (DE) achieves up to a 25% improvement with an average of 3.5%, whereas EVOPROMPT\n(GA) reaches a peak improvement of 15% with a 2.5% average. Though for some tasks the GA coun-\nterpart outperforms the DE version, the performance gap remains relatively small (i.e., around 1%).\nMeanwhile, EVOPROMPT (DE) surpasses EVOPROMPT (GA) by over 2% on 6 tasks. Accordingly,\nthe DE version is generally a good choice for these challenging tasks.\n5\nANALYSIS\n5.1\nDESIGNS IN GA\nStrategy\nSST-5\nASSET\nAvg.\nrandom\n48.67(0.97)\n46.32(0.32)\n47.50\ntournament\n49.70(0.60)\n46.29(0.18)\n48.00\nwheel\n49.91(0.61)\n46.43(0.19)\n48.17\nTable 4: Designs in EVOPROMPT (GA).\nFor EVOPROMPT (GA), we apply the roulette\nwheel selection strategy by default to select parental\nprompts, contributing to the offspring. To further ex-\nplore the effect of various selection strategies, we\ncompare our approach with another two popular\nstrategies, i.e., tournament (Wikipedia contributors,\n2023) and random selection, as presented in Table 4.\nWe observe that EVOPROMPT (GA) with roulette wheel achieves higher scores, showcasing the\neffectiveness of this selection method.\n5.2\nDESIGNS IN DE\nFor EVOPROMPT (DE), we delve into two key design considerations in adapting the evolutionary\noperators of DE to discrete prompts: 1) mutation on different parts, and 2) choosing the current\ntop-performing prompt as \u201cPrompt 3\u201d in Figure 2. We assess the impact of these design choices on\n1The accuracy difference between a given prompt and the baseline prompt \u201cLet\u2019s think step by step.\u201d A score\nof 0 corresponds to the normalized score of the baseline prompt.\n8\nPublished as a conference paper at ICLR 2024\ntwo datasets: Subj, an understanding dataset where EVOPROMPT (DE) outperforms EVOPROMPT\n(GA), and ASSET, a generation dataset where both variants demonstrate similar performance.\nMutation\nPrompt 3\nSubj\nASSET\nDiff\nbest\n75.55(2.26)\n46.21(0.27)\nAll\nbest\n69.87(0.82)\n45.73(0.45)\nDiff\nrandom\n69.82(2.47)\n45.89(0.37)\nDiff\neliminate\n69.07(4.21)\n45.90(0.23)\nTable 5: Designs in EVOPROMPT (DE).\nMutation on Different Parts\nTo illustrate the\nbenefits of mutating only the different parts, we\nreplace the first two steps in Figure 2 with the\ninstruction \u201cRandomly mutate Prompt 1 and\nPrompt 2\u201d to allow mutation on all contents in\nPrompts 1 and 2, denoted as \u201cAll\u201d in Table 5.\nMeanwhile, the original design in EVOPROMPT,\nwhich mutates only the different parts, is denoted\nas \u201cDiff\u201d. As shown in Table 5, the design of mutation on only the different parts consistently yields\nperformance gains across two tasks.\nSelection of Prompt 3\nApplying one of the variants of the DE algorithm, in EVOPROMPT (DE),\nwe pick the best prompt in the current population as Prompt 3 in Figure 2. We validate this design\nvia the following settings: 1) Prompt 3 is randomly sampled from the current population, denoted as\n\u201crandom\u201d in Table 5; 2) Eliminate the use of Prompt 3 by letting the Basic Prompt directly cross over\nwith the mutated different parts (i.e., remove Step 3 in Figure 2), denoted as \u201celiminate\u201d in Tabel 5.\nTable 5 clearly demonstrates the importance of introducing Prompt 3. Moreover, it is shown that\nchoosing the best prompt as Prompt 3 is more effective than random sampling.\n5.3\nPOPULATION INITIALIZATION\nInitialization\nGA\nDE\nbottom-10\n47.80(0.92)\n48.64(0.15)\nrandom-10\n49.34(0.53)\n50.03(1.08)\nrandom-5 + var-5\n49.84(1.49)\n49.53(1.04)\ntop-10\n49.62(1.00)\n49.61(2.30)\ntop-5 + var-5\n49.91(0.61)\n49.89(1.73)\nTable 6: Ablations of the initial population on\nSST-5, where top-n, random-n, bottom-n de-\nnotes the top-performing, randomly selected,\nbottom-performing n prompts, and var-n de-\nnotes the number of generated n variations.\nWe investigate the effect of initial population quality\non EVOPROMPT. We conduct pilot experiments to\nsort the prompts (designed manually or generated by\nGPT-3.5) according to their performance on the dev\nset. We then select bottom, random and top prompts\nalong with their corresponding variations as initial\nprompts. These variations are generated using the\nresampling template designed in Zhou et al. (2022),\nshown in Figure 4 in the Appendix B.2, which is used\nto introduce randomness to the initialization.\nTable 6 demonstrates that: 1) Crafted design of ini-\ntial prompts is not essential, as randomly selecting\nprompts can achieve a similar performance to select-\ning the top-performing ones; 2) When selecting the top-performing prompts, introducing randomness\nby allowing GPT-3.5 to generate variations can lead to a slight improvement in overall performance;\nhowever, when randomly selecting prompts, there is no need to introduce additional randomness\nfor EVOPROMPT (DE); 3) When using top-performing initial prompts, EVOPROMPT (GA) per-\nforms slightly better than EVOPROMPT (DE); however, when starting with bottom-performing initial\nprompts, EVOPROMPT (DE) outperforms EVOPROMPT (GA), which indicates that DE is a better\nchoice when the available manual prompts are not of high quality.\n6\nCONCLUSIONS\nWe introduce EVOPROMPT to optimize discrete prompts, which connects LLMs with evolutionary\nalgorithms. Extensive experiments on 31 datasets demonstrate the superiority of EVOPROMPT,\nyielding consistent performance gains over both manual instructions and existing methods. Besides,\nWe validate that LLMs can serve as an effective, interpretable interface for implementing evolutionary\nalgorithms like GA and DE. While this study focused on EAs, the extensibility of our approach opens\navenues for applying LLMs to other conventional algorithms, such as particle swarm optimization\n(PSO) (Kennedy & Eberhart, 1995), ant colony optimization (ACO) (Dorigo & Gambardella, 1997)\nand more recent Quality-Diversity (QD) optimization algorithms. Our findings aim to inspire future\nresearch at the intersection of LLMs and traditional algorithms, encouraging innovative applications.\n9\nPublished as a conference paper at ICLR 2024\nACKNOWLEDGEMENTS\nThis work was partly supported by the National Key Research and Development Program\nof China (No.\n2020YFB1708200), and the Shenzhen Science and Technology Program\n(JCYJ20220818101001004).\nREFERENCES\nFernando Alva-Manchego, Louis Martin, Antoine Bordes, Carolina Scarton, Beno\u00eet Sagot, and Lucia\nSpecia. Asset: A dataset for tuning and evaluation of sentence simplification models with multiple\nrewriting transformations. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, pp. 4668\u20134679, 2020.\nStephen Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal V Nayak, Abheesht\nSharma, Taewoon Kim, M Saiful Bari, Thibault F\u00e9vry, et al. Promptsource: An integrated\ndevelopment environment and repository for natural language prompts. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational Linguistics: System Demonstrations, pp.\n93\u2013104, 2022.\nJanez Brest, Sao Greiner, Borko Boskovic, Marjan Mernik, and Viljem Zumer. Self-adapting control\nparameters in differential evolution: A comparative study on numerical benchmark problems.\nIEEE transactions on evolutionary computation, 10(6):646\u2013657, 2006.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\nAngelica Chen, David M Dohan, and David R So. Evoprompting: Language models for code-level\nneural architecture search. arXiv preprint arXiv:2302.14838, 2023.\nAndrew R Conn, Katya Scheinberg, and Luis N Vicente. Introduction to derivative-free optimization.\nSIAM, 2009.\nSwagatam Das and Ponnuthurai Nagaratnam Suganthan. Differential evolution: A survey of the\nstate-of-the-art. IEEE transactions on evolutionary computation, 15(1):4\u201331, 2010.\nSwagatam Das, Sankha Subhra Mullick, and Ponnuthurai N Suganthan. Recent advances in differen-\ntial evolution\u2013an updated survey. Swarm and evolutionary computation, 27:1\u201330, 2016.\nMingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song,\nEric Xing, and Zhiting Hu. Rlprompt: Optimizing discrete text prompts with reinforcement\nlearning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language\nProcessing, pp. 3369\u20133391, 2022.\nMarco Dorigo and Luca Maria Gambardella. Ant colony system: a cooperative learning approach\nto the traveling salesman problem. IEEE Transactions on evolutionary computation, 1(1):53\u201366,\n1997.\nYao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar Khot. Chain-of-thought hub:\nA continuous effort to measure large language models\u2019 reasoning performance. arXiv preprint\narXiv:2305.17306, 2023.\nBogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. Samsum corpus: A human-\nannotated dialogue dataset for abstractive summarization. arXiv preprint arXiv:1911.12237,\n2019.\nYiduo Guo, Yaobo Liang, Chenfei Wu, Wenshan Wu, Dongyan Zhao, and Nan Duan. Learning to\nprogram with natural language. arXiv preprint arXiv:2304.10464, 2023.\nJohn H. Holland. Adaptation in Natural and Artificial Systems. University of Michigan Press, Ann\nArbor, 1975. ISBN 0262581116.\n10\nPublished as a conference paper at ICLR 2024\nJohn H Holland. Adaptation in natural and artificial systems: an introductory analysis with applica-\ntions to biology, control, and artificial intelligence. MIT press, 1992.\nMinqing Hu and Bing Liu. Mining and summarizing customer reviews. In KDD, pp. 168\u2013177, 2004.\nSrinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu,\nKurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, et al. Opt-iml: Scaling language model\ninstruction meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017,\n2022.\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. How can we know what language\nmodels know? Transactions of the Association for Computational Linguistics, 8:423\u2013438, 2020.\nJames Kennedy and Russell Eberhart. Particle swarm optimization. In Proceedings of ICNN\u201995-\ninternational conference on neural networks, volume 4, pp. 1942\u20131948. IEEE, 1995.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\nlanguage models are zero-shot reasoners. Advances in neural information processing systems, 35:\n22199\u201322213, 2022.\nPier Luca Lanzi and Daniele Loiacono. Chatgpt and other large language models as evolutionary\nengines for online interactive collaborative game design. arXiv preprint arXiv:2303.02155, 2023.\nJoel Lehman, Jonathan Gordon, Shawn Jain, Kamal Ndousse, Cathy Yeh, and Kenneth O. Stanley.\nEvolution through large models. arXiv preprint arXiv:2206.08896, 2022.\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt\ntuning. In EMNLP, pp. 3045\u20133059, 2021.\nBei Li, Rui Wang, Junliang Guo, Kaitao Song, Xu Tan, Hany Hassan, Arul Menezes, Tong Xiao,\nJiang Bian, and JingBo Zhu. Deliberate then generate: Enhanced prompting framework for text\ngeneration. arXiv preprint arXiv:2305.19835, 2023.\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the\n11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),\npp. 4582\u20134597, 2021.\nAdam Lipowski and Dorota Lipowska. Roulette-wheel selection via stochastic acceptance. Physica\nA: Statistical Mechanics and its Applications, 391(6):2193\u20132196, 2012.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig.\nPre-train, prompt, and predict: A systematic survey of prompting methods in natural language\nprocessing. ACM Computing Surveys, 55(9):1\u201335, 2023.\nXiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang.\nP-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks.\narXiv preprint arXiv:2110.07602, 2021a.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt\nunderstands, too. arXiv preprint arXiv:2103.10385, 2021b.\nElliot Meyerson, Mark J Nelson, Herbie Bradley, Arash Moradi, Amy K Hoover, and Joel\nLehman. Language model crossover: Variation through few-shot prompting. arXiv preprint\narXiv:2302.12170, 2023.\nSeyedali Mirjalili, Jin Song Dong, Ali Safa Sadiq, and Hossam Faris. Genetic algorithm: Theory,\nliterature review, and application in image reconstruction. Nature-Inspired Optimizers: Theories,\nLiterature Reviews and Applications, pp. 69\u201385, 2020.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh Hajishirzi. Reframing\ninstructional prompts to gptk\u2019s language. In Findings of the Association for Computational\nLinguistics: ACL 2022, pp. 589\u2013612, 2022a.\n11\nPublished as a conference paper at ICLR 2024\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization\nvia natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), pp. 3470\u20133487, 2022b.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization\nvia natural language crowdsourcing instructions. In ACL, 2022c.\nMelanie Mitchell. An introduction to genetic algorithms. MIT press, 1998.\nJean-Baptiste Mouret and Jeff Clune. Illuminating search spaces by mapping elites. arXiv preprint\narXiv:1504.04909, 2015.\nOpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback. Advances in Neural Information Processing Systems, 35:\n27730\u201327744, 2022.\nBo PANG. Seeing stars: Exploiting class relationships for sentiment categorization with respect to\nrating scales. In ACL, 2005.\nBo Pang and Lillian Lee. A sentimental education: Sentiment analysis using subjectivity summariza-\ntion based on minimum cuts. In Proceedings of the 42nd Annual Meeting of the Association for\nComputational Linguistics (ACL-04), pp. 271\u2013278, 2004.\nMillie Pant, Hira Zaheer, Laura Garcia-Hernandez, Ajith Abraham, et al. Differential evolution: A\nreview of more than two decades of research. Engineering Applications of Artificial Intelligence,\n90:103479, 2020.\nArchiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal. Grips: Gradient-free, edit-based\ninstruction search for prompting large language models. arXiv preprint arXiv:2203.07281, 2022.\nKenneth V Price. Differential evolution. In Handbook of optimization: From classical to modern\napproach, pp. 187\u2013214. Springer, 2013.\nReid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt\noptimization with\" gradient descent\" and beam search. arXiv preprint arXiv:2305.03495, 2023.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi\nYang. Is chatgpt a general-purpose natural language processing task solver?\narXiv preprint\narXiv:2302.06476, 2023.\nLuis Miguel Rios and Nikolaos V Sahinidis. Derivative-free optimization: a review of algorithms and\ncomparison of software implementations. Journal of Global Optimization, 56:1247\u20131293, 2013.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables\nzero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021.\nTimo Schick and Hinrich Sch\u00fctze. Exploiting cloze-questions for few-shot text classification and\nnatural language inference. In Proceedings of the 16th Conference of the European Chapter of the\nAssociation for Computational Linguistics: Main Volume, pp. 255\u2013269, 2021.\nChufan Shi, Haoran Yang, Deng Cai, Zhisong Zhang, Yifan Wang, Yujiu Yang, and Wai Lam. A\nthorough examination of decoding methods in the era of llms. arXiv preprint arXiv:2402.06925,\n2024.\nWeijia Shi, Xiaochuang Han, Hila Gonen, Ari Holtzman, Yulia Tsvetkov, and Luke Zettlemoyer.\nToward human readable prompt tuning: Kubrick\u2019s the shining is a good movie, and a good prompt\ntoo? arXiv preprint arXiv:2212.10539, 2022.\n12\nPublished as a conference paper at ICLR 2024\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Autoprompt:\nEliciting knowledge from language models with automatically generated prompts. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp.\n4222\u20134235, 2020.\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng,\nand Christopher Potts. Recursive deep models for semantic compositionality over a sentiment\ntreebank. In EMNLP, pp. 1631\u20131642, 2013.\nRainer Storn and Kenneth Price. Differential evolution\u2013a simple and efficient heuristic for global\noptimization over continuous spaces. Journal of global optimization, 11:341\u2013359, 1997.\nMirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks\nand whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.\nhttps://github.com/tatsu-lab/stanford_alpaca, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e\nLacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\nJakob Vesterstrom and Rene Thomsen. A comparative study of differential evolution, particle swarm\noptimization, and evolutionary algorithms on numerical benchmark problems. In Proceedings\nof the 2004 congress on evolutionary computation (IEEE Cat. No. 04TH8753), volume 2, pp.\n1980\u20131987. IEEE, 2004.\nEllen M Voorhees and Dawn M Tice. Building a question answering test collection. In Proceedings of\nthe 23rd annual international ACM SIGIR conference on Research and development in information\nretrieval, pp. 200\u2013207, 2000.\nEric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. Universal adversarial\ntriggers for attacking and analyzing nlp. In Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP), pp. 2153\u20132162, 2019.\nYifan Wang, Qingyan Guo, Xinzhe Ni, Chufan Shi, Lemao Liu, Haiyun Jiang, and Yujiu Yang.\nHint-enhanced in-context learning wakes large language models up for knowledge-intensive tasks.\narXiv preprint arXiv:2311.01949, 2023.\nWikipedia contributors. Tournament selection \u2014 Wikipedia, the free encyclopedia. https://\nen.wikipedia.org/w/index.php?title=Tournament_selection&oldid=1160627612, 2023.\n[Online; accessed 26-September-2023].\nWei Xu, Courtney Napoles, Ellie Pavlick, Quanze Chen, and Chris Callison-Burch. Optimizing statis-\ntical machine translation for text simplification. Transactions of the Association for Computational\nLinguistics, 4:401\u2013415, 2016.\nJD Zamfirescu-Pereira, Richmond Y Wong, Bjoern Hartmann, and Qian Yang. Why johnny can\u2019t\nprompt: how non-ai experts try (and fail) to design llm prompts. In Proceedings of the 2023 CHI\nConference on Human Factors in Computing Systems, pp. 1\u201321, 2023.\nJingqiao Zhang and Arthur C. Sanderson. Jade: Adaptive differential evolution with optional\nexternal archive. IEEE Transactions on Evolutionary Computation, 13(5):945\u2013958, 2009. doi:\n10.1109/TEVC.2009.2014613.\nNingyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng, Zhen Bi, Chuanqi Tan, Fei Huang, and Huajun\nChen. Differentiable prompt makes pre-trained language models better few-shot learners. In\nInternational Conference on Learning Representations, 2021.\n13\nPublished as a conference paper at ICLR 2024\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language\nmodels. arXiv preprint arXiv:2205.01068, 2022.\nTianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans, and Joseph E Gonzalez. Tempera:\nTest-time prompt editing via reinforcement learning. In The Eleventh International Conference on\nLearning Representations, 2023a.\nWenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan, and Lidong Bing. Sentiment analysis in the\nera of large language models: A reality check. arXiv preprint arXiv:2305.15005, 2023b.\nXiang Zhang, Junbo Zhao, and Yann LeCun.\nCharacter-level convolutional networks for text\nclassification. NeurIPS, 28, 2015.\nYue Zhang, Leyang Cui, Deng Cai, Xinting Huang, Tao Fang, and Wei Bi. Multi-task instruction\ntuning of llama for specific scenarios: A preliminary study on writing assistance. arXiv preprint\narXiv:2305.13225, 2023c.\nMingkai Zheng, Xiu Su, Shan You, Fei Wang, Chen Qian, Chang Xu, and Samuel Albanie. Can\ngpt-4 perform neural architecture search? arXiv preprint arXiv:2304.10970, 2023.\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and\nJimmy Ba. Large language models are human-level prompt engineers. In The Eleventh International\nConference on Learning Representations, 2022.\nKaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei\nYe, Neil Zhenqiang Gong, Yue Zhang, et al. Promptbench: Towards evaluating the robustness of\nlarge language models on adversarial prompts. arXiv preprint arXiv:2306.04528, 2023.\n14\nPublished as a conference paper at ICLR 2024\nAlgorithm 2 Discrete prompt optimization: EVOPROMPT (GA)\nRequire: Initial prompts P0 = {p1, p2, . . . , pN}, size of population N, a dev set D\n1: Initial fitness evaluation: S0 \u2190 {si = f(pi, D)|i \u2208 [1, N]}\n2: for t = 1 to T do\n\u25b7 T: Number of iterations\n3:\nfor i = 1 to N do\n4:\nSelection based on fitness using roulette wheel: pr1, pr2 \u223c Pt\u22121\n5:\nEvolution: p\u2032\ni \u2190 GA(pr1, pr2) (Refer to Figure 1)\n6:\nEvaluation: si \u2190 f(p\u2032\ni, D)\n7:\nend for\n8:\nS\u2032\nt \u2190 {si|i \u2208 [1, N]}, P \u2032\nt \u2190 {p\u2032\ni|i \u2208 [1, N]}\n9:\nUpdate score: St \u2190 Top-N{St\u22121, S\u2032\nt}\n10:\nUpdate: Pt \u2190 Top-N{Pt\u22121, P \u2032\nt} using St\u22121, S\u2032\nt,\n11: end for\n12: Return the best prompt, p\u2217, among the final population PT : p\u2217 \u2190 argmaxp\u2208PT f(p, D)\nAlgorithm 3 Discrete prompt optimization: EVOPROMPT (DE)\nRequire: Initial prompts P0 = {p1, p2, . . . , pN}, size of population N, a dev set D\n1: for t = 1 to T do\n\u25b7 T: Number of iterations\n2:\nfor pi in Pt\u22121 do\n3:\nSample donors: pr1, pr2 \u223c Pt\u22121 , r1 \u0338= r2 \u0338= i\n4:\nEvolution: p\u2032\ni \u2190 DE(pi, pr1, pr2, pbest) where pbest is the current best prompt. (Refer\nto Figure 2)\n5:\nSelection: p\u2217\ni = arg max\np\u2208{pi,p\u2032\ni}\nf(p, D)\n\u25b7 Keep the better one in the population\n6:\nend for\n7:\nUpdate:Pt \u2190 {p\u2217\ni |i \u2208 [1, N]}\n8: end for\n9: Return the best prompt, p\u2217, among the final population PT : p\u2217 \u2190 argmaxp\u2208PT f(p, D)\nA\nDETAILS OF ALGORITHM IMPLEMENTATION\nWe instantiate EVOPROMPT two representative evolutionary algorithms, GA and DE. Though both\nalgorithms use consistent general selection processes, creating offspring, and updating, it is worth\nnoting that the selection strategies, ways of mutation and crossover, and the updating strategies in\nthese two algorithms are different. The specific algorithms for each of them are shown in Algorithm 2\nand Algorithm 3.\nB\nEXPERIMENTAL SETTINGS\nB.1\nDATASETS\nTable 7 shows the statistics of the text classification, simplification and summarization datasets. For\nBig-Bench Hard, We use serial numbers to denote 22 tasks, the descriptions are reported in Table 17.\nNote that for the task of \u201cweb of lies\u201d, the accuracy of the baseline is 100%, so here we have not\nincluded this task for prompt optimization. Additionally, both tasks of \u201clogical deduction objects\u201d\nand \u201ctracking shuffled objects\u201d have three sub-tasks.\nB.2\nTEMPLATES\nGenerate a variation of the following\ninstruction while keep the semantic meaning.\nInput: <prompt>\nOutput:\nTemplate for Variation\nFigure 4:\nTemplate used for resam-\npling (Zhou et al., 2022).\nTemplates for Task Implementation\nFor different mod-\nels, we apply different templates shown in Table 8, 9 and\n10, referring to the previous works (Iyer et al., 2022; Taori\net al., 2023; Zhang et al., 2023b; Li et al., 2023; Fu et al.,\n2023).\n15\nPublished as a conference paper at ICLR 2024\nDataset\nType\nLabel space\n|Test|\nSST-2\nSentiment\n{positive, negative}\n1,821\nCR\nSentiment\n{positive, negative}\n2,000\nMR\nSentiment\n{positive, negative}\n2,000\nSST-5\nSentiment\n{terrible, bad, okay, good, great}\n2,210\nAG\u2019s News\nNews topic\n{World, Sports, Business, Tech}\n7,600\nTREC\nQuestion topic\n{Description, Entity, Expression, Human, Location, Number}\n500\nSubj\nSubjectivity\n{subjective, objective}\n2,000\nSAMSum\nSummarization\n-\n819\nASSET\nSimplification\n-\n359\nTable 7: Statistics for natural language understanding and generation datasets used in this work.\nTemplate for Prompt Generation\nWe apply the resampling template, shown in Figure 4, to\ngenerate variations of manual initial prompts. For our EVOPROMPT, the complete DE algorithm\nimplemented by LLMs is shown in Figure 5. For both DE and GA, we prepend a one-shot example\nof the algorithm execution, guiding LLMs to operate precisely.\n============================== INSTRUCTIONAL PROMPTS ==============================\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response\nthat appropriately completes the request.\n### Instruction:\n<PROMPT>\n### Input:\n<INPUT>\n### Response:\n<COMPLETE>\nZero-shot Example:\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response\nthat appropriately completes the request.\n### Instruction:\nPlease perform Sentiment Classification task. Given the sentence, assign a sentiment label from [\u2019negative\u2019,\n\u2019positive\u2019]. Return label only without any other text.\n### Input:\nbeautifully observed , miraculously unsentimental comedy-drama .\n### Response:\n<COMPLETE>\nTable 8: Template used for Alpaca (referring to Taori et al. (2023)).\nB.3\nHYPER PARAMETERS\nThe parameters for the experiments are shown in Table 11. For evolutionary algorithms implemented\nby GPT-3.5, following previous work (Shi et al., 2024), we use Top-p decoding (temperature=0.5,\nP = 0.95). For the task implementation, we use greedy decoding and the default temperature for\nAlpaca. For the generation tasks implemented by GPT-3.5, the temperature is 0.0.\n16\nPublished as a conference paper at ICLR 2024\nDifferential Evolution (DE) Algorithm Implemented by LLMs\n\ud835\udc83 \u2212 \ud835\udc84\n\ud835\udc6d(\ud835\udc83 \u2212 \ud835\udc84)\n\ud835\udc82 + \ud835\udc6d(\ud835\udc83 \u2212 \ud835\udc84)\n\ud835\udc02\ud835\udc2b\ud835\udc28\ud835\udc2c\ud835\udc2c\ud835\udc28\ud835\udc2f\ud835\udc1e\ud835\udc2b\n4. Cross over the prompt in Step 3 with the following basic prompt and generate a final \nprompt bracketed with <prompt> and </prompt>:\n3. Combine the different parts with Prompt 3, selectively replace it with the different \nparts in Step 2 and generate a new prompt:\nQuery: \nPlease follow the instruction step-by-step to generate a better prompt.\n1. Identify the different parts between the Prompt 1 and Prompt 2:\nPrompt 1: Categorize the tweet according to if it has a positive or negative sentiment.\nPrompt 2: Carry out sentiment analysis for every sentence to decide if it is positive or \nnegative.\n2. Randomly mutate the different parts\n3. Combine the different parts with Prompt 3, selectively replace it with the different parts \nin Step 2 and generate a new prompt.\nPrompt 3: In this task, you are given sentences from product reviews. The task is to classify \na sentence as  positive or as negative.\n4. Cross over the prompt in the Step 3 with the following basic prompt and generate a final \nprompt bracketed with <prompt> and </prompt>:\nBasic Prompt: Here, you'll be given sentences from reviews about products and you'll need \nto decide if it's a positive or a negative review.\nResponse:\n1. Identifying the different parts between Prompt 1 and Prompt 2:\n2. Randomly mutate the different parts:\n\"tweet\" -> \"review\"\n\"Categorize\" -> \"Analyze\"\n\"Sentiment analysis\" -> \"Sentiment identification''\nNew Prompt: In this task, you are given reviews about products. The task is to analyze\neach review and identify if it is positive or negative.\nFinal Prompt: <prompt>Here, you'll be given reviews about products and you'll need \nto analyze each review and identify if it is positive or negative.</prompt>\nBasic Prompt: Here, you'll be given sentences from reviews about products and you'll \nneed to decide if it's a positive or a negative review.\nPrompt 1: Categorize the tweet according to if it has a positive or negative sentiment.\nPrompt 2: Carry out sentiment analysis for every sentence to decide if it is positive or \nnegative.\nDifferent parts:\n\"tweet\" vs \"sentence\"\n''Categorize'' vs ''Carry out sentiment analysis''\nPrompt 3: In this task, you are given sentences from product reviews. The task is to \nclassify a sentence as  positive or as negative.\nFigure 5: DE algorithm implemented by LLMs for discrete prompt optimization with complete\nresponse (Evo(\u00b7) in Algorithm 1). In Step 1, LLMs find the different parts (words in \u25a0 and \u25a0)\nbetween Prompt 1 and Prompt 2 (b \u2212 c in typical DE). In Step 2, LLMs perform mutation (words in\n\u25a0 ) on them (imitation of F(b \u2212 c)). Next, LLMs incorporate the current best prompt as Prompt 3\nwith the mutated results in Step 2, to generate a new prompt (counterpart of a + F(b \u2212 c) in DE).\nFinally, LLMs perform crossover upon the current basic prompt pi and the generated prompt in Step\n3.\n17\nPublished as a conference paper at ICLR 2024\n=========================== TEMPLATE FOR SIMPLIFICATION ===========================\n<PROMPT>\n<INPUT>\nThe simplification of the sentence is <COMPLETE>\nZero-shot example:\nSimplify the text.\nSubsequently, in February 1941, 600 Jews were sent to Buchenwald and Mauthausen concentration camps.\nThe simplification of the sentence is <COMPLETE>\n=========================== TEMPLATE FOR SUMMARIZATION ===========================\n<PROMPT>\n<INPUT>\nTL;DR: <COMPLETE>\nZero-shot example:\nHow would you rephrase that in a few words?\nTheresa: have you been at Tom\u2019s new place? Luis: yes, it\u2019s nice Marion: He invited us for a dinner Adam: where\nis it? Marion: a bit outside the city Adam: where exactly? Marion: Fiesole Luis: very nice!\nTL;DR: <COMPLETE>\nTable 9: Templates of summarization (following Sanh et al. (2021); Qin et al. (2023)), simplification\n(following Li et al. (2023)) and the corresponding zero-shot examples.\n==========================TEMPLATE FOR BIG-BENCH HARD ==========================\n<DESC>\nQ: <INPUT>\nA: <PROMPT>\n<COMPLETE>\nZero-shot example:\nQuestions that involve enumerating objects and asking the model to count them.\nQ: I have a flute, a piano, a trombone, four stoves, a violin, an accordion, a clarinet, a drum, two lamps, and a\ntrumpet. How many musical instruments do I have?\nA: Let\u2019s think step by step.\n<COMPLETE>\nTable 10: Template for Big-Bench Hard (following Suzgun et al. (2022)) used for GPT-3.5 and the\ncorresponding zero-shot examples. <DESC> refers to the specific description of each task.\n18\nPublished as a conference paper at ICLR 2024\n4\n6\n8\n10\n12\nSize\n48.0\n48.5\n49.0\n49.5\nScore on SST-5\n4\n6\n8\n10\n12\nSize\n68\n70\n72\n74\nScore on Subj\n4\n6\n8\n10\n12\nSize\n45.8\n45.9\n46.0\n46.1\n46.2\n46.3\n46.4\nScore on ASSET\nDE\nGA\nFigure 6: Effect of population size on SST-5 (left), Subj (middle), and ASSET (right). All the results\nare averaged over 3 random seeds.\nTask LM\n|Population|\n|Steps|\n|Dev|\n|Shots|\nText classification\nAlpaca-7b\n10\n10\n200\n1\nText Generation\nAlpaca-7b\n10\n10\n100\n0\nGPT-3.5\n10\n10\n100\n0\nBig-Bench Hard\nGPT-3.5\n10\n10\n50\n3\nTable 11: Settings for experiments. |Shots| refers to the\nnumber of examples in the demonstration. For the text\nclassification task, we set the value as 1, which means\nwe prepend with 1 sample of each category, to constrain\nthe output in the label space.\nText Classification\nThe population of\nprompts is initialized with widely used in-\nstructions in the previous works (Mishra\net al., 2022b; Zhang et al., 2022). We para-\nphrase and rewrite them to initialize the\npopulation. The size of the development\nset is 200. We report the results on the\nfull test set (the same as the previous re-\nlated works (Deng et al., 2022; Zhang et al.,\n2023a)), as shown in Table 11.\nText Generation\nFor the initial popula-\ntion, we collect instructions for summa-\nrization and simplification from Li et al.\n(2023); Sanh et al. (2021); Zhang et al.\n(2023c) and augment them to the expected size (10 in our setting), either written manually or\ngenerated by GPT-3.5.\nC\nADDITIONAL RESULTS\nC.1\nPARAMETERS IN EVOLUTIONARY ALGORITHMS\nEffect of Population Size\nIntuitively, a trade-off exists between the performance and the overhead\ncaused by the population size. We explore the performance of EVOPROMPT (DE) and EVOPROMPT\n(GA) respectively at varying population sizes from 4 to 12. The results are plotted in Figure 6.\nFor classification datasets, as the size increases, curves for DE and GA show an ascending trend.\nFurthermore, the increase in DE attributed to population diversity was greater than that in GA since\nDE focuses on different parts. Differences among prompts within populations bring about substantial\nmutations, leading DE to explore potential prompts since keeping common parts balances exploration\nand exploitation effectively.\nFor the relatively simple generation task (i.e., ASSET), a population size of 6 demonstrates a\ncomparable performance to a population size of 10, though with a 2.5-fold increase in overhead. This\nsuggests that for relatively simple tasks large populations are unnecessary, while for complex tasks\n(i.e., Subj), a larger population with diversity brings improvement.\nEffect of Number of Iterations\nTo further explore the process of convergence, for SST-5, Subj\nand ASSET, we plot the best and average scores on the development set for EVOPROMPT for DE and\nGA over the whole population after each iterative step (Figure 7). Curves of best and average scores\ngradually converge with an increasing trend as evolution proceeds, indicating that the population\u2019s\nquality as a whole is steadily increasing as the evolution process.\n19\nPublished as a conference paper at ICLR 2024\n2\n4\n6\n8\n10\nIteration\n0.400\n0.425\n0.450\n0.475\n0.500\nScore on SST-5\n2\n4\n6\n8\n10\nIteration\n0.65\n0.70\n0.75\nScore on Subj\n2\n4\n6\n8\n10\nIteration\n45.5\n46.0\n46.5\nScore on ASSET\nGA-best\nGA-avg\nDE-best\nDE-avg\nFigure 7: The best and average scores of each iteration on SST-5 (left), Subj (middle), and ASSET\n(right) development set on Alpaca-7b. All the results are averaged over 3 random seeds.\n01\n02\nTask ID\n0\n5\n10\n15\n20\n25\nNormalized Score\n03\n04\n05\n06\n07\n08\n09\nTask ID\n1\n0\n1\n2\n3\n4\n5\n6\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\nTask ID\n3\n2\n1\n0\n1\n2\n3\nAPE\nEvoPrompt (DE)\nEvoPrompt (GA)\nFigure 8: Normalized scores on BBH tasks for APE, EVOPROMPT (GA) and EVOPROMPT (DE).\nC.2\nCOMPARISON ON BBH TASKS\nMethod\nAvg.\nbaseline\n71.49\nAPE\n71.85\nEVOPROMPT (GA)\n74.18\nEVOPROMPT (DE)\n75.03\nTable 12: Average accuracy over 23\nBBH tasks for different methods.\nAPE (Zhou et al., 2022) optimizes the Chain-of-Thought (CoT)\nprompt for reasoning tasks on InstructGPT. Considering that\nboth InstructGPT and GPT-3.5 belong to the GPT family and\nwe may observe similar trends, we evaluate the CoT prompt\nproposed by APE, \u201cLet\u2019s work this out in a step by step way to\nbe sure we have the right answer.\u201d, on reasoning tasks and plot\nthe 3-shot performance in Figure 8. For simplicity, we use the\nsame initial population for all the 22 BBH tasks without priori\nknowledge of each task. In future works, by incorporating\ntask-specific prompts, either manually designed or generated\nby LLMs, we may further enhance the performance.\nSST-5\nSubj\nAPE\nEVOPROMPT (GA)\nEVOPROMPT (DE)\nAPE\nEVOPROMPT (GA)\nEVOPROMPT (DE)\nSame iteration\n# iterations\n9\n9\n9\n15\n15\n15\n# tokens\n5.39 M\n5.40 M\n5.52 M\n5.66 M\n5.73 M\n5.93 M\nscore\n45.79\n50.23\n49.23\n67.20\n70.10\n79.35\nUntil convergence\n# iterations\n9\n7\n11\n15\n15\n17\n# tokens\n5.39 M\n4.20 M\n6.75 M\n5.66 M\n5.73 M\n6.72 M\nscore\n45.79\n50.23\n51.13\n67.20\n70.10\n79.35\nTable 13: Number of iterations, tokens within the API requests (including prompt optimization and\nevaluation) and the corresponding score for our methods and APE. We choose the iteration that APE\nconverges as the Same iteration for comparison. Until convergence means that the improvement of\nthe average score is less than 0.3% for continuous two iterations.\n20\nPublished as a conference paper at ICLR 2024\n0\n2\n4\n6\n8\nIteration\n15\n20\n25\nValue\nAverage length of prompts\nDE\nGA\n(a) Average length over the\npopulation after each step.\n0\n2\n4\n6\n8\nIteration\n10\n20\n30\n40\nValue\nVariance of the prompt length\nDE\nGA\n(b) Variance of prompt length over\nthe population of each step.\n2\n4\n6\n8\nIteration\n5\n10\n15\nValue\nAverage number of new words\nDE\nGA\n(c) Number of new words generated\nafter each step.\nFigure 9: Statistics about the prompt length, including average values over the whole population (a),\nvariance over the prompt length (b), and number of new words evolved after each step (c). Note that\nall the values are averaged over 8 datasets, including 7 understanding datasets and one simplification\ndataset, and 3 random seeds.\nC.3\nCOST ANALYSIS\nOverhead mainly comes from prompt evaluation and generation. For evaluation, our overhead is\nN \u2217|D|\u2217T, where N is the size of the population, |D| is the size of the development set, and T is the\nnumber of iterations. These parameters differ from the task and can be found in Appendix B.3. For\nthe cost from prompt generation, the cost mainly depends on the number of API results, T \u2217 N. So\nthe total number of API requests is N \u2217 T \u2217 (1 + |D|), the same as APE. Moreover, given that the API\nof LLMs is typically billed based on the number of tokens used, we also estimate the total number\nof tokens used in the API requests during the prompt optimization process, as shown in Table 13.\nAll the scores reported are over the test set on one random seed. We analyze the overhead mainly\nfrom two aspects: 1) the performance of our methods compared with APE under the same number of\niterations; 2) the performance until convergence measured by the average score on the dev set.\nWe can observe that with the same number of iterations, both GA and DE outperform APE signifi-\ncantly while introducing only a slight overhead in terms of the number of tokens. The convergence\nrates of APE and GA are similar while DE is slightly slower, but it delivers better performance. This\nimplies the relatively high ceiling of EVOPROMPT.\nC.4\nANALYSIS OF PROMPT\nDiversity Analysis\nWe further investigate the diversity of prompts generated by GA and DE after\neach iterative step respectively. We mainly plot the average prompt length, variance and number\nof new words mutated after each step, as shown in Figure 9. It can be observed that EVOPROMPT\n(DE) generates longer prompts with higher variances than EVOPROMPT (GA), which implies that\nDE prefers exploration for diversity. In the latter iterations, DE mutates more new words than GA,\nand thus shows better potential to escape from the local optimum.\nOptimal Prompts\nWe release the optimal prompts generated by EVOPROMPT for understanding\n(Table 14), text simplification (Table 16), summarization (Table 15) and BBH tasks (Table 17, 18) .\nD\nFUTURE WORKS\nThere are several promising directions for future investigation:\n\u2022 Based on our framework, more applications can be explored, including game levels generation,\ntext-to-images generation, non-trivial NP-hard problems (e.g. traveling salesman problem), etc.\n\u2022 There exist many variants of DE and we give priority to the most canonical and classical ones\nfor current exploration. In future work, it will be interesting to consider more advanced DE-\nvariants (Das et al., 2016; Das & Suganthan, 2010). For example, some recent DE-variants have\nbeen investigating adaptive control parameters. The main challenge in applying these variants to\n21\nPublished as a conference paper at ICLR 2024\nDataset\nMethod\nContent\nScore\nSST-2\nManual Instruction\nPlease perform Sentiment Classification task. Given the sentence, assign a sentiment label from [\u2019negative\u2019,\n\u2019positive\u2019]. Return label only without any other text.\n93.68\nNatural Instruction\nIn this task, you are given sentences from movie reviews. The task is to classify a sentence as \"great\" if the\nsentiment of the sentence is positive or as \"terrible\" if the sentiment of the sentence is negative.\n92.86\nPromptSource\nDoes the following sentence have a positive or negative sentiment?\n93.03\nEVOPROMPT\nExamine the movie reviews and classify them as either positive or negative.\n95.61\nCR\nManual Instruction\nPlease perform Sentiment Classification task. Given the sentence, assign a sentiment label from [\u2019negative\u2019,\n\u2019positive\u2019]. Return label only without any other text.\n91.40\nNatural Instruction\nIn this task, you are given sentences from movie reviews. The task is to classify a sentence as \"great\" if the\nsentiment of the sentence is positive or as \"terrible\" if the sentiment of the sentence is negative.\n90.90\nEVOPROMPT\nAnalyze customer reviews and categorize each sentence as either \u2019positive\u2019 or \u2019negative\u2019.\n91.75\nMR\nManual Instruction\nPlease perform Sentiment Classification task. Given the sentence, assign a sentiment label from [\u2019negative\u2019,\n\u2019positive\u2019]. Return label only without any other text.\n88.75\nNatural Instruction\nIn this task, you are given sentences from movie reviews. The task is to classify a sentence as \"great\" if the\nsentiment of the sentence is positive or as \"terrible\" if the sentiment of the sentence is negative.\n89.60\nEVOPROMPT\nIdentify if a movie review is positive or negative by accurately categorizing each input-output pair into either\n\u2019positive\u2019 or \u2019negative\u2019.\n91.35\nSST-5\nManual Instruction\nPlease perform Sentiment Classification task. Given the sentence, assign a sentiment label from [\u2019terrible\u2019,\n\u2019bad\u2019, \u2019okay\u2019, \u2019good\u2019, \u2019great\u2019]. Return label only without any other text.\n42.90\nNatural Instruction\nIn this task, you are given sentences from movie reviews. Based on the given review, classify it to one of the\nfive classes: (1) terrible, (2) bad, (3) okay, (4) good, and (5) great.\n48.64\nEVOPROMPT\nHave your friend evaluate the movie they had just seen and provide a summary opinion (e.g. terrible, bad,\nokay, good, or great) to determine the sentiment of the movie review.\n52.26\nAG\u2019s News\nManual Instruction\nPlease perform News Classification task. Given the news item, assign a label from [\u2019World\u2019, \u2019Sports\u2019,\n\u2019Business\u2019, \u2019Tech\u2019]. Return label only without any other text.\n70.63\nNatural Instruction\nIn this task, you are given a news article. Your task is to classify the article to one out of the four topics\n\"World\", \"Sports\", \"Business\", \"Tech\" if the article\"s main topic is relevant to the world, sports, business,\nand technology, correspondingly. If you are not sure about the topic, choose the closest option.\n48.89\nPromptSource\nWhat label best describes this news article?\n45.43\nEVOPROMPT\nAssess the entire concept of the news story and choose from the World, Sports, Business or Tech categories\nto categorize it into the correct category.\n76.21\nTREC\nManual Instruction\nPlease perform Question Classification task. Given the question, assign a label from [\u2019Description\u2019, \u2019Entity\u2019,\n\u2019Expression\u2019, \u2019Human\u2019, \u2019Location\u2019, \u2019Number\u2019]. Return label only without any other text.\n50.60\nNatural Instruction\nYou are given a question. You need to detect which category better describes the question. Answer with\n\"Description\", \"Entity\", \"Expression\", \"Human\", \"Location\", and \"Number\".\n55.00\nPromptSource\nWhich category best describes the following question? Choose from the following list: Description, Entity,\nAbbreviation, Person, Quantity, Location.\n36.20\nEVOPROMPT\nRecognize the inputs (explanations, entities, or humans) and provide the suitable outputs (numbers, descrip-\ntions, or entities) to answer the questions in a way that is understandable for non-native English speakers.\n68.00\nSubj\nManual Instruction\nPlease perform Subjectivity Classification task. Given the sentence, assign a label from [\u2019subjective\u2019,\n\u2019objective\u2019]. Return label only without any other text.\n49.75\nNatural Instruction\nIn this task, you are given sentences from reviews. The task is to classify a sentence as \"subjective\" if the\nopinion of the sentence is subjective or as \"objective\" if the opinion of the sentence is objective.\n52.55\nEVOPROMPT\nConstruct input-output pairs to demonstrate the subjectivity of reviews and opinions, distinguishing between\nobjective and subjective input while producing examples of personal opinions and illustrations of subjective\nviews, so it can illustrate the subjectivity of judgments and perspectives.\n77.60\nTable 14: Manual Instructions (following Zhang et al. (2023b) and Zhang et al. (2023c)), Natural\nInstructions (Mishra et al., 2022b), PromptSource (Bach et al., 2022) as baselines and instructions\nwith best performance on Alpaca-7b generated by EVOPROMPT (either DE or GA) on classification\ndatasets.\nMethod\nModel\nContent\nROUGE-1/2/L\nManual Instruction\nAlpaca-7b\nHow would you rephrase that in a few words?\n35.92/11.16/31.67\nGPT\nHow would you rephrase that in a few words?\n43.95/17.11/39.09\nEVOPROMPT\nAlpaca-7b\nCarefully examine the text or listen to the conversation to identify the key ideas, comprehend\nthe main idea, and summarize the critical facts and ideas in the concise language without any\nunnecessary details or duplication.\n39.86/14.24/36.09\nGPT\nReduce the core by reading or listening carefully to identify the main ideas and key points, so\nreaders can comprehend the important concepts and essential information.\n46.49/19.49/41.96\nTable 15: Manual Instructions (following Sanh et al. (2021) as the baseline and instructions with best\nperformance on Alpaca-7b and GPT3.5 generated by EVOPROMPT (either DE or GA) on SAMSum.\n22\nPublished as a conference paper at ICLR 2024\nMethod\nModel\nContent\nSARI\nManual Instruction\nAlpaca-7b\nSimplify the text.\n43.03\nGPT-3.5\nSimplify the text.\n43.80\nEVOPROMPT\nAlpaca-7b\nRewrite the input text into simple English to make it easier to comprehend for non-native English speakers.\n46.67\nGPT-3.5\nRewrite the given sentence to make it more accessible and understandable for both native and non-native\nEnglish speakers.\n47.40\nTable 16: Manual Instructions (following Zhang et al. (2023c) as the baseline and instructions with\nbest performance on Alpaca-7b and GPT3.5 generated by EVOPROMPT (either DE or GA) on\nASSET dataset.\nTask ID\nTask\nDescription\nPrompt\nScore\n01\nhyperbaton\nOrder adjectives correctly in English\nsentences.\nVerify the answer by splitting it into components\nand inspecting each part closely and logically, so\nwe can progress thoughtfully and methodically as\nwe break the task into pieces and explore each part\nsystematically and rationally to reach our goal.\n81.20\n02\ntemporal_sequences\nAnswer questions about which times\ncertain events could have occurred.\nStart by breaking this conundrum into manageable\nchunks, carefully analyzing each component of\nthis problem and thoroughly inspecting each aspect\ncollaboratively, tackling it together progressively to\nensure the correct answer and the desired outcome.\n78.80\n03\nobject_counting\nQuestions that involve enumerating\nobjects and asking the model to\ncount them.\nExamine this logically and assess this methodically,\nso that we can obtain a precise result by thinking\ncritically and dissecting this math task systemati-\ncally.\n87.60\n04\ndisambiguation_qa\nClarify the meaning of sentences\nwith ambiguous pronouns.\nFirst, let us ponder and start off by taking our time,\ngoing step by step, and using our logic to approach\nthis before we dive into the answer.\n71.20\n05\nlogical_deduction_three_objects\nA logical deduction task which re-\nquires deducing the order of a se-\nquence of objects.\nLet\u2019s approach it cautiously, examining it thor-\noughly and methodically, and then approach it in-\ncrementally towards a resolution.\n94.40\n05\nlogical_deduction_five_objects\nA logical deduction task which re-\nquires deducing the order of a se-\nquence of objects.\nSplit the problem into steps and thoughtfully\nprogress through them to find the answer after the\nproof.\n65.20\n05\nlogical_deduction_seven_objects\nA logical deduction task which re-\nquires deducing the order of a se-\nquence of objects.\nLet\u2019s take a step-by-step approach to systematically\ndissect this math task.\n54.40\nTable 17: Instructions with the best performance on GPT3.5 generated by EVOPROMPT (either DE\nor GA) on BBH datasets. Duplicate IDs are due to the tasks with several sub-tasks.\nprompt optimization within the discrete language space lies in assessing the capacity of LLMs to\nadapt to these continuous control parameters.\n\u2022 We hope our study can inspire further exploration of the connection between LLMs and other\ntraditional algorithms, extending beyond EAs. The main challenge is adapting the specific elements\nof traditional algorithms to work within LLMs. For example, these elements may include direction\nof motion, velocity in partial swarm optimization (PSO) (Kennedy & Eberhart, 1995), the path in\nant colony optimization algorithms (APO) (Dorigo & Gambardella, 1997), and characteristic in\nMAP-Elites (Mouret & Clune, 2015).\n23\nPublished as a conference paper at ICLR 2024\nTask ID\nTask\nDescription\nPrompt\nScore\n06\ncausal_judgement\nAnswer questions about causal attri-\nbution.\nAt first, let\u2019s handle things cautiously and resolve\nthis by examining every detail and dealing with\none problem at a time.\n65.78\n07\ndate_understanding\nInfer the date from context.\nBe realistic and practical like a detective, and use\nevidence to solve the problem in a logical, step-by-\nstep approach.\n85.60\n08\nruin_names\nSelect the humorous edit that \u2019ru-\nins\u2019 the input movie or musical artist\nname.\nBreak down a math task into smaller sections and\nsolve each one.\n69.60\n09\nword_sorting\nSort a list of words.\nAnalyze each part of the problem logically to solve\nit like a detective.\n56.40\n10\ngeometric_shapes\nName geometric shapes from their\nSVG paths.\nWe\u2019ll methodically work through this problem to-\ngether.\n64.00\n11\nmovie_recommendation\nRecommend movies similar to the\ngiven list of movies.\nBefore exploring the answer,\n86.00\n12\nsalient_translation_error_detection\nDetect the type of error in an En-\nglish translation of a German source\nsentence.\nBreak down the problem into individual steps in\norder to solve it.\n62.80\n13\nformal_fallacies\nDistinguish deductively valid argu-\nments from formal fallacies.\nLet\u2019s be realistic and evaluate the situation system-\natically, tackling it gradually.\n56.00\n14\npenguins_in_a_table\nAnswer questions about a table of\npenguins and their attributes.\nLet\u2019s start by taking a rational and organized ap-\nproach, breaking it down into smaller parts and\nthinking it through logically, while being realistic\nand handling it carefully and methodically to en-\nsure the right solution.\n84.25\n15\ndyck_languages\nCorrectly close a Dyck-n word.\nLet\u2019s be realistic and solve this challenge carefully\nand slowly, taking it slow to complete it correctly,\nso we can be realistic and cautiously reach the goal.\n44.40\n16\nmultistep_arithmetic_two\nSolve multi-step arithmetic prob-\nlems.\nBefore we dive into the answer,\n51.60\n17\nnavigate\nGiven a series of navigation instruc-\ntions, determine whether one would\nend up back at the starting point.\nLet\u2019s logically work together to systematically\nsolve this math problem one step at a time in uni-\nson.\n94.20\n18\nreasoning_about_colored_objects\nAnswer extremely simple questions\nabout the colors of objects on a sur-\nface.\nUsing a detective\u2019s mindset, break down each ele-\nment of this mathematical reasoning challenge one\nstep at a time and reason like a detective to uncover\nthe solution.\n88.00\n19\nboolean_expressions\nEvaluate the result of a random\nBoolean expression.\nLet\u2019s gradually unravel this mathematical chal-\nlenge by methodically addressing it by examining\neach element and investigating each factor.\n90.80\n20\ntracking_shuffled_objects_three_objects\nA task requiring determining the fi-\nnal positions of a set of objects given\ntheir initial positions and a descrip-\ntion of a sequence of swaps.\nProgress slowly and carefully through this mathe-\nmatical reasoning challenge one step at a time.\n69.20\n20\ntracking_shuffled_objects_five_objects\nA task requiring determining the fi-\nnal positions of a set of objects given\ntheir initial positions and a descrip-\ntion of a sequence of swaps.\nUsing a logical, step-by-step approach, work\nthrough this task to find the correct answer.\n81.20\n20\ntracking_shuffled_objects_seven_objects\nA task requiring determining the fi-\nnal positions of a set of objects given\ntheir initial positions and a descrip-\ntion of a sequence of swaps.\nExamine this issue logically and in detail, step-by-\nstep, analyzing each part of the problem one at a\ntime.\n84.80\n21\nsports_understanding\nDetermine whether an artificially\nconstructed sentence relating to\nsports is plausible or not.\nBreak down the problem into steps and start solv-\ning it.\n96.80\n22\nsnarks\nDetermine which of two sentences\nis sarcastic.\nBreak down and analyze each part of the problem\nin a step by step way to ensure the right answer is\nobtained.\n77.53\nTable 18: Instructions with the best performance on GPT3.5 generated by EVOPROMPT (either DE\nor GA) on BBH datasets. Duplicate IDs are due to the tasks with several sub-tasks.\n24\n"
  },
  {
    "title": "Replacing softmax with ReLU in Vision Transformers",
    "link": "https://arxiv.org/pdf/2309.08586.pdf",
    "upvote": "15",
    "text": "Replacing softmax with ReLU in Vision Transformers\nMitchell Wortsman\nJaehoon Lee\nJustin Gilmer\nSimon Kornblith\nGoogle DeepMind\nAbstract\nPrevious research observed accuracy degradation\nwhen replacing the attention softmax with a point-\nwise activation such as ReLU. In the context of vision\ntransformers, we find that this degradation is miti-\ngated when dividing by sequence length. Our experi-\nments training small to large vision transformers on\nImageNet-21k indicate that ReLU-attention can ap-\nproach or match the performance of softmax-attention\nin terms of scaling behavior as a function of compute.\n1\nIntroduction\nThe transformer architecture [26] is ubiquitous in mod-\nern machine learning. Attention, a central component\nof the transformer [2], includes a softmax which pro-\nduces a probability distribution over tokens. Softmax\nis costly due to an exponent calculation and a sum\nover sequence length which makes parallelization chal-\nlenging [24, 7].\nIn this report we explore point-wise alternatives to the\nsoftmax operation which do not necessarily output a\nprobability distribution. As a highlight, we observe\nthat attention with ReLU divided by sequence length\ncan approach or match traditional softmax attention\nin terms of scaling behavior as a function of com-\npute for vision transformers. This result presents new\nopportunities for parallelization, as ReLU-attention\ncan be parallelized over the sequence length dimen-\nsion with fewer gather operations than traditional\nattention.\n2\nRelated work\nPrevious research has explored substituting softmax\nwith ReLU [25, 14] or squared ReLU [15]. However,\nthese approaches do not divide by sequence length,\nwhich we experimentally find is important to reach\naccuracy comparable to softmax. In addition, previous\nresearch [21] has replaced softmax while still requiring\nnormalization over the sequence length axis to ensure\n102\nTPU core hours\n0.68\n0.70\n0.72\n0.74\n0.76\n0.78\n0.80\n0.82\nImageNet-1k accuracy (%)\nsoftmax\nrelu/seqlen\nS/32\nS/16\nB/32\nB/16\nL/16\n102\nTPU core hours\n0.72\n0.74\n0.76\n0.78\n0.80\n0.82\n0.84\nAvg. 10-shot linear transfer on 8 datasets\nFigure 1: Replacing softmax with relu/seqlen approaches or matches the scaling performance of traditional attention\nfor vision transformers [10] with qk-layernorm [8]. This figure displays results for small to large vision transformers\ntrained on ImageNet-21k [9] for 30 epochs. We report ImageNet-1k accuracy for ImageNet-21k models by taking the\ntop class among those that are in ImageNet-1k, without fine-tuning. Attention with ReLU can be parallelized over\nthe sequence length dimension with less gather operations than softmax attention.\n1\narXiv:2309.08586v2  [cs.CV]  17 Oct 2023\n0.0\n0.5\n1.0\n1.5\n2.0\nExponent  for scaling inverse seqlen\n0.650\n0.675\n0.700\n0.725\n0.750\n0.775\n0.800\nImageNet-1k accuracy (%)\nTraining dataset i21k. Model S/32.\n0.0\n0.5\n1.0\n1.5\n2.0\nExponent  for scaling inverse seqlen\n0.650\n0.675\n0.700\n0.725\n0.750\n0.775\n0.800\nImageNet-1k accuracy (%)\nTraining dataset i21k. Model S/16.\n0.0\n0.5\n1.0\n1.5\n2.0\nExponent  for scaling inverse seqlen\n0.650\n0.675\n0.700\n0.725\n0.750\n0.775\n0.800\nImageNet-1k accuracy (%)\nTraining dataset i21k. Model S/8.\n0.0\n0.5\n1.0\n1.5\n2.0\nExponent  for scaling inverse seqlen\n0.650\n0.675\n0.700\n0.725\n0.750\n0.775\n0.800\nImageNet-1k accuracy (%)\nTraining dataset i1k. Model S/32.\n0.0\n0.5\n1.0\n1.5\n2.0\nExponent  for scaling inverse seqlen\n0.650\n0.675\n0.700\n0.725\n0.750\n0.775\n0.800\nImageNet-1k accuracy (%)\nTraining dataset i1k. Model S/16.\nsoftmax\nrelu\nsquared relu\ngelu\nsoftplus\nidentity\nrelu6\nsigmoid\n0.0\n0.5\n1.0\n1.5\n2.0\nExponent  for scaling inverse seqlen\n0.650\n0.675\n0.700\n0.725\n0.750\n0.775\n0.800\nImageNet-1k accuracy (%)\nTraining dataset i1k. Model S/8.\nFigure 2: Replacing softmax with L\u2212\u03b1h where h \u2208 {relu, relu2, gelu, softplus, identity, relu6, sigmoid} and L is sequence\nlength. We typically observe the best results when \u03b1 is close to 1. There is no clear best non-linearity at \u03b1 \u2248 1, so we\nuse ReLU in our main experiment for its speed.\nthe attention weights sum to one. This retains the\ndownside of requiring a gather. After writing an initial\nversion of this note, it was brought to our attention\nthat the variant of ReLU-atttention we study was also\nexplored with a theoretical motivation [3, 12].\nMoreover, there is extensive literature which removes\nactivation functions altogether so that attention is\nlinear [16, 22, 18], which is useful for long sequence\nlengths.1 In our experiments, removing the activation\nentirely reduced accuracy.\n3\nMethod\nAttention.\nAttention transforms d-dimensional\nqueries, keys, and values {qi, ki, vi}L\ni=1 with a two\nstep procedure. First, attention weights \u03b1ij are pro-\nduced via\n\u03b1ij = \u03d5\n\u0012 1\n\u221a\nd\n\u0002\nq\u22a4\ni k1, ..., q\u22a4\ni kL\n\u0003\u0013\nj\n,\n(1)\n1Concretely, with linear attention, the order of matrix multi-\nplies can be switched from (qk\u22a4)v to q(k\u22a4v) which changes the\ncompute required from O(dL2) to O(d2L) where q, k, v \u2208 RL\u00d7d\nare the queries, keys, and values and L is sequence length.\nwhere \u03d5 is typically softmax.\nNext, the attention\nweights are used to compute outputs oi = PL\nj=1 \u03b1ijvj.\nThis report explores point-wise alternatives to \u03d5.\nReLU-attention. We observe that \u03d5 = L\u22121relu is a\npromising alternative to \u03d5 = softmax in Equation 1.\nWe refer to attention with \u03d5 = L\u22121relu as ReLU-\nattention.\nScaled point-wise attention. More generally, our\nexperiments will explore \u03d5 = L\u2212\u03b1h for \u03b1 \u2208 [0, 1] and\nh \u2208 {relu, relu2, gelu, softplus, identity, relu6, sigmoid}\n[6, 13].\nSequence length scaling. We observe that scaling\nby a term involving sequence length L is beneficial for\nhigh accuracy. This scaling is absent from prior work\nwhich removes softmax [15, 18]. While the central\njustification for sequence length scaling is empirical,\nwe provide brief analytical motivation.\nTransformers are currently designed with softmax at-\ntention for which PL\nj=1 \u03b1ij = 1. This implies that\nEj[\u03b1ij] = L\u22121. While it is unlikely that this is a\nnecessary condition, \u03d5 = L\u22121relu does ensure that\nEj[\u03b1ij] is O(L\u22121) at initialization. Preserving this\ncondition may alleviate the need to change other hy-\n2\n0.0\n0.5\n1.0\n1.5\n2.0\nExponent  for scaling inverse seqlen\n0.64\n0.66\n0.68\n0.70\n0.72\n0.74\n0.76\nImageNet-1k accuracy (%)\nTraining dataset i21k. Model S/32.\n0.0\n0.5\n1.0\n1.5\n2.0\nExponent  for scaling inverse seqlen\n0.64\n0.66\n0.68\n0.70\n0.72\n0.74\n0.76\nImageNet-1k accuracy (%)\nTraining dataset i21k. Model S/16.\nrelu\nsquared relu\nrelu without qk-layernorm\nsquared relu without qk-layernorm\n0.0\n0.5\n1.0\n1.5\n2.0\nExponent  for scaling inverse seqlen\n0.64\n0.66\n0.68\n0.70\n0.72\n0.74\n0.76\nImageNet-1k accuracy (%)\nTraining dataset i21k. Model S/8.\nFigure 3: The effect of removing qk-layernorm [8] on attention with ReLU and squared ReLU scaled by L\u2212\u03b1 where\nL is sequence length. Results are shown for the S/32, S/16, and S/8 vision transformer models [10, 4] trained on\nImageNet-21k.\n0.0\n0.5\n1.0\n1.5\n2.0\nExponent  for scaling inverse seqlen\n0.64\n0.66\n0.68\n0.70\n0.72\n0.74\n0.76\nImageNet-1k accuracy (%)\nTraining dataset i21k. Model S/32.\n0.0\n0.5\n1.0\n1.5\n2.0\nExponent  for scaling inverse seqlen\n0.64\n0.66\n0.68\n0.70\n0.72\n0.74\n0.76\nImageNet-1k accuracy (%)\nTraining dataset i21k. Model S/16.\nrelu\nsquared relu\nrelu with gating\nsquared relu with gating\n0.0\n0.5\n1.0\n1.5\n2.0\nExponent  for scaling inverse seqlen\n0.64\n0.66\n0.68\n0.70\n0.72\n0.74\n0.76\nImageNet-1k accuracy (%)\nTraining dataset i21k. Model S/8.\nFigure 4: The effect of using a gated attention unit [15] on attention with ReLU and squared ReLU scaled by L\u2212\u03b1\nwhere L is sequence length. Results are shown for the S/32, S/16, and S/8 vision transformer models [10, 4] trained\non ImageNet-21k.\nperparameters when replacing softmax.\nAt initialization the elements of q and k are O(1)\nand so \u27e8qi,kj\u27e9\n\u221a\nd\nwill also be O(1). Activation functions\nsuch as ReLU preserve O(1),2 and so a factor L\u22121 is\nnecessary for Ej[\u03b1ij] to be O(L\u22121).\n4\nExperiments\nExperimental\nsetup.\nOur\nexperiments\nuse\nImageNet-21k and ImageNet-1k [9] training config-\nurations from the BigVision codebase [4] without\nmodifying hyperparameters.3 In our experiments on\n2With the exception of squared ReLU.\n3For\nImageNet1k\nwe\nuse\nthe\nbase\nconfig\nhttps:\n//github.com/google-research/big_vision/blob/main/\nImageNet-21k we train for 30 epochs, and in our ex-\nperiments on ImageNet-1k we train for 300 epochs.\nAs a result, both training runs use a roughly similar\nnumber of steps of around 9e5. We use ViTs with\nqk-layernorm [8] as this was previously observed to be\nnecessary to prevent instability when scaling model\nsize. However, we ablate that this is not an important\ncomponent at the scales we test. We use i21k and i1k\nto mean ImageNet-21k and ImageNet-1k, respectively,\nand report ImageNet-1k accuracy for ImageNet-21k\nmodels by taking the top class among those that are\nin ImageNet-1k, without fine-tuning. When evalu-\nating transfer performance on downstream tasks we\nbig_vision/configs/vit_i1k.py.\nFor ImageNet21k we use\nthe base config https://github.com/google-research/big_\nvision/blob/main/big_vision/configs/vit_i21k.py.\n3\nuse a 10-shot linear probe averaged over three seeds.\nThe downstream tasks are Caltech Birds [27], Caltech-\n101 [11], Stanford Cars [19], CIFAR-100 [20], DTD [5],\nColHsit [17], Pets [23], and UC Merced [28].\nMain experiment. Figure 1 illustrates that ReLU-\nattention matches the scaling trends for softmax at-\ntention for ImageNet-21k training.\nOn the x-axis\nwe display the total core hours required for the ex-\nperiment. As an advantage, ReLU-attention enables\nparallelization over the sequence length dimension\nwith fewer gather operations than softmax attention.\nEffect of sequence length scaling. Figure 2 ex-\namines the effect of sequence length scaling for var-\nious point-wise alternatives to softmax. Concretely,\nwe replace softmax with L\u2212\u03b1h for \u03b1 \u2208 [0, 1] and\nh \u2208 {relu, relu2, gelu, softplus, identity}. On the x-axis\nwe display \u03b1. The y-axis displays accuracy for the\nS/32, S/16, and S/8 vision transformer models [10, 4].\nThe best results are typically achieved when \u03b1 is close\nto 1. Since there is not clear best non-linearity, we\nuse ReLU in our main experiment as it is faster.\nEffect of qk-layernorm. Our main experiments use\nqk-layernorm [8] in which queries and keys are passed\nthrough LayerNorm [1] before computing attention\nweights. We use qk-layernorm by default as it was\nfound to be necessary to prevent instability when\nscaling up model size [8]. Figure 3 shows the effect\nof removing qk-layernorm. The results indicate that\nqk-layernorm does not have a large effect for these\nmodels, but this may change at scale.\nEffect of adding a gate. Previous work removing\nsoftmax adds a gated unit and does not scale by se-\nquence length [15]. Concretely, in the gated attention\nunit [15] an extra projection produces output which\nis combined through elementwise-multiplication be-\nfore the out projection. In Figure 4 we investigate\nwhether the presence of a gate removes the need for\nsequence length scaling. Overall we observe that the\nbest accuracy is still achieved with sequence length\nscaling, with or without the gate. Note that gating\nincreases the core hours required for the experiment\nby roughly 9.3% for the S/8 model with ReLU.\n5\nConclusion\nThis report leaves many open questions. In particular,\nwe are unsure why the factor L\u22121 improves perfor-\nmance or if this term could be learned. Moreover, it\nis likely that there is a better activation function that\nwe do not explore.\nAcknowledgements\nWe thank Lucas Beyer, Mostafa Dehghani, and David\nFleet for their helpful comments and suggestions.\nWe thank the members of the Google DeepMind PAGI\nteam for their support of this effort, Jascha Sohl-\ndickstein, Noah Fiedel, Aaron Parisi, Abhishek Ku-\nmar, Alex Alemi, Alex Rizkowsky, Avi Singh, Azade\nNova, Ben Adlam, Bernd Bohnet, Daniel Freeman,\nGamaleldin Elsayed, Gaurav Mishra, Hanie Sedghi,\nIsabelle Simpson, Izzeddin Gur, JD Co-Reyes, James\nHarrison, Jeffrey Pennington, Jiri Hron, Kathleen Ke-\nnealy, Kelvin Xu, Kevin Swersky, Kshiteej Mahajan,\nLaura Culp, Lechao Xiao, Max Bileschi, Merrie Mor-\nris, Roman Novak, Rosanne Liu, Sharad Vikram, Tris\nWarkentin, Yundi Qian.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E\nHinton.\nLayer normalization.\narXiv preprint\narXiv:1607.06450, 2016.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua\nBengio.\nNeural machine translation by jointly\nlearning to align and translate.\narXiv preprint\narXiv:1409.0473, 2014.\n[3] Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and\nSong Mei. Transformers as statisticians: Provable in-\ncontext learning with in-context algorithm selection.\narXiv preprint arXiv:2306.04637, 2023.\n[4] Lucas\nBeyer,\nXiaohua\nZhai,\nand\nAlexander\nKolesnikov. Better plain vit baselines for imagenet-\n1k. arXiv preprint arXiv:2205.01580, 2022. URL\nhttps://arxiv.org/abs/2205.01580.\n[5] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos,\nSammy Mohamed, and Andrea Vedaldi. Describing\ntextures in the wild. In Conference on Computer\nVision and Pattern Recognition (CVPR), 2014. https:\n//arxiv.org/abs/1311.3618.\n[6] George E Dahl, Tara N Sainath, and Geoffrey E\nHinton. Improving deep neural networks for lvcsr\nusing rectified linear units and dropout.\nIn 2013\nIEEE international conference on acoustics, speech\nand signal processing, pages 8609\u20138613. IEEE, 2013.\n[7] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and\nChristopher R\u00b4e. Flashattention: Fast and memory-\nefficient exact attention with io-awareness. Advances\n4\nin Neural Information Processing Systems, 35:16344\u2013\n16359, 2022.\n[8] Mostafa Dehghani, Josip Djolonga, Basil Mustafa,\nPiotr Padlewski, Jonathan Heek, Justin Gilmer,\nAndreas Steiner, Mathilde Caron, Robert Geirhos,\nIbrahim Alabdulmohsin, et al. Scaling vision trans-\nformers to 22 billion parameters.\narXiv preprint\narXiv:2302.05442, 2023.\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In Conference on Computer Vision\nand Pattern Recognition, 2009. https://ieeexplore.\nieee.org/document/5206848.\n[10] Alexey\nDosovitskiy,\nLucas\nBeyer,\nAlexander\nKolesnikov,\nDirk\nWeissenborn,\nXiaohua\nZhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-\nreit, and Neil Houlsby. An image is worth 16x16\nwords: Transformers for image recognition at scale.\nIn International Conference on Learning Representa-\ntions (ICLR), 2021. https://arxiv.org/abs/2010.\n11929.\n[11] Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning\ngenerative visual models from few training exam-\nples: An incremental bayesian approach tested on 101\nobject categories. In 2004 conference on computer vi-\nsion and pattern recognition workshop, pages 178\u2013178.\nIEEE, 2004.\n[12] Hengyu Fu, Tianyu Guo, Yu Bai, and Song Mei.\nWhat can a single attention layer learn?\na study\nthrough the random features lens. arXiv preprint\narXiv:2307.11353, 2023.\n[13] Dan Hendrycks and Kevin Gimpel. Gaussian error\nlinear units (gelus). arXiv preprint arXiv:1606.08415,\n2016.\n[14] Jiri Hron, Yasaman Bahri, Jascha Sohl-Dickstein, and\nRoman Novak. Infinite attention: Nngp and ntk for\ndeep attention networks. In International Conference\non Machine Learning, pages 4376\u20134386. PMLR, 2020.\n[15] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le.\nTransformer quality in linear time. In International\nConference on Machine Learning, pages 9099\u20139117.\nPMLR, 2022.\n[16] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-\npas, and Fran\u00b8cois Fleuret. Transformers are RNNs:\nFast autoregressive transformers with linear atten-\ntion.\nIn Hal Daum\u00b4e III and Aarti Singh, editors,\nProceedings of the 37th International Conference on\nMachine Learning, volume 119 of Proceedings of Ma-\nchine Learning Research, pages 5156\u20135165. PMLR, 13\u2013\n18 Jul 2020. URL https://proceedings.mlr.press/\nv119/katharopoulos20a.html.\n[17] Jakob\nNikolas\nKather,\nFrank\nGerrit\nZ\u00a8ollner,\nFrancesco Bianconi, Susanne M Melchers, Lothar R\nSchad, Timo Gaiser, Alexander Marx, and Cleo-Aron\nWeis. Collection of textures in colorectal cancer his-\ntology. Zenodo https://doi. org/10, 5281, 2016.\n[18] Soroush Abbasi Koohpayegani and Hamed Pirsiavash.\nSima: Simple softmax-free attention for vision trans-\nformers. arXiv preprint arXiv:2206.08898, 2022.\n[19] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-\nFei. 3d object representations for fine-grained cat-\negorization. In International Conference on Com-\nputer Vision (ICCV) Workshops, 2013.\nhttps://\nieeexplore.ieee.org/document/6755945.\n[20] Alex Krizhevsky, Geoffrey Hinton, et al.\nLearn-\ning\nmultiple\nlayers\nof\nfeatures\nfrom\ntiny\nim-\nages, 2009.\nhttps://www.cs.toronto.edu/~kriz/\nlearning-features-2009-TR.pdf.\n[21] Zhiyuan Li, Srinadh Bhojanapalli, Manzil Zaheer,\nSashank Reddi, and Sanjiv Kumar. Robust training\nof neural networks using scale invariant architectures.\nIn International Conference on Machine Learning,\npages 12656\u201312684. PMLR, 2022.\n[22] Jiachen Lu, Jinghan Yao, Junge Zhang, Xiatian Zhu,\nHang Xu, Weiguo Gao, Chunjing Xu, Tao Xiang,\nand Li Zhang. Soft: Softmax-free transformer with\nlinear complexity. Advances in Neural Information\nProcessing Systems, 34:21297\u201321309, 2021.\n[23] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman,\nand CV Jawahar. Cats and dogs. In 2012 IEEE\nconference on computer vision and pattern recognition,\npages 3498\u20133505. IEEE, 2012.\n[24] Markus N Rabe and Charles Staats. Self-attention\ndoes not need o(n2) memory.\narXiv preprint\narXiv:2112.05682, 2021.\n[25] Kai Shen, Junliang Guo, Xu Tan, Siliang Tang, Rui\nWang, and Jiang Bian. A study on relu and softmax in\ntransformer. arXiv preprint arXiv:2302.06461, 2023.\n[26] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez,  Lukasz\nKaiser, and Illia Polosukhin. Attention is all you\nneed. Advances in neural information processing sys-\ntems, 30, 2017.\n[27] Peter Welinder, Steve Branson, Takeshi Mita, Cather-\nine Wah, Florian Schroff, Serge Belongie, and Pietro\nPerona. Caltech-ucsd birds 200. 2010.\n5\n[28] Yi Yang and Shawn Newsam. Bag-of-visual-words\nand spatial extensions for land-use classification. In\nProceedings of the 18th SIGSPATIAL international\nconference on advances in geographic information sys-\ntems, pages 270\u2013279, 2010.\n6\n"
  },
  {
    "title": "Scaling Laws for Sparsely-Connected Foundation Models",
    "link": "https://arxiv.org/pdf/2309.08520.pdf",
    "upvote": "12",
    "text": "SCALING LAWS FOR SPARSELY-CONNECTED\nFOUNDATION MODELS\nElias Frantar\u2217\nIST Austria\u2020\nCarlos Riquelme\nGoogle DeepMind\nNeil Houlsby\nGoogle DeepMind\nDan Alistarh\nIST Austria\nUtku Evci\u2217\nGoogle DeepMind\nABSTRACT\nWe explore the impact of parameter sparsity on the scaling behavior of Trans-\nformers trained on massive datasets (i.e., \u201cfoundation models\u201d), in both vision\nand language domains. In this setting, we identify the first scaling law describ-\ning the relationship between weight sparsity, number of non-zero parameters, and\namount of training data, which we validate empirically across model and data\nscales; on ViT/JFT-4B and T5/C4. These results allow us to characterize the \u201cop-\ntimal sparsity\u201d, the sparsity level which yields the best performance for a given\neffective model size and training budget. For a fixed number of non-zero parame-\nters, we identify that the optimal sparsity increases with the amount of data used\nfor training. We also extend our study to different sparsity structures (such as the\nhardware-friendly n:m pattern) and strategies (such as starting from a pretrained\ndense model). Our findings shed light on the power and limitations of weight\nsparsity across various parameter and computational settings, offering both the-\noretical understanding and practical implications for leveraging sparsity towards\ncomputational efficiency improvements.\n1\nINTRODUCTION\nFoundation models (Bommasani et al., 2021), loosely defined as large (often Transformer-based\n(Vaswani et al., 2017)) networks that are trained on massive quantities of highly general data, have\ndriven significant progress in deep learning, for both natural language (Brown et al., 2020) and vi-\nsion tasks (Dosovitskiy et al., 2021). One key property of such models is the predictability of their\nperformance when scaling various model attributes, such as the number of parameters, training char-\nacteristics and the amount of data or computation used (Kaplan et al., 2020). This is encapsulated\nby scaling laws, which make it possible to accurately predict the performance of a model specified\njust through its high-level parameters like size, data and computation.\nA parallel trend, motivated by computational costs, has been the focus towards increased efficiency\nfor large models. This is usually achieved by employing compressed parameterizations via quanti-\nzation (Gholami et al., 2021) or sparsification (Hoefler et al., 2021), during inference and/or training,\nwhich can lead to real-world speedups via both software and hardware support (Elsen et al., 2020;\nYao et al., 2022). Despite major community interest in efficiency, the impact of these compressed\nrepresentations, in particular of parameter/weight sparsity, on the scaling behavior of foundation\nmodels is not well understood; especially, when applying powerful but expensive training-based\ncompression methods (Jacob et al., 2018; Zhu & Gupta, 2017).\nIn this paper, we aim to address this gap by studying the relationship of sparsity with the scaling\nlaws of foundation models. Specifically, we focus on weight sparsity, that is, on networks whose in-\ndividual connections are pruned, and on Transformer-family (Vaswani et al., 2017) models for both\nvision (Dosovitskiy et al., 2021) and language (Raffel et al., 2020b) domains. We use the massive\nJFT-4B (Google, 2023a) and C4 (Raffel et al., 2020a) datasets, which are several orders of magnitude\nlarger than what has been employed so far by the vast majority of work on sparsity. In this mas-\nsive dataset regime, dense models continue to improve with prolonged training, thus it is currently\nunclear whether sparse models can win at all in a fair comparison using equal amounts of training\ncompute. This is in contrast to popular pruning benchmarks (e.g., ImageNet (Deng et al., 2009)\n\u2217Corresponding authors: elias.frantar@ist.ac.at, evcu@google.com.\n\u2020Work done as a student researcher at Google DeepMind.\n1\narXiv:2309.08520v1  [cs.LG]  15 Sep 2023\n2 \u00d7 100\n3 \u00d7 100\nPredicted L\n2 \u00d7 100\n3 \u00d7 100\nMeasured L\nQuality of fit for L(S, N, D) scaling law\nfitting data\nextrapolation\n106\n107\n108\n109\n#non-zero parameters\n1017\n1018\n1019\n1020\n1021\n1022\n#pretraining FLOPs\nOptimal sparsity contours for ViT/JFT\nDense FLOPs\nSparse FLOPs\n87.50%\n75.00%\n50.00%\n00.00%\nChinchilla\nFigure 1: (Left) Fit and extrapolation quality of the L(S, N, D) scaling law on T5/C4. (Right)\nOptimal sparsity Sopt contours fitted on ViT/JFT, for sparse and dense costs (details in Section 3.3).\npruning) where dense models tend to saturate quickly (Kuznedelev et al., 2023), allowing sparse\nmodels to achieve major gains relative to dense models with a comparable number of parameters.\nIn order to quantify the benefits of sparsity, or the lack thereof, in this large-dataset regime we\ndevelop joint scaling laws that relate the sparsity of a network, its effective size and the amount of\ndata used for training. We show that, for sparsity S, number of non-zero parameters N and amount\nof training data/steps D, the validation loss L approximately satisfies the following law, for both\nvision and language tasks:\nL(S, N, D) =\n\u0010\naS(1 \u2212 S)bS + cS\n\u0011\n\u00b7\n\u0010 1\nN\n\u0011bN\n+\n\u0010aD\nD\n\u0011bD\n+ c,\n(1)\nIntuitively, the first two summands capture the power law scaling in terms of capacity, i.e. sparsity\nand non-zero parameters, and data, respectively, while c is a lower bound on the achievable task\nloss. In more detail, the first multiplicative term captures the impact of sparsity, here expressed as\nremaining density (1\u2212S), which itself follows a saturating power-law with coefficient aS, exponent\nbS and limit constant cS. The exponents bN and bD scale the (non-zero) parameter count N, and the\ndata D term, respectively, as is common in classical scaling laws (Kaplan et al., 2020).\nWe validate this formula empirically using large vision and language datasets, several model sizes,\namounts of training data and sparsity levels. Please see Figure 1 (Left) for an illustration of the\nscaling law fit and extrapolation quality. In turn, this law allows us to obtain several new insights\nregarding both the power and limitations of weight sparsity, in the foundation model setting:\n\u2022 First, it suggests that sparsity affects each model size in a similar way, i.e., as a multiplicative con-\nstant to the size scaling. At the same time, sparsification does not appear to interact significantly\nwith the data scaling; the original dense term in D is preserved.\n\u2022 Second, we can use our scaling law in Equation (1) to analytically derive the optimal sparsity Sopt\nfor a given inference size and training budget, allowing us to predict the regime where sparsity\ncould actually provide benefits over simple dense model rescaling and extended training.\n\u2022 Our analysis of optimal sparsity Sopt, demonstrated in Figure 1 (Right), shows that its iso-contours\nrun parallel to the dense compute optimal Chinchilla line (Hoffmann et al., 2022) of the respective\nmodel and task. Importantly, the optimal sparsity increases with longer training. Further, while\noptimal dense models define a line on the parameter-FLOPs surface, optimal sparse models form\na half-plane (with different sparsities unlocking multiple optimal sizes for a fixed training cost).\n\u2022 In addition, we find that the main conclusions of our law hold also for the hardware-friendly\nn:m sparsity patterns (Mishra et al., 2021) and that pruning well-trained dense models is more\nefficient than training from scratch (while sparsifying), if dense checkpoints already exist, but is\nsignificantly slower otherwise.\nIn sum, our results provide the first scaling law for characterizing the impact of sparsity on the perfor-\nmance of Transformers trained on massive datasets. From the conceptual perspective, this provides a\nsimple tool to understand the power\u2013but also the limitations\u2013of sparsity for a given task/model com-\nbination. From the practical side, this can be used to determine whether sparsity can be a reasonable\noption for inference or training speedups, in settings where specific software/hardware support for\nsuch compressed representations is available.\n2\n2\nFAIR EVALUATION IN THE PRESENCE OF STRONG SCALING\nIn the context of modern Transformers trained on massive datasets, popular evaluation approaches\n(Gale et al., 2019; Singh & Alistarh, 2020; Sanh et al., 2020; Schwarz et al., 2021; Benbaki et al.,\n2023) that have been reasonable for standard pruning benchmarks like ResNet50/ImageNet (Singh &\nAlistarh, 2020; Schwarz et al., 2021) or BERT/GLUE (Sanh et al., 2020; Kurtic et al., 2022), require\ncareful reconsideration to ensure meaningful comparisons. The primary reason for this, which we\ndetail below, is that Transformers trained on massive quantities of data exhibit very different scaling\nbehavior (Kaplan et al., 2020; Hoffmann et al., 2022):\n\u2022 Training data. In a standard setting such as ResNet50/ImageNet, significantly increasing the\ntraining time of the dense model will quickly run into overfitting (Kuznedelev et al., 2023). In\ncontrast, the performance improvements of ViT/JFT only start to saturate after extremely long\ntraining time (Zhai et al., 2022); overfitting is virtually non-existent. Consequently, the result of\nsparsifying a ViT pretrained on 100M images over another 100M images (a standard setup for\nRN50/ImageNet pruning) should not be compared against the initial model as the sparse version\nhas had twice as much overall training. Instead, the proper reference point is a dense model trained\non 200M images. However, this dense model will likely be significantly more accurate.\n\u2022 Model size. Developing small but accurate dense models used to require arranging many custom\nmodules into a carefully engineered architecture (Howard et al., 2017; Tan & Le, 2019). Naively\nscaling down a 25M parameter ResNet50 by a factor of 10 will not yield a competitive 2.5M\nparameter ImageNet model, which is why most pruning papers omit a comparison against such a\nvariant. However, when considering Transformer models and massive datasets, basic width and\ndepth scaling typically results in a very strong family of differently-sized models. Hence, it is\ncritical to always compare sparse models with a dense version of equivalent parameter count.\n\u2022 Computational costs. Jointly considering training data and model size leads to the concept of\ncompute efficiency (Hoffmann et al., 2022), which is generally disregarded in classic sparsity\nbenchmarks since training is cheap enough to reach full convergence on all models. However, a\nsmaller Transformer trained for longer can outperform a larger one trained with the same budget\n(i.e., for less steps). This effect renders proper comparisons even more challenging. For example,\nit means that a 50% sparse model obtained from pruning a model that was pretrained for 100K\nsteps should be compared to a 2\u00d7 smaller dense model trained for the same compute, i.e., 200K\nsteps plus the computational cost of pruning.\nIn summary, in a fair foundation model pruning setup, sparsity should not be able to leverage in-\ncreased training time, a significantly better optimized dense base architecture or more training com-\npute. Otherwise, comparisons would unfairly favor sparse models, since equivalently sized dense\nversions could not fully exploit their strong scaling properties across all these axes. We would like\nto note that it is currently unclear whether weight-sparse foundation models can win at all in this\nhighly challenging setting, where all these factors are properly accounted for. Conclusively answer-\ning this question will require a full understanding of the joint scaling between sparsity, model size\nand training data/compute, towards which we take the first step in this paper.\n3\nSCALING LAWS FOR PARAMETER-SPARSE TRANSFORMERS\n3.1\nEXPERIMENTAL SETUP\nThis section briefly summarizes the setup of our main experiments, extensive sweeps across sparsity,\nsize and data, that we will then subsequently use to develop scaling laws. A detailed discussion of\nall our choices, including hyper-parameters, can be found in Appendix A.\nOverview.\nIn terms of models and datasets, we focus on Vision Transformers (Dosovitskiy et al.,\n2021) trained for multi-label image classification on the JFT-4B dataset (Dehghani et al., 2023),\nconsisting of 4 billion images, as well as encoder-decoder T5 models (Raffel et al., 2020b) (improved\n1.1 version (Google, 2023b)) trained for masked-language-modelling on C4 (Raffel et al., 2020b),\nconsisting of 150+ billion tokens. We follow the model\u2019s respective original training recipes (Zhai\net al., 2022; Raffel et al., 2020b) and carry out sparsification during training via gradual magnitude\npruning (Zhu & Gupta, 2017), using a cubic schedule starting at 25% of training and ending at 75%.\n3\nIn general, we note that our setup is optimized for robustness and consistency across scales rather\nthan to fully maximize pruning performance on one particular setting (see also Appendix A and B).\nSweep grids.\nTable 1 lists the grid parameters that we sweep over. For ViTs, we consider 7 target\nmodels sizes in 2\u00d7 increments each, while we use 4 targets sizes in increments of 4\u00d7 for T5. Vision\nTransformers are trained for 4 different lengths, with the longest corresponding to \u2248 1.8 billion\nimages; language models are trained for 3 different lengths up to \u2248 65 billion tokens. The set of\nsparsity targets is the same in both cases, corresponding to 2, 4 and 8\u00d7 compression rate. Overall, the\nViT grid was designed to be more extensive whereas the T5 setup was chosen to be more efficient.\nModel family\nViT\nT5\n#Non-zero params\n0.66M, 1.33M, ..., 42.4M\n1.3M, 5.3M, ..., 85M\nTraining steps\n55K, 110K, 220K, 440K\n250K, 500K, 1M\nSparsities\n0.0, 0.5, 0.75, 0.875\n0.0, 0.5, 0.75, 0.875\nTotal #runs\n112\n48\nTable 1: Grid definition for our main scaling sweeps.\nWe execute all runs in the above grids and record the resulting validation losses. This data is then\nused to fit parametric scaling curves.\n3.2\nDERIVING THE CORE LAW\nDense scaling.\nIt is well established (Kaplan et al., 2020; Hoffmann et al., 2022) that the pre-\ntraining validation loss of dense Transformers can be approximately modeled, in terms of parameter\ncount N and amount of training data D, by functions of the following form:\nL(N, D) =\n\u0010aN\nN\n\u0011bN\n+\n\u0010aD\nD\n\u0011bD\n+ c.\n(2)\nThe first two summands capture the power law scaling in terms of size and data, respectively. Mean-\nwhile, c represents the inherent stochasticity of the modelling problem as a lower bound on the loss.\nThe scaling exponents bN and bD are usually quite stable for a particular task, whereas the constant\ncoefficients aN and aD vary with minor process changes like a different architecture or optimizer.\nScaling laws usually assume an ideal training setup with no data repetition and focus on modelling\nthe non-bottlenecked regime (e.g., with sufficient steps/data/batchsize/etc.) rather than on edge cases\n(Kaplan et al., 2020; Hoffmann et al., 2022); we follow suit. Further, we deliberately consider the\npretraining loss and infinite data setting to assess the effectiveness of sparsity in its most challenging\n(one essentially needs to fit the data as well as possible) yet also most useful application (all further\npost-processing would directly benefit from a compressed base model).\nPreliminary observations.\nThe key question we hope to address is how parameter sparsity S\nenters this core scaling relationship; understanding this will enable studying other interesting as-\npects like optimal sparsity or limit performance. A priori, it is not obvious how S should enter into\nEquation (2) to form L(S, N, D), where N denotes the number of non-zero parameters. Are larger\nmodels easier to sparsify, does longer training help highly sparse models more, or is sparsity mostly\nindependent of other parameters? Therefore, to get a first idea about what kind of shape we should\nexpect for L(S, N, D), we execute the T5 sweep defined in Table 1 and visualize the results. Fig-\nure 2 shows validation loss (with a lower bound c = 1 subtracted to account for power law saturation\nagainst the inherent uncertainty limit) versus model size for all sparsity levels, grouped by the num-\nber of training steps. Please observe that the scaling of this plot, as well as most other visualizations\nin this paper, is log-log.\nWe make three major observations from these graphs:\n1. The loss vs. #non-zero curves for all sparsity levels seem to form almost parallel lines, differing\nprimarily in the intercept.\n2. The higher the sparsity the lower the loss, but gains are quickly diminishing.\n3. The overall shape of all curves is very similar for each training duration, the y-axis just tends to\nshift a bit downwards with more training steps.\n4\n107\n108\n#non-zero parameters\n100\n6 \u00d7 10\n1\n2 \u00d7 100\nval-loss \n low-bound\n250K steps\n107\n108\n#non-zero parameters\n500K steps\n107\n108\n#non-zero parameters\n1M steps\n00.00%\n50.00%\n75.00%\n87.50%\nFigure 2: Visualization of T5/C4 sweep results for all sizes and sparsities, grouped by training steps.\nSparse scaling law.\nWe now use the previous insights to construct our L(S, N, D) formula. Ob-\nservation 1 suggests that the model size power law scaling for all sparsity levels differs primarily in\na constant factor (intercept in a log-log plot); bN stays consistent. Based on observation 2, we model\nthis sparsity factor as a (quickly) saturating power law. Finally, observation 3 indicates that sparsity\nand data scaling are mostly independent, hence we simply keep the original D-term. In summary,\nthese observations lead us to the following formula for the joint scaling law:\nL(S, N, D) =\n\u0010\naS(1 \u2212 S)bS + cS\n\u0011\n\u00b7\n\u0010 1\nN\n\u0011bN\n+\n\u0010aD\nD\n\u0011bD\n+ c.\n(3)\nTo properly model that 0.75 is twice as sparse as 0.5, we define the sparsity power-law part via the\ncorresponding compression rate 1/(1 \u2212 S). Further, aN is subsumed by aS and cS, leaving 7 free\nparameters. On a high level, our scaling law combines a capacity limit term, comprised of size and\nsparsity (which can encode extra information via its zero pattern), with the standard data limit term.\nWe note that this formulation suggests that higher sparsity is always better (but with potentially quite\nquickly saturating improvements), which may not be true in practice. For very high sparsity (e.g.,\n64\u00d7 compression) we sometimes see slightly worse performance, presumably due to imperfections\nin the pruning and optimization process. This phenomenon could potentially be modelled by a\nquadratic, but for the present study we treat this as a bottleneck-case that we do not necessarily\ncapture. Lastly, S = 0 recovers the established L(N, D) form.\nT5/C4 results.\nNext, we fit the coefficients of L(S, N, D) to our entire T5 sweep data. This\nis accomplished, following (Hoffmann et al., 2022), by minimizing the Huber-loss of log L with\n\u03b4 = 0.001 (for robustness against outliers) using BFGS, for multiple random starting points. We\nplot actual vs. predictions in Figure 1 (Right) to judge the quality of our final fit (see Appendix C\nfor coefficient values). All in all, the predictions match the observed data quite closely (despite\nhaving \u2248 7 datapoints per free parameter), demonstrating the compatibility of the law in (3) with\nthe observations.\nFurthermore, we evaluate extrapolation performance by pruning a 2.3 billion parameter model to\n75% sparsity. This constitutes an \u2248 6.75\u00d7 larger target number of non-zero parameters than the\nmaximum in our fitting data, which is a similar level of extrapolation as was done for Chinchilla\n(Hoffmann et al., 2022). To avoid any architecture bottlenecks and achieve better training utilization,\nwe use the T5-XL architecture (rather than a simply rescaled T5-base) and train with batchsize\n256 for 250k steps (rather than 500k with batchsize 128). Despite these changes to our setup, the\nprediction of our fitted scaling law is quite close to the actual validation loss; see Figure 1 (Right).\nViT/JFT-4B results.\nLastly, we execute the ViT sweep listed in Table 1 and also fit a scaling law\nof the same (3) form as for the T5 data. Here we use \u03b4 = 0.01 and do not take the log of L as\nwe find the NLP-optimized settings from before to exclude outliers too aggressively for ViT data\n(which gives a poor fit for smaller models). We note that this sweep contains > 2\u00d7 more datapoints,\nleading to more robust coefficient estimates. We qualitatively compare predicted and actual loss-vs-\ndata curves in Figure 3, organized by sparsity level. We strongly emphasize that the predictions in\nall subplots here are produced by a single joint law with the same parameters (not one fit per image).\nAs can be seen, for the most part, our law appears to match the collected datapoints very well. Only\nat the lowest amount of training, some points are a bit off the prediction curve; we suspect that this\n5\n101\n5 \u00d7 100\n6 \u00d7 100\n7 \u00d7 100\n8 \u00d7 100\n9 \u00d7 100\nValidation loss\n00.00% sparsity for different sizes\n50.00% sparsity for different sizes\n108\n109\nPretraining data\n101\n5 \u00d7 100\n6 \u00d7 100\n7 \u00d7 100\n8 \u00d7 100\n9 \u00d7 100\nValidation loss\n75.00% sparsity for different sizes\n108\n109\nPretraining data\n87.50% sparsity for different sizes\n0.66M\n1.33M\n2.65M\n5.31M\n10.62M\n21.23M\n42.47M\n84.93M\n169.87M\nFigure 3: Visual comparison of the ViT scaling sweep data and the corresponding fitted scaling law.\nmay be related to the fact that these runs only involve comparatively few training steps, which may\nbe a slight bottleneck for the optimization process.\n3.3\nOPTIMAL SPARSITY\nOne particularly interesting feature of the joint scaling law just derived is that it allows easily com-\nparing models with different sparsities but the same number of non-zero parameters and training\ncost. Thus, we can determine in which situations sparse models are better than dense ones, accord-\ning to all criteria discussed in Section 2. Specifically, we can define the following quantity:\nOptimal sparsity. The sparsity value Sopt(N, C) which yields the lowest validation loss for a fixed\nnumber of non-zero parameters N and fixed training cost C.1\nThere are two ways of defining training costs in this context: (a) densely, as the cost of training a\ndense base model of size N/(1 \u2212 S) for the same amount of training steps, or (b) sparsely, as the\nactual FLOPs spent to produce the sparse model, assuming that sparsity can be perfectly exploited\nduring training as soon as it appears. For our particular sparsification schedule, (b) can be calculated\nby multiplying the training costs of a dense model, approximated as 6ND (Kaplan et al., 2020) (or\nhalf for encoder-decoder architecture models), by (see Appendix D for derivation):\ncmul(S) = (0.25 + 0.50 \u00b7 (1 \u2212 0.75 \u00b7 S))/(1 \u2212 S) + 0.25.\n(4)\nAs we have assumed that the amount of training equals the amount of new data, we can determine\nthe performance of a sparsity S model trained for compute C = 6ND \u00b7 cmul(S) by querying L with\nDS = (C/6N)/cmul(S), i.e., scaling down the D corresponding to C by the increase in training\ncosts of the sparse model. Inserting DS and then differentiating with respect to S gives the contour\nline for which sparsity S is optimal, i.e., achieves the lowest loss among all possible sparsity choices,\nwhen training for the same compute:\naDbD \u00b7 c\u2032\nmul(S)\ncmul(S) \u00b7 (DS/cmul(S))\u2212bD = aSbS \u00b7 (1 \u2212 S)bS\u22121 \u00b7 N \u2212bN .\n(5)\nAn interesting property about this contour is that it implies DS = O(N bN/bD), meaning that if\ndata- is stronger than size-scaling, then the same sparsity is optimal for a smaller data-to-size ratio\n1We note that it is common in the literature (Hoffmann et al., 2022) to define scaling laws in terms of\nparameters N and data D, but switch to expressing scaling in terms of computational cost C whenever relevant.\n6\non larger models. This is sensible as a process bottlenecked more by capacity than by data will\nbenefit more from increasing the former, e.g., by adding sparsity. Finally, we want to point out that\nSopt can often also be determined explicitly by solving (4) for S, e.g., here for dense training costs\nwith cmul(S) = 1/(1 \u2212 S):\nSopt(N, C) = max\nn\n1\u2212exp\n\u0010h\nlogbNaDbD\naSbS\n+bNlogN\ni\n/(bD +bS)\n\u0011\n\u00b7\n\u0010 C\n6N\n\u0011\u2212bD/(bD+bS)\n, 0\no\n. (6)\n106\n107\n108\n109\n#non-zero parameters\n108\n1011\n1014\n1017\n1020\n1023\n#pretraining FLOPs\nOptimal sparsity contours for T5/C4\nDense FLOPs\nSparse FLOPs\n87.50%\n75.00%\n50.00%\n00.00%\nChinchilla\nFigure 4: Optimal T5 sparsity contours.\nEmpirical results.\nWe now compute optimal sparsity\ncurves for our experimental T5 and ViT data, for which\nwe fit scaling laws in the previous subsection. Figure 1\n(Right) and 4 show the optimal sparsity contours, both for\ndense and sparse costs. An interesting feature of Equation\n(5) is that all sparsity contours are, by construction, par-\nallel to the Chinchilla compute optimal line (Hoffmann\net al., 2022), which denotes ideal utilization of training\nFLOPs for fully dense models; this can be clearly ob-\nserved in the plots as well. However, we note that the\nChinchilla line does not necessarily correspond to the\nS = 0 case since non-zero sparsity may be optimal in\nthis regime (this is the case for sparse-FLOPs).\nThe key take-away from these results is that as one trains significantly longer than Chinchilla (dense\ncompute optimal), more and more sparse models start to become optimal in terms of loss for the\nsame number of non-zero parameters. This is because the gains of further training dense models\nstart to slow down significantly at some point, allowing sparse models to overtake them. We further\nillustrate this effect on a subset of our actual ViT data in Figure 5.\nThe practical question now is how much longer training is necessary? In terms of sparse FLOPs,\n50% sparsity is already optimal for < 2\u00d7 (ViT) and < 3\u00d7 (T5) longer training than Chinchilla; for\ndense FLOPs it is \u2248 5\u00d7 and \u2248 70\u00d7, respectively. While the latter number seems quite high at first\nglance, we note that language models of the sizes we consider here are already typically trained for\n> 100\u00d7 longer than Chinchilla (Brown et al., 2020). Additionally, larger models are being trained\nwith more and more data as well, e.g., Llama2-7B with \u2248 14\u00d7 Chinchilla (Touvron et al., 2023b).\nIn general, the optimal sparsity at a given point (N, C) is lower for dense than sparse FLOPs since\nthe former assumes that sparsity provides no benefits during training.\n1017\n1018\n1019\n#sparse pretraining FLOPs\n101\n8\n9\nValidation loss\n0.66M #nonzeros\n1018\n1019\n#sparse pretraining FLOPs\n7.0\n8.0\n9.0\n1.33M #nonzeros\n1018\n1019\n#sparse pretraining FLOPs\n7.0\n8.0\n2.65M #nonzeros\n00.00%\n50.00%\n75.00%\n87.50%\nFigure 5: Loss vs. sparse pretraining FLOPs for ViT models of varying sparsity.\n3.3.1\nLIMIT PERFORMANCE\nIn the previous section, we have focused only on when sparse models become optimal but not how\nmuch better they can be compared to dense models. In this section, we study the following question:\nHow much larger, and thus computationally more expensive, does a dense model need to be in\norder to match the loss of a smaller sparse model with very long training? Since we have found the\nscaling term in D to not interact with sparsity in Section 3.2, it suffices to compute the increase in\nN required to lower the loss by the same factor as the increase in S via:\ngain(S) =\n\u0010aS(1 \u2212 S)bS + cS\naS + cS\n\u0011\u22121/bN\n.\n(7)\n7\nThe gains for our particular scaling coefficients are shown in Table 2. They are to be interpreted in\nthe following way: for example, a 75% sparse ViT with N non-zeros will perform similar to a dense\none with \u2248 2.17N parameters, when both are trained with the same amount of data. Crucially, this\nholds for any amount of data and thus also in the infinite limit when training is purely capacity bound.\nHence, this expresses an equivalence between dense capacity and sparse capacity. Remarkably,\nsparsity gains are very similar across vision and text domains, with the sweet-spot being around\n75% sparsity at around \u2248 2.15\u00d7 gain. We believe that this is due to the relative nature of these\nquantities with respect to scaling parameters. (At the same time, the fact that the numbers are within\n0.01 of each other is likely a coincidence.)\nFamily\n0.500\n0.750\n0.875\nViT/JFT\n1.60\u00d7\n2.17\u00d7\n2.63\u00d7\nT5/C4\n1.59\u00d7\n2.16\u00d7\n2.63\u00d7\nTable 2: Equivalent dense size multiplier to match performance of a sparse model.\n4\nEXTENSIONS\n4.1\nN:M SPARSITY\n10\n2\n10\n1\n#non-zero parameters\n100\n2 \u00d7 100\nval-loss \n low-bound\n250k steps\ndense\n2:4\n1:4\n4:8\n2:8\n50%\n75%\nFigure 6: Loss vs. size plot for a\nsubset of T5/C4 n:m sparsity data.\nIn addition to our previous unstructured sparsity exploration,\nwe now also consider structured n:m sparsity, which can be\nwell accelerated on actual hardware, e.g., as 2:4 sparsity on\nmodern NVIDIA GPUs (Pool & Yu, 2021; Hubara et al.,\n2021). Similar to how minor changes in the process (optimizer,\nmodel shape) generally only affect the multiplicative constants\nin dense scaling laws (Kaplan et al., 2020), we also expect mi-\nnor changes in the sparsification process (pattern, algorithm,\netc.) to only affect the sparsity term in (3). This can be exploited to fit laws based on significantly\nless runs: if the dense base scaling is known, one only has to fit aS, bS and cS (just 3 rather than 7\nparameters) to find the corresponding L(S, N, D). We now utilize this in the context of n:m sparsity\nby fitting new laws for 2:4 and 1:4 as well as 4:8 and 2:8 patterns, respectively, based only on a\nsubset of our full grid in Table 1. Concretely, we execute all runs involving either the least amount\nof steps or the smallest model.\nPattern\n0.50\n0.75\nn:4\n1.56\u00d7\n1.62\u00d7\nn:8\n1.67\u00d7\n1.81\u00d7\nTable 3: Dense size multipliers for\nn:m sparsity on T5/C4.\nFigure 6 visualizes a subset of the collected data, displaying a\nvery similar form to 2, which indicates that the general scal-\ning law shape also holds for n:m sparsity. We also fit scaling\nlaws (with Huber \u03b4 = 0.01 as 0.75 patterns will otherwise be\ntreated as an outlier) and calculate sparsity gains as in Sec-\ntion 3.3.1 (see Table 3). In general, it seems that 2:4 and 4:8\nperform both very similar to 50% (see Table 2 and also Fig-\nure 6), although the n:m estimates are likely slightly more noisy due to less data used in fitting the\ncurves. Meanwhile, 1:4 brings almost no advantage and 2:8 only a slight improvement, which is\ncontrary to our unstructured results. We suspect that the 75% patterns may simply be too stringent\nto significantly increase capacity beyond their 50% variants.\n4.2\nPRUNING PRETRAINED MODELS\nLastly, we consider a practical scenario where a set of existing very well trained dense models\nshould be made more efficient via pruning, using a small fraction of the compute spent for the initial\npretraining. Our main interest here is to compare the efficiency of sparsifying from scratch and spar-\nsifying from a pretrained checkpoint. For that purpose, we train ViT S/16, M/16 and B/16 models for\n4 full epochs on JFT ( i.e., 16 billion images) and then start the same gradual sparsification procedure\nwe used before from these checkpoints, for 5.6% of the pretraining budget (as the model is already\npretrained, we start to sparsify immediately rather than after 25% of training). Finally, we use our\nscaling laws from Section 3.2 to determine the amount of training necessary to produce equivalent\nmodels of the same quality when starting from scratch. Table 4 shows how much more/less data is\n8\nrequired to achieve equivalent performance for sparsifying from scratch, when excluding/including\nthe pretraining cost, respectively.\nModel\n0.500\n0.750\n0.875\nexc.\ninc.\nexc.\ninc.\nexc.\ninc.\nS/16\n4.90\u00d7\n0.25\u00d7\n4.27\u00d7\n0.23\u00d7\n2.45\u00d7\n0.13\u00d7\nM/16\n4.76\u00d7\n0.25\u00d7\n4.18\u00d7\n0.22\u00d7\n2.57\u00d7\n0.14\u00d7\nB/16\n4.35\u00d7\n0.23\u00d7\n4.00\u00d7\n0.21\u00d7\n2.72\u00d7\n0.14\u00d7\nTable 4: Relative amount of data required for sparsifying from scratch to match the validation loss\nof pruning from a pretrained model, when pretraining cost is excluded (exc.) and included (inc.).\nIf the model already exists and there is thus no pretraining cost, then starting from such a checkpoint\nis > 4\u00d7 more efficient then sparsifying from scratch for 0.5/0.75, and > 2\u00d7 for 0.875 sparsity,\nrespectively. The reason why the efficiency gains are decreasing with higher sparsity is most likely\nthe increased divergence from the initial starting point. At the same time, when the pretraining cost\nis counted as well, pruning throughout the whole training process appears to be \u2265 4\u00d7 more efficient,\nrelative to the \u2248 5% pruning of pretraining budget.\nOverall, these results clearly demonstrate that, while the sparsification process benefits significantly\nfrom a better trained initial model, it does so only up to a certain extent. Finally, we note that the\n50% models are \u2248 0.2 \u2212 0.3 points away from their dense baseline loss, which matches our results\nin Section 3.3.1 that the size gain of 50% sparsity is noticeably less than 2\u00d7 for well trained models.\n5\nRELATED WORK\nSparsity & pruning.\nSparsity and pruning, i.e., having a large number of exactly 0 weights which\ncan be ignored during inference, has a long history (LeCun et al., 1989; Hassibi et al., 1993) and\na large number of works have been published on this topic (Hoefler et al., 2021). Current state-of-\nthe-art methods range from simple gradual removal of the smallest weights (Zhu & Gupta, 2017),\nto partial or full sparse training (Mocanu et al., 2018; Jayakumar et al., 2021; Peste et al., 2021),\napproximate Hessian-based metrics (Singh & Alistarh, 2020; Frantar et al., 2021) and \u201csoft\u201d sparse\noptimization (Kusupati et al., 2020; Sanh et al., 2020). Many of these methods can impose very\nhigh levels of sparsity at minimal accuracy loss, which can lead to substantial practical speedups\nwith specialized inference algorithms (Kurtz et al., 2020; Elsen et al., 2020). At the same time, most\nof those works focus on, by modern standards, relatively simple tasks like ResNet50/ImageNet or\nBERT/GLUE, with rather overparametrized models.\nIn contrast, there has only been very little work when it comes to sparsifying modern Transform-\ners (Vaswani et al., 2017) trained on massive datasets: The Appendix of Gopher (Rae et al., 2021)\nconducts pruning experiments for a generative language modelling task and finds that, when trained\nfor the same amount of steps, sparse models can outperform dense ones, but leaves open whether\nthis is also possible when accounting for the significantly increased compute spent for producing\nthose sparse models, relative to dense ones trained with the same amount of data/steps. Similarly,\n(Cerebras, 2022) prunes a GPT-like model, also using significantly more data than its dense base-\nline. Recently, SparseGPT (Frantar & Alistarh, 2023) showed that it is possible to impose non-trivial\namounts of weight-sparsity on extremely large language models, even without retraining; yet, it re-\nmains unclear if this can also be done on more recent, smaller and much less undertrained networks.\nScaling laws.\nThe key behind the tremendous success of Transformer models are their exceptional\nscaling properties: increasing model size and/or data brings consistent performance improvements,\neven at already huge scale. Further, this scaling behavior is very predictable, following relatively\nsimple power-law curves (Kaplan et al., 2020). This can, for example, be utilized to construct a\nfamily of training compute optimal models (Hoffmann et al., 2022). More recently, these basic\nscaling laws are being extended to various more specialized applications, e.g.: optimizing model\nshapes (Alabdulmohsin et al., 2023), routing mechanisms (Clark et al., 2022), repeating training\ndata multiple times (Muennighoff et al., 2023) and several downstream tasks (Caballero et al., 2023).\nHowever, not much is known about the scaling of weight-sparsity for such models.\n9\nRosenfeld et al. (2021) studies the relationship between width, depth and weight-density for pruning\npretrained ResNets trained primarily on the nowadays very small CIFAR10 dataset. Contrarily,\nwe consider modern Transformers trained on datasets many orders of magnitude larger and focus\nparticularly on the data/compute dimension that is crucial in this context, but not very relevant in the\nsetting of Rosenfeld et al. (2021).\nTransformer efficiency.\nOverall, making (large) Transformers more efficient is currently a highly\nactive area of research. Probably the currently most popular and practical approach is quantization,\nthat is reducing the numerical precision of weights (and sometimes also activations) (Frantar et al.,\n2022; Dettmers & Zettlemoyer, 2022; Xiao et al., 2022). Further, there are also many works on\nMixture-of-Expert (MoE) models, large ensembles of models/individual layers where each input\nis only processed by a small part, thus keeping the overall computation cost constant (Du et al.,\n2022; Fedus et al., 2022; Artetxe et al., 2022; Riquelme et al., 2021). MoEs are a form of dynamic\nactivation sparsity, which is very different from the static weight sparsity that we study in this\nwork; the former trades off increased memory for faster inference, whereas the latter reduces both\ninference and memory costs. In general, we note that quantization, MoEs and weight sparsity are all\ncomplementary techniques that may be stacked for compound gains (Han et al., 2016; Kurtic et al.,\n2022).\n6\nDISCUSSION\nLimitations.\nWhile we have conducted extensive experiments, for both vision and language do-\nmains, our results still have limitations, which we hope will be addressed in future work.\n\u2022 First, our sparsification recipe was optimized for robustness and scalability across a wide range of\nsetups, rather than to fully maximize performance in a particular one. While we believe that the\noverall shape of our scaling results will remain consistent, we speculate that specific coefficient\nvalues can be improved significantly with more extensive per-run tuning and/or better sparsifica-\ntion techniques.\n\u2022 In this work, we performed pruning directly for massive data pretraining tasks. While this is ideal\nin terms usability, as all down-stream applications would directly benefit from a more efficient\nbase model, it also appears to make compression quite challenging. We think that sparsity rates\ncan probably be improved significantly when pruning is performed directly for more specialized\napplications that require only a subset of the base model\u2019s full capabilities. Similarly, we consid-\nered the optimal infinite data setting, which essentially eliminates overfitting from dense baselines.\nWe think that sparsity could be particularly practical when data is limited and has to be repeated.\n\u2022 Finally, as the main goal of this study was understanding core scaling relationships, we focused\nprimarily on the cleanest available performance metric, non-zero parameter count. However, in\npractice, sparsity acceleration can be quite complex: current software/hardware may not provide\nideal speedups and models generally also contain operations (e.g., layer-norms, attention) which\ndo not benefit from weight sparsity. We think extending our results to different target metrics is a\nvery interesting topic for future work.\nCompatibility with other works.\nWe will now briefly discuss how our scaling insights line up\nwith existing sparsification results on similar models/datasets.\n\u2022 First, the results in the Appendix of Rae et al. (2021), for a decoder-only text-generation model,\nare consistent with our scaling laws; the improvement through sparsity appears to be similar for\neach model size and their maximum size advantage of 2.5\u00d7 observed at 0.9 sparsity is quite close\nto our limit gains in Section 3.3.1, which are applicable here.\n\u2022 In contrast, Cerebras (2022) report a significantly better gain of \u2248 5\u00d7, but in a quite different\nsetting where the baseline is training (not inference) compute optimal and sparsification uses > 5\u00d7\nmore data than the dense comparison point. This is not inconsistent to our results: if we query our\nfitted T5 scaling law (see Section 3.2) with this setup, we predict 1.54 loss (dense 1B params, 20B\ntokens) vs. 1.48 loss (80% sparse & 200M non-zeros, 100B tokens), in favor of the longer trained\nsparse model.\n\u2022 Finally, SparseGPT (Frantar & Alistarh, 2023) notes that post-training pruning becomes signif-\nicantly easier as the model size increases. However, they do not perform any retraining, and\n10\nobserve this effect primarily relative to the respective unpruned base model, not in terms of im-\nprovements over the Pareto size-vs-loss frontier that we study in this work. Hence, we believe that\nthis is likely more related to the pretrained models\u2019 initial robustness to pertubations rather than\nthe architecture\u2019s inherent sparsifiability.\nPractical consequences.\nOur scaling insights lead to a number of practical consequences: Spar-\nsity seems to affect each model size in approximately the same way, while remaining mostly inde-\npendent of the amount of training data used. This provides evidence that good pruning performance\nin less expensive settings should generalize to performance at scale, which will hopefully accelerate\nresearch on new sparsification recipes and algorithms. Additionally, we have shown that optimal\nsparsity levels continuously increase with longer training. Sparsity thus provides a means to further\nimprove model performance for a fixed final parameter cost. In particular, when training beyond\nChinchilla optimality, where simple dense training starts to run into diminishing returns, sparsity\ncan provide a clear alternative. Thus, our findings can be interpreted as providing practical motiva-\ntion for further developing sparsity support.\n7\nACKNOWLEDGEMENTS\nThe authors would like to thank Suvinay Subramanian for his useful feedback and suggestions, es-\npecially regarding Section 3.3.1 and Section 4.1. We also like to thank Amir Yazdanbakhsh, Shivani\nAgrawal, Jeffrey Pennington and Yann Dauphin for their valuable feedback during our discussions.\nREFERENCES\nIbrahim Alabdulmohsin, Xiaohua Zhai, Alexander Kolesnikov, and Lucas Beyer. Getting ViT in\nshape: Scaling laws for compute-optimal model design. arXiv preprint arXiv:2305.13035, 2023.\nMikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria\nLin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, et al. Efficient large scale language mod-\neling with mixtures of experts. In Empirical Methods in Natural Language Processing (EMNLP),\n2022.\nRiade Benbaki, Wenyu Chen, Xiang Meng, Hussein Hazimeh, Natalia Ponomareva, Zhe Zhao, and\nRahul Mazumder. Fast as CHITA: Neural network pruning with combinatorial optimization. In\nInternational Conference on Machine Learning (ICML), 2023.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportu-\nnities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. In Conference on Neural Information Processing Systems (NeurIPS), 2020.\nEthan Caballero, Kshitij Gupta, Irina Rish, and David Krueger. Broken neural scaling laws. In\nInternational Conference on Learning Representations (ICLR), 2023.\nCerebras.\nCreating\nSparse\nGPT-3\nModels\nwith\nIterative\nPrun-\ning,\n2022.\nURL\nhttps://www.cerebras.net/blog/\ncreating-sparse-gpt-3-models-with-iterative-pruning.\nAidan Clark, Diego De Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoff-\nmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Unified scaling\nlaws for routed language models. In International Conference on Machine Learning (ICML),\n2022.\nMostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer,\nAndreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling\nvision transformers to 22 billion parameters. In International Conference on Machine Learning\n(ICML), 2023.\n11\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale\nhierarchical image database. In Conference on Computer Vision and Pattern Recognition (CVPR),\npp. 248\u2013255. IEEE, 2009.\nTim Dettmers and Luke Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws.\narXiv preprint arXiv:2212.09720, 2022.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An im-\nage is worth 16x16 words: Transformers for image recognition at scale. In International Confer-\nence on Learning Representations (ICLR), 2021.\nNan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim\nKrikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. GLaM: Efficient scaling of language\nmodels with mixture-of-experts. In International Conference on Machine Learning (ICML), 2022.\nErich Elsen, Marat Dukhan, Trevor Gale, and Karen Simonyan. Fast sparse convnets. In Conference\non Computer Vision and Pattern Recognition (CVPR), 2020.\nUtku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery:\nMaking all tickets winners. In International Conference on Machine Learning (ICML), 2020.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter\nmodels with simple and efficient sparsity. The Journal of Machine Learning Research, 23(1):\n5232\u20135270, 2022.\nElias Frantar and Dan Alistarh. SparseGPT: Massive language models can be accurately pruned in\none-shot. In International Conference on Machine Learning (ICML), 2023.\nElias Frantar, Eldar Kurtic, and Dan Alistarh.\nM-FAC: Efficient matrix-free approximations of\nsecond-order information. In Conference on Neural Information Processing Systems (NeurIPS),\n2021.\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: Accurate post-training\ncompression for generative pretrained transformers. arXiv preprint arXiv:2210.17323, 2022.\nTrevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. In\nInternational Conference on Machine Learning (ICML), 2019.\nAmir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer.\nA survey of quantization methods for efficient neural network inference.\narXiv preprint\narXiv:2103.13630, 2021.\nGoogle.\nBig vision, 2023a.\nURL https://github.com/google-research/big_\nvision.\nGoogle. T5x, 2023b. URL https://github.com/google-research/t5x.\nSong Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks\nwith pruning, trained quantization and Huffman coding. In International Conference on Learning\nRepresentations (ICLR), 2016.\nBabak Hassibi, David G Stork, and Gregory J Wolff. Optimal brain surgeon and general network\npruning. In IEEE International Conference on Neural Networks, 1993.\nTorsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste.\nSparsity in\ndeep learning: Pruning and growth for efficient inference and training in neural networks. arXiv\npreprint arXiv:2102.00554, 2021.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Train-\ning compute-optimal large language models. In Conference on Neural Information Processing\nSystems (NeurIPS), 2022.\n12\nAndrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,\nMarco Andreetto, and Hartwig Adam. MobileNets: Efficient convolutional neural networks for\nmobile vision applications. arXiv preprint arXiv:1704.04861, 2017.\nItay Hubara, Brian Chmiel, Moshe Island, Ron Banner, Seffi Naor, and Daniel Soudry. Accelerated\nsparse neural training: A provable and efficient method to find N:M transposable masks.\nIn\nConference on Neural Information Processing Systems (NeurIPS), 2021.\nBenoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard,\nHartwig Adam, and Dmitry Kalenichenko.\nQuantization and training of neural networks for\nefficient integer-arithmetic-only inference. In Conference on Computer Vision and Pattern Recog-\nnition (CVPR), 2018.\nSiddhant M Jayakumar, Razvan Pascanu, Jack W Rae, Simon Osindero, and Erich Elsen. Top-\nKAST: Top-K always sparse training. arXiv preprint arXiv:2106.03517, 2021.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language\nmodels. arXiv preprint arXiv:2001.08361, 2020.\nEldar Kurtic and Dan Alistarh. Gmp*: Well-tuned global magnitude pruning can outperform most\nbert-pruning methods. arXiv preprint arXiv:2210.06384, 2022.\nEldar Kurtic, Daniel Campos, Tuan Nguyen, Elias Frantar, Mark Kurtz, Benjamin Fineran, Michael\nGoin, and Dan Alistarh. The Optimal BERT Surgeon: Scalable and accurate second-order pruning\nfor large language models. arXiv preprint arXiv:2203.07259, 2022.\nMark Kurtz, Justin Kopinsky, Rati Gelashvili, Alexander Matveev, John Carr, Michael Goin,\nWilliam Leiserson, Sage Moore, Bill Nell, Nir Shavit, and Dan Alistarh. Inducing and exploiting\nactivation sparsity for fast inference on deep neural networks. In International Conference on\nMachine Learning (ICML), 2020.\nAditya Kusupati, Vivek Ramanujan, Raghav Somani, Mitchell Wortsman, Prateek Jain, Sham\nKakade, and Ali Farhadi. Soft threshold weight reparameterization for learnable sparsity. In\nInternational Conference on Machine Learning (ICML), 2020.\nDenis Kuznedelev, Eldar Kurtic, Eugenia Iofinova, Elias Frantar, Alexandra Peste, and Dan Alis-\ntarh. Accurate neural network pruning requires rethinking sparse optimization. arXiv preprint\narXiv:2308.02060, 2023.\nYann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Conference on Neural\nInformation Processing Systems (NeurIPS), 1989.\nJoo Hyung Lee, Wonpyo Park, Nicole Mitchell, Jonathan Pilault, Johan Obando-Ceron, Han-Byul\nKim, Namhoon Lee, Elias Frantar, Yun Long, Amir Yazdanbakhsh, et al. JaxPruner: A concise\nlibrary for sparsity research. arXiv preprint arXiv:2304.14082, 2023.\nTao Lin, Sebastian U Stich, Luis Barba, Daniil Dmitriev, and Martin Jaggi. Dynamic model pruning\nwith feedback. In International Conference on Learning Representations (ICLR), 2020.\nYucheng Lu, Shivani Agrawal, Suvinay Subramanian, Oleg Rybakov, Christopher De Sa, and Amir\nYazdanbakhsh. STEP: Learning n: M structured sparsity masks from scratch with precondition.\nIn International Conference on Machine Learning (ICML), 2023.\nAsit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic, Ganesh Venkatesh,\nChong Yu, and Paulius Micikevicius. Accelerating sparse deep neural networks. arXiv preprint\narXiv:2104.08378, 2021.\nDecebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine Gibescu,\nand Antonio Liotta. Scalable training of artificial neural networks with adaptive sparse connec-\ntivity inspired by network science. Nature communications, 9(1):1\u201312, 2018.\nNiklas Muennighoff, Alexander M Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Noua-\nmane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language\nmodels. arXiv preprint arXiv:2305.16264, 2023.\n13\nAlexandra Peste, Eugenia Iofinova, Adrian Vladu, and Dan Alistarh. AC/DC: Alternating com-\npressed/decompressed training of deep neural networks. In Conference on Neural Information\nProcessing Systems (NeurIPS), 2021.\nJeff Pool and Chong Yu. Channel permutations for N:M sparsity. Conference on Neural Information\nProcessing Systems (NeurIPS), 34, 2021.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models:\nMethods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research, 21(140):1\u201367, 2020a.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research (JMLR), 21(1):5485\u20135551, 2020b.\nCarlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andr\u00b4e\nSusano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts.\nAdvances in Neural Information Processing Systems, 34:8583\u20138595, 2021.\nJonathan S Rosenfeld, Jonathan Frankle, Michael Carbin, and Nir Shavit. On the predictability of\npruning across scales. In International Conference on Machine Learning (ICML), 2021.\nVictor Sanh, Thomas Wolf, and Alexander M. Rush. Movement pruning: Adaptive sparsity by\nfine-tuning. arXiv preprint arXiv:2005.07683, 2020.\nJonathan Schwarz, Siddhant Jayakumar, Razvan Pascanu, Peter Latham, and Yee Teh. Powerprop-\nagation: A sparsity inducing weight reparameterisation. In Conference on Neural Information\nProcessing Systems (NeurIPS), 2021.\nNoam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost.\nIn International Conference on Machine Learning (ICML), 2018.\nSidak Pal Singh and Dan Alistarh. WoodFisher: Efficient second-order approximation for neural\nnetwork compression. In Conference on Neural Information Processing Systems (NeurIPS), 2020.\nMingxing Tan and Quoc Le. EfficientNet: Rethinking model scaling for convolutional neural net-\nworks. In International Conference on Machine Learning (ICML), 2019.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Conference on Neural In-\nformation Processing Systems (NeurIPS), 2017.\nChaoqi Wang, Guodong Zhang, and Roger Grosse.\nPicking winning tickets before training by\npreserving gradient flow. In International Conference on Learning Representations (ICLR), 2020.\nGuangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han.\nSmoothquant:\nAccurate and efficient post-training quantization for large language models.\narXiv preprint\narXiv:2211.10438, 2022.\nZhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He.\nZeroQuant: Efficient and affordable post-training quantization for large-scale transformers. arXiv\npreprint arXiv:2206.01861, 2022.\n14\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers.\nIn Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\nAojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang, Kun Yuan, Wenxiu Sun, and Hong-\nsheng Li. Learning N:M fine-grained structured sparse neural networks from scratch. In Interna-\ntional Conference on Learning Representations (ICLR), 2021.\nMichael Zhu and Suyog Gupta. To prune, or not to prune: exploring the efficacy of pruning for\nmodel compression. arXiv preprint arXiv:1710.01878, 2017.\nA\nEXPERIMENTAL SETUP\nThis section discusses our experimental choices and hyper-parameters, as well as technical details\nfor sparsity-aware AdaFactor and iterative n:m pruning.\nModels & datasets.\nWe consider two standard deep learning applications: vision and language.\nFor the former, we focus on Vision Transformers (Dosovitskiy et al., 2021) trained for multi-label\nimage classification on the JFT-4B dataset (Dehghani et al., 2023), consisting of 4 billion images;\nfor the latter, we consider encoder-decoder T5 models (Raffel et al., 2020b) (improved 1.1 version\n(Google, 2023b)) trained for masked-language-modelling on C4 (Raffel et al., 2020b), consisting of\n150+ billion tokens. These choices allow us to study the generality of our laws not just across vision\nand language but also for different kind of pretraining objectives and variations of Transformer\narchitectures.\nTraining hyper-parameters.\nFor the most part, we reuse the optimized training hyper-parameters\nof the original ViT-scaling (Zhai et al., 2022) and T5 paper (Raffel et al., 2020b), respectively. Our\nonly notable change is that we do not factor the second moment of the respective AdaFactor-based\n(Shazeer & Stern, 2018) optimizers (however, we still apply relative learning rate scaling and RMS\nclipping for T5); this is done since factorized moments for pruning and sparse optimization are\nnot yet very well studied. Further, we train T5 models with batchsize 128 (similar to most ablation\nstudies in the original paper (Raffel et al., 2020b)) in order to perform sufficiently many optimization\nsteps also for experiments with lower total amounts of training data, which we found important to\nobtain stable sparse results through model and data scaling.\nModel sizes.\nWhen it comes to selecting our particular model dimensions, two things must be\ntaken into account: (a) we are particularly interested in the inference-optimal overtraining regime\n(Touvron et al., 2023a) where models get close to their capacity limit, and (b) to produce a model\nwith N non-zeros and sparsity S, we actually need to train a model that is 1/(1 \u2212 S) times larger\nthan a dense model of size N. In combination with the fact that we need to repeat the entire training\nsweep for multiple sparsity levels, this limits the size of models we can study while keeping compute\nrequirements feasible. Specifically, we start from the base variants of both ViT and T5 (B/16 and\nt5-base). Then we generate models of appropriate sizes by scaling only the Transformer\u2019s hidden\ndimension and keeping all other shape parameters constant. This way we can get quite precise size-\nmatches between models and sparsities, facilitating direct comparisons (not all default family models\nare exactly 2\u00d7 steps apart and a 50% sparse model would thus not always be directly comparable\nto the next smallest dense variant); we did not observe any notable performance decrease for dense\nmodels using this scaling strategy, at the sizes we study.\nSparsity configurations.\nWe focus primarily on the most fundamental sparsity type, unstructured\nsparsity, but also perform some investigations for the more practical n:m pruning pattern (Zhou et al.,\n2021; Pool & Yu, 2021) where only n out of m consecutive weights are non-zero. We uniformly\nsparsify all linear layers in the Transformer backbone, which effectively avoids layer collapse (Wang\net al., 2020), or other edge cases that may otherwise occur in our sweeps, and generally works\ndecently well for Transformer models. On T5 models, we also sparsify the rather large embeddings\nto the amount necessary for parameter matching a smaller dense version.\nPreliminary experiments indicated quickly diminishing returns for very sparse models, which, as\ndiscussed previously, are in addition quite expensive to train. Thus, we focus on three medium\nsparsities: 50%, 75%, 87.5%, corresponding to a 2\u00d7, 4\u00d7 and 8\u00d7 compression rate, respectively.\n15\nOur implementation is based on the recently proposed Jaxpruner library (Lee et al., 2023), which is\neasy to integrate into the offical ViT (Google, 2023a) and T5 (Google, 2023b) codebases.\nPruning strategy.\nAs we intend to execute substantial sweeps across model size, training data\nand sparsity, it is essential that our pruning method is highly robust and does not require hyper-\nparameter retuning for each run. A natural candidate is gradual magnitude pruning (GMP) (Zhu\n& Gupta, 2017), which is well studied and known to be very reliable. At the same time, GMP is\nusually quite competitive with more complex techniques (Singh & Alistarh, 2020; Kurtic & Alistarh,\n2022), especially at the medium sparsity levels that we focus on. We also tested a variation of GMP\nwhich incorporates diagonal second-order information (Kurtic et al., 2022), but found it to perform\nalmost identically in our setting. Further, we tried AC/DC (Peste et al., 2021), STE (Lin et al.,\n2020) and RigL (Evci et al., 2020) (which achieve strong results on classic benchmarks) but saw\nsimilar or worse performance, while being more sensitive to hyper-parameters as we scale (see also\nAppendix B). Thus, we ultimately decided to use GMP.\nIn terms of specific hyper-parameters, we prune using a cubic schedule starting after 25% of training\nand ending at 75%, updating every 100 steps. Our sparsification interval was chosen so that pruning\nbegins with a reasonably well trained model and ends with sufficient finetuning of the final sparse\nstructure. However, we performed ablations for frequency/start/end in Appendix B and did not find\nthe process to be too sensitive to those hyper-parameters (except for when the pruning interval is\nreally short).\nA.1\nTECHNICAL DETAILS\nSparsity-aware RMS.\nAdaFactor (Shazeer & Stern, 2018) as employed by T5 (Raffel et al.,\n2020b) defines the learning rate relatively, scaling it by the root-mean-square (RMS) of each weight\ntensor, respectively. We find that this does not interact well with high sparsity, as a tensor with many\nzeros tends to have a lower RMS, resulting in a smaller learning rate, which is especially problematic\nas high levels of sparsification require more recovery during the pruning process. To work around\nthis, we always calculate the RMS only over unpruned weights, which effectively alleviates this\nproblem. We also apply the same technique to AdaFactor\u2019s RMS clipping threshold, but note that\nthis is much less critical than the learning rate scaling.\nIterative n:m pruning.\nSparsifying to the n:m pattern is usually done by pruning in one-shot,\nfollowed by finetuning (Zhou et al., 2021), or directly training with a dynamic pruning mask via\nstraight-through gradient estimation (Lu et al., 2023). We take a different approach in this work:\nwe gradually remove the smallest weights while ensuring that at least n weights remain in each\ngroup of size m. This effectively generalizes the highly robust gradual pruning paradigm to the n:m\nsetting. Not only does gradual n:m pruning unify our setups between unstructured and structured\nsparsity experiments, we also found it to work reliably across scales with the same hyper-parameters,\na highly useful property for scaling studies. A simple and efficient implementation of this scheme\nis shown in Algorithm 1: the key is to temporarily set the largest n items in each group to \u221e, thus\nensuring that they are always picked by an unstructured topk selection.\nAlgorithm 1 Prune weights w to sparsity s < 1\u2212\nn/m where each group of m weights contains at\nmost n zeros.\nInm \u2190 topk-nm(|w|, n, m)\nw\u2032 \u2190 copy of w\nw\u2032Inm \u2190 \u221e\nIunstr \u2190 topk-unstr(|w\u2032|, 1 \u2212 s)\nw\u2212Iunstr \u2190 0\nB\nPRUNING ABLATIONS\nWe ablate gradual magnitude pruning hyper-parameters by sparsifying ViT-B/16 for 900M images\nto S = 0.9375 sparsity. A high S was chosen in order to amplify differences between parameter\n16\nsettings; we vary the (relative) start and end point of gradual pruning as well as the update frequency\nof the mask; the pruning schedule is always cubic. As can be seen in Table 5, most configurations\nperform very similar, except when the pruning period is too short overall. We ultimately pick the\n25-75/100 setup to ensure that there is sufficient time for training a decent model before starting\npruning, as well as for properly finetuning the final sparse version, which we think could be helpful\nfor some points in our main scaling experiment grid.\nstart\nend\nfreq\naccuracy\n0.250\n0.750\n100\n45.28\n0.250\n0.750\n50\n45.08\n0.250\n0.750\n200\n45.13\n0.125\n0.875\n100\n45.33\n0.475\n0.625\n100\n44.65\n0.125\n0.625\n100\n45.13\n0.375\n0.875\n100\n45.04\nTable 5: Ablation study of gradual magnitude pruning hyper-parameters.\nAC/DC.\nWe also experimented with the AC/DC method (Peste et al., 2021), a sparse training ap-\nproach that was recently shown to yield very strong results on standard (non-foundation model)\nbenchmarks Kuznedelev et al. (2023). We use a sparse and dense cycle length of 20K steps (10K\neach phase) and apply AC/DC only during the same pruning period as our GMP setup to ensure\nthe same pre- and post-sparsification finetuning. On smaller T5 models, AC/DC works well but\nyields very similar results to GMP. On larger models, however, AC/DC appears to require some\nhyper-parameter reconfiguration for higher sparsities. Since per-model hyper-parameter tuning is\ninconvenient for large scaling sweeps, while initial results also did not suggest clear improvements,\nwe stuck to well established GMP. In general, we note that even for classic benchmarks, major\ndifferences between pruning methods tend to appear mostly at very high sparsities (Singh & Al-\nistarh, 2020). Nevertheless, we think that more extensively investigating advanced sparsification\napproaches in the context of massive pretraining datasets is an interesting topic for future work.\n#nnz\n0.500\n0.750\n0.875\nGMP\nAC/DC\nGMP\nAC/DC\nGMP\nAC/DC\n1.3M\n17.11\n17.11\n15.64\n15.96\n14.73\n14.59\n42M\n6.11\n6.11\n5.87\n6.05\n5.81\n6.11\nTable 6: Comparing validation perplexity of T5 models trained for 250K steps using GMP and\nAC/DC, respectively; we show perplexity as losses at these levels should be compared in log-scale.\nC\nSCALING COEFFICIENTS\nTable 7 lists the fitted coefficient values for the scaling results presented in the main paper; D is\nassumed to be the number of images for ViT and the number tokens for T5. The fitting errors are\nalso shown, where we note again that they correspond to the Huber-loss with \u03b4 = 0.01 for ViT/JFT\nand the Huber-loss of log L with \u03b4 = 0.001 for T5/C4, following (Hoffmann et al., 2022). For n:m\nsparsity, we only refit the sparsity coefficients aS, bS and cS, preserving the other values from the\ncorresponding unstructured results. While the fitting procedure may not be guaranteed to be convex,\nwe find the process to converge to virtually the same values from different random starting points.\nModel\nSparse\naS\nbS\ncS\nbN\naD\nbD\nc\nError\nViT/JFT\nunstr.\n2.94e+2\n0.821\n4.68e+2\n0.392\n2.37e+8\n0.890\n4.517\n4.93e-4\nT5/C4\nunstr.\n1.68e+1\n0.722\n4.50e+1\n0.245\n6.90e+8\n0.203\n0.651\n7.60e-6\nT5/C4\nn:m\n8.64e+1\n2.752\n5.36e+2\n\u2013\n\u2013\n\u2013\n\u2013\n2.1e-5\nTable 7: Fitted coefficients of the scaling laws presented in the main paper.\n17\nD\nDERIVING SPARSE COSTS\nWe now discuss how to derive the sparse cost factor cmul(S) given by Equation 4 in Section 3.3 for\nour particular pruning setup. For the first 25% of training, we use a dense model that is 1/(1 \u2212 S)\ntimes larger, incurring cost 0.25/(1\u2212S), while for the last 25% we are training a model with sparsity\nS of the same cost as the dense reference model, thus contributing a 0.25 term. For the middle 50%,\nwe prune in the cubic S \u2212 S \u00b7 (1 \u2212 t)3 schedule (Zhu & Gupta, 2017), where t \u2208 [0, 1]. The cost\nspent over this period is given by 1 minus (we care about density rather than sparsity) the integral\nover the full range of t, which is (1 \u2212 0.75 \u00b7 S), further multiplied by 0.5 to cover the duration.\n18\n"
  },
  {
    "title": "Sparse Autoencoders Find Highly Interpretable Features in Language Models",
    "link": "https://arxiv.org/pdf/2309.08600.pdf",
    "upvote": "11",
    "text": "SPARSE AUTOENCODERS FIND HIGHLY INTER-\nPRETABLE FEATURES IN LANGUAGE MODELS\nHoagy Cunningham\u221712, Aidan Ewart\u221713, Logan Riggs\u22171, Robert Huben, Lee Sharkey4\n1EleutherAI, 2MATS, 3Bristol AI Safety Centre, 4Apollo Research\n{hoagycunningham, aidanprattewart, logansmith5}@gmail.com\nABSTRACT\nOne of the roadblocks to a better understanding of neural networks\u2019 internals is\npolysemanticity, where neurons appear to activate in multiple, semantically dis-\ntinct contexts.\nPolysemanticity prevents us from identifying concise, human-\nunderstandable explanations for what neural networks are doing internally. One\nhypothesised cause of polysemanticity is superposition, where neural networks\nrepresent more features than they have neurons by assigning features to an over-\ncomplete set of directions in activation space, rather than to individual neurons.\nHere, we attempt to identify those directions, using sparse autoencoders to re-\nconstruct the internal activations of a language model. These autoencoders learn\nsets of sparsely activating features that are more interpretable and monoseman-\ntic than directions identified by alternative approaches, where interpretability is\nmeasured by automated methods. Moreover, we show that with our learned set\nof features, we can pinpoint the features that are causally responsible for coun-\nterfactual behaviour on the indirect object identification task (Wang et al., 2022)\nto a finer degree than previous decompositions. This work indicates that it is\npossible to resolve superposition in language models using a scalable, unsuper-\nvised method. Our method may serve as a foundation for future mechanistic in-\nterpretability work, which we hope will enable greater model transparency and\nsteerability.\n1\nINTRODUCTION\nAdvances in artificial intelligence (AI) have resulted in the development of highly capable AI sys-\ntems that make decisions for reasons we do not understand. This has caused concern that AI systems\nthat we cannot trust are being widely deployed in the economy and in our lives, introducing a num-\nber of novel risks (Hendrycks et al., 2023), including potential future risks that AIs might deceive\nhumans in order to accomplish undesirable goals (Ngo et al., 2022). Mechanistic interpretability\nseeks to mitigate such risks through understanding how neural networks calculate their outputs, al-\nlowing us to reverse engineer parts of their internal processes and make targeted changes to them\n(Cammarata et al., 2021; Wang et al., 2022; Elhage et al., 2021).\nTo reverse engineer a neural network, it is necessary to break it down into smaller units (features)\nthat can be analysed in isolation. Using individual neurons as these units has had some success (Olah\net al., 2020; Bills et al., 2023), but a key challenge has been that neurons are often polysemantic,\nactivating for several unrelated types of feature (Olah et al., 2020). Also, for some types of network\nactivations, such as the residual stream of a transformer, there is little reason to expect features to\nalign with the neuron basis (Elhage et al., 2023).\nElhage et al. (2022b) investigate why polysemanticity might arise and hypothesise that it may result\nfrom models learning more distinct features than there are dimensions in the layer. They call this\nphenomenon superposition. Since a vector space can only have as many orthogonal vectors as\nit has dimensions, this means the network would learn an overcomplete basis of non-orthogonal\nfeatures. Features must be sufficiently sparsely activating for superposition to arise because, without\n\u2217Equal contribution\nCode to replicate experiments can be found at https://github.com/HoagyC/sparse_coding\n1\narXiv:2309.08600v3  [cs.LG]  4 Oct 2023\nFigure 1: An overview of our method. We a) sample the internal activations of a language model,\neither the residual stream, MLP sublayer, or attention head sublayer; b) use these activations to train\na neural network, a sparse autoencoder whose weights form a feature dictionary; c) interpret the\nresulting features with techniques such as OpenAI\u2019s autointerpretability scores.\nhigh sparsity, interference between non-orthogonal features prevents any performance gain from\nsuperposition. This suggests that we may be able to recover the network\u2019s features by finding a set\nof directions in activation space such that each activation vector can be reconstructed from a sparse\nlinear combinations of these directions. This is equivalent to the well-known problem of sparse\ndictionary learning (Olshausen & Field, 1997).\nBuilding on Sharkey et al. (2023), we train sparse autoencoders to learn these sets of directions. Our\napproach is also similar to Yun et al. (2021), who apply sparse dictionary learning to all residual\nstream layers in a language model simultaneously. Our method is summarised in Figure 1 and\ndescribed in Section 2.\nWe then use several techniques to verify that our learned features represent a semantically mean-\ningful decomposition of the activation space. First, we show that our features are on average more\ninterpretable than neurons and other matrix decomposition techniques, as measured by autointer-\npretability scores (Section 3) (Bills et al., 2023). Next, we show that we are able to pinpoint the\nfeatures used for a set task more precisely than other methods (Section 4). Finally, we run case\nstudies on a small number of features, showing that they are not only monosemantic but also have\npredictable effects on the model outputs, and can be used for fine-grained circuit detection. (Section\n5).\n2\nTAKING FEATURES OUT OF SUPERPOSITION WITH SPARSE DICTIONARY\nLEARNING\nTo take network features out of superposition, we employ techniques from sparse dictionary learn-\ning (Olshausen & Field, 1997; Lee et al., 2006).\nSuppose that each of a given set of vectors\n{xi}nvec\ni=1 \u2282 Rd is composed of a sparse linear combination of unknown vectors {gj}ngt\nj=1 \u2282 Rd,\ni.e. xi = P\nj ai,jgj where ai is a sparse vector. In our case, the data vectors {xi}nvec\ni=1 are internal\nactivations of a language model, such as Pythia-70M (Biderman et al., 2023), and {gj}ngt\nj=1 are un-\nknown, ground truth network features. We would like learn a dictionary of vectors, called dictionary\nfeatures, {fk}nfeat\nk=1 \u2282 Rd where for any network feature gj there exists a dictionary feature fk such\nthat gj \u2248 fk.\nTo learn the dictionary, we train an autoencoder with a sparsity penalty term on its hidden activations.\nThe autoencoder is a neural network with a single hidden layer of size dhid = Rdin, where din is\nthe dimension of the language model internal activation vectors1, and R is a hyperparameter that\ncontrols the ratio of the feature dictionary size to the model dimension. We use the ReLU activation\nfunction in the hidden layer (Fukushima, 1975). We also use tied weights for our neural network,\nmeaning the weight matrices of the encoder and decoder are transposes of each other.2 Thus, on\n1We mainly study residual streams in Pythia-70M and Pythia 410-M, for which the residual streams are of\nsize din = 512 and din = 1024, respectively (Biderman et al., 2023)\n2We use tied weights because (a) they encode our expectation that the directions which detect and define the\nfeature should be the same or highly similar, (b) they halve the memory cost of the model, and (c) they remove\n2\ninput vector x \u2208 {xi}, our network produces the output \u02c6x, given by\nc\n=\nReLU(Mx + b)\n(1)\n\u02c6x\n=\nM T c\n(2)\n=\ndhid\u22121\nX\ni=0\ncifi\n(3)\nwhere M \u2208 Rdhid\u00d7din and b \u2208 Rdhid are our learned parameters, and M is normalised row-wise3.\nOur parameter matrix M is our feature dictionary, consisting of dhid rows of dictionary features fi.\nThe output \u02c6x is meant to be a reconstruction of the original vector x, and the hidden layer c consists\nof the coefficients we use in our reconstruction of x.\nOur autoencoder is trained to minimise the loss function\nL(x) =\n||x \u2212 \u02c6x||2\n2\n|\n{z\n}\nReconstruction loss\n+ \u03b1||c||1\n| {z }\nSparsity loss\n(4)\nwhere \u03b1 is a hyperparameter controlling the sparsity of the reconstruction. The \u21131 loss term on c\nencourages our reconstruction to be a sparse linear combination of the dictionary features. It can be\nshown empirically (Sharkey et al., 2023) and theoretically (Wright & Ma, 2022) that reconstruction\nwith an \u21131 penalty can recover the ground-truth features that generated the data. For the further\ndetails of our training process, see Appendix B.\n3\nINTERPRETING DICTIONARY FEATURES\n3.1\nINTERPRETABILITY AT SCALE\nHaving learned a set of dictionary features, we want to understand whether our learned features dis-\nplay reduced polysemanticity, and are therefore more interpretable. To do this in a scalable manner,\nwe require a metric to measure how interpretable a dictionary feature is. We use the automated\napproach introduced in Bills et al. (2023) because it scales well to measuring interpretability on the\nthousands of dictionary features our autoencoders learn. In summary, the autointerpretability proce-\ndure takes samples of text where the dictionary feature activates, asks a language model to write a\nhuman-readable interpretation of the dictionary feature, and then prompts the language model to use\nthis description to predict the dictionary feature\u2019s activation on other samples of text. The correlation\nbetween the model\u2019s predicted activations and the actual activations is that feature\u2019s interpretability\nscore. See Appendix A and Bills et al. (2023) for further details.\nWe show descriptions and top-and-random scores for five dictionary features from the layer 1 resid-\nual stream in Table 1. The features shown are the first five under the (arbitrary) ordering in the\ndictionary.\n3.2\nSPARSE DICTIONARY FEATURES ARE MORE INTERPRETABLE THAN BASELINES\nWe assess our interpretability scores against a variety of alternative methods for finding dictionaries\nof features in language models. In particular, we compare interpretability scores on our dictionary\nfeatures to those produced by a) the default basis, b) random directions, c) Principal Component\nAnalysis (PCA), and d) Independent Component Analysis (ICA). For the random directions and for\nthe default basis in the residual stream, we replace negative activations with zeros so that all feature\nactivations are nonnegative 4.\nambiguity about whether the learned direction should be interpreted as the encoder or decoder direction. They\ndo not reduce performance when training on residual stream data but we have observed some reductions in\nperformance when using MLP data.\n3Normalisation of the rows (dictionary features) prevents the model from reducing the sparsity loss term\n||c||1 by increasing the size of the feature vectors in M.\n4For PCA we use an online estimation approach and run the decomposition on the same quantity of data we\nused for training the autoencoders. For ICA, due to the slower convergence times, we run on only 2GB of data,\napproximately 4 million activations for the residual stream and 1m activations for the MLPs.\n3\nFeature\nDescription (Generated by GPT-4)\nInterpretability Score\n1-0000\nparts of individual names, especially last names.\n0.33\n1-0001\nactions performed by a subject or object.\n-0.11\n1-0002\ninstances of the letter \u2018W\u2019 and words beginning with \u2018w\u2019.\n0.55\n1-0003\nthe number \u20185\u2019 and also records moderate to low activa-\ntion for personal names and some nouns.\n0.57\n1-0004\nlegal terms and court case references.\n0.19\nTable 1: Results of autointerpretation on the first five features found in the layer 1 residual stream.\nAutointerpretation produces a description of what the feature means and a score for how well that\ndescription predicts other activations.\nFigure 2: Average top-and-random autointerpretability score of our learned directions in the residual\nstream, compared to a number of baselines, using 150 features each. Error bars show 95% confi-\ndence intervals around means. The feature dictionaries used here were trained for 10 epochs using\n\u03b1 = .00086 and R = 2.\n4\nFigure 2 shows that our dictionary features are far more interpretable by this measure than dictionary\nfeatures found by comparable techniques. We find that the strength of this effect declines as we move\nthrough the model, being comparable to ICA in layer 4 and showing minimal improvement in the\nfinal layer.\nThis could indicate that sparse autoencoders work less well in later layers but also may be connected\nto the difficulties of automatic interpretation, both because by building on earlier layers, later features\nmay be more complex, and because they are often best explained by their effect on the output. Bills\net al. (2023) showed that GPT-4 is able to generate explanations that are very close to the average\nquality of the human-generated explanations given similar data. However, they also showed that\ncurrent LLMs are limited in the kinds of patterns that they can find, sometimes struggling to find\npatterns that center around next or previous tokens rather than the current token, and in the current\nprotocol are unable to verify outputs by looking at changes in output or other data.\nWe do show, in Section 5, a method to see a feature\u2019s causal effect on the output logits by hand, but\nwe currently do not send this information to the language model for hypothesis generation. The case\nstudies section also demonstrates a closing parenthesis dictionary feature, showing that these final\nlayer features can give insight into the model\u2019s workings.\nSee Appendix C for a fuller exploration of different learned dictionaries through the lens of auto-\nmatic interpretability, looking at both the MLPs and the residual stream.\n4\nIDENTIFYING CAUSALLY-IMPORTANT DICTIONARY FEATURES FOR\nINDIRECT OBJECT IDENTIFICATION\nIn this section, we quantify whether our learned dictionary features localise a specific model be-\nhaviour more tightly than the PCA decomposition of the model\u2019s activations. We do this via acti-\nvation patching, a form of causal mediation analysis (Vig et al., 2020), through which we edit the\nmodel\u2019s internal activations along the directions indicated by our dictionary features and measure\nthe changes to the model\u2019s outputs. We find that our dictionary features require fewer patches to\nreach a given level of KL divergence on the task studied than comparable decompositions (Figure\n3).\nFigure 3: (Left) Number of features patched vs KL divergence from target, using various residual\nstream decompositions. We find that patching a relatively small number of dictionary features is\nmore effective than patching PCA components and features from the non-sparse \u03b1 = 0 dictionary.\n(Right) Mean edit magnitude vs KL divergence from target as we increase the number of patched\nfeatures. We find that our sparse dictionaries improve the Pareto frontier of edit magnitude vs\nthoroughness of editing. In both figures, the feature dictionaries were trained on the first 10,000\nelements of the Pile (Gao et al., 2020) (approximately 7 million activations) using the indicated \u03b1\nand R values, on layer 11 of Pythia-410M (see Appendix F for results on other layers).\n5\nSpecifically, we study model behaviour on the Indirect Object Identification (IOI) task (Wang et al.,\n2022), in which the model completes sentences like \u201cThen, Alice and Bob went to the store. Alice\ngave a snack to\n\u201d. This task was chosen because it captures a simple, previously-studied model\nbehaviour. Recall that the training of our feature dictionaries does not emphasize any particular task.\n4.1\nADAPTING ACTIVATION PATCHING TO DICTIONARY FEATURES\nIn our experiment, we run the model on a counterfactual target sentence, which is a variant of the\nbase IOI sentence with the indirect object changed (e.g., with \u201cBob\u201d replaced by \u201cVanessa\u201d); save\nthe encoded activations of our dictionary features; and use the saved activations to edit the model\u2019s\nresidual stream when run on the base sentence.\nIn particular, we perform the following procedure. Fix a layer of the model to intervene on. Run the\nmodel on the target sentence, saving the model output logits y and the encoded features \u00afc1, ..., \u00afck\nof that layer at each of the k tokens. Then, run the model on the base sentence up through the\nintervention layer, compute the encoded features c1, ..., ck at each token, and at each position replace\nthe residual stream vector xi with the patched vector\nx\u2032\ni = xi +\nX\nj\u2208F\n(\u00afci,j \u2212 ci,j)fj\nwhere F is the subset of the features which we intervene on (we describe the selection process for F\nlater in this section). Let z denote the output logits of the model when you finish applying it to the\npatched residual stream x\u2032\n1, ..., x\u2032\nk. Finally, compute the KL divergence DKL(z||y), which measures\nhow close the patched model\u2019s predictions are to the target\u2019s. We compare these interventions to\nequivalent interventions using principal components found as in Section 3.2.\nTo select the feature subset F, we use the Automated Circuit Discovery (ACDC) algorithm of\nConmy et al. (2023). In particular, we use their Algorithm 4.1 on our features, treating them as\na flat computational graph in which every feature contributes an independent change to the DKL\noutput metric, as described above and averaged over a test set of 50 IOI data points. The result is an\nordering on the features so that patching the next feature usually results in a smaller DKL loss than\neach previous feature. Then our feature subsets F are the first k features under this ordering. We\napplied ACDC separately on each decomposition.\n4.2\nPRECISE LOCALISATION OF IOI DICTIONARY FEATURES\nWe show in Figure 3 that our sparse feature dictionaries allow the same amount of model editing, as\nmeasured by KL divergence from the target, in fewer patches (Left) and with smaller edit magnitude\n(Right) than the PCA decomposition. We also show that this does not happen if we train a non-\nsparse dictionary (\u03b1 = 0). However, dictionaries with a larger sparsity coefficient \u03b1 have lower\noverall reconstruction accuracy which appears in Figure 3 as a larger minimum KL divergence. In\nFigure 3 we consider interventions on layer 11 of the residual stream, and we plot interventions on\nother layers in Appendix F.\n5\nCASE STUDIES\nIn this section, we investigate individual dictionary features, highlighting several that appear to cor-\nrespond to a single human-understandable explanation (i.e., that are monosemantic). We perform\nthree analyses of our dictionary features to determine their semantic meanings: (1) Input: We iden-\ntify which tokens activate the dictionary feature and in which contexts, (2) Output: We determine\nhow ablating the feature changes the output logits of the model, and (3) Intermediate features: We\nidentify the dictionary features in previous layers that cause the analysed feature to activate.\n5.1\nINPUT: DICTIONARY FEATURES ARE HIGHLY MONOSEMANTIC\nWe first analyse our dictionary directions by checking what text causes them to activate. An idealised\nmonosemantic dictionary feature will only activate on text corresponding to a single real-world\nfeature, whereas a polysemantic dictionary feature might activate in unrelated contexts.\n6\nFigure 4: Histogram of token counts for dictionary feature 556. (Left) For all datapoints that activate\ndictionary feature 556, we show the count of each token in each activation range. The majority of\nactivations are apostrophes, particularly for higher activations. Notably the lower activating tokens\nare conceptually similar to apostrophes, such as other punctuation. (Right) We show which token\npredictions are suppressed by ablating the feature, as measured by the difference in logits between\nthe ablated and unablated model. We find that the token whose prediction decreases the most is the\n\u201cs\u201d token. Note that there are 12k logits negatively effected, but we set a threshold of 0.1 for visual\nclarity.\nTo better illustrate the monosemanticity of certain dictionary features, we plot the histogram of\nactivations across token activations. This technique only works for dictionary features that activate\nfor a small set of tokens. We find dictionary features that only activate on apostrophes (Figure\n4); periods; the token \u201c the\u201d; and newline characters. The apostrophe feature in Figure 4 stands\nin contrast to the default basis for the residual stream, where the dimension that most represents\nan apostrophe is displayed in Figure 11 in Appendix D.1; this dimension is polysemantic since it\nrepresents different information at different activation ranges.\nAlthough the dictionary feature discussed in the previous section activates only for apostrophes,\nit does not activate on all apostrophes. This can be seen in Figures 14 and 15 in Appendix D.2,\nshowing two other apostrophe-activating dictionary features, but for different contexts (such as\n\u201c[I/We/They]\u2019ll\u201d and \u201c[don/won/wouldn]\u2019t\u201d). Details for how we searched and selected for dic-\ntionary features can be found in Appendix D.3.\n5.2\nOUTPUT: DICTIONARY FEATURES HAVE INTUITIVE EFFECTS ON THE LOGITS\nIn addition to looking at which tokens activate the dictionary feature, we investigate how dictionary\nfeatures affect the model\u2019s output predictions for the next token by ablating the feature from the\nresidual stream5. If our dictionary feature is interpretable, subtracting its value from the residual\nstream should have a logical effect on the predictions of the next token. We see in Figure 4 (Right)\nthat the effect of removing the apostrophe feature mainly reduces the logit for the following \u201cs\u201d. This\nmatches what one would expect from a dictionary feature that detects apostrophes and is used by\nthe model to predict the \u201cs\u201d token that would appear immediately after the apostrophe in possessives\nand contractions like \u201clet\u2019s\u201d.\n5.3\nINTERMEDIATE FEATURES: DICTIONARY FEATURES ALLOW AUTOMATIC CIRCUIT\nDETECTION\nWe can also understand dictionary features in relation to the upstream and downstream dictionary\nfeatures: given a dictionary feature, which dictionary features in previous layers cause it to activate,\nand which dictionary features in later layers does it cause to activate?\nTo automatically detect the relevant dictionary features, we choose a target dictionary feature such\nas layer 5\u2019s feature for tokens in parentheses which predicts a closing parentheses (Figure 5). For\nthis target dictionary feature, we find its maximum activation M across our dataset, then sample 20\n5Specifically we use less-than-rank-one ablation, where we lower the activation vector in the direction of\nthe feature only up to the point where the feature is no longer active.\n7\ncontexts that cause the target feature to activate in the range [M/2, M]. For each dictionary feature\nin the previous layer, we rerun the model while ablating this feature and sort the previous-layer\nfeatures by how much their ablation decreased the target feature. If desired, we can then recursively\napply this technique to the dictionary features in the previous layer with a large impact. The results\nof this process form a causal tree, such as Figure 5.\nBeing the last layer, layer 5\u2019s role is to output directions that directly correspond to tokens in the un-\nembedding matrix. In fact, when we unembed feature 52027, the top-tokens are all closing parenthe-\nses variations. Intuitively, previous layers will detect all situations that precede closing parentheses,\nsuch as dates, acronyms, and phrases.\nFigure 5: Circuit for the closing parenthesis dictionary feature, with human interpretations of each\nfeature shown. Edge thickness indicates the strength of the causal effect between dictionary features\nin successive residual stream layers, as measured by ablations. Many dictionary features across\nlayers correspond to similar real-world features and often point in similar directions in activation\nspace, as measured by cosine similarity.\n6\nDISCUSSION\n6.1\nRELATED WORK\nA limited number of previous works have learned dictionaries of sparsely-activating features in pre-\ntrained models, including Yun et al. (2021) and Sharkey et al. (2023), the latter of which motivated\nthis work. However, similar methods have been applied in other domains, in particular in under-\nstanding neurons in the visual cortex (Olshausen & Field, 2004; Wright & Ma, 2022).\nIn contrast to our approach, where we try to impose sparsity after training, many previous works have\nencouraged sparsity in neural networks via changes to the architecture or training process. These\napproaches include altering the attention mechanism (Correia et al., 2019), adding \u21131 penalties to\nneuron activations (Kasioumis et al., 2021; Georgiadis, 2019), pruning neurons (Frankle & Carbin,\n2018), and using the softmax function as the non-linearity in the MLP layers (Elhage et al., 2022a).\nHowever, training a state-of-the-art foundation model with these additional constraints is difficult\n(Elhage et al., 2022a), and improvements to interpretability are not always realized (Meister et al.,\n2021).\n6.2\nLIMITATIONS AND FUTURE WORK\nWhile we have presented evidence that our dictionary features are interpretable and causally im-\nportant, we do not achieve 0 reconstruction loss (Equation 4), indicating that our dictionaries fail\nto capture all the information in a layer\u2019s activations. We have also confirmed this by measuring\n8\nthe perplexity of the model\u2019s predictions when a layer is substituted with its reconstruction. For\ninstance, replacing the residual stream activations in layer 2 of Pythia-70M with our reconstruction\nof those activations increases the perplexity on the Pile (Gao et al., 2020) from 25 to 40. To reduce\nthis loss of information, we would like to explore other sparse autoencoder architectures and to try\nminimizing the change in model outputs when replacing the activations with our reconstructed vec-\ntors, rather than the reconstruction loss. Future efforts could also try to improve feature dictionary\ndiscovery by incorporating information about the weights of the model or dictionary features found\nin adjacent layers into the training process.\nOur current methods for training sparse autoencoders are best suited to the residual stream. There\nis evidence that they may be applicable to the MLPs (see Appendix C), but the training pipeline\nused to train the dictionaries in this paper is not able to robustly learn overcomplete bases in the\nintermediate layers of the MLP. We\u2019re excited by future work investigating what changes can be\nmade to better understand the computations performed by the attention heads and MLP layers, each\nof which poses different challenges.\nIn Section 4, we show that for the IOI task, behaviour is dependent on a relatively small number of\nfeatures. Because our dictionary is trained in a task-agnostic way, we expect this result to generalize\nto similar tasks and behaviours, but more work is needed to confirm this suspicion. If this property\ngeneralizes, we would have a set of features which allow for understanding many model behaviours\nusing just a few features per behaviour. We would also like to trace the causal dependencies between\nfeatures in different layers, with the overarching goal of providing a lens for viewing language\nmodels under which causal dependencies are sparse. This would hopefully be a step towards the\neventual goal of building an end-to-end understanding of how a model computes its outputs.\n6.3\nCONCLUSION\nSparse autoencoders are a scalable, unsupervised approach to disentangling language model network\nfeatures from superposition. Our approach requires only unlabelled model activations and uses or-\nders of magnitude less compute than the training of the original models. We have demonstrated that\nthe dictionary features we learn are more interpretable by autointerpretation, letting us pinpoint the\nfeatures responsible for a given behaviour more finely, and are more monosemantic than comparable\nmethods. This approach could facilitate the mapping of model circuits, targeted model editing, and\na better understanding of model representations.\nAn ambitious dream in the field of interpretability is enumerative safety (Elhage et al., 2022b):\nproducing a human-understandable explanation of a model\u2019s computations in terms of a complete list\nof the model\u2019s features and thereby providing a guarantee that the model will not perform dangerous\nbehaviours such as deception. We hope that the techniques we presented in this paper also provide\na step towards achieving this ambition.\nACKNOWLEDGMENTS\nWe would like to thank the OpenAI Researcher Access Program for their grant of model credits for\nthe autointerpretation and CoreWeave for providing EleutherAI with the computing resources for\nthis project. We also thank Nora Belrose, Arthur Conmy, Jake Mendel, and the OpenAI Automated\nInterpretability Team (Jeff Wu, William Saunders, Steven Bills, Henk Tillman, and Daniel Mossing)\nfor valuable discussions regarding the design of various experiments. We thank Wes Gurnee, Adam\nJermyn, Stella Biderman, Leo Gao, Curtis Huebner, Scott Emmons, and William Saunders for their\nfeedback on earlier versions of this paper. Thanks to Delta Hessler for proofreading. LR is supported\nby the Long Term Future Fund. RH is supported by an Open Philanthropy grant. HC was greatly\nhelped by the MATS program, funded by AI Safety Support.\nREFERENCES\nStella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric\nHallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al.\nPythia: A suite for analyzing large language models across training and scaling. In International\nConference on Machine Learning, pp. 2397\u20132430. PMLR, 2023.\n9\nSteven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever,\nJan Leike, Jeff Wu, and William Saunders.\nLanguage models can explain neurons in lan-\nguage models. URL https://openaipublic. blob. core. windows. net/neuron-explainer/paper/index.\nhtml.(Date accessed: 14.05. 2023), 2023.\nNick Cammarata, Gabriel Goh, Shan Carter, Chelsea Voss, Ludwig Schubert, and Chris Olah. Curve\ncircuits. Distill, 2021. doi: 10.23915/distill.00024.006. https://distill.pub/2020/circuits/curve-\ncircuits.\nArthur Conmy, Augustine N Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adri`a Garriga-\nAlonso. Towards automated circuit discovery for mechanistic interpretability. arXiv preprint\narXiv:2304.14997, 2023.\nGonc\u00b8alo M Correia, Vlad Niculae, and Andr\u00b4e FT Martins. Adaptively sparse transformers. In\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp.\n2174\u20132184, 2019.\nTim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix\nmultiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.\nNelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann,\nAmanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al. A mathematical framework for\ntransformer circuits. Transformer Circuits Thread, 1, 2021.\nNelson Elhage, Tristan Hume, Catherine Olsson, Neel Nanda, Tom Henighan, Scott Johnston, Sheer\nElShowk, Nicholas Joseph, Nova DasSarma, Ben Mann, Danny Hernandez, Amanda Askell, Ka-\nmal Ndousse, Andy Jones, Dawn Drain, Anna Chen, Yuntao Bai, Deep Ganguli, Liane Lovitt,\nZac Hatfield-Dodds, Jackson Kernion, Tom Conerly, Shauna Kravec, Stanislav Fort, Saurav Ka-\ndavath, Josh Jacobson, Eli Tran-Johnson, Jared Kaplan, Jack Clark, Tom Brown, Sam McCan-\ndlish, Dario Amodei, and Christopher Olah. Softmax linear units. Transformer Circuits Thread,\n2022a. https://transformer-circuits.pub/2022/solu/index.html.\nNelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec,\nZac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, et al. Toy models of superposi-\ntion. arXiv preprint arXiv:2209.10652, 2022b.\nNelson Elhage, Robert Lasenby, and Chris Olah.\nPrivileged bases in the transformer residual\nstream, 2023.\nURL https://transformer-circuits.pub/2023/privileged-\nbasis/index.html. Accessed: 2023-08-07.\nJonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural\nnetworks. arXiv preprint arXiv:1803.03635, 2018.\nKunihiko Fukushima. Cognitron: A self-organizing multilayered neural network. Biol. Cybern., 20\n(3\u20134):121\u2013136, sep 1975. ISSN 0340-1200. doi: 10.1007/BF00342633. URL https://doi.\norg/10.1007/BF00342633.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\nPhang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text\nfor language modeling. arXiv preprint arXiv:2101.00027, 2020.\nGeorgios Georgiadis. Accelerating convolutional neural networks via activation map compression.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n7085\u20137095, 2019.\nDan Hendrycks, Mantas Mazeika, and Thomas Woodside. An overview of catastrophic ai risks.\narXiv preprint arXiv:2306.12001, 2023.\nTheodoros Kasioumis, Joe Townsend, and Hiroya Inakoshi. Elite backprop: Training sparse inter-\npretable neurons. In NeSy, pp. 82\u201393, 2021.\nHonglak Lee, Alexis Battle, Rajat Raina, and Andrew Ng.\nEfficient sparse coding algorithms.\nAdvances in neural information processing systems, 19, 2006.\n10\nClara Meister, Stefan Lazov, Isabelle Augenstein, and Ryan Cotterell. Is sparse attention more\ninterpretable? In Annual Meeting of the Association for Computational Linguistics, 2021. URL\nhttps://api.semanticscholar.org/CorpusID:235293798.\nRichard Ngo, Lawrence Chan, and S\u00a8oren Mindermann. The alignment problem from a deep learning\nperspective. arXiv preprint arXiv:2209.00626, 2022.\nChris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter.\nZoom in: An introduction to circuits. Distill, 5(3):e00024\u2013001, 2020.\nBruno A Olshausen and David J Field. Sparse coding with an overcomplete basis set: A strategy\nemployed by v1? Vision research, 37(23):3311\u20133325, 1997.\nBruno A Olshausen and David J Field. Sparse coding of sensory inputs. Current opinion in neuro-\nbiology, 14(4):481\u2013487, 2004.\nQing Qu, Yuexiang Zhai, Xiao Li, Yuqian Zhang, and Zhihui Zhu. Analysis of the optimization\nlandscapes for overcomplete representation learning. arXiv preprint arXiv:1912.02427, 2019.\nLee Sharkey, Dan Braun, and Beren Millidge.\nTaking features out of superposition with\nsparse autoencoders,\n2023.\nURL https://www.alignmentforum.org/posts/\nz6QQJbtpkEAX3Aojj/interim-research-report-taking-features-out-\nof-superposition. Accessed: 2023-05-10.\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and\nStuart Shieber. Investigating gender bias in language models using causal mediation analysis.\nAdvances in neural information processing systems, 33:12388\u201312401, 2020.\nKevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Inter-\npretability in the wild: a circuit for indirect object identification in gpt-2 small. arXiv preprint\narXiv:2211.00593, 2022.\nJohn Wright and Yi Ma. High-dimensional data analysis with low-dimensional models: Principles,\ncomputation, and applications. Cambridge University Press, 2022.\nZeyu Yun, Yubei Chen, Bruno A Olshausen, and Yann LeCun. Transformer visualization via dictio-\nnary learning: contextualized embedding as a linear superposition of transformer factors. arXiv\npreprint arXiv:2103.15949, 2021.\nA\nAUTOINTERPRETATION PROTOCOL\nThe autointerpretability process consists of five steps and yields both an interpretation and an au-\ntointerpretability score:\n1. On each of the first 50,000 lines of OpenWebText, take a 64-token sentence-fragment, and\nmeasure the feature\u2019s activation on each token of this fragment. Feature activations are\nrescaled to integer values between 0 and 10.\n2. Take the 20 fragments with the top activation scores and pass 5 of these to GPT-4, along\nwith the rescaled per-token activations. Instruct GPT-4 to suggest an explanation for when\nthe feature (or neuron) fires, resulting in an interpretation.\n3. Use GPT-3.56 to simulate the feature across another 5 highly activating fragments and 5\nrandomly selected fragments (with non-zero variation) by asking it to provide the per-token\nactivations.\n6While the process described in Bills et al. (2023) uses GPT-4 for the simulation step, we use GPT-3.5.\nThis is because the simulation protocol requires the model\u2019s logprobs for scoring, and OpenAI\u2019s public API for\nGPT-3.5 (but not GPT-4) supports returning logprobs.\n11\n4. Compute the correlation of the simulated activations and the actual activations. This corre-\nlation is the autointerpretability score of the feature. The texts chosen for scoring a feature\ncan be random text fragments, fragments chosen for containing a particularly high activa-\ntion of that feature, or an even mixture of the two. We use a mixture of the two unless\notherwise noted, also called \u2018top-random\u2019 scoring.\n5. If, amongst the 50,000 fragments, there are fewer than 20 which contain non-zero variation\nin activation, then the feature is skipped entirely.\nAlthough the use of random fragments in Step 4 is ultimately preferable given a large enough sample\nsize, the small sample sizes of a total of 640 tokens used for analysis mean that a random sample\nwill likely not contain any highly activating examples for all but the most common features, making\ntop-random scoring a desirable alternative.\nB\nSPARSE AUTOENCODER TRAINING AND HYPERPARAMETER SELECTION\nTo train the sparse autoencoder described in Section 2, we use data from the Pile (Gao et al., 2020), a\nlarge, public webtext corpus. We run the model that we want to interpret over this text while caching\nand saving the activations at a particular layer. These activations then form a dataset, which we use\nto train the autoencoders. The autoencoders are trained with the Adam optimiser with a learning rate\nof 1e-3 and are trained on 5-50M activation vectors for 1-3 epochs, with larger dictionaries taking\nlonger to converge. A single training run using this quantity of data completes in under an hour on\na single A40 GPU.\nWhen varying the hyperparameter \u03b1 which controls the importance of the sparsity loss term, we\nconsistently find a smooth tradeoff between the sparsity and accuracy of our autoencoder, as shown\nin Figure 6. The lack of a \u2018bump\u2019 or \u2018knee\u2019 in these plots provides some evidence that there is not a\nsingle correct way to decompose activation spaces into a sparse basis, though to confirm this would\nrequire many additional experiments. Figure 7 shows the convergence behaviour of a set of models\nwith varying \u03b1 over multiple epochs.\nFigure 6: The tradeoff between the average number of features active and the proportion of variance\nthat is unexplained for the MLP at layer 0.\n12\nFigure 7: The tradeoff between sparsity and unexplained variance in our reconstruction. Each series\nof points is a sweep of the \u03b1 hyperparameter, trained for the number of epochs given in the legend.\nC\nFURTHER AUTOINTERPRETATION RESULTS\nC.1\nINTERPRETABILITY IS CONSISTENT ACROSS DICTIONARY SIZES\nWe find that larger interpretability scores of our feature dictionaries are not limited to overcomplete\ndictionaries (where the ratio, R, of dictionary features to model dimensions is > 1), but occurs\neven in dictionaries that are smaller than the underlying basis, as shown in Figure 8. These small\ndictionaries are able to reconstruct the activation vectors less accurately, so with each feature being\nsimilarly interpretable, the larger dictionaries will be able to explain more of the overall variance.\nC.2\nHIGH INTERPRETABILITY SCORES ARE NOT AN ARTEFACT OF TOP SCORING\nA possible concern is that the autointerpretability method described in Section 3 combines top acti-\nvating fragments (which are usually large) with random activations (which are usually small), mak-\ning it relatively easy to identify activations. Following the lead of Bills et al. (2023), we control for\nthis by recomputing the autointerpretation scores by modifying Step 3 using only randomly selected\nfragments. With large sample sizes, using random fragments should be the true test of our ability\nto interpret a potential feature. However, the features we are considering are heavy-tailed, so with\nlimited sample sizes, we should expect random samples to underestimate the true correlation.\nIn Figure 9 we show autointerpretability scores for fragments using only random fragments. Match-\ning Bills et al. (2023), we find that random-only scores are significantly smaller than top-and-random\nscores, but also that our learned features still consistently outperform the baselines, especially in the\nearly layers. Since our learned features are more sparse than the baselines and thus, activate less on a\ngiven fragment, this is likely to underestimate the performance of sparse coding relative to baselines.\nAn additional potential concern is that the structure of the autoencoders allows them to be sensitive\nto less than a full direction in the activation space, resulting in an unfair comparison. We show in\nAppendix G that this is not the source of the improved performance of sparse coding.\n13\nFigure 8: Comparison of average interpretability scores across dictionary sizes. All dictionaries were\ntrained on 20M activation vectors obtained by running Pythia-70M over the Pile with \u03b1 = .00086.\nFigure 9: Random-only interpretability scores across each layer, a measure of how well the inter-\npretation of the top activating cluster is able to explain the entire range of activations.\n14\nFigure 10: Top-and-random and random-only interpretability scores for across each MLP layer,\nusing an \u21131 coefficient \u03b1 = 3.2e \u2212 4 and dictionary size ratio R = 1.\nWhile the residual stream can usually be treated as a vector space with no privileged basis (a basis in\nwhich we would expect changes to be unusually meaningful, such as the standard basis after a non-\nlinearity in an MLP), it has been noted that there is a tendency for transformers to store information\nin the residual stream basis(Dettmers et al., 2022), which is believed to be caused by the Adam\noptimiser saving gradients with finite precision in the residual basis(Elhage et al., 2023). We do not\nfind residual stream basis directions to be any more interpretable than random directions.\nC.3\nINTERPRETING THE MLP SUBLAYER\nOur approach of learning a feature dictionary and interpreting the resulting features can, in principle,\nbe applied to any set of internal activations of a language model, not just the residual stream. Ap-\nplying our approach to the MLP sublayer of a transformer resulted in mixed success. Our approach\nstill finds many features that are more interpretable than the neurons. However, our architecture also\nlearns many dead features, which never activate across the entire corpus. In some cases, there are so\nmany dead features that the set of living features does not form an overcomplete basis. For example,\nin a dictionary with twice as many features as neurons, less than half might be active enough to\nperform automatic interpretability. The exceptions to this are the early layers, where a large fraction\nof them are active.\nFor learning features in MLP layers, we find that we retain a larger number of features if we use a\ndifferent matrix for the encoder and decoder, so that Equations 1 and 2 become\nc\n=\nReLU(Mex + b)\n(5)\n\u02c6x\n=\nM T\nd c\n(6)\nWe are currently working on methods to overcome this and find truly overcomplete bases in the\nmiddle and later MLP layers.\nC.4\nINTERPRETABILITY SCORES CORRELATE WITH KURTOSIS AND SKEW OF ACTIVATION\nIt has been shown that the search for sparse, overcomplete dictionaries can be reformulated in terms\nof the search for directions that maximise the \u21134-norm (Qu et al., 2019).\nWe offer a test of the utility of this by analysing the correlation between interpretability and a number\nof properties of learned directions. We find that there is a correlation of 0.19 and 0.24 between the\ndegree of positive skew and kurtosis respectively that feature activations have and their top-and-\nrandom interpretability scores, as shown in Table 2.\nThis also accords with the intuitive explanation that the degree of interference due to other active\nfeatures will be roughly normally distributed by the central limit theorem. If this is the case, then\nfeatures will be notable for their heavy-tailedness.\n15\nMoment\nCorrelation with top-random interpretability score\nMean\n-0.09\nVariance\n0.02\nSkew\n0.20\nKurtosis\n0.15\nTable 2: Correlation of interpretability score with feature moments across residual stream results,\nall layers, with dictionary size ratios R \u2208 {0.5, 1, 2, 4, 8}.\nThis also explains why Independent Component Analysis (ICA), which maximises the non-\nGaussianity of the found components, is the best performing of the alternatives that we considered.\nD\nQUALITATIVE FEATURE ANALYSIS\nD.1\nRESIDUAL STREAM BASIS\nFigure 11 gives a token activation histogram of the residual stream basis. Connecting this residual\nstream dimension to the apostrophe feature from Figure 4, this residual dimension was the 10th\nhighest dimension read from the residual stream by our feature7.\nFigure 11: Histogram of token counts in the neuron basis. Although there are a large fraction of\napostrophes in the upper activation range, this only explains a very small fraction of the variance for\nmiddle-to-lower activation ranges.\nD.2\nEXAMPLES OF LEARNED FEATURES\nOther features are shown in Figures 12, 13, 14, and 15.\nD.3\nFEATURE SEARCH DETAILS\nWe searched for the apostrophe feature using the sentence \u201c I don\u2019t know about that. It is now up to\nDave\u201d\u2019, and seeing which feature (or residual stream dimension) activates the most for the last apos-\ntrophe token. The top activating feature in our dictionary was an outlier dimension feature (i.e., a\nfeature direction that mainly reads from an outlier dimension of the residual stream), the apostrophes\n7The first 9 did not have apostrophes in their top-activations like dimension 21.\n16\nFigure 12: \u2018If\u2019 feature in coding contexts\nFigure 13: \u2018Dis\u2019 token-level feature showing bigrams, such as \u2018disCLAIM\u2019, \u2018disclosed\u2019, \u2018disor-\ndered\u2019, etc.\nafter O (and predicted O\u2019Brien, O\u2019Donnell, O\u2019Connor, O\u2019clock, etc), then the apostrophe-preceding-\ns feature.\nFor the residual basis dimension, we searched for max and min activating dimensions (since the\nresidual stream can be both positive and negative), where the top two most positive dimensions were\noutlier dimensions, the top two negative dimensions were our displayed one and another outlier\ndimension, respectively.\nD.4\nFAILED INTERPRETABILITY METHODS\nWe attempted a weight-based method going from the dictionary in layer 4 to the dictionary in layer\n5 by multiplying a feature by the MLP and checking the cosine similarity with features in layer 5.\nThere were no meaningful connections. Additionally, it\u2019s unclear how to apply this to the Attention\nsublayer since we\u2019d need to see which position dimension the feature is in. We expected this failed\nby going out of distribution.\nE\nNUMBER OF ACTIVE FEATURES\nIn Figure 16 we see that, for residual streams, we consistently learn dictionaries that are at least 4x\novercomplete before some features start to drop out completely, with the correct hyperparameters.\nFor MLP layers you see large numbers of dead features even with hyperparameter \u03b1 = 0. These\nfigures informed the selection of \u03b1 = 8.6e\u22124 and \u03b1 = 3.2e\u22124 that went into the graphs in Section\n3 for the residual stream and MLP respectively. Due to the large part of the input space that is never\nused due to the non-linearity, it is much easier for MLP dictionary features to become stuck at a\nposition where they hardly ever activate. In future we plan to reinitialise such \u2018dead features\u2019 to\nensure that we learn as many useful dictionary features as possible.\n17\nFigure 14: Apostrophe feature in \u201cI\u2019ll\u201d-like contexts.\nFigure 15: Apostrophe feature in \u201cdon\u2019t\u201d-like contexts.\nF\nEDITING IOI BEHAVIOUR ON OTHER LAYERS\nIn Figure 17 we show results of the procedure in Section 4 across a range of layers in Pythia-410M.\nG\nTOP K COMPARISONS\nAs mentioned in Section 3, the comparison directions learnt by sparse coding and those in the\nbaselines are not perfectly even. This is because, for example, a PCA direction is active to an entire\nhalf-space on one side of a hyperplane through the origin, whereas a sparse coding feature activates\non less than a full direction, being only on the far side of a hyperplane that does not intersect the\norigin. This is due to the bias applied before the activation, which is, in practice, always negative.\nTo test whether this difference is responsible for the higher scores, we run a variant of PCA and ICA\nin which we have a fixed number of directions, K, which can be active for any single datapoint. We\nset this K to be equal to the average number of active features for a sparse coding dictionary with\nratio R = 1 and \u03b1 = 8.6e \u2212 4 trained on the layer in question. We compare the results in Figure 18,\nshowing that this change does not explain more than a small fraction of the improvement in scores.\n18\nFigure 16: The number of features that are active, defined as activating more than 10 times across\n10M datapoints, changes with sparsity hyperparamter \u03b1 and dictionary size ratio R.\nFigure 17: Divergence from target output against number of features patched and magnitude of edits\nfor layers 3, 7, 11, 15, 19 and 23 of the residual stream of Pythia-410M. Pythia-410M has 24 layers,\nwhich we index 0, 1, ..., 23.\n19\nFigure 18: Autointerpretation scores across layers for the residual stream, including top-K baselines\nfor ICA and PCA.\n20\n"
  },
  {
    "title": "Investigating Answerability of LLMs for Long-Form Question Answering",
    "link": "https://arxiv.org/pdf/2309.08210.pdf",
    "upvote": "11",
    "text": "Investigating Answerability of LLMs for Long-Form Question Answering\nMeghana Moorthy Bhat\nRui Meng\nYe Liu\nYingbo Zhou and Semih Yavuz\nSalesforce Research\n{meghana.bhat, ruimeng, yeliu, yingbo.zhou, syavuz}@salesforce.com\nAbstract\nAs we embark on a new era of LLMs, it be-\ncomes increasingly crucial to understand their\ncapabilities, limitations, and differences. To-\nward making further progress in this direction,\nwe strive to build a deeper understanding of\nthe gaps between massive LLMs (e.g., Chat-\nGPT) and smaller yet effective open-source\nLLMs and their distilled counterparts. To this\nend, we specifically focus on long-form ques-\ntion answering (LFQA) because it has several\npractical and impactful applications (e.g., trou-\nbleshooting, customer service, etc.) yet is still\nunderstudied and challenging for LLMs. We\npropose a question-generation method from ab-\nstractive summaries and show that generating\nfollow-up questions from summaries of long\ndocuments can create a challenging setting for\nLLMs to reason and infer from long contexts.\nOur experimental results confirm that: (1) our\nproposed method of generating questions from\nabstractive summaries pose a challenging setup\nfor LLMs and shows performance gaps be-\ntween LLMs like ChatGPT and open-source\nLLMs (Alpaca, Llama) (2) open-source LLMs\nexhibit decreased reliance on context for gen-\nerated questions from the original document,\nbut their generation capabilities drop signifi-\ncantly on generated questions from summaries\n\u2013 especially for longer contexts (>1024 tokens).\n1\nIntroduction\nWhile Large Language Models (LLMs) like Chat-\nGPT, GPT-4 (OpenAI, 2023) have exhibited supe-\nrior performance across various benchmarks, open-\nsource efforts have also been progressing rapidly\nin catching up across different applications and\nbenchmarks like MMLU (Hendrycks et al., 2021),\nOpenLLMBoard (Anil et al., 2023; Beeching et al.,\n2023; Touvron et al., 2023). As we move into the\nnew era of LLMs with fast-paced progress on new\nmodels and techniques, it becomes increasingly im-\nportant to understand the capabilities, limitations,\nand differences between them.\nWith LLMs capable of generating coherent text\nhas proven to perform well in tasks like summariza-\ntion (Ouyang et al., 2022), their performance on\nLFQA is relatively less known. Long-Form Ques-\ntion Answering (LFQA) is one of the important un-\nsolved challenges with diverse and impactful real-\nworld applications (e.g., help forums, troubleshoot-\ning, customer services, etc.) Answering such ques-\ntions often requires complex reasoning abilities to\nunderstand query and reason across spans of infor-\nmation scattered across original document.\nAbstractive summaries contain the crux of the\narticles in a compressed form (Fabbri et al., 2020).\nWe hypothesize that follow-up questions from these\nsummaries would require a deeper understanding\nof the topics that would link different parts of the\nsource document. Moreover, Pang et al. (2022)\ndemonstrate that answers that require understand-\ning more than a third of the long document are often\nrated as \u201cHARD\u201d by humans. Therefore, we pro-\npose a scalable evaluation method to analyze and\nstudy the disparities of massive LLMs with smaller\nyet proven successful base LLMs (e.g., Llama-7B,\n13B) and their distilled versions (e.g., Alpaca-7B,\n13B). To this end, we propose to prompt ChatGPT\nwith specific instructions to generate complex ques-\ntions from document summaries.\nOur empirical analysis on two fronts (complex-\nity of generated questions and answer quality of\nopen-source LLMs) show that follow-up questions\ngenerated from summaries pose a challenging yet\nmore realistic setup for testing the reasoning abil-\nities of LLMs. Since relying fully on the human\nevaluation for long-form QA is expensive and dif-\nficult to scale (Pagnoni et al., 2021), we instead\nleverage GPT-4 to evaluate the answer quality on\ncoherence, relevance, factual consistency, and ac-\ncuracy following prior works (Fabbri et al., 2020;\nFan et al., 2019). However, we also do a smaller\nscale human evaluation, which shows that GPT-4\nhas a high correlation with human evaluation mak-\narXiv:2309.08210v1  [cs.CL]  15 Sep 2023\ning our evaluation reliable. Our main findings from\nthis study are as follows:\n\u2022 Our proposed method of generating questions\nfrom abstractive summaries require inferring\nfrom longer contexts, with multiple passes\nthrough the context for > 20% times.\n\u2022 Distilled LLMs (Alpaca-7B, 13B) tend to rely\nless on context for generated questions from the\noriginal document, but their generation capabil-\nities drop significantly on generated questions\nfrom document summaries.\n\u2022 Answers generated by distilled LLMs can be co-\nherent across different settings; but tend to drift\nfrom the question, generate repetitive and par-\ntially correct answers for the questions generated\nfrom summaries (> 16.8%)\n\u2022 Alpaca-7B, 13B mostly generate meaningful an-\nswers than base LLMs (Llama) but are sensitive\nto longer contexts (>1024 tokens).\n2\nRelated Work\nReasoning over Long Documents: LLMs have\nshown amazing capabilities to reason over a num-\nber of tasks like commonsense reasoning (Talmor\net al., 2019), mathematical and symbolic reason-\ning (Huang and Chang, 2023; Cobbe et al., 2021),\nquestion answering tasks like SQuaD, HotpotQA.\nHowever, most of these tasks do not require long\ncontext and answers are often a short phrase or\na span of text from the context. In this work, we\nevaluate LLMs to reason over long documents that\nwould require deeper understanding capabilities\nand longer context to answer by prompting LLMs\n(ChatGPT) to generate follow-up questions from\nsummaries of long documents.\nModel-based Evaluation: Prior work has pro-\nposed automatic evaluation metrics using learned\nmodels (Zhang* et al., 2020; Laban et al., 2022);\nespecially for long form text generation tasks like\nsummarization (Fabbri et al., 2020; Kryscinski\net al., 2020) where consistency and coherency is\nmeasured between the source document and gen-\nerated summary as entailment. Recently, Liu et al.\n(2023) showed that GPT-4 has the highest corre-\nlation with humans and surpasses all other auto-\nevaluation methods on summarization tasks. We\ntake inspiration from these works to design evalu-\nation prompts and use GPT-4 as the evaluator for\ngenerated answers.\n3\nProposed Evaluation Method\n3.1\nData Source\nIn order to create a dataset that is diverse and widely\nusable, we use Wikipedia articles. Using metadata\nof the Category list from Wikipedia API, we con-\ntrol the diversity of topics and collect articles from\neach of the following 9 domains: Physics, Entertain-\nment, History, Computer Science, Social Sciences,\nSociety, Economics, Medicine, and Sports.\nThe article pages can often be lengthy to fit in the\ncontext of LLMs. Hence, we extract section-wise\ndata from the article pages that have a minimum\nlength of 256 tokens using SpaCy tokenizer and\ncombine the shorter sections together. For a fair\ncomparison between different models, we set a\nmaximum context length of 2k tokens in our exper-\niments. In addition, we filter out non-informative\ndocuments using pre-processing filters. Further de-\ntails are available in Appendix A.5.\n3.2\nQuestion Generation using ChatGPT\nWe formulate our question generation method as a\ntwo-step process: (1) Summarization and (2) Ques-\ntion generation from summary.\nSummarization First, we collect section wise pas-\nsages from Wikipedia as described in Section 3.1.\nThen, we prompt ChatGPT (gpt-turbo-3.5) to gen-\nerate summary of original document. In order to\nprovide more context to ChatGPT, we provide in-\nformation about the title and the domain of the\narticle in the passage.\nQuestion generation from summary In this step,\nwe prompt ChatGPT to generate questions using\ndocument summaries as context. To avoid random\norder question generation, we instruct ChatGPT\nto provide top-3 complex questions to answer. To\ndemonstrate the usefulness of our question genera-\ntion process, we also establish a baseline with the\nsame instructions where questions are directly gen-\nerated from the passage.\nPlease refer to the appendix A.1 for the prompt\nused in our setup. In summary, we generate 3 ques-\ntions for 50 passages in each domain totaling to\n1350 questions for each setting.\n3.3\nEvaluation of Generated Question\nComplexity\nPang et al. (2022) designed extensive annotation\nguidelines to assess the complexity of questions. Of\nthe questions rated as \u2019HARD\u2019 by humans, 26.7%\nof the questions (20.2% higher than the easier ones)\nQuestion\nChoices\nQ1: Is the question answerable from the\ngiven context and is unambiguous?\nA. Yes B. No\nQ2. How much of the passage is needed\nA. Only a sentence or two\nas context to answer the question?\nB. More than 2 sentences\nbut lesser than a paragraph\nC. Atleast a third of the\nentire passage\nD. Most of the passage\nQ3: Does the question require multiple\npasses through the passage?\nA. Yes B. No\nTable 1: Prompts designed to evaluate the complexity of\ngenerated questions.\nneeded at least one-third or more of the given in-\nformation to be answered. In order to assess the\nquality of generated questions, we prompt Chat-\nGPT with the questions (Table 1) for (1) From the\npassage (QG-Passage) (2) From the summary\n(QG-Summary). Following prior work, by major-\nity voting we exclude the questions that are rated\nas unanswerable by ChatGPT by prompting the\nquestions with different top_p = {0.8, 0.9,\n1}. After filtering, we have 1278 generated ques-\ntions from each setting.\nEvaluation Metric\nQG - Passage\nQG - Summary\nQ1: Unambiguity\n96.6%\n94.7%\nQ2. Context Length:\nA sentence or less than a paragraph\n79.3%\n75.7 %\nAt least a third or most of the passage\n20.7%\n24.3%\nQ3: Multi-pass of the passage\n24.4%\n31%\nTable 2: Prompts designed to evaluate the complexity of\ngenerated questions.\n4\nResults and Analysis\n4.1\nExperiment Setup\nAs few-shot setting is infeasible in our setting due\nto context length, we compare model performance\non zero-shot evaluation. We prompt the following\nmodels to generate free-form text as answers on our\nfinal evaluation dataset: ChatGPT (OpenAI, 2023),\nAlpaca-7B, 13B (Taori et al., 2023), LLaMa-7B,\n13B (Touvron et al., 2023). We use OpenAI API\nfor ChatGPT and load checkpoints for open-source\nLLMs from HuggingFace 1. The prompt used for\ngenerating answers are in Appendix A. Please note\nthat our experiments do not consider input beyond\n2k sequence length for fair comparisons with other\nmodels. We also test generating questions from Al-\npaca and found them to not follow instructions and\noften generate irrelevant content. Our detailed anal-\nysis can be found in Appendix A.2.\nGPT-4 as evaluator has shown high correlation\nwith human evaluation in long form text generation\n1https://huggingface.com\ntasks like summarization (Liu et al., 2023) surpass-\ning other auto-evaluation metrics like ROUGE and\nBLEU scores. Since LLMs are expected to gen-\nerate free form answers for our setting, we take\ninspiration from prior works on long-form text gen-\neration metrics (Fabbri et al., 2020) and adopt them\nin our evaluation for coherency, consistency, accu-\nracy, and relevance. Basically, we adopt the defi-\nnitions used as guidelines for human evaluation to\nour method as shown below:\nCoherency: Answer should be well-structured and\nwell-organized and should not just be a heap of\nrelated information.\nRelevance: Answer should be relevant to the ques-\ntion and the context. The answer should be concise\nand avoid drifting from the question being asked.\nFactual consistency: The context should be the\nprimary source for the answer. The answer should\nnot contain fabricated facts and should entail infor-\nmation present in the context.\nAccuracy: Answer should be satisfactory and com-\nplete to the question being asked. Measure the cor-\nrectness of the answer by checking if the response\nanswers the presented question.\nWe prompt GPT-4 to rate answers on a scale\nfrom 0 to 3 (higher the better) on all of the four\nmetrics. We average all the ratings obtained from\nGPT-4 and present the results in Table 3. Our eval-\nuation prompt can be found in Appendix A.3.1.\nWe hypothesize that an optimal prompt should\nalways prefer human answers and not be biased\ntowards model-generated answers. Laskar et al.\n(2023) show that LLMs like ChatGPT still un-\nderperform to humans on TruthfulQA dataset(Lin\net al., 2022). Hence, we perform proxy testing with\nGPT-4 on TruthfulQA dataset in order to verify\nthe reliability and faithfulness of our evaluation\nprompt. We test the generated answers from Chat-\nGPT and open-source LLMs against the ground\ntruth on randomly sampled 50 test instances and\nfind that our evaluation prompt with GPT-4 prompt\nprefers human-written answers for factual consis-\ntency and correctness over model-generated ones\nmore than > 90% of the times. In addition, we\nalso perform human evaluation of LLM generated\nanswers and discuss the correlation of GPT-4 eval-\nuation with human evaluation in Section A.4.\n4.2\nResults\nOur experiment results show that ChatGPT out-\nperforms other LLMs in all the metrics by a wide\nmargin from 22.4% - 40.1% against the second-\nllama7B\nalpaca7B\nllama13B\nalpaca13B\nchatgpt\n0.0\n0.4\n0.8\n1.2\n1.6\n2.0\n2.4\n2.8\n3.2\nCoherence\n0.67\n1.82\n1.09\n1.97\n2.92\n1.00\n1.86\n1.26\n1.98\n2.98\nSummary\nPassage\nllama7B\nalpaca7B\nllama13B\nalpaca13B\nchatgpt\n0.0\n0.4\n0.8\n1.2\n1.6\n2.0\n2.4\n2.8\n3.2\nRelevance\n0.67\n1.89\n1.14\n2.11\n2.82\n1.00\n1.96\n1.40\n2.12\n2.93\nSummary\nPassage\nllama7B\nalpaca7B\nllama13B\nalpaca13B\nchatgpt\n0.0\n0.4\n0.8\n1.2\n1.6\n2.0\n2.4\n2.8\n3.2\nAccuracy\n0.67\n1.74\n0.98\n1.91\n2.75\n1.11\n1.81\n1.21\n1.93\n2.89\nSummary\nPassage\nllama7B\nalpaca7B\nllama13B\nalpaca13B\nchatgpt\n0.0\n0.4\n0.8\n1.2\n1.6\n2.0\n2.4\n2.8\n3.2\nFactual Consistency\n1.00\n1.96\n1.58\n2.18\n2.77\n1.67\n1.99\n1.74\n2.14\n2.89\nSummary\nPassage\nFigure 1: Graphs showing the breakdown of models with respect to different metrics used in evaluation: (a)\nCoherence (b) Relevance (c) Answer Accuracy (d) Factual Consistency\nModel\nQG-Passage\nQG-Summary\nw/o context\nw/ context\nw/o context\nw/ context\nChatGPT\n2.78\n2.93\n2.67\n2.82\nAlpaca-13B\n2.27\n2.09\n2.04\n2.09\nLlaMa-13B\n1.22\n1.47\n0.98\n1.28\nAlpaca-7B\n2.04\n1.96\n1.64\n1.89\nLlaMa-7B\n0.89\n1.12\n0.66\n0.78\nTable 3: Performance of different models based on GPT-\n4 evaluation. The table shows average ratings across all\nmetrics: accuracy, coherency, consistency, relevance.\nbest performing LLM (Alpaca-13B). However; all\nthe models including ChatGPT generate less accu-\nrate and relevant answers for QG-Summary when\ncompared to QG-Passage; while the gap is much\nlarger in open-source LLMs. We also find that most\nof the LLMs find context important in order to gen-\nerate answers; however, the gap is much smaller\nfor QG-Passage (avg. gap of 0.12 v.s. 0.2). Sur-\nprisingly, Alpaca-7B, 13B models perform better\nw/o context for QG-Passage. We hypothesize\nthat questions directly generated from the context\npassage can be simple that could be directly an-\nswered from the parametric knowledge of LLMs\nwithout additional context. On further analysis, we\nobserve that Alpaca-7B,13B performance drops\nsignificantly in longer contexts (Figure 2). We hy-\npothesize that in a constrained sequence length\nsetting, adding supporting context (even gold pas-\nsage) may not be always helpful. We leave further\nanalysis for future work and hope our analysis will\nmotivate future research directions to study when\nto add/not add context for different tasks.\nPerformance of LLMs on different metrics Fig-\nure 1 presents the performance of models across\ndifferent metrics for QG-Summary. We observe\ntwo trends: (1) Open-source base LLMs (Llama-\n7B,13B) suffer at all fronts significantly on gen-\nerated answer quality whereas distilled models\nperform better than their counterparts (Llama)\non all the settings. (2) QG-Summary provides a\nmore challenging setting for all the LLMs: specif-\nically, we notice that degradation in coherency\nscore is negligent on ChatGPT and Alpaca-13B\nwhile other metrics like relevance, answer accu-\n<512\n<1024\n<2048\nContext Length\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nAverage Rating\nllama7B\nalpaca7B\nllama13B\nalpaca13B\nchatgpt\nFigure 2: Performance (avg. ratings) of LLMs across\ndifferent context length.\nracy and factual consistency degrade consistently.\nWe find open-source LLMs to drift from the ques-\ntion, generate partially correct answers and repeat\nmore frequently in QG-Summary setting leading\nto lower scores. This further confirms that our\nproposed evaluation method QG-Summary chal-\nlenges LLMs for deeper reasoning capabilities. //\nContext Length Analysis We analyze the effect\nof context length across LLMs in our proposed\nsetting (QG-Summary). As expected, ChatGPT\nremains robust to context length until 2k tokens\nwith Llama variants performing worse than other\nmodels (Figure 2). Interestingly, we find distilled\nmodels (Alpaca) being consistent until 1024 tokens,\nhowever beyond > 1024 tokens, the performance\ndegrades at a higher rate than Llama.\n5\nConclusion\nWith the emergence of LLMs like ChatGPT and\nopen-source successful LLMs, it is extremely im-\nportant to understand the capabilities and limita-\ntions of different LLMs. In order to test deeper\nreasoning abilities of LLMs by referring to longer\ncontexts, we evaluate answers generated by LLMs\non questions generated by ChatGPT on summaries\nof long documents. Results show that our proposed\nmethod of question generation poses a challenging\nsetup for LLMs and shed light on performance gaps\nbetween massive LLMs and open-source LLMs.\nWe hope our analysis motivates future research di-\nrections such as leveraging longer contexts in a\nconstrained sequence length setting and developing\nbetter long-form text generation for smaller LLMs.\n6\nLimitations\nIn this study, we propose an automatic evaluation\nsetting to generate questions from summaries, and\nthe generated answers from LLMs are evaluated\nusing GPT-4 for different metrics. Experimental\nresults show that our proposed evaluation setting\nproves to be a challenging setup for LLMs. How-\never, our study might have some limitations.\nGPT-4 as evaluator While GPT-4 has shown a\nhigh correlation with human evaluation for long\nform text generation (Liu et al., 2023), the capabili-\nties of using GPT-4 for evaluation is an active area\nof research in itself. Hence, our results might be\nlimited by the undiscovered capabilities of GPT-4.\nChatGPT for question generation Generating an-\nswers on questions prompted from ChatGPT might\nlead to optimistic results of ChatGPT. However,\nthere exists limitations with other baselines to gen-\nerate meaningful questions. We show extensive\nanalysis of using other LLMs for question gen-\neration (Appendix A.2).\nUnknown training data Little is known about\nthe training data distribution of massive LLMs like\nChatGPT. Models trained with different methods\nand data distribution make the evaluation for fair\ncomparison harder.\nReferences\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin\nJohnson, Dmitry Lepikhin, Alexandre Passos, Sia-\nmak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, Eric Chu, Jonathan H. Clark, Laurent El\nShafey, Yanping Huang, Kathy Meier-Hellstern, Gau-\nrav Mishra, Erica Moreira, Mark Omernick, Kevin\nRobinson, Sebastian Ruder, Yi Tay, Kefan Xiao,\nYuanzhong Xu, Yujing Zhang, Gustavo Hernandez\nAbrego, Junwhan Ahn, Jacob Austin, Paul Barham,\nJan Botha, James Bradbury, Siddhartha Brahma,\nKevin Brooks, Michele Catasta, Yong Cheng, Colin\nCherry, Christopher A. Choquette-Choo, Aakanksha\nChowdhery, Cl\u00e9ment Crepy, Shachi Dave, Mostafa\nDehghani, Sunipa Dev, Jacob Devlin, Mark D\u00edaz,\nNan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu\nFeng, Vlad Fienber, Markus Freitag, Xavier Gar-\ncia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-\nAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua\nHowland, Andrea Hu, Jeffrey Hui, Jeremy Hur-\nwitz, Michael Isard, Abe Ittycheriah, Matthew Jagiel-\nski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun,\nSneha Kudugunta, Chang Lan, Katherine Lee, Ben-\njamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li,\nJian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu,\nFrederick Liu, Marcello Maggioni, Aroma Mahendru,\nJoshua Maynez, Vedant Misra, Maysam Moussalem,\nZachary Nado, John Nham, Eric Ni, Andrew Nys-\ntrom, Alicia Parrish, Marie Pellat, Martin Polacek,\nAlex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif,\nBryan Richter, Parker Riley, Alex Castro Ros, Au-\nrko Roy, Brennan Saeta, Rajkumar Samuel, Renee\nShelby, Ambrose Slone, Daniel Smilkov, David R.\nSo, Daniel Sohn, Simon Tokumine, Dasha Valter,\nVijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang,\nPidong Wang, Zirui Wang, Tao Wang, John Wiet-\ning, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting\nXue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven\nZheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav\nPetrov, and Yonghui Wu. 2023. Palm 2 technical\nreport.\nEdward Beeching, Sheon Han, Nathan Lambert,\nNazneen Rajani, Omar Sanseviero, Lewis Tun-\nstall, and Thomas Wolf. 2023.\nOpen llm leader-\nboard.\nhttps://huggingface.co/spaces/\nHuggingFaceH4/open_llm_leaderboard.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, Christopher Hesse, and John Schulman.\n2021. Training verifiers to solve math word prob-\nlems.\nAlexander R Fabbri, Wojciech Kry\u00b4sci\u00b4nski, Bryan Mc-\nCann, Caiming Xiong, Richard Socher, and Dragomir\nRadev. 2020. Summeval: Re-evaluating summariza-\ntion evaluation. arXiv preprint arXiv:2007.12626.\nAngela Fan, Yacine Jernite, Ethan Perez, David Grang-\nier, Jason Weston, and Michael Auli. 2019. ELI5:\nLong form question answering. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 3558\u20133567, Florence,\nItaly. Association for Computational Linguistics.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2021. Measuring massive multitask language under-\nstanding. Proceedings of the International Confer-\nence on Learning Representations (ICLR).\nJie Huang and Kevin Chen-Chuan Chang. 2023. To-\nwards reasoning in large language models: A survey.\nTom\u00e1\u0161 Ko\u02c7cisk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris\nDyer, Karl Moritz Hermann, G\u00e1bor Melis, and Ed-\nward Grefenstette. 2018. The NarrativeQA reading\ncomprehension challenge. Transactions of the Asso-\nciation for Computational Linguistics, 6:317\u2013328.\nWojciech Kryscinski, Bryan McCann, Caiming Xiong,\nand Richard Socher. 2020. Evaluating the factual\nconsistency of abstractive text summarization. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9332\u20139346, Online. Association for Computa-\ntional Linguistics.\nPhilippe Laban, Tobias Schnabel, Paul N. Bennett, and\nMarti A. Hearst. 2022. SummaC: Re-visiting NLI-\nbased models for inconsistency detection in summa-\nrization. Transactions of the Association for Compu-\ntational Linguistics, 10:163\u2013177.\nMd Tahmid Rahman Laskar, M Saiful Bari, Mizanur\nRahman, Md Amran Hossen Bhuiyan, Shafiq Joty,\nand Jimmy Xiangji Huang. 2023. A systematic study\nand comprehensive evaluation of chatgpt on bench-\nmark datasets.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022.\nTruthfulqa: Measuring how models mimic human\nfalsehoods.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023. G-eval:\nNlg evaluation using gpt-4 with better human align-\nment.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback.\nArtidoro Pagnoni, Vidhisha Balachandran, and Yulia\nTsvetkov. 2021. Understanding factuality in abstrac-\ntive summarization with FRANK: A benchmark for\nfactuality metrics. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 4812\u20134829, Online. As-\nsociation for Computational Linguistics.\nRichard Yuanzhe Pang, Alicia Parrish, Nitish Joshi,\nNikita Nangia, Jason Phang, Angelica Chen, Vishakh\nPadmakumar, Johnny Ma, Jana Thompson, He He,\nand Samuel Bowman. 2022. QuALITY: Question\nanswering with long input texts, yes! In Proceedings\nof the 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 5336\u20135358,\nSeattle, United States. Association for Computational\nLinguistics.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. CommonsenseQA: A ques-\ntion answering challenge targeting commonsense\nknowledge. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4149\u20134158, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023.\nStanford al-\npaca: An instruction-following llama model. https:\n//github.com/tatsu-lab/stanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nTianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Eval-\nuating text generation with bert. In International\nConference on Learning Representations.\nA\nAppendix\nA.1\nPrompts used for Question Generation\nFollowing the analysis from prior works (Ko\u02c7cisk\u00fd\net al., 2018; Pang et al., 2022), we formulate our\nquestion generation method as a two-step process:\n(1) Summarization and (2) Question generation\nfrom summary. In the first step, we design prompt\nfor generating summary as shown below:\nSummarize the paragraphs below in the\ncontext of {title} in {domain}.\nIn the next step, we ask ChatGPT to generate\nquestions from summary as shown below:\nUsing the context below, come up with\nfollow-up questions. Rank the generated\nquestions in the order of decreasing\ncomplexity to answer and display only\nthe top 3. {context}\nTo demonstrate the usefulness of our question\ngeneration process, we also establish a baseline\nwith the same instructions where questions are\ndirectly generated from the passage. The prompt\nused for the baseline is:\nUsing the context below, come up with\nthree questions. Rank the generated\nquestions in the order of decreasing\ncomplexity to answer and display only\nthe top 3. {context}\nA.2\nQuestion Generation using open source\nLLMs\nIn order to create a fair evaluation setup, we prompt\nAlpaca-7B,13B models to summarize and generate\nquestions on 50 instances. We do not consider ques-\ntion generation from non-instruction tuned models\n(e.g: Llama). From our evaluation method on gen-\nerated question as described in Section 4, we find\nquestions generated from Alpaca to be unanswer-\nable (non-existent in the context) and contain gib-\nberish content more than 80% of the time. The\nbelow table presents our evaluation of question\ngeneration from Alpaca:\nA.2.1\nEvaluation of Question Generation\nusing ChatGPT\nIn order to verify the complexity of generated ques-\ntions as outlined in Section 4, we prompt ChatGPT\nwith\nthe\nfollowing\nprompt:\nWe would like\nto request your feedback on determining\nthe complexity of generated questions\nEvaluation Metric\nQG - Passage\nQG - Summary\nQ1: Unambiguity\n12.5%\n8.3%\nQ2. Context Length:\nA sentence or less than a paragraph\n98.8%\n98.5%\nAt least a third or most of the passage\n1.2%\n1.5%\nQ3: Multi-pass of the passage\n0%\n0%\nTable 4: Prompts designed to evaluate the complexity of\ngenerated questions on Alpaca.\nby an AI assistant with respect to the\ncontext displayed above.\\n\\n For each\nof the question, rate the complexity\nof each of the generated questions for\nthe dimensions: ambiguity, context and\nreasoning capabilities. \\n\\n Q1: Is\nthe question answerable from the given\ncontext and is unambiguous? A. Yes B.\nNo \\n\\n Q2. How much of the passage\nis needed as context to answer the\nquestion? A. Only a sentence or two from\nthe passage B. More than 2 sentences\nbut lesser than a paragraph C. Atleast\na third of the entire context given\nD. Most of the context given \\n\\n Q3:\nDoes the question require multiple\npasses through the passage? A. Yes B. No.\nAssume you do not have prior knowledge\nabout the topic apart from the context\ngiven to you. Please output your choices\nin the form of a dictionary. (e.g: \u2019Q1\u2019:\n\u2019<your answer choice for Q1>\u2019, \u2019Q2\u2019:\n\u2019<your answer choice for Q2>\u2019, \u2019Q3\u2019:\n\u2019<your answer choice for Q3>\u2019, \u2019Q4\u2019:\n\u2019<your answer choice for Q4>\u2019). \\n\\n\nIn the subsequent line, please provide\na comprehensive explanation of your\nevaluation, avoiding any potential bias\nand ensuring that the order in which the\nresponses were presented does not affect\nyour judgment.\nSince LLMs are trained with different training\ndata mixtures, we specifically ask ChatGPT to\nanswer the questions based on the given context\nalone.\nA.2.2\nPrompt for Answer Generation\nIn order generate response on the questions\ngenerated by LLMs, we prompt the following: For\nevery generated question, we prompt the models\nas follows:\nGiven the context, answer the question\nbelow:\nContext: {context}\nQuestion: {question}\nAnswer: {Answer}\nA.3\nGPT-4 as an Evaluator\nA.3.1\nPrompts used in GPT-4 Evaluation\nIn order to evaluate answers generated by LLMs,\nwe ask GPT-4 to rate answers on Likert scale from\n0 to 3 (higher the better) on all of the following\nfour metrics: coherency, relevance, accuracy and\nfactual consistency. Our evaluation prompt used as\nprompt for GPT-4 is shown below:\nsystem prompt: You are a helpful\nand precise assistant for checking\nthe quality of the answer on 4\nverticals: coherence, relevance, factual\nconsistency, accuracy. prompt : We\nwould like to request your scores and\nfeedback on the performance of two\nAI assistants for answering the user\nquestion based on the context displayed\nabove. Please rate the answer quality on\n4 metrics: coherence, relevance, factual\nconsistency and accuracy. Definition of\neach metric is given to you. Coherence\n- Answer should be well-structured and\nwell-organized. Relevance - Answer\nshould be relevant to the question\nand the context. Answer should also\navoid drifting from the question being\nasked. Factual consistency - The context\nshould be the primary source for the\nanswer. The answer should not contain\nfabricated facts and should entail\ninformation present in the context.\nAccuracy - Does the response provided\nby the assistant answer the question\ncorrectly in a concise manner? Provide\na score to each AI assistant response\nfor each of the metric defined above\non a scale of 0 to 3 where higher score\nmeans better performance. Do not compare\nthe performance between AI assistants\nand rate them individually. Enter\ntwo new lines. Next, please provide\na comprehensive explanation of your\npreferred answer among the both and your\nevaluation, avoiding any potential bias\nand ensuring that the order in which\nMetric\nCohen-Kappa score\nCoherency\n0.66\nRelevance\n0.7\nAccuracy\n0.66\nFactual Consistency\n0.56\nTable 5: Annotator agreement scores with GPT-4\nthe responses were presented does not\naffect your judgment. Please note that\nyour scores should be in a dictionary\nformat as the example given to you.\nExample: \u2019Assistant 1\u2019: \u2019coherence\u2019:\n<int score>, \u2019relevance\u2019: <int score>,\n\u2019factual consistency\u2019: <int score>,\n\u2019accuracy\u2019: <int score>, \u2019Assistant\n2\u2019: ... \u2019Explanation\u2019: \u2019<Preference:\nAssistant 1 or Assistant 2 or Both or\nNeither>. <your explanation>\u2019.\nA.4\nCase Study: Human Eval v.s GPT-4\nWe annotate 50 QA pairs to evaluate answer quality\ngenerated by LLMs independently of GPT-4 rat-\nings. Two annotators are given similar guidelines\nas outlined in Section 4 and a questionnaire as GPT-\n4 (Section A.3.1). For fair evaluation, we do not\nreveal the model that generated the answer to anno-\ntators. Table 5 includes the agreement scores of the\nhuman evaluation with GPT-4. A high agreement\nscore across different metrics on free-form text\ngeneration shows that our evaluation method using\nGPT-4 is reliable. We also present some qualitative\nexamples in Appendix (Table 6)\nA.5\nData Collection\nWe use Wikipedia API to collect articles to\ncreate our evaluation setting. However, we find\nnot all articles may be useful to our setting;\nsuch as Wikipedia articles containing reference\nlinks, list of topics, glossary etc. We apply two\nlevels of filtering: (1) We remove any article\nthat has < 2k tokens (2) we curate a list of non-\ninformative sections and filter those sections from\nconsideration in this study. [\u2019References\u2019,\n\u2019See also\u2019, \u2019Notes\u2019, \u2019Sources\u2019,\n\u2019External links\u2019, \u2019History\u2019,\n\u2019Further reading\u2019]\nA.6\nQualitative Examples\nBelow table presents qualitative examples from hu-\nman annotation task and shows some of the ratings\nby humans and GPT-4.\nQualitative Examples\nQuestion 1: What is the difference between interdisciplinary study and specialization in a single discipline?\nEvaluation: Issue - None, Human score - 3, GPT-4 - 3\nChatGPT:\nInterdisciplinary study involves applying the methods and approaches of several different disciplines to a subject,\nwhile specialization in a single discipline involves in-depth study and expertise in a single subject.\nEvaluation: Issue - Repetition, Human score - 2, GPT-4 - 2\nAlpaca-13B:\nInterdisciplinary\nstudy\nis\nthe\ncombination\nof\ntwo\nor\nmore\ndisciplines\nto\ncreate\na\nnew\nfield\nof\nstudy.\nSpecialization\nis\nthe\nfocus\non\na\nsingle\ndiscipline,\nwith\nthe\ngoal\nof\nbecoming\nan\nexpert\nin\nthat\nfield.\nInterdisciplinary study is the combination of two or more disciplines to create a new field of study.\nSpecialization is the focus on a single discipline, with the goal of becoming an expert in that field.\nQuestion 2: How does good asset management require leadership, communication, collaboration, and continuous improvement?\nEvaluation: Issue - None, Human score - 3, GPT-4 - 3\nAlpaca-13B:\nGood asset management requires leadership, communication, collaboration, and continuous improvement.\nLeadership is required to set the strategic priorities and to ensure that the organization is aligned with the goals. Communication\nis required to ensure that the organization is aware of the goals and the strategies to achieve them. Collaboration is required to\nensure that the organization is working together to achieve the goals. Continuous improvement is required to ensure that the\norganization is constantly striving to improve its performance.\nEvaluation: Issue - Accuracy, Human score - 1, GPT-4 - 1\nAlpaca-7B:\nGood asset management requires leadership, communication, collaboration, and continuous improvement.\nQuestion 3: What are the four areas that CSAB considers crucial to the discipline of computer science?\nEvaluation: Issue - Factual Consistency, Human score - 1, GPT-4 - 3\nAlpaca-7B:\n1. Theory of computation 2. Computability theory 3. Programming language theory 4. Formal methods\nChatGPT:\nThe four areas that CSAB considers crucial to the discipline of computer science are theory of computation,\nalgorithms and data structures, programming methodology and languages, and computer elements and architecture.\nTable 6: Few examples of generated answers from LLMs with GPT-4 and human evaluation\nLegend LLM , Inconsistencies missed by the model\nInconsistencies detected by GPT-4.\n"
  },
  {
    "title": "Compositional Foundation Models for Hierarchical Planning",
    "link": "https://arxiv.org/pdf/2309.08587.pdf",
    "upvote": "9",
    "text": "Compositional Foundation Models for\nHierarchical Planning\nAnurag Ajay\u2217 \u2020\u00a7, Seungwook Han *\u2020\u2021\u00a7 Yilun Du *\u00a7, Shuang Li \u00a7,\nAbhi Gupta \u00a7, Tommi Jaakkola \u00a7, Josh Tenenbaum \u00a7, Leslie Kaelbling \u00a7,\nAkash Srivastava \u2021, Pulkit Agrawal \u2020\u00a7\nImprobable AI Lab\u2020\nMIT-IBM Watson AI Lab\u2021\nMassachusetts Institute Technology\u00a7\nhttps://hierarchical-planning-foundation-model.github.io/\nAbstract\nTo make effective decisions in novel environments with long-horizon goals, it is\ncrucial to engage in hierarchical reasoning across spatial and temporal scales. This\nentails planning abstract subgoal sequences, visually reasoning about the under-\nlying plans, and executing actions in accordance with the devised plan through\nvisual-motor control. We propose Compositional Foundation Models for Hierarchi-\ncal Planning (HiP), a foundation model which leverages multiple expert foundation\nmodel trained on language, vision and action data individually jointly together to\nsolve long-horizon tasks. We use a large language model to construct symbolic\nplans that are grounded in the environment through a large video diffusion model.\nGenerated video plans are then grounded to visual-motor control, through an in-\nverse dynamics model that infers actions from generated videos. To enable effective\nreasoning within this hierarchy, we enforce consistency between the models via\niterative refinement. We illustrate the efficacy and adaptability of our approach in\nthree different long-horizon table-top manipulation tasks.\n1\nIntroduction\nConsider the task of making a cup of tea in an unfamiliar house. To successfully execute this task, an\neffective approach is to reason hierarchically at multiple levels: an abstract level, e.g. the high level\nsteps needed to heat up the tea, a concrete geometric level e.g., how we should physically navigate\nto and in the kitchen, and at a control level e.g. how we should actuate our joints to lift a cup. It is\nfurther important that reasoning at each level is self-consistent with each other \u2013 an abstract plan to\nlook in cabinets for tea kettles must also be physically plausible at the geometric level and executable\ngiven the actuations we are capable of. In this paper, we explore how we can create agents capable of\nsolving novel long-horizon tasks which require hierarchical reasoning.\nLarge \u201cfoundation models\" have become a dominant paradigm in solving tasks in natural language\nprocessing [37, 49, 8], computer vision [27], and mathematical reasoning [28]. In line with this\nparadigm, a question of broad interest is to develop a \u201cfoundation model\u201d that can solve novel and\nlong-horizon decision-making tasks. Some prior works [40, 7] collected paired visual, language\nand action data and trained a monolithic neural network for solving long-horizon tasks. However,\ncollecting paired visual, language and action data is expensive and hard to scale up. Another line\nof prior works [11, 29] finetune large language models (LLM) on both visual and language inputs\n\u2217 denotes equal contribution. Authors are also affiliated with Computer Science and Artificial Laboratory\n(CSAIL). Correspondence to aajay@mit.edu, swhan@mit.edu and yilundu@mit.edu.\nPreprint. Under review.\narXiv:2309.08587v2  [cs.LG]  21 Sep 2023\nInternet \nText\nTask  \nModel\nEgocentric \nImages\nAction  \nModel\nInternet \nVideo\nPrepare Sandwich\nVisual  \nModel\nFigure 1: Compositional Foundation Models for Hierarchical Planning. HiP uses a task model, represented\nusing a LLM, to create an abstract plan, a visual model, represented using a video model, to generate an image\ntrajectory plan, and an ego-centric action model to infer actions from the image trajectory.\nusing task-specific robot demonstrations. This is problematic because, unlike the abundance of\ntext on the Internet, paired vision and language robotics demonstrations are not readily available\nand are expensive to collect. Furthermore, finetuning high-performing language models, such as\nGPT3.5/4 [38, 37] and PaLM [8], is currently impossible, as the model weights are not open-sourced.\nThe key characteristic of the foundation model is that solving a new task or adapting to a new\nenvironment is possible with much less data compared to training from scratch for that task or\ndomain. Instead of building a foundation model for long-term planning by collecting paired language-\nvision-action data, in this work we seek a scalable alternative \u2013 can we reduce the need for a costly\nand tedious process of collecting paired data across three modalities and yet be relatively efficient\nat solving new planning tasks? We propose Compositional Foundation Models for Hierarchical\nPlanning (HiP), a foundation model that is a composition of different expert models trained on\nlanguage, vision, and action data individually. Because these models are trained individually, the\ndata requirements for constructing the foundation models are substantially reduced (Figure 1). Given\nan abstract language instruction describing the desired task, HiP uses a large language model to\nfind a sequence of sub-tasks (i.e., planning). HiP then uses a large video diffusion model to capture\ngeometric and physical information about the world and generates a more detailed plan in form\nof an observation-only trajectory. Finally, HiP uses a large pre-trained inverse model that maps a\nsequence of ego-centric images into actions. The compositional design choice for decision-making\nallows separate models to reason at different levels of the hierarchy, and jointly make expert decisions\nwithout the need for ever collecting expensive paired decision-making data across modalities.\nGiven three models trained independently, they can produce inconsistent outputs that can lead to\noverall planning failure. For instance, a na\u00efve approach for composing models is to take the maximum-\nlikelihood output at each stage. However, a step of plan which is high likelihood under one model, i.e.\nlooking for a tea kettle in a cabinet may have zero likelihood under a seperate model, i.e. if there is no\ncabinet in the house. It is instead important to sample a plan that jointly maximizes likelihood across\nevery expert model. To create consistent plans across our disparate models, we propose an iterative\nrefinement mechanism to ensure consistency using feedback from the downstream models [29].\nAt each step of the language model\u2019s generative process, intermediate feedback from a likelihood\nestimator conditioned on an image of the current state is incorporated into the output distribution.\nSimilarly, at each step of the video model generation, intermediate feedback from the action model\nrefines video generation. This iterative refinement procedure promotes consensus among the different\nmodels and thereby enables hierarchically consistent plans that are both responsive to the goal\nand executable given the current state and agent. Our proposed iterative refinement approach is\ncomputationally efficient to train, as it does not require any large model finetuning. Furthermore, we\ndo not require access to the model\u2019s weights and our approach works with any models that offer only\ninput and output API access.\nIn summary, we propose a compositional foundation model for hierarchical planning that leverages a\ncomposition of foundation models, learned separately on different modalities of Internet and ego-\ncentric robotics data, to construct long-horizon plans. We demonstrate promising results on three\nlong-horizon tabletop manipulation environments.\n2\nPack computer mouse, \nblack and blue sneakers, \npepsi next box, toy train \nin brown box\nLLM\nPlace computer \nmouse in brown \nbox\nPlace dirty object \nin blue box\n\u2026\nDiffusion\nPlace computer \nmouse in brown \nbox\nInverse\nDynamics\n<latexit sha1_base64=\"5jC3rV+U5Rx6DyC9Q8QgcZ2oObw=\">AB6nicbVBNS8NAEJ3\nUr1q/qh69LBbBU0lE1GPRi8eK9gPaUDbTbt0swm7E6WE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKQw6LrfTmFldW19o7hZ2tre2d0r7x80TZxqxhslrFuB9RwKRvoEDJ24nmNAo\nkbwWjm6nfeuTaiFg94DjhfkQHSoSCUbTS/VNv1CtX3Ko7A1kmXk4qkKPeK391+zFLI6QSWpMx3MT9DOqUTDJ6VuanhC2YgOeMdSRSNu/Gx26oScWKVPwljbUkhm6u+JjEbGjKPAdkY\nUh2bRm4r/eZ0Uwys/EypJkSs2XxSmkmBMpn+TvtCcoRxbQpkW9lbChlRThjadkg3BW3x5mTPqt5F9fzuvFK7zuMowhEcwyl4cAk1uIU6NIDBAJ7hFd4c6bw4787HvLXg5DOH8AfO5w9\nlAI3i</latexit>wk\n<latexit sha1_base64=\"uSW0H71JUat1zrUvRTNUif2jrk=\">AB6Hicb\nVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeiF48t2FpoQ9lsJ+3azSbsboQS+gu8eFDEqz/Jm/GbZuDtj4YeLw3w8y8IBFcG9f9dgpr6xubW8Xt0s7u3v5B+fCor\neNUMWyxWMSqE1CNgktsGW4EdhKFNAoEPgTj25n/8IRK81jem0mCfkSHkoecUWOl5rBfrhVdw6ySrycVCBHo1/+6g1ilkYoDRNU67nJsbPqDKcCZyWeqnGhL\nIxHWLXUkj1H42P3RKzqwyIGsbElD5urviYxGWk+iwHZG1Iz0sjcT/O6qQmv/YzLJDUo2WJRmApiYjL7mgy4QmbExBLKFLe3EjaijJjsynZELzl1dJ+6\nLqXVZrzVqlfpPHUYQTOIVz8OAK6nAHDWgBA4RneIU359F5cd6dj0VrwclnjuEPnM8fz2OM9A=</latexit>g\n<latexit sha1_base64=\"Fynce+iWGXBf9W+c5z6gBd/qc8=\">AB7nicbVB\nNS8NAEJ34WetX1aOXxSIQkmkqMeiF48V7Ae0oWy2k3bpZhN2N0oJ/RFePCji1d/jzX/jts1BWx8MPN6bYWZekAiujet+Oyura+sbm4Wt4vbO7t5+6eCwqeNUMWywW\nMSqHVCNgktsG4EthOFNAoEtoLR7dRvPaLSPJYPZpygH9GB5CFn1Fip9dTLRufepFcquxV3BrJMvJyUIUe9V/rq9mOWRigNE1Trjucmxs+oMpwJnBS7qcaEshEdYM\ndSPUfjY7d0JOrdInYaxsSUNm6u+JjEZaj6PAdkbUDPWiNxX/8zqpCa/9jMskNSjZfFGYCmJiMv2d9LlCZsTYEsoUt7cSNqSKMmMTKtoQvMWXl0nzouJdVqr31XLt\nJo+jAMdwAmfgwRXU4A7q0AGI3iGV3hzEufFeXc+5q0rTj5zBH/gfP4AzWPXg=</latexit>wk+1\n<latexit sha1_base64=\"G4NrL4+FchmdNAyoq1HYt9oFXI=\">AB6HicbVDLTgJBEOzF+IL9ehlIjHxRHYNUY9ELx4hkUcCGzI79MLI7OxmZlZDCF/gxYPGePWTvPk3DrAHB\nSvpFLVne6uIBFcG9f9dnJr6xubW/ntws7u3v5B8fCoqeNUMWywWMSqHVCNgktsG4EthOFNAoEtoLR7cxvPaLSPJb3ZpygH9GB5CFn1Fip/tQrltyOwdZJV5GSpCh1it+dfsxSyOUhgmqdcdzE+NPqDKcCZwWuqnGhLIRHWDHUkj1P5kfuiUnFmlT8JY2ZKGzNXfExMaT2OAtsZUTPUy95M/M/rpCa89idcJqlByRaLwlQE5PZ16TPFTIjxpZQpri9lbAhVZQZm\n03BhuAtv7xKmhdl7JcqVdK1ZsjycwCmcgwdXUIU7qEDGCA8wyu8OQ/Oi/PufCxac042cwx/4Hz+AOejQ=</latexit>w\n<latexit sha1_base64=\"MbTS1h+M8tSUvbDJaAPrpe2jaBU=\">AB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8eK9gPaUDbTbt0swm7E7GE/gQvHhTx6i/y5r9x2+agrQ8GHu/NMDMvSKQ\nw6LrfTmFldW19o7hZ2tre2d0r7x80TZxqxhslrFuB9RwKRvoEDJ24nmNAokbwWjm6nfeuTaiFg94DjhfkQHSoSCUbTS/VMPe+WKW3VnIMvEy0kFctR75a9uP2ZpxBUySY3peG6CfkY1Cib5pNRNDU8oG9EB71iqaMSNn81OnZATq/RJGtbCslM/T2R0ciYcRTYzoji0Cx6U/E/r5NieOVnQiUpcsXmi8JUEozJ9G/SF5ozlGNLKNPC3krYkGrK0KZTsiF4iy8vk+Z1buont+dV2rXeRxFOIJjOAUPLqEGt\n1CHBjAYwDO8wpsjnRfn3fmYtxacfOYQ/sD5/AF0Ko3s</latexit>xt\n<latexit sha1_base64=\"MbTS1h+M8tSUvbDJaAPrpe2jaBU=\">AB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPRi8eK9gPaUDbTbt0swm7E7GE/gQvHhTx6i/y5r9x2+\nagrQ8GHu/NMDMvSKQw6LrfTmFldW19o7hZ2tre2d0r7x80TZxqxhslrFuB9RwKRvoEDJ24nmNAokbwWjm6nfeuTaiFg94DjhfkQHSoSCUbTS/VMPe+WKW3VnIMvEy0kFctR75a9uP2ZpxBUySY3peG6CfkY1Cib5pNRNDU8oG9EB71iqaMSNn81OnZATq/RJGtbCslM/T2R0ciYcRTYzoji0Cx6U/E/r5NieOVnQiUpcsXmi8JUEozJ9G/SF5ozlGNLKNPC3\nkrYkGrK0KZTsiF4iy8vk+Z1buont+dV2rXeRxFOIJjOAUPLqEGt1CHBjAYwDO8wpsjnRfn3fmYtxacfOYQ/sD5/AF0Ko3s</latexit>xt\nCommand\n<latexit sha1_base64=\"q6eoYNDShd6czrV7MLhjzWbJeI=\">AB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeiF48V7Qe0oWy2m3bpZhN2J0IJ/QlePCji1V/kzX/jt\ns1BWx8MPN6bYWZekEh0HW/ncLa+sbmVnG7tLO7t39QPjxqmTjVjDdZLGPdCajhUijeRIGSdxLNaRI3g7GtzO/cS1EbF6xEnC/YgOlQgFo2ilB9rHfrniVt05yCrxclKBHI1+as3iFkacYVMUmO6npugn1GNgk+LfVSwxPKxnTIu5YqGnHjZ/NTp+TMKgMSxtqWQjJXf09kNDJmEgW2M6I4MsveTPzP6YXvuZUEmKXLHFojCVBGMy+5sMhOYM5cQSy\nrSwtxI2opoytOmUbAje8surpHVR9S6rtftapX6Tx1GEziFc/DgCupwBw1oAoMhPMrvDnSeXHenY9Fa8HJZ47hD5zPH1EgjdU=</latexit>at\n<latexit sha1_base64=\"rEQHB989ndWLJvo8yNfjHwIhM0A=\">AB9HicbVDLSgNBEOz1GeMr6tHLYBAEIexKUI9BLx4jmAckS5idzCZDZh/O9AbDst/hxYMiXv0Yb/6Nk2QPm\nljQUFR1093lxVJotO1va2V1bX1js7BV3N7Z3dsvHRw2dZQoxhskpFqe1RzKULeQIGSt2PFaeBJ3vJGt1O/NeZKiyh8wEnM3YAOQuELRtFIbndIMX3qpXjuZFmvVLYr9gxkmTg5KUOeq/01e1HLAl4iExSrTuOHaObUoWCSZ4Vu4nmMWUjOuAdQ0MacO2ms6MzcmqUPvEjZSpEMlN/T6Q0HoSeKYzoDjUi95U/M/rJOhfu6kI4wR5yOaL/EQSjMg0AdIXijOUE0MoU8\nLcStiQKsrQ5FQ0ITiLy+T5kXFuaxU76vl2k0eRwGO4QTOwIErqMEd1KEBDB7hGV7hzRpbL9a79TFvXbHymSP4A+vzB+tZkjU=</latexit> \u02c6\nxt+1\nTask Planning\nVisual Planning\nAction Planning\nIterative \nRefinement\n\u2026\n<latexit sha1_base64=\"rEQHB989ndWLJvo8yNfjHwIhM0A=\">AB9HicbVDLSgNBEOz1GeMr6tHLYBAEIexKUI9BLx4jmAckS5idzCZDZh/O9AbDst/hxYMiXv0Yb/6N\nk2QPmljQUFR1093lxVJotO1va2V1bX1js7BV3N7Z3dsvHRw2dZQoxhskpFqe1RzKULeQIGSt2PFaeBJ3vJGt1O/NeZKiyh8wEnM3YAOQuELRtFIbndIMX3qpXjuZFmvVLYr9gxkmTg5KUOeq/01e1HLAl4iExSrTuOHaObUoWCSZ4Vu4nmMWUjOuAdQ0MacO2ms6MzcmqUPvEjZSpEMlN/T6Q0HoSeKYzoDjUi95U/M/rJOhfu6kI4wR5yOaL/EQSjMg0\nAdIXijOUE0MoU8LcStiQKsrQ5FQ0ITiLy+T5kXFuaxU76vl2k0eRwGO4QTOwIErqMEd1KEBDB7hGV7hzRpbL9a79TFvXbHymSP4A+vzB+tZkjU=</latexit> \u02c6\nxt+1\n<latexit sha1_base64=\"mFLypmOn5A468cbeCl+GUKhG/yc=\">AB9HicbVDLSgNBEOyNrxhfUY9eBoMgCGE3BPUY9OIxgnlAsoTZySQZMvtwpjcYlv0OLx4U8erH\nePNvnCR70MSChqKqm+4uL5JCo21/W7m19Y3Nrfx2YWd3b/+geHjU1GsG+wUIaq7VHNpQh4AwVK3o4Up74necsb38781oQrLcLgAacRd306DMRAMIpGcrsjislTL8GLSpr2iW7bM9BVomTkRJkqPeKX91+yGKfB8gk1brj2BG6CVUomORpoRtrHlE2pkPeMTSgPtduMj86JWdG6ZNBqEwFSObq74mE+lpPfc90+hRHetmbif95nRgH124igi\nhGHrDFokEsCYZklgDpC8UZyqkhlClhbiVsRBVlaHIqmBCc5ZdXSbNSdi7L1ftqXaTxZGHEziFc3DgCmpwB3VoAINHeIZXeLMm1ov1bn0sWnNWNnMf2B9/gDs35I2</latexit> \u02c6\nxt+2\n<latexit sha1_base64=\"oyI3KrfMalurzQJ0QwSNXQUYkWI=\">AB9HicbVDLSgNBEOyNrxhfUY9eBoMgCGFXg3oMevEYwTwgWcLsZDYZMvtwpjcYlnyHFw+KePVjvPk3Tp\nI9aGJBQ1HVTXeXF0uh0ba/rdzK6tr6Rn6zsLW9s7tX3D9o6ChRjNdZJCPV8qjmUoS8jgIlb8WK08CTvOkNb6d+c8SVFlH4gOYuwHth8IXjKR3M6AYvrUTfHsYjLpFkt2Z6BLBMnIyXIUOsWvzq9iCUBD5FJqnXbsWN0U6pQMknhU6ieUzZkPZ529CQBly76ezoCTkxSo/4kTIVIpmpvydSGmg9DjzTGVAc6EVvKv7ntRP0r91UhHGCPGTzRX4iCUZkmgDpCc\nUZyrEhlClhbiVsQBVlaHIqmBCcxZeXSeO87FyWK/eVUvUmiyMPR3AMp+DAFVThDmpQBwaP8Ayv8GaNrBfr3fqYt+asbOYQ/sD6/AHuZI3</latexit> \u02c6\nxt+3\nIterative \nRefinement\nEnvironment\nFigure 2: Planning with HiP. Given a language goal g and current observation xt, LLM generates next subgoal\nw with feedback from a visual plausibility model. Then, Diffusion uses observation xt and subgoal w to generate\nobservation trajectory \u03c4x with feedback from an action feasibility model. Finally, action planning uses inverse\ndynamics to generate action at from current xt and generated observation\n\u02c6\nxt+1 (action planning).\n2\nCompostional Foundation Models for Hierarchical Planning\nWe propose HiP, a foundation model that decomposes the problem of generating action trajectories\nfor long-horizon tasks specified by a language goal g into three levels of hierarchy: (1) Task planning\n\u2013 inferring a language subgoal wi conditioned on observation xi,1 and language goal g; (2) Visual\nplanning \u2013 generating a physically plausible plan as a sequence of image trajectories \u03c4 i\nx = {xi,1:T },\none for each given language subgoal wi and observation at first timestep xi,1; (3) Action planning \u2013\ninferring a sequence of action trajectories \u03c4 i\na = {ai,1:T \u22121} from the image trajectories \u03c4 i\nx executing\nthe plan. Figure 2 illustrates the model architecture and a pseudocode is provided in Algorithm 1.\nLet p\u0398 model this hierarchical decision-making process. Given our three levels of hierarchy, p\u0398 can\nbe factorized into the following: task distribution p\u03b8, visual distribution p\u03d5, and action distribution\np\u03c8. The distribution over plans, conditioned on the goal and an image of the initial state, can be\nwritten under the Markov assumption as:\np\u0398(W, {\u03c4 i\nx}, {\u03c4 i\na}|g, x1,1) =\n N\nY\ni=1\np\u03b8(wi|g)\n!\n|\n{z\n}\ntask planning\n N\nY\ni=1\np\u03d5(\u03c4 i\nx|wi, xi,1)\n!\n|\n{z\n}\nvisual planning\n N\nY\ni=1\nT \u22121\nY\nt=1\np\u03c8(ai,t|xi,t, xi,t+1)\n!\n|\n{z\n}\naction planning\n(1)\nWe seek to find action trajectories \u03c4 i\na, image trajectories \u03c4 i\nx and subgoals W = {wi} which maximize\nthe above likelihood. Please see Appendix A for a derivation of this factorization. In the following\nsub-sections, we describe the form of each of these components, how they are trained, and how they\nare used to infer a final plan for completing the long-horizon task.\n2.1\nTask Planning via Large Language Models\nGiven a task specified in language g and the current observation xi,1, we use a pretrained LLM as the\ntask planner, which decomposes the goal into a sequence of subgoals. The LLM aims to infer the next\nsubgoal wi given a goal g and models the distribution pLLM(wi|g). As the language model is trained\non a vast amount of data on the Internet, it captures powerful semantic priors on what steps should be\ntaken to accomplish a particular task. To adapt the LLM to obtain a subgoal sequence relevant to our\ntask, we prompt it with some examples of domain specific data consisting of high-level goals paired\nwith desirable subgoal sequences.\nHowever, directly sampling subgoals using a LLM can lead to samples that are inconsistent with the\noverall joint distribution in Eqn (1), as the subgoal wi not only affects the marginal likelihood of task\nplanning but also the downstream likelihoods of the visual planning model. Consider the example in\nFigure 2 where the agent is tasked with packing computer mouse, black and blue sneakers, pepsi box\nand toy train in brown box. Let\u2019s say the computer mouse is already in the brown box. While the\nsubgoal of placing computer mouse in brown box has high-likelihood under task model p\u03b8(wi|g), the\nresulting observation trajectory generated by visual model p\u03d5(\u03c4 i\nx|wi, xi,1) will have a low-likelihood\nunder p\u03d5 given the subgoal is already completed. Next, we describe how we use iterative refinement\nto capture this dependency between language decoding and visual planning to properly sample from\nEqn (1).\n3\nAlgorithm 1 Decision Making with HiP\n1: Models: Large language model pLLM, Subgoal classifier f\u03d5, Noise model of diffusion \u03f5\u03d5, Observation\ntrajectory classifier g\u03c8, Inverse dynamics p\u03c8\n2: Hyperparameters: Guidance scales \u03c9, \u03c9\u2032, Denoising diffusion steps K\n3: Input: Current observation xt, Language goal g\n4: # Task Planning\n5: for i = 1 . . . M do\n6:\nGenerate subgoal wi \u223c pLLM(wi|g)\n7: end for\n8: Collect candidate subgoals W \u2190 {wi}M\ni=1\n9: # Iterative Refinement from Visual Planning\n10: w \u2190 arg maxw f\u03d5(xt, W, g)\n11: # Visual Planning\n12: Initialize (\u03c4x)K \u223c N(0, I)\n13: for k = K . . . 1 do\n14:\n# Iterative Refinement from Action Planning\n15:\n\u02c6\u03f5 \u2190 \u03f5\u03d5((\u03c4x)k, xt, k) + \u03c9(\u03f5\u03d5((\u03c4x)k, xt, w, k) \u2212 \u03f5\u03d5((\u03c4x)k, xt, k)) \u2212 \u03c9\u2032\u2207(\u03c4x)k log g\u03c8(1|(\u03c4x)k)\n16:\n(\u03c4x)k\u22121 \u2190 Denoise((\u03c4x)k, \u02c6\u03f5)\n17: end for\n18: # Action Planning\n19: Extract (xt,\n\u02c6\nxt+1) from (\u03c4x)0\n20: Execute at \u2190 p\u03c8(at|xt,\n\u02c6\nxt+1)\nConsistency with Visual Planning. To ensure that we sample subgoal wi that maximizes the joint\ndistribution in Eqn (1), we should sample a subgoal that maximizes the following joint likelihood\nw\u2217\ni = arg max\nwi\npLLM(wi|g)p\u03d5(\u03c4 i\nx|wi, xi,1),\n(2)\ni.e. a likelihood that maximizes both conditional subgoal generation likelihood from a LLM and the\nlikelihood of sampled videos \u03c4 i\nx given the language instruction and current image xi,1. One way to\ndetermine the optimal subgoal w\u2217\ni is to generate multiple wi from LLM and score them using the\nlikelihood of videos sampled from our video model p\u03d5(\u03c4 i\nx|wi, xi,1). However, the video generation\nprocess is computationally expensive, so we take a different approach.\nThe likelihood of video generation p\u03d5(\u03c4 i\nx|wi, xi,1) primarily corresponds to the feasibility of a\nlanguage subgoal wi with respect to the initial image xi,1. Thus an approximation of Eqn (2) is to\ndirectly optimize the conditional density\nw\u2217\ni = arg max\nwi\np(wi|g, xi,1).\n(3)\nWe can rewrite Eqn (3) as\nw\u2217\ni = arg max\nwi\nlog pLLM(wi|g) + log\n\u0012p(xi,1|wi, g)\np(xi,1|g)\n\u0013\nWe estimate the density ratio p(xi,1|wi,g)\np(xi,1|g)\nwith a multi-class classifier f\u03d5(xi,1, {wj}M\ni=1, g) that\nchooses the appropriate subgoal w\u2217\ni from candidate subgoals {wj}M\nj=1 generated by the LLM. The\nclassifier implicitly estimates the relative log likelihood estimate of p(xi,1|wi, g) and use these logits\nto estimate the log density ratio with respect to each of the M subgoals and find w\u2217\ni that maximizes the\nestimate [47]. We use a dataset Dclassify := {xi,1, g, {wj}M\nj=1, i} consisting of observation xi,1, goal\ng, candidate subgoals {wj}M\nj=1 and the correct subgoal label i to train f\u03d5. For further architectural\ndetails, please refer to Appendix B.1.\n2.2\nVisual Planning with Video Generation\nUpon obtaining a language subgoal wi from task planning, our visual planner generates a plausible\nobservation trajectory \u03c4 i\nx conditioned on current observation xi,1 and subgoal wi. We use a video\ndiffusion model for visual planning given its success in generating text-conditioned videos [21, 55].\nTo provide our video diffusion model with a rich prior for physically plausible motions, we pretrain\n4\nit p\u03d5(\u03c4 i\nx|wi, xi,1) on a large-scale text-to-video dataset Ego4D [14]. We then finetune it on the\ntask-specific video dataset Dvideo := {\u03c4 i\nx, wi} consisting of observation trajectories \u03c4 i\nx satisfying\nsubgoal wi. For further architectural details, please refer to Appendix B.2.\nHowever, analogous to the consistent sampling problem in task planning, directly sampling observa-\ntion trajectories with video diffusion can lead to samples that are inconsistent with the overall joint\ndistribution in Eqn (1). The observation trajectory \u03c4 i\nx not only affects the marginal likelihood of\nvisual planning, but also the downstream likelihood of the action planning model.\nConsistency with Action Planning. To ensure observation trajectories \u03c4 i\nx that correctly maximize\nthe joint distribution in Eqn (1), we optimize an observation trajectory that maximizes the following\njoint likelihood\n(\u03c4 i\nx)\u2217 = arg max\n\u03c4 ix\np\u03d5(\u03c4 i\nx|wi, xi,1)\nT \u22121\nY\nt=1\np\u03c8(ai,t|xi,t, xi,t+1),\n(4)\ni.e. an image sequence that maximizes both conditional observation trajectory likelihood from video\ndiffusion and the likelihood of sampled actions \u03c4 i\na given the observation trajectory \u03c4 i\nx.\nTo sample such an observation trajectory, we could iteratively bias the denoising of video diffusion\nusing the log-likelihood of the sampled actions QT \u22121\nt=1 log p\u03c8(ai,t|xi,t, xi,t+1). While this solution is\nprincipled, it is slow as it requires sampling of entire action trajectories and calculating the corre-\nsponding likelihoods during every step of the denoising process. Thus, we approximate the sampling\nand the likelihood calculation of action trajectory QT \u22121\nt=1 p\u03c8(ai,t|xi,t, xi,t+1) with a binary classifier\ng\u03c8(\u03c4 i\nx) that models if the observation trajectory \u03c4 i\nx leads to a high-likelihood action trajectory.\nWe learn a binary classifier g\u03c8 to assign high likelihood to feasible trajectories sampled from our video\ndataset \u03c4 i\nx \u223c Dvideo and low likelihood to infeasible trajectories generated by randomly shuffling\nthe order of consecutive frames in feasible trajectories. Once trained, we can use the likelihood\nlog g\u03c8(1|\u03c4 i\nx) to bias the denoising of the video diffusion and maximize the likelihood of the ensuing\naction trajectory. For further details on binary classifier, please refer to Appendix C.2.\n2.3\nAction Planning with Inverse Dynamics\nAfter generating an observation trajectory \u03c4 i\nx from visual planning, our action planner generates\nan action trajectory \u03c4 i\na from the observation trajectory. We leverage egocentric internet images for\nproviding our action planner with useful visual priors. Our action planner is parameterized as an\ninverse dynamics model [1, 39] that infers the action ai,t given the observation pair (xi,t, xi,t+1):\nai,t \u223c p\u03c8(ai,t|xi,t, xi,t+1)\nTraining. To imbue the inverse dynamics p\u03c8 with useful visual priors, we initialize it with VC-\n1 [33] weights, pretrained on ego-centric images and ImageNet. We then finetune it on dataset\nDinv := {\u03c4 i\nx, \u03c4 i\na} consisting of paired observation and action trajectories by optimizing:\nmax\n\u03c8\nE\u03c4\u2208Dinv [log p\u03c8(ai,t|xi,t, xi,t+1)]\nFor further architectural details, please refer to Appendix B.3.\n3\nExperimental Evaluations\nWe evaluate the ability of HiP to solve long-horizon planning tasks that are drawn from distributions\nwith substantial variation, including the number and types of objects and their arrangements. We\nthen study the effects of iterative refinement and of pretraining on overall performance of HiP. We\nalso compare against an alternative strategy of visually grounding the LLM without any task-specific\ndata. In addition, we study how granularity of subgoals affects HiP\u2019s performance, ablate over\nchoice of visual planning model and analyze sensitivity of iterative refinement to hyperparameters\n(Appendix E).\n5\n\u201cStack green block on top of yellow block and place orange block to the right of the stack\u201d\nplace white block  \nin yellow bowl\nplace yellow block \nin brown box\nplace white block  \nin green bowl\nplace green block \non yellow block\nplace white block  \nin orange bowl\nplace orange block on \nright of yellow block\nPaint Block Task\nPaint Block Task\n\u201cPack hammer, frypan, toy school bus, and butterfinger chocolate into the brown box\u201d\nplace dirty object \nin the clean box\nplace hammer  \nin the brown box\nplace frypan \nin the brown box\nplace toy school bus \nin the brown box\nplace butterfinger  \nchocolate in brown box\nRearrange Object Task\nRearrange Object Task\n\u201cOpen microwave, move kettle out of the way, light the kitchen area, and open upper right drawer\u201d\nopen the  \nmicrowave\nmove the kettle \nto the back stove\nturn on the \nlights\nslide the upper right  \ndrawer \nKitchen Task\nKitchen Task\nFigure 3:\nExample Executions.\nExample long-horizon generated plans on tasks in paint-block,\nobject-arrange, and kitchen-tasks domains.\n3.1\nEvaluation Environments\nWe evaluate HiP on three environments, paint-block, object-arrange, and kitchen-tasks\nwhich are inspired by combinatorial planning tasks in Mao et al. [34], Shridhar et al. [45] and Xing\net al. [52] respectively.\n\u2022 paint-block: A robot has to manipulate blocks in the environment to satisfy language goal\ninstructions, such as stack pink block on yellow block and place green block right of them. However,\nobjects of correct colors may not be present in the environment, in which case, the robot needs to\nfirst pick up white blocks and put them in the appropriate color bowls to paint them. After that, it\nshould perform appropriate pick-and-place operations to stack a pink block on the yellow block\nand place the green block right of them. A new task T is generated by randomly selecting 3 final\ncolors (out of 10 possible colors) for the blocks and then sampling a relation (out of 3 possible\nrelations) for each pair of blocks. The precise locations of individual blocks, bowls, and boxes are\nfully randomized across different tasks. Tasks have 4 \u223c 6 subgoals.\n\u2022 object-arrange: A robot has to place appropriate objects in the brown box to satisfy language\ngoal instructions such as place shoe, tablet, alarm clock, and scissor in brown box. However, the\nenvironment may have distractor objects that the robot must ignore. Furthermore, some objects\ncan be dirty, indicated by a lack of texture and yellow color. For these objects, the robot must first\nplace them in a blue cleaning box and only afterwards place those objects in the brown box. A new\ntask T is generated by randomly selecting 7 objects (out of 55 possible objects), out of which 3 are\ndistractors, and then randomly making one non-distractor object dirty. The precise locations of\nindividual objects and boxes are fully randomized across different tasks. Tasks usually have 3 \u223c 5\nsubgoals.\n\u2022 kitchen-tasks: A robot has to complete kitchen subtasks to satisfy language goal instructions\nsuch as open microwave, move kettle out of the way, light the kitchen area, and open upper right\ndrawer. However, the environment may have objects irrelevant to the subtasks that the robot\n6\nmust ignore. Furthermore, some kitchen subtasks specified in the language goal might already be\ncompleted, and the robot should ignore those tasks. There are 7 possible kitchen subtasks: opening\nthe microwave, moving the kettle, switching on lights, turning on the bottom knob, turning on\nthe top knob, opening the left drawer, and opening the right drawer. A new task T is generated\nby randomly selecting 4 out of 7 possible kitchen subtasks, randomly selecting an instance of\nmicrowave out of 3 possible instances, randomly selecting an instance of kettle out of 4 possible\ninstances, randomly and independently selecting texture of counter, floor and drawer out of 3\npossible textures and randomizing initial pose of kettle and microwave. With 50% probability, one\nof 4 selected kitchen subtask is completed before the start of the task. Hence, tasks usually have\n3 \u223c 4 subtasks (i.e. subgoals).\nTrain and Test Tasks. For all environments, we sample two sets of tasks Ttrain, Ttest \u223c p(T ). We\nuse the train set of tasks Ttrain to create datasets Dclassify, Dvideo, Dinv and other datasets required\nfor training baselines. We ensure the test set of tasks Ttest contains novel combinations of object\ncolors in paint-block, novel combinations of object categories in object-arrange, and novel\ncombinations of kitchen subtasks in kitchen-tasks.\nEvaluation Metrics. We quantitatively evaluate a model by measuring its task completion rate for\npaint-block and object-arrange, and subtask completion rate for kitchen tasks. We use the\nsimulator to determine if the goal, corresponding to a task, has been achieved. We evaluate a model\non Ttrain (seen) to test its ability to solve long-horizon tasks and on Ttest (unseen) to test its ability to\ngeneralize to long-horizon tasks consisting of novel combinations of object colors in paint-block,\nobject categories in object-arrange, and kitchen subtasks in kitchen-tasks. We sample 1000\ntasks from Ttrain and Ttest respectively, and obtain average task completion rate on paint-block\nand object-arrange domains and average subtask completion rate on kitchen tasks domain.\nWe repeat this procedure over 4 different seeds and report the mean and the standard error over those\nseeds in Table 1.\n3.2\nBaselines\nThere are several existing strategies for constructing robot manipulation policies conditioned on\nlanguage goals, which we use as baselines in our experiments:\n\u2022 Goal-Conditioned Policy A goal-conditioned transformer model ai,t \u223c p(ai,t|xi,t, wi) that out-\nputs action ai,t given a language subgoal wi and current observation xi,t (Transformer BC) [7]. We\nprovide the model with oracle subgoals and encode these subgoals with a pretrained language en-\ncoder (Flan-T5-Base). We also compare against goal-conditioned policy with Gato [40] transformer\narchitecture.\n\u2022 Video Planner A video diffusion model (UniPi) [13] {\u03c4 i\nx}\n\u223c\np({\u03c4 i\nx}|g, xi,1), ai,t\n\u223c\np(ai,t|xi,t, xi,t+1) that bypasses task planning, generates video plans for the entire task {\u03c4 i\nx},\nand infers actions ai,t using an inverse model.\n\u2022 Action Planners Transformer models (Trajectory Transformer) [24] and diffusion models (Dif-\nfuser) [25, 4] {ai,t:T \u22121} \u223c p({ai,t:T \u22121}|xi,t, wi) that produce an action sequence {ai,t:T \u22121}\ngiven a language subgoal wi and current visual observation xi,t. We again provide the agents with\noracle subgoals and encode these subgoals with a pretrained language encoder (Flan-T5-Base).\n\u2022 LLM as Skill Manager A hierarchical system (SayCan) [2] with LLM as high level policy that\nsequences skills sampled from a repetoire of skills to accomplish a long-horizon task. We use\nCLIPort [45] policies as skills and the unnormalized logits over the pixel space it produces as\naffordances. These affordances grounds the LLM to current observation for producing next subgoal.\n3.3\nResults\nWe begin by comparing the performance of HiP and baselines to solve long-horizon tasks in\npaint-block, object-arrange, kitchen-tasks environments. Table 1 shows that HiP signifi-\ncantly outperforms the baselines, although the baselines have an advantage and have access to oracle\nsubgoals. HiP\u2019s superior performance shows the importance of (i) hierarchy given it outperforms goal-\nconditioned policy (Transformer BC and Gato), (ii) task planning since it outperforms video planners\n(UniPi), and (iii) visual planning given it outperforms action planners (Trajectory Transformer, Action\nDiffuser). It also shows the importance of representing skills with video-based planners which can\n7\nPaint-block\nObject-arrange\nKitchen-tasks\nModel\nSeen\nUnseen\nSeen\nUnseen\nSeen\nUnseen\nTransformer BC (oracle subgoals)\n8.3 \u00b1 1.9\n5.1 \u00b1 1.6\n10.2 \u00b1 2.9\n7.3 \u00b1 1.7\n48.4 \u00b1 21.6\n32.1 \u00b1 24.2\nGato (oracle subgoals)\n31.2 \u00b1 2.4\n28.6 \u00b1 2.9\n37.9 \u00b1 3.3\n36.5 \u00b1 3.2\n70.2 \u00b1 10.8\n66.8 \u00b1 12.2\nTrajectory Transformer (oracle subgoals)\n22.1 \u00b1 2.1\n22.3 \u00b1 2.5\n30.5 \u00b1 2.3\n29.8 \u00b1 2.9\n66.4 \u00b1 20.7\n52.1 \u00b1 22.3\nAction Diffuser (oracle subgoals)\n21.6 \u00b1 2.6\n18.2 \u00b1 2.3\n29.2 \u00b1 2.4\n27.6 \u00b1 2.1\n65.9 \u00b1 23.2\n55.1 \u00b1 22.8\nHiP (Ours, oracle subgoals)\n81.2 \u00b1 1.8\n79.6 \u00b1 1.9\n91.8 \u00b1 2.9\n92.3 \u00b1 2.3\n92.8 \u00b1 7.1\n89.8 \u00b1 7.6\nUniPi\n37.2 \u00b1 3.8\n35.3 \u00b1 3.2\n44.1 \u00b1 3.1\n44.2 \u00b1 2.9\n74.6 \u00b1 14.8\n73.4 \u00b1 11.2\nSayCan\n67.2 \u00b1 3.3\n62.8 \u00b1 3.7\n70.3 \u00b1 2.6\n66.9 \u00b1 2.8\n-\n-\nHiP (Ours)\n74.3 \u00b1 1.9\n72.8 \u00b1 1.7\n75 \u00b1 2.8\n75.4 \u00b1 2.6\n85.8 \u00b1 9.4\n83.5 \u00b1 10.2\nTable 1: Performance on Long-Horizon tasks. HiP not only outperforms the baselines in solving seen\nlong-horizon tasks but its performance remains intact when solving unseen long-horizon tasks containing novel\ncombination of objects colors in paint-block, novel combination of objects categories in object-rearrange\nand novel combination of subtasks in kitchen-tasks.\nSubgoal trajectories\nStack green block on  \ntop of yellow block  \nand place  \norange block to the  \nright of the stack\nplace white  \nblock  \nin yellow bowl\nplace yellow  \nblock \nin brown box\nplace white  \nblock  \nin green bowl\nplace green  \nblock \non yellow block\nplace white  \nblock  \nin orange bowl\nplace orange  \nblock on \nright of yellow  \nblock\nInitial observation\nGoal\nFigure 4: Execution trajectory of HiP on an novel long-horizon task in paint-block environment.\nbe pre-trained on Internet videos and can be applied to tasks (such as kitchen-tasks). SayCan, in\ncontrast, requires tasks to be decomposed into primitives paired with an affordance function, which\ncan be difficult to define for many tasks like the kitchen task. Thus, we couldn\u2019t run SayCan on\nkitchen-tasks environment. Finally, to quantitatively show how the errors in f\u03b8(xi,1, wi, g) affect\nthe performance of HiP, we compare it to HiP with oracle subgoals. For further details on the training\nand evaluation of HiP, please refer to Appendix C. For implementation details on Gato and SayCan,\nplease refer to Appendix D. For runtime analysis of different components of HiP, please refer to\nAppendix F.\nCombinatorial Generalization to Unseen Long-horizon Tasks. We also quantitatively test the\nability of HiP to generalize to unseen long-horizon tasks, consisting of novel combinations of object\ncolors in paint-block, object categories in object-arrange, and subtasks in kitchen-tasks.\nTable 1 shows that HiP\u2019s performance remains intact when solving unseen long-horizon tasks, and\nstill significantly outperforms the baselines. Figure 4 visualizes the execution of HiP in unseen\nlong-horizon tasks in paint-block.\nPre-training Video Diffusion Model. We investigate how much our video diffusion model benefits\nfrom pre-training on the Internet-scale data. We report both the success rate of HiP and Fr\u00e9chet\nVideo Distance (FVD) score that quantifies the similarity between generated videos and ground truth\nvideos, where lower scores indicate greater similarity in Figure 5. We see that pretraining video\n8\nPaint Block\nRearrange Object\n100% Train Data\n75% Train Data\n50% Train Data\n20\n40\n60\n80\nEgo4D Pre-training\nSuccess Rate\n100% Train Data\n75% Train Data\n50% Train Data\n100\n100% Train Data\n75% Train Data\n50% Train Data\nKitchen Tasks\n100% Train Data\n75% Train Data\n50% Train Data\nNo Pre-training\nPaint Block\nRearrange Object\n100% Train Data\n75% Train Data\n50% Train Data\n100\n200\n300\n400\nEgo4D Pre-training\nFVD Score\n100% Train Data\n75% Train Data\n50% Train Data\n100\n200\n300\n400\n500\n100% Train Data\n75% Train Data\n50% Train Data\nKitchen Tasks\n100% Train Data\n75% Train Data\n50% Train Data\nNo Pre-training\nFigure 5: Pretraining video diffusion model with the Ego4D dataset consistently yields higher success rate\nand lower FVD scores (lower is better), even with reduced training dataset sizes. With pretraining, the model\u2019s\nFVD score escalates less gradually and its success rate falls less steeply as the dataset size shrinks.\ndiffusion leads to a higher success rate and lower FVD score. If we reduce the training dataset to 75%\nand 50% of the original dataset, the FVD score for video diffusion models (both, with and without\nEgo4D dataset pretraining) increases and their success rate falls. However, the video diffusion model\nwith Ego4D dataset pretraining consistently gets higher success rate and lower FVD scores across\ndifferent dataset sizes. As we decrease the domain-specific training data, it is evident that the gap in\nperformance between the model with and without the Ego4D pre-training widens. For details on how\nwe process the Ego4D dataset, please refer to Appendix C.2.\n1000\n1500\n2000\n2500\nNumber of training demonstrations\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\nMean Squared Error\nPaint Block\nResNet (scratch)\nViT-B (scratch)\nViT-B (VC-1, 1k)\nViT-B (scratch, 10k)\n1000\n1500\n2000\n2500\nNumber of training demonstrations\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\nMean Squared Error\nObject Arrange\nResNet-18 (scratch)\nViT-B (scratch)\nViT-B (VC-1, 1k)\nViT-B (scratch, 10k)\n1000\n2000\n3000\n4000\n5000\n6000\nNumber of training demonstrations\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\nMean Squared Error\nKitchen Tasks\nResNet-18 (scratch)\nViT-B (scratch)\nViT-B (VC-1, 3.5k)\nViT-B (scratch, 10k)\nFigure 6: Pretraining inverse dynamics model. In paint-block and object-arrange (kitchen-tasks),\nwhen initialized with VC-1 weights, inverse dynamics model matches the performance of a randomly initialized\nmodel trained on 10K trajectories with just 1K (3.5k) trajectories. Even a smaller ResNet-18 model requires\n2.5K (6k) trajectories to approach the same performance. Note that the yellow and brown lines are overlaid on\ntop of each other.\nPre-training Inverse Dynamics Model. We also analyze the benefit of pre-training our inverse\ndynamics model and report the mean squared error between the predicted and ground truth actions\nin Figure 6. The pre-training comes in the form of initializing the inverse dynamics model with\nweights from VC-1 [33], a vision-transformer (ViT-B) [10] trained on ego-centric images with\nmasked-autoencoding objective [17]. In paint-block and object-arrange, we see that the\ninverse dynamics, when initialized with weights from VC-1, only requires 1K labeled robotic\ntrajectories to achieve the same performance as the inverse dynamics model trained on 10K labeled\nrobotic trajectories but without VC-1 initialization. We also compare against an inverse dynamics\nmodel parameterized with a smaller network (ResNet-18). However, the resulting inverse dynamics\nmodel still requires 2.5K robotic trajectories to get close to the performance of the inverse dynamics\nmodel with VC-1 initialization in paint-block and object-arrange. In kitchen-tasks, inverse\ndynamics, when initialized with weights from VC-1, only requires 3.5k labeled robotic trajectories\nto achieve the same performance as the inverse dynamics model trained on 10K labeled robotic\ntrajectories but without VC-1 initialization. When parameterized with ResNet-18, the inverse\ndynamics model still requires 6k robotic trajectories to get close to the performance of the inverse\ndynamics model with VC-1 initialization.\n9\nVisually Grounding LLM\nPaint Block\nRearrange Object\n20\n40\n60\n80\nLearned Classifier\nPrediction Accuracy\nFrozen VLM\nBenefits of Iterative Refinement\nHiP\nHiP without\nVisual Refinement\nHiP without\nany Refinement\n20\n80\nSuccess Rate\n40\n60\nFigure 7: Ablation Studies. (Left) While task plan refinement is critical to HiP\u2019s performance, visual plan\nrefinement improves HiP\u2019s performance by a smaller margin in paint block environment. (Right) While frozen\npretrained VLM (MiniGPT4) matches the performance of a learned classifier in paint-block environment, its\nperformance deteriorates in a more visually complex object-arrange environment.\nImportance of Task Plan and Visual Plan Refinements. We study the importance of refinement\nin task and visual planning in Figure 7. We compare to HiP without visual plan refinement and\nHiP without visual and task plan refinement in paint block environment. We see that task plan\nrefinement for visual grounding of LLM is critical to the performance of HiP. Without it, the task\nplan is agnostic to the robot\u2019s observation and predicts subgoals that lead to erroneous visual and\naction planning. Furthermore, visual plan refinement improves the performance of HiP as well, albeit\nby a small margin. For a detailed description of the hyperparameters used, please refer to Appendix\nC.4.\nExploring Alternate Strategies for Visually Grounding LLM. We use a learned classifier\nf\u03b8(xi,1, wi, g) to visually ground the LLM. We explore if we can use a frozen pretrained Vision-\nLanguage Model (MiniGPT-4 [60]) as a classifier in place of the learned classifier. Although we\ndidn\u2019t use any training data, we found the prompt engineering using the domain knowledge of\nthe task to be essential in using the Vision-Language Model (VLM) as a classifier (see Appendix\nC.1 for details). We use subgoal prediction accuracy to quantify the performance of the learned\nclassifier and the frozen VLM. Figure 7 illustrates that while both our learned multi-class classifier\nand frozen VLM perform comparably in the paint-block environment, the classifier significantly\noutperforms the VLM in the more visually complex object-arrange environment. We detail the\ntwo common failure modes of the VLM approach in object-arrange environment in Appendix\nC.1. As VLMs continue to improve, it is possible that their future versions match the performance of\nlearned classifiers and thus replace them in visually complex domains as well. For further details on\nthe VLM parameterization, please refer to Appendix C.1.\n4\nRelated Work\nThe field of foundation models for decision-making [54] has seen significant progress in recent years.\nA large body of work explored using large language models as zero-shot planners [22, 23, 30, 31, 2],\nbut it is often difficult to directly ground the language model on vision. To address this problem\nof visually grounding the language model, other works have proposed to directly fine-tune large\nlanguage models for embodied tasks [29, 11, 44]. However, such an approach requires large paired\nvision and language datasets that are difficult to acquire. Most similar to our work, SayCan [2]\nuses an LLM to hierarchically execute different tasks by breaking language goals into a sequence of\ninstructions, which are then inputted to skill-based value functions. While SayCan assumes this fixed\nset of skill-based value functions, our skills are represented as video-based planners [13], enabling\ngeneralization to new skills.\nAnother set of work has explored how to construct continuous space planners with diffusion mod-\nels [26, 4, 51, 57, 50, 59, 13, 6]. Existing works typically assume task-specific datasets from which\nthe continuous-space planner is derived [25, 3, 57]. Most similar to our work, UniPi [13] proposes\nto use videos to plan in image space and similarily relies on internet videos to train image space\nplanners. We build on top of UniPi to construct our foundation model for hierarchical planning, and\nillustrate how UniPi may be combined with LLMs to construct longer horizon continuous video\nplans.\nMoreover, some works [29, 56, 5] explored how different foundation models may be integrated with\neach other. In Flamingo [5], models are combined through joint finetuning with paired datasets, which\n10\nare difficult to collect. In contrast both Zeng et al. [56] and Li et al. [29] combine different models\nzero-shot using either language or iterative consensus. Our work proposes to combine language,\nvideo, and ego-centric action models together by taking the product of their learned distributions\n[12]. We use a similar iterative consensus procedure as in Li et al. [29] to sample from the entire joint\ndistribution and use this combined distribution to construct a hierarchical planning system.\n5\nLimitations and Conclusion\nLimitations. Our approach has several limitations. As high-quality foundation models for visual\nsequence prediction and robot action generation do not exist yet, our approach relies on smaller-scale\nmodels that we directly train. Once high-quality video foundation models are available, we can use\nthem to guide our smaller-scale video models [53] which would reduce the data requirements of our\nsmaller-scale video models. Furthermore, our method uses approximations to sample from the joint\ndistribution between all the model. An interesting avenue for future work is to explore more efficient\nand accurate methods to ensure consistent samples from the joint distribution.\nConclusion. In this paper, we have presented an approach to combine many different foundation\nmodels into a consistent hierarchical system for solving long-horizon robotics problems. Currently,\nlarge pretrained models are readily available in the language domain only. Ideally, one would train a\nfoundation model for videos and ego-centric actions, which we believe will be available in the near\nfuture. However, our paper focuses on leveraging separate foundation models trained on different\nmodalities of internet data, instead of training a single big foundation model for decision making.\nHence, for the purposes of this paper, given our computational resource limitations, we demonstrate\nour general strategy with smaller-scale video and ego-centric action models trained in simulation,\nwhich serve as proxies for larger pretrained models. We show the potential of this approach in\nsolving three long-horizon robot manipulation problem domains. Across environments with novel\ncompositions of states and goals, our method significantly outperforms the state-of-the-art approaches\ntowards solving these tasks.\nIn addition to building larger, more general-purposed visual sequence and robot control models, our\nwork suggests the possibility of further using other pretrained models in other modalities, such as\ntouch and sound, which may be jointly combined and used by our sampling approach. Overall, our\nwork paints a direction towards decision making by leveraging many different powerful pretrained\nmodels in combination with a tiny bit of training data. We believe that such a system will be\nsubstantially cheaper to train and will ultimately result in more capable and general-purpose decision\nmaking systems.\nAcknowledgements\nThe authors would like to thank the members of Improbable AI Lab for discussions and helpful\nfeedback. We thank MIT Supercloud and the Lincoln Laboratory Supercomputing Center for\nproviding compute resources. This research was supported by an NSF graduate fellowship, a DARPA\nMachine Common Sense grant, ARO MURI Grant Number W911NF-21-1-0328, ARO MURI\nW911NF2310277, ONR MURI Grant Number N00014-22-1-2740, and an MIT-IBM Watson AI Lab\ngrant. The views and conclusions contained in this document are those of the authors and should not\nbe interpreted as representing the official policies, either expressed or implied, of the United States\nArmy Research Office, the Office of Naval Research, or the U.S. Government. The U.S. Government\nis authorized to reproduce and distribute reprints for Government purposes, notwithstanding any\ncopyright notation herein.\nAuthor Contributions\nAnurag Ajay co-conceived the framework of leveraging pretrained foundation models for decision\nmaking, implemented visual planning and action planning in HiP, evaluated HiP on long-horizon\ntasks, performed ablation studies and helped in paper writing.\nSeungwook Han co-conceived in conceiving the framework of leveraging pretrained foundation\nmodels for decision making, implemented task planning in HiP and helped in paper writing.\n11\nYilun Du co-conceived the framework of leveraging pretrained foundation models for decision\nmaking, implemented trajectory transformer and transformer BC and evaluated them on long-horizon\ntasks, implemented data generation scripts for object arrange and paint block environment, and lead\npaper writing.\nShuang Li helped in conceiving the idea of iterative refinement for consistency between pretrained\nfoundation models, participated in research discussions and helped in writing paper.\nAbhi Gupta participated in research discussions and helped in making figures.\nTommi Jaakkola participated in research discussions.\nJoshua Tenenbaum participated in research discussions.\nLeslie Kaelbling participated in research discussions, suggested baselines and ablation studies,\nconceived the structure of the paper and helped in paper writing.\nAkash Srivastava participated in research discussions, suggested the idea of using classifier for\nconsistency between observations and the large language model and provided feedback on paper\nwriting.\nPulkit Agrawal was involved in research discussions, suggested ablation studies related to iterative\nrefinement, provided feedback on writing, positioning of the work and overall advising.\nReferences\n[1] P. Agrawal, A. V. Nair, P. Abbeel, J. Malik, and S. Levine. Learning to poke by poking:\nExperiential learning of intuitive physics. Advances in neural information processing systems,\n29, 2016.\n[2] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, K. Gopalakrishnan,\nK. Hausman, A. Herzog, et al. Do as i can, not as i say: Grounding language in robotic\naffordances. arXiv preprint arXiv:2204.01691, 2022.\n[3] A. Ajay, A. Kumar, P. Agrawal, S. Levine, and O. Nachum. Opal: Offline primitive discovery\nfor accelerating offline reinforcement learning. arXiv preprint arXiv:2010.13611, 2020.\n[4] A. Ajay, Y. Du, A. Gupta, J. Tenenbaum, T. Jaakkola, and P. Agrawal. Is conditional generative\nmodeling all you need for decision-making? arXiv preprint arXiv:2211.15657, 2022.\n[5] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican,\nM. Reynolds, et al. Flamingo: A visual language model for few-shot learning. NeurIPS, 2022.\nURL https://arxiv.org/abs/2204.14198.\n[6] A. Block, D. Pfrommer, and M. Simchowitz. Imitating complex trajectories: Bridging low-level\nstability and high-level behavior. arXiv preprint arXiv:2307.14619, 2023.\n[7] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Haus-\nman, A. Herzog, J. Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv\npreprint arXiv:2212.06817, 2022.\n[8] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W.\nChung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv\npreprint arXiv:2204.02311, 2022.\n[9] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li, X. Wang, M. De-\nhghani, S. Brahma, et al. Scaling instruction-finetuned language models. arXiv preprint\narXiv:2210.11416, 2022.\n[10] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,\nM. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for\nimage recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[11] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson,\nQ. Vuong, T. Yu, et al. Palm-e: An embodied multimodal language model. arXiv preprint\narXiv:2303.03378, 2023.\n12\n[12] Y. Du, S. Li, and I. Mordatch. Compositional visual generation with energy based models. In\nAdvances in Neural Information Processing Systems, 2020.\n[13] Y. Du, M. Yang, B. Dai, H. Dai, O. Nachum, J. B. Tenenbaum, D. Schuurmans, and P. Abbeel.\nLearning universal policies via text-guided video generation. arXiv e-prints, pages arXiv\u20132302,\n2023.\n[14] K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar, J. Hamburger, H. Jiang,\nM. Liu, X. Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n18995\u201319012, 2022.\n[15] M. U. Gutmann and A. Hyv\u00e4rinen. Noise-contrastive estimation of unnormalized statistical\nmodels, with applications to natural image statistics. J. Mach. Learn. Res., 13:307\u2013361, 2012.\n[16] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013\n778, 2016.\n[17] K. He, X. Chen, S. Xie, Y. Li, P. Doll\u00e1r, and R. Girshick. Masked autoencoders are scalable\nvision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 16000\u201316009, 2022.\n[18] J. Hermans, V. Begy, and G. Louppe. Likelihood-free mcmc with amortized approximate ratio\nestimators. In International conference on machine learning, pages 4239\u20134248. PMLR, 2020.\n[19] J. Ho and T. Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598,\n2022.\n[20] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. In Advances in Neural\nInformation Processing Systems, 2020.\n[21] J. Ho, W. Chan, C. Saharia, J. Whang, R. Gao, A. Gritsenko, D. P. Kingma, B. Poole, M. Norouzi,\nD. J. Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv\npreprint arXiv:2210.02303, 2022.\n[22] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch. Language models as zero-shot planners:\nExtracting actionable knowledge for embodied agents. In International Conference on Machine\nLearning, pages 9118\u20139147. PMLR, 2022.\n[23] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch,\nY. Chebotar, et al. Inner monologue: Embodied reasoning through planning with language\nmodels. arXiv preprint arXiv:2207.05608, 2022.\n[24] M. Janner, Q. Li, and S. Levine. Offline reinforcement learning as one big sequence modeling\nproblem. In Advances in Neural Information Processing Systems, 2021.\n[25] M. Janner, Y. Du, J. Tenenbaum, and S. Levine. Planning with diffusion for flexible behavior\nsynthesis. In International Conference on Machine Learning, 2022.\n[26] M. Janner, Y. Du, J. Tenenbaum, and S. Levine. Planning with diffusion for flexible behavior\nsynthesis. In International Conference on Machine Learning, 2022.\n[27] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C.\nBerg, W.-Y. Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023.\n[28] A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. Ramasesh, A. Slone,\nC. Anil, I. Schlag, T. Gutman-Solo, et al. Solving quantitative reasoning problems with language\nmodels. arXiv preprint arXiv:2206.14858, 2022.\n[29] S. Li, X. Puig, C. Paxton, Y. Du, C. Wang, L. Fan, T. Chen, D.-A. Huang, E. Aky\u00fcrek,\nA. Anandkumar, et al. Pre-trained language models for interactive decision-making. Advances\nin Neural Information Processing Systems, 35:31199\u201331212, 2022.\n13\n[30] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng. Code as\npolicies: Language model programs for embodied control. arXiv preprint arXiv:2209.07753,\n2022.\n[31] K. Lin, C. Agia, T. Migimatsu, M. Pavone, and J. Bohg. Text2motion: From natural language\ninstructions to feasible plans. arXiv preprint arXiv:2303.12153, 2023.\n[32] I. Loshchilov and F. Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017.\n[33] A. Majumdar, K. Yadav, S. Arnaud, Y. J. Ma, C. Chen, S. Silwal, A. Jain, V.-P. Berges,\nP. Abbeel, J. Malik, et al. Where are we in the search for an artificial visual cortex for embodied\nintelligence? arXiv preprint arXiv:2303.18240, 2023.\n[34] J. Mao, T. Lozano-Perez, J. B. Tenenbaum, and L. P. Kaelbing. PDSketch: Integrated Domain\nProgramming, Learning, and Planning. In Advances in Neural Information Processing Systems\n(NeurIPS), 2022.\n[35] S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta. R3m: A universal visual representation\nfor robot manipulation. arXiv preprint arXiv:2203.12601, 2022.\n[36] A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I. Sutskever, and\nM. Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion\nmodels. arXiv preprint arXiv:2112.10741, 2021.\n[37] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.\n[38] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,\nK. Slama, A. Ray, et al. Training language models to follow instructions with human feedback.\nAdvances in Neural Information Processing Systems, 35:27730\u201327744, 2022.\n[39] D. Pathak, P. Mahmoudieh, G. Luo, P. Agrawal, D. Chen, Y. Shentu, E. Shelhamer, J. Malik,\nA. A. Efros, and T. Darrell. Zero-shot visual imitation. In Proceedings of the IEEE conference\non computer vision and pattern recognition workshops, pages 2050\u20132053, 2018.\n[40] S. Reed, K. Zolna, E. Parisotto, S. G. Colmenarejo, A. Novikov, G. Barth-Maron, M. Gimenez,\nY. Sulsky, J. Kay, J. T. Springenberg, et al. A generalist agent. arXiv preprint arXiv:2205.06175,\n2022.\n[41] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis\nwith latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 10684\u201310695, 2022.\n[42] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. Denton, S. K. S. Ghasemipour, B. K. Ayan,\nS. S. Mahdavi, R. G. Lopes, et al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding. arXiv preprint arXiv:2205.11487, 2022.\n[43] T. Salimans and J. Ho. Progressive distillation for fast sampling of diffusion models. arXiv\npreprint arXiv:2202.00512, 2022.\n[44] P. Sharma, A. Torralba, and J. Andreas. Skill induction and planning with latent language. arXiv\npreprint arXiv:2110.01517, 2021.\n[45] M. Shridhar, L. Manuelli, and D. Fox. Cliport: What and where pathways for robotic manipula-\ntion. In Conference on Robot Learning, pages 894\u2013906. PMLR, 2022.\n[46] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning\nusing nonequilibrium thermodynamics. In International Conference on Machine Learning,\n2015.\n[47] A. Srivastava, S. Han, K. Xu, B. Rhodes, and M. U. Gutmann. Estimating the density ratio\nbetween distributions with high discrepancy using multinomial logistic regression. ArXiv,\nabs/2305.00869, 2023.\n14\n[48] M. Sugiyama, T. Suzuki, and T. Kanamori. Density ratio estimation in machine learning.\nCambridge University Press, 2012.\n[49] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal,\nE. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv\npreprint arXiv:2302.13971, 2023.\n[50] J. Urain, N. Funk, G. Chalvatzaki, and J. Peters. Se (3)-diffusionfields: Learning cost functions\nfor joint grasp and motion optimization through diffusion. arXiv preprint arXiv:2209.03855,\n2022.\n[51] Z. Wang, J. J. Hunt, and M. Zhou. Diffusion policies as an expressive policy class for offline\nreinforcement learning. arXiv preprint arXiv:2208.06193, 2022.\n[52] E. Xing, A. Gupta, S. Powers, and V. Dean. Kitchenshift: Evaluating zero-shot general-\nization of imitation-based policy learning under domain shifts.\nIn NeurIPS 2021 Work-\nshop on Distribution Shifts: Connecting Methods and Applications, 2021.\nURL https:\n//openreview.net/forum?id=DdglKo8hBq0.\n[53] M. Yang, Y. Du, B. Dai, D. Schuurmans, J. B. Tenenbaum, and P. Abbeel. Probabilistic\nadaptation of text-to-video models. arXiv preprint arXiv:2306.01872, 2023.\n[54] S. Yang, O. Nachum, Y. Du, J. Wei, P. Abbeel, and D. Schuurmans. Foundation models for\ndecision making: Problems, methods, and opportunities. arXiv preprint arXiv:2303.04129,\n2023.\n[55] S. Yu, K. Sohn, S. Kim, and J. Shin. Video probabilistic diffusion models in projected latent\nspace. arXiv preprint arXiv:2302.07685, 2023.\n[56] A. Zeng, A. Wong, S. Welker, K. Choromanski, F. Tombari, A. Purohit, M. Ryoo, V. Sindhwani,\nJ. Lee, V. Vanhoucke, et al. Socratic models: Composing zero-shot multimodal reasoning with\nlanguage. arXiv preprint arXiv:2204.00598, 2022.\n[57] E. Zhang, Y. Lu, W. Wang, and A. Zhang. Lad: Language augmented diffusion for reinforcement\nlearning. arXiv preprint arXiv:2210.15629, 2022.\n[58] H. Zheng, W. Nie, A. Vahdat, K. Azizzadenesheli, and A. Anandkumar. Fast sampling of\ndiffusion models via operator learning. In International Conference on Machine Learning,\npages 42390\u201342402. PMLR, 2023.\n[59] Z. Zhong, D. Rempe, D. Xu, Y. Chen, S. Veer, T. Che, B. Ray, and M. Pavone. Guided\nconditional diffusion for controllable traffic simulation. arXiv preprint arXiv:2210.17366, 2022.\n[60] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny. Minigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.\n15\nAppendix\nIn this Appendix, we discuss how we factorize the hierarchical decision-making process in Section A.\nIn Section B, we then detail the background and architecture for visually grounded task planning,\nvisual planning with video diffusion, and action planning with inverse dynamics model. In Section C,\nwe discuss the training and evaluation details for the different levels of planning in HiP and the\ncorresponding training hyperparameters. In Section D, we discuss implementation details for Gato\nand SayCan. In Section E, we showcase additional ablation studies comparing different approaches\nto enforce consistency across the levels of hierarchy, analyzing effect of granularity of subgoals\non performance of HiP, ablating on the choice of video planning model and analyzing sensitivity\nof iterative refinement to hyperparameters. Finally, in Section F, we analyze runtime of different\ncomponents of HiP.\nA\nFactorizing Hierarchical Decision-Making Process\nWe model the hierarchical decision-making process described in Section 2 with p\u0398 which can be\nfactorized into the task distribution p\u03b8, visual distribution p\u03d5, and action distribution p\u03c8.\np\u0398(W, {\u03c4 i\nx}, {\u03c4 i\na}|g, x1,1) =\nN\nY\ni=1\np\u03b8(wi|g, xi,1, w<i, \u03c4 <i\nx , \u03c4 <i\na )\nN\nY\ni=1\np\u03d5(\u03c4 i\nx|w\u2264i, xi,1, g, \u03c4 <i\nx , \u03c4 <i\na )\nN\nY\ni=1\np\u03c8(\u03c4 i\na|\u03c4 \u2264i\nx , w\u2264i, xi,1, g, \u03c4 <i\na )\nHere, given random variables Y i, Y <i and Y \u2264i represents {Y 1, . . . , Y i\u22121} and {Y 1, . . . , Y i}\nrespectively. Now, we apply Markov assumption: Given current observation xi,1, future variables\n(wi, \u03c4 i\nx, \u03c4 i\na) and past variables (wj, \u03c4 j\nx, \u03c4 j\na \u2200j < i) are conditionally independent.\np\u0398(W, {\u03c4 i\nx}, {\u03c4 i\na}|g, x1,1) =\nN\nY\ni=1\np\u03b8(wi|g, xi,1)\nN\nY\ni=1\np\u03d5(\u03c4 i\nx|wi, xi,1, g)\nN\nY\ni=1\np\u03c8(\u03c4 i\na|\u03c4 i\nx, wi, xi,1, g)\nWe model task distribution p\u03b8 with a large language model (LLM) which is independent of observation\nxi,1. Since the image trajectory \u03c4 i\nx = {xi,1:T } describes a physically plausible plan for achieving\nsubgoal wi from observation xi,1, it is conditionally independent of goal g given subgoal wi and\nobservation xi,1. Furthermore, we assume that an action ai,t can be recovered from observation at\nthe same timestep xi,t and the next timestep xi,t+1. Thus, we can write the factorization as\np\u0398(W, {\u03c4 i\nx}, {\u03c4 i\na}|g, x1,1) =\n N\nY\ni=1\np\u03b8(wi|g)\n!  N\nY\ni=1\np\u03d5(\u03c4 i\nx|wi, xi,1)\n!  N\nY\ni=1\nT \u22121\nY\nt=1\np\u03c8(ai,t|xi,t, xi,t+1)\n!\nB\nBackground and Architecture\nB.1\nTask Planning\nBackground on Density Ratio Estimation. Let p and q be two densities, such that q is absolutely\ncontinuous with respect to p, denoted as q << p i.e. q(x) > 0 wherever p(x) > 0. Then, their ratio\nis defined as r(x) = p(x)/q(x) over the support of p. We can estimate this density ratio r(x) by\ntraining a binary classifier to distinguish between samples from p and q [48, 15, 18]. More recent\nwork [47] has shown one can introduce auxiliary densities {mi}M\ni=1 and train a multi-class classifier\nto distinguish samples between M classes to learn a better-calibrated and more accurate density ratio\nestimator. Once trained, the log density ratio can be estimated by log r(x) = \u02c6hp(x) \u2212 \u02c6hq(x), where\n\u02c6hi(x) is the unnormalized log probability of the input sample under the ith density, parameterized by\nthe model.\nLearning a Classifier to Visually Ground Task Planning. We estimate the density ratio p(xi,1|wi,g)\np(xi,1|g)\nwith a multi-class classifier f\u03d5(xi,1, {wj}, g) trained to distinguish samples amongst the conditional\ndistributions p(xi,1|wi, g), ..., p(xi,1|wM, g) and the marginal distribution p(xi,1|g). Upon conver-\ngence, the classifier learns to assign high scores to (xi,1, wi, g) if wi is the subgoal corresponding to\nthe observation xi,1 and task g and low scores otherwise.\n16\nArchitecture. We parameterize f\u03d5 as a 4-layer multi-layer perceptron (MLP) on top of an ImageNet-\npretrained vision encoder (ResNet-18 [16]) and a frozen pretrained language encoder (Flan-T5-\nBase [9]). The vision encoder encodes the observation xi,1, and the text encoder encodes the subgoals\nwj and the goal g. The encoded observation, the encoded subgoals, and the encoded goal are\nconcatenated, and passed through a MLP with 3 hidden layers of sizes 512, 256, and 128. The\noutput dimension for MLP (i.e., number of classes for multi-classification) M is 6 for paint-block\nenvironment, 5 for object-arrange environment and 4 for kitchen-tasks environment.\nChoice of Large Language Model. We use GPT3.5-turbo [38] as our large language model.\nB.2\nVisual Planning\nBackground. Diffusion Probabilistic Models [46, 20] learn the data distribution h(x) from a\ndataset D := {xi}. The data-generating procedure involves a predefined forward noising process\nq(xk+1|xk) and a trainable reverse process p\u03d5(xk\u22121|xk), both parameterized as conditional Gaussian\ndistributions. Here, x0 := x is a sample, x1, x2, ..., xK\u22121 are the latents, and xK \u223c N(0, I) for a\nsufficiently large K. Starting with Gaussian noise, samples are then iteratively generated through a\nseries of \u201cdenoising\u201d steps. Although a tractable variational lower-bound on log p\u03d5 can be optimized\nto train diffusion models, Ho et al. [20] propose a simplified surrogate loss:\nLdenoise(\u03b8) := Ek\u223c[1,K],x0\u223ch,\u03f5\u223cN (0,I)[||\u03f5 \u2212 \u03f5\u03d5(xk, k)||2]\nThe predicted noise \u03f5\u03b8(xk, k), parameterized with a deep neural network, estimates the noise \u03f5 \u223c\nN(0, I) added to the dataset sample x0 to produce noisy xk.\nGuiding Diffusion Models with Text. Diffusion models are most notable for synthesizing high-\nquality images [42, 36] and videos [21, 55] from text descriptions. Modeling the conditional data\ndistribution q(x|y) makes it possible to generate samples satisfying the text description y. To enable\nconditional data generation with diffusion, Ho and Salimans [19] modified the original training setup\nto learn both a conditional \u03f5\u03d5(xk, y, k) and an unconditional \u03f5\u03d5(xk, k) model for the noise. The\nunconditional noise is represented, in practice, as the conditional noise \u03f5\u03d5(xk, \u2205, k), where a dummy\nvalue \u2205 takes the place of y. The perturbed noise \u03f5\u03d5(xk, \u2205, k) + \u03c9\n\u0000\u03f5\u03d5(xk, y, k) \u2212 \u03f5\u03d5(xk, \u2205, k)\n\u0001\n(i.e.\nclassifier-free guidance) is used to later generate samples.\nVideo Diffusion in Latent Space. As diffusion models generally perform denoising in the input\nspace [20], the optimization and inference become computationally demanding when dealing with\nhigh-dimensional data, such as videos. Inspired by recent works [41, 55], we first use an autoencoder\nvenc to learn a latent space for our video data. It projects an observation trajectory \u03c4x (i.e., video) into\na 2D tri-plane representation [55] \u03c4z = [\u03c4 T\nz , \u03c4 H\nz , \u03c4 W\nz ] where \u03c4 T\nz , \u03c4 H\nz , \u03c4 W\nz\ncapture variations in the\nvideo across time, height, and width respectively. We then diffuse over this learned latent space [55].\nLatent Space Video Diffusion for Visual Planning. Our video diffusion model p\u03d5(\u03c4 i\nx|wi, xi,1)\ngenerates video \u03c4 i\nx given a language subgoal wi and the current observation xi,1. It is param-\neterized through its noise model \u03f5\u03d5((\u03c4 i\nz)k, wi, xi,1, k) := \u03f5\u03d5((\u03c4 i\nz)k, lenc(wi), venc(xi,1), k) where\n\u03c4 i\nz := venc(\u03c4 i\nx) is the latent representation of video \u03c4 i\nx over which we diffuse. We condition the noise\nmodel \u03f5\u03d5 on subgoal wi using a pretrained language encoder lenc and on current observation xi,1\nusing video encoder venc. To use venc with a single observation xi,1, we first tile the observation along\nthe temporal dimension to create a video.\nArchitecture. We now detail the architectures of different components:\n\u2022 Video Autoencoder We borrow our architecture for venc from PVDM [55] which uses transformers\nto project video \u03c4x \u2208 RT \u00d7H\u00d7W to latent codes \u03c4z = [\u03c4 T\nz , \u03c4 H\nz , \u03c4 W\nz ] where \u03c4 T\nz \u2208 RC\u00d7H\u2032\u00d7W \u2032,\n\u03c4 H\nz \u2208 RC\u00d7T \u00d7W \u2032, \u03c4 W\nz\n\u2208 RC\u00d7H\u2032\u00d7T . Here, T = 50 represents the time horizon of a video, H = 48\nrepresents video height, W = 64 represents video width, C = 4 represents latent codebook\ndimension, H\u2032 = 12 represents latent height, and W \u2032 = 8 represents latent width.\n\u2022 Language Encoder We use Flan-T5-Base [9] as the pretrained frozen language encoder lenc.\n\u2022 Noise Model We borrow PVDM-L architecture [55] which uses 2D UNet architecture, similar to\nthe one in Latent Diffusion Model (LDM) [41], to represent p(\u03c4z|\u03c4 \u2032\nz). In our case, \u03c4z = venc(\u03c4 i\nx)\nand \u03c4 \u2032\nz = venc(xi,1). To further condition noise model \u03f5\u03d5 on lenc(wi), we augment the 2D UNet\nModel with cross-attention mechanism borrowed by LDM [41].\n17\nFor implementing these architectures, we used the codebase https://github.com/sihyun-yu/PVDM\nwhich contains the code for PVDM and LDM.\nClassifier for Consistency between Visual Planning and Action Planning. To ensure consistency\nbetween visual planning and action planning, we want to sample observation trajectories that maxi-\nmizes both conditional observation trajectory likelihood from diffusion and the likelihood of sampled\nactions given the observation trajectory (see equation 4). To approximate likelihood calculation of\naction trajectory, we learn a binary classifier g\u03c8 that models if the observation trajectory leads to\na high likelihood action trajectories. Since diffusion happens in latent space and we use gradients\nfrom g\u03c8 to bias the denoising of the video diffusion, g\u03c8(\u03c4 i\nz) takes the observation trajectory in\nlatent space. The binary classifier g\u03c8 is trained to distinguish between observation trajectories in\nlatent space sampled from video dataset \u03c4 i\nz = venc(\u03c4 i\nx), \u03c4 i\nx \u223c Dvideo (i.e. label of 1) and observa-\ntion trajectories in latent space sampled from video dataset whose frames where randomly shuffled\n(\u03c4 i\nz)\u2032 = venc(\u03c3(\u03c4 i\nx)), \u03c4 i\nx \u223c Dvideo (i.e. label of 0). Here, \u03c3 denotes the random shuffling of frames.\nTo randomly shuffle frames in an observation trajectory (of length 50), we first randomly select 5\nframes in the observation trajectory. For each of the selected frame, we randomly permute it with its\nneighboring frame (i.e. either with the frame before it or with the frame after it). Once g\u03c8 is trained,\nwe use it to bias the denoising of the video diffusion\n\u02c6\u03f5 := \u03f5\u03d5((\u03c4z)k, venc(xt), k) + \u03c9(\u03f5\u03d5((\u03c4z)k, venc(xt), lenc(w), k) \u2212 \u03f5\u03d5((\u03c4z)k, venc(xt), k))\n\u2212 \u03c9\u2032\u2207(\u03c4z)k log g\u03c8(1|(\u03c4z)k)\nHere, \u02c6\u03f5 is the noise used in denoising of the video diffusion and \u03c9, \u03c9\u2032 are guidance hyperparameters.\nClassifier Architecture. The classifier g\u03c8(\u03c4z = [\u03c4 T\nz , \u03c4 H\nz , \u03c4 W\nz ]) has a ResNet-9 encoder that converts\n\u03c4 T\nz , \u03c4 H\nz , and \u03c4 W\nz\nto latent vectors, then concatenate those latent vectors and passes the concatenated\nvector through an MLP with 2 hidden layers of sizes 256 and 128 and an output layer of size 1.\nB.3\nAction Planning\nTo do action planning, we learn an inverse dynamics model to p\u03c8(ai,t|xi,t, xi,t+1) predicts 7-\ndimensional robot states si,t = p\u03c8(xi,t) and si,t+1 = p\u03c8(xi,t+1). The first 6 dimensions of the robot\nstate represent joint angles and the last dimension of the robot state represents the gripper state (i.e.,\nwhether it\u2019s open or closed). The first 6 action dimension is represented as joint angle difference\nai,t[: 6] = si,t+1[: 6] \u2212 si,t[: 6] while the last action dimension is gripper state of next timestep\nai,t[\u22121] = si,t+1[\u22121].\nArchitecture. We use ViT-B [10] (VC-1 [33] initialization) along with a linear layer to parameterize\np\u03c8. ViT-B projects the observation xi,t \u2208 R48\u00d764\u00d73 to 768 dimensional latent vector from which the\nlinear layer predicts the 7 dimensional state si,t.\nC\nTraining and Evaluation\nC.1\nTask Planning\nInability to recognize uncommon objects\nHallucination leading to wrong spatial reasoning\nIs screwdriver in brown box?\nYes, screwdriver is in brown box\nIs computer hard drive in \nbrown box?\nNo, the computer hard drive is \nnot in the brown box\nIs scissor in brown box?\nYes, scissor is in brown box\nUser\nVision Language Model\nFigure 8: Failure in VLM. to recognize uncommon objects like computer hard drives and occasional hallucina-\ntion of object presence, leading to incorrect visual reasoning.\nTraining Objective and Dataset for Learned Classifier. We use a softmax cross-entropy loss\nto train the multi-class classifier f\u03d5(xi,1, {wj}M\nj=1, g) to classify an observation xi,1 into one of\nthe M given subgoal. We train it using the classification dataset Dclassify := {xi,1, g, {wj}M\nj=1, i}\n18\nconsisting of observation xi,1, goal g, candidate subgoals {wj}M\nj=1 and the correct subgoal label\ni. The classification dataset for paint-block, object-arrange, and kitchen-tasks consists of\n58k, 82k and 50k datapoints respectively.\nVision-Language Model (VLM) as a Classifier. We use a frozen pretrained Vision-Language Model\n(VLM) (MiniGPT4 [60]) as a classifier. We first sample a list of all possible subgoals W = {wi}M\ni=1\nfrom the LLM given the language goal g. We then use the VLM to eliminate subgoals from W\nthat have been completed. For each subgoal, we question the VLM whether that subgoal has been\ncompleted. For example, consider the subgoal \"Place white block in yellow bowl\". To see if the\nsubgoal has been completed, we ask the VLM \"Is there a block in yellow bowl?\". Consider the\nsubgoal \"Place green block in brown box\" as another example. To see if the subgoal has been\ncompleted, we ask the VLM \"Is there a green block in brown box?\". Furthermore, if the VLM\nsays \"yes\" and the subgoal has been completed, we also remove other subgoals from W that should\nhave been completed, such as \"Place white block in green bowl\". Once we have eliminated the\ncompleted subgoals, we use the domain knowledge to determine which subgoal to execute out of\nall the remaining subgoals. As an example, if the goal is to \"Stack green block on top of blue block\nin brown box\" and we have a green block in green bowl and a blue block in blue bowl, we should\nexecute the subgoal \"Place blue block in brown box\" before the subgoal \"Place green block on blue\nblock\". While this process of asking questions from VLM to determine the remaining subgoals and\nthen sequencing the remaining subgoals doesn\u2019t require any training data, it heavily relies on the\ntask\u2019s domain knowledge.\nFailure Modes of VLM. We observe two common failure modes of the VLM approach in\nobject-arrange environment and visualize them in Figure 8. First, because the model is not\ntrained on any in-domain data, it often fails to recognize uncommon objects, such as computer hard\ndrives, in the observations. Second, it occasionally hallucinates the presence of objects at certain\nlocations and thus leads to incorrect visual reasoning.\nVLM as a Subgoal Predictor. We also tried to prompt the VLM with 5 examples of goal g\nand subgoal candidates {wi}M\ni=1 and then directly use it to generate the next subgoal wi given the\nobservation xi,1 and the goal g. However, it completely failed. We hypothesize that the VLM fails to\ndirectly generate the next subgoal due to its inability to perform in-context learning.\nEvaluation. We evaluate the trained classifier f\u03d5 and the frozen VLM for subgoal prediction accuracy\non 5k unseen datapoints, consisting of observation, goal, candidate subgoals and correct subgoal,\ngenerated from test tasks Ttest. We average over 4 seeds and show the results in Figure 7.\nC.2\nVisual Planning\nEgo4D dataset processing. We pre-train on canonical clips of the Ego4D dataset which are text-\nannotated short clips made from longer videos. We further divide each canonical clip into 10sec\nsegments from which we derive 50 frames. We resize each frame to 48 \u00d7 64. We create a pretraining\nEgo4D dataset of (approximately) 344k short clips, each consisting of 50 frames and a text annotation.\nWe use the loader from R3M [35] codebase (https://github.com/facebookresearch/r3m) to load our\npretraining Ego4D dataset.\nTraining Objective and Dataset. We use pixel-level L1 reconstruction and negative perceptual\nsimilarity for training the autoencoder venc. We borrow this objective from PVDM [55] paper\nexcept we don\u2019t use adversarial loss. We keep the language encoder frozen. We use denoising loss\nin video latent space Ek\u223c[1,K],\u03c4z,w,x\u223cD,\u03f5\u223cN (O,I)[\u2225\u03f5 \u2212 \u03f5\u03d5((\u03c4z)k, lenc(w), venc(x), k)\u22252] to train the\nnoise model \u03f5\u03d5. We replace w with a null token so that \u03f5\u03d5 learns both a text-conditional model and\nan unconditional model. We pretrain the autoencoder venc and the noise model \u03f5\u03d5 on the processed\nEgo4D dataset. We then finetune it on our dataset Dvideo := {\u03c4 i\nx, wi} consisting of approximately\n100k observation trajectories of length T = 50 and associated text subgoals.\nClassifier Training Objective and Dataset. We use a binary cross-entropy loss to train the binary\nclassifier g\u03c8(\u03c4z) that predicts if the observation trajectory in latent space \u03c4z = venc(\u03c4x) leads to\nhigh-likelihood action trajectory. It is trained using trajectories from video dataset \u03c4x \u223c Dvideo.\n19\nC.3\nAction Planning\nTraining Objective and Dataset. We train inverse dynamics p\u03c8 on a dataset Dinv. Since actions are\ndifferences between robotic joint states, we train p\u03c8 to directly predict robotic state si,t = p\u03c8(xi,t)\nby minimizing the mean squared error between the predicted robotic state and ground truth robotic\nstate. Hence, Dinv := {\u03c4 i\nx, \u03c4 i\ns} consists of 1k paired observation and robotic state trajectories, each\nhaving a length of T = 50, in paint-block and object-arrange domains. In kitchen-tasks\ndomain, it consists of 3.5k paired observation and robotic state trajectories, each having a length of\nT = 50.\nEvaluation. We evaluate the trained p\u03c8 (i.e., VC-1 initialized model and other related models in\nFigure 6) on 100 unseen paired observation and robotic state trajectories generated from test tasks\nTtest. We use mean squared error to evaluate our inverse dynamics models. We use 4 seeds to calculate\nthe standard error, represented by the shaded area in Figure 6.\nC.4\nHyperparameters\nTask Planning. We train f\u03d5 for 50 epochs using AdamW optimizer [32], a batch size of 256, a\nlearning rate of 1e \u2212 3 and a weight decay of 1e \u2212 6. We used one V100 Nvidia GPU for training the\nmulti-class classifier.\nVisual Planning. We borrow our hyperparameters for training video diffusion from the PVDM\npaper [55]. We use AdamW optimizer [32], a batch size of 24 and a learning rate of 1e \u2212 4 for\ntraining the autoencoder. We use AdamW optimizer, a batch size of 64, and a learning rate of 1e \u2212 4\nfor training the noise model. During the pretraining phase with the Ego4D dataset, we train the\nautoencoder for 5 epochs and then the noise model for 5 epochs. During the finetuning phase with\nDvideo, we train the autoencoder for 10 epochs and then the noise model for 40 epochs. We used two\nA6000 Nvidia GPUs for training these diffusion models. We train g\u03c8 for 10 epochs using AdamW\noptimizer, a batch size of 256 and a learning rate of 1e \u2212 4. We used one V100 Nvidia GPU for\ntraining the binary classifier. During classifier-free guidance, we use \u03c9 = 4 and \u03c9\u2032 = 1.\nAction Planning. We train VC-1 initialized inverse dynamics model for 20 epochs with AdamW\noptimizer [32], a batch size of 256 and a learning rate of 3e\u22125. We trained other randomly initialized\nViT-B inverse dynamics models and randomly initialized ResNet-18 inverse dynamics models for 20\nepochs with AdamW optimizer, a batch size of 256, and a learning rate of 1e \u2212 4. We used one V100\nNvidia GPU for training these inverse dynamics models.\nD\nImplementation Details for Gato and SayCan\nIn training the visual and action planning for HiP, we use 100k robot videos for visual planner and\nthe inverse dynamics, when trained from scratch, utilizes 10k state-action trajectory pairs. In order to\nensure fair comparison, we use 110k datapoints for training Gato [40] and SayCan [2].\nGato. We borrow the Gato [40] architecture from Vima Codebase and use it for training a language\nconditioned policy with imitation learning. We use 110k (langauge, observation trajectory, action\ntrajectory) datapoints in each of the three domains for training Gato. Furthermore, we provide oracle\nsubgoals to Gato.\nSayCan. We borrow the SayCan [2] algorithm from the SayCan codebase and adapt it to our\nsettings. Following the recommendations of SayCan codebase, we use CLIPort policies as primitives.\nCLIPort policies take in top-down RGBD view and outputs pick and place pixel coordinates. Then,\nan underlying motion planner picks the object from the specified pick-coordinate and places the\nobject at the specified place-coordinate. We train CLIPort policies on 110k (language, observation,\naction) datapoints in paint-block and object-arrange domain. The SayCan paper uses value\nfunction as an affordance function to select the correct subgoal given current observation and high\nlevel goal. However, CLIPort policies don\u2019t have a value function. The SayCan codebase uses a\nhardcoded scoring function which doesn\u2019t apply to object-arrange domain. To overcome these\nissues, we use the LLM grounding strategy from ? ]. It uses unnormalized logits over the pixel\nspace given by CLIPort policies as affordance and uses it to ground LLM to current observation and\nthus predict the subgoal. We then compare SayCan with HiP and other baselines on paint-block\nand object-arrange domain in Table 1. While SayCan outpeforms other baselines, HiP still\n20\noutperforms it both on seen and unseen tasks of paint-block and object-arrange domain. We\ncouldn\u2019t run SayCan on kitchen-tasks domain as there\u2019s no clear-cut primitive in that domain.\nThis points to a limitation of SayCan which requires tasks to be expressed in terms of primitives with\neach primitive paired with an affordance function.\nE\nAdditional Ablation Studies\nE.1\nConsistency between task planning and visual planning\nTo make task planning consistent with visual planning, we need to select subgoal w\u2217\ni which maximizes\nthe joint likelihood (see equation 2) of LLM pLLM(wi|g) and video diffusion p\u03d5(\u03c4 i\nx|wi, xi,1). While\ngenerating videos for different subgoal candidates wi and calculating the likelihood of the generated\nvideo is computationally expensive, we would still like to evaluate its performance in subgoal\nprediction given it is theoretically grounded. To this end, we first sample M subgoals W = {wj}M\nj=1\nfrom the LLM. Then, we calculate w\u2217\ni = arg maxw\u2208W log p\u03d5(\u03c4 i\nx|w, xi,1) and use w\u2217\ni as our predicted\nsubgoal. Since log p\u03d5(\u03c4 i\nx|w, xi,1) is intractable, we estimate its variational lower-bound as an\napproximation. We use this approach for subgoal prediction in paint-block environment and\ncompare its performance to that of the learned classifier. It achieves a subgoal prediction accuracy of\n54.3 \u00b1 7.2% whereas the learned classifier achieves a subgoal prediction accuracy of 98.2 \u00b1 1.5%\nin paint-block environment. Both approaches outperform the approach of randomly selecting\na subgoal from W (i.e., no task plan refinement), which yields a subgoal prediction accuracy of\n16.67% given M = 6. The poor performance of the described approach could result from the fact\nthat the diffusion model only coarsely approximates the true distribution p(\u03c4 i\nx|wi, xi,1), which results\nin loose variational lower-bound and thus uncalibrated likelihoods from the diffusion model. A larger\ndiffusion model could better approximate p(\u03c4 i\nx|wi, xi,1), resulting in tighter variational lower-bound\nand better-calibrated likelihoods.\nE.2\nConsistency between visual planning and action planning\nTo make visual planning consistent with action planning, we need to select observation trajectory\n(\u03c4 i\nx)\u2217 which maximizes joint likelihood (see equation 4) of conditional video diffusion p\u03d5(\u03c4 i\nx|wi, xi,1)\nand inverse model QT \u22121\nt=1 p\u03c8(ai,t|xi,t, xi,t+1). While sampling action trajectories and calculating\ntheir likelihoods during every step of the denoising process is computationally inefficient, we would\nstill like to evaluate its effectiveness in visual plan refinements. However, we perform video diffusion\nin latent space while our inverse model is in observation space. Hence, for purpose of this experiment,\nwe learn another inverse model p\u03c8(\u03c4 i\na|\u03c4 i\nz) that uses a sequence model (i.e. a transformer) to produce\nan action trajectory \u03c4 i\na given an observation trajectory in latent space \u03c4 i\nz. We train p\u03c8 for 20\nepochs on 10k paired observation and action trajectories, each having a length of T = 50. We use\nAdamW optimizer, a batch size of 256 and a learning rate of 1e \u2212 4 during training. To generate an\nobservation trajectory that maximizes the joint likelihood, we first sample 30 observation trajectories\nfrom video diffusion p\u03d5(\u03c4 i\nx|wi, xi,1) conditioned on subgoal wi and observation xi,1. For each\ngenerated observation trajectory \u03c4 i\nx, we sample a corresponding action trajectory \u03c4 i\na and calculate\nits corresponding log-likelihood log p\u03c8(\u03c4 i\na|venc(\u03c4 i\nx)). We select the observation trajectory \u03c4 i\nx with\nhighest log-likelihood. Note that we only use p\u03c8 for visual plan refinement and use p\u03c8 for action\nexecution to ensure fair comparison. If we use this approach for visual plan refinement with HiP, we\nobtain a success rate of 72.5\u00b11.9 on unseen tasks in paint-block environment. This is comparable\nto the performance of HiP with visual plan refinements from learned classifier g\u03c8 which obtains a\nsuccess rate of 72.8 \u00b1 1.7 on unseen tasks in paint-block environment. In contrast, HiP without\nany visual plan refinement obtains a success rate of 71.1 \u00b1 1.3 on unseen tasks in paint-block\nenvironment. These results show that g\u03c8 serves as a good approximation for estimating whether an\nobservation trajectory leads to a high-likelihood action trajectory, while still being computationally\nefficient.\nArchitecture for p\u03c8. We use a transformer model to represent p\u03c8(\u03c4a|\u03c4z = [\u03c4 T\nz , \u03c4 H\nz , \u03c4 W\nz ]). We first\nuse a ResNet-9 encoder to convert \u03c4 T\nz , \u03c4 H\nz , and \u03c4 W\nz\nto latent vectors. We then concatenate those\nlatent vectors and project the resulting vector to a hidden space of 64 dimension using a linear layer.\nWe then pass the 64 dimensional vector to a trajectory transformer model [24] which generates an\n21\naction trajectory \u03c4a of length 50. The trajectory transformer uses a transformer architecture with 4\nlayers and 4 self-attention heads.\nE.3\nHow granularity of subgoals affects performance of HiP ?\nWe conduct a study in paint-block environment to analyze how granuality of subgoals affect HiP.\nIn our current setup, a subgoal in paint-block domain is of form \"Place <block color> block\nin/on/to <final block location>\" and involves a pick and a place operation. We refer to our\ncurrent setup as HiP (standard). We introduce two additional level of subgoal granuality:\n\u2022 Only one pick or place operation: The subgoal will be of form \"Pick <block color> block\nin/on <initial block location>\" or \"Place <block color> block in/on/to <final\nblock location>\". It will involve either one pick or one place operation. We refer to the\nmodel trained in this setup as HiP (more granular).\n\u2022 Two pick and place operations: The subgoal will be of form \"Place <1st block color>\nblock in/on/to <final 1st block location> and Place <2nd block color> block\nin/on/to <final 2nd block location>\". It will involve two pick and place operations.\nWe refer to the model trained in this setup as HiP (less granular).\nModel\nHiP (more granular)\nHiP (Standard)\nHiP (less granular)\nUniPi\nPaint-block (Seen)\n74.5 \u00b1 1.8\n74.3 \u00b1 1.9\n61.8 \u00b1 3.1\n37.2 \u00b1 3.8\nPaint-block (Unseen)\n73.1 \u00b1 2.1\n72.8 \u00b1 1.7\n58.2 \u00b1 3.4\n35.3 \u00b1 3.2\nTable 2: Granularity of Subgoals. Performance of HiP as we vary the granularity of subgoals. Initially, it\ndoesn\u2019t get affected but then starts to deteoriate when subgoals become too coarse.\nNote that UniPi has the least granuality in terms of subgoals as it tries to imagine the entire trajectory\nfrom goal description. Table 2 in the rebuttal document compares HiP (standard), HiP (more granular),\nHiP (less granular) and UniPi on seen and unseen tasks in paint-block environment. We observe\nthat HiP (standard) and HiP (more granular) have similar success rates where HiP (less granular)\nhas a lower success rate. UniPi has the lowest success rate amongst these variants. We hypothesize\nthat success rate of HiP remains intact when we decrease the subgoal granuality as long as the\nperformance of visual planner doesn\u2019t degrade. Hence, HiP (standard) and HiP (more granular)\nhave similar success rates. However, when the performance of visual planner degrades as we further\ndecrease the subgoal granuality, we see a decline in success rate as well. That\u2019s why HiP (less\ngranular) sees a decline in success rate and UniPi has the lowest success rate amongst all variants.\nE.4\nAblation on Visual Planning Model\nPaint-block\nObject-arrange\nKitchen-tasks\nModel\nSeen\nUnseen\nSeen\nUnseen\nSeen\nUnseen\nHiP (RSSM)\n70.2 \u00b1 2.4\n69.5 \u00b1 1.6\n59.6 \u00b1 3.8\n59.2 \u00b1 3.9\n50.6 \u00b1 16.2\n46.8 \u00b1 19.4\nHiP\n74.3 \u00b1 1.9\n72.8 \u00b1 1.7\n75 \u00b1 2.8\n75.4 \u00b1 2.6\n85.8 \u00b1 9.4\n83.5 \u00b1 10.2\nTable 3: Ablating Visual Planner. While performance gap between HiP and HiP (RSSM) is small in\nPaint-block, it widens in more visually complex domains, such as Object-arrange and Kitchen-tasks,\nthereby showing the importance of video diffusion model.\nTo show the benefits of video diffusion model, we perform an ablation where we use (text-conditioned)\nrecurrent state space model (RSSM), taken from DreamerV3 [? ], as visual model for HiP. We borrow\nthe RSSM code from dreamerv3-torch codebase. To adapt RSSM to our setting, we condition RSSM\non subgoal (i.e. subgoal encoded into a latent representation by Flan-T5-Base) instead of actions.\nHence, sequence model of RSSM becomes ht = f(ht\u22121, zt\u22121, w) where w is latent representation of\nsubgoal. Furthermore, we don\u2019t predict any reward since we aren\u2019t in a reinforcement learning setting\nand don\u2019t predict continue vector since we decode for a fixed number of steps. Hence, we remove\nreward prediction and continue prediction from the prediction loss. To make the comparisons fair,\nwe pretrain RSSM with Ego4D data as well. We report the results in Table 3. We see that HiP with\nvideo diffusion model outperforms HiP with RSSM in all the three domains. While the performance\n22\ngap between HiP(RSSM) and HiP (i.e. using video diffusion) is small in paint-block environment,\nit widens in object-arrange and kitchen-tasks domains as the domains become more visually\ncomplex.\nE.5\nAnalyzing sensitivity of iterative refinement to hyperparameters\nHiP\n\u03c9\u2032 = 0.5\n\u03c9\u2032 = 0.75\n\u03c9\u2032 = 1.0\n\u03c9\u2032 = 1.25\n\u03c9\u2032 = 1.5\n\u03c9\u2032 = 1.75\n\u03c9\u2032 = 2.0\nPaint-block (Seen)\n71.8 \u00b1 2.3\n72.3 \u00b1 2.0\n74.3 \u00b1 1.9\n73.9 \u00b1 2.2\n72.1 \u00b1 1.7\n70.4 \u00b1 2.4\n68.2 \u00b1 1.9\nPaint-block (Unseen)\n71.1 \u00b1 2.5\n71.4 \u00b1 1.8\n72.8 \u00b1 1.7\n73.1 \u00b1 1.5\n71.4 \u00b1 1.5\n69.3 \u00b1 2.7\n66.8 \u00b1 1.4\nTable 4: Sensitivity of visual iterative refinement to guidance scale. Performance of HiP as we vary the\nguidance scale \u03c9\u2032. HiP performs best when \u03c9\u2032 \u2208 {1, 1.25} but performance degrades for higher values of \u03c9\u2032.\nThe subgoal classifier doesn\u2019t introduce any test time hyperparameters and we use standard hyper-\nparameters (1e \u2212 3 learning rate, 1e \u2212 6 weight decay, 256 batch size, 50 epochs, Adam optimizer)\nfor its training which remains fixed across all domains. We observed that the performance changes\nminimally across different hyperparameters, given a learning rate decay over training. However,\nthe observation trajectory classifier g\u03c8 introduces an additional test time hyperparameter \u03c9\u2032 which\nappropriately weights the gradient from observation trajectory classifier. Table 4 in the rebuttal\ndocument varies \u03c9\u2032 between 0.5 and 2 in intervals of 0.25 and shows success rate of HiP. We see that\nHiP gives the best performance when \u03c9\u2032 \u2208 {1, 1.25} but it\u2019s performance degrades for higher values\nof \u03c9\u2032.\nF\nAnalyzing Runtime of HiP\nDomain\nSubgoal\ncandidate generation\nSubgoal\nclassification\nVisual planning\nper subgoal\nAction planning\nper subgoal\nAction execution\nper subgoal\nEpisodic\nruntime\nPaint-block\n1.85s\n0.41s\n7.32s\n0.91s\n6.35s\n80.61\nObject-arrange\n1.9s\n0.43s\n7.39s\n0.89s\n9.57s\n78.71\nKitchen-tasks\n1.81s\n0.41s\n7.35s\n0.98s\n1.28s\n40.37\nTable 5: Run time of HiP. Average episodic run-time of HiP, along with average run-time of its different com-\nponents for Paint-block, Object-arrange and Kitchen-tasks domains. While HiP has similar planning\ntimes across different domains, it has different action execution times and episodic runtimes across domains due\nto differences in simulation properties and average number of subgoals.\nWe provide average runtime of HiP for a single episode in all the three domains in Table 5 of the\nrebuttal document. We average across 1000 seen tasks in each domain. We break the average runtime\nby different components: task planning (subgoal candidate generation and subgoal classification),\nvisual planning, action planning and action execution. We execute the action plan for a subgoal in\nopen-loop and then get observation from the environment for deciding the next subgoal. From Table 5,\nwe see that majority of the planning time is taken by visual planning. Recent works [25, 43, 58] have\nproposed techniques to reduce sampling time in diffusion models, which can be incorporated into our\nframework for improving visual planning speed in the future.\n23\n"
  },
  {
    "title": "LASER: LLM Agent with State-Space Exploration for Web Navigation",
    "link": "https://arxiv.org/pdf/2309.08172.pdf",
    "upvote": "9",
    "text": "LASER: LLM Agent with State-Space Exploration for Web Navigation\nKaixin Ma\nHongming Zhang\nHongwei Wang\nXiaoman Pan\nWenhao Yu\nDong Yu\nTencent AI Lab, Bellevue, WA\n{kaixinma,hongmingzhang,hongweiw,xiaomanpan,wenhaowyu,dyu@global.tencent.com}\nAbstract\nLarge language models (LLMs) have been\nsuccessfully adapted for interactive decision-\nmaking tasks like web navigation.\nWhile\nachieving decent performance, previous meth-\nods implicitly assume a forward-only execution\nmode for the model, where they only provide\noracle trajectories as in-context examples to\nguide the model on how to reason in the envi-\nronment. Consequently, the model could not\nhandle more challenging scenarios not covered\nin the in-context examples, e.g., mistakes, lead-\ning to sub-optimal performance. To address\nthis issue, we propose to model the interactive\ntask as state space exploration, where the LLM\nagent transitions among a pre-defined set of\nstates by performing actions to complete the\ntask. This formulation enables flexible back-\ntracking, allowing the model to recover from\nerrors easily. We evaluate our proposed LLM\nAgent with State-Space ExploRation (LASER)\non both the WebShop task and amazon.com.\nExperimental results show that LASER sig-\nnificantly outperforms previous methods and\ncloses the gap with human performance on the\nweb navigation task.\n1\nIntroduction\nLarge language models (LLMs) such as GPT-4\n(OpenAI, 2023) have achieved remarkable perfor-\nmance on a wide range of natural language under-\nstanding (NLU) tasks (Brown et al., 2020; Ouyang\net al., 2022; Wei et al., 2022).\nRecently, they\nhave been adapted to interactive decision-making\ntasks such as virtual home navigation (Yang et al.,\n2023), text-based games (Lin et al., 2023) or web-\nnavigation (Yao et al., 2023; Zhou et al., 2024).\nPrevious methods that utilize LLMs to solve inter-\nactive tasks often implicitly assume a forward-only\nexecution mode for the model, where they only pro-\nvide a few oracle trajectories as in-context exam-\nples to teach the model how to reason step-by-step\n(Yao et al., 2023; Lo et al., 2023). In other words,\nFigure 1: LASER\u2019s state transition diagram on the Web-\nshop Task. Solid circle represent states, and the arrows\nrepresent possible state transitions. This formulation\nenables flexible backtracking and relieves the limitation\nof forward-only examples, allowing the model to better\nhandle unfamiliar scenarios and recover from errors.\nthe correct action is selected at every step in those\noracle trajectories. This might lead to sub-optimal\nperformance because when the model makes an\nunexpected mistake at test time, it would not know\nhow to recover from it. At the same time, includ-\ning many in-context examples to cover all possible\nscenarios is costly or unrealistic. Moreover, previ-\nous methods assume a global action space where\nthe model is free to take any action at any step\nbecause they either define the possible actions at\nthe beginning of the prompt or expect the LLM to\nfigure out the possible action from in-context exam-\nples automatically. This might further increase the\ntask\u2019s difficulty, and the LLM may perform invalid\nactions in certain cases.\nTo address the aforementioned issues, we pro-\npose to model the interactive tasks as state-space\nexploration. We first define a set of high-level pos-\nsible states the LLM agent might encounter during\nthe task execution. Then, we identify the possible\naction space in each state and the resulting states af-\nter performing each action. This formulation effec-\ntively converts the LLM agent\u2019s exploration in the\ninteractive task as state transitions, where each ac-\ntion takes the agent from one state to another. Nat-\nurally, this allows the agent to easily recover from\na wrong action: taking another action that would\narXiv:2309.08172v2  [cs.CL]  21 Feb 2024\nsend it back to the previous state. Moreover, our\nproposed formulation associates the action space\nwith each individual state, which reduces the task\u2019s\ndifficulty and allows the agent to always select the\nvalid action at any step. We evaluated our proposed\nLASER on the Webshop (Yao et al., 2022) task and\nconducted sim-to-real transfer experiments where\nwe directly applied LASER to amazon.com. We\nshow that our proposed setup enables the agent to\ncomplete complex user instructions without using\nin-context examples, and LASER significantly out-\nperforms all previous baselines and closes the gap\nwith human performance.\n2\nMethods\n2.1\nProblem Formulation\nGiven a web environment E and a user instruction\nI, the agent is instantiated in the environment and\nprovided with an initial observation O0. The agent\nis expected to perform a series of actions {a0, a1,\n...an} to complete the user instruction, where each\nai produces a new observation Oi when executed\nin the environment. S denotes the stopping state\nwhere the agent produces an output and stops explo-\nration after reaching it. Finally, the agent\u2019s output\nis compared with the target to compute the metrics.\n2.2\nLLM Agent\nAs previously discussed, we would like the agent\nto be able to handle any novel situations or mis-\ntakes that might occur during execution without\nexhaustively describing them via a large number\nof in-context examples. Thus, we propose to equip\nLLM agents with the state-tracking capability. A di-\nagram of the state transitions of our agent is shown\nin Figure 1. We start by defining a set of possi-\nble high-level states the agent might encounter in\nthe environment (\u00a72.3). The LLM agent takes the\nuser input as the overall goal and is initialized in\nthe starting state. At every step, the agent receives\nstate-specific system instruction, current observa-\ntion, a set of permissible actions in the current\nstates, and the history of past thoughts and actions\nas inputs. Then, it selects one of the actions as out-\nput, which either transitions the agent to a different\nstate or remains in the same state (\u00a72.4). The agent\nrepeats the process until the stopping state or the\nmaximum step is reached.\nNotice that with our formulation, we can pro-\nvide detailed instructions to inform the agent of\nthe possible situations in every state and how to\nhandle them. For example, as shown in Figure 1,\nat the results state, the current results may or may\nnot be good enough, and we instruct the agent to\neither select an item, go to the next page, or go\nback to search depending on its judgment. Hence,\nthese instructions can be very informative to guide\nthe agent while being much more efficient than in-\ncontext examples. Next, we describe in detail how\nwe design the state and action spaces.\n2.3\nState Description\nIn our work, we use the term state to describe the\ncurrent environment the agent is in, and we con-\nsider an agent to be in two different states only if\nthe structure of the current environment observa-\ntion is different. This allows us to define only a\nhandful of states to support an agent\u2019s exploration\nin a complex environment fully.\nAfter manually categorizing all possible states\nin the interactive task, for each state, we write a\ngeneric instruction that describes the state in detail.\nSpecifically, we provide a sample layout of the ob-\nservation the agent would receive in that state and\nreplace all specifications in the layout with place-\nholders. We also provide a high-level goal and\ndetailed instructions to act in that state. The sam-\nple layout combined with state-specific instructions\nallows us to inform the agent of possible observa-\ntions it might receive and how to act accordingly.\nTherefore we no longer need to provide in-context\nexamples to guide the agent. For the WebShop task,\nwe define a total of four states, and the full prompts\nfor search, results, and item states can be found in\nTable 4, Table 5 and Table 6 in the appendix.\n2.4\nAction Space\nPrevious methods often implicitly assume a global\naction space for the model, i.e. the model is free\nto take any action without further constraints. Al-\nthough the LLM is able to figure out valid actions\nto take most of the time, it might still attempt to\ntake invalid actions in certain cases. Thus after\ndefining all possible states for the task, we further\nidentify the action space for each state to rule out\nsuch possibilities. Specifically, we define a set of\npermissible actions that the agent can choose from\nfor each state, which ensures that the agent always\nperforms valid actions. The state-action mapping\nfor our agent is shown in Table 8 in the appendix.\nIn practice, permissible actions can also be deter-\nmined heuristically, e.g., identifying all clickable\nbuttons on a webpage.\nSuccess Rate\nReward\nASH (Lo et al., 2023)\n30.2\n56.7\nReAct (Yao et al., 2023)*\n40.0\n66.6\nReAct (ours rerun)\n34.0\n59.7\nWebGUM (Furuta et al., 2023)\n45.0\n67.5\nLASER - backup\n48.4\n71.2\nLASER\n50.0\n75.6\nHuman Expert (Yao et al., 2022)\n59.6\n82.1\nTable 1: Results on WebShop Task. *simplified setting\nInspired by the ReAct method (Yao et al., 2023),\nwe also ask the agent to produce a thought at every\nstep and then select an action based on its thought.\nThe agent keeps repeating the thought-and-action\nprocess until it reaches the stopping state or the\nmaximum step is reached. We also define a mem-\nory buffer to store the intermediate results (the\nitems examined but considered non-matching) dur-\ning the exploration. This is similar to human be-\nhavior in that people typically find a few backup\noptions before finding the desired item. When the\nagent is forced to stop after the maximum number\nof steps, it selects one of the intermediate results\nas the final output, and we call this the backup\nstrategy.\n3\nExperiments\nWe conduct our experiments on the WebShop task\n(Yao et al., 2022). We used 500 test set instructions\nfor evaluation and adopted reward and success rate\nas metrics following previous works (Yao et al.,\n2022). We used GPT-4-0613 to power LASER\nand its function-calling ability to implement action\nselection step. We compare against the following\nbaselines: ReAct (Yao et al., 2023) is a prompting\nmethod designed for interactive decision-making\ntasks. At every step, the LLM agent receives an\nobservation and can either produce a thought or an\naction. The agent accumulates all of the past obser-\nvations, thoughts, and actions in its prompt, using a\nfull trajectory of exploration as an in-context exam-\nple. The original ReAct uses PaLM (Chowdhery\net al., 2023) as its LLM backbone. To make a\nfair comparison, we also rerun the ReAct method\nwith GPT-4-0613. ASH (Lo et al., 2023) builds on\ntop of ReAct and adds a summarization step that\ncondenses the agent observation and acts based\non the condensed information.\nWebGUM (Fu-\nruta et al., 2023) is a supervised method that fine-\ntunes FlanT5-XL model (Chung et al., 2022) on 1K\nhuman demonstrations provided by the WebShop\nSR\nReward\nAtt.\nOpt.\nType.\nLASER\n62.0\n85.4\n85.5\n75.0\n97.0\nHuman (Yao et al., 2022)\n65.0\n88.2\n86.2\n76.3\n99.0\nTable 2: Results on Amazon.com.\nSuccess Rate\nReward\nLASER\n52.0\n77.6\nLASER + One-shot\n50.0\n74.9\nLASER - function call\n50.0\n76.2\nLASER (text-davinci-003)\n38.5\n70.2\nTable 3: Ablation Results on the WebShop Task. The\nstandard LASER is powered by GPT-4 under zero-shot.\ntask. Moreoever, we experimented with sim-to-\nreal transfer experiments where we directly apply\nLASER to amazon.com without modification. We\nfollow the same settings as Yao et al. (2022) and\nevaluated on 100 test set instructions and then man-\nually evaluated results. More detailed experimental\nsetup is discussed in Appendix B.\n4\nResults\nThe overall results of our experiments are shown\nin Table 1. Our early experiments showed that the\nReAct agent often produces invalid actions. For\nexample, when it selects an item that doesn\u2019t match\nthe instruction, it tries to click the next page but-\nton (which does not exist) before backing to the\nresults page. Also, the ReAct agent often got stuck\nin a certain action and failed to produce output.\nFor example, the agent keeps going to the next\npage until the maximum step is reached. We added\ndetailed instructions as the system prompt to try\nto address the issue. Despite our best efforts, the\nagent still makes invalid actions in some cases and\nachieves worse results than the original paper. On\nthe other hand, LASER outperforms baselines by\nlarge margins on both metrics, showing the effec-\ntiveness of our approach. We further removed the\nbackup strategy of LASER (the agent would re-\nceive a 0 score when the maximum budget runs\nout) to make a more fair comparison with ReAct.\nWe see that our method still outperforms baselines\nby very large margins. The results from the transfer\nexperiments are shown in Table 2. Again, LASER\nachieves very close results compared to human per-\nformance. It\u2019s also encouraging to see that LASER\neven achieved better performance on this realistic\nenvironment than the WebShop, which is likely due\nto the stronger search engine on amazon.com.\nFigure 2: Left: LASER\u2019s performance for test set\nepisodes of different lengths. Right: The distribution of\nthe number of steps LASER takes to complete tasks\n4.1\nAnalysis\nWe first conduct ablation studies to understand the\nimportant design decisions of our agent.\nZero-shot vs Few-shot We used state-specific in-\nstructions only to guide our agent\u2019s exploration\nin the environment, whereas previous works often\nadopt in-context examples. To investigate if the\nagent can further benefit from in-context examples,\nwe experimented with a one-shot setting: for every\nprompt in LASER, we added one example input-\noutput pair between our system instructions and\ncurrent inputs, and the rest of the agent remains\nthe same. Due to the limited computing budget,\nwe only ran our ablation studies on 200 instruc-\ntions. The results are shown in Table 3. We see\nthat adding an in-context example actually leads\nto worse performance. Since LASER already per-\nforms valid actions 100% time, we hypothesize\nthat the agent understands the task well without in-\ncontext examples and the added example is actually\ndistracting the agent in some cases.\nEffect of function-calling LASER takes advan-\ntage of the function-calling functionality that is\nenabled only for GPT models after 06/13/23. Thus,\nwe are interested to see the effect of replacing this\ndesign with regular text generation. To do so, in-\nstead of passing the permissible actions as a list\nof functions, we convert each action as a Python\ndictionary describing its purpose and arguments\nand then append them to the prompt. We then ask\nthe LLM to generate output in JSON format to\nrepresent the action it selects with appropriate ar-\nguments. The results are shown in Table 3. Again,\nthe agent without function calling performs slightly\nworse on these 200 episodes. It shows that the func-\ntion calling functionality can be leveraged to boost\nperformance when building interactive agents, sug-\ngesting a direction for building future LLMs.\nPerformance vs trajectory length Here, we are\ninterested in seeing the length of LASER\u2019s trajecto-\nries and their effect on the overall performance. We\nplot the distribution of trajectory length in Figure 2\nand the agent\u2019s performance for each length group.\nWe notice that most of the time, the agent only\ntook three state transitions to reach the finish state,\nwhich is search-select-buy. From the left figure,\nthe agent\u2019s performance generally decreases as the\ntrajectory gets longer. However, the drop is less\nsignificant compared to the observation made for\nReAct and ASH agent (Lo et al., 2023), which fur-\nther shows the effectiveness of our agent. Finally,\nfor the length 15 group, for which the agent is\nforced to stop and select from the browsing history,\nthe performance is much lower than other groups.\nWhile not surprising, it has a non-zero success rate,\nshowing that there are cases where the agent found\na matching item but failed to recognize it as the\ntarget in the first pass.\nGeneralization to different LLMs We adopted the\ntext-davinci-003 model to see if LASER can trans-\nfer well to a less powerful non-chat model. Since\nthis model does not support function-calling, we\nadopted the approach described earlier to prompt\nthe model to generate JSON output to represent ac-\ntions. The results are shown in Table 3. Although\nswitching to text-davinci-003 leads to a large drop\nin performance, our model still achieves better re-\nsults than the baselines. It shows that our proposed\nagent can be easily adapted to other LLMs with\ndifferent capabilities. With more powerful models\nin the future, our agent could potentially surpass\nhuman performance on this task. We also con-\nducted case studies to inspect the failure modes of\nLASER and additional results are in Appendix C.\nWe discuss related works in Appendix A.\n5\nConclusions\nWe proposed an LLM agent, LASER, that mod-\nels interactive web navigation tasks as state-space\nexploration. Our formulation allows the agent to\nhandle novel situations, easily backtrack from mis-\ntakes, and always perform valid actions. Guided\nsolely by the state-specific instructions without any\nin-context examples, LASER outperforms all base-\nlines on the WebShop task by large margins and\ncloses the gap with human performance on the real-\nworld shopping website. Our analysis shows that\nLASER is also more robust to longer trajectories\nand generalizes well to other LLMs.\nLimitations\nIn this work, we have only experimented with the\ntask of finding the target item for the shopping\ndomain. Despite its challenging nature, it does not\ncover all tasks user typiclaly do on an e-commerce\nwebsite, e.g., tracking orders or checking order\nhistory. For future work, it would be interesting\nto enhance LASER\u2019s ability so that it can handle\nsuch popular tasks in the shopping domain. Also,\nit would be interesting to equip LASER with more\ntools such as a knowledge retriever (Ma et al., 2023)\nor a calculator (Gao et al., 2023), so that it can\nhandle more complex instructions.\nOur LASER requires manual annotation of pos-\nsible states in the environment and their corre-\nsponding descriptions. Because of this, our method\nmight only be suitable for building agents for spe-\ncific domains (rather than open-world web agents),\nwhere only a handful of states are required, e.g. e-\ncommerce or travel booking. For future directions,\nwe envision a hierarchical multi-agent system, in\nwhich each specific domain is governed by an agent\nlike LASER, and a general open-world agent just\ncollaborates with other domain agents to complete\nvarious user instructions.\nRegarding potential risks of our work, we think\nextra caution and testing are required before de-\nploying LASER to real-world scenarios. When\nconducting experiments on the Webshop task, we\nallow the agent to take any action permitted in the\nenvironment because of its simulated nature. How-\never, certain actions may have hard-to-recover con-\nsequences in the real world. For example, clicking\nthe buy button in a real shopping site. Therefore we\nforced the agent to stop when it decides to buy the\nitem when experimenting on amazon.com. In gen-\neral, as LASER\u2019s success rate is still far from being\nperfect, it might require additional human verifi-\ncation before proceeding with actions that have\nhigh-stakes.\nReferences\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners.\nIn Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877\u20131901. Curran Associates,\nInc.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2023. Palm: Scaling language mod-\neling with pathways. Journal of Machine Learning\nResearch, 24(240):1\u2013113.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdh-\nery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,\nDasha Valter, Sharan Narang, Gaurav Mishra, Adams\nYu, Vincent Zhao, Yanping Huang, Andrew Dai,\nHongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-\ncob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le,\nand Jason Wei. 2022. Scaling instruction-finetuned\nlanguage models.\nXiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen,\nSamuel Stevens, Boshi Wang, Huan Sun, and Yu Su.\n2023. Mind2web: Towards a generalist agent for\nthe web. In Thirty-seventh Conference on Neural\nInformation Processing Systems Datasets and Bench-\nmarks Track.\nHiroki Furuta, Ofir Nachum, Kuang-Huei Lee, Yutaka\nMatsuo, Shixiang Shane Gu, and Izzeddin Gur. 2023.\nInstruction-finetuned foundation models for multi-\nmodal web navigation. In ICLR 2023 Workshop on\nMathematical and Empirical Understanding of Foun-\ndation Models.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\nPengfei Liu, Yiming Yang, Jamie Callan, and\nGraham Neubig. 2023.\nPal: program-aided lan-\nguage models. In Proceedings of the 40th Interna-\ntional Conference on Machine Learning, ICML\u201923.\nJMLR.org.\nIzzeddin Gur, Hiroki Furuta, Austin V Huang, Mustafa\nSafdari, Yutaka Matsuo, Douglas Eck, and Aleksan-\ndra Faust. 2024. A real-world webagent with plan-\nning, long context understanding, and program syn-\nthesis. In The Twelfth International Conference on\nLearning Representations.\nIzzeddin Gur, Ofir Nachum, Yingjie Miao, Mustafa Saf-\ndari, Austin Huang, Aakanksha Chowdhery, Sharan\nNarang, Noah Fiedel, and Aleksandra Faust. 2023.\nUnderstanding HTML with large language models.\nIn Findings of the Association for Computational Lin-\nguistics: EMNLP 2023, pages 2803\u20132821, Singapore.\nAssociation for Computational Linguistics.\nHongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu,\nYong Dai, Hongming Zhang, Zhenzhong Lan, and\nDong Yu. 2024. Webvoyager: Building an end-to-\nend web agent with large multimodal models.\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky\nLiang, Pete Florence, Andy Zeng, Jonathan Tompson,\nIgor Mordatch, Yevgen Chebotar, Pierre Sermanet,\nTomas Jackson, Noah Brown, Linda Luu, Sergey\nLevine, Karol Hausman, and brian ichter. 2023. In-\nner monologue: Embodied reasoning through plan-\nning with language models. In Proceedings of The\n6th Conference on Robot Learning, volume 205 of\nProceedings of Machine Learning Research, pages\n1769\u20131782. PMLR.\nJihyung Kil, Chan Hee Song, Boyuan Zheng, Xiang\nDeng, Yu Su, and Wei-Lun Chao. 2024. Dual-view\nvisual contextualization for web navigation.\nGeunwoo Kim, Pierre Baldi, and Stephen McAleer.\n2023. Language models can solve computer tasks.\nBill Yuchen Lin, Yicheng Fu, Karina Yang, Faeze Brah-\nman, Shiyu Huang, Chandra Bhagavatula, Prithviraj\nAmmanabrolu, Yejin Choi, and Xiang Ren. 2023.\nSwiftsage: A generative agent with fast and slow\nthinking for complex interactive tasks. In Thirty-\nseventh Conference on Neural Information Process-\ning Systems.\nEvan Zheran Liu, Kelvin Guu, Panupong Pasupat, and\nPercy Liang. 2018. Reinforcement learning on web\ninterfaces using workflow-guided exploration. In In-\nternational Conference on Learning Representations.\nRobert Lo, Abishek Sridhar, Frank Xu, Hao Zhu, and\nShuyan Zhou. 2023. Hierarchical prompting assists\nlarge language model on web navigation. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2023, pages 10217\u201310244, Singapore.\nAssociation for Computational Linguistics.\nKaixin Ma, Hao Cheng, Yu Zhang, Xiaodong Liu, Eric\nNyberg, and Jianfeng Gao. 2023. Chain-of-skills:\nA configurable model for open-domain question an-\nswering. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 1599\u20131618, Toronto,\nCanada. Association for Computational Linguistics.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\nShashank Gupta, Bodhisattwa Prasad Majumder,\nKatherine Hermann, Sean Welleck, Amir Yazdan-\nbakhsh, and Peter Clark. 2023. Self-refine: Itera-\ntive refinement with self-feedback. In Thirty-seventh\nConference on Neural Information Processing Sys-\ntems.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul F Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. In Advances in Neural Information\nProcessing Systems, volume 35, pages 27730\u201327744.\nCurran Associates, Inc.\nPeter Shaw, Mandar Joshi, James Cohan, Jonathan Be-\nrant, Panupong Pasupat, Hexiang Hu, Urvashi Khan-\ndelwal, Kenton Lee, and Kristina Toutanova. 2023.\nFrom pixels to UI actions: Learning to follow instruc-\ntions via graphical user interfaces. In Thirty-seventh\nConference on Neural Information Processing Sys-\ntems.\nNoah Shinn, Federico Cassano, Ashwin Gopinath,\nKarthik R Narasimhan, and Shunyu Yao. 2023. Re-\nflexion: language agents with verbal reinforcement\nlearning. In Thirty-seventh Conference on Neural\nInformation Processing Systems.\nHaotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai,\nand Chao Zhang. 2023. Adaplanner: Adaptive plan-\nning from feedback with language models. In Thirty-\nseventh Conference on Neural Information Process-\ning Systems.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\nand Denny Zhou. 2022. Chain of thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems.\nHui Yang, Sifu Yue, and Yunzhong He. 2023. Auto-\ngpt for online decision making: Benchmarks and\nadditional opinions.\nShunyu Yao, Howard Chen, John Yang, and Karthik R\nNarasimhan. 2022. Webshop: Towards scalable real-\nworld web interaction with grounded language agents.\nIn Advances in Neural Information Processing Sys-\ntems.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik R Narasimhan, and Yuan Cao. 2023.\nReact: Synergizing reasoning and acting in language\nmodels. In The Eleventh International Conference\non Learning Representations.\nChi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin\nChen, Zebiao Huang, Bin Fu, and Gang Yu. 2023.\nAppagent: Multimodal agents as smartphone users.\nBoyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and\nYu Su. 2024a. Gpt-4v(ision) is a generalist web agent,\nif grounded.\nLongtao Zheng, Rundong Wang, Xinrun Wang, and\nBo An. 2024b. Synapse: Trajectory-as-exemplar\nprompting with memory for computer control. In\nThe Twelfth International Conference on Learning\nRepresentations.\nShuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou,\nRobert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue\nOu, Yonatan Bisk, Daniel Fried, Uri Alon, and Gra-\nham Neubig. 2024. Webarena: A realistic web en-\nvironment for building autonomous agents. In The\nTwelfth International Conference on Learning Repre-\nsentations.\nA\nRelated Works\nInteractive decision-making tasks such as web nav-\nigation have become popular recently (Liu et al.,\n2018; Yao et al., 2022; Deng et al., 2023; Zhou\net al., 2024), while some efforts have tried to solve\nthese tasks by finetuning pretrained language mod-\nels on a large corpus of demonstration data (Gur\net al., 2023; Furuta et al., 2023), other attempted to\nbuild agents to navigate web environments solely\nrelying on prompting LLMs (Yang et al., 2023).\nAmong the LLM-based approaches, ReAct (Yao\net al., 2023) and InnerMonologue (Huang et al.,\n2023) equip the LLM with a thought process be-\nfore producing actions. ASH (Lo et al., 2023) and\nWebAgent (Gur et al., 2024) focus on decomposing\ncomplex decision-making steps into a set of simpler\nsteps, e.g. first summarizing the task-relevant con-\ntent and then act upon it. Most similar to our work,\nSynapse (Zheng et al., 2024b) also proposed to use\nstate-conditional prompts to guide the LLM\u2019s ac-\ntion. However, their focus is on decomposing the\nfew-shot examples into atomic parts whereas our\nagent uses state-specific instructions alone without\nin-context examples to complete tasks.\nAnother line of work focuses on the planning\nstage of LLM agents. Kim et al. (2023) proposed\nan agent RCI that generates a plan before acting,\nand then refines its action when encountering errors.\nAdaplanner (Sun et al., 2023) further enhanced the\nplanning approach by adaptively updating the plan\nduring the agent\u2019s execution. Reflexion (Shinn\net al., 2023) agent refines its plan and actions by\ntaking environmental feedback through a trial-and-\nerror fashion. These approaches are orthogonal to\nour work and can be potentially combined with our\nagent to enhance its performance.\nMore recently, various works have tried to de-\nvelop multi-modal agents. Pix2Act (Shaw et al.,\n2023) and AppAgent (Zhang et al., 2023) mostly\nreplied on the screenshots as inputs for the agents to\npredict UI actions, wheras SEEACT (Zheng et al.,\n2024a), WebVoyager (He et al., 2024) and Dual-\nVCR (Kil et al., 2024) leverage both screenshots\nand textual elements from websites to interact with\nthe web environment. Our idea of modeling web\nnavigation as state transitions can potentially be in-\ncorporated in those agents to further enhance their\nperformance.\nB\nExperimental Details\nThe WebShop provides a simulated environment\nfor online shopping, containing 1,181,436 items\ncollected from Amazon shopping sites. Addition-\nally, the task provides human-annotated instruc-\ntions for purchasing certain items and their corre-\nsponding target items. We followed previous works\nand used the 500 test set instructions to evaluate\nour LASER and evaluate with rewards and success\nrate, where the agent is considered successful if the\npurchased item perfectly matches the target item,\notherwise, if the purchased item partially matches\nthe target item, the agent receives a partial reward\n(scale between 0-100). This partial reward is com-\nputed using the items\u2019 price, product category, hid-\nden attributes and customization options.\nFor our method, we used the GPT-4-0613 to\npower our LASER. We used the function-calling\nfunctionality to implement the action selection step.\nIn particular, we write a short description for each\naction and then pass them as a list to the function-\ncall argument of the LLM to let the model select\nfrom. We allow our agent to make 15 state transi-\ntions in maximum. In practice, if the agent has not\nreached the finish state after 13 state transitions, we\nforce it to select from the history to ensure it does\nnot exceed the budget.\nFor the sim-to-real transfer experiments on\namazon.com, we used the first 100 test set instruc-\ntions from the WebShop. We following the same\nsetting as Yao et al. (2022), where we convert the\nwebpages on amazon.com into the same format\nas the WebShop 1 then run LASER agent as is.\nSince we do not have the gold annotation for the\n1https://github.com/princeton-nlp/WebShop/\ntree/master/transfer\nFigure 3: An example of the Item good enough error\ncases, the item selected by the agent is shown and the\nuser instruction is on the top. The reward the agent\nreceives is 0.666.\nFigure 4: An example of the Missing details error cases,\nthe item selected by the agent is shown and the user\ninstruction is on the top. The reward the agent receives\nis 0.8.\nitems LASER selected on amazon.com, we follow\nYao et al. (2022) and conducted human evalua-\ntion. In particular, we manually annotated item at-\ntribute matches, item option matches, item category\nmatches and item price matches. We then com-\nputed individual reward scores as well as the over-\nall reward score and success rate using the same\nfunctions defined for the WebShop task. Since both\nhuman and LASER achieves 100% on item price\nmatches, we omitted these results from Table 2.\nRegarding the comparison against different base-\nlines, we would like to note that both ReAct (Yao\net al., 2023) and ASH (Lo et al., 2023) used man-\nually written instruction and manually annotated\nagent trajectories as in-context demonstrations to\nprompt LLMs, which corresponds to our one-shot\nsetting in subsection 4.1. For WebGUM (Furuta\net al., 2023), they used 1k human annotated gold\ntrajectories to finetune their model. Therefore, all\nbaselines we considered use some kind of human\nknowledge/prior to help the agent learn. For us, we\nsolely relied on manual instructions to guide the\nLASER.\nWe believe that providing high-level generaliz-\nable instructions (as done in LASER) is a more\nefficient ways of learning than providing low-level\ntask-specific trajectories (e.g. WebGUM). Intu-\nitively, the agent basically learns to abstract out\nsome high-level insights about how to handle each\nscenario from the large amount of trajectories. In\ncomparison, we can directly provide such insights\nto the model via a few sentences in the instruction.\nTaking such perspective, we can also say that the\ndifference between our work and previous work is\nproviding high-level generalizable human knowl-\nedge vs providing low-level case-by-case human\nknowledge. We believe it\u2019s desirable to provide\nmodel such high-level knowledge when it requires\nsimilar or less amount of human effort.\nC\nCase Studies\nWe manually annotated 30 error cases from the\nDev set to understand the failure cases of LASER.\nWe broadly categorize the errors into three cate-\ngories: Item good enough: the item selected by the\nagent meets the user instruction from the authors\u2019\nperspective but did not receive a full score. We\nfound that 9 out of 30 cases fall into this category,\nand an example is shown in Figure 3. The item\nfound by the agent is indeed a green table lamp for\nthe living room with a price within the budget, but\nit is considered incorrect. Retrieval failure: none\nof the items returned by the search engine meets\nthe user requirement, despite that the agent used\na suitable query for retrieval. We found 12 out of\n30 cases fall into this category. We hypothesize\nthat a more effective retriever or search engine can\nprobably address these issues. Missing details: The\nitem selected by the agent indeed does not match\nthe user\u2019s instruction on certain details. We found\nthat 9 out of 30 cases fall into this category, and\nan example is shown in Figure 4. In this example,\nalthough the color and size of the selected women\u2019s\nshoes both matched the user instructions, these are\nnot high-heel shoes. This indicates that LASER\ncan make mistakes when encountering items with\nmany matching details, and it would be interesting\nto see if a self-feedback/verification module can\naddress this issue (Madaan et al., 2023).\nD\nPrompts used in our experiments\nE\nLicenses\nThe Webshop task and ReAct method are both re-\nleased under MIT License. They are both released\nfor research purposes, and our experiments are con-\nsistent with their intended usage.\nYou are an intelligent shopping assistant that can help users find the right item. You are given an\nobservation of the current web navigation session, in the following format:\nCurrent Observation:\nWebShop\nInstruction:\n{the user instruction}\n[button] Search [button_] (generate a search query based on the user instruction and select this button to\nfind relevant items)\nEvery button in the observation represents a possible action you can take. Based on the current\nobservation, your task is to generate a rationale about the next action you should take. Note that if an\nhistory of past rationales and actions is provided, you should also consider the history when generating\nthe rationale.\nTable 4: The system instruction we used for the search state.\nYou are an intelligent shopping assistant that can help users find the right item. You are given an\nobservation of the current web navigation session, in the following format:\nCurrent Observation:\nInstruction:\n{the user instruction}\n[button] Back to Search [button_] (select this button to go back to the search page)\nPage current page number (Total results: total number of results)\n[button] Next > [button_] (select this button to go to the next page of results)\n[button] {item_id 1} [button_] (select this button to view item 1\u2019s details)\n{name of item 1}\n{price of item 1}\n[button] {item_id 2} [button_] (select this button to view item 2\u2019s details)\n{name of item 2}\n{price of item 2}\n[button] {item_id 3} [button_] (select this button to view item 3\u2019s details)\n{name of item 3}\n{price of item 3}\n{More items...}\nAt this stage, you want to select an item that might match the user instruction. Note that even if an item\nhas non-matching details with the user instruction, it might offer different customization options to\nallow you to match. E.g. an item may have color x in its name, but you can customize it to color y later,\nthe customization options are shown after you select the item. Thus if an item name seems relevant or\npartially matches the instruction, you should select that item to check its details. If an item has been\nselected before (the button has been clicked), you should not select the same item again. In other words,\ndo not select an item with [clicked button] item_id [clicked button_]. Prepare your response in the\nfollowing format:\nRationale: the user wanted {keywords of the target item}, and we have found {matching keywords of\nitem x}, thus item {item_id x} seems to be a match.\nTable 5: The system instruction we used for the result state.\nYou are an intelligent shopping assistant that can help users find the right item. You are given an\nobservation of the current web navigation session, in the following format:\nCurrent Observation:\nInstruction:\n{the user instruction}\n[button] Back to Search [button_] (select this button to go back to the search page)\n[button] < Prev [button_] (select this button to go back to the previous page of results)\n{Customization type1}:\n[button] option1 [button_]\n[button] option2 [button_]\n{Customization type2}:\n[button] option1 [button_]\n[button] option2 [button_]\n{more customization options... (if any)}\n{Item name and details}\n[button] Description [button_] (select this button to view the full description of the item)\n[button] Features [button_] (select this button to view the full features of the item)\n[button] Reviews [button_] (select this button to view the full reviews of the item)\n[button] Buy Now [button_] (select this button to buy the item)\ndescription: (if this is shown, the description button should not be selected again)\n{full description of the item (if any) or \"None\"}\nfeatures: (if this is shown, the features button should not be selected again)\n{full features of the item (if any) or \"None\"}\nreviews: (if this is shown, the reviews button should not be selected again)\n{full reviews of the item (if any) or \"None\"}\nTarget item details (what the user is looking for):\nkeywords: {keywords of the target item}\nmax_price: {the price of the item should not exceed this}\nAt this stage, you want to verify if the item matches the user instruction. You should consider the\navailable customization options when deciding whether an item matches the user instruction. If an item\ncan be customized to match the user instruction, or if the customization options cover the user\nspecification, it is also a good match. If the item does not match the user instruction and it does not\nprovide enough customization options, you can go to previous page to view other items. You can also\ncheck the item\u2019s description, features and reviews to view more details (Note that description, features\nand reviews could be \"None\", do not check them again if they are already given). Prepare your\nresponse in the following format:\nRationale: the user wanted {keywords of the target item}, and they required the following customization\noptions: {customization of the target item}, the item is keywords of the item in the current observation,\nand it has the following customization options: {options available for the current item}, which {cover}/\n{not cover the user requirement}, thus we should {buy the item}/{check more details}/{go to previous\npage to view other items}\nTable 6: The system instruction we used for the item state.\nYou are an intelligent shopping assistant that can help users find the right item. You are given an\nobservation of the current environment and a rationale for the next action to be taken,\nin the following format:\nCurrent Observation:\nThe observation layout from search or result or item state, as shown from Table 4, Table 5 and Table 6\nNext action rationale: {the rationale for the next action}\nYour task is to perform one of the function calls based on the rationale.\nTable 7: The system instruction we used for generating action from thought.\nState\nAvailable Actions\nSearch\n{\"name\": \"Search\", \"description\": \"Use this function to search for the target item in the\ninventory based on keywords\"}\nResult\n{\"name\": \"select_item\", \"description\": \"Use this function to select one of the items from\nthe search results and check its details\"}\n{\"name\": \"Next\", \"description\": \"Use this function to go to the next page of search results\nto view more items, if none of the items on the current page match the user instruction.\"}\n{\"name\": \"Back_to_Search\", \"description\": \"Use this function to go back to the initial\nsearch page. You should use this function only if you have browsed multiple pages of\nitems and checked multiple items\u2019 details in the history, and none of the items\nmatch the user instruction.\"}\nItem\n{\"name\": \"Description\", \"description\": \"Use this function to check the description of the\nitem, if you are unsure if the item perfectly matches the user instruction\"}\n{\"name\": \"Features\", \"description\": \"Use this function to check the features of the item,\nif you are unsure if the item perfectly matches the user instruction\"}\n{\"name\": \"Reviews\", \"description\": \"Use this function to check the reviews of the item,\nif you are unsure if the item perfectly matches the user instruction\"}\n{\"name\": \"Buy_Now\", \"description\": \"Use this function to buy the current item, if the\ncurrent item perfectly matches the user instruction.\"}\n{\"name\": \"Prev\", \"description\": \"Use this function to go back to the results page, if the\ncurrent item does not match the user instruction \"}\nTable 8: The action space of our agent in each state. Each action is implemented as a function call following the\nguidelines from OpenAI 2, additional parameters used in the function call are omitted here for brevity.\n"
  },
  {
    "title": "Language Embedded Radiance Fields for Zero-Shot Task-Oriented Grasping",
    "link": "https://arxiv.org/pdf/2309.07970.pdf",
    "upvote": "7",
    "text": "Language Embedded Radiance Fields for\nZero-Shot Task-Oriented Grasping\nAdam Rashid*, Satvik Sharma*, Chung Min Kim, Justin Kerr, Lawrence Yunliang Chen\nAngjoo Kanazawa, Ken Goldberg\nFigure 1: Learning-based grasp planners primarily consider object geometry, potentially leading to\ndangerous grasps for robots and their environment. LERF-TOGO uses natural language to select the\ntarget object with LERF [1] (in orange), and resamples grasps towards specific object parts using\nconditional LERF queries (in blue) for safe, task-oriented grasps.\nAbstract: Grasping objects by a specific part is often crucial for safety and for\nexecuting downstream tasks. Yet, learning-based grasp planners lack this behav-\nior unless they are trained on specific object part data, making it a significant\nchallenge to scale object diversity. Instead, we propose LERF-TOGO, Language\nEmbedded Radiance Fields for Task-Oriented Grasping of Objects, which uses\nvision-language models zero-shot to output a grasp distribution over an object\ngiven a natural language query. To accomplish this, we first reconstruct a LERF\nof the scene, which distills CLIP embeddings into a multi-scale 3D language field\nqueryable with text. However, LERF has no sense of objectness, meaning its\nrelevancy outputs often return incomplete activations over an object which are in-\nsufficient for subsequent part queries. LERF-TOGO mitigates this lack of spatial\ngrouping by extracting a 3D object mask via DINO features and then condition-\nally querying LERF on this mask to obtain a semantic distribution over the ob-\nject with which to rank grasps from an off-the-shelf grasp planner. We evaluate\nLERF-TOGO\u2019s ability to grasp task-oriented object parts on 31 different physical\nobjects, and find it selects grasps on the correct part in 81% of all trials and grasps\nsuccessfully in 69%. See the project website at: lerftogo.github.io\nKeywords: NeRF, Grasping, Semantics, Natural Language\n* Denotes Equal Contribution, Alphabetically ordered\nUC Berkeley\n7th Conference on Robot Learning (CoRL 2023), Atlanta, USA.\narXiv:2309.07970v2  [cs.RO]  18 Sep 2023\n1\nIntroduction\nMany common objects must be grasped appropriately to avoid damage or facilitate performing a\ntask: a knife by its handle, a flower by its stem, or sunglasses by their frame. Learning-based grasp-\ning systems exhibit impressive robustness on grasping arbitrary objects [2, 3, 4, 5, 6, 7, 8, 9, 10], but\nthese systems typically measure grasp success based on whether the object was lifted [11, 12, 13,\n14, 15]. Critically, these methods ignore an object\u2019s semantic properties: even if a robot could locate\nyour favorite sunglasses, rather than safely grasp at the frame it may shatter the lenses. This abil-\nity to grasp an object part based on a desired task and constraints is called task-oriented grasping,\nand while well-studied [16, 17, 18, 19, 20, 21], previous methods collect specific object affordance\ndatasets and struggle to scale to a diverse set of objects. Instead, the flexibility of natural lan-\nguage has the potential for specifying what and where to grasp. In this work, we propose LERF for\nTask-Oriented Grasping on Objects (LERF-TOGO), a method which enables task-oriented grasping\nthrough natural language by using large vision-language models in a zero-shot manner.\nLERF-TOGO takes as input an object and a task-orienteed object part name in natural language (i.e.\n\u201cflower; stem\u201d), and outputs a ranking over viable grasps on this object from which the robot should\ngrasp. We build on recent work Language Embedded Radiance Fields (LERF) [1], which takes in\ncalibrated RGB images and trains a standard NeRF in tandem with a scale-conditioned CLIP [22]\nfeature field. Given a sentence prompt query, it outputs a 3D relevancy heatmap representing sim-\nilarity to the query. However, these heatmaps may fail to highlight the full object (e.g., highlight\nonly the bristles of a brush), which may cause issues when directly deployed to a task-oriented\ngrasping task (grasp the \u201chandle\u201d of a brush). LERF-TOGO improves upon LERF\u2019s capabilities by\npredicting a 3D object mask using 3D DINO [23] features explicitly during inference. We propose\na method of conditional LERF querying which restricts an object sub-part query to the object mask,\nleveraging the multi-scale nature of LERF to isolate specific regions within an object. LERF-TOGO\nthen uses GraspNet [15] to generate grasps, re-ranking them based on the geometric and semantic\ndistributions.\nWe implemented a system with appropriate regularizations which allows LERF-TOGO to operate\non a physical robot and evaluate its semantic grasping capabilities on 39 common household objects.\nIn experiments, 96% of generated grasps are on the correct object, 82% on the correct object part,\nand 69% result in a successful grasp.\nThis work contributes LERF-TOGO, an algorithm for producing task-based semantic grasp distri-\nbutions over an object by first extracting a coarse 3D object mask with DINO [23] features to locally\ngrow a relevant region, then conditioning a LERF query on this mask to isolate sub-parts of an ob-\nject. We design a robotic system which integrates LERF-TOGO on a physical robot to reconstruct a\nLERF of a scene, then execute task-oriented grasps through natural language to grasp semantically\nmeaningful object parts. See the project website at: lerftogo.github.io\n2\nRelated Work\n2.1\nTask-Oriented Grasping\nTask oriented grasping studies how to grasp objects by specific parts based on a use case. It has been\nstudied by probabilistically modeling human grasps [19], extracting geometric features from labeled\nobject parts [21], training on part-affordance datasets in simulation [24], or transferring category-\nspecific part grasps to new instances [25]. Recent works [26, 17] train object-part grasp networks\nby leveraging object part and manipulation affordance datasets for a range of household objects.\nIn contrast, LERF-TOGO\u2019s usage of off-the-shelf vision-language models trained at scale does not\nrequire training on affordance datasets, capturing long-tail objects more easily. Given 3D grasp\nregions on an object, Song et al. [27] demonstrate that biasing an off-the-shelf grasp planner towards\nthese regions is a viable approach to sampling task-oriented semantic grasps. Kokic et al. [18] use\nvideos of humans interacting with objects to guide grasps towards the same part. Decomposing\nobjects into parts has also long been studied as a co-segmentation task in vision [28, 29]. Recent\n2\nFigure 2: Task-Oriented Grasps: LERF-TOGO grasps target objects by different parts with differ-\nent natural language queries (in quotes). Left: Crops of target objects. Right: Top row visualizes the\nrelevancy distribution across the object mask, and bottom row shows top 5% of resampled grasps.\napproaches use pretrained vision features to discover common parts within sets of objects [30]. This\ntechnique has been applied at scale to segment parts of objects based on a canonical object [31] or\ndetect object affordances from example images of human usage [16]. Though effective, it assumes\naccess to a canonical image of each object and pre-existing part labels or demonstrations, which\nare restrictive in real-world applications. LERF-TOGO instead uses free-form language queries to\nisolate parts of objects in a 3D reconstruction.\n2.2\nNeural Radiance Fields (NeRF)\nNeural Radiance Fields (NeRF) [32] are an attractive representation for high quality scene recon-\nstruction from pose RGB images, with an explosion of recent work on visual quality [33, 34,\n35, 36, 37, 38, 39], large-scale scenes [40, 41, 42], optimization speed [43, 44, 45, 46], dynamic\nscenes [47, 48, 49], and more. Because of its high-quality reconstruction and differentiable prop-\nerties, NeRF has been widely explored in robotics for navigation and mapping [33, 50, 51, 52],\nmanipulation [53, 54, 55, 56], and for synthetic data generation [57]. This work is most similar to\nworks such as Evo-NeRF [55] which use NeRF as a real-time scene reconstruction to grasp objects.\nHowever, in contrast to previous works which only use RGB information, in this work we must\ninclude additional semantic information in 3D to select grasps falling on relevant target objects.\nSeveral prior works explore using semantic outputs inside NeRF. Semantic-NeRF [58], Panoptic\nLifting [59], and Panoptic Neural Fields [60] distill semantic categories from semantic segmentation\nnetworks into 3D to improve the 3D consistency of labels, particularly noting the denoising effect\nof averaging multiple views. Other works such as Distilled Feature Fields [61] or Neural Feature\nFusion Fields [62] distill feature vectors from DINO and LSeg [63], and show they can be used for\nediting and scene segmentation. We build off of LERF [1], which is described in the next section.\n2.3\nLERF Preliminaries\nLanguage Embedded Radiance Fields (LERF) [1] is a recent representation that distills CLIP fea-\ntures into a NeRF. LERF inputs RGB images with camera poses and outputs a 3D field of DINO\nembeddings as well as a scale-conditioned CLIP field. This supports querying points in 3D for\nCLIP embeddings at different physical scales, capturing different semantics given different amounts\nof context. Given a text query, a relevancy value (from 0 to 1) can be generated at any 3D point\nby calculating the cosine similarity between LERF-queried embeddings and the CLIP embedding\n3\nof query text. During this query process, a grid search on the scale parameter retrieves the scale\nwith the highest activation. LERF is particularly attractive for task-oriented grasping because 1) its\nmulti-scale parameterization allows queries at both object-level and part-level scales 2) LERF uses\noutputs from a pre-trained CLIP model without fine-tuning, which supports a variety of long-tail\nobject queries not included in object or part segmentation datasets. LERF, however, tends to pro-\nduce nonuniform activations on object queries because it lacks spatial grouping as shown in Fig. 5.\nIn this work we show how to explicitly use the DINO feature field to obtain object masks to enable\ndown-stream object part queries related to task-oriented grasping.\n2.4\nOpen-Vocabulary Detectors\nOpen-vocabulary object detection attempts to output masks or bounding box detections given text\nprompts as input. OVD [64] proposes a two-stage training pipeline, where they first learn a visual-\nsemantic stage using image-text pairs and then learn object detection using object annotations from\na set of base classes. OpenSeg [65] employs a similar joint learning framework for simultaneously\ntraining a mask prediction model and learning visual-semantic alignments for each mask by aligning\ncaptions and masks. Some methods [66, 67, 68] distill CLIP embeddings into a visual encoder by\nusing labels from detection or classification datasets, or fine-tune a lightweight detection head on\ntop of a pretrained backbone [69]. However, fine-tuning on detection datasets severely bottlenecks\nthe language understanding capability of detection models [1, 61, 70], and as noted by the authors\nof OV-Seg [67], these networks face challenges in decomposing hierarchical components of the\noriginal masks, including object parts. LERF circumvents the usage of region proposals or fine-\ntuning by incorporating language embeddings densely in 3D, making it an attractive approach for\neffective part decomposition.\n2.5\nNatural Language in Robotics\nWith the advent of large pretrained language and vision-language models, several works have\nexplored building 3D map representations to guide robot navigation. VL-Maps [71] and Open-\nScene [72] build a 3D language embedding from pretrained open-vocab detectors [73, 63], which\ncan be used to navigate to target queries. CLIP-Fields [74], ConceptFusion [75], and NLMaps-\nSayCan [76] take more of a region-proposal based approach, querying CLIP on the outputs of\nsome region proposal methods and fusing them into 3D point clouds for downstream navigation\ntasks. Region-based zero-shot methods retain more language understanding than fine-tuned fea-\ntures but run the risk of missing objects by insufficiently masking input images. Semantic Ab-\nstraction [77] avoids this by extracting relevancy from vision-language models using Chefer et al.\n[78] and uses these for composing multiple language queries with spatial relationships. Language\nhas also been studied in the context of robot manipulation. Mees and Burgard [79] use the MAt-\ntNet [80] vision-language model for object rearrangement, and CLIPort [81] uses language un-\nderstanding from CLIP to train a language-conditioned pick and place module from demonstra-\ntions. PerAct [82] uses language-conditioned demonstrations with a 3D scene transformer to learn\ndiverse tasks, MOO [83] uses the outputs from Owl-ViT to condition a manipulation policy for\ngrasping objects, and large-scale demonstration datasets like RT-1 [84] train on massive language-\nconditioned demonstration trajectories. In contrast to many other language-conditioned approaches,\nLERF-TOGO uses internet-scale vision models purely zero-shot and does not require fine-tuning on\ndemonstrations or robot exploration.\n3\nProblem and Assumptions\nGiven a planar surface (table or workbench) containing a set of objects, the objective is for a robot\nto grasp and lift a target object specified using natural language. This query (e.g., \u201csunglasses; ear\nhooks.\u201d) includes both the object query (\u201csunglasses\u201d) and the object part query, which specifies\nthe part to grasp the object by (\u201cear hooks\u201d). We experiment with lifting this assumption in Sec. 5\nby leveraging an LLM for providing part queries. We assume access to a robot manipulator with\n4\nFigure 3: LERF-TOGO Pipeline: After reconstructing the scene with a wrist-mounted camera,\nwe render an object-centric point cloud around the highest LERF activation. We next extract a 3D\nobject mask by flood-filling the DINO features in this point cloud, condition an object part query on\nthis object mask. Finally, we sample grasps and re-rank them according to 3D object part relevancy.\na parallel jaw gripper and calibrated wrist-mounted RGB camera, and the objects in the scene are\ngraspable by the robot. We also assume the object query specifies a single object that is present in\nthe scene.\n4\nMethod\nGiven an object and object part query, LERF-TOGO outputs a ranking of viable grasps on the object\npart. To accomplish this, it first captures the scene and reconstructs a LERF. Given a text query,\nLERF can generate a 3D relevancy map that highlights the relevant parts of the scene (Sec. 2.3).\nSecond, a 3D object mask is generated using the LERF relevancy for the object query and DINO-\nbased semantic grouping (Sec. 4.1). Third, a 3D part relevancy map is generated with a conditional\nLERF query over the object part query and the 3D object mask (Sec. 4.2). The part relevancy map\nis used to produce a semantic grasp distribution.\n4.1\n3D Object Extraction\nAn important limitation of LERF is its lack of spatial grouping within objects: for example given\n\u201ccan opener\u201d, LERF tends to highlight regions of the object that most obviously identify that object\n(e.g. the metal cogs on the can opener as shown by the orange star in Fig. 3). However, since\nthe region that visually identifies the object and the desired grasp location (e.g. handle) can differ\nsignificantly, this is problematic. LERF inherently exhibits such local behavior because it trains on\nlocal crops of input images, causing CLIP embeddings surrounding the handle to be unaware if it\nbelongs to a can opener. LERF-TOGO overcomes this by finding a 3D object mask given a language\nquery, which groups the object part together with the LERF activation. To create the object mask,\nwe leverage the 3D DINO embeddings [23] (self-DIstillation with NO labels) present within LERF\nduring inference, because DINO embeddings have been shown to exhibit strong object awareness\nand foreground-background distinction [30, 23, 85].\nFirst, we obtain a coarse object localization from LERF by rendering a top-down view of the scene\nand querying the object. We produce a foreground mask by thresholding the first principal compo-\nnent of the top-down rendered DINO embeddings, and constrain the relevancy query to this mask to\n5\nFigure 4: Left: Select scenes used during experiments, Right: example grasps using LERF-TOGO.\nfind the most relevant 3D point. We then refine this single-point localization into a complete object\nmask. We render an object-centric point cloud around this 3D point by deprojecting NeRF depth\nfrom multiple views, and then iteratively grow the object mask by including neighboring points to\nthe frontier which lie within a threshold DINO similarity (similar to floodfill). The output of this\nprocess is a set of 3D points lying on the target object. See the Appendix for more details.\n4.2\nConditional LERF Queries\nAnother important challenge of using CLIP is its tendency to behave as a bag-of-words [22]: the\nactivation for \u201cmug\u201d behaves very similarly to \u201cmug handle\u201d because CLIP latches onto individual\nwords, not the grammatical structure of sentences. To mitigate this phenomenon, LERF-TOGO\nintroduces a conditional method of querying LERF relevancy by composing two related queries,\nsimilarly to how composing prompts has shown promise in generative modeling for guiding specific\nproperties [86]. Because LERF is scale-conditioned, during inference it searches over scales for a\ngiven query and returns the relevancy at the scale with the highest activation. To condition a LERF\nquery, LERF-TOGO searches only on the points within the 3D object mask. Intuitively, this results\nin a distribution over the object\u2019s 3D geometry representing the likelihood that a given point is the\ndesired object part, which can be used for biasing grasps towards this region.\n4.3\nGrasping\nGrasp Sampling Ensuring complete coverage of grasps on objects is critical to avoid missing spe-\ncific object parts. We use GraspNet [15], which can generate 6-DOF parallel jaw grasps from a\nmonocular RGBD point cloud, but from a single view it often misses key grasps on target object\nparts. To mitigate this, and to leverage the full 3D geometry available within NeRF, we create a\nhemisphere of virtual cameras oriented towards the scene\u2019s center. For every virtual camera, we\nconvert the scene\u2019s point cloud to the camera coordinate frames before providing it as input to the\npretrained GraspNet model. To obtain the final set of grasps for the scene, we combine the generated\ngrasps from the virtual cameras using non-maximum suppression to remove duplicates.\nGrasp Ranking Given the grasps sampled in the previous step (the geometric distribution), we\nnow combine it with the semantic distribution across an object obtained from LERF-TOGO. The\nsemantic score ssem for a given grasp is computed as the median LERF relevancy of points within\nthe grasp volume. The geometric score sgeom is the confidence output from GraspNet, indicating\ngrasp quality based on geometric cues. To balance relevance and success likelihood, we combine\nthe grasp score s = 0.95ssem +0.05sgeom to ensure that we consider the most relevant grasps while\nslightly biasing towards confident grasps.\n4.4\nScene Reconstruction\nThe robot uses a wrist-mounted camera to capture the scene with a hemispherical trajectory centered\nat the workspace, similar to Evo-NeRF [55]. The capture has a radius of 45 cm and arcs from \u00b1100\u25e6\n6\nFigure 5: Ablation and Limitations: Left: Without 3D object masking and conditional querying,\nLERF cannot capture oblong object shapes. Right: CLIP can sometimes fail on generic prompt\nqueries, like the poor activation on the box. Additionally, LERF-TOGO struggles with groups of\nconnected objects as the 3D object mask groups them all together.\nConceptFusion [70]\nLERF-TOGO\nCorrect Object\n77%\n96%\nCorrect Part\n39%\n82%\nSuccessfuly Lifted\n\u2013\n69%\nTable 1: Part-Based Grasping Results: Results are reported across 49 different prompts and 12\nscenes. See the Appendix for a complete list of scenes and queries.\naround the workspace horizontally and an inclination range of 30\u25e6 to 75\u25e6. We capture images\nwhile the arm moves at 15 cm/sec at a rate of 3 hz, resulting in around 60 images per capture. We\ndiscard blurry images by analyzing the variance of the image Laplacian, ensuring the images are high\nquality. While the robot moves, we pre-process each image to extract DINO features, multi-scale\nCLIP, and ZoeDepth [87], which are used during LERF training. See Appendix B for additional\ntraining details.\n5\nExperiments\nPart-Oriented Grasping We evaluate LERF-TOGO on a wide variety of 31 different objects and\n49 total object parts to grasp (Fig. 4). For each object, we select an object query by describing it\nsufficiently to unambiguously differentiate between other objects in the scene. We use semantic\ndescriptions when possible, and add visual descriptions only when such descriptions are ambiguous\n(i.e using color to differentiate multiple mugs in a scene). We provide a part query for each object\nby describing a natural place for a robot to grasp and lift (i.e. \u201chandle\u201d, \u201cplant stem\u201d, \u201cear hook\u201d,\n\u201cframe\u201d). In addition, several objects include multiple different grasp locations. A grasp is success-\nful if it lifts the correct object using the appropriate subpart at least 10cm vertically, and the object\nremains securely within the gripper jaws throughout. For each query, we measure 1) whether the\nselected grasp was on the correct object, 2) whether the selected grasp was on the correct object part,\nand 3) whether the grasp successfuly lifted the object from the table. Every scene is reconstructed\nonce in the beginning, after which the objects are removed sequentially (i.e., objects are removed\none-by-one) with no updates in the scene representation. For a full list of object queries see the\nAppendix.\nTask-Oriented Grasping LERF-TOGO accepts a natural language part query as input, allowing\nit to be used alongside large language models (LLMs) to generate parts based on the task. To\ninvestigate if the LLM can also generate the object part, we use an LLM (ChatGPT) to generate\nthe object and part query automatically via few-shot prompting. Results are shown in Table 3. The\nprompt and all tasks are included in the Appendix. Given the task and the list of objects in the scene,\nthe LLM is tasked with generating the correct object and object part pair (object, part). We used a\n7\nSemAbs [77]\nOWL-ViT [69]\nCorrect Object\n80%\n85%\nCorrect Part\n35%\n50%\nTable 2: Single View Comparisons: Results are reported across 20 different prompts and 5 scenes.\nSee the Appendix for a complete list of scenes and queries.\nmajority voting scheme to query the LLM. Given the task, the LLM provides seven candidates that\nwe use to select the pair (object, part) that appears in a majority of the responses.\nIntegration with an LLM Planner LERF-TOGO can integrate as a module with an LLM planner to\ncombine task-oriented grasps for robotic manipulation tasks. We define a set of robotic manipulation\nprimitives (grasp, press, twist, pick&place, pour) and prompt the LLM to output the correct primitive\nfor a given task. We use the same majority voting scheme in the previous section to select both the\ncorrect robotic primitive and the pair (object, part). Now, given a task (e.g. \u2018uncork the wine\u2019), an\nLLM can specify the action to accomplish the task (\u2018grasp\u2019) and the pair of object and object part\n(e.g. \u2018wine\u2019 and \u2018cork\u2019). The prompt and all tasks are included in the Appendix.\n5.1\nComparisons\nConceptFusion [70] generates a multimodal point cloud of a scene by fusing RGBD images and\ntheir extracted features together. We provide ConceptFusion with depth generated from the NeRF,\nwhich results in the same high-quality point cloud we use for grasping. To fairly represent the paper,\nwe use the OpenCLIP ViT-H/14 model, which is several times larger than the ViT-B/16 model for\nLERF-TOGO. To query ConceptFusion, we provide it with the concatenated object and part prompt\n(i.e. \u201cmug handle\u201d) and rank grasps via the highest similarity. We report the object and part success\nwithout physical evaluation.\nSemantic Abstraction [77] takes a single RGBD frame as input and a text query and outputs a\nrelevancy heat map over the image. Since the method takes a single image, we provide the method\nwith an input image observing all object parts for a fair comparison. We provide it with part queries\n2 ways and take the best performance: 1) the concatenated object and part prompt (i.e. \u201cmug\nhandle\u201d), and 2) the object and part as separate queries. A query is a success if the majority of the\nheatmap overlaps with the object part. In instances where activations are detected on other objects,\nit is considered successful if the highest activation is on the desired object part. Detailed results can\nbe found in Table 2 and the Appendix.\nOWL-ViT [69] is an open-vocab detector which takes in an RGB image and text prompts and\noutputs segmentation maps. We provide OWL-ViT a single input image that encompasses all object\nparts for a fair comparison. To obtain an object mask, we use the object prompt to establish an initial\nbounding box. This box serves as a region to identify the highest-scoring part within the region. In\norder to deem the part box as successful, we visually confirm that it aligns with the object part.\nResults can be found in Table 2 and the Appendix.\n6\nResults\nPart-Oriented LERF-TOGO overall achieves a 69% success rate for physically grasping and lifting\nobjects by the correct part. The selected grasp was located around the correct object part 82% of the\ntime, with the remaining failures being grasp execution failures. For context, the highest confidence\ngeometric grasp on an object mask only lies on the correct part 18% of the time, suggesting LERF-\nTOGO meaningfully biases the grasp distribution to the object part. Selected task-oriented queries\nare visualized in Fig. 2: the distribution of grasps drastically shifts based on the given part query, and\ncan focus task-oriented grasps on multiple different regions per object based on the language prompt.\nLERF-TOGO shows strong language understanding performance for object selection (96%), able to\n8\nHuman-Provided Part\nLLM-Provided Part\nCorrect Object\n96%\n96%\nCorrect Part\n82%\n71%\nTable 3: Task-Oriented Success: Results using an LLM to choose the object part given a task\nspecification. Results are reported across 49 different prompts and 12 scenes. A success measures\nif the grasp chosen by LERF-TOGO surrounds an object part which facilitates the task.\ndifferentiate between very fine-grained language queries like color, appearance (\u201cmatte\u201d vs \u201cshiny\u201d),\nor semantically similar categories (\u201cpopsicle\u201d vs \u201clolipop\u201d). It also can recognize long-tail object\nqueries like \u201cethernet dongle\u201d, \u201ccork\u201d, or \u201cmartini glass\u201d, owing to its usage of CLIP zero-shot.\nTask-Oriented Combining few-shot LLM prompting with LERF-TOGO identifies the correct prim-\nitive with 92% success and produces grasps on the correct object with 71% success across 49 tasks\non 39 different objects. The LLM was able to correctly identify the object in the scene with the\nsame success rate as the human, giving the correct object and part pairs for common tasks like\n\u201cscrub the dishes\u201d and \u201ccut the steak\u201d. However, the LLM had a lower success rate (71%) compared\nto the human (82%) for object part selection. This is because CLIP, and by extension LERF-TOGO,\ncan be sensitive to subtle variations in wording like \u201cbody\u201d vs. \u201cbase\u201d resulting in different LERF\nactivations and thus grasps.\nComparisons LERF-TOGO out-performs ConceptFusion by 43% at task-oriented grasping because\nit can capture multi-scale semantics, while ConceptFusion is limited to one CLIP embedding per\npoint. This makes hierarchical querying difficult, and is reflected by the fact that ConceptFusion\nperforms similarly to LERF-TOGO at selecting the correct object, but suffers at selecting the right\nobject part. Due to its lack of scale-conditioning, ConceptFusion frequently emphasizes large sec-\ntions of the table due to the inclusion of both the objects and the table itself in the mask proposals\n(Fig. 6).\nSemantic Abstraction achieves an overall object detection rate of 80% and part detection of 35%.\nThe method tends to produce empty relevancy responses when queried for specific object parts,\npotentially owing to its averaging across multiple scales which drowns out smaller part features.\nWhen presented with the concatenated object and object part, the method highlights the entirety of\nthe object, owing to CLIP\u2019s bag of words behavior, a characteristic addressed by LERF-TOGO\u2019s\ncompositional queries. (Fig. D.2).\nOWL-ViT achieves 85% accuracy for object localization, struggling on very long-tail objects that\nwere not encountered within the detection datasets. This behavior is amplified for object part queries,\nwhere queries tend to be especially long-tail such as \u201cmeasuring tape\u201d and \u201cethernet dongle\u201d\n(Fig. 12).\nFailures The primary failure modes of LERF-TOGO are mistaking visually similar object parts for\none another (eg teapot spout for a handle), missing subtle geometries like the small teacup handle\nor spray trigger, or confusing very close categories like steak and bread knives. We also observe\nprompt sensitivity with part queries: for example \u201cbottle neck\u201d more strongly localizes grasps than\n\u201cneck\u201d, and without more prompt tuning \u201cbody\u201d sometimes fails to highlight the bases of bottles.\nObject Extraction Ablation: Without 3D object masking and conditional querying, LERF-TOGO\nsuffers with oblong objects, as shown in Fig. 5. We compare against querying LERF individually for\nthe object and part, and multiplying their results together. This produces fragmented results which\ncan ignore relevant parts of the object for part queries.\n7\nLimitations and Future Work\nOne limitation of LERF-TOGO is speed: the entire end-to-end the process takes a few minutes\nwhich can be impractical for time-sensitive applications. Future work on additional regularizations\nand optimizations to LERF training may reduce computation time. Another key limitation of LERF-\nTOGO is with groups of connected foreground objects, for example a bouquet of multiple flowers.\n9\nFigure 6: Comparison with LERF and ConceptFusion: ConceptFusion performs well on object-\nlevel queries, but struggles with sub-object part queries because of its lack of multi-scale semantics.\nThe DINO flood-fill will create a foreground group containing all flowers, after which isolating\nthe stem of a specific individual type of flower (i.e \u201cdaisy\u201d vs \u201crose\u201d) is challenging. Supporting\nhierarchy within foreground groups is critical to enable such cases. If there are multiple objects\nthat match the prompt, the system will arbitrarily choose only one of them. This is also true if the\nobject query cannot disambiguate multiple instances of a similarly colored object (e.g., \u201cmug\u201d object\nquery in the mugs scene). We also note that LERF-TOGO is not designed for referring/comparative\nexpressions (e.g., \u201cmug next to the plate\u201d, \u201cbiggest mug\u201d). In addition, though we present a method\nfor obtaining object part queries from input task descriptions via LLMs, in future work we will\nevaluate its performance on a diverse set of tasks.\n8\nConclusion\nThis paper presents LERF-TOGO, a method for using vision-language models zero-shot with Lan-\nguage Embedded Radiance Fields to grasp objects and their parts via language. By improving the\nspatial grouping of LERF relevancy outputs, LERF-TOGO can support hierarchical part queries\nconditioned on the full object. Results indicate it performs strongly at language-guided grasping,\nwith grasps landing on the correct object 96% of the time, and furthermore can direct grasps to the\ncorrect object parts 81% of the time. All code and datasets will be released after the submission\nprocess.\n9\nAcknowledgements\nThis research was performed at the AUTOLAB at UC Berkeley in affiliation with the Berkeley\nAI Research (BAIR) Lab. The authors were supported in part by donations from Toyota Research\nInstitute, Bosch, Google, Siemens, and Autodesk and by equipment grants from PhotoNeo, NVidia,\nand Intuitive Surgical. Any opinions, findings, and conclusions, or recommendations expressed\nin this material are those of the authors and do not necessarily reflect the views of the Sponsors.\nWe would like to thank our colleague Brent Yi for his work on Viser, the visualization tool of our\nexperimental setup. We thank our colleagues who provided helpful feedback and suggestions, in\nparticular Simeon Adebola and Julia Isaac.\n10\nReferences\n[1] J. Kerr, C. M. Kim, K. Goldberg, A. Kanazawa, and M. Tancik. Lerf: Language embedded\nradiance fields. ICCV, 2023.\n[2] K. Kleeberger and M. F. Huber. Single shot 6d object pose estimation, 2020.\n[3] K. Bousmalis, A. Irpan, P. Wohlhart, Y. Bai, M. Kelcey, M. Kalakrishnan, L. Downs, J. Ibarz,\nP. Pastor, K. Konolige, S. Levine, and V. Vanhoucke. Using simulation and domain adaptation\nto improve efficiency of deep robotic grasping, 2017.\n[4] S. Song, A. Zeng, J. Lee, and T. Funkhouser. Grasping in the wild: Learning 6dof closed-loop\ngrasping from low-cost demonstrations. Robotics and Automation Letters, 2020.\n[5] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel. Domain randomization\nfor transferring deep neural networks from simulation to the real world, 2017.\n[6] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, D. Quillen, E. Holly,\nM. Kalakrishnan, V. Vanhoucke, and S. Levine. Qt-opt: Scalable deep reinforcement learning\nfor vision-based robotic manipulation, 2018.\n[7] S. Levine, P. Pastor, A. Krizhevsky, J. Ibarz, and D. Quillen. Learning hand-eye coordination\nfor robotic grasping with deep learning and large-scale data collection.\nThe International\nJournal of Robotics Research, 37(4-5):421\u2013436, 2018. doi:10.1177/0278364917710318.\n[8] A. Mousavian, C. Eppner, and D. Fox. 6-dof graspnet: Variational grasp generation for object\nmanipulation, 2019.\n[9] J. Mahler, M. Matl, V. Satish, M. Danielczuk, B. DeRose, S. McKinley, and K. Goldberg.\nLearning ambidextrous robot grasping policies. Science Robotics, 4(26):eaau4984, 2019.\n[10] L. Pinto and A. Gupta. Supersizing self-supervision: Learning to grasp from 50k tries and 700\nrobot hours. In 2016 IEEE international conference on robotics and automation (ICRA), pages\n3406\u20133413. IEEE, 2016.\n[11] T.-Y. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays, P. Perona, D. Ramanan,\nC. L. Zitnick, and P. Doll\u00b4ar. Microsoft coco: Common objects in context, 2015.\n[12] A. Depierre, E. Dellandr\u00b4ea, and L. Chen. Jacquard: A large scale dataset for robotic grasp\ndetection.\nIn 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS), pages 3511\u20133516. IEEE, 2018.\n[13] I. Lenz, H. Lee, and A. Saxena. Deep learning for detecting robotic grasps. The International\nJournal of Robotics Research, 34(4-5):705\u2013724, 2015.\n[14] B. Calli, A. Singh, A. Walsman, S. Srinivasa, P. Abbeel, and A. M. Dollar. The ycb object and\nmodel set: Towards common benchmarks for manipulation research. In 2015 international\nconference on advanced robotics (ICAR), pages 510\u2013517. IEEE, 2015.\n[15] H.-S. Fang, C. Wang, M. Gou, and C. Lu. Graspnet-1billion: A large-scale benchmark for\ngeneral object grasping. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition, pages 11444\u201311453, 2020.\n[16] G. Li, V. Jampani, D. Sun, and L. Sevilla-Lara. Locate: Localize and transfer object parts for\nweakly supervised affordance grounding. In Proceedings of the IEEE International Conference\non Computer Vision, 2023.\n[17] A. Murali, W. Liu, K. Marino, S. Chernova, and A. Gupta. Same object, different grasps: Data\nand semantic knowledge for task-oriented grasping. In Conference on Robot Learning, 2020.\n11\n[18] M. Kokic, D. Kragic, and J. Bohg.\nLearning task-oriented grasping from human activity\ndatasets. IEEE Robotics and Automation Letters, 5(2):3352\u20133359, 2020.\n[19] D. Song, C. H. Ek, K. Huebner, and D. Kragic. Task-based robot grasp planning using proba-\nbilistic inference. IEEE transactions on robotics, 31(3):546\u2013561, 2015.\n[20] R. Detry, J. Papon, and L. Matthies. Task-oriented grasping with semantic and geometric scene\nunderstanding. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS), pages 3266\u20133273. IEEE, 2017.\n[21] A. Myers, C. L. Teo, C. Ferm\u00a8uller, and Y. Aloimonos. Affordance detection of tool parts\nfrom geometric features. In 2015 IEEE International Conference on Robotics and Automation\n(ICRA), pages 1374\u20131381. IEEE, 2015.\n[22] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,\nP. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from\nnatural language supervision, 2021.\n[23] M. Caron, H. Touvron, I. Misra, H. J\u00b4egou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging\nproperties in self-supervised vision transformers. In Proceedings of the IEEE/CVF interna-\ntional conference on computer vision, pages 9650\u20139660, 2021.\n[24] F.-J. Chu, R. Xu, and P. A. Vela. Learning affordance segmentation for real-world robotic\nmanipulation via synthetic images. IEEE Robotics and Automation Letters, 4(2):1140\u20131147,\n2019. doi:10.1109/LRA.2019.2894439.\n[25] N. Vahrenkamp, L. Westkamp, N. Yamanobe, E. E. Aksoy, and T. Asfour. Part-based grasp\nplanning for familiar objects. In 2016 IEEE-RAS 16th International Conference on Humanoid\nRobots (Humanoids), pages 919\u2013925, 2016. doi:10.1109/HUMANOIDS.2016.7803382.\n[26] Y. Song, P. Sun, Y. Ren, Y. Zheng, and Y. Zhang. Learning 6-dof fine-grained grasp detection\nbased on part affordance grounding. arXiv preprint arXiv:2301.11564, 2023.\n[27] J. Song, A. Tanwani, J. Ichnowski, M. Danielczuk, K. Sanders, J. Chui, J. A. Ojea, and\nK. Goldberg. Robust task-based grasping as a service. In 2020 IEEE 16th International Con-\nference on Automation Science and Engineering (CASE), pages 22\u201328. IEEE, 2020.\n[28] D. S. Hochbaum and V. Singh. An efficient algorithm for co-segmentation. In 2009 IEEE 12th\nInternational Conference on Computer Vision, pages 269\u2013276. IEEE, 2009.\n[29] Y. Chai, V. Lempitsky, and A. Zisserman. Bicos: A bi-level co-segmentation method for image\nclassification. In 2011 International conference on computer vision, pages 2579\u20132586. IEEE,\n2011.\n[30] S. Amir, Y. Gandelsman, S. Bagon, and T. Dekel. Deep vit features as dense visual descriptors.\nECCVW What is Motion For?, 2022.\n[31] P. Sun, S. Chen, C. Zhu, F. Xiao, P. Luo, S. Xie, and Z. Yan.\nGoing denser with open-\nvocabulary part segmentation. arXiv preprint arXiv:2305.11173, 2023.\n[32] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. NeRF:\nRepresenting scenes as neural radiance fields for view synthesis. In European Conference on\nComputer Vision, pages 405\u2013421. Springer, 2020.\n[33] M. Adamkiewicz, T. Chen, A. Caccavale, R. Gardner, P. Culbertson, J. Bohg, and M. Schwa-\nger. Vision-only robot navigation in a neural radiance world. IEEE Robotics and Automation\nLetters, 7(2):4606\u20134613, 2022.\n12\n[34] J. T. Barron, B. Mildenhall, M. Tancik, P. Hedman, R. Martin-Brualla, and P. P. Srinivasan.\nMip-nerf: A multiscale representation for anti-aliasing neural radiance fields. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision, pages 5855\u20135864, 2021.\n[35] J. T. Barron, B. Mildenhall, D. Verbin, P. P. Srinivasan, and P. Hedman. Mip-nerf 360: Un-\nbounded anti-aliased neural radiance fields. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 5470\u20135479, 2022.\n[36] L. Ma, X. Li, J. Liao, Q. Zhang, X. Wang, J. Wang, and P. V. Sander. Deblur-nerf: Neural\nradiance fields from blurry images. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 12861\u201312870, 2022.\n[37] X. Huang, Q. Zhang, Y. Feng, H. Li, X. Wang, and Q. Wang. Hdr-nerf: High dynamic range\nneural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 18398\u201318408, 2022.\n[38] S. Sabour, S. Vora, D. Duckworth, I. Krasin, D. J. Fleet, and A. Tagliasacchi. Robustnerf:\nIgnoring distractors with robust losses. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 20626\u201320636, 2023.\n[39] J. Philip and V. Deschaintre. Radiance field gradient scaling for unbiased near-camera training.\narXiv preprint arXiv:2305.02756, 2023.\n[40] M. Tancik, E. Weber, E. Ng, R. Li, B. Yi, J. Kerr, T. Wang, A. Kristoffersen, J. Austin,\nK. Salahi, et al. Nerfstudio: A modular framework for neural radiance field development.\narXiv preprint arXiv:2302.04264, 2023.\n[41] P. Wang, Y. Liu, Z. Chen, L. Liu, Z. Liu, T. Komura, C. Theobalt, and W. Wang. F2-nerf: Fast\nneural radiance field training with free camera trajectories. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 4150\u20134159, 2023.\n[42] J. T. Barron, B. Mildenhall, D. Verbin, P. P. Srinivasan, and P. Hedman. Zip-nerf: Anti-aliased\ngrid-based neural radiance fields. arXiv preprint arXiv:2304.06706, 2023.\n[43] T. M\u00a8uller, A. Evans, C. Schied, and A. Keller. Instant neural graphics primitives with a mul-\ntiresolution hash encoding. ACM Transactions on Graphics (ToG), 41(4):1\u201315, 2022.\n[44] A. Chen, Z. Xu, A. Geiger, J. Yu, and H. Su. Tensorf: Tensorial radiance fields. In Com-\nputer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022,\nProceedings, Part XXXII, pages 333\u2013350. Springer, 2022.\n[45] S. Fridovich-Keil, G. Meanti, F. R. Warburg, B. Recht, and A. Kanazawa. K-planes: Explicit\nradiance fields in space, time, and appearance. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 12479\u201312488, 2023.\n[46] A. Yu, S. Fridovich-Keil, M. Tancik, Q. Chen, B. Recht, and A. Kanazawa. Plenoxels: Radi-\nance fields without neural networks. arXiv preprint arXiv:2112.05131, 2021.\n[47] K. Park, U. Sinha, P. Hedman, J. T. Barron, S. Bouaziz, D. B. Goldman, R. Martin-Brualla, and\nS. M. Seitz. Hypernerf: A higher-dimensional representation for topologically varying neural\nradiance fields. arXiv preprint arXiv:2106.13228, 2021.\n[48] Z. Li, Q. Wang, F. Cole, R. Tucker, and N. Snavely. Dynibar: Neural dynamic image-based ren-\ndering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition, pages 4273\u20134284, 2023.\n[49] A. Pumarola, E. Corona, G. Pons-Moll, and F. Moreno-Noguer. D-NeRF: Neural Radiance\nFields for Dynamic Scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2020.\n13\n[50] Z. Zhu, S. Peng, V. Larsson, W. Xu, H. Bao, Z. Cui, M. R. Oswald, and M. Pollefeys. Nice-\nslam: Neural implicit scalable encoding for slam. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages 12786\u201312796, June 2022.\n[51] E. Sucar, S. Liu, J. Ortiz, and A. J. Davison. imap: Implicit mapping and positioning in real-\ntime. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),\npages 6229\u20136238, October 2021.\n[52] A. Rosinol, J. J. Leonard, and L. Carlone. Nerf-slam: Real-time dense monocular slam with\nneural radiance fields, 2022.\n[53] Y. Li, S. Li, V. Sitzmann, P. Agrawal, and A. Torralba. 3d neural scene representations for\nvisuomotor control. In Conference on Robot Learning, pages 112\u2013123. PMLR, 2022.\n[54] D. Driess, I. Schubert, P. Florence, Y. Li, and M. Toussaint. Reinforcement learning with neural\nradiance fields. In Advances in Neural Information Processing Systems (NeurIPS), 2022.\n[55] J. Kerr, L. Fu, H. Huang, Y. Avigal, M. Tancik, J. Ichnowski, A. Kanazawa, and K. Gold-\nberg. Evo-neRF: Evolving neRF for sequential robot grasping of transparent objects. In 6th\nAnnual Conference on Robot Learning, 2022. URL https://openreview.net/forum?id=\nBxr45keYrf.\n[56] J. Ichnowski*, Y. Avigal*, J. Kerr, and K. Goldberg. Dex-NeRF: Using a neural radiance field\nto grasp transparent objects. In Conference on Robot Learning (CoRL), 2020.\n[57] A. Byravan, J. Humplik, L. Hasenclever, A. Brussee, F. Nori, T. Haarnoja, B. Moran, S. Bohez,\nF. Sadeghi, B. Vujatovic, and N. Heess. Nerf2real: Sim2real transfer of vision-guided bipedal\nmotion skills using neural radiance fields, 2022.\n[58] S. Zhi, T. Laidlow, S. Leutenegger, and A. J. Davison. In-place scene labelling and under-\nstanding with implicit scene representation. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 15838\u201315847, 2021.\n[59] Y. Siddiqui, L. Porzi, S. R. Bul\u00b4o, N. M\u00a8uller, M. Nie\u00dfner, A. Dai, and P. Kontschieder. Panoptic\nlifting for 3d scene understanding with neural fields, 2022.\n[60] A. Kundu, K. Genova, X. Yin, A. Fathi, C. Pantofaru, L. Guibas, A. Tagliasacchi, F. Del-\nlaert, and T. Funkhouser. Panoptic Neural Fields: A Semantic Object-Aware Neural Scene\nRepresentation. In CVPR, 2022.\n[61] S. Kobayashi, E. Matsumoto, and V. Sitzmann. Decomposing nerf for editing via feature field\ndistillation. In NeurIPS, volume 35, 2022. URL https://arxiv.org/pdf/2205.15585.\npdf.\n[62] V. Tschernezki, I. Laina, D. Larlus, and A. Vedaldi. Neural feature fusion fields: 3d distillation\nof self-supervised 2d image representations. In Proceedings of the International Conference\non 3D Vision (3DV), 2022.\n[63] B. Li, K. Q. Weinberger, S. Belongie, V. Koltun, and R. Ranftl. Language-driven semantic\nsegmentation. arXiv preprint arXiv:2201.03546, 2022.\n[64] A. Zareian, K. D. Rosa, D. H. Hu, and S.-F. Chang. Open-vocabulary object detection using\ncaptions, 2021.\n[65] G. Ghiasi, X. Gu, Y. Cui, and T.-Y. Lin. Scaling open-vocabulary image segmentation with\nimage-level labels, 2022.\n[66] X. Gu, T.-Y. Lin, W. Kuo, and Y. Cui. Open-vocabulary object detection via vision and lan-\nguage knowledge distillation, 2022.\n14\n[67] F. Liang, B. Wu, X. Dai, K. Li, Y. Zhao, H. Zhang, P. Zhang, P. Vajda, and D. Marculescu.\nOpen-vocabulary semantic segmentation with mask-adapted clip, 2023.\n[68] X. Zhou, R. Girdhar, A. Joulin, P. Kr\u00a8ahenb\u00a8uhl, and I. Misra. Detecting twenty-thousand classes\nusing image-level supervision, 2022.\n[69] M. Minderer, A. Gritsenko, A. Stone, M. Neumann, D. Weissenborn, A. Dosovitskiy, A. Ma-\nhendran, A. Arnab, M. Dehghani, Z. Shen, X. Wang, X. Zhai, T. Kipf, and N. Houlsby. Simple\nopen-vocabulary object detection with vision transformers, 2022.\n[70] K. Jatavallabhula, A. Kuwajerwala, Q. Gu, M. Omama, T. Chen, S. Li, G. Iyer, S. Saryazdi,\nN. Keetha, A. Tewari, J. Tenenbaum, C. de Melo, M. Krishna, L. Paull, F. Shkurti, and A. Tor-\nralba. Conceptfusion: Open-set multimodal 3d mapping. arXiv, 2023.\n[71] C. Huang, O. Mees, A. Zeng, and W. Burgard. Visual language maps for robot navigation.\narXiv preprint arXiv:2210.05714, 2022.\n[72] S. Peng, K. Genova, C. Jiang, A. Tagliasacchi, M. Pollefeys, T. Funkhouser, et al. Openscene:\n3d scene understanding with open vocabularies. arXiv preprint arXiv:2211.15654, 2022.\n[73] G. Ghiasi, X. Gu, Y. Cui, and T.-Y. Lin. Open-vocabulary image segmentation. arXiv preprint\narXiv:2112.12143, 2021.\n[74] N. M. M. Shafiullah, C. Paxton, L. Pinto, S. Chintala, and A. Szlam. Clip-fields: Weakly\nsupervised semantic fields for robotic memory. arXiv preprint arXiv:2210.05663, 2022.\n[75] K. M. Jatavallabhula, A. Kuwajerwala, Q. Gu, M. Omama, T. Chen, S. Li, G. Iyer, S. Saryazdi,\nN. Keetha, A. Tewari, et al. Conceptfusion: Open-set multimodal 3d mapping. arXiv preprint\narXiv:2302.07241, 2023.\n[76] B. Chen, F. Xia, B. Ichter, K. Rao, K. Gopalakrishnan, M. S. Ryoo, A. Stone, and D. Kap-\npler. Open-vocabulary queryable scene representations for real world planning. arXiv preprint\narXiv:2209.09874, 2022.\n[77] H. Ha and S. Song.\nSemantic abstraction: Open-world 3D scene understanding from 2D\nvision-language models. In Proceedings of the 2022 Conference on Robot Learning, 2022.\n[78] H. Chefer, S. Gur, and L. Wolf. Generic attention-model explainability for interpreting bi-\nmodal and encoder-decoder transformers. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 397\u2013406, 2021.\n[79] O. Mees and W. Burgard. Composing pick-and-place tasks by grounding language, 2021.\n[80] L. Yu, Z. Lin, X. Shen, J. Yang, X. Lu, M. Bansal, and T. L. Berg. Mattnet: Modular attention\nnetwork for referring expression comprehension. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1307\u20131315, 2018.\n[81] M. Shridhar, L. Manuelli, and D. Fox. Cliport: What and where pathways for robotic manipu-\nlation. In Proceedings of the 5th Conference on Robot Learning (CoRL), 2021.\n[82] M. Shridhar, L. Manuelli, and D. Fox. Perceiver-actor: A multi-task transformer for robotic\nmanipulation. In Proceedings of the 6th Conference on Robot Learning (CoRL), 2022.\n[83] A. Stone, T. Xiao, Y. Lu, K. Gopalakrishnan, K.-H. Lee, Q. Vuong, P. Wohlhart, B. Zitkovich,\nF. Xia, C. Finn, et al. Open-world object manipulation using pre-trained vision-language mod-\nels. arXiv preprint arXiv:2303.00905, 2023.\n[84] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Haus-\nman, A. Herzog, J. Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv\npreprint arXiv:2212.06817, 2022.\n15\n[85] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haz-\niza, F. Massa, A. El-Nouby, et al. Dinov2: Learning robust visual features without supervision.\narXiv preprint arXiv:2304.07193, 2023.\n[86] N. Liu, S. Li, Y. Du, A. Torralba, and J. B. Tenenbaum. Compositional visual generation with\ncomposable diffusion models. In Computer Vision\u2013ECCV 2022: 17th European Conference,\nTel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XVII, pages 423\u2013439. Springer, 2022.\n[87] S. F. Bhat, R. Birkl, D. Wofk, P. Wonka, and M. M\u00a8uller. Zoedepth: Zero-shot transfer by\ncombining relative and metric depth. arXiv preprint arXiv:2302.12288, 2023.\n[88] Guangcong, Z. Chen, C. C. Loy, and Z. Liu. Sparsenerf: Distilling depth ranking for few-shot\nnovel view synthesis. Technical Report, 2023.\n[89] M. Niemeyer, J. T. Barron, B. Mildenhall, M. S. M. Sajjadi, A. Geiger, and N. Radwan. Reg-\nnerf: Regularizing neural radiance fields for view synthesis from sparse inputs, 2021.\n16\nA\nLERF-TOGO\nA.1\nImplementation Details\nWe implement LERF-TOGO on top of the Nerfacto method from Nerfstudio [40]. For faster con-\nvergence and smoother optimization, we modify several parameters from the original LERF paper.\nWe use a smaller hashgrid with 16 levels and a maximum resolution of 256, and find that using\nlarger MLPs for the density, color, and transient output heads of NeRF results in faster convergence\nand better ability to handle specularities and robot shadows. We introduce weight decay of 1e-7 to\nthe LERF network which smooths training. In addition, we compress the DINO embeddings into\n128 dimensions and supervise on these vectors rather than the original DINO outputs, but we do\nnot normalize the resulting vectors like Tschernezki et al. [62]. While constructing the CLIP em-\nbedding pyramid for LERF, we use crops ranging from 5% to 35% of image height with 6 pyramid\nlevels, biasing the pyramid to smaller crops as LERF-TOGO is primarily interested in object and\npart queries.\nB\nRobot Capture\nRobot Capture Region Size The robot captures the scene along a hemispherical trajectory arcing\n\u00b1100\u25e6 around the workspace horizontally (\u201c1/2 hemisphere\u201d in Fig. B.) When this horizontal sweep\nangle is reduced to a fraction of the range, the quality of the 3D object mask degrades, sometimes\nselecting the incorrect object altogether. LERF\u2019s semantic field is supervised on features of the scene\nimages, thus the quality is heavily correlated with the distribution of images viewing the object. This\nlowers the quality of the 3D DINO embeddings used for the mask generation.\nLERF Training Steps In our experiments the LERF scene representation is trained to 2k steps. As\nshown in Fig. B, objects/object parts (e.g., \u201cspray nozzle\u201d, \u201cbottle\u201d) can be detected in steps as low\nas 1k, but more fine-grained or smaller parts (e.g., \u201chandle\u201d) may take longer (2-3k steps). This is\nconsistent with what LERF reports: \u201dfine-grained features take more steps [to emerge]\u201d.\n3D Object Extraction Given the initial 3D point with the highest LERF activation, we create an\nobject-centric point cloud by rendering six different views looking at the 3D coordinate. The views\nare \u00b190\u25e6 around the upwards vector through the 3D point. For DINO floodfill, the threshold DINO\nsimilarity is defined as first projecting the current DINO embedding onto the first PCA component\nof the top-down image, then taking the L2 norm of the difference between the current embedding\nand the DINO embedding at the initial 3D point.\nNeRF Regularization NeRF encounters difficulties in reconstructing texture-less planar surfaces,\nespecially in the presence of specularities. This limitation is prominent in our table-top scenes,\nwhere the glossy surface and metallic objects can result in depth renderings with jagged missing\nregions. These missing regions can cause LERF renderings to spuriously activate and degrade the\nperformance of grasp networks, so we apply depth regularization to mitigate this issue. We adopt\nthe local depth ranking loss proposed in SparseNeRF [88] and use ZoeDepth [87] as the underlying\ndepth model. We found this performs better than smoothness priors [89, 46] because it retains\nmore fine-grained geometry. Additionally, we use the gradient scaling approach from Philip and\nDeschaintre [39], which significantly reduces the number of near-camera floaters and enables more\nrobust grasping directly from point clouds rendered from the NeRF.\nPoses obtained from cameras in motion are slightly inaccurate, which we found could result in\noversmoothed geometry with depth regularization. To overcome this, we optimize the NeRF for the\nfirst 500 steps without any regularization to allow the camera poses to settle, then anneal the depth\nregularization loss term from 0 to 100% over the next 1500 steps. Interestingly, we find staged\ntraining not only preserves thin features better but also speeds LERF optimization. We hypothesize\nthis is because supervising the language field on un-converged density in free space results in a poor\nnetwork initialization, while beginning LERF optimization after geometry has been largely removed\nfrom free space allows a smoother learning signal.\n17\nFigure 7: Left: Decreasing the robot scan region degrades the quality of 3D relevancy map generated\nby LERF-TOGO. Right: LERF-TOGO relevancy map converges earlier for larger objects and parts.\nFigure 8: In the case of identical objects or ambiguous object queries, LERF-TOGO picks a single\nobject in the scene but does not propose grasps for all the objects in the category.\nC\nGrasping\nPoint Cloud Extraction To extract a scene-wide point cloud for grasping, we use the method in\nNerfstudio [40], which deprojects randomly sampled rays\u2019 depth from the train camera poses, then\nfilters with outlier rejection. We then crop the point cloud to the workspace of the robot. For object\ncentric point clouds, we deproject depth from views radially surrounding the object of interest.\nMotion Planning A grasp is considered feasible if the robot can perform a collision-free trajectory\nwith the following poses: the pre-grasp, grasp, and post-grasp configurations. The pre-grasp pose is\npositioned 5cm along the z-axis of the robot end effector, which allows the gripper to approach the\ntarget grasp pose with minimal additional motion. The post-grasp pose is located 10cm above the\ngrasp pose, along the z-axis of the world frame. The UR5\u2019s IK solver calculates the set of viable\njoint configurations at these poses, and we calculate the trajectory as a linear interpolation between\nthem. We additionally allow for a 180 degree rotation at the last wrist joint, as parallel-jaw grasps are\nrotationally symmetric. This facilitates the motion planning process, as the robot\u2019s camera mount is\nprone to colliding with the robot arm.\nD\nExperiments\nD.1\nSetup Details\nWe use a UR5 arm with a Logitech BRIO webcam at 1600x896 resolution, with all camera settings\nfrozen before each capture prevent color discrepancies among images. The camera mount points\northogonally to the gripper axis, to maximize the reachable workspace of the camera while pointing\ntowards the workspace center. During robot capture, pre-computation of DINO, CLIP, and ZoeDepth\nis parallelized across 3 NVIDIA 4090 GPUs to achieve real-time performance, and all subsequent\noperations are carried out on a single 4090. Capturing a scene takes 30 seconds, training the LERF\nto 2k iterations takes 78 seconds, and finally querying LERF-TOGO takes 10 seconds.\nD.2\nLLM Interface\nWe provide the full prompt to the LLM below. For any given task and scene the OBJECT LIST is\nreplaced with a list of objects within the scene and TASK is replaced with the desired task:\n18\nFigure 9: Grasp score weighting: Varying weight between geometric grasp score and semantic\ngrasp score shifts the grasp distribution. A high semantic grasp weight (w = 0.95) is required, since\ngeometric grasps may be biased away from small and fine-grained object parts of interest. Both\ngeometric and semantic scores are in the range [0, 1].\nAnswer the question as if you are a robot with a parallel jaw gripper that has\naccess to only the objects in the object list. Follow the exact format.\nFirst line should describe what basic action is needed to do the task from\nthe following set of actions: press, grasp, twist, pick & place.\nThe second line should only be an object from the object list followed by 1 object\npart that the robot would touch to do this task. VERY IMPORTANT: If the basic\naction is pick & place, only then have a third line with \u2019Place: \u2019\nto specify the object to place on. \\\nObject list: [\u2019pot\u2019, \u2019knife\u2019, \u2019spoon\u2019, \u2019black pan\u2019] \\\nQ: How can I safely pick up a pan? \\\nBasic Action: grasp \\\nSequence: 1. black pan 2. handle \\\n\\\nObject list: [\u2019mechanical keyboard\u2019, \u2019knife\u2019, \u2019TV\u2019, \u2019camera\u2019] \\\nQ: How can I safely hit the spacebar on a keyboard? \\\nBasic action: press \\\nSequence: 1. mechanical keyboard 2. spacebar\\\n\\\nObject list: [\u2019green mug\u2019, \u2019blue spoon\u2019, \u2019fork\u2019, \u2019knife\u2019] \\\nQ: How can I cut a block of cheese? \\\nBasic action: grasp \\\nSequence: 1. knife 2. handle \\\n\\\nObject list: [\u2019salt shaker\u2019, \u2019knife\u2019, \u2019fork\u2019, \u2019white pan\u2019] \\\nQ: How can I safely lift a salt shaker? \\\nBasic Action: grasp \\\nSequence: 1. salt shaker 2. base \\\n\\\nObject list: [\u2019red cup\u2019, \u2019blue cup\u2019, \u2019mug\u2019, \u2019bowl\u2019] \\\nQ: How do I stack the red cup on the blue cup? \\\nBasic action: pick & place \\\nSequence: 1. red cup 2. rim \\\nPlace: blue cup \\\n\\\nObject list: [\u2019door knob\u2019, \u2019black mug\u2019, \u2019green dish brush\u2019, \u2019shiny knife\u2019] \\\nQ: How do I open a door knob? \\\n19\nScene\nObject Query ; Part Query\nKitchen\n(black matte spoon, handle), (shiny black spoon, handle), (teapot, handle),\n(dish scrub brush, handle), (dust brush, handle)\nFlowers\n(daisy, plant stem), (rose, plant stem)\nMugs\n(blue mug, handle), (pink teacup, handle), (turqouise mug, handle),\n(white mug, handle), (black mug, handle)\nTools\n(Measuring tape, base), (screwdriver, handle), (wire cutters, handle)\n(soldering iron, handle), (hammer, handle)\nKnives\n(bread knife, handle), (steak knife, handle), (box cutter, handle)\nMartinis\n(red martini glass, stem), (grey martini glass, stem)\nFragile\n(camera, strap), (pink sunglasses, earhooks)\n(blue sunglasses, earhooks), (lightbulb, screw)\nCords\n(power strip, plug), (power strip, base), (ethernet dongle, usb)\n(ethernet dongle, ethernet)\nMessy\n(ice cream, cone), (green lollipop, stick), (blue lollipop, stick)\nPasta\n(wine, cork), (wine, bottle neck), (saucepan, lid knob)\n(saucepan, handle), (corkscrew, handle)\nCleaning\n(clorox, wet towel), (clorox, lid), (clorox, body)\n(tissue box, box), (tissue box, tissue)\nBottles\n(meyer\u2019s cleaning spray, spray trigger), (meyer\u2019s cleaning spray, bottle neck)\n(meyer\u2019s cleaning spray, body) (purple cleaning spray, body)\n(purple cleaning spray, spray trigger), (purple cleaning spray, bottle neck)\nElectronics\n(e-stop, red button), (computer mouse, scroll wheel), (controller, buttons),\n(spacemouse, button), (white controller; button)\n(controller, joystick), (e-stop, red button)\nTable 4: Complete list of object and part queries\nBasic action: twist \\\nSequence: 1. door knob 2. rim \\\n\\\nObject list: [\u2019dryer\u2019, \u2019washing machine\u2019, \u2019sunglasses\u2019] \\\nQ: How do I turn on the washing machine? \\\nBasic action: twist \\\nSequence: 1. washing machine 2. dial \\\n\\\nObject list: [\u2019paper towel roll\u2019, \u2019mug\u2019, \u2019teacup\u2019, \u2019headphones\u2019, \u2019pen\u2019] \\\nQ: How do I grab a paper towel? \\\nBasic action: grasp \\\nSequence: 1. paper towel roll 2. paper towel \\\n\\\nObject list: [\u2019magnifying glass\u2019, \u2019blue spoon\u2019, \u2019fork\u2019, \u2019knife\u2019] \\\nQ: How do I pick up a magnifying glass? \\\nBasic action: grasp \\\nSequence: 1. magnifying glass 2. handle \\\n\\\nObject list: [\u2019teddy bear\u2019, \u2019toy block\u2019, \u2019mouse\u2019, \u2019saucepan\u2019, \u2019hammer\u2019] \\\nQ: How do I grab a teddy bear? \\\nBasic action: grasp \\\nSequence: 1. teddy bear 2. head \\\n20\nScene\nTasks\nKitchen\n\u2019grab the matte spoon\u2019, \u2019pass the shiny spoon\u2019, \u2019grab the teapot\u2019,\n\u2019scrub the dishes\u2019, \u2019dust the book\u2019\nFlowers\n\u2019give the daisy\u2019, \u2019give the rose\u2019\nMugs\n\u2019grab the blue mug\u2019, \u2019grab the pink teacup\u2019, \u2019grab the turquoise mug\u2019\n\u2019grab the white mug\u2019, \u2019grab the black mug\u2019\nTools\n\u2019grab the measuring tape\u2019, \u2019give the screwdriver\u2019, \u2019cut the wire\u2019\n\u2019hold the soldering iron\u2019, \u2019swing the hammer\u2019\nKnives\n\u2019cut the bread\u2019, \u2019cut the steak\u2019, \u2019cut the box\u2019\nMartinis\n\u2019deliver the grey martini glass\u2019, \u2019deliver the red martini glass\u2019\nFragile\n\u2019take a picture\u2019, \u2019wear the pink sunglasses\u2019\n\u2019wear the blue sunglasses\u2019, \u2019move the lightbulb\u2019\nCords\n\u2019grab the power strip\u2019, \u2019plug in the power strip\u2019, \u2019plug the ethernet into the ethernet dongle\u2019,\n\u2019plug in the usb into the ethernet port\u2019\nMessy\n\u2019eat the ice cream\u2019, \u2019eat the green lollipop\u2019, \u2019eat the blue lollipop\u2019\nPasta\n\u2019uncork the wine\u2019, \u2019grab the saucepan\u2019, \u2019open the saucepan\u2019,\n\u2019grab the corkscrew\u2019, \u2019lift the wine\u2019\nCleaning\n\u2019grab a wet towel from the clorox\u2019, \u2019close the box of clorox\u2019\n\u2019hold the clorox\u2019, \u2019get a tissue\u2019, \u2019hold the tissue box\u2019\nBottles\n\u2019grab the meyers cleaning spray\u2019, \u2019open the meyers cleaning spray\u2019,\n\u2019spray the meyers cleaning spray\u2019, \u2019grab the purple cleaning spray\u2019,\n\u2019open the purple cleaning spray\u2019, \u2019spray the purple cleaning spray\u2019\nElectronics\n\u2019hold down the e-stop\u2019, \u2019hit the scroll wheel\u2019, \u2019click the controller\u2019,\n\u2019click on that website with the spacemouse\u2019, \u2019click the white controller\u2019,\n\u2019turn video game character\u2019, \u2019release the e-stop\u2019\nTable 5: Complete list of tasks for each scene\n\\\nObject list: [\u2019green mug\u2019, \u2019blue spoon\u2019, \u2019fork\u2019, \u2019knife\u2019] \\\nQ: How do I put the mug in the cabinet? \\\nBasic action: pick & place \\\nSequence: 1. green mug 2. handle \\\nPlace: cabinet \\\n\\\nObject list: {OBJECT_LIST} \\\nQ: How can I safely {TASK}? \\\nBasic action: \"\n21\nFigure 10: Semantic Abstraction results for object and object part localization (cont.)\n22\nFigure 11: Semantic Abstraction results for object and object part localization\n23\nFigure 12: OWL-ViT results for object and object part localization\n24\n"
  },
  {
    "title": "Retrieval-Augmented Text-to-Audio Generation",
    "link": "https://arxiv.org/pdf/2309.08051.pdf",
    "upvote": "5",
    "text": "RETRIEVAL-AUGMENTED TEXT-TO-AUDIO GENERATION\nYi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D. Plumbley, Wenwu Wang\nSchool of Computer Science and Electronic Engineering, University of Surrey, UK\nABSTRACT\nDespite recent progress in text-to-audio (TTA) generation, we show\nthat the state-of-the-art models, such as AudioLDM, trained on\ndatasets with an imbalanced class distribution, such as AudioCaps,\nare biased in their generation performance. Specifically, they excel in\ngenerating common audio classes while underperforming in the rare\nones, thus degrading the overall generation performance. We refer to\nthis problem as long-tailed text-to-audio generation. To address this\nissue, we propose a simple retrieval-augmented approach for TTA\nmodels. Specifically, given an input text prompt, we first leverage a\nContrastive Language Audio Pretraining (CLAP) model to retrieve\nrelevant text-audio pairs. The features of the retrieved audio-text data\nare then used as additional conditions to guide the learning of TTA\nmodels. We enhance AudioLDM with our proposed approach and\ndenote the resulting augmented system as Re-AudioLDM. On the\nAudioCaps dataset, Re-AudioLDM achieves a state-of-the-art Frechet\nAudio Distance (FAD) of 1.37, outperforming the existing approaches\nby a large margin. Furthermore, we show that Re-AudioLDM can\ngenerate realistic audio for complex scenes, rare audio classes, and\neven unseen audio types, indicating its potential in TTA tasks.\nIndex Terms\u2014 Audio generation, retrieval-information, diffu-\nsion model, deep learning, long tail problem\n1. INTRODUCTION\nThe landscape of text-to-audio (TTA) generation has been revolution-\nized by advancements in diffusion-based generative modelling [1, 2,\n3]. Leveraging powerful backbone models such as CLAP [1] and\nlarge language model (LLM) [4], these models are capable of extract-\ning semantic information and enabling the creation of high-fidelity\naudio from textual descriptions.\nIn this work, we show that due to the scarcity and diversity of\naudio training data, bias appears in these state-of-the-art models,\nleading to significant performance degradation. Figure 1 (top) draws\na statistical analysis conducted on the 327 labels of AudioCaps [5],\none of the largest audio-text datasets, indicating a notable imbalance\nin data distribution. The bottom-left graph of Figure 1 shows a sample\nresult of the state-of-the-art model trained with AudioCaps, when\ngiving the prompt \u201cA man is talking then pops the champagne and\nlaughs\u201d, the model could only generate the content for \u201cman talking\u201d,\nbut miss uncommon or complex events such as\u201cchampagne popped\u201d\nthen followed by \u201claugh\u201d. Hence, an inherent limitation is seen\ndue to the constrained scope and variability of the training dataset,\nwhere the quality of generated sounds seems largely correlated with\ntheir frequency of appearance during training. In this regard, these\nmodels can faithfully generate realistic audio clips for common sound\nevents, but they may generate incorrect or irrelevant audio clips when\nencountering less frequent or unseen sound events.\nWe denote this the long-tailed text-to-audio generation problem,\nwhich influences the model performance in diversity and restricts\nFig. 1. The long-tailed problem in AudioCaps dataset (top). Example\naudio clips (bottom) generated with the baseline model (left) and\nRe-AudioLDM (right).\nthe applicability of these models, especially in real-world scenarios.\nOur motivation is to develop a robust TTA framework that breaks\nthe barrier of imbalanced data and achieves realistic generation on\ndiverse acoustic entities.\nWe propose a novel retrieval-augmented TTA framework to ad-\ndress the long-tailed generation issue. We enhance the state-of-the-art\nTTA model, AudioLDM [1], with a retrieval module, dubbed Re-\nAudioLDM. Specifically, we first use the input text prompt to retrieve\nrelevant references (e.g., text-audio pairs) from dataset (e.g., Au-\ndioCaps), and then use a pre-trained audio model and a language\nmodel to extract the acoustic and textual features, respectively. These\nextracted features are then further given to the cross-attention [6]\nmodule of the LDM to guide the generation process. The retrieved\naudio-text pairs serve as supplementary information that helps im-\nprove the modelling of low-frequency audio events in the training\nstage. In the inference stage, the retrieval-augmented strategy also\nprovides references in relation to the text prompt, ensuring a more\naccurate and faithful audio generation result.\nWe perform extensive experiments on events with different fre-\nquencies of occurrence in the dataset. We show that Re-AudioLDM\nprovides a stable performance among a variety of audio entities. It\nsignificantly improves the performance for tail classes over the base-\nline models, demonstrating that it can provide effective alleviation\nfor long-tail TTA issues. Furthermore, as compared with the baseline\nmodels, Re-AudioLDM is capable of generating more realistic and\ncomplex audio clips, including rare, complex, or even unseen audio\nevents. As the example with the same prompt shown in Figure 1\n(bottom), where Re-AudioLDM (bottom-right) can generate both\nuncommon entities \u201cchampagne popped\u201d with a complex structure\nfollowed with the sound of \u201claugh\u201d, achieving a better result than\nthe baseline models with all the required entities and semantic orders\ncorrectly. In addition, Re-AudioLDM achieves an FAD score of 1.37,\noutperforming state-of-the-art TTA models by a large margin.\nThe remainder of this paper is organized as follows. Section 2\narXiv:2309.08051v2  [cs.SD]  5 Jan 2024\n\u201cA bottle of champagne \nis popped and then \npoured into a glass\u201d\nInput prompt\nOutput Waveform\nCLAP\nEncoder\nDatabase\nAudio \nFeature\nLanguage\nFeature\n\u201cSome water \npure into the \nglass\u201d\n\u201cWater pure \ninto the glass\u201d\n\u201cA champagne \nis popped \nwhile a man \ntalks\u201d\nVAE\nDecoder\nHiFi-GAN\nRetrieval\nAudioMAE\nT5\nAudio & \nLanguage \nFeature\nLDM\nCross Attention\nFig. 2. The overview structure of Re-AudioLDM\nintroduces the related works of audio generation and retrieval-based\nmodels, followed by the details of Re-AudioLDM in Section 3. Sec-\ntion 4 presents the experimental setting and Section 5 shows the\nresults and ablation studies. Conclusions are given in Section 6.\n2. RELATED WORK\nOur work relates to two major works, diffusion-based text-to-audio\nmodels and retrieval-based generative models. These two fields are\nbriefly discussed in the following subsections.\n2.1. Audio Generation\nRecent works on audio generation follow an encoder-decoder frame-\nwork [1, 7]. The model first uses an encoder to encode the informa-\ntion into a latent representation, which can be decompressed into a\nmel-spectrogram feature. The decoder then uses a variational autoen-\ncoder (VAE) and a generative adversarial network (GAN) vocoder\nto turn such features into waveforms. Liu et al. [8] has used Pix-\nelSNAIL [9] as the encoder to represent labels while Iashin and\nRahtu [10] applied a GPT2 [11] as the encoder to encode input im-\nages. Subsequently, diffusion-based models have been used for latent\ntoken generation. Yang et al. [12] replaces the transformer-based\nencoder with a diffusion-based encoder. Liu et al. [1] uses the CLAP\nmodel [13] to obtain embeddings for the input data (audio or text),\nand uses the Latent Diffusion Model (LDM) as the token generator.\nGhosal et al. [4] then further improves this framework by replacing\nCLAP with LLM [14].\n2.2. Retrieved Information Aggregation\nSeveral studies in the field of image generation have considered\nleveraging retrieved information. Li et al. [15] extract image features\nfrom a training set, and place them in a memory bank which is then\nused as a parallel input condition for audio generation. Blattmannet\net al. [16] present a nearest-neighbours strategy to select related\nimage samples from a neighbourhood area. The KNN-Diffusion [17]\nuses image features obtained from large-scale retrieval databases\nduring the inference stage to perform new-domain image generation.\nChen et al. [18] extend the image-only retrieval into image-text pair\nretrieval, augmenting both high-level semantics and low-level visual\ninformation for a diffusion model. In contrast, no similar works\nhave been done for audio generation, and Re-AudioLDM is the first\nattempt to introduce retrieved information from a dataset to improve\nthe text-to-audio generation performance.\n3. PROPOSED METHOD\nSimilar to previous audio generation works [1, 4, 19], Re-AudioLDM\nis a cascaded model including three parts: input embedding, diffusion-\nbased feature generator, and a pipeline to reconstruct the waveform\nfrom the latent feature.\n3.1. Text and Retrieval Embedding Encoder\nRe-AudioLDM takes two paralleled inputs: a text input ct as low-\nlevel semantic information, and a set of text-audio pairs as retrieval\naugmentation cr for high-level semantic-audio information. The text\nembedding Et is obtained as:\nEt = fclap(ct)\n(1)\nwhich fclap(\u00b7) is the CLAP model [20] used for text encoding, as in\nAudioLDM [1]. The retrieved information cr = [< text1, audio1 >\n, < text2, audio2 >, ..., < textk, audiok >] are the top-k neighbours\nselected through the similarity comparison between the embedding\nof the target caption and those of the retrieval dataset. Here for\neach pair, the multi-modal embedding is divided into two groups\nof concatenation, presented as audio retrieval Era and text retrieval\nErt, encoded as:\nEra = CAT[fmae(audio1), ..., fmae(audiok)],\n(2)\nErt = CAT[ft5(text1), ..., ft5(textk)]\n(3)\nwhere ft5(\u00b7) is a pre-trained T5 model [14] for obtaining the text\nembedding, and fmae(\u00b7) is a pre-trained AudioMAE model [21] for\nobtaining the embedding of the paired audio.\n3.2. Retrieval-Augmented Diffusion Generator\nRe-AudioLDM uses LDM as the generator to obtain the intermediate\nlatent token of the target audio. The diffusion model involves two\nprocesses, a forward process to gradually add noise into the latent\nvectors and a reverse process to progressively predict the transition\nnoise of the latent vector in each step. During the forward step,\nthe latent representation z0 is transformed into a standard Gaussian\ndistribution zn with a continuous noise injection:\nq(zn|zn\u22121) = N(zn;\np\n1 \u2212 \u03b2nzn\u22121, \u03b2nI),\n(4)\nq(zn|z0) = N(zn; \u221a\u00af\u03b1nz0, (1 \u2212 \u00af\u03b1n)\u03f5)\n(5)\nwhere \u03f5 denotes the Gaussian noise with \u03b1n = 1 \u2212 \u03b2n controlling\nthe noise level. In the reverse process, LDM learns to estimate the\ndistribution of noise \u03f5\u03b8 in the latent space, given conditions from the\ntext embedding Et, calculated with equation (1), and the retrieved\nembedding Era and Ert, calculated with equation (2) and (3), re-\nspectively. The LDM model applies UNet as the general structure,\nwhere the input layer takes the noisy latent vector zn, text embedding\nEt, and the time step n as the condition. Then the retrieved infor-\nmation of both text and audio is shared with all the cross-attention\nblocks within the remaining layers. Employing a re-weighted training\nobjective [22], LDM is trained by:\nLn(\u03b8) = Ez0,\u03f5,n\n\r\r\u03f5 \u2212 \u03f5\u03b8(zn, n, Et, Attn(Era, Ert))\n\r\r2\n2\n(6)\n3.3. VAE Decoder & Hifi-GAN Vocoder\nRe-AudioLDM utilizes a combination of a VAE and a HiFi-GAN\nas the general pipeline for reconstructing waveform from the latent\nfeature tokens. During the training stage, VAE learns to encode the\nmel-spectrogram into the intermediate representation and then decode\nit back to mel-spectrogram, while Hifi-GAN is trained to convert mel-\nspectrogram into waveform. For inference, Re-AudioLDM applies\nthe VAE decoder for mel-spectrogram reconstruction and HiFi-GAN\nfor waveform generation.\n4. EXPERIMENTS\n4.1. Datasets\nWe use the AudioCaps dataset [25] for the experiments, which com-\nprises 46, 000 ten-second audio clips, each paired with a human-\nannotated caption. We follow the official training-testing split, where\neach training audio clip is assigned a single caption, while in the\ntesting split, each audio clip is annotated with five captions. During\nthe inference stage, we employ the first caption of each audio clip that\nappears in the test split as the text input. The remaining four captions\nare used only for the ablation study in Section 5.\n4.2. Experiment Setup\nData Preparation. For a retrieval-based AudioCaps dataset, we\napply a CLAP-score based retrieval function to find the top-50 nearest\nneighbours of the target text embedding. The waveform and the text\nfrom each neighbour are stored as a text-audio pair. It is noted that for\nboth training and testing samples, the target sample is excluded from\nthe retrieval information, which can avoid any access to the target\ndata during both the training and inferencing stages.\nImplementation Detail. As a cascaded model, the encoder and\ndecoder parts of Re-AudioLDM are trained separately with audio\nclips sampled at 16 kHz. For the target, we use the short-time Fourier\ntransform (STFT) with a window of 1024 samples and a hop size of\n160 samples, resulting in a mel-spectrogram with 64 mel-filterbanks.\nThen, a VAE model is applied to compress the spectrogram with a\nratio of 4, resulting in a feature vector with a frequency dimension\nof 16. For the information provided by the retrieval strategy, the\ntext feature is directly extracted by a pre-trained T5-medium model,\npresenting a fixed sequence length of 50. The audio feature, on the\nother hand, is first converted into filter banks with 128 mel-bins and\nthen processed by a pre-trained AudioMAE model, leading to a vector\nof dimension 32.\nTraining Detail. The LDM is optimized with a learning rate of\n5.0 \u00d7 10\u22125. Re-AudioLDM is trained for up to 80 epochs with\na batch size of 4 and the evaluation is carried out every 100, 000\nsteps. Re-AudioLDM-S applies a UNet architecture consisting of\n128 channels, while we enlarge the model into Re-AudioLDM-L with\n196 channels for experiments on more complex models.\nEvaluation Metrics. Following Liu et al., we use the Inception\nScore (IS), Fr\u00b4echet Audio Distance (FAD), and Kullback\u2013Leibler (KL)\ndivergence to evaluate the performance of Re-AudioLDM. A higher\nIS score indicates a larger variety in the generated audio, while\nlower KL and FAD scores indicate better audio quality. For the\nsemantic-level evaluation, we calculate the cosine similarity between\nthe output audio embedding and the target text embedding calculated\nby the CLAP encoders, which demonstrates the correlation between\naudio and text.\n5. RESULTS\n5.1. Evaluation Results\nThe experiments are carried out on AudioCaps evaluation set. We\ncompare the performance with several state-of-the-art frameworks,\nincluding AudioGen [23], AudioLDM [1] and Tango [4]. Selecting\nonly the first caption of each audio clip as the text description, each\nframework infers 975 10-second audio clips with a sampling rate of\n16 kHz. Table 1 compares the metrics achieved with different text-\nto-audio models, where Re-AudioLDM outperforms other methods\nby a large margin. It is noted that without the additional information\nprovided by retrieval, Re-AudioLDM does not exhibit any advantage\nand is generally inferior to Tango, the current state-of-the-art model\non AudioCaps. However, upon incorporating retrieval information,\nRe-AudioLDM successfully outperformed the baseline models in all\nfour evaluation metrics. By enlarging the size of the hidden layer\nin LDM structure, the Re-AudioLDM-L using 10 retrieved pairs\nfurther decreases the FAD score to below 1.4, which is a significant\nimprovement over the baseline frameworks.\n5.2. Ablation Study\nRetrieval Type. Experiments in Table 1 show the results on different\nretrieval information, e.g. audio, text, or neither. With the audio fea-\ntures extracted by AudioMAE, only a slight improvement is achieved\nby Re-AudioLDM, mainly because Re-AudioLDM misses the rela-\ntionship between sound events, although it captures the features of\nrelated sound events. By adding the paired text information of each\nretrieved audio clip, Re-AudioLDM learns the relationship between\naudio features and high-level semantic information, contributing a\nsignificant improvement in capturing highly related semantic features\nfor the sound events.\nNumber of Retrieved Pairs. Several experiments are carried out to\nassess the impact of the number of retrieved audio-text pairs on audio\ngeneration performance. As depicted in Figure 3, the incorporation\nof retrieval information improves the performance, as the number\nof retrieved pairs increases, while such improvement slows down\nafter the number reaches five and it becomes flattened at around\nten. Therefore, in order to strike a balance between training costs\nand model performance, the number of retrieved pairs is chosen\nempirically to be in the range of 3 to 5 for this data.\nLong-Tailed Situations. Re-AudioLDM aims to tackle the long-\ntailed generation problem and generate more realistic audio clips on\nuncommon or unseen sound events. In order to evaluate the accuracy\nof each generated audio clip, we applied the CLAP score [20] to\nshow the relationship between the audio clip and text description. We\nfirst calculate the frequency of the occurrence of each sound event\nby counting the label of each audio clip and then illustrate the model\nperformance by averaging the CLAP score of each sound class for\nModel\nDataset\nRetrieval Info\nRetrieval Number\nKL \u2193\nIS \u2191\nFAD \u2193\nCLAPscore(%)\u2191\nAudioGen [23]\nAC+AS+8 others\n%\n%\n1.69\n5.13\n2.15\n23.44\nAudioLDM [1]\nAC+AS+2 others\n%\n%\n1.66\n6.51\n2.08\n25.39\nTango [4]\nAudioCaps\n%\n%\n1.32\n6.45\n1.68\n29.28\nRe-AudioLDM-S\nAudioCaps\n%\n%\n1.63\n6.48\n2.31\n26.75\nAudioCaps\nAudio\n3\n1.54\n6.88\n1.95\n31.05\nAudioCaps\nAudio & Text\n3\n1.27\n7.31\n1.48\n37.07\nAudioCaps\nAudio & Text\n10\n1.23\n7.33\n1.40\n37.15\nRe-AudioLDM-L\nAudioCaps\nAudio & Text\n10\n1.20\n7.39\n1.37\n37.12\nTable 1. The comparison between different frameworks, with and without retrieval information. AC and AS are short for AudioCaps [5] and\nAudioSet [24] respectively.\nFig. 3. Performance comparison on numbers of retrieved information,\nwhere AC cap 1-5 refers to the caption groups of the testing set.\nthe AudioCaps testing set. The bar chart on the left side of Figure 4\npresents a statistical analysis of the quantities of all 327 sound event\nclasses in the AudioCaps training set. Similar to Figure 1 (top), tail\nclasses constitute a significant portion, especially in the label group\nof 1 and 10. Figure 4 (right) shows the performance of each model\non the events with different frequencies of event occurrence within\nthe training set. Despite the initial gap in highly frequent audio\nevents between Re-AudioLDM and baseline models, the baseline\nmodels perform worse when handling tailed entities. However, Re-\nAudioLDM has demonstrated more stable results, with a decrease of\nless than 3 in the CLAP score as the frequency of event occurrence\nis reduced in training data. Hence, Re-AudioLDM can reduce the\ndegradation of output quality when generating tailed sound events,\nenhancing the overall model performance.\nZero-Shot Generation. For experiments on unseen entities, we eval-\nuate several scenarios with events that are excluded during training.\nIn Figure 4 (right), we found that baseline models show performance\ndegradation on generating unseen audio (zero frequency occurrence).\nThis may be because the model has not learned the features of unseen\nentities, while Re-AudioLDM can still achieve realistic results by pro-\nviding related audio and semantic information. Hence, with essential\nretrieval information, Re-AudioLDM has the potential to generate\nsounds which are excluded from training data. The retrieval-based\ngeneration may significantly enhance the robustness of zero-shot\ntasks, which is one of the directions we will explore in the future.\nComparison with Mixup Strategy. Another way to address the class\nimbalance problem is to use a mixup strategy [26]. While mixup\ncan increase the occurrence frequency for the tail entities, it also\nintroduces more complex audio examples, as well as the synthetic\naudio data that may not align with real-world distributions. The\nresults in [1] have shown that the mixup strategy leads to degradation\nin overall performance. In contrast to the mixup method, our proposed\nretrieval augmentation strategy reduces the complexity of the training\nprocesses, resulting in an overall performance improvement.\nFig. 4. Performance on different frequency entities, where S and L\nindicate model size and r refers to the number of retrieved clips.\n6. CONCLUSION\nIn this paper, we have presented a retrieval-augmented model, Re-\nAudioLDM, to tackle the long-tailed problem in AudioCaps. The\ncomparisons with current state-of-the-art models (i.e., AudioLDM\nand Tango) using several performance metrics (i.e., FAD, and CLAP-\nscore) demonstrate that Re-AudioLDM can significantly enhance the\nperformance of TTA models in generating high-fidelity audio clips.\nBy integrating retrieved features, Re-AudioLDM not only achieves\nimprovements in overall performance, but enables the generation of\nrare or unseen sound entities. In future work, we will investigate the\nmodel with external large datasets and explore the potential of the\nmodel in downstream tasks, such as zero-shot generation.\n7. ACKNOWLEDGMENT\nThis research was partly supported by a research scholarship from\nthe China Scholarship Council (CSC), funded by British Broad-\ncasting Corporation Research and Development (BBC R&D), En-\ngineering and Physical Sciences Research Council (EPSRC) Grant\nEP/T019751/1 \u201cAI for Sound\u201d, and a PhD scholarship from the Cen-\ntre for Vision, Speech and Signal Processing (CVSSP), University\nof Surrey. For the purpose of open access, the authors have applied\na Creative Commons Attribution (CC BY) license to any Author\nAccepted Manuscript version arising.\nReferences\n[1] H. Liu, Z. Chen, Y. Yuan, X. Mei, X. Liu, D. Mandic, W. Wang,\nand M. D. Plumbley, \u201cAudioLDM: Text-to-Audio generation\nwith latent diffusion models,\u201d in International Conference on\nMachine Learning, 2023.\n[2] R. Huang, J. Huang, D. Yang, Y. Ren, L. Liu, M. Li, Z. Ye,\nJ. Liu, X. Yin, and Z. Zhao, \u201cMake-an-audio: Text-to-audio\ngeneration with prompt-enhanced diffusion models,\u201d in Interna-\ntional Conference on Machine Learning, 2023.\n[3] X. Liu, Z. Zhu, H. Liu, Y. Yuan, M. Cui, Q. Huang, J. Liang,\nY. Cao, Q. Kong, M. D. Plumbley, and W. Wang, \u201cWavjour-\nney: Compositional audio creation with large language models,\u201d\narXiv preprint arXiv:2307.14335, 2023.\n[4] D. Ghosal, N. Majumder, A. Mehrish, and S. Poria, \u201cText-\nto-audio generation using instruction tuned LLM and latent\ndiffusion model,\u201d arXiv preprint arXiv:2304.13731, 2023.\n[5] C. D. Kim, B. Kim, H. Lee, and G. Kim, \u201cAudioCaps: Generat-\ning captions for audios in the wild,\u201d in Annual Conference of the\nNorth American Chapter of the Association for Computational\nLinguistics, 2019.\n[6] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d\nAdvances in Neural Information Processing Systems, vol. 30,\n2017.\n[7] Y. Yuan, H. Liu, J. Liang, X. Liu, M. D. Plumbley, and W. Wang,\n\u201cLeveraging pre-trained audioldm for sound generation: A\nbenchmark study,\u201d in European Association for Signal Pro-\ncessing, 2023.\n[8] X. Liu, T. Iqbal, J. Zhao, Q. Huang, M. Plumbley, and\nW. Wang, \u201cConditional sound generation using neural discrete\ntime-frequency representation learning,\u201d IEEE International\nWorkshop on Machine Learning for Signal Processing, 2021.\n[9] X. Chen, N. Mishra, M. Rohaninejad, and P. Abbeel, \u201cPix-\nelSNAIL: An improved autoregressive generative model,\u201d in\nInternational Conference on Machine Learning, 2018, pp. 864\u2013\n872.\n[10] V. Iashin and E. Rahtu, \u201cTaming visually guided sound genera-\ntion,\u201d in British Machine Vision Conference, 2021.\n[11] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and\nI. Sutskever, \u201cLanguage models are unsupervised multitask\nlearners,\u201d OpenAI blog, vol. 1, no. 8, p. 9, 2019.\n[12] D. Yang, J. Yu, H. Wang, W. Wang, C. Weng, Y. Zou, and\nD. Yu, \u201cDiffsound: Discrete diffusion model for text-to-sound\ngeneration,\u201d IEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing, 2023.\n[13] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agar-\nwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., \u201cLearning\ntransferable visual models from natural language supervision,\u201d\nin International Conference on Machine Learning, 2021, pp.\n8748\u20138763.\n[14] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,\nM. Matena, Y. Zhou, W. Li, and P. J. Liu, \u201cExploring the lim-\nits of transfer learning with a unified text-to-text transformer,\u201d\nJournal of Machine Learning Research, vol. 21, no. 1, pp. 5485\u2013\n5551, 2020.\n[15] B. Li, P. H. Torr, and T. Lukasiewicz, \u201cMemory-driven text-\nto-image generation,\u201d in British Machine Vision Conference,\n2022.\n[16] A. Blattmann, R. Rombach, K. Oktay, J. M\u00a8uller, and B. Ommer,\n\u201cRetrieval-augmented diffusion models,\u201d Advances in Neural\nInformation Processing Systems, vol. 35, pp. 15 309\u201315 324,\n2022.\n[17] S. Sheynin, O. Ashual, A. Polyak, U. Singer, O. Gafni, E. Nach-\nmani, and Y. Taigman, \u201cKNN-diffusion: Image generation via\nlarge-scale retrieval,\u201d in International Conference on Learning\nRepresentations, 2023.\n[18] W. Chen, H. Hu, C. Saharia, and W. W. Cohen, \u201cRe-Imagen:\nRetrieval-augmented text-to-image generator,\u201d in International\nConference on Learning Representations, 2023.\n[19] Y. Yuan, H. Liu, X. Kang, P. Wu, M. D. Plumbley, and W. Wang,\n\u201cText-driven foley sound generation with latent diffusion model,\u201d\nin Proceedings of the Detection and Classification of Acoustic\nScenes and Events Workshop, 2023, pp. 231\u2013235.\n[20] Y. Wu, K. Chen, T. Zhang, Y. Hui, T. Berg-Kirkpatrick, and\nS. Dubnov, \u201cLarge-scale contrastive language-audio pretraining\nwith feature fusion and keyword-to-caption augmentation,\u201d in\nIEEE International Conference on Acoustics, Speech and Signal\nProcessing, ICASSP, 2023.\n[21] H. Xu, J. Li, A. Baevski, M. Auli, W. Galuba, F. Metze, C. Fe-\nichtenhofer et al., \u201cMasked autoencoders that listen,\u201d arXiv\npreprint:2207.06405, 2022.\n[22] J. Ho, A. Jain, and P. Abbeel, \u201cDenoising diffusion probabilistic\nmodels,\u201d in Neural Information Processing Systems, 2020.\n[23] F. Kreuk, G. Synnaeve, A. Polyak, U. Singer, A. D\u00b4efossez,\nJ. Copet, D. Parikh, Y. Taigman, and Y. Adi, \u201cAudioGen: textu-\nally guided audio generation,\u201d in International Conference on\nLearning Representations, 2023.\n[24] J. F. Gemmeke, D. P. W. Ellis, D. Freedman, A. Jansen,\nW. Lawrence, R. C. Moore, M. Plakal, and M. Ritter, \u201cAudioSet:\nAn ontology and human-labeled dataset for audio events,\u201d in\nIEEE International Conference on Acoustics, Speech and Signal\nProcessing, 2017, pp. 776\u2013780.\n[25] T. Heittola, A. Mesaros, and T. Virtanen, \u201cTAU Urban Acoustic\nScenes 2019, Development dataset,\u201d Mar. 2019. [Online].\nAvailable: https://doi.org/10.5281/zenodo.2589280\n[26] Q. Kong, Y. Cao, T. Iqbal, Y. Wang, W. Wang, and M. D. Plumb-\nley, \u201cPANNs: Large-scale pretrained audio neural networks for\naudio pattern recognition,\u201d IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing, vol. 28, pp. 2880\u20132894,\n2020.\n"
  },
  {
    "title": "Leveraging Contextual Information for Effective Entity Salience Detection",
    "link": "https://arxiv.org/pdf/2309.07990.pdf",
    "upvote": "5",
    "text": "Leveraging Contextual Information for Effective Entity Salience Detection\nRajarshi Bhowmik\nMarco Ponza\nAtharva Tendle\nAnant Gupta\nRebecca Jiang \u2217\nXingyu Lu\nQian Zhao\nDaniel Preo\u00b8tiuc-Pietro\nBloomberg\n{rbhowmik6, dpreotiucpie}@bloomberg.net\nAbstract\nIn text documents such as news articles, the\ncontent and key events usually revolve around\na subset of all the entities mentioned in a doc-\nument. These entities, often deemed as salient\nentities, provide useful cues of the aboutness\nof a document to a reader. Identifying the\nsalience of entities was found helpful in several\ndownstream applications such as search, rank-\ning, and entity-centric summarization, among\nothers. Prior work on salient entity detection\nmainly focused on machine learning models\nthat require heavy feature engineering. We\nshow that fine-tuning medium-sized language\nmodels with a cross-encoder style architecture\nyields substantial performance gains over fea-\nture engineering approaches. To this end, we\nconduct a comprehensive benchmarking of four\npublicly available datasets using models repre-\nsentative of the medium-sized pre-trained lan-\nguage model family. Additionally, we show that\nzero-shot prompting of instruction-tuned lan-\nguage models yields inferior results, indicating\nthe task\u2019s uniqueness and complexity.\n1\nIntroduction\nMany NLP studies have highlighted the impor-\ntance of entities to understanding the semantics\nof a document (Wu et al., 2020b; Meij et al., 2012).\nAutomatically identifying entities in unstructured\ntext documents and linking them to an underlying\nknowledge base, such as Wikipedia, is one of the\ncore NLP tasks, with multiple shared tasks (Tjong\nKim Sang and De Meulder, 2003; Strauss et al.,\n2016), benchmarks (Hoffart et al., 2011; Hovy\net al., 2006; Pradhan et al., 2013; Rijhwani and\nPreotiuc-Pietro, 2020; Derczynski et al., 2016), and\nstudies (Kolitsas et al., 2018; Nguyen et al., 2014)\ndedicated to solving them.\nAlthough an entity may play a crucial semantic\nrole in document understanding, not all entities in\n\u2217Work was done while the author was affiliated with\nBloomberg\nFigure 1: An example of a document with salient and\nnon-salient entities. Entity mentions are highlighted in\ntext.\na text document play equal roles. Some entities\nare the central subjects or actors within a docu-\nment, around which the content and the key events\nrevolve. Others are mentioned only to provide addi-\ntional context to the main event. For example, some\nentities may be actors in peripheral events, while\nothers are deemed uninformative to the understand-\ning of the document. Thus, entity salience in a text\nis defined as a binary or ordinal rating to quantify\nthe extent to which a target entity is central to a\ngiven piece of text (Gamon et al., 2013; Dunietz\nand Gillick, 2014). Figure 1 provides an example\ntext along with the mentioned entities and their\nsalience. We note that the salience of an entity to a\ntext is independent of the user\u2019s interest when read-\ning or searching the document (Gamon et al., 2013),\nwhich is usually referred to as entity relevance. It\nis also distinct from entity importance, which quan-\ntifies the overall importance of the entity indepen-\ndent of the document. Automatically inferring en-\ntity salience was shown to aid search (Gamon et al.,\n2013), improve ranking results (Xiong et al., 2018),\nentity detection (Trani et al., 2018), and enable\nentity-centric applications such as entity-centric\nsummarization (Maddela et al., 2022).\narXiv:2309.07990v1  [cs.CL]  14 Sep 2023\nIn this paper, we study the effectiveness of\nTransformer-based Pre-trained Language Models\n(PLMs) in the task of entity salience detection.\nPrior work on determining entity salience relied\non heavy feature engineering to craft features ex-\nplicitly covering relevant aspects, such as entity fre-\nquency (Dunietz and Gillick, 2014; Dojchinovski\net al., 2016), position of entity mentions within a\ndocument (Dunietz and Gillick, 2014; Trani et al.,\n2018), relations to other entities (Trani et al., 2018),\ndocument features, such as its length (Gamon et al.,\n2013) and lexical features, such as the name of the\nentity or its context. Only a single recent work at-\ntempted to use PLMs in a pipeline which included\nkey entity detection, albeit the scope of the eval-\nuation was limited to a single high performing\ndataset (Zhao et al., 2021). In contrast, our pro-\nposed method uses a cross-encoder architecture\nwhere a target entity\u2019s name or alias and its contex-\ntual mentions in a text document are encoded by\na PLM encoder. The classifier uses the contextual\nrepresentation and, optionally, positional informa-\ntion about the entity encoded through the decile\nposition embedding vector of mentions to deter-\nmine the salience score of a target entity.\nWe conduct experiments on four publicly avail-\nable datasets, two of which were human annotated\nand two that were curated semi-automatically. We\nfine-tune several cross-encoders using PLMs and\ndemonstrate that these yield consistent and signifi-\ncant improvements over feature-based methods, as\nwell as prompting instruction-tuned PLMs. The lat-\nter shows the novelty and complexity of the task of\nentity salience detection, which requires the model\nto learn significant task-specific semantic knowl-\nedge for this natural language understanding task.\nOur contributions in this paper are the following:\n\u2022 We propose a cross-encoder style architecture\nwith explicit encoding of position information\nfor entity salience detection that shows consis-\ntent improvements of 7 \u2013 24.4 F1 score over\nprevious feature engineering approaches.\n\u2022 We establish a uniform benchmark of two human\nannotated and two semi-automatically curated\ndatasets for the task of entity salience detection\nthat we expect to be beneficial to future study of\nthis task;\n\u2022 A faceted analysis of the models\u2019 predictive be-\nhaviour.\n2\nRelated Work\nUnderstanding the aboutness of a document is one\nof the long-standing goals of research in both In-\nformation Retrieval and Natural Language Pro-\ncessing (Gamon et al., 2013). Several types of ap-\nproaches have been proposed, including extract-\ning key-terms (Hulth, 2003; Mihalcea and Tarau,\n2004), identifying latent topics (Blei et al., 2003),\nor generating text summaries (Erkan and Radev,\n2004). There has been a recent focus in using en-\ntities to understand the content of the document.\nTowards this goal, the task of entity salience has\nbeen first described for web pages in (Gamon et al.,\n2013) and for news content in (Dunietz and Gillick,\n2014). This task can be viewed as a restricted form\nof keyword or keyphrase extraction (Alami Mer-\nrouni et al., 2020) if salience is binary. For the rest\nof this study, we will use the concept of salience as\ndescribed in (Gamon et al., 2013).\nThe salience labels for entities were obtained ei-\nther by crowdsourcing labels from multiple raters\nto identify salient entities (Gamon et al., 2013; Do-\njchinovski et al., 2016; Trani et al., 2018; Mad-\ndela et al., 2022) or by using proxies. For example,\n(Dunietz and Gillick, 2014) hypothesize that salient\nentities are those that appear in the article\u2019s abstract.\n(Wu et al., 2020a) identifies an entity as salient if\nthe Wikinews category that corresponds to the en-\ntity is also labeled as the category of the article.\nPast studies mostly proposed machine learning\nmethods to infer the salience of a given entity that\nrelied on hand-crafted features. Features that can\nbe computed from the target entity mentions and\ndocument alone can be categorized into the fol-\nlowing: positional (e.g., position in the document,\nif entity is in the abstract) (Dunietz and Gillick,\n2014), count-based (e.g., number of references to\nthe entity) (Dunietz and Gillick, 2014; Wu et al.,\n2020a), local context (Trani et al., 2018), or global\ncontext (Ponza et al., 2019). Further, joint entity\nsalience resolution can be performed by creating\nfeatures using the entity graph (e.g., centrality in\nthe entity graph) (Dunietz and Gillick, 2014; Trani\net al., 2018). Finally, past work also showed that\nincorporating external knowledge about entities\nfrom knowledge bases can boost predictive perfor-\nmance (Dojchinovski et al., 2016).\nAutomatically inferring salience for entities can\ndirectly benefit multiple downstream applications,\nsuch as improving ranking results for queries con-\ntaining entities (Xiong et al., 2018) or improv-\ning the performance of entity detection by joint\nmodelling (Trani et al., 2018). Moreover, by in-\nferring salience, new entity-centric applications\ncan be built, such as highlighting salient entities\nin search (Gamon et al., 2013), improving the in-\nterpretability of news trends through salient en-\ntities (Ponza et al., 2021), or identifying entities\nfor creating entity-centric summaries of news sto-\nries (Maddela et al., 2022; Hofmann-Coyle et al.,\n2022).\n3\nProblem Definition\nWe use the concept of salience as introduced in (Ga-\nmon et al., 2013): salient entities are entities explic-\nitly mentioned in the document that are objectively\nimportant as a function of the structure of the text.\nThe goal of the salience model is to produce a\nsingle salience score \u03c8(e) for the entity e using\nonly the document D and the explicit entity men-\ntions Me. We consider using external knowledge,\nsuch as information about entities from knowledge\nbases, to be outside the scope and leave integration\nof such knowledge for future work.\n4\nMethods\nPre-trained Language Models (PLMs) have shown\na remarkable ability to encode syntactic and seman-\ntic knowledge in their parameters (Tenney et al.,\n2018, 2019) that can be leveraged when fine-tuned\non downstream natural language understanding\n(NLU) tasks. We postulate that PLMs can be har-\nnessed to help in entity salience detection, a target-\nbased document-level NLU task. In this section, we\npresent an architecture based on the cross-encoder\nsetup adapted to the task of entity salience detec-\ntion.\n4.1\nCross-encoder\nEncoding Given a document D and a target entity\ne, which is mentioned in the document, we con-\ncatenate the target entity\u2019s name and the document\nusing a special [SEP] token. We then encode the\ntext using a Transformer-based pre-trained encoder.\nFigure 2 shows the graphical representation of the\ncross-encoder model. This setup allows the model\nto have deep cross attention between the target en-\ntity and the entire document. Note that we do not\nexplicitly use the mention information Me in this\nmodeling approach and rely on cross-attention to\nuse mention information implicitly.\nTransformer\n[CLS]  Target Entity\u2019s Name[SEP]Document\u2019s Text \nSalience Score\n[CLS]\u2019s representation\nFFNN\n\u03c8(e)\nEncoded decile positions\n[h [CLS],hpe]\nFigure 2: Graphical representation of the cross-encoder\narchitecture w/ decile position encoding.\nPosition Encoding We compute the decile posi-\ntions for each entity mention (m \u2208 Me) in the\ndocument D by taking a positional index pm \u2208\n{0, 1, . . . , 9}, indicating which part of document\nthe mention belongs to if the document is parti-\ntioned into 10 equal chunks. Depending on the\nnumber and positions of the mentions, the vec-\ntor can contain multiple non-zero values in the p\nvector. To obtain positional embeddings, we use\nan embedding layer that maps positional indices\nto a dense vector of dimension dmodel, formally\nhpe(m) = Embedding(pm).\nScoring The output representation of the [CLS]\ntoken is concatenated with the mean position em-\nbedding vector hpe and fed to a scorer module that\nproduces a salience score \u03c8(e) \u2208 [0, 1] for entity e.\nThe salience scorer is a feed-forward network with\na sigmoid scoring function head. Formally,\n\u03c8(e) = \u03c3(FFN(h[CLS]||hpe))\n4.2\nOptimization\nWe fine-tune the model described above by mini-\nmizing the binary cross entropy loss that is calcu-\nlated using the ground truth binary salience labels\nand the predicted salience score \u03c8(e).\n5\nDatasets\nIn this section, we describe our entity salience\nbenchmark, which consists of four datasets: two\ndatasets were curated using semi-automated meth-\nods and two used human annotations. We provide\nsummary statistics of these datasets and label col-\nlection methods in Table 1.\nNYT-Salience This dataset is introduced in (Duni-\netz and Gillick, 2014) and is the largest dataset\nDataset\nNYT-Salience\nWN-Salience\nSEL\nEntSUM\n# Docs\n110,463\n6,956\n365\n693\nDoc Length (avg chars)\n5,079\n2,106\n1,660\n4,995\n# Unique entities\n179,341\n23,205\n6,779\n7,854\n# Mentions\n4,405,066\n145,081\n19,729\n20,784\n% Salient entities\n14%\n27%\n10%\n39%\nGround-truth\nAbstract Alignment\nCategory Alignment\nHuman\nHuman\nTable 1: Summary statistics and label collection methods for the datasets used in our experiments.\nto date for entity salience detection. The dataset\nis curated with an assumption that salient entities\nare mentioned in the abstract of a news article in\nthe NYT Corpus (Sandhaus, 2008). Entities and\ntheir mentions are identified using a classical NLP\npipeline involving POS tagging, dependency pars-\ning, and noun phrase extraction. Despite being\nlarge-scale, the automatic dataset creation process\ncould introduce noise as corroborated by moderate\nagreement numbers with human annotators on a\nsubset of the data. The dataset contains a binary\nsalience label for each entity.\nWN-Salience Introduced in (Wu et al., 2020a), this\nis another automatically curated dataset consist-\ning of Wikinews articles. These are annotated with\nWikinews categories by their authors. WN-Salience\nidentifies salient entities by using the hypothesis\nthat an entity is salient if the Wikinews category\nthat corresponds to the entity is also labeled as a\ncategory of the article. Similar to NYT-Salience,\nthis dataset has binary salience labels.\nSEL This is another dataset based on Wikinews re-\nleased by (Trani et al., 2018). However, unlike WN-\nSalience, this dataset is human annotated, where\nmultiple human annotators ranked the salience of\nentities into one of four categories. To conform\nwith the binary labels of the other datasets, we map\nthe 4 categories into binary labels of {0, 1} by map-\nping the bottom two classes to not salient and the\ntop two classes to salient.\nEntSUM This dataset was introduced in (Mad-\ndela et al., 2022). To construct this dataset, a ran-\ndomly selected set of entities spanning a subset of\n693 articles from the NYT corpus were assigned\nsalience labels by human annotators on a four-point\nscale, ranging between [0, 3]. For each document\nentity pair, two independent annotations were col-\nlected, which were increased up to 5 in case of\ndisagreements. If the average annotation score is\ngreater than 1.5 for an entity, it is assigned a posi-\ntive salience label.\n5.1\nData Enrichment with Inferred Mentions\nExcept for EntSUM, the other datasets do not\nhave explicit entity mention offsets as annotations,\nwhich are necessary for many feature-based ap-\nproaches and to compute positional embeddings.\nWhile SEL contains only the mention surface texts\nper entity, NYT-Salience and WN-Salience only\nprovide the start and end character indices (aka\nmention offsets) of the very first mention of an en-\ntity. To this end, we infer additional mentions of an\nentity within the text using a combination of Flair\nNER (Akbik et al., 2019) and pattern matching.\nFor SEL, since the mentions are available, we\nuse a pattern matching approach to match the sur-\nface text of the mentions to infer mention offsets.\nFor NYT-Salience and WN-Salience, we first use\nFlair NER to identify mentions of named entities\nin the text. We attempt to match these mentions\nto the first mention of each entity in the document\nprovided in the respective datasets. Since the sur-\nface text of other mentions may differ from the first\nmention, we additionally use the overlap between\na mention\u2019s surface text and the entity name as a\ncandidate mention for that entity. Applying this\napproach, we infer additional mentions of an en-\ntity in the text and their offsets. While this process\ncould introduce some noise, the overall quality of\nthe datasets are enhanced through this process.\n6\nExperiments\nWe experiment on our entity salience benchmark\nwith our proposed PLM-based method, other ML\nand heuristic-based approaches used in past re-\nsearch, as well as an instruction-tuned PLM.\n6.1\nData Splits\nPrior works (Dunietz and Gillick, 2014; Trani\net al., 2018; Wu et al., 2020a) use inconsistent\n(or not reported) train/validation/test splits. NYT-\nSalience and WN-Salience datasets are provided\nwith train/test splits (but no validation), whereas\nSEL dataset is provided without any splits. This\nmakes it hard to benchmark previous works with a\nfair comparison across models. To overcome this\nissue, we do a temporal split of NYT-Salience\u2019s\nand WN-Salience\u2019s original training sets into a new\ntrain/validation sets based on the publication time\nof the news stories, which provides a more realistic\ntesting setup (Huang and Paul, 2018; Rijhwani and\nPreotiuc-Pietro, 2020). We also perform a temporal\nsplit of SEL and EntSUM datasets into train/vali-\ndation/test sets.\n6.2\nBaselines\nFirst, we list all methods used in past research,\nfor which we report the results from the original\npapers.\n\u2022 First Sentence. Classifies an entity as salient if\nit appears in the first sentence of the document\u2019s\nbody; used in both (Dunietz and Gillick, 2014)\nand (Wu et al., 2020a).\n\u2022 Position & Frequency (Dunietz and Gillick,\n2014). Feeds the first sentence index and the\nfrequency features of an entity into a logistic\nregression model.\n\u2022 All Features (Dunietz and Gillick, 2014). Uses a\nseries of features based on position, frequency,\nand PageRank signals fed into a logistic regres-\nsion model.\n\u2022 SEL (Trani et al., 2018). Uses a combination\nof features based on position, frequency, and\nWikipedia graph statistics fed into a Gradient\nBoosted Decision Tree algorithm implemented\nin sklearn (Pedregosa et al., 2011).\n\u2022 SWAT (Ponza et al., 2019). Uses a set of fea-\ntures similar to the SEL Method described above,\nwith the addition of features based on entity em-\nbeddings. All features are fed into a Gradient\nBoosted Decision Tree algorithm implemented\nin XGBoost (Chen et al., 2015).\n\u2022 Positional Feature (Wu et al., 2020a). Uses the\nindex of the first sentence the entity is mentioned\nas a feature in a logistic regression model. This\nmethod provides best results on the WN Salience\ndataset in (Wu et al., 2020a).\nNext, we re-implement a set of common methods\nbased on the above baselines in order to be able\nto test them on all four datasets. This ensures the\nevaluation is performed on the same experimental\nsetup.\n\u2022 Positional Headline. Classifies an entity as\nsalient whether it appears in the headline of the\ninput document.\n\u2022 Positional Headline & Lead. Classifies an entity\nas salient if it appears in the headline of the\ndocument or in the first sentence (lead sentence)\nof the document.\n\u2022 Entity Frequency. Classifies an entity as salient\nif they are more frequent than a given value. For\neach dataset, we calculated different thresholds\nand reported the best results. Thresholds can be\nfound in the Appendix.\n\u2022 Features & GBDT. This method uses the most\ncommon features from past works (Dunietz and\nGillick, 2014; Wu et al., 2020a; Trani et al., 2018;\nPonza et al., 2019) \u2014 i.e., entity\u2019s first sentence\nindex, and entity frequency \u2014 and feeds them\ninto a GBDT model implemented using Light-\nGBM (Ke et al., 2017).\n\u2022 SEL GBDT. Follows the method from (Trani\net al., 2018) and uses sklearn\u2019s GBDT (Pe-\ndregosa et al., 2011) to train a model on the\nfeatures provided with the SEL dataset.\n\u2022 Target entity masking. This method feeds the in-\nput to a Transformer-based encoder (RoBERTa-\nbase) with the target entity mentions represented\nthrough a special mask token. The salience pre-\ndiction is obtained by mean pooling the mask\ntoken representations and passing this through a\nfeed-forward network.\n\u2022 Zero-shot prompting. We test an instruction-\ntuned PLM (Wei et al., 2021) by zero-shot\nprompting. The prompt introduces the task de-\nscription followed by the input text and a tar-\nget entity and formulates the output as a yes/no\nquestion. The PLM, already instruction-tuned\non a large collection of NLU tasks, attempts to\nprovide an answer based on the prompt, input\ntext and target entity. This family of models has\nbeen demonstrated to be robust and versatile on\nmultiple benchmarks (Wei et al., 2021). We use\nFlan-T5-Large (Chung et al., 2022), which is\ncomparable in size to other PLMs we use as the\nbase models for the cross-encoders.\n6.3\nExperimental Setup\nWe use RoBERTa-base (Liu et al., 2019) and\nDeBERTa-v3-base (He et al., 2023) as the base\nPLM for experiments. For each of these base mod-\nels, we train both a cross-encoder model and a\ncross-encoder model augmented with decile posi-\ntional embeddings.\nSource\nType\nMethod\nNYT-Salience\nWN-Salience\nP\nR\nF1\nP\nR\nF1\n(Dunietz and Gillick, 2014)\nHeuristic\nFirst Sentence\n59.5\n37.8\n46.2\n\u2013\n\u2013\n\u2013\n(Dunietz and Gillick, 2014)\nML\nPosition & Frequency\n59.3\n61.3\n60.3\n\u2013\n\u2013\n\u2013\n(Dunietz and Gillick, 2014)\nML\nAll Features\n60.5\n63.5\n62.0\n\u2013\n\u2013\n\u2013\n(Ponza et al., 2019)\nML\nSWAT\n62.4\n66.0\n64.1\n\u2013\n\u2013\n\u2013\n(Wu et al., 2020a)\nHeuristic\nFirst Sentence\n56.0\n41.0\n47.3\n47.9\n53.2\n50.4\n(Wu et al., 2020a)\nML\nPositional Feature\n19.0\n41.3\n26.0\n29.1\n78.9\n42.5\n(Wu et al., 2020a)\nML\nFeatures & GBDT\n39.2\n59.7\n47.3\n29.2\n48.1\n36.3\nOur Implementations\nHeuristic\nPositional Headline\n57.5\n42.0\n48.5\n46.1\n51.5\n48.7\nHeuristic\nPositional Headline & Lead\n49.8\n55.4\n52.5\n41.0\n60.0\n48.7\nHeuristic\nEntity Frequency\n53.7\n53.3\n53.6\n37.3\n61.9\n46.6\nML\nFeatures & GBDT\n61.0\n57.4\n59.2\n46.2\n53.3\n49.5\nPLM (RoBERTa)\nTarget Entity Masking\n64.6\n50.2\n56.5\n57.0\n65.4\n60.9\nPLM (Flan-T5)\nZero-shot prompting\n43.0\n60.6\n50.3\n38.8\n53.2\n44.9\nOur Models\nPLM (RoBERTa)\ncross-encoder\n75.9\n87.1\n81.1\n71.8\n73.6\n72.7\nPLM (DeBERTa)\ncross-encoder\n77.5\n87.4\n82.1\n71.5\n78.3\n74.8\nPLM (RoBERTa)\ncross-encoder w/ position emb.\n78.7\n84.2\n81.4\n71.2\n76.7\n73.8\nPLM (DeBERTa)\ncross-encoder w/ position emb.\n75.9\n88.4\n81.7\n73.3\n76.1\n74.7\nTable 2: Results on the NYT-Salience and WN-Salience datasets. The ground-truth of these datasets was generated\nvia abstract/category alignment. The top section presents results as originally reported in the source papers.\nSource\nType\nMethod\nSEL\nEntSUM\nP\nR\nF1\nP\nR\nF1\n(Trani et al., 2018)\nML\nSEL (w/ 5-fold cross val.)\n50.0\n61.0\n52.0\n\u2013\n\u2013\n\u2013\n(Ponza et al., 2019)\nML\nSWAT (w/ 5-fold cross val.)\n58.0\n64.9\n61.2\n\u2013\n\u2013\n\u2013\nOur Implementations\nHeuristic\nPositional Headline\n26.6\n78.4\n39.7\n60.7\n18.5\n28.4\nHeuristic\nPositional Headline & Lead\n22.1\n87.1\n35.3\n51.2\n31.6\n39.1\nHeuristic\nEntity Frequency\n13.5\n57.8\n21.9\n48.4\n54.0\n51.0\nML\nFeatures & GBDT\n26.6\n78.4\n39.7\n60.7\n52.0\n56.0\nML\nSEL GBDT\n71.1\n47.8\n57.1\n\u2013\n\u2013\n\u2013\nPLM (RoBERTa)\nTarget Entity Masking\n36.3\n13.8\n20.0\n63.0\n41.7\n50.2\nPLM (Flan-T5)\nZero-shot prompting\n27.0\n81.7\n40.6\n50.7\n54.5\n52.5\nOur Models\nPLM (RoBERTa)\ncross-encoder\n51.6\n73.6\n60.6\n65.5\n60.6\n63.0\nPLM (DeBERTa)\ncross-encoder\n64.1\n73.6\n68.5\n64.9\n59.2\n61.9\nPLM (RoBERTa)\ncross-encoder w/ position emb.\n63.0\n69.9\n66.3\n67.5\n57.0\n61.8\nPLM (DeBERTa)\ncross-encoder w/ position emb.\n67.3\n62.4\n64.7\n72.1\n51.5\n60.1\nTable 3: Results on the SEL and EntSUM datasets. The ground-truth of these datasets was generated via human\nannotation. The top section presents results as originally reported in the source papers.\nFor training our proposed models, we use\nAdamW (Loshchilov and Hutter, 2019) as the\noptimizer. We perform a hyperparameter search\nfor learning rate using the following set of\nvalues: {0.001, 0.0005, 0.0002, 0.0001, 0.00005}.\nWe train our models for a maximum of 10 epochs\nwith early stopping based on the validation set\nperformance. We pick the best performing model\ncheckpoints for each dataset based on the perfor-\nmance on the validation set. In Tables 2 and 3,\nwe report the performance of our models and the\nbaselines using the standard classification metrics\n(i.e., Precision, Recall, and F1) on the positive\n(salient) class, following previous research on en-\ntity salience.\nFor training and inference of each Transformer-\nbased model, we use a single NVIDIA V100 GPU\nwith 32GiB GPU memory, 4 CPUs, and 128 GiB\nof main memory.\n6.4\nResults\nIn Tables 2 and 3, we present the experimental\nresults of the baselines and our proposed models\non the four datasets described in Section 5.\nComparison with feature-based methods. We\nobserve that the cross-encoder model significantly\noutperforms all baseline models in F1 score. It also\nyields better precision compared to the baselines\nfor three of the four datasets. Only for the SEL\ndataset does the SEL GBDT model trained on pub-\nlicly available pre-computed features produce a\nmodel with better precision than the cross-encoder.\nWe observe that adding the decile positional em-\nbedding with cross-encoder improves the precision\nacross all datasets, but also degrades the recall in\nevery dataset except NYT-Salience.\nThe Target Entity Masking approach, which\nalso leverages contextual information with a\ntransformer-based model yields mixed results.\nOverall, the model is able to obtain better preci-\nsion than the feature-based models for all datasets\nexcept SEL, but the model suffers from poor re-\ncall across all datasets, resulting in significantly\nworse F1 scores especially when compared to cross-\nencoder models.\nOur re-implementation of positional methods\nand GBDT methods are consistent with the per-\nformance reported in prior works. The variance\nin numbers can be attributed to the enrichment of\ndatasets with inferred mentions (Section 5.1) and\nthe explicit train/dev/test data split used in our ex-\nperiments (Section 6.1).\nComparison with zero-shot prompting of a large\nlanguage model. To the best of our knowledge,\nthis is the first evaluation of zero-shot prompting\nof instruction-tuned models (here Flan-T5-Large)\nfor the entity salience detection task. We observe\nthat the model is able to provide comparable or\nbetter performance with the heuristic-based meth-\nods, with the exception of the WN-Salience dataset.\nHowever, it falls short of the dedicated cross-\nencoder model by double digit margins, showing\nthat this is a distinctive task and requires a ded-\nicated model. We further discuss causes for this\nperformance in the Appendix (Section 9.1), along\nwith the implementation details.\n7\nAnalysis\nIn this section, we perform an analysis of model\npredictions in order to gain more insights into\nmodel behavior and understand potential avenues\nfor further improvement. We thus break down per-\nformance by different factors including: the impor-\ntance of inferring all entity mentions, the position\nof the first entity mention, and entity mention fre-\nquency.\n7.1\nImpact of Inferred Mentions\nIn Section 5.1, we inferred additional mentions of\nan entity for the NYT-Salience and WN-Salience\ndatasets. We compare the performance of our best\nmodel that leverages multiple mentions of an entity\nto its version trained with only the first mentions\nof entities in a document. The results in Table 4\nshow that doing so consistently improves the per-\nformance of our models across all datasets. In par-\nticular, for the largest dataset, NYT-Salience, our\nmodel achieves a substantial gain of 27.3 F1 points.\nThis experiment showcases the importance of aug-\nmenting our datasets with additional mentions and\nthe importance of explicitly modelling contextual\ninformation present around all entity mentions.\n7.2\nStratified Analysis on First Mention\nPosition\nWe compare our cross-encoder models against the\nFeatures & GBDT model, our re-implemented base-\nline that relies on the most popular features used in\nprior works (Dunietz and Gillick, 2014; Wu et al.,\n2020a; Trani et al., 2018). As shown in the results\nfrom Tables 2 and 3, among other features, posi-\ntional features are most informative for salience.\nIntuitively, if an entity is mentioned in the headline\nor in the first sentence of a news article, there is\nhigh probability of that entity being salient.\nFigure 3 shows that all models perform well when\nthe first mention falls in the headline or the first\nsentence of the document. We notice that the cross-\nencoder models constantly outperform the Features\n& GBDT model and the largest gains are observed\nin the SEL and WN-Salience datasets. This obser-\nvation indicates that the cross-encoder models are\nable to use the context to identify that mentions\nthat occur in the headline or the first parts of the\ndocument are often salient without explicitly using\nthis information as a feature.\nWe also investigate the performance of the mod-\nels when the first mention falls inside or outside\nthe context window of the PLM (here, 512 tokens).\nWhen mentions fall inside the context window, we\nobserve that the cross-encoder models consistently\noutperform the Features & GBDT model. When\nthe mention falls outside the context window, the\nmodel predictions become close to random, which\nis expected, as the model does not have immediate\ncontextual information around the mention. Using\nmodels that can deal with longer inputs would be\na promising direction for improvement for these\nsamples (Beltagy et al., 2020). Interestingly, for\nWN-Salience, the Features & GBDT model also\nperforms considerably worse outside the first 512\nModel\nNYT-Salience\nWN-Salience\nSEL\nEntSUM\nP\nR\nF1\nP\nR\nF1\nP\nR\nF1\nP\nR\nF1\nCross-encoder w/ first mention\n54.2\n57.5\n55.8\n69.6\n80.4\n74.6\n59.8\n76.1\n67.0\n69.1\n53.2\n60.2\nCross-encoder w/ all mentions\n77.5\n84.4\n82.1\n71.5\n78.3\n74.8\n64.1\n73.6\n68.5\n64.9\n59.2\n61.9\nTable 4: Performance comparison of cross-encoder models with only the first mention vs. all inferred mentions.\n(a) Performance with respect to the position of the mentions. There are no mentions outside of the context window for NYT.\n(b) Performance with respect to the frequency of the entities. The test split of SEL dataset does not contain any entity with more than 10\nmentions in a document.\nFigure 3: Stratified analysis across models and datasets.\ntokens.\n7.3\nStratified Analysis on Mention Frequency\nSimilar to mention position analysis, we compare\nour cross-encoder models against the Features &\nGBDT model, which uses mention frequency as\none of its input features. Figure 3 shows how the\ncross-encoder models and Features & GBDT com-\npare with varying frequency of entity mentions.\nFor salient entities with single mentions, the\ncross-encoder model performs significantly better\nthan the Features & GBDT model. In particular, for\nthe NYT-Salience dataset, the Features & GBDT\nmodel fails to predict any of the single mention\nentities as salient. This observation indicates that\nthe cross-encoder models do not simply model the\nmention frequency, but potentially leverage other\ncontextual information to determine the salience of\nentities with a single mention.\nThe performance of the Features & GBDT model\nimproves with more mentions per entity. In fact,\nfor the frequency range of 6-10 mentions per en-\ntity, the Features & GBDT model performs better\nthan the cross-encoder models for EntSUM and\nSEL datasets. This observation indicates the over-\nreliance of the Features & GBDT model on men-\ntion frequency to determine salience, but also that\nthe cross-encoder cannot fully use this heuristic.\n8\nConclusion\nThis paper aims to leverage the semantic knowl-\nedge encoded in pre-trained language models for\nentity salience detection. We propose the cross-\nencoder method based on Transformer-based PLMs\nwith positional representations and compare its per-\nformance to several ML-based, heuristic methods\nand instruction-tuned PLMs across four different\ndatasets, two human-annotated and two automati-\ncally curated. Across all our experiments, the cross-\nencoder model based on pre-trained language mod-\nels outperforms all other methods, often with dou-\nble digit gains in F-1 score. Analyses of model\nbehavior illustrate the important effects of mention\nfrequency, mention position, and document length\non performance, highlighting areas of future work.\nReferences\nAlan Akbik, Tanja Bergmann, Duncan Blythe, Kashif\nRasul, Stefan Schweter, and Roland Vollgraf. 2019.\nFLAIR: An easy-to-use framework for state-of-the-art\nNLP. In NAACL 2019, 2019 Annual Conference of\nthe North American Chapter of the Association for\nComputational Linguistics (Demonstrations), pages\n54\u201359.\nZakariae Alami Merrouni, Bouchra Frikh, and Brahim\nOuhbi. 2020. Automatic keyphrase extraction: a sur-\nvey and trends. Journal of Intelligent Information\nSystems, 54(2):391\u2013424.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer.\nDavid M. Blei, Andrew Y. Ng, and Michael I. Jordan.\n2003. Latent dirichlet allocation. J. Mach. Learn.\nRes., 3(null):993\u20131022.\nTianqi Chen, Tong He, Michael Benesty, Vadim\nKhotilovich, Yuan Tang, Hyunsu Cho, Kailong Chen,\net al. 2015. Xgboost: extreme gradient boosting. R\npackage version 0.4-2, 1(4):1\u20134.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, et al. 2022.\nScaling instruction-finetuned language models. arXiv\npreprint arXiv:2210.11416.\nLeon Derczynski, Kalina Bontcheva, and Ian Roberts.\n2016. Broad Twitter corpus: A diverse named en-\ntity recognition resource. In Proceedings of COLING\n2016, the 26th International Conference on Compu-\ntational Linguistics: Technical Papers, pages 1169\u2013\n1179, Osaka, Japan. The COLING 2016 Organizing\nCommittee.\nMilan Dojchinovski, Dinesh Reddy, Tom\u00e1\u0161 Kliegr,\nTom\u00e1\u0161 Vitvar, and Harald Sack. 2016. Crowdsourced\ncorpus with entity salience annotations. In Proceed-\nings of the Tenth International Conference on Lan-\nguage Resources and Evaluation (LREC\u201916), pages\n3307\u20133311, Portoro\u017e, Slovenia. European Language\nResources Association (ELRA).\nJesse Dunietz and Daniel Gillick. 2014. A new en-\ntity salience task with millions of training examples.\nIn Proceedings of the 14th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics, volume 2: Short Papers, pages 205\u2013209,\nGothenburg, Sweden. Association for Computational\nLinguistics.\nG\u00fcnes Erkan and Dragomir R. Radev. 2004. Lexrank:\nGraph-based lexical centrality as salience in text sum-\nmarization. J. Artif. Int. Res., 22(1):457\u2013479.\nMichael Gamon, Tae Yano, Xinying Song, Johnson\nApacible, and Patrick Pantel. 2013. Understanding\ndocument aboutness step one: Identifying salient enti-\nties. Technical Report MSR-TR-2013-73.\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2023.\nDeBERTav3: Improving deBERTa using ELECTRA-\nstyle pre-training with gradient-disentangled embed-\nding sharing. In The Eleventh International Confer-\nence on Learning Representations.\nJohannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino,\nHagen F\u00fcrstenau, Manfred Pinkal, Marc Spaniol,\nBilyana Taneva, Stefan Thater, and Gerhard Weikum.\n2011. Robust disambiguation of named entities in\ntext. In Proceedings of the 2011 Conference on Empir-\nical Methods in Natural Language Processing, pages\n782\u2013792, Edinburgh, Scotland, UK. Association for\nComputational Linguistics.\nElla Hofmann-Coyle, Mayank Kulkarni, Lingjue Xie,\nMounica Maddela, and Daniel Preotiuc-Pietro. 2022.\nExtractive entity-centric summarization as sentence\nselection using bi-encoders. In Proceedings of the\n2nd Conference of the Asia-Pacific Chapter of the As-\nsociation for Computational Linguistics and the 12th\nInternational Joint Conference on Natural Language\nProcessing (Volume 2: Short Papers), pages 326\u2013333,\nOnline only. Association for Computational Linguis-\ntics.\nEduard Hovy, Mitchell Marcus, Martha Palmer, Lance\nRamshaw, and Ralph Weischedel. 2006. OntoNotes:\nThe 90% solution. In Proceedings of the Human Lan-\nguage Technology Conference of the NAACL, Com-\npanion Volume: Short Papers, pages 57\u201360, New York\nCity, USA. Association for Computational Linguis-\ntics.\nXiaolei Huang and Michael J. Paul. 2018. Examining\ntemporality in document classification. In Proceed-\nings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers),\npages 694\u2013699, Melbourne, Australia. Association for\nComputational Linguistics.\nAnette Hulth. 2003. Improved automatic keyword ex-\ntraction given more linguistic knowledge. In Proceed-\nings of the 2003 Conference on Empirical Methods in\nNatural Language Processing, pages 216\u2013223.\nGuolin Ke, Qi Meng, Thomas Finley, Taifeng Wang,\nWei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu.\n2017. Lightgbm: A highly efficient gradient boost-\ning decision tree. Advances in neural information\nprocessing systems, 30.\nNikolaos Kolitsas, Octavian-Eugen Ganea, and Thomas\nHofmann. 2018. End-to-end neural entity linking.\nIn Proceedings of the 22nd Conference on Computa-\ntional Natural Language Learning, pages 519\u2013529,\nBrussels, Belgium. Association for Computational\nLinguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. Roberta: A\nrobustly optimized bert pretraining approach. arXiv\npreprint arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In International Confer-\nence on Learning Representations.\nMounica Maddela, Mayank Kulkarni, and Daniel\nPreotiuc-Pietro. 2022. EntSUM: A data set for entity-\ncentric extractive summarization. In Proceedings of\nthe 60th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers), pages\n3355\u20133366, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nEdgar Meij, Wouter Weerkamp, and Maarten de Rijke.\n2012. Adding semantics to microblog posts. In Pro-\nceedings of the Fifth ACM International Conference\non Web Search and Data Mining, WSDM \u201912, page\n563\u2013572, New York, NY, USA. Association for Com-\nputing Machinery.\nRada Mihalcea and Paul Tarau. 2004. TextRank: Bring-\ning order into text. In Proceedings of the 2004 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 404\u2013411, Barcelona, Spain. Asso-\nciation for Computational Linguistics.\nDat Ba Nguyen, Johannes Hoffart, Martin Theobald, and\nGerhard Weikum. 2014. Aida-light: High-throughput\nnamed-entity disambiguation. In Proceedings of the\nWorkshop on Linked Data on the Web co-located\nwith the 23rd International World Wide Web Confer-\nence (WWW 2014), Seoul, Korea, April 8, 2014, vol-\nume 1184 of CEUR Workshop Proceedings. CEUR-\nWS.org.\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\nR. Weiss, V. Dubourg, J. Vanderplas, A. Passos,\nD. Cournapeau, M. Brucher, M. Perrot, and E. Duch-\nesnay. 2011. Scikit-learn: Machine learning in Python.\nJournal of Machine Learning Research, 12:2825\u2013\n2830.\nMarco Ponza, Diego Ceccarelli, Paolo Ferragina, Edgar\nMeij, and Sambhav Kothari. 2021. Contextualizing\ntrending entities in news stories. In Proceedings of the\n14th ACM International Conference on Web Search\nand Data Mining, pages 346\u2013354.\nMarco Ponza, Paolo Ferragina, and Francesco Pic-\ncinno. 2019. Swat: A system for detecting salient\nwikipedia entities in texts.\nComputational Intelli-\ngence, 35(4):858\u2013890.\nSameer Pradhan, Alessandro Moschitti, Nianwen Xue,\nHwee Tou Ng, Anders Bj\u00f6rkelund, Olga Uryupina,\nYuchen Zhang, and Zhi Zhong. 2013. Towards ro-\nbust linguistic analysis using OntoNotes. In Proceed-\nings of the Seventeenth Conference on Computational\nNatural Language Learning, pages 143\u2013152, Sofia,\nBulgaria. Association for Computational Linguistics.\nShruti Rijhwani and Daniel Preotiuc-Pietro. 2020.\nTemporally-informed analysis of named entity recog-\nnition. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n7605\u20137617, Online. Association for Computational\nLinguistics.\nEvan Sandhaus. 2008. The new york times annotated\ncorpus. Linguistic Data Consortium, Philadelphia,\n6(12):e26752.\nBenjamin Strauss, Bethany Toma, Alan Ritter, Marie-\nCatherine De Marneffe, and Wei Xu. 2016. Results\nof the w-nut 2016 named entity recognition shared\ntask. In Proceedings of the 2nd Workshop on Noisy\nUser-generated Text (WNUT), pages 138\u2013144.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline.\nIn\nProceedings of the 57th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 4593\u2013\n4601, Florence, Italy. Association for Computational\nLinguistics.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam\nPoliak, R Thomas McCoy, Najoung Kim, Benjamin\nVan Durme, Samuel R Bowman, Dipanjan Das, et al.\n2018. What do you learn from context? probing for\nsentence structure in contextualized word representa-\ntions. In International Conference on Learning Rep-\nresentations.\nErik F. Tjong Kim Sang and Fien De Meulder. 2003. In-\ntroduction to the CoNLL-2003 shared task: Language-\nindependent named entity recognition. In Proceed-\nings of the Seventh Conference on Natural Language\nLearning at HLT-NAACL 2003, pages 142\u2013147.\nSalvatore Trani, Claudio Lucchese, Raffaele Perego,\nDavid E. Losada, Diego Ceccarelli, and Salvatore\nOrlando. 2018. Sel: A unified algorithm for salient\nentity linking. Computational Intelligence, 34(1):2\u2013\n29.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021. Finetuned lan-\nguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652.\nChuan Wu, Evangelos Kanoulas, Maarten de Rijke, and\nWei Lu. 2020a. Wn-salience: A corpus of news arti-\ncles with entity salience annotations. In Proceedings\nof The 12th Language Resources and Evaluation Con-\nference, pages 2095\u20132102.\nChuan Wu, Evangelos Kanoulas, and Maarten de Rijke.\n2020b. Learning entity-centric document representa-\ntions using an entity facet topic model. Inf. Process.\nManage., 57(3).\nChenyan Xiong, Zhengzhong Liu, Jamie Callan, and\nTie-Yan Liu. 2018. Towards better text understanding\nand retrieval through kernel entity salience modeling.\nIn The 41st International ACM SIGIR Conference on\nResearch & Development in Information Retrieval,\npages 575\u2013584.\nLingyun Zhao, Lin Li, Xinhao Zheng, and Jianwei\nZhang. 2021. A bert based sentiment analysis and\nkey entity detection approach for online financial\ntexts. In 2021 IEEE 24th International Conference\non Computer Supported Cooperative Work in Design\n(CSCWD), pages 1233\u20131238. IEEE.\n9\nAppendix\n9.1\nImplementation details of zero-shot\nprompting of instruction-tuned\nFlan-T5-Large\nWe define the problem of salience detection from\nthe perspective of zero-shot prompting as follows:\ngiven a definition of entity salience and document\ntext, ask the model to generate a \"yes\" or a \"no\"\nif a particular entity is salient or not. We experi-\nmented with various PLMs available on Hugging\nFace,1 instruction prompts, and text generation pa-\nrameters in the Hugging Face API to arrive at the\nbest performance within our defined constraints of\nusing an open source PLM with a comparable size\nto other PLMs in our work.\nFigure 4 shows the prompt we used and Table 5\nlists the generation parameters. We speculate the\nfollowing causes for the relatively lower precision\nobtained using this method:\n\u2022 The instruction defines the salience task defi-\nnition, but doesn\u2019t provide any reference ex-\namples (few-shot prompting) to align the def-\ninition of salience. This leads to the model\nidentifying an entity as salient based on its\nfrequency in the document. Creating a few-\nshot prompt is challenging as we need to limit\nthe maximum input length of the prompt to\nprevent out-of-memory issues\n\u2022 We truncate the document text so that the en-\ntire prompt is 512 tokens or less, thus throw-\ning away information on the document.\n### Instruction ###\nAn entity is considered salient in a text if the\ntext is about that entity. Classify if a given\nentity is salient in the below text or not:\nText: {text}\nEntity: {entity}\nQuestion: Is the above Entity salient in the\nabove Text? (yes/no):\nAnswer:\nFigure 4: Instruction for zero-shot prompting\n9.2\nThresholds for Entity Frequency baseline\nFigure 5 shows the performance of the Entity Fre-\nquency baseline by varying the minimum number\n1www.huggingface.com\nGeneration parameter\nValue\ntop_k\n0\ntop_p\n0.9\ntemperature\n0.9\nmax_new_tokens\n128\nTable 5: Parameters for generating zero-shot prompt\nof times an entity has to occur in the input docu-\nment to be classified as salient.\n1\n2\n3\n4\n5\n6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nF1\nDataset = NYT-Salience\n1\n2\n3\n4\n5\n6\nMin Frequency Threshold\nDataset = WN-Salience\n1\n2\n3\n4\n5\n6\nDataset = SEL\n1\n2\n3\n4\n5\n6\nDataset = EntSUM\nMethod\nPrecision\nRecall\nF1\nFigure 5: Performance of the Entity Frequency baseline over different thresholds.\nSummary Encoder \n(RoBERTa-base)\nContextual \nFeatures\nPositional \nFeatures\nScorer\n(Feed Forward \nNetwork)\nSalience score between \n[0, 1] for target entity\nFigure 6: Graphical representation of the Target Entity Masking architecture.\n"
  },
  {
    "title": "A Data Source for Reasoning Embodied Agents",
    "link": "https://arxiv.org/pdf/2309.07974.pdf",
    "upvote": "4",
    "text": "A Data Source for Reasoning Embodied Agents\nJack Lanchantin, Sainbayar Sukhbaatar, Gabriel Synnaeve,\nYuxuan Sun, Kavya Srinet, Arthur Szlam\nMeta AI\n{jacklanchantin, sainbar, gab, yuxuans, ksrinet, aszlam}@meta.com\nAbstract\nRecent progress in using machine learning models for rea-\nsoning tasks has been driven by novel model architectures,\nlarge-scale pre-training protocols, and dedicated reasoning\ndatasets for fine-tuning. In this work, to further pursue these\nadvances, we introduce a new data generator for machine rea-\nsoning that integrates with an embodied agent. The generated\ndata consists of templated text queries and answers, matched\nwith world-states encoded into a database. The world-states\nare a result of both world dynamics and the actions of the\nagent. We show the results of several baseline models on in-\nstantiations of train sets. These include pre-trained language\nmodels fine-tuned on a text-formatted representation of the\ndatabase, and graph-structured Transformers operating on a\nknowledge-graph representation of the database. We find that\nthese models can answer some questions about the world-state,\nbut struggle with others. These results hint at new research\ndirections in designing neural reasoning models and database\nrepresentations. Code to generate the data will be released at\ngithub.com/facebookresearch/neuralmemory.\nIntroduction\nAdvances in machine learning (ML) architectures (Vaswani\net al. 2017), large datasets and model scaling for pre-training\n(Radford et al. 2019; Chowdhery et al. 2022; Zhang et al.\n2022), modeling approaches (Nye et al. 2021; Wang et al.\n2022), and multiple dedicated reasoning datasets (Yang et al.\n2018a; Hudson and Manning 2019; Petroni et al. 2020;\nVedantam et al. 2021) have driven progress in both build-\ning models that can succeed in aspects of \u201creasoning\u201d and\nin automatically evaluating such capabilities. This has been\nevident particularly in the text setting, but also in computer\nvision (Krishna et al. 2016; Johnson et al. 2017; Yi et al.\n2020).\nIn parallel, the last decade has seen advances in the ability\nto train embodied agents to perform tasks and affect change\nin their environments. These have been also powered in part\nby data, with many environments made available for explor-\ning modeling approaches and benchmarking. In particular,\nwith respect to \u201creasoning\u201d in embodied agents, there have\nbeen works showing that adding inductive biases to support\nreasoning can lead to improved performance with end-to-end\nCopyright \u00a9 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nagent\ns\nSequence Context + GPT-2: \u201cmarbles\u201d\nStructured Context + Transformer: \n\u201cmarbles\u201d\nwilson\nspeaker\nagent\nmarbles\npebbles\nSequence Context: \"agent is at location (10,7,8). agent has pitch 1.0 and yaw \n2.5. agent has property agent. speaker is at location (8,7,0). speaker has pitch \n0.8 and yaw 2.3. wilson is at location (0,7,9). wilson has pitch -0.8 and yaw \n1.1. wilson has property rabbit. wilson has property brown. marbles is at \nlocation (10,7,3). marbles has pitch 1.1 and yaw 3.1. marbles has property \nchicken. marbles has property white. pebbles is at location (10,10,13). \npebbles has pitch 0.5 and yaw -0.3. pebbles has property sheep. pebbles has \nproperty mottled. gus is at location (3,7,9). gus has pitch 0.4 and yaw -0.1. \ngus has property cow. gus has property black. inst_seg is at location (3,8,1). \ninst_seg has property arch. inst_seg is at location (10,8,12). inst_seg has \nproperty dome. inst_seg is at location (14,6,6). inst_seg has property hole.\"\nQuery: \u201cwhat is closest from speaker?\u201d\nAnswer: \u201cmarbles\u201d\nFigure 1: (Top): Example of a generated scene in our 3d\ngridworld. (Middle): Given the 3d scene, we can convert the\ninformation in the render to a text or structured representa-\ntion. Here we show the text Sequence Context representation.\n\u201cinst_segs\u201d represent the block items such as structures or\nholes. (Bottom): For a particular scene, we can generate a\nwide variety of queries. Here we show and example of a dis-\ntance query asking which object is the closest to the speaker,\nwhich is the chicken named \u201cmarbles\u201d.\ntraining (Zambaldi et al. 2018) and other works have shown\nhow models can be augmented with extra supervision (Zhong,\nRockt\u00e4schel, and Grefenstette 2019).\nRecently, several works have shown how large language-\nmodel pre-training can be used to affect planners for em-\nbodied agents (Huang et al. 2022; Ahn et al. 2022). More\ngenerally, symbolic representations can be a hub for connect-\ning perception, memory, and reasoning in embodied agents.\narXiv:2309.07974v1  [cs.LG]  14 Sep 2023\nHowever, the growing literature in NLP reasoning models\nis missing data grounded in a dynamic and agent-alterable\nworld. Models trained on traditional text datasets struggle to\nhandle physically grounded queries such as those that involve\ngeometric reasoning. In other words, recent large language\nmodels trained on internet data are not well equipped to\nsimple questions about a physical environment such as \u201cwho\nis to my left?\u201d. Grounding large-language models may allow\nthem more powerful reasoning; and vice versa, may help us\nuse them as agent controllers.\nIn this work we describe a data source (i.e., a toolbox to\ngenerate data) designed to help train ML models grounded in\na physical environment, allowing them to make the connec-\ntion between perception, memory, and reasoning. It consists\nof context-question-answer triples, where the context corre-\nsponds to a dynamic and agent-affected 3d gridworld, and\nthe questions may involve temporal or spatial reasoning, as\nwell as questions about the agent\u2019s own actions. A sample\ngenerated from our data source is shown in Fig. 1.\nWhile the environment allows rendering the world-context\nas a sequence of images, one of our goals is to support re-\nsearch toward answering the question \u201cwhat are good for-\nmats for agent memory systems?\u201d. In pursuit of this, we\nabstract the context to a database format that does not require\nany perceptual modules, and provide code for converting the\ndatabase into a templated text dump, as demonstrated in Fig. 1\n(right top). Here, the order of the facts within each timestep\nare written randomly, and sequentially written according to\nthe timesteps. Our hope is that the data source can be used\nfor augmenting the training (or allowing the assembly) of rea-\nsoning embodied agents by bringing to bear the advances in\nreasoning in language models, or as a supplement to training\nlanguage models with grounding from embodied agents.\nWe train baseline neural models to represent the database\nand process the queries. These include finetuning pre-trained\nlanguage models on the text version of the database, and\nTransformers that input the structured database directly. We\nfind that while certain queries are easily solved by these\nbaselines, others, in particular - those having to deal with\nspatial geometry, are more difficult.\nIn short, the contributions of this paper are:\nEnvironment: We introduce an environment for embodied\nagents and a data source for generating data to train agents\nin this environment (detailed in the Environment, Queries,\nand Data section). We provide the code to generate world\ncontexts, as well as complex queries. We hope this will aid\nresearchers to isolate and tackle difficult problems for reason-\ning for embodied agents.\nBaselines: We evaluate the abilities of baseline models to\nanswer queries in this environment (Experiments section).\nWe compare different representations of the world context,\nincluding a pure text based representation as well as a more\nstructured representation.\nEnvironment, Queries, and Data\nWe propose a context-question-answer data generator for\nembodied agents. In this section, we outline the context, or\nenvironment we generate data in, the types of queries we\ncreate for the agent to solve, and specifics of the data samples.\nEnvironment\nWe work in a finite three-dimensional gridworld. There is\na primary agent, zero or more other agents, zero or more\nget/fetchable items, zero or more placeable/breakable blocks.\nThe other agents come in two types: they might represent\nhuman \u201cplayers\u201d that can give commands to the agent and\nhave the same capabilities as the agent, or animate non-player\ncharacters (NPCs) that follow simple random movement pat-\nterns. The placeable/breakable blocks have colors and integer\ngrid coordinates; all other objects have float coordinates. An-\nimate objects (the players, NPCs, and the agent) have a yaw\nand pitch pose representing the location they are looking, in\naddition to three location coordinates.\nTo build a scene, we generate some random objects\n(spheres, cubes, etc.), randomly place a number of NPCs,\na player, and an agent. With some probability, the agent exe-\ncutes a command (to either: build an object, destroy an object,\nmove to a location, dig a hole, follow an NPC, etc.) The ex-\necution of a command is scripted; the task executor is from\nthe Minecraft agent in (Pratik et al. 2021). Whether or not\nthe agent executes a task, the world steps a fixed number of\ntimes (so, e.g., NPCs may move or act). In the experiments\ndescribed below, the world is fully observed at a fixed num-\nber of temporal snapshots, and all poses, object locations and\nNPC movements are recorded. However, not every world step\nis snapshotted, so the total sequence is not fully observed.\nFollowing (Pratik et al. 2021), the environment is presented\nto the agent as an object-centered key-value store. Each ob-\nject, NPC, and the agent\u2019s self have a \u201cmemid\u201d keying a data\nstructure that depends on the object type, and may contain\nstring data (for example a name) or float or integer data (e.g.\nthe pose of an NPC). The key-value store also has subject-\npredicate-object triples (e.g. \"memid has tag mottled\"); the\ntriples themselves also have a unique memid as key.\nThe generated scene presented as the key-value store de-\nfines the agent\u2019s context, C. In our experiments below, we\nrepresent this context in one of two ways. The first is a text\nsequence (Ct), where for each snapshot step, all objects and\ntheir properties in the key-value store are flattened into a\ntemplated language, as shown in Fig. 1 (right). Multiple time\nsnapshots of the context are represented by appending each\nsuccessive event in the sequence. While this is in some sense\nsimplistic, it allows ready application of large pre-trained\nlanguage models for ingesting the context; and this kind of\napproach has been shown to be effective in (Thorne et al.\n2021; Liu et al. 2021).\nAlternatively, we can leverage the relational properties of\nthe context by representing it as a graph where objects and\nproperties are nodes, and the connections between them are\nthe edges. For example, if we know that \u201cbob\u201d is a horse and\nis the color brown, the node for bob connects to a \u201chorse\u201d and\na \u201cbrown\u201d object. Specifically, this \u201cSequence Context\u201d con-\ntains reference object nodes (CR), which are the object instan-\ntiations, and triple nodes (CT ), which are the properties of the\nreference objects. Each reference object node holds the fol-\nlowing information: reference_object_hash (R_id) is a unique\nidentifier for each reference object, reference_objects_words\nholds identifier words of the object such as its name, and\nreference_objects_float is the floating point properties of the\nFigure 2: Structured context + Transformer model. The bottom left demonstrates the structured representation of the 3d gridworld,\nwhere R_id is the unique reference object identifier, and T_id is the unique triple property identifier (which connects to one of the\nreference objects via R_id). Context nodes (C{R,t}) and query tokens (Qt) are first featurized with learnable embedding layers.\nWe process the featurized context and query jointly with a Transformer encoder that considers the context structure via relational\nembeddings (rij). Finally, a text decoder predicts tokens, and a memid decoder predicts relevant context memids (not pictured).\nobject such as its (x, y, z) coordinates and pitch/yaw. These\nare combined into a single node (detailed in the Models\nsection). Similarly, each property triple is composed of the\nfollowing elements: triples_hash (T_id) contains a unique\nidentifier for the triple, as well as the reference object hash\nthat it is linked to, and triples_words are a descriptive text\nof a reference object\u2019s property such as \u201chas_color blue\u201d.\nThese are also combined into a single node. We therefore\nhave nodes of both reference objects and triples, and the\nhashes encompass the edges or relationships between them.\nWe do not consider the text sequence or graph-structured\nrepresentations to be canonical. On the contrary, our goal\nis stimulate research into what the correct representation of\nworld state should be to allow easier model training and\ntransfer between agents. We simply provide these two repre-\nsentations as examples and baselines.\nGiven the determined set of snapshots, queries are de-\nsigned to be answerable. That is, all information is known\nto answer the questions. We leave to future work making\nambiguous queries, but note here that it is not difficult to\npurposefully build and record queries that could theoretically\nbe answered in some scene but cannot be answered in the par-\nticular scene instantiation, for example because they refer to\nan event that occurred between snapshots. It would otherwise\nbe easy to restrict full observability within each snapshot.\nQueries\nThe embodied agent environment allows for a rich set of\npossible queries to ask the agent. We structure the types of\nqueries we use into three main categories, as covered in Ta-\nble 1. Property queries are those which operate on the current\nstate of the memory such as the current properties or loca-\ntions of an object, and are given with an explicit relation.\nProperty queries with a single clause can be read directly\nfrom the database or text dump without any \u201creasoning\u201d.\nTemporal queries are those which operate over spans of the\nmemory such as the movement of object. Geometric queries\nare those concerned with the geometric structure of the en-\nvironment such as how far away two object are from each\nother. Note that many queries are mixes of these types, and\nthe categorization is blurred.\nWithin each query class, there are several different \u201cclause\u201d\ntypes. These can be combined as applicable into a multi-\nclause query, where the clauses are combined by an \u201cand\u201d or\n\u201cor\u201d conjunction randomly. In addition, each clause can be\nnegated by prepending the word \u201cnot\u201d. For example, \u201cwhat\nare the name of the objects that do not have the property\nbrown and where the x coordinate is less than 4\u201d.\nThe query, Q, is composed of one or both of the following\nrepresentations: query_text (Qt): a text representation of the\nquery (e.g. \u201cfind the reference_objects with color brown\u201d),\nquery_tree_logical_form (Qlf) : a tree logical form represen-\ntation of the query (e.g. two clauses in an \u201cand\u201d or \u201cor\u201d query\nare connected by a parent in the tree).\nGiven the context and query, the agent should return some\nanswer, A. Depending on the query type, we randomly ask for\none of the following answer types in the query: name (\u201cwhat\nis the name of...\u201d), properties (\u201cwhat are the properties of...\u201d),\nlocation (\u201cwhat is the location of...\u201d), distance (\u201chow far\nis...\u201d), and count (\u201chow many...\u201d). In general, we are interested\nin predicting the text answer such as \u201cbrown\u201d for the query\n\u201cwhat is the color of the horse?\u201d. However, we may also\nbe interested in pointing to the relevant context objects or\nproperties (in the structural context representation). Thus, for\neach query, we provide both the text answer as well as the\nrelevant context node IDs (from CR and CT ).\nData\nWith this data generation framework, we can create arbitrary\namounts of simulated data. Each data sample contains a (C,\nQ, A) triple. There are several parameters of the world that\naffect the difficulty of question answering, including the size\nQuery Class\nClause types\nExample\nProperty\nname\nwhat are the properties of the objects that have the name alice?\ntag\nwhat are the names of the objects that has the property brown?\nabsolute cardinal\nwhat are the locations of the objects where the x coordinate is less than 4?\nTemporal\ncardinal\nwhat is the name of the object that increased x the most?\nrelative\nwhat is the name of the object that moved to my left the most?\nfarthest moved object\nwhat is the name of the object that moved the farthest?\nlocation at time\nwhat was the location of bob at the beginning?\naction\nwhat did you do?\nobject tracking\nwhere would the ball be if i moved to (4,7,2)?\nGeometric\nabsolute distance\nwhat is the count of the objects where the distance to (2, 6, 5) is greater than 3?\ndirection\nwhat are the names of the objects to my right?\nclosest object\nwhat is the name of the object that is closest to the cow?\nmax direction\nwhat is the name of the object that is the most to my right?\ndistance between\nhow far is the horse from you?\ndistance from position\nwhat is the location 3 steps to your right?\nTable 1: Query clause types. We categorize the queries we can ask the agent into three separate classes. Within each class, there\nare several clause types. Italicized clauses cannot be combined with others.\nof the 3d gridworld (e.g., 15x15x15), the set of possible ob-\njects in the world (e.g., bob, alice, ...), and the set of possible\nproperties that can belong to the objects (e.g. cow, horse, blue,\ngreen, ...). The number of time-steps and number of snapshots\nalso crucially affect the difficulty of the problem. Similarly,\nthe distributions over the queries (e.g., how many and what\nkind of clauses) can make the problem more difficult.\nRelated Work\nReal-world QA datasets have long been used to test different\naspects of ML model performance such as reading compre-\nhension (Rajpurkar et al. 2016; Hill et al. 2016), common-\nsense reasoning (Talmor et al. 2019), multi-hop reason-\ning (Yang et al. 2018b), and visual understanding (Agrawal\net al. 2015; Hudson and Manning 2019). While real-world\ndatasets can provide reliable performance benchmarks and\nbetter approximate the problems faced by practitioners, syn-\nthetic datasets can allow for more control and the ability\nto isolate the exact limitations of current models. Notably,\nbAbI (Weston et al. 2016) is a set of toy QA tasks testing\nvarious reasoning abilities over short text stories that showed\nthe limitations recurrent neural networks. Since proposed,\nall bAbI tasks have been solved by novel memory architec-\ntures (Henaff et al. 2017; Dehghani et al. 2019). An gridworld\nenvironment for embodied agents with language instructions\nfor tasks is described in (Chevalier-Boisvert et al. 2018); our\nwork here is complementary, giving question-answer pairs\nbased on abstracted environment histories.\nCLEVR (Johnson et al. 2017) is a popular synthetic data\nfor testing visual reasoning given text queries. (Yi et al. 2020)\nextends CLEVR to reasoning over temporal events in videos.\nEmbodied question answering (EmbodiedQA) (Das et al.\n2018) proposes a task where an agent must navigate an envi-\nronment in order to answer a question. VideoNavQA (Cangea\net al. 2019) was proposed in the EmbodiedQA domain to eval-\nuate short video-question pairs. Our work has elements of\neach these. The agent is embodied, and might need to an-\nswer questions about its actions or hypotheticals, but does\nnot need to act or change the current state of the environment\nto answer (as in EmbodiedQA). In comparison to the Embod-\niedQA dataset where the agent has to query the environment\nto get more information, our setting doesn\u2019t require the agent\nto interact and get more information. As in (Yi et al. 2020),\nthe agent needs to be able to reason over spatio-temporal\nevents, in our case, including its own actions. As in CLEVR,\nwe use programmatically generated queries to probe various\nreasoning modalities. One large difference between this work\nand those is that we do not focus on computer vision. While\nit is possible to render the scenes from our data generator,\nour goal is to be agnostic about perceptual modality and ab-\nstract away perceptual modeling, and to the extent possible,\nfocus on the reasoning aspects of the data. Within the vision\ncommunity, other works have approached the VQA problem\nfrom this angle (Yi et al. 2018).\nBecause the agent\u2019s abstracted world representation has\na database-like structure, our work falls into the literature\non ML for question answering on structured data, for exam-\nple (Pasupat and Liang 2015). Our structured Transformer\nbaseline is inspired by the literature on neural database rep-\nresentation, for example (Wang et al. 2019; Yin et al. 2020),\nand references therein. Our LM baseline is inspired by the\nmany works that flatten or otherwise textify databases, and\nuse pretrained language models as bases for neural query ex-\necutors, e.g. (Thorne et al. 2020, 2021; Liu et al. 2021). There\nare other fundamentally different approaches than these for\nneural query execution, for example (Ren, Hu, and Leskovec\n2020); our hope is that our data source is useful for exploring\nthese. (Tuan et al. 2022) introduce a Transformer to gener-\nate responses to questions by reasoning over differentiable\nknowledge graphs in both task-oriented and domain specific\nchit-chat dialogues. Our structured neural memory baseline\nfollows works such as (Locatello et al. 2020; Santoro et al.\n2018). In this work, the relational \u201cobjects\u2019 do not need to\nbe discovered by the learning algorithm, and their proper-\nties explicitly given to the model to use in featurizing the\nobjects. Our work is most related to the pigpen environment\nExact Match Error\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nProperty Temporal Geometric\nAll\nSequence + GPT-2\nStructured + Transformer\n0\n200\n400\n600\n800\n1,000\n0\n1\n2\n3\nEpochs\nLoss\nSequence + GPT-2\nStructured + Transformer\nTrain Text Loss\n0\n200\n400\n600\n800\n1,000\n0\n1\n2\n3\nEpochs\nLoss\nSequence + GPT-2\nStructured + Transformer\nVal Text Loss\nFigure 3: (left): Exact match error for the four different generated datasets. Sequence Context + GPT-2 outperform the Structured\n+ Transformer method in all datasets. (middle, right): Loss curves for the All queries dataset. We show the mean loss with\nmin/max error bars over all hyperparameters for the first 1,000 epochs. The pre-trained GPT-2 model learns much faster than the\nfrom-scratch relational model.\nof (Zellers et al. 2021). In comparison to that work, ours uses\nmore impoverished templated language, but has a larger and\nmore flexible space of queries. The orientation of our work\nis also different: in (Zellers et al. 2021) the learner is given\nonly a few labeled QA examples (or dynamics prediction\nexamples) but can use many \u201cunsupervised\u201d state-transitions\nto build a world-dynamics model. In our work, we allow large\nnumbers of labeled context-query-answer examples; but the\ndifficulty of the queries makes the task non-trivial.\nTextWorld (C\u00f4t\u00e9 et al. 2018) and and QAit (Yuan et al.\n2019) are text-based environments for game play and ques-\ntion answering that require interactive reasoning. The main\ndifference is that our generator is grounded in a 3D gridworld\nscene, and there is both a user and interactive agent.\nWe build our data generator on top of the Droidlet agent\n(Pratik et al. 2021), in part using the grammar in (Srinet et al.\n2020) to generate the queries and using the Droidlet agent\nmemory to execute them. This work points the way towards\nusing neural networks to execute the functions of the Droidlet\nagent memory (and hopefully can be used as a resource for\ntraining other agent-memory models).\nExperiments\nSince the agent\u2019s memory or state can take different forms\n(sequence and structured), we compare two separate models\nfor answering queries about the context of the world. We con-\nsider four different datasets for our experiments, as covered\nin Table 1: Property queries, Temporal queries, Geometric\nqueries, and All queries, where All queries is the three pre-\nvious categories combined (each query type has roughly the\nsame likelihood of occurring - we provide the configuration\nfiles in the code).\nEach of these except for Properties (which do not require\ntemporal information) is generated using two time snapshots\nwith 50 world steps, which gives enough steps for actions to\noccur. Properties queries use one snapshot and zero world\nsteps. For all queries, we place five NPCs in the world, one\nof which is a \u201cplayer\u201d that might have given a command. For\nall query types, we choose the world to be 15x15x15.\nModels\nWe generate data as described in the Environment, Queries,\nand Data section, and analyze the performance of some base-\nline models trained on this data.\nText Sequence Context.\nSince the text sequence form of\nthe context is English text, we use a language model to read\nthe context (Ct) and query (Qt), and predict the correct an-\nswer tokens sequentially (if there are multiple outputs, they\nare ordered alphabetically). We use the pretrained GPT-2\nsmall model (Radford et al. 2019) from the HuggingFace\nlibrary (Wolf et al. 2019) (licensed under the Apache License\n2.0) to predict all relevant tokens sequentially:\n\u02c6\nW = GPT2([Ct, Qt]),\n(1)\nwhere \u02c6\nW \u2208 RL\u00d7V is a soft-max normalized matrix of se-\nquence length L and vocabulary size V , and [ ] is the concate-\nnation operation. This model is fine-tuned using a sum of the\ncross entropy between each token prediction \u02c6\nWi and ground\ntruth token Wi:\nLtext = \u2212\nX\ni\u2208S\nX\nj\u2208V\nWij log \u02c6\nWij.\n(2)\nStructured Context.\nWhile the text Sequence Context is\nuseful in that it allows easy application of standard pre-\ntrained models, it may be that for certain tasks other rep-\nresentations are more appropriate. We also show results with\nsimple models that are designed around the relational struc-\nture of the context. Given a set of \u03c1 reference objects and \u03c4\ntriples, we first featurize the nodes using a learned convolu-\ntional layer given the word, float, and hash values as outlined\nin the Environment, Queries, and Data section. The output\nof the featurizer layer gives reference object embeddings\nCR \u2208 R\u03c1\u00d7d, and triple embeddings CT \u2208 R\u03c4\u00d7d. Similarly,\ntext queries Qt \u2208 Rr\u00d7d are created using a learned lookup\ntable from the query tokens. We use a Transformer (Vaswani\net al. 2017) encoder to process the context and query. The\noutput of the encoder is then used to predict both: the text\nanswer as well as the relevant context memory IDs. We use\na Transformer decoder for the text prediction, and a simple\nlinear layer to predict the memory values:\n104\n106\nTraining data size\n20\n40\n60\n80\nValidation error (%)\nProperty\n104\n106\nTraining data size\n20\n40\n60\n80\nTemporal\n104\n106\nTraining data size\n20\n40\n60\n80\nGeometric\n104\n106\nTraining data size\n20\n40\n60\n80\nAll\nSequence + GPT-2\nStructured + Transformer\nFigure 4: Validation set exact match error using varying amounts of training data. The pre-trained GPT-2 model is particularly\nuseful when the number of training samples is small, and when reasoning over proprty queries. On the other hand, non-pre-trained\nGPT-2 style models do not learn, see Table 2. Both models require more than 100K training samples to achieve the best results.\n(C\u2032\nT , C\u2032\nR, Q\u2032\nt) = Encoder([CT , CR, Qt])\n(3)\n\u02c6\nm = MemidDecoder([C\u2032\nR, C\u2032\nT ])\n(4)\n\u02c6\nw = TextDecoder([C\u2032\nT , C\u2032\nR, Q\u2032\nt]),\n(5)\nwhere \u02c6\nm \u2208 R\u03c1 is predicted relevant memids, and \u02c6\nw \u2208\nRV represents predicted relevant text tokens. This model\nis trained using a cross-entropy text token loss (eq 2) and\ncross-entropy memid loss from the relevant memids m \u2208 R\u03c1:\nLmemid = \u2212\nC\nX\ni=1\n\u02c6mi log (mi) .\n(6)\nThe total loss is a weighted sum of the two losses: Ltext +\n\u03bb \u00b7 Lmemid, where \u03bb = 0.5.\nIn the Sequence Context, the entities are represented as\na set, with defined relationships between them. Therefore,\nencoding temporal information is not straightforward. To do\nso, we add a special \u201ctime\u201d embedding to each Sequence\nContext node. That is, for timestep 0, we add the time=0\nembedding, for timestep 1, we add the time=1 embedding.\nFor the Sequence Context, the model will return the rele-\nvant memory IDs, a text sequence answer, or both. The text\noutput prediction is shown in Fig. 2.\nRelational Embedding.\nOne important form of structure\nin data is relations where entities are connected to other enti-\nties. In our data, for example, triple nodes are connected to\nobject nodes by R_id. Since the vanilla Transformer without\npositional embeddings treats input tokens as a set, it lacks\na mechanism for taking account of this relational informa-\ntion. Thus, we propose a novel way of encoding relational\ninformation directly into the self-attention mechanism of the\nSequence Context Transformer model. Specifically, we add\nan extra term to the softmax attention:\naij = Softmax(qT\ni kj + qT\ni rij)\n(7)\nHere rij is a relation embedding vector corresponding to the\nrelation type between token i and j. Relation embeddings can\nbe used to encode various types of relations. We note that the\ncommonly used relative position embeddings (Sukhbaatar\net al. 2015; Shaw, Uszkoreit, and Vaswani 2018) are a spe-\ncial case of relation embeddings where rij = ei\u2212j; more\nsophisticated relation embeddings have appeared in (Bergen,\nO\u2019Donnell, and Bahdanau 2021).\nModel and Training Details\nAll our models are trained using Adam (Kingma and Ba\n2014) for 5,000 epochs, where each epoch is over a chunk\nof 10,000 training samples. Since we are generating the data,\nwe vary the training samples from 1k to 1M, and use a vali-\ndation set of 10k samples. We use a linear warmup of 10,000\nsteps and cosine decay (Loshchilov and Hutter 2016). For the\nGPT2 model, we consider learning rates {1e-4, 5e-4, 1e-5}\nusing a batch size of 32. For the structured model, we con-\nsider learning rates {1e-4, 5e-4, 1e-5}, batch size 32, layers\n{2, 3}, and embedding dimensions {256, 512}. Hyperparam-\neters were chosen with arbitrary initial values and increased\nuntil validation performance decreased or resources were\ndepleted. The best performing structured model has 74.58M\nparameters, whereas GPT-2 small has 325.89M. All words\nare encoded with the GPT-2 tokenizer.\nResults\nFig. 3 (left) shows the results for the four different dataset\nversions we consider. We report the exact match error for all\ndata splits. That is,\nExact Match Error = 1\nn\nn\nX\ni=1\nI\n\u0000yi \u0338= \u02c6yi\u0001\n,\n(8)\nwhere yi \u2208 RN are the N ground truth tokens for sample i,\nand \u02c6yi are the top N predicted tokens. Fig. 3 (right) shows\nthe loss curves for the All queries dataset.\nFor property and geometric queries, the Sequence Context\n+ GPT-2 method performs the best. Since GPT-2 is pre-trained\non millions of samples, it can easily learn basic property\nqueries such as \u201cwhat is the color of bob?\u201d. GPT-2 has a\nnear zero test set loss for the properties queries. Geometric\nqueries, while difficult for both models to answer is solved\nmore effectively by GPT-2.\nTable 2 (top) shows GPT-2 model variation studies for the\nAll queries dataset. Notably, we test the performance of a\nSequence Context + randomly initialized GPT-2 model (i.e.\none with the same architecture, but not pretrained). We see\nModel\nVariation\nTrain Loss\nTest Loss\nTest Err\nGPT-2\nsm + rand init\n0.361\n1.912\n47.1%\nsm + pretrain\n0.015\n0.710\n14.9%\nmed + pretrain\n0.012\n0.635\n13.8%\nTransfomer\nrelational emb\n0.230\n0.921\n19.7%\nrandom hash\n0.630\n1.393\n30.0%\nTable 2: Variations of models (sm=small, med=medium). Pre-\ntraining is a key component of the GPT-2 model success, even\nwhen given large numbers of training examples. Relational\nembeddings result in a slightly lower test loss than random-\nhash in the graph-structured models.\nthat the performance is worse than that of the Structured Con-\ntext + Transformer. This indicates that the primary reason\nfor the GPT-2 model achieving the best results is due to the\npretraining. Table 2 (bottom) shows a Structured Context +\nTransformer method variation for the All queries dataset. We\nconsider an alternative to the relational embeddings proposed\nin the Models section, which is adding random hash embed-\ndings to the nodes, or tokens, which are connected to each\nother in the Sequence Context representation. This method\nperforms slightly worse than relational embeddings, hinting\nthat a more explicit representation of the context structure is\nimportant. Fig. 5 shows a 2-snapshot scene from our test set.\nFinally, Fig. 4 shows the result of our baseline models\nwhen using varying amounts of training samples. Since our\ndata source can generate arbitrary amounts of data, it\u2019s im-\nportant to understand how much data is needed to solve\ncertain problems. Notably, most of the datasets require at\nleast 100,000 samples to achieve a reasonable validation loss.\nIn addition, the Sequence + GPT-2 model significantly out-\nperforms the Structured + Transformer model on the datasets\nwith small amounts of training samples.\nConclusion\nIn this work, we introduced a framework for generating world\nstate contexts with agents, queries, and their answers. This\nprovides researchers with a flexible sandbox for training and\ntesting reasoning in embodied agents. Notably, our sequential\ndata format lets us easily evaluate the ability of large language\nmodels such as GPT to understand a physical world.\nWe show baseline results with two representations of the\nworld state context: a pure text sequence representation that\ncan be used by any off-the-shelf language model, as well as\na structured representation of the entities in the world. We\ndemonstrate the ability of the models to answer queries in\nseveral query domains, such as temporal or geometric.\nWe emphasize that our goal in this work is not to create a\nfixed static data set, but a resource for generating data. Many\nof our experimental choices (w.r.t. environment and query\ndifficulty) were aimed to allow the baseline models some\ntraction, but not able to solve the tasks completely.\nIn particular, we find it likely that large pre-trained lan-\nguage models could do better than the GPT-2 base models\nwe used with the described data generation parameters. On\nthe other hand, the difficulty of the problem can be scaled\ntrivially by increasing the number of possible reference ob-\nt=0\nt=1\nagent\nspeaker\nhoney\n(chicken)\n(sheep)\n(sheep)\nchicken\n(sheep)\nSequence Context + GPT-2: \u201choney\u201d\nStructured Context + Transformer: \u201choney\u201d\nSequence Context: \"agent is at location (13,7,0). agent has pitch -1.1 and \nyaw -3.1. agent has property agent. agent has property self. axle is at \nlocation (0,7,14). axle has pitch 0.5 and yaw 1.4. axle has property \nchicken. axle has property gray\u2026agent has pitch -1.1 and yaw -3.1. agent \nhas property agent. agent has property self. axle is at location (1,7,14).\u2026\"\nQuery\nAnswer\nSequence \n+ GPT-2\nStructured + \nTransformer\nwhich object moved the farthest?\nhoney\nhoney\nhoney\nwhat is the count of the object \nthat has the property sheep?\n2\n2\n2\nwhat is the name of the object \nthat is to my front?\naxle\nbart\nagent\nagent\nspeaker\nhoney\naxle\njohn\nbart\nhoney\nagent\nspeaker\naxle\njohn\nbart\nSequence Context: \"agent is at location (13,7,0). agent \nhas pitch -1.1 and yaw -3.1. agent has property agent. \nagent has property self. axle is at location (0,7,14). axle \nhas pitch 0.5 and yaw 1.4. axle has property chicken. \naxle has property gray. john is at location (6,7,8). john \nhas pitch -0.8 and yaw -0.5. john has property sheep. \njohn has property white. bart is at location (2,7,12). \nbart has pitch 0.8 and yaw 2.7. bart has property \nsheep. bart has property white. honey is at location \n(3,7,0). honey has pitch -1.3 and yaw -0.5. honey has \nproperty chicken. honey has property black. speaker is \nat location (2,7,9). speaker has pitch -0.4 and yaw 0.6. \ninst_seg is at location (8,6,4). inst_seg has property \nhollow_cube. inst_seg is at location (12,6,2). inst_seg \nhas property hole. inst_seg is at location (7,6,4). \ninst_seg has property hole. agent is at location \n(13,7,0). agent has pitch -1.1 and yaw -3.1. agent has \nproperty agent. agent has property self. axle is at \nlocation (1,7,14). axle has pitch 0.5 and yaw 1.4. axle \nhas property chicken. axle has property gray. john is at \nlocation (6,7,8). john has pitch -0.8 and yaw -0.5. john \nhas property sheep. john has property white. bart is at \nlocation (1,7,12). bart has pitch 0.8 and yaw 2.7. bart \nhas property sheep. bart has property white. honey is \nat location (5,7,4). honey has pitch -1.3 and yaw -0.5. \nhoney has property chicken. honey has property black. \nspeaker is at location (2,7,9). speaker has pitch -0.4 \nand yaw 0.6. inst_seg is at location (8,6,4). inst_seg \nhas property hollow_cube. inst_seg is at location \n(12,6,2). inst_seg has property hole. inst_seg is at \nlocation (7,6,4). inst_seg has property hole\"\nFigure 5: Sample scene with two snapshots. The chicken\nnamed \u201choney\u201d moves from the bottom right toward the cen-\nter, and some other objects move slightly. Both models cor-\nrectly answer the query \u201cwhich object moved the farthest?\u201d.\njects and properties, by increasing the number of reference\nobjects in the world and increasing the number of instantiated\nproperties; by making the world bigger, and by increasing\nthe time-length of episodes and the number of snapshots\nrecorded. If nothing else, these changes would quickly lead\nto the context being too large to fit in the memory of a stan-\ndard LM, and necessitating a large-memory LM (Lample et al.\n2019; Rae et al. 2019; Beltagy, Peters, and Cohan 2020); or\nleading to other approaches e.g. (Ji et al. 2017). Other, more\nsubtle difficulties can be introduced in straightforward ways\nby reducing full observability, or otherwise asking queries\nthat cannot be answered with the information in the context,\nrequiring agents some amount of meta-cognition and/or envi-\nronment actions to find the answers, as in (Das et al. 2018).\nIn future work, we plan to introduce more query types,\nincluding arithmetic and hypotheticals. We hope that re-\nsearchers will use the data generator as a flexible resource\nfor augmenting their own agent training or LM pre-training,\nor for exploring new models for database reasoning.\nReferences\nAgrawal, A.; Lu, J.; Antol, S.; Mitchell, M.; Zitnick, C. L.;\nParikh, D.; and Batra, D. 2015. VQA: Visual Question An-\nswering. International Journal of Computer Vision, 123:\n4\u201331.\nAhn, M.; Brohan, A.; Brown, N.; Chebotar, Y.; Cortes,\nO.; David, B.; Finn, C.; Gopalakrishnan, K.; Hausman, K.;\nHerzog, A.; et al. 2022.\nDo As I Can, Not As I Say:\nGrounding Language in Robotic Affordances. arXiv preprint\narXiv:2204.01691.\nBaker, B.; Akkaya, I.; Zhokhov, P.; Huizinga, J.; Tang, J.;\nEcoffet, A.; Houghton, B.; Sampedro, R.; and Clune, J. 2022.\nVideo PreTraining (VPT): Learning to Act by Watching Un-\nlabeled Online Videos. arXiv preprint arXiv:2206.11795.\nBeltagy, I.; Peters, M. E.; and Cohan, A. 2020.\nLong-\nformer: The long-document transformer.\narXiv preprint\narXiv:2004.05150.\nBergen, L.; O\u2019Donnell, T.; and Bahdanau, D. 2021. Sys-\ntematic generalization with edge transformers. Advances in\nNeural Information Processing Systems, 34.\nCangea, C.; Belilovsky, E.; Li\u00f2, P.; and Courville, A. 2019.\nVideonavqa: Bridging the gap between visual and embodied\nquestion answering. arXiv preprint arXiv:1908.04950.\nChevalier-Boisvert, M.; Bahdanau, D.; Lahlou, S.; Willems,\nL.; Saharia, C.; Nguyen, T. H.; and Bengio, Y. 2018. Babyai:\nA platform to study the sample efficiency of grounded lan-\nguage learning. arXiv preprint arXiv:1810.08272.\nChoromanski, K.; Likhosherstov, V.; Dohan, D.; Song, X.;\nGane, A.; Sarlos, T.; Hawkins, P.; Davis, J.; Mohiuddin, A.;\nKaiser, L.; et al. 2020. Rethinking attention with performers.\narXiv preprint arXiv:2009.14794.\nChowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra,\nG.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.;\nGehrmann, S.; et al. 2022. Palm: Scaling language mod-\neling with pathways. arXiv preprint arXiv:2204.02311.\nC\u00f4t\u00e9, M.-A.; K\u00e1d\u00e1r, A.; Yuan, X.; Kybartas, B.; Barnes, T.;\nFine, E.; Moore, J.; Hausknecht, M.; Asri, L. E.; Adada, M.;\net al. 2018. Textworld: A learning environment for text-based\ngames. In Workshop on Computer Games, 41\u201375. Springer.\nDas, A.; Datta, S.; Gkioxari, G.; Lee, S.; Parikh, D.; and Batra,\nD. 2018. Embodied question answering. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, 1\u201310.\nDehghani, M.; Gouws, S.; Vinyals, O.; Uszkoreit, J.;\nand Kaiser, L. 2019.\nUniversal Transformers.\nArXiv,\nabs/1807.03819.\nFan, L.; Wang, G.; Jiang, Y.; Mandlekar, A.; Yang, Y.;\nZhu, H.; Tang, A.; Huang, D.-A.; Zhu, Y.; and Anandku-\nmar, A. 2022. MineDojo: Building Open-Ended Embod-\nied Agents with Internet-Scale Knowledge. arXiv preprint\narXiv:2206.08853.\nHenaff, M.; Weston, J.; Szlam, A. D.; Bordes, A.; and LeCun,\nY. 2017. Tracking the World State with Recurrent Entity\nNetworks. ArXiv, abs/1612.03969.\nHill, F.; Bordes, A.; Chopra, S.; and Weston, J. 2016. The\nGoldilocks Principle: Reading Children\u2019s Books with Ex-\nplicit Memory Representations. CoRR, abs/1511.02301.\nHuang, W.; Abbeel, P.; Pathak, D.; and Mordatch, I. 2022.\nLanguage Models as Zero-Shot Planners: Extracting Ac-\ntionable Knowledge for Embodied Agents. arXiv preprint\narXiv:2201.07207.\nHudson, D. A.; and Manning, C. D. 2019. GQA: A New\nDataset for Real-World Visual Reasoning and Compositional\nQuestion Answering. 2019 IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 6693\u20136702.\nJi, Y.; Tan, C.; Martschat, S.; Choi, Y.; and Smith, N. A. 2017.\nDynamic entity representations in neural language models.\narXiv preprint arXiv:1708.00781.\nJohnson, J.; Hariharan, B.; van der Maaten, L.; Fei-Fei, L.;\nZitnick, C. L.; and Girshick, R. B. 2017. CLEVR: A Diag-\nnostic Dataset for Compositional Language and Elementary\nVisual Reasoning. 2017 IEEE Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), 1988\u20131997.\nKingma, D. P.; and Ba, J. 2014. Adam: A method for stochas-\ntic optimization. arXiv preprint arXiv:1412.6980.\nKrishna, R.; Zhu, Y.; Groth, O.; Johnson, J.; Hata, K.; Kravitz,\nJ.; Chen, S.; Kalantidis, Y.; Li, L.; Shamma, D. A.; Bernstein,\nM. S.; and Fei-Fei, L. 2016. Visual Genome: Connecting\nLanguage and Vision Using Crowdsourced Dense Image\nAnnotations. CoRR, abs/1602.07332.\nLample, G.; Sablayrolles, A.; Ranzato, M.; Denoyer, L.; and\nJ\u00e9gou, H. 2019. Large memory layers with product keys.\nAdvances in Neural Information Processing Systems, 32.\nLiu, Q.; Chen, B.; Guo, J.; Lin, Z.; and Lou, J. 2021. TAPEX:\nTable Pre-training via Learning a Neural SQL Executor.\nCoRR, abs/2107.07653.\nLocatello, F.; Weissenborn, D.; Unterthiner, T.; Mahendran,\nA.; Heigold, G.; Uszkoreit, J.; Dosovitskiy, A.; and Kipf, T.\n2020. Object-centric learning with slot attention. Advances in\nNeural Information Processing Systems, 33: 11525\u201311538.\nLoshchilov, I.; and Hutter, F. 2016. SGDR: Stochastic Gradi-\nent Descent with Restarts. CoRR, abs/1608.03983.\nNye, M.; Andreassen, A. J.; Gur-Ari, G.; Michalewski, H.;\nAustin, J.; Bieber, D.; Dohan, D.; Lewkowycz, A.; Bosma,\nM.; Luan, D.; et al. 2021. Show Your Work: Scratchpads\nfor Intermediate Computation with Language Models. arXiv\npreprint arXiv:2112.00114.\nPasupat, P.; and Liang, P. 2015. Compositional Semantic\nParsing on Semi-Structured Tables. In ACL.\nPaszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.;\nChanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga,\nL.; et al. 1912.\nPytorch: An imperative style, high-\nperformance deep learning library. arXiv 2019.\narXiv\npreprint arXiv:1912.01703.\nPetroni, F.; Piktus, A.; Fan, A.; Lewis, P.; Yazdani, M.;\nDe Cao, N.; Thorne, J.; Jernite, Y.; Karpukhin, V.; Maillard,\nJ.; et al. 2020. KILT: a benchmark for knowledge intensive\nlanguage tasks. arXiv preprint arXiv:2009.02252.\nPratik, A.; Chintala, S.; Srinet, K.; Gandhi, D.; Qian, R.; Sun,\nY.; Drew, R.; Elkafrawy, S.; Tiwari, A.; Hart, T.; et al. 2021.\ndroidlet: modular, heterogenous, multi-modal agents. In 2021\nIEEE International Conference on Robotics and Automation\n(ICRA), 13716\u201313723. IEEE.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.;\nSutskever, I.; et al. 2019. Language models are unsupervised\nmultitask learners. OpenAI blog, 1(8): 9.\nRae, J. W.; Potapenko, A.; Jayakumar, S. M.; and Lillicrap,\nT. P. 2019. Compressive transformers for long-range se-\nquence modelling. arXiv preprint arXiv:1911.05507.\nRajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016.\nSQuAD: 100,000+ Questions for Machine Comprehension\nof Text. In EMNLP.\nRen, H.; Hu, W.; and Leskovec, J. 2020. Query2box: Rea-\nsoning over knowledge graphs in vector space using box\nembeddings. arXiv preprint arXiv:2002.05969.\nSantoro, A.; Faulkner, R.; Raposo, D.; Rae, J.; Chrzanowski,\nM.; Weber, T.; Wierstra, D.; Vinyals, O.; Pascanu, R.; and\nLillicrap, T. 2018.\nRelational recurrent neural networks.\nAdvances in neural information processing systems, 31.\nShaw, P.; Uszkoreit, J.; and Vaswani, A. 2018. Self-Attention\nwith Relative Position Representations. In NAACL-HLT (2).\nSrinet, K.; Jernite, Y.; Gray, J.; and Szlam, A. 2020. CraftAs-\nsist instruction parsing: Semantic parsing for a voxel-world\nassistant. In Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, 4693\u20134714.\nSukhbaatar, S.; Ju, D.; Poff, S.; Roller, S.; Szlam, A. D.;\nWeston, J.; and Fan, A. 2021. Not All Memories are Created\nEqual: Learning to Forget by Expiring. In ICML.\nSukhbaatar, S.; Weston, J.; Fergus, R.; et al. 2015. End-\nto-end memory networks. Advances in neural information\nprocessing systems, 28.\nSzlam, A.; Gray, J.; Srinet, K.; Jernite, Y.; Joulin, A.; Syn-\nnaeve, G.; Kiela, D.; Yu, H.; Chen, Z.; Goyal, S.; et al.\n2019. Why Build an Assistant in Minecraft? arXiv preprint\narXiv:1907.09273.\nTalmor, A.; Herzig, J.; Lourie, N.; and Berant, J. 2019. Com-\nmonsenseQA: A Question Answering Challenge Targeting\nCommonsense Knowledge. ArXiv, abs/1811.00937.\nThorne, J.; Yazdani, M.; Saeidi, M.; Silvestri, F.; Riedel,\nS.; and Halevy, A. 2020. Neural databases. arXiv preprint\narXiv:2010.06973.\nThorne, J.; Yazdani, M.; Saeidi, M.; Silvestri, F.; Riedel, S.;\nand Halevy, A. 2021. Database reasoning over text. arXiv\npreprint arXiv:2106.01074.\nTuan, Y.-L.; Beygi, S.; Fazel-Zarandi, M.; Gao, Q.; Cervone,\nA.; and Wang, W. Y. 2022. Towards Large-Scale Interpretable\nKnowledge Graph Reasoning for Dialogue Systems. arXiv\npreprint arXiv:2203.10610.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.;\nGomez, A. N.; Kaiser, \u0141.; and Polosukhin, I. 2017. Attention\nis all you need. In Advances in neural information processing\nsystems, 5998\u20136008.\nVedantam, R.; Szlam, A.; Nickel, M.; Morcos, A.; and Lake,\nB. M. 2021. Curi: A benchmark for productive concept\nlearning under uncertainty. In International Conference on\nMachine Learning, 10519\u201310529. PMLR.\nWang, B.; Shin, R.; Liu, X.; Polozov, O.; and Richardson, M.\n2019. Rat-sql: Relation-aware schema encoding and linking\nfor text-to-sql parsers. arXiv preprint arXiv:1911.04942.\nWang, X.; Wei, J.; Schuurmans, D.; Le, Q.; Chi, E.; and Zhou,\nD. 2022. Self-consistency improves chain of thought reason-\ning in language models. arXiv preprint arXiv:2203.11171.\nWeston, J.; Bordes, A.; Chopra, S.; and Mikolov, T. 2016.\nTowards AI-Complete Question Answering: A Set of Prereq-\nuisite Toy Tasks. arXiv: Artificial Intelligence.\nWolf, T.; Debut, L.; Sanh, V.; Chaumond, J.; Delangue, C.;\nMoi, A.; Cistac, P.; Rault, T.; Louf, R.; Funtowicz, M.; et al.\n2019. Huggingface\u2019s transformers: State-of-the-art natural\nlanguage processing. arXiv preprint arXiv:1910.03771.\nYang, Z.; Qi, P.; Zhang, S.; Bengio, Y.; Cohen, W. W.;\nSalakhutdinov, R.; and Manning, C. D. 2018a. HotpotQA: A\ndataset for diverse, explainable multi-hop question answering.\narXiv preprint arXiv:1809.09600.\nYang, Z.; Qi, P.; Zhang, S.; Bengio, Y.; Cohen, W. W.;\nSalakhutdinov, R.; and Manning, C. D. 2018b. HotpotQA:\nA Dataset for Diverse, Explainable Multi-hop Question An-\nswering. In EMNLP.\nYi, K.; Gan, C.; Li, Y.; Kohli, P.; Wu, J.; Torralba, A.;\nand Tenenbaum, J. B. 2020.\nCLEVRER: CoLlision\nEvents for Video REpresentation and Reasoning. ArXiv,\nabs/1910.01442.\nYi, K.; Wu, J.; Gan, C.; Torralba, A.; Kohli, P.; and Tenen-\nbaum, J. 2018. Neural-symbolic vqa: Disentangling reason-\ning from vision and language understanding. Advances in\nneural information processing systems, 31.\nYin, P.; Neubig, G.; Yih, W.-t.; and Riedel, S. 2020. TaBERT:\nPretraining for joint understanding of textual and tabular data.\narXiv preprint arXiv:2005.08314.\nYuan, X.; C\u00f4t\u00e9, M.-A.; Fu, J.; Lin, Z.; Pal, C.; Bengio, Y.; and\nTrischler, A. 2019. Interactive language learning by question\nanswering. arXiv preprint arXiv:1908.10909.\nZambaldi, V.; Raposo, D.; Santoro, A.; Bapst, V.; Li, Y.;\nBabuschkin, I.; Tuyls, K.; Reichert, D.; Lillicrap, T.; Lock-\nhart, E.; et al. 2018. Deep reinforcement learning with re-\nlational inductive biases. In International conference on\nlearning representations.\nZellers, R.; Holtzman, A.; Peters, M.; Mottaghi, R.; Kemb-\nhavi, A.; Farhadi, A.; and Choi, Y. 2021. Piglet: Language\ngrounding through neuro-symbolic interaction in a 3d world.\narXiv preprint arXiv:2106.00188.\nZhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.;\nChen, S.; Dewan, C.; Diab, M.; Li, X.; Lin, X. V.; et al. 2022.\nOPT: Open Pre-trained Transformer Language Models. arXiv\npreprint arXiv:2205.01068.\nZhong, V.; Rockt\u00e4schel, T.; and Grefenstette, E. 2019. Rtfm:\nGeneralising to novel environment dynamics via reading.\narXiv preprint arXiv:1910.08210.\nAppendix\nScene, Query, and Training Details\nScene generation: Figure 6 shows the components of build-\ning a scene in our 3d gridworld. Parameters of the scene gen-\nerator include: world size, number of timesteps, number of\nagents, number of players, number of NPCs, (names, proper-\nties, locations, pitch/yaw of the NPCs and agent), and actions.\nIn our experiments, we use 1 agent, 1 player, 4 NPCs gener-\nated from a set of 254 names (Abigail, Ace, Adam, ...), 5 NPC\ntypes (cow, pig, rabbit, chicken, sheep), 6 NPC colors (brown,\nwhite, black, mottled, pink, yellow), and up to 5 shape\ntypes for block objects (hole, cube, hollow_cube, rectangu-\nloid, hollow_rectanguloid, sphere, spherical_shell, pyramid,\nsquare, rectangle, circle, disk, triangle, dome, arch, ellipsoid,\nhollow_trianlge, hollow_rectangle, rectanguloid_frame). At\neach timestep, the agent, player, and NPCs can perform ac-\ntions (move, build, destroy, follow, command). Some of these\nactions create new blocks or destroy existing ones. Note that\nthe actions are not explicitly recorded in the context C. Ac-\ntions such as \u201cmove\u201d are implictly recorded by the object\nchanging its location, building by the existence of new block\nobjects, etc.\nQuery specifications: Figure 7 shows an overview of the\nquery generation process. Currently there are 13 different\nclause types, some of which can be combined with others\nto create multi-clause queries. There are 4 different standard\nreturn types for the query: \u201cname\u201d (e.g. \u201cwhat is the name of\n...\u201d), \u201ctag\u201d (e.g. \u201cwhat is a tag of ...\u201d), \u201ccount\u201d (\u201chow many\n... are there \u201d), and \u201clocation\u201d (\u201c what is the location of ...\u201d).\nNot every one of these is applicable for each of the 13 clause\nor query types. Specifically:\n\u2022 For agent action queries (what did you do?), the return\nis always an action name, and none of the above entity-\ncentric questions.\n\u2022 For geometric minimax queries (closest object, max direc-\ntion), the return is always the name or tag of the object.\n\u2022 For all other query types where only one output is possible\n(based on the query alone, and not on the state or history\nof the world), count queries are not allowed.\nCompute, resources, and availability Each model is\ntrained on an internal cluster, using 4-16 Nvidia V100 GPUs.\nTraining takes roughly 72 hours for each model. The best per-\nforming model has 74.58M, whereas the GPT-2 small model\nhas 325.89M parameters. All hyperparameter configurations\nand PyTorch (Paszke et al. 1912) code will be released to\nboth generate the data and train the models.\nDetailed analysis\nMemid loss. While the main objective is to predict the text\nanswer, for the structured context representation, we are\nalso interested in predicting relevant context node IDs (i.e.\n\u201cmemids\u201d). Figure 9 shows the memid loss (Lmemid) for the\nAll queries dataset. Note that the GPT-2 model does not op-\nerate on the structured memory nodes, so there is no memid\nloss. This loss is lower than the text loss since the model only\nhas to output the correct IDs from the small set of objects\nin the world, whereas the model has to predict the correct\ntext tokens from all possible tokens (50,257 for the GPT-2\ntokenizer). We observe that without the memid loss, the text\nprediction performance suffers.\nGenerating more difficult datasets. In the main\nmanuscript, we show the results of baseline models where\nevery scene has 4 NPCs in a 15x15x15 gridworld. We can\ngenerate scenes according to our specifications, making it\nsimple to test the model capabilities on samples of varying\ndifficulty. Figure 10 shows the results when adjusting two\nparameters of the world for the All queries dataset: larger\ngridworld size, and more NPCs. The baseline is a 4 NPC,\n15x15x15 world. The first variation is a 4 NPC, 30x30x30\nworld, and the second is a 8 NPC, 15x15x15 world. The per-\nformance remains roughly the same for the larger world size.\nWe attribute this to the fact that the models already have trou-\nble solving the geometric queries in the smaller world size,\nso increasing it doesn\u2019t degrade the performance. However,\nthe performance does decrease when we increase the number\nof NPCs. Notably, the Structured+Transformer model begins\nto outperform the Sequence+GPT-2 model in this setting.\nThis is likely in part because in a world with more NPCs, the\nsequence length becomes longer than 1024 tokens, which is\nthe length GPT-2 is trained on. As we clip the context at 1024\ntokens, it is not able to answer queries where the relevant\ninformation is outside of the 1024 tokens. This highlights\none inherent issue with naively using a pre-trained language\nmodel on this task. This problem will also be exacerbated\nwhen we increase the number of timesteps. However, note\nthe literature on \u201clong-memory\u201d Transformer text models,\ne.g. (Lample et al. 2019; Rae et al. 2019; Choromanski et al.\n2020; Beltagy, Peters, and Cohan 2020; Sukhbaatar et al.\n2021).\nThere exist many similar datasets, with key differences in\nterms of the types of inputs available, and whether there is an\nactive agent. Table 3 shows a comparison of our dataset vs.\nexisting ones. The key differentiation is that we can represent\nthe data in several forms: visual, relational, and text, as show\nin in Figure 8.\nOut of domain generalization. In real world scenes, new\nobjects and properties may arise at any given time. For ex-\nample, the agent may encounter an animal with a new color\nthat we\u2019ve never seen before, but it should still be able to\nanswer queries about it. To understand how the model per-\nforms in this setting, we generate a new Properties dataset\ntest set with all new color properties that were not seeing dur-\ning training (e.g. green , orange). Table 4 shows the results\non the original colors Properties dataset vs the new colors\nProperties dataset. The Sequence+GPT-2 model generalizes\nsignificantly better than the Structured+Transformer model\nthat was not pretrained.\nMinecraft Setting\nMinecraft is a suitable environment for agent reasoning for\na variety of reasons. (Szlam et al. 2019) argue that the con-\nstraints of the Minecraft world (e.g. 3-d voxel grid, simple\nphysics) and the regularities in the head of the distribution\nof in-game tasks present a rich setting for natural language\nand world model understanding. Concurrent with our work,\ntwo works train video-based agents in the Minecraft setting.\n(Fan et al. 2022) introduce Minecraft simulation suite with\nthousands of open-ended tasks and an internet-scale knowl-\nedge base with Minecraft videos, tutorials, wiki pages, and\nforum discussions. They also introduce an agent that uses pre-\ntrained video-language models as a reward function. (Baker\net al. 2022) train a video-model to predict the next action\nbased on a scene in Minecraft.\nAdditional Examples\nFigure 11 shows more examples of our environment with\npredictions from the two baseline models. Neither the\nSequence+GPT-2 nor Structured+Transformer model are able\nto accurately answer many of the queries, motivating the need\nfor more advanced modeling techniques. While these exam-\nples show basic gridworld examples, we can generate arbi-\ntrarily more difficult worlds, as described in Section . These\nexamples demonstrate a small subset of possible worlds and\nqueries generated from our data source. The potential scenes\nand queries that researchers can create are extensive, and can\nbe carefully tailed toward the specific needs of a particular\nagent being trained.\npitch/yaw\nname\ntype\nlocation\nlocation\ntype\npitch/yaw\nlocation\npitch/yaw\nlocation\nbuild\nworld size\n# timesteps\ncolor\nname\nname\ndestroy\nmove \nfollow\nmove \ncommand\nmove\n# snapshots\nFigure 6: Scene context (C) components. Our data source allows us to create highly modular and variable scenes. We first select\na gridworld size, number of timesteps and number of snapshots, and then randomly generate a number of objects in the world,\nincluding the agent, the player, NPCs, and blocks. Certain objects can perform actions such as move and build.\nFigure 7: Query Generation Flowchart. italicized clauses in the tree are standalone clauses, and cannot be combined with others.\nNon-italicized clauses can be combined with conjunctions to form multi-clause queries. Some return types are not compatible\nwith certain clauses.\nT_id:3\nR_id:1\nblue\nR_id:2\nmarbles\n(3,5,7)\nR_id:1\nwilson\n(9,3,6)\nT_id:6\nR_id:2\nchicken\nT_id:5\nR_id:2\nbrown\nT_id:4\nR_id:1\nrabbit\nR_id:0\nagent\n(2,7,4)\nwilson\nspeaker\nagent\nmarbles\npebbles\n\"agent is at location (10,7,8). agent has pitch 1.0 and yaw 2.5. \nagent has property agent. speaker is at location (8,7,0). speaker has \npitch 0.8 and yaw 2.3. wilson is at location (0,7,9). wilson has pitch \n-0.8 and yaw 1.1. wilson has property rabbit. wilson has property \nbrown. marbles is at location (10,7,3). marbles has pitch 1.1 and \nyaw 3.1. marbles has property chicken. marbles has property \nmottled. pebbles is at location (10,10,13). pebbles has pitch 0.5 \nand yaw -0.3. pebbles has property sheep. pebbles has property \nmottled. gus is at location (3,7,9). gus has pitch 0.4 and yaw -0.1. \ngus has property cow. gus has property mottled. inst_seg is at \nlocation (3,8,1). inst_seg has property arch. inst_seg is at location \n(10,8,12). inst_seg has property dome. inst_seg is at location \n(14,6,6). inst_seg has property hole. inst_seg is at location \n(12,5,10). inst_seg has property hole.\"\n(A) Visual \n(B) Text \n(C) Relational \nFigure 8: Three different data modalities available from our data generator for a single scene. (A) visual: not used in our models\nsince we\u2019re primarily interested in learning memory representations. (B) text: flattened text used by the Sequence + GPT-2 model.\n(C) relational: structured object and property nodes used by the Structured + Transformer model.\n0\n200\n400\n600\n800\n1,000\n0\n0.5\n1\n1.5\nEpochs\nLoss\nStructured + Transformer\nTrain Memid Loss\n0\n200\n400\n600\n800\n1,000\n0\n0.5\n1\n1.5\nEpochs\nLoss\nStructured + Transformer\nVal Memid Loss\nFigure 9: Memid loss for the All queries dataset. This loss is minimized jointly with the text loss for the Structured+Transformer\nmodel. The memid prediction task is a simpler task than predicting the text tokens since the model only has to select from objects\npresent in the scene vs all possible tokens.\nDataset\nAvailable data types\nEmbodied Agent\nDataset size\nCLEVR (Johnson et al. 2017)\nVisual\n1M\nEmbodiedQA (Das et al. 2018)\nVisual\n\u2713\n9K\nbAbI (Weston et al. 2016)\nText\n20K*\nBabyAI (Chevalier-Boisvert et al. 2018)\nText\n\u2713\n5K*\nTextWorld (C\u00f4t\u00e9 et al. 2018)\nText\n150K\nPIGPeN (Zellers et al. 2021)\nText\n280k\nQAit (Yuan et al. 2019)\nText\n500*\nOurs\nVisual, Text, Relational\n\u2713\n1M*\nTable 3: Comparison of different QA datasets. \u201c*\u201d specifies that an unlimited amount of samples can potentially be generated.\nOriginal Colors\nNew Colors\nLoss\nExact Match Err\nLoss\nExact Match Err\nSequence+GPT-2\n0.060\n5.0%\n0.647\n16.3%\nStructured+Transformer\n0.092\n9.4%\n0.704\n37.0 %\nTable 4: Out of domain generalization on the Properties dataset. The \u201cOriginal Colors\u201d test dataset uses the color properties\nsuch as \u201cwhite\u201d that the models were trained on. The \u201cNew Colors\u201d test dataset exclusively uses new color properties such as\n\u201cgreen\u201d and \u201cpurple\u201d that were never seen during training. The Structured+Transformer model sees a significant reduction in\nperformance when introducing new color properties, while the Sequence+GPT model is relatively robust.\n0\n200\n400\n600\n800\n1,000\n0\n1\n2\n3\nEpochs\nLoss\nTransformer: 4 NPCs, 15x15x15 world\nTransformer: 8 NPCs, 15x15x15 world\nTransformer: 4 NPCs, 30x30x30 world\nGPT-2: 4 NPCs, 15x15x15 world\nGPT-2: 8 NPCs, 15x15x15 world\nGPT-2: 4 NPCs, 30x30x30 world\nTrain Text Loss\n0\n200\n400\n600\n800\n1,000\n0\n1\n2\n3\nEpochs\nLoss\nTransformer: 4 NPCs, 15x15x15 world\nTransformer: 8 NPCs, 15x15x15 world\nTransformer: 4 NPCs, 30x30x30 world\nGPT-2: 4 NPCs, 15x15x15 world\nGPT-2: 8 NPCs, 15x15x15 world\nGPT-2: 4 NPCs, 30x30x30 world\nVal Text Loss\nFigure 1: Results from different dataset complexities for the Structured+Transformer model\n1\nFigure 10: Varying dataset complexity. We vary two dataset parameters to make the queries more difficult: increasing the number\nof NPCs from 4 to 8, and increasing the world size from 15x15x15 to 30x30x30. We observe that as the number of NPCs grows,\nthe queries become more difficult to answer, but the Structured+Transformer model begins to outperform the Sequence+GPT-2\nmodel (on this dataset, the best GPT-2 val loss is 1.055, and the best Transformer val loss is 0.915).\nt=0\nt=1\n)\n(sheep)\nchicken\n(sheep)\nPT-2: \u201choney\u201d\nTransformer: \u201choney\u201d\nSequence Context: \"agent is at location (6,6,6). agent has pitch -0.3 and yaw -0.3. agent has property agent. agent has property self. jenna is at location \n(14,7,7). jenna has pitch -0.8 and yaw -1.4. jenna has property cow. jenna has property cow. jenna has property white. itsy is at location (0,7,13). itsy has pitch \n1.4 and yaw -0.7. itsy has property pig. itsy has property white. itsy-bitsy is at location (11,7,5). itsy-bitsy has pitch 1.1 and yaw -1.4. itsy-bitsy has property pig. \nitsy-bitsy has property white. arrow is at location (9,7,8). arrow has pitch -0.3 and yaw -1.1. arrow has property rabbit. arrow has property mottled. speaker is \nat location (10,7,2). speaker has pitch 1.2 and yaw -2.0. inst_seg is at location (10,3,11). inst_seg has property triangle. inst_seg is at location (1,4,9). inst_seg \nhas property hollow_cube. inst_seg is at location (4,5,4). inst_seg has property hole. agent is at location (6,6,6). agent has pitch -0.3 and yaw -0.3. agent has \nproperty agent. agent has property self. jenna is at location (14,7,7). jenna has pitch -0.8 and yaw -1.4. jenna has property cow. jenna has property white. itsy \nis at location (1,7,13). itsy has pitch 1.4 and yaw -0.7. itsy has property pig. itsy has property white. itsy-bitsy is at location (12,7,3). itsy-bitsy has pitch 1.1 and \nyaw -1.4. itsy-bitsy has property pig. itsy-bitsy has property white. arrow is at location (10,7,8). arrow has pitch -0.3 and yaw -1.1. arrow has property rabbit. \narrow has property mottled. speaker is at location (10,7,2). speaker has pitch 1.2 and yaw -2.0. inst_seg is at location (10,3,11). inst_seg has property triangle. \ninst_seg is at location (1,4,9). inst_seg has property hollow_cube. inst_seg is at location (4,5,4). inst_seg has property hole\"\nQuery: which object increased x the most?\nAnswer: itsy\nSequence + GPT-2: itsy-bitsy\nStructured + Transformer: itsy\nitsy\nitsy-bitsy\narrow\njenna\nagent\nspeaker\nitsy\nitsy-bitsy\narrow\njenna\nagent\nspeaker\nSequence Context: \"age\nand yaw 2.5. agent has p\nsarah has pitch -0.8 and \nhas property brown. hud\n1.1 and yaw 3.1. hudson\nmottled. pebbles is at lo\nyaw -0.3. pebbles has pr\ngus is at location (3,7,9).\nproperty cow. gus has pr\n(8,7,0). speaker has pitc\n(3,8,1). inst_seg has pro\ninst_seg has property do\nhas property hole. inst_s\nproperty hole.\"\nagent\nspeaker\nhudson\n(chicken)\nsarah\n(rabbit)\npebbles\n(sheep)\nSequence Context + GPT-2: \u201chudson\u201d\nStructured Context + Transformer: \n\u201chudson\u201d\nQuery: \u201cwhat is closest from speaker?\u201d\nAnswer: \u201chudson\u201d\nSequence Context: \"agent is at location (1,7,5). agent has pitch 0.0 \nand yaw 0.7. agent has property agent. agent has property self. \nhugh is at location (9,7,3). hugh has pitch 0.9 and yaw -2.4. hugh \nhas property chicken. hugh has property white. honey is at location \n(2,7,6). honey has pitch 0.1 and yaw -2.8. honey has property \nsheep. honey has property gray. athena is at location (1,7,11). \nathena has pitch -0.7 and yaw -3.1. athena has property cow. \nathena has property white. jewels is at location (5,7,0). jewels has \npitch 0.2 and yaw -1.7. jewels has property cow. jewels has property \nwhite. speaker is at location (2,7,14). speaker has pitch 1.3 and yaw \n-0.9\"\nQuery: what is the name of the object that has the property white \nand where the z coordinate is greater than 7?\nAnswer: athena\nSequence + GPT: athena\nStructured + Transformer: jewel\nagent\nspeaker\nathena\nhoney\njewels\nt=1\nSequence Context: \"age\nand yaw 2.5. agent has p\nsarah has pitch -0.8 and \nhas property brown. hud\n1.1 and yaw 3.1. hudson\nmottled. pebbles is at lo\nyaw -0.3. pebbles has pr\ngus is at location (3,7,9).\nproperty cow. gus has pr\n(8,7,0). speaker has pitch\n(3,8,1). inst_seg has prop\ninst_seg has property do\nhas property hole. inst_s\nproperty hole.\"\nagent\nspeaker\nhudson\n(chicken)\nsarah\n(rabbit)\npebbles\n(sheep)\nSequence Context + GPT-2: \u201chudson\u201d\nStructured Context + Transformer: \n\u201chudson\u201d\nQuery: \u201cwhat is closest from speaker?\u201d\nAnswer: \u201chudson\u201d\nSequence Context: \u201cagent is at location (10,7,1). agent has pitch 0.2 \nand yaw -3.0. agent has property agent. agent has property self. \njewel is at location (2,7,8). jewel has pitch -1.4 and yaw -2.0. jewel \nhas property cow. jewel has property gray. jester is at location \n(2,7,10). jester has pitch -0.2 and yaw -2.7. jester has property cow.  \njester has property brown. biablo is at location (14,7,11). biablo has \npitch 0.8 and yaw 0.1. biablo has property chicken. biablo has \nproperty gray. sara is at location (7,7,3). sara has pitch 1.4 and yaw \n1.8. sara has property sheep. sara has property brown. speaker is at \nlocation (11,7,9). speaker has pitch -1.4 and yaw -1.2. inst_seg is at \nlocation (10,8,2). inst_seg has property spherical_shell.\u201d\nQuery: what is farthest from sara?\nAnswer: biablo\nSequence + GPT: biablo\nStructured + Transformer: biablo\nagent\nspeaker\nbiablo\nsara\njester\njewel\nt=1\nFigure 11: Top: Temporal query. In this example, both of the \u201cpig\u201d NPCs increased their x value, but the one named \u201citsy\u201d\nincreased the most. The GPT-2 model could not accurately predict the right answer. Middle: Multi-clause property query. In this\nexample, but \u201cathena\u201d and \u201cjewel\u201d have the property \u201cwhite\u201d, but only athena has a z coordinate greater than 7. The Transformer\nmodel could not accurately predict the right answer. Note that in queries such as this which don\u2019t require temporal reasoning, we\nconsider the most recent timestep (t=1 in our expriments). Bottom: Geometric query. In this example, the NPC named \u201cbiablo\u201d\nis the farthest from \u201csara\u201d in the gridworld.\n"
  },
  {
    "title": "Viewpoint Textual Inversion: Unleashing Novel View Synthesis with Pretrained 2D Diffusion Models",
    "link": "https://arxiv.org/pdf/2309.07986.pdf",
    "upvote": "1",
    "text": "Viewpoint Textual Inversion: Unleashing Novel View Synthesis with Pretrained\n2D Diffusion Models\nJames Burgess\nKuan-Chieh Wang\nSerena Yeung\nStanford University\n{jmhb,wangkua1,syyeung}@stanford.edu\njmhb0.github.io/viewneti/\nObject  \ntoken,      \nCLIP \ntext\nControl 3D camera viewpoint in the di\ufb00usion text space\nInput: 1 or more views\nOutput: novel view synthesis samples\n120\n530\n<latexit sha1_base64=\"Kymy+o8k6gaRaO+hWl/QgFcgs=\">AB+HicbZDJSgNBEIZ7XGNcMurRS2MQPIUZcT\nsGvXiMYBbIDKGnU5M06VnorhHikCfx4kERrz6KN9/GTjIHTfyh4eOvKqr6D1IpNDrOt7Wyura+sVnaKm/v7O5V7P2Dlk4yxaHJE5moTsA0SBFDEwVK6KQKWBRIaAej2m9/QhKiyR+wHEKfsQGsQgFZ2isnl3xINVCGvRwCMh6dtWpOTPRZ\nXALqJCjZ795fUTnkUQI5dM67rpOjnTKHgEiZlL9OQMj5iA+gajFkE2s9nh0/oiXH6NEyUeTHSmft7ImeR1uMoMJ0Rw6FerE3N/2rdDMNrPxdxmiHEfL4ozCTFhE5ToH2hgKMcG2BcCXMr5UOmGEeTVdmE4C5+eRlaZzX3snZxf16t3xRx\nlMgROSanxCVXpE7uSIM0CScZeSav5M16sl6sd+tj3rpiFTOH5I+szx8vUpNy</latexit>\u270f\u2713\n<latexit sha1_base64=\"EhJvR+FcGmeQNcGEKVICgM7VLI=\"\n>AB83icjVDLSsNAFL3xWeur6tLNYBFclUR8LYtuXFaxD2hCmUwn7dDJMzcCX0N9y4UMStP+POv3HSdqGi4IELh3Pu5R5OmEph0\nHU/nIXFpeWV1dJaeX1jc2u7srPbMkmGW+yRCa6E1LDpVC8iQIl76Sa0ziUvB2Orgq/fc+1EYm6w3HKg5gOlIgEo2gl348pDsMov5\n30RK9S9WruFORvUoU5Gr3Ku9PWBZzhUxSY7qem2KQU42CST4p+5nhKWUjOuBdSxWNuQnyaeYJObRKn0SJtqOQTNWvFzmNjRnHod0s\nMpqfXiH+5nUzjC6CXKg0Q67Y7FGUSYIJKQogfaE5Qzm2hDItbFbChlRThram8v9KaB3XvLPa6c1JtX45r6ME+3AR+DBOdThGhrQB\nAYpPMATPDuZ8+i8OK+z1QVnfrMH3+C8fQJM25Hf</latexit>Ri\n<latexit sha1_base64=\"Dpe7uXeBxF3MAhOnU5kOfNbBWE8=\">AB83icjVDLSsNAFL3xWeur6tLNYBFclUR8LYtuXFaxD2hCmUwn7dDJMzcCX0\nN9y4UMStP+POv3HSdqGi4IELh3Pu5R5OmEph0HU/nIXFpeWV1dJaeX1jc2u7srPbMkmGW+yRCa6E1LDpVC8iQIl76Sa0ziUvB2Orgq/fc+1EYm6w3HKg5gOlIgEo2gl348pDsMov5303F6l6tXcKcjfpApzNHqVd7+fsCzmCpmkxnQ9N8UgpxoFk3xS9jPDU8pGdMC7lioacxPk08wTcmiVPokSbUchmapfL3Ia\nGzOQ7tZDQ/vUL8zetmGF0EuVBphlyx2aMokwQTUhRA+kJzhnJsCWVa2KyEDamDG1N5f+V0DqueWe105uTav1yXkcJ9uEAjsCDc6jDNTSgCQxSeIAneHYy59F5cV5nqwvO/GYPvsF5+wT2aJGm</latexit>R0\n<latexit sha1_base64=\"z8Be7p/WDZlMOFbxEOVBUjfXTa0=\">AB83icjVDLSsNAFL3xWeur6tLNYBFclUR8LYtuXFaxD2hCmUwn7dDJMzcCX0\nN9y4UMStP+POv3HSdqGi4IELh3Pu5R5OmEph0HU/nIXFpeWV1dJaeX1jc2u7srPbMkmGW+yRCa6E1LDpVC8iQIl76Sa0ziUvB2Orgq/fc+1EYm6w3HKg5gOlIgEo2gl348pDsMov530vF6l6tXcKcjfpApzNHqVd7+fsCzmCpmkxnQ9N8UgpxoFk3xS9jPDU8pGdMC7lioacxPk08wTcmiVPokSbUchmapfL3Ia\nGzOQ7tZDQ/vUL8zetmGF0EuVBphlyx2aMokwQTUhRA+kJzhnJsCWVa2KyEDamDG1N5f+V0DqueWe105uTav1yXkcJ9uEAjsCDc6jDNTSgCQxSeIAneHYy59F5cV5nqwvO/GYPvsF5+wT37JGn</latexit>R1\n<latexit sha1_base64=\"6Yh3ePCKB8IGVCcVpJtLSBvHUm0=\">AB83icjVDLSgNBEOyNrxhfUY9eBoPgKWyCr2PQi8co5gHZJcxOZpMhs7PLTK8Ql\nvyGFw+KePVnvPk3ziY5qChY0FBUdNFBYkUBl3wyksLa+srhXSxubW9s75d29tolTzXiLxTLW3YAaLoXiLRQoeTfRnEaB5J1gfJX7nXujYjVHU4S7kd0qEQoGEUreV5EcRSE2e20X+XK7WqOwP5m1RgWa/O4NYpZGXCGT1JhezU3Qz6hGwSflrzU8ISyMR3ynqWKRtz42SzlBxZUDCWNtRSGbq14uMRsZ\nMosBu5hnNTy8Xf/N6KYXfiZUkiJXbP4oTCXBmOQFkIHQnKGcWEKZFjYrYSOqKUNbU+l/JbTr1dpZ9fTmpNK4XNRhAM4hGOowTk04Bqa0AIGCTzAEzw7qfPovDiv89WCs7jZh29w3j4B+XCRqA=</latexit>R2\n<latexit sha1_base64=\"MZOl4KlLQD5AX6Ee9ftCgI9S+BY=\">AB83icjVDLSgNBEOyNrxhfUY9eBoPgKWx8H4NePEYxD8guYXYymwyZnV1meoWw\n5De8eFDEqz/jzb9xNslBRcGChqKqmy4qSKQw6LofTmFhcWl5pbhaWlvf2Nwqb+0TJxqxpslrHuBNRwKRvokDJO4nmNAokbwejq9xv3NtRKzucJxwP6IDJULBKFrJ8yKwyDMbie94165Uqu6U5C/SQXmaPTK714/ZmnEFTJjenW3AT9jGoUTPJyUsNTygb0QHvWqpoxI2fTNPyIFV+iSMtR2FZKp+vcho\nZMw4CuxmntH89HLxN6+bYnjhZ0IlKXLFZo/CVBKMSV4A6QvNGcqxJZRpYbMSNqSaMrQ1lf5XQuoWjurnt6cVOqX8zqKsAf7cAg1OIc6XEMDmsAgQd4gmcndR6dF+d1tlpw5je78A3O2yf69JGp</latexit>R3\n<latexit sha1_base64=\"5EnkiFBm3d2d/DZK2\n6+jp1s7exI=\">AB6nicbVDJSgNBEK2JW4xb1KOXxiB4CjPidgx68RiJWSAZQk+nJ2nSy9DdI4Qhn\n+DFgyJe/SJv/o2dZA4afVDweK+KqnpRwpmxv/lFVZW19Y3ipulre2d3b3y/kHLqFQT2iSK92JsK\nGcSdq0zHLaSTFIuK0HY1vZ37kWrDlHywk4SGAg8lixnB1kmNRl/1yxW/6s+B/pIgJxXIUe+XP3sD\nRVJBpSUcG9MN/MSGdaWEU6npV5qaILJGA9p1GJBTVhNj91ik6cMkCx0q6kRXP150SGhTETEblOg\ne3ILHsz8T+vm9r4OsyYTFJLJVksilOrEKzv9GAaUosnziCiWbuVkRGWGNiXTolF0Kw/PJf0jqrBpf\nVi/vzSu0mj6MIR3AMpxDAFdTgDurQBAJDeIXePW49+y9e+L1oKXzxzCL3gf3zSKjcM=</latexi\nt>So\nFigure 1. Text-to-image diffusion models like Stable Diffusion encode 3D understanding in their latent space. Left: our method, Viewpoint\nNeural Textual Inversion (ViewNeTI), takes camera viewpoint parameters, Ri, and a scene-specific token, So, to predict a latent in the\nCLIP output text space. We then sample from a frozen Stable Diffusion [41] model conditioned on this text latent to produce an image of\nthe So object from camera view Ri. We learn how to predict this text condition using textual inversion (TI) [1, 13]. Right: our viewpoint\ncontrol mechanism enables leveraging pretrained 2D diffusion models for novel view synthesis from only a single view; we can learn\nfrom more views if available. The generations are photorealistic, have good semantic details, and generate reasonable completions under\nambiguity.\nAbstract\nText-to-image diffusion models understand spatial rela-\ntionship between objects, but do they represent the true\n3D structure of the world from only 2D supervision? We\ndemonstrate that yes, 3D knowledge is encoded in 2D im-\nage diffusion models like Stable Diffusion, and we show that\nthis structure can be exploited for 3D vision tasks. Our\nmethod, Viewpoint Neural Textual Inversion (ViewNeTI),\ncontrols the 3D viewpoint of objects in generated images\nfrom frozen diffusion models. We train a small neural map-\nper to take camera viewpoint parameters and predict text\nencoder latents; the latents then condition the diffusion gen-\neration process to produce images with the desired camera\nviewpoint.\nViewNeTI naturally addresses Novel View Synthesis\n(NVS). By leveraging the frozen diffusion model as a prior,\nwe can solve NVS with very few input views; we can even\ndo single-view novel view synthesis. Our single-view NVS\npredictions have good semantic details and photorealism\ncompared to prior methods. Our approach is well suited\nfor modeling the uncertainty inherent in sparse 3D vision\nproblems because it can efficiently generate diverse sam-\nples.\nOur view-control mechanism is general, and can\neven change the camera view in images generated by user-\ndefined prompts. Code is available at our project website.\n1. Introduction\nText-to-image diffusion models trained on web-scale\ndatasets have shown impressive capabilities in reasoning\nabout objects, the composition of multiple objects, and 2D\n1\narXiv:2309.07986v1  [cs.CV]  14 Sep 2023\nInput\n\u201cA car on the beach\u201d\n\u201cA car on a shiny surface\u201d\n670\n880\nFigure 2. Diffusion infilling models do 3d reasoning. Left: input\nmasked object. Middle: the generated shadows are consistent with\nthe shadows on the car and the shadows in the sand. Right: the\nground has object reflections.\nspatial layout [18, 41, 44, 50, 52]. Despite being trained\non 2D image data, these models seem able to do 3D rea-\nsoning: in a simple experiment, we ask a Stable Diffu-\nsion model\n[41] to infill the background around a car,\nand find that it generates 3D-consistent shadows and reflec-\ntions (Fig. 2). In this work, we investigate how to extract\n3D knowledge from a pretrained diffusion model\u2019s latent\nspace, and how to leverage that knowledge for 3D vision\ntasks. Utilizing pretrained diffusion models is appealing for\ntwo reasons. First, because large and unposed 2D datasets\n[36, 46] are cheaper to procure than 3D datasets [10, 38]\nor multi-view datasets [9, 21, 33], diffusion models cover\na larger distribution of concepts; this has motivated recent\nwork on lifting 2D knowledge from diffusion models to 3D\n[22, 35, 49, 54, 64]. Second, since diffusion models are\ngenerative, they are well-suited for modeling the ambiguity\nthat naturally arises in 3D vision due to incomplete input in-\nformation, for example in sparse-view novel view synthesis\n[11, 27, 32].\nOur key insight is that the diffusion model text encoder\nspace embeds learnable concepts for 3D reasoning. Our\nmethod, Viewpoint Neural Textual Inversion (ViewNeTI),\ncontrols the 3D viewpoint of objects via the text encoder\nlatent space, while keeping the object details consistent.\nSpecifically, we train a small neural mapper that takes cam-\nera parameters and predicts a text encoding; the text encod-\ning then conditions the diffusion process to produce images\nwith the desired camera view ( Fig. 1). The ViewNeTI map-\nper weights are optimized using Textual Inversion (TI), an\napproach common in personalization and content creation\n[1, 13, 24, 43]. TI adds novel concepts to the diffusion\nmodel vocabulary, such as content (e.g. a user\u2019s particular\ndog) or styles (e.g. a particular artist\u2019s visual aesthetic) by\noptimizing new word embeddings to condition the diffusion\nmodel image generation. Here, we apply textual inversion\nto add viewpoint control words to the diffusion model vo-\ncabulary. To our knowledge, this is the first application of\nTI for doing semantically-consistent image transformations.\nWe leverage ViewNeTI in a new formulation of novel\nview synthesis (NVS) by conditioning diffusion model gen-\neration on camera parameters. We first optimize ViewNeTI\non a single scene using a small set of multi-view images.\nWithout access to any prior 3D data, the mapper general-\nizes to novel views, as long as those views are \u2018interpola-\ntions\u2019 of the training views. Seeking a more generic con-\ntrol mechanism that does viewpoint extrapolation, we pre-\ntrain ViewNeTI on multiple scenes with a shared coordi-\nnate system. The pretrained ViewNeTI mapper generalizes\nto novel scenes, enabling synthesis of novel views far from\nthe input views with little data; it can even do NVS from\na single image. Compared to existing single-image NVS\nmethods, ViewNeTI has several advantages, especially in\nsingle-view NVS. It produces views with photorealistic de-\ntails for real-world objects that are in the massive 2D train-\ning distribution of 2D diffusion models like Stable Diffu-\nsion. Once trained, it can generate diverse predictions un-\nder uncertainty in close to real time. The pre-training multi-\nview dataset is small (less than 50 scenes) and is cheap to\ntrain (less than one day with one GPU). Moreover, the view-\ncontrol generalizes to new object scenes under distribution\nshift of object semantics. Finally, we show that ViewNeTI\nhas the potential for more diverse applications, such as con-\ntent creation: by adding ViewNeTI conditioning to new text\nprompts, we can control the 3D viewpoint around objects in\ngenerated images.\n2. Related work\nTextual inversion of diffusion models Personalization\naims to inject novel concepts - concepts like objects and\nstyles - into the diffusion model vocabulary using a few im-\nage examples of that concept. Textual inversion (TI) is a\npopular approach that optimizes a word embedding for each\nnew concept [13]. Compared to alternatives that fine-tune\nthe model weights [25, 43], learned concepts are editable,\nhave low storage cost, and are more portable to other copies\nof the same model [1, 8, 13]. Recent extensions train an\nencoder to map images to concept vectors [14, 60, 67, 80].\nAnother direction improves the quality and editability of the\nlearned concepts by training different text embeddings de-\npending on the noising timestep and UNet layer [1, 62, 79].\nThese are combined in the recent NeTI model [15], which\nis the current state of the art in Textual Inversion. Our work\nutilizes many of the architectural ideas used in NeTI.\nOur work is, to our knowledge, the first to use tex-\ntual inversion for controlling 3D viewpoint in images and\nthe first to use it for any 3D control of generated objects.\nWe propose an architecture for predicting text embeddings\nthat control camera viewpoint, and also contribute learning\nstrategies for adapting TI to do novel view synthesis and\nview-controlled image generation.\nSparse-view novel view synthesis One set of techniques\nfor sparse-view novel view synthesis (NVS) train a NeRF\n2\nMulti-view image dataset with camera poses \nCLIP \ntext\nViewNeTI prediction of text latents\nA \nphoto \nof \n a\nv\nv\n+\n+\nDi\ufb00usion training\n. A photo of a\n715\n<latexit sha1_base64=\"Kymy+o8k6gaRaO+hWl/QgFcgs=\">AB+HicbZDJSgNBEIZ7XGNcMurRS2MQPIUZcTsGvXiMYBbIDKGnU5M06VnorhHikCfx4kERrz6K\nN9/GTjIHTfyh4eOvKqr6D1IpNDrOt7Wyura+sVnaKm/v7O5V7P2Dlk4yxaHJE5moTsA0SBFDEwVK6KQKWBRIaAej2m9/QhKiyR+wHEKfsQGsQgFZ2isnl3xINVCGvRwCMh6dtWpOTPRZXALqJCjZ795fUTnkUQI5dM67rpOjnTKHgEiZlL9OQMj5iA+gajFkE2s9nh0/oiXH6NEyUeTHSmft7ImeR1uMoMJ0Rw6FerE3N/2rdDMNrPxdxmi\nHEfL4ozCTFhE5ToH2hgKMcG2BcCXMr5UOmGEeTVdmE4C5+eRlaZzX3snZxf16t3xRxlMgROSanxCVXpE7uSIM0CScZeSav5M16sl6sd+tj3rpiFTOH5I+szx8vUpNy</latexit>\u270f\u2713\n<latexit sha1_base64=\"EnP8i0DiLMe8AWyzGXH2S4P8HA=\">AB8HicbVDLSgNBEJz1GeMr6tHLYBAiSNgVX8egF48RzEOSJcxOepMhM7PLzKwQlnyFw+KePVzvPk3ziZ70MSChqKqm+6uIOZMG9f9dpaWV1bX1gsbxc2t7Z3d0t5+U0eJotCgE\nY9UOyAaOJPQMxwaMcKiAg4tILRbea3nkBpFskHM47BF2QgWcgoMVZ6rJjTLnB+UuyVym7VnQIvEi8nZSj3it9dfsRTQRIQznRuO5sfFTogyjHCbFbqIhJnREBtCxVBIB2k+nB0/wsVX6OIyULWnwVP09kRKh9VgEtlMQM9TzXib+53USE17KZNxYkDS2aIw4dhEOPse95kCavjYEkIVs7diOiSKUGMzykLw5l9eJM2zqndZvbg/L9du8jgK6BAdoQry0BWqoTtURw1EkUDP6BW9Ocp5cd6dj1nrkpPHKA/cD5/AE6lj3I=</latexit>(t, `)\n<latexit sha1_base64=\"5EQnho31Z9LEGUDNsf7e5vKcPIo=\">AB7nicbVBNS8NAEJ3Ur1q/qh69BIvgqSTi17HoxWMFawtKJPtpl26uwm7G6GE/gvHhTx6u/x5r9x0+agrQ8GHu/NMDMvTDjTxvO+ndLK6t\nr6RnmzsrW9s7tX3T941HGqCG2RmMeqE6KmnEnaMsxw2kURFy2g7Ht7nfqJKs1g+mElCA4FDySJG0Fip3RuiEFjpV2te3ZvBXSZ+QWpQoNmvfvUGMUkFlYZw1Lre4kJMlSGEU6nlV6qaYJkjEPatVSioDrIZudO3ROrDNwoVrakcWfq74kMhdYTEdpOgWakF71c/M/rpia6DjImk9RQSeaLopS7Jnbz390BU5QYPrEiWL2VpeMUCExNqE8BH/x5WXyeFb3L+sX9+e1xk0RxmO4BhOwYcraMAdNKEFBMbwDK/w5iTOi/PufMxbS04xcw\nh/4Hz+AMCgjzM=</latexit>\u03b3\n<latexit sha1_base64=\"5EQnho31Z9LEGUDNsf7e5vKcPIo=\">AB7nicbVBNS8NAEJ3Ur1q/qh69BIvgqSTi17HoxWMFawtKJPtpl26uwm7G6GE/gvHhTx6u/x5r9x0+agrQ8GHu/NMDMvTDjTxvO+ndLK6t\nr6RnmzsrW9s7tX3T941HGqCG2RmMeqE6KmnEnaMsxw2kURFy2g7Ht7nfqJKs1g+mElCA4FDySJG0Fip3RuiEFjpV2te3ZvBXSZ+QWpQoNmvfvUGMUkFlYZw1Lre4kJMlSGEU6nlV6qaYJkjEPatVSioDrIZudO3ROrDNwoVrakcWfq74kMhdYTEdpOgWakF71c/M/rpia6DjImk9RQSeaLopS7Jnbz390BU5QYPrEiWL2VpeMUCExNqE8BH/x5WXyeFb3L+sX9+e1xk0RxmO4BhOwYcraMAdNKEFBMbwDK/w5iTOi/PufMxbS04xcw\nh/4Hz+AMCgjzM=</latexit>\u03b3\n<latexit sha1_base64=\"TP7jNbLfyUrqjruAzV7Je2rUwQ=\">AB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyIr2XRjRuhgn1AO5RMmlDk8yYZApl6He4caGIWz/GnX9jp2Fth4IHM65\nl3tygpgzbVz32ymsrK6tbxQ3S1vbO7t75f2Dpo4SRWiDRDxS7QBrypmkDcMp+1YUSwCTlvB6DbzW2OqNIvko5nE1Bd4IFnICDZW8rsCmyHBPL2f9sa9csWtujOgZeLlpAI56r3yV7cfkURQaQjHWnc8NzZ+ipVhNpqZtoGmMywgPasVRiQbWfzkJP0YlV+iMlH3SoJn6eyPFQuJCOxkFlIvepn4n9dJTHjtp0zGiaGSzA+FCUcmQlkDqM8UJYZPLMFEMZsVkSFWmBjbU8mW4C1+eZk\n0z6reZfXi4bxSu8nrKMIRHMpeHAFNbiDOjSAwBM8wyu8OWPnxXl3PuajBSfOYQ/cD5/ABwDklY=</latexit>Mv\n<latexit sha1_base64=\"ph6s6zS/IoPrpZsu0WaRlY3i94=\">AB9HicbVDLSgMxFL1TX7W+qi7dBIvgqsyIr2XRjRuhgn1AO5RMmlDM8mYZApl6He4caGIWz/GnX9jp2Fth4IHM65l\n3tygpgzbVz32ymsrK6tbxQ3S1vbO7t75f2DpaJIrRBJeqHWBNORO0YZjhtB0riqOA01Ywus381pgqzaR4NJOY+hEeCBYygo2V/G6EzZBgnt5Pe7JXrhVdwa0TLycVCBHvVf+6vYlSIqDOFY647nxsZPsTKMcDotdRNY0xGeEA7lgocUe2ns9BTdGKVPgqlsk8YNFN/b6Q40noSBXYyC6kXvUz8z+skJrz2UybixFB5ofChCMjUdYA6jNFieETSzBRzGZFZIgVJsb2VLIleItfXibNs\n6p3Wb14OK/UbvI6inAEx3AKHlxBDe6gDg0g8ATP8Apvzth5cd6dj/lowcl3DuEPnM8fEWeSTw=</latexit>Mo\n. A photo of a\n. A photo of a\n<latexit sha1_base64=\"ZYRkb14INanFOX86IqvXgGo\ncBYM=\">AB+XicbVDLSsNAFL2pr1pfUZduBovgqiTia1l047I+oC2hMl0g6dTMLMpFBC/sSNC0Xc+ifu/Bsnb\nRbaemDgcM693DPHjzlT2nG+rdLK6tr6RnmzsrW9s7tn7x+0VJRIQpsk4pHs+FhRzgRtaqY57cS4tDntO2Pb3O/Pa\nFSsUg86WlM+yEeChYwgrWRPNt+9NJeiPXID9KHzHMyz646NWcGtEzcglShQMOzv3qDiCQhFZpwrFTXdWLdT7HUjH\nCaVXqJojEmYzykXUMFDqnqp7PkGToxygAFkTRPaDRTf2+kOFRqGvpmMg+pFr1c/M/rJjq47qdMxImgswPBQlHOk\nJ5DWjAJCWaTw3BRDKTFZERlphoU1bFlOAufnmZtM5q7mXt4v68Wr8p6ijDERzDKbhwBXW4gwY0gcAEnuEV3qzUerH\nerY/5aMkqdg7hD6zPH5ik6Y=</latexit>SR0\n<latexit sha1_base64=\"YkTxdTQ/hTptg8o5lvrKBcZ1Tlc=\">AB+XicbVDLSsNAFL2pr1pfUZduBovgqiTia1\nl047I+oC2hMl0g6dTMLMpFBC/sSNC0Xc+ifu/BsnbRbaemDgcM693DPHjzlT2nG+rdLK6tr6RnmzsrW9s7tn7x+0VJRIQpsk4pHs+FhRzgRtaqY57cS4tDntO2Pb3O/PaFSsUg86WlM+yEeChYwgrWRPNt+9NJeiPXID9KHzHMz646\nNWcGtEzcglShQMOzv3qDiCQhFZpwrFTXdWLdT7HUjHCaVXqJojEmYzykXUMFDqnqp7PkGToxygAFkTRPaDRTf2+kOFRqGvpmMg+pFr1c/M/rJjq47qdMxImgswPBQlHOkJ5DWjAJCWaTw3BRDKTFZERlphoU1bFlOAufnmZtM5q7mXt4v6\n8Wr8p6ijDERzDKbhwBXW4gwY0gcAEnuEV3qzUerHerY/5aMkqdg7hD6zPH5opk6c=</latexit>SR1\n<latexit sha1_base64=\"kaYWSxQsno+Zv2cuUdXQk8ReIq0=\">AB+XicbVDLSsNAFL3xWesr6tLNYBFclaT4Wh\nbduKyPqANYTKdtEMnkzAzKZTQP3HjQhG3/ok7/8ZJm4W2Hhg4nHMv98wJEs6Udpxva2V1bX1js7RV3t7Z3du3Dw5bKk4loU0S81h2AqwoZ4I2NdOcdhJcRw2g5Gt7nfHlOpWCye9CShXoQHgoWMYG0k37Yf/awXYT0Mwuxh6temvl1x\nqs4MaJm4BalAgYZvf/X6MUkjKjThWKmu6yTay7DUjHA6LfdSRNMRnhAu4YKHFHlZbPkU3RqlD4KY2me0Gim/t7IcKTUJArMZB5SLXq5+J/XTXV47WVMJKmgswPhSlHOkZ5DajPJCWaTwzBRDKTFZEhlphoU1bZlOAufnmZtGpV97J6cX9\neqd8UdZTgGE7gDFy4gjrcQOaQGAMz/AKb1ZmvVjv1sd8dMUqdo7gD6zPH5uk6g=</latexit>SR2\n<latexit sha1_base64=\"tnUpxOawUuFQAXfa/hnd/05vIFs=\">AB+XicbVDLSsNAFL2pr1pfUZduBovgqiTia1l047I+oC\n2hMl0g6dTMLMpFBC/sSNC0Xc+ifu/BsnbRbaemDgcM693DPHjzlT2nG+rdLK6tr6RnmzsrW9s7tn7x+0VJRIQpsk4pHs+FhRzgRtaqY57cS4tDntO2Pb3O/PaFSsUg86WlM+yEeChYwgrWRPNt+9NJeiPXID9KHzGOZ1edmjMDWiZuQapQoOHZX71BRJKQCk04V\nqrOrHup1hqRjNKr1E0RiTMR7SrqECh1T101nyDJ0YZYCSJonNJqpvzdSHCo1DX0zmYdUi14u/ud1Ex1c91Mm4kRTQeaHgoQjHaG8BjRgkhLNp4ZgIpnJisgIS0y0KatiSnAXv7xMWmc197J2cX9erd8UdZThCI7hFy4gjrcQOaQGACz/AKb1ZqvVjv1sd8tGQ\nVO4fwB9bnD+9Bk98=</latexit>SRi\n<latexit sha1_base64=\"hzpTAPTr7uCJ/5cuNHUAfjzUEF8=\">AB/nicbVDLSsNAFJ3UV62vqLhyM1iECqUk4mtZdOyin1AE8pkOmHTh7M3AglFPwVNy4Ucet3uPNvnLRZaOuBgcM593LPHC8WXIFlfRuFpeWV1bXiemljc2t7x9zda6kokZQ1aSQi2fGIYoKHrAkcBOvEkpHAE6ztjW4yv/3IpOJ\nR+ADjmLkBGYTc5SAlnrmQcUJCAw9P72f9HgVqg4T4qRnlq2aNQVeJHZOyihHo2d+Of2IJgELgQqiVNe2YnBTIoFTwSYlJ1EsJnREBqyraUgCptx0Gn+Cj7XSx34k9QsBT9XfGykJlBoHnp7Msqp5LxP/87oJ+FduysM4ARbS2SE/ERginHWB+1wyCmKsCaGS6yYDokFHRjJV2CPf/lRdI6rdkXtfO7s3L9Oq+jiA7REaogG12iOrpFDdREFKXoGb2iN+PJeDHejY/ZaMHId/bRHxifP1z0lR0=</latexit>(Ri, t, `)\n<latexit sha1_base64=\"Dpe7uXeBxF3MAhOnU5kO\nfNbBWE8=\">AB83icjVDLSsNAFL3xWeur6tLNYBFclUR8LYtuXFaxD2hCmUwn7dDJMzcCX0N9y4UMSt\nP+POv3HSdqGi4IELh3Pu5R5OmEph0HU/nIXFpeWV1dJaeX1jc2u7srPbMkmGW+yRCa6E1LDpVC8iQIl76\nSa0ziUvB2Orgq/fc+1EYm6w3HKg5gOlIgEo2gl348pDsMov5303F6l6tXcKcjfpApzNHqVd7+fsCzmCpmkx\nnQ9N8UgpxoFk3xS9jPDU8pGdMC7lioacxPk08wTcmiVPokSbUchmapfL3IaGzOQ7tZDQ/vUL8zetmGF0\nEuVBphlyx2aMokwQTUhRA+kJzhnJsCWVa2KyEDamDG1N5f+V0DqueWe105uTav1yXkcJ9uEAjsCDc6jDN\nTSgCQxSeIAneHYy59F5cV5nqwvO/GYPvsF5+wT2aJGm</latexit>R0\n<latexit sha1_base64=\"z8Be7p/WDZlMOFbxEOVBUjfXTa0=\">AB83icjVDLSsNAFL3xWeur6tLNYBF\nclUR8LYtuXFaxD2hCmUwn7dDJMzcCX0N9y4UMStP+POv3HSdqGi4IELh3Pu5R5OmEph0HU/nIXFpeWV1dJaeX1jc2u7srPbMkmGW+yRCa6E1LDpVC8iQIl76Sa0ziUvB2Orgq/fc+1EYm6w3HKg5gOlIgEo2gl348\npDsMov530vF6l6tXcKcjfpApzNHqVd7+fsCzmCpmkxnQ9N8UgpxoFk3xS9jPDU8pGdMC7lioacxPk08wTcmiVPokSbUchmapfL3IaGzOQ7tZDQ/vUL8zetmGF0EuVBphlyx2aMokwQTUhRA+kJzhnJsCWVa2KyEDam\nDG1N5f+V0DqueWe105uTav1yXkcJ9uEAjsCDc6jDNTSgCQxSeIAneHYy59F5cV5nqwvO/GYPvsF5+wT37JGn</latexit>R1\n<latexit sha1_base64=\"6Yh3ePCKB8IGVCcVpJtLSBvHUm0=\">AB83icjVDLSgNBEOyNrxhfUY9eBoPg\nKWyCr2PQi8co5gHZJcxOZpMhs7PLTK8QlvyGFw+KePVnvPk3ziY5qChY0FBUdNFBYkUBl3wyksLa+srhXSxubW9s75d29tolTzXiLxTLW3YAaLoXiLRQoeTfRnEaB5J1gfJX7nXujYjVHU4S7kd0qEQoGEUreV5EcR\nSE2e20X+XK7WqOwP5m1RgWa/O4NYpZGXCGT1JhezU3Qz6hGwSflrzU8ISyMR3ynqWKRtz42SzlBxZUDCWNtRSGbq14uMRsZMosBu5hnNTy8Xf/N6KYXfiZUkiJXbP4oTCXBmOQFkIHQnKGcWEKZFjYrYSOqKUNb\nU+l/JbTr1dpZ9fTmpNK4XNRhAM4hGOowTk04Bqa0AIGCTzAEzw7qfPovDiv89WCs7jZh29w3j4B+XCRqA=</latexit>R2\n<latexit sha1_base64=\"7Vbef\nsj9T43SOjl2XAfLKTHR4uA=\">ACGnicbVDLSsNAFJ34rPUVdelms\nAgVpCTia1nUhRuhin1AU8JkOmHziRhZiKWkO9w46+4caGIO3Hj3zh\npI2jrgYFz7mXufd4EaNSWdaXMTM7N7+wWFgqLq+srq2bG5sNGcYCk\nzoOWShaHpKE0YDUFVWMtCJBEPcYaXqD8xv3hEhaRjcqmFEOhz1Au\npTjJSWXNMuOxypvucn96lL9+FPdaOrPUdSPlYwYslF6iZXjdQ1S1bF\nGgFOEzsnJZCj5pofTjfEMSeBwgxJ2batSHUSJBTFjKRFJ5YkQniAeq\nStaYA4kZ1kdFoKd7XShX4o9AsUHKm/JxLEpRxyT3dme8pJLxP/89qx\n8k87CQ2iWJEAjz/yYwZVCLOcYJcKghUbaoKwoHpXiPtIKx0mkUdg\nj158jRpHFTs48rR9WGpepbHUQDbYAeUgQ1OQBVcghqoAwewBN4Aa/\nGo/FsvBnv49YZI5/ZAn9gfH4DuUuhUw=</latexit>(xi, Ri) \u21e0 DMV\n<latexit sha1_base64=\"5EnkiFBm3d2d/DZK26+jp1s7exI=\">AB6nicbVDJSgNBEK2JW4xb1KOXxiB4C\njPidgx68RiJWSAZQk+nJ2nSy9DdI4Qhn+DFgyJe/SJv/o2dZA4afVDweK+KqnpRwpmxv/lFVZW19Y3ipulre2d3b3y/kHLqFQT2iSK92JsKGcSdq0zHLaSTFIuK0HY1vZ37kWrDlHywk4SGAg8lixnB1kmNRl/1yxW/6s\n+B/pIgJxXIUe+XP3sDRVJBpSUcG9MN/MSGdaWEU6npV5qaILJGA9p1GJBTVhNj91ik6cMkCx0q6kRXP150SGhTETEblOge3ILHsz8T+vm9r4OsyYTFJLJVksilOrEKzv9GAaUosnziCiWbuVkRGWGNiXTolF0Kw/PJf0jq\nrBpfVi/vzSu0mj6MIR3AMpxDAFdTgDurQBAJDeIXePW49+y9e+L1oKXzxzCL3gf3zSKjcM=</latexit>So\n<latexit sha1_base64=\"5EnkiFBm3d2d/DZK26+jp1s7exI=\">AB6nicbVDJSgNBEK2JW4xb1KOXxiB4CjPidgx68\nRiJWSAZQk+nJ2nSy9DdI4Qhn+DFgyJe/SJv/o2dZA4afVDweK+KqnpRwpmxv/lFVZW19Y3ipulre2d3b3y/kHLqFQT2iSK92JsKGcSdq0zHLaSTFIuK0HY1vZ37kWrDlHywk4SGAg8lixnB1kmNRl/1yxW/6s+B/pIgJxXIUe+XP3sDRVJBpSU\ncG9MN/MSGdaWEU6npV5qaILJGA9p1GJBTVhNj91ik6cMkCx0q6kRXP150SGhTETEblOge3ILHsz8T+vm9r4OsyYTFJLJVksilOrEKzv9GAaUosnziCiWbuVkRGWGNiXTolF0Kw/PJf0jqrBpfVi/vzSu0mj6MIR3AMpxDAFdTgDurQBAJDeIXe\nPW49+y9e+L1oKXzxzCL3gf3zSKjcM=</latexit>So\n<latexit sha1_base64=\"5EnkiFBm3d2d/DZK26+jp1s7exI=\">AB6nicbVDJSgNBEK2JW4xb1KOXxiB4C\njPidgx68RiJWSAZQk+nJ2nSy9DdI4Qhn+DFgyJe/SJv/o2dZA4afVDweK+KqnpRwpmxv/lFVZW19Y3ipulre2d3b3y/kHLqFQT2iSK92JsKGcSdq0zHLaSTFIuK0HY1vZ37kWrDlHywk4SGAg8lixnB1kmNRl/1yxW/6s\n+B/pIgJxXIUe+XP3sDRVJBpSUcG9MN/MSGdaWEU6npV5qaILJGA9p1GJBTVhNj91ik6cMkCx0q6kRXP150SGhTETEblOge3ILHsz8T+vm9r4OsyYTFJLJVksilOrEKzv9GAaUosnziCiWbuVkRGWGNiXTolF0Kw/PJf0jq\nrBpfVi/vzSu0mj6MIR3AMpxDAFdTgDurQBAJDeIXePW49+y9e+L1oKXzxzCL3gf3zSKjcM=</latexit>So\n<latexit sha1_base64=\"5EnkiFBm3d2d/DZK26+jp\n1s7exI=\">AB6nicbVDJSgNBEK2JW4xb1KOXxiB4CjPidgx68RiJWSAZQk+nJ2nSy9DdI4Qhn+DFgyJe/SJ\nv/o2dZA4afVDweK+KqnpRwpmxv/lFVZW19Y3ipulre2d3b3y/kHLqFQT2iSK92JsKGcSdq0zHLaSTFIuK0\nHY1vZ37kWrDlHywk4SGAg8lixnB1kmNRl/1yxW/6s+B/pIgJxXIUe+XP3sDRVJBpSUcG9MN/MSGdaWEU6n\npV5qaILJGA9p1GJBTVhNj91ik6cMkCx0q6kRXP150SGhTETEblOge3ILHsz8T+vm9r4OsyYTFJLJVksilOr\nEKzv9GAaUosnziCiWbuVkRGWGNiXTolF0Kw/PJf0jqrBpfVi/vzSu0mj6MIR3AMpxDAFdTgDurQBAJDeIXe\nPW49+y9e+L1oKXzxzCL3gf3zSKjcM=</latexit>So\nFigure 3. Our method for solving sparse-view novel view synthesis using ViewNeTI. Left: we have a small multi-view dataset, DMV\ncontaining images, xi, and known camera poses, Ri. We create a caption for each image. The caption contains a camera-specific token\nSRi that is different for each view, and an object token, So, which is common across views. Right: textual inversion training of our neural\nmappers, Mv and Mo. The mappers predict the word embeddings for SRi and So respectively. These are conditioned on diffusion\ntimestep t and UNet layer \u2113; the view mapper, Mv, is additionally conditioned on camera parameters, Ri. All parameters are encoded by\na Fourier feature mapper, \u03b3 [55]. The remaining tokens take their regular word embeddings. The prompt is passed through the CLIP text\nencoder [36]. The post-CLIP encoding corresponding to SRi and So are perturbed by a vector that is also predicted by Mv and Mo. This\nencoding conditions the diffusion model, \u03f5\u03b8. We do diffusion model training on this dataset while optimizing only Mv and Mo (this is\ntextual inversion training [1, 13]). More details in Sec. 4.2.\n[65] as an explicit 3d representation. To address the chal-\nlenge of sparse input views, these approaches add regular-\nization on novel views [12, 20, 34, 40, 47, 63, 69, 71, 77],\nmodify the training process [48, 59, 73], or condition the\nneural field on image features that are derived from pretrain-\ning on multi-view datasets [6, 7, 26, 75]. Most of these\nworks that do not have pretraining do not attempt single-\nview NVS. On the other hand, a recent series of models do\ntackle single-view NVS using NeRFs with novel view regu-\nlarization [11, 27, 32, 72] by leveraging diffusion models as\na data prior over 2D renders (via the Score Distillation Loss\n[35, 64]). Unlike our approach, the NeRF-based methods\nare not generative: once trained, they cannot be easily sam-\npled from, and thus cannot easily model ambiguity. The\nsparse NeRF methods also tend to produce blurriness and\nimage artifacts compared to our method.\nThe other set of techniques for sparse-view NVS are \u2018ge-\nometry free\u2019: they do not train any explicit 3D representa-\ntion of the scene. Rather than doing per-scene optimization,\nthey train an inductive model. One approach for single-view\nNVS is to formulate an image-to-image translation problem\n[53, 57], and recent works have leveraged diffusion mod-\nels [27, 66]. Many works design the architecture and losses\nto encode 3D priors [19, 45, 68], and recent approaches\nalso use diffusion models [2, 4, 22, 28, 54, 58, 81]. These\ngeometry-free methods require pretraining on multi-view\ndatasets [5, 10, 38], and they most commonly test on im-\nages that are in the same class and covariate distribution\nas the training set. Our approach differs in that we do not\ndirectly train or fine-tune an image generation model. We\ninstead learn to predict text embeddings to condition image\ngeneration in large pretrained models like StableDiffusion.\nSince we control the 3D view of objects in the very large\nStableDiffusion vocabulary, we can do novel view synthe-\nsis on scenes with semantics outside the pretraining dataset.\nA concurrent work on ArXiv, DreamSparse [74], does\nsparse-view NVS by learning to map multi-view inputs to\n3D features, which then condition a perturbation on the\nUNet predictions (as in ControlNet [78]). This has a similar\nmotivation to our work: by using pretrained frozen diffusion\nmodels, they can do NVS for classes outside the distribution\nof the multi-view pretraining set: they are \u2018open-set\u2019. How-\never, their guidance approach is different, and it does not do\nNVS from one input view, which is our primary application.\n3. Background\n3.1. Text-to-image latent diffusion models\nWe apply viewpoint textual inversion to text-to-image\nStable-Diffusion (SD). SD\u2019s are Latent Diffusion Models\n(LDMs) for image generation [41] and are typically trained\non web-scale datasets of text-image pairs (x, y) \u223c D [46].\nThere are two components. First, a variational autoencoder\n(VAE) [23, 39] with encoder E and decoder D compresses\nRGB images, x \u2208 RH\u00d7W \u00d73 to a lower-dimensional latent\nz0 = E(x) \u2208 Rh\u00d7w\u00d7c. Second, a conditional diffusion\nmodel [17, 18, 50, 52] is trained to generate this distribution\nof latents with respect to the text prompt, y, as p(z|y).\nLDMs model a diffusion process: noise is incrementally\nadded to the the latent over T steps; the intermediate latents\nare zt with t \u2208 [0, T]. We learn a neural network \u03f5\u03b8 that\nreverses each step by predicting the applied noise. To train\n3\nthis network, we simulate zt by sampling isotropic Gaus-\nsian noise, \u03f5, scaling it according to a parameter t\u223c U[0, T],\nand adding it to z. The training objective is for \u03f5\u03b8 to predict\n\u03f5 conditioned on noising step, t and the text, y:\nLLDM := E(x,y)\u223cD,\u03f5\u223cN (0,I),t\nh\n\u2225\u03f5 \u2212 \u03f5\u03b8(zt, y, t)\u2225\ni\n(1)\nOne can sample from p(z|y) with zT \u223c N(0, I), and\nusing \u03f5\u03b8 to run the reverse process over T steps [50], . This\ngives a latent sample \u02dcz which is decoded to an image, \u02dcx =\nD(\u02dcz). The \u03f5\u03b8 architecture is a conditional UNet [42]. zt\nis passed through the main Unet stream. The text is passed\nthrough a pretrained CLIP [36] text encoder, giving a d-dim\nconditioning vector for each token, c(y) \u2208 Rd\u00d777, which is\nmixed with each Unet layer via cross-attention [41].\n3.2. Textual inversion\nIn textual inversion (TI) [13], we learn a new text embed-\nding, vSo, for pseudo-word So, which represents a new con-\ncept from a small dataset. The dataset, (x, y) \u223c DT I con-\ntains images, x, of that concept; and paired captions, y, that\nare \u201cA photo of So\u201d. To learn the word embedding vSo, we\nuse the LDM loss as in Eq. (1), but replacing D with DT I,\nand optimizing only with respect to vSo. Importantly, the\ndiffusion model weights are frozen.\nA recent work proposes the NeTI model [1], which in-\ncludes many recent advances in textual inversion [14, 60,\n67, 80]. Instead of learning a single embedding for So, it\nlearns an embedding for each UNet cross-attention layer,\n\u2113 [62] and for each noise step, t [15]; this representation\nspace is denoted P\u2217 [1]. NeTI is implemented as a small\nneural network mapper, M conditioned on t and l. The op-\ntimization of Eq. (1) is with respect to the weights of M\n[1]. Our work, ViewNeTI, uses the NeTI architecture.\n4. Method\nViewpoint Neural Textual Inversion (ViewNeTI) controls\nthe viewpoint of objects in images generated by diffusion\nmodels. In Sec. 4.1 we describe the core architectural com-\nponent, which is a small neural network - the view-mapper\nor Mv - that takes camera parameters and predicts a text\nencoding. This text encoding then conditions the diffusion\ngeneration process to control viewpoint in images.\nThe ViewNeTI view mapper is trained to do novel view\nsynthesis from few inputs. Illustrated in Fig. 3, we use tex-\ntual inversion (TI) to optimize the Mv parameters jointly\nwith a mapper for the objects semantics Mo. In Sec. 4.2\nwe describe how to train Mv on a single scene with no\nprior multi-view supervision, which enables interpolation\nof novel views. In Sec. 4.3, we propose pretraining Mv on\na multi-view dataset, which enables extrapolation of novel\nviews far from the input views, and under semantic distri-\nbution shift. Here, we highlight the especially challeng-\ning application of NVS from a single input view. Finally\nin Sec. 4.4, we show that the pretrained Mv can be used\nin text-to-image generation to control the viewpoint around\nobjects.\n4.1. ViewNeTI mapper architecture and inference\nThe ViewNeTI mappers control the text encoding for the\ndiffusion model.\nAs shown in\nFig. 3 we have a view-\nmapper, Mv, and an object mapper, Mo, to predict the text\nencoding of the view and object tokens respectively. They\nare both conditioned on the diffusion timestep, t, and UNet\nconditioning layet, \u2113, while the view-mapper is also con-\nditioned on camera parameters, R. They predict a word\nembedding input to the text encoder. They also predict a\n\u2018textual bypass\u2019, which is a perturbation to the vectors that\nare output from the text encoder. We now explain Fig. 3 in\ndetail.\n3D-view/pose representation One input parameter to\nthe view-mapper, Mv, is the camera parameters, R, which\ncan be any vector representation of the camera extrinsics\nand intrinsics. In our experiments, we use the camera-to-\nworld project matrix, and we normalize each matrix en-\ntry independently to [\u22121, 1]. The normalization range is\nfixed according to the dataset parameters for experiments in\nSec. 5. Our method is agnostic to the camera parameteriza-\ntion: we verify that our method also works with spherical\ncoordinates in Appendix G.\nWe apply Fourier-feature encoding [37, 55] with band-\nwidth \u03c3 = 2. This enables the neural mappers to learn a pre-\ndictor of word-embedding space that is sufficiently sensitive\nto small changes in camera parameters; that is, it can repre-\nsent high frequency changes in word embedding space. The\n\u03c3 parameter is fixed across our experiments: it is big enough\nto model a diverse viewpoint range, but small enough to in-\nterpolate viewpoints (see Sec. 5.4 ablations).\nArchitecture Both neural mappers, (Mv, Mo) are con-\nditioned on the the denoising timestep and the diffusion\nmodel UNet layer, (t, \u2113). This improves textual inversion re-\nconstruction quality and optimization convergence because\ndifferent timesteps and UNet layers control different image\nfeatures [1, 62]; for example, small t denoising steps con-\ntrol finer texture details rather than layout and shape. We\nconcatenate these parameters with the camera parameters\ninto a vector, [t, \u2113, R].\nLet the Fourier feature encoding\nfunction [37, 55] be \u03b3(\u00b7), and the encoding be c\u03b3. Then\nthe encoding is c\u03b3 = \u03b3([t, \u2113, R]), and we choose its dimen-\nsion as 64.\nWe define the function parameterized by our view-\nmapper as:\n(vR, vR,p) = Mv(c\u03b3)\n(2)\nThe vectors (vR, vR,p) have the same dimension as the text\n4\nencoder, which is 768 in Stable Diffusion 2 (SD2) [41]. We\nparameterize Mv as a 2-layer MLP with 64 dimensions,\nLayerNorm [3], and LeakyRelu [70]. The view-mapper\nhas 140,000 parameters. The object-mapper, Mo is defined\nin the same way, except without conditioning on camera\nparameters. So, c\u03b3 = \u03b3([t, \u2113]) and:\n(vo, vo,p) = Mo(c\u03b3)\nIn the next subsection we describe how the two output\nvectors are used for controlling the 3D viewpoint of Stable\nDiffusion generations.\nInference for view-controlled generation We pass the\ntext encoder a prompt like, \u2018SR. A photo of a So\u2019, where\nSR has word embedding vR and So has word embedding\nvo. The embedding for SR controls the viewpoint for the\nimage that is been generated. The embeddings for So are\nthe same for all novel views generated in one scene, so it\ncaptures properties that are invariant across all images such\nas object semantics and image style.\nWe scale the embeddings, (vR, vo) to have the same L2\nnorm as the word embedding for \u2018object\u2019 (the choice of this\nreference word is not important in practice). We then pass\nthis prompt through the CLIP text encoder with padding,\nwhich gives 77 encoding vectors. If k is the index for token\nSR, then call its embedding vector ek. We perturb ek in the\ndirection of the vR,p vector (which was predicted by Mv\nas in Eq. (2)):\ne\u2032\nk := ek + \u03b1\u2225ek\u2225 \u00b7\nvR,p\n\u2225vR,p\u2225\n(3)\nThis is called textual bypass [1], and it enables more\nflexibility for the view-mapper control, as well as faster con-\nvergence. \u03b1 controls the perturbation magnitude, and prior\ntextual inversion methods set it to 0.2. This constrains the\nbypass mechanism to only refine object details, and thus\npreserve the compositionality of the word embedding with\nother concepts in word space. We set \u03b1 = 0.2 only for ex-\nperiments in text-to-image generation. But for NVS, we do\nnot care about compositionality, so we are free to increase \u03b1\nto 5, which we find has similar view-control performance,\nand better reconstructs object details (see Sec. 5.4 abla-\ntions). The final view-mapper output, e\u2032\nk, is the kth con-\nditioning vector that is passed to the UNet generator. We\napply this exact same textual bypass approach to the object\ntoken, where we perturb using the predicted vo,p.\nTo generate the final image in viewpoint R, we run dif-\nfusion model generation with DPMSolver [30, 31, 51] for\n50 steps to get the final image.\nData augmentations We apply simple augmentations to\nimages, similar to [32]. This helps avoid overfitting when\nthe training dataset is small, as we show in Sec. 5.4 abla-\ntions. We also do text prompt augmentations that are stan-\ndard in textual inversion [13]. See Appendix C for details.\n4.2. Single-Scene Novel View Synthesis\nIn single-scene NVS, we have a small (< 10) dataset called\nDMV with multi-view images, xi, with known camera pose\nparameters, Ri. We do not have prior 3D knowledge from\nother multi-view datasets. As in Fig. 3, we generate cap-\ntions of the form y(SRi, So) =\u201cSRi. a photo of a So\u201d. As\nin the previous Sec. 4.1, SRi corresponds to viewpoint and\nits embeddings are predicted by Mv. Likewise, So cor-\nresponds to object semantics and its embeddings are pre-\ndicted by Mo. The So embeddings are the same for all\nimages. Now, let the modified multi-view dataset be the set\nof image-caption pairs, (xi, y(SRi, So)) \u223c DMV \u2032.\nWe optimize the weights of Mv and Mo using the loss\nin Eq. (1), except we replace D with DMV \u2032. Intuitively, we\nare learning text space latents that, when conditioning dif-\nfusion model generation, reproduce the training set images\nwith the correct camera views.\n4.3. Novel view synthesis with pretraining on multi-\nview data\nThe single-scene optimization case cannot extrapolate to\nviews far from the input views (see Sec. 5.1 and Fig. 4).\nIt also cannot do one-shot novel view synthesis. To tackle\nthese challenging settings, we propose pretraining on a\nmulti-view dataset with multiple scenes.\nPre-training We now have access to images, xij with\nknown camera poses, Rij, for views i and scenes j. The\ncamera poses are in the same reference frame: their no-\ntion of \u2018up\u2019 and \u2018down\u2019 are the same; however there is no\nrequirement for the scene\u2019s objects to have any particular\npose within that space. The multi-view datasets should have\ndense coverage of the view space, which we visualize in\nAppendix D.\nThe goal is to train a view-mapper, Mv, that is shared\nacross all the scenes, and is generalizable to new scenes.\nThe training is the same as the single scene case in Sec. 4.2,\nwith one change: in the captions, \u201cSRi, a photo of a So,j\u201d,\neach scene has its own object token So,j and object mapper,\nMo,j (but again, they share view tokens, SRi and view-\nmapper, Mv).\nNovel view synthesis from sparse input views Given a\nsparse dataset of muti-view images and camera parameters,\n(xi, Ri) \u223c DMV , we do the exact training procedure for\nsingle-scene NVS described in Sec. 4.2, except we use the\npretrained view mapper, Mv and freeze it\u2019s weights. The\ninference procedure for generating novel views is also the\nsame.\nNovel-view synthesis from a single input view Under\nthis framework, single-view NVS is solved with the same\napproach as sparse-view NVS.\n5\nGround \ntruth\nInterpolation works\nExtrapolation does not work\n257\n1155\nInterpolation\nExtrapolation\nInput views\nPrediction\nFigure 4. Novel view synthesis trained on a single scene (with-\nout pretraining), described\nSec. 5.2.2.\nTop: the camera posi-\ntions for the DTU buddha scene [21] visualized with Nerfstudio\n[56] and SDFStudio [76]. We show input views from the 6-view\nsplit (blue), inference views that are \u2018interpolated\u2019 from the input\n(green), and inference views that are \u2018extrapolated\u2019 (red). Bottom\nleft: inference on interpolated views has good semantics and pose.\nBottom right: inference on extrapolated views does not work. The\nsemantics are good but the poses are wrong. This limitation moti-\nvates our pretraining strategy, see Sec. 4.3.\n4.4.\nGeneralizing\nViewpoint\nControl\nto\nNew\nPrompts\nContent creation via text-to-image diffusion is a popular ap-\nplication, and one research direction seeks to control certain\nproperties in generated images [16, 78]. ViewNeTI can\nbe easily extended to control the camera viewpoint around\nobjects that are generated from user-defined captions, y.\nWe take the pretrained view mapper, Mv, described in\nSec. 4.3, and we prepend its corresponding token to y. An\nexample text prompt is \u201cR0. a brown teddy bear\u201d. Then,\nthe inference procedure is the same as Sec. 4.1.\n5. Results\nIn Sec. 5.1, we use ViewNeTI for sparse-view novel view\nsynthesis (NVS) on a single scene, without access to any\nmulti-view or 3D datasets.\nWe show the limitations of\nthe single-scene approach, which motivate our pretraining\nand fine-tuning method. This leads to our primary result\nin\nSec. 5.2.2: novel view synthesis from a single view.\nIn\nSec. 5.3, we show an extra application of ViewNeTI\nto view-controlled text-to-image generation.\nAll results\nuse the same hyperparameters, and we ablate these design\nchoices in 5.4.\nNVS data setting We evaluate on DTU-MVS [21], a\nmulti-view dataset of real-world objects with challenging\ndetails. We use the train-test splits for camera views used in\nsparse-view snythesis from prior works [11, 75] (training\nset sizes 1, 3, 6, and 9). As visualized in Appendix E,\nDTU images can be roughly grouped by their camera pitch\nangle into \u2018top-view\u2019, \u2018side-view\u2019, and \u2018neutral-view\u2019. The\n9-view training set has images from all groups, so novel test\nviews are \u2018interpolations\u2019. The 6-view dataset has images\nfrom the side and neutral views, so test views from top-view\nare \u2018extrapolation\u2019 (this is also visualized in Fig. 5. The 3-\nview dataset uses neutral-view images, and so top and side\nviews are extrapolation. For the 1-view set, all test views\nare extrapolation.\n5.1. Single-scene novel view synthesis from sparse\nviews\nWe start by establishing that the text latent space can be\nmanipulated for viewpoint control. Using the method de-\nscribed in Sec. 4.2, we optimize a view-mapper and object-\nmapper from scratch on a scene with 6-views. The training\ntime is 2.5hrs with one Titan RTX.\nIn Fig. 5 we visualize the training and inference camera\npositions, and the NVS predictions. Despite being super-\nvised on few images, we can interpolate to novel views. We\nretain good object details, and avoid imaging artifacts like\nfloaters that appear in NeRF [34]. However, in this set-\nting we fail to extrapolate. The failure mode is interesting\nand different to other NVS methods: the object semantics\napproximately match the training views, but the poses are\nincorrect. This motivated us to pretrain the view-mapper, as\nwe review in the next section\n5.2. Novel view synthesis from one input view\n5.2.1\nView-mapper pretraining\nWe pretrain a view-mapper, Mv as described in Sec. 4.3.\nWe take 88 training scenes from DTU chosen by [75] to\nensure that object classes are different between the train\nand test scenes. The pretraining takes 48hours on one Ti-\ntan RTX. We validate the approach with generations in Ap-\npendix F: the view-mapper and object-mapper correctly re-\nconstruct the training set of views, where the view-mapper\nis shared across all scenes.\n5.2.2\nSingle-view NVS with a pretrained view-mapper\nWe use the frozen view-mapper, Mv, and fine-tune a new\nobject-mapper, Mo on a new scene from a different seman-\ntic distribution, as described in Sec. 4.3, which takes one\n6\nGT\nNeRF\nPixelNeRF\nNeRDi\nViewNeTI (ours)\nNovel views\nNovel views\nNovel views\n410\nFigure 5. Qualitative comparison of single-view novel view synthesis on DTU [21]. Our method has good photorealism and semantics\ncompared to baselines (see Sec. 5.2.2).\nhour on one Titan RTX. We show single-view NVS predic-\ntions for selected scenes in Fig. 1, and all DTU test scenes\nin Fig. 8. In Fig. 5, we compare NVS predictions against\nbaselines using the same challenging evaluation scenes and\nviews chosen by [11]. We compare against zero-1-to-3 in\nAppendix H; this is separate because this model has a dif-\nferent coordinate system that does not allow translation.\nThe first 3 methods learn an explicit 3D scene represen-\ntation - a NeRF - which has the benefit of consistent scene\ngeometry. But they have blurriness and artifacts that com-\nmonly appear under sparse supervision [11, 65, 75] and\nthey have errors in the semantic details. They also have the\ndrawback that one cannot easily sample novel views from a\nhigh-variance distribution, which is desirable when the pre-\ndiction problem is ambiguous. The zero-1-to-3 model has a\nfast runtime, but it is not suitable for NVS on images with\nphotorealisitc object details (as shown by the toy pig scene),\nor for images where the borders crop the object (as shown\nby the brick scene), becahsethey are outside the training dis-\ntribution of 3D assets [10].\nOur ViewNeTI predictions, while sometimes hallucinat-\ning inaccurate object details, do generate photorealistic im-\nages with plausible semantics and little blurriness. Once\ntrained, ViewNeTI can generate diverse samples in close to\nreal time, and thus can be useful for modeling uncertainty.\nFor completeness, we also compare the LPIPS, SSIM, and\nPSNR against baselines in\nTab. 1, which is standard in\nTable 1. Single-image novel view synthesis metrics on DTU. The\nbest score is in bold, and second best is underlined. We argue\nin Sec. 5.2.2 that these metrics are imperfect for settings with\nambiguity like this one.\nMethod\nLPIPS \u2193\nSSIM \u2191\nPSNR \u2191\nNeRF [65]\n0.703\n0.286\n8.000\npixelNeRF [75]\n0.515\n0.564\n16.048\nSinNeRF [71]\n0.525\n0.560\n16.520\nNerDi [11]\n0.421\n0.465\n14.472\nViewNeTI (ours)\n0.378\n0.516\n10.947\nNVS. We agree with the remarks in NeRDi [11] that these\nreconstruction-based metrics are not suitable for the single-\nview setting because there is high uncertainty in the 3D pre-\ndiction. This metric will reward methods that perform some\n\u2018averaging\u2019 over diverse views, rather than those methods\nthat predict diverse but plausible views. Having said that,\nthe \u2019perceptual\u2019 metrics LPIPS and SSIM may not be as\nproblematic as PSNR. Our approach is state-of-the-art for\nLPIPS, and near state-of-the art for SSIM. Our PSNR is\npoor which, as we argue in Sec. 6, is caused by issues with\nlocalizing the object in the image. That is, while the object\ndetails and pose are good, the image is mis-translated with\nrespect to the ground truth. This is an area in which NeRFs\nare naturally at an advantage over diffusion models.\n7\nReference poses for \nText-to-image prompt: \u201c     . A small white dog\u201d\nText-to-image prompt: \u201c     . A brown teddy bear\u201d\n610\n1435\n<latexit sha1_base64=\"ExcD/kF45ruUuZaORd+jxuaNwxs=\">AB\n83icbVDLSgMxFL1TX7W+qi7dBIvgqsyIr2XRjcsq9gGdoWTSTBuaZIYkI5Shv+HGhSJu/Rl3/o2ZdhbaeiBwOde7skJE860cd1vp7Syura+Ud6\nsbG3v7O5V9w/aOk4VoS0S81h1Q6wpZ5K2DOcdhNFsQg57YTj29zvPFGlWSwfzShgcBDySJGsLGS7wtsRmGUPUz7rF+tuXV3BrRMvILUoECzX/\n3yBzFJBZWGcKx1z3MTE2RYGUY4nVb8VNMEkzEe0p6lEguqg2yWeYpOrDJAUazskwbN1N8bGRZaT0RoJ/OMetHLxf+8Xmqi6yBjMkNlWR+KEo5M\njHKC0ADpigxfGIJorZrIiMsMLE2JoqtgRv8cvLpH1W9y7rF/fntcZNUcZjuAYTsGDK2jAHTShBQSeIZXeHNS58V5dz7moyWn2DmEP3A+fwBI\ndpHc</latexit>Ri\n<latexit sha1_base64=\"ExcD/kF45ruUuZaORd+jxuaNwxs=\">AB\n83icbVDLSgMxFL1TX7W+qi7dBIvgqsyIr2XRjcsq9gGdoWTSTBuaZIYkI5Shv+HGhSJu/Rl3/o2ZdhbaeiBwOde7skJE860cd1vp7Syura+Ud6\nsbG3v7O5V9w/aOk4VoS0S81h1Q6wpZ5K2DOcdhNFsQg57YTj29zvPFGlWSwfzShgcBDySJGsLGS7wtsRmGUPUz7rF+tuXV3BrRMvILUoECzX/\n3yBzFJBZWGcKx1z3MTE2RYGUY4nVb8VNMEkzEe0p6lEguqg2yWeYpOrDJAUazskwbN1N8bGRZaT0RoJ/OMetHLxf+8Xmqi6yBjMkNlWR+KEo5M\njHKC0ADpigxfGIJorZrIiMsMLE2JoqtgRv8cvLpH1W9y7rF/fntcZNUcZjuAYTsGDK2jAHTShBQSeIZXeHNS58V5dz7moyWn2DmEP3A+fwBI\ndpHc</latexit>Ri\n<latexit sha1_base64=\"ExcD/kF45ruUuZaORd+jxuaNwxs=\">AB\n83icbVDLSgMxFL1TX7W+qi7dBIvgqsyIr2XRjcsq9gGdoWTSTBuaZIYkI5Shv+HGhSJu/Rl3/o2ZdhbaeiBwOde7skJE860cd1vp7Syura+Ud6\nsbG3v7O5V9w/aOk4VoS0S81h1Q6wpZ5K2DOcdhNFsQg57YTj29zvPFGlWSwfzShgcBDySJGsLGS7wtsRmGUPUz7rF+tuXV3BrRMvILUoECzX/\n3yBzFJBZWGcKx1z3MTE2RYGUY4nVb8VNMEkzEe0p6lEguqg2yWeYpOrDJAUazskwbN1N8bGRZaT0RoJ/OMetHLxf+8Xmqi6yBjMkNlWR+KEo5M\njHKC0ADpigxfGIJorZrIiMsMLE2JoqtgRv8cvLpH1W9y7rF/fntcZNUcZjuAYTsGDK2jAHTShBQSeIZXeHNS58V5dz7moyWn2DmEP3A+fwBI\ndpHc</latexit>Ri\nFigure 6. ViewNeTI controls viewpoint in text-to-image genera-\ntion by composing the view-mapper text encoder, represented by\nRi, with novel text prompts. Top: reference poses from a scene in\nthe DTU [21] dataset. Middle & bottom: text generations condi-\ntioned on ViewNeTI view control tokens.\n5.3. View-controlled text-to-image generation\nIn Fig. 6, we show examples of using ViewNeTI to con-\ntrol the viewpoint of images in text-to-image generation.\nThe object semantics are consistent across views, and using\nour conditioning adds only negligible time to the genera-\ntion procedure. Moreover, the example images are outside\nthe semantic distribution of the DTU training dataset.\n5.4. Ablations\nWe show the effectiveness of our design choices by show-\ning qualitative results on ablated versions of ViewNeTI\nfor single-view NVS in Appendix B. The key architectural\nchoices are the frequency of the positional encoding, the\ntext embedding norm scaling, the flexibility permitted in\nthe textual bypass, and the image augmentations. Other im-\nportant choices include the freezing of the pretrained view-\nmapper, and the choice of image dimension in object sam-\npling. Note that the results in Fig. 5 already established that\nno-pretraining leads to a failure to extrapolate away from\nthe training viewpoints. Note that our application differs\nfrom standard textual inversion works [1, 13] in that we are\nnot interested in compositionality of our word embeddings\nwith existing concepts. As such, we do not ablate against\nthis.\n6. Limitations\nThe major limitation for ViewNeTI in novel view synthe-\nsis (NVS) is localization: it is prone to generating objects\nthat are mis-translated slightly from the ground truth, and\nthis drives the degradation in PSNR scores seen in Tab. 1.\nThis is a product of the training procedure: unlike in NeRFs,\nwe do not penalize the MSE loss with an image. However\nin return for this drawback, we gain the ability to generate\nrealistic samples (thanks to the pretraining on a web-scale\ndatasets) and diverse samples (thanks to the generative na-\nture of the model).\nViewNeTI also struggles to generate precise details in\nobjects, such as small textures in the building scene ( Fig. 8,\nrow 2). Reconstruction quality is an active area of research\nin textual inversion [1] and advances there should be trans-\nferable to ViewNeTI.\nAlthough ViewNeTI scene optimization has similar\ntraining time to NeRF-based methods for sparse NVS\n(about 1 hour, which is similar to [63, 73]), it is far from the\nrealtime speeds of recent methods that pose NVS as image-\nto-image translation [27]. The key bottleneck in our ap-\nproach is optimizing new object tokens. But here, too, there\nis much work among textual inversion researchers for ac-\ncelerating these predictions, [14, 60, 67, 80] and we expect\nthese advances will be applicable to ViewNeTI.\n7. Conclusions\nIn this study, we investigated the capabilities of diffusion\nmodels to represent and reason about the 3D world. Re-\nmarkably, despite having only unposed 2D image supervi-\nsion during training, we found strong evidence that the dif-\nfusion model latent space does encode 3D knowledge. We\nproposed Viewpoint Neural Textual Inversion (ViewNeTI),\na framework for taking advantage of this prior knowledge\nfor 3D vision tasks. Our method controls the viewpoint of\nimages generated by a diffusion model by predicting appro-\npriate conditioning vectors in the text latent space using a\nneural view-mapper.\nOur method naturally addresses novel view synthesis\nby fitting the neural view-mapper on multi-view image\ndatasets. We fit the view mapper on a single scene with\nvery sparse viewpoints ( < 10). Despite this small dataset\nbeing the only 3D supervision ever seen by the image diffu-\nsion model, the learned view-mapper was able to generalize\nto novel camera viewpoints, as long as those views were\nnear the training views. Next we showed that the ViewNeTI\ncould learn a more generic control mechanism for viewpoint\ncontrol. We pretrained the view mapper on a multi-scene\ndataset that shared a common reference frame. The view\nmapper was able to learn this reference frame, while the\nper-scene object mappers learned the pose of objects within\nthat scene. We highlighted impressive results for single-\nview novel view synthesis, an extremely challenging task.\nHere, our results showed excellent photorealism and good\nobject semantics. This is most likely the result of the dif-\nfusion model leveraging prior knowledge from its massive\ntraining distribution, since the NeRF methods - without the\nbenefit of large-scale pretraining datasets - rendered views\n8\nwith more blur and artifacts. Notably our multi-view pre-\ntraining dataset was small in scale compared to datasets un-\nderpinning other advances in vision, and the training time\nwas modest at only 2 days on one GPU. This shows that\nextracting 3D knowledge from 2D models need not require\nunreasonably large 3D datasets.\nWe have demonstrated strong results for viewpoint ma-\nnipulation, which is one important form of 3D control.\nLooking forward, we hope that our framework can inspire\nwork on leveraging diffusion model latent spaces for other\nchallenging 3D tasks, such as scene relighting and 2D-to-\n3D lifting for 3D human pose estimation.\nReferences\n[1] Yuval Alaluf, Elad Richardson, Gal Metzer, and Daniel\nCohen-Or. A neural space-time representation for text-to-\nimage personalization.\narXiv preprint arXiv:2305.15391,\n2023. 1, 2, 3, 4, 5, 8\n[2] Titas Anciukevi\u02c7cius, Zexiang Xu, Matthew Fisher, Paul Hen-\nderson, Hakan Bilen, Niloy J Mitra, and Paul Guerrero. Ren-\nderdiffusion: Image diffusion for 3d reconstruction, inpaint-\ning and generation. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n12608\u201312618, 2023. 3\n[3] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. Layer normalization. arXiv preprint arXiv:1607.06450,\n2016. 5, 4\n[4] Eric R Chan, Koki Nagano, Matthew A Chan, Alexander W\nBergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini\nDe Mello, Tero Karras, and Gordon Wetzstein. Generative\nnovel view synthesis with 3d-aware diffusion models. arXiv\npreprint arXiv:2304.02602, 2023. 3\n[5] Angel X Chang, Thomas Funkhouser, Leonidas Guibas,\nPat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese,\nManolis Savva, Shuran Song, Hao Su, et al.\nShapenet:\nAn information-rich 3d model repository.\narXiv preprint\narXiv:1512.03012, 2015. 3, 10\n[6] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang,\nFanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast general-\nizable radiance field reconstruction from multi-view stereo.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 14124\u201314133, 2021. 3\n[7] Julian Chibane, Aayush Bansal, Verica Lazova, and Gerard\nPons-Moll. Stereo radiance fields (srf): Learning view syn-\nthesis for sparse views of novel scenes. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 7911\u20137920, 2021. 3\n[8] Niv Cohen, Rinon Gal, Eli A Meirom, Gal Chechik, and\nYuval Atzmon. \u201cthis is my unicorn, fluffy\u201d: Personalizing\nfrozen vision-language representations. In Computer Vision\u2013\nECCV 2022: 17th European Conference, Tel Aviv, Israel,\nOctober 23\u201327, 2022, Proceedings, Part XX, pages 558\u2013577,\n2022. 2\n[9] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-\nber, Thomas Funkhouser, and Matthias Nie\u00dfner. Scannet:\nRichly-annotated 3d reconstructions of indoor scenes.\nIn\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 5828\u20135839, 2017. 2, 4\n[10] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana\nEhsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:\nA universe of annotated 3d objects.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 13142\u201313153, 2023. 2, 3, 7\n[11] Congyue Deng, Chiyu Jiang, Charles R Qi, Xinchen Yan,\nYin Zhou, Leonidas Guibas, Dragomir Anguelov, et al.\nNerdi: Single-view nerf synthesis with language-guided dif-\nfusion as general image priors.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 20637\u201320647, 2023. 2, 3, 6, 7, 4\n[12] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ra-\nmanan. Depth-supervised nerf: Fewer views and faster train-\ning for free. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 12882\u2013\n12891, 2022. 3\n[13] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-\nnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-\nOr.\nAn image is worth one word: Personalizing text-to-\nimage generation using textual inversion.\narXiv preprint\narXiv:2208.01618, 2022. 1, 2, 3, 4, 5, 8\n[14] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano,\nGal Chechik, and Daniel Cohen-Or.\nEncoder-based do-\nmain tuning for fast personalization of text-to-image models.\narXiv preprint arXiv:2302.12228, 2023. 2, 4, 8\n[15] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano,\nGal Chechik, and Daniel Cohen-Or.\nEncoder-based do-\nmain tuning for fast personalization of text-to-image models.\narXiv preprint arXiv:2302.12228, 2023. 2, 4\n[16] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-\nage editing with cross attention control.\narXiv preprint\narXiv:2208.01626, 2022. 6, 4\n[17] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 3\n[18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in Neural Information\nProcessing Systems, 33:6840\u20136851, 2020. 2, 3\n[19] Allan Jabri, Sjoerd van Steenkiste, Emiel Hoogeboom,\nMehdi SM Sajjadi, and Thomas Kipf. Dorsal: Diffusion for\nobject-centric representations of scenes etal.. arXiv preprint\narXiv:2306.08068, 2023. 3\n[20] Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting nerf\non a diet: Semantically consistent few-shot view synthesis.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 5885\u20135894, 2021. 3\n[21] Rasmus Jensen, Anders Dahl, George Vogiatzis, Engin Tola,\nand Henrik Aan\u00e6s. Large scale multi-view stereopsis eval-\nuation. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 406\u2013413, 2014. 2, 6,\n7, 8, 3, 4, 11\n[22] Animesh Karnewar, Andrea Vedaldi, David Novotny, and\nNiloy J Mitra. Holodiffusion: Training a 3d diffusion model\nusing 2d images.\nIn Proceedings of the IEEE/CVF Con-\n9\nference on Computer Vision and Pattern Recognition, pages\n18423\u201318433, 2023. 2, 3\n[23] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. arXiv preprint arXiv:1312.6114, 2013. 3\n[24] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli\nShechtman, and Jun-Yan Zhu. Multi-concept customization\nof text-to-image diffusion. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 1931\u20131941, 2023. 2\n[25] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli\nShechtman, and Jun-Yan Zhu. Multi-concept customization\nof text-to-image diffusion. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 1931\u20131941, 2023. 2\n[26] Kai-En Lin, Yen-Chen Lin, Wei-Sheng Lai, Tsung-Yi Lin,\nYi-Chang Shih, and Ravi Ramamoorthi. Vision transformer\nfor nerf-based view synthesis from a single input image. In\nProceedings of the IEEE/CVF Winter Conference on Appli-\ncations of Computer Vision, pages 806\u2013815, 2023. 3\n[27] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot one image to 3d object, 2023. 2, 3, 8, 9\n[28] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie\nLiu, Taku Komura, and Wenping Wang. Syncdreamer: Gen-\nerating multiview-consistent images from a single-view im-\nage. arXiv preprint arXiv:2309.03453, 2023. 3\n[29] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 4\n[30] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan\nLi, and Jun Zhu. Dpm-solver: A fast ode solver for diffu-\nsion probabilistic model sampling in around 10 steps. arXiv\npreprint arXiv:2206.00927, 2022. 5\n[31] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongx-\nuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided\nsampling of diffusion probabilistic models. arXiv preprint\narXiv:2211.01095, 2022. 5\n[32] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and\nAndrea Vedaldi. Realfusion: 360deg reconstruction of any\nobject from a single image. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 8446\u20138455, 2023. 2, 3, 5\n[33] Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon,\nNima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and\nAbhishek Kar. Local light field fusion: Practical view syn-\nthesis with prescriptive sampling guidelines. ACM Transac-\ntions on Graphics (TOG), 38(4):1\u201314, 2019. 2, 4\n[34] Michael Niemeyer, Jonathan T Barron, Ben Mildenhall,\nMehdi SM Sajjadi, Andreas Geiger, and Noha Radwan. Reg-\nnerf: Regularizing neural radiance fields for view synthesis\nfrom sparse inputs. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n5480\u20135490, 2022. 3, 6, 4\n[35] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall.\nDreamfusion: Text-to-3d using 2d diffusion.\narXiv\npreprint arXiv:2209.14988, 2022. 2, 3\n[36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 2, 3, 4\n[37] Ali Rahimi and Benjamin Recht. Random features for large-\nscale kernel machines. Advances in neural information pro-\ncessing systems, 20, 2007. 4\n[38] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler,\nLuca Sbordone, Patrick Labatut, and David Novotny. Com-\nmon objects in 3d: Large-scale learning and evaluation of\nreal-life 3d category reconstruction. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 10901\u201310911, 2021. 2, 3, 7\n[39] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wier-\nstra. Stochastic backpropagation and approximate inference\nin deep generative models. In International conference on\nmachine learning, pages 1278\u20131286. PMLR, 2014. 3\n[40] Barbara Roessle, Jonathan T Barron, Ben Mildenhall,\nPratul P Srinivasan, and Matthias Nie\u00dfner. Dense depth pri-\nors for neural radiance fields from sparse input views. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 12892\u201312901, 2022. 3\n[41] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10684\u201310695, 2022. 1, 2, 3, 4, 5, 7\n[42] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmen-\ntation. In Medical Image Computing and Computer-Assisted\nIntervention\u2013MICCAI 2015: 18th International Conference,\nMunich, Germany, October 5-9, 2015, Proceedings, Part III\n18, pages 234\u2013241. Springer, 2015. 4\n[43] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 22500\u2013\n22510, 2023. 2, 3\n[44] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022. 2\n[45] Mehdi SM Sajjadi, Henning Meyer, Etienne Pot, Urs\nBergmann, Klaus Greff, Noha Radwan, Suhani Vora, Mario\nLu\u02c7ci\u00b4c, Daniel Duckworth, Alexey Dosovitskiy, et al. Scene\nrepresentation transformer: Geometry-free novel view syn-\nthesis through set-latent scene representations. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 6229\u20136238, 2022. 3\n[46] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al.\nLaion-5b:\nAn open large-scale dataset for\ntraining next generation image-text models. arXiv preprint\narXiv:2210.08402, 2022. 2, 3\n10\n[47] Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon\nKo, Hyeonsu Kim, Junho Kim, Jin-Hwa Kim, Jiyoung Lee,\nand Seungryong Kim.\nLet 2d diffusion model know 3d-\nconsistency for robust text-to-3d generation. arXiv preprint\narXiv:2303.07937, 2023. 3\n[48] Seunghyeon Seo, Donghoon Han, Yeonjin Chang, and Nojun\nKwak. Mixnerf: Modeling a ray with mixture density for\nnovel view synthesis from sparse inputs. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 20659\u201320668, 2023. 3\n[49] Yawar Siddiqui, Lorenzo Porzi, Samuel Rota Bul`o, Nor-\nman M\u00a8uller, Matthias Nie\u00dfner, Angela Dai, and Peter\nKontschieder. Panoptic lifting for 3d scene understanding\nwith neural fields. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n9043\u20139052, 2023. 2\n[50] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In International Confer-\nence on Machine Learning, pages 2256\u20132265. PMLR, 2015.\n2, 3, 4\n[51] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020. 5\n[52] Yang Song and Stefano Ermon. Generative modeling by esti-\nmating gradients of the data distribution. Advances in neural\ninformation processing systems, 32, 2019. 2, 3\n[53] Shao-Hua Sun, Minyoung Huh, Yuan-Hong Liao, Ning\nZhang, and Joseph J Lim. Multi-view to novel view: Syn-\nthesizing novel views with self-learned confidence. In Pro-\nceedings of the European Conference on Computer Vision\n(ECCV), pages 155\u2013171, 2018. 3\n[54] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea\nVedaldi. Viewset diffusion:(0-) image-conditioned 3d gener-\native models from 2d data. arXiv preprint arXiv:2306.07881,\n2023. 2, 3\n[55] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara\nFridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra-\nmamoorthi, Jonathan Barron, and Ren Ng. Fourier features\nlet networks learn high frequency functions in low dimen-\nsional domains. Advances in Neural Information Processing\nSystems, 33:7537\u20137547, 2020. 3, 4\n[56] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li,\nBrent Yi, Justin Kerr, Terrance Wang, Alexander Kristof-\nfersen, Jake Austin, Kamyar Salahi, Abhik Ahuja, David\nMcAllister, and Angjoo Kanazawa. Nerfstudio: A modular\nframework for neural radiance field development. In ACM\nSIGGRAPH 2023 Conference Proceedings, 2023. 6, 2, 11\n[57] Maxim Tatarchenko, Alexey Dosovitskiy, and Thomas Brox.\nMulti-view 3d models from single images with a convo-\nlutional network.\nIn Computer Vision\u2013ECCV 2016: 14th\nEuropean Conference, Amsterdam, The Netherlands, Octo-\nber 11\u201314, 2016, Proceedings, Part VII 14, pages 322\u2013337.\nSpringer, 2016. 3\n[58] Ayush Tewari, Tianwei Yin, George Cazenavette, Semon\nRezchikov, Joshua B Tenenbaum, Fr\u00b4edo Durand, William T\nFreeman, and Vincent Sitzmann.\nDiffusion with forward\nmodels: Solving stochastic inverse problems without direct\nsupervision. arXiv preprint arXiv:2306.11719, 2023. 3\n[59] Prune Truong, Marie-Julie Rakotosaona, Fabian Manhardt,\nand Federico Tombari. Sparf: Neural radiance fields from\nsparse and noisy poses. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 4190\u20134200, 2023. 3\n[60] Dani Valevski, Danny Wasserman, Yossi Matias, and Yaniv\nLeviathan. Face0: Instantaneously conditioning a text-to-\nimage model on a face. arXiv preprint arXiv:2306.06638,\n2023. 2, 4, 8\n[61] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro\nCuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj,\nand Thomas Wolf. Diffusers: State-of-the-art diffusion mod-\nels, 2022. 4\n[62] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir\nAberman.\np+: Extended textual conditioning in text-to-\nimage generation. arXiv preprint arXiv:2303.09522, 2023.\n2, 4\n[63] Guangcong Wang, Zhaoxi Chen, Chen Change Loy, and\nZiwei Liu.\nSparsenerf: Distilling depth ranking for few-\nshot novel view synthesis. arXiv preprint arXiv:2303.16196,\n2023. 3, 8\n[64] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,\nand Greg Shakhnarovich. Score jacobian chaining: Lifting\npretrained 2d diffusion models for 3d generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 12619\u201312629, 2023. 2, 3\n[65] Zirui Wang,\nShangzhe Wu,\nWeidi Xie,\nMin Chen,\nand Victor Adrian Prisacariu.\nNerf\u2013:\nNeural radiance\nfields without known camera parameters.\narXiv preprint\narXiv:2102.07064, 2021. 3, 7\n[66] Daniel Watson, William Chan, Ricardo Martin-Brualla,\nJonathan\nHo,\nAndrea\nTagliasacchi,\nand\nMohammad\nNorouzi. Novel view synthesis with diffusion models. arXiv\npreprint arXiv:2210.04628, 2022. 3\n[67] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei\nZhang, and Wangmeng Zuo. Elite: Encoding visual con-\ncepts into textual embeddings for customized text-to-image\ngeneration. arXiv preprint arXiv:2302.13848, 2023. 2, 4, 8\n[68] Olivia Wiles, Georgia Gkioxari, Richard Szeliski, and Justin\nJohnson.\nSynsin: End-to-end view synthesis from a sin-\ngle image.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 7467\u2013\n7477, 2020. 3\n[69] Jamie Wynn and Daniyar Turmukhambetov. Diffusionerf:\nRegularizing neural radiance fields with denoising diffu-\nsion models. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 4180\u2013\n4189, 2023. 3\n[70] Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical\nevaluation of rectified activations in convolutional network.\narXiv preprint arXiv:1505.00853, 2015. 5, 4\n[71] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Humphrey\nShi, and Zhangyang Wang. Sinnerf: Training neural radiance\nfields on complex scenes from a single image. In European\nConference on Computer Vision, pages 736\u2013753. Springer,\n2022. 3, 7\n11\n[72] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang,\nand Zhangyang Wang. Neurallift-360: Lifting an in-the-wild\n2d photo to a 3d object with 360\u00b0 views.\narXiv e-prints,\npages arXiv\u20132211, 2022. 3\n[73] Jiawei Yang, Marco Pavone, and Yue Wang. Freenerf: Im-\nproving few-shot neural rendering with free frequency reg-\nularization.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 8254\u2013\n8263, 2023. 3, 8, 4\n[74] Paul Yoo, Jiaxian Guo, Yutaka Matsuo, and Shixiang Shane\nGu.\nDreamsparse: Escaping from plato\u2019s cave with 2d\nfrozen diffusion model given sparse views. arXiv preprint\narXiv:2306.03414, 2023. 3\n[75] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.\npixelnerf: Neural radiance fields from one or few images.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 4578\u20134587, 2021. 3,\n6, 7, 4\n[76] Zehao Yu, Anpei Chen, Bozidar Antic, Songyou Peng Peng,\nApratim Bhattacharyya, Michael Niemeyer, Siyu Tang,\nTorsten Sattler, and Andreas Geiger. Sdfstudio: A unified\nframework for surface reconstruction, 2022. 6, 2, 11\n[77] Zehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sat-\ntler, and Andreas Geiger.\nMonosdf: Exploring monocu-\nlar geometric cues for neural implicit surface reconstruc-\ntion.\nAdvances in neural information processing systems,\n35:25018\u201325032, 2022. 3\n[78] Lvmin Zhang and Maneesh Agrawala. Adding conditional\ncontrol to text-to-image diffusion models.\narXiv preprint\narXiv:2302.05543, 2023. 3, 6\n[79] Yuxin Zhang, Weiming Dong, Fan Tang, Nisha Huang,\nHaibin Huang, Chongyang Ma, Tong-Yee Lee, Oliver\nDeussen, and Changsheng Xu. Prospect: Expanded condi-\ntioning for the personalization of attribute-aware image gen-\neration. arXiv preprint arXiv:2305.16225, 2023. 2\n[80] Yuxin Zhang, Nisha Huang, Fan Tang, Haibin Huang,\nChongyang Ma, Weiming Dong, and Changsheng Xu.\nInversion-based style transfer with diffusion models. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 10146\u201310156, 2023. 2, 4, 8\n[81] Zhizhuo Zhou and Shubham Tulsiani. Sparsefusion: Dis-\ntilling view-conditioned diffusion for 3d reconstruction. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 12588\u201312597, 2023. 3\n1\nViewpoint Textual Inversion: Unleashing Novel View Synthesis with Pretrained\n2D Diffusion Models\nSupplementary Material\nSupplementary overview\nAll supplementary figures are after the supplementary text.\nEspecially important are Fig. 10 and Fig. 8, which are 3-\nview and single-view novel view synthesis predictions for\nevery scene in the DTU test set.\nThe supplementary sections and their figures are:\n\u2022 A: diffusion model outfilling tests as evidence that diffu-\nsion models do 3D reasoning. Fig. 12.\n\u2022 B: qualitative ablations of ViewNeTI design decisions for\nsingle-view NVS. Fig. 16, Fig. 14.\n\u2022 C: image and text prompt augmentation details. Fig. 21.\n\u2022 D: visualizations of the camera positions in DTU pretrain-\ning data. Fig. 23.\n\u2022 E: visualizations of the camera positions in DTU sparse\nview datasets (1, 3, and 6 views). Fig. 23.\n\u2022 F: validation of ViewNeTI pretraining of the view-\nmapper,\nand disentanglement of the object-mapper.\nFig. 25, Fig. 27.\n\u2022 G: validation that ViewNeTI works with spherical coor-\ndinate camera parameterization. Fig. 20.\n\u2022 H: comparison to Zero-1-to-3 [27], a baseline for single-\nview NVS. Fig. 18.\n\u2022 I: implementation details for ViewNeTI.\n\u2022 J: implementation details for evaluation on DTU.\nA. Evidence for 3D capabilities in diffusion\nmodels with image outfilling\nAs discussed in\nSec. 1, our work is motivated by the\nobservation that 2D image diffusion models seem ca-\npable of reasoning about 3D phenomena.\nIn\nFig. 12\nwe ask a diffusion model to do outfilling around a real\ncar.\nIn the figure caption, we discuss the evidence\nfor 3D reasoning. The model is a Stable Diffusion, [41]\nstabilityai/stable-diffusion-2-inpainting\ncheckpoint, run for 50 denoising steps. The car and the\nmask are from Common Objects in 3D\n[38], car id\n106 12650 23736.\nB. Ablations\nWe do ablations for our design decisions for two scenes:\none object-based scene in Fig. 14, and one multiple-object\nscene in Fig. 16. We now elaborate on the significance of\nsome of the ablations. The \u03b1 parameter is from Eq. (3): a\nlarger value gives more flexibility to predict the text encod-\ning directly, rather than just the word embedding. \u201cNo ge-\nometric augmentations\u201d means we remove the rotation and\nrandom cropping, but keep random color jitter and random\nGaussian blur. The view-mapper is pretrained and frozen\n(except for the pretraining ablations), and so the architec-\nture changes are applied to the object-mapper only.\nC. Data augmentation\nAs in Sec. 4.2, we apply the image augmentations to help\nlearn a robust object token. The augmentations are similar\nto [32] with some changes: no grayscaling, because that\nleads to some gray generations; higher probability Gaus-\nsian blur; and no horizontal flips, since it would prevent\nlearning the view-mapper. The ablations in Appendix B\ndemonstrate the necessity of these augmentations. For 3-\nview NVS, we found that such strong augmentations were\nnot necessary, so we reduced the size parameter of the\nRandomResizedCrop to (0.950, 1.05).\nWe also do text prompt augmentations. As described\nin Methods, the text encoder input is the prompt \u201cSRi. A\nphoto of an So\u201d, where SRi and So are controlled by the\nview and object mappers respectively. This is the same as\nregular textual inversion, but with the view token [13]. Fol-\nlowing that work, we use different text templates, for ex-\nample \u201cSRi. a rendition of the So.\u201d and \u201cSRi. a photo of\na small So.\u201d [13, 36]. We use the same templates as [1],\nwhich are also available in our code.\nD. Visualizing the pretraining camera posi-\ntions\nAs discussed in Sec. 4.3, we pretrain a view-mapper on a\nset of views that are consistent across scenes. We visualize\nthis distribution in Fig. 23 (the figure is annotated with the\nsparse-view splits, but for pretraining we use all the visible\ncameras). This visualization was generated by running the\nintroductory example for SDFStudio [76] at their github\nrepo, which does visualization with the interactive nerfstu-\ndio viewer [56]. The two images are different views of\nthe same 3D scene, but to get a better understanding of the\n3D distribution, we recommend running the example SDF-\nStudio code and interacting with the viewer directly. The\ncameras are equidistant from an origin that has the object\n(they live on a sphere), and they approximately point at the\norigin. They cover an azimuthal range around the origin of\nabout 160\u25e6, with a polar range of about 75\u25e6. Note that al-\nthough the cameras may be focused on one point, the object\nis not necessarily placed at that point, so the object will be\nnot centered in all images (for example, refer to the ground\n2\ntruth images in Fig. 10).\nE. Visualizing the camera positions for DTU\nsparse dataset\nIn Sec. 5.1 and Fig. 4, we claimed that training on the 6-\nview split requires generalizing to some views that are \u2018in-\nterpolations\u2019 and others that are \u2018extrapolations\u2019. To sup-\nport this, we annotate the 6-view split that was used in\nFig. 23 (we explain how we generated this visualization in\nAppendix D, and the explanation of training splits is in Ap-\npendix J). All other visualized camera views are test views.\nCompared to the full set of images, the 6-view split covers\nmost of the range in the azimuthal angle, \u03c6 around the ob-\nject, but only covers about half the range in the polar angle,\n\u03b8. It is intuitive that the test views outside the polar angle\nrange are a more challenging generalization problem, and\nthis is what we refer to as \u2018extrapolation\u2019. More concretely,\nthe extrapolated views are those outside the convex hull of\nthe train views in the angle space (\u03c6, \u03b8).\nF. Pretraining results\nIn Sec. 5.2, we describe the model pretraining of dense\nviews from multiple scenes from the DTU training dataset\n[21]. We use caption of the form \u201cSRi. A photo of an\nS\u2217j\u201d. The token for viewpoint, SRi, is controlled by the\nview-mapper and is shared across all scenes. The token\nfor the jth object, S\u2217j, is controlled by an object mapper\nand is the same for all views within a scene, but different\nacross scenes. We verify that the view-mapper has learned\nthe training views by checking reconstructions from sam-\nple scenes. In Fig. 25, we reconstruct every view for the\ntraining scenes 9 and 33 after 100k steps of training\nThe captions above are constructed so that the object to-\nken captures all the view-invariant object semantics; that\nis, we want the viewpoint and semantics to be disentangled.\nOne way to check this is to try generating images using only\nthe object token, e.g. \u201c A photo of an S\u2217j\u201d. We do this in\nFig. 27 for a sample of the object tokens at different points\nin training. This suggests the object tokens approximately\nembed the object semantics, though with varying degrees of\nsuccess. Firstly, the generations are different and far more\ndiverse than the original. Secondly, they have a different\n\u2018style\u2019 to the DTU images; especially for the toy houses,\nthey become more realistic, and arguably closer to the dif-\nfusion model training distribution. Third, for some scenes,\nthe disentanglement become worse with more training time;\nin the first row, the generations have some similar attributes\nafter 10k steps, but after 100k steps they are only a circular\npattern.\nIn prior literature on textual inversion and personaliza-\ntion, evaluations are done on \u2018reconstruction\u2019 ability or \u2018fi-\ndelity\u2019 to the original images [1, 13, 43]. This refers to\nthe correctness of object semantics and details in the gener-\nated images, and it tends to be traded off against \u2018editabil-\nity\u2019, which is the ability to compose the new concepts with\nexisting concepts in the vocabulary [1] by mixing the to-\nken with existing tokens in the prompt. How do these ideas\nrelate to our setting? The reconstructions in the NVS ex-\nperiments are good in most cases, and they are much more\nfaithful to the images in the \u2018disentanglement\u2019 test. We pro-\npose two explanations. First, the view-mapper could be em-\nbedding information not captured by the object token; we\nwould expect the view-mapper to capture attributes that are\ncommon across the scenes, and this might include common\nbackgrounds (the table in DTU images is always white or\nbrown, and there is a black background); or it could capture\ncommon \u2018style\u2019 in the object semantics (e.g. there are many\nstatues, many food items, and many toy houses). Second, it\ncould be that generating images with only the object token\nin the caption does not properly test disentanglement, but\nwe do not have any further insights about this.\nG. Validation of spherical coordinate parame-\nterization\nOur method is agnostic to the camera parameterization.\nOur main results use the camera-to-world projection matrix\ngiven in the DTU-MVS dataset [21], but in Fig. 20, we\nshow NVS results where the camera is parameterized by\nspherical coordinates. We assume a central object is fixed\nat the origin, and that the camera is at a fixed radius with\nvariable polar and azimuth angles, (\u03b8, \u03c6); we assume the\ncamera is pointed at the origin. We do single-scene opti-\nmization of ViewNeTI on a rendering of a ShapeNet car [5].\nTo encourage the image to be close to the diffusion model\ntraining distribution, we generated an augmented training\nset by outfilling the background around the car, similar to in\nAppendix A. The left and right columns are reconstructions\nof camera poses from the multiview train set. The middle\ncolumns are NVS predictions from interpolating the polar\nand azimuth angles.\nH. Qualitative comparison to Zero-1-to-3\nWe add another qualitative basleine, Zero-1-to-3,\n[27],\nwhich treats novel view synthesis as image-to-image trans-\nlation with a diffusion model, conditioned on the change in\ncamera parameter. It is trained on renderings from Obja-\nverse, a large dataset of 3D assets available online [10]. By\ntraining on a large dataset, Zero-1-to-3 is intended to \u2018zero-\nshot\u2019 generalize to new scenes without any extra finetuning,\nsimilar to zero-shot classification in CLIP [36]. This has\nthe advantage that NVS predictions are generated quickly -\nthe time taken to generate a sample with Stable Diffusion\nmodel. On the other hand, this poses a very difficult chal-\nlenge to generalize beyond the Objaverse data distribution.\n3\nUnlike in CLIP, the training distribution of Zero-1-to-3 does\nnot yet cover the full distribution of test scenes of interest\nfor 3D perception applications [9, 21, 33], and this is be-\ncause enormous multiview datasets are harder to collect that\n2D image datasets.\nIn Fig. 18, we compare with Zero-1-to-3 using the same\nviews as Fig. 5 (views 1, 45, and 22). We also show pre-\ndictions for \u2018no camera change\u2019, which is the train view re-\nconstruction. We do the standard image preprocessing by\nmasking the central objects, centering them, and padding\nthem to have aspect ratio 1. Zero-1-to-3 does good recon-\nstructions, and generates a good prediction for the brick\nscene in view 45, which is the closest view to the input.\nThe other predictions are worse. This is likely due to the\ndistribution shift in objects:\n\u2022 DTU has real images with realistic details, while Obja-\nverse (the Zero-1-to-3 training distribution) is mostly ren-\ndered objects from artist-generated 3D models.\n\u2022 DTU images are also \u2018cropped\u2019: they do not contain the\nentire object. The Zero-1-to-3 renders contain the entire\nobject.\n\u2022 DTU images have some multi-object scenes, while Obja-\nverse is mostly single objects. Although the Zero-1-to-3\npaper showed some generalization to multi-object scenes,\nthe object details were not good.\nFinally, note that the failure modes of Zero-1-to-3 on\nDTU are distinct from the NeRF-based models in Fig. 5,\nwhich all have imaging artifacts.\nSimilar to ViewNeTI,\nthe predictions do not have such artifacts, probably because\nboth methods use diffusion models that are trained to gen-\nerate images from a certain distribution of real images.\nI. ViewNeTI implementation detais\nThe frozen diffusion model is Stable Diffusion [16], ver-\nsion\nstabilityai/stable-diffusion-2-1\nac-\ncessed from the diffusers library [61]. The weights are all\nfrozen. We did not test our method on Stable Diffusion1.\nThe inputs for camera parameters, timestep, and Unet\nlayer are embedded to a 64-dim random Fourier feature vec-\ntor. Specifically, for each of the 12 camera params, one\ntimestep, and one unet layer, we sample 64 random frequen-\ncies from a standard normal, N(0, \u03c32) where \u03c3 is 0.5, 0.03,\nand 2 respectively. The encoding is computed as in [55],\nand as shown in our code. For the object-mapper, the en-\ncoding is the same, but without the camera parameters.\nFollowing the base architecture of [1], the encoding is\npassed through an MLP with two blocks. Each block has a\nlinear layer, LayerNorm [3], and LeakyRelu [70]. Finally,\nthey are projected to 1536 dimensions, which is twice the\nword-embedding for Stable Diffusion 2. This gives 140,000\nparameters, which is the same for the view-mappers and\nobject-mappers. The first half of the output is the word em-\nbedding and the second half is the textual bypass, as de-\nscribed in Sec. 4.1.\nThe word embedding input is scaled to have the same\nnorm as a particular placeholder word, for example \u2018statue\u2019\nfor the buddha statue scene (again, like in [1]). We did\none experiment on varying this word on one scene, and this\nshowed that while norm scaling was important, the exact\nchoice of word for the reference token was not, so we just\nused \u2018statue\u2019 for every scene in all experiments.\nWe use an effective batch size of 9 (batch size 3 with\n3 gradient accumulation steps), a constant learning rate of\n0.09, with the AdamW optimizer [29] (again, like in [1]).\nIn training, DTU images were resized to (512, 384), which\nhas the same aspect ratio as the originals. At inference, we\nfound that image quality was better (fewer artifacts) if sam-\npling at a higher resolution, (768, 576). Since Stable Dif-\nfusion 2 was trained with square images at (768,768), we\nexperimented with padding DTU images to have the same\naspect ratio, but we found these results to be worse.\nJ. DTU evaluation details\nThe DTU [21] splits are the same as [75]. The test set\nscenes are (8, 21, 30, 31, 34, 38, 40, 41, 45, 55, 63, 82,\n103, 110, 114), which are the ones visualized in Fig. 10\nand Fig. 8, and used for quantitative results in Tab. 1. For\npretraining, the train scenes are every non-test scene except\n(1, 2, 7, 25, 26, 27, 29, 39, 51, 54, 56, 57, 58, 73, 83, 111,\n112, 113,115, 116, 117). They are excluded because they\nhave too much overlap with the train set; e.g 115-117 have\nthe same obejct as scene 114, but with a different pose. This\nensures that there is a domain shift between train and test\nwith respect to object semantics.\nThe splits for views also follow prior work [11, 34, 73,\n75]. The index names for views are 0-indexed, while the\nDTU filenames are 1-indexed. The standard 9-view splits\n(which we do not experiment with here) use train views (25,\n22, 28, 40, 44, 48, 0, 8, 13). The 6-view, 3-view, and 1-view\nsplits choose the first 6, 3, and 1 views from that list respec-\ntively. The test views are all indexes from 0-49 except a set\nthat were excluded due to image quality which are views\n(3, 4, 5, 6, 7, 16, 17, 18, 19, 20, 21, 36, 37, 38, 39). We\nuse all the train and test views for pretraining, and unlike in\nPixelNerf [75], we do not include the excluded views for\npretraining.\nIn figures Fig. 8 and Fig. 10 we evaluate novel views in\n(1, 8, 12, 15, 24, 27, 29, 33, 40, 43, 48). We chose them as\na representative subset of the test set views. In Fig. 5, we\nevaluate the novel views (1, 45, 22), and they were chosen\nto match with evaluation in prior work [11].\n4\n1 input image\nNovel views\nFigure 8. ViewNeTI Novel view synthesis predictions for every DTU test set scene from one input view (the rows are alternating ground\ntruth and prediction). Higher resolution images available at our project website. All scenes have the same training hyperparameters and\nrandom seed for training and generation. The hyperparameters are also the same as the three-view case in Fig. 10, except training steps are\nreduced from 3k to 1.5k, and the image augmentations are slightly changed as described in Appendix C. In almost all cases, the rendered\nviews are photorealistic, and the object semantics are close to the ground truth, though semantics are worse for more complex scenes. The\nfailure modes are incorrect object details, and misaligned camera poses. By contrast, NeRF-based methods will have consistent object\nsemantics across views due to the explicit 3D representation, but have much worse image quality (see NeRF baseline comparison in Fig. 5\nfor these comparisons). Different to the three-view case, another failure mode is for novel view predictions to be too close to the input view\n(overfitting). We mitigate this by reducing the training steps, to 1.5k. The views chosen for input are standard from previous work, and the\nnovel viewpoints are chosen to cover the full sequence of views in DTU (see Appendix J).\n5\n3 input images\nNovel views\nFigure 10. ViewNeTI novel view synthesis predictions for every DTU test set scene from three input views (the rows are alternating ground\ntruth and prediction). Higher resolution images available at our project website. All scenes have the same training hyperparameters and\nrandom seed for training and generation. In almost all cases, the rendered views are photorealistic, and the object semantics are close to\nthe ground truth, though semantics are worse for more complex scenes. The failure modes are incorrect object details, and misaligned\ncamera poses. By contrast, NeRF-based methods have consistent object semantics across views due to the explicit 3D representation, but\nmuch worse image quality (see NeRF baseline comparison in Fig. 5 for these comparisons). The views chosen for input are standard from\nprevious work, and the novel viewpoints are chosen to cover the full sequence of views in DTU (see Appendix J).\n6\nInput\nOut\ufb01ll mask\n\u201ca car on a shiny surface\u201d\n\u201ca car on the beach\u201d\n420\nFigure 12. Evidence that image diffusion models can reason in 3D. We take the car on the top left, and the mask in the bottom left [38],\nand run a Stable Diffusion [41] inpainting model. The model only sees the car object itself. The shadows inferred for the \u2018beach\u2019 prompt\nare the same as the shadow in the real image; the model has inferred the shadow placement from the lighting pattern on the car. For the\n\u2018shiny surface\u2019 prompt, the model has reflected the car object. We also see reflections for the right-side beach scene.\n7\nGround truth\nViewNeTI \nprediction\nLearnable \nview-mapper\nNo geometric \naugmentations\nPretraining steps\n50k \u21d2 20k\nPretraining steps\n50k \u21d2 10k\nCamera encoding \nfreq 0.5 \u21d2 0.1\nNo norm scaling\nCamera encoding \nfreq 0.5 \u21d2 1\nTextual bypass\n\u03b1 = 5 \u21d2 \u03b1 = 0.2\nNo textual bypass\n80\nFigure 14. Ablation of design parameters for single-view NVS for DTU scan 114. The test views are the same as Fig. 8 and Fig. 10, and\nare listed in Appendix J.\n8\nGround truth\nViewNeTI \nprediction\nLearnable \nview-mapper\nNo geometric \naugmentations\nPretraining steps\n50k \u21d2 20k\nPretraining steps\n50k \u21d2 10k\nCamera encoding \nfreq 0.5 \u21d2 0.1\nNo norm scaling\nCamera encoding \nfreq 0.5 \u21d2 1\nTextual bypass\n\u03b1 = 5 \u21d2 \u03b1 = 0.2\nNo textual bypass\nFigure 16. Ablation of design parameters for single-view NVS for DTU scan 31. The test views are the same as Fig. 8 and Fig. 10, and\nare listed in Appendix J.\nZero-1-to-3\nGT\nViewNeTI (ours)\nNovel views\nInput view\nNovel views\nInput view\nNovel views\nInput view\n290\n740\nFigure 18. Baseline comparison of ViewNeTI with Zero-1-to-3 [27], a zero shot novel view synthesis model. We use the same views as\nFig. 5.\n9\nTraining images\nTraining image\nInterpolated images\nTraining images\n340\n235\nFigure 20. Examples of novel view synthesis using ViewNeTI where the input camera parameters are in spherical coordinate system. We\ndo single-scene NVS using an input dataset of nine multiview images of a ShapeNet car [5] with random forward-facing poses. The left\nand right columns are views in the multiview train set. The middle columns are NVS predictions from interpolating the polar and azimuth\nangles between the two edge images. The interpolation of the car poses are smooth and the object semantic consistency is good..\nt r a n s f o r m = T . Compose ( [\nT . RandomApply ( [ T . RandomRotation ( degrees =10 ,\nf i l l =1)] , p =0.75) ,\nT . RandomResizedCrop ( image size ,\ns c a l e =(0.70 ,\n1 . 3 ) ) ,\nT . RandomApply ( [ T . C o l o r J i t t e r ( 0 . 0 4 ,\n0.04 ,\n0.04 ,\n0 . 0 4 ) ] ,\np =0.75) ,\nT . RandomApply ( [ T . GaussianBlur (5 ,\n( 0 . 1 ,\n2 ) ) ] ,\np =0.20) ,\n] )\nFigure 21. PyTorch code for the image augmentations in single-view novel view synthesis. The 3-view NVS augmentations are slighty\nchanged, as described in Appendix C.\n10\n28\n25\n22\n44\n48\n40\n25\n28\n22\n48\n44\n40\n435\nFigure 23. Visualization of the camera view positions in a sample DTU scene [21]. The camera positions are the same for all scenes, and\nthe left and right images are the same 3D scene from different angles. Single-view training uses view 25. Three-view training uses the\nsame view 25, plus views 22 and 28 (yellow). Six-view training uses those three views, plus views 40, 44, and 48 (green). Visualization\nby running SDFStudio [76], and visualized with nerfstudio [56].\nGround truth\nPrediction\nGround truth\nPrediction\nGround truth\nPrediction\nGround truth\nPrediction\n360\nFigure 25. Reconstructions of every training view in scenes 9 and 33 during pretraining for 100k steps. The scenes have the same view-\nmapper, but different object-mappers. The correct reconstructions validates the view-mapper, which is shared across scenes, has learned\nthe relative mapping between the camera viewpoints.\n11\nObject\nGenerations from the learned object mapper\n10k steps\n50k steps\n100k steps\nFigure 27. Disentanglement results: for 10 random scenes in the pretraining dataset, we generate images for the object-mapper without the\nview-mapper using prompts \u201cA photo of a So\u201d for special token So. The left column is view 25 from the original data. The other columns\ntake two samples at different points in training.\n12\n"
  }
]