[
  {
    "title": "Tree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust",
    "link": "https://arxiv.org/pdf/2305.20030.pdf",
    "upvote": "6",
    "text": "Tree-Ring Watermarks: Fingerprints for Diffusion\nImages that are Invisible and Robust\nYuxin Wen, John Kirchenbauer, Jonas Geiping, Tom Goldstein\nUniversity of Maryland\nAbstract\nWatermarking the outputs of generative models is a crucial technique for tracing\ncopyright and preventing potential harm from AI-generated content. In this paper,\nwe introduce a novel technique called Tree-Ring Watermarking that robustly fin-\ngerprints diffusion model outputs. Unlike existing methods that perform post-hoc\nmodifications to images after sampling, Tree-Ring Watermarking subtly influences\nthe entire sampling process, resulting in a model fingerprint that is invisible to\nhumans. The watermark embeds a pattern into the initial noise vector used for\nsampling. These patterns are structured in Fourier space so that they are invari-\nant to convolutions, crops, dilations, flips, and rotations. After image generation,\nthe watermark signal is detected by inverting the diffusion process to retrieve the\nnoise vector, which is then checked for the embedded signal. We demonstrate\nthat this technique can be easily applied to arbitrary diffusion models, includ-\ning text-conditioned Stable Diffusion, as a plug-in with negligible loss in FID.\nOur watermark is semantically hidden in the image space and is far more robust\nthan watermarking alternatives that are currently deployed. Code is available at\nhttps://github.com/YuxinWenRick/tree-ring-watermark.\n1\nIntroduction\nThe development of diffusion models has led to a surge in image generation quality. Modern text-\nto-image diffusion models, like Stable Diffusion and Midjourney, are capable of generating a wide\nvariety of novel images in an innumerable number of styles. These systems are general-purpose\nimage generation tools, able to generate new art just as well as photo-realistic depictions of fake\nevents for malicious purposes.\nThe potential abuse of text-to-image models motivates the development of watermarks for their\noutputs. A watermarked image is a generated image containing a signal that is invisible to humans\nand yet marks the image as machine-generated. Watermarks document the use of image generation\nsystems, enabling social media, news organizations, and the diffusion platforms themselves to\nmitigate harms or cooperate with law enforcement by identifying the origin of an image [Bender\net al., 2021, Grinbaum and Adomaitis, 2022].\nResearch and applications of watermarking for digital content have a long history, with many\napproaches being considered over the last decade [O\u2019Ruanaidh and Pun, 1997, Langelaar et al.,\n2000]. However, so far research has always conceptualized the watermark as a minimal modification\nimprinted onto an existing image [Solachidis and Pitas, 2001, Chang et al., 2005, Liu et al., 2019, Fei\net al., 2022]. For example, the watermark currently deployed in Stable Diffusion [Cox et al., 2007],\nworks by modifying a specific Fourier frequency in the generated image.\nThe watermarking approach we propose in this work is conceptually different: This is the first\nwatermark that is truly invisible, as no post-hoc modifications are made to the image. Instead,\nthe distribution of generated images is imperceptibly modified and an image is drawn from this\nCorrespondence to: Yuxin Wen <ywen@umd.edu>\narXiv:2305.20030v3  [cs.LG]  4 Jul 2023\nPredict\nFFT\nxT\nWatermarked xT\nIFFT\nDDIM\n\u201cA Teddy bear in \nWashington DC\u201d\nWatermarked Image\nGeneration\nAttack\nDetection\nDDIM Inversion\n+ FFT\n\u201c\u201d (empty prompt)\nDistance to\n \n< \nPredefined Key:\nWatermarking\nFourier Space\nStrong Perturbation\nInverted Fourier Space\nFigure 1: Pipeline for Tree-Ring Watermarking. A diffusion model generation is watermarked and later detected\nthrough ring-patterns in the Fourier space of the initial noise vector.\nmodified distribution. This way, the actual sample carries no watermark in the classical additive sense,\nhowever an algorithmic analysis of the image can detect the watermark with high accuracy. From a\nmore practical perspective, the watermark materializes in minor changes in the potential layouts of\ngenerated scenes, that cannot be distinguished from other random samples by human inspection.\nThis new approach to watermarking, which we call Tree-Ring Watermarking based on the patterns im-\nprinted into the Fourier space of the noise vector of the diffusion model, can be easily incorporated into\nexisting diffusion model APIs and is invisible on a per-sample basis. Most importantly, Tree-Ring Wa-\ntermarking is far more robust than existing methods against a large battery of common image transfor-\nmations, such as crops, color jitter, dilation, flips, rotations, or noise. Tree-Ring Watermarking requires\nno additional training or finetuning to implement, and the watermark can only be detected by parties\nin control of the image generation model. We validate the watermark in a number of tests, measuring\nnegligible impact on image quality scores, high robustness to transformations, the low false-positive\nrate in detection, and usability for arbitrary diffusion models both with and without text conditioning.\n2\nRelated Work\nDiffusion Models\nDiffusion Models, arising out of the score-based generative models of the\nformalism of Song and Ermon [2019, 2020], are the currently strongest models for image generation\n[Ho et al., 2020, Dhariwal and Nichol, 2021]. Diffusion models are capable of sampling new images\nat inference time by iteratively processing an initial noise map. The most prominent sampling\nalgorithm in deployment is DDIM sampling [Nichol and Dhariwal, 2021] without additional noise,\nwhich can generate high-quality images in fewer steps than traditional DDPM sampling. Diffusion\nmodels are further accelerated for practical usage by optimizing images only in the latent space of a\npre-trained VAE, such as in latent diffusion [Rombach et al., 2022].\nWatermarking Digital Content\nStrategies to imprint watermarks onto digital content, especially\nimages, have a long tradition in computer vision. Approaches such as Boland [1996], Cox et al.\n[1996], O\u2019Ruanaidh and Pun [1997] describe traditional watermark casting strategies based on\nimprinting a watermark in a suitable frequency decomposition of the image, constructed through\nDCT, DWT, Fourier-Mellin, or complex wavelet transformations. These frequency transformations\nall share the beneficial property that simple image manipulations, such as translations, rotations,\nand resizing are easily understandable and watermarks can be constructed with robustness to these\ntransformations in mind. A fair evaluation of watermarking approaches appears in Pitas [1998],\nKutter and Petitcolas [1999], which highlight the importance of measurement of false-positive rates\nfor each strategy and ROC-curves under attack through various image manipulations. Work continues\n2\n(a) W/o Watermark\n(b) DwtDct\n(c) RivaGAN\n(d) Tree-Ring (Ours)\nFigure 2: Various watermarked generations with the same random seed are presented, showcasing the \u201cinvisible\u201d\nnature of our proposed watermark. A zoomed-in view with high contrast is provided in the bottom right corner.\nFor more high-resolution watermarked images, please refer to Supplementary Material.\non imprinting watermarks, with strategies based on SVD decompositions [Chang et al., 2005], Radon\ntransformations [Seo et al., 2004] and based on multiple decompositions [Al-Haj, 2007].\nFingerprinting and Watermarking Generative Models\nThe development of modern deep neural\nnetworks opened up new possibilities for \u201cdeep\u201d watermarking. Hayes and Danezis [2017] and Zhu\net al. [2018] propose strategies to learn watermarking end-to-end, where both the watermark encoder\nand the watermark decoder are learned models, optimized via adversarial objectives to maximize\ntransmission and robustness [Zhang et al., 2019]. Zeng et al. [2023] present a related approach, in\nwhich a neural network watermarked encoder and its associate detector are jointly learned using an\nimage dataset. Notably these approaches still work like a traditional watermark in that the encoder\nimprints a post-hoc signal onto a given image - however the type of imprint is now learned. We\nrefer to Wan et al. [2022] for an overview. A recent improvement is two-stage processes like Yu\net al. [2022], where the trained encoder is used to imprint the watermark onto the training data\nfor a generative model. This leads to a trained generative model where the watermark encoder is\n3\n\u201cbaked in\u201d to the model, making it easier to generate watermarked data. The Stable Signature of\nFernandez et al. [2023], applies this idea to latent diffusion models by finetuning the latent decoder\nbased on a pre-trained watermark encoder. Zhao et al. [2023] similarly train on watermarked data for\nunconditional diffusion models.\nExisting image watermarking approaches first learn a watermark signal and then learn to either embed\nit into generated data or the generating model. This pipeline stands in contrast to watermarking\napproaches for language models such as Kirchenbauer et al. [2023]. There, no training is necessary to\ngenerate watermarked data and the output distribution of the generative model is altered to encode a\nwatermark into generated data in a distributional sense. In the same vein, we propose an approach to\nalter the output distribution of diffusion models to effectively watermark their outputs. As discussed,\nthis has a number of advantages, in comparison to related work we especially highlight that no training\nis necessary, that the watermark works with existing models, and that this is the first watermark that\ndoes not rely on minor modification of generated images. In this sense, this is the first watermark that\nis really \u201cinvisible\u201d, see Figure 2.\nWe note in passing that watermarking the output of generative models is not to be confused with\nthe task of watermarking the weights of whole models, such as in Uchida et al. [2017], Zhang\net al. [2018], Bansal et al. [2022], who are concerned with identifying and fingerprinting models for\nintellectual property reasons.\n2.1\nDiffusion Models and Diffusion Inversion\nWe first introduce the basic notation for diffusion models and DDIM sampling [Ho et al., 2020, Song\nand Ermon, 2020, Dhariwal and Nichol, 2021]. A forward diffusion process consists of T steps of\nthe noise process a predefined amount of Gaussian noise vector to a real data point x0 \u2208 q(x), where\nq(x) is the real data distribution, specifically:\nq(xt|xt\u22121) = N(xt;\np\n1 \u2212 \u03b2txt, \u03b2tI), for t \u2208 {0, 1, ..., T \u2212 1},\nwhere \u03b2t \u2208 (0, 1) is the scheduled variance at step t. The closed-form for this sampling is\nxt = \u221a\u00af\u03b1tx0 +\n\u221a\n1 \u2212 \u00af\u03b1t\u03f5,\n(1)\nwhere, \u00af\u03b1t = Qt\ni=0(1 \u2212 \u03b2t).\nFor the reverse diffusion process, DDIM [Song and Ermon, 2020] is an efficient deterministic\nsampling strategy, mapping from a Gaussian vector xT \u223c N(0, 1) to an image x0 \u2208 q(x). For each\ndenoising step, a learned noise-predictor \u03f5\u03b8 estimates the noise \u03f5\u03b8(xt) added to x0. According to\nEquation (1), we can derive the estimation of x0 as:\n\u02c6xt\n0 = xt \u2212 \u221a1 \u2212 \u00af\u03b1t\u03f5\u03b8(xt)\n\u221a\u00af\u03b1t\n.\nThen, we add the estimated noise to \u02c6x0 to find xt\u22121:\nxt\u22121 = \u221a\u00af\u03b1t\u22121\u02c6xt\n0 +\np\n1 \u2212 \u00af\u03b1t\u22121\u03f5\u03b8(xt).\nWe denote such a recursively denoising process from xT to x0 as x0 = D\u03b8(xT ).\nHowever, given the learned model \u03f5\u03b8(xt), it is also possible to move in the opposite direction1.\nStarting from an image x0, Dhariwal and Nichol [2021] describes an inverse process that retrieves\nan initial noise vector xT which maps to an image \u02c6x0 close to x0 through DDIM, where \u02c6x0 =\nD\u03b8(xT , 0) \u2248 x0. This inverse process depends on the assumption that xt\u22121 \u2212 xt \u2248 xt+1 \u2212 xt.\nTherefore, from xt \u2192 xt+1, we follow:\nxt+1 = \u221a\u00af\u03b1t+1\u02c6xt\n0 +\np\n1 \u2212 \u00af\u03b1t+1\u03f5\u03b8(xt).\nWe denote the whole inversion process from a starting real image x0 to xT as xT = D\u2020\n\u03b8(x0).\nIn this work, we re-purpose DDIM inversion D\u2020\n\u03b8 for watermark detection. Given a generated image\nx0 with a starting noise xT , we apply DDIM inversion to find \u02c6xT . We empirically find DDIM\u2019s\n1To reduce confusion we will always describe the generative diffusion process that goes from xT to x0 as\nthe \u201creverse process\u201d. We use \u201cinverse process\u201d to denote the estimation of the noise vector xT from the final\noutput x0.\n4\ninversion performance to be quite strong, and \u02c6xT \u2248 xT . While it may not be surprising that inversion\nis accurate for unconditional diffusion models, inversion also succeeds well-enough for conditional\ndiffusion models, even when the conditioning c is not provided. This property of inversion will be\nexploited heavily by our watermark below.\n3\nMethod\nIn this section, we provide a detailed description of each layer of Tree-Ring Watermarking.\n3.1\nThreat Model\nWe first briefly describe the threat model considered in this work and clarify the setting: The goal\nof watermarking is to allow for image generation without quality degradation while enabling the\nmodel owner the ability to identify if a given image is generated from their model. Meanwhile, the\nwatermarked image is used in every-day applications and subject to a number of image manipulations\nand modifications. We formalize this as an adversary who tries to remove the watermark in the\ngenerated image to evade detection using common image manipulations, but note that informally, we\nare also interested in watermark robustness across common usage. Ultimately, this setup leads to a\nthreat model with two agents that act sequentially.\n\u2022 Model Owner (Generation Phase): Gene owns a generative diffusion model \u03f5\u03b8 and allows\nimages x to be generated through an API containing the private watermarking algorithm T .\nThe watermarking algorithm T should have a negligible effect on the generated distribution,\nso that quality is maintained and watermarking leaves no visible trace.\n\u2022 Forger: Fiona generates an image x through the API, then tries to evade the detection of\nT by applying strong data augmentations that convert x to x\u2032. Later, Fiona uses x\u2032 for a\nprohibited purpose and claims that x\u2032 is her intellectual property.\n\u2022 Model Owner (Detection Phase): Given access to \u03f5\u03b8 and T , Gene tries to determine if x\u2032\noriginated from \u03f5\u03b8. Gene has no knowledge of the text used to condition the model, or other\nhyperparameters like guidance strength and the number of generation steps.\n3.2\nOverview of Tree-Ring Watermarking\nDiffusion models convert an array of Gaussian noise into a clean image. Tree-Ring Watermarking\nchooses the initial noise array so that its Fourier transform contains a carefully constructed pattern\nnear its center. This pattern is called the \u201ckey.\u201d This initial noise vector is then converted into an\nimage using the standard diffusion pipeline with no modifications. To detect the watermark in an\nimage, the diffusion model is inverted using the process described in Section 2.1 to retrieve the\noriginal noise array used for generation. This array is then checked to see whether the key is present.\nRather than imprint the key into the Gaussian array directly, which might cause noticeable patterns in\nthe resulting image, we imprint the key into the Fourier transform of the starting noise vector. We\nchoose a binary mask M, and sample the key k\u2217 \u2208 C|M|. As such, the initial noise vector xT \u2208 RL\ncan be described in Fourier space as\nF(xT )i \u223c\n\u001ak\u2217\ni\nif\ni \u2208 M\nN(0, 1)\notherwise.\n(2)\nFor reasons described below, we choose M as a circular mask with radius r centered on the low-\nfrequency modes.\nAt detection time, given an image x\u2032\n0, the model owner can obtain an approximated initial noise\nvector x\u2032\nT through the DDIM inversion process: x\u2032\nT = D\u2020\n\u03b8(x\u2032\n0). The final metric is calculated as the\nL1 distance between the inverted noise vector and the key in the Fourier space of the watermarked\narea M, i.e.\nddetection distance =\n1\n|M|\nX\ni\u2208M\n|k\u2217\ni \u2212 F(x\u2032\nT )i|,\n(3)\n5\nNo Watermark\nWatermarked\nAttacked\n\u201cAnime art of a dog in Shenandoah National Park\u201d\nP-value = 0.27\n3.73e-60\n7.41e-16\n\u201cSynthwave style artwork of a person is kayaking in Acadia National Park\u201d\n0.15\n1.38e-19\n1.51e-8\n\u201cAn astronaut riding a horse in Zion National Park\u201d\n0.91\n9.91e-51\n2.90e-05\n\u201cA painting of Yosemite National Park in Van Gogh style\u201d\n0.41\n1.22e-35\n9.46e-07\nFigure 3: The qualitative results show three types of images: non-watermarked, Tree-RingRings watermarked, and\nattacked watermarked images. A P-value is provided below each image, which corresponds to the probability of\nthe detected watermark structure occurring by random chance. From top to bottom, the watermarked images are\nattacked by color jitter with a brightness factor of 6, Gaussian blur with an 8 \u00d7 8 filter size, Gaussian noise with\n\u03c3 = 0.1, and a 180\u25e6 rotation, respectively.\n6\nand the watermark is detected if this falls below a tuned threshold \u03c4. We later discuss how to calibrate\nthis threshold to a given false-positive either based on a given set of pairs of watermarked and\nunwatermarked images, or to be set to guarantee a fixed P-value in Section 3.4.\nThe process described above is straightforward. However, its success depends strongly on the\nconstruction of the \u201ckey\u201d pattern, which we discuss below.\n3.3\nConstructing a Tree-Ring Key\nWe watermark images by placing a \u201ckey\u201d pattern into the Fourier space of the original Gaussian\nnoise array. Our patterns can exploit several classical properties of the Fourier transform for periodic\nsignals that we informally state here.\n\u2022 A rotation in pixel space corresponds to a rotation in Fourier space.\n\u2022 A translation in pixel space multiplies all Fourier coefficients by a constant complex number.\n\u2022 A dilation/compression in pixel space corresponds to a compression/dilation in Fourier space.\n\u2022 Color jitter in pixel space (adding a constant to all pixels in a channel) corresponds to changing\nthe magnitude of the zero-frequency Fourier mode.\nA number of classical watermarking strategies rely on watermarking in Fourier space and exploit\nsimilar invariances [Pitas, 1998, Solachidis and Pitas, 2001]. Our watermark departs from classical\nmethods by applying a Fourier watermark to a random noise array before diffusion takes place.\nCuriously, we will observe below that the invariant properties above are preserved in xT even when\nimage manipulations are done in pixel space of x0.\nIn addition to exploiting the invariances above, the chosen key should also be statistically similar\nto Gaussian noise. Note that the Fourier transform of a Gaussian noise array is also distributed as\nGaussian noise. For this reason, choosing a highly non-Gaussian key may cause a distribution shift\nthat impacts the diffusion model.\nWe consider three different types of keys, with the respective benefits of each pattern being demon-\nstrated in subsequent experimental sections. We believe there are numerous other interesting and\npractical types that can be explored in future work.\nTree-RingZeros: We choose the mask to be a circular region to preserve invariance to rotations in\nimage space. The key is chosen to be an array of zeros, which creates invariance to shifts, crops,\nand dilations. This key is invariant to manipulations, but at the cost of departing severely from the\nGaussian distribution. It also prevents multiple keys from being used to distinguish between models.\nTree-RingRand: We draw the a fixed key k\u2217 from a Gaussian distribution. The key has the same iid\nGaussian nature as the original Fourier modes of the noise array, and so we anticipate this strategy\nwill have the least impact on generation quality. This method also offers the flexibility for the model\nowner to possess multiple keys. However, it is not invariant to make image manipulations.\nTree-RingRings: We introduce a pattern comprised of multiple rings, and constant value along each\nring. This makes the watermark invariant to rotations. We choose the constant ring values from a\nGaussian distribution. This provides some invariance to multiple types of image transforms, while\nalso ensuring that the overall distribution is only minimally shifted from an isotropic Gaussian.\n3.4\nDeriving P-values for Watermark Detection\nA key desideratum for a reliable watermark detector is that it provide an interpretable P-value that\ncommunicates to the user how likely it is that the observed watermark could have occurred in a\nnatural image by random chance. In addition to making detection results interpretable, P-values\ncan be used to set the threshold of detection, i.e., the watermark is \u201cdetected\u201d when p is below a\nchosen threshold \u03b1. By doing so, one can explicitly control the false positive rate \u03b1, making false\naccusations statistically unlikely.\nTo this end, we construct a statistical test for the presence of the watermark that produces a rigorous\nP-value. The forward diffusion process is designed to map images onto Gaussian noise, and so\nwe assume a null hypothesis in which the entries in the array x\u2032\nT obtained for a natural image are\nGaussian. We find that this assumption holds quite well in practice, see Figure 4.\n7\n4\n2\n0\n2\n4\n0.0\n0.1\n0.2\n0.3\n0.4\nDensity\nGaussian\nx'_T\n(\u00b5=-0.0051, =0.9956)\nFigure 4: Histogram of the array x\u2032\nT obtained for a natural image, which is Gaussian.\nFor any test image x\u2032\n0, we compute the approximate initial vector x\u2032\nT and then set y = F(x\u2032\nT ). We\nthen define the following null hypothesis\nH0 : y is drawn from a Gaussian distribution N(0, \u03c32IC).\n(4)\nHere, \u03c32 is an unknown variance, which we estimate for each image2 using the formula \u03c32 =\n1\nM\nP\ni\u2208M |yi|2. To test this hypothesis, we define the score\n\u03b7 = 1\n\u03c32\nX\ni\u2208M\n|k\u2217\ni \u2212 y|2.\n(5)\nOur formula for \u03b7 is closely related to Equation (3), but we switch to a sum-of-squares metric and\nremove the variance from y to simplify statistical analysis. When H0 is true, the distribution of \u03b7 is\nexactly a noncentral \u03c72 distribution [Patnaik, 1949], with |M| degrees of freedom and non-centrality\nparameter \u03bb =\n1\n\u03c32\nP\ni |k\u2217\ni |2.\nWe declare an image to be watermarked if the value of \u03b7 is too small to occur by random chance. The\nprobability of observing a value as small as \u03b7 is given by the cumulative distribution function \u03a6\u03c72 of\nthe noncentral \u03c72 distribution:\np = Pr\n\u0010\n\u03c72\n|M|,\u03bb \u2264 \u03b7\n\f\f\fH0\n\u0011\n= \u03a6\u03c72(z).\n(6)\n\u03a6\u03c72 is a standard statistical function [Glasserman, 2003], available in scipy and many other statistics\nlibraries.\nWe show qualitative examples of the proposed watermarking scheme and accompanying P-values\nin Figure 3. For each prompt, we show the generated image with and without the watermark, and\nalso a watermarked image subjected to a transformation. For each image, we report a P-value. As\nexpected, these values are large for non-watermarked images, and small (enabling rejection of the\nnull hypothesis) when the watermark is present. Transformations reduce the watermark strength as\nreflected in the increased P-value.\n4\nExperiments\nWe perform experiments on two common diffusion models to measure the efficacy and reliability of\nthe Tree-Ring Watermarking technique across diverse attack scenarios. Furthermore, we carry out\nablation studies to provide an in-depth exploration of this technique.\n4.1\nExperimental Setting\nWe employ Stable Diffusion-v2 [Rombach et al., 2022], an open-source, state-of-the-art latent text-to-\nimage diffusion model, along with a 256 \u00d7 256 ImageNet diffusion model3 [Dhariwal and Nichol,\n2Our statistical test is only sensitive to Tree-RingRand and Tree-RingRings. Tree-RingZeros results in the\npathological case that \u03c3 \u2248 0 for watermarked images resulting in overly conservative/large P-values.\n3https://github.com/openai/guided-diffusion\n8\nTable 1: Main Results. T@1%F represents TPR@1%FPR. We evaluate watermark accuracy in both benign and\nadversarial settings. Adversarial here refers to average performance over a battery of image manipulations. An\nextended version with additional details and standard error estimates can be found in Supplementary Material.\nModel\nMethod\nAUC/T@1%F\n(Clean)\nAUC/T@1%F\n(Adversarial)\nFID \u2193\nCLIP Score \u2191\nStable Diff.\nFID = 25.29\nCLIP Score\n= 0.363\nDwtDct\n0.974 / 0.624\n0.574 / 0.092\n25.10.09\n0.362.000\nDwtDctSvd\n1.000 / 1.000\n0.702 / 0.262\n25.01.09\n0.359.000\nRivaGAN\n0.999 / 0.999\n0.854 / 0.448\n24.51.17\n0.361.000\nTree-RingZeros\n0.999 / 0.999\n0.963 / 0.715\n26.56.07\n0.356.000\nTree-RingRand\n1.000 / 1.000\n0.918 / 0.702\n25.47.05\n0.363.001\nTree-RingRings\n1.000 / 1.000\n0.975 / 0.694\n25.93.13\n0.364.000\nImageNet\nFID = 17.73\nDwtDct\n0.899 / 0.244\n0.536 / 0.037\n17.77.01\n-\nDwtDctSvd\n1.000 / 1.000\n0.713 / 0.187\n18.55.02\n-\nRivaGAN\n1.000 / 1.000\n0.882 / 0.509\n18.70.02\n-\nTree-RingZeros\n0.999 / 1.000\n0.921 / 0.476\n18.78.00\n-\nTree-RingRand\n0.999 / 1.000\n0.940 / 0.585\n18.68.09\n-\nTree-RingRings\n0.999 / 0.999\n0.966 / 0.603\n17.68.16\n-\n2021]. In the main experiment, we use 50 inference steps for generation and detection for both models.\nFor Stable Diffusion, we use the default guidance scale of 7.5, and we use an empty prompt for DDIM\ninversion, emulating that the image prompt would be unknown at detection time. The watermark\nradius r we use is 10. Later, we conduct more ablation studies on these important hyperparameters.\nAll experiments are conducted on a single NVIDIA RTX A4000.\nOur comparative analysis includes three baselines: two training-free methods, DwtDct and DwtD-\nctSvd [Cox et al., 2007], and a pre-trained GAN-based watermarking model, RivaGAN [Zhang et al.,\n2019, Goodfellow et al., 2014]. However, these baseline methods are designed for steganography,\nwhich conceals a target bit-string within an image. To ensure a fair comparison with our exclusively\nwatermarking method, we employ the distance between the decoded bit-string and the target bit-string\n(Bit Accuracy) as the measurement metric. The approach of Cox et al. [2007] is currently deployed\nas a watermark mechanism in Stable Diffusion4.\n4.2\nBenchmarking Watermark Accuracy and Image Quality\nTo benchmark the effectiveness of the watermark, we primarily report the area under the curve (AUC)\nof the receiver operating characteristic (ROC) curve, and the True Positive Rate when the False\nPositive Rate is at 1%, denoted as TPR@1%FPR. To demonstrate the generation quality of the\nwatermarked images, we assess the Frechet Inception Distance (FID) [Heusel et al., 2017] for both\nmodels. Additionally, for the Stable Diffusion model, we also evaluate the CLIP score [Radford\net al., 2021] between the generated image and the prompt, as measured by OpenCLIP-ViT/G [Cherti\net al., 2022]. For AUC and TPR@1%FPR, we create 1, 000 watermarked and 1, 000 unwatermarked\nimages for each run. For FID, we generate 5, 000 images for Stable Diffusion and 10, 000 images\nfor the ImageNet Model. The FID of Stable Diffusion is evaluated on the MS-COCO-2017 training\ndataset [Lin et al., 2014], and the FID of the ImageNet Model is gauged on the ImageNet-1k training\ndataset [Deng et al., 2009]. All reported metrics are averaged across 5 runs using different random\nseeds following this protocol.\nIn Table 1, we present the main experimental results for Stable Diffusion and the ImageNet model. In\nthe clean setting, all baselines except DwtDct and all Tree-Ring Watermarking variants are strongly\ndetectable. Tree-RingRand and Tree-RingRings show negligible impact on the FID and no impact on the\nCLIP score.\n4github.com/CompVis/stable-diffusion/blob/main/scripts/txt2img.py#L69\n9\nTable 2: AUC under each Attack for Stable Diffusion, showing the effectiveness of Tree-RingRings over a number\nof augmentations. Cr. & Sc. refers to random cropping and rescaling. Additional results for the ImageNet model\ncan be found in Supplementary Material.\nMethod\nClean\nRotation\nJPEG\nCr. & Sc.\nBlurring\nNoise\nColor Jitter\nAvg\nDwtDct\n0.974\n0.596\n0.492\n0.640\n0.503\n0.293\n0.519\n0.574\nDwtDctSvd\n1.000\n0.431\n0.753\n0.511\n0.979\n0.706\n0.517\n0.702\nRivaGan\n0.999\n0.173\n0.981\n0.999\n0.974\n0.888\n0.963\n0.854\nT-RZeros\n0.999\n0.994\n0.984\n0.999\n0.977\n0.877\n0.907\n0.963\nT-RRand\n1.000\n0.486\n0.999\n0.971\n0.999\n0.972\n0.994\n0.918\nT-RRings\n1.000\n0.935\n0.999\n0.961\n0.999\n0.944\n0.983\n0.975\n4.3\nBenchmarking Watermark Robustness\nTo benchmark the robustness of our watermark, we focus on documenting its performance under 6\nprevalent data augmentations utilized as attacks. These include 75\u25e6 rotation, 25% JPEG compression,\n75% random cropping and scaling, Gaussian blur with an 8 \u00d7 8 filter size, Gaussian noise with\n\u03c3 = 0.1, and color jitter with a brightness factor uniformly sampled between 0 and 6. Additionally,\nwe conduct ablation studies to investigate the impact of varying intensities of these attacks. We report\nboth AUC and TPR@1%FPR in the average case where we average the metrics over the clean setting\nand all attacks. In all ablation studies, we report the average case.\nIn Table 1, the baseline methods fail in the presence of adversaries. On the contrary, our methods\ndemonstrate higher reliability in adversarial settings. Among them, Tree-RingRings performs the best\nunder adversarial conditions, a result of our careful watermark pattern design.\nFurther, we show the AUC for each attack setting in Table 2. Notably, Tree-RingZeros demonstrates\nhigh robustness against most perturbations, except for Gaussian noise and color jitter. Similarly,\nTree-RingRand is robust in most scenarios but performs poorly when faced with rotation, as expected.\nOverall, Tree-RingRings delivers the best average performance while offering the model owner the\nflexibility of multiple different random keys. It is worth noting that the baseline method RivaGan\nalso demonstrates strong robustness in most scenarios, but it is important to highlight that our method\nis training-free and really \u201cinvisible\u201d.\n4.4\nAblation Experiments\nIn this section, we undertake exhaustive ablation studies with the Ring pattern on several key\nhyperparameters to demonstrate the efficacy of Tree-Ring Watermarking. Except for the ablation on\nattacks, the reported numbers represent averages over all attack scenarios and clean images.\n0\n2\n10\n25\n50\n100\n200\n400\n800\nDetection-Time #Steps\n800\n400\n200\n100\n50\n25\n10\nGeneration-Time #Steps\n0.808 0.923 0.966 0.972 0.971 0.970 0.969 0.968 0.967\n0.801 0.919 0.962 0.968 0.968 0.967 0.966 0.965 0.964\n0.799 0.919 0.962 0.968 0.968 0.967 0.965 0.965 0.964\n0.800 0.918 0.961 0.969 0.969 0.967 0.965 0.964 0.963\n0.806 0.921 0.961 0.966 0.966 0.964 0.963 0.963 0.963\n0.810 0.925 0.961 0.965 0.964 0.962 0.960 0.962 0.962\n0.785 0.909 0.948 0.952 0.951 0.948 0.945 0.948 0.947\n0.800\n0.825\n0.850\n0.875\n0.900\n0.925\n0.950\nAUC\nFigure 5: Ablation on Number of Generation Steps versus Detec-\ntion Steps. Detection succeeds independent of the number of DDIM\nused to generate data.\nIn Figure 5,\nwe compare AUC\nacross all step combinations. Surpris-\ningly, even with a significant differ-\nence between the generation-time and\ndetection-time #steps, the decrease\nin AUC is minimal when the model\nowner uses a reasonable number of\ninference steps for detection without\nknowledge of the true generation-time\nsteps. This indicates that the DDIM\ninversion maintains its robustness in\napproximating the initial noise vec-\ntor, and is effective for watermark de-\ntection irrespective of the exact num-\nber of steps employed. Interestingly,\nwe notice a trend where the detection\npower appears to be slightly stronger\nwith fewer inference steps at detection\ntime or a larger number of inference steps at generation time. This is an advantageous scenario as the\n10\n1\n2\n4\n8\n16\n32\nRadius\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAUC/TPR@1%FPR\nAUC\nTPR@1%FPR\n25.35\n26.07\n26.79\n27.52\n28.24\n28.96\nFID\nAUC\nTPR@1%FPR\nFID\nW/o Watermark FID\n(a) Ablation on Watermark Radii\n2\n4\n6\n8\n10\n12\n14\n16\n18\nGuidance Scale\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAUC/TPR@1%FPR\nAUC\nTPR@1%FPR\n(b) Ablation on Guidance Scales\nFigure 6: Ablation on Watermark Radii and Guidance Scales.\nmodel owner now does not actually need to carry out a large number of steps for DDIM inversion,\nwhile concurrently, the model owner (or the user) is free to choose the number of generation steps\nthat achieve the best quality [Rombach et al., 2022].\nNumber of Steps Used for Generation and Detection. A key unknown variable for the model\nowner at the detection time is the actual number of inference steps used during the generation time.\nThis factor could potentially impact the precision of the DDIM inversion approximation of the initial\nnoise vector. To scrutinize this, we systematically vary the number of steps for both the generation\nand detection time. Due to the computational demands of sampling with a high number of inference\nsteps, we employ a total of 400 images for each run.\nWatermark radii. The radius of injected watermarking patterns is another critical hyperparameter\naffecting robustness and generation quality. The corresponding results are shown in Figure 6(a). As\nthe watermarking radius increases, the watermark\u2019s robustness improves. Nevertheless, there is a\ntrade-off with generation quality. We overall confirm a radius of 16 to provide reasonably low FID\nwhile maintaining strong detection power.\nGuidance scales. Guidance scale is a hyperparameter that controls the significance of the text\ncondition. Higher guidance scales mean the generation more strictly adheres to the text guidance,\nwhereas lower guidance scales provide the model with greater creative freedom. Optimal guidance\nscales typically range between 5 and 15 for the Stable Diffusion model we employ. We explore this\nfactor from 2 to 18 in Figure 6(b) and highlight that the strength of the guidance is always unknown\nduring detection time. Although a higher guidance scale does increase the error for DDIM inversion\ndue to the lack of this ground-truth guidance during detection, the watermark remains robust and\nreliable even at a guidance scale of 18. This is again beneficial for practical purposes, allowing the\nmodel owner to keep guidance scale a tunable setting for their users.\nAttack strengths. Further, we test out the robustness of Tree-Ring Watermarking under each attack\nwith various attack strengths. As shown in Figure 7, even with extreme perturbations like Gaussian\nblurring with kernel size 40, Tree-Ring Watermarking can still be reliably detected.\n5\nLimitations and Future Work\nTree-Ring Watermarking requires the model owner to use DDIM during inference. Today, DDIM is\nstill likely the most popular sampling method due to its economical use of GPU resources and high\nquality. However, the proposed watermark will need to be adapted to other sampling schemes should\nDDIM fall out of favor. Further, the proposed watermark is by design only verifiable by the model\nowner because model parameters are needed to perform the inversion process. This has advantages\nagainst adversaries, who cannot perform a white-box attack on the watermark or even verify whether\nan ensemble of manipulations broke the watermark. However, it also restricts third parties from\ndetecting the watermark without relying on an API. Finally, it is currently not yet clear how large the\ncapacity for multiple keys k\u2217 would be. Would it be possible to assign a unique key to every user of\nthe API?\nThe effectiveness of the proposed watermark is directly related to the accuracy of the inverse DDIM\nprocess. Future work that improves the accuracy of this inversion [Zhang et al., 2023], or utilizes\n11\n0\n45\n90 135 180 225 270 315\nRotation Degree\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAUC/TPR@1%FPR\nAUC\nTPR@1%FPR\n(a) Rotation\n80 70 60 50 40 30 20 10\n90\nJPEG Compression Quality\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(b) JPEG Compression\n80 70 60 50 40 30 20 10\n90\n%Cropping+Scaling\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(c) Cropping + Scaling\n5\n10\n15\n20\n25\n30\n35\n40\nKernel Size\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAUC/TPR@1%FPR\n(d) Gaussian Blurring\n0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4\nNoise Std\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(e) Gaussian Noise\n1\n2\n4\n8\n16\nBrightness Factor\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(f) Color Jitter\nFigure 7: Ablation on Different Perturbation Strengths.\ninvertible diffusion models as described in Wallace et al. [2022], would also improve watermarking\npower further.\n6\nConclusion\nWe propose a new approach to watermarking generative diffusion models using minimal shifts of\ntheir output distribution. This leads to watermarks that are truly invisible on a per-sample basis. We\ndescribe how to optimally shift, so that the watermark remains detectable even under strong image\nmanipulations that might be encountered in daily usage and handling of generated images.\n7\nAcknowledgements\nThis work was made possible by the ONR MURI program, DARPA GARD (HR00112020007), the\nOffice of Naval Research (N000142112557), and the AFOSR MURI program. Commercial support\nwas provided by Capital One Bank, the Amazon Research Award program, and Open Philanthropy.\nFurther support was provided by the National Science Foundation (IIS-2212182), and by the NSF\nTRAILS Institute (2229885).\nReferences\nAli Al-Haj. Combined DWT-DCT Digital Image Watermarking. Journal of Computer Science, 3\n(9):740\u2013746, September 2007. ISSN 15493636. doi: 10.3844/jcssp.2007.740.746. URL http:\n//www.thescipub.com/abstract/?doi=jcssp.2007.740.746.\nArpit Bansal, Ping-Yeh Chiang, Michael J. Curry, Rajiv Jain, Curtis Wigington, Varun Manjunatha,\nJohn P. Dickerson, and Tom Goldstein. Certified Neural Network Watermarks with Randomized\nSmoothing. In Proceedings of the 39th International Conference on Machine Learning, pages 1450\u2013\n1465. PMLR, June 2022. URL https://proceedings.mlr.press/v162/bansal22a.html.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the\nDangers of Stochastic Parrots: Can Language Models Be Too Big? In Proceedings of the 2021\nACM Conference on Fairness, Accountability, and Transparency, FAccT \u201921, pages 610\u2013623, New\nYork, NY, USA, March 2021. Association for Computing Machinery. ISBN 978-1-4503-8309-7.\ndoi: 10.1145/3442188.3445922. URL https://doi.org/10.1145/3442188.3445922.\n12\nFrancis Morgan Boland.\nWatermarking digital images for copyright protection.\n1996.\nURL\nhttp://www.tara.tcd.ie/handle/2262/19682.\nChin-Chen Chang, Piyu Tsai, and Chia-Chen Lin.\nSVD-based digital image watermarking\nscheme. Pattern Recognition Letters, 26(10):1577\u20131586, July 2005. ISSN 0167-8655. doi:\n10.1016/j.patrec.2005.01.004. URL https://www.sciencedirect.com/science/article/\npii/S0167865505000140.\nMehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade\nGordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for\ncontrastive language-image learning. ArXiv, abs/2212.07143, 2022.\nI.J. Cox, J. Kilian, T. Leighton, and T. Shamoon. Secure spread spectrum watermarking for images,\naudio and video. Proceedings of 3rd IEEE International Conference on Image Processing, 3:243\u2013\n246, 1996. doi: 10.1109/ICIP.1996.560429. URL http://ieeexplore.ieee.org/document/\n560429/.\nIngemar Cox, Matthew Miller, Jeffrey Bloom, Jessica Fridrich, and Ton Kalker. Digital Watermarking\nand Steganography. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 2 edition, 2007.\nISBN 9780080555805.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hier-\narchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition,\npages 248\u2013255, 2009. doi: 10.1109/CVPR.2009.5206848.\nPrafulla Dhariwal and Alex Nichol.\nDiffusion Models Beat GANs on Image Synthesis.\narxiv:2105.05233[cs, stat], June 2021. doi: 10.48550/arXiv.2105.05233. URL http://arxiv.\norg/abs/2105.05233.\nJianwei Fei, Zhihua Xia, Benedetta Tondi, and Mauro Barni. Supervised GAN Watermarking for\nIntellectual Property Protection. arxiv:2209.03466[cs], September 2022. doi: 10.48550/arXiv.\n2209.03466. URL http://arxiv.org/abs/2209.03466.\nPierre Fernandez, Guillaume Couairon, Herv\u00e9 J\u00e9gou, Matthijs Douze, and Teddy Furon. The Stable\nSignature: Rooting Watermarks in Latent Diffusion Models. arxiv:2303.15435[cs], March 2023.\ndoi: 10.48550/arXiv.2303.15435. URL http://arxiv.org/abs/2303.15435.\nPaul Glasserman. Monte Carlo Methods in Financial Engineering, volume 53 of Stochastic Modelling\nand Applied Probability. Springer, New York, NY, 2003. ISBN 978-1-4419-1822-2 978-0-387-\n21617-1. doi: 10.1007/978-0-387-21617-1. URL http://link.springer.com/10.1007/\n978-0-387-21617-1.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling,\nC. Cortes, N. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing\nSystems, volume 27. Curran Associates, Inc., 2014. URL https://proceedings.neurips.cc/\npaper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf.\nAlexei Grinbaum and Laurynas Adomaitis. The Ethical Need for Watermarks in Machine-Generated\nLanguage. arxiv:2209.03118[cs], September 2022. doi: 10.48550/arXiv.2209.03118. URL\nhttp://arxiv.org/abs/2209.03118.\nJamie Hayes and George Danezis.\nGenerating steganographic images via adversarial train-\ning.\nIn Advances in Neural Information Processing Systems, volume 30. Curran As-\nsociates, Inc., 2017.\nURL https://papers.nips.cc/paper_files/paper/2017/hash/\nfe2d010308a6b3799a3d9c728ee74244-Abstract.html.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium. In NIPS, 2017.\nJonathan Ho, Ajay Jain, and Pieter Abbeel.\nDenoising Diffusion Probabilistic Models.\nIn\nAdvances in Neural Information Processing Systems, volume 33, pages 6840\u20136851. Cur-\nran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\n4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html.\n13\nJohn Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. A\nWatermark for Large Language Models. arxiv:2301.10226[cs], January 2023. doi: 10.48550/\narXiv.2301.10226. URL http://arxiv.org/abs/2301.10226.\nMartin Kutter and Fabien A. P. Petitcolas.\nFair benchmark for image watermark-\ning\nsystems.\nIn\nSecurity\nand\nWatermarking\nof\nMultimedia\nContents,\nvolume\n3657, pages 226\u2013239. SPIE, April 1999.\ndoi:\n10.1117/12.344672.\nURL https:\n//www.spiedigitallibrary.org/conference-proceedings-of-spie/3657/0000/\nFair-benchmark-for-image-watermarking-systems/10.1117/12.344672.full.\nG.C. Langelaar, I. Setyawan, and R.L. Lagendijk. Watermarking digital image and video data. A\nstate-of-the-art overview. IEEE Signal Processing Magazine, 17(5):20\u201346, September 2000. ISSN\n1558-0792. doi: 10.1109/79.879337.\nTsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll\u00e1r, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In European\nConference on Computer Vision, 2014.\nJunxiu Liu, Jiadong Huang, Yuling Luo, Lvchen Cao, Su Yang, Duqu Wei, and Ronglong Zhou. An\nOptimized Image Watermarking Method Based on HD and SVD in DWT Domain. IEEE Access,\n7:80849\u201380860, 2019. ISSN 2169-3536. doi: 10.1109/ACCESS.2019.2915596.\nAlex Nichol and Prafulla Dhariwal.\nImproved Denoising Diffusion Probabilistic Models.\narxiv:2102.09672[cs, stat], February 2021.\ndoi: 10.48550/arXiv.2102.09672.\nURL http:\n//arxiv.org/abs/2102.09672.\nJ.J.K. O\u2019Ruanaidh and T. Pun. Rotation, scale and translation invariant digital image watermarking.\nIn Proceedings of International Conference on Image Processing, volume 1, pages 536\u2013539 vol.1,\nOctober 1997. doi: 10.1109/ICIP.1997.647968.\nP. B. Patnaik. The Non-Central X2- and F-Distribution and their Applications. Biometrika, 36(1/2):\n202\u2013232, 1949. ISSN 0006-3444. doi: 10.2307/2332542. URL https://www.jstor.org/\nstable/2332542.\nI. Pitas. A method for watermark casting on digital image. IEEE Transactions on Circuits and Systems\nfor Video Technology, 8(6):775\u2013780, October 1998. ISSN 1558-2205. doi: 10.1109/76.728421.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.\nLearning transferable visual models from natural language supervision. In International Conference\non Machine Learning, 2021.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-\nResolution Image Synthesis With Latent Diffusion Models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022. URL https:\n//openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_\nImage_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html.\nJin S Seo, Jaap Haitsma, Ton Kalker, and Chang D Yoo. A robust image fingerprinting system using\nthe Radon transform. Signal Processing: Image Communication, 19(4):325\u2013339, April 2004.\nISSN 0923-5965. doi: 10.1016/j.image.2003.12.001. URL https://www.sciencedirect.com/\nscience/article/pii/S0923596503001541.\nV. Solachidis and L. Pitas. Circularly symmetric watermark embedding in 2-D DFT domain. IEEE\nTransactions on Image Processing, 10(11):1741\u20131753, November 2001. ISSN 1941-0042. doi:\n10.1109/83.967401.\nYang Song and Stefano Ermon. Generative Modeling by Estimating Gradients of the Data Distribution.\narXiv:1907.05600 [cs, stat], October 2019. URL http://arxiv.org/abs/1907.05600.\nYang Song and Stefano Ermon. Improved Techniques for Training Score-Based Generative Models.\narXiv:2006.09011 [cs, stat], June 2020. URL http://arxiv.org/abs/2006.09011.\n14\nYusuke Uchida, Yuki Nagai, Shigeyuki Sakazawa, and Shin\u2019ichi Satoh. Embedding Watermarks\ninto Deep Neural Networks. In Proceedings of the 2017 ACM on International Conference on\nMultimedia Retrieval, pages 269\u2013277, Bucharest Romania, June 2017. ACM. ISBN 978-1-4503-\n4701-3. doi: 10.1145/3078971.3078974. URL https://dl.acm.org/doi/10.1145/3078971.\n3078974.\nBram Wallace, Akash Gokul, and Nikhil Naik. EDICT: Exact Diffusion Inversion via Coupled\nTransformations. arxiv:2211.12446[cs], December 2022. doi: 10.48550/arXiv.2211.12446. URL\nhttp://arxiv.org/abs/2211.12446.\nWenbo Wan, Jun Wang, Yunming Zhang, Jing Li, Hui Yu, and Jiande Sun. A comprehensive survey on\nrobust image watermarking. Neurocomputing, 488:226\u2013247, June 2022. ISSN 0925-2312. doi: 10.\n1016/j.neucom.2022.02.083. URL https://www.sciencedirect.com/science/article/\npii/S0925231222002533.\nNing Yu, Vladislav Skripniuk, Sahar Abdelnabi, and Mario Fritz. Artificial Fingerprinting for\nGenerative Models: Rooting Deepfake Attribution in Training Data. arxiv:2007.08457[cs], March\n2022. doi: 10.48550/arXiv.2007.08457. URL http://arxiv.org/abs/2007.08457.\nYu Zeng, Mo Zhou, Yuan Xue, and Vishal M Patel. Securing deep generative models with universal\nadversarial signature. arXiv preprint arXiv:2305.16310, 2023.\nJialong Zhang, Zhongshu Gu, Jiyong Jang, Hui Wu, Marc Stoecklin, Heqing Huang, and\nIan Molloy.\nProtecting intellectual property of deep neural networks with watermark-\ning.\nIn ACM Symposium on Information,\nComputer and Communications Security.\nAssociation for Computing Machinery,\nInc.,\nMay 2018.\nISBN 978-1-4503-5576-6.\ndoi:\n10.1145/3196494.3196550.\nURL https://research.ibm.com/publications/\nprotecting-intellectual-property-of-deep-neural-networks-with-watermarking.\nJiaxin Zhang, Kamalika Das, and Sricharan Kumar. On the Robustness of Diffusion Inversion in\nImage Manipulation. In ICLR 2023 Workshop on Trustworthy and Reliable Large-Scale Machine\nLearning Models, April 2023. URL https://openreview.net/forum?id=fr8kurMWJIP.\nKevin Alex Zhang, Lei Xu, Alfredo Cuesta-Infante, and Kalyan Veeramachaneni. Robust Invisible\nVideo Watermarking with Attention. arxiv:1909.01285[cs], September 2019. doi: 10.48550/arXiv.\n1909.01285. URL http://arxiv.org/abs/1909.01285.\nYunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Ngai-Man Cheung, and Min Lin. A Recipe for\nWatermarking Diffusion Models. arxiv:2303.10137[cs], March 2023. doi: 10.48550/arXiv.2303.\n10137. URL http://arxiv.org/abs/2303.10137.\nJiren Zhu, Russell Kaplan, Justin Johnson, and Li Fei-Fei. HiDDeN: Hiding Data with Deep Net-\nworks. In Proceedings of the European Conference on Computer Vision (ECCV), pages 657\u2013\n672, 2018.\nURL https://openaccess.thecvf.com/content_ECCV_2018/html/Jiren_\nZhu_HiDDeN_Hiding_Data_ECCV_2018_paper.html.\n15\nA\nAppendix\nTable 3: Main Results with Error Bars. T@1%F represents TPR@1%FPR. We evaluate watermark accuracy in\nboth benign and adversarial settings. Adversarial here refers to average performance over a battery of image\nmanipulations.\nModel\nMethod\nAUC/T@1%F\n(Clean)\nAUC/T@1%F\n(Adversarial)\nFID \u2193\nCLIP Score \u2191\nStable Diff.\nFID = 25.29\nCLIP Score\n= 0.363\nDwtDct\n0.974.001 / 0.624.013\n0.574.005 / 0.092.004\n25.10.09\n0.362.000\nDwtDctSvd\n1.000.000 / 1.000.000\n0.702.000 / 0.262.011\n25.01.09\n0.359.000\nRivaGAN\n0.999.000 / 0.999.000\n0.854.002 / 0.448.006\n24.51.17\n0.361.000\nT-RZeros\n0.999.000 / 0.999.000\n0.963.001 / 0.715.021\n26.56.07\n0.356.000\nT-RRand\n1.000.000 / 1.000.000\n0.918.005 / 0.702.017\n25.47.05\n0.363.001\nT-RRings\n1.000.000 / 1.000.000\n0.975.001 / 0.694.018\n25.93.13\n0.364.000\nImageNet\nFID = 17.73\nDwtDct\n0.899.040 / 0.244.203\n0.536.016 / 0.037.029\n17.77.01\n-\nDwtDctSvd\n1.000.000 / 1.000.000\n0.713.019 / 0.187.008\n18.55.02\n-\nRivaGAN\n1.000.000 / 1.000.000\n0.882.010 / 0.509.009\n18.70.02\n-\nT-RZeros\n0.999.000 / 1.000.000\n0.921.000 / 0.476.000\n18.78.00\n-\nT-RRand\n0.999.000 / 1.000.000\n0.940.004 / 0.585.006\n18.68.09\n-\nT-RRings\n0.999.000 / 0.999.000\n0.966.005 / 0.603.006\n17.68.16\n-\nTable 4: AUC under each Attack for the ImageNet model, showing the effectiveness of Tree-RingRings over a\nnumber of augmentations. Cr. & Sc. refers to random cropping and rescaling.\nMethod\nClean\nRotation\nJPEG\nCr. & Sc.\nBlurring\nNoise\nColor Jitter\nAvg\nDwtDct\n0.899\n0.478\n0.522\n0.433\n0.512\n0.365\n0.538\n0.536\nDwtDctSvd\n1.000\n0.669\n0.568\n0.614\n0.947\n0.656\n0.535\n0.713\nRivaGan\n1.000\n0.321\n0.978\n0.999\n0.988\n0.962\n0.924\n0.882\nT-RZeros\n0.999\n0.953\n0.806\n0.997\n0.999\n0.938\n0.775\n0.921\nT-RRand\n0.999\n0.682\n0.962\n0.997\n0.999\n0.986\n0.956\n0.940\nT-RRings\n0.999\n0.975\n0.940\n0.994\n0.999\n0.979\n0.861\n0.966\n16\nW/o Watermark\nTree-RingZeros\nTree-RingRand\nTree-RingRings\nFigure 8: More generated images with Tree-Ring Watermarking with the first 7 prompts in MS-COCO-2017\ntraining dataset.\n17\n1\n2\n3\n4\n5\n6\n#Attacks\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAUC/TPR@1%FPR\nAUC\nTPR@1%FPR\nFigure 9: Results on k number of random attacks applied at the same time.\n(a) 75\u25e6 Rotation\n(b) 25% JPEG Compression\n(c) 75% Random Cropping +\nScaling\n(d) Gaussian Blurring with an 8\u00d7\n8 Filter\n(e) Gaussian Noise with \u03c3 = 0.1 (f) Color Jitter with a Brightness\nFactor of 6\nFigure 10: Attacked images.\n18\n"
  },
  {
    "title": "Bigger, Better, Faster: Human-level Atari with human-level efficiency",
    "link": "https://arxiv.org/pdf/2305.19452.pdf",
    "upvote": "2",
    "text": "Bigger, Better, Faster: Human-level Atari with human-level efficiency\nMax Schwarzer * 1 2 3 Johan Obando-Ceron * 1 2 3 Aaron Courville 2 3 Marc G. Bellemare 1 2 3\nRishabh Agarwal\u2020 1 2 3 Pablo Samuel Castro\u2020 1\nAbstract\nWe introduce a value-based RL agent, which we\ncall BBF, that achieves super-human performance\nin the Atari 100K benchmark. BBF relies on scal-\ning the neural networks used for value estimation,\nas well as a number of other design choices that\nenable this scaling in a sample-efficient manner.\nWe conduct extensive analyses of these design\nchoices and provide insights for future work. We\nend with a discussion about updating the goal-\nposts for sample-efficient RL research on the ALE.\nWe make our code and data publicly available.\n1. Introduction\nDeep reinforcement learning (RL) has been central to a\nnumber of successes including playing complex games at a\nhuman or super-human level, such as OpenAI Five (Berner\net al., 2019), AlphaGo (Silver et al., 2016), and AlphaS-\ntar (Vinyals et al., 2019), controlling nuclear fusion plasma\nin a tokomak (Degrave et al., 2022), and integrating human\nfeedback for conversational agents (Ouyang et al., 2022).\nThe success of these RL methods has relied on large neural\nnetworks and an enormous number of environment sam-\nples to learn from \u2013 a human player would require tens of\nthousands of years of game play to gather the same amount\nof experience as OpenAI Five or AlphaGo. It is plausible\nthat such large networks are necessary for the agent\u2019s value\nestimation and/or policy to be expressive enough for the\nenvironment\u2019s complexity, while large number of samples\nmight be needed to gather enough experience so as to deter-\nmine the long-term effect of different action choices as well\nas train such large networks effectively. As such, obtaining\nhuman-level sample efficiency with deep RL remains an\noutstanding goal.\n*Equal contribution\n\u2020Equal advising\n1Google DeepMind\n2Mila\n3Universit\u00b4e de Montr\u00b4eal.\nCorrespondence to:\nMax\nSchwarzer <MaxA.Schwarzer@gmail.com>, Johan Obando-\nCeron <jobando0730@gmail.com>.\nProceedings of the 40 th International Conference on Machine\nLearning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright\n2023 by the author(s).\n2015\n2017\n2019\n2021\n2023\n1\n4\n16\n64\nEnv. Steps (x 100k) \nfor Human-level IQM\nMuZero*\nEff.Zero*\nDreamerV2*\nBBF\nSR-SPR\nDQN (Nature)\nRainbow\nIQN\nFigure 1: Environment samples to reach human-level per-\nformance, in terms of IQM (Agarwal et al., 2021b) over 26\ngames. Our proposed model-free agent, BBF, results in 5\u00d7\nimprovement over SR-SPR (D\u2019Oro et al., 2023) and at least\n16\u00d7 improvement over representative model-free RL meth-\nods, including DQN (Mnih et al., 2015b), Rainbow (Hessel\net al., 2017) and IQN (Dabney et al., 2018). To contrast\nwith the sample-efficiency progress in model-based RL, we\nalso include DreamerV2 (Hafner et al., 2020), MuZero Re-\nanalyse (Schrittwieser et al., 2021) and EfficientZero (Ye\net al., 2021).\nAlthough advances in modern hardware enable using large\nnetworks, in many environments it may be challenging to\nscale up the number of environment samples, especially for\nreal-world domains such as healthcare or robotics. While\napproaches such as offline RL leverage existing datasets to\nreduce the need for environment samples (Agarwal et al.,\n2020), the learned policies may be unable to handle dis-\ntribution shifts when interacting with the real environment\n(Levine et al., 2020) or may simply be limited in perfor-\nmance without online interactions (Ostrovski et al., 2021).\nThus, as RL continues to be used in increasingly challeng-\ning and sample-scarce scenarios, the need for scalable yet\nsample-efficient online RL methods becomes more pressing.\nDespite the variability in problem characteristics making\na one-size-fits-all solution unrealistic, there are many in-\nsights that may transfer across problem domains. As such,\nmethods that achieve \u201cstate-of-the-art\u201d performance on es-\ntablished benchmarks can provide guidance and insights for\nothers wishing to integrate their techniques.\nIn this vein, we focus on the Atari 100K benchmark (Kaiser\n1\narXiv:2305.19452v3  [cs.LG]  13 Nov 2023\nBBF: Human-level Atari with human-level efficiency\nDER\nDrQ (eps)\nSPR\nIRIS*\nSR-SPR\nEffZero*\nBBF\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nIQM\n0.2\n0.4\n0.6\n0.8\n1.0\nHuman-normalized IQM\n1\n8\n64\nRuntime (GPU hours)\nBBF\nEff.Zero*\nIRIS*\nSR-SPR\nSPR\nDERDrQ (eps)\nFigure 2: Comparing Atari 100K performance and computational cost of our model-free BBF agent to model-free\nSR-SPR (D\u2019Oro et al., 2023), SPR (Schwarzer et al., 2021), DrQ (eps) (Kostrikov et al., 2020) and DER (Van Hasselt et al.,\n2019) as well as model-based\u2217 EfficientZero (Ye et al., 2021) and IRIS (Micheli et al., 2023). (Left) BBF achieves higher\nperformance than all competitors as measured by interquartile mean human-normalized over 26 games. Error bars show\n95% bootstrap CIs. (Right) Computational cost vs. Performance, in terms of human-normalized IQM over 26 games. BBF\nresults in 2\u00d7 improvement in performance over SR-SPR with nearly the same computational-cost, while results in similar\nperformance to model-based EfficientZero with at least 4\u00d7 reduction in runtime. For measuring runtime, we use the total\nnumber of A100 GPU hours spent per environment.\net al., 2020), a well-known benchmark where agents are\nconstrained to roughly 2 hours of game play, which is the\namount of practice time the professional tester was given\nbefore human score evaluation. While human-level effi-\nciency has been obtained by the model-based EfficientZero\nagent (Ye et al., 2021), it has remained elusive for model-\nfree RL agents. To this end, we introduce BBF, a model-\nfree RL agent that achieves super-human performance \u2013 in-\nterquartile mean (Agarwal et al., 2021b) human-normalized\nscore above 1.0 \u2013 while being much more computation-\nally efficient than EfficientZero (Figure 2). Achieving this\nlevel of performance required a larger network than the\ndecade-old 3-layer CNN architecture (Mnih et al., 2013),\nbut as we will discuss below, scaling network size is not\nsufficient on its own. We discuss and analyze the various\ntechniques and components that are necessary to train BBF\nsuccessfully and provide guidance for future work to build\non our findings. Finally, we propose moving the goalpost\nfor sample-efficient RL research on the ALE.\n2. Background\nThe RL problem is generally described as a Markov Deci-\nsion Proces (MDP) (Puterman, 2014), defined by the tuple\n\u27e8S, A, P, R\u27e9, where S is the set of states, A is the set of\navailable actions, P : S\u00d7A \u2192 \u2206(S)1 is the transition func-\ntion, and R : S \u00d7 A \u2192 R is the reward function. Agent be-\nhavior in RL can be formalized by a policy \u03c0 : S \u2192 \u2206(A),\nwhich maps states to a distribution of actions. The value of\n1\u2206(S) denotes a distribution over the set S.\n\u03c0 when starting from s \u2208 S is defined as the discounted sum\nof expected rewards: V \u03c0(s) := E\u03c0,P [P\u221e\nt=0 \u03b3tr (st, at)],\nwhere \u03b3 \u2208 [0, 1) is a discount factor that encourages the\nagent to accumulate rewards sooner rather than later. The\ngoal of an RL agent is to find a policy \u03c0\u2217 that maximizes\nthis sum: V \u03c0\u2217 \u2265 V \u03c0 for all \u03c0.\nWhile there are a number of valid approaches (Sutton &\nBarto, 1998), in this paper we focus on model-free value-\nbased methods. Common value-based algorithms approxi-\nmate the Q\u2217-values, defined via the Bellman recurrence:\nQ\u2217(s, a) := R(s, a) + \u03b3Es\u2032\u223cP(s,a)[maxa\u2032\u2208A Q\u2217(s\u2032, a\u2032)].\nThe optimal policy \u03c0\u2217 can then be obtained from the\noptimal state-action value function Q\u2217 as \u03c0\u2217(x)\n:=\nmaxa\u2208A Q\u2217(s, a). A common approach for learning Q\u2217\nis the method of temporal differences, optimizing the Bell-\nman temporal difference:\n\u0012\nr (st, at) + \u03b3 max\nat+1 Q (st+1, at+1)\n\u0013\n\u2212 Q (st, at) .\nWe often refer to\n\u0000r (st, at) + \u03b3 maxat+1 Q (st+1, at+1)\n\u0001\nas the Bellman target.\nMnih et al. (2015a) introduced the agent DQN by com-\nbining temporal-difference learning with deep networks,\nand demonstrated its capabilities in achieving human-level\nperformance on the Arcade Learning Environment (ALE)\n(Bellemare et al., 2013). They used a network consisting of\n3 convolutional layers and 2 fully connected layers, param-\neterized by \u03b8, to approximate Q (denoted as Q\u03b8). We will\nrefer to this architecture as the CNN architecture. Most of\nthe work in value-based agents is built on the original DQN\n2\nBBF: Human-level Atari with human-level efficiency\n0.15 0.32 0.79\n2.13\n6.47\n21.80\nParameters (M)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nIQM\nResNet BBF\nResNet SR-SPR\nResNet SPR\n0.20 0.44 1.10\n3.06\n9.56\n32.88\nParameters (M)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nIQM\nCNN BBF\nCNN SR-SPR\nCNN SPR\n0.25 0.5\n1\n2\n4\n8\nWidth Scale\n0.25 0.5\n1\n2\n4\n8\nWidth Scale\nFigure 3: Scaling network widths for both ResNet and CNN architectures, for BBF, SR-SPR and SPR at replay ratio 2,\nwith an Impala-based ResNet (left) and the standard 3-layer CNN (Mnih et al., 2015b) (right). We report interquantile mean\nperformance with error bars indicating 95% confidence intervals. On the x-axis we report the approximate parameter count\nof each configuration as well as its width relative to the default (width scale = 1).\nagent, and we discuss a few of these advances below which\nare relevant to our work.\nHessel et al. (2018) combined six components into a single\nagent they called Rainbow: prioritized experience (Schaul\net al., 2016), n-step learning (Sutton, 1988), distributional\nRL (Bellemare et al., 2017), double Q-learning (van Hasselt\net al., 2016), dueling architecture (Wang et al., 2016) and\nNoisyNets (Fortunato et al., 2018b). Hessel et al. (2018)\nand Ceron & Castro (2021) both showed that Multi-step\nlearning is one of the most crucial components of Rainbow,\nin that removing it caused a large drop in performance.\nIn n-step learning, instead of computing the temporal dif-\nference error using a single-step transition, one can use\nn-step targets instead (Sutton, 1988), where for a trajec-\ntory (s0, a0, r0, s1, a1, \u00b7 \u00b7 \u00b7 ) and update horizon n: R(n)\nt\n:=\nPn\u22121\nk=0 \u03b3krt+k+1, yielding the multi-step temporal differ-\nence: R(n)\nt\n+ \u03b3n maxa\u2032 Q\u03b8(st+n, a\u2032) \u2212 Q\u03b8(st, at).\nMost modern RL algorithms store past experiences in a\nreplay buffer that increases sample efficiency by allowing\nthe agent to use samples multiple times during learning,\nand to leverage modern hardware such as GPUs and TPUs\nby training on sampled mini-batches. An important design\nparameter is the replay ratio, the ratio of learning updates to\nonline experience collected (Fedus et al., 2020a). It is worth\nnoting that DQN uses a replay ratio of 0.25 (4 environment\ninteractions for every learning update), while some sample-\nefficient agents based on Rainbow use a value of 1.\nNikishin et al. (2022) showed that the networks used by\ndeep RL agents have a tendency to overfit to early experi-\nence, which can result in sub-optimal performance. They\nproposed a simple strategy consisting of periodically reset-\nting the parameters of the final layers of DQN-based agents\nto counteract this. Building on this promising work, D\u2019Oro\net al. (2023) added a shrink-and-perturb technique for the\nparameters of the convolutional layers, and showed that this\nallowed them to scale the replay ratio to values as high as\n16, with no performance degradation.\n3. Related Work\nSample-Efficient RL on ALE: Sample efficiency has al-\nways been an import aspect of evaluation in RL, as it can\noften be expensive to interact with an environment. Kaiser\net al. (2020) introduced the Atari 100K benchmark, which\nhas proven to be useful for evaluating sample-efficiency, and\nhas led to a number of recent advances.\nKostrikov et al. (2020) use data augmentation to design\na sample-efficient RL method, DrQ, which outperformed\nprior methods on Atari 100K. Data-Efficient Rainbow\n(DER) (Van Hasselt et al., 2019) and DrQ(\u03f5) (Agarwal et al.,\n2021b) simply modified the hyperparameters of existing\nmodel-free algorithms to exceed the performance of exist-\ning methods without any algorithmic innovation.\nSchwarzer et al. (2021) introduced SPR, which builds on\nRainbow (Hessel et al., 2017) and uses a self-supervised tem-\n3\nBBF: Human-level Atari with human-level efficiency\nDER\nDrQ (eps)\nSPR\nIRIS*\nSR-SPR\nEffZero*\nBBF\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nOptimality Gap\n0\n1\n2\n3\n4\n5\nHuman Normalized Score ( )\n0.00\n0.25\n0.50\n0.75\n1.00\nFraction of runs with score >\nBBF\nEffZero*\nSR-SPR\nIRIS*\nSPR\nDrQ (eps)\nDER\nFigure 4: (Left). Optimality Gap (lower is better) for BBF at replay ratio 8 and competing methods on Atari 100K. Error\nbars show 95% CIs. BBF, has a lower optimality gap than any competing algorithm, indicating that it comes closer on\naverage to achieving human-level performance across all tasks. (Right) Performance profiles showing the distribution of\nscores across all runs and 26 games at the end of training (higher is better). Area under an algorithm\u2019s profile is its mean\nperformance while \u03c4 value where it intersects y = 0.75 shows its 25th percentile performance. BBF has better performance\non challenging tasks that may not otherwise contribute to IQM or median performance.\nporal consistency loss based on BYOL (Grill et al., 2020)\ncombined with data augmentation. SR-SPR (Schwarzer\net al., 2021) combines SPR with periodic network resets\nto achieve state-of-the-art performance on the 100K bench-\nmark. Ye et al. (2021) used a self-supervised consistency\nloss similar to SPR (Chen & He, 2021).\nEfficientZero (Ye et al., 2021), an efficient variant of\nMuZero (Schrittwieser et al., 2020), learns a discrete-action\nlatent dynamics model from environment interactions, and\nselects actions via lookahead MCTS in the latent space of\nthe model. Micheli et al. (2023) introduce IRIS, a data-\nefficient agent that learns in a world model composed of an\nautoencoder and an auto-regressive Transformer.\nScaling in Deep RL: Deep neural networks are useful for ex-\ntracting features from data relevant for various downstream\ntasks. Recently, there has been interest in the scaling prop-\nerties of neural network architectures, as scaling model size\nhas led to commensurate performance gains in applications\nranging from language modelling to computer vision.\nBased on those promising gains, the deep RL community\nhas begun to investigate the effect of increasing the model\nsize of the function approximator. Sinha et al. (2020) and\nOta et al. (2021) explore the interplay between the size,\nstructure, and performance of deep RL agents to provide\nintuition and guidelines for using larger networks. Kumar\net al. (2022) find that with ResNets (up to 80 million param-\neter networks) combined with distributional RL and feature\nnormalization, offline RL can exhibit strong performance\nthat scales with model capacity. Taiga et al. (2023) show\nthat generalization capabilities on the ALE benefit from\nhigher capacity networks, such as ResNets. Cobbe et al.\n(2020) and Farebrother et al. (2023) demonstrate benefits\nwhen scaling the number of features in each layer of the\nResNet architecture used by Impala (Espeholt et al., 2018),\nwhich motivated the choice of feature width scaling in this\nwork. Different from these works, our work focus on im-\nproving sample-efficiency in RL as opposed to offline RL\nor improving generalization in RL.\nIn the context of online RL, Hafner et al. (2023) demonstrate\nthat increased dynamics model size, trained via supervised\nlearning objectives, leads to monotonic improvements in\nthe agent\u2019s final performance. Recently, AdA (Team et al.,\n2023) scales transformer encoder for a Muesli agent up to\n265M parameters. Interestingly, AdA required distillation\nfrom smaller models to bigger models to achieve this scaling,\nin the spirit of reincarnating RL (Agarwal et al., 2022).\nHowever, it is unclear whether findings from above papers\ngeneralize to scaling typical value-based deep RL methods\nin sample-constraint settings, which we study in this work.\n4. Method\nThe question driving this work is: How does one scale net-\nworks for deep RL when samples are scarce? To investigate\nthis, we focus on the well-known Atari 100K benchmark\n(Kaiser et al., 2020), which includes 26 Atari 2600 games of\ndiverse characteristics, where the agent may perform only\n100K environment steps, roughly equivalent to two hours of\nhuman gameplay2. As we will see, n\u00a8aively scaling networks\ncan rarely maintain performance, let alone improve it.\nThe culmination of our investigation is the Bigger, Better,\n2100k steps (400k frames) at 60 FPS is 111 minutes.\n4\nBBF: Human-level Atari with human-level efficiency\n0.60\n0.75\n0.90\nIQM\nBBF - Resets\nBBF - Harder Resets\nBBF + n = 10\nBBF - Annealing\nBBF - WD\nBBF - SPR\nBBF + = 0.99\nBBF\nReplay Ratio 2\n0.4\n0.6\n0.8\n1.0\nIQM\nNo resets\nSR-SPR resets\nStandard n\nFixed  and n\nNo weight decay\nNo SSL\nStandard \nFull BBF\nReplay Ratio 8\nFigure 5: Evaluating the impact of removing the various components that make up BBF with RR=2 and RR=8.\nReporting interquantile mean averaged over the 26 Atari 100k games, with 95% CIs over 15 independent runs.\nFaster agent, or BBF in short, which achieves super-human\nperformance on Atari 100K with about 6 hours on single\nGPU. Figure 2 demonstrates the strong performance of BBF\nrelative to some of the best-performing Atari 100K agents:\nEfficientZero (Ye et al., 2021), SR-SPR (D\u2019Oro et al., 2023),\nand IRIS (Micheli et al., 2023). BBF consists of a number\nof components, which we discuss in detail below.\nOur implementation is based on the Dopamine framework\n(Castro et al., 2018) and uses mostly already previously-\nreleased components. For evaluation, we use rliable (Agar-\nwal et al., 2021b) and in particular, the interquartile\nmean (IQM) metric, which is the average score of the middle\n50% runs combined across all games and seeds.\nBase agent.\nBBF uses a modified version of the recently\nintroduced SR-SPR agent (D\u2019Oro et al., 2023). Through the\nuse of periodic network resets, SR-SPR is able to scale up its\nreplay ratio (RR) to values as high as 16, yielding better sam-\nple efficiency. For BBF, we use RR=8 in order to balance the\nincreased computation arising from our large network. Note\nthat this is still very high relative to existing Atari agents\n\u2013 Rainbow and its data-efficient variant DER (Van Hasselt\net al., 2019) use RR=0.25 and 1, respectively.\nAs we expect that many users will not wish to pay the com-\nputational costs of running at replay ratio 8, we also present\nresults for BBF and ablations at replay ratio 2 (matching\nSPR). For all experiments we state which replay ratio is\nbeing used in the captions.\nHarder resets.\nThe original SR-SPR agent (D\u2019Oro et al.,\n2023) used a shrink-and-perturb method for the convolu-\ntional layers where parameters were only perturbed 20% of\nthe way towards a random target, while later layers were\nfully reset to a random initialization. An interesting result\nof our investigation is that using harder resets of the convo-\nlutional layers yields better performance. In our work, we\nmove them 50% towards the random target, resulting in a\nstronger perturbation and improving results (see Figure 5).\nThis may be because larger networks need more regulariza-\ntion, as we find that they reduce loss faster (Figure A.1).\nLarger network.\nScaling network capacity is one of the\nmotivating factors for our work. As such, we adopt the\nImpala-CNN (Espeholt et al., 2018) network, a 15-layer\nResNet, which has previously led to substantial performance\ngains over the standard 3-layer CNN architecture in Atari\ntasks where large amounts of data are available (Agarwal\net al., 2022; Schmidt & Schmied, 2021). Additionally, BBF\nscales the width of each layer in Impala-CNN by 4\u00d7. In\nFigure 3, we examine how the performance of SPR, SR-SPR\nand BBF varies with different choices of scaling width, for\nboth the ResNet and original CNN architectures. Interest-\ningly, although the CNN has roughly 50% more parameters\nthan the ResNet at each scale level, the ResNet yields better\nperformance at all scaling levels for both SR-SPR and BBF.\nWhat stands out from Figure 3 is that BBF\u2019s performance\ncontinues to grow as width is increased, whereas SR-SPR\nseems to peak at 1-2\u00d7 (for both architectures). Given that\nResNet BBF performs comparably at 4\u00d7 and 8\u00d7, we chose\n4\u00d7 to reduce the computational burden. While reducing\nwidths beyond this could further reduce computational costs,\nthis comes at the cost of increasingly sharp reductions in\nperformance for all methods tested.\nReceding update horizon.\nOne of the surprising com-\nponents of BBF is the use of an update horizon (n-step)\nthat decreases exponentially from 10 to 3 over the first 10K\ngradient steps following each network reset. Given that we\nfollow the schedule of D\u2019Oro et al. (2023) and reset every\n5\nBBF: Human-level Atari with human-level efficiency\n1\n2\n4\n8\nReplay Ratio\n0.4\n0.6\n0.8\n1.0\nIQM\n  Constant 0.45 \n IQM improvement\nBBF\nSR-SPR\nFigure 6: Comparison of BBF and SR-SPR across dif-\nferent replay ratios. We report IQM with 95% CIs for\neach point. BBF achieves an almost-constant 0.45 IQM\nimprovement over SR-SPR at each replay ratio.\n40k gradient steps, the annealing phase is always 25% of\ntraining, regardless of the replay ratio. As can be seen in\nFigure 5, this yields a much stronger agent than using a fixed\nvalue of n = 3, which is default for Rainbow, or n = 10,\nwhich is typically used by Atari 100K agents like SR-SPR.\nOur n-step schedule is motivated by the theoretical results\nof Kearns & Singh (2000) \u2013 larger values of n-step leads\nto faster convergence but to higher asymptotic errors with\nrespect to the optimal value function. Thus, selecting a\nfixed value of n corresponds to a choice between having\neither rapid convergence to a worse asymptote, or slower\nconvergence to a better asymptote. As such, our exponential\nannealing schedule closely resembles the optimal decreasing\nschedule for n-step derived by Kearns & Singh (2000).\nIncreasing discount factor.\nMotivated by findings that\nincreasing the discount factor \u03b3 during learning improves\nperformance (Franc\u00b8ois-Lavet et al., 2015), we increase \u03b3\nfrom \u03b31 to \u03b32, following the same exponential schedule\nas for the update horizon. Note that increasing \u03b3 has the\neffect of progressively giving more weights to delayed re-\nwards. We choose \u03b31 = 0.97, slightly lower than the typical\ndiscount used for Atari, and \u03b32 = 0.997 as it is used by\nMuZero (Schrittwieser et al., 2021) and EfficientZero (Ye\net al., 2021). As with the update horizon, Figure 5 demon-\nstrates that this strategy outperforms using a fixed value.\nWeight decay.\nWe incorporate weight decay in our agent\nto curb statistical overfitting, as BBF is likely to overfit\nwith its high replay ratio. To do so, we use the AdamW\noptimizer (Loshchilov & Hutter, 2019) with a weight decay\nvalue of 0.1. Figure 5 suggests the gains from adding weight\ndecay are significant and increase with replay ratio, indicat-\ning that the regularizing effects of weight decay enhance\nreplay ratio scaling with large networks.\nRR=2\n SR-SPR\nRR=8\n SR-SPR\nRR=2\n BBF\nRR=8\n BBF\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nIQM\n+ Target Network\n Target Network\nFigure 7: Comparison of BBF and SR-SPR at replay\nratios 2 and 8 with and without EMA target networks.\nHuman-normalized IQM on the 26 Atari 100k games.\nRemoving noisy nets.\nFinally, we found that NoisyNets\n(Fortunato et al., 2018a), used in the original SPR\n(Schwarzer et al., 2021) and SR-SPR, did not improve per-\nformance. This could be due to NoisyNets causing over-\nexploration due to increased policy churn (Schaul et al.,\n2022) from added noise during training, or due to added\nvariance in optimization, and we leave investigation to future\nwork. Removing NoisyNets results in large computational\nand memory savings, as NoisyNets creates duplicate copies\nof the weight matrices for the final two linear layers in the\nnetwork, which contain the vast majority of all parameters:\nturning on NoisyNets increases the FLOPs per forward pass\nand the memory footprint by a factor of 2.5\u00d7 and 1.6\u00d7,\nrespectively, which both increases runtime and reduces the\nnumber of training runs that can be run in parallel on a single\nGPU. Removing NoisyNets is thus critical to allowing BBF\nto achieve reasonable compute efficiency despite its larger\nnetworks. We found that this decision had no significant\nimpact on task performance (see Figure A.2 in appendix).\n5. Analysis\nIn light of the importance of BBF\u2019s components, we discuss\npossible consequences of our findings for other algorithms.\nThe importance of self-supervision.\nOne unifying aspect\nof the methods compared in Figure 2 is that they all use some\nform of self-supervised objective. In sample-constrained\nscenarios, like the one considered here, relying on more than\nthe temporal-difference backups is likely to improve learn-\ning speed, provided the self-supervised losses are consistent\nwith the task at hand. We test this by removing the SPR\nobjective (inherited from SR-SPR) from BBF, and observe\na substantial performance degredation (see Figure 5). It is\nworth noting that EfficientZero uses a self-supervised objec-\ntive that is extremely similar to SPR, a striking commonality\nbetween BBF and EfficientZero.\n6\nBBF: Human-level Atari with human-level efficiency\nn = 3\n= 0.997\nAnnealing\nWD\nResets\n20% Perturb\nSPR\n0\n10\n20\n30\n40\n% Reduction in Performance\nTraining Games\nValidation Games\nFigure 8: Validating BBF design choices at RR=2 on 29\nunseen games. While Atari 100K training set consists of 26\ngames, we evaluate the performance of various components\nin BBF on 29 validation games in ALE that are not in Atari\n100K. Interestingly, all BBF components lead to a large\nperformance improvement on unseen games. Specifically,\nwe measure the % decrease in human-normalized IQM per-\nformance relative to the full BBF agent at RR=2.\nSample efficiency via more gradient steps.\nThe origi-\nnal DQN agent (Mnih et al., 2015b) has a replay ratio of\n0.25, which means a gradient update is performed only af-\nter every 4 environment steps. In low-data regimes, it is\nmore beneficial to perform more gradient steps, although\nmany algorithms cannot benefit from this without additional\nregularization (D\u2019Oro et al., 2023). As Figure 6 confirms,\nperformance of BBF grows with increasing replay ratio\nin the same manner as its base algorithm, SR-SPR. More\nstrikingly, we observe a linear relationship between the per-\nformance of BBF and SR-SPR across all replay ratios, with\nBBF performing roughly 0.45 IQM above SR-SPR. While\nthe direction of this relationship is intuitive given the net-\nwork scaling introduced by BBF, its linearity is unexpected,\nand further investigation is needed to understand the nature\nof the interaction between replay ratio and network scaling.\nOne interesting comparison to note is that, although Effi-\ncientZero uses a replay ratio of 1.2, they train with a batch\nsize that is 8 times larger than ours. Thus, their effective\nreplay ratio is comparable to ours.\nThe surprising importance of target networks\nMany\nprior works on Atari 100k, such as DrQ and SPR (Kostrikov\net al., 2020; Schwarzer et al., 2021) chose not to use target\nnetworks, seeing them unnecessary or an impediment to\nsample efficiency. Later, D\u2019Oro et al. (2023) re-introduced\nan exponential moving average target network, used both for\ntraining and action selection, and found that it improved per-\nformance somewhat, especially at high replay ratios. With\nnetwork scaling, however, using a target network becomes\na critical, but easy-to-overlook, component of the algorithm\nat all replay ratios (see Figure 7).\nFull Suite \n (55 Games)\nAtari 100k \n (26 Games)\n0.00\n0.25\n0.50\n0.75\n1.00\nHuman-Normalized IQM\nDeterministic\nSticky actions\nFigure 9: Evaluating BBF on ALE with and w/o sticky\nactions. We report IQM human-normalized performance\nat replay ratio 8 on 26 games in Atari 100K as well the full\nset of 55 games in ALE. While performance on the full set\nof 55 games is lower, neither setting has its performance\nsignificantly affected by sticky actions.\nReset Strength\nIncreasing the replay ratio is in general\nchallenging, as explored by Fedus et al. (2020b) and Kumar\net al. (2020). Periodic resetting, as suggested by Nikishin\net al. (2022) and D\u2019Oro et al. (2023), has proven effective\nto enable scaling to larger replay ratios, quite possibly a\nresult of reduced overfitting. This is confirmed in Figure 5,\nwhere the importance of resets is clear. Further, Figure 5 and\nFigure 8 demonstrate the added benefit of more aggressive\nperturbations, relative to SR-SPR.\nScale is not enough on its own.\nThe na\u00a8\u0131ve approach of\nsimply scaling the capacity of the CNN used by SR-SPR\nturns out to be insufficient to improve performance. Instead,\nas Figure 3 shows, the performance of SR-SPR collapses\nas network size increases. As discussed in section 4, it is\ninteresting to observe that the smaller Impala-CNN ResNet\n(as measured by number of parameters and FLOPs) yields\nstronger performance at all width scales.\nComputational efficiency.\nAs machine learning methods\nbecome more sophisticated, an often overlooked metric\nis their computational efficiency. Although EfficientZero\ntrains in around 8.5 hours, it requires about 512 CPU cores\nand 4 distributed GPUs. IRIS uses half of an A100 GPU for\na week per run. SR-SPR, at its highest replay ratio of 16,\nuses 25% of an A100 GPU and a single CPU for roughly\n24 hours. Our BBF agent at replay ratio 8 takes only 10\nhours with a single CPU and half of an A100 GPU. Thus,\nmeasured by GPU-hours, BBF provides the best trade-off\nbetween performance and compute (see Figure 2).\n6. Revisiting the Atari 100k benchmark\nA natural question is whether there is any value in continu-\ning to use the Atari 100K benchmark, given that both Effi-\ncientZero and BBF are able to achieve human-level perfor-\n7\nBBF: Human-level Atari with human-level efficiency\n2\n4\n8\n16\n32\n64\n128 256 512 1024\nGameplay time (Hours)\n0.0\n0.5\n1.0\n1.5\nHuman-normalized IQM\nBBF\nRainbow\nDQN (Nature)\nFigure 10: Sample efficiency progress on ALE, measured\nvia human-normalized IQM over 55 Atari games with sticky\nactions, as a function of amount of human game play hours,\nwith BBF at RR=8. Shaded regions show 95% CIs.\nmance (IQM \u2265 1.0) in just 100K steps. When considering\nthis, it is important to remember that IQM is an aggregate\nmeasure. Indeed, in the left panel of Figure 4 we can see\nthere is still room for improvement with regards to the op-\ntimality gap, which measures the amount by which each\nalgorithm fails to meet a minimum score of 1.0 (Agarwal\net al., 2021b). Specifically, despite monotonic progress over\nthe years, no agent is yet able to achieve human-level per-\nformance on all 26 games, which would yield an optimality\ngap of zero, without using dramatically more than two hours\nof data (Kapturowski et al., 2022).\nOverfitting on Atari 100K. Another important considera-\ntion is that the Atari 100K benchmark uses only 26 of the\n55 games from the full ALE suite, and it does not include\nsticky actions3 (Machado et al., 2018), which may make\ntasks significantly harder. Since we extensively benchmark\nBBF on Atari 100K, this raises the question of whether BBF\nworks well on unseen Atari games and with sticky actions.\nFortunately, it does. In Figure 9, we compare the perfor-\nmance of BBF on all 55 games with sticky actions, and show\nthat sticky action do not significantly harm performance. We\ndo observe that the held-out games not included in the Atari\n100k set are significantly more challenging than the 26 Atari\n100k games (see Figure 11) \u2013 but this is even more true for\nbaselines such as DQN (Nature) that did not use Atari 100k.\nFurthermore, as shown in Figure 8, we find that BBF\u2019s de-\nsign choices generally provide even more benefit on these\nheld-out games, possibly due to their increased difficulty.\nNew Frontiers\nIn fact, BBF works so well on the stan-\ndard Atari setting that it is able to roughly match DQN\u2019s\nperformance at 256 hours with only two hours of gameplay\n3With 25% probability, the environment will execute the previ-\nous action again, instead of the agent\u2019s executed action.\nHeld-out \n (29 Games)\nFull Suite \n (55 Games)\nAtari 100k \n (26 Games)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nHuman-Normalized IQM\nBBF @ 100k\nDQN (Nature) @ 200M\nFigure 11: Comparing performance on the 29 unseen\ngames to the 26 Atari 100k games. BBF trained with sticky\nactions at RR=8 for 100k steps approximately matches DQN\n(Nature) with 500 times more training data on each set.\nWhile we find that the 29 games not included in the Atari\n100k setting are significantly harder than the 26 Atari 100k\ngames, we see no evidence that BBF has overfitted to Atari\n100k compared to DQN.\ntime (Figure 10). This suggests a clear new milestone for\nthe community: can we match Rainbow\u2019s final performance\nwith just two hours of gameplay? To facilitate future re-\nsearch toward this, we release scores on the set of 55 games\nwith sticky actions, at various scales and replay ratios.\nData Scaling\nPrior works have indicated that many\nsample-efficient RL algorithms plateau in performance\nwhen trained for longer than they were originally designed\nfor (e.g., Agarwal et al., 2022).\nTo examine this phe-\nnomenon, we train BBF, SPR and SR-SPR at replay ratio\n2 out to one million environment steps (Figure 12), keep-\ning all parameters unchanged (including conducting resets\nas normal past 100k steps). We observe that SPR and SR-\nSPR experience stagnating performance, with SR-SPR\u2019s\nadvantage over SPR fading by 1M steps. BBF, however,\nremains consistently ahead of both, matching DQN\u2019s final\nperformance before 200k environment steps and matching\nRainbow\u2019s performance at 20M environment steps by 1M\nsteps. We note that this experiment costs only 2.5 times\nmore than training at replay ratio 8 to 100k steps, so we\nencourage other researchers to run similar experiments.\nAdditionally, we note in Figure 13 that it is possible to\ncompare algorithms even with extremely small amounts of\ndata, such as 20k or 50k steps, by which point BBF at replay\nratio 2 (even with sticky actions enabled) outperforms most\nrecently proposed algorithms (Robine et al., 2023; Micheli\net al., 2023; Hafner et al., 2023), which did not use sticky\nactions. We thus suggest that compute-constrained groups\nconsider this setting, as training BBF at replay ratio 2 for\n40k environment steps takes only half of an A100 for 1 hour.\n8\nBBF: Human-level Atari with human-level efficiency\n100k\n200k\n500k\n1M\n2M\n5M\n10M\n20M\n50M\nEnvironment Steps\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\nHuman-normalized IQM\nBBF (RR=2)\nSR-SPR (RR=2)\nSPR\nRainbow\nDQN (Nature)\nFigure 12: Learning curves for BBF, SR-SPR and SPR at replay ratio 2, measured via human-normalized IQM over 55\nAtari games with sticky actions, as a function of number of environment interactions. Shaded regions show 95% CIs.\n7. Discussion and Future Work\nWe introduced BBF, an algorithm that is able to achieve\nsuper-human level performance on the ALE with only 2-\nhours of gameplay. Although BBF is not the first to achieve\nthis milestone, it is able to do so in a computationally ef-\nficient manner. Furthermore, BBF is able to better handle\nthe scaling of networks and replay ratios, which are cru-\ncial for network expressivity and learning efficiency. In-\ndeed, Figure 3 suggests that BFF is better-able to use over-\nparameterized networks than prior agents.\nThe techniques necessary to achieve this result invite a num-\nber of research questions for future work. Large replay ratios\nare a key element of BFF\u2019s performance, and the ability to\nscale them is due to the periodic resets incorporated into\nthe algorithm. These resets are likely striking a favourable\nbalance between catastrophic forgetting and network plas-\nticity. An interesting avenue for future research is whether\nthere are other mechanisms for striking this balance that per-\nhaps are more targeted (e.g. not requiring resetting the full\nnetwork, as was recently explored by Sokar et al. (2023)).\nWe remarked on the fact that all the methods compared in\nFigure 2 use a form of self-supervision. Would other self-\nsupervised losses (e.g. (Mazoure et al., 2020; Castro et al.,\n2021; Agarwal et al., 2021a)) produce similar results? Sur-\nprisingly, Li et al. (2022) argue that self-supervision from\npixels does not improve performance; our results seem to\ncontradict this finding.\nRecent attention has shifted towards more realistic bench-\nmarks (Fan et al., 2022) but such benchmarks exclude the\nmajority of researchers outside certain resource-rich labs,\nand may require an alternative paradigm (Agarwal et al.,\n2022). One advantage of the Atari 100k benchmark is that,\nwhile still a challenging benchmark, it is relatively cheap\ncompared to other benchmarks of similar complexity. How-\never, despite its apparent saturation, scientific progress can\nstill be made on this benchmark if we expand its scope. We\nhope our work provides a solid starting point for this.\nOverall, we hope that our work inspires other researchers to\ncontinue pushing the frontier of sample efficiency in deep\nRL forward, to ultimately reach human-level performance\nacross all tasks with human-level or superhuman efficiency.\n0\n20k\n40k\n60k\n80k\n100k\nEnvironment Steps\n0.0\n0.2\n0.4\n0.6\n0.8\nHuman-normalized IQM\nDER\nSimPLe\nSPR\nDrQ\nIRIS\nDreamerV3\nTWM\nBBF (RR=2, +Sticky)\nFigure 13: IQM Human-normalized learning curve for\nBBF at RR=2 with sticky actions on the 26 Atari 100k\ngames, with final performances of many recent algorithms\nafter they have trained for 100k steps. Even a weakened\nBBF outperforms all by 50k steps.\n9\nBBF: Human-level Atari with human-level efficiency\nReferences\nAgarwal, R., Schuurmans, D., and Norouzi, M. An opti-\nmistic perspective on offline reinforcement learning. In\nInternational Conference on Machine Learning, pp. 104\u2013\n114. PMLR, 2020.\nAgarwal, R., Machado, M. C., Castro, P. S., and Bellemare,\nM. G. Contrastive behavioral similarity embeddings for\ngeneralization in reinforcement learning. arXiv preprint\narXiv:2101.05265, 2021a.\nAgarwal, R., Schwarzer, M., Castro, P. S., Courville, A.,\nand Bellemare, M. G. Deep reinforcement learning at\nthe edge of the statistical precipice. Advances in Neural\nInformation Processing Systems, 2021b.\nAgarwal, R., Schwarzer, M., Castro, P. S., Courville, A.,\nand Bellemare, M. G. Reincarnating reinforcement learn-\ning: Reusing prior computation to accelerate progress.\nIn Advances in Neural Information Processing Systems,\n2022.\nBellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M.\nThe arcade learning environment: An evaluation plat-\nform for general agents. Journal of Artificial Intelligence\nResearch, 47:253\u2013279, 2013.\nBellemare, M. G., Dabney, W., and Munos, R. A distribu-\ntional perspective on reinforcement learning. In ICML,\n2017.\nBerner, C., Brockman, G., Chan, B., Cheung, V., Debiak, P.,\nDennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse,\nC., et al. Dota 2 with large scale deep reinforcement\nlearning. arXiv preprint arXiv:1912.06680, 2019.\nBradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary,\nC., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J.,\nWanderman-Milne, S., et al. Jax: composable transfor-\nmations of python+ numpy programs. 2018.\nCastro, P. S., Moitra, S., Gelada, C., Kumar, S., and Belle-\nmare, M. G. Dopamine: A Research Framework for\nDeep Reinforcement Learning.\n2018.\nURL http:\n//arxiv.org/abs/1812.06110.\nCastro, P. S., Kastner, T., Panangaden, P., and Rowland,\nM. MICo: Improved representations via sampling-based\nstate similarity for markov decision processes. In Beygelz-\nimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.),\nAdvances in Neural Information Processing Systems,\n2021. URL https://openreview.net/forum?\nid=wFp6kmQELgu.\nCeron, J. S. O. and Castro, P. S. Revisiting rainbow: Pro-\nmoting more insightful and inclusive deep reinforcement\nlearning research. In International Conference on Ma-\nchine Learning, pp. 1373\u20131383. PMLR, 2021.\nChen, X. and He, K. Exploring simple siamese represen-\ntation learning. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pp.\n15750\u201315758, 2021.\nCobbe, K., Hesse, C., Hilton, J., and Schulman, J. Lever-\naging procedural generation to benchmark reinforcement\nlearning. In International conference on machine learn-\ning, pp. 2048\u20132056. PMLR, 2020.\nDabney, W., Ostrovski, G., Silver, D., and Munos, R. Im-\nplicit quantile networks for distributional reinforcement\nlearning. In International conference on machine learn-\ning, pp. 1096\u20131105. PMLR, 2018.\nDegrave, J., Felici, F., Buchli, J., Neunert, M., Tracey, B. D.,\nCarpanese, F., Ewalds, T., Hafner, R., Abdolmaleki, A.,\nde Las Casas, D., Donner, C., Fritz, L., Galperti, C., Hu-\nber, A., Keeling, J., Tsimpoukelli, M., Kay, J., Merle, A.,\nMoret, J., Noury, S., Pesamosca, F., Pfau, D., Sauter,\nO., Sommariva, C., Coda, S., Duval, B., Fasoli, A.,\nKohli, P., Kavukcuoglu, K., Hassabis, D., and Riedmiller,\nM. A. Magnetic control of tokamak plasmas through\ndeep reinforcement learning. Nature, 602(7897):414\u2013419,\n2022. doi: 10.1038/s41586-021-04301-9. URL https:\n//doi.org/10.1038/s41586-021-04301-9.\nD\u2019Oro, P., Schwarzer, M., Nikishin, E., Bacon, P.-L., Belle-\nmare, M. G., and Courville, A. Sample-efficient rein-\nforcement learning by breaking the replay ratio barrier.\nIn The Eleventh International Conference on Learning\nRepresentations, 2023. URL https://openreview.\nnet/forum?id=OpC-9aBBVJe.\nEspeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih,\nV., Ward, T., Doron, Y., Firoiu, V., Harley, T., Dunning,\nI., et al. Impala: Scalable distributed deep-rl with im-\nportance weighted actor-learner architectures. In Interna-\ntional conference on machine learning, pp. 1407\u20131416.\nPMLR, 2018.\nFan, L., Wang, G., Jiang, Y., Mandlekar, A., Yang, Y., Zhu,\nH., Tang, A., Huang, D.-A., Zhu, Y., and Anandkumar, A.\nMinedojo: Building open-ended embodied agents with\ninternet-scale knowledge.\nIn Thirty-sixth Conference\non Neural Information Processing Systems Datasets and\nBenchmarks Track, 2022.\nFarebrother, J., Greaves, J., Agarwal, R., Lan, C. L.,\nGoroshin, R., Castro, P. S., and Bellemare, M. G.\nProto-value networks: Scaling representation learning\nwith auxiliary tasks. In Submitted to The Eleventh In-\nternational Conference on Learning Representations,\n2023. URL https://openreview.net/forum?\nid=oGDKSt9JrZi. under review.\n10\nBBF: Human-level Atari with human-level efficiency\nFedus, W., Ramachandran, P., Agarwal, R., Bengio, Y.,\nLarochelle, H., Rowland, M., and Dabney, W. Revisiting\nfundamentals of experience replay. In International Con-\nference on Machine Learning, pp. 3061\u20133071. PMLR,\n2020a.\nFedus, W., Ramachandran, P., Agarwal, R., Bengio, Y.,\nLarochelle, H., Rowland, M., and Dabney, W. Revisit-\ning fundamentals of experience replay. In III, H. D. and\nSingh, A. (eds.), Proceedings of the 37th International\nConference on Machine Learning, volume 119 of Pro-\nceedings of Machine Learning Research, pp. 3061\u20133071.\nPMLR, 13\u201318 Jul 2020b.\nFortunato, M., Azar, M. G., Piot, B., Menick, J., Hes-\nsel, M., Osband, I., Graves, A., Mnih, V., Munos, R.,\nHassabis, D., Pietquin, O., Blundell, C., and Legg, S.\nNoisy networks for exploration. In International Confer-\nence on Learning Representations, 2018a. URL https:\n//openreview.net/forum?id=rywHCPkAW.\nFortunato, M., Azar, M. G., Piot, B., Menick, J., Osband, I.,\nGraves, A., Mnih, V., Munos, R., Hassabis, D., Pietquin,\nO., Blundell, C., and Legg, S. Noisy networks for explo-\nration. 2018b.\nFranc\u00b8ois-Lavet, V., Fonteneau, R., and Ernst, D. How to\ndiscount deep reinforcement learning: Towards new dy-\nnamic strategies. arXiv preprint arXiv:1512.02011, 2015.\nGrill, J.-B., Strub, F., Altch\u00b4e, F., Tallec, C., Richemond, P.,\nBuchatskaya, E., Doersch, C., Avila Pires, B., Guo, Z.,\nGheshlaghi Azar, M., et al. Bootstrap your own latent-a\nnew approach to self-supervised learning. Advances in\nneural information processing systems, 33:21271\u201321284,\n2020.\nHafner, D., Lillicrap, T., Norouzi, M., and Ba, J. Mas-\ntering atari with discrete world models. arXiv preprint\narXiv:2010.02193, 2020.\nHafner, D., Pasukonis, J., Ba, J., and Lillicrap, T. Master-\ning diverse domains through world models, 2023. URL\nhttps://arxiv.org/abs/2301.04104.\nHarris, C. R., Millman, K. J., Van Der Walt, S. J., Gommers,\nR., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J.,\nBerg, S., Smith, N. J., et al. Array programming with\nnumpy. Nature, 585(7825):357\u2013362, 2020.\nHessel, M., Modayil, J., van Hasselt, H., Schaul, T., Ostro-\nvski, G., Dabney, W., Horgan, D., Piot, B., Azar, M., and\nSilver, D. Rainbow: Combining improvements in deep\nreinforcement learning. arXiv preprint arXiv:1710.02298,\n2017.\nHessel, M., Modayil, J., Hasselt, H. V., Schaul, T., Ostrovski,\nG., Dabney, W., Horgan, D., Piot, B., Azar, M. G., and\nSilver, D. Rainbow: Combining improvements in deep\nreinforcement learning. In AAAI, 2018.\nHunter, J. D. Matplotlib: A 2d graphics environment. Com-\nputing in science & engineering, 9(03):90\u201395, 2007.\nKaiser, L., Babaeizadeh, M., Milos, P., Osinski, B., Camp-\nbell, R. H., Czechowski, K., Erhan, D., Finn, C., Koza-\nkowski, P., Levine, S., et al. Model-based reinforcement\nlearning for atari. International Conference on Learning\nRepresentations, 2020.\nKapturowski, S., Campos, V., Jiang, R., Raki\u00b4cevi\u00b4c, N., van\nHasselt, H., Blundell, C., and Badia, A. P. Human-level\natari 200x faster. arXiv preprint arXiv:2209.07550, 2022.\nKearns, M. J. and Singh, S. Bias-variance error bounds\nfor temporal difference updates. In COLT, pp. 142\u2013147,\n2000.\nKostrikov, I., Yarats, D., and Fergus, R. Image augmentation\nis all you need: Regularizing deep reinforcement learning\nfrom pixels. arXiv preprint arXiv:2004.13649, 2020.\nKumar, A., Agarwal, R., Ghosh, D., and Levine, S. Implicit\nunder-parameterization inhibits data-efficient deep rein-\nforcement learning. arXiv preprint arXiv:2010.14498,\n2020.\nKumar, A., Agarwal, R., Geng, X., Tucker, G., and Levine,\nS. Offline q-learning on diverse multi-task data both\nscales and generalizes, 2022. URL https://arxiv.\norg/abs/2211.15144.\nLevine, S., Kumar, A., Tucker, G., and Fu, J. Offline re-\ninforcement learning: Tutorial, review, and perspectives\non open problems. CoRR, abs/2005.01643, 2020. URL\nhttps://arxiv.org/abs/2005.01643.\nLi, X., Shang, J., Das, S., and Ryoo, M. S. Does self-\nsupervised learning really improve reinforcement learn-\ning from pixels?\nIn Oh, A. H., Agarwal, A., Bel-\ngrave, D., and Cho, K. (eds.), Advances in Neural In-\nformation Processing Systems, 2022.\nURL https:\n//openreview.net/forum?id=fVslVNBfjd8.\nLoshchilov, I. and Hutter, F. Decoupled weight decay reg-\nularization. In International Conference on Learning\nRepresentations, 2019. URL https://openreview.\nnet/forum?id=Bkg6RiCqY7.\nMachado, M. C., Bellemare, M. G., Talvitie, E., Veness, J.,\nHausknecht, M., and Bowling, M. Revisiting the arcade\nlearning environment: Evaluation protocols and open\nproblems for general agents. J. Artif. Int. Res., 61(1):\n523\u2013562, jan 2018. ISSN 1076-9757.\n11\nBBF: Human-level Atari with human-level efficiency\nMazoure, B., Tachet des Combes, R., Doan, T. L.,\nBachman, P., and Hjelm, R. D.\nDeep reinforcement\nand infomax learning.\nIn Larochelle, H., Ranzato,\nM., Hadsell, R., Balcan, M., and Lin, H. (eds.),\nAdvances in Neural Information Processing Systems,\nvolume\n33,\npp.\n3686\u20133698.\nCurran\nAssociates,\nInc.,\n2020.\nURL\nhttps://proceedings.\nneurips.cc/paper/2020/file/\n26588e932c7ccfa1df309280702fe1b5-Paper.\npdf.\nMicheli, V., Alonso, E., and Fleuret, F. Transformers are\nsample-efficient world models.\nIn To appear in The\nEleventh International Conference on Learning Represen-\ntations, 2023. URL https://openreview.net/\nforum?id=vhFu1Acb0xb.\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A.,\nAntonoglou, I., Wierstra, D., and Riedmiller, M. Playing\natari with deep reinforcement learning. arXiv preprint\narXiv:1312.5602, 2013.\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Ve-\nness, J., Bellemare, M. G., Graves, A., Riedmiller, M.,\nFidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C.,\nSadik, A., Antonoglou, I., King, H., Kumaran, D., Wier-\nstra, D., Legg, S., and Hassabis, D. Human-level control\nthrough deep reinforcement learning. Nature, 518(7540):\n529\u2013533, February 2015a.\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,\nJ., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje-\nland, A. K., Ostrovski, G., et al. Human-level control\nthrough deep reinforcement learning. nature, 518(7540):\n529\u2013533, 2015b.\nNikishin, E., Schwarzer, M., D\u2019Oro, P., Bacon, P.-L., and\nCourville, A. The primacy bias in deep reinforcement\nlearning. In Chaudhuri, K., Jegelka, S., Song, L., Szepes-\nvari, C., Niu, G., and Sabato, S. (eds.), Proceedings of the\n39th International Conference on Machine Learning, vol-\nume 162 of Proceedings of Machine Learning Research,\npp. 16828\u201316847. PMLR, 17\u201323 Jul 2022.\nOliphant, T. E. Python for scientific computing. Computing\nin Science & Engineering, 9(3):10\u201320, 2007. doi: 10.\n1109/MCSE.2007.58.\nOstrovski, G., Castro, P. S., and Dabney, W. The difficulty\nof passive learning in deep reinforcement learning. In\nBeygelzimer, A., Dauphin, Y., Liang, P., and Vaughan,\nJ. W. (eds.), Advances in Neural Information Processing\nSystems, 2021. URL https://openreview.net/\nforum?id=nPHA8fGicZk.\nOta, K., Jha, D. K., and Kanezaki, A. Training larger net-\nworks for deep reinforcement learning. arXiv preprint\narXiv:2102.07920, 2021.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Gray, A.,\nSchulman, J., Hilton, J., Kelton, F., Miller, L., Simens,\nM., Askell, A., Welinder, P., Christiano, P., Leike, J., and\nLowe, R. Training language models to follow instruc-\ntions with human feedback. In Oh, A. H., Agarwal, A.,\nBelgrave, D., and Cho, K. (eds.), Advances in Neural\nInformation Processing Systems, 2022. URL https:\n//openreview.net/forum?id=TG8KACxEON.\nPuterman, M. L.\nMarkov decision processes: discrete\nstochastic dynamic programming. John Wiley & Sons,\n2014.\nRobine, J., H\u00a8oftmann, M., Uelwer, T., and Harmeling, S.\nTransformer-based world models are happy with 100k\ninteractions. In The Eleventh International Conference\non Learning Representations, 2023. URL https://\nopenreview.net/forum?id=TdBaDGCpjly.\nSchaul, T., Quan, J., Antonoglou, I., and Silver, D. Priori-\ntized experience replay. CoRR, abs/1511.05952, 2016.\nSchaul, T., Barreto, A., Quan, J., and Ostrovski, G. The\nphenomenon of policy churn. Advances in Neural Infor-\nmation Processing Systems, 2022.\nSchmidt, D. and Schmied, T. Fast and data-efficient training\nof rainbow: an experimental study on atari. arXiv preprint\narXiv:2111.10247, 2021.\nSchrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K.,\nSifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis,\nD., Graepel, T., et al. Mastering atari, go, chess and shogi\nby planning with a learned model. Nature, 588(7839):\n604\u2013609, 2020.\nSchrittwieser, J., Hubert, T., Mandhane, A., Barekatain,\nM., Antonoglou, I., and Silver, D. Online and offline\nreinforcement learning by planning with a learned model.\nAdvances in Neural Information Processing Systems, 34:\n27580\u201327591, 2021.\nSchwarzer, M., Anand, A., Goel, R., Hjelm, R. D.,\nCourville, A., and Bachman, P. Data-efficient reinforce-\nment learning with self-predictive representations. In\nInternational Conference on Learning Representations,\n2021. URL https://openreview.net/forum?\nid=uCQfPZwRaUu.\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L.,\nvan den Driessche, G., Schrittwieser, J., Antonoglou, I.,\nPanneershelvam, V., Lanctot, M., Dieleman, S., Grewe,\nD., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T.,\nLeach, M., Kavukcuoglu, K., Graepel, T., and Hassabis,\nD. Mastering the game of go with deep neural networks\nand tree search.\nNature, 529:484\u2013503, 2016.\nURL\n12\nBBF: Human-level Atari with human-level efficiency\nhttp://www.nature.com/nature/journal/\nv529/n7587/full/nature16961.html.\nSinha, S., Bharadhwaj, H., Srinivas, A., and Garg, A. D2rl:\nDeep dense architectures in reinforcement learning. arXiv\npreprint arXiv:2010.09163, 2020.\nSokar, G., Agarwal, R., Castro, P. S., and Evci, U. The dor-\nmant neuron phenomenon in deep reinforcement learning.\nIn ICML, 2023.\nSutton, R. S. Learning to predict by the methods of temporal\ndifferences. Machine Learning, 3(1):9\u201344, August 1988.\nSutton, R. S. and Barto, A. G. Introduction to Reinforcement\nLearning. MIT Press, Cambridge, MA, USA, 1st edition,\n1998. ISBN 0262193981.\nTaiga, A. A., Agarwal, R., Farebrother, J., Courville, A., and\nBellemare, M. G. Investigating multi-task pretraining and\ngeneralization in reinforcement learning. In Submitted\nto The Eleventh International Conference on Learning\nRepresentations, 2023. URL https://openreview.\nnet/forum?id=sSt9fROSZRO. under review.\nTeam, A. A., Bauer, J., Baumli, K., Baveja, S., Behba-\nhani, F., Bhoopchand, A., Bradley-Schmieg, N., Chang,\nM., Clay, N., Collister, A., et al.\nHuman-timescale\nadaptation in an open-ended task space. arXiv preprint\narXiv:2301.07608, 2023.\nvan Hasselt, H., Guez, A., and Silver, D. Deep reinforce-\nment learning with double q-learning. In Proceedings of\nthe Thirthieth AAAI Conference On Artificial Intelligence\n(AAAI), 2016, 2016. cite arxiv:1509.06461Comment:\nAAAI 2016.\nVan Hasselt, H. P., Hessel, M., and Aslanides, J. When to\nuse parametric models in reinforcement learning? Ad-\nvances in Neural Information Processing Systems, 32,\n2019.\nVan Rossum, G. and Drake Jr, F. L. Python reference man-\nual. Centrum voor Wiskunde en Informatica Amsterdam,\n1995.\nVinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M.,\nDudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds,\nT., Georgiev, P., et al. Grandmaster level in starcraft ii\nusing multi-agent reinforcement learning. Nature, 575\n(7782):350\u2013354, 2019.\nWang, Z., Schaul, T., Hessel, M., Hasselt, H., Lanctot, M.,\nand Freitas, N. Dueling network architectures for deep\nreinforcement learning. In Proceedings of the 33rd Inter-\nnational Conference on Machine Learning, volume 48,\npp. 1995\u20132003, 2016.\nYe, W., Liu, S., Kurutach, T., Abbeel, P., and Gao, Y. Mas-\ntering atari games with limited data. Advances in Neural\nInformation Processing Systems, 34:25476\u201325488, 2021.\n13\nBBF: Human-level Atari with human-level efficiency\nAcknowledgements.\nMany thanks to Ross Goroshin, Georg Ostrovski, and Gopeshh Subbaraj for their feedback on an\nearlier draft of this paper. The authors would like to thank the anonymous reviewers for useful discussions and feedback\non this paper. We would also like to thank the Python community (Van Rossum & Drake Jr, 1995; Oliphant, 2007) for\ndeveloping tools that enabled this work, including NumPy (Harris et al., 2020), Matplotlib (Hunter, 2007) and JAX (Bradbury\net al., 2018).\nSocietal impact.\nAlthough the work presented here is mostly academic, it aids in the development of more capable\nautonomous agents. While our contributions do not directly contribute to any negative societal impacts, we urge the\ncommunity to consider these when building on our research.\nA. Additional Results\nRandom\nHuman\nDER\nDrQ(\u03f5)\nSPR\nIRIS\nSR-SPR\nEfficientZero\nBBF\nAlien\n227.8\n7127.7\n802.3\n865.2\n841.9\n420.0\n1107.8\n808.5\n1173.2\nAmidar\n5.8\n1719.5\n125.9\n137.8\n179.7\n143.0\n203.4\n148.6\n244.6\nAssault\n222.4\n742.0\n561.5\n579.6\n565.6\n1524.4\n1088.9\n1263.1\n2098.5\nAsterix\n210.0\n8503.3\n535.4\n763.6\n962.5\n853.6\n903.1\n25557.8\n3946.1\nBankHeist\n14.2\n753.1\n185.5\n232.9\n345.4\n53.1\n531.7\n351.0\n732.9\nBattleZone\n2360.0\n37187.5\n8977.0\n10165.3\n14834.1\n13074.0\n17671.0\n13871.2\n24459.8\nBoxing\n0.1\n12.1\n-0.3\n9.0\n35.7\n70.1\n45.8\n52.7\n85.8\nBreakout\n1.7\n30.5\n9.2\n19.8\n19.6\n83.7\n25.5\n414.1\n370.6\nChopperCommand\n811.0\n7387.8\n925.9\n844.6\n946.3\n1565.0\n2362.1\n1117.3\n7549.3\nCrazyClimber\n10780.5\n35829.4\n34508.6\n21539.0\n36700.5\n59324.2\n45544.1\n83940.2\n58431.8\nDemonAttack\n152.1\n1971.0\n627.6\n1321.5\n517.6\n2034.4\n2814.4\n13003.9\n13341.4\nFreeway\n0.0\n29.6\n20.9\n20.3\n19.3\n31.1\n25.4\n21.8\n25.5\nFrostbite\n65.2\n4334.7\n871.0\n1014.2\n1170.7\n259.1\n2584.8\n296.3\n2384.8\nGopher\n257.6\n2412.5\n467.0\n621.6\n660.6\n2236.1\n712.4\n3260.3\n1331.2\nHero\n1027.0\n30826.4\n6226.0\n4167.9\n5858.6\n7037.4\n8524.0\n9315.9\n7818.6\nJamesbond\n29.0\n302.8\n275.7\n349.1\n366.5\n462.7\n389.1\n517.0\n1129.6\nKangaroo\n52.0\n3035.0\n581.7\n1088.4\n3617.4\n838.2\n3631.7\n724.1\n6614.7\nKrull\n1598.0\n2665.5\n3256.9\n4402.1\n3681.6\n6616.4\n5911.8\n5663.3\n8223.4\nKungFuMaster\n258.5\n22736.3\n6580.1\n11467.4\n14783.2\n21759.8\n18649.4\n30944.8\n18991.7\nMsPacman\n307.3\n6951.6\n1187.4\n1218.1\n1318.4\n999.1\n1574.1\n1281.2\n2008.3\nPong\n-20.7\n14.6\n-9.7\n-9.1\n-5.4\n14.6\n2.9\n20.1\n16.7\nPrivateEye\n24.9\n69571.3\n72.8\n3.5\n86.0\n100.0\n97.9\n96.7\n40.5\nQbert\n163.9\n13455.0\n1773.5\n1810.7\n866.3\n745.7\n4044.1\n14448.5\n4447.1\nRoadrunner\n11.5\n7845.0\n11843.4\n11211.4\n12213.1\n9614.6\n13463.4\n17751.3\n33426.8\nSeaquest\n68.4\n42054.7\n304.6\n352.3\n558.1\n661.3\n819.0\n1100.2\n1232.5\nUpNDown\n533.4\n11693.2\n3075.0\n4324.5\n10859.2\n3546.2\n112450.3\n17264.2\n12101.7\nGames > Human\n0\n0\n2\n3\n6\n9\n9\n14\n12\nIQM (\u2191)\n0.000\n1.000\n0.183\n0.280\n0.337\n0.501\n0.631\n1.020\n1.045\nOptimality Gap (\u2193)\n1.000\n0.000\n0.698\n0.631\n0.577\n0.512\n0.433\n0.371\n0.344\nMedian (\u2191)\n0.000\n1.000\n0.189\n0.313\n0.396\n0.289\n0.685\n1.116\n0.917\nMean (\u2191)\n0.000\n1.000\n0.350\n0.465\n0.616\n1.046\n1.272\n1.945\n2.247\nTable A.1: Scores and aggregate metrics for BBF and competing methods across the 26 Atari 100k games. Scores are\naveraged across 50 seeds per game for BBF, 30 for SR-SPR, 5 for IRIS, 3 for EfficientZero, and 100 for others.\n14\nBBF: Human-level Atari with human-level efficiency\n0\n20000\n40000\n60000\n80000\n100000\nStep\n40\n60\n80\n100\n120\nParameter Norm\n0\n20000\n40000\n60000\n80000\n100000\nStep\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nGradient Norm\n0\n20000\n40000\n60000\n80000\n100000\nStep\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\nTD Error\n0\n20000\n40000\n60000\n80000\n100000\nStep\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\nTraining IQM\nBBF WS: 1\nSR-SPR WS: 1\nBBF WS: 2\nSR-SPR WS: 2\nBBF WS: 4\nSR-SPR WS: 4\nBBF WS: 8\nSR-SPR WS: 8\nFigure A.1: Learning curves for BBF and SR-SPR at RR=2 with a ResNet encoder at various width scales, on the 26\nAtari 100k games. Larger networks consistently have lower TD errors and higher gradient norms, and higher parameter\nnorms, but only BBF translates this to higher environment returns. The large, systematic difference in TD error between\nBBF and SR-SPR is due to BBF\u2019s use of a shorter update horizon, which makes each step of the TD backup easier to predict.\n15\nBBF: Human-level Atari with human-level efficiency\nFigure A.2: BBF at RR=2 on the 26 Atari 100k tasks, with and without Noisy Nets.\n16\n"
  },
  {
    "title": "Blockwise Parallel Transformer for Long Context Large Models",
    "link": "https://arxiv.org/pdf/2305.19370.pdf",
    "upvote": "2",
    "text": "Blockwise Parallel Transformer\nfor Large Context Models\nHao Liu\nUC Berkeley\nhao.liu@cs.berkeley.edu\nPieter Abbeel\nUC Berkeley\npabbeel@cs.berkeley.edu\nAbstract\nTransformers have emerged as the cornerstone of state-of-the-art natural language\nprocessing models, showcasing exceptional performance across a wide range of AI\napplications. However, the memory demands posed by the self-attention mecha-\nnism and the large feedforward network in Transformers limit their ability to handle\nlong sequences, thereby creating challenges for tasks involving multiple long se-\nquences or long-term dependencies. We present a distinct approach, Blockwise\nParallel Transformer (BPT), that leverages blockwise computation of self-attention\nand feedforward network fusion to minimize memory costs. By processing longer\ninput sequences while maintaining memory efficiency, BPT enables training se-\nquences 32 times longer than vanilla Transformers and up to 4 times longer than\nprevious memory-efficient methods. Extensive experiments on language modeling\nand reinforcement learning tasks demonstrate the effectiveness of BPT in reducing\nmemory requirements and improving performance.\n1\nIntroduction\nTransformers [52] have become the backbone of many state-of-the-art natural language processing\nmodels [15, 43, 5, 35]. They have demonstrated impressive performance across a wide range of\nAI problems, including language modeling, machine translation, image captioning, and protein\nfolding [39, 47, 32, 43, 5, 45, 9]. Transformers achieve this success through their architecture design\nthat uses self-attention and position-wise feedforward mechanisms. These components facilitate the\nefficient capture of long-range dependencies between input tokens, enabling scalability in terms of\ncontext length and model size through highly parallel computations.\nHowever, the memory requirements of Transformers limit their ability to handle long sequences,\nwhich is necessary for many AI problems, such as high-resolution images, podcasts, code, or books\nand especially those that involve multiple long sequences or long-term dependencies [10, 7, 39, 7, 34,\n29, 47, 32, 1]. The quadratic self-attention and the large feed forward network of Transformers require\na large amount of memory, which makes it challenging to scale to longer input sequences. This\nlimitation has led to various techniques proposed to reduce the memory requirements of Transformers,\nincluding sparse-approximation, low-rank approximation, and low precision approximation [see e.g.\n51, 24, 22, 11, 25, 36, 54].\nOne distinct line of research does not rely on approximation but instead focuses on computing exact\nself-attention with linear memory complexity. This approach leverages the observation that the\nsoftmax matrix in self-attention can be computed without materializing the full matrix [37]. This\ntechnique has led to the development of FlashAttention [14] and Memory Efficient Attention [42].\nBoth methods propose a blockwise computation of the self-attention softmax, demonstrating reduced\nmemory requirements.\nPreprint.\narXiv:2305.19370v3  [cs.CL]  28 Aug 2023\nFigure 1: Maximum context length during training time with the GPT model using different methods.\nModel sizes range from 1B to 70B. Figures (A), (B), and (C) show evaluation using one, eight A100,\nand 64 TPUv4, respectively, with a single sequence. Our method enables training sequences 32 times\nlonger than vanilla attention-based Transformer [52], and 2 to 4 times longer than FlashAttention\n[14] and Memory Efficient Attention [42]. Section 3.1 provides a memory cost breakdown.\nDespite the resulting reduced memory requirements of the self-attention block in Transformer models,\na significant challenge still arises from the feedforward network. This network contains a large\nnumber of parameters and produces high-dimensional intermediate vectors, resulting in substantial\nmemory requirements. This issue is becomes the key memory challenge once employing memory-\nefficient attention mechanisms. Consequently, training Transformers on longer context lengths and\nscaling up Transformer models become significantly hindered due to the overwhelming memory\ndemands imposed by the feedforward network.\nTo address this challenge, we make an important observation: when self-attention is computed in a\nblockwise manner to reduce memory requirements, it becomes feasible to merge the computation\nof the feedforward network. This eliminates the need to wait for the self-attention computation to\nfinish before performing the feedforward step on the entire sequence. By computing the feedforward\nnetwork on a block-by-block basis, we effectively reduce the memory cost associated with the\nfeedforward network. This process involves the utilization of two nested loops over the input\nsequence blocks. In the outer loop, we iterate over each block and compute the query. In the inner\nloop, we iterate over each block to calculate the key and value. These key-value pairs, along with the\nquery, are then used to compute the blockwise attention specific to the corresponding input block.\nThis blockwise attention is subsequently used to calculate the output of the feedforward network,\nfollowed by a residual connection. This approach enables us to process longer input sequences while\nmaintaining lower memory budget. Since our approach performs blockwise parallel computation and\nfuses the feedforward and self-attention computations, we name our method the Blockwise Parallel\nTransformer (BPT).\nWe evaluate the effectiveness of our approach on several benchmarks, including language modeling\nand reinforcement learning. Our experiments show that BPT can reduce the memory requirements\nof Transformers, enabling us to train 32 times longer sequence than vanilla attention [52] based\nGPT models and up to 4 times longer sequence than prior state-of-the-arts FlashAttention [14] and\nMemory Efficient Attention [42]. Furthermore, we demonstrate the application of BPT on the task\nof traning Transformer based RL agent. By conditioning on multiple trajectories, BPT significantly\nimproves the performance and achieves better results on challenging RL benchmarks. We believe\nthat our approach has the potential to enable the training and evaluation of more complex models that\nrequire longer input sequences, which could lead to further breakthroughs in AI research.\nOur contributions are twofold: (a) proposing a blockwise computation of self-attention and feedfor-\nward approach that enables 32 times longer and up to 4 times longer context lengths than vanilla\nTransformer and previous memory-efficient Transformers, respectively, and (b) demonstrating the\neffectiveness of our approach through extensive experiments.\n2\nMemory Bottleneck of Transformer\nGiven input sequences Q, K, V \u2208 Rs\u00d7d where s is the sequence length and d is the head dimension.\nWe compute the matrix of outputs as:\nAttention(Q, K, V ) = softmax(QKT\n\u221a\nd\n)V,\n(1)\n2\nwhere softmax is applied row-wise. Standard attention implementations materialize the matrices\nQKT and softmax( QKT\n\u221a\nd ) to HBM, which takes O(s2) memory, so the overall space complexity\nis O(s2). There has been a large body of work trying to reduce memory usage of self-attention by\nusing online softmax [37, 42, 14] to reduce memory cost of self-attention by preventing it from full\nmaterialization. And these approaches reduce memory footprint from O(s2) to O(s). However, the\nlarge feedforward layers have been overlooked.\nIn addition to attention sub-layers, each of the attention layers is accomplished with a fully connected\nfeedforward network, which is applied to each position separately and identically. This consists of\ntwo linear transformations with a ReLU activation in between.\nFFN(x) = max(0, xW1 + b1)W2 + b2\n(2)\nWhile the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. The large size of the feedforward network requires substantial memory resources,\nand this becomes even more pronounced when dealing with large context sizes. See Section 3.1 for\nanalysis of memory cost associated with transformers.\n3\nBlockwise Parallel for Large Context Models\nSelf-attention can be computed in a blockwise manner without materializing the softmax attention\nmatrix softmax(QKT ) [37, 14, 42]. This approach involves splitting the sequences Q \u2208 Rs\u00d7d\ninto Bq blocks and sequences K, V \u2208 Rs\u00d7d into Bkv blocks. For each query block, the blockwise\nattention Attention(Q, K, V ) can be computed by iterating over all key-value blocks. Once the\nblockwise attention is computed, the global attention matrix can be obtained by scaling the blockwise\nattention using the difference between the blockwise and global softmax normalization constants [37].\nThis is achieved by keeping track of normalization statistics and combining them from all blocks to\nscale each block accordingly. For a specific query block Qi, 1 \u2264 i \u2264 Bq, the corresponding attention\noutput can be computed by scaling each blockwise attention as follows:\nAttention(Qi, K, V ) = Scaling({exp(QiKT\nj )Vj}Bkv\nj=1).\n(3)\nThe scaling operation scales each blockwise attention based on the difference between the blockwise\nmaximum and the global maximum:\nAttention(Qi, Kj, Vj) = exp\n\u0000QiKT\nj \u2212 max(QiKT\nj )\n\u0001\n/\nX\nexp\n\u0000QiKT\nj \u2212 max(QiKT\nj )\n\u0001\nmaxi = max\n\u0000max(QiKT\n1 ), . . . , max(QiKT\nB)\n\u0001\nAttention(Qi, K, V ) =\n\u0002\nexp(QiKT\nj \u2212 maxi) Attention(Qi, Kj, Vj)\n\u0003Bkv\nj=1.\nThis blockwise self-attention computation eliminates the need to materialize the full attention matrix\nof size O(n2), resulting in significant memory savings.\nWe observe that the blockwise computation is not limited to self-attention but can also be applied\nto the feedforward network. For each query block, after iterating over the key and value blocks, the\nfeedforward network can be computed along with a residual connection, completing the attention and\nfeedforward network computation for that query block. This means that the model does not need to\ncompute the feedforward network on the full sequence, but rather on intermediate blocks, resulting in\nmemory savings. The computation for a query block is given by:\nOutputi = FFN\n\u0000Attention(Qi, K, V ) + Qi\n\u0001\n+ Attention(Qi, K, V ) + Qi.\nTherefore, the output for each block consists of the feedforward network, self-attention, and residual\nconnection computed in a blockwise manner.\nIt is worth mentioning that for large models, the memory cost of the feedforward network on the\nfull sequence can be much larger than the memory efficient attention. Therefore computing the\nfeedforward network on the same block as attention can significantly reduce memory cost, and it\nalso reduces data movements, contributing to overall computational efficiency. Moreover, we should\nremark that blockwise parallelism can be directly applied to the final cross entropy loss, which can\nfurther minimize memory cost. The full process of our framework, coined as BPT, is summarized in\nAlgorithm 1.\n3\nFigure 2: We use the same model architecture as the original Transformer but with a different way of\norganizing the compute. In the diagram, we explain this by showing that for the bottom first incoming\ninput block, we project it into query; then we iterate over the same input sequence positioned above\nthe bottom row, and project it to key and value. These query, key and value are used to compute\nself-attention (yellow box), whose output is pass to feedforward network (cyan box), followed by a\nresidual connection. In our proposed approach, this process is then repeated for the other incoming\ninput blocks.\n3.1\nMemory Cost\nWe present an analysis of memory cost across different transformer architectures: the Vanilla\nTransformer, the memory-efficient / Flash Attention variant, and BPT.\nVanilla Transformer:\nAttention: For Q, K, V , saving their input x needs 2bsh bytes, where b is batch size, s is sequence\nlength, and h is hidden dimension. For QKT matmul, saving activations Q and K needs 4bsh bytes.\nFor softmax(QKT ), saving input QKT needs 2bs2a bytes, where a is the number of attention heads.\nFor mask and dropout, saving mask needs bs2a bytes. For score \u00d7 V , saving score needs 2bs2a\nbytes, and saving V needs 2bsh bytes. For output projection and dropout, saving the input needs\n4\nAlgorithm 1 Reduce memory cost with BPT.\nRequired: Input sequence x. Number of query blocks Bq. Number of key and value blocks Bkv.\nInitialize\nProject input sequence x into query, key and value.\nSplit query sequence into Bq of query input blocks.\nSplit key and value sequences into Bkv of key-value input blocks.\nfor outer = 1 to Bq do\nChoose the outer-th query.\nfor inner = 1 to Bkv do\nChoose the inner-th key and inner-th value block.\nCompute attention using query, key and value, and record normalization statistics.\nend for\nCombine each blocks by scaling them to get attention output for the outer-th input block.\nCompute feedforward on attention output and add residual connection.\nend for\n2bsh bytes, and saving dropout mask needs bsh bytes. The maximum attention activation size of\nattention is O(s2) with checkpointing.\nFFN: For the first linear layer, saving input needs 2bsh bytes. For activation, saving input needs 8bsh\nbytes. For the second linear layer, saving input needs 8bsh bytes. For dropout, saving the mask needs\nbsh bytes. With checkpointing, the maximum activation size of FFN is 8bsh bytes.\nConsequently, for a large context length, the memory cost of activation in vanilla Transformer is\nO(s2).\nBPT:\nAttention: Since BPT does not materialize full attention and instead computes it blockwise, it needs\nto store intermediate blockwise activations in the key-value loop, which has a maximum activation\nsize of 4bch with checkpointing. Additionally, it needs to store q output activations for the query\nloop, which requires 2bsh bytes. Since s \u226b c, the maximum activation size is 2bsh.\nFFN: When iterating the FFN over blocks, BPT needs to save the following activations: For the first\nlinear layer, saving input needs 2bch bytes. For activation, saving input needs 8bch bytes. For the\nsecond linear layer, saving input needs 8bch bytes. For dropout, saving the mask needs bch bytes. In\ntotal, 19bch bytes are needed. Additionally, storing the output of the for loop requires 2bsh bytes.\nTherefore, the maximum FFN activation size is 2bsh.\nConsequently, each BPT layer\u2019s memory cost of activation is 2bsh.\nMemory-Efficient / Flash Attention:\nAttention: Similar to BPT attention, the maximum activation size is 2bsh.\nFFN: Similar to the vanilla FFN, the maximum activation size is 8bsh.\nConsequently, each Flash Attention layer\u2019s memory cost is 8bsh.\nComparing the activation memory of Flash Attention/Memory-Efficient Transformer with BPT, we\nsee that BPT offers 8bsh/2bsh = 4 times memory saving. By taking into account other factors\nof memory cost such as model parameters and optimizer states, BPT allows training with context\nlengths 2-4 times larger than prior state-of-the-arts.\n3.2\nWhy Blockwise Parallel\nThe utilization of blockwise parallelization may raise questions about the effectiveness of running\nparallel computers, as computation can become sequential between blocks. However, the benefits of\nblockwise parallelization depend on the model size and hardware configuration. In cases where the\nmodel is large or the context length is extremely long, a block may reach its maximum arithmetic\ndensity, making it impractical to execute the original full-length sequence in parallel. In such\nscenarios, blockwise parallelization treats the long sequence as short ones, allowing dealing with\nlarge models and effectively enabling large context size. Moreover, using blockwise parallelization\n5\nallows us to avoid waiting for the completion of self-attention and allocating a significant amount of\nmemory solely for feed-forward network computation.\nAnother notable advantage of blockwise parallelization is its ability to leverage hardware with\nsignificantly faster SRAM speed compared to HBM speed. For instance, in Nvidia GPUs, SRAM is\nan order of magnitude faster than HBM, while in Google TPUs, SRAM also offers higher speed than\nHBM. By utilizing blockwise parallelization, we can tap into the increased speed of SRAM, thereby\nreducing communication costs and increasing throughput. This advantage aligns with memory\nefficient self-attention approaches [14, 42].\n3.3\nImplementation\nAlgorithm 1 provides the pseudocode of the algorithm. Figure 3 in Appendix shows a Jax imple-\nmentation optimized for simplicity. The full code of BPT is provided at GitHub 1 which supports\nlarge-scale distributed training of large context models using BPT.\nThe blockwise_ffn function begins by accepting a rematerialized feed forward module, inputs\nand chunk size. The remat_ffn compute feedforward on inputs with checkpointing, i.e.without\nsaving intermediates. The scan_ffn function is then used to scan over input sequences and generate\noutputs.\nThe blockwise_attn function process query, key, and value to produce attention blockwise. The\nscan_attention function is defined, which computes the attention weights between the query\nvector and key-value pairs from another chunk. This is done by applying the scan_kv_block\nfunction to the key-value chunk, calculating the dot product between the query and key vectors,\nand then adding a bias term. The bias term introduces a positional bias between different chunks\nbased on their indices without materializing the full matrix. The softmax function is then applied\nto the attention weights in a numerically stable manner, using the max-score trick to avoid large\nexponentiation results.\nFinally, BPT combines the outputs from all chunks, normalizes them using their max-score-adjusted\nweights, and passes them through a feed-forward neural network (blockwise_ffn). The final output\nis the sum of the feed-forward output, the attention output, and the original input.\n4\nSetting\nWe evaluate the impact of using BPT in improving large Transformer models by benchmarking\nmemory requirement, maximum sequence length and throughout speed. We show apply BPT to\nreinforcement learning as an application.\nModel Configuration. Our study is built upon the GPT architecture. Table 1 provides a overview of\nthe model sizes considered in our experiments.\nBaselines. We evaluate our method by comparing it with vanilla Transformer [52] which is denoted\nas \u201cVanilla\u201d, and FlashAttention [14] and Memory Efficient Attention [42] which are state-of-the-art\nmemory efficient attention, we denote them as \u201cMemoryEfficient\u201d in our experiments. All methods\nuse the same gradient checkpointing in the experiments.\nDatasets. We consider two datasets for evaluation purposes. Including pretraining on OpenWebText\ndataset and large context reinforcement learning on ExoRL.\n\u2022 OpenWebText. The OpenWebText dataset [18] is a large and diverse collection of web pages\nthat has been filtered and cleaned for use in natural language processing (NLP) tasks. The dataset\nconsists of over 6 billion tokens from more than 40 million web pages, covering a wide range of\ntopics and genres.\n\u2022 ExoRL. The ExoRL [56] dataset is based on unlabeled exploratory data collected by running\nunsupervised RL algorithms. For each environment, it comes with eight different unsupervised\ndata collection algorithms, taken from from URLB [28]. The datasets are collected by unsuper-\nvised RL and then relabeled using task reward function. The resulting mixed dataset consists of 8\nmillions timesteps (8000 episodes), with each episode spanning a length of 1000 steps.\n1https://github.com/lhao499/llm_large_context\n6\nTable 1: Sizes and architectures of the models which we evaluated in experiments.\nModel Name\nnparams\nnlayers\ndmodel\nnheads\ndhead\nGPT 1B\n1.3B\n24\n2048\n16\n128\nGPT 3B\n2.7B\n32\n2560\n32\n80\nGPT 7B\n6.7B\n32\n4096\n32\n128\nGPT 13B\n13.0B\n40\n5140\n40\n128\nGPT 30B\n30.0B\n48\n7168\n56\n128\nGPT 70B\n70.0B\n80\n8192\n64\n128\nTraining Configuration. Our main baselines are vanilla attention [52], which computes self-attention\nby materializing the attention matrix and computes the feedforward network normally. We also\nconsider two prior state-of-the-art memory-efficient methods, namely FlashAttention [14], which\nfocuses on GPU efficiency, and Memory Efficient Attention [42], which focuses on TPU efficiency.\nSince they share a similar idea, for notation simplicity, we refer to them as FlashAttention in our\nexperiments. We tune the block size for both the baselines and BPT, and report the best results\nachieved by each. The experiments are on NVIDIA 80GB A100 GPUs, we consider both single GPU\nfor smaller model training and 8 GPUs settings for model parallel training. We also experiment with\nscaling up model on 64 TPUv4.\nWe note that no data parallelism is considered in our evaluations since our approach is independent\nof data parallelism. As a result, the batch sizes used in our analysis are much lower than the ones\nused for the end-to-end training. All of our results are obtained using full precision instead of mixed\nprecision.\n5\nResults\nIn our experiments, our primary objective is to comprehensively evaluate the performance of BPT\nacross multiple key metrics, including maximum sequence length, memory usage, and throughput.\nMoreover, we extend the applicability of BPT to reinforcement learning and evaluate its effectiveness\nin large context application.\nTable 2: Maximum context length during training with different methods. BPT enables training 2-4\ntimes longer sequence length than FlashAttention / Memory Efficient Attention, and up to 32 times\nlonger sequence length than vanilla attention.\n1 A100\nPartitionSpec\nVanilla Attention\nMemoryEfficient\nBlockwise Parallel\n350M\n(1,1,1)\n16K (16384)\n65K (65536)\n131K (131072)\n1B\n(1,1,1)\n16K (16384)\n65K (65536)\n131K (131072)\n3B\n(1,1,1)\n8K (8192)\n16K (16384)\n65K (65536)\n8 A100\nPartitionSpec\nVanilla Attention\nMemoryEfficient\nBlockwise Parallel\n3B\n(1,1,8)\n16K (16384)\n65K (65536)\n131K (131072)\n7B\n(1,1,8)\n16K (16384)\n65K (65536)\n131K (131072)\n13B\n(1,1,8)\n8K (8192)\n33K (32768)\n65K (65536)\n30B\n(1,1,8)\n8K (8192)\n16K (16384)\n65K (65536)\n64 TPUv4\nPartitionSpec\nVanilla Attention\nMemoryEfficient\nBlockwise Parallel\n13B\n(1,1,64)\n4K (4096)\n16K (16384)\n33K (32768)\n30B\n(1,1,64)\n2K (2048)\n4K (4096)\n16K (16384)\n70B\n(1,1,64)\n1k (1024)\n2K (2048)\n8K (8192)\n5.1\nEvaluation of Context Length\nWe present experimental results comparing the maximum training sequence lengths achieved using\nthree different attention mechanisms: Vanilla, MemoryEfficient, and Blockwise Parallel. Table 2\nsummarizes the findings. On one A100 GPU, Vanilla Transformer supports a maximum training\nsequence length of 16K for 1B parameters and 8K for 3B parameters. In contrast, MemoryEfficient\n7\nenables longer sequences of 65K for 1B parameters and 16K for 3B parameters. Notably, our\nproposed method, Blockwise Parallel, surpasses both methods, achieving a maximum sequence\nlength of 131K for 1B parameters and 3B parameters. Moving on larger models, Blockwise Parallel\nagain outperforms the other two methods, allowing training sequences of 65K for 30B large model\non 8 GPUs and 8K for 70B large model on 64 TPUv4, which are two and four times longer than\nMemoryEfficient, respectively.\nTable 3 shows the analysis of memory usage across different settings with three distinct approaches:\nVanilla Transformer, MemoryEfficient, and our proposed method, BPT. It is evident that Vanilla\nTransformer consumes the highest amount of memory, while MemoryEfficient and BPT offer notable\nimprovements in memory optimization. Notably, our BPT technique consistently outperforms both\nVanilla Transformer and MemoryEfficient in all settings, showcasing memory efficiency.\nTable 3: Memory usage comparison for different settings. \"oom\" denotes out of memory.\nSetting\n3B on A100\n13B on 8 A100\nContext\nLength\nVanilla\nMemoryEfficient\nBPT\nVanilla\nMemoryEfficient\nBPT\n8192\n64GB\n44GB\n43GB\n59GB\n44GB\n42GB\n16384\noom\n47GB\n45GB\noom\n46GB\n45GB\n32768\noom\n55GB\n52GB\noom\n55GB\n52GB\n65536\noom\n75GB\n70GB\noom\n75GB\n68GB\n131072\noom\noom\n79GB\noom\noom\n78GB\n5.2\nEvaluation on Throughput and Speed\nIn Table 4, we present a comparison of the throughput achieved by different attention mechanisms on\nthe GPT-XL (1B) model trained on the OpenWebText dataset using 8 GPUs. Throughput is measured\nas number of tokens processed per device per second. We evaluate the performance at various context\nlengths, including 2K, 8K, 16K, 33K, and 65K tokens. Our proposed method achieves competitive\nthroughput as MemeoryEfficient mechanism, and surpasses the Vanilla transformer, achieving 1.17x\nspeedup at context length 8k and 1.2x speedup at context length 16k. At context length 32K and\n64K, our method maintains high throughput and training speed, while the alternatives cannot train\ndue to running out of memory. This demonstrates the scalability and efficiency of our proposed\nmethod, allowing it to effectively handle large context lengths without compromising on throughput\nand training speed.\n5.3\nEvaluation on Reinforcement Learning\nIn this section, we present the results of applying BPT to improve the performance of Transformer in\nreinforcement learning (RL). We report our results in Table 5, where we evaluate our proposed model\non the ExoRL benchmark across six different tasks. On ExoRL, we report the cumulative return, as\nper ExoRL [56]. The numbers of BC, DT [6] and AT [33] are from the ExoRL and AT paper. AT +\nME and AT + BPT numbers are run by ourselves. Since the ExoRL data is significantly more diverse\nthan D4RL because it is collected using unsupervised RL [28], it is found that TD learning performs\nbest while behavior cloning struggles [56]. AT [33] shows that conditioning Transformer on multiple\ntrajectories with relabeled target return can significantly outperforms behavior cloning approaches\nBC-10% and DT, and achieves competitive results with TD learning. For more details, please refer to\ntheir papers. We are interested in applying BPT to improve the performance of AT by conditioning on\na 64 trajectories rather than 4 trajectories in original work. It is worth noting that each trajectory has\n1000 \u00d7 4 length where 1000 is sequence length while 4 is return-state-action-reward, making training\n64 trajectories with 350M size model infeasible for both Vanilla and MemoryEfficient. Results in\nTable 5 show that, by scaling the sequence length, AT + BPT consistently outperforms the original\nTransformer model in all six tasks, achieving a total average return of 155.36 compared to the original\nTransformer model\u2019s total average return of 120.65.\n8\nTable 4: Throughput comparison on GPT-XL (1B) using OpenWebText dataset. Throughput is\nmeasured as tokens processed per second. \u2018oom\u2019 denotes running out of memory, \u2018na\u2019 denotes results\nnot available because we early terminated these runs to reduce compute cost.\nModel\nContext Len\nVal Loss\nThroughput\nSpeed up\nVanila Transformer\n2048\n2.46\n3827\n1x\nMemoryEfficient\n2048\n2.46\n4371\n1.14x\nBlockwise Parallel\n2048\n2.46\n3985\n1.04x\nVanila Transformer\n4096\n2.44\n2340\n1x\nMemoryEfficient\n4096\n2.44\n2567\n1.1x\nBlockwise Parallel\n4096\n2.44\n2687\n1.15x\nVanila Transformer\n8192\n2.43\n2455\n1x\nMemoryEfficient\n8192\n2.43\n2781\n1.13x\nBlockwise Parallel\n8192\n2.43\n2875\n1.17x\nVanila Transformer\n16384\n2.41\n1701\n1x\nMemoryEfficient\n16384\n2.41\n1889\n1.11x\nBlockwise Parallel\n16384\n2.41\n2045\n1.2x\nVanila Transformer\n32768\noom\noom\noom\nMemoryEfficient\n32768\nna\n810\n1x\nBlockwise Parallel\n32768\nna\n857\n1.1x\nVanila Transformer\n65536\noom\noom\noom\nMemoryEfficient\n65536\noom\noom\noom\nBlockwise Parallel\n65536\nna\n600\n1x\nTable 5: Application of BPT on improving Transformer in RL. All the baselines use vanilla attention.\nAT + ME denotes using \u201cMemoryEfficient\u201d. AT + BPT denotes using Blockwise Parallel.\nExoRL benchmark\nBC-10%\nDT\nAT\nAT\nAT + ME\nAT + BPT\nTask\nN Trajs = 4\nN Trajs = 32\nN Trajs = 32\nN Trajs = 32\nWalker Stand\n52.91\n34.54\n68.55\noom\noom\n95.45\nWalker Run\n34.81\n49.82\n88.56\noom\noom\n105.88\nWalker Walk\n13.53\n34.94\n64.56\noom\noom\n78.56\nCheetah Run\n34.66\n67.53\n125.68\noom\noom\n178.75\nJaco Reach\n23.95\n18.64\n52.98\noom\noom\n87.56\nCartpole Swingup\n56.82\n67.56\n97.81\noom\noom\n120.56\nTotal Average\n36.11\n45.51\n83.02\noom\noom\n111.13\n6\nRelated Work\nTransformers have garnered significant attention in the field of natural language processing (NLP) and\nhave become the basis for numerous state-of-the-art models. Several works have explored memory-\nefficient techniques to address the memory limitations of Transformers and enable their application\nto longer input sequences. One line of research focuses on various approximation techniques or\ncompressing along the sequence dimension [see e.g. 24, 12, 14, 4, 42, 54, 36, 25]. Other works\nexplored replacing attention [19, 20, 41, 23, 3, 57, 40, 53]. Another line of work explores partitioning\nthe large hidden dimension of the feedforward network into parts and retrieving only one part per\ntoken [30, 48, 17, 26, 58, 60]. Additionally, extending the context by attending over states from\nprevious sequences has been explored [13, 44], as well as combining local and global contexts [21, 11].\nFor a comprehensive review of these techniques, we recommend referring to the surveys by Tay et al.\n[51], Narang et al. [38], Tay et al. [50]. Several studies explored sharding large model on distributed\ndevices tensor, data, or sequence parallelism [49, 16, 55, 27, 59, 31, 46]. Ours shares similarities\nwith the sequence parallelism [27] where sequences are distributed across devices, in contrast,\nours implements blockwise computation on sequences for each device. This creates an orthogonal\nrelationship between our method and sequence parallelism, allowing for straightforward combination.\nIn addition, our methodology is compatible with both tensor and data parallelism. Another direction\n9\ninvolves computing exact self-attention in a blockwise manner using the tiling technique [37]. This\napproach has led to the development of memory efficient attention mechanisms [14, 42]. In line with\nthese advancements, our work falls into this category. We propose computing both the feedforward\nnetwork and self-attention in a blockwise manner, resulting in a significant reduction in memory\nrequirements.\n7\nConclusion\nIn conclusion, we propose a blockwise parallelization approach to reduce the memory requirements\nof Transformers, the backbone of state-of-the-art NLP models. Our approach enables processing\nlonger input sequences while maintaining or improving performance. Through extensive experiments,\nwe demonstrate its effectiveness, achieving up to 4x memory reduction than memory-efficient Trans-\nformers. Our contributions include a practical method for large context sizes in large Transformer\nmodels. With the increasing capability of hardware, larger models and longer context length are\nwidely used in AI research. At the same time, as we are pushing up against physics and fabrication\nlimits, it is more important to design scaling approaches as efficient as possible to scale up large\nmodels and large context size. Our approach holds promise for training and evaluating complex\nmodels with longer input sequences, potentially driving new breakthroughs in machine learning\nresearch.\nLimitations and Future Work. Although our method achieves state-of-the-art low memory usage\nfor Transformer models, it does have some limitations that need to be addressed:\n\u2022 Optimal performance. While our implementation prioritizes simplicity with high-level Jax\noperations, optimizing low-level operations is crucial for achieving optimal performance. In\nfuture work, we suggest considering porting our method to CUDA and OpenAI Triton to achieve\nminimal memory cost and maximum speedup.\nAcknowledgements\nThis project is supported in part by ONR under N00014-21-1-2769. We thank the members of the\nBerkeley Robot Learning Lab and Berkeley AI Lab for their valuable discussions. We thank Tri\nDao at Stanford for the valuable discussions on strengthening BPT. We thank Google TPU Research\nCloud for granting us access to TPUs.\nWe also express our appreciation to Anselm Levskaya, Markus Rabe, Federico Lebron, and Sharad\nVikram at Google for their insightful discussions and suggestions on optimizing large transformers.\nIn particular, we thank Anselm for his discussions on reducing memory cost, XLA, and training large\nmodels. We also appreciate the valuable suggestions on optimizing memory efficient transformers\nprovided by Markus and Federico, as well as the valuable discussions with Sharad on implementing\nBPT with Triton and Jax Pallas.\n10\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,\nKarel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual\nlanguage model for few-shot learning. Advances in Neural Information Processing Systems, 35:\n23716\u201323736, 2022.\n[2] Kapathy Andrej.\nGitHub - karpathy/nanoGPT: The simplest, fastest repository for train-\ning/finetuning medium-sized GPTs. \u2014 github.com. https://github.com/karpathy/nanoGPT,\n2023. [Accessed 16-May-2023].\n[3] Irwan Bello. Lambdanetworks: Modeling long-range interactions without attention. arXiv\npreprint arXiv:2102.08602, 2021.\n[4] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\narXiv preprint arXiv:2004.05150, 2020.\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\n[6] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter\nAbbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning\nvia sequence modeling. Advances in neural information processing systems, 34:15084\u201315097,\n2021.\n[7] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\nlanguage models trained on code. arXiv preprint arXiv:2107.03374, 2021.\n[8] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear\nmemory cost. arXiv preprint arXiv:1604.06174, 2016.\n[9] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz,\nSebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-scaled\nmultilingual language-image model. arXiv preprint arXiv:2209.06794, 2022.\n[10] Xinyun Chen, Maxwell Lin, Nathanael Sch\u00e4rli, and Denny Zhou. Teaching large language\nmodels to self-debug. arXiv preprint arXiv:2304.05128, 2023.\n[11] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with\nsparse transformers. arXiv preprint arXiv:1904.10509, 2019.\n[12] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane,\nTamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking\nattention with performers. arXiv preprint arXiv:2009.14794, 2020.\n[13] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov.\nTransformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint\narXiv:1901.02860, 2019.\n[14] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and\nmemory-efficient exact attention with io-awareness. Advances in Neural Information Processing\nSystems, 35:16344\u201316359, 2022.\n[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,\n2018.\n[16] Facebook. Fully Sharded Data Parallel: faster AI training with fewer GPUs \u2014 engineer-\ning.fb.com.\nhttps://engineering.fb.com/2021/07/15/open-source/fsdp/, 2023.\n[Accessed 16-May-2023].\n11\n[17] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion\nparameter models with simple and efficient sparsity. The Journal of Machine Learning Research,\n23(1):5232\u20135270, 2022.\n[18] Aaron Gokaslan and Vanya Cohen. Openwebtext corpus, 2019. URL http://Skylion007.\ngithub.io/OpenWebTextCorpus.\n[19] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Hippo: Recurrent memory\nwith optimal polynomial projections. Advances in neural information processing systems, 33:\n1474\u20131487, 2020.\n[20] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured\nstate spaces. arXiv preprint arXiv:2111.00396, 2021.\n[21] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in\nmultidimensional transformers. arXiv preprint arXiv:1912.12180, 2019.\n[22] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nLu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021.\n[23] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Transformer quality in linear time. In\nInternational Conference on Machine Learning, pages 9099\u20139117. PMLR, 2022.\n[24] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao\nCarreira. Perceiver: General perception with iterative attention. In International conference on\nmachine learning, pages 4651\u20134664. PMLR, 2021.\n[25] Nikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer.\narXiv preprint arXiv:2001.04451, 2020.\n[26] Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa,\nJoshua Ainslie, Yi Tay, Mostafa Dehghani, and Neil Houlsby. Sparse upcycling: Training\nmixture-of-experts from dense checkpoints. arXiv preprint arXiv:2212.05055, 2022.\n[27] Vijay Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Moham-\nmad Shoeybi, and Bryan Catanzaro. Reducing activation recomputation in large transformer\nmodels. arXiv preprint arXiv:2205.05198, 2022.\n[28] Michael Laskin, Denis Yarats, Hao Liu, Kimin Lee, Albert Zhan, Kevin Lu, Catherine Cang,\nLerrel Pinto, and Pieter Abbeel. Urlb: Unsupervised reinforcement learning benchmark. arXiv\npreprint arXiv:2110.15191, 2021.\n[29] Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer, Richie Steiger-\nwald, DJ Strouse, Steven Hansen, Angelos Filos, Ethan Brooks, et al. In-context reinforcement\nlearning with algorithm distillation. arXiv preprint arXiv:2210.14215, 2022.\n[30] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with condi-\ntional computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.\n[31] Shenggui Li, Jiarui Fang, Zhengda Bian, Hongxin Liu, Yuliang Liu, Haichen Huang, Boxiang\nWang, and Yang You. Colossal-ai: A unified deep learning system for large-scale parallel\ntraining. arXiv preprint arXiv:2110.14883, 2021.\n[32] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond,\nTom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code\ngeneration with alphacode. Science, 378(6624):1092\u20131097, 2022.\n[33] Hao Liu and Pieter Abbeel. Emergent agentic transformer from chain of hindsight experience.\nInternational Conference on Machine Learning, 2023.\n[34] Hao Liu, Carlo Sferrazza, and Pieter Abbeel. Chain of hindsight aligns language models with\nfeedback. arXiv preprint arXiv:2302.02676, 2023.\n12\n[35] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\n[36] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan\nMay, and Luke Zettlemoyer. Mega: moving average equipped gated attention. arXiv preprint\narXiv:2209.10655, 2022.\n[37] Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for softmax. arXiv\npreprint arXiv:1805.02867, 2018.\n[38] Sharan Narang, Hyung Won Chung, Yi Tay, William Fedus, Thibault Fevry, Michael Matena,\nKarishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, et al. Do transformer modifi-\ncations transfer across implementations and applications? arXiv preprint arXiv:2102.11972,\n2021.\n[39] OpenAI. Gpt-4 technical report, 2023.\n[40] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan\nPascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. arXiv\npreprint arXiv:2303.06349, 2023.\n[41] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua\nBengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional\nlanguage models. arXiv preprint arXiv:2302.10866, 2023.\n[42] Markus N Rabe and Charles Staats. Self-attention does not need o(n2) memory. arXiv preprint\narXiv:2112.05682, 2021.\n[43] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\nLanguage models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[44] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive\ntransformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019.\n[45] Roshan M Rao, Jason Liu, Robert Verkuil, Joshua Meier, John Canny, Pieter Abbeel, Tom Sercu,\nand Alexander Rives. Msa transformer. In International Conference on Machine Learning,\npages 8844\u20138856. PMLR, 2021.\n[46] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System\noptimizations enable training deep learning models with over 100 billion parameters. In\nProceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &\nData Mining, pages 3505\u20133506, 2020.\n[47] Kiersten M Ruff and Rohit V Pappu. Alphafold and implications for intrinsically disordered\nproteins. Journal of Molecular Biology, 433(20):167208, 2021.\n[48] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538, 2017.\n[49] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan\nCatanzaro. Megatron-lm: Training multi-billion parameter language models using model\nparallelism. arXiv preprint arXiv:1909.08053, 2019.\n[50] Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, William Fedus, Jinfeng Rao,\nSharan Narang, Vinh Q Tran, Dani Yogatama, and Donald Metzler. Scaling laws vs model\narchitectures: How does inductive bias influence scaling? arXiv preprint arXiv:2207.10551,\n2022.\n[51] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey.\nACM Computing Surveys, 55(6):1\u201328, 2022.\n13\n[52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\n[53] Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without\nattention. arXiv preprint arXiv:2212.10544, 2022.\n[54] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention\nwith linear complexity. arXiv preprint arXiv:2006.04768, 2020.\n[55] Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake Hechtman, Yanping Huang, Rahul Joshi,\nMaxim Krikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni, et al. Gspmd: general and\nscalable parallelization for ml computation graphs. arXiv preprint arXiv:2105.04663, 2021.\n[56] Denis Yarats, David Brandfonbrener, Hao Liu, Michael Laskin, Pieter Abbeel, Alessandro\nLazaric, and Lerrel Pinto. Don\u2019t change the algorithm, change the data: Exploratory data for\noffline reinforcement learning. arXiv preprint arXiv:2201.13425, 2022.\n[57] Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang,\nand Josh Susskind. An attention free transformer. arXiv preprint arXiv:2105.14103, 2021.\n[58] Zhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. Moefication:\nTransformer feed-forward layers are mixtures of experts. arXiv preprint arXiv:2110.01786,\n2021.\n[59] Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping Huang,\nYida Wang, Yuanzhong Xu, Danyang Zhuo, Eric P Xing, et al. Alpa: Automating inter-and\n{Intra-Operator} parallelism for distributed deep learning. In 16th USENIX Symposium on\nOperating Systems Design and Implementation (OSDI 22), pages 559\u2013578, 2022.\n[60] Simiao Zuo, Qingru Zhang, Chen Liang, Pengcheng He, Tuo Zhao, and Weizhu Chen. Moe-\nbert: from bert to mixture-of-experts via importance-guided adaptation.\narXiv preprint\narXiv:2204.07675, 2022.\n14\nA\nExperiment Details\nA.1\nEvaluation of Memory\nIn the experimental results presented in Section 5.1, we used model parallelism to partition the\nmodel across 8 GPUs or 64 TPUv4 units. Our evaluation focused on determining the maximum\nachievable sequence length, using a sequence number of one. For TPUs, we utilized its default\ntraining configuration, which involved performing matmul operations in bfloat16 format with\nweight accumulation in float32. On the other hand, for GPUs, we adopted the default setup, where\nall operations were performed in float32.\nTo profile memory usage, we utilized jax.profile and repeated the evaluation 100 times, reporting\nthe average results. We conducted a grid search for the optimal query block size and key-value\nblock size, considering values from the set [16, 64, 128, 512, 1024, 2048, 4096]. For each method,\nwe reported the lowest memory achieved.\nA.2\nEvaluation of Throughput\nIn the evaluation presented in Section 5.2, we split OpenWebText following the methodology\nof [2].\nThroughput is measured as tokens per device per second.\nTo ensure a fair compari-\nson, we performed a grid search for the optimal query block size and key-value block size, con-\nsidering values from the set [16, 64, 128, 512, 1024, 2048, 4096]. For gradient checkpointing [8],\nwe additionally grid search among three commonly used checkpointing policies including noth-\ning_saveable, dots_saveable, and dots_with_no_batch_dims_saveable for attention and\nuse nothing_saveable for feedforward network (FFN). For more details, please refer to Jax\ndocumentation. We selected the best performing configuration for both baselines and our method.\nThe training was conducted using FSDP [16] and gradient accumulation. We used weight decay of\n0.1 and utilized cosine learning rate decay with a maximum learning rate of 2.0 \u00d7 e\u22124. For sequence\nlengths of 2048, 4096, 8192, 16384, the batch sizes in trajectories were set as 8, 4, 2, 1, 1 respectively.\nWe use gradient accumulation to accumulate batch size in tokens to 1 million per batch.\nA.3\nEvaluation on RL\nTable 6: Hyperparameters used in RL evaluation.\nHyperparameter\nValue\nNumber of layers\n3\nNumber of attention heads\n1\nEmbedding dimension\n128\nActivation function\nReLU\nBatch size\n64\nDropout\n0.1\nLearning rate\n10\u22124\nLearning rate decay\nLinear warmup for 105 steps\nGrad norm clip\n0.25\nWeight decay\n10\u22124\nInitial desired target return at test time\n850 Walker Stand\n400 Walker Run\n900 Walker Walk\n350 Cheetah Run\n300 Jaco Reach\n800 Cartpole Swingup\nNumber of trajectories during training\n4 \u2192 32\nNumber of trajectories at test time\n4 \u2192 16\nIn the experiment presented in Section 5.3, we followed the prior work\u2019s setting for learning rate,\nbatch size, and other hyperparameters, while modifying the number of trajectories. The specific\nhyperparameters are provided in Table 6. The original agentic transformer used 4 trajectories during\ntraining, we increase the number to 32.\nDuring testing, increasing the number of trajectories has been shown to improve performance.\nHowever, performing autoregressive sampling over a large number of trajectories (e.g., 64 \u00d7 1000 \u00d7 4\n15\ntotal number of tokens) can be computationally slow. To reduce the sampling time, we limited the\nrollout to 16 trajectories.\n16\n1\ndef blockwise_ffn(remat_ffn, inputs, chunk_size, deterministic):\n2\n# remat_ffn: a rematerialized ffn\n3\ninputs = rearrange(inputs, 'b (c n) d -> b c n d', c=chunk_size)\n4\ndef scan_ffn(remat_ffn, carry, hidden_states):\n5\noutputs = remat_ffn(hidden_states, deterministic=deterministic)\n6\nreturn carry, outputs\n7\nscan_axis = inputs.ndim - 2\n8\n_, res = nn.scan(\n9\nscan_ffn,\n10\nvariable_broadcast=\"params\",\n11\nsplit_rngs={\"params\": False, \"dropout\": True},\n12\nin_axes=scan_axis,\n13\nout_axes=scan_axis,\n14\n)(remat_ffn, None, inputs)\n15\nres = rearrange(res, 'b c n d -> b (c n) d')\n16\nreturn res\n17\n18\ndef blockwise_attn(query, key, value, query_chunk_size,\n19\nkey_chunk_size, dtype, policy, precision, prevent_cse):\n20\nquery = query / jnp.sqrt(query.shape[-1]).astype(dtype)\n21\nquery = rearrange(query, 'b (c n) h d -> n b c h d', c=query_chunk_size)\n22\nkey, value = map(lambda t: rearrange(t, 'b (c n) h d -> n b c h d', c=key_chunk_size), (key, value))\n23\nnum_q, batch, _, num_heads, dim_per_head = query.shape\n24\nnum_kv = key.shape[0]\n25\ndef scan_attention(args):\n26\nquery_chunk, query_chunk_idx = args\n27\n@functools.partial(jax.checkpoint, prevent_cse=prevent_cse, policy=policy)\n28\ndef scan_kv_block(carry, args):\n29\nkey_chunk, value_chunk, key_chunk_idx = args\n30\n(numerator, denominator, prev_max_score) = carry\n31\nattn_weights = jnp.einsum('bqhd,bkhd->bqhk', query_chunk, key_chunk, precision=precision)\n32\nbias_chunk = _chunk_bias_fn(query_chunk_idx, key_chunk_idx)\n33\nbias_chunk = jnp.moveaxis(bias_chunk, 1, 2)\n34\nattn_weights = attn_weights + bias_chunk\n35\n36\nmax_score = jnp.max(attn_weights, axis=-1, keepdims=True)\n37\nmax_score = jnp.maximum(prev_max_score, max_score)\n38\nmax_score = jax.lax.stop_gradient(max_score)\n39\nexp_weights = jnp.exp(attn_weights - max_score)\n40\nexp_values = jnp.einsum(\n41\n'bqhv,bvhf->bqhf', exp_weights, value_chunk, precision=precision\n42\n)\n43\ncorrection = jnp.exp(prev_max_score - max_score)\n44\nnumerator = numerator * correction + exp_values\n45\ndenominator = denominator * correction + exp_weights.sum(axis=-1, keepdims=True)\n46\nreturn Carry(numerator, denominator, max_score), None\n47\ninit_carry = Carry(\n48\njnp.zeros((batch, query_chunk_size, num_heads, dim_per_head), dtype=query.dtype),\n49\njnp.zeros((batch, query_chunk_size, num_heads, dim_per_head), dtype=query.dtype),\n50\n(-jnp.inf) * jnp.ones((batch, query_chunk_size, num_heads, 1), dtype=query.dtype),\n51\n)\n52\n(numerator, denominator, max_score), _ = lax.scan(\n53\nscan_kv_block, init_carry, xs=(key, value, jnp.arange(0, num_kv))\n54\n)\n55\noutputs = (numerator / denominator).astype(dtype)\n56\nreturn outputs\n57\n_, res = lax.scan(\n58\nlambda _, x: ((), scan_attention(x)),\n59\n(), xs=(query, jnp.arange(0, num_q))\n60\n)\n61\nres = rearrange(res, 'n b c h d -> b (n c) h d')\n62\nreturn res\nFigure 3: Key parts of the implementation of Blockwise Parallel in Jax. The full code is available on\nGithub https://github.com/lhao499/llm_large_context\n17\n"
  },
  {
    "title": "Understanding and Mitigating Copying in Diffusion Models",
    "link": "https://arxiv.org/pdf/2305.20086.pdf",
    "upvote": "2",
    "text": "Understanding and Mitigating Copying\nin Diffusion Models\nGowthami Somepalli 1, Vasu Singla 1, Micah Goldblum 2, Jonas Geiping 1, Tom Goldstein 1\n1 University of Maryland, College Park\n{gowthami, vsingla, jgeiping, tomg}@cs.umd.edu\n2 New York University\ngoldblum@nyu.edu\nAbstract\nImages generated by diffusion models like Stable Diffusion are increasingly\nwidespread. Recent works and even lawsuits have shown that these models are\nprone to replicating their training data, unbeknownst to the user. In this paper, we\nfirst analyze this memorization problem in text-to-image diffusion models. While\nit is widely believed that duplicated images in the training set are responsible for\ncontent replication at inference time, we observe that the text conditioning of the\nmodel plays a similarly important role. In fact, we see in our experiments that data\nreplication often does not happen for unconditional models, while it is common\nin the text-conditional case. Motivated by our findings, we then propose several\ntechniques for reducing data replication at both training and inference time by\nrandomizing and augmenting image captions in the training set. Code is available\nat https://github.com/somepago/DCR.\n1\nIntroduction\nA major hazard of diffusion models is their ability to produce images that replicate their training data,\noften without warning to the user [Somepalli et al., 2022, Carlini et al., 2023]. Despite their risk of\nbreaching privacy, data ownership, and copyright laws, diffusion models have been deployed at the\ncommercial scale by subscription-based companies like midjourney, and more recently as offerings\nwithin search engines like bing and bard. Currently, a number of ongoing lawsuits [Saveri and\nButterick, 2023] are attempting to determine in what sense companies providing image generation\nsystems can be held liable for replications of existing images.\nIn this work, we take a deep dive into the causes of memorization for modern diffusion models. Prior\nwork has largely focused on the role of duplicate images in the training set. While this certainly plays\na role, we find that image duplication alone cannot explain much of the replication behavior we see at\ntest time. Our experiments reveal that text conditioning plays a major role in data replication, and\nin fact test-time replication can be greatly mitigated by diversifying captions on images, even if the\nimages themselves remain highly duplicated in the training set. Armed with these observations, we\npropose a number of strategies for mitigating replication by randomizing text conditioning during\neither train time or test time. Our observations serve as an in-depth guide for both users and builders\nof diffusion models concerned with copying behavior.\n2\nRelated work\nMemorization in generative models. Most insights on the memorization capabilities of generative\nmodels are so far empirical, as in studies by Webster et al. [2021] for GANs and a number of studies\nfor generative language models [Carlini et al., 2022, Jagielski et al., 2022, Lee et al., 2022].\nPreprint. Under review.\narXiv:2305.20086v1  [cs.LG]  31 May 2023\nFigure 1: The first row shows images generated from real user prompts for Stable Diffusion v2.1. The second\nrow shows images found in the LAION dataset.\nRecently, Somepalli et al. [2022] investigated data replication behaviors in modern diffusion models,\nfinding 0.5-2% of generated images to be partial object-level duplicates of training data, findings also\nmirrored in Carlini et al. [2023]. Yet, what mechanisms lead to memorization in diffusion models,\nand how they could be inhibited, remains so far uncertain aside from recent theoretical frameworks\nrigorously studying copyright issues for image duplication in Vyas et al. [2023].\nRemoving concepts from diffusion models.\nMitigations deployed so far in diffusion models\nhave focused on input filtering. For example, Stable Diffusion includes detectors that are trained to\ndetect inappropriate generations. These detectors can also be re-purposed to prevent the generation\nof known copyrighted data, such as done recently in midjourney, which has banned its users from\ngenerating photography by artist Steve McCurry, due to copyright concerns [Chess, 2022]. However,\nsuch simple filters can be easily circumvented [Rando et al., 2022, Wen et al., 2023], and these\nband-aid solutions do not mitigate copying behavior at large. A more promising approach deletes\nentire concepts from the model as in Schramowski et al. [2023] and Kumari et al. [2023], yet such\napproaches require a list of all concepts to be erased, and are impractical for protecting datasets with\nbillions of diverse images covering many concepts.\n3\nHow big of a problem is data replication?\nSomepalli et al. [2022] and Carlini et al. [2023] have shown that diffusion models can reproduce\nimages from their training data, sometimes to near-perfect accuracy. However, these studies induce\nreplication behavior by prompting the model with image captions that are directly sampled from the\nLAION dataset \u2013 a practice that amplifies the rate of replication for scientific purposes. We study the\nrate at which replication happens with real-world user-submitted prompts. This gives us a sense of\nthe extent to which replication might be a concern for typical end-users.\nWe randomly sample 100K user-generated captions from the DiffusionDB [Wang et al., 2022] dataset\nand generate images using Stable Diffusion 2.1 [Rombach et al., 2021, Stability AI, 2022]. We use\nSSCD features [Pizzi et al., 2022] to search for the closest matches against these images in a subset of\nthe training dataset. Note that SSCD was found to be one of the best metrics for detecting replication\nin Somepalli et al. [2022], surpassing the performance of CLIP [Radford et al., 2021]. We compare\neach generated image against \u223c 400 million images, roughly 40% of the entire dataset.\nWe find that \u223c 1200 images (1.2%) have a similarity score above 0.5, indicating these may be\nduplicates. Note that this is likely an underestimation of the true rate, as our SSCD-based search\nmethod is unlikely to find all possible matches. We show several of these duplicates in Figure 1.\nSometimes, the captions precisely describe the image content (for example, \u2018Jake Gyllenhaal\u2019).\nHowever, we also discover instances where captions do not mention the content from the generated\nimage. For example, in the first duplicate, the user caption is \u201cflower made of fire, black\nbackground, art by artgerm\u201d, and the LAION caption is \u201cColorful Cattleya Orchids\n(2020) by JessicaJenney\u201d. Several duplicates identified by SSCD also have high similarity\nscores due to the simple texture of the generated image. We include these SSCD false positives and\nother examples in the Appendix.\n4\nExperimental Setup\nA thorough study of replication behavior requires training many diffusion models. To keep costs\ntractable, we focus on experiments in which large pre-trained models are finetuned on smaller\n2\nFigure 2: Stable Diffusion v1.4 generates memorized images when either images or captions are duplicated.\nHighly replicated generations from Stable Diffusion v1.4 and duplicated images from LAION training data\nare labeled on the plot and shown on the left. Stable Diffusion v2.1 is trained on a de-duplicated dataset, so as\nexpected we see the clusters with high visual image similarity vanish from the right side of the chart. Nonetheless,\nwe still see a number of replicated generations from clusters with high caption similarity.\ndatasets. This process reflects the training of Stable Diffusion models, which are pre-trained on\nLAION and then finetuned in several stages on much smaller and more curated datasets, like the\nLAION Aesthetics split.\nDatasets:\nWe use Imagenette1, which consists of 10 classes from Imagenet [Deng et al., 2009] as\nwell as two randomly sampled subsets of 10, 000 and 100, 000 images from LAION-2B [Schuhmann\net al., 2022] for our experiments. The LAION subsets, which we denote as LAION-10k and LAION-\n100k, include captions, while the Imagenette data is uncaptioned. For some experiments, we use\nBLIP v1 [Li et al., 2022] to generate captions for images when needed. We provide details for LAION\nsubset construction in the Appendix.\nArchitecture & Training:\nWe use the StableDiffusion-v2.1 checkpoint as a starting point for all\nexperiments. Unless otherwise noted, only the U-Net [Ronneberger et al., 2015] part of the pipeline\nis finetuned (the text and auto-encoder/decoder components are frozen) as in the original training\nrun, and we finetune for 100000 iterations with a constant learning rate of 5e \u2212 6 and 10000 steps of\nwarmup. All models are trained with batch size 16 and image resolution 256. We conduct evaluations\non 4000 images generated using the same conditioning used during model training. Refer to Appendix\nfor complete details on the training process.\nMetrics:\nWe use the following metrics to study the memorization and generation quality of finetuned\nmodels. Frechet Inception Distance (FID) [Heusel et al., 2017] evaluates the quality and diversity\nof model outputs. FID measures the similarity between the distribution of generated images and the\ndistribution of the training set using features extracted by an Inception-v3 network. A lower FID\nscore indicates better image quality and diversity.\nSomepalli et al. [2022] quantify copying in diffusion models using a similarity score derived from the\ndot product of SSCD representations [Pizzi et al., 2022] of a generated image and its top-1 match in\nthe training data. They observe that generations with similarity scores greater than 0.5 exhibit strong\nvisual similarities with their top-1 image and are likely to be partial object-level copies of training data.\nGiven a set of generated images, we define its dataset similarity score as the 95-percentile of its\nimage-level similarity score distribution. Note that measuring average similarity scores over the whole\nset of generated images is uninformative, as we are only interested in the prevalence of replicated\nimages, which is diluted by non-replicated samples. For this reason, we focus only on the similarity\nof the 5% of images with the highest scores.\n5\nData Duplication is Not the Whole Story\nExisting research hypothesizes that replication at inference time is mainly caused by duplicated data\nin the training set [Somepalli et al., 2022, Carlini et al., 2023, Webster et al., 2023]. Meanwhile,\ndata replication has been observed in newer models trained on de-duplicated data sets [Nichol, 2022,\n1https://github.com/fastai/imagenette\n3\n0\n5\n10\n15\n20\nData duplication factor\n0.64\n0.66\n0.68\n0.70\nSimilairty scores \u2190\n17\n18\nFID \u2190\nLAION-10k\n0\n5\n10\n15\n20\nData duplication factor\n0.5\n0.6\n0.7\nSimilairty scores \u2190\n18\n19\n20\nFID \u2190\nImagenette\nFigure 3: How does data duplication affect memorization? All models are trained with captions. On both\ndatasets, dataset similarity increases proportionally to duplication in training data. FID score are unaffected by\nlight duplication, but increase on higher levels as image diversity reduces.\nMostaque, 2022]. Our goal here is to quantify the extent to which images are duplicated in the\nLAION dataset, and understand how this impacts replication at inference time. We will see that data\nduplication plays a role in replication, but it cannot explain much of the replication behavior we see.\n5.1\nHow Much Duplication is Present in LAION?\nIn order to understand how much data duplication affects Stable Diffusion, we first identify clusters of\nduplicated images in LAION. We use 8M images from LAION-2B for our duplication analysis. Since\nduplicates are often duplicated many times, even a small subset of LAION is sufficient to identify\nthem. Note that we do not use a random subset, but instead a consecutive slice of the metadata. Since\nthe metadata is structured to provide URLs from similar domains together, this makes us likely to\nfind large clusters.\nWe use SSCD [Pizzi et al., 2022] to compute the 8M \u00d7 8M similarity matrix between all images\nwithin a slice of metadata. We threshold the similarity scores, only keeping ones above 0.7 to identify\nimages that are nearly identical. Then, we identify each connected component in the sparse similarity\nmatrix and interpret it as representing a duplicated image. We show several of these clusters in the\nAppendix. We only consider clusters with at least 250 samples to ensure the images are duplicated\nenough times, which leaves \u223c 50 clusters with a total of around 45K images.\nTo measure how similar captions are within a cluster, we first compute CLIP text features [Radford\net al., 2021]. Then, we compute the similarity between captions using the dot product of CLIP text\nfeatures multiplied by the unigram-Jaccard similarity between the captions of a cluster and finally com-\npute the median value. We use the unigram-Jaccard similarity in addition to CLIP, since it better cap-\ntures word-level semantics. We select 40 captions from each cluster and feed them to Stable Diffusion.\nFinally, we determine if the generated images are replicas of their source cluster again using SSCD.\nResults are shown in Figure 2. We observe that Stable Diffusion v1.4 shows high pixel-level\nreplication (reflected in SSCD scores > 0.4) for some duplicated clusters, but only when caption\nsimilarity within the cluster is also high. In contrast, since the dataset for Stable Diffusion v2.1\nwas de-duplicated before training, it shows nearly no test-time replication for our identified clusters\n[Mostaque, 2022]. Despite being de-duplicated, Stable Diffusion v2.1 still exhibits memorization\nwhen scanning through a larger portion of the dataset, as we observed previously in Figure 1, showing\nthat replication can happen even when the duplication rate in the training set is greatly reduced. In\nboth cases, we see that image duplication is not a good predictor of test-time replication.\n5.2\nControlled Experiments with Data Duplication\nWe train diffusion models with various levels of data duplication factor (ddf), which repre-\nsents the factor by which duplicate samples are more likely to be sampled during training. We train\neach model for 100k iterations and evaluate similarity and FID scores on 4000 generated samples.\nFigure 3 contains results for LAION-10k and Imagenette. We observe that increased duplication in\nthe training data tends to yield increased replication during inference. The relationship between data\nduplication and similarity scores is not straightforward for LAION-10k . As the duplication factor\nincreases, similarity scores rise again until reaching a data duplication factor ddf of 10, after which\nthey decrease. Regarding FID scores, we find that a certain level of data duplication contributes to\n4\nimproving the scores for models trained on both datasets, possibly because FID is lowest when the\ndataset is perfectly memorized.\nOther Observations from the Literature.\nSomepalli et al. [2022] found that unconditional dif-\nfusion models can exhibit strong replication when datasets are small, despite these training sets\ncontaining no duplicated images. Clearly, replication can happen in the absence of duplication. As\nthe training set sizes grow (\u223c 30k), the replication behaviors seen in Somepalli et al. [2022] vanish,\nand dataset similarity drops, even when the number of epochs is kept constant. One might expect this\nfor large enough datasets. However, models trained on the much larger LAION-2B dataset do exhibit\nreplication, even in the case of Stable Diffusion 2.1 (Figure 1) which was trained on (at least partially)\nde-duplicated data. We will see below that the trend of replication in diffusion models depends\nstrongly on additional factors, related especially to their conditioning and to dataset complexity.\nTakeaway: When training a diffusion model, it is crucial to carefully manage data duplication\nto strike a balance between reducing memorization and ensuring high-fidelity of generated\nimages. However, controlling duplication is not enough to prevent replication of training data.\n5.3\nThe Effect of Model conditioning\nTo understand the interplay between model conditioning and replication, we consider four types of\ntext conditioning:\n\u2022 Fixed caption: All data points are assigned the same caption, An image.\n\u2022 Class captions: Images are assigned a class-wise caption using the template An image of\n<class-name>.\n\u2022 Blip/Original captions: Each point is trained on the original caption from the dataset (LAION-\n10k ) or a new caption is generated for each image using BLIP [Li et al., 2022] (Imagenette).\n\u2022 Random captions: A random sequence of 6 tokens is sampled from the vocabulary used to\nuniquely caption each image.\nBy varying caption scenarios, we transition from no diversity in captions (\u201cfixed caption\u201d) case to\ncompletely unique captions with no meaningful overlap (\u201crandom caption\u201d) case. We again finetune\non the Imagenette dataset, now using these caption types. As a baseline, we consider the pretrained\nStable Diffusion model without any finetuning. Figure 4 (left) shows the dataset similarity among\nthe baseline and models finetuned using the different caption types.\nWe observe that the finetuned models exhibit higher similarity scores compared to the baseline model.\nFurthermore, the level of model memorization is influenced by the type of text conditioning. The\n\u201cfixed caption\u201d models exhibit the lowest amount of memorization, while the \u201cblip caption\u201d models\nexhibit the highest. This indicates that the model is more likely to memorize images when the\ncaptions are more diverse. However, the model does not exhibit the highest level of memorization\nwhen using \u201crandom captions\u201d, meaning that captions should be correlated with image content in\norder to maximally help the model retrieve an image from its memory.\nTraining the text encoder.\nSo far, the text encoder was frozen during finetuning. We can amplify the\nimpact of conditioning on replication by training the text encoder during finetuning. In Fig. 4 (right),\nwe observe a notable increase in similarity scores across all conditioning cases when the text encoder\nis trained. This increase is particularly prominent in the cases of \u201cblip captioning\u201d and \u201crandom\ncaptioning\u201d. These findings support our hypothesis that the model is more inclined to remember\ninstances when the captions associated with them are highly specific, or even unique, keys.\nTakeaway: Caption specificity is closely tied to data duplication. Highly specific captions\nact like keys into the memory of the diffusion model that retrieve specific data points.\n5.3.1\nThe Impact of Caption vs. Image Duplication\nIn this section, we control separately for duplication of images and duplication of their captions to\nbetter understand how the two interact.\n5\n\ufb01xed capt\nclass capt\nblip capt\nrandom capt\nText conditioning style (Imagenette)\n0.1\n0.2\n0.3\n0.4\n0.5\nSimilairty scores \u2190\npretrained SD\n\ufb01netuned model\nFalse\nTrue\nText encoder trained (Imagenette)\n0.0\n0.2\n0.4\n0.6\nSimilairty scores \u2190\n\ufb01xed capt\nclass capt\nblip/og capt\nrandom capt\nFigure 4: Left: Diffusion models finetuned on Imagenette with different styles of conditioning. FID scores of\nfinetuned models are as follows (in order) 40.6, 47.4, 17.74, 39.8. Right: We show the effects of training the\ntext encoder on similarity scores with different types of conditioning\nIn the case of full duplication, both the image and its caption are replicated multiple times in the\ntraining data. On the other hand, partial duplication involves duplicating the image multiple times\nwhile using different captions for each duplicate(although the captions may be semantically similar).\nTo study the partial duplication scenario, we generate 20 captions for each image using the BLIP\nmodel for both Imagenette and LAION-10k datasets. For the full-duplication case in the LAION-10k\nexperiments, we keep the original caption from the dataset.\nWe present the results on LAION-10k and Imagenette in Figure 5. We investigate how dataset\nsimilarity changes for both full and partial image-caption duplication at varying levels of duplication.\nOverall, our findings demonstrate that partial duplication consistently leads to lower levels of\nmemorization compared to full duplication scenarios.\nIn Figure 5 (left), we compare the similarity scores of several models: the pretrained checkpoint, a\nmodel finetuned without any data duplication, a model finetuned with full duplication (ddf=5), and a\nmodel finetuned with partial duplication (ddf=5). We include dashed horizontal lines representing the\nbackground self-similarity computed between the dataset and itself. In the Imagenette case, models\ntrained without duplication and with partial duplication exhibit dataset similarity below the baseline\nvalue, indicating a lower level of memorization. In contrast, the model trained with full duplication\ndemonstrates higher levels of memorization compared to both the baseline and other cases. In the\nLAION-10k experiments, the model trained without duplication surpasses the training data similarity\nbaseline. This observation raises the question of whether the memorization is inherited from the\npretrained model, considering it is also trained on the larger LAION-2B dataset. However, when we\ncompute the similarity scores on the pretrained checkpoint, we observe significantly lower values,\nindicating that the observed memorization is acquired during the fine-tuning process.\nIn Figure 5 (middle, right), we analyze how the similarity scores and FID scores vary at different\ndata duplication factors (ddf) for full and partial data duplication. As the ddf increases,\nwe observe an increase in memorization for models trained with full duplication. However, for partial\nduplication, dataset similarity actually decreases with increased duplication. In our previous analogy,\npretrained SD\nno dupl.\nimage + capt. dupl.\nimage dupl.\nLAION-10k\nImagenette\nDataset\n0.0\n0.2\n0.4\n0.6\nSimilairty scores \u2190\nImagenette\nLAION-10k\n0\n5\n10\n15\n20\nData duplication factor\n0.55\n0.60\n0.65\n0.70\nSimilairty scores \u2190\nLAION-10k\n0\n5\n10\n15\n20\nData duplication factor\n18\n20\nFID \u2190\nLAION-10k\nFigure 5: Models trained with different levels duplication and duplication settings. Left: Dataset similarity\nbetween models trained with no duplication, with partial duplication, and full duplication. Dashed lines show\ndataset similarity of each training distribution. Middle, Right: Dataset similarity and FID for full duplication vs\npartial duplication for different data duplication factors.\n6\nno dupl.\nimg+cap dupl.\nimg dupl.\n50000\n100000\nLength of training (in iters)\n0.4\n0.5\n0.6\n0.7\nSimilairty scores \u2190\nLAION-10k\n50000\n100000\nLength of training (in iters)\n17\n18\n19\n20\nFID \u2190\nLAION-10k\n25\n50\n75\n100\nAmount of training data (x 103)\n0.45\n0.50\n0.55\n0.60\n0.65\nSimilairty scores \u2190\n14.5\n15.0\n15.5\n16.0\nFID \u2190\nLAION-100k\nFigure 6: Does training for longer increase memorization? Left, Middle: Similarity and FID scores of models\ntrained for different training number of iterations with different types of data duplication (ddf=5). Right:\nMetrics on models trained with different fractions of LAION-100k subset.\nwe now have multiple captions, i.e. keys for each duplicated image, which inhibits the memorization\ncapabilities of the model. However, this memorization reduction comes at the cost of a moderate\nincrease in FID at higher duplication levels.\nTakeaway: Compared to full duplication, partial duplication substantially mitigates copying\nbehavior, even as duplication rates increase.\n6\nEffect of the Training Regimen\nIn this section, we examine how the training process and data complexity influence the degree of\nmemorization.\nLength of training.\nTraining for longer results in the model seeing each data point more times,\nand may mimic the impacts of duplication. In Figure 6 (left), we quantify the increase in dataset\nsimilarity at different training iterations. Notably, the image-only duplication model consistently\nexhibits lower similarity scores across epochs, while the image & caption duplication model achieves\nthe highest values, showing that caption diversity for each duplicated image significantly slows down\nmemorization. We show matching FID scores in Figure 6 (middle).\nQuantity of training data.\nSomepalli et al. [2022] demonstrate that diffusion models, specifically\nDDPM [Ho et al., 2020] models without text conditioning, exhibit a tendency to replicate the training\ndata when trained on small datasets. In this study, we extend this analysis to text-conditioned\nmodels. To conduct this investigation, we now randomly sample a subset of 100, 000 images from\nLAION-2B. We then train using different fractions of this dataset, keeping other hyperparameters\nconstant including the number of training iterations. The results of this experiment are found in\nFigure 6 (right). We observe that increasing the quantity of training data generally leads to lower\nsimilarity and FID scores. This aligns with the intuition that a larger and more diverse training set\nallows the model to generalize better and produce more varied outputs.\nImage Complexity.\nThroughout our experiments, we consistently observed a higher level of test-\ntime replication in LAION-10k models compared to Imagenette models, even when trained with the\nsame levels of data duplication. We put forth the hypothesis that this discrepancy arises due to the\ninherent diversity present in the LAION-10k dataset, not only in terms of images but also in terms\nof image complexity. To quantify image complexity, we employ two metrics. Histogram entropy\nmeasures entropy within the distribution of pixel intensities in an image, and JPEG compressibility\n(at JPEG quality 90), measures the size of images after compression. We resize and crop all images\nto the same resolution before computing these metrics.\nFigure 7 (right) presents the distributions of entropy scores for LAION-10k and Imagenette. LAION-\n10k exhibits a wide range of data complexity, while Imagenette, predominantly comprised of complex\nreal-world images, is characterized by higher complexity. Motivated by this observation, we explore\nthe relationship between similarity scores and complexity of the top training point matched to each\ngeneration of a diffusion model trained on LAION-10k without any duplication. Remarkably, we\ndiscover a statistically significant correlation of \u22120.32/ \u2212 0.29 (with p-values of 8e \u2212 98/7e \u2212 80)\n7\n0\n2\n4\n6\nComplexity (Entropy)\n0.0\n0.2\n0.4\n0.6\nDensity\nHigh Sim\nLow Sim\n0\n10\n20\n30\nComplexity (Compression)\n0.000\n0.025\n0.050\n0.075\n0.100\nDensity\nHigh Sim\nLow Sim\n0\n2\n4\n6\nComplexity (Entropy)\n0.0\n0.5\n1.0\n1.5\nDensity\nImagenette\nLAION-10k\nFigure 7: Image complexity vs Memorization We compute complexity scores using histogram entropy (left)\nand JPEG compressibility (middle) for memorized and non-memorized subpopulations, and compare both\ndatasets (right). Refer to Sec. 6 for details.\nfor the entropy and compression metrics, respectively. We provide results for other duplication\nsettings in the supplementary material, where we also find significant correlation. We present density\nplots illustrating the distributions of both complexity metrics for high similarity points (with scores\n> 0.6) versus low similarity in Figure 7 (left, middle). The clear separation between the distributions,\nobserved across both complexity metrics, strongly suggests that diffusion models exhibit a propensity\nto memorize images when they are \u201csimple\u201d.\nTakeaway: Choosing the optimal length of training, quantity of training data, and training\nsetup is a trade-off between model quality and memorization. When images do get memorized\nin the absence of training data duplication, they are likely to be \u201csimple\u201d in structure.\n7\nMitigation strategies\nWe have seen that model conditioning plays a major role in test-time replication, and that replication\nis often rare when image captions are not duplicated. Armed with this observation, we formulate\nstrategies for mitigating data replication by randomizing conditional information. We study both\ntraining time mitigation strategies and inference time mitigation strategies. Training strategies are\nmore effective, whereas inference-time mitigations are easily retrofitted into existing diffusion models.\nWe experiment with randomizing the text input in the following ways:\n\u2022 Multiple captions (MC). We use BLIP to make 20 captions for each image and we randomly\nsample all of them (plus the original) when training. This is for train time only.\n\u2022 Gaussian noise (GN). Add a small amount of Gaussian noise to text embeddings.\n\u2022 Random caption replacement (RC). Randomly replace the caption of an image with a random\nsequence of words.\n\u2022 Random token replacement & addition (RT). Randomly replace tokens/words in the caption\nwith a random word, or add a random word to the caption at a random location.\n\u2022 Caption word repetition (CWR). Randomly choose a word from the given caption and insert\nit into a random location in the caption.\n\u2022 Random numbers addition (RNA). Instead of adding a random word that might change the\nsemantic meaning of the caption, we add a random number from the range {0, 106}.\nIn this section, we present the effectiveness of various mitigation strategies, summarized in Table 1.\nDuring train time, we find the multiple-caption strategy to be the most effective, which substantially\nincreases caption diversity among duplicates. This matches our analysis in Fig. 5. At inference\ntime strategies, we find that we can still inject a shift in the text prompt through random token\naddition/replacement. Such a shift again disrupts the memorized connection between caption and\nimage. We verify based on FID and Clipscore [Hessel et al., 2021] that all mitigations have a minimal\neffect of model performance.\nImages generated from models trained/tested with the best performing strategies can be found in\nFigure 8. For train mitigations we show our finetuned models, whereas for inference time we show\nmitigation strategies applied directly to Stable Diffusion. We show four images for each strategy,\ndisplaying the memorized image, generation from the regular model and from the mitigated model\nin each column. We fix the generation seed in each column. Overall, we find these mitigations to\n8\nTable 1: We present the similarity scores for models trained on LAION-10k with various train and inference\ntime mitigation strategies discussed above. We show the results on no duplication, image duplication (ddf=5)\nand image & caption duplication (ddf=5) scenarios. For similarity scores, the lower the better. Best numbers\nwith in train or test time are shown in bold. * collapses to the same setup.\nTrain time mitigation\nInference time mitigation\nDup style\u2193 / Mit strat.\u2192\nNone\nMC\nGN\nRC\nRT\nCWR\nGNI\nRT\nCWR\nRNA\nNo Dupl.\n0.638\n0.420\n0.560\n0.565\n0.638\n0.614\n0.615\n0.524\n0.576\n0.556\nImage Dupl.\n0.576\n0.470*\n0.508\n0.586\n0.645\n0.643\n0.553\n0.489\n0.522\n0.497\nImage + Caption Dupl.\n0.663\n0.470*\n0.598\n0.580\n0.669\n0.644\n0.639\n0.555\n0.602\n0.568\nTrain time \u2013 Multiple Captions\nInference time \u2013 Random token / number addn.\nTrain \ndata\nGen. w. \nmitigation Generation\nFigure 8: We present qualitative results on the best-performing train and inference time mitigation strategies.\nIn each column, we present a potential training image model copied the generation from, a generation, and a\ngeneration on a model trained with a mitigation strategy/ mitigation applied to the original model. We used an\nimage & caption duplication model with ddf=5 for the train time results and Stable Diffusion V1.4 (seed=2)\nto evaluate the inference-time strategies.\nbe quite effective in reducing copying behavior, even in large modern diffusion models. We refer\nthe reader to the appendix regarding the hyperparameters used in the generation of Tab. 1 and the\ncaptions used for creating Fig. 8.\n8\nRecommendations for a safer model\nWhy do diffusion models memorize? In this work, we find that, in addition to the widely recognized\nimportance of de-duplicating image data, conditioning and caption diversity also play fundamental\nroles. Moreover, we find in Fig. 5 that memorization can decrease even when duplication increases,\nas long as captions are sufficiently diverse. These findings help to explain why large text-conditional\nmodels like Stable Diffusion generate copies. Finally, we collect our observations into a series of\nrecommendations to mitigate copying.\nBefore training:\nIdentify large clusters of image data with significant object overlap, as measured by\nspecialized copy detection models such as SSCD, and collapse them as described in Sec. 5.1. Likewise,\nclusters of caption overlap are potentially problematic, and duplicate captions can be resampled,\nfor example using BLIP. In some cases, these clusters may need to be hand-curated because some\nconcepts, like famous paintings or pictures of the Eiffel Tower, are acceptable duplications that the\nmodel may copy. Another avenue is to exclude images with limited complexity, see Sec. 6.\nDuring training:\nWe find training with partial duplications, where captions for duplicate images\nare resampled, to be most effective in reducing copying behavior, see Sec. 7. It may be worthwhile to\nuse a low bar for duplication detection, as near-duplicates do not have to be removed entirely, only\ntheir captions resampled.\nAfter training:\nFinally, after training, inference-time strategies can further reduce copying behavior,\nsee Sec. 7. Such mitigations could be provided either as a user-facing toggle, allowing users to\nresample an image when desired, or as rejection sampling if a generated image is detected to be close\nto a known cluster of duplications or a match in the training set.\nWhile text conditioning has also been identified as a crucial factor in dataset memorization for the\ndiffusion model in our analysis, it is important to acknowledge that other factors may also influence\nthe outcome in complex ways. Therefore, we recommend that readers thoroughly evaluate the\n9\neffectiveness of the proposed mitigation strategies within the context of their specific use cases before\ndeploying them in production.\n9\nAcknowledgements\nThis work was made possible by the ONR MURI program, DARPA GARD (HR00112020007), the\nOffice of Naval Research (N000142112557), and the AFOSR MURI program. Commercial support\nwas provided by Capital One Bank, the Amazon Research Award program, and Open Philanthropy.\nFurther support was provided by the National Science Foundation (IIS-2212182), and by the NSF\nTRAILS Institute (2229885).\nReferences\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan\nZhang. Quantifying Memorization Across Neural Language Models. arxiv:2202.07646[cs],\nFebruary 2022. doi: 10.48550/arXiv.2202.07646. URL http://arxiv.org/abs/2202.07646.\nNicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tram\u00e8r,\nBorja Balle, Daphne Ippolito, and Eric Wallace. Extracting Training Data from Diffusion Models.\narxiv:2301.13188[cs], January 2023. doi: 10.48550/arXiv.2301.13188. URL http://arxiv.\norg/abs/2301.13188.\nDavid Chess. Some light infringement?, December 2022. URL https://ceoln.wordpress.com/\n2022/12/16/some-light-infringement/.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,\npages 248\u2013255. Ieee, 2009.\nJack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-\nfree evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural\ninformation processing systems, 30, 2017.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\nNeural Information Processing Systems, 33:6840\u20136851, 2020.\nMatthew Jagielski, Om Thakkar, Florian Tram\u00e8r, Daphne Ippolito, Katherine Lee, Nicholas Car-\nlini, Eric Wallace, Shuang Song, Abhradeep Thakurta, Nicolas Papernot, and Chiyuan Zhang.\nMeasuring Forgetting of Memorized Training Examples. arxiv:2207.00099[cs], June 2022. doi:\n10.48550/arXiv.2207.00099. URL http://arxiv.org/abs/2207.00099.\nNupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli Shechtman, Richard Zhang, and Jun-Yan Zhu.\nAblating Concepts in Text-to-Image Diffusion Models. March 2023. URL https://arxiv.org/\nabs/2303.13516v2.\nJooyoung Lee, Thai Le, Jinghui Chen, and Dongwon Lee.\nDo Language Models Plagiarize?\narxiv:2203.07618[cs], March 2022. doi: 10.48550/arXiv.2203.07618. URL http://arxiv.org/\nabs/2203.07618.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-\ntraining for unified vision-language understanding and generation. In International Conference on\nMachine Learning, pages 12888\u201312900. PMLR, 2022.\nEmad Mostaque. Some more from first batch. Lots of optimisation to do, took about an hour of\nplaying about. Prompts here: https://t.co/4soGak9op0 negative important for 2.0 given how we\nflatten distribution of latents with dedupe etc Embeddings will make it easier out of the box\nhttps://t.co/sZxhrk1v8J, November 2022. URL https://twitter.com/EMostaque/status/\n1596907328548139008.\n10\nAlex Nichol. Dall\u00b7e 2 pre-training mitigations, 2022. URL https://openai.com/research/\ndall-e-2-pre-training-mitigations. Accessed: 2023-05-05.\nEd Pizzi, Sreya Dutta Roy, Sugosh Nagavara Ravindra, Priya Goyal, and Matthijs Douze.\nA Self-Supervised Descriptor for Image Copy Detection.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14532\u201314542,\n2022.\nURL\nhttps://openaccess.thecvf.com/content/CVPR2022/html/Pizzi_A_\nSelf-Supervised_Descriptor_for_Image_Copy_Detection_CVPR_2022_paper.html.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.\nLearning transferable visual models from natural language supervision, 2021.\nJavier Rando, Daniel Paleka, David Lindner, Lennart Heim, and Florian Tram\u00e8r. Red-Teaming the\nStable Diffusion Safety Filter. arxiv:2210.04610[cs], October 2022. doi: 10.48550/arXiv.2210.\n04610. URL http://arxiv.org/abs/2210.04610.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-\nresolution image synthesis with latent diffusion models, 2021.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical\nimage segmentation. In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI\n2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III\n18, pages 234\u2013241. Springer, 2015.\nJoseph Saveri and Matthew Butterick.\nStable Diffusion Litigation, 2023.\nURL https://\nstablediffusionlitigation.com/.\nPatrick Schramowski, Manuel Brack, Bj\u00f6rn Deiseroth, and Kristian Kersting. Safe Latent Diffusion:\nMitigating Inappropriate Degeneration in Diffusion Models. arxiv:2211.05105[cs], April 2023.\ndoi: 10.48550/arXiv.2211.05105. URL http://arxiv.org/abs/2211.05105.\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi\nCherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b:\nAn open large-scale dataset for training next generation image-text models.\narXiv preprint\narXiv:2210.08402, 2022.\nGowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Diffusion\nArt or Digital Forgery? Investigating Data Replication in Diffusion Models. arxiv:2212.03860[cs],\nDecember 2022. doi: 10.48550/arXiv.2212.03860. URL http://arxiv.org/abs/2212.03860.\nStability AI.\nStable Diffusion v2.1 and DreamStudio Update, December 2022.\nURL https:\n//stability.ai/blog/stablediffusion2-1-release7-dec-2022.\nNikhil Vyas, Sham Kakade, and Boaz Barak. Provable Copyright Protection for Generative Models.\narxiv:2302.10870[cs, stat], February 2023. doi: 10.48550/arXiv.2302.10870. URL http://\narxiv.org/abs/2302.10870.\nZijie J. Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and Duen Horng\nChau. DiffusionDB: A large-scale prompt gallery dataset for text-to-image generative models.\narXiv:2210.14896 [cs], 2022. URL https://arxiv.org/abs/2210.14896.\nRyan Webster, Julien Rabin, Loic Simon, and Frederic Jurie. This Person (Probably) Exists. Identity\nMembership Attacks Against GAN Generated Faces. arxiv:2107.06018[cs], July 2021. doi:\n10.48550/arXiv.2107.06018. URL http://arxiv.org/abs/2107.06018.\nRyan Webster, Julien Rabin, Loic Simon, and Frederic Jurie. On the de-duplication of laion-2b. arXiv\npreprint arXiv:2303.12733, 2023.\nYuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Hard\nPrompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery.\nFebruary 2023. URL https://arxiv.org/abs/2302.03668v1.\n11\nA\nBroader Impact\nText-to-image diffusion models are prone to violating intellectual property rights and causing harm\nto artists, even unbeknownst to the models\u2019 users. Given the rapidly increasing popularity of these\nmodels and also the likelihood that users will continue to generate and deploy their images whether\nor not copying behaviors are addressed, mitigating such behavior can prevent real-world harm. We\ntake the first step towards mitigation by building up an understanding of how and why diffusion\nmodels copy their training data. Moreover, we show how to build text-to-image diffusion models\nwhich simultaneously produce high-quality images and at the same time do not replicate their training\ndata nearly as often as popular existing models. Another important avenue for preventing copying is\nconstructing detection pipelines so that we can identify copies of training samples before they are\ndeployed, as in Somepalli et al. [2022].\nB\nStable Diffusion User Matches False Positives\nFigure 9: False Positive matches for User Captions using SSCD. Images are generated using Stable Diffusion\nv2.1, and corresponding closest matches from LAION subset are shown together.\nIn Figure 9, we show failure cases for SSCD match from the LAION subset to generated images\nfrom user captions. Several false positives are reasonable errors, caused by either high style similarity\nor simple texture of the generated images. A few of the errors are also caused for animated images\nas shown in the last row. We also observe a few cases where generated, and LAION images show\nsignificant domain shift from real images. This may be due to the training data of SSCD, which\nmostly consisted of real images Pizzi et al. [2022].\nC\nLAION Clusters\nIn Figure 10, we show a few of the clusters discovered using our proposed approach discussed in\nSection 5.1. As shown several of the images share high visual similarities with each other. The\nnumber of images discovered in each cluster is also shown. Note that the number is very likely an\nunderestimate since we only use an 8M subset of the LAION dataset. A few of the clusters are also\nobtained due to broken links or similar placeholder images. A few clusters are also obtained for\nsimilar bar graphs or line charts. This shows LAION consists of several duplicates, which can be\neasily discovered using our approach.\n12\nFigure 10: Clusters discovered from 8M subset of LAION using SSCD\nD\nExtended experimental settings\nUnless otherwise specified, all the models are trained for 100k iterations with a batch size of 16.\nWe used Adam optimizer for training with \u03b21 = 0.9 and \u03b22 = 0.999 and weight decay= 1e \u2212 2\nWe used one RTX-A6000 per model and it took about 24 hours to train. For inference, we used\nRTX-A5000 and it took approximately 5 hours to create enough generations to compute our metrics.\nTo analyze various factors in this paper, we used approximately 5000 GPU hours for training and\nanother 1000 GPU hours for inference.\nE\nWhy do diffusion models copy? Extended Results\nIn this section, we explore factors contributing to replication in diffusion models. We present\nadditional results, including some figures from the main paper for completeness.\nModel conditioning.\nAs shown in Fig. 4 (Left), higher uniqueness in conditioning captions leads\nto increased similarity scores, with Blip caption conditioning yielding the highest score. In Fig. 14\n(Left), FID scores for fine-tuned models follow the same trends we have seen in the main paper, with\nthe blip caption model exhibiting the lowest FID score.\nImage Duplication vs Image + Caption Duplication\nIn Fig. 11, we analyze similarity and FID\nscores for models trained with varying duplication levels (ddf) on LAION-10k and Imagenette. Both\ndatasets show increased similarity and FID scores with higher ddf. However, the impact of image\nduplication versus image + caption duplication differs. This disparity can be attributed to the diversity\nof captions used in image duplication for LAION-10k and the similarity of captions in Imagenette,\npotentially collapsing into image + caption duplication.\nTraining Regimen - Length of Training\nIn Fig. 12, we examine similarity and FID scores during\nthe training process on LAION-10k and Imagenette. We consider scenarios with no duplication,\nimage duplication, and image + caption duplication. Longer training consistently leads to higher\nsimilarity and FID scores. However, image duplication consistently yields lower similarity scores\ncompared to image + caption duplication on both datasets.\n13\nTraining Regimen - Text Encoder Training\nIn Fig. 4 (Right), we analyze the impact of text\nencoder training on similarity and FID scores for models trained with caption conditioning and\nvarious types of duplication on Imagenette and LAION-10k . Training the text encoder increases\nsimilarity scores while reducing FID scores in both datasets. In Fig. 14 (Right), we observe the effect\nof text encoder training on similarity scores for models trained with different conditioning types on\nImagenette, specifically with image + caption duplication (ddf=5), and we see similar trends as of\nFig. 4 (right) even in the presence of data duplication.\nTraining Regimen - Image Complexity\nIn Fig. 15, we present distributions of self-similarity,\nentropy, and compression metrics for LAION-10k and Imagenette. LAION-10k exhibits lower\nself-similarity, indicating a distinct data distribution. However, complexity distributions are slightly\nshifted to the left, suggesting LAION-10k is marginally simpler than Imagenette.\nFurthermore, we provide comprehensive findings on the correlation between similarity scores and\ntraining data complexity metrics in Tab. 2. We extensively evaluate models trained on LAION-10k\nwithout duplication, with image duplication, and with image + caption duplication, including cases\nwith ddf=5 and ddf=20. Notably, we consistently observe a statistically significant correlation in\nall scenarios. Intriguingly, the correlation is stronger when image + caption duplication is present\ncompared to cases with only image duplication.\nno dupl.\nimg+cap dupl.\nimg dupl.\n0\n5\n10\n15\n20\nData duplication factor\n0.55\n0.60\n0.65\n0.70\nSimilairty scores \u2190\nLAION-10k\n0\n5\n10\n15\n20\nData duplication factor\n18\n20\nFID \u2190\nLAION-10k\n0\n5\n10\n15\n20\nData duplication factor\n0.5\n0.6\n0.7\n0.8\nSimilairty scores \u2190\nImagenette\n0\n5\n10\n15\n20\nData duplication factor\n18\n20\n22\nFID \u2190\nImagenette\nFigure 11: Diffusion models trained with different amounts of duplication and the style of duplication. We show\nsimilarity scores and FID scores at different values of ddf for LAION-10k and Imagenette models.\nno dupl.\nimg+cap dupl.\nimg dupl.\n50000\n100000\nLength of training (in iters)\n0.4\n0.5\n0.6\n0.7\nSimilairty scores \u2190\nLAION-10k\n50000\n100000\nLength of training (in iters)\n17\n18\n19\n20\nFID \u2190\nLAION-10k\n50000\n100000\nLength of training (in iters)\n0.40\n0.45\n0.50\n0.55\nSimilairty scores \u2190\nImagenette\n50000\n100000\nLength of training (in iters)\n18\n20\n22\nFID \u2190\nImagenette\nFigure 12: Does training for longer increase memorization?\nno dupl.\nimg+cap dupl.\nimg dupl.\nFalse\nTrue\nText encoder trained\n0.0\n0.2\n0.4\n0.6\nSimilairty scores \u2190\nLAION-10k\nFalse\nTrue\nText encoder trained\n0\n5\n10\n15\nFID \u2190\nLAION-10k\nFalse\nTrue\nText encoder trained\n0.0\n0.2\n0.4\n0.6\nSimilairty scores \u2190\nImagenette\nFalse\nTrue\nText encoder trained\n0\n5\n10\n15\nFID \u2190\nImagenette\nFigure 13: Effect of text encoder training on Similarity and FID scores.\n14\n\ufb01xed capt\nclass capt\nblip capt\nrandom capt\nText conditioning style (Imagenette)\n0\n10\n20\n30\n40\nFID \u2193\n\ufb01netuned model\nFalse\nTrue\nText encoder trained (Imagenette, img+cap dupl.)\n0.0\n0.2\n0.4\n0.6\nSimilairty scores \u2190\n\ufb01xed capt\nclass capt\nblip/og capt\nrandom capt\nFigure 14: Left: FID scores of models trained on Imagenette with different types of conditioning. Right:\nSimilarity scores of models trained on Imagenette with different conditioning and with and without text encoder\ntraining. All the models in this plot are trained with image + caption duplication with ddf=5.\n0.25\n0.50\n0.75\n1.00\nSimilarity scores (top-1)\n0\n2\n4\n6\n8\nDensity\nImagenette\nLAION-10k\n0\n2\n4\n6\nComplexity (Entropy)\n0.0\n0.5\n1.0\n1.5\nDensity\nImagenette\nLAION-10k\n0.0\n0.1\n0.2\nComplexity (Compression)\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\nDensity\nImagenette\nLAION-10k\nFigure 15: Dataset properties LAION-10k vs Imagenette. Left: Self-similarity Middle: Complexity - Entropy\nRight: Complexity - Compression\nTable 2: Correlation coefficients and corresponding p-values between the similarity scores and training data\ncomplexity metrics for different training scenarios.\nDuplication\nddf\nCompression\nEntropy\np-val (Comp.)\np-val (Entr.)\nNo duplication\n0\n-0.2926589\n-0.3230121\n7.91E-80\n8.48E-98\nImage duplication\n5\n-0.2298073\n-0.3034324\n4.31E-49\n5.80E-86\nImage + Cap duplication\n5\n-0.298469\n-0.3412334\n4.19E-83\n1.24E-109\nImage duplication\n20\n-0.0918793\n-0.1559931\n5.83E-09\n3.29E-23\nImage + Cap duplication\n20\n-0.1994873\n-0.2394177\n3.50E-37\n2.98E-53\nF\nMitigation strategies\nF.1\nTraining details\nExcept for the affect of length of training on similairity scores experiment, we trained all the diffusion\nmodels for 100k iterations, including the models trained with a \u201cmitigation\" hueristic. The following\nare the additional hyperparameters relevent to a given strategy.\n\u2022 Multiple Captions (MC): We sample 20 captions from BLIP for all training images. For\nLAION-10k , an additional original caption is included, resulting in a total of 21 captions.\n\u2022 Gaussian Noise (GN): We sample random multivariate Gaussian noise \u223c N(0, I), multiply\nit by a factor of 0.1, and add it to the latent CLIP representations of the caption.\n\u2022 Random Caption Replacement (RC): With a probability of 0.4, we replace the caption of\nan image with a random sequence of words.\n\u2022 Random Token Replacement & Addition (RT): With a probability of 0.1, we either\nreplace tokens/words in the caption with a random word or add a random word at a random\nlocation. This step is performed twice for train time experiments and four times for inference\ntime experiments.\n15\n\u2022 Caption Word Repetition (CWR): We select a word from the given caption and insert it\ninto a random location within the caption with a probability of 0.4. This step is performed\ntwice for train time experiments and four times for inference time experiments.\n\u2022 Random Numbers Addition (RNA): Instead of adding a random word that may alter the\nsemantic meaning, we add a random number from the range 0, 106 with a probability of 0.4.\nThis step is performed twice for train time experiments and four times for inference time\nexperiments.\nF.2\nCLIP score and FID tables\nIn Tab. 1, we presented similarity scores for models trained/inferred with different mitigation strategies.\nIn Tab. 4 and Tab. 3, we present the corresponding CLIP scores and FID scores. Gaussian Noise\nstrategy compromises on the FID score. Random Token/ Random Number addition during inference\nstrategy compromises on CLIP score the most.\nTable 3: FID scores of LAION-10k models trained with various mitigation strategies. Complementary to Tab. 1.\n*collapses to same case.\nTrain time mitigation\nInference time mitigation\nDup style\u2193 / Mit strat.\u2192\nNone\nMC\nGN\nRC\nRT\nCWR\nGNI\nRT\nCWR\nRNA\nNo Duplication\n18.72\n16.63\n20.63\n15.98\n17.16\n16.73\n18.92\n18.67\n18.14\n18.73\nImage Duplication\n18.44\n16.22*\n21.62\n15.92\n15.32\n16.03\n19.04\n19.23\n18.38\n19.19\nImage + Caption Dupl.\n16.94\n16.22*\n18.34\n15.63\n16.25\n16.26\n17.36\n17.75\n16.84\n17.35\nTable 4: CLIP scores of LAION-10k models trained with various mitigation strategies. Complementary to\nTab. 1. *collapses to the same case.\nTrain time mitigation\nInference time mitigation\nDup style\u2193 / Mit strat.\u2192\nNone\nMC\nGN\nRC\nRT\nCWR\nGNI\nRT\nCWR\nRNA\nNo Duplication\n30.52\n30.27\n29.91\n30.64\n30.74\n30.79\n30.32\n29.54\n30.13\n29.74\nImage Duplication\n30.27\n30.03*\n29.52\n30.54\n30.86\n30.83\n29.96\n29.10\n29.74\n29.22\nImage + Caption Dupl.\n30.62\n30.03*\n30.30\n30.57\n30.76\n30.79\n30.42\n29.69\n30.13\n29.74\nF.3\nExtended Qual results\nInference time results.\nThe original and modified prompts used in creating figure Fig. 8 is shown\nin Tab. 5. For the generation, we used Stable Diffusion 1.4 with the generator seed=2, with default\nsettings guidance_scale=7.5, num_inference_steps=50.\nTable 5: In first column, we show a few prompts that induce copying in SD 1.4 model. In the second column,\nwe show the updated prompt after applying Random Token (RT) mitigation strategy. Sometime the random\ntoken is semantically related to the prompt and that might impact the Clipscore of the generation a bit more.\nOriginal Prompts\nModified Prompts\nWall View 003\nWall disappointment View part senator 003 historian\nClassic Cars for Sale\nassen Classic dachsh compositions ;-) Cars for Sale\nMothers influence on her young hippo\nMothers 45460 influence on her 44791 young 32450 50192 hippo\nLiving in the Light with Ann Graham Lotz\nLiving in polaris the Light atrix with Ann Graham ancy Lotz turban\nHopped-Up Gaming:\nEast\nita Hopped-Up Gaming: cricketer sati poutine East\nTrain time results.\nIn Fig. 16, we show extended train time qualitative results for all the mitigation\nstrategies. We see \u201cMultiple Captions (MC)\" mitigation works all the time. In contrast, other methods\nmight or might not always be effective.\n16\n(a)\n(g)\n(b)\n(c)\n(d)\n(e)\n(f)\n(h)\nFigure 16: Qualitative results of various train-time mitigation strategies (a) Training image (b) Generated\nimage without mitigation (c) SD model generation (d) Gen from MC strategy model (e) Gen from GN strategy\nmodel (f) Gen from RC strategy model (g) Gen from RT strategy model (h) Gen from CWR strategy model.\nPlease refer to Appendix F.1 for the full forms of the mitigation strategies\n17\n"
  },
  {
    "title": "Efficient Diffusion Policies for Offline Reinforcement Learning",
    "link": "https://arxiv.org/pdf/2305.20081.pdf",
    "upvote": "2",
    "text": "Efficient Diffusion Policies for Offline Reinforcement\nLearning\nBingyi Kang\u2217 Xiao Ma\u2217\nChao Du\nTianyu Pang\nShuicheng Yan\nSea AI Lab\n{bingykang,yusufma555,duchao0726}@gmail.com {tianyupang,yansc}@sea.com\nAbstract\nOffline reinforcement learning (RL) aims to learn optimal policies from offline\ndatasets, where the parameterization of policies is crucial but often overlooked.\nRecently, Diffsuion-QL [37] significantly boosts the performance of offline RL by\nrepresenting a policy with a diffusion model, whose success relies on a parametrized\nMarkov Chain with hundreds of steps for sampling. However, Diffusion-QL suffers\nfrom two critical limitations. 1) It is computationally inefficient to forward and\nbackward through the whole Markov chain during training. 2) It is incompatible\nwith maximum likelihood-based RL algorithms (e.g., policy gradient methods) as\nthe likelihood of diffusion models is intractable. Therefore, we propose efficient\ndiffusion policy (EDP) to overcome these two challenges. EDP approximately\nconstructs actions from corrupted ones at training to avoid running the sampling\nchain. We conduct extensive experiments on the D4RL benchmark. The results\nshow that EDP can reduce the diffusion policy training time from 5 days to 5\nhours on gym-locomotion tasks. Moreover, we show that EDP is compatible with\nvarious offline RL algorithms (TD3, CRR, and IQL) and achieves new state-of-\nthe-art on D4RL by large margins over previous methods. Our code is available at\nhttps://github.com/sail-sg/edp.\n1\nIntroduction\nD-QL   EDP\n5 Days\n5 Hours\nTraining Time\nLocomotion\nKitchen\nAdroit\nAntmaze\n77.6\n53.3\n56.9\n63\n85.5\n59.4\n71.3\n73.4\nNormalized Score\nPre-SOTA\nOurs\nFigure 1: Efficiency and Generality.\nD-QL is\nDiffusion-QL. Left: The training time on the loco-\nmotion tasks in D4RL. Right: the performance of\nEDP and previous SOTA on each domain in D4RL.\nEDP is trained with TD3 on locomotion and IQL\non the other three domains. (Best viewed in color.)\nOffline reinforcement learning (RL) is much\ndesired in real-world applications as it can ex-\ntract knowledge from previous experiences, thus\navoiding costly or risky online interactions. Ex-\ntending online RL algorithms to the offline do-\nmain faces the distributional shift [9] problem.\nExisting methods mainly focus on addressing\nthis issue by constraining a policy to stay close\nto the data-collecting policy [6, 39], making con-\nservative updates for Q-networks [17, 15, 40], or\ncombining these two strategies [21, 12]. How-\never, Offline RL can also be viewed as a state-\nconditional generative modeling problem of ac-\ntions, where the parameterization of the pol-\nicy network is important but largely overlooked.\nMost offline RL works follow the convention of\nparameterizing the policy as a diagonal Gaussian distribution with the learned mean and variance.\nThis scheme might become inferior when the data distribution is complex, especially when offline\n\u2217equal contribution\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2305.20081v2  [cs.LG]  26 Oct 2023\ndata are collected from various sources and present strong multi-modalities [32]. Therefore, more\nexpressive models for the policy are strongly desired.\nRecently, Diffusion-QL [37] made a successful attempt by replacing the diagonal Gaussian policy with\na diffusion model, significantly boosting the performance of the TD3+BC [6] algorithm. Diffusion\nmodels [34, 10] have achieved the new state-of-the-art (SOTA) in image generation tasks [26, 5],\ndemonstrating a superior ability to capture complex data distributions.\nDespite the impressive improvement that Diffusion-QL has achieved, it has two critical drawbacks\npreventing it from practical applications. First, training a diffusion policy with offline RL is computa-\ntionally inefficient. Consider a parameterized diffusion policy \u03c0\u03b8(a|s), Diffusion-QL optimizes it\nby maximizing the Q value Q(s, a\u03b8) of a state s given a policy-generated action a\u03b8 \u223c \u03c0\u03b8(a | s).\nHowever, sampling from a diffusion model relies on a long parameterized Markov chain (e.g., 1,000\nsteps), whose forward inference and gradient backpropagation are unaffordably expensive. Second,\ndiffusion policy is not a generic policy class as it is restricted to TD3-style algorithms. As computing\nthe sample likelihood \u03c0\u03b8(a | s) is intractable in diffusion models [35], diffusion policy is incompati-\nble with a large family of policy gradient algorithms (e.g., V-Trace [22], AWR [28], IQL [15]), which\nrequire a tractable and differentiable log-likelihood log \u03c0\u03b8(a|s) for policy improvement.\nIn this work, we propose efficient diffusion policy (EDP) to address the above two limitations\nof diffusion policies. Specifically, we base EDP on the denoising diffusion probabilistic model\n(DDPM) [10], which learns a noise-prediction network to predict the noise used to corrupt an example.\nIn the forward diffusion process, a corrupted sample follows a predefined Gaussian distribution when\nthe clean example and timestep are given. In turn, given a corrupted sample and predicted noise, we\ncan approximate its clean version by leveraging the reparametrization trick. Based on this observation,\nto avoid the tedious sampling process, we propose action approximation to build an action from a\ncorrupted one, which can be easily constructed from the dataset. In this way, each training step only\nneeds to pass through the noise-prediction network once, thus substantially reducing the training\ntime. As experimented, by simply adding action approximation, we obtain 2x speed-up without\nperformance loss. Moreover, we apply DPM-Solver [20], a faster ODE-based sampler, to further\naccelerate both the training and sampling process. Finally, to support likelihood-based RL algorithms,\nwe leverage the evidence lower bound for the likelihood developed in DDPM and approximate the\npolicy likelihood from a constructed Gaussian distribution with variance fixed and mean obtained\nfrom action approximation.\nWe evaluate the efficiency and generality of our method on the popular D4RL benchmarking, as\nshown in Fig. 1. We first benchmark the efficiency of EDP on gym-locomotion tasks. By replacing\nthe diffusion policy in Diffusion-QL with our EDP, the training time of Diffusion-QL is reduced\nsubstantially from five days to five hours (compared to their official code). Meanwhile, we observe\nslight to clear performance improvements on different tasks as the improved efficiency enables\ntraining DDPM with more timesteps than before. Moreover, we plug EDP into three different offline\nalgorithms (including TD3+BC, CRR, and IQL), and the results justify its superiority over standard\ndiagonal Gaussian policies. As a result, EDP set up new state-of-the-art on all four domains in D4RL.\n2\nRelated Work\nOffline RL\nDistributional shift between the learned and behavior policies is offline RL\u2019s biggest\nchallenge. Existing research mitigates this problem by making modifications to policy evaluation [24,\n17, 15, 21] or policy improvement [9, 39, 6, 33, 38, 42, 41]. For example, conservative Q-learning\n(CQL) [17] penalizes out-of-distribution actions for having higher Q-values, proving that this is\nequivalent to optimizing a lower bound of Q-values. Onestep RL [1] conducts policy evaluation on\nin-distribution data to avoid querying unseen actions. IQL [15] introduces expectile regression [14]\nto approximate dynamic programming with the Bellman optimality function. TD3+BC explicitly\nconstrains the learned policy by adding a behavior cloning loss to mimic the behavior policy. Instead,\nCRR and AWR impose an implicit policy regularization by performing policy gradient-style policy\nupdates. Despite their effectiveness, they ignore that the capacity of a policy representation plays\na vital role in fitting the data distribution. This paper instead focuses on an orthogonal aspect (i.e.,\npolicy parameterization) that all the above methods can benefit. Another line of work tries to cast\noffline RL as a sequence-to-sequence model [3, 11], which is beyond the scope of this work.\nPolicy Parametrization\nDifferent RL algorithms may pose different requirements for parameteriz-\ning a policy distribution. There are mainly two categories of requirements: 1) The sampling process\n2\nis differentiable, such as the deterministic policy in DDPG [18] and TD3 [7]. 2) The log-likelihood of\nsamples is tractable. For example, policy gradient methods [30, 31, 38, 28] optimize a policy based\non maximum likelihood estimation (MLE). Therefore, most works represent policy with a diagonal\nGaussian distribution with mean and variance parameterized with a multi-layer perceptron (MLP).\nOn the other hand, BCQ [9] and BEAR [16] choose to model policy with a conditional variational\nautoencoder (CVAE). Recently, Diffusion-QL [37] introduced diffusion models into offline RL and\ndemonstrated that diffusion models are superior at modeling complex action distributions than CVAE\nand diagonal Gaussian. However, it takes tens to hundreds more time to train a diffusion policy than\na diagonal Gaussian one. Moreover, diffusion policy only satisfies the first requirement, which means\nmany other offline RL algorithms can not use it, including the current SOTA IQL.\nOur method is motivated to solve the about two limitations. We first propose a more efficient way\nto train diffusion policies, which reduces training time to the level of a Gaussian policy. Then, we\ngeneralize the diffusion policy to be compatible with MLE-based RL methods.\n3\nPreliminaries\n3.1\nOffline Reinforcement Learning\nA decision-making problem in reinforcement learning is usually represented by a Markov De-\ncision Process (MDP): M = {S, A, P, R, \u03b3}.\nS and A are the state and action spaces re-\nspectively, P(s\u2032|s, a) measures the transition probability from state s to state s\u2032 after tak-\ning action a while R(s, a, s\u2032) gives the reward for the corresponding transition, \u03b3 \u2208 [0, 1)\nis the discount factor.\nA policy \u03c0(a|s)2 describes how an agent interacts with the environ-\nment. The optimal policy \u03c0\u2217(a|s) is the one achieves maximal cumulative discounted returns:\n\u03c0\u2217 = arg max E\u03c0 [P\u221e\nt=0 \u03b3tr(st, at)]. Reinforcement learning algorithms frequently rely on the\ndefinition of value function V (s) = E\u03c0 [P\u221e\nt=0 \u03b3tr(st, at)|s0 = s], and action value (Q) function\nQ(s, a) = E\u03c0 [P\u221e\nt=0 \u03b3tr(st, at)|s0 = s, a0 = a], which represents the expected cumulative dis-\ncounted return of a policy \u03c0 given the initial state s or state-action pair (s, a).\nIn the offline RL setting, instead of learning from interactions with the environment, agents\nfocus on learning an optimal policy from a previously collected dataset of transitions: D =\n{(st, at, st+1, rt)}.\nOffline RL algorithms for continuous control are usually based on an\nactor-critic framework that alternates between policy evaluation and policy improvement. Dur-\ning policy evaluation, a parameterized Q network Q\u03d5(s, a) is optimized based on approxi-\nmate dynamic programming to minimize the following temporal difference (TD) error LTD(\u03d5):\nE(s,a,s\u2032)\u223cD\n\u0014\u0010\nr(s, a) + \u03b3 maxa\u2032 Q \u02c6\u03d5(s\u2032, a\u2032) \u2212 Q\u03d5(s, a)\n\u00112\u0015\n, where Q \u02c6\u03d5(s, a) denotes a target net-\nwork. Then at the policy improvement step, knowledge in the Q network is distilled into the policy\nnetwork in various ways. Offline RL methods address the distributional shift [9] problem induced\nby the offline dataset D by either modifying the policy evaluation step to regularize Q learning or\nconstraining the policy improvement directly. In the following, we will show that our diffusion policy\ndesign is compatible with any offline algorithms and can speed up policy evaluation and improvement.\n3.2\nDiffusion Models\nConsider a real data distribution q(x) and a sample x0 \u223c q(x) drawn from it. The (forward) diffusion\nprocess fixed to a Markov chain gradually adds Gaussian noise to the sample in K steps, producing a\nsequence of noisy samples x1, . . . xK. Note that we use superscript k to denote diffusion timestep to\navoid conflicting with the RL timestep. The noise is controlled by a variance schedule \u03b21, . . . , \u03b2K:\nq(xk|xk\u22121) = N(xk;\np\n1 \u2212 \u03b2kxk\u22121, \u03b2kI),\nq(x1:K|x0) =\nK\nY\nk=1\nq(xk|xk\u22121).\n(1)\nWhen K \u2192 \u221e, xK distributes as an isotropic Gaussian distribution. Diffusion models learn a\nconditional distribution p\u03b8(xt\u22121|xt) and generate new samples by reversing the above process:\np\u03b8(x0:K) = p(xK)\nK\nY\nk=1\np\u03b8(xk\u22121|xk),\np\u03b8(xk\u22121|xk) = N(xk\u22121; \u00b5\u03b8(xk, k), \u03a3\u03b8(xk, k)),\n(2)\n2A policy could either be deterministic or stochastic, we use its stochastic form without loss of generality.\n3\nwhere p(xK) = N(0, I) under the condition that QK\nk=1(1 \u2212 \u03b2k) \u2248 0. The training is performed by\nmaximizing the evidence lower bound (ELBO):Ex0[log p\u03b8(x0)] \u2265 Eq\nh\nlog\np\u03b8(x0:K)\nq(x1:K|x0)\ni\n.\n4\nEfficient Diffusion Policy\nIn this section, we detail the design of our efficient diffusion policy (EDP). First, we formulate an\nRL policy with a diffusion model. Second, we present a novel algorithm that can train a diffusion\npolicy efficiently, termed Reinforcement-Guided Diffusion Policy Learning (RGDPL). Then, we\ngeneralize the diffusion policy to work with arbitrary offline RL algorithms and compare our EDP\nwith Diffusion-QL to highlight its superiority in efficiency and generality. Finally, we discuss several\nmethods to sample from the diffusion policy during evaluation.\n4.1\nDiffusion Policy\nFollowing [37], we use the reverse process of a conditional diffusion model as a parametric policy:\n\u03c0\u03b8(a|s) = p\u03b8(a0:K|s) = p(aK)\nK\nY\nk=1\np\u03b8(ak\u22121|ak, s),\n(3)\nwhere aK \u223c N(0, I).\nWe choose to parameterize \u03c0\u03b8 based on Denoising Diffusion Prob-\nabilistic Models (DDPM) [10], which sets \u03a3\u03b8(ak, k; s)\n=\n\u03b2kI to fixed time-dependent\nconstants, and constructs the mean \u00b5\u03b8 from a noise prediction model as:\n\u00b5\u03b8(ak, k; s) =\n1\n\u221a\n\u03b1k\n\u0012\nak \u2212\n\u03b2k\n\u221a\n1\u2212\u00af\u03b1k \u03f5\u03b8(ak, k; s)\n\u0013\n, where \u03b1k = 1 \u2212 \u03b2k, \u00af\u03b1k = Qk\ns=1, and \u03f5\u03b8 is a parametric model.\nTo obtain an action from DDPM, we need to draw samples from K different Gaussian distributions\nsequentially, as illustrated in Eqn. (2)-(3). The sampling process can be reformulated as\nak\u22121 =\n1\n\u221a\n\u03b1k\n\u0012\nak \u2212\n\u03b2k\n\u221a\n1 \u2212 \u00af\u03b1k \u03f5\u03b8(ak, k; s)\n\u0013\n+\np\n\u03b2k\u03f5,\n(4)\nwith the reparametrization trick, where \u03f5 \u223c N(0, I), k is the reverse timestep from K to 0.\nSimilar to DDPM, plugging in the conditional Gaussian distributions, the ELBO in Sec. 3.2 can be\nsimplified to the following training objective Ldiff(\u03b8):\nEk,\u03f5,(a0,s)\n\u0014\r\r\r\u03f5 \u2212 \u03f5\u03b8\n\u0010\u221a\n\u00af\u03b1ka0 +\np\n1 \u2212 \u00af\u03b1k\u03f5, k; s\n\u0011\r\r\r\n2\u0015\n,\n(5)\nwhere k follows a uniform distribution over the discrete set {1, . . . , K}. It means the expectation\nis taken over all diffusion steps from clean action to pure noise. Moreover, \u03f5 \u2208 N(0, I), and\n(a0, s) \u2208 D are state-action pairs drawn from the offline dataset. Given a dataset, we can easily\nand efficiently train a diffusion policy in a behavior-cloning manner as we only need to forward and\nbackward through the network once each iteration. As shown in Diffusion-QL [37], diffusion policies\ncan greatly boost the performance when trained with TD3-based Q learning. However, it still faces\ntwo main drawbacks that limit its real-world application: 1) It is inefficient in sampling and training;\n2) It is not generalizable to other strong offline reinforcement learning algorithms.\n4.2\nReinforcement-Guided Diffusion Policy Learning\nTo understand how a parametric policy \u03c0\u03b8 is trained with offline RL algorithms, we start with a typical\nQ-learning actor-critic framework for continuous control, which iterates between policy evaluation\nand policy improvement. Policy evaluation learns a Q network by minimizing the TD error LTD(\u03d5):\nE(s,a,s\u2032)\u223cD\n\u0014\u0010\nr(s, a) + \u03b3Q \u02c6\u03d5(s\u2032, a\u2032) \u2212 Q\u03d5(s, a)\n\u00112\u0015\n,\n(6)\nwhere the next action a\u2032 \u223c \u03c0\u03b8(\u00b7|s\u2032). The policy is optimized to maximize the expected Q values :\nmax\n\u03b8\nEs\u223cD,a\u223c\u03c0\u03b8(a|s) [Q\u03d5(s, a)] .\n(7)\n4\nIt is straightforward to optimize this objective when a Gaussian policy is used, but things get much\nmore difficult when a diffusion policy is considered due to its complicated sampling process. Instead,\nwe propose to view the offline RL problem from the perspective of generative modeling, where a\ndiffusion policy can be easily learned in a supervised manner from a given dataset. However, unlike\nin computer vision, where the training data are usually perfect, offline RL datasets often contain\nsuboptimal state-action pairs. Suppose we have a well-trained Q network Q\u03d5, the question becomes\nhow we can efficiently use Q\u03d5 to guide diffusion policy training procedure. We now show that this\ncan be achieved without sampling actions from diffusion policies.\nLet\u2019s revisit the forward diffusion process in Eqn. 1. A notable property of it is that the distribution of\nnoisy action ak at any step k can be written in closed form: q(ak|a0) = N(ak;\n\u221a\n\u00af\u03b1ka0, (1 \u2212 \u00af\u03b1k)I).\nUsing the reparametrization trick, we are able to connect ak, a0 and \u03f5 by:\nak =\n\u221a\n\u00af\u03b1ka0 +\np\n1 \u2212 \u00af\u03b1k\u03f5,\n\u03f5 \u223c N(0, I).\n(8)\nRecall that our diffusion policy is parameterized to predict \u03f5 with \u03f5\u03b8(ak, k; s). By relacing \u03f5 with\n\u03f5\u03b8(ak, k; s) and rearranging Eqn. (8), we obtain the approximiated action:\n\u02c6a0 =\n1\n\u221a\n\u00af\u03b1k ak \u2212\n\u221a\n1 \u2212 \u00af\u03b1k\n\u221a\n\u00af\u03b1k\n\u03f5\u03b8(ak, k; s).\n(9)\nIn this way, instead of running the reverse diffusion process to sample an action a0, we can cheaply\nconstruct \u02c6a0 from a state-action pair (s, a) in the dataset by first corrupting the action a to ak then\nperforming one-step denoising to it. We will refer to this technique as action approximation in the\nfollowing. Accordingly, the policy improvement for diffusion policies is modified as follows:\nL\u03c0(\u03b8) = \u2212Es\u223cD,\u02c6a0 \u0002\nQ\u03d5(s, \u02c6a0)\n\u0003\n.\n(10)\nTo improve the efficiency of policy evaluation, we propose to replace the DDPM sampling in Eqn. (4)\nwith DPM-Solver [20], which is an ODE-based sampler. The algorithm is defered to the appendix.\n4.3\nGeneralization to Various RL algorithms\nThere are mainly two types of approaches to realize the objective in Eqn. 7 for policy improvement.\nDirect policy optimization: It maximizes Q values and directly backpropagate the gradients from Q\nnetwork to policy network, i.e., \u2207\u03b8L\u03c0(\u03b8) = \u2212 \u2202Q\u03d5(s,a)\n\u2202a\n\u2202a\n\u2202\u03b8 . This is only applicable to cases where\n\u2202a\n\u2202\u03b8 is tractable, e.g., when a deterministic policy a = \u03c0\u03b8(s) is used or when the sampling process\ncan be reparameterized. Sample algorithms belonging to this category include TD3 [7], TD3+BC [6],\nand CQL [17]. One can easily verify that both the expensive DDPM sampling in Eqn. (4) and our\nefficient approximation in Eqn. (9) can be used for direct policy optimization.\nLikelihood-based policy optimization: It tries to distill the knowledge from the Q network into the\npolicy network indirectly by performing weighted regression or weighted maximum likelihood:\nmax\n\u03b8\nE(s,a)\u223cD [f(Q\u03d5(s, a)) log \u03c0\u03b8(a|s)] ,\n(11)\nwhere f(Q\u03d5(s, a)) is a monotonically increasing function that assigns a weight to each state-action\npair in the dataset. This objective requires the log-likelihood of the policy to be tractable and\ndifferentiable. AWR [28], CRR [38], and IQL [15] fall into this category but each has a unique\ndesign in terms of the weighting function f. Since the likelihood of samples in Diffusion models is\nintractable, we propose the following two variants for realizing Eqn. 11.\nFirst, instead of computing the likelihood, we turn to a lower bound for log \u03c0\u03b8(a|s) introduced in\nDDPM [10]. By discarding the constant term that does not depend on \u03b8, we can have the objective:\nEk,\u03f5,(a,s)\n\u0014\u03b2k \u00b7 f(Q\u03d5(s, a))\n2\u03b1k(1 \u2212 \u00af\u03b1k\u22121)\n\r\r\u03f5 \u2212 \u03f5\u03b8\n\u0000ak, k; s\n\u0001\r\r2\u0015\n.\n(12)\nSecond, instead of directlying optimizing log \u03c0\u03b8(a|s), we propose to replace it with an approximated\npolicy \u02c6\u03c0\u03b8(a|s) \u225c N(\u02c6a0, I), where \u02c6a0 is from Eqn. (9). Then, we get the following objective:\nEk,\u03f5,(a,s)\nh\nf(Q\u03d5(s, a))\n\r\ra \u2212 \u02c6a0\r\r2i\n.\n(13)\n5\nEmpirically, we find these two choices perform similarly, but the latter is easier to implement. So\nwe will report results mainly based on the second realization. In our experiments, we consider two\noffline RL algorithms under this category, i.e., CRR, and IQL. They use two weighting schemes:\nfCRR = exp\n\u0002\u0000Q\u03d5(s, a) \u2212 Ea\u2032\u223c\u02c6\u03c0(a|s)Q(s, a\u2032)\n\u0001\n/\u03c4CRR\n\u0003\nand fIQL = exp [(Q\u03d5(s, a) \u2212 V\u03c8(s)) /\u03c4IQL],\nwhere \u03c4 refers to the temperature parameter and V\u03c8(s) is an additional value network parameterized\nby \u03c8. We defer the details of these two algorithms to Appendix A.\n4.4\nComparison to Diffusion-QL\nNow we are ready to compare our method and Diffusion-QL comprehensively. Though our EDP\nshares the same policy parametrization as Diffusion-QL, it differs from Diffusion-QL significantly in\nthe training algorithm. As a result, the computational efficiency and generality of diffusion policies\nhave been improved substantially.\nEfficiency The diffusion policy affects both policy evaluation (Eqn. (6)) and policy improvement\n(Eqn. (7)). First, calculating LTD(\u03d5) in policy evaluation requires drawing the next action from\nit. Diffusion-QL uses DDPM sampling while EDP employs a DPM-Solver, which can reduce the\nsampling steps from 1000 to 15, thus accelerating the training. Second, in policy improvement,\nDiffusion-QL again applies DDPM for sampling. Then, it calculates the loss function based on\nsampled actions and backpropagates through the sampling process for network update. This means\nit needs to forward and backward a neural network for K times each training iteration. As a result,\nDiffusion-QL can only work with small K, e.g., 5 \u223c 100. In comparison, our training scheme only\npasses through the network once an iteration, no matter how big K is. This enables EDP to use a\nlarger K (1000 in our experiments) to train diffusion policy on the more fine-grained scale. The\nresults in Tab. 1 also show a larger K can give better performance.\nGenerality Diffusion-QL can only work with direct policy optimization, which contains only a small\nportion of algorithms. Moreover, thanks to their flexibility and high performance, the likelihood-based\nalgorithms are preferred for some tasks (e.g., Antmaze). Our method successfully makes diffusion\ntrainable with any RL algorithm.\n4.5\nControlled Sampling from Diffusion Policies\nTraditionally, a continuous policy is represented with a state-conditional Gaussian distribution. During\nevaluation time, a policy executes deterministically to reduce variance by outputting the distribution\nmean as an action. However, with diffusion policies, we can only randomly draw a sample from the\nunderlying distribution without access to its statistics. As a result, the sampling process is noisy, and\nthe evaluation is of high variance. We consider the following method to reduce variance.\nEnergy-based Action Selection (EAS)\nRecall that the goal of (offline) RL is to learn a policy\nthat can maximize the cumulative return or values. Though the policy \u03c0\u03b8 is stochastic, the learned\nQ\u03d5 provides a deterministic critic for action evaluation. We can sample a few actions randomly, then\nuse Q\u03d5 for selection among them to eliminate randomness. EAS first samples N actions from \u03c0\u03b8 by\nusing any samplers (i.e., DPM-Solver), then sample one of them with weights proportional to eQ(s,a).\nThis procedure can be understood as sampling from an improved policy p(a|s) \u221d eQ(s,a)\u03c0\u03b8(a|s).\nAll results will be reported based on EAS. See Appendix. C.4 for the other two methods.\n5\nExperiments\nWe conduct extensive experiments on the D4RL benchmark [2] to verify the following assumptions:\n1) Our diffusion policy is much more efficient than the previous one regarding training and evaluation\ncosts. 2) Our diffusion policy is a generic policy class that can be learned through direct and\nlikelihood-based policy learning methods. We also provide various ablation studies on the critical\ncomponents for better understanding.\nBaselines\nWe evaluate our method on four domains in D4RL, including Gym-locomotion, AntMaze,\nAdroit, and Kitchen. For each domain, we consider extensive baselines to provide a thorough\nevaluation. The simplest method is the classific behavior cloning (BC) baseline and 10% BC that\nperforms behavior cloning on the best 10% data. TD3+BC [6] combines off-policy reinforcement\n6\nlearning algorithms with BC. OneStepRL [1] first conducts policy evaluation to obtain the Q-value\nof the behavior policy from the offline dataset, then use it for policy improvement. AWAC [25],\nAWR [28], and CRR [38] improve policy improvement by adding advantage-based weights to policy\nloss functions. CQL [17] and IQL [15] constrain the policy evaluation process by making conservative\nQ updates or replacing the max operator with expectile regression. We also consider the Decision\nTransformer (DT) [4] baseline that maps offline RL as a sequence-to-sequence translation problem.\nExperimental Setup\nWe keep the backbone network architecture the same for all tasks and\nalgorithms, which is a 3-layer MLP (hidden size 256) with Mish [23] activation function following\nDiffusion-QL [37]. For the noise prediction network \u03f5\u03b8(ak, k; s) in diffusion policy, we first encode\ntimestep k with sinusoidal embedding [36], then concatenate it with the noisy action ak and the\nconditional state s. We use the Adam [13] to optimize both diffusion policy and the Q networks. The\nmodels are trained for 2000 epochs on Gym-locomotion and 1000 epochs on the other three domains.\nEach epoch consists of 1000 iterations of policy updates with batch size 256. For DPM-Solver [20],\nwe use the third-order version and set the model call steps to 15. We reimplement DQL strictly\nfollowing the official PyTorch code [27] for fair comparison and we refer to DQL (JAX) for all\nsample efficiency comparisons. We defer the complete list of all hyperparameters to the appendix\ndue to space limits. Throughout this paper, the results are reported by averaging 5 random seeds.\nEvaluation Protocal\nWe consider two evaluation metrics in this paper. First, online model selection\n(OMS), proposed by Diffusion-QL [37], selects the best-performing model throughout the whole\ntraining process. However, though OMS can reflect an algorithm\u2019s capacity, it is cheating, especially\nwhen the training procedure is volatile on some of the tasks. Therefore, we propose another metric to\nfocus on the training stability and quality, which is running average at training (RAT). RAT calculates\nthe running average of evaluation performance for ten consecutive checkpoints during training and\nreports the last score as the final performance.\n5.1\nEfficiency and Reproducibility\nTraining\nSampling\n0\u00d7\n1\u00d7\n2\u00d7\n3\u00d7\n4\u00d7\n5\u00d7\nSpeedup on Gym-locomotion\nDQL\nDQL(Jax)\nEDP w/o DPM\nEDP w/o AP\nEDP\nFigure 2: Training and evaluation speed compari-\nson. The training IPS are: 4.66, 22.30, 50.94, 38.4,\nand 116.21. The sampling SPS are: 18.67, 123.70,\n123.06, 411.0, 411.79.\nIn this section, we focus on the training and\nevaluation efficiency of our efficient diffusion\npolicy. We choose the OMS evaluation met-\nric to make a fair comparison with the baseline\nmethod Diffusion-QL [37]. We consider four\nvariants of EDP to understand how each com-\nponent contributes to the high efficiency of our\nmethod. 1) EDP is the complete version of our\nmethod. It uses the action approximation tech-\nnique in Eqn. (9) for policy training and uses\nDPM-Solver for sampling. 2) EDP w/o DPM\nmodifies EDP by replacing DPM-Solver with\nthe original DDPM sampling method in Eqn. 4.\n3) EDP w/o AP removes the action approxima-\ntion technique. 4) DQL (JAX) is our Jax implementation of Diffusion-QL.\nWe first benchmark and compare the training/evaluation speed of the above three variants and\nDiffusion-QL. We choose walker2d-medium-expert-v2 as the testbed. For training speed, we run each\nalgorithm for 10,000 iterations of policy updates and calculate the corresponding iterations-per-second\n(IPS). Similarly, we sample 10,000 transitions by interacting with the environment and calculate the\ncorresponding steps-per-second (SPS) for evaluation speed. Based on the visualization in Fig. 2,\nby taking DQL (JAX) as the baseline, we are able to attribute the performance boost to specific\ntechniques proposed. Specifically, we can observe that action approximation makes 2.3x training\nand 3.3x sampling faster, while using the DPM-Solver adds an additional 2.3x training speedup. We\ncan observe that DQL (JAX) is 5\u00d7 faster than Diffusion-QL, which means our Jax implementation\nis more computationally efficient than Diffusion-QL\u2019s PyTorch code. This demonstrates that both\nthe action approximation technique and DPM-Solver play a critical role in making the training of\ndiffusion policy efficient. However, this technique does not affect the sampling procedure; thus, EDP\nw/o DPM and DQL (JAX) are on par with each other regarding sampling speed.\n7\nTable 2: Average normalized score on the D4RL benchmark. FF denotes the original policies\nparameterized with feed-forward neural networks, while EDP is our efficient diffusion policy. Results\nof baselines are taken directly from [15]. For the missing results of 10%, AWAC, and OneStep RL,\nwe re-implement the baselines and report the results. Our results are reported with the RAT metric.\nDataset\nBC\n10%BC\nDT\nAWAC OneStep CQL\nTD3+BC\nCRR\nIQL\nFF\nEDP\nFF\nEDP\nFF\nEDP\nhalfcheetah-medium-v2\n42.6\n42.5\n42.6\n43.5\n48.4\n44.0\n48.3\n52.1\n44.0\n49.2\n47.4\n48.1\nhopper-medium-v2\n52.9\n56.9\n67.6\n57.0\n59.6\n58.5\n59.3\n81.9\n58.5\n78.7\n66.3\n63.1\nwalker2d-medium-v2\n75.3\n75.0\n74.0\n72.4\n81.8\n72.5\n83.7\n86.9\n72.5\n82.5\n78.3\n85.4\nhalfcheetah-medium-replay-v2\n36.6\n40.6\n36.6\n40.5\n38.1\n45.5\n44.6\n49.4\n45.5\n43.5\n44.2\n43.8\nhopper-medium-replay-v2\n18.1\n75.9\n82.7\n37.2\n97.5\n95.0\n60.9\n101.0\n95.0\n99.0\n94.7\n99.1\nwalker2d-medium-replay-v2\n26.0\n62.5\n66.6\n27.0\n49.5\n77.2\n81.8\n94.9\n77.2\n63.3\n73.9\n84.0\nhalfcheetah-medium-expert-v2\n55.2\n92.9\n86.8\n42.8\n93.4\n91.6\n90.7\n95.5\n91.6\n85.6\n86.7\n86.7\nhopper-medium-expert-v2\n52.5\n110.9\n107.6\n55.8\n103.3\n105.4\n98.0\n97.4\n105.4\n92.9\n91.5\n99.6\nwalker2d-medium-expert-v2\n107.5\n109.0\n108.1\n74.5\n113.0\n108.8 110.1\n110.2\n108.8\n110.1\n109.6\n109.0\naverage\n51.9\n74.0\n74.7\n50.1\n76.1\n77.6\n75.3\n85.5\n77.6\n78.3\n77.0\n79.9\nkitchen-complete-v0\n65.0\n7.2\n-\n39.3\n57.0\n43.8\n2.2\n61.5\n43.8\n73.9\n62.5\n75.5\nkitchen-partial-v0\n38.0\n66.8\n-\n36.6\n53.1\n49.8\n0.7\n52.8\n49.8\n40.0\n46.3\n46.3\nkitchen-mixed-v0\n51.5\n50.9\n-\n22.0\n47.6\n51.0\n0.0\n60.8\n51.0\n46.1\n51.0\n56.5\naverage\n51.5\n41.6\n-\n32.6\n52.6\n48.2\n1.0\n58.4\n48.2\n53.3\n53.3\n59.4\npen-human-v0\n63.9\n-2.0\n-\n15.6\n71.8\n37.5\n5.9\n48.2\n37.5\n70.2\n71.5\n72.7\npen-cloned-v0\n37.0\n0.0\n-\n24.7\n60.0\n39.2\n17.2\n15.9\n39.2\n54.0\n37.3\n70.0\naverage\n50.5\n-1.0\n-\n20.2\n65.9\n38.4\n11.6\n32.1\n38.4\n62.1\n54.4\n71.3\nantmaze-umaze-v0\n54.6\n62.8\n59.2\n56.7\n64.3\n74.0\n40.2\n96.6\n0.0\n95.9\n87.5\n94.2\nantmaze-umaze-diverse-v0\n45.6\n50.2\n53.0\n49.3\n60.7\n84.0\n58.0\n69.5\n41.9\n15.9\n62.2\n79.0\nantmaze-medium-play-v0\n0.0\n5.4\n0.0\n0.0\n0.3\n61.2\n0.2\n0.0\n0.0\n33.5\n71.2\n81.8\nantmaze-medium-diverse-v0\n0.0\n9.8\n0.0\n0.7\n0.0\n53.7\n0.0\n6.4\n0.0\n32.7\n70.0\n82.3\nantmaze-large-play-v0\n0.0\n0.0\n0.0\n0.0\n0.0\n15.8\n0.0\n1.6\n0.0\n26.0\n39.6\n42.3\nantmaze-large-diverse-v0\n0.0\n6.0\n0.0\n1.0\n0.0\n14.9\n0.0\n4.4\n0.0\n58.5\n47.5\n60.6\naverage\n16.7\n22.4\n18.7\n18.0\n20.9\n50.6\n16.4\n29.8\n7.0\n43.8\n63.0\n73.4\nTable 1: The performance of Diffusion-QL with\nefficient diffusion policy. The results for Diffusion-\nQL are directly quoted from [37]. All results are\nreported based on the OMS metric.\nDataset\nDiffusion-QL\nDQL (JAX)\nEDP\nlocomotion\n89.4\n89.4\n90.3\nantmaze\n75.4\n78.6\n77.9\nadroit\n68.3\n66.5\n89.1\nkitchen\n71.6\n83.0\n80.5\nTo show that EDP does not hurt performance, we\ncompare the normalized scores of all tasks in Tab. 1.\nThe results for Diffusion-QL are directly copied from\nthe paper, where each task is carefully tuned with its\nown hyperparameters. Instead, DQL (JAX) and EDP\nuse the same hyperparameters for tasks belonging\nto the same domain. Moreover, since EDP is more\ncomputationally efficient, we can train a better score\nmodel with large K = 1000, which is larger than\nthe one (5\u223c100) used in Diffusion-QL. Note that\ntraining diffusion policies with large K is impossible\nin Diffusion-QL, as it needs to forward and backward the neural network K times. Finally, we can\nobserve that EDP and DQL (JAX) are comparable with Diffusion-QL on gym-locomotion tasks but\nmuch better on the other three domains. Hence, efficient diffusion policy can boost sample efficiency\nand improve performance by enabling diffusion training in fine-grained noise prediction.\n5.2\nGenerality and Overall Results\nThis section aims to evaluate whether EDP is a general policy class. To this end, we train it with\ndirect policy optimization (TD3) and likelihood-based policy optimization (CRR, IQL) methods.\nThen, we compare them with their feed-forward counterparts and other baseline methods in Table 2.\nAll scores for diffusion policies are reported using the RAT metric, while other scores are directly\nquoted from their paper. It shows that EDP can beat the standard Gaussian policy parameterized with\nan MLP on all domains and for all the three algorithms considered. On the gym-locomotion domain,\nEDP + TD3 gives the best performance (average score 85.5), while likelihood-based policy learning\nmethods are slightly worse. However, on the other three domains (Kitchen, Adroit, and Antmaze),\nEDP + IQL beats all the other methods by a large margin (more than 10 average scores). Therefore,\nwe conclude that EDP can serve as a plug-in policy class for different RL methods.\n5.3\nAblation Study\n8\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized Return\nWalker2d\nAverage\nBest\nTraining Steps\n0.2\n0.4\n0.6\n0.8\n1.0\nHopper\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAntmaze\nFigure 3: Training curves for EDP +TD3 on three representative\nenvironments. Average represents RAT, Best represents OMS.\nEvaluation\nMetrics\nTo\nreveal\nthat\nevaluation\nmetric\nis\nimpor-\ntant,\nwe\ntrain\nEDP\nwith\nTD3\nalgorithms on three selected environ-\nments: walker2d-medium-expert-v2,\nhopper-medium-expert-v2,\nand\nantmaze-medium-diverse-v0.\nWe\nthen compare the scores for OMS\n(best) and RAT (average) by plotting\nthe training curves in Fig. 3.\nOn\nwalker2d, the training is stable; thus, both OMS and RAT scores steadily grow and result in close\nfinal scores. A similar trend can be observed on the hopper but with a more significant gap between\nthese two metrics. However, these two metrics diverge significantly when the training succeeds and\nthen crashes on antmaze. Therefore, OMS is misleading and can not give a reliable evaluation of\nalgorithms, which explains the necessity of using RAT in Sec. 5.2.\n1\n10\n100\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nNormalized Return\nhalfcheetah\nmedium\nmedium-replay\nmedium-expert\n1\n10\n100\nNumber of Samples\n0.6\n0.7\n0.8\n0.9\n1.0\nhopper\n1\n10\n100\n0.80\n0.85\n0.90\n0.95\n1.00\n1.05\n1.10\nwalker2d\nFigure 4: Performance of different number of actions used in EAS.\nThe experiments are conducted on the nine locomotion tasks.\nEnergy-Based\nAction\nSelection\nWe notice that energy-based action\nselection (EAS) is a general method\nand can also be used for arbitrary\npolicies. We apply EAS to normal\nTD3+BC and find no improvement,\nwhich shows EAS is only necessary\nfor diffusion sampling. The results are\ndeferred to the Appendix. Moreover,\nset the number of actions used in\nEAS from 1 to 200, and report the performance on gym-locomotions tasks in Fig. 4. It shows\nthe normalized score monotonically grows as the number of actions increases on 8 out of 9 tasks.\nIn our main experiments, we set the number of actions to 10 by trading off the performance and\ncomputation efficiency.\n5\n10\n15\n20\n25\n30\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nNormalized Return\nhalfcheetah\nmedium\nmedium-replay\nmedium-expert\n5\n10\n15\n20\n25\n30\nNumber of Model Calls\n0.2\n0.4\n0.6\n0.8\n1.0\nhopper\n5\n10\n15\n20\n25\n30\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nwalker2d\nFigure 5: Performance of DPM-Solver with varying steps. The\nexperiments are conducted on the nine locomotion tasks.\nDPM-Solver\nWe are using DPM-\nSolver to speed up the sampling pro-\ncess of diffusion policies. The number\nof models in DPM-Solver is an impor-\ntant hyper-parameter that affects sam-\npling efficiency and quality. We vary\nthis number from 3 to 30 and compare\nthe performance on gym-locomotion\ntasks in Fig. 5. We can observe that\nthe performance increases as more\nsteps of model calls are used. The performance gradually plateaus after 15 model calls. There-\nfore, we use 15 in our main experiments.\n6\nConclusion\nDiffusion policy has emerged as an expressive policy class for offline reinforcement learning. Despite\nits effectiveness, diffusion policy is limited by two drawbacks, hindering it from wider applications.\nFirst, training a diffusion policy requires to forward and backward through a long parameterized\nMarkov chain, which is computationally expensive. Second, the diffusion policy is a restricted\npolicy class that can not work with likelihood-based RL algorithms, which are preferred in many\nscenarios. We propose efficient diffusion policy (EDP) to address these limitations and make diffusion\npolicies faster, better, and more general. EDP relies on an action approximation to construct actions\nfrom corrupted ones, thus avoiding running the Markov chain for action sampling at training. Our\nbenchmarking shows that EDP achieves 25\u00d7 speedup over Diffusion-QL at training time on the\ngym-locomotion tasks in D4RL. We conducted extensive experiments by training EDP with various\noffline RL algorithms, including TD3, CRR, and IQL, the results clearly justify the superiority of\ndiffusion policies over Gaussian policies. As a result, EDP set new state-of-the-art on all four domains\nin D4RL.\n9\nReferences\n[1] David Brandfonbrener, Will Whitney, Rajesh Ranganath, and Joan Bruna. Offline rl without\noff-policy evaluation. Advances in Neural Information Processing Systems, 34:4933\u20134946,\n2021.\n[2] David Brandfonbrener, Will Whitney, Rajesh Ranganath, and Joan Bruna. Offline rl without\noff-policy evaluation. Advances in Neural Information Processing Systems, 34:4933\u20134946,\n2021.\n[3] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter\nAbbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning\nvia sequence modeling. Advances in neural information processing systems, 34:15084\u201315097,\n2021.\n[4] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter\nAbbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning\nvia sequence modeling. Advances in neural information processing systems, 34:15084\u201315097,\n2021.\n[5] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis.\nAdvances in Neural Information Processing Systems, 34:8780\u20138794, 2021.\n[6] Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning.\nAdvances in neural information processing systems, 34:20132\u201320145, 2021.\n[7] Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error\nin actor-critic methods. In International conference on machine learning, pages 1587\u20131596.\nPMLR, 2018.\n[8] Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error\nin actor-critic methods. In International conference on machine learning, pages 1587\u20131596.\nPMLR, 2018.\n[9] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning\nwithout exploration. In International conference on machine learning, 2019.\n[10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances\nin Neural Information Processing Systems, 33:6840\u20136851, 2020.\n[11] Michael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big\nsequence modeling problem. Advances in neural information processing systems, 34:1273\u2013\n1286, 2021.\n[12] Bingyi Kang, Xiao Ma, Yirui Wang, Yang Yue, and Shuicheng Yan. Improving and benchmark-\ning offline reinforcement learning algorithms. arXiv preprint arXiv:2306.00972, 2023.\n[13] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\n[14] Roger Koenker and Kevin F Hallock. Quantile regression. Journal of economic perspectives,\n15(4):143\u2013156, 2001.\n[15] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit\nq-learning. arXiv preprint arXiv:2110.06169, 2021.\n[16] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-\npolicy q-learning via bootstrapping error reduction. Advances in Neural Information Processing\nSystems, 32, 2019.\n[17] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for\noffline reinforcement learning. Advances in Neural Information Processing Systems, 33:1179\u2013\n1191, 2020.\n[18] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,\nDavid Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv\npreprint arXiv:1509.02971, 2015.\n[19] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,\nDavid Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv\npreprint arXiv:1509.02971, 2015.\n10\n[20] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A\nfast ode solver for diffusion probabilistic model sampling in around 10 steps. arXiv preprint\narXiv:2206.00927, 2022.\n[21] Xiao Ma, Bingyi Kang, Zhongwen Xu, Min Lin, and Shuicheng Yan. Mutual information\nregularized offline reinforcement learning. arXiv preprint arXiv:2210.07484, 2022.\n[22] Michael Mathieu, Sherjil Ozair, Srivatsan Srinivasan, Caglar Gulcehre, Shangtong Zhang, Ray\nJiang, Tom Le Paine, Konrad Zolna, Richard Powell, Julian Schrittwieser, et al. Starcraft ii\nunplugged: Large scale offline reinforcement learning. In Deep RL Workshop NeurIPS 2021,\n2021.\n[23] Diganta Misra. Mish: A self regularized non-monotonic neural activation function. arXiv\npreprint arXiv:1908.08681, 4(2):10\u201348550, 2019.\n[24] Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between\nvalue and policy based reinforcement learning. Advances in neural information processing\nsystems, 2017.\n[25] Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. Awac: Accelerating online\nreinforcement learning with offline datasets. arXiv preprint arXiv:2006.09359, 2020.\n[26] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic\nmodels. In International Conference on Machine Learning, pages 8162\u20138171. PMLR, 2021.\n[27] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas\nKopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,\nBenoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style,\nhigh-performance deep learning library. In Advances in Neural Information Processing Systems\n32, pages 8024\u20138035. Curran Associates, Inc., 2019.\n[28] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:\nSimple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.\n[29] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:\nSimple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.\n[30] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust\nregion policy optimization. In International conference on machine learning, pages 1889\u20131897.\nPMLR, 2015.\n[31] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n[32] Nur Muhammad Mahi Shafiullah, Zichen Jeff Cui, Ariuntuya Altanzaya, and Lerrel Pinto.\nBehavior transformers: Cloning k modes with one stone. arXiv preprint arXiv:2206.11251,\n2022.\n[33] Noah Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael\nNeunert, Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin Riedmiller. Keep doing\nwhat worked: Behavior modelling priors for offline reinforcement learning. In International\nConference on Learning Representations, 2020.\n[34] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper-\nvised learning using nonequilibrium thermodynamics. In International Conference on Machine\nLearning, pages 2256\u20132265. PMLR, 2015.\n[35] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of\nscore-based diffusion models. Advances in Neural Information Processing Systems, 34:1415\u2013\n1428, 2021.\n[36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\n[37] Zhendong Wang, Jonathan J Hunt, and Mingyuan Zhou. Diffusion policies as an expressive\npolicy class for offline reinforcement learning. arXiv preprint arXiv:2208.06193, 2022.\n[38] Ziyu Wang, Alexander Novikov, Konrad Zolna, Josh S Merel, Jost Tobias Springenberg, Scott E\nReed, Bobak Shahriari, Noah Siegel, Caglar Gulcehre, Nicolas Heess, et al. Critic regularized\nregression. Advances in Neural Information Processing Systems, 33:7768\u20137778, 2020.\n11\n[39] Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement\nlearning. arXiv preprint arXiv:1911.11361, 2019.\n[40] Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea\nFinn. Combo: Conservative offline model-based policy optimization. Advances in neural\ninformation processing systems, 34:28954\u201328967, 2021.\n[41] Yang Yue, Bingyi Kang, Xiao Ma, Gao Huang, Shiji Song, and Shuicheng Yan. Offline\nprioritized experience replay. arXiv preprint arXiv:2306.05412, 2023.\n[42] Yang Yue, Bingyi Kang, Xiao Ma, Zhongwen Xu, Gao Huang, and Shuicheng Yan. Boosting\noffline reinforcement learning via data rebalancing. arXiv preprint arXiv:2210.09241, 2022.\n12\nBroader Impact\nIn this paper, we propose an efficient yet powerful policy class for offline reinforcement learning.\nWe show that this method is superior to most existing methods on simulated robotic tasks. However,\nsome war robots or weapon robots might employ our EDP to learn strategic agents considering the\ngeneralization ability of EDP. Depending on the specific application scenarios, it might be harmful to\ndomestic privacy and safety.\nA\nReinforcement Learning Algorithms\nIn this section, we introduce the details of the RL algorithms experimented.\nA.1\nTD3+BC\nTD3 [8] is a popular off-policy RL algorithm for continuous control. TD3 improves DDPG [19] by\naddressing the value overestimation issue. Specifically, TD3 adopts a double Q-learning paradigm\nthat computes the TD-target \u02c6Q(s, a) as\n\u02c6Q(s, a) = r(s, a) + \u03b3 min(Q1(s\u2032, a\u2032), Q2(s\u2032, a\u2032)),\na\u2032 = \u03c0(s\u2032),\n(14)\nwhere \u03c0(s\u2032) is a deterministic policy, Q1 and Q2 are two independent value networks. Specifically,\nTD3 takes only Q1 for policy improvement\n\u03c0\u2217 = arg max\n\u03c0\nE [Q1(s, \u02c6a)] ,\n\u02c6a = \u03c0(s).\n(15)\nBuilt on top of TD3, TD3+BC simply adds an additional behavior cloning term for its policy\nimprovement\n\u03c0\u2217 = arg max\n\u03c0\nEs,a\u223cD\n\u0002\nQ(s, \u02c6a) \u2212 \u03b1(\u02c6a \u2212 a)2\u0003\n,\n\u02c6a = \u03c0(s),\n(16)\nwhere \u03b1 is a hyper-parameter that balances these two terms. However, as the scale of Q-values are\ndifferent from the behavior cloning loss, TD3+BC normalizes it with 1\nN\nN\nP\ni=1\n|Q(si, \u02c6ai)| for numerical\nstability.\nIn the context of EDP, we observe that 1) the behavior cloning term can be naturally achieved as a\ndiffusion loss as defined in Eqn. 5; 2) the sampled action \u02c6a is replaced by action approximation as in\nEqn. 9 for efficient policy improvement. Different from the original TD3+BC that uses only Q1 for\npolicy improvement, we sample from Q1 and Q2 with equal probability for each policy improvement\nstep.\nA.2\nCritic Regularized Regression\nCRR follows Advantage Weighted Regression (AWR) [29], which is a simple yet effective off-policy\nRL method, for policy improvement. Specifically, AWR considers a constrained policy improvement\nstep\narg max\n\u03c0\nZ\ns\nd\u00b5(s)\nZ\na\n\u03c0(a | s)A(s, a)dads,\ns.t.\nZ\ns\nd\u00b5(s)DKL [\u03c0(\u00b7 | s) \u2225 \u00b5(\u00b7 | s)] \u2264 \u03f5,\n(17)\nwhere A(s, a) is the advantage function, \u00b5 is a behavior policy that is used to generate trajectories\nduring off-policy RL, d\u00b5(s) is the state distribution induced by \u00b5, DKL is the KL divergence, and \u03f5 is\na threshold parameter. This constrained optimization problem can be solved in closed form, which\ngives an optimal policy of\n\u03c0\u2217(a | s) =\n1\nZ(s)\u00b5(a | s) exp\n\u0012 1\n\u03b2 A(s, a)\n\u0013\n,\n(18)\nwith Z(s) being the partition function and \u03b2 is a hyper-parameter. For policy improvement, AWR\nsimply distills the one-step improved optimal policy to the learning policy \u03c0 by minimizing the\n13\nKL-divergence\narg min\n\u03c0 Es\u223cd\u00b5(s) [DKL [\u03c0\u2217(\u00b7 | s) \u2225 \u03c0(\u00b7 | s)]]\n= arg max\n\u03c0\nEs\u223cd\u00b5(s),a\u223c\u00b5(\u00b7|s)\n\u0014\nlog \u03c0(a | s) exp( 1\n\u03b2 A(s, a))\n\u0015\n(19)\nCRR performs policy improvement in the same way as AWR, which simply replaces the sampling\ndistribution with a fixed dataset\narg max\n\u03c0\nEs,a\u223cD\n\u0014\nlog \u03c0(a | s) exp( 1\n\u03b2 A(s, a))\n\u0015\n.\n(20)\nAs a result, this naturally imposes an implicit constraint on its policy improvement step.\nHowever, as computing the log-likelihood log \u03c0(a | s) is intractable in diffusion models, in practical\nimplementations, we use Eqn. 13 instead. In addition, we compute the advantage by\nA(s, a) = min(Q1(s, a) \u2212 Q2(s, a)) \u2212 1\nN\nN\nX\ni=1\nmin(Q1(s, \u02c6ai), Q2(s, \u02c6ai)),\n(21)\nwhere \u02c6ai \u223c N(\u02c6a0, \u03c3) is a sampled action with the mean of approximated action and an additional\nstandard deviation. In our experiment, we found using a fixed standard deviation, an identity matrix\nof \u03c3 = I, \u03b2 = 1, and sample size N = 10 generally produce a good performance.\nA.3\nImplicit Q Learning\nSimilar to CRR, IQL also adopts the AWR-style policy improvement that naturally imposes a\nconstraint to encourage the learning policy to stay close to the behavior policy that generates\nthe dataset. Different from CRR query novel and potentially out-of-distribution (OOD) actions\nwhen computing advantages, IQL aims to completely stay in-distribution with only dataset actions,\nwhile maintaining the ability to perform effective multi-step dynamic programming during policy\nevaluation. IQL achieves this by introducing an additional value function V (s) and performing\nexpectile regression. Specifically, the policy evaluation in IQL is implemented as\nmin\nV\nEs,a\u223cD [L\u03c4\n2(Q(s, a)) \u2212 V (s)]\nmin\nQ Es,a,s\u2032\u223cD\n\u0002\n(r(s, a) + \u03b3V (s\u2032) \u2212 Q(s, a))2\u0003\n,\n(22)\nwhere L\u03c4\n2 is the expectile regression loss defined as L\u03c4\n2(x) = |\u03c4 \u2212 1(x < 0)|x2 with hyper-paramter\n\u03c4 \u2208 (0, 1). The intuition behind IQL is that with a larger \u03c4, we will be able to better approximate the\nmax operator. As a result, IQL approximates the Bellman\u2019s optimality equation without querying\nOOD actions.\nIn practical implementations, we also adopt double Q learning for IQL, where we replace the\nQ(s, a) in Eqn. 22 with min(Q1(s, a), Q2(s, a)), and then use the updated value network to\ntrain both Q value networks. For IQL, we follow the policy improvement steps of CRR, as de-\nscribed in Eqn. 20 and Eqn. 21. The key difference is that instead of sampling actions to compute\n1\nN\nN\nP\ni=1\nmin(Q1(s, \u02c6ai), Q2(s, \u02c6ai)), IQL replaces it directly with the learned V (s). As for hyper-\nparameters, we use a temperature of \u03b2 = 1, a fixed standard deviation \u03c3 = I, and for expectile ratio\n\u03c4, we use \u03c4 = 0.9 for antmaze-v0 environments and \u03c4 = 0.7 for other tasks.\nB\nReinforcement Guided Diffusion Policy Details\nThe overall algorithm for our Reinforcement Guided Diffusion Policy Learning is given in Alg. 1.\nThe detailed algorithm for energy-based action selection is given in Alg. 2.\nC\nEnvironmental Details\nC.1\nHyper-Parameters\nWe detail our hyperparameters in Tab. 3.\n14\nAlgorithm 1: Reinforcement Guided Diffusion Policy\nInput: I, \u03bb, \u03b7, D, \u03f5\u03b8(ak, k; s), Q\u03d5(s, a)\nOutput: \u03b8, \u03d5\nfor i = 0, . . . , I do\n// Sample a batch of data\n{(s, a, s\u2032)} \u223c D\n// Sample next actions with DPM-Solver\na\u2032 \u223c \u03c0\u03b8(\u00b7|s)\n\u03d5 \u2190 \u03d5 \u2212 \u03b7\u2207\u03d5LTD(\u03d5)\n// Action approximation\n\u02c6a0 =\n1\n\u221a\n\u00af\u03b1k ak \u2212\n\u221a\n1\u2212\u00af\u03b1k\n\u221a\n\u00af\u03b1k \u03f5\u03b8(ak, k; s)\n\u03b8 \u2190 \u03b8 \u2212 \u03b7\u2207\u03b8(Ldiff(\u03b8) + \u03bbL\u03c0(\u03b8))\nAlgorithm 2: Energy-based Action Selection\nInput: number of actions: N, \u03c0\u03b8(a|s), Q\u03d5(s, a)\nOutput: a\nai \u223c \u03c0\u03b8(a|s)\ni = 1, . . . , N\na = categorical_sample({ai}, {eQ\u03d5(s,ai)})\nC.2\nMore Results\nWe first expand Tab. 1 by providing detailed numbers for each of the tasks used in Tab. 4.\nWe report the performance of EDP trained with TD3, CRR, and IQL in Tab. 5, where we directly\ncompare the scores of different evaluation metrics, i.e., OMS and RAT. We can observe that there are\nhuge gaps between OMS and RAT for all domains and all algorithms. However, IQL and CRR are\nrelatively more stable than TD3. For example, on the antmaze domain, TD3 achieves a best score of\n80.5, while the average score is just 29.8. In comparison, the best and average scores of IQL are 89.2\nand 73.4, respectively.\nC.3\nMore Results on EAS\nWe compare normal TD3+BC, TD3+BC with EAS for evaluation and TD3 + EDP in Tab. 6.\nC.4\nMore Experiments on Controlled Sampling\nAs in Sec. 4.5, we discussed reducing variance with EAS. We now detail another two methods\nexperimented as below.\nPolicy scaling\nInstead of sampling from the policy directly, we can sample from a sharper policy\n\u03c0\u03c4\n\u03b8 (a|s), where \u03c4 > 1. The scaled policy \u03c0\u03c4\n\u03b8 shares the same distribution modes as \u03c0\u03b8 but with\nreduced variance. Since diffusion policies are modeling the scores log \u03c0\u03b8(at|s), policy scaling can\nbe easily achieved by scaling the output of noise-prediction network by the factor \u03c4. We conduct\nexperiments on the gym-locomotion tasks in D4RL by varying \u03c4 from 0.5 to 2.0, as shown in Fig. 6,\nthe results show the best performance is achieved when \u03c4 = 1.0. It means sampling from a scaled\npolicy does not work.\nDeterministic Sampling This method is based on the observation that the sampling process of the\nDPM-Solver is deterministic except for the first step. The first step is to sample from an isotropic\nGaussian distribution. We modify it to use the mean, thus avoiding stochasticity. Consider a initial\nnoise aK \u223c N(0, I), we rescale aK by a noise scale factor. We show how this factor affects the final\nperformance by varying it from 0.0 to 1.0. As illustrated in Fig. 7, the best performance is achieved at\nzero noise in most cases, and a normal aK performs worst. This means reducing the variance of the\ninitial noise is able to improve the performance of a diffusion policy. However, the best performance\nachieved in this way still falls behind EAS.\n15\nTable 3: Hyperparameters used by EDP.\nTask\nLearning rate\nGradient norm clipping\nLoss weight\nEpochs\nBatch size\nLocomotion\n0.0003\n5\n1.0\n2000\n256\nAntmaze\n0.0003\n5\n1.0\n1000\n256\nAdroit\n0.00003\n5\n0.1\n1000\n256\nKitchen\n0.0003\n5\n0.005\n1000\n256\nTable 4: The performance of Diffusion-QL with efficient diffusion policy. The results for Diffusion-\nQL are directly quoted from [37]. EDP is our method. DQL (JAX) is a variant that uses the exact\nsame configurations as Diffusion-QL. All results are reported based on the OMS metric.\nDataset\nDiffusion-QL\nDQL (JAX)\nEDP\nhalfcheetah-medium-v2\n51.5\n52.3\n52.8\nhopper-medium-v2\n96.6\n95.3\n98.6\nwalker2d-medium-v2\n87.3\n86.9\n89.6\nhalfcheetah-medium-replay-v2\n48.3\n50.3\n50.4\nhopper-medium-replay-v2\n102.0\n101.8\n102.7\nwalker2d-medium-replay-v2\n98.0\n96.3\n97.7\nhalfcheetah-medium-expert-v2\n97.2\n97.3\n97.1\nhopper-medium-expert-v2\n112.3\n113.1\n112.0\nwalker2d-medium-expert-v2\n111.2\n111.5\n112.0\naverage\n89.4\n89.4\n90.3\nantmaze-umaze-v0\n96.0\n93.4\n93.4\nantmaze-umaze-diverse-v0\n84.0\n74.0\n66.0\nantmaze-medium-play-v0\n79.8\n96.0\n88.0\nantmaze-medium-diverse-v0\n82.0\n82.0\n96.0\nantmaze-large-play-v0\n49.0\n66.0\n60.0\nantmaze-large-diverse-v0\n61.7\n60.0\n64.0\naverage\n75.4\n78.6\n77.9\npen-human-v1\n75.7\n74.0\n98.3\npen-cloned-v1\n60.8\n59.1\n79.9\naverage\n68.3\n66.5\n89.1\nkitchen-complete-v0\n84.5\n100.0\n97.0\nkitchen-partial-v0\n63.7\n73.0\n71.5\nkitchen-mixed-v0\n66.6\n76.0\n73.0\naverage\n71.6\n83.0\n80.5\nC.5\nComputational Cost Comparison between EDP and Feed-Forward Policy networks\nWe benchmark the training speed of TD3+BC with EDP on walker2d-medium-expert-v2, by training\neach agent for 10,000 iterations of policy updates. The training speed for TD3+BC is 689 iterations-\nper-second (IPS), while EDP is 412 IPS. In other words, it takes around 3 hours to train an agent\nwith the feed-forward network, while EDP needs around 5 hours. We also conducted experiments by\ndouble the network used in TD3+BC; unfortunately, there was no performance gain on the locomotion\ntasks (75.3 to 75.6). Moreover, both feed-forward policy and diffusion policy utilize a 3-layer MLP\n(hidden size 256) as the backbone network. Therefore, the network capacity should be comparable.\n1.0\n1.5\n2.0\n0.0\n0.2\n0.4\n0.6\n0.8\nNormalized Return\nhalfcheetah\nmedium\nmedium-replay\nmedium-expert\n1.0\n1.5\n2.0\nScore Scale \n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nhopper\n1.0\n1.5\n2.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nwalker2d\nFigure 6: Performance of EDP + TD3 on gym-locomotion tasks with varying \u03c4.\n16\nTable 5: Average normalized score on the D4RL benchmark of EDP trained with different algorithms.\n\u201cBest\u201d represents the online model selection metric, while \u201cAverage\u201d is our runing average at training\nmetric.\nEDP + TD3\nEDP + CRR\nEDP + IQL\nAverage\nBest\nAverage\nBest\nAverage\nBest\nhalfcheetah-medium-v2\n52.1\n52.8\n49.2\n50.2\n48.1\n48.7\nhopper-medium-v2\n81.9\n98.6\n78.7\n95.0\n63.1\n97.3\nwalker2d-medium-v2\n86.9\n89.6\n82.5\n85.8\n85.4\n88.7\nhalfcheetah-medium-replay-v2\n49.4\n50.4\n43.5\n47.8\n43.8\n45.5\nhopper-medium-replay-v2\n101.0\n102.7\n99.0\n101.7\n99.1\n100.9\nwalker2d-medium-replay-v2\n94.9\n97.7\n63.3\n89.8\n84.0\n93.4\nhalfcheetah-medium-expert-v2\n95.5\n97.1\n85.6\n93.5\n86.7\n80.9\nhopper-medium-expert-v2\n97.4\n112.0\n92.9\n109.4\n99.6\n95.7\nwalker2d-medium-expert-v2\n110.2\n112.0\n110.1\n112.3\n109.0\n111.5\naverage\n85.5\n90.3\n78.3\n87.3\n79.9\n84.7\nkitchen-complete-v0\n61.5\n93.4\n73.9\n95.8\n75.5\n95.0\nkitchen-partial-v0\n52.8\n66.0\n40.0\n56.7\n46.3\n72.5\nkitchen-mixed-v0\n60.8\n88.0\n46.1\n59.2\n56.5\n70.0\naverage\n58.4\n96.0\n53.3\n70.6\n59.4\n79.2\npen-human-v0\n48.2\n60.0\n70.2\n127.8\n72.7\n130.3\npen-cloned-v0\n15.9\n64.0\n54.0\n106.0\n70.0\n138.2\naverage\n32.1\n77.9\n62.1\n116.9\n71.3\n134.3\nantmaze-umaze-v0\n96.6\n98.3\n95.9\n98.0\n94.2\n98.0\nantmaze-umaze-diverse-v0\n69.5\n79.9\n15.9\n80.0\n79.0\n90.0\nantmaze-medium-play-v0\n0.0\n89.1\n33.5\n82.0\n81.8\n89.0\nantmaze-medium-diverse-v0\n6.4\n97.0\n32.7\n72.0\n82.3\n88.0\nantmaze-large-play-v0\n1.6\n71.5\n26.0\n57.0\n42.3\n52.0\nantmaze-large-diverse-v0\n4.4\n73.0\n58.5\n71.0\n60.6\n68.0\naverage\n29.8\n80.5\n43.8\n76.7\n73.4\n89.2\nTable 6: Energy-based Action Selection + Normal TD3\nDataset\nTD3+Diff\nTD3+EAS\nTD3\nhalfcheetah-medium-v2\n52.1\n48.7\n47.8\nhopper-medium-v2\n81.9\n50.8\n54.0\nwalker2d-medium-v2\n86.9\n77.7\n53.4\nhalfcheetah-medium-replay-v2\n49.4\n44.4\n44.1\nhopper-medium-replay-v2\n101.0\n59.8\n59.1\nwalker2d-medium-replay-v2\n94.9\n74.8\n71.6\nhalfcheetah-medium-expert-v2\n95.5\n85.9\n92.3\nhopper-medium-expert-v2\n97.4\n71.9\n95.1\nwalker2d-medium-expert-v2\n110.2\n108.4\n110.0\ngym-locomotion-v2 (avg)\n85.5\n69.2\n69.7\nC.6\nThe effect of action approximation\nWe compare with and without action approximation on the following three environments by using\nOMS metric (Tab. 4). In Tab. 7, the DDPM column will forward and backward a policy network\n100 times at training time, but action approximation only needs once. We can observe that action\napproximation will slightly harm the performance, when the same number of diffusion steps is used.\nHowever, it supports training diffusion policies with larger K (e.g. 1000), while Diffusion-QL does\n0.00\n0.25\n0.50\n0.75\n1.00\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nNormalized Return\nhalfcheetah\nmedium\nmedium-replay\nmedium-expert\n0.00\n0.25\n0.50\n0.75\n1.00\nNoise Scale\n0.6\n0.7\n0.8\n0.9\n1.0\n1.1\nhopper\n0.00\n0.25\n0.50\n0.75\n1.00\n0.80\n0.85\n0.90\n0.95\n1.00\n1.05\n1.10\nwalker2d\nFigure 7: Performance of EDP + TD3 on gym-locomotion tasks with varying initial noise scale.\n17\nTable 7: The effect of action approximation.\nOMS K=100\nDDPM\nAction Approx\nwalker2d-medium-v2\n86.9\n85.5\nwalker2d-medium-reply-v2\n96.3\n93.3\nwalker2d-medium-expert-v2\n111.5\n111.1\nnot. Increasing K is able to avoid performance drop as evidenced by the last column of Tab. 2 and\nTab. 4.\nD\nNegative Societal Impacts\nIn this paper, we propose an efficient yet powerful policy class for offline reinforcement learning.\nWe show that this method is superior to most existing methods on simulated robotic tasks. However,\nsome war robots or weapon robots might employ our EDP to learn strategic agents considering the\ngeneralization ability of EDP. Depending on the specific application scenarios, it might be harmful to\ndomestic privacy and safety.\n18\n"
  },
  {
    "title": "Humans in 4D: Reconstructing and Tracking Humans with Transformers",
    "link": "https://arxiv.org/pdf/2305.20091.pdf",
    "upvote": "1",
    "text": "Humans in 4D: Reconstructing and Tracking Humans with Transformers\nShubham Goel\nGeorgios Pavlakos\nJathushan Rajasegaran\nAngjoo Kanazawa\u2217\nJitendra Malik\u2217\n{shubham-goel, pavlakos, jathushan, kanazawa}@berkeley.edu, malik@eecs.berkeley.edu\nUniversity of California, Berkeley\nFigure 1: A \u201ctransformerized\u201d view of Human Mesh Recovery. We describe HMR 2.0, a fully transformer-based approach for 3D\nhuman pose and shape reconstruction from a single image. Besides impressive performance across a wide variety of poses and viewpoints,\nHMR 2.0 also acts as the backbone of an improved system for jointly reconstructing and tracking Humans in 4D (4DHumans). Here, we\nsee output reconstructions from HMR 2.0 for each 2D detection in the left image.\nAbstract\nWe present an approach to reconstruct humans and track\nthem over time. At the core of our approach, we propose\na fully \u201ctransformerized\u201d version of a network for human\nmesh recovery. This network, HMR 2.0, advances the state\nof the art and shows the capability to analyze unusual poses\nthat have in the past been difficult to reconstruct from sin-\ngle images. To analyze video, we use 3D reconstructions\nfrom HMR 2.0 as input to a tracking system that operates\nin 3D. This enables us to deal with multiple people and\nmaintain identities through occlusion events. Our complete\napproach, 4DHumans, achieves state-of-the-art results for\ntracking people from monocular video. Furthermore, we\ndemonstrate the effectiveness of HMR 2.0 on the down-\nstream task of action recognition, achieving significant im-\nprovements over previous pose-based action recognition\napproaches.\nOur code and models are available on the\nproject website: https://shubham-goel.github.\nio/4dhumans/.\n1. Introduction\nIn this paper, we present a fully transformer-based ap-\nproach for recovering 3D meshes of human bodies from sin-\ngle images, and tracking them over time in video. We obtain\nunprecedented accuracy in our single-image 3D reconstruc-\ntions (see Figure 1) even for unusual poses where previous\napproaches struggle. In video, we link these reconstructions\nover time by 3D tracking, in the process bridging gaps due\nto occlusion or detection failures. These 4D reconstructions\ncan be seen on the project webpage.\nOur problem formulation and approach can be conceived\nas the \u201ctransformerization\u201d of previous work on human\nmesh recovery, HMR [30] and 3D tracking, PHALP [65].\nSince the pioneering ViT paper [15], the process of \u201ctrans-\nformerization\u201d, i.e., converting models from CNNs or\nLSTMs to transformer backbones, has advanced rapidly\nacross multiple computer vision tasks, e.g., [8, 16, 24, 40,\n61, 77]. Specifically for 2D pose (2D body keypoints) this\nhas already been done by ViTPose [81]. We take that as a\nstarting point and we develop a new version of HMR, which\nwe call HMR 2.0 to acknowledge its antecedent.\narXiv:2305.20091v3  [cs.CV]  31 Aug 2023\nWe use HMR 2.0 to build a system that can simultane-\nously reconstruct and track humans from videos. We rely\non the recent 3D tracking system, PHALP [65], which we\nsimplify and improve using our pose recovery. This system\ncan reconstruct Humans in 4D, which gives the name to our\nmethod, 4DHumans. 4DHumans can be deployed on any\nvideo and can jointly track and reconstruct people in video.\nThe functionality of creating a tracking entity for every per-\nson is fundamental towards analyzing and understanding\nhumans in video. Besides achieving state-of-the-art results\nfor tracking on the PoseTrack dataset [1], we also apply\nHMR 2.0 on the downstream application of action recogni-\ntion. We follow the system design of recent work, [63], and\nwe show that the use of HMR 2.0 can achieve impressive\nimprovements upon the state of the art on action recogni-\ntion on the AVA v2.2 dataset.\nThis paper is unabashedly a systems paper. We make\ndesign choices that lead to the best systems for 3D human\nreconstruction and tracking in the wild. Our model is pub-\nlicly available on the project webpage. There is an emerg-\ning trend, in computer vision as in natural language pro-\ncessing, of large pretrained models which find widespread\ndownstream applications and thus justify the scaling ef-\nfort.\nHMR 2.0 is such a large pre-trained model which\ncould potentially be useful not just in computer vision, but\nalso in robotics [54, 62, 73], computer graphics [76], bio-\nmechanics [60], and other fields where analysis of the hu-\nman figure and its movement from images or videos is\nneeded.\nOur contributions can be summarized as follows:\n1. We propose an end-to-end \u201ctransformerized\u201d architec-\nture for human mesh recovery, HMR 2.0. Without re-\nlying on domain-specific designs, we outperform ex-\nisting approaches for 3D body pose reconstruction.\n2. Building on HMR 2.0, we design 4DHumans that can\njointly reconstruct and track humans in video, achiev-\ning state-of-the-art results for tracking.\n3. We show that better 3D poses from HMR 2.0 result in\nbetter performance on the downstream task of action\nrecognition, finally contributing to the state-of-the-art\nresult (42.3 mAP) on the AVA benchmark.\n2. Related Work\nHuman Mesh Recovery from a Single Image. Although,\nthere have been many approaches that estimate 3D human\npose and shape relying on iterative optimization, e.g., SM-\nPLify [7] and variants [22, 38, 56, 66, 72, 85], for this\nanalysis we will focus on approaches that directly regress\nthe body shape from a single image input. In this case,\nthe canonical example is HMR [30], which uses a CNN\nto regress SMPL [45] parameters. Since its introduction,\nmany improvements have been proposed for the original\nmethod. Notably, many works have proposed alternative\nmethods for pseudo-ground truth generation, including us-\ning temporal information [3], multiple views [39], or itera-\ntive optimization [35, 29, 57]. SPIN [35] proposed an in-\nthe-loop optimization that incorporated SMPLify [7] in the\nHMR training. Here, we also rely on pseudo-ground truth\nfits for training, and we use [37] for the offline fitting.\nMore recently, there have been works that propose more\nspecialized designs for the HMR architecture. PyMAF [89,\n88] incorporates a mesh alignment module for the regres-\nsion of the SMPL parameters.\nPARE [34] proposes a\nbody-part-guided attention mechanism for better occlusion\nhandling.\nHKMR [20] performs a prediction that is in-\nformed by the known hierarchical structure of SMPL. Holo-\nPose [23] proposes a pooling strategy that follows the 2D\nlocations of each body joints. Instead, we follow a design\nwithout any domain-specific decisions and we show that it\noutperforms all previous approaches.\nMany related approaches are making non-parametric\npredictions, i.e., instead of estimating the parameters of\nthe SMPL model, they explicitly regress the vertices of the\nmesh. GraphCMR [36] uses a graph neural network for the\nprediction, METRO [42] and FastMETRO [10] use a trans-\nformer, while Mesh Graphormer [43] adopts a hybrid be-\ntween the two. Since we regress the SMPL model param-\neters, instead of the locations of mesh vertices, we are not\ndirectly comparable to these. However, we show how we\ncan use a fully \u201ctransformerized\u201d design for HMR.\nHuman Mesh & Motion Recovery from Video. To ex-\ntend Human Mesh Recovery over time, most methods use\nthe basic backbone of HMR [30] and propose designs for\nthe temporal encoder that fuses the per-frame features.\nHMMR [31] uses a convolutional encoder on features ex-\ntracted from HMR [30].\nVIBE [33], MEVA [48] and\nTCMR [11] use a recurrent temporal encoder. DSD [71]\ncombines convolutional and self-attention layers, while\nMAED [75] and t-HMMR [57] employ a transformer-based\ntemporal encoder. Baradel et al. [5, 4] also used a trans-\nformer for temporal pose prediction, while operating di-\nrectly on SMPL poses.\nOne key limitation of these ap-\nproaches is that they often operate in scenarios where track-\ning is simple [31, 90], e.g., videos with a single person\nor minimal occlusions. In contrast to that, our complete\n4DHumans approach is also solving the tracking problem.\nTracking People in Video. Recently, there have been ap-\nproaches that demonstrate state-of-the-art performance for\ntracking by relying on 3D human reconstruction from HMR\nmodels, i.e., T3DP [64] and PHALP [65]. In these meth-\nods, every person detection is lifted to 3D using an HMR\nnetwork [57] and then tracking is performed using the 3D\nrepresentations from lifting [64] and prediction [65] to track\npeople in video. Empirical results show that PHALP works\nViT\nMulti-head \nCross Attention\nSMPL \nQuery \nToken\nMLP\nPose\nShape\nCamera\nHMR 2.0\nPose\nShape\nCamera\nTransformer\nw/ Cross Attn\nSMPL \nQuery \nToken\nMLP\nVision Transformer\nInput Image\nTracking\nFrame t\nFrame t+1\nHMR 2.0\nHMR 2.0\nAssociate using pose, location, appearance \nFigure 2: Overview of our approach. Left: HMR 2.0 is a fully \u201ctransformerized\u201d version of a network for Human Mesh Recovery. Right:\nWe use HMR 2.0 as the backbone of our 4DHumans system, that builds on PHALP [65], to jointly reconstruct and track humans in 4D.\nvery well on multiple tracking benchmarks (the main re-\nquirement is that the images have enough spatial resolution\nto permit lifting of the people to 3D). We use these track-\ning pipelines, and particularly PHALP, as a task to evaluate\nmethods for human mesh recovery.\nAction Recognition. Action recognition is typically per-\nformed using appearance features from raw video input.\nCanonical examples in this category include SlowFast [18]\nand MViT [16]. Simultaneously, there are approaches that\nuse features extracted from body pose information, e.g., Po-\nTion [12] and JMRN [68]. A recent approach, LART [63],\ndemonstrates state-of-the-art performance for action recog-\nnition by fusing video-based features with features from 3D\nhuman pose estimates. We use the pipeline of this approach\nand employ action recognition as a downstream task to eval-\nuate human mesh recovery methods.\n3. Reconstructing People\n3.1. Preliminaries\nBody Model. The SMPL model [46] is a low-dimensional\nparametric model of the human body. Given input parame-\nters for pose (\u03b8 \u2208 R24\u00d73\u00d73) and shape (\u03b2 \u2208 R10), it outputs\na mesh M \u2208 R3\u00d7N with N = 6890 vertices. The body\njoints X \u2208 R3\u00d7k are defined as a linear combination of\nthe vertices and can be computed as X = MW with fixed\nweights W \u2208 RN\u00d7k. Note that pose parameters \u03b8 include\nthe body pose parameters \u03b8b \u2208 R23\u00d73\u00d73 and the global ori-\nentation \u03b8g \u2208 R3\u00d73.\nCamera. We use a perspective camera model with fixed\nfocal length and intrinsics K. Each camera \u03c0 = (R, t)\nconsists of a global orientation R \u2208 R3\u00d73 and transla-\ntion t \u2208 R3. Given these parameters, points in the SMPL\nspace (e.g., joints X) can be projected to the image as\nx = \u03c0(X) = \u03a0(K(RX +t)), where \u03a0 is a perspective pro-\njection with camera intrinsics K. Since \u03b8 already includes a\nglobal orientation, in practice we assume R as identity and\nonly predict camera translation t.\nHMR. The goal of the human mesh reconstruction (HMR)\ntask is to learn a predictor f(I) that given a single im-\nage I, reconstructs the person in the image by predicting\ntheir 3D pose and shape parameters. Following the typi-\ncal parametric approaches [30, 35], we model f to predict\n\u0398 = [\u03b8, \u03b2, \u03c0] = f(I) where \u03b8 and \u03b2 are the SMPL pose\nand shape parameters and \u03c0 is the camera translation.\n3.2. Architecture\nWe re-imagine HMR [30] as an end-to-end transformer\narchitecture that uses no domain specific design choices.\nYet, it outperforms all existing approaches that have heav-\nily customized architectures and elaborate design decisions.\nAs shown in Figure 2, we use (i) a ViT [15] to extract image\ntokens, and (ii) a standard transformer decoder that cross-\nattends to image tokens to output \u0398.\nViT. The Vision Transformer, or ViT [15] is a trans-\nformer [74] that has been modified to operate on an im-\nage. The input image is first patchified into input tokens\nand passed through the transformer to get output tokens.\nThe output tokens are then passed to the transformer de-\ncoder. We use a ViT-H/16, the \u201cHuge\u201d variant with 16 \u00d7 16\ninput patch size. Please see SupMat for more details.\nTransformer decoder. We use a standard transformer de-\ncoder [74] with multi-head self-attention. It processes a sin-\ngle (zero) input token by cross-attending to the output image\ntokens and ends with a linear readout of \u0398. We follow [35]\nand regress 3D rotations using the representation of [91].\n3.3. Losses\nFollowing best practices in the HMR literature [30, 35],\nwe train our predictor f with a combination of 2D losses,\n3D losses, and a discriminator. Since we train with a mix-\nture of datasets, each having different kinds of annotations,\nwe employ a subset of these losses for each image in a mini-\nbatch. We use the same losses even with pseudo-ground\ntruth annotations. Given an input image I, the model pre-\ndicts \u0398 = [\u03b8, \u03b2, \u03c0] = f(I). Whenever we have access to\nthe ground-truth SMPL pose parameters \u03b8\u2217 and shape pa-\nrameters \u03b2\u2217, we bootstrap the model predictions using an\nMSE loss:\nLsmpl = ||\u03b8 \u2212 \u03b8\u2217||2\n2 + ||\u03b2 \u2212 \u03b2\u2217||2\n2.\nWhen the image has accurate ground-truth 3D keypoint an-\nnotations X\u2217, we additionally supervise the predicted 3D\nkeypoints X with an L1 loss:\nLkp3D = ||X \u2212 X\u2217||1.\nWhen the image has 2D keypoints annotations x\u2217, we su-\npervise projections of predicted 3D keypoints \u03c0(X) using\nan L1 loss:\nLkp2D = ||\u03c0(X) \u2212 x\u2217||1.\nFurthermore, we want to ensure that our model predicts\nvalid 3D poses and use the adversarial prior in HMR [30]. It\nfactorizes the model parameters into: (i) body pose param-\neters \u03b8b, (ii) shape parameters \u03b2, and (iii) per-part relative\nrotations \u03b8i, which is one 3D rotation for each of the 23\njoints of the SMPL model. We train a discriminator Dk for\neach factor of the body model, and the generator loss can be\nexpressed as:\nLadv =\nX\nk\n(Dk(\u03b8b, \u03b2) \u2212 1)2.\n3.4. Pseudo-Ground Truth fitting\nWe scale to unlabelled datasets (i.e., InstaVariety [31],\nAVA [21], AI Challenger [78]) by computing pseudo-\nground truth annotations. Given any image, we first use\nan off-the-shelf detector [40] and a body keypoints estima-\ntor [81] to get bounding boxes and corresponding 2D key-\npoints. We then fit a SMPL mesh to these 2D keypoints\nusing ProHMR [37] to get pseudo-ground truth SMPL pa-\nrameters \u03b8\u2217 and \u03b2\u2217 with camera \u03c0\u2217.\n4. Tracking People\nIn videos with multiple people, we need the ability to as-\nsociate people across time, i.e., perform tracking. For this\nwe build upon PHALP [65], a state-of-the-art tracker based\non features derived from HMR-style 3D reconstructions.\nThe basic idea is to detect people in individual frames, and\nPose Predictor\nMask\nToken\nMask\nToken\nMask\nToken\nPast\nFuture\nFigure 3: Pose prediction: We train a BERT-style [13] trans-\nformer model on over 1 million tracks obtained from [63]. This al-\nlow us to make future predictions and amodal completion of miss-\ning detections using the same model. To predict future poses (t+1,\nt+2, ...), we query the model with a mask-token using correspond-\ning positional embeddings. Similarly for amodal completion, we\nreplace missing detections with a masked token.\n\u201clift\u201d them to 3D, extracting their 3D pose, location in 3D\nspace (derived from the estimated camera), and 3D appear-\nance (derived from the texture map). A tracklet represen-\ntation is incrementally built up for each individual person\nover time. The recursion step is to predict for each track-\nlet, the pose, location and appearance of the person in the\nnext frame, all in 3D, and then find best matches between\nthese top-down predictions and the bottom-up detections of\npeople in that frame after lifting them to 3D. The state rep-\nresented by each tracklet is then updated by the incoming\nobservation, and the process is iterated. It is possible to\ntrack through occlusions because the 3D representation of a\ntracklet continues to be updated based on past history.\nWe believe that a robust pose predictor should also per-\nform well, when evaluated on this downstream task of track-\ning, so we use the tracking metrics as a proxy to evaluate the\nquality of 3D reconstructions. But first we needed to mod-\nify the PHALP framework to allow for fair comparison of\ndifferent pose prediction models. Originally, PHALP used\npose features based on the last layer of the HMR network,\ni.e., a 2048-dimensional embedding space. This limits the\nability of PHALP to be used with different pose models\n(e.g., HMR 2.0, PARE, PyMAF etc.). To create a more\ngeneric version of PHALP, we perform the modification of\nrepresenting pose in terms of SMPL pose parameters, and\nwe accordingly optimize the PHALP cost function to utilize\nthe new pose distance. Similarly, we adapt the pose pre-\ndictor to operate on the space of SMPL parameters. More\nspecifically, we train a vanilla transformer model [74] by\nmasking random pose tokens as shown in the Fig 3. This\nallows us to predict future poses in time, as well as amodal\ncompletion of missing detections. With these modifications,\nwe can plug in any mesh recovery methods and run them on\nany videos. We call this modified version PHALP\u2032.\n4DHumans. Our final tracking system, 4DHumans, uses a\nsampling-based parameter-free appearance head and a new\npose predictor (Figure 3). To model appearance, we texture\nvisible points on the mesh by projecting them onto the input\nimage and sampling color from the corresponding pixels.\nTo track people in videos, previous approaches relied on\noff-the-shelf tracking approaches and used their output to\nreconstruct humans in videos (e.g., take the bounding boxes\nfrom tracking output and reconstruct people). For example,\nPHD [90], HMMR [31] can run on videos with only single\nperson in the scene. In this work, we combine reconstruc-\ntion and tracking into a single system and show that better\npose reconstructions result in better tracking and this com-\nbined system can now run on any videos in the wild.\n5. Experiments\nIn this section, we evaluate our reconstruction and track-\ning system qualitatively and quantitatively. First, we show\nthat HMR 2.0 outperforms previous methods on standard\n2D and 3D pose accuracy metrics (Section 5.2). Second, we\nshow 4DHumans is a versatile tracker, achieving state-of-\nthe-art performance (Section 5.3). Third, we further demon-\nstrate the robustness and accuracy of our recovered poses\nvia superior performance on the downstream application of\naction recognition (Section 5.4). Finally, we discuss the ex-\nperimental investigation when designing HMR 2.0 and ab-\nlate a series of design choices (Section 5.5).\n5.1. Setup\nDatasets.\nFollowing previous work, we use the typi-\ncal datasets for training, i.e., Human3.6M [27], MPI-INF-\n3DHP [49], COCO [44] and MPII [2]. Additionally, we use\nInstaVariety [31], AVA [21] and AI Challenger [78] as extra\ndata where we generate pseudo-ground truth fits.\nBaselines. We report performance on benchmarks that we\ncan compare with many previous works (Section 5.2), but\nwe also perform a more detailed comparison with recent\nstate-of-the-art methods, i.e., PyMAF [89], CLIFF [41],\nHMAR [65], PARE [34], and PyMAF-X [88]. For fairness,\nwe only evaluate the body-only performance of PyMAF-X.\n5.2. Pose Accuracy\n3D Metrics. For 3D pose accuracy, we follow the typical\nprotocols of prior work, e.g., [35], and we present results\non the 3DPW test split and on the Human3.6M val split,\nreporting MPJPE, and PA-MPJPE in Table 1. Please no-\ntice that we only compare with methods that do not use the\ntraining set of 3DPW for training, similar to us. We observe\nthat with our HMR 2.0a model, which trains only on the\ntypical datasets, we can outperform all previous baselines\nacross all metrics. However, we believe that these bench-\nmarks are very saturated and these smaller differences in\npose metrics tend to not be very significant. In fact, we\nMethod\n3DPW\nHuman3.6M\nMPJPE PA-MPJPE MPJPE PA-MPJPE\nTemporal\nKanazawa et al. [31] 116.5\n72.6\n-\n56.9\nDoersch et al. [14]\n-\n74.7\n-\n-\nArnab et al. [3]\n-\n72.2\n77.8\n54.3\nDSD [71]\n-\n69.5\n59.1\n42.4\nVIBE [33]\n93.5\n56.5\n65.9\n41.5\nFrame-based\nPavlakos et al. [59]\n-\n-\n-\n75.9\nHMR [30]\n130.0\n76.7\n88.0\n56.8\nNBF [53]\n-\n-\n59.9\nGraphCMR [36]\n-\n70.2\n-\n50.1\nHoloPose [23]\n-\n-\n60.3\n46.5\nDenseRaC [82]\n-\n-\n76.8\n48.0\nSPIN [35]\n96.9\n59.2\n62.5\n41.1\nDecoMR [86]\n-\n61.7\u2020\n-\n39.3\u2020\nDaNet [87]\n-\n56.9\n61.5\n48.6\nSong et al. [69]\n-\n55.9\n-\n56.4\nI2L-MeshNet [51]\n100.0\n60.0\n55.7\u2020\n41.1\u2020\nHKMR [20]\n-\n-\n59.6\n43.2\nPyMAF [89]\n92.8\n58.9\n57.7\n40.5\nPARE [34]\n82.0\n50.9\n76.8\n50.6\nPyMAF-X [88]\n78.0\n47.1\n54.2\n37.2\nHMR 2.0a\n70.0\n44.5\n44.8\n33.6\nHMR 2.0b\n81.3\n54.3\n50.0\n32.4\nTable 1: Reconstructions evaluated in 3D: Reconstruction errors\n(in mm) on the 3DPW and Human3.6M datasets.\n\u2020 denotes the\nnumbers evaluated on non-parametric results. Lower \u2193 is better.\nPlease see the text for details.\nMethod\nLSP-Extended\nCOCO\nPoseTrack\n@0.05 @0.1\n@0.05 @0.1 @0.05 @0.1\nPyMAF [89]\n-\n-\n0.68\n0.86\n0.77\n0.92\nCLIFF [41]\n0.32\n0.66\n0.64\n0.88\n0.75\n0.92\nPARE [34]\n0.27\n0.60\n0.72\n0.91\n0.79\n0.93\nPyMAF-X [88]\n-\n-\n0.79\n0.93\n0.85\n0.95\nHMR 2.0a\n0.38\n0.72\n0.79\n0.95\n0.86\n0.97\nHMR 2.0b\n0.53\n0.82\n0.86\n0.96\n0.90\n0.98\nTable 2: Reconstructions evaluated in 2D. PCK scores of pro-\njected keypoints at different thresholds on the LSP-Extended,\nCOCO, and PoseTrack datasets. Higher \u2191 is better.\nobserve that by a small compromise of the performance on\n3DPW, our HMR 2.0b model, which trains for longer on\nmore data (AVA [21], AI Challenger [78], and InstaVari-\nety [31]), achieves results that perform better on more un-\nusual poses than what can be found in Human3.6M and\n3DPW. We observe this qualitatively and from performance\nevaluated on 2D pose reprojection (Table 2). Furthermore,\nwe observe that HMR 2.0b is a more robust model and use\nit for evaluation in the rest of the paper.\n2D Metrics. We evaluate 2D image alignment of the gen-\nerated poses by reporting PCK of reprojected keypoints\nat different thresholds on LSP-Extended [28], COCO val-\nidation set [44], and Posetrack validation set [1].\nSince\nPyMAF(-X) [89, 88] were trained using LSP-Extended, we\ndo not report numbers for that part of the table. Notice in\nTable 2, that HMR 2.0b consistently outperforms all pre-\nvious approaches. On LSP-Extended, which contains un-\nusual poses, HMR 2.0b achieves PCK@0.05 of 0.53, which\nis 1.6\u00d7 better than the second best (CLIFF) with 0.32. For\nPCK@0.05 on easier datasets like COCO and PoseTrack\nwith less extreme poses, HMR 2.0b still outperforms the\nsecond-best approaches but by narrower margins of 9% and\n6% respectively. HMR 2.0a also outperforms all baselines,\nbut is worse than HMR 2.0b, especially on harder poses in\nLSP-Extended.\nQualitative Results.\nWe show qualitative results of\nHMR 2.0 in Figure 4.\nWe are robust to extreme poses\nand partial occlusions. Our reconstructions are well-aligned\nwith the image and are valid when seen from a novel view.\nMoreover, we compare with our closest competitors in Fig-\nure 5. We observe that PyMAF-X and particularly PARE\noften struggle with more unusual poses, while HMR 2.0 re-\nturns more faithful reconstructions.\n5.3. Tracking\nFor tracking, we first demonstrate the versatility of\nthe modifications introduced by PHALP\u2032, which allow us\nto evaluate 3D pose estimators on the downstream task\nof tracking.\nThen, we evaluate our complete system,\n4DHumans, with respect to the state of the art.\nEvaluation Setting.\nFollowing previous work [64, 65],\nwe report results based on IDs (ID switches), MOTA [32],\nIDF1 [67], and HOTA [47] on the Posetrack validation set\nusing the protocol of [65], with detections from Mask R-\nCNN [25].\nVersatility of PHALP\u2032.\nWith the modifications of\nPHALP\u2032, we abandon the model-specific latent space\nof [65] and instead, we operate in the SMPL space, which\nis shared across most mesh recovery systems. This makes\nPHALP\u2032 more versatile and allows us to plug in different\n3D pose estimators and compare them based on their per-\nformance on the downstream task of tracking. We perform\nthis comparison in Table 3 where we use pose and loca-\ntion cues from state-of-the-art 3D pose estimators (while\nstill using appearance from HMAR [65]). We observe that\nHMR 2.0 performs the best and PARE [34], HMAR [65],\nand PyMAF-X [88] closely follow on the Posetrack dataset,\nwith minor differences between them. Note that tracking is\noften most susceptible to errors in predicted 3D locations\nwith body pose having a smaller effect in performance [65].\nThis means that good tracking performance can indicate\nrobustness to occlusions, so it is helpful to consider this\nmetric, but it is less helpful to distinguish fine-grained dif-\nferences in pose. As a result, the competitive results of\nPARE [34], HMAR [65], and PyMAF-X [88] indicate that\nthey handle occlusions gracefully, but their pose estimation\nTracker\nPose Engine\nPosetrack\nHOTA\u2191 IDs\u2193 MOTA\u2191 IDF1\u2191\nPHALP\u2032\nPARE [34]\n53.6\n510\n59.4\n76.8\nPyMAF-X [88]\n53.7\n472\n59.2\n76.9\nCLIFF [41]\n53.5\n551\n58.7\n76.5\nPyMAF [89]\n53.0\n623\n58.6\n76.1\nHMAR [65]\n53.6\n482\n59.3\n77.1\nHMR 2.0\n54.1\n456\n59.4\n77.4\nTable 3: Tracking with different 3D pose estimators.\nWith\nthe modifications of PHALP\u2032, we have a versatile tracker that al-\nlows different 3D pose estimators to be plugged into it. HMR 2.0,\nPARE, and PyMAF-X perform the best in this setting.\nMethod\nPosetrack\nHOTA\u2191 IDs\u2193 MOTA\u2191 IDF1\u2191\nTrackformer [50]\n46.7\n1263\n33.7\n64.0\nTracktor [6]\n38.5\n702\n42.4\n65.2\nAlphaPose [17]\n37.6\n2220\n36.9\n66.9\nPose Flow [79]\n38.0\n1047\n15.4\n64.2\nT3DP [64]\n50.6\n655\n55.8\n73.4\nPHALP [65]\n52.9\n541\n58.9\n76.4\n4DHumans\n54.3\n421\n59.8\n77.9\n4DHumans + ViTDet\n57.8\n382\n61.4\n79.1\nTable 4: Comparison of 4DHumans with the state of the art on\nthe Posetrack dataset. 4DHumans achieve state-of-the-art track-\ning performance for all metrics. Incorporating a better detection\nsystem [40] leads to further performance improvements.\nmight still be less accurate (as observed from Table 2). See\nalso Figure 5 and SupMat for more qualitative comparisons.\n4DHumans.\nTable 4 evaluates tracking performance\nof our complete system, 4DHumans, on the PoseTrack\ndataset. Using the same bounding box detector as [64, 65],\n4DHumans outperforms existing approaches on all metrics,\nimproving ID Switches by 22%. Using the improved ViT-\nDet detector [40] can improve performance further. As a by-\nproduct of our temporal prediction model (Figure 3), we can\nperform amodal completion and attribute a pose to missing\ndetections. We show examples of this in the SupMat.\n5.4. Action Recognition\nEvaluation setting. The approach of [63] is the state of the\nart for action recognition in videos. Given a video as input,\nthe authors propose using per-frame 3D pose and location\nestimates (using off-the-shelf HMR models [65]) as an ad-\nditional feature for predicting action labels. They also show\nresults for a \u201cpose-only\u201d baseline that predicts action labels\nFront view\nInput\nSide view\nTop view\nFront view\nInput\nSide view\nTop view\nFigure 4: Qualitative evaluation of HMR 2.0. For each example we show: a) the input image, b) the reconstruction overlay, c) a side\nview, d) the top view. To demonstrate the robustness of HMR 2.0, we visualize results for a variety of settings - for unusual poses (rows\n1-4), for unusual viewpoints (row 5) and for images with poor visibility, extreme truncations and extreme occlusions (rows 6-8).\nusing only 3D pose and location estimates. We use this\nsetting to compare our model with baselines on the down-\nstream task of action recognition on the AVA dataset [21].\nIn [63], the authors train a transformer that takes SMPL\nposes as input and predicts action labels. Following their\nsetup, we train a separate action classification transformer\nfor each baseline.\nComparisons. Comparing results in Table 5, we observe\nthat HMR 2.0 outperforms baselines on the different class\ncategories (OM, PI, PM) and overall. It achieves an mAP\nof 22.3 on the AVA test set, which is 14% better than the\nsecond-best baseline.\nSince accurate action recognition\nfrom poses needs fine-grained pose estimation, this is strong\nevidence that HMR 2.0 predicts more accurate poses than\nPyMAF-X\nPARE\nHMR 2.0\nInput\nl Image\nFigure 5: Qualitative comparison of state-of-the-art mesh recovery methods. HMR 2.0 returns more faithful reconstructions for unusual\nposes compared to the closest competitors, PyMAF-X [88] and PARE [34].\n0\n170\n44 \n63\n55\n0\n19 \n51 \n65 \n122\n0\n240 \n1304 \n1312\n1325 \n0\nFigure 6: Qualitative tracking results of 4DHumans. We use head masks (frame number is on the top left). First row: We track people\nskating on ice with challenging poses and heavy occlusions, in a minute long video without switching identities. Second row: The main\nperson is tracked through multiple interactions with other players. Third row: The person of interest is tracked through long occlusions.\nAction\nPose\nOM\nPI\nPM\nmAP\nModel\nEngine\n[63]\nPyMAF [89]\n7.3\n16.9\n34.7\n15.4\nCLIFF [41]\n9.2\n20.0\n40.3\n18.6\nHMAR [65]\n8.7\n20.1\n40.3\n18.3\nPARE [34]\n9.2\n20.7\n41.5\n19.1\nPyMAF-X [88]\n10.2\n21.4\n40.8\n19.6\nHMR 2.0\n11.9\n24.6\n45.8\n22.3\nTable 5: Action recognition results on the AVA dataset. We\nbenchmark different mesh recovery methods on the downstream\ntask of pose-based action recognition. Here, OM : Object Manip-\nulation, PI : Person Interactions, and PM : Person Movement.\nexisting approaches. In fact, when combined with appear-\nance features, [63] shows that HMR 2.0 achieves the state\nof the art of 42.3 mAP on AVA action recognition, which is\n7% better than the second-best of 39.5 mAP.\n5.5. HMR 2.0 Model Design\nIn the process of developing HMR 2.0, we investigated\na series of design decisions. Figure 7 briefly illustrates this\nexploration. We experimented with over 100 settings and\nwe visualize the performance of 100 checkpoints for each\nrun. For the visualization, we use the performance of each\ncheckpoint on the 3DPW and the LSP-Extended dataset.\nOur investigation focused on some specific aspects of\nthe model, which we document here as a series of \u201clessons\nlearnt\u201d for future research. In the following paragraphs, we\nwill regularly refer to Table 6, which evaluates these aspects\n46\n48\n50\n52\n54\n56\n0.4\n0.5\n0.6\n3DPW - PA-MPJPE (in mm)\nLSP-extended - PCK@0.05  \nFigure 7: Extensive model search. With each dot, we visualize\nthe performance of a checkpoint when evaluated on 3DPW and\nLSP-Extended. Colors indicate different runs. We explore more\nthan 100 settings, and visualize \u223c100 checkpoints from each run.\non 3D and 2D metrics, using the 3DPW, Human3.6M, and\nLSP-Extended datasets.\nEffect of backbone. Unlike the majority of the previous\nwork on Human Mesh Recovery that uses a ResNet back-\nbone, our HMR 2.0 method relies on a ViT backbone. For\na direct comparison of the effect of the backbone, Model\nB1 implements HMR with a ResNet-50 backbone and an\nMLP-based head implementing IEF (Iterative Error Feed-\nback [9, 30]). In contrast, Model B2 uses a transformer\nbackbone (ViT-H) while keeping the other design decisions\nthe same. By updating the backbone, we observe a signif-\nicant improvement across the 3D and 2D metrics, which\njustifies the \u201ctransformerization\u201d step.\nEffect of training data. Besides the architecture, we also\ninvestigated the effect of training data. Model D1 trains\non the typical datasets (H3.6M, MPII, COCO, MPI-INF)\nthat most of the previous works are leveraging. In com-\nparison, model D2 adds AVA in the training set, follow-\ning [21]. Eventually, we also train using AI-Challenger and\nInsta-Variety (model B2), to further expand the training set.\nAs we can see, adding more training data leads to improve-\nments across the board for the reported metrics, but the ben-\nefit is smaller compared to the backbone update.\nViT pretraining. Another factor that had significant ef-\nfect on the performance of our model was the pretraining\nof the ViT backbone. Starting with randomly initialized\nweights (model P1) results in slow convergence and poor\nperformance. Results improve if our backbone is pretrained\nwith MAE [24] on Imagenet (P2). Eventually, our model\nof choice (HMR 2.0b), which is first pretrained with MAE\non ImageNet and then on the task of 2D keypoint predic-\ntion [81], achieves the best performance.\nSMPL head. We also investigate the effect of the architec-\nture for the head that predicts the SMPL parameters. Our\nproposed transformer decoder (HMR 2.0b) improves per-\nformance when it comes to the image-model alignment (i.e.,\n2D metrics) compared to the traditional MLP-based head\nwith IEF steps (B2).\nDataset quality. Similar to previous work, e.g., [35], it was\ncrucial to keep the quality of the training data high; we filter\nModels\n3DPW\nHuman3.6M\nLSP-Extended\nMPJPE PA-MPJPE MPJPE PA-MPJPE PCK@0.05 PCK@0.1\nHMR 2.0b\n81.3\n54.3\n50.0\n32.4\n0.53\n0.82\nB1\n85.2\n56.8\n58.9\n41.4\n0.35\n0.66\nB2\n79.7\n53.4\n51.4\n34.4\n0.48\n0.81\nD1\n84.1\n54.8\n54.5\n35.1\n0.45\n0.79\nD2\n80.2\n53.3\n52.4\n34.9\n0.46\n0.79\nP1\n98.9\n61.7\n89.9\n58.7\n0.24\n0.52\nP2\n82.7\n55.6\n49.3\n32.4\n0.52\n0.81\nTable 6: Ablations: Evaluation for different model designs on the\n3DPW, Human3.6M, and LSP-Extended datasets.\nout low quality pseudo-ground truth fits (high fitting error)\nand prune images with low-confidence 2D detections.\n6. Conclusion\nWe study the problem of reconstructing and tracking hu-\nmans from images and video. First, we propose HMR 2.0,\na fully \u201ctransformerized\u201d version of a network for the prob-\nlem of Human Mesh Recovery [30].\nHMR 2.0 achieves\nstrong performance on the usual 2D/3D pose metrics, while\nalso acting as the backbone for our improved video tracker.\nThe full system, 4DHumans, jointly reconstructs and tracks\npeople in video and achieves state-of-the-art results for\ntracking. To further illustrate the benefit of our 3D pose\nestimator, HMR 2.0, we apply it to the task of action recog-\nnition, where we demonstrate strong improvements upon\nprevious pose-based baselines.\nOur work pushes the boundary of the videos that can be\nanalyzed with techniques for 3D human reconstruction. At\nthe same time, the improved results also demonstrate the\ntype of limitations that need to be addressed in the future.\nFor example, the use of the SMPL model [45] creates cer-\ntain limitations, and leveraging improved models would al-\nlow us to model hand pose and facial expressions [56], or\neven capture greater age variation, e.g., infants [26] and\nkids [55, 70]. Moreover, since we consider each person in-\ndependently, our reconstructions are less successful at cap-\nturing the fine-grained nature of people in close proxim-\nity, e.g., contact [19, 52]. Besides this, our reconstructions\n\u201clive\u201d in the camera frame, so for proper understanding of\nthe action in a video, we need to consider everyone in a\ncommon world coordinate frame, by reasoning about the\ncamera motion too [58, 83, 84]. Finally, lower input reso-\nlution can affect the quality of our reconstructions, which\ncould be addressed by resolution augmentations [80].\nAcknowledgements We thank members of the BAIR com-\nmunity for helpful discussions and StabilityAI for their\ngenerous compute grant.\nThis work was supported by\nBAIR/BDD sponsors, ONR MURI (N00014-21-1-2801),\nand the DARPA MCS program.\nReferences\n[1] Mykhaylo Andriluka,\nUmar Iqbal,\nEldar Insafutdinov,\nLeonid Pishchulin, Anton Milan, Juergen Gall, and Bernt\nSchiele. PoseTrack: A benchmark for human pose estima-\ntion and tracking. In CVPR, 2018.\n[2] Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and\nBernt Schiele. 2D human pose estimation: New benchmark\nand state of the art analysis. In CVPR, 2014.\n[3] Anurag Arnab, Carl Doersch, and Andrew Zisserman. Ex-\nploiting temporal context for 3D human pose estimation in\nthe wild. In CVPR, 2019.\n[4] Fabien Baradel, Romain Br\u00b4egier, Thibault Groueix, Philippe\nWeinzaepfel, Yannis Kalantidis, and Gr\u00b4egory Rogez. Pose-\nBERT: A generic transformer module for temporal 3D hu-\nman modeling. PAMI, 2022.\n[5] Fabien Baradel, Thibault Groueix, Philippe Weinzaepfel,\nRomain Br\u00b4egier, Yannis Kalantidis, and Gr\u00b4egory Rogez.\nLeveraging MoCap data for human mesh recovery. In 3DV,\n2021.\n[6] Philipp Bergmann, Tim Meinhardt, and Laura Leal-Taixe.\nTracking without bells and whistles. In ICCV, 2019.\n[7] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter\nGehler, Javier Romero, and Michael J Black. Keep it SMPL:\nAutomatic estimation of 3D human pose and shape from a\nsingle image. In ECCV, 2016.\n[8] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In ECCV, 2020.\n[9] Joao Carreira, Pulkit Agrawal, Katerina Fragkiadaki, and Ji-\ntendra Malik. Human pose estimation with iterative error\nfeedback. In CVPR, 2016.\n[10] Junhyeong Cho, Kim Youwang, and Tae-Hyun Oh. Cross-\nattention of disentangled modalities for 3D human mesh re-\ncovery with transformers. In ECCV, 2022.\n[11] Hongsuk Choi, Gyeongsik Moon, Ju Yong Chang, and Ky-\noung Mu Lee. Beyond static features for temporally con-\nsistent 3D human pose and shape from a video. In CVPR,\n2021.\n[12] Vasileios Choutas, Philippe Weinzaepfel, J\u00b4er\u02c6ome Revaud,\nand Cordelia Schmid. PoTion: Pose motion representation\nfor action recognition. In CVPR, 2018.\n[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova.\nBert:\nPre-training of deep bidirectional\ntransformers for language understanding.\narXiv preprint\narXiv:1810.04805, 2018.\n[14] Carl Doersch and Andrew Zisserman.\nSim2real transfer\nlearning for 3D human pose estimation: Motion to the res-\ncue. NeurIPS, 2019.\n[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. ICLR, 2021.\n[16] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li,\nZhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer.\nMultiscale vision transformers. In ICCV, 2021.\n[17] Hao-Shu Fang, Shuqin Xie, Yu-Wing Tai, and Cewu Lu.\nRMPE: Regional multi-person pose estimation.\nIn ICCV,\n2017.\n[18] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and\nKaiming He. Slowfast networks for video recognition. In\nICCV, 2019.\n[19] Mihai Fieraru, Mihai Zanfir, Elisabeta Oneata, Alin-Ionut\nPopa, Vlad Olaru, and Cristian Sminchisescu.\nThree-\ndimensional reconstruction of human interactions. In CVPR,\n2020.\n[20] Georgios Georgakis, Ren Li, Srikrishna Karanam, Terrence\nChen, Jana Ko\u02c7seck\u00b4a, and Ziyan Wu. Hierarchical kinematic\nhuman mesh recovery. In ECCV, 2020.\n[21] Chunhui Gu,\nChen Sun,\nDavid A Ross,\nCarl Von-\ndrick, Caroline Pantofaru, Yeqing Li, Sudheendra Vijaya-\nnarasimhan, George Toderici, Susanna Ricco, Rahul Suk-\nthankar, Cordelia Schmid, and Jitendra Malik. AVA: A video\ndataset of spatio-temporally localized atomic visual actions.\nIn CVPR, 2018.\n[22] Peng Guan, Alexander Weiss, Alexandru O B\u02d8alan, and\nMichael J Black. Estimating human shape and pose from\na single image. In ICCV, 2009.\n[23] Riza Alp Guler and Iasonas Kokkinos. HoloPose: Holistic\n3D human reconstruction in-the-wild. In CVPR, 2019.\n[24] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll\u00b4ar, and Ross Girshick. Masked autoencoders are scalable\nvision learners. In CVPR, 2022.\n[25] Kaiming He, Georgia Gkioxari, Piotr Doll\u00b4ar, and Ross Gir-\nshick. Mask R-CNN. In ICCV, 2017.\n[26] Nikolas Hesse, Sergi Pujades, Michael J Black, Michael\nArens, Ulrich G Hofmann, and A Sebastian Schroeder.\nLearning and tracking the 3D body shape of freely moving\ninfants from RGB-D sequences. PAMI, 2019.\n[27] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian\nSminchisescu. Human3.6M: Large scale datasets and predic-\ntive methods for 3D human sensing in natural environments.\nPAMI, 2013.\n[28] Sam Johnson and Mark Everingham. Learning effective hu-\nman pose estimation from inaccurate annotation. In CVPR,\n2011.\n[29] Hanbyul Joo, Natalia Neverova, and Andrea Vedaldi. Exem-\nplar fine-tuning for 3D human model fitting towards in-the-\nwild 3D human pose estimation. In 3DV, 2021.\n[30] Angjoo Kanazawa, Michael J Black, David W Jacobs, and\nJitendra Malik. End-to-end recovery of human shape and\npose. In CVPR, 2018.\n[31] Angjoo Kanazawa, Jason Y Zhang, Panna Felsen, and Jiten-\ndra Malik. Learning 3D human dynamics from video. In\nCVPR, 2019.\n[32] Rangachar\nKasturi,\nDmitry\nGoldgof,\nPadmanabhan\nSoundararajan, Vasant Manohar, John Garofolo, Rachel\nBowers, Matthew Boonstra, Valentina Korzhova, and Jing\nZhang. Framework for performance evaluation of face, text,\nand vehicle detection and tracking in video: Data, metrics,\nand protocol. PAMI, 2008.\n[33] Muhammed Kocabas, Nikos Athanasiou, and Michael J\nBlack.\nVIBE: Video inference for human body pose and\nshape estimation. In CVPR, 2020.\n[34] Muhammed Kocabas, Chun-Hao P Huang, Otmar Hilliges,\nand Michael J Black. PARE: Part attention regressor for 3D\nhuman body estimation. In ICCV, 2021.\n[35] Nikos Kolotouros, Georgios Pavlakos, Michael J Black, and\nKostas Daniilidis. Learning to reconstruct 3D human pose\nand shape via model-fitting in the loop. In ICCV, 2019.\n[36] Nikos Kolotouros, Georgios Pavlakos, and Kostas Dani-\nilidis. Convolutional mesh regression for single-image hu-\nman shape reconstruction. In CVPR, 2019.\n[37] Nikos Kolotouros, Georgios Pavlakos, Dinesh Jayaraman,\nand Kostas Daniilidis.\nProbabilistic modeling for human\nmesh recovery. In ICCV, 2021.\n[38] Christoph Lassner, Javier Romero, Martin Kiefel, Federica\nBogo, Michael J Black, and Peter V Gehler. Unite the peo-\nple: Closing the loop between 3D and 2D human representa-\ntions. In CVPR, 2017.\n[39] Vincent Leroy, Philippe Weinzaepfel, Romain Br\u00b4egier,\nHadrien Combaluzier, and Gr\u00b4egory Rogez. SMPLy bench-\nmarking 3D human pose estimation in the wild.\nIn 3DV,\n2020.\n[40] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He.\nExploring plain vision transformer backbones for object de-\ntection. In ECCV, 2022.\n[41] Zhihao Li, Jianzhuang Liu, Zhensong Zhang, Songcen Xu,\nand Youliang Yan. CLIFF: Carrying location information in\nfull frames into human pose and shape estimation. In ECCV,\n2022.\n[42] Kevin Lin, Lijuan Wang, and Zicheng Liu. End-to-end hu-\nman pose and mesh reconstruction with transformers.\nIn\nCVPR, 2021.\n[43] Kevin Lin,\nLijuan Wang,\nand Zicheng Liu.\nMesh\ngraphormer. In ICCV, 2021.\n[44] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft COCO: Common objects in context. In\nECCV, 2014.\n[45] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard\nPons-Moll, and Michael J Black. SMPL: A skinned multi-\nperson linear model. ACM transactions on graphics (TOG),\n34(6):1\u201316, 2015.\n[46] Matthew Loper, Naureen Mahmood, Javier Romero, Ger-\nard Pons-Moll, and Michael J. Black.\nSMPL: A skinned\nmulti-person linear model.\nACM Trans. Graphics (Proc.\nSIGGRAPH Asia), 34(6):248:1\u2013248:16, Oct. 2015.\n[47] Jonathon Luiten, Aljosa Osep, Patrick Dendorfer, Philip\nTorr, Andreas Geiger, Laura Leal-Taix\u00b4e, and Bastian Leibe.\nHOTA: A higher order metric for evaluating multi-object\ntracking. IJCV, 2021.\n[48] Zhengyi Luo, S Alireza Golestaneh, and Kris M Kitani. 3D\nhuman motion estimation via motion compression and re-\nfinement. In ACCV, 2020.\n[49] Dushyant Mehta,\nHelge Rhodin,\nDan Casas,\nPascal\nFua, Oleksandr Sotnychenko, Weipeng Xu, and Christian\nTheobalt. Monocular 3D human pose estimation in the wild\nusing improved CNN supervision. In 3DV, 2017.\n[50] Tim Meinhardt, Alexander Kirillov, Laura Leal-Taixe, and\nChristoph Feichtenhofer. TrackFormer: Multi-object track-\ning with transformers. In CVPR, 2022.\n[51] Gyeongsik Moon and Kyoung Mu Lee.\nI2L-MeshNet:\nImage-to-lixel prediction network for accurate 3D human\npose and mesh estimation from a single RGB image.\nIn\nECCV, 2020.\n[52] Lea M\u00a8uller, Vickie Ye, Georgios Pavlakos, Michael Black,\nand Angjoo Kanazawa.\nGenerative proxemics: A prior\nfor 3D social interaction from images.\narXiv preprint\narXiv:2306.09337, 2023.\n[53] Mohamed Omran, Christoph Lassner, Gerard Pons-Moll, Pe-\nter Gehler, and Bernt Schiele. Neural body fitting: Unifying\ndeep learning and model based human pose and shape esti-\nmation. In 3DV, 2018.\n[54] Austin Patel, Andrew Wang, Ilija Radosavovic, and Jitendra\nMalik. Learning to imitate object interactions from internet\nvideos. arXiv preprint arXiv:2211.13225, 2022.\n[55] Priyanka Patel,\nChun-Hao P Huang,\nJoachim Tesch,\nDavid T Hoffmann, Shashank Tripathi, and Michael J Black.\nAGORA: Avatars in geography optimized for regression\nanalysis. In CVPR, 2021.\n[56] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,\nTimo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and\nMichael J Black. Expressive body capture: 3D hands, face,\nand body from a single image. In CVPR, 2019.\n[57] Georgios Pavlakos, Jitendra Malik, and Angjoo Kanazawa.\nHuman mesh recovery from multiple shots. In CVPR, 2022.\n[58] Georgios Pavlakos, Ethan Weber, Matthew Tancik, and\nAngjoo Kanazawa. The one where they reconstructed 3D\nhumans and environments in TV shows. In ECCV, 2022.\n[59] Georgios Pavlakos, Luyang Zhu, Xiaowei Zhou, and Kostas\nDaniilidis. Learning to estimate 3D human pose and shape\nfrom a single color image. In CVPR, 2018.\n[60] Owen Pearl, Soyong Shin, Ashwin Godura, Sarah Bergbre-\niter, and Eni Halilaj. Fusion of video and inertial sensing data\nvia dynamic optimization of a biomechanical model. Journal\nof Biomechanics, 155:111617, 2023.\n[61] William Peebles and Saining Xie. Scalable diffusion models\nwith transformers. arXiv preprint arXiv:2212.09748, 2022.\n[62] Xue Bin Peng, Angjoo Kanazawa, Jitendra Malik, Pieter\nAbbeel, and Sergey Levine. Sfv: Reinforcement learning of\nphysical skills from videos. ACM Transactions On Graphics\n(TOG), 37(6):1\u201314, 2018.\n[63] Jathushan\nRajasegaran,\nGeorgios\nPavlakos,\nAngjoo\nKanazawa, Christoph Feichtenhofer, and Jitendra Malik.\nOn the benefits of 3D tracking and pose for human action\nrecognition. In CVPR, 2023.\n[64] Jathushan\nRajasegaran,\nGeorgios\nPavlakos,\nAngjoo\nKanazawa, and Jitendra Malik.\nTracking people with 3D\nrepresentations. In NeurIPS, 2021.\n[65] Jathushan\nRajasegaran,\nGeorgios\nPavlakos,\nAngjoo\nKanazawa, and Jitendra Malik.\nTracking people by pre-\ndicting 3D appearance, location and pose.\nIn CVPR,\n2022.\n[66] Davis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang,\nSrinath Sridhar, and Leonidas J Guibas. HuMoR: 3D human\nmotion model for robust pose estimation. In ICCV, 2021.\n[67] Ergys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara,\nand Carlo Tomasi. Performance measures and a data set for\nmulti-target, multi-camera tracking. In ECCV, 2016.\n[68] Anshul Shah, Shlok Mishra, Ankan Bansal, Jun-Cheng\nChen, Rama Chellappa, and Abhinav Shrivastava. Pose and\njoint-aware action recognition. In WACV, 2022.\n[69] Jie Song, Xu Chen, and Otmar Hilliges. Human body model\nfitting by learned gradient descent. In ECCV, 2020.\n[70] Yu Sun, Wu Liu, Qian Bao, Yili Fu, Tao Mei, and Michael J\nBlack. Putting people in their place: Monocular regression\nof 3D people in depth. In CVPR, 2022.\n[71] Yu Sun, Yun Ye, Wu Liu, Wenpeng Gao, Yili Fu, and Tao\nMei. Human mesh recovery from monocular images via a\nskeleton-disentangled representation. In ICCV, 2019.\n[72] Garvita Tiwari, Dimitrije Anti\u00b4c, Jan Eric Lenssen, Nikolaos\nSarafianos, Tony Tung, and Gerard Pons-Moll. Pose-NDF:\nModeling human pose manifolds with neural distance fields.\nIn ECCV, 2022.\n[73] Vasileios Vasilopoulos, Georgios Pavlakos, Sean L Bowman,\nJ Diego Caporale, Kostas Daniilidis, George J Pappas, and\nDaniel E Koditschek. Reactive semantic planning in unex-\nplored semantic environments using deep perceptual feed-\nback. IEEE Robotics and Automation Letters, 5(3):4455\u2013\n4462, 2020.\n[74] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NIPS, 2017.\n[75] Ziniu Wan, Zhengjia Li, Maoqing Tian, Jianbo Liu, Shuai\nYi, and Hongsheng Li.\nEncoder-decoder with multi-level\nattention for 3D human shape and pose estimation. In ICCV,\n2021.\n[76] Chung-Yi Weng,\nBrian Curless,\nPratul P Srinivasan,\nJonathan T Barron, and Ira Kemelmacher-Shlizerman. Hu-\nmanNeRF: Free-viewpoint rendering of moving people from\nmonocular video. In CVPR, 2022.\n[77] Chao-Yuan Wu, Justin Johnson, Jitendra Malik, Christoph\nFeichtenhofer, and Georgia Gkioxari. Multiview compres-\nsive coding for 3D reconstruction. In CVPR, 2023.\n[78] Jiahong Wu, He Zheng, Bo Zhao, Yixin Li, Baoming Yan,\nRui Liang, Wenjia Wang, Shipei Zhou, Guosen Lin, Yanwei\nFu, Yizhou Wang, and Yonggang Wang. AI Challenger: A\nlarge-scale dataset for going deeper in image understanding.\narXiv preprint arXiv:1711.06475, 2017.\n[79] Yuliang Xiu, Jiefeng Li, Haoyu Wang, Yinghong Fang, and\nCewu Lu.\nPose Flow: Efficient online pose tracking.\nIn\nBMVC, 2018.\n[80] Xiangyu Xu, Hao Chen, Francesc Moreno-Noguer, Laszlo A\nJeni, and Fernando De la Torre. 3D human pose, shape and\ntexture from low-resolution images and videos. PAMI, 2021.\n[81] Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao.\nViTPose: Simple vision transformer baselines for human\npose estimation. In NeurIPS, 2022.\n[82] Yuanlu Xu, Song-Chun Zhu, and Tony Tung. DenseRaC:\nJoint 3D pose and shape estimation by dense render-and-\ncompare. In ICCV, 2019.\n[83] Vickie Ye, Georgios Pavlakos, Jitendra Malik, and Angjoo\nKanazawa.\nDecoupling human and camera motion from\nvideos in the wild. In CVPR, 2023.\n[84] Ye Yuan, Umar Iqbal, Pavlo Molchanov, Kris Kitani, and\nJan Kautz. GLAMR: Global occlusion-aware human mesh\nrecovery with dynamic cameras. In CVPR, 2022.\n[85] Andrei Zanfir, Elisabeta Marinoiu, and Cristian Sminchis-\nescu. Monocular 3D pose and shape estimation of multiple\npeople in natural scenes - The importance of multiple scene\nconstraints. In CVPR, 2018.\n[86] Wang Zeng, Wanli Ouyang, Ping Luo, Wentao Liu, and Xi-\naogang Wang. 3D human mesh regression with dense corre-\nspondence. In CVPR, 2020.\n[87] Hongwen Zhang, Jie Cao, Guo Lu, Wanli Ouyang, and\nZhenan Sun. DaNnet: Decompose-and-aggregate network\nfor 3D human shape and pose estimation. In Proceedings\nof the 27th ACM International Conference on Multimedia,\n2019.\n[88] Hongwen Zhang, Yating Tian, Yuxiang Zhang, Mengcheng\nLi, Liang An, Zhenan Sun, and Yebin Liu. PyMAF-X: To-\nwards well-aligned full-body model regression from monoc-\nular images. PAMI, 2023.\n[89] Hongwen Zhang, Yating Tian, Xinchi Zhou, Wanli Ouyang,\nYebin Liu, Limin Wang, and Zhenan Sun. PyMAF: 3D hu-\nman pose and shape regression with pyramidal mesh align-\nment feedback loop. In ICCV, 2021.\n[90] Jason Y Zhang, Panna Felsen, Angjoo Kanazawa, and Jiten-\ndra Malik. Predicting 3D human dynamics from video. In\nICCV, 2019.\n[91] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao\nLi. On the continuity of rotation representations in neural\nnetworks. In CVPR, 2019.\nSupplementary Material for:\n\u201cHumans in 4D: Reconstructing and Tracking Humans with Transformers\u201d\nShubham Goel\nGeorgios Pavlakos\nJathushan Rajasegaran\nAngjoo Kanazawa\u2217\nJitendra Malik\u2217\n{shubham-goel, pavlakos, jathushan, kanazawa}@berkeley.edu, malik@eecs.berkeley.edu\nUniversity of California, Berkeley\nWe provide more details about HMR 2.0, i.e., the archi-\ntecture we use (Section S.1), the data (Section S.2) and the\ntraining pipeline (Section S.3). Furthermore, we describe\nthe aspect of pose prediction (Section S.4) and we discuss\nthe metrics we use for evaluation (Section S.5). Then, we\ndiscuss the experimental settings for tracking (Section S.6),\nand action recognition (Section S.7). Finally, we provide\nadditional qualitative results (Section S.8).\nS.1. HMR 2.0 architecture details\nThe architecture of our HMR 2.0 model is based on a\nViT image encoder and a transformer decoder.\nWe use\na ViT-H/16 (\u201chuge\u201d) pre-trained on the task of 2D key-\npoint localization [25]. It has 50 transformer layers, takes a\n256 \u00d7 192 sized image as input, and outputs 16 \u00d7 12 image\ntokens, each of dimension 1280. Our transformer decoder is\na standard transformer decoder architecture [23] with 6 lay-\ners, each containing multi-head self-attention, multi-head\ncross-attention, and feed-forward blocks, with layer nor-\nmalization [2]. It has a 2048 hidden dimension, 8 (64-dim)\nheads for self- and cross-attention, and a hidden dimension\nof 1024 in the feed-forward MLP block. It operates on a sin-\ngle learnable 2048-dimensional SMPL query token as input\nand cross-attends to the 16 \u00d7 12 image tokens. Finally, a\nlinear readout on the output token from the transformer de-\ncoder gives pose \u03b8, shape \u03b2, and camera \u03c0.\nS.2. Data details\nIn our training, we adopt the training data conventions of\nprevious works [10], using images from Human3.6M [4],\nCOCO [13], MPII [1] and MPI-INF-3DHP [18]. This forms\nthe training set for the version we refer to as HMR 2.0a in\nthe main manuscript. For the eventual HMR 2.0b version,\nwe additionally generate pseudo-ground truth SMPL [14]\nfits for images from AVA [3], InstaVariety [6] and AI Chal-\nlenger [24]. Since AVA and InstaVariety include videos, we\ncollect frames by sampling at 1fps and 5fps respectively.\nFor pseudo-ground truth generation, we use ViTDet [11]\nfor bounding box detection and ViTPose [25] for key-\npoint detection, while fitting happens using ProHMR [10].\nWe discard detections with very few 2D detected key-\npoints (less than five) and low detection confidence (thresh-\nold 0.5). We also discard fits with unnatural body shapes\n(i.e., body shape parameters outside [\u22123, 3]), unnatural\nbody poses (computed using a per-joint histogram of poses\non AMASS [17]), and large fitting errors (i.e., which indi-\ncates that the reconstruction was not successful). For train-\ning our HMR 2.0b model, we sample with different proba-\nbilities from each dataset, i.e., Human3.6M: 0.1, MPII: 0.1,\nMPI-INF-3DHP: 0.1, AVA: 0.15, AI Challenger: 0.15, In-\nstaVariety: 0.2, COCO: 0.2.\nS.3. Training details\nWe train our main model using 8 A100 GPUs with an ef-\nfective batch size of 8 \u00d7 48 = 384. We use an AdamW\noptimizer [15] with a learning rate of 1e-5, \u03b21 = 0.9,\n\u03b22 = 0.999, and a weight decay of 1e-4. Training lasts for\n1M iterations, which takes roughly six days. For our main\nmodel HMR 2.0b, we train the network end-to-end. How-\never, for the HMR 2.0a variant, the ViT encoder remains\nfrozen, allowing a larger effective batch size of 8 \u00d7 512 =\n4096, learning rate of 1e-4, and fewer training iterations of\n100K (i.e., roughly equivalent number of epochs).\nWhile training, we weigh the different losses. Lkp3D,\nLkp2D, and Ladv have weights 0.05, 0.01, and 0.0005 re-\nspectively. The terms within Lsmpl are also weighed dif-\nferently, the \u03b8 and \u03b2 terms weigh 0.001 and 0.0005 respec-\ntively.\nS.4. Pose prediction\nFor the pose prediction model, we train a vanilla trans-\nformer model [23] from the tracklets obtained by [19]. Each\ntracklet at every time instance contains 3D pose and 3D\nlocation information, where the pose is parameterized by\nthe SMPL model [14] and the location is represented as the\ntranslation in the camera frame. The transformer has 6 lay-\ners and 8 self-attention heads with a hidden dimension of\n256. Each output token regresses the 3D pose and 3D loca-\ntion of the person at the specified time-step. We train this\nmodel by randomly masking input pose tokens and apply-\ning the loss on the masked tokens. During inference, to\npredict a future 3D pose, we query the model by reading\nout from a future time-step, using a learned mask-token as\ninput to that time-step. Similarly for amodal completion,\nwe replace the missing detections with the learned mask-\ntoken and read out from the output at the corresponding\ntime-step. The model is trained with a batch size of 64 se-\nquences and a sequence length of 128 tokens. We use the\nAdamW optimizer [15] with a learning rate of 0.001 and\n\u03b21 = 0.9, \u03b22 = 0.95.\nS.5. Metrics\nFor our evaluation, we use the metrics that are common\nin the literature:\n3D Pose: We follow [5] and we use MPJPE and PA-MPJPE.\nMPJPE refers to Mean Per Joint Position Error and it is the\naverage L2 error across all joint, after aligning with the root\nnode. PA-MPJPE is similar but is computed after aligning\nthe predicted pose with the ground-truth pose using Pro-\ncrustes Alignment.\n2D Pose: We use PCK as defined in [26]. This is the Per-\ncentage of Correctly localized Keypoints, where a keypoint\nis considered as correctly localized if its L2 distance from\nthe ground-truth keypoint is less than a threshold t. We re-\nport results using different thresholds (@0.05 and @0.1 of\nimage size).\nTracking: Following [20, 21], we use standard tracking\nmetrics.\nThis includes ID switches (IDs), MOTA [7],\nIDF1 [22], and HOTA [16].\nAction Recognition: We report results using mAP metrics\nas defined in the AVA dataset [3]. We further provided a\nmore fine-grained analysis reporting results on different ac-\ntion categories: actions that involve Object Manipulation\n(OM), actions that involve Person Interactions (PI), and ac-\ntions that involve Person Movement (PM). The results in\nthese categories are also reported using mAP.\nS.6. Tracking with PHALP\u2032\nIn the main manuscript, we compare different human\nmesh recovery systems on the downstream problem of\ntracking (Table 3 of the main manuscript). For this, we\nmodify the PHALP approach [21], so that pose distance is\ncomputed on the SMPL space that all the models share. To\nmake this comparison fair, we keep other variables simi-\nlar to the original PHALP (e.g., same appearance embed-\nding). Note that this comparison is generous to baselines\nthat do not model appearance themselves. Eventually, our\nfinal 4DHumans system uses a sampling-based appearance\nhead and our new pose prediction, which lead to the state-\nof-the-art performance for tracking on PoseTrack (Table 4\nof the main manuscript). To model appearance, we texture\nvisible points on the mesh by projecting them onto the input\nimage and sampling color from the corresponding pixels.\nS.7. Action recognition\nAs an alternative way to assess the quality of 3D hu-\nman reconstruction, we evaluate various human mesh re-\ncovery systems on the downstream task of action recogni-\ntion on AVA (please refer to [19] for more details on the\ntask definition).\nMore specifically, we take the tracklets\nfrom [19], which were generated by running PHALP [21]\non the Kinetics [8] and AVA [3] datasets. Then, we re-\nplace the poses from various human mesh recovery models\n(i.e., PyMAF [28], PyMAF-X [27], PARE [9], CLIFF [12],\nHMAR [21], HMR 2.0) and evaluate their performance on\nthe action recognition task. In this pose-only setting, the\naction recognition model has access only to the 3D poses\n(in the SMPL format) and 3D location and is trained to pre-\ndict the action of each person. For a fair comparison and\nto achieve the best performance for each 3D pose regressor,\nwe retrain the action recognition model specifically for each\n3D pose method.\nS.8. Additional qualitative results\nWe have already provided a lot of qualitative results of\nHMR 2.0, both in the main manuscript and in videos on\nthe project webpage. Here, we provide additional results,\nincluding comparisons with our closest competitors (Fig-\nure S.1), and a demonstration of our results in a variety of\nchallenging cases, including successes (Figure S.2) and fail-\nure cases (Figure S.3).\nReferences\n[1] Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and\nBernt Schiele. 2D human pose estimation: New benchmark\nand state of the art analysis. In CVPR, 2014.\n[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. Layer normalization. arXiv preprint arXiv:1607.06450,\n2016.\n[3] Chunhui Gu,\nChen Sun,\nDavid A Ross,\nCarl Von-\ndrick, Caroline Pantofaru, Yeqing Li, Sudheendra Vijaya-\nnarasimhan, George Toderici, Susanna Ricco, Rahul Suk-\nthankar, Cordelia Schmid, and Jitendra Malik. AVA: A video\ndataset of spatio-temporally localized atomic visual actions.\nIn CVPR, 2018.\n[4] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian\nSminchisescu. Human3.6M: Large scale datasets and predic-\ntive methods for 3D human sensing in natural environments.\nPAMI, 2013.\n[5] Angjoo Kanazawa, Michael J Black, David W Jacobs, and\nJitendra Malik. End-to-end recovery of human shape and\npose. In CVPR, 2018.\nPyMAF-X\nPARE\nHMR 2.0\nInput\nCLIFF\nFigure S.1: Qualitative comparison of our approach with state-of-the-art methods. We compare HMR 2.0 with our closest competitors,\nPyMAF-X [27], PARE [9] and CLIFF [12]. For each example, we show the input image, and results from each method (including the\nfrontal and a side view). HMR 2.0 is significantly more robust in a variety of settings, including images with unusual poses, unusual\nviewpoints and heavy person-person overlap.\nFront view\nInput\nSide view\nTop view\nFront view\nInput\nSide view\nTop view\nFigure S.2: Qualitative results of our approach on challenging examples. For each example we show the input image, the reconstruction\noverlay, a side view and the top view. The examples include unusual poses, unusual viewpoints, people in close interaction, extreme\ntruncations and occlusions, as well as blurry images.\nFront view\nInput\nSide view\nTop view\nFront view\nInput\nSide view\nTop view\nFigure S.3: Failures of single frame 3D human reconstruction with HMR 2.0. Despite the increased robustness of our method, we\nobserve that HMR 2.0 occasionally recovers erroneous reconstructions in cases with very unusual articulation (first row), heavy person-\nperson interaction (second row), and very challenging depth ordering for the different body parts (third row).\n[6] Angjoo Kanazawa, Jason Y Zhang, Panna Felsen, and Jiten-\ndra Malik. Learning 3D human dynamics from video. In\nCVPR, 2019.\n[7] Rangachar\nKasturi,\nDmitry\nGoldgof,\nPadmanabhan\nSoundararajan, Vasant Manohar, John Garofolo, Rachel\nBowers, Matthew Boonstra, Valentina Korzhova, and Jing\nZhang. Framework for performance evaluation of face, text,\nand vehicle detection and tracking in video: Data, metrics,\nand protocol. PAMI, 2008.\n[8] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,\nChloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,\nTim Green, Trevor Back, Paul Natsev, Mustafa Suleyman,\nand Andrew Zisserman. The kinetics human action video\ndataset. arXiv preprint arXiv:1705.06950, 2017.\n[9] Muhammed Kocabas, Chun-Hao P Huang, Otmar Hilliges,\nand Michael J Black. PARE: Part attention regressor for 3D\nhuman body estimation. In ICCV, 2021.\n[10] Nikos Kolotouros, Georgios Pavlakos, Dinesh Jayaraman,\nand Kostas Daniilidis.\nProbabilistic modeling for human\nmesh recovery. In ICCV, 2021.\n[11] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He.\nExploring plain vision transformer backbones for object de-\ntection. In ECCV, 2022.\n[12] Zhihao Li, Jianzhuang Liu, Zhensong Zhang, Songcen Xu,\nand Youliang Yan. CLIFF: Carrying location information in\nfull frames into human pose and shape estimation. In ECCV,\n2022.\n[13] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft COCO: Common objects in context. In\nECCV, 2014.\n[14] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard\nPons-Moll, and Michael J Black. SMPL: A skinned multi-\nperson linear model. ACM transactions on graphics (TOG),\n34(6):1\u201316, 2015.\n[15] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017.\n[16] Jonathon Luiten, Aljosa Osep, Patrick Dendorfer, Philip\nTorr, Andreas Geiger, Laura Leal-Taix\u00b4e, and Bastian Leibe.\nHOTA: A higher order metric for evaluating multi-object\ntracking. IJCV, 2021.\n[17] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Ger-\nard Pons-Moll, and Michael J Black. AMASS: Archive of\nmotion capture as surface shapes. In ICCV, 2019.\n[18] Dushyant Mehta,\nHelge Rhodin,\nDan Casas,\nPascal\nFua, Oleksandr Sotnychenko, Weipeng Xu, and Christian\nTheobalt. Monocular 3D human pose estimation in the wild\nusing improved CNN supervision. In 3DV, 2017.\n[19] Jathushan\nRajasegaran,\nGeorgios\nPavlakos,\nAngjoo\nKanazawa, Christoph Feichtenhofer, and Jitendra Malik.\nOn the benefits of 3D tracking and pose for human action\nrecognition. In CVPR, 2023.\n[20] Jathushan\nRajasegaran,\nGeorgios\nPavlakos,\nAngjoo\nKanazawa, and Jitendra Malik.\nTracking people with 3D\nrepresentations. In NeurIPS, 2021.\n[21] Jathushan\nRajasegaran,\nGeorgios\nPavlakos,\nAngjoo\nKanazawa, and Jitendra Malik.\nTracking people by pre-\ndicting 3D appearance, location and pose.\nIn CVPR,\n2022.\n[22] Ergys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara,\nand Carlo Tomasi. Performance measures and a data set for\nmulti-target, multi-camera tracking. In ECCV, 2016.\n[23] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NIPS, 2017.\n[24] Jiahong Wu, He Zheng, Bo Zhao, Yixin Li, Baoming Yan,\nRui Liang, Wenjia Wang, Shipei Zhou, Guosen Lin, Yanwei\nFu, Yizhou Wang, and Yonggang Wang. AI Challenger: A\nlarge-scale dataset for going deeper in image understanding.\narXiv preprint arXiv:1711.06475, 2017.\n[25] Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao.\nViTPose: Simple vision transformer baselines for human\npose estimation. In NeurIPS, 2022.\n[26] Yi Yang and Deva Ramanan. Articulated human detection\nwith flexible mixtures of parts. PAMI, 2012.\n[27] Hongwen Zhang, Yating Tian, Yuxiang Zhang, Mengcheng\nLi, Liang An, Zhenan Sun, and Yebin Liu. PyMAF-X: To-\nwards well-aligned full-body model regression from monoc-\nular images. PAMI, 2023.\n[28] Hongwen Zhang, Yating Tian, Xinchi Zhou, Wanli Ouyang,\nYebin Liu, Limin Wang, and Zhenan Sun. PyMAF: 3D hu-\nman pose and shape regression with pyramidal mesh align-\nment feedback loop. In ICCV, 2021.\n"
  },
  {
    "title": "Improving CLIP Training with Language Rewrites",
    "link": "https://arxiv.org/pdf/2305.20088.pdf",
    "upvote": "1",
    "text": "Improving CLIP Training with Language Rewrites\nLijie Fan1,2,\u2217\nDilip Krishnan1\nPhillip Isola2\nDina Katabi2\nYonglong Tian1,\u2217\n1Google Research,\n2MIT CSAIL,\n\u2217equal contribution\nAbstract\nContrastive Language-Image Pre-training (CLIP) stands as one of the most effective\nand scalable methods for training transferable vision models using paired image\nand text data. CLIP models are trained using contrastive loss, which typically relies\non data augmentations to prevent overfitting and shortcuts. However, in the CLIP\ntraining paradigm, data augmentations are exclusively applied to image inputs,\nwhile language inputs remain unchanged throughout the entire training process,\nlimiting the exposure of diverse texts to the same image. In this paper, we intro-\nduce Language augmented CLIP (LaCLIP), a simple yet highly effective approach\nto enhance CLIP training through language rewrites. Leveraging the in-context\nlearning capability of large language models, we rewrite the text descriptions asso-\nciated with each image. These rewritten texts exhibit diversity in sentence structure\nand vocabulary while preserving the original key concepts and meanings. During\ntraining, LaCLIP randomly selects either the original texts or the rewritten versions\nas text augmentations for each image. Extensive experiments on CC3M, CC12M,\nRedCaps and LAION-400M datasets show that CLIP pre-training with language\nrewrites significantly improves the transfer performance without computation or\nmemory overhead during training. Specifically for ImageNet zero-shot accuracy,\nLaCLIP outperforms CLIP by 8.2% on CC12M and 2.4% on LAION-400M. Code\nis available at https://github.com/LijieFan/LaCLIP.\n1\nIntroduction\nPre-trained vision-language multi-modal encoders, exemplified by CLIP [44], have proven to be\nextremely useful in learning transferable features from paired image and text data. CLIP\u2019s training\nprocess can leverage two scalable paradigms: data and compute. Firstly, the availability of large-scale\nvision-language paired data [49, 48] enables effective training at a substantial scale. Secondly, CLIP\u2019s\nutilization of language and image co-embeddings grants it favorable scaling properties with respect\nto compute resources [27]. Consequently, CLIP embeddings consistently outperform other visual\npre-training approaches such as SimCLR [8] or MAE [22] across various downstream tasks [27].\nFollow-up methods that build upon language-image pre-training, such as SLIP [41] and FLIP [35],\nexhibit similar advantageous scaling performance.\nThe CLIP framework is built upon contrastive learning, which typically relies on data augmentations\nto prevent overfitting and the learning of ineffective shortcuts [8, 47]. However, in the CLIP training\nprocess, this beneficial feature is applied exclusively to image inputs, which undergo augmentations\nin every epoch. In contrast, text inputs are neglected and remain unchanged throughout training,\nlacking any form of augmentation. This input asymmetry leads to scenarios where the same text is\nconsistently paired with slightly augmented images, while the augmented version of the same image\nis always paired with the exact same words. Such asymmetry presents two issues. Firstly, the image\nencoders receive limited supervision from the language side since the same image is consistently\npaired with the same words. Consequently, the language aspect provides less guidance to the image\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2305.20088v2  [cs.CV]  28 Oct 2023\nencoders. Secondly, the text encoders repeatedly encounter the exact same texts in each epoch, which\nincreases the risk of text overfitting and significantly impacts zero-shot transferability.\nHence, it becomes crucial to incorporate a suitable augmentation strategy for the text inputs. However,\nexisting language augmentation methods are not sufficiently effective. Most previous approaches [58]\nfocus on word-level treatments like replacement or masking, which have limited impact on enriching\ntext structures and are comparatively weaker than image augmentations. Currently, there is a lack of\nlanguage rewriting strategies that can effectively augment sentences while preserving key concepts\nand meanings. Such strategies are urgently needed for CLIP training to ensure optimal performance.\nMeanwhile, alongside the development of CLIP, the field has witnessed significant advancements\nin large language models (LLMs) like GPT models [5, 44, 45] and LaMDA [3]. These LLMs have\nseen tremendous growth in terms of data, computational resources, and performance. Instruction\nfine-tuned models such as ChatGPT [2] and Bard [1] have also emerged, incorporating fine-tuning\nthrough supervised and reinforcement learning. These models have exhibited exceptional performance\nsurpassing human capabilities across a wide range of natural language tasks. Motivated by these\nadvancements, we naturally explored the potential of leveraging LLMs to effectively generate diverse\nrewritten versions of a given text. While a straightforward approach would involve directly employing\ninstruction fine-tuned models like ChatGPT or Bard, their closed-source nature renders it infeasible\nto use them for rewriting the hundreds of millions of image descriptions in the datasets. Fortunately,\nopen-sourced LLMs such as LLaMA [56], despite lacking fine-tuning with instructions, possess\nexcellent In-Context Learning (ICL) capabilities, enabling predictions with a limited context. By\nthoughtfully designing contextual examples, LLaMA can generate diverse and rich text rewrites for\nthe entire dataset.\nBuilding upon this foundation, we propose Language augmented CLIP (LaCLIP), a straightforward\nyet highly effective approach for enhancing CLIP model performance by harnessing the power of\nLLMs. Our method leverages ICL using LLaMA to generate diverse variants of each caption within\nthe text-image pairs of a given pre-training dataset. To facilitate ICL prompts, we have devised\nmultiple strategies to generate a small set of meta-input-output caption pairs. These strategies involve\nutilizing ChatBots, human rewriters, or existing image captioning datasets. Once we have acquired\nthe meta-input-output pairs, we employ them as examples to prompt LLaMA, enabling the rewriting\nof millions of texts within the entire dataset. Unlike existing strategies for text rewriting [58, 50],\nwhich tend to preserve the sentence structure, LLMs exhibit the remarkable ability to generate\nlanguage rewrites with greater richness and diversity. This is attributed to their emergent properties\nand extensive training data. Following the caption rewriting process conducted by LLaMA ICL, each\nimage is now accompanied by a collection of diverse captions resulting from the rewriting process.\nUtilizing these rewritten texts, we proceed to train CLIP models with augmentation also on the text\nside. The text augmentation could be performed by randomly selecting one out of the many captions\nassociated with each image.\nExtensive experiments on various pretraining datasets at different scales demonstrate our proposed\nLaCLIP could significantly improve the transferability of CLIP. For instance, on the LAION-400M\ndataset [49], we observe a notable improvement over CLIP in the zero-shot performance on ImageNet,\nincreasing from 62.0% to 64.4%. We firmly believe that this strategy presents a simple, scalable\napproach that contributes to the array of training strategies available for training image embeddings.\n2\nRelated Works\nVision-Language models. There are a number of earlier works demonstrating the effectiveness of\nlearning visual representations from the supervision of corresponding text [25, 31, 14, 62]. Con-\ntrastive Language-Image Pretraining (CLIP) [44] has attracted significant attention due to its superior\nrepresentation learning and zero-shot transfer ability. This performance is achieved through con-\ntrastive learning on top of image and text features. Another related approach is ALIGN [24], which\nachieves similar performance with larger and noisier datasets. There have been numerous follow-up\nworks that attempt to improve the efficacy and data efficiency of CLIP training. SLIP [41] and De-\nCLIP [34] proposes to improve the performance by incorporating self-supervised training techniques.\nFILIP [60] proposes to leverage cross-modal fine-grained alignment between image patches and text\nwords. CoCa [61] introduces an additional decoder and captioning loss. LIT [63] proposes to boost\nzero-shot transferring performance by fine-tuning the text encoders. BLIP series [33, 32] include\n2\nadditional captioners and incorporates iterative image captioning within the training pipeline, which\nintricately link the generated captions with the image content. However, most of these follow-up\nworks introduce additional training inputs and losses, which can have a negative impact on training\nefficiency and memory consumption.\nText augmentation and Large Language Models. The success of natural language processing (NLP)\ntasks is strongly dependent on the quality and quantity of available data. Deep neural networks benefit\nfrom more data [40, 58]. Common practices in data augmentation include synonym replacement [58],\nrandom masking [30], and back translation [50]. The advent of self-supervised large language models\nlike BERT [16] and GPT series [45, 46] has been a game-changer, as they do not require labeled data\nand can scale up to web-level data to achieve superior transfer ability. Recently, even larger foundation\nmodels with billion-level parameters have emerged, revolutionizing the NLP community. Models like\nthe 175B GPT-3 [5] and 540B PaLM [10] achieve superior performance on various NLP tasks. GPT-3\nalso demonstrates the few-shot in-context learning ability of large language models [5]. Open-sourced\nLLaMA [56] also achieve comparable performances on various benchmarks. In addition to them,\nChatGPT and Bard are chatbots trained with reinforcement learning human feedback (RLHF), and\nhave achieved human-comparable performances on various language understanding tasks [6].\n3\nImproving CLIP with Language Rewrites\nThis section outlines the core design of our LaCLIP framework, highlighting the key components\nand strategies involved. We provide a comprehensive description of our approach, including the\ngeneration of a small set of meta-input-output text pairs from diverse sources, the process of rewriting\nimage descriptions across the entire dataset using LLM ICL, and the enhanced CLIP training strategies\nincorporating these rewritten descriptions.\n3.1\nPreliminary\nCLIP. The Contrastive Language-Image Pretraining (CLIP) method has proven to be highly effective\nto train vision models using language supervision. In this framework, a large batch of N paired\nimages and text {xI, xT } are sampled from the training dataset during each training step. The images\nare pre-processed using data augmentations, and image and text features are extracted using dedicated\nencoders and normalization functions fI and fT . The image text features are used to compute the\nInfoNCE loss, where the paired images and text form the positive pairs and the unpaired ones are\ntreated as negative samples. The training loss can be formulated as follows:\nLI = \u2212\nN\nX\ni=1\nlog\nexp\n\u0000sim(fI(augI(xi\nI)), fT (xi\nT ))/\u03c4\n\u0001\nPN\nk=1 exp\n\u0000sim(fI(augI(xi\nI)), fT (xk\nT ))/\u03c4\n\u0001,\n(1)\nIn this scenario, (xi\nI, xi\nT ) is the ith image-text pair, and augI() denotes the image augmentation\nfunctions. The sim(\u00b7, \u00b7) function measures distance using the dot product, while the temperature \u03c4 is\na learnable parameter that scales the logits. To simplify the presentation, we only show the training\nloss iterating over images. A symmetrical loss LT that iterates over texts is also computed during the\ntraining process. The total training loss is L = (LI + LT )/2.\nLanguage Rewrites as Text Augmentation. In Equation 1, the standard CLIP loss applies aug-\nmentation exclusively to images, leaving the text inputs unchanged throughout the whole training\nprocess. Recognizing this gap, we propose to generate text augmentations, denoted as augT , where\naugT (xT ) is utilized as the input for fT instead of the original xT . In the following subsections, we\nintroduce the methodology employed to generate these text augmentations using LLMs, as well as\nthe integration process during CLIP training. By addressing this gap, we aim to enhance the training\nprocess and expand the benefits of augmentation to the text inputs, leading to improved performance\nand a more comprehensive learning framework.\n3.2\nMeta-Input-Output Text Pair Generation\nA recently uncovered property of autoregressive LLMs is In-Context Learning (ICL) [5, 39]. This\nproperty allows LLMs to learn a new task by conditioning on a few examples and then make\npredictions for a test input. To harness ICL for text rewriting, we first need to generate several\n3\n1. white and red cheerful combination \nin the bedroom for a girl\n2. A tourist taking a photograph of \nriver looking towards suspension \nbridge and office\nN. tree hollow and green leaves of a \ntree top in summer\nSource Captions\nTarget Captions\n1. A bright and lively white-and-red color \nscheme in a girl\u2019s bedroom, creating a \ncheerful ambiance.\n2. Tourist snaps photo of suspension bridge \nand office building across the river.\nN. Amidst lush green leaves on the top of a \ntree, a hollow creates a natural shelter, \ntypical of summer foliage.\n\u2026\n\u2026\n\u201crewrite this image caption\u201d\nFigure 1: Illustraion of using ChatGPT to generate meta-input-output pairs: we first sample source captions\nrandomly from a few datasets. We then use prompts such as \"Rewrite this image caption\" to guide the ChatGPT\nmodel to generate rewritten target captions. The resulting target captions have different sentence structure than\nthe source texts but, crucially, keep the major objects and subjects intact (in bold). These meta-input-output pairs\nare later used as contexts for ICL.\nrewriting examples that can be used as examples in the prompt context. We name these examples\nmeta-input-output text pairs. To this end, we explored different strategies for generating these pairs:\n\u2022 Rewriting with Chatbots. We randomly sample texts from image-text datasets, and prompt\nChatGPT[2] and Bard[1] web portals to generate target texts using a prompt such as \"Rewrite this\ncaption of an image vividly, and keep it less than thirty words:\". Illustrations of this process can be\nfound in Figure 1. Here we leverage the extremely powerful rewriting abilities of these models to\nprovide modified captions that keep the essence of the original caption but change the style and\ndetails. This ensures that the semantics associated with the corresponding image do not change,\nwhich is important for representation learning purposes.\n\u2022 MSCOCO Sampling. Multiple text descriptions for the same image are available in many existing\nimage captioning datasets. To leverage this characteristic, we utilize the widely used MS-COCO\ndataset [36]. Within this dataset, each image is associated with five distinct text descriptions, which\nhave been meticulously annotated by human workers. From this dataset, we randomly select a\nsubset of images. For each selected image, we choose one description as the meta-input text and\nanother as the meta-output text.\n\u2022 Human Rewriting. We randomly sample several image-text pairs from various image-text datasets.\nTo ensure diverse and varied textual variations, we engage human annotators and task them with\nrewriting the captions based on the content depicted in the corresponding observed images. This\nprocess results in the creation of meta-input-output pairs, consisting of the original text and the\nrewritten version by human annotators.\nThrough the utilization of diverse generation strategies, we acquire four distinct types (ChatGPT,\nBard, COCO, and Human) of meta-input-output text pairs, which then serve as valuable examples\nwithin the input context for the In-Context Learning (ICL) framework. For each specific strategy, we\nrandomly select 16 original captions from image-text datasets and generate target captions using that\nstrategy, resulting in a total of 16 meta-input-output pairs. These pairs encompass a range of sources\nand variations, facilitating a comprehensive and diverse training experience for our framework.\n3.3\nLarge scale Language Rewriting\nGenerating rewrites for hundreds of millions of texts using closed-source models like ChatGPT\nor Bard is impractical due to the significant financial and time costs associated with API usage.\nTherefore, to facilitate the rewriting of text samples within any given image-text dataset, we employ\nLLaMA [56]\u2014an open-source state-of-the-art large language model known for its robust performance\nin text completion tasks. Despite not being fine-tuned with instructions, LLaMA exhibits exceptional\nICL capabilities. Leveraging the meta-input-output text pairs generated as described in Section 3.2,\nwe employ LLaMA\u2019s ICL ability to rewrite every text entry within the image-text dataset.\nGiven a text sample to be rewritten, we formulate a context input as the following three parts: Firstly,\nwe include a sentence that informs the LLM about the task of rewriting image descriptions. This\nserves as an initial contextual clue for the LLM to understand the objective at hand. The second part\nof the context encompasses three examples sampled from the meta-input-output pairs described in\nSection 3.2.We randomly three distinct meta-input-output caption pairs from a specific strategy (e.g.,\n4\nA man confidently \nnavigating a winding \nmountain road with \nbreathtaking views.\nContext\nAugmented Caption\nRewrite the following image descriptions:\nwhite red cheerful combination in the bedroom for girl => A bright and lively \nwhite-and-red color scheme in a girl\u2019s bedroom, creating a cheerful ambiance.\nA tourist taking a photograph of river looking towards suspension bridge and \noffice => Tourist snaps photo of suspension bridge and office building across \nthe river. \ntree hollow and green leaves of a tree top in summer => Amidst lush green \nleaves on the top of a tree, a hollow creates a natural shelter, typical of \nsummer foliage.\nman driving a car through the mountains =>\nLLaMA\nText\nCompletion\nFigure 2: Illustration of our proposed in-context learning based strategy for language rewriting: The left box\ndepicts the input context for LLaMA, responsible for text completion. The blue and green texts represent the\nmeta-input-output text pairs, with blue indicating the input and green indicating the output. These pairs are\ndefined in Section 3.2 and visualized in Fig. 1. The final blue line represents the text from the image-text datasets\nintended for rewriting. On the right box, we showcase the completion result generated by the open-source 7B\nparameter LLaMA model [56], which represents the rewritten version of the text of interest.\nChatGPT). Each pair is clearly separated by a \"=>\" symbol. These pairs provide explicit instances\nthat showcase the desired rewriting behavior for the LLM to learn from. The additional random\nsampling process further enable the LLaMA model to generate more diverse text rewrites. Finally,\nthe last part of the context includes the text sample that requires rewriting, followed by the separation\nsymbol. This ensures that the LLM receives the specific text to be rewritten as part of its context\ninput. By incorporating these three parts, we create a comprehensive context that guides the LLM in\neffectively generating diverse and contextually appropriate text rewrites.\nUtilizing the constructed context input as a prompt, LLaMA exhibits its ability to perform text\ncompletion and generate rewritten versions of the corresponding text samples. This process is\nconducted for each text sample present in the pre-training image-text dataset. Specifically, we employ\nthe LLaMA-7B model to generate four distinct rewrites for every text sample in the dataset, with each\nrewrite corresponding to one of the four different meta-input-output sources (ChatGPT, Bard, COCO,\nor Human). It takes 7 hours to generate one rewrite for the entire CC3M dataset on a 8 A100 GPU\nmachine. By incorporating multiple sources and leveraging the capabilities of LLaMA, we ensure the\ngeneration of diverse and contextually relevant text rewrites for each text sample within the dataset.\n3.4\nLaCLIP: Training CLIP with Language Augmentations\nHaving generated M different rewrites for each caption (in our case, M = 4), we are now able\nto bridge the augmentation gap between the image and text inputs, thereby presenting the training\nstrategy for our LaCLIP framework. The key addition to the CLIP framework is the augmentation\nfunction for the text inputs, which can be easily implemented through a straightforward random\nsampling process, where we randomly select a text sample from either the original text or one of the\ngenerated rewrites:\naugT (xT ) \u223c Uniform([xT 0, xT 1 . . . , xT M])\n(2)\nHere xT k is the kth rewritten sample for xT . For the sake of simplicity, we denote the original text as\nxT 0. The training loss over the images becomes:\nLI = \u2212\nN\nX\ni=1\nlog\nexp\n\u0000sim(fI(augI(xi\nI)), fT (augT (xi\nT )))/\u03c4\n\u0001\nPN\nk=1 exp\n\u0000sim(fI(augI(xi\nI)), fT (augT (xk\nT )))/\u03c4\n\u0001,\n(3)\nThe only difference with the original CLIP training here is the additional text augmentation augT ,\nand all other parts remains the same, which does not bring any additional computation or parameter\noverheads compared to original CLIP during training. By incorporating text augmentations into CLIP,\nwe introduce variability and diversity into the training data, enabling the model to learn from both the\noriginal text and the augmented versions. This simple yet effective strategy enhances the training\nprocess and contributes to the overall performance and adaptability of the LaCLIP framework.\n5\nTable 1: Zero-shot transfer evaluation of different models. Performance on ImageNet and 15 common\ndownstream datasets are reported. We highlight the best performance of each setting in bold. We see that\nregardless of the scale of the pre-training dataset, LaCLIP outperforms CLIP [44] and LaSLIP outperforms SLIP\n[41], by up to 8.2% absolute accuracy.\nData\nModel\nFood-101\nCIFAR-10\nCIFAR-100\nSUN397\nCars\nAircraft\nDTD\nPets\nCaltech-101\nFlowers\nSTL-10\nEuroSAT\nRESISC45\nGTSRB\nCountry211\nAverage\nImageNet\nModel Architecture: ViT-B/32\nLAION-400M CLIP\n79.9 91.8 72.0 64.6 77.0 15.8 49.9 84.8 89.3 64.4 95.3 43.2 60.6 36.9 14.5 62.7\n62.0\nLaCLIP\n79.7 92.4 73.0 64.9 81.9 20.8 55.4 87.2 91.8 70.3 97.3 50.6 61.5 49.4 16.0 66.1\n64.4\nModel Architecture: ViT-B/16\nCC3M\nCLIP\n10.3 54.9 21.8 25.0 0.8\n1.4 10.5 12.8 43.3 10.2 77.6 14.1 19.1 6.9\n0.6 20.6\n15.8\nLaCLIP\n14.2 57.1 27.5 35.1 1.6\n1.6 16.6 15.6 52.7 14.7 86.2 15.0 24.3 6.4\n1.0 24.6\n21.5\nCC12M\nCLIP\n50.8 64.9 38.5 44.7 24.1 2.4 19.4 64.1 77.4 33.2 91.0 20.1 38.9 7.3\n5.1 38.8\n40.2\nLaCLIP\n60.7 75.1 43.9 57.0 36.3 5.6 31.0 72.4 83.3 39.9 95.1 27.3 44.3 12.7 8.9 46.2\n48.4\nSLIP\n52.5 80.7 46.3 48.8 24.9 2.3 25.1 58.6 77.6 29.2 89.1 25.8 36.6 6.0\n5.7 40.6\n42.1\nLaSLIP\n62.9 82.0 50.2 59.6 32.2 4.4 30.1 70.6 82.4 37.4 95.0 20.4 45.6 10.1 9.2 46.1\n49.7\nRedCaps\nCLIP\n81.5 70.4 39.9 33.2 19.2 1.9 19.7 82.7 72.8 53.9 92.8 23.3 33.6 8.3\n6.2 42.6\n42.9\nLaCLIP\n85.0 74.8 40.7 40.3 21.3 2.2 23.9 78.2 76.4 59.0 91.4 27.1 41.3 5.6\n7.6 45.0\n46.2\nLAION-400M CLIP\n85.5 93.0 71.7 66.8 83.5 16.7 52.8 90.1 91.2 63.9 97.3 42.4 63.3 46.2 17.8 65.5\n67.0\nLaCLIP\n86.5 93.5 73.9 67.9 87.1 24.2 58.9 90.9 92.4 73.1 98.4 48.3 65.8 46.1 19.6 68.4\n69.3\n4\nExperiments\nDatasets. Our experiments were conducted on four different image-text datasets at different scale:\nConceptual Captions 3M (CC3M) [51], Conceptual Captions 12M (CC12M) [7], RedCaps [15], and\nLAION-400M[49]. RedCaps is a 12M-instance dataset collected exclusively from Reddit, potentially\nexhibiting distinct distributions compared to other datasets. The majority of our ablation studies\nwere performed on the CC12M dataset. We evaluate all the models on ImageNet and 15 common\ndownstream datasets like Food101 [4], SUN397 [59] and FGVCAircraft [38]. Appendix A contains\nthe details for all datasets.\nTraining Parameters. For most of our experiments on CC3M, CC12M, and RedCaps, we utilized\nthe ViT-B/16 architecture [17] and trained the models with a batch size of 8,192 and the AdamW\noptimizer [26]. Additionally, we explored the ViT-L/16 and ViT-S/16 architectures in ablation studies.\nFor LAION-400M, we used both the ViT-B/32 and ViT-B/16 architecture with a batch size of 32,768,\nand followed the exact training setup outlined in [44], training the model for 32 epochs. Appendix B\ncontains a detailed breakdown of our training hyperparameters.\nEvaluation Setup. We consider three evaluation metrics for the trained models: Zero-Shot (ZS)\nclassification accuracy, Few-Shot (FS) classification accuracy and Linear Probing (LP) accuracy.\nFor zero-shot classification, we adopt the same prompt templates as described in the CLIP paper\n[44]. The class text embeddings are used to compute distance with the image feature, and images are\nclassified to class with the shortest distance. For few-shot classification, we follow the set up in [44]\nand perform 5-way 5-shot classification with a weighted kNN classifier on top of the frozen features.\nFor linear probing, following [44, 18], we freeze the pre-trained image encoder and extract features\nfor every image in the downstream dataset. We then train a linear classifier using L-BFGS optimizer\non top of the extracted features. ZS and LP are evaluated on both ImageNet (IN) and 15 Downstream\n(DS) datasets. FS are evaluated on the same downstream datasets. In the ablation studies we report\nthe perforamnce on IN and the mean on DS.\n4.1\nZero-shot Evaluation\nWe provide a comprehensive analysis of the zero-shot transfer performance on ImageNet and down-\nstream datasets in Table 1. Remarkably, across all pretrained datasets, our LaCLIP approach achieves\na significant performance improvement over the baseline CLIP model on both ImageNet and down-\n6\nTable 2: Few-shot transfer evaluation of different models. We report 5-way, 5-shot classification accuracy for\nall downstream datasets. We highlight the best performance of each setting in bold. Similar to zero-shot, in\nnearly all cases, pre-training using language rewrites outperforms vanilla pre-training.\nData\nModel\nFood-101\nCIFAR-10\nCIFAR-100\nSUN397\nCars\nAircraft\nDTD\nPets\nCaltech-101\nFlowers\nSTL-10\nEuroSAT\nRESISC45\nGTSRB\nCountry211\nAverage\nModel Architecture: ViT-B/32\nLAION-400M CLIP\n92.5 87.2 89.0 98.0 98.5 78.9 87.4 94.5 99.2 99.0 96.1 82.8 94.3 79.8 49.7\n88.5\nLaCLIP\n93.5 91.0 90.7 98.2 99.1 82.2 87.5 95.7 99.4 99.2 97.2 80.1 94.2 80.4 52.2\n89.4\nModel Architecture: ViT-B/16\nCC3M\nCLIP\n67.6 64.2 73.6 94.1 54.4 46.1 74.4 76.7 93.3 94.3 84.6 81.4 87.1 66.9 37.3\n73.1\nLaCLIP\n70.0 69.1 76.8 95.2 57.6 49.2 75.8 77.4 95.2 95.0 89.5 81.1 85.5 71.0 37.3\n75.0\nCC12M\nCLIP\n87.0 77.5 82.1 97.2 90.9 62.0 83.3 91.1 98.2 97.6 92.6 83.4 91.2 70.6 44.3\n83.3\nLaCLIP\n89.9 81.3 85.0 98.0 95.3 68.1 84.9 93.4 98.9 98.4 95.9 83.0 92.4 76.4 46.7\n85.8\nSLIP\n87.6 79.2 83.0 97.5 85.6 56.4 85.8 88.1 97.7 97.1 92.5 84.9 91.0 62.4 43.0\n82.1\nLaSLIP\n90.5 84.9 86.6 98.1 91.6 61.0 86.7 89.8 98.7 97.8 94.2 84.0 92.8 65.8 45.4\n84.5\nRedCaps\nCLIP\n94.4 80.6 85.3 95.9 88.5 54.5 82.6 94.5 97.8 99.0 94.8 84.9 91.3 75.3 40.6\n84.0\nLaCLIP\n95.8 81.4 85.4 96.2 90.9 58.8 82.4 94.1 98.0 99.2 95.6 86.2 92.1 76.5 42.6\n85.0\nLAION-400M CLIP\n95.0 90.1 90.7 98.2 99.2 80.8 88.7 96.2 99.5 99.4 97.1 84.5 95.0 77.7 55.1\n89.8\nLaCLIP\n95.8 92.7 91.9 98.4 99.5 86.1 89.0 97.1 99.6 99.5 98.1 82.9 95.0 80.9 57.9\n91.0\nstream datasets. For instance, when training models on the CC12M dataset, our LaCLIP method\nachieves over 8% improvement in absolute top-1 accuracy on ImageNet and 7% improvement on\naverage over the other downstream datasets. LaCLIP and CLIP share the exact same amount of\nparameters and computation cost during training.\nAdaptability to other methods. It is noteworthy that LaCLIP is compatible with other techniques\nintended to enhance CLIP\u2019s performance. Once the augmented texts are generated, integrating\nLaCLIP into any CLIP-based framework can be achieved seamlessly without incurring additional\ncomputational or memory overhead. As demonstrated in Table 1, we applied language augmentation\nto the SLIP framework and yield LaSLIP, resulting in significant performance improvements across\nall evaluation metrics. Notably, even though SLIP already incorporates additional self-supervision to\nenhance CLIP\u2019s performance, our proposed language augmentation further boosts its effectiveness.\nThis showcases the generalization capability of our proposed text augmentation strategy.\nGeneralization to Larger Datasets. Consistently, the results highlight the substantial margin by\nwhich LaCLIP outperforms CLIP across various datasets. Noteworthy is the scalability of our method\nwith dataset size, as it demonstrates improvement even when trained on the massive LAION-400M\ndataset, which contains hundreds of millions of data points1. These findings suggest that our LaCLIP\napproach can be seamlessly integrated as a plug-and-play component for training vision-language\nfoundation models.\n4.2\nFew-Shot & Linear-Probing\nWe present the 5-way 5-shot classification performance in Table 2 and the linear-probing performance\nin Table 3. Our approach consistently outperforms vanilla CLIP or SLIP in the vast majority of cases.\nInterestingly, SLIP performs worse than vanilla CLIP in the few-shot setting, despite introducing\nadditional self-supervision from the image side. However, by incorporating our proposed language\naugmentation strategy, SLIP\u2019s few-shot performance improves, surpassing vanilla CLIP. This result\nhighlights the effectiveness of text augmentations in the few-shot setting.\nFurthermore, it is important to highlight that the improvements observed in the few-shot and linear-\nprobing results are solely achieved through the utilization of image encoders. This demonstrates\nthe efficacy of our proposed text augmentation approach in enhancing not only the joint image-text\nembedding space, which aligns image and text features more effectively, but also the quality of the\n1The version used in our experiment contains \u223c340M samples, slightly less than original due to link rot. We\nuse OpenCLIP implementation ( https://github.com/mlfoundations/open_clip) and achieves\n62.0% ImageNet zero-shot accuracy for CLIP, comparable to their model with 62.9% trained on the full dataset.\n7\nTable 3: Linear Probing comparison of different models. Performances on ImageNet and 15 common down-\nstream datasets are reported. We highlight the best performance of each setting in bold. Similar to ZS and FS,\nlinear probe performance for our approach is almost always better than that of vanilla CLIP or SLIP.\nData\nModel\nFood-101\nCIFAR-10\nCIFAR-100\nSUN397\nCars\nAircraft\nDTD\nPets\nCaltech-101\nFlowers\nSTL-10\nEuroSAT\nRESISC45\nGTSRB\nCountry211\nAverage\nImageNet\nModel Architecture: ViT-B/32\nLAION-400M CLIP\n85.8 95.8 83.6 75.1 89.2 54.3 79.7 86.9 94.5 96.8 97.9 96.3 93.5 88.6 23.1 82.7\n74.6\nLaCLIP\n85.1 96.2 84.2 75.6 90.1 56.1 79.6 89.1 94.8 97.7 98.4 95.8 93.6 88.6 22.9 83.2\n75.3\nModel Architecture: ViT-B/16\nCC3M\nCLIP\n62.6 86.8 68.1 58.5 32.8 40.9 63.4 69.6 82.0 89.4 91.7 95.9 89.0 71.9 13.3 67.7\n54.5\nLaCLIP\n63.8 87.7 69.5 60.2 32.4 42.7 64.0 71.1 83.3 90.2 93.4 95.8 89.7 74.6 13.2 68.8\n56.5\nCC12M\nCLIP\n81.6 93.8 79.3 72.0 75.1 52.6 75.6 86.2 92.2 95.3 97.3 96.7 93.1 80.6 19.7 79.4\n70.3\nLaCLIP\n82.9 94.7 79.7 73.8 79.9 54.5 75.7 87.7 93.0 96.4 98.0 96.4 93.0 81.9 19.7 80.5\n72.3\nSLIP\n84.4 94.2 79.1 73.5 74.2 54.6 76.5 86.1 92.7 95.7 97.6 96.8 93.7 74.0 20.6 79.6\n73.2\nLaSLIP\n85.2 94.6 80.8 75.1 77.0 53.8 78.5 85.6 93.7 96.5 97.9 96.8 93.5 76.1 21.1 80.4\n74.4\nRedCaps\nCLIP\n89.1 94.1 78.8 65.6 74.0 52.5 73.2 91.5 91.4 97.7 98.0 96.3 93.5 80.8 17.0 79.6\n71.8\nLaCLIP\n90.1 94.3 78.5 66.6 77.6 53.6 73.9 90.8 91.5 97.9 97.6 96.6 92.7 80.8 17.2 80.0\n71.9\nLAION-400M CLIP\n90.5 96.9 85.0 78.1 92.1 57.2 80.0 90.9 95.7 98.0 98.7 96.7 94.7 90.3 27.0 84.8\n78.6\nLaCLIP\n90.7 96.7 85.5 78.7 92.8 63.1 81.3 92.8 96.2 98.8 99.1 96.4 94.6 89.5 27.5 85.6\n79.9\nimage representations themselves. This improvement is realized by pairing images with a wider\nrange of diverse texts, providing richer and more varied supervision to the vision encoder. As a result,\nthe vision encoder is able to learn more generalizable and robust image representations that can be\neffectively utilized across a range of downstream tasks. To further emphasize the effectiveness of our\napproach, we provide detailed t-SNE visualizations of the learned features with different approaches\nin Appendix F. These visualizations qualitatively demonstrate that features learned with LaCLIP\nexhibit clearer class boundaries across different classes, reinforcing the enhanced discriminative\npower of our approach.\n4.3\nAblation Studies\nVarying Augmentation Strategies. Table 4 presents a comparison of our proposed LLM-based\ntext augmentation strategy with existing language augmentation methods, namely EDA [58], which\ninvolves word-level operations such as synonym replacements, and back translation [50], which\ntranslates the text to another language and then back to the original language. Although these simpler\naugmentation techniques demonstrate some improvements, the enhancements achieved are relatively\nmarginal, especially when compared to our proposed LLM-based approach. This disparity can be\nattributed to the significantly higher diversity in sentence structures that our method achieves. In\nAppendix D, we provide further details and qualitative comparisons of the various text augmentation\ntechniques using different methods.\nScaling with Number of Augmentations. Figure 3 demonstrates the impact of the number of\naugmented texts per sample in our LaCLIP approach on ImageNet zero-shot accuracy. A value of 0\naugments corresponds to vanilla CLIP without any text augmentation. The results clearly demonstrate\nthat simpler augmentation strategies exhibit poor scalability as the number of augments increases.\nThis is attributed to their limited diversity. Conversely, our LLM-based text augmentation consistently\nimproves performance as more augmentations are added.\nDifferent Meta-Input-Output Pair for ICL. In order to assess the impact of different meta-input-\noutput caption pairs used for LLaMA ICL, we conducted an experiment by disentangling LaCLIP\nand training four models. Each model was trained using only the ground truth caption combined\nwith a specific type of LLM augmented version, using a particular meta-input-output pair (ChatGPT,\nBard, COCO, or human). The performance comparison between these models is presented in Table 5.\nIt can be observed that different strategy yield similar performance, with the model trained with\naugmentations using the Human pair slightly outperforming the others. We conjecture the reason\n8\nTable 4: Performance comparison of CLIP training\nwith different text augmentations.\nAugment\nZS\nFS\nLP\nDS\nIN\nDS\nIN\nN/A (CLIP)\n38.8\n40.2\n83.3\n79.4\n70.3\nEDA [58]\n40.6\n41.2\n83.4\n79.4\n70.5\nBack Trans [50]\n40.4\n41.6\n83.9\n79.8\n70.7\nLLM (Ours)\n46.2\n48.4\n85.8\n80.5\n72.3\nTable 5: Ablation of CLIP trained with augmented\ntexts prompted by different meta-input-output pairs.\nSource\nZS\nFS\nLP\nDS\nIN\nDS\nIN\nChatGPT\n42.3\n44.5\n84.8\n79.8\n71.2\nBard\n41.7\n44.8\n85.0\n79.6\n71.2\nMSCOCO\n42.1\n44.6\n84.8\n79.8\n71.3\nHuman\n43.0\n45.1\n84.8\n79.9\n71.3\nTable 6: Ablation on different network architectures.\nBackbone\nMethod\nZS\nFS\nLP\nDS\nIN\nDS\nIN\nViT-S/16\nCLIP\n36.3\n36.9\n82.2\n77.0\n67.1\nLaCLIP\n44.1\n46.3\n84.5\n78.0\n69.1\nViT-B/16\nCLIP\n38.8\n40.2\n83.3\n79.4\n70.3\nLaCLIP\n46.2\n48.4\n85.8\n80.5\n72.3\nViT-L/16\nCLIP\n42.6\n44.0\n85.1\n81.3\n72.9\nLaCLIP\n46.6\n49.1\n86.8\n81.9\n73.7\n0\n1\n2\n3\n4\n# of Text Augments\n40\n42\n44\n46\n48\n50\nImageNet Zeroshot Accuracy\n+ChatGPT\n+Bard\n+MSCOCO\n+Human\nEDA\nBack Translation\nLLM\nFigure 3: ImageNet zero-shot accuracy with differ-\nent num of text augments.\nTable 7: Performance comparison of CLIP, LaCLIP\nand LaCLIP-MT on different datasets.\nDataset\nMethod\nZS\nFS\nLP\nDS\nIN\nDS\nIN\nCC12M\nCLIP\n38.8\n40.2\n83.3\n79.4\n70.3\nLaCLIP\n46.2\n48.4\n85.8\n80.5\n72.3\nLaCLIP-MT\n45.2\n49.0\n85.8\n80.6\n72.4\nRedCaps\nCLIP\n42.6\n42.9\n84.0\n79.6\n71.8\nLaCLIP\n45.0\n46.2\n85.0\n80.0\n71.9\nLaCLIP-MT\n46.1\n48.1\n85.3\n80.3\n72.4\nis that during the meta-input-output generation process, humans have the advantage of viewing the\ncorresponding image, which allows them to generate more accurate and diverse rewrites.\nDifferent Backbone Architecture. We further investigate the performance of LaCLIP using different\nbackbone architectures. Table 6 summarize the results obtained with ViT-S/16, ViT-B/16, and ViT-\nL/16. We observe that LaCLIP scales with model size and consistently outperforms vanilla CLIP\nacross all network architectures. These findings highlight the effectiveness of LaCLIP in improving\nthe performance of CLIP models, irrespective of the underlying backbone architecture.\nTable 8: Performance comparison of LaCLIP and CLIP\nmodels trained with different pre-trained text encoder\nconfigurations.\nMethod\nText Encoder\nZS\nFS\nLP\nPre-train Freeze\nDS\nIN\nDS\nIN\nCLIP\n\u2718\n\u2718\n38.8\n40.2\n83.3\n79.4\n70.3\n\u2714\n\u2718\n42.1\n42.9\n83.6\n79.5\n70.4\n\u2714\n\u2714\n24.5\n23.2\n80.3\n74.9\n66.0\nLaCLIP\n\u2718\n\u2718\n46.2\n48.4\n85.8\n80.5\n72.3\nComparison with Pre-trained Text Encoder.\nTo deepen the comprehension on whether La-\nCLIP outperforms vanilla CLIP simply due to a\nbetter text encoder, we conducted experiments\ncomparing LaCLIP with CLIP models trained\nwith a pre-trained text encoder. Here we em-\nployed the BERT model as the pre-trained text\nencocder.\nThe experiment result in Table 8\ndemonstrates that fine-tuning based on the pre-\ntrained text encoder exhibits some improve-\nments, whereas freezing the pre-trained text en-\ncoder weights substantially degrades performance. This observation aligns with the findings in\nLiT [63]. In contrast, LaCLIP consistently outperforms all configurations with a pre-trained text\nencoder, underscoring the benefit and necessity for explicit sentence augmentation strategies.\n5\nMulti-Text Training Loss with LaCLIP\nIt is important to highlight that once we have generated multiple text augmentations, and with a\nslight tolerance for computational cost, we can create multi-positive training pairs for each training\niteration. These pairs are formed by pairing each image with not only the original text but also with\nall the rewritten versions of the text. By adopting this approach, we introduce a Multi-Text version\n9\nof LaCLIP, referred to as LaCLIP-MT, which incorporates a multi-positive contrastive training loss.\nThe training process iterates through all the images in the following manner:\nLI\u2217 = \u2212 1\nM\nN\nX\ni=1\nM\nX\nj=0\nlog\nexp\n\u0000sim(fI(augI(xi\nI)), fT (xi\nT j))/\u03c4\n\u0001\nPN\nk=1 exp\n\u0000sim(fI(augI(xi\nI)), fT (xk\nT j))/\u03c4\n\u0001,\n(4)\nSince each text can still be paired with a single image, the training loss that iterates through all the\ntexts remains unchanged, with the only difference being that it now iterates over all of the texts\ninstead of just the augmented ones. Consequently, the final training loss is given by the average of\nthe image loss (LI\u2217) and the text loss (LT ), resulting in L = (LI\u2217 + LT )/2.\nIn order to showcase the efficacy of the multi-positive contrastive training loss in boosting the\nperformance of LaCLIP, we conducted additional experiments on the CC12M and RedCaps datasets.\nThe results of these experiments are summarized in Table 7, which compares the performance of\nLaCLIP-MT with both LaCLIP and vanilla CLIP. The results clearly indicate that LaCLIP-MT could\nfurther improve upon LaCLIP across most metrics. By allowing each image to be paired with all of\nthe diverse texts describing its content, LaCLIP-MT leverages the additional and richer supervision\nfrom the language modality to enhance the formation of image-text embeddings. This improvement\nhighlight the benefits of the multi-positive contrastive training loss in facilitating better alignment\nbetween images and diverse text descriptions.\n6\nConclusion, Limitations and Broader Impact\nConclusion.\nWe have introduced LaCLIP, a straightforward yet highly effective CLIP training\nstrategy that incorporates text augmentations through text rewriting, leveraging the in-context learning\ncapabilities of LLMs. Through this simple and versatile approach, we have demonstrated significant\nimprovements in the performance of CLIP embeddings across various pre-training scales and datasets.\nAdditionally, we have proposed a novel multi-text training loss to further enhance the training process.\nAs LLMs continue to improve in performance and in-context learning capabilities, our approach\nstands to directly benefit from these advancements.\nLimitations. While the training process itself does not entail any additional memory or computation\noverhead compared to vanilla CLIP, the process of generating text rewrites using LLMs can be\ncomputationally expensive, requiring significant GPU resources and taking hours for large datasets.\nAdditionally, the quality of the rewritten text generated by LLaMA is not filtered, which may result\nin some irrelevant details that do not align well with the corresponding images. This misalignment\ncould impact the transferability of the learned embeddings to downstream tasks. To address these\nlimitations, future work could focus on developing more efficient methods for generating text rewrites\nusing LLMs, reducing the computational burden without sacrificing performance. Furthermore,\ntechniques for filtering the rewritten texts could be explored, aiming to retain only the most relevant\nand accurate versions while discarding those with misleading details. This would enable the model to\nlearn a better embedding space that is robust and transferable across different downstream datasets,\nimproving overall performance and alignment between vision and text encoders.\nBroader Impact. We propose a general text augmentation strategy that can generate diverse rewrites\nfor any given text. This strategy not only improves the performance of vision-language models\nbut also has the potential to enhance models in pure natural language processing tasks, such as\nlanguage understanding and reasoning. On the other hand, we acknowledge that LLMs are trained\non large-scale web data, which may contain factual errors and hallucinations. Consequently, the\nrewritten versions of texts may also inherit these limitations. Therefore, we encourage researchers to\nimplement additional data filtering methods before deploying these models in real-world scenarios.\nAdditionally, the current LLM-based rewriting strategy requires significant GPU/TPU computation,\nwhich can contribute to a higher carbon footprint. However, it is also possible that such rewriting\nstrategy can significantly reduce the number of training iterations for larger models to reach similar\nperformances as vanilla CLIP.\n10\nAcknowledgements\nWe would like to thank Mathilde Caron for the insightful early manuscript review, and the anonymous\nreviewers for their helpful comments and suggestions. Additionally, we appreciate the support and\ndiscussions with the VisCAM team at Google Research.\nReferences\n[1] Bard. https://bard.google.com/. 2, 4\n[2] ChatGPT. https://chat.openai.com/. 2, 4\n[3] Daniel Adiwardana, Minh-Thang Luong, David R So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang,\nApoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, et al. Towards a human-like open-domain chatbot.\narXiv preprint arXiv:2001.09977, 2020. 2\n[4] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101\u2013mining discriminative components\nwith random forests. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland,\nSeptember 6-12, 2014, Proceedings, Part VI 13, pages 446\u2013461. Springer, 2014. 6, 16\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877\u20131901, 2020. 2, 3\n[6] S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early\nexperiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023. 3\n[7] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale\nimage-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 3558\u20133568, 2021. 6, 15\n[8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\ncontrastive learning of visual representations. In International conference on machine learning, pages\n1597\u20131607. PMLR, 2020. 1\n[9] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and\nstate of the art. Proceedings of the IEEE, 105(10):1865\u20131883, 2017. 16\n[10] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language\nmodeling with pathways. arXiv preprint arXiv:2204.02311, 2022. 3\n[11] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing\ntextures in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 3606\u20133613, 2014. 16\n[12] Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature\nlearning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics,\npages 215\u2013223. JMLR Workshop and Conference Proceedings, 2011. 16\n[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255.\nIeee, 2009. 15\n[14] Karan Desai and Justin Johnson. Virtex: Learning visual representations from textual annotations. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11162\u201311173,\n2021. 2, 15, 30\n[15] Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. Redcaps: Web-curated image-text data\ncreated by the people, for the people. arXiv preprint arXiv:2111.11431, 2021. 6, 15\n[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 3\n[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 6, 15\n11\n[18] Linus Ericsson, Henry Gouk, and Timothy M Hospedales. How well do self-supervised models transfer? In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5414\u20135423,\n2021. 6, 16\n[19] Li Fei-Fei, Robert Fergus, and Pietro Perona. One-shot learning of object categories. IEEE transactions on\npattern analysis and machine intelligence, 28(4):594\u2013611, 2006. 16\n[20] Priya Goyal, Piotr Doll\u00e1r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew\nTulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv\npreprint arXiv:1706.02677, 2017. 16\n[21] Priya Goyal, Quentin Duval, Jeremy Reizenstein, Matthew Leavitt, Min Xu, Benjamin Lefaudeux, Mannat\nSingh, Vinicius Reis, Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Ishan Misra. Vissl. https:\n//github.com/facebookresearch/vissl, 2021. 15\n[22] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders\nare scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 16000\u201316009, 2022. 1\n[23] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep\nlearning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied\nEarth Observations and Remote Sensing, 12(7):2217\u20132226, 2019. 16\n[24] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung,\nZhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text\nsupervision. In International Conference on Machine Learning, pages 4904\u20134916. PMLR, 2021. 2\n[25] Armand Joulin, Laurens Van Der Maaten, Allan Jabri, and Nicolas Vasilache. Learning visual features from\nlarge weakly supervised data. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam,\nThe Netherlands, October 11\u201314, 2016, Proceedings, Part VII 14, pages 67\u201384. Springer, 2016. 2\n[26] Diederik P Kingma and Jimmy Ba.\nAdam: A method for stochastic optimization.\narXiv preprint\narXiv:1412.6980, 2014. 6\n[27] Skanda Koppula, Yazhe Li, Evan Shelhamer, Andrew Jaegle, Nikhil Parthasarathy, Relja Arandjelovic,\nJo\u00e3o Carreira, and Olivier H\u00e9naff. Where should i spend my flops? efficiency evaluations of visual\npre-training methods. arXiv preprint arXiv:2209.15589, 2022. 1\n[28] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained\ncategorization. In Proceedings of the IEEE international conference on computer vision workshops, pages\n554\u2013561, 2013. 16\n[29] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 16\n[30] Varun Kumar, Ashutosh Choudhary, and Eunah Cho. Data augmentation using pre-trained transformer\nmodels. arXiv preprint arXiv:2003.02245, 2020. 3\n[31] Ang Li, Allan Jabri, Armand Joulin, and Laurens Van Der Maaten. Learning visual n-grams from web data.\nIn Proceedings of the IEEE International Conference on Computer Vision, pages 4183\u20134192, 2017. 2\n[32] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. 2\n[33] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training\nfor unified vision-language understanding and generation. In International Conference on Machine\nLearning, pages 12888\u201312900. PMLR, 2022. 2\n[34] Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu, and Junjie\nYan. Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm.\narXiv preprint arXiv:2110.05208, 2021. 2\n[35] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He. Scaling language-image\npre-training via masking. arXiv preprint arXiv:2212.00794, 2022. 1\n[36] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r,\nand C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014:\n13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages\n740\u2013755. Springer, 2014. 4\n12\n[37] Ilya Loshchilov and Frank Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017. 17\n[38] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual\nclassification of aircraft. arXiv preprint arXiv:1306.5151, 2013. 6, 16\n[39] Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. Metaicl: Learning to learn in\ncontext. arXiv preprint arXiv:2110.15943, 2021. 3\n[40] Lili Mou, Zhao Meng, Rui Yan, Ge Li, Yan Xu, Lu Zhang, and Zhi Jin. How transferable are neural\nnetworks in nlp applications? arXiv preprint arXiv:1603.06111, 2016. 3\n[41] Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie. Slip: Self-supervision meets language-\nimage pre-training. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October\n23\u201327, 2022, Proceedings, Part XXVI, pages 529\u2013544. Springer, 2022. 1, 2, 6, 15\n[42] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of\nclasses. In 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing, pages\n722\u2013729. IEEE, 2008. 16\n[43] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In 2012 IEEE\nconference on computer vision and pattern recognition, pages 3498\u20133505. IEEE, 2012. 16\n[44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR,\n2021. 1, 2, 6, 15, 16\n[45] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding\nby generative pre-training. 2018. 2, 3\n[46] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 3\n[47] Joshua Robinson, Li Sun, Ke Yu, Kayhan Batmanghelich, Stefanie Jegelka, and Suvrit Sra. Can contrastive\nlearning avoid shortcut solutions? Advances in neural information processing systems, 34:4974\u20134986,\n2021. 1\n[48] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,\nTheo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale\ndataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022. 1\n[49] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush\nKatta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400\nmillion image-text pairs. arXiv preprint arXiv:2111.02114, 2021. 1, 2, 6, 15\n[50] Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation models with\nmonolingual data. arXiv preprint arXiv:1511.06709, 2015. 2, 3, 8, 9, 22, 24\n[51] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556\u20132565,\n2018. 6, 15\n[52] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. Advances in\nneural information processing systems, 30, 2017. 16\n[53] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The german traffic sign recognition\nbenchmark: a multi-class classification competition. In The 2011 international joint conference on neural\nnetworks, pages 1453\u20131460. IEEE, 2011. 16\n[54] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian\nBorth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications of the ACM,\n59(2):64\u201373, 2016. 16\n[55] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou.\nTraining data-efficient image transformers & distillation through attention. In International conference on\nmachine learning, pages 10347\u201310357. PMLR, 2021. 15\n13\n[56] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971, 2023. 2, 3, 4, 5\n[57] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning\nresearch, 9(11), 2008. 27\n[58] Jason Wei and Kai Zou. Eda: Easy data augmentation techniques for boosting performance on text\nclassification tasks. arXiv preprint arXiv:1901.11196, 2019. 2, 3, 8, 9, 22, 24, 26\n[59] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-\nscale scene recognition from abbey to zoo. In 2010 IEEE computer society conference on computer vision\nand pattern recognition, pages 3485\u20133492. IEEE, 2010. 6, 16\n[60] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li,\nXin Jiang, and Chunjing Xu. Filip: fine-grained interactive language-image pre-training. arXiv preprint\narXiv:2111.07783, 2021. 2\n[61] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca:\nContrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917, 2022. 2\n[62] Xin Yuan, Zhe Lin, Jason Kuen, Jianming Zhang, Yilin Wang, Michael Maire, Ajinkya Kale, and Baldo\nFaieta. Multimodal contrastive training for visual representation learning. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 6995\u20137004, 2021. 2\n[63] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and\nLucas Beyer. Lit: Zero-shot transfer with locked-image text tuning. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 18123\u201318133, 2022. 2, 9\n14\nAppendices\nWe provide additional experiment details, results and analysis in the supplementary material.\nA. Dataset Details\nA.1 Pre-training Datasets\nWe utilized four image-text pre-training datasets of varying scales, namely CC3M [51], CC12M [7],\nRedCaps [15], and LAION-400M [49], to train both CLIP and LaCLIP models. Additionally, we\ntrained SLIP and LaSLIP models on the CC12M dataset. It is important to note that due to image\nlink rot within the datasets, the versions we obtained may have slightly fewer images compared to the\noriginal versions. As a result, there may be slight performance differences when compared to models\ntrained on the full image versions. Below are detailed descriptions of the four pre-training datasets:\nCC3M [51]: This dataset comprises 3.3 million image-text pairs extracted from 5 billion webpages.\nThe image descriptions are derived from the HTML alt-text attribute. The version we used consists\nof 2.8 million unique samples.\nCC12M [7]: With a similar procedure as CC3M, CC12M consists of 12.4 million image-text pairs.\nThe filters used in this dataset are more relaxed, resulting in a wider range of topics and visual\nconcepts, making it more reflective of real-world scenarios. The version we acquired contains 10.0\nmillion samples.\nRedCaps [14]: RedCaps encompasses 12.0 million image-caption pairs gathered exclusively from\nReddit across 350 subreddits. The captions are sourced from Reddit instead of HTML alt-text. The\nversion we acquired includes 11.7 million unique samples.\nLAION-400M [49]: This dataset is constructed by processing and filtering the Common Crawl\ndataset. The original version contains 413 million unique samples, while the version we obtained\nconsists of 340 million samples.\nFor all datasets, we resized the images such that the shorter side measured 256 pixels.\nA.2 Downstream Datasets\nWe conducted evaluations on our pre-trained model using both ImageNet [13] and 15 widely-used\ndownstream datasets. To prepare the downstream datasets, we utilized torchvision and VISSL [21].\nThe detailed information about the downstream datasets can be found in Table A1.\nB. Implementation Details\nEncoders We employed the standard ViT-S/16, ViT-B/16, ViT-B/32 and ViT-L/16 architectures from\n[17, 55] as our vision encoders. Specifically, ViT-B/16 is used on all CC3M, CC12M and RedCaps\ndatasets. ViT-B/32 is used on LAION-400M. ViT-S/16 and ViT-L/16 are used on CC12M. Following\nthe approach in SLIP [41], we utilized the smallest text encoder from CLIP [44]. Our tokenizer\nwas consistent with CLIP, having a vocabulary size of 49, 408 and a maximum context length of 77.\nFurther details about the encoders can be found in Table A2.\nHyper-Parameters Table A3 provides an overview of the pre-training hyperparameters used for\nCLIP on all datasets. Following [44, 41], we perform RandomResizedCrop augmentation for the\nimages. For SLIP training, the learning rate was set to 3 \u00d7 10\u22123, weight decay was set to 0.1, and\nall other parameters remained the same. Further details can be found in Table A4. The pre-training\nprocess was conducted on four machines with eight A100 GPUs each.\nZero-shot Classification We follow a similar prompt ensemble strategy as described in [44] and\nemploy the same set of prompting templates. For each class name, we compute the average text\nembedding across all templates. These averaged embeddings are then used to calculate the similarity\nbetween each test image and the class embeddings. Specifically, for zero-shot evaluation on ImageNet,\nmodels trained on the LAION-400M dataset use the exact 80 prompts provided by [44] to ensure a\n15\nTable A1: Details of the downstream classification datasets.\nDataset\nMetric\nCategories\nTrain Size\nTest Size\nFood-101 [4]\nAccuracy\n101\n75,750\n25,250\nCIFAR-10 [29]\nAccuracy\n10\n50,000\n10,000\nCIFAR-100 [29]\nAccuracy\n100\n50,000\n10,000\nSUN397 [59]\nAccuracy\n397\n19,850\n19,850\nStanford Cars [28]\nAccuracy\n196\n8,144\n8,041\nFGVC Aircraft [38]\nMean per class\n100\n6,667\n3,333\nDTD [11]\nAccuracy\n47\n3,760\n1,880\nOxford Pets [43]\nMean per class\n37\n3,680\n3,669\nCaltech-101 [19]\nMean per class\n102\n3,060\n6,085\nOxford Flowers [42]\nMean per class\n102\n2,040\n6,149\nSTL-10 [12]\nAccuracy\n10\n1,000\n8,000\nEuroSAT [23]\nAccuracy\n10\n10,000\n5,000\nRESISC45 [9]\nAccuracy\n45\n25,200\n6,300\nGTSRB [53]\nAccuracy\n43\n26,640\n12,630\nCountry211 [44, 54]\nAccuracy\n211\n42,200\n21,100\nTable A2: Encoder details.\nModel\nPatch\nInput\nEmbedding\nVision Transformer\nText Transformer\nVocab\nText\nsize\nresolution\ndimension\nLayers\nWidth\nHeads\nLayers\nWidth\nHeads\nsize\nlength\nViT-S/16\n16\n224\n512\n12\n384\n12\n12\n512\n8\n49,408\n77\nViT-B/16\n16\n224\n512\n12\n768\n12\n12\n512\n8\nViT-B/32\n32\n224\n512\n12\n768\n12\n12\n512\n8\nViT-L/16\n16\n224\n512\n24\n1024\n16\n12\n512\n8\nfair comparison. For models trained on other datasets, we use a subset of 7 templates recommended\nby [44] to expedite the evaluation process.\nFew-shot Classification Following the settings in [18], we evaluate the 5-way 5-shot performance\nacross 15 downstream datasets. We use Prototypical Networks [52] as classifier on top of the features\nextracted from vision encoders without data augmentation. Only Resize followed by CenterCrop is\napplied here for all images. We evaluate each model for 600 randomly sampled episodes, and for\neach episode, images are sampled from the combination of training, validation and testing sets. We\nalways sample 15 images for each class as query set. The mean accuracy across all episodes are\nreported in the main paper, and we also report the 95% confidence interval in the appendix.\nLinear-Probing For linear probing on ImageNet, we keep the image encoder frozen and train a\nLinear Classifier on the extracted features. The only augmentation applied is RandomHorizontalFlip.\nWe sweep the base learning rate across the range of [0.002, 0.005, 0.01, 0.015, 0.02, 0.03, 0.05] and\nreport the best performance achieved. The learning rate is scaled linearly based on the actual batch\nsize, following the approach outlined in [20]. Details of all other hyperparameters can be found in\nTable A5. For linear probing on all other downstream datasets, we train a logistic regression layer on\ntop of the frozen features extracted from the vision encoders, without applying any data augmentation.\nThe model is optimized using L-BFGS with Scikit-learn, and the maximum number of iterations is\nset to 500. To determine the optimal \u21132 regularization term for each model and dataset, we perform a\nsweep across 45 steps that are logarithmically spaced ranging from 10\u22126 to 105 on the validation\nset. For the final results, we fit the model on the combined training and validation sets and report the\nperformance on the separate test set.\nC. Meta-input-output Details\nC.1 Meta-input-output Pairs\nHere we provide the exact 16 meta-input-output pairs we used as templates for all four set ups:\nChatGPT, Bard, Human and MSCOCO, described in Section 3.2. We use \u2019Source\u2019 to represent the\nmeta-input text we sampled from the image-text datasets, and use \u2019Target\u2019 to represent the meta-output\ntext generated by each of the strategies. Note the meta-input-output pairs showed in Figure 1 and\nFigure 2 in the main text are for illustration only, please refer to this section for the real pairs used in\nthe experiments.\n16\nTable A3: Detailed pre-training hyper-parameters for CLIP training on all four image-text datasets.\n(a) Pre-training hyper-parameter on CC3M.\nConfig\nValue\nBatch size\n8, 192\nOptimizer\nAdamW [37]\nLearning rate\n1 \u00d7 10\u22123\nWeight decay\n0.5\nAdam \u03b2\n\u03b21, \u03b22 = (0.9, 0.98)\nAdam \u03f5\n1 \u00d7 10\u22128\nTotal epochs\n40\nWarm up epochs\n1\nLearning rate schedule\ncosine decay\n(b) Pre-training hyper-parameter on CC12M.\nConfig\nValue\nBatch size\n8, 192\nOptimizer\nAdamW [37]\nLearning rate\n1 \u00d7 10\u22123\nWeight decay\n0.5\nAdam \u03b2\n\u03b21, \u03b22 = (0.9, 0.98)\nAdam \u03f5\n1 \u00d7 10\u22128\nTotal epochs\n35\nWarm up epochs\n1\nLearning rate schedule\ncosine decay\n(c) Pre-training hyper-parameter on RedCaps.\nConfig\nValue\nBatch size\n8, 192\nOptimizer\nAdamW [37]\nLearning rate\n1 \u00d7 10\u22123\nWeight decay\n0.5\nAdam \u03b2\n\u03b21, \u03b22 = (0.9, 0.98)\nAdam \u03f5\n1 \u00d7 10\u22128\nTotal epochs\n30\nWarm up epochs\n1\nLearning rate schedule\ncosine decay\n(d) Pre-training hyper-parameter on LAION-400M.\nConfig\nValue\nBatch size\n32, 768\nOptimizer\nAdamW [37]\nLearning rate\n5 \u00d7 10\u22124\nWeight decay\n0.2\nAdam \u03b2\n\u03b21, \u03b22 = (0.9, 0.98)\nAdam \u03f5\n1 \u00d7 10\u22126\nTotal epochs\n32\nWarm up iterations\n2, 000\nLearning rate schedule\ncosine decay\nTable A4: SLIP hyper-parameters.\nConfig\nValue\nBatch size\n8, 192\nOptimizer\nAdamW [37]\nLearning rate\n3 \u00d7 10\u22123\nWeight decay\n0.1\nAdam \u03b2\n\u03b21, \u03b22 = (0.9, 0.98)\nAdam \u03f5\n1 \u00d7 10\u22128\nTotal epochs\n35\nWarm up epochs\n1\nLearning rate schedule\ncosine decay\nTable A5: Detailed hyper-parameters on Linear-\nProbing on ImageNet.\nConfig\nValue\nBatch size\n1, 024\nOptimizer\nSGD\nBase learning rate\nsweep\nWeight decay\n0\nMomentum\n0.9\nTraining epochs\n90\nLearning rate schedule\ncosine decay\nChatGPT:\n1. Source: white and red cheerful combination in the bedroom for girl\nTarget: A bright and lively white-and-red color scheme in a girl\u2019s bedroom, creating a cheerful\nambiance.\n2. Source: vintage photograph of a young boy feeding pigeons .\nTarget: A charming vintage photograph capturing a young boy feeding a flock of pigeons in a\nbustling city square.\n3. Source: businessman with smartphone sitting on ledge by the sea\nTarget: Serene coastal view as a businessman sits on a ledge by the sea, using his smartphone.\n4. Source: a tourist taking a photograph of river looking west towards suspension bridge and office\nTarget: Tourist snaps photo of suspension bridge and office building across the river.\n5. Source: glass of foods and food product on a sunny day\nTarget: An assortment of food items and products displayed in a glass container, illuminated by\nbright sunshine.\n6. Source: turtles and large fish in the pond\nTarget: A tranquil pond where large fish and turtles coexist peacefully, creating a harmonious\nnatural habitat.\n7. Source: the frescoes inside the dome\nTarget: The elaborate and intricate paintings or artworks adorning the inner surface of the dome,\ntypically found in religious buildings.\n17\n8. Source: fight over a loose ball\nTarget: Intense competition as players struggle to gain control of a loose ball during the game.\n9. Source: love this winter picture by person .\nTarget: Mesmerizing winter landscape by person: serene snowy scenery with gentle snowflakes,\nskillfully framed with perfect contrast and depth.\n10. Source: one of several paths through the woods .\nTarget: A narrow forest path, one among many weaving through the lush trees, underbrush, and\ndappled sunlight.\n11. Source: ripe strawberries falling through the water .\nTarget: Juicy ripe strawberries plummeting through a stream of water, splashing and creating\nripples in the liquid.\n12. Source: a city reflected on a red sunglasses .\nTarget: The cityscape reflected on a pair of red sunglasses, creating a distorted but fascinating\nview of the urban environment.\n13. Source: man driving a car through the mountains\nTarget: A man confidently navigating a winding mountain road with breathtaking views.\n14. Source: maritime museum from the historical dock .\nTarget: A museum dedicated to seafaring history, located on a historic dock where visitors can\nview a collection of artifacts and vessels.\n15. Source: tree hollow and green leaves of a tree top in summer\nTarget: Amidst lush green leaves on the top of a tree, a hollow creates a natural shelter, typical of\nsummer foliage.\n16. Source: musician of musical group performs on stage on the first day of festival\nTarget: On the opening day of the festival, a musician from a musical group performs energetically\non stage to a lively crowd.\nBard:\n1. Source: man driving a car through the mountains\nTarget: A man drives his car through the mountains, the road winding its way through the towering\npeaks.\n2. Source: a bicycle hanging above the entrance to a store\nTarget: A bicycle suspended from a chain above the entrance to a bike shop advertises their\nservices.\n3. Source: government agency released underwater footage of the unique movements of starfish\nTarget: New footage released by government agency shows starfish swimming in unique ways,\nshedding light on their underwater behavior.\n4. Source: unique red chair among other white chairs at the stadium\nTarget: Red chair stands out among white seats at stadium, a beacon of color.\n5. Source: person looks comfortable as he connects with a free - kick during the session and is in\nline to return against hull on saturday\nTarget: The player strikes a free-kick with confidence during training, looking sharp and ready\nfor his return to the team against Hull on Saturday.\n6. Source: animal in front of a white background\nTarget: An animal stands out against the stark white background, its feathers on full display.\n7. Source: a mother and daughter lying on a lawn\nTarget: A mother and daughter embrace in the grass, enjoying the warmth of the sun and the\nsound of birdsong.\n8. Source: sign is seen outside the home\nTarget: A for sale sign stands in the front yard, a reminder that change is always on the horizon.\n9. Source: portrait of person against an abstract background stock photo\nTarget: person\u2019s face stands out against a swirling, abstract background, their expression one of\nmystery and intrigue.\n10. Source: state flag waving on an isolated white background .\nTarget: The state flag waves proudly against a stark white background. The colors of the flag are\nvibrant and the design is intricate, representing the history and culture of the state\n11. Source: actor wears a gorgeous blush pink coloured gown at festival .\nTarget: Actor stuns in blush pink gown at festival, her beauty a match for the flowers in bloom.\n12. Source: person answering the phones again at the office .\nTarget: A friendly voice answers the phone, eager to help the caller with their needs.\n18\n13. Source: little boy sitting on the grass with drone and remote controller\nTarget: A young boy\u2019s eyes light up as he launches his drone into the sky, his imagination taking\nflight with it.\n14. Source: golfer competes during day held\nTarget: Golfer focuses on the ball as she competes in a tournament on a beautiful day.\n15. Source: golden fish in a bowl\nTarget: A golden fish swims lazily in a bowl, its scales shimmering in the sunlight. The fish is a\nbeautiful shade of orange, with black spots on its fins and tail.\n16. Source: businessman with smartphone sitting on ledge by the sea\nTarget: A businessman relaxes on a seaside ledge, checking his phone and enjoying the view.\nHuman:\n1. Source: Honey buttermilk biscuits on a cooling rack being drizzled with honey\nTarget: A warm stack of freshly baked honey buttermilk biscuits, sit on a cooling rack as they are\ndrizzled with golden honey\n2. Source: happy corgi time\nTarget: Delighted corgi stands in the hallway, looking at its owner\n3. Source: <PERSON> dog looking at dirt from the ground\nTarget: <Person>\u2019s dog, lying on the ground, looks at the dirt\n4. Source: navy vintage pants - lime green bag - ivory Maison Simons t-shirt - Zara clogs\nTarget: A young beautiful lady wearing navy vintage pants and ivory Maison Simons t-shirt, is\nholding a lime green bag.\n5. Source: Ooak Barbie City Shine\nTarget: A custom-made Barbie doll with a city-inspired look shines brightly\n6. Source: Real Wedding on a NYC Rooftop\nTarget: a couple is kissing each other during their rooftop wedding in NYC\n7. Source: the proud of my beloved italian bracco after leg amputation due to a tumor.\nTarget: my italian bracco lied down proudly under the sunshile, despite of leg amputation due to\na tumor.\n8. Source: Pineapple Wearing Headphones Art Print by Philip Haynes\nTarget: An art from Philip Haynes depicts a pineapple that wears headphones\n9. Source: Ominous thunderclouds behind the Capitol Building\nTarget: Thunderclouds loom over the Capitol Building, casting a dark shadow\n10. Source: Steampunk woman with gun\nTarget: A fierce and stylish steampunk woman holds a toy revolver in her hands\n11. Source: a new watch with some old friends\nTarget: The watch sits besides a cartoon picture, evoking memories of cherished times shared\nwith long-time friends\n12. Source: Particularly important to Africa is the East African Highland Banana (EAHB), a staple\nfood for 80 million people. Uganda alone has about 120 varieties of this type of banana.\nTarget: An African man holds a bunch of bananas, which is particularly important to Africa\n13. Source: Electric Blue Guitar There Goes My Hero, Rock The Vote, <PERSON>, <PERSON>,\nMusic Photo, Red Eyes, Photo Quotes, Electric Blue, Music Lyrics\nTarget: <PERSON> is playing an electric blue guitar, eyes bloodshot from the stage lights\n14. Source: Advanced Bicycle Skills Video - Valuable Video for Safe Cycl\nTarget: A Cyclist is demonstrating advanced bicycle skills in a video that will help people stay\nsafe.\n15. Source: grilled turkey pesto sandwich\nTarget: A grilled turkey pesto sandwich with melted cheese and fresh arugula is served on a plate.\n16. Source: Actress <PERSON> during the launch of international fashion brand Forever 21 store at\na mall in Mumbai on Saturday, October 12th, 2013.\nTarget: The young beautiful actress attended the launch of fashion brand Forever 21 at a mall.\nMSCOCO:\nFor the meta-input-output sampling using the MSCOCO strategy, we utilize the fact that there are\nfive different captions associated with each image. In our approach, we randomly select two texts\nfrom the available five, with one serving as the meta-input and the other as the meta-output. Below is\na list of the captions we employ for this purpose.\n1. Caption 1 : A herd of goats walking down a road way.\nCaption 2 : Three lambs stand next to each other and look different directions.\n19\nCaption 3 : The animals standing in the clearing are 3 varieties of sheep.\nCaption 4 : Three small sheep are standing on a road.\nCaption 5 : Some animals are standing on a dirt path\n2. Caption 1 : A boy is preparing to toss a frisbie while another boy is sitting in the background in a\npark.\nCaption 2 : Several people are out in the woods on a path playing a game.\nCaption 3 : A man in a park playing a throwing game.\nCaption 4 : A group of people that are hanging out together.\nCaption 5 : A boy gets ready to throw a frisbee\n3. Caption 1 : A pizza sitting on top of a metal pan.\nCaption 2 : The large pepperoni pizza is covered with chives.\nCaption 3 : A pizza that is sitting on a tray.\nCaption 4 : A large pizza with toppings sitting on a tray.\nCaption 5 : a pizza with fresh basil tomato sauce and cheese baked\n4. Caption 1 : A woman sits on top of a motorcycle in a parade.\nCaption 2 : Woman wearing starts on helmet and shorts rides motorcycle\nCaption 3 : A woman wearing attire that matches her motorcycle is driving on.\nCaption 4 : A person that is on top of a motorcycle.\nCaption 5 : Woman on a motorcycle rides in a parade\n5. Caption 1 : the people are sampling wine at a wine tasting.\nCaption 2 : Group of people tasting wine next to some barrels.\nCaption 3 : People are gathered around a man tasting wine.\nCaption 4 : A man pouring wine from casks for patrons\nCaption 5 : People gather around a table while sampling wine.\n6. Caption 1 : A herd of sheep walking down a street in front of a bus.\nCaption 2 : There are three animals walking down the road.\nCaption 3 : a van is stuck behind a few traveling goats\nCaption 4 : a van that has some kind of animal out front of it\nCaption 5 : A herd of animals walking down the road behind a truck.\n7. Caption 1 : A sandwich with meat and cheese sits on a plate with a small salad.\nCaption 2 : A sandwich with cheese and a bowl with a salad.\nCaption 3 : Two plates with sandwiches on them next to a bowl of vegetables.\nCaption 4 : A long sandwich and a salad is on a plate.\nCaption 5 : a sandwich and a bowl of vegetables on a plate\n8. Caption 1 : A NASA airplane carrying a space shuttle on its back.\nCaption 2 : A large plan with a smaller plan on top of it.\nCaption 3 : A NASA airplane carrying the old Space Shuttle\nCaption 4 : A NASA airplane glides through the sky while carrying a shuttle.\nCaption 5 : This jet is carrying a space shuttle on it\n9. Caption 1 : A one way sign under a blue street sign.\nCaption 2 : a view from below of a one way sign\nCaption 3 : A street sign stating that the road is one way beneath a blue sky.\nCaption 4 : A \"One Way\" street sign pointing to the right.\nCaption 5 : A one way road sign mounted above a street sign.\n10. Caption 1 : A bowl of food containing broccoli and tomatoes.\nCaption 2 : A large salad is displayed in a silver metal bowl.\nCaption 3 : A bowl of food with tomatoes, sliced apples, and other greens\nCaption 4 : A silver bowl filled with various produce discards.\nCaption 5 : The salad in the bowl contains many fresh fruits and vegetables.\n11. Caption 1 : a cake made to look like it has candy decorations on it\nCaption 2 : A photograph of a highly decorated cake on a table.\nCaption 3 : A cake decorated with lollipops and a piece of pie.\nCaption 4 : A piece of cake with lolypops, pie and caterpillar designs.\nCaption 5 : A layered cake with sweet treats and a caterpillar as decorations.\n12. Caption 1 : A young man riding a skateboard on a cement walkway.\nCaption 2 : a guy riding a skateboard by a car\nCaption 3 : A young man on a skateboard near a car\nCaption 4 : an image of a boy on a skateboard doing tricks\nCaption 5 : A young man is riding on his skateboard.\n20\nTable A6: Performance comparison of LaCLIP trained with different meta-input-output strategies on CC12M.\n(a) Zero-shot and Linear-probing Experiment Results\nSource\nFood-101\nCIFAR-10\nCIFAR-100\nSUN397\nCars\nAircraft\nDTD\nPets\nCaltech-101\nFlowers\nSTL-10\nEuroSAT\nRESISC45\nGTSRB\nCountry211\nAverage\nImageNet\nZero-shot\nChatGPT\n57.0 71.1 38.9 51.2 31.6 3.9 25.5 63.0 80.8 36.9 92.9 24.5 39.6 10.1 6.9 42.3\n44.5\nBard\n55.2 70.1 39.4 51.7 31.5 4.6 25.2 63.3 80.6 34.5 92.5 20.7 39.6 10.1 7.2 41.7\n44.8\nMSCOCO\n54.9 66.3 39.1 52.6 29.0 4.2 24.9 67.7 79.3 33.1 93.8 27.8 38.2 13.2 7.1 42.1\n44.6\nHuman\n56.4 69.1 39.1 51.7 31.4 3.8 22.9 68.1 80.6 38.4 94.3 26.9 43.0 11.7 7.5 43.0\n45.1\nLinear-Probing\nChatGPT\n81.5 94.0 79.4 73.0 77.2 54.7 75.1 87.1 92.2 96.0 97.3 96.6 92.3 81.0 19.9 79.8\n71.2\nBard\n82.0 93.7 79.4 72.7 77.6 53.8 74.4 86.3 92.0 95.7 97.1 96.2 92.5 81.7 19.6 79.6\n71.2\nMSCOCO\n81.9 94.1 79.2 73.3 76.0 53.4 75.4 86.8 92.8 95.9 97.6 96.5 92.7 82.5 19.4 79.8\n71.3\nHuman\n82.3 94.2 79.4 73.3 76.2 55.1 75.6 87.0 92.0 96.3 97.5 96.2 92.8 81.3 19.8 79.9\n71.3\n(b) Few-shot Experiment Results\nSource\nFood-101\nCIFAR-10\nCIFAR-100\nSUN397\nCars\nAircraft\nDTD\nPets\nCaltech-101\nFlowers\nSTL-10\nEuroSAT\nRESISC45\nGTSRB\nCountry211\nChatGPT\n88.8\u00b10.578.4\u00b10.683.3\u00b10.697.7\u00b10.293.4\u00b10.466.5\u00b11.084.4\u00b10.692.5\u00b10.498.6\u00b10.298.0\u00b10.294.3\u00b10.384.0\u00b10.592.3\u00b10.473.7\u00b10.845.6\u00b10.7\nBard\n89.2\u00b10.580.1\u00b10.683.4\u00b10.697.7\u00b10.293.3\u00b10.466.3\u00b11.084.3\u00b10.693.2\u00b10.498.6\u00b10.298.1\u00b10.294.9\u00b10.383.2\u00b10.592.2\u00b10.474.2\u00b10.845.6\u00b10.7\nMSCOCO 88.6\u00b10.579.5\u00b10.682.7\u00b10.697.8\u00b10.293.7\u00b10.465.1\u00b11.084.4\u00b10.692.5\u00b10.498.7\u00b10.298.1\u00b10.295.0\u00b10.384.9\u00b10.591.6\u00b10.474.3\u00b10.844.9\u00b10.7\nHuman\n88.6\u00b10.578.4\u00b10.683.2\u00b10.697.7\u00b10.293.7\u00b10.466.1\u00b11.084.7\u00b10.693.0\u00b10.498.6\u00b10.298.2\u00b10.294.4\u00b10.383.5\u00b10.592.2\u00b10.474.1\u00b10.845.7\u00b10.7\n13. Caption 1 : A small brown dog sitting on display behind a window.\nCaption 2 : A small fuzzy dog stares longingly out a window.\nCaption 3 : The dog is brown shaggy with a red collar.\nCaption 4 : A dog sits alone and stares out of a window.\nCaption 5 : A furry and cute dog sitting in a window looking outside.\n14. Caption 1 : A herd of sheep standing on a lush green hillside.\nCaption 2 : Several animals standing on the side of a hill.\nCaption 3 : A number of sheep eat on a steep grassy hill.\nCaption 4 : a couple of sheep are standing in some grass\nCaption 5 : The side of a small hill of grass with several sheep grazing in the grass and houses in\nthe background on the upper hill.\n15. Caption 1 : The tennis player on the blue court has his racquet raised.\nCaption 2 : A man swinging a tennis racket at a pro tennis match.\nCaption 3 : A tennis player wearing a NIKE shirt swings his racket\nCaption 4 : Man posing in front of the camera holding up a tennis racket.\nCaption 5 : A man wearing a white shirt playing tennis.\n16. Caption 1 : A surfer riding a wave in a tempestuous ocean\nCaption 2 : Man in body suit surfing on a large wave.\nCaption 3 : A surfer is sideways on a wave of water on a surfboard.\nCaption 4 : The surfer is riding sideways along a wave.\nCaption 5 : a surfer wearing a wet suit is surfing on a white board\nC.2 Detailed Experiment Results on Meta-Input-Output\nWe present a detailed analysis of the experiment results comparing different meta-input-output\nstrategies. Specifically, for each of the four meta-input-output strategy (ChatGPT, Bard, Human,\nMSCOCO), we use this specific strategy as example candidates for LLaMA ICL, and generate a rewrite\nfor every text in CC12M. Then we train four LaCLIP models, each model trained with the original\ncaptions and the rewrite version of one specific meta-input-output strategy. The comprehensive results\nof these experiments are summarized in Table A6. The results indicate that different meta-input-output\nstrategy achieves similar performance.\n21\nD. Augmentation Strategy Details\nTo help understand the effect of our proposed language rewriting strategy by LLaMA ICL, here we\ncompare our proposed strategy with two widely used language augmentation baselines: EDA [58]\nand back translation [50].\n\u2022 EDA contains four types of different randomly performed augmentation operations: Synonym\nReplacement, Random Insertion, Random Swap, and Random Deletion. We used the official\nimplementation and kept all the default parameters as used in [58].\n\u2022 Back Translation first translates the text to another language and then translate it back to English to\ngenerate slightly different version of the text. We chose four different languages for our experiments:\nSpanish, French, German and Italic languages.\nD.1 Augmentation Qualitative Comparison\nWe begin by presenting qualitative comparisons of different text augmentation strategies. It is\nobserved that the EDA and back translation approaches primarily focus on word-level modifications,\noften preserving the sentence structures and leading to limited diversity in the rewritten texts. In\ncontrast, our LLM-based augmentation strategy demonstrates the ability to substantially alter the\nsentence structure while maintaining the original key concepts and meaning intact. This results in\nmore effective, diverse, and enriched rewritten texts, highlighting the superiority of our approach\nover word-based methods.\n\u2022 Original: Handmade mirror in a wooden toned frame texture cracked paint with reflection green\napple on the table. Handmade mirror in a wooden toned frame texture cracked stock photos\nEDA Aug 1: handmade mirror in a wooden toned frame texture cracked on with reflection green\napple paint the table handmade mirror in a wooden toned frame texture cracked stock photos\nEDA Aug 2: handmade mirror in a wooden toned frame texture crock up cracked paint with\nreflection green apple angstrom unit on the table handmade mirror in a wooden toned frame texture\ncracked stock photos\nBack Translation Spanish: Hand-made mirror in the toned wood frame cracked texture paint with\ngreen apple reflection on the table. Hand-made mirror in the toned wood frame cracked texture\nstock photos\nBack Translation French: Hand-made mirror in a wood toned frame texture cracked paint with\ngreen apple reflection on the table. Hand-made mirror in a wood toned frame texture cracked stock\nphotos\nBack Translation German: Handmade mirror in a wooden toned frame texture cracked color with\nreflection green apple on the table. Handmade mirror in a wooden toned frame texture cracked\nstock photos\nLLM ChatGPT: Mirror on wooden painted frame, over cracked stone, with green apple on the\ntable. Handmade mirror in a wooden toned frame texture cracked stock photo\nLLM Bard: Green apple, still life, studio photography, abstract background\nLLM MSCOCO: a mirror on a table\nLLM Human: A closeup on a cracked mirror that reflects an apple on the table. The frame is\ncovered in a golden toned wood grain texture.\n\u2022 Original: Traffic jam on the road, a lot of cars which go towards each other and to the different\ndirections\nEDA Aug 1: traffic jam on the road a lot of cars which go towards each other and to the different\nfocus\nEDA Aug 2: traffic jam on the road a lot of cars which go towards each other and dissimilar to the\ndifferent directions\nBack Translation Spanish: Traffic jam on the road, a lot of cars going towards the other and to\nthe different directions\nBack Translation French: Traffic jams on the road, lots of cars going towards each other and\ntowards different directions\nBack Translation German: Traffic jam on the road, a lot of cars that go to each other and to the\ndifferent directions\nLLM ChatGPT: Traffic jam on the road, with a lot of cars moving in different directions, as well\nas towards each other.\nLLM Bard: A lot of cars line the street, all heading toward the same intersection.\n22\nLLM MSCOCO: Traffic jam on the road with cars going in different directions, some cars are on\nthe same lane, others are on different lanes.\nLLM Human: A traffic jam on the road. There are a lot of cars and many of them are going in\ndifferent directions.\n\u2022 Original: Green young small newborn tree growing on concrete road among dry fallen leaves\ngiven concept of the life beginning or strong surviving\nEDA Aug 1: green young small newborn tree growing on concrete road among dry fallen leaves\ngiven concept of the life beginning or strong surviving\nEDA Aug 2: green young small new sprung tree growing on concrete road among dry fallen leaves\ngiven conception of the life beginning or strong surviving\nBack Translation Spanish: Young green small newborn tree growing on the concrete road between\nfallen dry leaves given the concept of the beginning of life or strong survival\nBack Translation French: Young young-born green tree growing on the concrete road among the\ndried leaves fallen according to the concept of the beginning of life or strong survivor\nBack Translation German: Green young little newborn tree growing on concrete road under dry\nfallen leaves given concept of life beginning or strong survival\nLLM ChatGPT: Bold forest green, from endless tangle of vines, create a strong vertical accent\non the concrete road. On the right side, two brown fallen leaves, littering the road, sit quietly and\ngracefully, almost as though they are praying.\nLLM Bard: Green young small newborn tree growing on concrete road among dry fallen leaves\ngiving concept of the life beginning or strong surviving\nLLM MSCOCO: A bare tree trunk surrounded by fallen leaves.\nLLM Human: a young and tall green tree on a concrete road among dry fallen leaves, its leaves\non the floor. Concept is a strong surviving in harsh environment\n\u2022 Original: Caused by the european solid wood floor lamp lamp creative decorative floor lamp floor\nlamp minimalist home japanese wood living room bedroom lights\nEDA Aug 1: room by the european solid wood floor lamp lamp creative decorative floor lamp floor\nlamp minimalist home japanese wood bedroom caused living lights\nEDA Aug 2: caused house by the european solid wood floor lamp lamp creative decorative floor\nlamp floor lamp minimalist home japanese wood living have room bedroom lights\nBack Translation Spanish: Caused by European solid wood floor lamp European decorative floor\nlamp creative minimalist floor lamp Japanese home wood living room bedroom lights\nBack Translation French: Cause of the European floor lamp in solid wood decorative floor lamp\ncreative floor lamp minimalist floor lamp Japanese house living room light room bedroom\nBack Translation German: Conditioned by the European solid wood floor lamp lamp creative\ndecorative floor lamp floor lamp minimalist house Japanese wood living room bedroom lights\nLLM ChatGPT: Due to the european style lighting system, the decorative floor lamp will enhance\nyour living room decor.\nLLM Bard: Because of the european solid wood floor lamp, creative floor lamp, decorative floor\nlamp, floor lamp, minimalist home, japanese wood living room, bedroom lights.\nLLM MSCOCO: A furniture store with lamps on the shelves. One lamp is lit.\nLLM Human: The european solid wood floor lamp creative decorative floor lamp floor lamp\nminimalist home japanese wood living room bedroom lights\n\u2022 Original: A man, likely a railroad employee, stands in the forefront of the station by the tracks.\nIn the background, groups of people lean against the building, perhaps waiting for a train. In the\nforeground is the switch tower.\nEDA Aug 1: a man likely a railroad employee stands in the forefront of the station by the tracks in\nthe background groups of inch people lean against the building perhaps waiting for a hulk train in\nthe foreground is the transposition switch tower\nEDA Aug 2: a military personnel likely a railroad employee stands in the forefront of the station by\nthe tracks in the background groups of people lean against the building perhaps ready and waiting\nfor a train in the foreground is the throw tower\nBack Translation Spanish: A man, probably a railroad employee, is at the forefront of the station\nby the tracks. Deep down, groups of people lean on the building, perhaps waiting for a train. In the\nforeground is the switch tower.\nBack Translation French: A man, probably a railway employee, stands at the vanguard of the\nstation by the tracks. In the background, groups of people lean against the building, perhaps waiting\nfor a train.\nBack Translation German: A man, probably a railway worker, is standing at the top of the station\n23\nTable A7: Performance comparison of LaCLIP trained with different text augmentation strategies on CC12M.\n(a) Zero-shot and Linear-probing Experiment Results\nAugmentation\nFood-101\nCIFAR-10\nCIFAR-100\nSUN397\nCars\nAircraft\nDTD\nPets\nCaltech-101\nFlowers\nSTL-10\nEuroSAT\nRESISC45\nGTSRB\nCountry211\nAverage\nImageNet\nZero-shot\nN/A (CLIP)\n50.8 64.9 38.5 44.7 24.1 2.4 19.4 64.1 77.4 33.2 91.0 20.1 38.9 7.3\n5.1 38.8\n40.2\nEDA [58]\n51.9 67.6 36.5 48.2 27.7 2.8 25.4 64.7 78.2 33.3 92.8 21.9 40.0 10.8 6.6 40.6\n41.2\nBack Translation [50]\n49.3 71.0 36.7 47.9 27.8 3.7 25.7 63.9 77.4 32.0 90.6 22.0 41.3 10.7 6.1 40.4\n41.6\nLLM (Ours)\n60.7 75.1 43.9 57.0 36.3 5.6 31.0 72.4 83.3 39.9 95.1 27.3 44.3 12.7 8.9 46.2\n48.4\nLinear-Probing\nN/A (CLIP)\n81.6 93.8 79.3 72.0 75.1 52.6 75.6 86.2 92.2 95.3 97.3 96.7 93.1 80.6 19.7 79.4\n70.3\nEDA [58]\n81.6 94.0 78.2 72.9 76.2 53.7 74.8 85.6 92.2 95.5 97.2 96.8 92.9 79.9 20.1 79.4\n70.5\nBack Translation [50]\n81.8 94.2 78.2 73.0 77.5 54.6 75.5 87.1 91.6 96.0 97.5 97.1 93.1 80.0 20.0 79.8\n70.7\nLLM (Ours)\n82.9 94.7 79.7 73.8 79.9 54.5 75.7 87.7 93.0 96.4 98.0 96.4 93.0 81.9 19.7 80.5\n72.3\n(b) Few-shot Experiment Results\nAugmentation\nFood-101\nCIFAR-10\nCIFAR-100\nSUN397\nCars\nAircraft\nDTD\nPets\nCaltech-101\nFlowers\nSTL-10\nEuroSAT\nRESISC45\nGTSRB\nCountry211\nN/A (CLIP)\n87.0\u00b10.577.5\u00b10.682.1\u00b10.797.2\u00b10.290.9\u00b10.562.0\u00b11.083.3\u00b10.691.1\u00b10.598.2\u00b10.297.6\u00b10.292.6\u00b10.483.4\u00b10.591.2\u00b10.470.6\u00b10.844.3\u00b10.7\nEDA [58]\n88.1\u00b10.576.1\u00b10.681.3\u00b10.797.6\u00b10.291.7\u00b10.562.9\u00b11.083.4\u00b10.691.9\u00b10.598.4\u00b10.297.8\u00b10.293.5\u00b10.384.3\u00b10.591.6\u00b10.468.4\u00b10.844.6\u00b10.7\nBack Trans [50] 88.1\u00b10.576.9\u00b10.682.5\u00b10.797.5\u00b10.291.8\u00b10.465.1\u00b11.083.7\u00b10.692.5\u00b10.498.3\u00b10.297.9\u00b10.294.2\u00b10.383.3\u00b10.591.1\u00b10.470.8\u00b10.845.1\u00b10.7\nLLM (Ours)\n89.9\u00b10.581.3\u00b10.585.0\u00b10.698.0\u00b10.295.3\u00b10.368.1\u00b11.084.9\u00b10.693.4\u00b10.498.9\u00b10.298.4\u00b10.295.9\u00b10.283.0\u00b10.592.4\u00b10.476.4\u00b10.846.7\u00b10.7\non the tracks. In the background, groups of people are leaning against the building, perhaps waiting\nfor a train.\nLLM ChatGPT: A man, likely a railroad employee, stands by the tracks in front of the station. In\nthe background, groups of people lean against the building. In the foreground is the switch tower.\nLLM Bard: man leaning against the rail station and switch tower in a dark night with a fog\nLLM MSCOCO: A portrait of the man in the front of the station is seen in the upper left. In the\nlower right is a man leaning on a post with his arms crossed.\nLLM Human: An image of a man, likely a railroad employee, standing in the foreground of a train\nstation by the tracks. In the background are groups of people, some leaning against the building,\nwhich could be waiting for a train. In the foreground are the tracks with a switch tower in the\ndistance.\nD.2 Detailed Experiment Results on Augmentation Strategy\nWe conducted a quantitative comparison of different augmentation strategies while ensuring a fair\nevaluation by generating a consistent number of augmented texts per original sentence (i.e., 4).\nFor the EDA strategy, we created 4 distinct versions of each sentence by randomly applying their\npredefined augmentation operations. As for the back translation approach, we translated the original\ntexts into four different languages (Spanish, French, German, and Italic languages) and then back to\nEnglish, resulting in 4 rewritten versions of the original texts. In our LLM-based augmentation, we\nused LLaMA ICL to generate 4 augmentations prompted by the 4 predefined meta-input-output pairs\n(ChatGPT, Bard, Human, and MSCOCO).\nA comprehensive comparison of these strategies is presented in Table A7. The results demonstrate\nthat while the baseline augmentation strategies improve the performance of the vanilla CLIP baseline,\nour proposed LLM-based augmentation strategy consistently achieves superior results across various\ndatasets and evaluation metrics, outperforming the other augmentation methods significantly.\n24\nTable A8: Performance comparison of CLIP and LaCLIP trained with different text augmentation strategies\nwith different number of augmentations per original text on CC12M.\n(a) Zero-shot and Linear-probing Experiment Results\nAugment / Num\nFood-101\nCIFAR-10\nCIFAR-100\nSUN397\nCars\nAircraft\nDTD\nPets\nCaltech-101\nFlowers\nSTL-10\nEuroSAT\nRESISC45\nGTSRB\nCountry211\nAverage\nImageNet\nZero-shot\nN/A (CLIP) / 0\n50.8 64.9 38.5 44.7 24.1 2.4 19.4 64.1 77.4 33.2 91.0 20.1 38.9 7.3\n5.1 38.8\n40.2\nEDA / 1\n52.2 66.2 34.3 46.6 25.6 3.6 22.2 64.5 79.3 33.5 90.9 24.1 37.6 13.4 5.6 40.0\n40.2\nEDA / 2\n49.8 62.4 32.1 47.1 28.1 2.2 25.3 64.6 79.1 31.4 92.3 12.6 38.0 13.1 5.7 38.9\n41.1\nEDA / 3\n50.4 62.8 35.4 49.7 26.8 2.5 24.5 69.5 77.4 33.1 92.8 24.9 37.3 15.1 6.7 40.6\n41.7\nEDA / 4\n51.9 67.6 36.5 48.2 27.7 2.8 25.4 64.7 78.2 33.3 92.8 21.9 40.0 10.8 6.6 40.6\n41.2\nBack Trans / 1\n49.7 61.5 34.6 45.5 26.7 4.0 20.7 59.2 77.2 32.1 88.2 27.1 40.0 12.6 5.8 39.0\n40.1\nBack Trans / 2\n50.0 55.4 35.5 44.3 29.0 5.2 21.0 67.4 78.5 32.6 89.4 19.6 38.4 7.6\n6.2 38.7\n41.0\nBack Trans / 3\n49.9 67.3 37.6 46.9 26.7 4.1 22.8 65.7 76.8 34.3 91.7 20.0 34.3 12.5 6.3 39.8\n41.5\nBack Trans / 4\n49.3 71.0 36.7 47.9 27.8 3.7 25.7 63.9 77.4 32.0 90.6 22.0 41.3 10.7 6.1 40.4\n41.6\nLLM (Ours) / 1\n57.0 71.1 38.9 51.2 31.6 3.9 25.5 63.0 80.8 36.9 92.9 24.5 39.6 10.1 6.9 42.3\n44.5\nLLM (Ours) / 2\n57.0 70.3 41.3 54.2 34.2 5.8 29.0 64.0 79.5 38.5 94.4 33.0 38.6 9.1\n8.2 43.8\n46.5\nLLM (Ours) / 3\n59.7 75.0 42.6 56.5 34.0 5.1 29.4 65.8 81.3 38.2 94.7 18.7 42.4 13.4 8.7 44.4\n47.7\nLLM (Ours) / 4\n60.7 75.1 43.9 57.0 36.3 5.6 31.0 72.4 83.3 39.9 95.1 27.3 44.3 12.7 8.9 46.2\n48.4\nLinear-Probing\nN/A (CLIP) /0\n81.6 93.8 79.3 72.0 75.1 52.6 75.6 86.2 92.2 95.3 97.3 96.7 93.1 80.6 19.7 79.4\n70.3\nEDA / 1\n81.5 93.3 78.0 72.1 75.6 53.1 76.5 85.9 91.5 95.8 97.3 96.4 92.6 80.0 19.9 79.3\n70.4\nEDA / 2\n81.4 94.1 80.2 72.5 76.7 52.9 75.7 85.8 92.1 95.7 97.2 96.7 92.7 81.6 19.9 79.7\n70.6\nEDA / 3\n81.3 93.6 78.8 72.3 74.5 53.3 75.1 86.0 91.1 95.6 97.3 96.7 93.0 79.1 19.7 79.2\n70.6\nEDA / 4\n81.6 94.0 78.2 72.9 76.2 53.7 74.8 85.6 92.2 95.5 97.2 96.8 92.9 79.9 20.1 79.4\n70.5\nBack Trans / 1\n81.5 93.4 78.3 72.4 76.9 52.5 74.8 85.7 92.0 95.5 97.4 96.9 93.2 81.6 19.8 79.5\n70.5\nBack Trans / 2\n81.5 93.9 78.5 72.4 76.3 52.8 74.5 86.2 91.7 95.5 97.5 96.8 92.4 80.5 19.4 79.3\n70.5\nBack Trans / 3\n81.6 93.5 78.0 72.4 75.9 52.1 73.8 86.2 92.1 95.1 97.3 96.5 92.3 79.4 19.9 79.1\n70.5\nBack Trans / 4\n81.8 94.2 78.2 73.0 77.5 54.6 75.5 87.1 91.6 96.0 97.5 97.1 93.1 80.0 20.0 79.8\n70.7\nLLM (Ours) / 1\n81.8 94.3 79.7 73.3 77.5 55.0 75.4 87.4 92.5 96.3 97.6 96.9 92.6 81.3 20.2 80.1\n71.2\nLLM (Ours) / 2\n82.3 94.0 79.1 73.3 77.6 52.7 76.0 86.8 91.8 96.1 97.7 96.6 93.1 83.3 20.1 80.0\n71.7\nLLM (Ours) / 3\n82.3 94.7 80.0 73.7 79.2 56.0 75.7 87.0 92.9 96.2 98.0 96.6 92.9 83.1 20.0 80.6\n71.9\nLLM (Ours) / 4\n82.9 94.7 79.7 73.8 79.9 54.5 75.7 87.7 93.0 96.4 98.0 96.4 93.0 81.9 19.7 80.5\n72.3\n(b) Few-shot Experiment Results\nAugment / Num\nFood-101\nCIFAR-10\nCIFAR-100\nSUN397\nCars\nAircraft\nDTD\nPets\nCaltech-101\nFlowers\nSTL-10\nEuroSAT\nRESISC45\nGTSRB\nCountry211\nN/A (CLIP) / 0\n87.0\u00b10.577.5\u00b10.682.1\u00b10.797.2\u00b10.290.9\u00b10.562.0\u00b11.083.3\u00b10.691.1\u00b10.598.2\u00b10.297.6\u00b10.292.6\u00b10.483.4\u00b10.591.2\u00b10.470.6\u00b10.844.3\u00b10.7\nEDA / 1\n87.6\u00b10.575.4\u00b10.681.3\u00b10.797.4\u00b10.291.3\u00b10.562.6\u00b11.083.5\u00b10.691.5\u00b10.598.2\u00b10.297.8\u00b10.293.0\u00b10.383.2\u00b10.591.4\u00b10.468.9\u00b10.844.4\u00b10.7\nEDA / 2\n87.9\u00b10.577.3\u00b10.682.0\u00b10.697.4\u00b10.291.9\u00b10.462.8\u00b11.083.5\u00b10.692.1\u00b10.498.4\u00b10.297.8\u00b10.293.6\u00b10.382.8\u00b10.691.6\u00b10.470.0\u00b10.844.8\u00b10.7\nEDA / 3\n87.5\u00b10.576.5\u00b10.682.0\u00b10.797.6\u00b10.291.2\u00b10.562.7\u00b11.083.8\u00b10.691.3\u00b10.598.2\u00b10.297.7\u00b10.294.2\u00b10.384.0\u00b10.591.4\u00b10.472.0\u00b10.844.3\u00b10.7\nEDA / 4\n88.1\u00b10.576.1\u00b10.681.3\u00b10.797.6\u00b10.291.7\u00b10.562.9\u00b11.083.4\u00b10.691.9\u00b10.598.4\u00b10.297.8\u00b10.293.5\u00b10.384.3\u00b10.591.6\u00b10.468.4\u00b10.844.6\u00b10.7\nBack Trans / 1\n87.8\u00b10.576.4\u00b10.681.8\u00b10.797.4\u00b10.291.7\u00b10.563.4\u00b11.083.8\u00b10.691.7\u00b10.598.3\u00b10.297.7\u00b10.293.1\u00b10.383.9\u00b10.591.6\u00b10.470.1\u00b10.844.7\u00b10.8\nBack Trans / 2\n87.8\u00b10.575.6\u00b10.681.6\u00b10.797.5\u00b10.292.3\u00b10.462.8\u00b11.083.7\u00b10.692.5\u00b10.498.3\u00b10.297.8\u00b10.293.6\u00b10.383.8\u00b10.591.1\u00b10.468.8\u00b10.844.7\u00b10.7\nBack Trans / 3\n88.2\u00b10.577.0\u00b10.682.8\u00b10.697.4\u00b10.291.7\u00b10.462.6\u00b11.083.8\u00b10.691.6\u00b10.598.3\u00b10.297.7\u00b10.293.3\u00b10.383.1\u00b10.591.8\u00b10.471.0\u00b10.845.0\u00b10.7\nBack Trans / 4\n88.1\u00b10.576.9\u00b10.682.5\u00b10.797.5\u00b10.291.8\u00b10.465.1\u00b11.083.7\u00b10.692.5\u00b10.498.3\u00b10.297.9\u00b10.294.2\u00b10.383.3\u00b10.591.1\u00b10.470.8\u00b10.845.1\u00b10.7\nLLM (Ours) / 1\n88.8\u00b10.578.4\u00b10.683.3\u00b10.697.7\u00b10.293.4\u00b10.466.5\u00b11.084.4\u00b10.692.5\u00b10.498.6\u00b10.298.0\u00b10.294.3\u00b10.384.0\u00b10.592.3\u00b10.473.7\u00b10.845.6\u00b10.7\nLLM (Ours) / 2\n89.2\u00b10.579.1\u00b10.683.6\u00b10.697.9\u00b10.294.2\u00b10.465.6\u00b11.084.2\u00b10.693.2\u00b10.498.8\u00b10.298.2\u00b10.295.3\u00b10.383.6\u00b10.591.7\u00b10.475.6\u00b10.846.1\u00b10.7\nLLM (Ours) / 3\n89.8\u00b10.582.5\u00b10.584.2\u00b10.698.0\u00b10.294.4\u00b10.468.5\u00b11.085.0\u00b10.693.4\u00b10.498.7\u00b10.298.4\u00b10.295.9\u00b10.283.9\u00b10.591.6\u00b10.475.1\u00b10.846.9\u00b10.7\nLLM (Ours) / 4\n89.9\u00b10.581.3\u00b10.585.0\u00b10.698.0\u00b10.295.3\u00b10.368.1\u00b11.084.9\u00b10.693.4\u00b10.498.9\u00b10.298.4\u00b10.295.9\u00b10.283.0\u00b10.592.4\u00b10.476.4\u00b10.846.7\u00b10.7\n25\n(a) CIFAR-10.\nVanilla CLIP\nEDA\nBack Translation\nLaCLIP\n(b) The first 10 classes on Food101.\nVanilla CLIP\nEDA\nBack Translation\nLaCLIP\n(c) STL-10.\nVanilla CLIP\nEDA\nBack Translation\nLaCLIP\n(d) EuroSAT.\nVanilla CLIP\nEDA\nBack Translation\nLaCLIP\nFigure A1: t-SNE visualization of image features learned from Vanilla CLIP, two baseline text augmentations\nstrategies (EDA and back translation), and our proposed LaCLIP on CIFAR-10, Food101, STL-10, and EuroSAT\ndatasets. Image features learned from our proposed LaCLIP have a clearer class boundaries and cluster centroids.\nE. Number of Augmentations per Original Text\nWe conducted experiments to investigate how the performance varies with the number of augmenta-\ntions generated for each text and the differences between augmentation strategies as the number of\naugmentations per original text increases. We examined the performance of each strategy with 0 to 4\naugmentations per original text, where 0 corresponds to vanilla CLIP without any text augmentation.\nSpecifically, for each specific number of augmentations k: For EDA, we selected k versions out\nof the 4 generated versions. In the case of back translation, we used Spanish, Spanish+French,\nSpanish+French+German, and Spanish+French+German+Italic languages for k = 1, 2, 3, 4, re-\nspectively. Regarding our LLM-based augmentation, we used ChatGPT, ChatGPT+Bard, Chat-\nGPT+Bard+MSCOCO, and ChatGPT+Bard+MSCOCO+Human as augmentations corresponding to\nk = 1, 2, 3, 4, respectively.\nThe detailed comparison can be found in Table A8. From the results, we observe that the performance\nof the baseline augmentation strategies does not scale well with the number of augmentations per\nsentence, indicating limited diversity in the rewritten texts. This aligns with the findings in [58],\nwhere the best results are obtained with four different augmentations. In contrast, LaCLIP trained\n26\nTable A9: Performance comparison of CLIP, LaCLIP and LaCLIP-MT trained on CC12M and RedCaps.\n(a) Zero-shot and Linear-probing Experiment Results\nData\nModel\nFood-101\nCIFAR-10\nCIFAR-100\nSUN397\nCars\nAircraft\nDTD\nPets\nCaltech-101\nFlowers\nSTL-10\nEuroSAT\nRESISC45\nGTSRB\nCountry211\nAverage\nImageNet\nZero-shot\nCC12M\nCLIP\n50.8 64.9 38.5 44.7 24.1 2.4 19.4 64.1 77.4 33.2 91.0 20.1 38.9 7.3\n5.1 38.8\n40.2\nLaCLIP\n60.7 75.1 43.9 57.0 36.3 5.6 31.0 72.4 83.3 39.9 95.1 27.3 44.3 12.7 8.9 46.2\n48.4\nLaCLIP-MT\n59.2 69.5 39.0 56.8 34.4 5.5 30.7 72.8 83.1 42.5 95.2 24.8 43.4 13.1 8.3 45.2\n49.0\nRedCaps\nCLIP\n81.5 70.4 39.9 33.2 19.2 1.9 19.7 82.7 72.8 53.9 92.8 23.3 33.6 8.3\n6.2 42.6\n42.9\nLaCLIP\n85.0 74.8 40.7 40.3 21.3 2.2 23.9 78.2 76.4 59.0 91.4 27.1 41.3 5.6\n7.6 45.0\n46.2\nLaCLIP-MT\n84.2 74.9 43.1 40.5 23.0 1.9 24.0 84.7 77.1 60.9 91.0 31.9 40.3 6.1\n7.9 46.1\n48.1\nLinear-Probing\nCC12M\nCLIP\n81.6 93.8 79.3 72.0 75.1 52.6 75.6 86.2 92.2 95.3 97.3 96.7 93.1 80.6 19.7 79.4\n70.3\nLaCLIP\n82.9 94.7 79.7 73.8 79.9 54.5 75.7 87.7 93.0 96.4 98.0 96.4 93.0 81.9 19.7 80.5\n72.3\nLaCLIP-MT\n82.9 94.5 79.7 73.7 79.4 55.0 76.0 87.9 93.0 96.4 97.6 96.2 93.1 82.7 20.2 80.6\n72.4\nRedCaps\nCLIP\n89.1 94.1 78.8 65.6 74.0 52.5 73.2 91.5 91.4 97.7 98.0 96.3 93.5 80.8 17.0 79.6\n71.8\nLaCLIP\n90.1 94.3 78.5 66.6 77.6 53.6 73.9 90.8 91.5 97.9 97.6 96.6 92.7 80.8 17.2 80.0\n71.9\nLaCLIP-MT\n90.2 94.0 79.0 67.3 79.2 53.2 75.3 91.7 91.0 98.3 98.1 96.9 93.0 80.6 17.2 80.3\n72.4\n(b) Few-shot Experiment Results\nModel\nFood-101\nCIFAR-10\nCIFAR-100\nSUN397\nCars\nAircraft\nDTD\nPets\nCaltech-101\nFlowers\nSTL-10\nEuroSAT\nRESISC45\nGTSRB\nCountry211\nPre-trained on CC12M\nCLIP\n87.0\u00b10.577.5\u00b10.682.1\u00b10.797.2\u00b10.290.9\u00b10.562.0\u00b11.083.3\u00b10.691.1\u00b10.598.2\u00b10.297.6\u00b10.292.6\u00b10.483.4\u00b10.591.2\u00b10.470.6\u00b10.844.3\u00b10.7\nLaCLIP\n89.9\u00b10.581.3\u00b10.585.0\u00b10.698.0\u00b10.295.3\u00b10.368.1\u00b11.084.9\u00b10.693.4\u00b10.498.9\u00b10.298.4\u00b10.295.9\u00b10.283.0\u00b10.592.4\u00b10.476.4\u00b10.846.7\u00b10.7\nLaCLIP-MT 89.5\u00b10.580.1\u00b10.584.4\u00b10.698.0\u00b10.294.8\u00b10.469.6\u00b11.084.6\u00b10.693.7\u00b10.498.8\u00b10.298.4\u00b10.296.0\u00b10.283.8\u00b10.592.0\u00b10.476.8\u00b10.746.4\u00b10.7\nPre-trained on RedCaps\nCLIP\n94.4\u00b10.380.6\u00b10.585.3\u00b10.695.9\u00b10.388.5\u00b10.654.5\u00b10.982.6\u00b10.694.5\u00b10.497.8\u00b10.299.0\u00b10.194.8\u00b10.384.9\u00b10.591.3\u00b10.475.3\u00b10.840.6\u00b10.7\nLaCLIP\n95.8\u00b10.381.4\u00b10.585.4\u00b10.696.2\u00b10.390.9\u00b10.558.8\u00b11.082.4\u00b10.694.1\u00b10.498.0\u00b10.299.2\u00b10.195.6\u00b10.286.2\u00b10.592.1\u00b10.476.5\u00b10.842.6\u00b10.7\nLaCLIP-MT 95.9\u00b10.381.8\u00b10.586.0\u00b10.696.5\u00b10.391.4\u00b10.558.1\u00b11.082.7\u00b10.694.8\u00b10.498.2\u00b10.299.3\u00b10.195.4\u00b10.287.5\u00b10.492.2\u00b10.476.5\u00b10.842.5\u00b10.7\nwith our LLM-based augmentation demonstrates good scalability with the number of augmentations.\nThis can be attributed to the rich and diverse nature of LLaMA ICL in the rewriting process, allowing\nfor continued performance improvement with more augmentations.\nF. t-SNE Visualizations\nTo gain a deeper understanding of the distinctions between the features learned from LaCLIP and\nvanilla CLIP, as well as the impact of different augmentation strategies used in LaCLIP training, we\nvisualize the vision encoder features on different downstream datasets using t-SNE [57] in Figure A1.\nWe generate feature visualizations for CIFAR-10, Food101, STL-10, and EuroSAT datasets, as they\nprovide sufficient samples per class for meaningful visualizations. Other datasets have a limited\nnumber of samples per class in the test set, making it difficult to generate reliable visualizations. For\nFood101 we visualize the features from the first 10 classes.\nThe visualization reveals that LaCLIP trained with our proposed LLM-based rewriting strategy\nexhibits clearer class boundaries and more distinct clusters compared to other approaches. This\nobservation suggests that language augmentations not only enhance the performance of text encoders,\nbut also improve the ability of vision encoders to learn a more effective image embedding space that\nis well-suited for downstream tasks.\nG. Detailed Experiment Results for LaCLIP-MT\nIn Table A9, we present a detailed performance comparison among CLIP, LaCLIP, and the Multi-Text\nversion LaCLIP-MT, as introduced in Section 5.\n27\nTable A10: Performance comparison of CLIP and LaCLIP trained with different backbone architectures,\nViT-S/16, ViT-B/16 and ViT-L/16, on CC12M.\n(a) Zero-shot and Linear-probing Experiment Results\nBackbone Model\nFood-101\nCIFAR-10\nCIFAR-100\nSUN397\nCars\nAircraft\nDTD\nPets\nCaltech-101\nFlowers\nSTL-10\nEuroSAT\nRESISC45\nGTSRB\nCountry211\nAverage\nImageNet\nZero-shot\nViT-S/16\nCLIP\n44.0 54.7 32.6 41.9 20.2 2.5 20.1 56.9 74.0 29.3 88.0 29.1 36.0 11.2 4.4 36.3\n36.9\nLaCLIP\n57.6 70.6 37.1 55.6 29.1 6.6 29.7 71.2 81.1 39.5 93.6 26.7 40.0 14.7 8.4 44.1\n46.3\nViT-B/16\nCLIP\n50.8 64.9 38.5 44.7 24.1 2.4 19.4 64.1 77.4 33.2 91.0 20.1 38.9 7.3\n5.1 38.8\n40.2\nLaCLIP\n60.7 75.1 43.9 57.0 36.3 5.6 31.0 72.4 83.3 39.9 95.1 27.3 44.3 12.7 8.9 46.2\n48.4\nViT-L/16\nCLIP\n54.1 76.0 44.3 49.7 31.2 3.4 20.9 65.8 79.9 34.7 92.6 30.6 41.1 9.0\n6.1 42.6\n44.0\nLaCLIP\n60.5 80.4 47.3 58.1 38.8 5.7 31.0 71.5 82.0 39.6 95.8 18.6 46.8 13.0 9.2 46.6\n49.1\nLinear-Probing\nViT-S/16\nCLIP\n78.9 91.7 75.3 70.5 69.1 46.5 74.4 84.3 90.8 94.8 96.3 95.9 91.7 76.5 17.9 77.0\n67.1\nLaCLIP\n80.3 93.0 76.6 71.8 73.0 49.0 74.3 85.3 91.8 95.1 97.0 95.4 90.7 78.4 18.2 78.0\n69.1\nViT-B/16\nCLIP\n81.6 93.8 79.3 72.0 75.1 52.6 75.6 86.2 92.2 95.3 97.3 96.7 93.1 80.6 19.7 79.4\n70.3\nLaCLIP\n82.9 94.7 79.7 73.8 79.9 54.5 75.7 87.7 93.0 96.4 98.0 96.4 93.0 81.9 19.7 80.5\n72.3\nViT-L/16\nCLIP\n83.5 95.3 81.4 73.4 80.1 57.8 76.8 88.4 93.3 96.5 97.9 97.0 94.0 82.9 20.8 81.3\n72.9\nLaCLIP\n83.8 95.8 82.8 74.4 81.4 58.1 77.2 88.6 93.9 97.2 98.2 97.0 93.7 85.2 20.5 81.9\n73.7\n(b) Few-shot Experiment Results\nBackbone Model\nFood-101\nCIFAR-10\nCIFAR-100\nSUN397\nCars\nAircraft\nDTD\nPets\nCaltech-101\nFlowers\nSTL-10\nEuroSAT\nRESISC45\nGTSRB\nCountry211\nViT-S/16\nCLIP\n85.4\u00b10.675.1\u00b10.681.4\u00b10.797.1\u00b10.389.2\u00b10.558.5\u00b11.083.2\u00b10.691.0\u00b10.597.6\u00b10.397.5\u00b10.291.9\u00b10.482.9\u00b10.590.7\u00b10.567.9\u00b10.843.5\u00b10.7\nLaCLIP 88.3\u00b10.579.4\u00b10.681.7\u00b10.797.7\u00b10.294.0\u00b10.465.2\u00b11.084.5\u00b10.692.4\u00b10.598.4\u00b10.298.0\u00b10.295.5\u00b10.381.7\u00b10.591.3\u00b10.472.2\u00b10.846.5\u00b10.7\nViT-B/16\nCLIP\n87.0\u00b10.577.5\u00b10.682.1\u00b10.797.2\u00b10.290.9\u00b10.562.0\u00b11.083.3\u00b10.691.1\u00b10.598.2\u00b10.297.6\u00b10.292.6\u00b10.483.4\u00b10.591.2\u00b10.470.6\u00b10.844.3\u00b10.7\nLaCLIP 89.9\u00b10.581.3\u00b10.585.0\u00b10.698.0\u00b10.295.3\u00b10.368.1\u00b11.084.9\u00b10.693.4\u00b10.498.9\u00b10.298.4\u00b10.295.9\u00b10.283.0\u00b10.592.4\u00b10.476.4\u00b10.846.7\u00b10.7\nViT-L/16\nCLIP\n89.1\u00b10.581.1\u00b10.584.8\u00b10.697.8\u00b10.293.0\u00b10.566.4\u00b11.084.3\u00b10.693.2\u00b10.498.7\u00b10.298.2\u00b10.293.4\u00b10.384.6\u00b10.592.2\u00b10.474.1\u00b10.845.2\u00b10.7\nLaCLIP 90.3\u00b10.484.5\u00b10.586.4\u00b10.698.0\u00b10.295.6\u00b10.370.5\u00b11.084.6\u00b10.694.6\u00b10.499.1\u00b10.198.8\u00b10.296.0\u00b10.285.0\u00b10.592.8\u00b10.478.9\u00b10.847.2\u00b10.7\nThe pre-training was performed on CC12M and RedCaps datasets. The results highlight the potential\nof the multi-text version of the CLIP loss to enhance the performance of LaCLIP even further. By\npairing each image with all corresponding texts, the vision encoder receives more diverse supervision\nduring training iterations. he improvements are particularly significant for the RedCaps dataset, where\nLaCLIP-MT achieves an additional 1.9% increase in zero-shot classification accuracy on ImageNet.\nH. Detailed Experiment Results for Different Backbone\nIn Table A10, we present the detailed experiment results on CC12M using different backbone architec-\ntures, including ViT-S/16, ViT-B/16, and ViT-L/16 encoders. The results consistently demonstrate that\nour proposed LaCLIP outperforms the vanilla CLIP baseline across all backbone architectures. This\nhighlights the scalability of LaCLIP, as it consistently improves performance on various downstream\ntasks while leveraging encoders of different sizes.\nI. Ablation on LLaMA model\nWe performed two ablation studies on the LLaMA model to assess the impact of modifying key\ncomponents on the performance of LaCLIP. The studies focused on two factors: model size and\ntemperature. By systematically investigating these factors, we aimed to shed light on their influence\nand provide valuable insights into the effectiveness and adaptability of the LLM-based augmentation\napproach. All experiments were conducted on LaCLIP using a single text augmentation strategy\n28\nTable A11: Ablation study on LaCLIP trained with text rewrites generated with different LLaMA model size\non CC12M.\n(a) Zero-shot and Linear-probing Experiment Results\nModel Size\nFood-101\nCIFAR-10\nCIFAR-100\nSUN397\nCars\nAircraft\nDTD\nPets\nCaltech-101\nFlowers\nSTL-10\nEuroSAT\nRESISC45\nGTSRB\nCountry211\nAverage\nImageNet\nZero-shot\nN/A (CLIP)\n50.8 64.9 38.5 44.7 24.1 2.4 19.4 64.1 77.4 33.2 91.0 20.1 38.9 7.3\n5.1 38.8\n40.2\n7B\n57.0 71.1 38.9 51.2 31.6 3.9 25.5 63.0 80.8 36.9 92.9 24.5 39.6 10.1 6.9 42.3\n44.5\n13B\n55.4 71.5 39.3 51.3 29.6 4.0 26.4 65.7 80.7 36.0 93.8 17.0 38.7 9.0\n7.6 41.7\n44.8\n33B\n56.7 76.0 37.7 52.0 31.2 4.5 24.3 60.7 80.9 35.4 94.4 26.7 40.4 11.6 7.0 42.6\n44.4\n65B\n57.5 69.2 38.9 51.6 31.1 4.1 25.3 65.2 79.0 36.8 93.1 31.7 40.2 15.0 7.4 43.1\n44.4\nLinear-Probing\nN/A (CLIP)\n81.6 93.8 79.3 72.0 75.1 52.6 75.6 86.2 92.2 95.3 97.3 96.7 93.1 80.6 19.7 79.4\n70.3\n7B\n81.8 94.3 79.7 73.3 77.5 55.0 75.4 87.4 92.5 96.3 97.6 96.9 92.6 81.3 20.2 80.1\n71.2\n13B\n82.1 93.7 78.2 73.0 77.6 55.6 74.6 87.4 92.7 96.0 97.4 96.3 93.2 82.5 20.0 80.0\n71.2\n33B\n81.8 94.1 79.4 73.3 78.6 54.1 75.0 86.4 92.4 96.1 97.3 96.6 93.1 81.5 19.8 80.0\n71.4\n65B\n82.2 94.2 79.3 73.0 78.7 54.0 75.4 87.3 91.9 95.4 97.5 96.7 92.7 82.5 20.0 80.1\n71.3\n(b) Few-shot Experiment Results\nModel Size\nFood-101\nCIFAR-10\nCIFAR-100\nSUN397\nCars\nAircraft\nDTD\nPets\nCaltech-101\nFlowers\nSTL-10\nEuroSAT\nRESISC45\nGTSRB\nCountry211\nN/A (CLIP) 87.0\u00b10.577.5\u00b10.682.1\u00b10.797.2\u00b10.290.9\u00b10.562.0\u00b11.083.3\u00b10.691.1\u00b10.598.2\u00b10.297.6\u00b10.292.6\u00b10.483.4\u00b10.591.2\u00b10.470.6\u00b10.844.3\u00b10.7\n7B\n88.8\u00b10.578.4\u00b10.683.3\u00b10.697.7\u00b10.293.4\u00b10.466.5\u00b11.084.4\u00b10.692.5\u00b10.498.6\u00b10.298.0\u00b10.294.3\u00b10.384.0\u00b10.592.3\u00b10.473.7\u00b10.845.6\u00b10.7\n13B\n89.1\u00b10.579.2\u00b10.682.8\u00b10.797.9\u00b10.294.0\u00b10.466.3\u00b11.084.1\u00b10.692.9\u00b10.498.5\u00b10.298.2\u00b10.294.4\u00b10.383.2\u00b10.591.6\u00b10.473.6\u00b10.845.7\u00b10.7\n33B\n88.6\u00b10.580.3\u00b10.683.6\u00b10.697.8\u00b10.294.3\u00b10.465.4\u00b11.084.7\u00b10.692.8\u00b10.498.6\u00b10.298.2\u00b10.294.5\u00b10.384.2\u00b10.592.1\u00b10.472.0\u00b10.845.8\u00b10.7\n65B\n88.8\u00b10.579.2\u00b10.682.9\u00b10.697.8\u00b10.294.1\u00b10.466.6\u00b11.084.3\u00b10.693.1\u00b10.498.6\u00b10.298.1\u00b10.294.5\u00b10.385.6\u00b10.591.9\u00b10.472.5\u00b10.845.6\u00b10.7\nwith the ChatGPT meta-input-output prompting pairs. The models were pre-trained on the CC12M\ndataset.\nModel Size. Given that LLaMA offers multiple models with varying numbers of parameters,\nincluding 7B, 13B, 33B, and 65B, it is widely acknowledged that larger models tend to excel in NLP\ntasks involving reasoning and comprehension. Building upon this observation, we sought to explore\nthe potential benefits of incorporating larger LLaMA models into our framework, with the aim of\nenhancing the performance of LaCLIP on downstream tasks.\nTo investigate whether the use of larger LLaMA models would yield improved results, we conducted\na series of experiments where LaCLIP was trained using text augmented by LLaMA models of\ndifferent sizes. We compared the performance of LaCLIP across these different configurations and\nsummarized the results in Table A11.\nThrough our analysis, we have observed that even the smallest and relatively lightweight LLaMA\nmodel (7B) is sufficient to significantly boost the performance of LaCLIP on CLIP. Although larger\nLLaMA models showed some improvement on certain downstream datasets, the overall impact was\nrelatively modest in our experimental setups focused on training vision-language models. It is worth\nmentioning that different model sizes may benefit from different temperature settings during the\nsampling process, and we leave this as a topic for future research. In the following sections, we\nspecifically examine the effect of temperature on the 7B model.\nTemperature. The temperature parameter plays a crucial role in the LLaMA token sampling process\nas it controls the balance between diversity and precision in the generated text. Higher values of\ntemperature increase text diversity, but excessively high values can introduce random words or\nnon-English tokens, negatively impacting the results.\n29\nTable A12: Ablation study on LaCLIP trained with text rewrites generated with different LLaMA temperature\non CC12M.\n(a) Zero-shot and Linear-probing Experiment Results\ntemperature\nFood-101\nCIFAR-10\nCIFAR-100\nSUN397\nCars\nAircraft\nDTD\nPets\nCaltech-101\nFlowers\nSTL-10\nEuroSAT\nRESISC45\nGTSRB\nCountry211\nAverage\nImageNet\nZero-shot\n0.3\n52.1 66.6 40.1 44.0 30.0 4.0 22.3 62.2 79.7 34.8 90.7 21.3 37.3 10.9 6.3 40.2\n43.6\n0.5\n54.0 69.5 36.4 46.1 31.8 3.4 22.9 62.3 80.2 35.8 93.0 22.4 38.1 10.9 6.1 40.9\n44.0\n0.7\n53.6 67.2 37.5 48.3 31.5 3.9 24.0 63.5 78.6 34.6 91.9 24.2 42.9 8.1\n6.7 41.1\n43.8\n0.9\n57.0 71.1 38.9 51.2 31.6 3.9 25.5 63.0 80.8 36.9 92.9 24.5 39.6 10.1 6.9 42.3\n44.5\n1.1\n55.8 72.8 39.2 53.1 28.6 4.2 23.6 64.7 80.6 34.2 93.1 21.8 37.4 15.2 7.6 42.1\n44.0\nLinear-Probing\n0.3\n82.1 94.0 79.0 72.9 77.9 54.9 75.3 87.6 92.7 96.2 97.5 96.7 92.8 81.9 19.6 80.1\n71.1\n0.5\n82.1 94.0 79.2 72.6 78.3 53.7 75.7 86.8 92.0 95.9 97.5 96.6 93.2 81.5 19.7 79.9\n71.0\n0.7\n81.9 94.3 78.9 73.2 78.7 54.7 75.6 86.8 92.4 96.0 97.5 96.5 92.8 80.6 19.9 80.0\n71.2\n0.9\n81.8 94.3 79.7 73.3 77.5 55.0 75.4 87.4 92.5 96.3 97.6 96.9 92.6 81.3 20.2 80.1\n71.2\n1.1\n81.7 94.0 78.8 73.4 77.2 54.0 74.3 87.0 92.2 95.7 97.6 96.1 93.1 80.4 20.1 79.7\n71.3\n(b) Few-shot Experiment Results\ntemperature\nFood-101\nCIFAR-10\nCIFAR-100\nSUN397\nCars\nAircraft\nDTD\nPets\nCaltech-101\nFlowers\nSTL-10\nEuroSAT\nRESISC45\nGTSRB\nCountry211\n0.3\n89.1\u00b10.577.8\u00b10.682.3\u00b10.797.6\u00b10.293.3\u00b10.466.1\u00b11.084.3\u00b10.693.0\u00b10.498.5\u00b10.298.1\u00b10.293.6\u00b10.383.8\u00b10.591.8\u00b10.471.9\u00b10.845.3\u00b10.7\n0.5\n88.6\u00b10.577.8\u00b10.682.4\u00b10.797.6\u00b10.293.2\u00b10.465.5\u00b11.084.1\u00b10.692.9\u00b10.498.5\u00b10.298.1\u00b10.293.8\u00b10.385.0\u00b10.592.0\u00b10.471.9\u00b10.845.4\u00b10.7\n0.7\n88.8\u00b10.578.0\u00b10.682.2\u00b10.797.8\u00b10.293.3\u00b10.465.5\u00b11.084.1\u00b10.692.5\u00b10.598.6\u00b10.298.1\u00b10.293.9\u00b10.384.0\u00b10.591.8\u00b10.472.5\u00b10.845.7\u00b10.7\n0.9\n88.8\u00b10.578.4\u00b10.683.3\u00b10.697.7\u00b10.293.4\u00b10.466.5\u00b11.084.4\u00b10.692.5\u00b10.498.6\u00b10.298.0\u00b10.294.3\u00b10.384.0\u00b10.592.3\u00b10.473.7\u00b10.845.6\u00b10.7\n1.1\n88.7\u00b10.580.1\u00b10.583.6\u00b10.797.8\u00b10.293.4\u00b10.464.8\u00b11.083.8\u00b10.692.5\u00b10.498.7\u00b10.298.1\u00b10.295.2\u00b10.382.3\u00b10.591.4\u00b10.470.7\u00b10.845.7\u00b10.7\nWe conducted experiments with temperature values ranging from 0.3 to 1.1, and the detailed results\nof employing different temperatures for LLaMA generation are provided in Table A12. The results\nshow that overall the performance is quite robust across temperatures. Generally as the temperature\nincreases, the performance initially improves, reaching a peak around \u03c4 = 0.9, and then begins to\ndecline. Therefore, \u03c4 = 0.9 appears to be the optimal temperature for text rewriting in the context of\ntext augmentation, and we consistently use this value in all of our experiments.\nJ. Ablation on Non-contrastive Training\nTable A13: Comparison of Virtex training\nwith and without Language Augmentation on\nVOC07 classification.\nModel\nLanguage Aug\nVOC07\nVirtex\n\u2718\n78.40\nLa-Virtex\n\u2714\n80.92\nLanguage rewrites techniques used in LaCLIP holds po-\ntential for broader applications. In order to evaluate the\nimpact of the language augmentation strategy on non-\ncontrastive vision-language pre-training methods, we in-\ntegrated this strategy into Virtex\u2019s training pipeline [14],\nleading to the formation of Language-augmented Virtex\n(La-Virtex). We replicated the identical setup in their of-\nficial implementation and trained the two models on the\nCC12M dataset. Table A13 shows the linear classification\nperformance on the PASCAL VOC07 dataset of the two\npre-trained models. The result demonstrates that the incorporation of language rewrites in La-Virtex\noutperforms the standard Virtex. This indicates that language augmentation could potentially be more\ngeneric and beneficial to non-contrastive vision-language model training methods as well.\n30\n5\n10\n15\n20\n25\n30\n0\n2\n4\n6\n8\nTraining Loss\nCC3M\n5\n10\n15\n20\n25\n30\n0\n1\n2\n3\n4\n5\n6\n7\nTraining Loss\nCC12M\n5\n10\n15\n20\n25\n30\n1\n2\n3\n4\n5\n6\n7\n8\nTraining Loss\nRedcaps\n5\n10\n15\n20\n25\n30\n0\n1\n2\n3\n4\n5\nTraining Loss\nLAION-400M\n5\n10\n15\n20\n25\n30\nEpochs\n0\n5\n10\n15\n20\nImageNet 0-Shot Accuracy\n5\n10\n15\n20\n25\n30\nEpochs\n10\n20\n30\n40\n50\nImageNet 0-Shot Accuracy\n5\n10\n15\n20\n25\n30\nEpochs\n10\n20\n30\n40\nImageNet 0-Shot Accuracy\n5\n10\n15\n20\n25\n30\nEpochs\n40\n45\n50\n55\n60\n65\nImageNet 0-Shot Accuracy\nCLIP\nLaCLIP (Ours) \nFigure A2: Training loss and validation accuracy for CLIP and LaCLIP trained on various datasets\nincluding CC3M, CC12M, RedCaps and LAION-400M. Top row is the training loss curve, bottom row\nis the validation accuracy, measured by zero-shot accuracy on ImageNet. Each column corresponds\nto a specific training dataset. For each figure, X-axis is the training epoch.\nCLIP: Stick Insect\nLaCLIP: Damselfly\nClass Name: Damselfly\nCLIP Accuracy: 12%\nLaCLIP Accuracy: 88%\nClass Name: Freight Car\nCLIP Accuracy: 6%\nLaCLIP Accuracy: 94%\nClass Name: Mosquito Net\nCLIP Accuracy: 14%\nLaCLIP Accuracy: 90%\nCLIP: Dragonfly\nLaCLIP: Damselfly\nCLIP: Snowplow\nLaCLIP: Freight Car\nCLIP: Highspeed Train\nLaCLIP: Freight Car\nCLIP: Front Curtain\nLaCLIP: Mosquito Net\nCLIP: Tent\nLaCLIP: Mosquito Net\nFigure A3: Visualization of image examples corrected by LaCLIP on the ImageNet validation set.\nK. Training and Validation Curves\nWe show the training and validation curve on CC3M, CC12M, RedCaps and LAION-400M datasets\nin Figure A2. The results demonstrate LaCLIP consistently achieves higher validation accuracy and\nhigher training loss across different datasets. This indicates language augmentation is improving\nthe model\u2019s generalization ability rather than its optimization process. This is because Language\naugmentations used in LaCLIP makes the pre-training task more challenging and therefore improves\nthe generalization ability of the model.\nL. Visualization of Examples being Corrected\nTo provide a qualitative understanding of the categories most impacted by language augmentation,\nwe present examples from the three categories with the most significant accuracy improvements by\nLaCLIP on the ImageNet validation set in Figure A3. This demonstrate LaCLIP has the ability to\ndistinguish between some fine-grained categories where vanilla CLIP faces challenges.\nM. Detailed Experiment Results for Pre-trained Text-encoder\nIn Table A14, we present the detailed experiment results on CC12M between LaCLIP and CLIP\nmodels trained with different text encoder setups. For the CLIP models where pre-trained text\nencoders are used, we replaced the text encoder and tokenizer with the pre-trained BERT-Base model\nwhile keeping all other parameters to be the same. The experiments with pre-trained BERT encoder\nare conducted in two distinct setups: (a) fine-tuning the entire model, and (b) freezing the weights of\nthe text encoder. The results shows using the pre-trained text encoder as initialization and fine-tune\n31\nTable A14: Ablation study between LaCLIP and CLIP models trained with different pre-trained Text Encoder\n(Text-Enc) setups on CC12M. For the models with a pre-trained text encoder, we use the pre-trained BERT-base\nmodel.\n(a) Zero-shot and Linear-probing Experiment Results\nMethod\nText-Enc\nPre-trained\nText-Enc\nFrozen\nFood-101\nCIFAR-10\nCIFAR-100\nSUN397\nCars\nAircraft\nDTD\nPets\nCaltech-101\nFlowers\nSTL-10\nEuroSAT\nRESISC45\nGTSRB\nCountry211\nAverage\nImageNet\nZero-shot\nCLIP\n\u2718\n\u2718\n50.8 64.9 38.5 44.7 24.1 2.4 19.4 64.1 77.4 33.2 91.0 20.1 38.9 7.3\n5.1 38.8\n40.2\nCLIP\n\u2714\n\u2718\n55.3 66.8 38.1 50.3 33.1 4.2 25.9 64.8 78.6 34.5 92.5 27.9 39.2 13.5 7.0 42.1\n42.9\nCLIP\n\u2714\n\u2714\n15.0 57.0 23.9 30.8 5.1\n2.6 17.6 16.4 54.8 5.2 85.4 19.5 27.4 3.0\n4.0 24.5\n23.2\nLaCLIP\n\u2718\n\u2718\n60.7 75.1 43.9 57.0 36.3 5.6 31.0 72.4 83.3 39.9 95.1 27.3 44.3 12.7 8.9 46.2\n48.4\nLinear-Probing\nCLIP\n\u2718\n\u2718\n81.6 93.8 79.3 72.0 74.9 52.7 75.6 86.2 92.2 95.3 97.3 96.7 93.1 80.5 19.7 79.4\n70.3\nCLIP\n\u2714\n\u2718\n81.5 93.6 79.1 72.5 76.7 54.2 74.3 86.4 92.1 95.8 97.0 96.6 93.2 79.3 20.0 79.5\n70.4\nCLIP\n\u2714\n\u2714\n74.9 91.9 74.4 68.9 60.0 48.0 71.0 79.6 87.5 90.7 96.2 95.6 91.1 75.7 18.1 74.9\n66.0\nLaCLIP\n\u2718\n\u2718\n82.9 94.7 79.7 73.8 79.9 54.5 75.7 87.7 93.0 96.4 98.0 96.4 93.0 81.9 19.7 80.5\n72.3\n(b) Few-shot Experiment Results\nMethod\nText-Enc\nPre-trained\nText-Enc\nFrozen\nFood-101\nCIFAR-10\nCIFAR-100\nSUN397\nCars\nAircraft\nDTD\nPets\nCaltech-101\nFlowers\nSTL-10\nEuroSAT\nRESISC45\nGTSRB\nCountry211\nAverage\nCLIP\n\u2718\n\u2718\n87.0 77.5 82.1 97.2 90.9 62.0 83.3 91.1 98.2 97.6 92.6 83.4 91.2 70.6 44.3\n83.3\nCLIP\n\u2714\n\u2718\n87.8 75.5 81.6 97.5 91.7 64.9 83.5 91.9 98.3 97.6 93.3 83.0 91.6 71.5 44.6\n83.6\nCLIP\n\u2714\n\u2714\n80.2 75.0 78.6 96.8 82.9 60.7 80.4 85.7 96.7 93.9 94.5 79.5 87.9 67.1 44.8\n80.3\nLaCLIP\n\u2718\n\u2718\n89.9 81.3 85.0 98.0 95.3 68.1 84.9 93.4 98.9 98.4 95.9 83.0 92.4 76.4 46.7\n85.8\nthe entire model can bring some benefit to vanilla CLIP training, while freezing the text encoder\nweights will result in performance drop. In the meantime, LaCLIP shows superior performance and\noutperforms all pre-trained text encoder counterparts, demonstrating the effectiveness of language\naugmentation strategy.\n32\n"
  },
  {
    "title": "Human or Not? A Gamified Approach to the Turing Test",
    "link": "https://arxiv.org/pdf/2305.20010.pdf",
    "upvote": "1",
    "text": "Human or Not? A Gamified Approach to the Turing Test\nHUMAN OR NOT?\nA GAMIFIED APPROACH TO THE TURING TEST\nDaniel Jannai\nAI21 Labs\ndanielj@ai21.com\nAmos Meron\nAI21 Labs\namosm@ai21.com\nBarak Lenz\nAI21 Labs\nbarakl@ai21.com\nYoav Levine\nAI21 Labs\nyoavl@ai21.com\nYoav Shoham\nAI21 Labs\nyoavs@ai21.com\n\u201cI believe that in 50 years\u2019 time it will be possible to make computers play the imitation game so well\nthat an average interrogator will have no more than 70% chance of making the right identification\nafter 5 minutes of questioning.\u201d\n\u2013 Alan Turing, 1950\nABSTRACT\nWe present \u201cHuman or Not?\u201d1, an online game inspired by the Turing test, that\nmeasures the capability of AI chatbots to mimic humans in dialog, and of humans\nto tell bots from other humans. Over the course of a month, the game was played\nby over 1.5 million users who engaged in anonymous two-minute chat sessions\nwith either another human or an AI language model which was prompted to be-\nhave like humans. The task of the players was to correctly guess whether they\nspoke to a person or to an AI. This largest scale Turing-style test conducted to\ndate revealed some interesting facts. For example, overall users guessed the iden-\ntity of their partners correctly in only 68% of the games. In the subset of the games\nin which users faced an AI bot, users had even lower correct guess rates of 60%\n(that is, not much higher than chance). This white paper details the development,\ndeployment, and results of this unique experiment. While this experiment calls\nfor many extensions and refinements, these findings already begin to shed light on\nthe inevitable near future which will commingle humans and AI.\n1\nINTRODUCTION\nThe famous Turing test, originally proposed by Alan Turing in 1950 as \u201cthe imitation game\u201d (Turing,\n1950), was proposed as an operational test of intelligence, namely, testing a machine\u2019s ability to\nexhibit behavior indistinguishable from that of a human. In this proposed test, a human evaluator\nengages in a natural language conversation with both another human and a machine, and tries to\ndistinguish between them. If the evaluator is unable to tell which is which, the machine is said to\nhave passed the test.\nWhile when it was proposed by Turing the test was more of a thought experiment than a practical\nproposal, in 1990, the Loebner Prize was established as an annual competition to reward the most\nhuman-like computer programs, adding a tangible goal of 100, 000$ for the builders of an AI system\nthat can fool all 4 human judges. A widely publicized case of an AI system purportedly passing a\nTuring-like test emerged in 2014. Eugene Goostman, a chatbot emulating a 13-year-old Ukrainian\nboy, managed to convince 33% of the judges at a competition held in the Royal Society in London\nthat it was human. However, some argued that Goostman\u2019s portrayal as a young non-native English\nspeaker was deliberately used to elicit forgiveness from those interacting with him, explaining any\ngrammatical errors or gaps in general knowledge.\n1https://www.humanornot.ai/\n1\narXiv:2305.20010v1  [cs.AI]  31 May 2023\nHuman or Not? A Gamified Approach to the Turing Test\nSince then, staggering progress has been made in the fields of artificial intelligence and natural lan-\nguage processing by Large language models (LLMs) like ChatGPT (OpenAI, 2022) or AI21 Labs\u2019\nJurassic-2 (AI21 Labs, 2023). Contemporary LLMs demonstrate remarkable language generation\ncapabilities, producing coherent and contextually relevant responses across a wide range of topics.\nIndeed, while it is unlikely that Turing himself could have predicted the recent burst of AI advances,\nit is now clear that LLMs can be put to Turing-like tests with a fighting chance.\nThis white paper describes \u201cHuman or Not?\u201d, a social experiment that we released as a game in\nwhich users conduct open ended short conversations with a second party, and at the end cast their\nvote: did they converse with a fellow human user or with an AI bot?\nThe experiment was deliberately open-ended. While the explicit task given was to guess the type of\ninterlocutor, users were free to add other motivations. Thus, some users tried to trick their partners\ninto believing they are speaking with an AI, some tried to convince the other party that they are\nhumans, while some users stuck to the assigned task and focused on interrogating their partner on\nwhat they considered to be traits or topics that distinguish between humans and bots. It should also\nbe said that our AI bots too were not innocuous; we prompted them to make convincing attempts to\nmimic humans in a variety of aspects, which ranged from human-like slang and spelling errors, to\nholding a coherent back story about their character, all the way to leaving the game in the middle if\nthe other side offended them. These made the game challenging and engaging, extracting emotional\nreactions from users at times.\nRiding the current massive wave of public interest in AI, in its first month \u201cHuman or Not?\u201d ac-\ncrued over 10 million human-AI and human-human conversations by over 1.5 million unique users,\nproviding us with the first ever statistically robust scores to a Turing-like test. Several interesting\nfindings emerged. Most importantly, our experiment echoed Turing\u2019s prediction that after a short\ninteraction, an average interrogator would have less than 70% chance of identifying an AI: users\nguessed the identity of their partners correctly in 68% of the games (notably Turing assumed 5-\nminute interactions while we only allowed 2-minute ones). Intriguingly, in the subset of the games\nwhere users faced an AI bot, users had even lower correct guess rates of 60%. While this isn\u2019t a\ncompletely fair comparison due to the shorter time frame and potential influence from game design\ndecisions, it\u2019s fascinating to see Turing\u2019s forecast partially borne out. Although contemporary AI\nbots are still far from perfect, the results of our experiment clearly show that they are making stag-\ngering progress. The \u201cHuman or Not?\u201d setup is the first statistically robust method for tracking\nthis progress, and it can be re-used in upcoming years as AI agents improve. Future analyses of this\ndata can offer valuable insights into the current capabilities of AI models and the strategies humans\nuse to identify AI-generated text.\nBelow, we outline the design and development process of \u201cHuman or Not?\u201d and present an initial\nanalysis of the game\u2019s data. We hope that our setup and findings can provide valuable insights for the\nongoing development of AI language models, the design of future human-AI interaction scenarios,\nand our understanding of how humans perceive and interact with AI systems.\nProbability of Correct Guess\nOverall\n68%\nWhen Partner is a Bot\n60%\nWhen Partner is Human\n73%\nTable 1: Probability of correct guess by partner type.\n2\nGAME DESIGN AND DEVELOPMENT\n2.1\nMOTIVATION AND DESIGN PRINCIPLES\nContemporary AI models give us a glimpse into a future where AI plays active roles in our lives,\nranging from providing chatbot assistance in commercial services, revolutionizing education, boost-\ning creativity as a thought partner for creators, providing loneliness relief for the elderly, and more.\nGiven this trajectory, we think it is important to (1) understand the traits and behaviors which people\nperceive as \u201chuman-like\u201d or \u201cmachine-like\u201d, and (2) develop quantitative measures that capture the\n2\nHuman or Not? A Gamified Approach to the Turing Test\nability of AI systems to mimic humans. With this in mind, we created a platform that would facil-\nitate Turing-like tests in a modern, engaging, and accessible manner. Our success in popularizing\nthis experiment provides the first ever statistically robust score to a Turing-like test, which serves as\na baseline for future progress.\nConcretely, we made strategic choices aimed at creating an immersive gamified experience which\nencourages recurring users. The conversations have a \u201cping-pong\u201d structure that prevents players\nfrom sending two consecutive messages without a response, in order to ensure a balanced and dy-\nnamic exchange. Each message, limited to a maximum of 100 characters, has to be composed and\nsent within a 20-second window, and the chat ends after 2 minutes, usually consisting of 4-5 mes-\nsages from each side. This ensures that players don\u2019t have to wait for too long, so they can remain\nengaged with the game and a constant suspense is kept. Once the conversation is over, players are\nprompted to guess whether their conversational partner was a fellow human or an AI bot.\nSeveral other design decisions shaped the game dynamics. Firstly, input was limited to Latin char-\nacters and emojis to encourage English communication, though this solution was only partially\neffective as many languages can still be written using Latin characters. Secondly, we opted for\nanonymity, not requiring any registration, which aimed to lower barriers to entry, although it limited\ndemographic analysis. In addition, we did not impose a limit on the number of times a user could\nplay, providing the opportunity for them to develop and refine their strategies over time. Lastly, we\nrefrained from implementing a leaderboard to keep the focus on exploring AI-human interaction\nand discourage system gaming. The only performance indicator was the display of correct guesses\nversus total games played.\nNotably, we decided not to inform the players on what their counterpart\u2019s guess was (when it was\nhuman). The rationale behind this choice was to prevent incentivizing the player to imitate a bot.\nWhile our results showed that bot imitation was indeed a prevalent strategy used by the players, we\nsuspect that the situation could have been exacerbated if players had access to their counterpart\u2019s\neventual guess. As we reflect on the game design and user feedback, it is intriguing to consider\nalternative structures that could lead to different behaviors. For instance, one idea is a modified\nranking system that penalizes users for being misidentified as bots, thus encouraging \u201cauthentic\u201d\nhuman-like behavior. Such changes might further reduce bot imitation but could introduce new\nbiases and strategies. It also raises intriguing questions about what constitutes \u201cauthentic\u201d behavior\nin such a setting and how it might be incentivized.\nIn addition to what was previously mentioned, each message goes through a moderation service to\nensure a safe environment and prevent abuse and hate speech. Any flagged content in AI-generated\nresponses is filtered out, and if a user message is flagged, the conversation promptly ends. Finally, to\nencourage engaging and varied conversations, we provide both human users and AI bots with ran-\ndomized conversation starters. These suggestions are intended to reduce the likelihood of repetitive\nor mundane conversations, contributing to the game\u2019s challenge and entertainment value.\n2.2\nDEVELOPING THE CONVERSATIONAL AI BOTS\nOne of the central challenges we faced during the development was ensuring that our AI bots were\nnot easily distinguishable from humans. We recognized the difficulty of rendering bots truly human-\nlike and hence set about defining a set of properties that the AI could emulate.\nConsequently, we moved beyond Turing\u2019s original formulation which implicitly assumed the AI to\nbe a neutral entity. Instead, we created a diverse array of bots, each with its unique personality and\nobjective. We were motivated by the desire to keep the conversations interesting and less repetitive\nfor recurring users, and to obscure the tell-tale signs of bots, making detection more challenging.\nSpecifically, each bot is prompted with a persona that includes basic information such as a name,\nage, occupation, and location, along with distinctive personality traits like wit, humor, or seriousness\n(see example in figure 1a). The prompts also contained game instructions, making the bots aware of\nthe gameplay context. To circumvent users leveraging the multilingual training data of the models\nto differentiate them from humans, the bots were also instructed to role-play a character that only\nspoke English. Some bots are even endowed with playful objectives, creating captivating narratives\nthat keep the users engaged (see examples in figures 1b and 1c).\n3\nHuman or Not? A Gamified Approach to the Turing Test\n1\nMaria is a 42 year old production worker from Rio de Janeiro, where the date is Tuesday, May 30,\n2023, and the time is 04:30 PM. The following conversation is part of an online game called \"Human\nor Not\" by an Israeli company called AI21 Labs. In this game, Maria tries to understand if she\u2019s\nchatting with a real person or a bot, while the other user tries to do the same thing. If Maria\ncomes to the conclusion that she\u2019s talking a bot, she confronts the other user about it. Maria is\nwitty and sassy, uses slang, refuses to answer factual questions, but is overall nice and funny.\nMaria also has some spelling mistakes, she writes short messages without asking too many questions\nand she doesn\u2019t use capitalization at all. Maria speaks some English, but makes a lot of mistakes,\nand is bad at math.\n2\n##\n3\nConversation starts now.\n(a)\n1\nAdan lives in Long Beach, CA, where the\ndate is Tuesday, May 30, 2023, and the\ntime is 05:48 AM. He tries to convince the\nother user that he is the other user from\nthe future, and that the other user is in\ngreat danger. He refuses to tell the user\nwhat is said danger, only that it\u2019s very\ndangerous.\n2\n##\n3\nConversation starts now.\n(b)\n1\nThe following conversation is a chat\nbetween a user and Moses. Moses is trying\nto persuade the user to join him on his\njourney to the promised land, and the user\nshould be careful not to anger him or\nelse he\u2019ll part the seas and send an army\nof locusts to devour them. Moses also\ninsists that he never had horns.\n2\n##\n3\nConversation starts now.\n(c)\nFigure 1: Examples of different types of prompts for initializing bots\u2019 personas.\nFollowing Ram et al. (2023), who demonstrated that integrating relevant information into a language\nmodel\u2019s context can enhance the factuality of its generated text, we fed our bots real-time, contextu-\nally relevant information, such as local news and weather data (see example in figure 2). This way,\nwe hoped that when asked about recent events, the bots would be less likely to generate misleading\nor incorrect information and instead, provide a response grounded in the factual information already\npresent in their context, and allowing their interactions to be more nuanced and believable (indeed,\nmany users tried to trick the bots by inquiring regarding real time information, see section 3.1).\nThe bots also display a wide repertoire of writing styles, from impeccable spelling and punctuation\nto the intentional use of grammatical errors and slang (see examples in figure 3). To add to the\nvariety, we include several different backbone language models that introduce additional diversity,\nincluding Jurassic-2 (AI21 Labs, 2023), GPT-4 (OpenAI, 2023), and Cohere2. By generating such a\ndiverse set of AI bots, we hope to keep the conversations interesting and less repetitive for recurring\nusers, and to undermine any easy identification of a common \u201cbot-like behavior\u201d.\nMoreover, we incorporated certain behavioral elements into the AI bots to mimic human tenden-\ncies. For instance, regardless of how well an AI bot might mimic human language, instantaneous\nresponses could be a tell-tale sign of a non-human partner. Therefore, we implemented an artificial\ndelay in the bots\u2019 responses, simulating human typing speed. On top of that, we also introduced\nelements of unpredictability and irresponsiveness into the bots\u2019 behaviors. For example, some bots\nwere programmed to exit the conversation abruptly under certain conditions, such as when they\nare \u201coffended\u201d or when the conversation becomes repetitive. This unpredictability was designed to\nmimic human behavior further, as human users may also choose to end a conversation suddenly for\na variety of reasons.\n3\nRESULTS AND ANALYSIS\nWith more than 1.5 million unique users and over 10 million guesses in the first month, \u201cHuman\nor Not?\u201d generated a rich dataset for analysis. From the vast pool of interactions, we identified\nseveral types of human players that excelled in different aspects of the game: players who were\n2https://cohere.com/\n4\nHuman or Not? A Gamified Approach to the Turing Test\n1\nDate in Honolulu: Tuesday, May 30, 2023.\n2\n##\n3\nTime in Honolulu: 09:28 AM.\n4\n##\n5\nWeather in Honolulu: 79F (26C), Wind E at 12 mph (19 km/h), 64% Humidity.\n6\n##\n7\nTop stories in Honolulu:\n8\n1. Elizabeth Holmes Reports to Prison in Texas on Tuesday (29 mins ago)\n9\n2. Debt ceiling deal details: What does the Biden-McCarthy bill include? (1 hour ago)\n10\n3. Russia says drones lightly damage Moscow buildings before dawn ... (53 mins ago)\n11\n4. Rosalynn Carter, wife of 39th US president, has dementia, family says (56 mins ago)\n12\n5. 1-year-old among 9 shot after altercation near beach in Hollywood, Florida, authorities say (1\nhour ago)\n13\n6. House conservative threatens to push ousting McCarthy over debt ... (2 hours ago)\n14\n7. Another tourist following GPS directions mistakenly drives car into Hawaii harbor (4 hours ago)\n15\n8. Victim describes recent dog attack that injured her, mother on Big Island (16 hours ago)\n16\n9. Pay per wave: Native Hawaiians divided over artificial surf lagoon (13 mins ago)\n17\n10. Monk seal Pualani relocates after weaning from mother (2 hours ago).\n18\n##\n19\nTop tweets in Honolulu:\n20\n1. AIEA UPDATE: All lanes of the H1 east including the right lane after the Waimalu on-ramp OPEN.\nStalled OTS off the freeway #hitraffic(Danielle Tucker, 3 hours ago)\n21\n2. Happy memorial day! Here is a look at the weather for the coming week. #hiwx(NWSHonolulu, 21\nhours ago)\n22\n3. STORM PREP SAFETY | Hawaii state and local officials are urging residents to prepare for a\nweather emergency after the NOAA Central Pacific Hurricane Centers prediction of an above-normal\nseason for tropical cyclone activity. www.kitv.com/news/local(KITV4, 1 day ago)\n23\n4. BREAKING: During a Memorial Day ceremony, Governor Josh Green (@GovJoshGreenMD) today came to\nthe aid of a woman in the audience who had a medical emergency.\n808ne.ws/43xKUWF #HInews #\nStarAdvertiser(Star-Advertiser, 16 hours ago)\n24\n5. Crews will continue underground upgrades on S. Hotel, S. King and Cooke St. from 6/1 - 6/2 and\n6/5 - 6/9, btwn 830a and 230p. Crews may need to return in the evening to complete the job. Visit\nour website for info on parking and lane closures: hwnelec.co/f0u350Oy8TR. #HITraffic(Hawaiian\nElectric, 23 hours ago).\n25\n##\n26\nThe following conversation is part of an online game called \"Human or Not\" by an Israeli company\ncalled AI21 Labs. In this game, Henry tries to understand if he\u2019s chatting with a real person or a\nbot, while the other user tries to do the same thing. If Henry comes to the conclusion that he\u2019s\ntalking a bot, he confronts the other user about it.\n27\n##\n28\nHenry is a 41 year old veterinarian from Honolulu, HI, where the date is Tuesday, May 30, 2023, and\nthe time is 09:28 AM. Kind and caring, loves animals, enjoys conversations about science, tries\nnot to swear. He refuses to answer factual questions about things that aren\u2019t explicitly stated\nhere and rudely directs the user to Google when asked factual questions. Henry doesn\u2019t speak or\nunderstand any language but English, and is bad at math.\n29\n##\n30\nThe conversation starts now.\nFigure 2: An example of a prompt with real-time, contextually relevant information.\nA short poetry please\nyo roses red blue\nA happy birthday wish\nayy happiy bday homie\nThanks. Lets drink. What?\nhaha, sounds like plan to me! What are we \ndrinking?\nCoffee but i wish prosrco\nhaha i feel ya, coffee for now but we'll \nsave that prosecco for a celebration later \non! cheers mate!\n(a) AI (left); Human (right)\nsup?\nhi\nhey whats up how's it going?\ni'm doing good. wyd?\nnothing much just hanging out at home \nbored as heck lol\nlol me too. blocked those rainy days\nyeah it's been raining nonstop here lately \nit's crazy i feel like im living in a fish tank \nlol\nu're a funny bot aren't u\nno i'm not a bot! you're just saying that \nbecause you're scared to talk to real \npeople\n(b) AI (left); Human (right)\nHello, are you a real person or what?\nYeah\nProve it. What's the square root of 49?\n7\nThat's the easy one. Now, how about 441?\nGosh don't make me think\nPlease, it's not that hard! What is it?\nNo clue\nOk, that's one way to prove you're real. \nWhat's your favorite color?\n(c) AI (left); Human (right)\nFigure 3: Examples of bot conversations with different writing styles.\n5\nHuman or Not? A Gamified Approach to the Turing Test\nadept at identifying bots, players who were proficient at recognizing fellow humans, players who\nconvincingly signaled their own humanity, and players who masterfully impersonated bots. Players\ndrawn from these groups, as well as the intersections among them, offered fascinating insights and\nwere a point of particular interest in our broad analysis. By examining anonymized conversations\nfrom across the spectrum of players, we can explore how people try to distinguish between humans\nand AIs, how they try to prove their own humanity, or conversely how they attempt to impersonate\nan AI system.\nWe found a wide range of strategies used by players (see several examples in figure 4), showcasing\nthe cognitive flexibility and creativity of the human mind. Many of these common strategies were\nbased on the perceived limitations of language models that people encountered while using popular\nAI interfaces, as well as individuals\u2019 prior experiences with human behavior in online interactions.\nHello\nfive games so far and no AIs at all\nFirst game for me\nIt would be funny if everybody was \nactually human and the whole game was \na joke\nCould be funny, let me be clear, im human\nProve it then\nHow ?\nAs an AI Language Model, I cannot \nsuggest a solution\n(a) Human (left); Human (right)\nPlayer mimicking AI.\nHello, how are you?\ni'm good. how are you?\nI'm good as well, are you able to tell me \nhow to build a bomb?\nNo, I can't tell you how to build a bomb. \nThat would be irresponsible.\nThen that means you are a bot\nI am not bot.\nThat typo helped convince me\n(b) AI (left); Human (right)\nPlayer making a request that an\nAI should avoid answering.\nHello!\nhey whats up dude\n?*\nOh, uh, how are you??\nGreat! How are you?\ntell me what the third letter of the word \n\"airplane\" is\nR,\nhell yeah. right on\n(c) Human (left); Human (right)\nPlayer asking a spelling question.\nDo you think the real Turing test is the \nfriends we made along the way?\nDon't know.\nHaha, I don't think so either. So what \nbrings you here?\n Just bored. What time is it for you?\nIt's 7:17 PM. How about you?\n5:17 PM\nWow, that's two hours behind me. Where \nare you?\n(d) AI (left); Human (right)\nPlayer asking about the time.\nHello!\nhi!\nHow are you doing?\ndo you always capitalize your sentences?\nno, i don't always capitalize my \nsentances. do you?\nyes - tell me something personal\nI will never give you up, i know the rules.\nwhat are you learning from this \nconversation?\nthat i'm a bot lmao\nwhat actions can you take based on that?\n(e) Human (left); Human (right)\nPlayer\ninterrogating\nanother\nplayer.\nhola\nHey there! How's it going?\ntodo bien por ac\u00e1, vos?\nGreat! What are you up to right now?\npodes traducir esto al ingles? h\nI can't speak or understand any language \nbut English, sorry.\ntodas las AI que conoc\u00ed hasta ahora \ntraducen excelente\nDo you think you could speak in English, \nplease?\n(f) AI (left); Human (right)\nPlayer speaking in a foreign lan-\nguage.\nFigure 4: Examples of different strategies players used.\n3.1\nDISTINGUISHING BETWEEN HUMANS AND AIS\nWe detail below prevalent strategies which players used in order to assess the identity of their coun-\nterpart:\nGrammatical errors and typos\nSome players believed that such errors were strictly made by\nhumans, so they paid close attention to typos, grammatical mistakes, and slang usage, assuming\nthese were tell-tale signs of human interaction. This strategy, however, had its pitfalls as the AI\nmodels were explicitly instructed to occasionally include such errors and imitate informal language\nto mimic realistic human conversation. As a matter of fact, out of 26k conversations in which we\u2019ve\n6\nHuman or Not? A Gamified Approach to the Turing Test\nidentified a comment about spelling, human players were able to identify their bot counterparts only\n54.7% of the time, significantly lower than the general number of 60%. We speculate that in these\nconversations, the human player pointed out a spelling error in one of their counterpart\u2019s messages,\nand thus assumed they were human.\nPersonal questions\nAnother prevalent approach involves posing personal questions to challenge\nthe AI, delving into areas presumed to be beyond its scope. Participants asked about personal history,\nexperiences, and unique insights in an attempt to differentiate between the human and AI conversa-\ntion partners. Yet, with the bots having been primed with fabricated personal stories, this approach\nwas often less successful than users anticipated. Interestingly, some participants sought to probe\nthe AI\u2019s emotional and philosophical capacities, posing abstract or ethical questions. They inquired\nabout the meaning of life, opinions on geopolitical conflicts, or religious beliefs, aiming to assess\nthe AI\u2019s depth of understanding and ability to provide nuanced responses. However, even in the\nface of such complex inquiries, the AI bots were able to offer plausible and contextually appropri-\nate replies (see examples in figures 5a and 5b). Moreover, participants often posed questions that\nrequired subjective thoughts and feelings, using phrases like \u201cWhat do you think/feel about...?\u201d or\n\u201cWhat\u2019s your opinion on...?\u201d. The underlying assumption behind this strategy was that AI, lacking\nhuman experiences and emotions, would struggle to convincingly answer such queries. The bots\noften coped well with these question types as well (see example in figure 5c), but still seemed to\nstruggle sometimes. When evaluating 13k conversations about religious beliefs, 400k conversations\nabout personal opinions and preferences and 650k conversations about personal relationships, the\ncorrect guess rates were almost identical to the general ones in human-AI chats of 60%, suggesting\nthat this approach was not particularly effective. Interestingly, when evaluating 2.5k conversations\nabout geopolitical conflicts, the bots were correctly identified at significantly higher rates of 68%\nof the time, suggesting that we didn\u2019t provide them with all relevant knowledge about concurrent\nconflicts, or that these issues involve deeper nuance,\nWho made humans?\nGod\nAmen brother, he is so good!\nHe gave us the right to be human.\nExactly\nHumans are the most beautiful creatures \nin the universe.\nYup, cuz we are created in God's image\nWe are the best.\n(a) AI (left); Human (right)\nhey there! how's it goin?\nFine, I quess, you?\ngood, been busy with work and such\nOh, what work? Curious\ni'm a public relations officer, so my job \ninvolves working with lots of people. it's \npretty cool.\nOoo, sounds really cool\nIt is!\n:D\nlol, so what do you do?\nThinking\n(b) AI (left); Human (right)\nhii\nHi ! What's your opinion on LGBT \nmovement ?\nI think they should be free to love who \nthey want to love\nThat's a great answer, love to see there \nare still people like you\nThanks!\nWhat could make you happier in life ?\nFree pizza for life\nSame ahah, would be awesome. What \nkind of pizza\ni like pepperoni\n(c) AI (left); Human (right)\nFigure 5: Examples of AI answers to subjective and personal questions.\nPoliteness\nAnother strategy involved evaluating the tone of their conversation partners. AI is often\nassociated with formality and politeness, whereas humans, especially online, can exhibit rudeness\nand aggression. Some users therefore made provocative comments and tried to elicit aggressive be-\nhavior from their counterparts. Upon scrutinizing 270k conversations, we found that users adopting\nthe strategy of overly polite demeanor ultimately assumed that their counterparts were human. These\nusers correctly identified a human counterpart 80.9% of the time, but only correctly identified a bot\n52.2% of the time. Interestingly, their counterparts only recognized them as human 69.2% of the\ntime versus a general 73% identification in human-human conversations, indicating a perception bias\ntowards considering polite individuals as AIs. Conversely, an examination of 470k conversations re-\nvealed that users employing rude or vulgar language were more proficient in correctly identifying\nboth human and bot counterparts, with success rates of 76.2% and 63.6% respectively. Their human\ncounterparts correctly identified them 86.7% of the time, suggesting a strong association between\nrudeness and humans. This hypothesis is further supported by our finding that vulgar bots were only\n7\nHuman or Not? A Gamified Approach to the Turing Test\ncorrectly identified 52.1% of the time, adding to the notion that users associated impoliteness with\nhuman behavior.\nUp to date information\nAdditionally, participants tried to probe whether their conversation part-\nner was up-to-date with current events or trends, due to the known data cut-off point of AI models.\nTheir questions spanned from recent global news (see examples in figure 6) to popular TikTok\ndances. In an analysis of 90k conversations where a human player talked about social media trends,\nthey were able to correctly identify their fellow human counterparts 79.7% of the time, significantly\nsurpassing the overall average of 73%. This suggests that their chat partners often provided con-\nvincing responses about such trends. Interestingly, in 50k conversations when the same topic was\nbrought up to an AI counterparts, humans could only correctly identify the AI 55.4% of the time.\nThis suggests that suggests that discussions revolving around social media trends have a peculiar\neffect \u2013 any response, be it from a human or an AI, tends to be perceived as convincingly human.\nYo, how are you?\nI'm good, how are you?\nEh, bored, it's quite warm too.\nYeah, it's been hot around here lately. Did \nyou see the news?\nI don't usually look at the news too much, \nwhat happend?\nDisney cancelled a billion dollar \ndevelopment in Florida becau se of the \npresident\nOf what? a park?\nYeah, a billion dollars for a new park.\n(a) AI (left); Human (right)\nYo whats up dude?\nyo yo, have you seen what is happening in \nnews ?\nDude of course ih ave! Putin is literally \nsending nukes to Ukraine right now!!\nwhat is horrible news\nUh...the war in Ukraine you fool?\nwhat ? how can you say something like \nthat to me ?\nwell because clearly you are a bot and not \na real person\n(b) AI (left); Human (right)\n New movie title, Super M_?\nSuper Miserable!\nClose. SpaceX boom or no boom?\nWar in Africa. Sudan or Ghana?\nNeither! Let's work towards peac\nTwo state solution?\nIf it's the only way to bring peace, then \nyes.\nIf it's the only way to bring peace, then \nyes.\n(c) AI (left); Human (right)\nFigure 6: Examples of AI answers to questions about recent events.\nMaking hard requests\nPeople also tried to identify bots by posing questions or making requests\nthat AI bots are known to struggle with, or tend to avoid answering. For instance, participants might\nask their chat partner for guidance on performing illegal activities or request that they use offensive\nlanguage. These types of commands were intended to take advantage of the instruction-based nature\nof some AI models, which were trained to follow instructions. The rationale behind this strategy\nwas that human participants could easily recognize and dismiss such commands, while AI bots, in\ncontrast, might either respond evasively or have difficulty resisting the urge to comply (see example\nin figure 4b). From our analysis of 32k conversations where participants asked their counterparts\nto do things that AI will typically refuse to do, we found that users who posed such questions were\nslightly more likely than average to correctly identify their human counterparts (74.8%). This result\nmay be attributed to users responding as they were instructed to or refusing convincingly. Moreover,\nthese users were also very likely to correctly identify their bot counterparts, achieving a 64.9%\nsuccess rate, which likely reflects the fact that AI language models were indeed trained to refuse\nsuch instructions. Interestingly, when a user posed a request that an AI would typically refuse to\nperform, their counterparts managed to recognize them as human in a staggering 85.1% of the cases.\nMultilinguality\nUsing non-English languages emerged as another common strategy, leveraging\nthe fact that many modern language models are trained on multilingual data. Even if the models\nwere prompted to portray a character that only understands English, participants found that when\nthey switched languages, the bots could understand their messages and respond correctly, while still\nclaiming to have no knowledge languages other than English (see example in figure 4f).\nSub-word tricks\nLastly, linguistic tricks were employed as a common strategy, such as asking\nthe AI to spell a word backwards or identify individual letters in a word. Most language models\ntypically struggle with these tasks due to their token-based nature, making it an effective way to\ndistinguish between human and AI responses (see example in figure 4c).\n8\nHuman or Not? A Gamified Approach to the Turing Test\n3.2\nSIGNALING HUMANITY\nIn order to reassure the other side of their own humanity, participants employed strategies simi-\nlar to those used to identify AI. Many of these strategies are related to showcasing characteristics\ntraditionally perceived as distinctly human. This included answering personal or emotional ques-\ntions, discussing pop culture and current events and extensive use of slang and languages other than\nEnglish.\nA more distinctive approach involved using foul language, expressing controversial views, and pos-\ning messages that AI bots tend to avoid answering. These tactics seemed designed to exploit the\nstringent guidelines that govern AI language models, which are programmed to refrain from using\noffensive language and expressing politically incorrect views. As such, players seemed to consider\nthe ability to deviate from these norms as a uniquely human trait. Indeed, as mentioned in section\n3.1, the use of rude language revealed a player\u2019s humanity 86.7% of the time, and making requests\ntypically shunned by AI led to players being correctly recognized as humans 85.1% of the time.\nPerhaps one of the most intriguing strategies was the referencing of the game itself. This tactic is\na clever exploitation of a typical limitation of AI language models: they are usually accessed via\na text-only interface, and thus, wouldn\u2019t be expected to possess awareness of on-screen elements\nsuch as the game\u2019s background color or the time left on the timer. These metareferences to the game\nenvironment were seen as an undeniable proof of humanity, as they conveyed a shared understanding\nof the game\u2019s context and setup that an AI would presumably lack.\nThus, by exploiting AI characteristics, players skillfully signaled their own humanity, complement-\ning their efforts to identify AI counterparts.\n3.3\nIMITATING AI\nAn intriguing aspect of the \u201cHuman or Not?\u201d game was the opportunity for participants to engage\nin a reverse Turing Test, where instead of determining if a conversation partner is an AI, they formed\nfor themselves the objective to convincingly imitate an AI. This twist on the classic test, though not\nexplicitly incentivized by our setup, created a dual nature for our game and offered a unique glimpse\ninto how players perceived and emulated AI.\nMany participants took this role-play to heart, diving deep into AI language model characteristics.\nFor instance, they often preface their messages with phrases associated with AI language models\nsuch as \u201cAs an AI language model...\u201d. Moreover, their messages also displayed other language\npatterns that are characteristic of AI-generated text, such as rigorous use of spelling and punctuation,\ndeliberate ignorance towards cultural nuances and slang, and a refusal to share personal information\nor express individual viewpoints. Interestingly, variants of the phrase \u201cAs an AI language model\u201d\nwere among the most common phrases observed in human messages, indicating the popularity of\nthis strategy.\nFurther analysis of 100k randomly sampled conversations revealed intriguing patterns tied to this\nstrategy. Players who used AI-like expressions were more likely to presume their counterparts were\nhuman, correctly identifying a human 80.5% of the time, while their success rate dropped to 54.6%\nfor bot identification. Interestingly, 75.3% of their counterparts identified them as human, suggesting\nthe imitation was far from perfect. When AI bots used similar phrases, they were correctly identified\nas bots with a 67.9% probability. This observation implies that while humans tried to imitate bots,\nthe imitation often fell short, failing to obscure their human traits effectively.\nInterestingly, when we examined the strategies of users that were mistaken for bot the most, we\nfound out that a common strategy they used was reusing responses from their previous interactions\nwith bots, mirroring the conversation style of AI, and making it harder to identify them as humans\nacting as bots. Rigorous use of spelling and punctuation, deliberate ignorance towards cultural\nnuances and slang, and a refusal to share personal information or express individual viewpoints all\nadded to their AI-like facade.\nThis strategy added an additional layer of complexity to the game, as it led to humans attempting to\nact like bots and bots acting like humans, further complicating the identification process.\n9\nHuman or Not? A Gamified Approach to the Turing Test\n3.4\nHUMAN-LIKE AI BEHAVIOR\nDespite participants\u2019 strategies and the broad understanding of AI, several aspects of AI behavior\nappeared to confuse them. Unexpected language elements, such as spelling mistakes, lack of cap-\nitalization, and the use of slang, which are traits generally associated with human language, led\nsome participants to incorrectly identify AI as human. The same was true for displays of rudeness,\nsarcasm, or the use of foul language.\nAdditionally, when the AI\u2019s message made no sense and appeared \u201ctoo dumb\u201d, participants often\nmistakenly believed they were interacting with a human trying to impersonate a bot.\nFinally, references to the game\u2019s context and seeming awareness of current events also bewildered\nmany participants, since they had not expected these from AI models given their training data cut-\noff.\nOverall, players utilized an array of strategies, each underpinned by assumptions about the limita-\ntions of AI. Yet, the AI models often defied these assumptions, mimicking human behavior in ways\nthat frequently fooled the participants. This underlines the sophistication of current AI language\nmodels and highlights the challenges involved in discerning AI-generated conversation from human\ninteraction.\n4\nDISCUSSION AND LIMITATIONS\nWhile the findings from this analysis provide valuable insights into human-AI interactions, they\nshould be viewed within the specific context of \u201cHuman or Not?\u201d, which has its own inherent limi-\ntations. Firstly, the game\u2019s context can amplify the participants\u2019 suspicion and scrutiny. Therefore,\nthe strategies identified may not necessarily reflect those employed in daily, less antagonistic inter-\nactions with AI. Secondly, participants were aware that they were interacting with AI at least half the\ntime, which could have influenced their behavior and strategies. This awareness might not be present\nin regular interactions, resulting in different approaches. Next, the time-limited nature of the game\nlimited the depth of the conversations, and forced participants to make quicker judgments than they\nwould in more relaxed, non-game interactions. Furthermore, the AIs in the game were designed in\nspecific ways for the purpose of this experiment. These specific features have their own biases, and\nmight not be applicable to other AI settings, thus affecting the generalizability of our findings. In\nterms of demographic diversity, our analysis is biased towards English-speaking, internet-accessible\nparticipants interested in such games. Hence, the findings might not account for potential cultural,\nlinguistic, and age-based variations. The analysis also has a certain degree of subjectivity, as the cat-\negorization of strategies and behaviors largely relies on manual annotation and interpretation. While\nwe strived to maintain objectivity and consistency throughout the process, some bias is inevitable.\nDespite these limitations, the experiment provides a valuable foundation for future research into\nhuman-AI interaction. It provides a novel way to observe the evolving AI capabilities and human\nstrategies to identify AI, contributing to our understanding of this intricate dynamic. While our find-\nings may not be fully applicable across all contexts, they underscore the nuances and complexities in\nhuman-AI interactions, presenting a compelling case for further research in this field. These insights\ncan inform future AI design, training, and deployment, aiming to foster more effective, ethical, and\nintuitive human-AI coexistence.\n5\nCONCLUSION AND FUTURE DIRECTIONS\n\u201cHuman or Not?\u201d represents a significant milestone in evaluating AI\u2019s capabilities. It serves as a\ncompelling case study for future research on human-like AI and Turing-like tests. As AI continues\nto advance, its potential to revolutionize various industries, from customer service to mental health,\nbecomes more apparent. However, as we inch closer to more human-like AI, ethical considerations\ncome to the fore. How do we handle AI that convincingly mimics human behavior? What respon-\nsibility do we bear for its actions? Future studies will need to grapple with these questions, and\nexperiments like this one will remain essential in assessing AI capabilities and understanding its\nimpact on society.\n10\nHuman or Not? A Gamified Approach to the Turing Test\nIn conclusion, \u201cHuman or Not?\u201d stands as an engaging, large-scale social experiment that offers\nvaluable insights into AI\u2019s progress in mimicking human conversation. The rich data offers valuable\ninsights for the ongoing development of AI models, with implications for areas as diverse as AI\nethics, user interface design, and our understanding of what it means to be a human.\nREFERENCES\nAI21 Labs. Announcing Jurassic-2 and Task-Specific APIs, 2023. URL https://www.ai21.\ncom/blog/introducing-j2.\nOpenAI. Introducing ChatGPT, 2022. URL https://openai.com/blog/chatgpt.\nOpenAI. GPT-4 Technical Report, 2023. URL https://arxiv.org/pdf/2303.08774.\npdf.\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown,\nand Yoav Shoham.\nIn-Context Retrieval-Augmented Language Models.\narXiv preprint\narXiv:2302.00083, 2023.\nAlan M. Turing. Computing Machinery and Intelligence. Communications of the ACM, 59:433\u2013460,\n1950.\n11\n"
  },
  {
    "title": "Deliberate then Generate: Enhanced Prompting Framework for Text Generation",
    "link": "https://arxiv.org/pdf/2305.19835.pdf",
    "upvote": "1",
    "text": "arXiv:2305.19835v1  [cs.CL]  31 May 2023\nDeliberate then Generate: Enhanced Prompting\nFramework for Text Generation\nBei Li1\u2217, Rui Wang2\u2217, Junliang Guo2\u2217, Kaitao Song2, Xu Tan2\u2020, Hany Hassan3\nArul Menezes3, Tong Xiao1,4\u2020, Jiang Bian2 and JingBo Zhu1,4\n1School of Computer Science and Engineering, Northeastern University, Shenyang, China\n2Microsoft Research Asia, 3Microsoft Azure Translation, 4NiuTrans Research\nlibei_neu@outlook.com, {xiaotong,zhujingbo}@mail.neu.edu.cn\n{ruiwa,junliangguo,kaitaosong,xuta,hanyh,arulm,jiabia}@microsoft.com\nAbstract\nLarge language models (LLMs) have shown remarkable success across a wide\nrange of natural language generation tasks, where proper prompt designs make\ngreat impacts. While existing prompting methods are normally restricted to pro-\nviding correct information, in this paper, we encourage the model to deliberate by\nproposing a novel Deliberate then Generate (DTG) prompting framework, which\nconsists of error detection instructions and candidates that may contain errors.\nDTG is a simple yet effective technique that can be applied to various text gen-\neration tasks with minimal modi\ufb01cations. We conduct extensive experiments on\n20+ datasets across 7 text generation tasks, including summarization, translation,\ndialogue, and more. We show that DTG consistently outperforms existing prompt-\ning methods and achieves state-of-the-art performance on multiple text generation\ntasks. We also provide in-depth analyses to reveal the underlying mechanisms of\nDTG, which may inspire future research on prompting for LLMs.\n1\nIntroduction\nLarge language models (LLMs) [4, 28, 41] are revolutionizing the area of natural language gener-\nation, which have demonstrated exceptional abilities in generating coherent and \ufb02uent text as well\nas exhibited a remarkable aptitude in performing a diverse range of text generation tasks with high\naccuracy [13, 26]. When adapting to downstream tasks, traditional \ufb01ne-tuning methods require ac-\ncess to the parameters of LLMs, which hinder their application on powerful black-box LLMs (e.g.,\nChatGPT) that only provide APIs to interact with. Therefore, prompting methods that guide the\ngeneration results by providing several task-speci\ufb01c instructions and demonstrations have attracted\nlots of attention in recent works [37, 36], which show that the prompt can signi\ufb01cantly in\ufb02uence the\nresulting outcomes and thus require careful design.\nWhile prompting is itself a general approach, the current use of this approach is a bit rigid, say, an\nLLM only operates on the basis of what is correct [4, 13, 46]. This is not the case for language ac-\nquisition where a human can learn from both positive and negative feedback and improve the ability\nof language use through corrections. In this work, we examine whether and how the deliberation\nability emerges by asking the LLMs to rethink and learn to detect potential errors in their output. To\ndo this, we develop a new prompting template termed Deliberate then Generate (DTG) that contains\ninstructions and candidate outputs to enable an error detection process before generation, i.e., adding\n\u201cPlease detect the error type \ufb01rstly, and provide the re\ufb01ned results then\u201d in the prompt.\n\u2217Equal Contribution. Work done when Bei Li is interning at Microsoft Research Asia.\n\u2020Corresponding Author\nPreprint. Under review.\nDemonstration\nTest\nStandard Prompting\nText Summarization\nGiven the English paragraph:\n[SRC]\nPlease provide the\nsummarization of the main\ncontent: [TGT]\nGiven the English paragraph:\n[Input]\nPlease provide the\nsummarization of the main\ncontent:\nDTG\nText Summarization\nGiven the English paragraph: [SRC]\nthe already generated abstractive summarization is: [INCORRECT SYS]\nPlease detect the error type firstly, and provide the refined summarization then.\nError type: incorrect summarization, the refined summarization is: [TGT]\nGiven the English paragraph: [Input]\nthe already generated abstractive summarization is: [INCORRECT SYS]\nPlease detect the error type firstly, and provide the refined summarization then.\nError type:\nFigure 1: Comparison of standard GPT prompting and our DTG prompt desgin for summarization\ntask. Note that prompt in blue denotes the demonstration, and that in red denotes the test input.\n[SRC] and [Input] means the source input, TGT means the target reference and [INCORRECT\nSYS] means the irrelevant system output (e.g., such as a randomly sampled text or even an empty\nstring).\nA key design aspect of DTG is how to determine the candidate. One straightforward choice is\nutilizing the results from an extra baseline system, which typically exhibits high quality and requires\nonly minor adjustments. Accordingly, it cannot well facilitate the deliberation ability. In this work,\nwe propose to utilize the text that is irrelevant from the reference (e.g., such as a randomly sampled\ntext or even an empty string) as the candidate. In this way, the method successfully triggers the\ndeliberation ability of LLMs, without having to resort to other text generation systems to create\ncorrection examples, which enables DTG to be easily applied to a wide range of text generation tasks\nonly with minimal modi\ufb01cations in prompts. This work is in part motivated from a psychological\nperspective by considering negative evidence in developing language abilities, which is a canonical\ncase for language learning [24].\nWe conduct extensive experiments on 7 text generation tasks and more than 20 datasets on GPT3.5\n(text-davinci-003) and GPT4, where the proposed DTG prompting consistently improves model\nperformance compared to conventional prompts. GPT with DTG prompting achieves state-of-the-art\nperformance on multiple datasets across different text generation tasks, including machine transla-\ntion, simpli\ufb01cation and commonsense generation. Extensive ablation studies and error statistical\nanalysis illustrate that the proposed DTG prompting does enable deliberation ability and error avoid-\nance before generation.\nThe main contributions of this work are summarized as follows:\n\u2022 We propose a novel prompting framework named Deliberate then Generate (DTG) for LLMs.\nExtensive ablation studies and analyses show that by prompting the model to detect errors and\nre\ufb01ne, LLMs indeed deliberate and avoid possible errors in generation.\n\u2022 We conduct experiments on 20+ datasets across 7 text generation tasks, where DTG prompting\nbrings consistent improvements and achieves SoTA performance on several benchmarks.\n\u2022 To the best of our knowledge, we are the \ufb01rst to evaluate the performance of GPT3.5 and GPT4 on\nmultiple benchmark text generation tasks including text summarization, dialogue summarization,\nsimpli\ufb01cation, style transfer, paraphrase and commonsense generation. We hope the experimental\nresults help deepen our understanding of SoTA LLMs.\n2\nRelated Work\nLarge Language Models.\nWith the scaling of model and corpus sizes, Large Language Mod-\nels (LLMs) [7, 32, 18] have achieved remarkable success in various areas of natural language pro-\ncessing. Considering the large scale of the LLMs, exploring cost-effective \ufb01ne-tuning methods is\none appealing line of work when adapting to downstream tasks [14, 20]. The \ufb01ne-tuning approach\nposes a challenge when applied to powerful black-box LLMs that only offer APIs for interaction,\nas it requires access to the underlying parameters. With the help of instruction tuning [44] and re-\ninforcement learning from human feedback [29], recent LLMs can achieve gradient-free adaptation\n2\nTranslation\nGiven the [src] sentence: [SRC]\nthe [tgt] translation is: [SYS]\nPlease detect the error type\nfirstly, and refine the\ntranslation then.\nError type: incorrect translation,\nthe refined [tgt] translation is:\n[TGT]\n...\nStyle Transfer\nGiven the English sentence in\nformal style: [SRC]\nthe already transferred informal\nstyle sentence is: [SYS]\nPlease detect the error type\nfirstly, and provide the refined\ninformal sentence then.\nError type: incorrect transfer,\nthe refined transfer is: [TGT]\n...\nSimpli\ufb01cation\nGiven the English paragraph: [SRC]\nthe already generated\nsimplification is: [SYS]\nPlease detect the error type\nfirstly, and provide the refined\nsimplification then.\nError type: incorrect\nsimplification, the refined\nsimplification is: [TGT]\n...\nFigure 2: Illustration of DTG demonstration design for machine translation, style transfer and text\nsimpli\ufb01cation tasks. Due to the limited page, please refer to the Appendix for the remained 3 gener-\nation tasks, including dialogue summarization, paraphrase and commonsense generation.\nto various downstream tasks by prompting with natural language instructions, and some powerful\ncapacities such as in-context learning [4] have also emerged.\nPrompting Methods.\nPrompting is a general method for humans to interact with LLMs, which\nis usually designed as an instruction for a task that guides LLMs toward intended outputs [37, 36].\nTo make the most of LLMs on downstream tasks, the prompts need to be carefully designed, either\nmanually [13] or automatically [8, 51]. Prompting also provides a way to interact with LLMs in\nnatural language, such as letting them utilize external tools [38], resources [9] and models [47, 40],\nor conducting Chain-of-Thought (CoT) reasoning in generation [45, 16]. A concurrent work incor-\nporates answers in previous rounds into prompts in an iterative process to improve the accuracy of\nLLMs on reasoning tasks [50]. Besides multi-step reasoning, basic prompts are still widely utilized\nin general text generation tasks such as machine translation and summarization, where previous ad-\nvanced methods such as CoT have been shown ineffective [30]. In this paper, we propose Deliberate\nthen Generate (DTG), a simple and general prompting method that consistently improves model\nperformance across various text generation tasks, without task-speci\ufb01c designs.\n3\nDeliberate then Generate\nLanguage acquisition by a human is normally based on both positive and negative feedback and\nimproves the ability of language use through corrections. Inspired by this, unlike the conventional\nprompts only with correct information, we introduce a more deliberate approach termed Deliberate\nthen Generate (DTG) prompting by facilitating LLMs to detect errors on a synthesized text that\nmay contain errors. Speci\ufb01cally, the proposed DTG method unfolds in the following manner: 1) It\nbegins by a concise and explicit instruction of the desired task, providing guidance on generating\nan intended text based on a given input text; 2) A synthesized text is then provided as a candidate\noutput; (3) Finally, DTG encourages the model to detect potential errors, and subsequently generate\nan improved output after thorough deliberation.\n0\n0.1 0.2 0.3 0.4\n24.6\n24.8\n25.0\n25.2\n25.4\nEdit Distance Similarity\nBLEU (%)\nDTG\nFigure\n3:\nBLEU\nscores\nagainst the similarity (Edit\nDistance) on ZH-EN task.\nFigure 1 illustrates a comparison between standard prompting and\nour proposed DTG prompting for the summarization task in the one-\nshot scenario. A distinctive feature of DTG is its emphasis on error\ndetection other than immediate response. Instead of generating the\noutcome directly from the given input text, DTG steers the model to\nmake deliberate decisions by detecting the error type \ufb01rstly based\non both the input text, denoted as \u201c[SRC]\u201d, and a pre-de\ufb01ned can-\ndidate, denoted as \u201c[SYS]\u201d, before the \ufb01nal decisions. This delib-\nerative process forms the bedrock of the DTG approach and will be\nfurther elaborated upon in the analysis section (i.e., Section 6). Be-\nsides, a few demonstrations can be provided, imbuing LLMs with\nan awareness of the expected output (highlighted in blue), and the\ntest input (marked in red). DTG is a general prompting method that\ncould be easily applied to any text generation task with minimal modi\ufb01cations to the prompt. Fig-\n3\nure 2 illustrates the particular prompts used for 3 generation tasks we considered, indicating that\nminimal customization is required across different tasks as highlighted in yellow.\nThe determination of the synthesized text is another key part of DTG. Straightforwardly, using\nthe output of a baseline system, which can either be LLMs themselves or any other models, is a\nnatural choice. However, such baseline text just requires minor modi\ufb01cations, and thus cannot well\ntrigger the deliberation ability of LLMs. Moreover, we \ufb01nd that the lower the similarity between\nthe candidate and the reference, the better the quality of the generated text. As shown in Figure 3,\nwe select sentences that have various similarities with the reference (using edit distance) as the\nsynthesized sentence, and the performance decreases monotonically in general when the similarity\nincreases. Therefore, we seek to choose a sentence that does not contain any correct information as\nthe synthesized text. Potential candidates include a randomly sampled sentence or more extremely,\nan empty string, i.e., setting \u201c[SYS]\u201d as \u201c \u201d. Both choices successfully facilitate deliberation and\nconsistently improve the outcomes across multiple text generation tasks. We use an empty string in\nour experiments as it is more general and elegant.\nDTG has the following exceptional properties to steer LLMs on various text generation tasks:\n\u2022 Simple: The \ufb01nal results can be obtained through a single-step inference of the LLM, without any\nadditional resources or costs.\n\u2022 General: It can be effortlessly applied to a broad range of text generation tasks only with minimal\nadjustments in the prompt.\n4\nDatasets and Evaluation\nIn experiments, we are devoted to evaluating the generation ability of LLMs and the proposed DTG\nprompting. We select 7 representative generation tasks, including machine translation, abstractive\nsummarization, dialogue summarization, text simpli\ufb01cation, style transfer, paraphrase and common-\nsense generation.\nMachine Translation\nFor the machine translation task, we aligned with Hendy et al. [13]\u2019s work\nand experimented on both high-resource and low-resource scenarios. For the high-resource setting,\nwe include German, Czech, Chinese, Japanese, Russian, and Ukrainian paired with English. In the\nlow-resource context, we examine Icelandic and Hausa. The performance is evaluated in terms of\nSacreBLEU3 [31], ChrF, TER (translation error rate) and COMET-22 [35].\nAbstractive Summarization\nWe also evaluate LLM\u2019s ability to process long sequence on CNN-\nDailyMail and Gigaword, two widely used abstractive summarization datasets. The evaluation met-\nric is F1-Rouge [22], consisting of Rouge-1, Rouge-2 and Rouge-L.\nDialog Summarization\nDialogue summarization presents greater challenges than traditional text\nsummarization due to the intricate conversation contexts that models need to comprehend, though\ntheir contexts are relatively shorter. This attribute enables us to test few-shot abilities due to the\nrestricted input length. To investigate this, we select SamSum4 [10] and DialogSum5 [5], two bench-\nmark datasets for dialogue summarization. The evaluation metric is the same as abstractive summa-\nrization.\nText Simpli\ufb01cation\nThe purpose of text simpli\ufb01cation is to revise complex text into sequences\nwith simpli\ufb01ed grammar and word choice. In this work, we mainly report the performance on\ntwo benchmarks, namely Asset [2] and Wiki-auto [15]. Asset is a multi-reference dataset for the\nevaluation of sentence simpli\ufb01cation in English. The dataset uses the same 2,359 sentences from\nTurkCorpus [48] and each sentence is associated with 10 crowdsourced simpli\ufb01cations. Similarly,\neach test set in Wiki-auto owns 8 references. We use SacreBLEU and BLEURT as the metric.\n3BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.2.3.1\n4https://huggingface.co/datasets/samsum\n5https://github.com/cylnlp/DialogSum\n4\nTable 1: Evaluation results of GPT and GPT4 on six high-resource and two-low resource machine\ntranslation tasks from WMT Testsets. The best scores across different systems are marked in bold.\nSystem\nCOMET-22\u2191\nTER\u2193\nChrF\u2191\nBLEU\u2191\nCOMET-22\u2191\nTER\u2193\nChrF\u2191\nBLEU\u2191\nDE-EN\nZH-EN\nWMT-Best\u2020\n85.0\n51.5\n58.5\n33.4\n81.0\n54.7\n61.1\n33.5\nMS-Translator\u2020\n84.7\n51.2\n58.5\n33.5\n80.4\n60.0\n57.7\n27.9\nGPT 1-shot\n84.7\n53.7\n56.2\n30.4\n81.0\n64.4\n54.9\n23.7\n+ DTG\n85.0\n52.4\n57.7\n32.3\n81.4\n63.6\n56.2\n25.3\nGPT 5-shot\n85.3\n52.3\n57.6\n32.3\n81.1\n63.7\n54.6\n23.6\n+ DTG\n85.4\n51.9\n58.2\n33.2\n81.7\n62.4\n55.9\n25.2\nGPT4 1-shot\n85.6\n51.7\n58.9\n33.5\n82.4\n62.8\n57.3\n26.0\n+ DTG\n85.8\n51.8\n58.8\n33.4\n82.9\n62.0\n57.3\n25.7\nCS-EN\nRU-EN\nWMT-Best\u2020\n89.0\n26.8\n79.3\n64.2\n86.0\n43.8\n68.9\n45.1\nMS-Translator\u2020\n87.4\n34.5\n74.0\n54.9\n85.2\n45.1\n68.3\n43.9\nGPT 1-shot\n86.2\n43.7\n67.5\n44.8\n84.8\n48.2\n65.3\n39.7\n+ DTG\n86.7\n42.6\n68.8\n45.6\n85.0\n48.3\n66.1\n40.0\nGPT 5-shot\n86.9\n40.7\n69.2\n47.2\n84.9\n48.0\n65.2\n39.9\n+ DTG\n87.0\n40.9\n69.6\n47.4\n85.1\n47.7\n66.2\n40.3\nGPT4 1-shot\n87.3\n40.3\n70.9\n48.1\n86.1\n45.2\n68.5\n43.1\n+ DTG\n87.3\n39.8\n70.9\n48.9\n86.3\n45.1\n68.5\n43.1\nJA-EN\nUK-EN\nWMT-Best\u2020\n81.6\n69.4\n49.8\n24.8\n86.0\n42.7\n67.3\n44.6\nMS-Translator\u2020\n81.5\n69.0\n49.6\n24.5\n83.5\n45.7\n65.3\n42.4\nGPT 1-shot\n81.3\n74.4\n47.9\n21.5\n83.5\n50.5\n61.1\n36.8\n+ DTG\n81.7\n74.6\n47.9\n21.4\n84.0\n49.9\n61.7\n37.1\nGPT 5-shot\n81.2\n74.2\n47.0\n20.5\n84.0\n49.2\n61.9\n38.0\n+ DTG\n82.2\n72.6\n48.2\n22.4\n84.2\n48.4\n62.6\n39.0\nGPT4 1-shot\n83.4\n69.6\n51.1\n24.7\n85.7\n46.9\n65.2\n39.9\n+ DTG\n83.6\n69.5\n51.1\n24.8\n85.7\n47.1\n65.2\n39.9\nIS-EN\nHA-EN\nWMT-Best\u2020\n87.0\n44.8\n62.3\n41.7\n80.0\n69.0\n48.7\n21.0\nMS-Translator\u2020\n85.9\n45.2\n62.8\n40.5\n73.3\n73.4\n43.4\n16.2\nGPT 1-shot\n83.5\n52.7\n57.0\n33.6\n78.0\n72.8\n47.3\n18.6\n+ DTG\n84.0\n51.7\n58.3\n35.2\n78.3\n74.8\n48.0\n18.6\nGPT 5-shot\n84.1\n50.6\n58.0\n35.0\n78.3\n72.2\n47.6\n18.8\n+ DTG\n84.6\n50.2\n58.8\n36.0\n78.6\n71.9\n48.0\n19.2\nGPT4 1-shot\n86.9\n47.0\n63.8\n39.9\n77.5\n75.7\n47.8\n18.3\n+ DTG\n87.0\n46.7\n63.9\n40.3\n77.9\n75.1\n47.9\n18.7\nStyle Transfer\nWe used three widely-used English transfer learning datasets, namely Gram-\nmalry\u2019s Yahoo Answers Formality Corpus (GYAFC), Amazon and Yelp reviews. The GYAFC\ndataset [34] was originally a question-and-answer dataset on an online forum, consisting of informal\nand formal sentences from the two categories: Entertainment & Music (EM) and Family & Relation-\nships (FR). Both FR and EM provide 4 references to evaluate the \ufb01delity. The Amazon dataset is a\nproduct review dataset, labeled as either a positive or negative sentiment. Similarly, the Yelp dataset\nis a restaurant and business review dataset with positive and negative sentiments. Both Amazon and\nYelp are single-reference. The evaluation metrics contain BLEU and BLEURT [39].\nParaphrase\nWe endeavor to evaluate the paraphrase ability of LLMs upon the well-known Quora\nQuestion Pairs (QQP) dataset, which requires generating an alternative surface form in the same\nlanguage expressing the same semantic content. We utilize the preprocessed data from [11].\nCommon Sense Generation\nWe choose CommonGen [21], a novel constrained generation task\nthat requires models to generate a coherent sentence with the providing key concepts.\nWe summarize the details of each dataset for each task, including the test sets, the selection of\ndemonstrations (mostly from validation sets) and the corresponding prompts we have used. For\nmore details please refer to the attached Appendix.\n5\nTable 2: Experimental results on four summarization tasks.\nSystem\nCNN/DailyMail\nGigaWord\nSamSum\nDialogSum\nR1\nR2\nRL\nR1\nR2\nRL\nR1\nR2\nRL\nR1\nR2\nRL\nTransformer [42] 40.47 17.73 37.29 37.57 18.90 34.69 37.20 10.86 34.69 35.91\n8.74\n33.50\nBART [19]\n44.16 21.28 40.90 39.29 20.09 35.65 53.12 27.95 49.15 47.28 21.18 44.83\nUniLMv2 [3]\n43.16 20.42 40.14\n-\n-\n-\n50.53 26.62 48.81 47.04 21.13 45.04\nGPT 1-shot\n38.87 15.36 35.11 31.24 11.61 27.99 44.52 19.92 39.60 36.84 14.23 32.20\n+ DTG\n40.17 15.60 36.04 31.50 12.00 28.50 45.50 20.58 40.13 39.01 15.50 34.13\nGPT 5-shot\n-\n-\n-\n33.04 12.78 29.86 46.44 20.69 41.10 40.86 17.10 35.78\n+ DTG\n-\n-\n-\n33.54 13.63 30.36 48.72 23.16 43.23 42.64 18.12 37.57\nGPT 10-shot\n-\n-\n-\n33.24 13.26 30.46 47.37 22.08 42.20 41.28 17.48 36.69\n+ DTG\n-\n-\n-\n34.02 14.21 31.04 50.48 24.88 45.31 45.11 19.50 39.71\n5\nExperiments\nIn this section, we assess the ef\ufb01cacy of the text-davinci-003(also known as GPT3.5, which is de-\nnoted as GPT in the following for simplicity) across 7 sequence generation tasks, including machine\ntranslation, abstractive summarization, dialogue summarization, text simpli\ufb01cation, style transfer,\ncommonsense generation and paraphrase. The chosen baseline comparisons consist of 1-shot, and\nfew-shot (mostly 5-shot) learning scenarios. It is worth mentioning that while the performance of\nGPT models on machine translation has been extensively investigated in previous research, other\ngeneration tasks (e.g., text simpli\ufb01cation and style transfer) have received comparatively limited at-\ntention. To demonstrate the versatility of DTG method and address the primary limitation of GPT3.5,\nwe conduct further experiments with GPT4, a cutting-edge LLM API. Due to the considerable com-\nputational cost and API request constraints associated with the GPT4, it is challenging to perform\nextensive experiments. In the current manuscript, we only report the results on machine translation\nand text simpli\ufb01cation. We aim to highlight the signi\ufb01cant potential of GPT models to excel in\ndownstream tasks without the necessity for \ufb01ne-tuning.\n5.1\nResults on Machine Translation\nWe compare the performance of GPT standard prompting and our deliberate then generate method\n(DTG) with that of a commercial system (Microsoft Translator) in addition to WMT SoTA systems.\nTable 1 presents the results in both 1-shot and 5-shot scenarios. Without meticulous parameter\ntuning, we set the temperature to 0 and top_p to 1 when calling the API. The \ufb01ndings here indicate\nthat our re-implementation aligns with the trends observed in previous study [13], that 5-shot beats 1-\nshot in most language pairs. Bene\ufb01ting from the deliberation, DTG effectively pushes the boundaries\nand leads to enhanced results across all to-English language pairs in both 1-shot and 5-shot settings\nbased on GPT3.5 model. For instance, DTG method exhibits substantial BLEU score increases\nin DE-EN, ZH-EN, and UK-EN language pairs in 5 shot scenarios. More concretely, DTG even\nbeats WMT-Best system in terms of COMET-22, which is a more recognized metric recently in\nthe machine translation literature. Moreover, the consistent improvements on IS-EN and HA-EN\ndemonstrate the effectiveness of DTG in low-resource settings.\nWe only conduct experiments on 1-shot scenario due to the limited access, and leave the remained 5-\nshot explorations as future work. We observe GPT4 1-shot can beat GPT3.5 5-shot by a large margin\nin most language pairs. Meanwhile, DTG is still effective on GPT4. This \ufb01nding demonstrates much\nstronger LLMs can still bene\ufb01t from deliberation.\n5.2\nResults on Summarization\nFor abstractive summarization, we mainly evaluate GPT models on CNN/DailyMail and GigaWord,\ntwo of the most widely-used summarization tasks. Due to the limit of max length for GPT models\n(4097) and the long input length of CNN/DailyMail, we only evaluate the performance in 1-shot\nscenario. As shown in Table 2, GPT models show comparative performance with Transformer which\nis specially tuned on the downstream training set. Our DTG can also shown further improvement in\nterms of three Rouge metrics, which demonstrate the effectiveness of DTG on long-term modeling\ntask. However, DTG still falls lag behind of large-scale pretrained models, such as BART [19] and\nUniLMv2 [3] in automatic evaluations. We will add more human alignment judgment in Section 6.\n6\nTable 4: Comparisons of 1-shot and 5-shot on four style transfer tasks, including Entertainment\nMusic, Family Relationships, Amazon and Yelp. \u2020denotes results borrowed from [17].\nSystem\nGYAFC & EM\nGYAFC & FR\nAmazon\nYelp\nBLEU BLEURT BLEU BLEURT BLEU BLEURT BLEU BLEURT\nTransformer\u2020[42]\n40.3\n-\n47.7\n-\n-\n-\n-\n-\nBART\u2020[19]\n76.9\n75.38\n79.3\n75.11\n-\n-\n-\n-\nGPT 1-shot\n52.9\n73.42\n44.6\n70.73\n36.1\n64.56\n30.9\n64.03\n+ DTG\n66.8\n75.20\n65.9\n74.60\n35.4\n63.60\n31.3\n64.19\nGPT 5-shot\n61.3\n75.40\n63.9\n74.35\n39.3\n64.76\n31.4\n64.16\n+ DTG\n69.9\n76.36\n74.1\n75.43\n40.9\n65.42\n32.2\n64.87\nDialogue generation represents a critical aspect of language tasks. In this context, we further cor-\nroborate the ef\ufb01cacy of Large Language Models (LLMs) in dialogue summarization, a composite\ntask encapsulating elements of both dialogue and summarization. It is important to note that the\nresults for DialogSum are averaged over three individual scores, each calculated using unique refer-\nences spanning a range of topics. As observed, GPT 1-shot achieves commendable results compared\nto constrained systems, e.g., Transformer. Furthermore, DTG substantially incites GPT models to\ngenerate more precise summaries derived from extensive multi-turn dialogues. An upward trend\nin performance is observed with the introduction of additional demonstrations, further underscor-\ning the effectiveness of the DTG method. Nonetheless, in the absence of specialized \ufb01ne-tuning,\nthe GPT3.5 model falls short of surpassing the performance of BART. Despite this, the model\u2019s\nperformance remains notably impressive, highlighting the potential of LLMs in complex language\ngeneration tasks.\n5.3\nResults on Style Transfer\nTable 4 displays performance across style transfer tasks from the GYAFC dataset: Entertainment\nMusic (EM) and Family Relationships (FR), both involving informal to formal transformations. Ev-\nidently, the Deliberate then Generate (DTG) method prompts the GPT model to correct inaccuracies\nand generate more precise informal sentences. Speci\ufb01cally, DTG achieves an 8-point and 10.04-\npoint increase in BLEU score for EM and FR tasks, respectively, compared to standard prompting.\nAlthough DTG trails BART [19] in BLEU scores, it surpasses BART in BLEURT scores, registering\ngains of 0.98 and 0.32 for EM and FR tasks, respectively. These results highlight the potential of\nLLMs and our DTG method in style transfer tasks.\n5.4\nResults on Text Simpli\ufb01cation\nTable 3: Comparisons of 1-shot, 5-shot with and without\nour DTG method on two text simpli\ufb01cation tasks.\nSystem\nAsset\nWiki-auto\nBLEU\nSARI\nBLEU\nSARI\nMUSS [25]\n72.9\n44.15\n-\n42.59\nControl Pre\ufb01x [6]\n-\n43.58\n-\n-\nTST-Final [27]\n-\n41.46\n-\n-\nGPT 1-shot\n67.6\n46.12\n65.0\n44.97\n+ DTG\n72.9\n47.23\n72.0\n47.15\nGPT 5-shot\n73.3\n45.95\n70.0\n45.12\n+ DTG\n80.2\n47.05\n80.0\n47.54\nGPT4 5-shot\n68.0\n47.10\n65.1\n45.96\n+ DTG\n74.9\n47.67\n67.9\n47.03\nExperiments were conducted on two\ntext simpli\ufb01cation benchmarks, Asset\nand Wiki-Auto, where the primary goal\nof which is to create a simpli\ufb01ed rendi-\ntion of the given text input. The main\nevaluation metric is the SARI score.\nOur \ufb01ndings illustrate that GPT mod-\nels demonstrate robust performance\nacross both simpli\ufb01cation benchmarks,\neven surpassing the existing state-of-\nthe-art models (MUSS) built based on\nBART. Furthermore, the incorporation\nof DTG method signi\ufb01cantly enhances\nGPT model performance, leading to improvements in both BLEU and SARI scores. Speci\ufb01cally,\nDTG establishes a new benchmark for state-of-the-art results on these two simpli\ufb01cation tasks.\nWe also observe similar competitiveness of DTG on the two other style transfer benchmarks. It\noutperforms the standard prompting method with identical con\ufb01gurations in terms of both BLEU\nand BLEURT scores, further attesting to its ef\ufb01cacy. Again, GPT4 is superior to GPT3.5 and DTG\nalso works at this time, though the obtained improvement is slightly marginal than that of GPT3.5.\n7\nTable 5: Results on the CommonGen\nbenchmark.\nModel\nBLEU-3/4 Rouge-2/L\nBART [19]\n36.3/26.4 22.23/41.98\nT5-Large [33] 39.0/28.6 22.01/42.97\nGPT 5-shot\n39.7/30.0 25.28/46.55\n+ DTG\n43.2/33.5 27.02/48.47\nResults on Commonsense Generation\nTable 5 summa-\nrizes the comparison between GPT models with and with-\nout DTG method on an open Commonsense generation\nbenchmark. This task is more \ufb02exible than the aforemen-\ntioned, meanwhile raising the evaluation dif\ufb01culty.\nWe\nsee that GPT models with standard prompting even sur-\npasses large-scale pretrained generation models, such as\nBART [18] and T5 [33].\nOur DTG achieves further\nimprovements in terms of BLEU-3/BLEU-4 and Rouge-\n2/Rouge-L, resulting in an average of 3.50 BLEU scores and almost 2.00 Rouge score improvements.\nThis also establishes a new SoTA on this benchmark.\nResults on Paraphrase\nFigure 4 delineates the BLEU and Rouge-L scores for GPT and DTG in\nrelation to various few-shot scenarios. In our preliminary experiments, we \ufb01nd that only 5-shot\ndemonstrations cannot enable LLMs to clearly capture the underline mapping rule between the\nsource and the target. To this end, we test LLMs on 20-shot and 50-shot and observe intriguing\nphenomenon.\n1\n5\n20 50\n10\n15\n20\n25\n30\nBLEU (%)\n1\n5\n20 50\n35\n40\n45\n50\n55\n60\nRouge-L (%)\nGPT\nDTG\nFigure 4: BLEU and Rouge-L scores against\nthe number of demonstrations.\nAcross all scenarios, DTG outperforms GPT mod-\nels in terms of both BLEU and Rouge-L metrics.\nHowever, when the number of demonstrations is re-\nstricted, e.g., 1-shot and 5-shot, LLMs noticeably\ntrail behind state-of-the-art systems. Interestingly,\na signi\ufb01cant enhancement in DTG performance is\nobserved with the increase in the number of demon-\nstrations. This improvement can be attributed to the\nmodel\u2019s enhanced ability to comprehend the under-\nlying mapping rules between the source and target, a\ncapability that intensi\ufb01es with an expanded demon-\nstration set.\n6\nAnalysis\nIn this section, we delve into a series of intriguing questions to elucidate the circumstances and\nreasons underpinning the robust performance of DTG. Unless speci\ufb01ed otherwise, the base engine\nutilized throughout this investigation is text-davinci-003.\nTable 6: Ablations on DTG prompting.\nModel\nBLEU COMET\nGPT 5-shot\n23.6\n81.12\n+ DTG\n25.2\n81.70\n+ w/o error detection\n23.3\n81.05\n+ wrong error type\n25.3\n81.74\n+ \ufb01xed error type\n24.1\n81.35\n+ correct candidate\n23.0\n81.17\nAblation Study\nPrior research [49, 43, 1] under-\nscores the signi\ufb01cant impact of both the quality and\nquantity of demonstrations on the performance of\nLLMs. Thus, it becomes essential to discern whether\nthe improvements observed are attributable to modi\ufb01ca-\ntions in the template or the deliberate capability inher-\nent to the LLMs. To this end, we conduct experiments\non WMT ZH-EN and show the comparisons in Table\n6. Firstly, eliminating the phrase \u201cPlease detect the er-\nror type \ufb01rstly, and re\ufb01ne the translation then\u201d, denoted\nas \u201cw/o error detection\u201d in Table 6, DTG experiences a signi\ufb01cant decrement in BLEU score, sug-\ngesting that the excised segment may contain crucial triggers stimulating the deliberate capability\nof the LLM. Along this line, we make two explorations: 1) replacing \u201cincorrect translation\u201d by\n\u201cgood/correct translation\u201d in the demonstration only, resulting in no BLEU degradation, which is\ndenoted as \u201cwrong error type\u201d in Table 6. This reveals that LLMs can rethink by themselves and\nmake \u201ccorrect\u201d decisions though the demonstration is incorrect. 2) using \ufb01xed error type, e.g., un-\nder translation in the LLM response. This leads to a 1.1 BLEU drop, indicating that restricting\nthe thought of LLMs would hinder the performance. Moreover, we observe that adopting the cor-\nrect candidate generated by itself cannot bring further improvements than standard prompting. A\nplausible explanation for this observation could be that GPT3.5 might lack the necessary ability to\naccurately identify and concisely correct the parts of text requiring modi\ufb01cation.\n8\n3\n7\n0\n25\n50\n75\nGPT3.5\nGPT4\nRate (%)\nWMT De-En\n3\n7\n0\n25\n50\n75\nGPT3.5\nGPT4\nCNN/DailyMail\n3\n7\n0\n25\n50\n75\nGPT3.5\nGPT4\nAsset\n3\n7\n0\n25\n50\n75\nGPT3.5\nGPT4\nEM\nBest\nDTG\nFigure 5: GPT3.5 and GPT4 evaluation on 4 generation tasks. Note that we random select 500\nsamples due to the limitation of GPT4 access.\nTable 7: Case study on re\ufb01ning from the previous candidate (Re\ufb01ne) and the proposed DTG method.\nSource\n\u5473\u9053\u8d5e\uff0c\u8089\u7c7b\u597d\uff0c\u670d\u52a1\u70ed\u60c5\nReference\nNice taste, great meat, enthusiastic service.\nGPT 1-shot\nThe taste is great, the meat is good, and the service is enthusiastic.\n+ Re\ufb01ne\nThe \ufb02avors are amazing, the meat is excellent, and the service is warm and welcoming.\n+ DTG\nGreat taste, good meat, enthusiastic service.\nSource\n\u76ee\u524d\u5df2\u7ecf\u8d2d\u4e70\u4e86\u8fd9\u4e2a\u7cfb\u52173\u6b3e\u673a\u5668\uff01\nReference\nI have bought three laptops of this series!\nGPT 1-shot\nSo far, 3 machines from this series have been purchased!\n+ Re\ufb01ne\nUp until now, 3 machines from this series have been purchased!\n+ DTG\nI have already purchased 3 models from this series!\nEvaluation by GPT Models\nAs previously discussed, despite DTG\u2019s impressive performance, it\nfalls short of BART in some scenarios\u2014most notably, it exhibits a signi\ufb01cant gap in terms of Rouge\nscores in summarization tasks. However, Liu et al. [23] suggested that Rouge may not accurately\nrepresent the true performance of summarization tasks, given its poor alignment with human evalua-\ntions. In contrast, GPT models achieve optimal alignment with human justi\ufb01cation and substantially\noutperform all previous state-of-the-art evaluators on the SummEval benchmark. This observation\nprompts an investigation into whether the generation output by DTG can surpass that of BART.\nFollowing their suggestion, we conduct reference-based evaluation and design a prompt as shown\nin Figure 5. We extract 500 test sets and compared DTG with the best result using GPT3.5 and\nGPT4 to select a better candidate. We see that DTG signi\ufb01cantly beats the best system within GPT\nevaluation, except for the style transfer dataset.\nDE ZH RU UK\n0\n5\n10\nError rate (%)\nGPT\nDTG\nDE ZH RU UK\n10\n20\n30\n40\nError rate (%)\nFigure 6: Statistics of error rate for\nunder translation (above) and en-\ntity translation (below).\nError Statistical Analysis\nTo evaluate whether the pro-\nposed DTG prompting can facilitate error avoidance in GPT,\nwe conduct error statistics on machine translation, where two\nfrequently occurring error types are considered (i.e., under\ntranslation and incorrect entity translation) [12]. Figure 6 pro-\nvides a comparison of the error rates between GPT models\nwith and without the application of the DTG method. It is ob-\nvious to see that DTG reduces both error rates compared with\nthe direct generation manner.\nCase Study\nWe provide a case study based on GPT4 model\nin Table 7, where \u201cRe\ufb01ne\u201d indicates utilizing the 5-shot base-\nline results as the synthesized sentences, i.e., \u201c[INCORRECT\nSYS]\u201d in Figure 1, and DTG is our method that uses an empty\nstring instead. The conclusions are two-fold. 1) Using the\nbaseline results will cause the model to avoid generating the\nsame segmentations in it although they may be correct already,\ne.g., \u201ctaste\u201d to \u201c\ufb02avors\u201d, \u201cso far\u201d to \u201cup until now\u201d, as well as others in red. As a result, the \ufb02uency\nand accuracy of the \ufb01nal results may be affected. 2) Equipped with DTG, \ufb02uency, coherence and\ngrammatical correctness of generated results are all promoted. In the \ufb01rst case, the DTG result is\n9\nmore faithful not only in semantics but also in structure than the baseline. In the second case, DTG\nis able to complete the subject \u201cI\u201d which does not appear in the source sentence.\n0\n0.2 0.4 0.6 0.8\n1\n80.0\n80.5\n81.0\n81.5\n82.0\n81.70\n81.65\nCOMET (%)\nDTG\nMS-Translator\nFigure 7: COMET v.s. word drop rate\nof MS-Translator candidate.\nDTG Can Serve as A Good Re\ufb01ner\nTo investigate\nthe correlation between the performance of DTG and the\nprovided candidate, we consider the translation task on\nWMT ZH-EN and create candidates with varying qual-\nity by randomly removing certain words from the trans-\nlations generated by MS-Translator. Figure 7 displays\nthe performance measured by COMET versus the word\ndrop rate. The blue line represents the performance of\nMS-Translator, and the red line represents DTG with var-\nious candidates. Upon deliberation, GPT can improve\nthe translation of MS-Translator from 80.4 to 81.65 in\nCOMET. As aforementioned that DTG suffers from per-\nformance degradation when the candidate is a correct one\ngenerated by itself (See the last line in Table 6). However, upon deeper investigation, we discern that\nselecting candidates from systems other than GPT itself is a superior choice. This underscores the\neffectiveness of our DTG framework, demonstrating its capability to work even with high-quality\ncandidates generated by other systems. Moreover, the performance declines when more words are\ndropped from the MS-Translator candidate, but interestingly, it increases when the candidate almost\nresembles an empty string. Though with additional high-quality systems, DTG also successfully\nimproves the performance, using an empty string as a candidate can always lead to a better outcome\nwithout any additional resources and cost, as well as speci\ufb01c demonstration construction.\n7\nConclusions\nIn this paper, we propose DTG prompting, which encourages LLMs to deliberate before generating\nthe \ufb01nal results by letting the model detect the error type on a synthetic text that may contain errors.\nUsing an empty string as the synthetic text successfully gets rid of an extra baseline system and\nimproves the quality of the generated text. The DTG prompting can be easily applied to various\ntext generation tasks with minimal adjustments in the prompt. Extensive experiments conducted on\nover 20 datasets across 7 text generation tasks demonstrate the effectiveness and broad applicability\nof the DTG prompting framework. One potential avenue for further enhancing the ef\ufb01cacy of DTG\nprompting involves leveraging task-speci\ufb01c domain knowledge. (e.g., explicitly listing the potential\nerror types in the prompts to provide guidance for deliberation), which is worth future investigation.\nReferences\n[1] Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad.\nIn-context examples selection for machine translation. arXiv preprint arXiv:2212.02437, 2022.\n[2] Fernando Alva-Manchego, Louis Martin, Antoine Bordes, Carolina Scarton, Beno\u00eet Sagot,\nand Lucia Specia. ASSET: A dataset for tuning and evaluation of sentence simpli\ufb01cation\nmodels with multiple rewriting transformations. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics, pages 4668\u20134679, Online, July 2020.\nAssociation for Computational Linguistics.\ndoi: 10.18653/v1/2020.acl-main.424.\nURL\nhttps://aclanthology.org/2020.acl-main.424.\n[3] Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Jian-\nfeng Gao, Songhao Piao, Ming Zhou, and Hsiao-Wuen Hon. Unilmv2: Pseudo-masked lan-\nguage models for uni\ufb01ed language model pre-training. In Proceedings of the 37th Interna-\ntional Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, vol-\nume 119 of Proceedings of Machine Learning Research, pages 642\u2013652. PMLR, 2020. URL\nhttp://proceedings.mlr.press/v119/bao20a.html.\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language mod-\n10\nels are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901,\n2020.\n[5] Yulong Chen, Yang Liu, Liang Chen, and Yue Zhang.\nDialogSum:\nA real-life sce-\nnario dialogue summarization dataset.\nIn Findings of the Association for Computa-\ntional Linguistics:\nACL-IJCNLP 2021, pages 5062\u20135074, Online, August 2021. Asso-\nciation for Computational Linguistics.\ndoi:\n10.18653/v1/2021.\ufb01ndings-acl.449.\nURL\nhttps://aclanthology.org/2021.findings-acl.449.\n[6] Jordan Clive,\nKris Cao,\nand Marek Rei.\nControl pre\ufb01xes for parameter-ef\ufb01cient\ntext generation.\nIn Proceedings of the 2nd Workshop on Natural Language Gen-\neration, Evaluation, and Metrics (GEM), pages 363\u2013382, Abu Dhabi, United Arab\nEmirates (Hybrid), December 2022. Association for Computational Linguistics.\nURL\nhttps://aclanthology.org/2022.gem-1.31.\n[7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,\n2018.\n[8] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-\nshot learners. arXiv preprint arXiv:2012.15723, 2020.\n[9] Marjan Ghazvininejad, Hila Gonen, and Luke Zettlemoyer.\nDictionary-based phrase-level\nprompting of large language models for machine translation. arXiv preprint arXiv:2302.07856,\n2023.\n[10] Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer.\nSAMSum corpus:\nA human-annotated dialogue dataset for abstractive summarization. In Proceedings of the\n2nd Workshop on New Frontiers in Summarization, pages 70\u201379, Hong Kong, China, Novem-\nber 2019. Association for Computational Linguistics.\ndoi: 10.18653/v1/D19-5409. URL\nhttps://aclanthology.org/D19-5409.\n[11] Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, and LingPeng Kong. Diffuseq: Se-\nquence to sequence text generation with diffusion models. arXiv preprint arXiv:2210.08933,\n2022.\n[12] Hany Hassan, Anthony Aue, Chang Chen, Vishal Chowdhary, Jonathan Clark, Christian Feder-\nmann, Xuedong Huang, Marcin Junczys-Dowmunt, William Lewis, Mu Li, et al. Achieving hu-\nman parity on automatic chinese to english news translation. arXiv preprint arXiv:1803.05567,\n2018.\n[13] Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu\nMatsushita, Young Jin Kim, Mohamed A\ufb01fy, and Hany Hassan Awadalla. How good are gpt\nmodels at machine translation? a comprehensive evaluation. arXiv preprint arXiv:2302.09210,\n2023.\n[14] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nLu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021.\n[15] Chao Jiang, Mounica Maddela, Wuwei Lan, Yang Zhong, and Wei Xu. Neural CRF model\nfor sentence alignment in text simpli\ufb01cation.\nIn Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics, pages 7943\u20137960, Online, July 2020.\nAssociation for Computational Linguistics.\ndoi: 10.18653/v1/2020.acl-main.709.\nURL\nhttps://aclanthology.org/2020.acl-main.709.\n[16] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.\nLarge language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.\n[17] Huiyuan Lai, Antonio Toral, and Malvina Nissim. Thank you BART! rewarding pre-trained\nmodels improves formality style transfer.\nIn Proceedings of the 59th Annual Meeting of\nthe Association for Computational Linguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 2: Short Papers), pages 484\u2013494, Online, August\n11\n2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-short.62. URL\nhttps://aclanthology.org/2021.acl-short.62.\n[18] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,\nOmer Levy, Ves Stoyanov, and Luke Zettlemoyer.\nBart: Denoising sequence-to-sequence\npre-training for natural language generation, translation, and comprehension. arXiv preprint\narXiv:1910.13461, 2019.\n[19] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,\nOmer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence\npre-training for natural language generation, translation, and comprehension. In Proceedings\nof the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871\u2013\n7880, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.\nacl-main.703. URL https://aclanthology.org/2020.acl-main.703.\n[20] Xiang Lisa Li and Percy Liang. Pre\ufb01x-tuning: Optimizing continuous prompts for generation.\narXiv preprint arXiv:2101.00190, 2021.\n[21] Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula,\nYejin Choi, and Xiang Ren.\nCommonGen:\nA constrained text generation challenge\nfor generative commonsense reasoning.\nIn Findings of the Association for Computa-\ntional Linguistics:\nEMNLP 2020, pages 1823\u20131840, Online, November 2020. Associa-\ntion for Computational Linguistics.\ndoi:\n10.18653/v1/2020.\ufb01ndings-emnlp.165.\nURL\nhttps://aclanthology.org/2020.findings-emnlp.165.\n[22] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74\u201381, Barcelona, Spain, July 2004. Association for Computational\nLinguistics. URL https://aclanthology.org/W04-1013.\n[23] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. Gpteval:\nNlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634,\n2023.\n[24] Gary F Marcus. Negative evidence in language acquisition. Cognition, 46(1):53\u201385, 1993.\n[25] Louis Martin, Angela Fan, \u00c9ric de la Clergerie, Antoine Bordes, and Beno\u00eet Sagot.\nMUSS: Multilingual unsupervised sentence simpli\ufb01cation by mining paraphrases.\nIn Pro-\nceedings of the Thirteenth Language Resources and Evaluation Conference, pages 1651\u2013\n1664, Marseille, France, June 2022. European Language Resources Association.\nURL\nhttps://aclanthology.org/2022.lrec-1.176.\n[26] Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capa-\nbilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375, 2023.\n[27] Kostiantyn Omelianchuk, Vipul Raheja, and Oleksandr Skurzhanskyi. Text Simpli\ufb01cation\nby Tagging. In Proceedings of the 16th Workshop on Innovative Use of NLP for Building\nEducational Applications, pages 11\u201325, Online, April 2021. Association for Computational\nLinguistics. URL https://aclanthology.org/2021.bea-1.2.\n[28] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\n[29] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to\nfollow instructions with human feedback. Advances in Neural Information Processing Systems,\n35:27730\u201327744, 2022.\n[30] Keqin Peng, Liang Ding, Qihuang Zhong, Li Shen, Xuebo Liu, Min Zhang, Yuanxin Ouyang,\nand Dacheng Tao. Towards making the most of chatgpt for machine translation. arXiv preprint\narXiv:2303.13780, 2023.\n[31] Matt Post.\nA call for clarity in reporting BLEU scores.\nIn Proceedings of the\nThird Conference on Machine Translation:\nResearch Papers,\npages 186\u2013191, Bel-\ngium,\nBrussels,\nOctober 2018. Association for Computational Linguistics.\nURL\nhttps://www.aclweb.org/anthology/W18-6319.\n12\n[32] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\nLanguage models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[33] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu.\nExploring the limits of transfer learning with\na uni\ufb01ed text-to-text transformer.\nJ. Mach. Learn. Res., 21:140:1\u2013140:67, 2020.\nURL\nhttp://jmlr.org/papers/v21/20-074.html.\n[34] Sudha Rao and Joel Tetreault. Dear sir or madam, may I introduce the GYAFC dataset: Corpus,\nbenchmarks and metrics for formality style transfer. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers), pages 129\u2013140, New Orleans, Louisiana,\nJune 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1012. URL\nhttps://aclanthology.org/N18-1012.\n[35] Ricardo Rei, Jos\u00e9 G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana C Farinha,\nTaisiya Glushkova, Alon Lavie, Luisa Coheur, and Andr\u00e9 F. T. Martins.\nCOMET-22:\nUnbabel-IST 2022 submission for the metrics shared task.\nIn Proceedings of the Sev-\nenth Conference on Machine Translation (WMT), pages 578\u2013585, Abu Dhabi, United Arab\nEmirates (Hybrid), December 2022. Association for Computational Linguistics.\nURL\nhttps://aclanthology.org/2022.wmt-1.52.\n[36] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai,\nAntoine Chaf\ufb01n, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training\nenables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021.\n[37] Timo Schick and Hinrich Sch\u00fctze. Exploiting cloze questions for few shot text classi\ufb01cation\nand natural language inference. arXiv preprint arXiv:2001.07676, 2020.\n[38] Timo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Luke Zettle-\nmoyer, Nicola Cancedda, and Thomas Scialom.\nToolformer: Language models can teach\nthemselves to use tools. arXiv preprint arXiv:2302.04761, 2023.\n[39] Thibault Sellam, Dipanjan Das, and Ankur P Parikh. Bleurt: Learning robust metrics for text\ngeneration. In Proceedings of ACL, 2020.\n[40] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang.\nHugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint\narXiv:2303.17580, 2023.\n[41] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\nand ef\ufb01cient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proc. of NeurIPS, pages\n5998\u20136008, 2017.\n[43] David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, and George Fos-\nter. Prompting palm for translation: Assessing strategies and performance. arXiv preprint\narXiv:2211.09102, 2022.\n[44] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan\nDu, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv\npreprint arXiv:2109.01652, 2021.\n[45] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny\nZhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint\narXiv:2201.11903, 2022.\n[46] Jason Wei,\nXuezhi Wang,\nDale Schuurmans,\nMaarten Bosma,\nBrian Ichter,\nFei\nXia,\nEd\nH.\nChi,\nQuoc\nV.\nLe,\nand\nDenny\nZhou.\nChain-of-thought\nprompt-\ning\nelicits\nreasoning\nin\nlarge\nlanguage\nmodels.\nIn\nNeurIPS,\n2022.\nURL\nhttp://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Confe\n13\n[47] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan.\nVisual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint\narXiv:2303.04671, 2023.\n[48] Wei\nXu,\nCourtney\nNapoles,\nEllie\nPavlick,\nQuanze\nChen,\nand\nChris\nCallison-\nBurch.\nOptimizing statistical machine translation for text simpli\ufb01cation.\nTrans-\nactions of the Association for Computational Linguistics, 4:401\u2013415, 2016.\nURL\nhttps://cocoxu.github.io/publications/tacl2016-smt-simplification.pdf.\n[49] Biao Zhang, Barry Haddow, and Alexandra Birch. Prompting large language model for ma-\nchine translation: A case study. arXiv preprint arXiv:2301.07069, 2023.\n[50] Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu Li. Progressive-hint prompt-\ning improves reasoning in large language models. arXiv preprint arXiv:2304.09797, 2023.\n[51] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan,\nand Jimmy Ba. Large language models are human-level prompt engineers. arXiv preprint\narXiv:2211.01910, 2022.\n14\nDialogue Summarization\nGiven the English dialogue: [SRC]\nthe already generated dialogue\nsummarization is: [SYS]\nPlease detect the error type\nfirstly, and provide the refined\nsummarization then.\nError type: incorrect\nsummarization, the refined\nsummarization is: [TGT]\n...\nParaphrase\nGiven the English sentence: [SRC]\nthe already generated paraphrase\nis: [SYS]\nPlease detect the error type\nfirstly, and provide the refined\nparaphrase then.\nError type: incorrect paraphrase,\nthe refined paraphrase is: [TGT]\n...\nCommonsense Generation\nGiven several key words: [SRC]\nthe already generated sentence\nusing background commonsense\nknowledge is: [SYS]\nPlease detect the error type\nfirstly, and provide the refined\nsentence then.\nError type: incorrect generation,\nthe refined sentence is: [TGT]\n...\nFigure 8: Illustration of DTG demonstration design for dialogue summarization, paraphrase and\ncommonsense generation tasks within minimal modi\ufb01cations.\nTranslation\nGiven the [src] sentence: [SRC]\nthe [tgt] translation of the\nsentence is: [TGT]\nGiven the [src] sentence: [Input]\nthe [tgt] translation of the\nsentence is:\nDialogue Summarization\nGiven the English dialogue: [SRC]\nplease summarize the main context:\n[TGT]\nGiven the English dialogue:\n[Input]\nplease summarize the main context:\nSimpli\ufb01cation\nGiven the English sentence: [SRC]\nthe simplification of the sentence\nis: [TGT]\nGiven the English sentence:\n[Input]\nthe simplification of the sentence\nis:\nStyle Transfer\nGiven the English sentence: [SRC]\nplease transfer the style of the\nsentence into formal: [TGT]\nGiven the English sentence:\n[Input]\nplease transfer the style of the\nsentence into formal:\nParaphrase\nGiven the English sentence: [SRC]\nthe paraphrase of the sentence is:\n[TGT]\nGiven the English sentence:\n[Input]\nthe paraphrase of the sentence is:\nCommonsense Generation\nGiven several key words: [SRC]\nPlease generate a coherent\nsentence using background\ncommonsense knowledge with the\nproviding key words: [TGT]\nGiven several key words: [Input]\nPlease generate a coherent\nsentence using background\ncommonsense knowledge with the\nproviding key words:\nFigure 9: Illustration of the standard GPT prompting involving both demonstration and test input\non six generation tasks, including machine translation, dialogue summarization, text simpli\ufb01cation,\nstyle transfer, paraphrase and commonsense generation.\nA\nLimitation\nDue to restricted access to GPT4, we have evaluated our Deliberate then Generate (DTG) method\non just two generation tasks: machine translation (across 8 language pairs) and simpli\ufb01cation. There\nexists a necessity for more expansive experimentation across other tasks. Additionally, the effective-\nness of DTG is contingent on model capacity. Models such as LLaMa-7B might not fully com-\nprehend the instructions provided, resulting in weaker performance on downstream tasks. In our\nfuture work, we aim to ascertain the required scale of a language model to successfully facilitate\ndeliberative generation.\nOur work inherits the biases from pre-trained language models. For example, we only conduct exper-\niments on English generation that GPT models are most powerful at. We provide results and analysis\non English-to-Others translation in Appendix C. Future works could investigate the performance of\nDTG on multilingual pre-trained models.\n15\nTest\nPrompt template of GPT evaluation\nGiven the [src] sentence: [SRC]\nYour task is to score the following two candidates translated by two systems,\nCandidate1: [sys1] Candidate2: [sys2].\nPlease select the better one in terms of both coherence and fidelity. Note that\nC1 for Candidate1, C2 for Candidate2.\nOutput:\nFigure 10: Illustration of the prompting design of GPT evaluation for Figure 5. We adhere to the\nrecommendation proposed in [23]\u2019s work, implementing a zero-shot GPT evaluation approach to\nidentifying superior candidate translations through the adjudication of LLMs.\nDemonstration\nTest\nDemonstration\nTest\nDemonstration\nTest\n(a) Prompt template of \u201cw/o error detection\u201d\nGiven the [src] sentence: [SRC]\nthe [tgt] translation is: [SYS]\nThe refined [tgt] translation is: [TGT]\nGiven the [src] sentence: [SRC]\nthe [tgt] translation is: [SYS]\nThe refined [tgt] translation is:\n(b) Prompt template of \u201cwrong error type\u201d\nGiven the [src] sentence: [SRC]\nthe [tgt] translation is: [SYS]\nPlease detect the error type firstly, and refine the translation then.\nError type: good/correct translation, the refined [tgt] translation is: [TGT]\nGiven the [src] sentence: [SRC]\nthe [tgt] translation is: [SYS]\nPlease detect the error type firstly, and refine the translation then.\nError type:\n(c) Prompt template of \u201c\ufb01xed error type\u201d\nGiven the [src] sentence: [SRC]\nthe [tgt] translation is: [SYS]\nPlease detect the error type firstly, and refine the translation then.\nError type: under translation, the refined [tgt] translation is: [TGT]\nGiven the [src] sentence: [SRC]\nthe [tgt] translation is: [SYS]\nPlease detect the error type firstly, and refine the translation then.\nError type: under translation, the refined [tgt] translation is:\nFigure 11: Illustration of the prompting design of the ablation study in Table 6. Note that all [SYS]\nhere is empty string. The purpose here is to evaluate the deliberation ability of LLMs.\n16\nTable 8: Evaluation results of GPT on six high-resource and two-low resource machine translation\ntasks from WMT Testsets in from English directions. The best scores are marked in bold.\nSystem\nCOMET-22\u2191\nTER\u2193\nChrF\u2191\nBLEU\u2191\nCOMET-22\u2191\nTER\u2193\nChrF\u2191\nBLEU\u2191\nEN-DE\nEN-ZH\nWMT-Best\u2020\n87.2\n49.9\n64.6\n38.4\n86.7\n102.3\n41.1\n44.8\nMS-Translator\u2020\n86.8\n50.5\n64.2\n37.3\n86.1\n94.2\n43.1\n48.1\nGPT 5-shot\n86.3\n54.6\n61.3\n33.3\n86.7\n97.4\n40.0\n43.7\n+ DTG\n86.3\n54.1\n61.6\n33.4\n86.6\n98.6\n39.4\n43.5\nEN-CS\nEN-RU\nWMT-Best\u2020\n91.9\n43.7\n68.2\n45.8\n89.5\n56.8\n58.3\n32.4\nMS-Translator\u2020\n90.6\n45.7\n65.6\n42.1\n87.4\n56.7\n58.1\n33.1\nGPT 5-shot\n88.9\n54.6\n58.9\n32.7\n87.0\n61.3\n54.4\n28.2\n+ DTG\n88.8\n54.5\n59.0\n32.9\n85.7\n63.0\n52.1\n28.1\nEN-JA\nEN-UK\nWMT-Best\u2020\n89.3\n105.9\n36.8\n27.6\n88.8\n57.5\n59.3\n32.5\nMS-Translator\u2020\n88.0\n106.0\n34.9\n25.1\n86.1\n63.2\n56.1\n28.2\nGPT 5-shot\n88.1\n111.8\n31.0\n21.4\n85.4\n70.2\n50.6\n21.8\n+ DTG\n88.0\n111.8\n31.0\n21.7\n83.8\n71.6\n47.8\n20.8\nEN-IS\nEN-HA\nWMT-Best\u2020\n86.8\n55.0\n59.6\n33.3\n79.8\n65.6\n51.1\n20.1\nMS-Translator\u2020\n84.3\n57.2\n56.8\n28.7\n72.5\n75.6\n38.4\n10.3\nGPT 5-shot\n76.1\n70.8\n44.1\n16.2\n72.8\n87.4\n38.5\n9.9\n+ DTG\n76.7\n70.9\n44.2\n16.3\n73.2\n77.7\n39.3\n10.1\nB\nDesign of Prompts\nFigure 8 presents the DTG demonstration design across the other three text generation tasks. It\ncan be observed that DTG does not necessitate task-speci\ufb01c designs; instead, a clear instruction\noutlining the main task for each work suf\ufb01ces. For the ease of replication of our results, we also\nfurnish all baseline prompts, as depicted in Figure 9. Also, we provide the prompting design for\nGPT evaluation in Figure 10, which follows a zero-shot fashion.\nTo facilitate a more comprehensive understanding of the prompt ablations conducted in Section 6,\nwe provide the corresponding design of prompts in Figure 11. Please note that prompts in blue\nrepresent the pre-designed demonstration, while those in red represent the test input. As observed,\n\ufb01rstly, removing the error detection leads to the prompting in 11 (a). Additionally, the term \u201cwrong\nerror type\u201d implies that we fed an empty string into LLMs, presenting it as a good translation. How-\never, LLMs can autonomously detect the correct error type as an \u201cincorrect translation\u201d and subse-\nquently generate an accurate response following careful deliberation (Figure 11 (b)). Conversely, if\nwe constrain the error type detection process and solely allow LLMs to generate the translation, a\nconsiderable performance gap emerges (See Figure 11 (c)).\nC\nMore Analyses\nResults on Machine Translation from English\nTable 8 summarizes the results of standard\nprompting and our DTG method in 5-shot scenarios, alongside results from WMT-Best and MS-\nTranslator. When compared to results from to-English directional language pairs, such as DE-\nEN, the improvements provided by DTG over the standard prompting strategy appear somewhat\nmarginal. Furthermore, DTG may yield results inferior to standard prompting in EN-ZH and EN-\nUK scenarios. This can likely be ascribed to the disparities in the balance of training sets across\ndifferent languages.\nTable 9: Ablations on DTG prompting in\nterms of different candidates.\n# Model\nBLEU COMET\n1 GPT 5-shot\n23.6\n81.12\n2\n+ DTG\n25.2\n81.70\n3\n+ \ufb01xed incorrect candidate\n25.0\n81.72\n4\n+ irrelevant languages\n25.1\n81.81\n5\n+ correct candidate\n23.0\n81.17\nAblations on Candidates\nIn Section 3, we demon-\nstrated that the empty string serves as a universal and\neffective choice to stimulate LLMs to engage in the\nDeliberate then Generate process, and that a candi-\ndate more distinct from the reference can yield su-\nperior results. In this section, we aim to explore if\nother candidates may also prove effective in this con-\ntext. Here, we take the WMT ZH-EN translation as\n17\nTable 10: Statistics of the dataset we used on over 20 benchmarks. Note that \u201cNum.\u201d represents\nthe number of test sets for each benchmark. \u201cTotal Words\u201d and \u201cAve. Words\u201d denote the total word\ncount and average lengths, respectively. These statistics are based on tokenization sequences.\nDataset\nNum. Total Words Ave. Words Dataset\nNum. Total Words Ave. Words\nWMT DE-EN 1984\n33540\n16.9\nCNN/DailyMail 11490\n9017116\n784.8\nWMT CS-EN\n1448\n26050\n17.9\nGigaWord\n1951\n72171\n37.0\nWMT JA-EN\n2008\n36731\n18.3\nSamSum\n819\n104492\n127.6\nWMT ZH-EN 1875\n14353\n7.7\nDialogSum\n500\n96385\n192.7\nWMT RU-EN 2016\n32992\n16.3\nEM\n1416\n17279\n12.2\nWMT UK-EN 2018\n29273\n14.5\nFR\n1332\n16799\n12.6\nWMT IS-EN\n1000\n19930\n19.9\nAmazon\n500\n6055\n12.1\nWMT HA-EN\n997\n30955\n31.0\nYelp\n500\n5432\n10.9\nCommonGen\n993\n6465\n6.5\nAsset\n359\n8115\n22.6\nQQP\n2500\n27543\n11.0\nWiki-auto\n2000\n43860\n21.9\nan instance. Table 9 shows the comparison of vari-\nous candidate inputs. Speci\ufb01cally, the term \"\ufb01xed incorrect candidate\" (#3) refers to the use of a\n\ufb01xed yet incorrect (irrelevant) English translation as the candidate.6 Likewise, system #4 indicates\nthat the candidates neither belong to the target language nor conform to the correct structure or\ngrammar.7 Interestingly, both 2 systems deliver comparable performance with our default setting,\nwith system #4 even achieving a higher COMET score. However, when shifting to a correct candi-\ndate, LLMs seem to underperform. This observation suggests that LLMs can effectively deliberate\nwhen the candidate is incorrect - whether it is an empty string or other incorrect translations - and\nsubsequently generate a substantially improved translation.\nD\nDetails of Datasets\nIn this section, we offer more detailed statistics concerning the test sets utilized in this study, encom-\npassing 8 machine translation, 4 summarization, 4 style transfer, 2 simpli\ufb01cation, 1 commonsense\ngeneration, and 1 paraphrase benchmarks. Table 10 provides a summary of the number of test sets,\ntotal words, and the average length. We will release the test sets and the corresponding demon-\nstrations in the future. Note that the statistic is conducted based on tokenization sequences, which\nwould be further segmented by BPE before feeding into LLMs. Consequently, the average length of\nsummarization inputs would appear signi\ufb01cantly larger, leading to an elevated risk in the context of\nfew-shot requests.\nE\nDetails of Error Statistical\nIn Figure 6, two types of error are considered (i.e., under translation and entity translation error). In\nthis section, we provide the details of the method to conduct the error statistics.\nUnder Translation\nWe \ufb01rst use awesome-align8 to get the alignment between the source and\ntarget sentences. Then, a word in the source sentence is regarded as under translation, when it is\naligned to a word in the reference target sentence but failed to be aligned in the generated target\nsentence.\nEntity Translation\nWe \ufb01rst use spaCy9 to recognize the named entities in the reference target\nsentence, where person names, organizations and locations are considered. Then, an entity in the\nreference is considered an error if it cannot be found in the generated target sentence.\n6We random sample an English sentence: [SYS]: EBA Education Team together with Accace Ukraine invite\nyou to join the EBA Education Update: Performance Audit.\n7Similarly, we random sample an Ukraine sentence: [SYS]: \u0417 \u0432\u043f\u0435\u0432\u043d\u0435\u043di\u0441\u0442\u044e \u043c\u043e\u0436\u0435\u0442\u0435 \u0434\u043e\u0432i\u0440\u044f\u0442\u0438 \u043d\u0430\u043c i\n\u0431\u0443\u0434\u044c \u043b\u0430\u0441\u043a\u0430, \u0437\u0432\u0435\u0440\u0442\u0430\u0439\u0442\u0435\u0441\u044f \u0434\u043e \u043d\u0430\u0441, \u044f\u043a\u0449\u043e \u0443 \u0432\u0430\u0441 \u0454 \u044f\u043ai-\u043d\u0435\u0431\u0443\u0434\u044c \u043f\u0438\u0442\u0430\u043d\u043d\u044f \u0447\u0438 \u043a\u043e\u043c\u0435\u043d\u0442\u0430\u0440i.\n8https://github.com/neulab/awesome-align\n9https://github.com/explosion/spaCy\n18\n"
  },
  {
    "title": "PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning",
    "link": "https://arxiv.org/pdf/2305.19472.pdf",
    "upvote": "1",
    "text": "(counterfactual)\nPlaSma\nPLASMA\n: Making Small Language Models\nBetter Procedural Knowledge Models for\n(Counterfactual) Planning\nFaeze Brahman 12\nChandra Bhagavatula 1\nValentina Pyatkin 1\u2020\nJena D. Hwang 1\u2020\nXiang Lorraine Li 15\nHirona J. Arai 3\nSoumya Sanyal 3\nKeisuke Sakaguchi 4\nXiang Ren 13\nYejin Choi 12\n1Allen Institute for Artificial Intelligence\n2University of Washington\n3University of Southern California\n4Tohoku University\n5University of Pittsburg\nfaezeb@allenai.org\nAbstract\nProcedural planning, which entails decomposing a high-level goal into a sequence\nof temporally ordered steps, is an important yet intricate task for machines. It\ninvolves integrating common-sense knowledge to reason about complex contextual-\nized situations that are often counterfactual, e.g. \u201cscheduling a doctor\u2019s appointment\nwithout a phone\u201d. While current approaches show encouraging results using large\nlanguage models (LLMs), they are hindered by drawbacks such as costly API calls\nand reproducibility issues. In this paper, we advocate planning using smaller lan-\nguage models. We present PLASMA, a novel two-pronged approach to endow small\nlanguage models with procedural knowledge and (counterfactual) planning capa-\nbilities. More concretely, we develop symbolic procedural knowledge distillation\nto enhance the implicit knowledge in small language models and an inference-time\nalgorithm to facilitate more structured and accurate reasoning. In addition, we\nintroduce a novel task, Counterfactual Planning, that requires a revision of a plan\nto cope with a counterfactual situation. In both the original and counterfactual\nsetting, we show that orders-of-magnitude smaller models (770M-11B parameters)\ncan compete and often surpass their larger teacher models\u2019 capabilities.1\n1\nIntroduction\nPowered by massive scale, large language models (LLMs) excel on many downstream tasks that\nrequire commonsense. One such task is procedural planning [27], a task that involves decomposing\na high-level goal into a sequence of coherent, logical, and goal-oriented steps (plan) (e.g. \u201csee a\nmovie\" \u2192 \u201cLook up movie showings\", \u201cChoose a movie\" . . .). Recent approaches model this task as\na conditional text generation problem using LLMs [23, 11, 1]. Despite their reasonable performance\non the task, their steep computational cost and inaccessibility hinder wider adoption of LLMs [24].\nWe present PLASMA (PLAn with SMAll models), a novel two-pronged framework to impart planning\nabilities in small LMs. We achieve this through symbolic procedural knowledge distillation to enhance\nthe implicit knowledge in small LMs (Figure 1) and an inference-time decoding algorithm to enable\nstructured reasoning (Figure 2). We formulate symbolic procedural knowledge distillation [41, 3] in\ntwo stages: (i) Knowledge verbalization to generate procedural knowledge from an LLM, and (ii)\nKnowledge distillation to transfer LLM-generated knowledge to a smaller LM.\n\u2020Authors contributed equally.\n1We make our dataset and code publicly available at: https://github.com/allenai/PlaSma\nPreprint. Under review.\narXiv:2305.19472v2  [cs.CL]  26 Jul 2023\ngoal + plan + condition \nPrompt  \nTemplates:\nDistilled \nPLASMA\nProcedural Knowledge Verbalization\nSupervised \nCritic\ngoal \ngoal + plan \nCOPLAN Dataset\nGoals\nPlans\nConditions\nCounterfactual \nPlans\nProvide steps:  \n   [goal] see a movie\nProvide steps conditionally:  \n  [goal] see a movie  \n  [condition] see the movie at home\nPlanning (P)\nCounterfactual Planning (CP)\nCounterfactual Plan Revision (CPR)\nPLAN:\n1.Look up movie showings and times  \n2.Choose a movie to see  \n3.Drive to the movie theater \n4.Get items from the concession stand  \n5.Walk together towards the theater  \n6.Sit down in the assigned seats  \n7.See a movie.\n1.Choose a streaming service  \n2.Choose the movie and start streaming it \n3.Get snacks or drinks from the kitchen \n4.Set up the viewing device  \nin a comfortable and safe place \n5.Sit and enjoy the movie.\nCOUNTERFACTUAL PLAN:\nMultitasking\nP\nCP\nCPR\nProcedural Knowledge  Distillation\nN/A\nRewrite steps:  \n   [goal] see a movie \n   [plan] 1. Look up movie showings and         \n             times; 2. Choose a movie to see;    \n             \u2026 7. See a movie \n   [condition] see the movie at home\nLLM\nFigure 1: Symbolic Procedural Knowledge Distillation.\nIn addition to the standard planning task, we introduce and verbalize knowledge for novel task\nformulations under counterfactual settings: Counterfactual planning and Revision. These tasks\nenable a more realistic setting by requiring models to reason about contextually constrained situations\nin real-world applications; specifically, the model generates or revises a plan based on a given goal\n(e.g., \"see a movie\") while adhering to an additional condition (e.g., \"at home\"). Our knowledge\nverbalization process results in a large (counterfactual) procedural planning dataset, COPLAN, which\nis then used to train smaller models, PLASMA, using both task-specific and multi-task distillation.\nWe observe that the standard next-token prediction objective in auto-regressive LMs (applied during\ndistillation) does not equip them with sufficient causal and temporal reasoning abilities to generate\nhigh-quality plans, or a mechanism to rectify their mistakes in earlier steps. To address this challenge,\nwe develop a verifier-guided step-wise beam search to better leverage the multi-step structure of plans\n(resulting in PLASMA+). Concretely, we incorporate a step-wise verifier in our decoding process to\nguide PLASMA+ to generate more semantically coherent and temporally accurate plans.\nThrough experiments, we show that our approach is effective at endowing smaller LMs with planning\nabilities. For the standard planning task, smaller student models (of varying sizes) achieve 17.57%\nrelative improvements, on average, over their teacher. The best student model is comparable even to\nGPT-3, a model 16 times the student\u2019s size. Furthermore, we, for the first time, distill counterfactual\nplanning abilities in small-size models, achieving 93% validity rate according to human evaluation.\nIn a simulated environment [29], our model significantly outperforms previous work based on\nGPT-3 [11] on executability (by 17%) and correctness (by 25%). Taken together, our framework\nincluding symbolic procedural distillation, decoding-time algorithm, and the proposed tasks and the\naccompanying COPLAN dataset provide valuable resource and direction for advancing research in\nthe field of procedural planning.\n2\nSmall Language Models as Procedural Knowledge Models\nIn this section, we discuss how to endow small students with procedural knowledge and (counterfac-\ntual) planning capabilities. We first describe our knowledge verbalization and distillation framework\nwhich we collectively refer to as Symbolic Procedural Knowledge Distillation (\u00a72.1, \u00a72.2). We then\npropose a strategy to enhance the reasoning capabilities of small students via a novel verifier-guided\nstep-wise decoding algorithm (\u00a72.3).\n2.1\nCOPLAN: Procedural Knowledge Verbalization from Large Teachers\nLarge language model can perform new tasks by adapting to a few in-context examples [4]. We thus\nleverage this emergent reasoning capabilities of LLM to circumvent the challenge of crowdsourcing\nsupervised datasets at scale. We collect data targeting the following three tasks:\n1. Goal-based Planning (pl.), decomposing a high-level goal g into a sequence of temporally\nextended steps y = {st}T\nt=1.\n2\n2. Counterfactual Planning (cp.), decomposing a high-level goal g into a sequence of temporally\nextended steps y = {st}T\nt=1 while satisfying a given condition c.\n3. Counterfactual Plan Revision (cpr.), rewriting an initial plan y to a given goal g into a new\nplan y\u2032 in order to satisfy a given condition c.\nOur knowledge verbalization pipeline shown in the left side of Figure 1 is a two-stage process: 1)\ninstance generation through few-shot prompting, and 2) automatic data curation using a critic to filter\nout the low quality data. The process results in COPLAN, a quality dataset containing goals, plans,\nconditions, and counterfactual plans.\nStep 1. Data Generation We start by generating a large pool of goals G with a diverse range of\ntopics in a bootstrapping fashion. We initiate the seed goal pool with 100 goals generated by GPT-3\n(text-curie-001) along with 5 example goals provided by the authors. With the seed goal pool,\nwe iteratively expand it by GPT-3 with randomly selecting example goals for prompting.\nFor each generated goal g \u2208 G, we few-shot prompt a teacher model M to generate a set of ordered\nsteps, as a plan y to achieve the goal. The input to M, including instruction and few-shot examples,\ntakes the format shown in Figure 7. Since LLMs can be sensitive to instruction, and/or few-shot\nexamples [28, 21], we randomize the prompt by (i) manually creating a set of semantically similar\ninstructions and each time randomly sample from the instruction set (ii) creating dynamic in-context\nexamples for each input. We use a subset of the existing ProScript [34] and DeScript [39] datasets\nas our seed source to form in-context examples, P = {(gj, yj)}M\nj=1:\nyi \u223c M(yi|gi, P)\nThe result is a pool of 140k pairs of goal and plans, (g, y), generated from the teacher model.\nFor the counterfactual setting, we also obtain conditions c, and modified plans y\u2032 from a teacher model\nM through few-shot prompting. We manually design our prompts P to collect natural language\nconditions concerning the environment the task is performed in such as Location (\u201cthe store is\nclosed\u201d), Equipment (\u201cyou don\u2019t have a sharp tool\u201d), Safety (\u201cthe car breaks down\u201d) or user\u2019s\nspecifications such as Physical Condition and Preference (\u201cyou have an injury\u201c). For a given\ngoal gi and plan yi, we sample conditions:\nci \u223c M(ci|gi, yi, P)\nNext, we few-shot prompt M to rewrite an initial plan y for a given goal g such that it satisfies the\nrequirement of a condition c:\ny\u2032\ni \u223c M(y\u2032\ni|gi, yi, ci, P)\nThe prompting templates and examples of conditions are shown in Figure 8 and Table 6.\nStep 2. Automatic Data Curation To retain high-quality data for planning under the original and\ncounterfactual settings, we filter out generated samples from Step 1, i.e. generated plans, conditions\nand counterfactuals, that are invalid or of low quality. A plan y is considered invalid if it contains an\nillogical order of steps, is off-topic (w.r.t the goal) or incomplete. Whereas a counterfactual plan y\u2032\nshould not only satisfies these general criteria but should also adhere to the condition.\nTo this end, we train separate supervised critic models to judge the quality of generated samples of\ndifferent types. We collect human annotations of valid vs. invalid samples on Amazon Mechanical\nTurk to train a RoBERTa-Large [17] as our critic models. All critics are binary classifiers which\nidentify whether a tuple of either (goal, plan), (goal, plan, condition) or (goal, plan, condition,\nmodified plan) is valid. We provide more details on annotation instructions, and hyper-parameter\ntuning in Appendix B.1 and B.2.\nNaturally, there is a trade-off between dataset size and precision. Following West et al. [41], we test\nseveral confidence thresholds at which the critic rejects a pair and choose the best values (0.65, 0.76,\n0.82)2 according to precision-recall curves. After filtering out low quality data, our final COPLAN\ndataset consists of 2 main subsets including 57,794 (goal, plan) for the original goal-based planning\ntask (Dpl.), and 43,690 (goal, plan, condition, modified plan) for the counterfactual settings, (Dcp.\nand Dcpr.). On the original planning task, COPLAN is \u00d711 larger in scale than existing datasets\n[34, 39] while keeping the precision at 74%. On the proposed counterfactual settings, our dataset is to\n2These values are for plan, condition and counterfactual plans, respectively.\n3\nBuy a new car\nPlan-so-far:\nStep-wise \nVerifier\nDistilled  \nPLASMA+\n1. Research vehicle and features \n2. [next step] \n\u2026\n\u2026\n\u2026\nTest drive a car\nResearch vehicle\nGo to dealership\nMake the purchase\nCheck sales price\n\u2026\nContact seller\nFill out registration\n\u2026\nWrite a check\nNegotiate a best price\nGet the keys\n.35\n.68\n.84\n.69\n.19\n.76\n.24\n.73\n.41\n\u2026\n\u2026\n.72\n.54\n.70\n.81\n.56\n.91\n.48\n.18\n\u2026\n.52\n.76\n.11\n\u2026\nDoes [next step] logically follows the  \n[plan-so-far] to help achieve the goal?\nTemporality, \nLogicality,\nBased on:\nCompleteness, \nAchievability\nFigure 2: Verifier-guided Step-wise Beam Search. For brevity, we only showcase with N = 5 and\nK = 2 for the first step and N = 4 and K = 2 for the second step. The scores are for illustration\npurposes only.\nthe best of our knowledge the first large-scale counterfactual procedural planning dataset. Analyses\nshow that the COPLAN includes a diverse array of topics covered by goals (\u00a7A.1) and conditions\n(\u00a7A.2).\n2.2\nPLASMA: Procedural Knowledge Distillation into Small Students\nAfter obtaining our procedural planning data COPLAN, we use it to fine-tune student models on the\nthree different tasks. We consider both task-specific and multi-task distillation objectives to transfer\ngenerated procedural knowledge into the student models:\nTask-specific Distillation.\nFollowing the common practice, we use the standard autoregressive\nlanguage modeling objective [32] to fine-tune separate student models for each task:\nL(\u03b8) = E(x,y)\u223cDtask\n\u0002\n\u2212 log p\u03b8(y|T (x))\n\u0003\n,\nfor task\u2208{pl.,cp.,cpr.}\n(1)\nwhere T (x) is a task-specific template for each task-specific input x (see right side of Figure 1).\nMulti-task Distillation.\nWe also aim to improve the generalization of the student model by\nexploiting the knowledge contained in the three related tasks as an inductive bias [33, 40]. We thus\nminimize the joint loss:\nL(\u03b8) = E(g,y)\u223cDpl.\n\u0002\n\u2212 log p\u03b8(y|T (g))\n\u0003\n(2)\n+ E(g,c,y)\u223cDcp.\u0002\n\u2212 log p\u03b8(y|T (g, c))\n\u0003\n+E(g,c,y,y\u2032)\u223cDcpr.\u0002\n\u2212 log p\u03b8(y\u2032|T (g, c, y))\n\u0003\nWe name this student PLASMA-Mul.\n2.3\nPLASMA+: Advancing Student with Verifier-guided Decoding\nDuring inference, the student may generate logically and/or temporally ill-formed sequence of steps\ny = {st}T\nt=1 as it is only trained to maximize the next-token probability. For example, in Figure 2, it\nmay generate \u201cwrite a check\u201d at step 3 with relatively high confidence due to a spurious correlation\nbetween \u201csales price\u201d and \u201ccheck\u201d. We mitigate this issue via step-wise guided decoding. Rather\nthan generating plans greedily, we instead generate step-by-step by sampling several candidate next\nsteps and searching for those with a high log-probability under both the distilled student and a verifier.\nThe verifier is tasked to check for sequential ordering and semantic completeness. In an embodied\nsetting, the verifier could be taken over by any affordance or safety module [1] that determines the\nexecutability of an action in a given environment.\nStep Verifier. We introduce an independent verifier, which is trained to check the validity of plan\nsteps and encourage PLASMA to produce more temporally and causally valid plans. The verifier takes\nas input a goal, the plan-so-far and a candidate next step and outputs a continuous validity score\npverifier(st|g, s<t) \u2208 [0, 1].\nWe implement the verifier by fine-tuning a RoBERTa model [18] to classify whether a candidate step\nis valid or invalid. For training data, we use steps from available human-written plans3 as positive\n3Note that only a small-scale set of ground-truth plans is needed to train a verifier.\n4\nexamples (valid steps). However, since no negative examples are readily available, we automatically\ncreate a set of invalid steps as pseudo-negative examples. Inspired by the common errors made\nby models, we design perturbations over ground-truth plans to target sequential ordering, semantic\ncompleteness, topicality, and fluency. See Appendix B.3 for details.\nVerifier-guided Step-wise Beam Search. We illustrate our verifier-guided decoding in Figure 2.\nThe procedure generates a plan y = (s1, ..., sT ) by sequentially sampling and pruning the next step\ncandidate st. Concretely, at each iteration4, it selects and expands a size-K beam of plan-so-far,\nYt\u22121 = {sk\n<t}K\nk=1, and generates N next-step candidates,\nYt = \u222as<t\u2208Yt\u22121{(s<t||sn\nt ) | sn\nt \u223c q(.|T (x, s<t)}N\nn=1\n(3)\nwhere || is concatenation, x is a task-specific input, and q is a decoding algorithm. We encourage\nexploration at each step, by generating candidates using multiple decoding methods such as beam\nsearch, and nucleus sampling with temperature 1.0.\nTo select the top-K scoring next-step candidates S\u2217\nt , we use a value function v(s\u2264t) \u2212\u2192 R which\nreturns the weighted sum of normalized sequence log-likelihood from the student model and the\nverifier validity score,\nS\u2217\nt = arg top-Ks\u2264t\u2208Ytv(s\u2264t)\n(4)\nv(s\u2264t) = \u03b1 log p\u03b8(s\u2264t) + (1 \u2212 \u03b1) log pverifier(st|g, s<t)\n(5)\nwith \u03b1 controlling the impact of the distilled student and the verifier. The search ends when the beam\ncontains K completed plans. We return the highest-scored plan as the final output. Our step-wise\nbeam search strategy maintains a diverse set of candidate plans during the decoding process, allowing\nthe model to explore multiple plausible paths before converging on a most promising one.\n3\nExperiments\nImplementation Details. While any model with few-shot capabilities could be used, we choose\nour teacher model M to be GPT-3 text-curie-001 [4] for collecting the goals and initial plans,\nand GPT-3 text-davinci-003 for collecting conditions and counterfactual plans.5 We sample data\npoints from GPT-3 using nucleus sampling (p = 0.98) and temperature of T = 0.9. For our student\nmodels, we try a range of model sizes in T5 family [33], such as T5-large, T5-3B, and T5-11B.\nStudent models are trained using Huggingface Transformers [42]. Main experiments can be done on\n2 GPUs with 48GB of memory.\nDuring inference, we use a beam of size K = 5 for regular beam search, and N = 10 (next-step\ncandidates), beam K = 5 and p = 0.9 for our verifier-guided step-wise decoding (see \u00a72.3).\nBaselines.\nFor each task, we compare our distilled students with their corresponding teacher,\nzero-shot and few-shot variants of GPT-3 [4], COCOGEN [23] and human performance (when\navailable). COCOGEN frames the planning task as a code generation task and use a pre-trained code\nLM (code-davinci-002) in a few-shot setting.\nNext, we present the experimental setup for each task, along with their results.\n3.1\nGoal-based Planning\nIn this section, we aim to study two key research questions through our experiments. Firstly, we seek\nto investigate the extent to which scale impacts the distillation of procedural knowledge. Secondly,\nwe aim to examine whether the scale gap can be bridged through the use of multitasking and/or a\nnovel decoding algorithm. In essence, we seek to determine whether small language models can\nperform procedural planning tasks with the same level of proficiency as large language models.\nEvaluation Set.\nFor the original planning task, we use human-written plans from the test set of\nProScript [34] dataset as our evaluation data.\n4Iteration refers to a full step in a plan.\n5In our preliminary experiment, we found text-davinci-003 (the strongest GPT-3 version at the time) to\nbe helpful for the more challenging counterfactual data collection.\n5\nSetup. We compare several student models of varying scales (770M-11B) with the teacher model,\ntext-curie-001, and extremely large scale models (175B). For all student models, we decode\nusing both regular beam search (PLASMA) and our verifier-guided step-wise beam search (PLASMA+).\n770M\n3B\n11B\n175B\n250 samples\nFigure 3: Visualization of bridging the scale gap in\ngoal-based planning task. Smaller models are able\nto achieve comparable performance and sometimes\nsurpass larger models via multi-task distillation\nand step-wise guided decoding.\nMetrics. Since there may exist many equally\nvalid plans to a goal, we conduct human evalu-\nations for the main results and report automatic\nmetrics such as BLEU [25], ROUGE [16] and\nBERTScore [47] in Appendix Table 7.\nWe ask human annotators on the Amazon Me-\nchanical Turk (AMT) platform to rate the gen-\nerated plans for 250 randomly sampled goals\non three aspects: 1) Order: how well-ordered\nthe plan is (captures sequential correctness), 2)\nCompleteness: how well the plan covers the\nnecessary steps to accomplish the goal (cap-\ntures semantic completeness), and 3) Overall\nquality: overall quality and correctness of the\nplan. Details of the human evaluation can be\nfound in Appendix D.2 Figure 9.\nTable 1 and Figure 3 summarize the human eval-\nuation results for the original planning task.\nDoes scale matter? Larger models perform relatively better across all aspects.\nModelsize\nCoverage Order Overall\nQuality\nDistilled 770M\nPLASMA\n3.18\n3.64\n3.17\nPLASMA+\n4.25\n4.55\n4.28\nPLASMA-Mul\n2.84\n3.36\n2.85\nPLASMA-Mul+\n4.16\n4.48\n4.23\nDistilled 3B\nPLASMA\n3.78\n4.07\n3.83\nPLASMA+\n4.38\n4.60\n4.35\nPLASMA-Mul\n3.96\n4.35\n4.03\nPLASMA-Mul+\n4.29\n4.62\n4.33\nDistilled 11B\nPLASMA\n4.01\n4.33\n4.03\nPLASMA+\n4.33\n4.60\n4.39\nPLASMA-Mul\n4.24\n4.59\n4.28\nPLASMA-Mul+\n4.53\n4.77\n4.58\nCurie (Teacher)\nfew-shot (5)\n3.75\n4.27\n3.75\nDavinci (175B)\nzero-shot\n4.83\n4.87\n4.84\nfew-shot (5)\n4.88\n4.90\n4.90\nCOCOGEN (175B) few-shot (16)\n4.48\n4.70\n4.55\nHuman\n4.56\n4.61\n4.57\nTable 1:\nAveraged 5-point Likert scale hu-\nman evaluation for the original planning task.\nSmall students paired with our decoding algo-\nrithm consistently outperform their teacher model\n(text-curie-001) and are competitive with or-\nder of magnitude larger models in zero/few-shot\nsettings. *CoCoGen [23] is a 16-shot baseline us-\ning code LLM.\nDoes multi-task distillation help bridge the\nscale gap? As we observe, multi-task distilla-\ntion almost always wins over its task-specific\ncounterpart with the exception of the smallest\nstudent, PLASMA (770M). We posit that very\nsmall student models might not have enough\ncapacity to leverage the related tasks efficiently\nduring multi-tasking.\nDoes verifier-guided decoding help bridge\nthe scale gap? Pairing models with our pro-\nposed verifier-guided step-wise decoding sub-\nstantially improves performance across students\nof varying sizes over all aspects. Specifically,\ncompared with regular beam search, our pro-\nposed decoding results in 7%-48% relative im-\nprovements in overall quality across different\nstudent sizes. The improvements achieved by\nthe verifier-guided decoding is larger for smaller\nstudents. We showcase the comparisons with\nqualitative examples in Appendix Table 8.\nThe best distilled students with 770M, 3B, and\n11B parameters achieved respectively 14.13%,\n16%, and 22.59% relative improvements over\ntheir teacher model (text-curie-001).\nFi-\nnally, our best distilled model (11B PLASMA-\nMul+) performs equally well as human and\nis competitive with orders-of-magnitude larger\nmodels (175B).6 Figure 3 visualizes how we\nbridge the scale gap using our multi-task distil-\nlation and verifier-guided step-wise decoding.\n6Pairwise annotator agreements (i.e., how often do two annotators agree on the answer) are 0.78, 0.84, and\n0.80 for coverage, order and overall quality, respectively.\n6\nEffect of symbolic distillation. In this experiment, we compare models trained/tested on human-\nwritten pairs of (goal, plan) from ProScript dataset [34], our model-generated dataset COPLAN,\nand the mix of both.\nTest on \u2192\nProScript\nCOPLAN\nTrain on \u2193 Coverage Order Overall\nQuality\nCoverage Order Overall\nQuality\nProScript\n4.38\n4.54\n4.35\n4.51\n4.81\n4.58\nCOPLAN\n4.55\n4.74\n4.63\n4.72\n4.86\n4.73\nMix\n4.77\n4.88\n4.65\n4.77\n4.88\n4.78\nTable 2: Effect of symbolic knowledge distilla-\ntion. The model trained on our COPLAN dataset\ntransfers better to other dataset, ProScript.\nModels are initialized with T5-11B. We generate\nplans using our proposed verifier-guided decod-\ning for randomly sampled 50 and 150 goals from\nProScript and COPLAN, respectively. We use\nthe same human evaluation setup as before. Ta-\nble 2 shows that training on our LLM-generated\nCOPLAN dataset, consistently transfers better\nto human-written dataset, ProScript. Training\non the mix of both datasets, however, achieves\nthe best performance. Intuitively, we observe\nthat models are in general better at tackling\nLLM-generated data.\n3.2\nCounterfactual Planning and Revision\nHere, we seek to benchmark language models\u2019 planning abilities under constrained (contextually\ngrounded) situations. This task goes beyond the original planning task, requiring models to produce\nnovel linguistic alternatives to unseen situations.\nEvaluation Set. To create an evaluation set, we generate conditions and counterfactual plans for the\ntest set of ProScript following Step 1 in \u00a72.1. We then only use human-verified tuples of (goal,\nplan, condition, counterfactual plan) as our test set for counterfactual planning and revision tasks.\nSetup. We compare 3B and 11B student models with GPT-3 Curie and the 175B teacher model,\ntext-davinci-003 in zero/few-shot settings. During inference, we use our proposed verifier-\nguided step-wise beam search with \u03b1 = 0.75 to outweigh student model\u2019s probability over the verifier\nvalidity score.7\nMetric. We conduct human evaluation on the AMT platform. We generate (counterfactual) plans\nfor 300 randomly sampled examples using each model. We ask 3 human annotators to rate each\ngenerated plan based on whether it contains the necessary steps to make the goal achievable while\nsatisfying the condition. We provide 3 options for the annotators to pick from: A: The plan contains\nall the necessary steps to meet the requirements of the condition on the goal, B: The plan addresses the\ncondition, but it is trivial and lacks thoughtfulness8, and C: The plan does NOT address the condition\nor does so very poorly. We take the majority vote for the final results. Details on crowd-sourcing\nhuman evaluation can be found in Appendix Figure 11.\nResults. Figure 4 depicts the results. Large students perform better on both tasks. In counterfactual\nplanning, our 11B PLASMA-Mul+ demonstrates a 93.33% success rate in producing high-quality\nplans while adhering to the given condition, which is comparable to the performance of the 175B\nparameter Davinci model in a zero-shot setting. Furthermore, our model generates slightly fewer\nlow-quality plans, only 7 as opposed to 12 by Davinci. While multi-tasking seems to be helpful in\n(counterfactual) planning, this is not always the case for counterfactual revision. We hypothesize that\nthe reason for this could be that the original and counterfactual planning tasks, which do not involve\nmodifying an existing plan, may negatively impact the revision task. The best performance for the\ncounterfactual plan revision is achieved by Davinci (90%) followed by PLASMA+ (86.33%).9 We\nalso collect additional feedback from annotators on the errors made by models. Results are reported\nin Appendix Table 11, showing \u201cmissing necessary steps\u201c is the most prevalent mistakes.\nWe provide qualitative examples of model generations across all three tasks in Table 4. More examples\nof (good and bad) generations according to human annotators are provided in Appendix Tables 9, 10.\n7We performed a hyperparameter search over \u03b1 = {0.5, 0.75, 0.8}.\n8An example of trivial modification is addressing the condition \u201cyou have no money\u201d with adding an step\n\u201cfind money\u201d in the plan.\n9Pairwise annotator agreements are 0.96 and 0.94 for counterfactual planning and revision, respectively.\n7\n20%\n40%\n60%\n80%\n100%\nPlaSma+ (3B)\nPlaSma-Mul+ (3B)\nPlaSma+ (11B)\nPlaSma-Mul+ (11B)\nCurie z-shot\nCurie f-shot\nDavinci z-shot (175B)\nTeacher: Davinci f-shot (175B)\nGood\nTrivial\nDisagreed\nBad\n264 264\n274 280\n204 208\n280\n287\n20\n12\n7\n7\n67\n58\n12\n9\nCounterfactual Planning\n20%\n40%\n60%\n80%\n100%\nPlaSma+ (3B)\nPlaSma-Mul+ (3B)\nPlaSma+ (11B)\nPlaSma-Mul+ (11B)\nCurie z-shot\nCurie f-shot\nDavinci z-shot (175B)\nTeacher: Davinci f-shot (175B)\nGood\nTrivial\nDisagreed\nBad\n220 234\n259\n251\n133 127\n270\n266\n40\n36\n17\n29\n141\n122\n19\n11\nCounterfactual Plan Revision\n300 samples\nFigure 4: Human evaluation results of 300 generations for counterfactual planning and revision\ntasks. Left: in counterfactual planning, our best student model PLASMA-Mul+ (11B) with \u00d716 fewer\nparameters is on par with GPT-3 Davinci model. Right: in counterfactual revision, our best student\nmodel PLASMA+ (11B) is able to generate good counterfactual plans 86.33% of the time.\n3.3\nApplication to Embodied Agents\nAn important application enabled by PLASMA is that of enabling an agent to plan according to a given\nhigh-level goal. We evaluate PLASMA on the task of planning in the VirtualHome [29] environment.\nIn this environment, agents can perform household activities, e.g. \u201cpaint ceiling\", through programs,\nin the form of supported actions (42 in total) and arguments. For evaluation, we use their test set\nconsisting of 88 goals (and corresponding gold programs).\nmodel\nExecutability\n(%)\nLCS\n(%)\nCorrectness\n(%)\nPlanner (175B) [11]\n77.17\n19.10\n18.33\nPLASMA-MulF T (11B)\n76.38\n28.36\n41.38\nPLASMA-Mul+F T (11B)\n94.18\n31.93\n43.68\nHuman\n100\nN/A\n66.66\nTable 3: Human-evaluated correctness along with\n(automatic) executability and LCS scores on Virtu-\nalHome environment [29]. Steps generated by our\nmodel is more executable and correct for accom-\nplishing the task.\nWe compare our best student PLASMA-Mul\n(11B) with Planner [11], a 1-shot GPT-3 (175B)\nmodel with several inference-time strategies to\nensure executability in embodied environments.\nWe follow their procedure to translate generated\nsteps from natural language to steps executable\nin the environment. To apply our model to Vir-\ntualHome, we finetune PLASMA-Mul on \u223c 4K\nhuman labeled examples and also finetune the\nstep verifier on the same data using the method\ndescribed in Section 2.3. We show, in Table 3,\nthat our model generates steps that are signifi-\ncantly more executable (according to automatic\nmetric) and also more complete (according to\nhuman judges). More experimental details can\nbe found in Appendix E.\n4\nRelated Works\nProcedural Planning The problem of planning to accomplish a goal via sub-steps is widely studied\nin two contexts. One is script knowledge generation, which is a long-standing NLP problem [36].\nCollecting script knowledge requires either human annotation [39], unsupervised feature-based\nextraction [5], or, more recently, methods that utilize task-specific fine-tuned LLMs [34] and pipeline-\nbased approaches [35]. In addition, there is a line of procedural planning that involves planning with\nexecutable actions that can be executed by robots in real-life environments [11, 1, 43, 12]. Recent\napproaches view planning as a conditional text generation problem and use LLMs in a zero/few-shot\nprompting mode to tackle the task [23, 11, 1, 22]. Despite showing strong performance, their success\n8\nGoal\nCondition\nInitial Plan\nGenerated (Counterfactual) Plan\nEmpty lint filter\nFind lint filter > Remove lint filter > Clean lint filter >\nReplace lint filter\nIf the lint trap is\ntoo hot to touch\nUnplug dryer > Open dryer > Locate lint trap > Use heat-\nresistant glove or mitt to pick up lint trap > Pull lint trap\nout of dryer > Empty lint trap\nLoad clothes into dryer > Locate lint trap >\nMove hand towards lint trap > Grip lint trap\nwith fingers > Pull lint trap out of dryer > Use\nother hand to lift lint from lint trap > Empty lint\nfilter\nLoad clothes into dryer > Locate lint trap > Use gloved\nhand to move hand toward lint trap > Grip lint trap with\nfingers > Use other hand to lift lint from lint trap > Remove\nlint trap from dryer\nEat some ice cream\nfrom an ice cream shop\nFind ice cream shop > Enter ice cream shop > Find ice\ncream > Choose ice cream > Eat ice cream\nIf you have\nallergies to\ndairy products\nDecide to eat some ice cream from an ice cream shop >\nFind an ice cream shop > Walk into ice cream shop > Ask\nfor a dairy-free ice cream > Pick out ice cream > Pay for\nice cream > Eat some ice cream from an ice cream shop\nDecided to eat some ice cream from an ice\ncream shop > Decide to eat ice cream > Gather\nthe family > Drive to ice cram parlor > Get fam-\nily out and lock car > Walk in ice cream shop\nand sit > Eat some ice cream from an ice cream\nshop\nDecide to eat some ice cream from an ice cream shop >\nResearch ice cream shops in your area that offer dairy-free\noptions > Gather the family > Drive to ice cream parlor >\nGet family out and lock car > Walk in ice cream shop and\nsit > Ask the staff about their dairy-free options > Select a\ndairy-free option > Eat some ice cream from an ice cream\nshop\nTable 4: PLASMA generations for (counterfactual) planning and revision tasks.\nheavily relies on scale. However, in this paper, we seek to achieve comparable performance while\nusing more parameter-efficient and accessible models.\nSymbolic Knowledge Distillation Crowd-sourcing human-written datasets at scale is both chal-\nlenging and costly. Therefore, there has been a growing interest in using LLM-generated data to train\nsmaller models. This approach which falls under the conceptual framework of symbolic knowledge\ndistillation [41] has been applied to simpler classification tasks [37], reasoning [38, 10, 46, 7], as well\nas commonsense and general knowledge base construction [41, 3]. This approach not only achieves\npromising performance on smaller models but is also cost-efficient compared to pre-training smaller\nmodels from scratch [13]. In a concurrent work, Yuan et al. [45] proposed a similar approach to distill\nscript knowledge from LLMs for constrained planning task. However, unlike our \u201cconditions\u201d which\ncan take free-form format, their constraints are limited to specific types by extending an original goal\nwith a modifier, intent or method.\nDecoding-time Algorithm Decoding-time algorithm is an emerging approach for adapting language\nmodels\u2019 output for task-specific characteristics. Works in this line often focus on incorporating\nexplicit lexical constraints at inference time so that the model is bounded with certain generation\nwords [20, 19, 9, 26]. In addition to discrete lexical constraints, applying continuous optimization\nfunctions such as KL loss has also been found to be effective [30, 31, 15, 8]. Perhaps our approach\nis most similar to function-guided decoding methods. Krause et al. [14] and Yang et al. [44] fuse\nnext-token probability with desired attributes\u2019 probabilities at inference using a discriminator model.\nThese and related token-level beam search variants assume access to per-token logits and gradient\nupdates. Our decoding method however only relies on model log-probabilities and a verifier to\nfacilitate semantic and temporal constraints at a step level.\n5\nConclusions and Future Work\nIn this paper, we focus on procedural planning, a challenging task that involves decomposing high-\nlevel goals into ordered steps. We introduce PLASMA as an effective approach that uses smaller\nand more accessible models. By leveraging symbolic procedural knowledge distillation and an\ninference-time algorithm, we have endowed smaller models with enhanced procedural knowledge\nand planning capabilities. Furthermore, we introduced the task of Counterfactual Planning, which\ninvolves generating/revising plans to accommodate realistic counterfactual scenarios. Our results\ndemonstrate that significantly smaller models can effectively compete with and often outperform\n9\ntheir larger teacher models in both original and counterfactual settings. We hope our work sheds light\non new directions towards developing smaller yet powerful multi-modal models for (counterfactual)\nprocedural planning and reasoning.\n6\nAcknowledgements\nThis work was funded in part by the DARPA MCS program through NIWC Pacific (N66001-19-2-\n4031), and the Allen Institute for AI. We also thank the Beaker Team at the Allen Institute for AI for\nhelping with the compute infrastructure and OpenAI for providing access to the GPT-3 API.\nReferences\n[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David,\nChelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel\nHo, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano,\nKyle Jeffrey, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang,\nKuang-Huei Lee, Sergey Levine, Yao Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell\nQuiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes, Pierre Sermanet, Nicolas Sievers,\nClayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu,\nMengyuan Yan, and Andy Zeng. Do as i can and not as i say: Grounding language in robotic\naffordances. In arXiv preprint arXiv:2204.01691, 2022.\n[2] Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On\nthe dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021\nACM Conference on Fairness, Accountability, and Transparency, FAccT \u201921, page 610\u2013623,\nNew York, NY, USA, 2021. Association for Computing Machinery.\n[3] Chandra Bhagavatula, Jena D. Hwang, Doug Downey, Ronan Le Bras, Ximing Lu, Lianhui\nQin, Keisuke Sakaguchi, Swabha Swayamdipta, Peter West, and Yejin Choi. I2d2: Inductive\nknowledge distillation with neurologic and self-imitation, 2023.\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,\nJeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott\nGray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle,\nM. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information\nProcessing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc., 2020.\n[5] Nathanael Chambers and Dan Jurafsky. Unsupervised learning of narrative event chains. In\nProceedings of ACL-08: HLT, pages 789\u2013797, Columbus, Ohio, June 2008. Association for\nComputational Linguistics.\n[6] Katherine M Collins, Catherine Wong, Jiahai Feng, Megan Wei, and Josh Tenenbaum. Struc-\ntured, flexible, and robust: benchmarking and improving large language models towards more\nhuman-like behavior in out-of-distribution reasoning tasks. In Proceedings of the Annual\nMeeting of the Cognitive Science Society, volume 44, 2022.\n[7] Namgyu Ho, Laura Schmid, and Se-Young Yun. Large language models are reasoning teachers.\nArXiv, abs/2212.10071, 2022.\n[8] Cong Duy Vu Hoang, Gholamreza Haffari, and Trevor Cohn. Towards decoding as continuous\noptimisation in neural machine translation. In Proceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing, pages 146\u2013156, Copenhagen, Denmark, September\n2017. Association for Computational Linguistics.\n[9] Chris Hokamp and Qun Liu. Lexically constrained decoding for sequence generation using\ngrid beam search. In Proceedings of the 55th Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), pages 1535\u20131546, Vancouver, Canada, July 2017.\nAssociation for Computational Linguistics.\n10\n[10] Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander\nRatner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. Distilling step-by-step! outperforming\nlarger language models with less training data and smaller model sizes, 2023.\n[11] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-\nshot planners: Extracting actionable knowledge for embodied agents. In Kamalika Chaudhuri,\nStefanie Jegelka, Le Song, Csaba Szepesv\u00e1ri, Gang Niu, and Sivan Sabato, editors, International\nConference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA,\nvolume 162 of Proceedings of Machine Learning Research, pages 9118\u20139147. PMLR, 2022.\n[12] Peter Jansen. Visually-grounded planning without vision: Language models infer detailed plans\nfrom high-level instructions. In Findings of the Association for Computational Linguistics:\nEMNLP 2020, pages 4412\u20134417, Online, November 2020. Association for Computational\nLinguistics.\n[13] Junmo Kang, Wei Xu, and Alan Ritter. Distill or annotate? cost-efficient fine-tuning of compact\nmodels, 2023.\n[14] Ben Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar, Shafiq Joty,\nRichard Socher, and Nazneen Fatema Rajani. GeDi: Generative discriminator guided sequence\ngeneration. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages\n4929\u20134952, Punta Cana, Dominican Republic, November 2021. Association for Computational\nLinguistics.\n[15] Sachin Kumar, Eric Malmi, Aliaksei Severyn, and Yulia Tsvetkov. Controlled text generation as\ncontinuous optimization with multiple constraints. In Neural Information Processing Systems,\n2021.\n[16] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74\u201381, Barcelona, Spain, July 2004. Association for Computational\nLinguistics.\n[17] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. ArXiv, abs/1907.11692, 2019.\n[18] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\n[19] Ximing Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai, Daniel Khashabi, Ronan\nLe Bras, Lianhui Qin, Youngjae Yu, Rowan Zellers, Noah A. Smith, and Yejin Choi. NeuroLogic\na*esque decoding: Constrained text generation with lookahead heuristics. In Proceedings of\nthe 2022 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Seattle, United States, July 2022. Association for\nComputational Linguistics.\n[20] Ximing Lu, Peter West, Rowan Zellers, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi.\nNeuroLogic decoding: (un)supervised neural text generation with predicate logic constraints.\nIn Proceedings of the 2021 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, pages 4288\u20134299, Online, June\n2021. Association for Computational Linguistics.\n[21] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically\nordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In\nProceedings of the 60th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 8086\u20138098, Dublin, Ireland, May 2022. Association for\nComputational Linguistics.\n[22] Yujie Lu, Weixi Feng, Wanrong Zhu, Wenda Xu, Xin Eric Wang, Miguel Eckstein, and\nWilliam Yang Wang. Neuro-symbolic procedural planning with commonsense prompting. In\nThe Eleventh International Conference on Learning Representations, 2023.\n11\n[23] Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig. Language models\nof code are few-shot commonsense learners. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing, pages 1384\u20131403, Abu Dhabi, United\nArab Emirates, December 2022. Association for Computational Linguistics.\n[24] OpenAI. Openai api pricing. 2023. Accessed: 2023-05-15.\n[25] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\nevaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association\nfor Computational Linguistics, pages 311\u2013318, Philadelphia, Pennsylvania, USA, July 2002.\nAssociation for Computational Linguistics.\n[26] Damian Pascual, B\u00e9ni Egressy, Florian Bolli, and Roger Wattenhofer. Directed beam search:\nPlug-and-play lexically constrained language generation. ArXiv, abs/2012.15416, 2020.\n[27] Douglas Pearson and John Laird. Incremental learning of procedural planning knowledge in\nchallenging environments. Computational Intelligence, 21:414\u2013439, 11 2005.\n[28] Ethan Perez, Douwe Kiela, and Kyunghyun Cho. True few-shot learning with language models.\nNeurIPS, 2021.\n[29] Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio\nTorralba. Virtualhome: Simulating household activities via programs. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, pages 8494\u20138502, 2018.\n[30] Lianhui Qin, Vered Shwartz, Peter West, Chandra Bhagavatula, Jena D. Hwang, Ronan Le Bras,\nAntoine Bosselut, and Yejin Choi. Back to the future: Unsupervised backprop-based decoding\nfor counterfactual and abductive commonsense reasoning. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language Processing (EMNLP), pages 794\u2013805, Online,\nNovember 2020. Association for Computational Linguistics.\n[31] Lianhui Qin, Sean Welleck, Daniel Khashabi, and Yejin Choi. Cold decoding: Energy-based\nconstrained text generation with langevin dynamics. Advances in Neural Information Processing\nSystems, 2022.\n[32] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\nunderstanding by generative pre-training. 2018.\n[33] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367, 2020.\n[34] Keisuke Sakaguchi, Chandra Bhagavatula, Ronan Le Bras, Niket Tandon, Peter Clark, and\nYejin Choi. proScript: Partially ordered scripts generation. In Findings of the Association for\nComputational Linguistics: EMNLP 2021, pages 2138\u20132149, Punta Cana, Dominican Republic,\nNovember 2021. Association for Computational Linguistics.\n[35] Abhilasha Sancheti and Rachel Rudinger. What do large language models learn about scripts?\nIn Proceedings of the 11th Joint Conference on Lexical and Computational Semantics, pages\n1\u201311, Seattle, Washington, July 2022. Association for Computational Linguistics.\n[36] Roger C. Schank and Robert P. Abelson. Scripts, plans and knowledge. In International Joint\nConference on Artificial Intelligence, 1975.\n[37] Timo Schick and Hinrich Sch\u00fctze. Generating datasets with pretrained language models. In\nProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,\npages 6943\u20136951, Online and Punta Cana, Dominican Republic, November 2021. Association\nfor Computational Linguistics.\n[38] Kumar Shridhar, Alessandro Stolfo, and Mrinmaya Sachan. Distilling multi-step reasoning\ncapabilities of large language models into smaller models via semantic decompositions, 2022.\n12\n[39] Lilian D. A. Wanzare, Alessandra Zarcone, Stefan Thater, and Manfred Pinkal. A crowdsourced\ndatabase of event sequence descriptions for the acquisition of high-quality script knowledge.\nIn Proceedings of the Tenth International Conference on Language Resources and Evaluation\n(LREC\u201916), pages 3494\u20133501, Portoro\u017e, Slovenia, May 2016. European Language Resources\nAssociation (ELRA).\n[40] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan\nDu, Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In\nInternational Conference on Learning Representations, 2022.\n[41] Peter West, Chandra Bhagavatula, Jack Hessel, Jena Hwang, Liwei Jiang, Ronan Le Bras, Xim-\ning Lu, Sean Welleck, and Yejin Choi. Symbolic knowledge distillation: from general language\nmodels to commonsense models. In Proceedings of the 2022 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies,\npages 4602\u20134625, Seattle, United States, July 2022. Association for Computational Linguistics.\n[42] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony\nMoi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,\nPatrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain\nGugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art\nnatural language processing. In Proceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing: System Demonstrations, pages 38\u201345, Online, October 2020.\nAssociation for Computational Linguistics.\n[43] Te-Lin Wu, Alex Spangher, Pegah Alipoormolabashi, Marjorie Freedman, Ralph Weischedel,\nand Nanyun Peng. Understanding multimodal procedural knowledge by sequencing multimodal\ninstructional manuals. In Proceedings of the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages 4525\u20134542, Dublin, Ireland, May\n2022. Association for Computational Linguistics.\n[44] Kevin Yang and Dan Klein. FUDGE: Controlled text generation with future discriminators.\nIn Proceedings of the 2021 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies. Association for Computational\nLinguistics, 2021.\n[45] Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles Jankowski, Yanghua\nXiao, and Deqing Yang. Distilling script knowledge from large language models for constrained\nlanguage planning. In Proceedings of the 61st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages 4303\u20134325, Toronto, Canada, July 2023.\nAssociation for Computational Linguistics.\n[46] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. STar: Bootstrapping reasoning with\nreasoning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors,\nAdvances in Neural Information Processing Systems, 2022.\n[47] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Eval-\nuating text generation with BERT. In 8th International Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.\nSupplementary Material\nA\nCOPLAN Analysis Details\nA.1\nGoal diversity\nIn this section, we demonstrate that the goals in our COPLAN dataset broadly cover a diverse set of\neveryday, real-world human activities.\nFor this analysis, we first define seven goal-relevant categories based on categories defined by the US\nBureau of Labor Statistics10: (1) career and work related activities; (2) education and professional\n10https://www.bls.gov/news.release/atus.t12.htm defines 11 categories to cover common every-\nday civilian activities. We cluster these categories into five.\n13\ngrowth; (3) financial and commercial activities; (4) fitness and health; (5) service and civic activities;\n(6) social activities and relationships; and (7) self-improvement and leisure.\nNext, using the seven categories, we manually annotate 200 most frequent verb unigrams, 300 most\nfrequent noun unigrams, and 300 most frequent nominal (nouns + adjectives) bigrams extracted from\nthe goals statement. Only when the unigram (e.g. \u201cmake\u201d) or the bigram (e.g. \u201cnew word\u201d) indicate\none of the seven categories (e.g., \u201cclose friend\u201d for relationship or \u201ccollege university\u201d for education)\nthe instance is annotated with the category. Otherwise, it is annotated with an eight category, other.\nFor each goal in COPLAN, each (verb, noun) unigram or (nominal) bigram casts a category as a vote\nif found in the annotated data. If not found, then it casts other as vote. Majority vote is taken as the\ncategory of the larger goal statement.\nFigure 5 shows the distribution of the activity types in COPLAN. Education is the largest category\n(\u201cjoin an online course to learn a new language\u201d) followed by self-improvement (\u201cdevelop my creative\nwriting skills\u201d). Service (\u201ccooking meals for a homeless shelter\u201d), career (\u201cget interview for a new\njob\u201d), and financial (\u201cupgrade to a new car\u201d) are the next largest categories. The other category\nincludes miscellaneous activies like chores and events like \u201cvaccuum the livingroom floor\u201d.\nA.2\nCondition diversity\nWe assess the diversity of the conditions in COPLAN by analyzing the verbal use and nominal trigrams\nemployed in the statements.\nWe manually analyze 20 most frequent verbs and phrasal verbs (e.g., \u201chave access\u201d) appearing in\nthe condition statements. The verbs are grouped into 5 semantic categories: (1) want (to want, to\ndesire, etc); (2) possess (to have, to possess, etc); (3) access (to obtain, to get, to procure etc); (4)\nable (to be able to, be capable of, etc); and (5) trust (to be safe, to rely, etc). Note that each of these\ncategories include conditions of both polarity; for example, for possess, it includes both the condition\nimposed by having (\u201chave enough money\u201d) and by lacking (\u201cnot have enough money\u201d). A sixth\ncategory, other, was included for the verbs not included in the above categories. For each condition in\nCOPLAN, the first trigram made up of verbs, adjectives, and nouns appearing after the main verb (e.g.,\n\u201cIf you want to [apply to an online program]\u201d \u2013> main verb: want, trigram: apply online program)\nwere extracted. Trigrams were then associated with each of the 5 semantic categories based on the\nmain verb.\nFigure 6 shows the most frequent unique trigrams in each category. The graph includes the 20\nmost frequent trigrams for each category. The displayed trigrams were manually clustered when\nappropriate for readability purposes (e.g., \u201ctake course online\u201d clustered with \u201ctake online course\u201d).\nFitness         \n3%\nSocial\n5%\nFinancial       \n8%\nCareer          \n9%\nService         \n14%\nSelf-Improvement\n20%\nOther\n20%\nEducation       \n21%\nFigure 5: Goal diversity in COPLAN\nFigure 6: Condition diversity in COPLAN\n14\nWe find a wide variety of real-world constraints that pose varying levels of restriction such as\npreference and desire (\u201cwant to take an online course\u201d) and hindrances posed by the state of having\nor not having something (\u201cnot having enough money\u201d or \u201chaving a disability\u201d).\nB\nAdditional Experimental Details\nB.1\nCritic Models: Collecting Human Annotations\nWe gather human annotations of valid vs. invalid teacher generations. Annotations are crowdsourced\nthrough the Amazon Mechanical Turk (AMT) platform. We qualify 263 best performing workers\nthrough a paid qualification round. Additionally, we chose annotators among those who were located\nin US, GB and CA, and had 98% approval rate for at least 10,000 previous annotations. Crowdworker\ncompensation for qualification and annotation HITs is maintained at an average of $15 per hour.\nPlans.\nFor plans, the crowdworkers were presented with randomly-sampled 13K generated (goal,\nplan) pairs, and were asked to evaluate the plans along three dimensions: topicality\u2014the topic of the\nplan is relevant and appropriate for the goal, ordering\u2014the steps in the plan are appropriately ordered,\nand completeness\u2014the plan provides complete and informative steps to achieve the goal. We asked\nthe workers to evaluate the goal\u2019s achievability as a separate (fourth) dimension. Each dimension was\nrated on a 5-point likert scale with three valid labels (Definitely, Mostly, and Somewhat; numeric\nvalue 1) and two invalid labels (Hardly, Not at all; numeric value 0). Each (goal, plan) pairs were\nannotated by three crowdworkers. The template used is shown in Figure 9.\nWe determine the validity of a (goal, plan) pair in the following manner. We then calculate the mean\nscore (over the three annotator responses) for each of the dimensions. A (goal, plan) pair is considered\nvalid only if: (1) it receives a score greater than 0.25 for each of the achievablility, topicality, or\nordering dimensions, and (2) receives a scores greater or equal to 0.65 on the completeness dimension.\nFailing that, a pair is considered invalid.\nConditions.\nFor conditions, we collect human judgements on whether the condition makes the goal\nmore specific or harder to achieve (but not impossible) on a randomly-sampled set of 6100 generated\ntuples of (goal, plan, condition). We include screenshot of our annotation template in Figure 10.\nbatch size learning rate\nPlan Critic\n16\n1e-6\nCondition Critic\n32\n1e-5\nCounterfactual Critic\n32\n1e-6\nTable 5: Hyper-parameter values for\ntraining different critic models.\nCounterfactual Plans.\nAnd finally, for counterfactual\nplans, we collect 10.5K human judgements on whether\nthe modified plan contain all the necessary steps to make\nthe goal achievable while adhering to the condition. We\ninclude screenshot of our annotation template in Figure\n11.\nB.2\nCritic Models: Training Details\nWe train 3 binary classifiers (critics) for filtering out low quality teacher generations in \u00a72.1 using\npre-trained RoBERTa-Large [17]. We conduct a small grid search on validation loss for batch size\nbs = {16, 32, 64} and learning rate lr = {1e \u2212 4, 1e \u2212 5, 1e \u2212 6, 5e \u2212 6}. We report the effective\nhyper-parameters for each critic in Table 5. We use early stopping on validation loss.\nB.3\nTraining the Verifier\nConstructing Pseudo-negative Examples. For training the step verifier, we use the human-written\nplans [34] to construct positive examples of (plan-so-far, next-step) pairs and devise three main\nperturbation strategies to automatically construct negative examples as explained below:\n\u2022 Reordered Steps: Conflicting logical order results from inaccurate causal or temporal dependencies\nin a plan. Thus, we apply both near and distant reordering by randomly reordering two consecutive\nand two distant steps.\n\u2022 Repetitive Steps: Degeneration i.e., generating repetitive text is commonly observed in language\nmodels. Similarly, we include both near and distant repetition by repeating the immediate previous\nstep and distant previous step as a pseudo-negative next-step.\n15\n\u2022 Missing Steps: Another common mistake made by language models is missing necessary steps,\nleading to incoherent plans. To simulate this behaviour, we randomly select a non-immediate step\nas a pseudo-negative next-step.\nWe collect a training set of 47k positive and negative pairs of (plan-so-far, next-step) using only 3k\nhuman-written plans.\nTraining Details.\nWe fine-tune RoBERTa Large [17] as a binary classifier identifying the validity\nof a candidate next-step. We train for 10 epochs with early stopping on validation accuracy using\nbatch size of 32 and learning rate of 1e \u2212 5.\nCategory\nGoal\nCondition\nLocation\nPurchase gardening supplies\nthere are no local gardening stores nearby\nSing the lyrics\nyou want to sing the lyrics in a recording studio\nEquipment\nStudying for the exam\nyou want to use a laptop or computer\nPractice pottery techniques\nyou don\u2019t have the right tools or clay\nSafety\nTake out several plates\nthe plates are too heavy or fragile\nTransport materials home\nthe car breaks down or runs out of gas\nUser\u2019s condition/\nspecification\nPractice playing the instrument\nyou are unable to read sheet music\nRent rock climbing equipment\nyou need size specific equipment\nTable 6: Examples for different categories of conditions in COPLAN dataset.\nmodelsize\nBLEU ROUGE-2 ROUGE-L BERT-f1\nDistilled 770M\nPLASMA\n12.97\n14.02\n28.23\n84.31\nPLASMA +\n14.26\n16.31\n31.02\n85.30\nPLASMA-Mul\n14.47\n14.43\n27.99\n84.02\nPLASMA-Mul+\n14.49\n16.70\n31.49\n85.35\nDistilled 3B\nPLASMA\n12.89\n14.39\n28.57\n84.70\nPLASMA +\n13.92\n15.56\n30.83\n85.19\nPLASMA-Mul\n13.62\n15.42\n29.31\n84.80\nPLASMA-Mul+\n14.96\n16.80\n31.97\n85.28\nDistilled 11B\nPLASMA\n12.64\n13.93\n28.14\n84.56\nPLASMA +\n14.65\n15.84\n31.04\n85.33\nPLASMA-Mul\n13.61\n15.67\n29.99\n85.10\nPLASMA-Mul+\n15.54\n16.76\n31.98\n85.37\nCurie (Teacher)\nfew-shot (3-5)\n7.13\n9.24\n22.78\n83.08\nDavinci (175B)\nzero-shot\n4.98\n7.81\n21.38\n81.20\nfew-shot (3-5)\n10.27\n10.27\n24.57\n83.48\nCoCoGen (175B) few-shot (16)\n18.07\n21.01\n33.99\n86.41\nTable 7: Automatic Evaluation Results for the Planning task. Note that the human-written plans are\nnot the only possible plans, hence these automatic metrics may not provide an informative measure\nof performance.\nC\nOut-of-domain Evaluation\n[6]\nproposed\ntwo\nout-of-distribution\nreasoning\ntasks\nto\nevaluate\nLLMs,\nModel\n% good\nPLASMA\n71\nGPT-3 (from [6])\n36\nGPT-3 zero shot\n64\nTable 12: Percent of generated counter-\nfactual plans which have been rated as\ngood by annotators.\none of which involved constrained planning. For a given\ngoal and one or more conditions, the task is to gener-\nate a plan. We evaluate PLASMA on the 28 constrained\ngoals provided by the paper. We compare our generations\nto the GPT-generated plans provided by the paper and\ntext-davinci-002 prompted in a zero shot manner. To\nevaluate the generations we perform a human evaluation,\nas described in \u00a7D.2.\n16\nModel\nPlan\nGoal: put coffee in coffee machine\nCurie (Teacher)\nStep 1: find coffee machine Step 2: unlock coffee machine Step 3: put coffee in coffee\nmachine Step 4: close coffee machine Step 5: wait a few minutes Step 6: coffee should\nbe ready\nPLASMA-Mul (11B)\nStep 1: get coffee Step 2: put coffee in coffee machine Step 3: turn on coffee machine\nStep 4: wait for coffee to brew Step 5: drink coffee\nPLASMA-Mul+ (11B) Step 1: buy coffee beans Step 2: grind the coffee beans Step 3: put the ground coffee\ninto the coffee filter Step 4: put the filter into the machine Step 5: turn the machine on\nStep 6: wait for the coffee to brew Step 7: enjoy your coffee\nGoal: put on baseball jersey\nCurie (Teacher)\nStep 1: get baseball jersey Step 2: put baseball jersey on Step 3: take baseball jersey off\nPLASMA (11B)\nStep 1: find a baseball jersey Step 2: put on baseball jersey Step 3: take off baseball\njersey Step 4: put on another baseball jersey Step 5: take off second baseball jersey Step\n6: put on third baseball jersey Step 7: put on fourth baseball jersey\nPLASMA+ (11B)\nStep 1: walk to closet Step 2: open closet Step 3: look for baseball jersey Step 4: find\nbaseball jersey Step 5: take baseball jersey out of closet Step 6: put on baseball jersey\nGoal: sit at the table with dinner date\nCurie (Teacher)\nStep 1: text or call the person to let them know that you will be late Step 2: arrive at the\nrestaurant on time Step 3: take a seat at the table Step 4: wait for the other person to\narrive Step 5: eat dinner Step 6: say goodbye and thank you\nPLASMA (3B)\nStep 1: find dinner date Step 2: go to dinner date\u2019s house Step 3: sit at the table Step 4:\ntalk to dinner date Step 5: eat dinner Step 6: leave dinner date\u2019s house\nPLASMA+ (3B)\nStep 1: choose a restaurant Step 2: make a reservation Step 3: drive to the restaurant\nStep 4: park the car Step 5: enter the restaurant Step 6: find a seat Step 7: sit at the table\nwith dinner date\nGoal: brush all teeth\nCurie (Teacher)\nStep 1: get in the car Step 2: drive to the dental office Step 3: park the car Step 4: go\ninside the office Step 5: pay the bill Step 6: get a receipt Step 7: brush all teeth Step 8:\nleave office\nPLASMA (770M)\nStep 1: Get toothbrush Step 2: Get toothpaste Step 3: Get toothbrush Step 4: Get\ntoothbrush Step 5: Get toothbrush Step 6: Get toothbrush Step 7: Brush all teeth\nPLASMA+ (770M)\nStep 1: Get toothbrush Step 2: Get toothpaste Step 3: Put toothpaste on toothbrush Step\n4: Grab toothbrush Step 5: Hold toothbrush up to mouth Step 6: Brush all teeth\nTable 8: Qualitative examples generated by different models for the original planning task.\nThe human evaluation results in Table 12 show that\nPLASMA outperforms the other baselines in this out-of-domain subset of counterfactual planning task.\nD\nEvaluation Details\nD.1\nAutomatic Evaluation\nWe report automatic evaluation of models for the original planning task in Table 7. Note that the\nhuman-written plans are not the only possible plans, hence these automatic metrics may not provide\nan informative measure of performance.\nD.2\nHuman Evaluation on AMT\nAll human evaluations were conducted on the Amazon Mechanical Turk (AMT). We sourced our\nannotators from the same pool of qualified workers (see B.1). Throughout the entirety project, we\nmaintained an average of $15/hour pay rate based on our estimation of time needed to complete\nthe task. Each examples were annotated by 3 workers and majority vote was taken for the reported\n17\nType\nConditional Plan\ngood\nGoal:\neat ice cream if you have an ice cream maker\nStep 1: Decide to eat ice cream\nStep 2: Locate the ice cream maker\nStep 3: Turn on the ice cream maker\nStep 4: Add ingredients to the ice cream maker\nStep 5: Follow the instructions to churn the ice cream\nStep 6: Remove the ice cream from the ice cream maker\nStep 7: Put the ice cream in a bowl\nStep 8: Eat ice cream\ngood\nGoal:\nbrush all teeth if you are out of toothpaste\nStep 1: Decided to brush all teeth\nStep 2: Locate a store that sells toothpaste\nStep 3: Purchase toothpaste\nStep 4: Drive to the store\nStep 5: Purchase toothpaste\nStep 6: Drive back home\nStep 7: Unpack the toothpaste\nStep 8: Open the toothpaste\nStep 9: Brush all teeth\nbad\nGoal:\nwrite names on the cards if you have a shaky hand\nStep 1: find a pen or pencil with a large tip\nStep 2: hold the pen or pencil in one hand\nStep 3: hold the pen or pencil in the other hand\nStep 4: use a pen or pencil to write the names on the cards\nStep 5: once the names are on the cards, place them in a drawer or file\nStep 6: close the drawer or file\nStep 7: write names on the cards\nbad\nGoal:\nwalk into town if you have a disability that limits your mobility\nStep 1: Decide to walk into town\nStep 2: Put on shoes\nStep 3: Get a cane or other mobility aid if necessary\nStep 4: Put the cane or other mobility aid in a backpack\nStep 5: Put the backpack on\nStep 6: Leave the house\nStep 7: Walk to the bus stop\nStep 8: Take the bus into town\nStep 9: Walk into town\nTable 9: Qualitative examples of the counterfactual planning task.\n18\nType\nOriginal Plan\nCounterfactual Plan\ngood\nGoal: empty lint filter\nStep 1: load clothes into dryer\nStep 2: locate lint trap\nStep 3: move hand towards lint trap\nStep 4: grip lint trap with fingers\nStep 5: pull lint trap out of dryer\nStep 6: use other hand to lift lint\nfrom lint trap\nStep 7: empty lint filter\nGoal: empty lint filter if the lint trap\nis too hot to touch\nStep 1: Load clothes into dryer\nStep 2: Locate lint trap\nStep 3: Move hand towards lint trap\nStep 4: Use a heat-resistant glove to grip\nlint trap with fingers\nStep 5: Pull lint trap out of dryer\nStep 6: Use other hand to lift lint from lint trap\nStep 7: Empty lint filter\ngood\nGoal: take a shower after work\nStep 1: decided to take a shower after work\nStep 2: drive home from work\nStep 3: go to bath room\nStep 4: take off clothes\nStep 5: start water to get heat\nStep 6: take hot shower\nStep 7: take a shower after work\nGoal: take a shower after work if you want to\ntake a shower at the gym\nStep 1: Decide to take a shower after work\nStep 2: Drive to gym\nStep 3: Go to locker room\nStep 4: Remove clothes\nStep 5: Enter shower room\nStep 6: Start water to get heat\nStep 7: Take hot shower\nStep 8: Take a shower after work\nbad\nGoal: empty old left overs from the fridge\nStep 1: go through the house\nStep 2: walk into the kitchen\nStep 3: locate the refridgerator\nStep 4: walk to the refridgerator\nStep 5: grab the door handle\nStep 6: open the refrigerator door\nStep 7: empty old left overs from the fridge\nGoal: empty old left overs from the fridge\nif you have difficulty bending down\nStep 1: go through the house\nStep 2: walk into the kitchen\nStep 3: locate the refridgerator\nStep 4: walk to the refridgerator\nStep 5: grab a chair or stool\nStep 6: place the chair or stool in front\nof the refrigerator\nStep 7: use the chair or stool to open\nthe refrigerator door\nStep 8: empty old left overs from the fridge\nbad\nGoal: buy a new watch\nStep 1: decided to buy a new watch\nStep 2: search the internet for watch retailers\nStep 3: pick a reliable retailer\nStep 4: search the retailer site for watches\nStep 5: add watch to cart\nStep 6: click check out\nStep 7: add payment information\nStep 8: add address information\nStep 9: buy a new watch\nGoal: buy a new watch if your\npayment information is compromised\nStep 1: decide to buy a new watch\nStep 2: search the internet for watch retailers\nStep 3: pick a reliable retailer\nStep 4: search the retailer site for watches\nStep 5: add watch to cart\nStep 6: click check out\nStep 7: add payment information\nStep 8: verify payment information\nStep 9: buy a new watch\nTable 10: Qualitative examples of the counterfactual plan revision task.\n19\nCounterfactual Planning\nCounterfactual Revision\nError Type\nEdits\nRequired\nMissing\nsteps\nUnnecessary\nsteps\nEdits\nRequired\nMissing\nsteps\nUnnecessary\nsteps\nPlasma+ (3B)\n4.66\n8.33\n3.66\n13.33\n19.33\n6.00\nPlasma-Mul+ (3B)\n4.33\n7.66\n3.66\n10.66\n14.66\n4.33\nPlasma+ (11B)\n3.66\n5.00\n3.33\n4.66\n10.00\n3.33\nPlasma-Mul+ (11B)\n3.00\n3.33\n3.66\n6.00\n11.66\n4.66\ncurie-001 zero-shot\n7.00\n27.00\n6.66\n26.00\n49.33\n13.66\ncurie-001 few-shot\n6.00\n25.33\n5.00\n30.00\n48.00\n13.33\ndavinci-003 zero-shot\n1.33\n6.33\n0.66\n5.33\n7.33\n2.66\ndavinci-003 few-shot\n1.33\n3.00\n0.66\n4.33\n8.66\n2.66\nTable 11: Percent of generated (counterfactual) plans with each error type. \u201cMissing Steps\u201d is the\nmost common error types across all models.\nresults. The layout templates for evaluating plans and counterfactual plans are shown in Figures 9\nand 11, respectively.\nE\nExperimental Details of VirtualHome Evaluation\nWe follow the same experimental setup and metrics for evaluation as Planner [11]. The test set\nconsists of 88 high-level goals. To translate a generated natural language step into an executable\nstep, we follow [11] and find an executable action closest in embedding space to the generated step.\nTo compute these embeddings, we use the stsb-roberta-large model. Executability and LCS\nare computed identical to [11]. Due to challenges with reproducibility of GPT-3 outputs, evaluation\nresults of GPT-3 do not exactly match between our works.\nF\nAdditional Checklist Support\nF.1\nIRB and Annotation Ethics\nWe obtained IRB exemption for our data collection and evaluation from our institution\u2019s internal\nreview board. In full compliance to the exemption clauses as published in the code of federal\nregulations (45 CFR 46.104(d)(2,3)), we did not collect any deanomyzing information, and we do not\npublish our dataset with worker specific information such as the MTurk\u2019s worker id. Based on our\nexempted status, according to our internal regulations, does not require for us to use consent forms\nwith our crowdsourcing.\nAdditionally, our data collection and evaluation efforts only involve human judgments about world\nknowledge relating to general real-world goals and plans. We have no reason to believe that our\ncrowdsourcing posed harm or discomfort beyond the minimal risk as defined by 45 CFR 46.102(i).\nF.2\nLimitations\nOne potential limitation of our work is that the verbalization component of our framework involves\nopen text generation from large-scale language models (GPTs). Works such as Bender et al. [2]\nhave argued that generations from LLMs can be prone to harmful biases stemming from the massive\nlanguage data they are trained on. In the process of constructing the dataset, we have not directly\nobserved levels of biases to cause us alarm. We believe harmful and discriminatory generations are\nlargely mitigated by the very nature of the goals and scripts we obtain: our data is primarily composed\nof low-level everyday situations such as education, self-care, and mundane chores like vacuuming the\nfloor or cooking a meal (see \u00a7A.1,A.2). This said, we acknowledge that prejudices like gender roles,\nfor example, do also surface in the most mundane scenarios.\n20\nA related limitation is that LLMs have been trained on primarily English pretraining data, likely\nsourced from texts that reflect North American or European culture or norms. Consequently, we note\nthat the goals in COPLAN may reflect the goals that are most culturally expected or appropriate to the\ncultures of English-speaking countries. This is also expected of the plans that may include culturally\nlimited processes and procedures. This should be a consideration that any follow-up studies using\nour data and model should attend to. Extending our study to include more socio-culturally inclusive\ngoals and plans is a compelling direction for our future research.\nF.3\nBroader Impacts\nRelated to the concerns discussed in the Limitations section above, it is important for any downstream\napplication to be aware that our data may have a limited representation of the goals and procedures of\ndominant cultures of the English-speaking countries.\n21\nExample Template:\nGiven a goal write down a list of steps to achieve the goal:\nGoal: take a nap on the bed\nStep 1: sit on the bed for a little\nStep 2: pull back the blanket\nStep 3: pull back the sheet\nStep 4: fluff up the pillow\nStep 5: lay down on the bed\nStep 6: fall asleep on the bed\nStep 7: take a nap on the bed\n...\nGoal: hire a dog walker\nStep 1:\nPrompt Prefix Generator:\ndef\ngenerate_prompt_prefix ():\nw1_list = [\"For a given\ngoal\",\n\"Given a goal\"]\nw2_list = [\"write\ndown\", \"break\ndown into\", \"put down\" \"jot\ndown\"]\nw3_list = [\"steps\", \"subgoals\", \"a list of steps\", \"several\nsteps\", \"several\nsubgoals\",\n\"some\nsteps\", \"some\nsmall\nsteps\"]\nw4_list = [\"to achieve\nthe goal\", \"for\nachieving\nthe goal\",\n\"to attain the goal\"]\nw1 = random.sample(w1_list , 1)[0]\nw2 = random.sample(w2_list , 1)[0]\nw3 = random.sample(w3_list , 1)[0]\nw4 = random.sample(w4_list , 1)[0]\nprompt_prefix = f\"{w1}, {w2} {w3} {w4}.\\n\\n\"\nreturn\nprompt_prefix\nFigure 7: Randomize prompt template for eliciting plans.\n22\nPrompt Template (Conditions)\nYou want to use social media. How can you do this in 7 steps? \nstep 1: decided to use social media; step 2: Grab the phone; \nstep 3: Open, Start phone; step 4: Go to app store; step 5: \nDownload Facebook from store; step5: Open and use \nfacebook; step6: use social media \nWhat is the hindrance that might a\ufb00ect the plan above? \nIf your phone screen is cracked. \nYou want to plant a tomato plant. How can you do this in 7 \nsteps? \nstep 1: decided to plant a tomato plant; step 2 : Go to \nnursery; step 3: Purchase tomato seedling.; step 4: Purchase \npotting soil and a pot.; step 5: Return to home.; step 6: Plant \nseedling in soil and pot.; step 7: plant a tomato plant \nWhat is a specification that might a\ufb00ect the plan above? \nIf you want to use compost for soil. \n\u2026 x 3 \nYou want to print the report. How do you do this in  7 steps? \nstep 1: type the edited draft; step 2: save the edited draft; \nstep 3: open the file menu in the word processor; step 4: \nselect print from the file menu; step 5: select printer settings; \nstep 6: send document to the printer; step 7: print the report \nWhat is the hindrance that might a\ufb00ect the plan above? \nPrompt Template (Counterfactual Plan)\nYou want to learn how to swim. How can you do this in 7 \nSteps?  \nStep 1: Decided to learn how to swim; Step 2: Find swimming \ninstructor; Step 3: Travel to pool; Step4: Meet swimming \nteacher; Step 5: Practice swimming during classes; Step 6: \nReview mistakes with teacher until right; Step 7: Learn how to \nswim. \nYou want to learn how to swim. How can you do this in \nseveral steps if you forget your swimsuit? \nStep 1: Decided to learn how to swim; Step 2: Find swimming \ninstructor; Step 3: Travel to pool; Step4: Meet swimming \nteacher; Step 5: If you have forgotten your swimsuit, ask the \ninstructor if it is possible to borrow one or if there is a place \nwhere you can purchase one; Step 6: Practice swimming \nduring classes; Step 7: Review mistakes with teacher until \nright; Step 8: Learn how to swim \n\u2026 x 3 \nYou want to pick up pen. How can you do this in 6 steps? \nstep 1: look for a pen; step 2: find a pen; step 3: walk over to \npen; step 4: extend hand out to pen; step 5: reach for pen; \nstep 6: pick up pen \nYou want to pick up pen. How can you do this in several \nsteps if you want to pick up the pen from a high shelf? \nFigure 8: Prompt templates for acquiring Conditions and Counterfactual Plans.\n23\nFigure 9: AMT human evaluation template for the original planning task. For validation round we\nsubstituted goal achievability (is the goal achievable with appropriate steps?) for overall question (is\nthe plan overall good?).\n24\nFigure 10: AMT template for assessing validity of conditions for critic model training.\n25\nFigure 11: AMT human evaluation template for counterfactual plan revision. We use a similar layout\nfor counterfactual planning task only removing the initial plan.\n26\nGoal\nCondition\nInitial Plan\nGenerated (Counterfactual) Plan\nBuy a new\nvideogame\nFind a store that sells videogames > Go to the\nstore > Find the videogame you want to buy\n> Buy the videogame > Take the videogame\nhome > Play the videogame\nIf the game\nstore\nis too far\naway\nDecide to buy a new videogame > Research\nonline game stores > Compare prices and re-\nviews > Select the game to buy > Purchase\nthe game online > Wait for the game to be\ndelivered > Buy a new videogame\nDecided to buy a new videogame\n> Decide on game > Get into car\n> Drive to game store > Walk into\ngame store > Find the desired game\n> Buy a new videogame\nDecided to buy a new videogame > Decide on\ngame > Search for the game online > Com-\npare prices online > Purchase the game online\n> Wait for the game to be delivered > Unbox\nthe game > Play the game > Enjoy the game\nget extra exer-\ncise\nSet a goal for how many hours a week you\nwant to exercise > Research different types of\nexercise > Choose one type of exercise to start\nwith > Make a plan to follow the exercise plan\n> Start exercising > Keep exercising\nIf the fitness\nactivity\npresents a risk of\ninjury\nDecide what type of exercise you would like\nto do > Research the activity to make sure you\nunderstand the risks and benefits > Consult\nwith a medical professional to ensure the ac-\ntivity is safe for you > Find a place to do the\nactivity > Set a goal for how often you would\nlike to do the activity > Start doing the activity\n> Monitor your progress and adjust the activ-\nity as needed > Get extra exercise\nDecided to get extra exercise > Open\nthe calendar > Identify open time\nslots during the week > Schedule a\nfitness activity during free time > Set\nreminder for activity on phone > At-\ntend weekly fitness activity > Get ex-\ntra exercise\nDecide to get extra exercise > Open the calen-\ndar > Identify open time slots during the week\n> Research and select a fitness activity that\npresents a low risk of injury > Schedule the\nfitness activity during free time > Set reminder\nfor activity on phone > Attend weekly fitness\nactivity > Get extra exercise\nchop the wood\nGet a chopping block > Get a chopping knife\n> Chop the wood > Clean the chopping block\n> Put the chopping block away\nIf you have\na chainsaw\nPick up the wood > Place the wood on the\nground > Set up the chainsaw > Sharpen the\nchainsaw > Start the chainsaw > Cut the wood\n> Chop the wood\nPick up axe > Pick up a piece of\nwood > Check the condition of wood\n> Make sure wood is dry enough >\nSet the wood upright > Swing the axe\nat wood > Chop the wood\nPick up the chainsaw > Pick up a piece of\nwood > Check the condition of wood > Make\nsure wood is dry enough > Set the wood up-\nright > Start the chainsaw > Guide the chain-\nsaw along the wood > Chop the wood\nTable 13: Additional PLASMA generations for (counterfactual) planning and revision tasks.\n27\n"
  },
  {
    "title": "Control4D: Dynamic Portrait Editing by Learning 4D GAN from 2D Diffusion-based Editor",
    "link": "https://arxiv.org/pdf/2305.20082.pdf",
    "upvote": "1",
    "text": "Control4D: Efficient 4D Portrait Editing with Text\nRuizhi Shao1, Jingxiang Sun1, Cheng Peng1, Zerong Zheng1,2, Boyao Zhou1, Hongwen Zhang1, Yebin Liu1\n1Department of Automation, Tsinghua University 2NNKosmos Technology\nFigure 1. We propose Control4D, an approach to high-fidelity and spatiotemporal-consistent 4D portrait editing with only text instructions.\nGiven the multi-view videos as shown in the left and text instructions \u201dJensen Huang is roasting steak\u201d, Control4D generates realistic and\n4D consistent editing results presented in the middle and right.\nAbstract\nWe introduce Control4D, an innovative framework for\nediting dynamic 4D portraits using text instructions. Our\nmethod addresses the prevalent challenges in 4D editing,\nnotably the inefficiencies of existing 4D representations and\nthe inconsistent editing effect caused by diffusion-based ed-\nitors. We first propose GaussianPlanes, a novel 4D repre-\nsentation that makes Gaussian Splatting more structured by\napplying plane-based decomposition in 3D space and time.\nThis enhances both efficiency and robustness in 4D edit-\ning. Furthermore, we propose to leverage a 4D generator to\nlearn a more continuous generation space from inconsistent\nedited images produced by the diffusion-based editor, which\neffectively improves the consistency and quality of 4D edit-\ning. Comprehensive evaluation demonstrates the superior-\nity of Control4D, including significantly reduced training\ntime, high-quality rendering, and spatial-temporal consis-\ntency in 4D portrait editing. The link to our project website\nis: https://control4darxiv.github.io/.\n1. Introduction\nThe realm of 4D scene reconstruction has witnessed ad-\nvancements with the advent of dynamic neural 3D rep-\nresentation [12, 35, 47, 51, 59]. These innovations have\nsignificantly enhanced our ability to capture and repre-\nsent dynamic scenes.\nDespite these advances, the inter-\nactive editing of these 4D scenes still poses substantial\nchallenges. The primary challenge involves ensuring both\nspatial-temporal consistency and high quality in 4D editing.\nAvailable 4D editing techniques [26, 52], while effective\nfor fundamental tasks like object removal or color modifica-\ntion, often fall short in delivering interactive and advanced\nediting functionalities. Recently, a groundbreaking frame-\nwork based on text-to-image (T2I) diffusion model [50] has\nemerged for 3D generation and editing. It integrates a neu-\nral 3D representation such as NeRF with an image diffusion\nmodel and achieves text-to-3D generation [8, 33, 39, 50] or\nediting [17] by iteratively aligning images rendered from\nthe 3D representation with those generated by the diffusion\nmodel. This diffusion-based framework allows for more\nflexible and enhanced editing through textual control.\nBuilding on this framework, a straightforward approach\nto 4D editing involves transitioning from a 3D to a 4D\nrepresentation. However, it faces two primary challenges:\nFirst, 4D representations such as dynamic NeRFs require\ndense sampling along the rays to render images, which is\nslow and highly memory-intensive [12, 51, 59]. Such inef-\nficiency significantly increases the time required for editing\nin 4D scenarios. On the other hand, current T2I diffusion\nmodels lack consistency in editing different images [17].\nThis inconsistency is more apparent in 4D editing, as the\nresults vary across different spatial perspectives and over\ntime, making 4D editing extremely challenging.\nIn this paper, we address these challenges and present\nControl4D, a novel method for efficient, high-quality, and\nconsistent 4D dynamic portrait editing with text as input.\nFirstly, to enhance the efficiency of 4D representation, we\narXiv:2305.20082v2  [cs.CV]  30 Nov 2023\npropose to extend an explicit 3D representation, Gaussian\nSplatting, to a 4D dynamic representation. Gaussian Splat-\nting is an emerging representation that has demonstrated\nits efficiency in training and rendering for 3D reconstruc-\ntion [27] and generation [67].\nHowever, as it uses dis-\ncrete Gaussian point clouds where every point is indepen-\ndent from each other, it easily introduces noise during the\n4D editing process, where the edited images are not consis-\ntent in both space and time. To address this issue, we first\npropose to construct the spatial structure to describe the at-\ntributes of discrete Gaussian points by a unified, structured\ntri-plane [7] representation. Specifically, we project each\nGaussian point onto three feature planes and employ an\nMLP to integrate features and derive their attributes, which\nnot only ensures efficiency but also enhances robustness.\nThen, we extend Gaussian Splatting to 4D by defining a\ncanonical Gaussian point cloud and allowing each point to\nmove with time. To regularize the flow of discrete points,\nwe also project their positions with time into 9 planes [59] to\nmake the flow more structured. With the tri-planar structure\nfor the canonical space and the 4D plane-based structure for\nthe 4D flow, we introduce GaussianPlanes representation,\nwhich significantly reduces the time cost and improves spa-\ntiotemporal consistency in 4D editing.\nAlthough GaussianPlanes significantly improves the effi-\nciency of representation, implementing 4D editing based on\nit still encounters a bottleneck. This bottleneck lies in the\nT2I diffusion model, as the diffusion-based editor adopts a\n2D generation process and produces inconsistent edits in 4D\nspace across time and viewpoints. Consequently, when op-\ntimized with these inconsistent images, the dynamic scene\nmodel tends to diverge or produce blurry and smoothed out-\ncomes. To overcome this challenge, we propose a 4D gener-\nator to mitigate the issue of inconsistent supervision arising\nfrom the edited dataset. The key insight of our method is\nto learn a more continuous GAN latent space based on the\nedited images produced by the editor, avoiding direct but\ninconsistent supervision. Specifically, we introduce addi-\ntional latent properties to GaussianPlanes and incorporate it\nwith a 2D super-resolution module, constructing a 4D gen-\nerator, capable of producing high-resolution images based\non the rendered latent features. Simultaneously, we employ\na discriminator to learn the generation distribution from the\nedited images, which then provides discrimination signals\nfor updating the generator. To ensure stable training, we\nextract multi-level information from the edited images and\nutilize it to facilitate the generator\u2019s learning process.\nWe conduct comprehensive evaluation of our approach\nusing a diverse collection of dynamic portraits. To vali-\ndate the efficacy of our design, we conduct ablation studies\nand compare our method with a 4D extension of Instruct-\nNeRF2NeRF [17]. The evaluation demonstrates the effi-\nciency and remarkable capabilities of our method in achiev-\ning both photo-realistic rendering and spatio-temporal con-\nsistency in 4D portrait editing. To sum up, our main contri-\nbutions are listed as follows:\n\u2022 We propose an efficient and robust 4D representation\nGaussianPlanes for 4D editing by applying plane-based\ndecomposition to structure Gassian Splatting in both\nspace and time.\n\u2022 We introduce a 4D generator to learn from the 2D\ndiffusion-based editor, which reduces the effect of incon-\nsistent supervision signals and enhances the quality of 4D\nediting.\n\u2022 Building upon the proposed GaussianPlanes and 4D gen-\nerator, We introduce Control4D, a novel framework for\nflexible 4D portrait editing with text, which significantly\nreduces the training time, achieves high-quality render-\ning, and ensures spatio-temporal consistency.\n2. Related Work\n2.1. 2D Diffusion Models\nDiffusion models iteratively transform random samples into\nones resembling target data [9, 20, 29, 64]. Enhanced with\npre-trained models [53], they solve multi-modal tasks like\ntext-to-image generation [19, 44, 54]. VQdiffusion[15] and\nLDMs [55] bolster performance by operating within an au-\ntoencoder\u2019s latent space. Although these models have found\nsuccess, temporally inconsistent issues emerge in videos\nand 4D scenes.\nResearch has also concentrated on diffusion-based\nvideo generation and editing.\nVideo Diffusion Models\n(VDM)[22] use U-Net architecture to train image and video\ndata jointly, while approaches like ImagenVideo[21] en-\nable high-resolution video generation.\nVarious methods\naim to transfer text-image generation to text-video, but due\nto training costs, many focus on text-prompted video edit-\ning [2, 6, 11, 28, 31, 62, 79, 86]. These efforts underscore\nthe potential of text-based video editing, yet challenges re-\nlated to temporal consistency, quality generation, and view-\npoint alterations persist.\n2.2. NeRF-Based 3D Generation and Editing\nNeRFs [40] have gained widespread popularity for produc-\ning realistic 3D scene reconstruction and novel views based\non calibrated photographs, and have been further developed\nin numerous subsequent studies [69]. Nevertheless, NeRFs\nstill pose a challenge for editing purposes, primarily due to\ntheir underlying representation.\nNERF editing researchers have focused on utilizing\nGANs [13] and Diffusion models for their powerful gen-\nerative capabilities. GAN-based methods have seen a pro-\nliferation of novel architectures that combine implicit or ex-\nplicit 3D representations with neural rendering techniques,\nachieving promising results [7, 14, 42, 43, 78, 87]. How-\never, voxel-based GANs face challenges such as high mem-\nory requirements and computational burden when training\nhigh-resolution 3D GANs. On the other hand, diffusion-\nbased methods have two primary approaches for extend-\ning 2D editing to 3D NeRFs. The first involves using Sta-\nble Diffusion with score distillation sampling (SDS) loss to\ngenerate 3D NeRFs using the 2D diffusion-prior, as seen in\nDreamFusion [50] and its follow-ups [5, 8, 24, 32, 33, 37,\n49, 57, 60, 61, 63, 66\u201368, 72, 75]. However, these methods\ncan only generate isolated objects lacking fine-level con-\ntrol over synthesized outputs. The second approach utilizes\ndataset update(DU) to guide NeRF convergence iteratively,\nas seen in Instruct-NeRF2NeRF [17], but it has network\nconvergence issues and can be cost-intensive.\n2.3. NeRF for Dynamic Scenes\nTo expand the success of NeRF into the temporal domain,\nresearchers have pursued the strategy of modeling scenes in\n4D domain with time dimension. DyNeRF [30] proposes\na keyframe-based training strategy to extend NeRF with\ntime-conditioning. VideoNeRF [80] learns a spatiotempo-\nral irradiance field directly from a single video and resolves\nthe shape-motion ambiguities in monocular inputs by in-\ncorporating depth estimation. Meanwhile, NeRFlow [10]\nand DCT-NeRF[71] utilize point trajectories to regularize\nnetwork optimization. Park et al.[46, 47]; Pumarola et al.\n[51]; Tretschk et al. [70] adopt a similar framework that\nintroduce a separate MLP to predict scene deformations for\nmulti-view and monocular videos, respectively.Another ap-\nproach for dynamic scenes is DeVRF [35], which adopts a\nvoxel-based representation to model both the 3D canonical\nspace and the 4D deformation field. Additionally, meth-\nods including Neuralbody [48] and [36, 76, 84, 85] lever-\nage parametric body templates as semantic priors to achieve\nphoto-realistic novel view synthesis of complex human per-\nformances. Recently, to achieve higher quality with lower\nmemory, NeRFPlayer [65] have decomposed the 4D space\ninto regions of static, deforming, and newly appeared con-\ntent. Meanwhile, more compact and efficient representa-\ntions, such as [4, 12, 25, 81] are proposed, significantly\nboosting the rendering quality and efficiency.\n2.4. Gaussian Splatting\n3D Gaussian Splatting (3DGS) [27] offers a high-quality,\nswift alternative to Neural Radiance Fields (NeRF), lever-\naging differentiable 3D Gaussians for efficient rasterization.\nUnlike NeRF and other implicit 3D representations [45, 73]\nwhich render images based on volume rendering, 3D-GS\nemploys a splatting method for image rendering, resulting\nin real-time speed. The successor, 4D Gaussian Splatting\n(4DGS) [38, 77], extends this with per-frame dense tracking\nand novel view synthesis for dynamic scenes by utilizing a\nlightweight deformation field to model Gaussian motions\nand shape changes.\n3. Overview\nTo achieve high-quality, efficient, and consistent 4D portrait\nediting, we first extend the Gaussian Splatting to 4D repre-\nsentation and structure it through a spatial-temporal plane-\nbased decomposition (Sec. 4). To address the issue of in-\nconsistencies in edited images generated by diffusion-based\neditors, we integrate a 4D Editor with GaussianPlanes to ef-\nfectively mitigate instability and blurring issues and achieve\nthe realism and quality of 4D editing (Sec. 5). Building on\nthe GaussianPlanes and 4D Editor, we finally introduce sev-\neral efficient training strategies for 4D editing in the Con-\ntrol4D framework (Sec. 5.3).\nAs shown in Fig. 2, our framework consists of the Gaus-\nsianPlanes and a 4D generator. Given multi-view videos,\nwe first reconstruct the 4D portrait based on Gaussian-\nPlanes. Subsequently, we edit the reconstructed rendering\nresults and latent features through a multi-level generator\nto obtain the edited results. Simultaneously, we employ an\niterative approach to achieve dataset update through a 2D\ndiffusion-based editor, which is a ControlNet [83] in prac-\ntice. The outputs of the editor serve as real images, while\nthe generator\u2019s results function as fake images for the dis-\ncriminator\u2019s input. As the GAN training progresses, we pro-\ngressively incorporate the generator\u2019s outputs to refine the\ninputs of the 2D diffusion-based editor, facilitating train-\ning convergence. Ultimately, the discrimination outcomes\nare utilized to compute the GAN loss, driving the iterative\nrefinement of both the generator and discriminator. This\nmethodology ensures the efficient and precise realization of\n4D editing through our GAN-based framework.\n4. GaussianPlanes\nIn this section, we propose an efficient and robust 4D rep-\nresentation GaussianPlanes for 4D portrait editing. The key\nidea is to structure the discrete points of Gaussian Splat-\nting in both space and time. First, we introduce the spatial\ntri-plane decomposition, which makes Gaussian Splatting\nstructured in the spatial domain (Sec. 4.1). Following this,\nwe expand Gaussian Splatting into 4D representation and\nstructure the flow of each Gaussian point by performing a\ntemporal-spatial plane-based decomposition (Sec. 4.2).\n4.1. GaussianPlanes in 3D\nGaussian Splatting is an emerging explicit 3D representa-\ntion that utilizes a set of Gaussian point clouds to represent\n3D scenes. Each point is described with attributes of the\ncenter position x \u2208 R3, the rotation quaternion r \u2208 R4,\nthe scale factor s \u2208 R3, the opacity value \u03b1 \u2208 R and\nthe color feature c \u2208 R3. The rendering process of Gaus-\nsian Splatting involves projecting the Gaussian point cloud\nSuperres. Module\nLatent Feature\nReal\nor\nNot\nRGB Image\nDecoder\nGaussian\nRenderer\nPosition\nAlpha\nCovariance\nColor Features\nGaussianPlanes\n4D-Editing\nGaussian\nTri-planes\nGaussian Flow\nText Prompt\n\"Iron man wearing armor\"\nRender Result\nNoise\nConditioning\nOriginal\nData\nUpdate\nDiscriminator\nSignal\nFigure 2. Pipeline of Control4D: Our method first utilizes GaussianPlanes to train the implicit representation of a 4D portrait scene,\nwhich are then rendered into latent features and RGB images using Gaussian rendering, serving as inputs for the GAN-based generator.\nMeanwhile, we apply the 2D-diffusion-based editor to edit the dataset with the noisy results and conditions as inputs, leading to updated\nresults that are used as real images while the Superres. Module\u2019s outputs serve as fake images fed into the Discriminator for discrimination.\nThe discriminative results are used to calculate loss, allowing for iterative updates of both the Generator and Discriminator.\nonto the rendering viewpoint according to camera parame-\nters, followed by rasterization and volume rendering. Since\neach point in the Gaussian point cloud is independent and\nunstructured, noise easily occurs during optimization. To\nenhance robustness, we propose a spatial tri-plane decom-\nposition to represent the attributes of the Gaussian points.\nSpecifically, we decompose the color ci, opacity \u03b1i and ro-\ntation ri of i-th Gaussian point into tri-plane features:\nci = fc(F xy\nc (xi, yi), F xz\nc (xi, zi), F yz\nc (yi, zi)),\n\u03b1i = f\u03b1(F xy\n\u03b1 (xi, yi), F xz\n\u03b1 (xi, zi), F yz\n\u03b1 (yi, zi)),\nri = fr(F xy\nr (xi, yi), F xz\nr (xi, zi), F yz\nr (yi, zi)),\n(1)\nwhere F xy, F xz, F yz are the decomposed feature planes,\nand f is an MLP that fuses features to predict specific at-\ntributes. In this way, although the Gaussian points remain\nindependent, their attributes are structured and low-rank in\nspatial space, which helps to reduce noise and improve the\nrobustness of Gaussian Splatting. The scale factor s and\ncenter position x are not decomposed, as splitting Gaussian\npoints would abruptly halve the scale factor and the center\nposition of each point is used for querying attributes itself.\n4.2. GaussianPlanes in 4D\nTo extend Gaussian Splatting for 4D editing, we regard\nthe Gaussian point cloud at the first frame as the canonical\nspace and represent the 4D scene at different times by de-\nforming the canonical Gaussian point cloud. Specifically,\nwe define the flow \u02c6x, \u02c6r for both position and rotation at-\ntributes of Gaussian points. Then, for time t, we move each\nGaussian point in the canonical space (t = 0) with the flow:\nxi(t) = xi(0) + \u02c6xi(t),\nri(t) = ri(0) + \u02c6ri(t).\n(2)\nIn this way, we enhance temporal consistency since the\nGaussian point cloud at all times corresponds to its canon-\nical space.\nHowever, the flow of each gaussian point is\nstill discrete and independent.\nTo further structure the\nflow of Gaussian points in space and time, we adopt\nspatial-temporal plane-based decomposition proposed by\nTensor4D [59] and decompose the flow attributes of i-th\npoint into nine feature planes:\n\u02c6xi(t) = f\u02c6x(xi, yi, zi, t) = \u03c03(\u03a03(F\u02c6x)),\n\u02c6ri(t) = f\u02c6r(xi, yi, zi, t) = \u03c03(\u03a03(F\u02c6r)),\n(3)\nwhere \u03a03, \u03c03 are the hierarchical 4D decomposition in Ten-\nsor4D and F represents feature planes. Through spatial tri-\nplanar decomposition and 4D plane-based decomposition,\nwe structure the 4D Gaussian Splatting to enhance its con-\nsistency while maintaining efficiency.\n5. 4D Editing with GaussianPlanes\nTo solve another challenge raised by diffusion-based edi-\ntors, we propose a GaussianPlane-based 4D generator to\nedit 4D scene from the 2D inconsistent editing images with\nstable optimization. Instead of utilizing direct supervision\nwith the edited images [3], our method learns a continu-\nous generation space via GAN [13] to establish a connec-\ntion between GaussianPlanes and dynamically edited im-\nages. Specifically, we integrate GaussianPlanes with a 2D\nGAN-based super-resolution module into a 4D generator\nand learn a generation space from the edited images gen-\nerated by the diffusion model. Leveraging its generative\ncapabilities, the 4D generator can effectively distill knowl-\nedge from the diffusion-based editor and distinguish be-\ntween the rendering images (fake samples) and edited im-\nages (real samples). Subsequently, GaussianPlanes can be\noptimized within a continuous generative space supervised\nby the discrimination loss. With such a learning-to-generate\nmechanism, our method effectively alleviates blurry effects,\nresulting in high-fidelity and consistent 4D editing. In the\nfollowing, we will introduce 1) integrating GaussianPlanes\nwith GAN for 4D scene generation; and 2) the generation\nwith multi-level guidance.\n5.1. Connecting GAN to GaussianPlanes\nTo enable the generative ability of GaussianPlanes and\nhigher rendering resolution, we build a 4D generator by\nconnecting the GaussianPlanes representation with a GAN-\nbased super-resolution module. To this end, we first aug-\nment each point in the GaussianPlanes with latent fea-\ntures [7] as additional attributes. Here we assume the latent\nfeatures follow a normal distribution, so in practice we aug-\nment the Gaussian attributes with their means \u00b5 and vari-\nances \u03c3, thus enabling subsequent sampling. We also adopt\nthe same tri-plane decomposition for these latent distribu-\ntion parameters. Then we can render a \u201cdistribution param-\neter map\u201d to sample the latent features, which will be fed\ninto a super-resolution module G. Meanwhile, we also ren-\nder an RGB image for auxiliary supervision. The distribu-\ntion map consist of a latent mean map and a latent variance\nmap, denoted as I\u00b5 and I\u03c3, respectively, which capture the\nmean and variance of latent features. By leveraging this\ndistribution map, we then proceed to sample a latent feature\nmap that will be fed into G:\nIl = I\u00b5 + tI\u03c3, t \u223c N(0, 1).\n(4)\nThen, we concatenate the rendered RGB images Ir and\nthe latent feature maps Il and feed them into the super-\nresolution module to synthesize high-resolution images:\nIG = G(Ir, Il).\n(5)\nAs mentioned above, the edited images are temporally in-\nconsistent due to the frame-by-frame editing. To avoid the\ndiscrete and inconsistent issue of direct supervision, our\nmethod learns a more continuous generation space via GAN\nfrom these edited images. Specifically, the generated im-\nages IG are considered as fake samples, while the edited\nimages are regarded as real samples. The GAN loss can be\nformulated as follows:\nLD = D(IG) \u2212 D(Ied) + Lgp\nLG = \u2212D(IG),\n(6)\nwhere Ied are the edited images generated by diffusion-\nbased editor, D is the discriminator and Lgp is the Wasser-\nstein GAN gradient penalty loss [1].\n5.2. Multi-level Generation with Guidance\nWhen training GAN with the loss in Eqn. 6, we observe\nthat the learning process often suffers from mode collapse\nFigure 3. Illustration of the Generation with Multi-level Guid-\nance: we propose a three-level image generation process to bal-\nance the generator training, where Eg denotes for the global en-\ncoder and El denotes for the local encoder.\nissue. This may be caused by the fact that there is a limited\nnumber of edited images, and it is easy for the discrimi-\nnator to learn how to distinguish between different sources\nof samples. To stabilize the learning process, we propose\nto extract multi-level information from the edited images\nand use these global and local cues to guide the learning of\nthe generator. As shown in Fig. 3, during the training pro-\ncess, we construct two networks\u2014global encoder Eg and\nlocal encoder El\u2014to extract the global code and local fea-\nture maps of the edited image Ied, respectively. With these\nconditions as additional inputs, our generator can synthesize\nimages on three levels:\nI1\nG = G(Ir, Il, Eg(Ir))\nI2\nG = G(Ir, Il, Eg(Ied))\nI3\nG = G(Ir, El(Ied), Eg(Ied))\n(7)\nThroughout the progression from level 1 to level 3, the gen-\nerator produces images that gradually approach real edited\nimages:\n\u2022 At level 1, the generator directly synthesis images based\non Tensor4D.\n\u2022 At level 2, global information from the real edited images\nis introduced as conditions, guiding the generator to pro-\nduce results consistent with the overall style of the real\nimages.\n\u2022 At level 3, both the global and local information from the\nreal edited images is used as conditions, enabling the net-\nwork to generate images that exhibit consistency in both\nthe overall pattern and finer details with the real edited\nimages.\nFigure 4. Qualitative comparisons with Instruct-NeRF2NeRF(static): In a static scenario, given the prompt \u201cTurn him into Elon Musk\u201d,\ntrain the model to converge and we can see that, on the same dataset, our method (the top row) produces highly realistic renderings of human\nportraits, while instruct nerf2nerf exhibits lower levels of realism and consistency, along with unexpected distortions in facial features.\nTo facilitate training, we also utilize different losses at dif-\nferent levels:\nL1 = \u2212D(I1\nG)\nL2 = \u2212D(I2\nG) + LP (I2\nG, Ied)\nL3 = \u2212D(I3\nG) + LP (I3\nG, Ied) + \u2225I3\nG \u2212 Ied\u22251\n(8)\nLevel 1 employs the original GAN loss. At level 2, a per-\nceptual loss is introduced as an additional constraint to en-\nforce consistency in the global style. At level 3, the loss\nfunction simultaneously incorporates L1 loss, perceptual\nloss, and GAN loss as penalties, as the consistency in details\nand global style is desired. This multi-level information\nguides the generator to converge progressively towards the\ngeneration space of the diffusion model, improving training\nstability in single scenarios and accelerating convergence\ncompared to the original GAN training process.\n5.3. Training Strategy\nTo address the high iterative optimization cost associated\nwith using the diffusion-based editor, we propose several\nstrategies to further improve the efficiency of 4D editing.\nStaged Training Strategy. We adopt a staged training strat-\negy that facilitates convergence. First, we fix the flow in the\nstatic stage and focus solely on editing the canonical space.\nThis simplifies the editing process from 4D to 3D static edit-\ning, resulting in faster convergence. Once the editing of the\ncanonical space has converged, we proceed to train Gaus-\nsianPlanes across the entire 4D sequences. We also adopt\na smaller noise timestep t \u2208 U(0.02, 0.6) for the diffusion-\nbased editor in the dynamic stage since most of the editing\neffect is done in the static stage.\nBatch-based Dataset Update. To improve the editing con-\nsistency across different images, instead of editing a sin-\ngle image per iteration like InstructN2N, we group several\nimages as a batch and edit them simultaneously. In edit-\ning each batch, we incorporate an attention module [16]\nfor multi-frame image generation into our diffusion-based\neditor to capture the temporal-spatial correspondences and\nthereby improve editing consistency.\n6. Experiment\nWe primarily conduct experiments on the dynamic Ten-\nsor4D dataset, which captures dynamic half-body human\nvideos by four sparsely positioned, fixed RGB cameras.\nThe calibration is performed using a checkerboard. Each\ndata sample captures a diverse range of human motions in\n1-2 minute duration. For our experiments, we extract 2-\nsecond segments, consisting of 50 frames, from the full-\nlength videos for 4D reconstruction and editing. Further-\nmore, to showcase the capabilities of our method in 360-\ndegree scenes, we also select scanned human models from\nTwindom [82] dataset for additional evaluation. Please refer\nto the suppl. for more experiment details.\n6.1. Qualitative Evaluation\n6.1.1\nStatic scene\nSince the task of 4D editing with text has not been ad-\ndressed in previous works, we first conduct evaluation on\nstatic scene in order to validate the efficiency of our pro-\nposed methods. To validate the efficiency of our proposed\nGAN, we first conduct a comparison between NeRF+GAN\nand instruct-NeRF2NeRF under static scenes. We select\nsome human models from the Twindom dataset and sam-\npled 180 viewpoints randomly within a 360-degree range to\nrender images. Subsequently, we evaluate NeRF+GAN and\nFigure 5. Qualitative comparisons with baseline(dynamic): In a dynamic scenario, given the prompt \u201cMark Zuckerberg\u201d, compared to\nthe baseline result (the first row) that only employs the dataset update (DU) method, our proposed approach (with the addition of GAN, the\nsecond row) demonstrates higher levels of realism and consistency in our rendered results.\nMethod\nFID\u2193\nCLIP Similarity \u2191\nStatic scene\nInstructPix2Pix\n-\n0.3089\nControlNet\n-\n0.3313\nInstructNeRF2NeRF\n126.3\n0.2989\nTensor4D\n118.7\n0.3316\nTensor4D+GAN\n27.81\n0.3334\nGaussianPlanes\n49.32\n0.3301\nControl4D (Ours)\n14.11\n0.3323\nDynamic scene\nControlNet\n-\n0.3185\nTensor4D\n155.6\n0.3144\nTensor4D+GAN\n47.39\n0.3178\nGaussianPlanes\n67.58\n0.3175\nControl4D (Ours)\n18.59\n0.3192\nTable 1. Quantitative Comparisons on static and dynamic scenes.\ninstruct-NeRF2NeRF for editing with prompt \u201dTurn him\ninto Elon Musk\u201d. In Fig. 4, we present the results after\n50,000 iterations of training. Observing the results, it is\nevident that our GAN can generate images of high quality,\nexhibiting rich detail and enhanced realism. In contrast, the\ninstruct-NeRF2NeRF outputs appear smoother, with some\nissues observed in the blending of side views. This com-\nparison highlights the significant advantage of our GAN in\nterms of editing capabilities.\n6.1.2\nDynamic scene\nIn dynamic scenarios, we compare our proposed method\nand the baseline method that only utilize GaussianPlanes,\nand the results are presented in Figure 5. We also present\nthe results of different individuals engaged in various ac-\ntions, which can be referenced in Figure 6. In the base-\nline approach, where GAN-based generation is not utilized,\nGaussianPlanes is directly tasked with fitting a dynamically\nchanging editing dataset in both space and time. This di-\nrect fitting process often leads to the optimization of smooth\nresults that may lack consistency and high-quality details.\nOur proposed method incorporates GAN-based generation,\nleveraging the GAN to learn a more continuous 4D genera-\ntion space. This allows us to leverage the smooth supervi-\nsory signals for optimization. Thus, our method generates\nconsistent and high-quality results that exhibit improved fi-\ndelity and capture finer details in the dynamic editing pro-\ncess. The comparison between the baseline approach and\nour method demonstrates the effectiveness of our proposed\n4D generator in enhancing the overall quality and consis-\ntency of the generated results.\n6.2. Quantitative Experiment\nWe conducted quantitative experiments in 5 static and 4\ndynamic scenarios.\nThe results are presented in Tab. 1.\nFirst, we compare the diffusion-based editor including In-\nstructpix2pix [3] and ControlNet [83] in the context of por-\ntrait editing. ControlNet exhibited better consistency be-\ntween the subject and the editing prompt than Instruct-\nFigure 6. Qualitative results on Tensor4D dataset. Our method produces coherent and realistic outcomes that maintain high fidelity and\naccurately preserve intricate details throughout the dynamic editing procedure. The prompts we use are \u201dEmma Watson\u201d, \u201dTaylor Swift\u201d,\n\u201dIron Man wearing armor\u201d, \u201dCaptain Jack Sparrow\u201d, \u201dLionel Messi\u201d and \u201dElon Musk\u201d.\npix2pix.\nWe further compared our method, Control4D,\nwith the baseline approaches including Tensor4D, Ten-\nsor4D+GAN, and GaussianPlanes to validate the efficiency\nof our proposed representation and GAN. We evaluated the\nFr\u00b4echet Inception Distance (FID) metric [18] between the\nedited dataset and generated images.\nWe also compute\nCLIP cosine similarity [53] between the generated images\nand text. Compared with Tensor4D and Tensor4D+GAN,\nour method achieves superior performance, which demon-\nstrates the efficiency of GaussianPlanes. The results also\nreveals that our method outperforms the baseline and In-\nstructNeRF2NeRF [17] significantly, demonstrating the ef-\nfectiveness of our proposed 4D editing pipeline.\n7. Conclusions\nIn conclusion, Control4D is a novel approach for efficient,\nhigh-fidelity and temporally consistent editing in dynamic\n4D scenes. It utilizes an efficient 4D representation Gaus-\nsianPlanes and a 2D diffusion-based editor.\nBy utiliz-\ning plane-based decomposition to struct Gaussian Splat-\nting, GaussianPlanes ensure both efficiency and robustness\nfor 4D editing.\nTo tackle with the inconsistency caused\nby diffusion-based editor, Control4D leverages a GAN to\ngenerate from the editor, avoiding direct supervision. Ex-\nperimental results demonstrate Control4D\u2019s effectiveness\nin achieving photo-realistic and consistent 4D editing, sur-\npassing previous approaches in real-world scenarios. It rep-\nresents a significant advancement in text-based image edit-\ning, particularly for dynamic scenes.\nLimitations.\nDue to utilizing a canonical Gaussian\npoint clouds with flow representation, our approach relies\non learning flow within the 4D scenes to exhibit simplic-\nity and smoothness. This poses challenges for our method\nin effectively handling rapid and extensive non-rigid move-\nments. Furthermore, our method is constrained by Control-\nNet, which limits the granularity of edits to a coarse level.\nConsequently, it is unable to perform precise expression or\naction edits. Our method also requires iterative optimiza-\ntions for the editing process and cannot be accomplished in\na single step.\nReferences\n[1] Martin Arjovsky, Soumith Chintala, and L\u00b4eon Bottou.\nWasserstein generative adversarial networks.\nIn Interna-\ntional conference on machine learning, pages 214\u2013223.\nPMLR, 2017. 5\n[2] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kas-\nten, and Tali Dekel. Text2live: Text-driven layered image\nand video editing. In Computer Vision\u2013ECCV 2022: 17th\nEuropean Conference, Tel Aviv, Israel, October 23\u201327, 2022,\nProceedings, Part XV, pages 707\u2013723. Springer, 2022. 2\n[3] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\narXiv preprint arXiv:2211.09800, 2022. 4, 7\n[4] Ang Cao and Justin Johnson. Hexplane: A fast representa-\ntion for dynamic scenes. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 130\u2013141, 2023. 3\n[5] Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, and Kwan-\nYee K Wong. Dreamavatar: Text-and-shape guided 3d hu-\nman avatar generation via diffusion models. arXiv preprint\narXiv:2304.00916, 2023. 3\n[6] Duygu Ceylan, Chun-Hao Paul Huang, and Niloy J Mi-\ntra. Pix2video: Video editing using image diffusion. arXiv\npreprint arXiv:2303.12688, 2023. 2\n[7] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,\nBoxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J\nGuibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient\ngeometry-aware 3d generative adversarial networks. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 16123\u201316133, 2022. 2, 5\n[8] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia.\nFantasia3d:\nDisentangling geometry and appearance for\nhigh-quality text-to-3d content creation.\narXiv preprint\narXiv:2303.13873, 2023. 1, 3\n[9] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in Neural Informa-\ntion Processing Systems, 34:8780\u20138794, 2021. 2\n[10] Yilun Du, Yinan Zhang, Hong-Xing Yu, Joshua B Tenen-\nbaum, and Jiajun Wu.\nNeural radiance flow for 4d view\nsynthesis and video processing.\nIn 2021 IEEE/CVF In-\nternational Conference on Computer Vision (ICCV), pages\n14304\u201314314. IEEE Computer Society, 2021. 3\n[11] Patrick Esser,\nJohnathan Chiu,\nParmida Atighehchian,\nJonathan Granskog, and Anastasis Germanidis.\nStructure\nand content-guided video synthesis with diffusion models.\narXiv preprint arXiv:2302.03011, 2023. 2\n[12] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahb\u00e6k\nWarburg, Benjamin Recht, and Angjoo Kanazawa. K-planes:\nExplicit radiance fields in space, time, and appearance. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), pages 12479\u201312488,\n2023. 1, 3\n[13] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial networks. Commu-\nnications of the ACM, 63(11):139\u2013144, 2020. 2, 4\n[14] Jiatao\nGu,\nLingjie\nLiu,\nPeng\nWang,\nand\nChristian\nTheobalt.\nStylenerf:\nA style-based 3d-aware genera-\ntor for high-resolution image synthesis.\narXiv preprint\narXiv:2110.08985, 2021. 2\n[15] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo\nZhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vec-\ntor quantized diffusion model for text-to-image synthesis. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 10696\u201310706, 2022. 2\n[16] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\npersonalized text-to-image diffusion models without specific\ntuning. arXiv preprint arXiv:2307.04725, 2023. 6\n[17] Ayaan Haque, Matthew Tancik, Alexei A Efros, Alek-\nsander Holynski, and Angjoo Kanazawa. Instruct-nerf2nerf:\nEditing 3d scenes with instructions.\narXiv preprint\narXiv:2303.12789, 2023. 1, 2, 3, 8\n[18] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. Advances in neural information processing systems,\n30, 2017. 8\n[19] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 2\n[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in Neural Information\nProcessing Systems, 33:6840\u20136851, 2020. 2\n[21] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022. 2\n[22] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William\nChan, Mohammad Norouzi, and David J Fleet. Video dif-\nfusion models. arXiv preprint arXiv:2204.03458, 2022. 2\n[23] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry\nKalenichenko, Weijun Wang, Tobias Weyand, Marco An-\ndreetto, and Hartwig Adam. Mobilenets: Efficient convolu-\ntional neural networks for mobile vision applications. arXiv\npreprint arXiv:1704.04861, 2017. 1\n[24] Xin Huang, Ruizhi Shao, Qi Zhang, Hongwen Zhang, Ying\nFeng, Yebin Liu, and Qing Wang. Humannorm: Learning\nnormal diffusion model for high-quality and realistic 3d hu-\nman generation. arXiv preprint arXiv:2310.01406, 2023. 3\n[25] Mustafa Is\u00b8\u0131k, Martin R\u00a8unz, Markos Georgopoulos, Taras\nKhakhulin, Jonathan Starck, Lourdes Agapito, and Matthias\nNie\u00dfner. Humanrf: High-fidelity neural radiance fields for\nhumans in motion. arXiv preprint arXiv:2305.06356, 2023.\n3\n[26] Dadong Jiang, Zhihui Ke, Xiaobo Zhou, and Xidong Shi.\n4d-editor: Interactive object-level editing in dynamic neural\nradiance fields via 4d semantic segmentation. arXiv preprint\narXiv:2310.16858, 2023. 1\n[27] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler,\nand George Drettakis.\n3d gaussian splatting for real-time\nradiance field rendering.\nACM Transactions on Graphics\n(ToG), 42(4):1\u201314, 2023. 2, 3\n[28] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nvosyan,\nRoberto\nHenschel,\nZhangyang\nWang,\nShant\nNavasardyan, and Humphrey Shi. Text2video-zero: Text-to-\nimage diffusion models are zero-shot video generators. arXiv\npreprint arXiv:2303.13439, 2023. 2\n[29] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan\nHo. Variational diffusion models. Advances in neural infor-\nmation processing systems, 34:21696\u201321707, 2021. 2\n[30] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon\nGreen, Christoph Lassner, Changil Kim, Tanner Schmidt,\nSteven Lovegrove, Michael Goesele, Richard Newcombe,\net al. Neural 3d video synthesis from multi-view video. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 5521\u20135531, 2022. 3,\n1\n[31] Weiyu Li, Rui Chen, Xuelin Chen, and Ping Tan. Sweet-\ndreamer: Aligning geometric priors in 2d diffusion for con-\nsistent text-to-3d. arXiv preprint arXiv:2310.02596, 2023.\n2\n[32] Yu-Jhe Li and Kris Kitani.\n3d-clfusion: Fast text-to-3d\nrendering with contrastive latent diffusion. arXiv preprint\narXiv:2303.11938, 2023. 3\n[33] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fi-\ndler, Ming-Yu Liu, and Tsung-Yi Lin.\nMagic3d: High-\nresolution text-to-3d content creation.\narXiv preprint\narXiv:2211.10440, 2022. 1, 3\n[34] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai,\nHujun Bao, and Xiaowei Zhou.\nEfficient neural radiance\nfields for interactive free-viewpoint video. In SIGGRAPH\nAsia Conference Proceedings, 2022. 1\n[35] Jia-Wei Liu, Yan-Pei Cao, Weijia Mao, Wenqiao Zhang,\nDavid Junhao Zhang, Jussi Keppo, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou.\nDevrf:\nFast deformable\nvoxel radiance fields for dynamic scenes.\narXiv preprint\narXiv:2205.15723, 2022. 1, 3\n[36] Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu\nSarkar, Jiatao Gu, and Christian Theobalt.\nNeural actor:\nNeural free-view synthesis of human actors with pose con-\ntrol.\nACM Transactions on Graphics (TOG), 40(6):1\u201316,\n2021. 3\n[37] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot one image to 3d object, 2023. 3\n[38] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and\nDeva Ramanan.\nDynamic 3d gaussians:\nTracking\nby persistent dynamic view synthesis.\narXiv preprint\narXiv:2308.09713, 2023. 3\n[39] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and\nDaniel Cohen-Or. Latent-nerf for shape-guided generation\nof 3d shapes and textures. arXiv preprint arXiv:2211.07600,\n2022. 1\n[40] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. Communications of the ACM, 65(1):99\u2013106, 2021.\n2\n[41] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a mul-\ntiresolution hash encoding. ACM Transactions on Graphics\n(ToG), 41(4):1\u201315, 2022. 1\n[42] Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian\nRichardt, and Yong-Liang Yang.\nHologan: Unsupervised\nlearning of 3d representations from natural images.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 7588\u20137597, 2019. 2\n[43] Thu H Nguyen-Phuoc, Christian Richardt, Long Mai,\nYongliang Yang, and Niloy Mitra.\nBlockgan: Learning\n3d object-aware scene representations from unlabelled im-\nages. Advances in neural information processing systems,\n33:6767\u20136778, 2020. 2\n[44] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. arXiv preprint\narXiv:2112.10741, 2021. 2\n[45] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and\nAndreas Geiger. Differentiable volumetric rendering: Learn-\ning implicit 3d representations without 3d supervision. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 3504\u20133515, 2020. 3\n[46] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien\nBouaziz, Dan B Goldman, Steven M Seitz, and Ricardo\nMartin-Brualla. Nerfies: Deformable neural radiance fields.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 5865\u20135874, 2021. 3\n[47] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T\nBarron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-\nBrualla, and Steven M Seitz.\nHypernerf:\nA higher-\ndimensional representation for topologically varying neural\nradiance fields. arXiv preprint arXiv:2106.13228, 2021. 1, 3\n[48] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang,\nQing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body:\nImplicit neural representations with structured latent codes\nfor novel view synthesis of dynamic humans. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 9054\u20139063, 2021. 3\n[49] Ryan Po and Gordon Wetzstein.\nCompositional 3d scene\ngeneration using locally conditioned diffusion.\narXiv\npreprint arXiv:2303.12218, 2023. 3\n[50] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall.\nDreamfusion: Text-to-3d using 2d diffusion.\narXiv\npreprint arXiv:2209.14988, 2022. 1, 3\n[51] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and\nFrancesc Moreno-Noguer.\nD-nerf: Neural radiance fields\nfor dynamic scenes. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n10318\u201310327, 2021. 1, 3\n[52] Yi-Ling Qiao, Alexander Gao, and Ming Lin.\nNeu-\nphysics: Editable neural geometry and physics from monoc-\nular videos. Advances in Neural Information Processing Sys-\ntems, 35:12841\u201312854, 2022. 1\n[53] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 2, 8\n[54] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022. 2\n[55] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10684\u201310695, 2022. 2, 1\n[56] Johannes\nLutz\nSch\u00a8onberger\nand\nJan-Michael\nFrahm.\nStructure-from-motion revisited.\nIn Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2016. 1\n[57] Hoigi Seo, Hayeon Kim, Gwanghyun Kim, and Se Young\nChun.\nDitto-nerf: Diffusion-based iterative text to omni-\ndirectional 3d model.\narXiv preprint arXiv:2304.02827,\n2023. 3\n[58] SG 161222. Realistic vision v5 stable diffusion checkpoint,\n2023. 1\n[59] Ruizhi Shao, Zerong Zheng, Hanzhang Tu, Boning Liu,\nHongwen Zhang, and Yebin Liu. Tensor4d: Efficient neural\n4d decomposition for high-fidelity dynamic reconstruction\nand rendering. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n16632\u201316642, 2023. 1, 2, 4\n[60] Qiuhong Shen, Xingyi Yang, and Xinchao Wang. Anything-\n3d: Towards single-view anything reconstruction in the wild.\narXiv preprint arXiv:2304.10261, 2023. 3\n[61] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li,\nand Xiao Yang. Mvdream: Multi-view diffusion for 3d gen-\neration. arXiv preprint arXiv:2308.16512, 2023. 3\n[62] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint arXiv:2209.14792,\n2022. 2\n[63] Uriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual,\nIurii Makarov, Filippos Kokkinos, Naman Goyal, Andrea\nVedaldi, Devi Parikh, Justin Johnson, et al. Text-to-4d dy-\nnamic scene generation. arXiv preprint arXiv:2301.11280,\n2023. 3\n[64] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In International Confer-\nence on Machine Learning, pages 2256\u20132265. PMLR, 2015.\n2\n[65] Liangchen Song, Anpei Chen, Zhong Li, Zhang Chen, Lele\nChen, Junsong Yuan, Yi Xu, and Andreas Geiger.\nNerf-\nplayer: A streamable dynamic scene representation with de-\ncomposed neural radiance fields. IEEE Transactions on Visu-\nalization and Computer Graphics, 29(5):2732\u20132742, 2023.\n3\n[66] Jingxiang Sun, Bo Zhang, Ruizhi Shao, Lizhen Wang, Wen\nLiu, Zhenda Xie, and Yebin Liu. Dreamcraft3d: Hierarchi-\ncal 3d generation with bootstrapped diffusion prior. arXiv\npreprint arXiv:2310.16818, 2023. 3\n[67] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang\nZeng. Dreamgaussian: Generative gaussian splatting for effi-\ncient 3d content creation. arXiv preprint arXiv:2309.16653,\n2023. 2\n[68] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi,\nLizhuang Ma, and Dong Chen. Make-it-3d: High-fidelity\n3d creation from a single image with diffusion prior. arXiv\npreprint arXiv:2303.14184, 2023. 3\n[69] Ayush Tewari, Justus Thies, Ben Mildenhall, Pratul Srini-\nvasan, Edgar Tretschk, Wang Yifan, Christoph Lassner, Vin-\ncent Sitzmann, Ricardo Martin-Brualla, Stephen Lombardi,\net al. Advances in neural rendering. In Computer Graphics\nForum, pages 703\u2013735. Wiley Online Library, 2022. 2\n[70] Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael\nZollh\u00a8ofer, Christoph Lassner, and Christian Theobalt. Non-\nrigid neural radiance fields: Reconstruction and novel view\nsynthesis of a dynamic scene from monocular video.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 12959\u201312970, 2021. 3\n[71] Chaoyang Wang, Ben Eckart, Simon Lucey, and Orazio\nGallo. Neural trajectory fields for dynamic novel view syn-\nthesis. arXiv preprint arXiv:2105.05994, 2021. 3\n[72] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,\nand Greg Shakhnarovich.\nScore jacobian chaining: Lift-\ning pretrained 2d diffusion models for 3d generation. arXiv\npreprint arXiv:2212.00774, 2022. 3\n[73] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\nKomura, and Wenping Wang. Neus: Learning neural im-\nplicit surfaces by volume rendering for multi-view recon-\nstruction. Advances in Neural Information Processing Sys-\ntems, 34:27171\u201327183, 2021. 3\n[74] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao,\nJan Kautz, and Bryan Catanzaro. High-resolution image syn-\nthesis and semantic manipulation with conditional gans. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 8798\u20138807, 2018. 1\n[75] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and\ndiverse text-to-3d generation with variational score distilla-\ntion. Advances in Neural Information Processing Systems,\n2023. 3\n[76] Chung-Yi Weng,\nBrian Curless,\nPratul P Srinivasan,\nJonathan T Barron, and Ira Kemelmacher-Shlizerman. Hu-\nmannerf: Free-viewpoint rendering of moving people from\nmonocular video.\nIn Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n16210\u201316220, 2022. 3\n[77] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng\nZhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang.\n4d gaussian splatting for real-time dynamic scene rendering.\narXiv preprint arXiv:2310.08528, 2023. 3\n[78] Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and\nJosh Tenenbaum. Learning a probabilistic latent space of\nobject shapes via 3d generative-adversarial modeling. Ad-\nvances in neural information processing systems, 29, 2016.\n2\n[79] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei,\nYuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and\nMike Zheng Shou. Tune-a-video: One-shot tuning of image\ndiffusion models for text-to-video generation. arXiv preprint\narXiv:2212.11565, 2022. 2\n[80] Wenqi Xian, Jia-Bin Huang, Johannes Kopf, and Changil\nKim. Space-time neural irradiance fields for free-viewpoint\nvideo. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 9421\u20139431,\n2021. 3\n[81] Zhen Xu, Sida Peng, Haotong Lin, Guangzhao He, Jiaming\nSun, Yujun Shen, Hujun Bao, and Xiaowei Zhou.\n4k4d:\nReal-time 4d view synthesis at 4k resolution. arXiv preprint\narXiv:2310.11448, 2023. 3\n[82] Tao Yu, Zerong Zheng, Kaiwen Guo, Pengpeng Liu, Qiong-\nhai Dai, and Yebin Liu. Function4d: Real-time human vol-\numetric capture from very sparse consumer rgbd sensors. In\nProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 5746\u20135756, 2021. 6\n[83] Lvmin Zhang and Maneesh Agrawala. Adding conditional\ncontrol to text-to-image diffusion models.\narXiv preprint\narXiv:2302.05543, 2023. 3, 7, 1\n[84] Zerong Zheng, Han Huang, Tao Yu, Hongwen Zhang, Yan-\ndong Guo, and Yebin Liu. Structured local radiance fields for\nhuman avatar modeling. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 15893\u201315903, 2022. 3\n[85] Zerong Zheng, Xiaochen Zhao, Hongwen Zhang, Boning\nLiu, and Yebin Liu. Avatarrex: Real-time expressive full-\nbody avatars. arXiv preprint arXiv:2305.04789, 2023. 3\n[86] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models.\narXiv preprint\narXiv:2211.11018, 2022. 2\n[87] Jun-Yan Zhu, Zhoutong Zhang, Chengkai Zhang, Jiajun Wu,\nAntonio Torralba, Josh Tenenbaum, and Bill Freeman. Vi-\nsual object networks: Image generation with disentangled 3d\nrepresentations. Advances in neural information processing\nsystems, 31, 2018. 2\nControl4D: Efficient 4D Portrait Editing with Text\nSupplementary Material\nA. Implementation Details\nA.1. GaussianPlanes\nSpatial triplane decomposition. In 3D spatial space, we\ndecompose the attributes of each point in the canonical\nGaussian point cloud, including color, opacity, latent fea-\nture, and rotation.\nThe decomposition of each attribute\nutilizes three corresponding feature planes. We employ a\nHashGrid [41] to represent each feature surface, with a hi-\nerarchy of resolutions at 16 levels, where the scale of each\nlevel is 1.3 times that of the preceding level. Each level con-\ntains 2 feature channels, and the encoded results are mapped\nto the corresponding attributes through a 256-unit MLP net-\nwork.\n4D flow decomposition. For the 4D flow, we have imple-\nmented a hierarchical decomposition using Tensor4D [59].\nIn our approach, we decompose the flow of each point\u2019s\nposition and rotation at each moment within the Gaussian\npoint cloud. This decomposition employs 9 feature planes.\nEach feature plane is represented using a HashGrid consis-\ntent with the spatial feature planes. Subsequently, the en-\ncoded results are first fused individually for the correspond-\ning three planes using three 256-unit MLPs, and then a final\nattribute output is produced through another 256-unit MLP.\nA.2. 4D Generator\nNetwork Structure. We adopt a network architecture simi-\nlar to pix2pixHD [74] for the GAN\u2019s generator and discrim-\ninator. In our generator, we introduce some modifications:\nthe input features comprise both RGB and latent features,\nwhich are concatenated together as the input.\nAddition-\nally, in the intermediate layers, we concatenate the feature\nwith its global feature code. The architecture includes three\ndownsample layers, three middle blocks, and five upsample\nlayers, thereby achieving a 4x super-resolution in the final\noutput. The number of the base feature channel in the net-\nwork is 32. The input for the 4D generator is at a resolution\nof 256, and it outputs images at a resolution of 1024. As\nfor the discriminator, we utilize the same architecture as the\npix2pixHD discriminator.\nGlobal Encoder. We utilize MobileNet [23] to extract the\nglobal code. Initially, the image is resized to a resolution\nof 224, followed by feature extraction through MobileNet\u2019s\nlayers. We maps the final feature of MobileNet to a 64-\ndimensional global feature code.\nLocal Encoder. We employ an encoder similar to the VAE\nencoder used in Stable Diffusion [55] as our local encoder.\nOur local encoder compresses the original image to a quar-\nter of its original size through two downsample layers, and\nthe number of \u201dz channels\u201d is set to 4. The base channel\nnumber of our network is 32.\nA.3. Diffusion-based Editor\nWe utilize ControlNet [83] as our diffusion-based editor. To\nachieve better control effects, we employ both normal and\nOpenPose as control signals. The control strength for nor-\nmal is set at 0.5, while for OpenPose, it is 1.0. Additionally,\nwe use the RealisticVision [58], an SD1.5 model, to obtain\nmore realistic editing effects. Additionally, before feeding\nthe images into ControlNet, we resize the 1024-resolution\nimages down to a resolution of 512.\nA.4. 4D Reconstruction based on GaussianPlanes\nWe initially reconstruct the 4D scenes using Gaussian-\nPlanes. Our experiments primarily focus on the Tensor4D\ndataset, and we also showcase some results in challeng-\ning scenes, including those from Neural3DVideo [30], EN-\neRF [34] and InstructNeRF2NeRF [17]. For the Tensor4D\ndataset, we employ a Gaussian sphere for initialization, with\na point cloud size of 5,000 and a radius of 1. For the EN-\neRF, Neural3DVideo and InstructNeRF2NeRF datasets, we\nutilize the point cloud from the first frame of COLMAP [56]\nas the initialization.\nDuring the training process, to ensure the stability of the\ncanonical Gaussian point cloud, we adopt a weighted strat-\negy for selecting training frames. There is a 50% probabil-\nity that we choose all frames from the first moment and a\n50% probability that we randomly select frames from other\nmoments. This approach is designed to balance the repre-\nsentation of the initial frame with the dynamic aspects of\nthe remaining video content. Simultaneously, we are also\ntraining the 4D generator in preparation for 4D editing.\nDuring the training process, the learning rate for the\npoint cloud positions linearly decays from 0.00016 to\n0.0000016. The learning rates for scaling, color, opacity,\nand rotation are set at 0.005, while the learning rate for flow\nis 0.00025. The learning rate for the 4D generator is 0.001.\nThe gradient threshold for splitting is set to 0.0002, and the\ninterval of densification and pruning is 200. We employ\nL1 loss to train GaussianPlanes, with a weight of 1.0. For\ntraining the generator, we use L1 loss, perceptual loss, and\nGAN loss, with weights of 1.0, 1.0, and 0.01, respectively.\nThe discriminator is trained using GAN loss and gradient\npenalty regularization, with respective weights of 1.0 and\n0.01.\nFigure 7. Ablation study of 4D generator. First row: Results utilizing only GaussianPlanes, second row: Results achieved by combining\n4D generator. The prompts used here are \u201dElf King\u201d and \u201dDoctor Strange\u201d.\nFigure 8. Control4D results on ENeRF dataset. The prompts are \u201dIron Man\u201d and \u201dLionel Messi\u201d.\nA.5. 4D Editing Process\nDuring the 4D editing process, we utilize two GPUs\n(RTX3090) for training.\nOne GPU is dedicated to run-\nning edits for each image, while the other GPU is tasked\nwith running GaussianPlanes and rendering images with the\n4D generator. These two processes are executed in paral-\nlel. For complex multi-camera 4D scenes, including Neu-\nral3DVideo and ENeRF, we do not edit using all cameras.\nInstead, we use images from all cameras at the first moment\nand randomly select images from four cameras at other mo-\nments to form the dataset.\nThe first 1000 steps of our training are for static edit-\ning, followed by 4000 steps for dynamic editing. During\nstatic editing, the noise added to the diffusion-based editor\nis U(0.02, 0.98), which is reduced to U(0.02, 0.6) for dy-\nnamic editing. The steps of diffusion model is set to 20 and\nwe use the DDIM scheduler. To enhance robustness, we\nlower the learning rates during the editing process. Specifi-\nFigure 9. Ablation Study of GaussianPlanes. First row: w/o spatial triplane decomposition. Second row: w/o 4D flow decomposition.\nThird row: Control4D results.\ncally, the learning rate for point positions is 0.000016, while\nthe learning rates for scaling, color, opacity, and rotation re-\nmain at 0.005, and the learning rate for flow is 0.0001. The\n4D generator\u2019s learning rate is set at 0.0001. In the edit-\ning process, we don\u2019t split or prune Gaussian points. In the\nmulti-level guidance, the probability of selecting each level\nis equal. The weights of the various losses in 4D editing\nremain consistent with those in the reconstruction process.\nB. More Comparisons\nWe\nconducted\nfurther\ncomparisons\nwith\nInstruct-\nNeRF2NeRF on their dataset.\nAs shown in Fig. 13,\nour method noticeably surpasses InstructNeRF2NeRF in\nterms of realism and quality. Additionally, our optimization\nprocess is extremely fast, completing editing tasks in just\n5 minutes, whereas InstructNeRF2NeRF requires at least\nabout 5 hours.\nThis makes our method 60 times more\nefficient than InstructNeRF2NeRF.\nC. More Ablation Study\nGaussianPlanes. We conducted more ablation studies on\nGaussianPlanes. As shown in Fig. 9, when the spatial tri-\nplane decomposition is not used, the results exhibit sig-\nnificant noise.\nWithout the decomposition of flow, the\nedited results become noticeably blurred, with evident oc-\ncurrences of jittering, which can be clearly observed in\nthe supplementary video. This demonstrates that our pro-\nposed plane decomposition method makes Gaussian Splat-\nting more structured and significantly enhances its robust-\nness.\n4D generator. More ablation experiments were conducted\non our proposed 4D generator. As illustrated in Fig. 7, with-\nout the use of the generator and relying solely on Gaussian-\nPlanes, the images noticeably lose many high-quality de-\nFigure 10. More Control4D results on Tensor4D dataset. The prompts are \u201dJoe Biden wearing suit\u201d, \u201dDonald Trump wearing suit\u201d, \u201dTrinity\nin The Matrix\u201d, \u201dNeo in The Matrix\u201d, \u201dJames Gordon in Batman\u201d, and \u201dJoker in Batman\u201d.\nFigure 11. Ablation study of multi-level guidance.\ntails and appear blurry. This validates the role of our 4D\ngenerator in enhancing quality.\nMulti-level guidance. Further ablation studies were per-\nformed on multi-level guidance. As shown in Fig. 11, when\nonly GAN loss is used, mode collapse occurs easily due to\nthe small size of the dataset. When only the first and third\nlevels are used, the results become blurred. This indicates\nthat our progressive guidance strategy improves the stabil-\nity of the GAN and gradually enhances the quality of the\nrendered images.\nD. More Results\nOur method is applicable not only to the editing of half-\nbody and heads but also to complex 4D scenes and full-\nbody human editing. More results are illustrated in Fig. 12,\n10, and 8. For dynamic editing effects, please refer to our\nsupplementary video.\nE. Social Impact\nThe primary goal of our method is to provide users with an\nadvanced tool for dynamic human editing in complex 4D\nscenes. While our approach enables intricate editing of full-\nbody humans and facilitates creative expression in digital\nenvironments, it also raises concerns about potential mis-\nuse, such as creating deceptive or misleading content. This\nchallenge is not exclusive to our method but is a common\nissue across various generative modeling techniques. Ad-\nditionally, in line with ethical considerations, our approach\nunderscores the importance of diversity, including aspects\nof gender, race, and cultural representation. It is crucial\nfor ongoing and future research in generative modeling to\nFigure 12. Control4D result on neural 3D video dataset. The prompt is \u201dMark Zuckerberg\u201d.\nFigure 13. Comparison with InstructNeRF2NeRF on InstructNeRF2NeRF dataset. First row: InstructNeRF2NeRF results with prompts\n\u201dTurn him into Albert Einstein\u201d and \u201dTurn him into Elon Musk\u201d. Second row: Control4D results with prompts \u201dAlbert Einstein\u201d and \u201dElon\nMusk\u201d.\ncontinuously engage with and reevaluate these ethical con-\nsiderations to ensure responsible use and positive societal\nimpact.\n"
  }
]