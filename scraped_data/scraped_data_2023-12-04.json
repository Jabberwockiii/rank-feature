[
  {
    "title": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces",
    "link": "https://arxiv.org/pdf/2312.00752.pdf",
    "upvote": "123",
    "text": "Mamba: Linear-Time Sequence Modeling with Selective State Spaces\nAlbert Gu*\n1 and Tri Dao*\n2\n1Machine Learning Department, Carnegie Mellon University\n2Department of Computer Science, Princeton University\nagu@cs.cmu.edu, tri@tridao.me\nAbstract\nFoundation models, now powering most of the exciting applications in deep learning, are almost universally\nbased on the Transformer architecture and its core attention module. Many subquadratic-time architectures\nsuch as linear attention, gated convolution and recurrent models, and structured state space models (SSMs)\nhave been developed to address Transformers\u2019 computational ine\ufb03ciency on long sequences, but they have not\nperformed as well as attention on important modalities such as language. We identify that a key weakness of\nsuch models is their inability to perform content-based reasoning, and make several improvements. First, simply\nletting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing\nthe model to selectively propagate or forget information along the sequence length dimension depending on\nthe current token. Second, even though this change prevents the use of e\ufb03cient convolutions, we design a\nhardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simpli\ufb01ed\nend-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast\ninference (5\u00d7 higher throughput than Transformers) and linear scaling in sequence length, and its performance\nimproves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves\nstate-of-the-art performance across several modalities such as language, audio, and genomics. On language\nmodeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice\nits size, both in pretraining and downstream evaluation.\n1\nIntroduction\nFoundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have\nemerged as an e\ufb00ective paradigm in modern machine learning. The backbone of these FMs are often sequence\nmodels, operating on arbitrary sequences of inputs from a wide variety of domains such as language, images,\nspeech, audio, time series, and genomics (Brown et al. 2020; Dosovitskiy et al. 2020; Ismail Fawaz et al. 2019;\nOord et al. 2016; Poli et al. 2023; Sutskever, Vinyals, and Quoc V Le 2014). While this concept is agnostic to\na particular choice of model architecture, modern FMs are predominantly based on a single type of sequence\nmodel: the Transformer (Vaswani et al. 2017) and its core attention layer (Bahdanau, Cho, and Bengio 2015)\nThe e\ufb03cacy of self-attention is attributed to its ability to route information densely within a context window,\nallowing it to model complex data. However, this property brings fundamental drawbacks: an inability to model\nanything outside of a \ufb01nite window, and quadratic scaling with respect to the window length. An enormous body\nof research has appeared on more e\ufb03cient variants of attention to overcome these drawbacks (Tay, Dehghani,\nBahri, et al. 2022), but often at the expense of the very properties that makes it e\ufb00ective. As of yet, none of these\nvariants have been shown to be empirically e\ufb00ective at scale across domains.\nRecently, structured state space sequence models (SSMs) (Gu, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021)\nhave emerged as a promising class of architectures for sequence modeling. These models can be interpreted as a\ncombination of recurrent neural networks (RNNs) and convolutional neural networks (CNNs), with inspiration\nfrom classical state space models (Kalman 1960). This class of models can be computed very e\ufb03ciently as either a\nrecurrence or convolution, with linear or near-linear scaling in sequence length. Additionally, they have principled\n*Equal contribution.\n1\nmechanisms for modeling long-range dependencies (Gu, Dao, et al. 2020) in certain data modalities, and have\ndominated benchmarks such as the Long Range Arena (Tay, Dehghani, Abnar, et al. 2021). Many \ufb02avors of\nSSMs (Gu, Goel, and R\u00e9 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Y. Li et al. 2023; Ma et al.\n2023; Orvieto et al. 2023; Smith, Warrington, and Linderman 2023) have been successful in domains involving\ncontinuous signal data such as audio and vision (Goel et al. 2022; Nguyen, Goel, et al. 2022; Saon, Gupta, and Cui\n2023). However, they have been less e\ufb00ective at modeling discrete and information-dense data such as text.\nWe propose a new class of selective state space models, that improves on prior work on several axes to achieve the\nmodeling power of Transformers while scaling linearly in sequence length.\nSelection Mechanism.\nFirst, we identify a key limitation of prior models: the ability to e\ufb03ciently select\ndata in an input-dependent manner (i.e. focus on or ignore particular inputs). Building on intuition based on\nimportant synthetic tasks such as selective copy and induction heads, we design a simple selection mechanism by\nparameterizing the SSM parameters based on the input. This allows the model to \ufb01lter out irrelevant information\nand remember relevant information inde\ufb01nitely.\nHardware-aware Algorithm.\nThis simple change poses a technical challenge for the computation of the model;\nin fact, all prior SSMs models must be time- and input-invariant in order to be computationally e\ufb03cient. We\novercome this with a hardware-aware algorithm that computes the model recurrently with a scan instead of\nconvolution, but does not materialize the expanded state in order to avoid IO access between di\ufb00erent levels of the\nGPU memory hierarchy. The resulting implementation is faster than previous methods both in theory (scaling\nlinearly in sequence length, compared to pseudo-linear for all convolution-based SSMs) and on modern hardware\n(up to 3\u00d7 faster on A100 GPUs).\nArchitecture.\nWe simplify prior deep sequence model architectures by combining the design of prior SSM\narchitectures (Dao, Fu, Saab, et al. 2023) with the MLP block of Transformers into a single block, leading to a\nsimple and homogenous architecture design (Mamba) incorporating selective state spaces.\nSelective SSMs, and by extension the Mamba architecture, are fully recurrent models with key properties that\nmake them suitable as the backbone of general foundation models operating on sequences. (i) High quality:\nselectivity brings strong performance on dense modalities such as language and genomics. (ii) Fast training and\ninference: computation and memory scales linearly in sequence length during training, and unrolling the model\nautoregressively during inference requires only constant time per step since it does not require a cache of previous\nelements. (iii) Long context: the quality and e\ufb03ciency together yield performance improvements on real data up\nto sequence length 1M.\nWe empirically validate Mamba\u2019s potential as a general sequence FM backbone, in both pretraining quality and\ndomain-speci\ufb01c task performance, on several types of modalities and settings:\n\u2022 Synthetics. On important synthetic tasks such as copying and induction heads that have been proposed as being\nkey to large language models, Mamba not only solves them easily but can extrapolate solutions inde\ufb01nitely long\n(>1M tokens).\n\u2022 Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transform-\ners on modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g.\nreducing FID on a challenging speech generation dataset by more than half). In both settings, its performance\nimproves with longer context up to million-length sequences.\n\u2022 Language Modeling. Mamba is the \ufb01rst linear-time sequence model that truly achieves Transformer-quality\nperformance, both in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters,\nwe show that Mamba exceeds the performance of a large range of baselines, including very strong modern\nTransformer training recipes based on LLaMa (Touvron et al. 2023). Our Mamba language model has 5\u00d7\ngeneration throughput compared to Transformers of similar size, and Mamba-3B\u2019s quality matches that of\nTransformers twice its size (e.g. 4 points higher avg. on common sense reasoning compared to Pythia-3B and\neven exceeding Pythia-7B).\nModel code and pre-trained checkpoints are open-sourced at https://github.com/state-spaces/mamba.\n2\nProject\nDiscretize\n\ud835\udc65!\n\u210e!\"#\n\u210e!\n\ud835\udc66!\n\ud835\udc34\n\ud835\udc36!\n\ud835\udc35!\nSelection Mechanism\nGPU \nSRAM\nGPU HBM\n\u2206!\nSelective State Space Model\nwith Hardware-aware State Expansion\nFigure 1: (Overview.) Structured SSMs independently map each channel (e.g. \ud437 = 5) of an input \ud465 to output \ud466 through a higher\ndimensional latent state \u210e (e.g. \ud441 = 4). Prior SSMs avoid materializing this large efective state (\ud437\ud441, times batch size \ud435 and sequence\nlength \ud43f) through clever alternate computation paths requiring time-invariance: the (\u2206, A, B, C) parameters are constant across\ntime. Our selection mechanism adds back input-dependent dynamics, which also requires a careful hardware-aware algorithm to\nonly materialize the expanded states in more efcient levels of the GPU memory hierarchy.\n2\nState Space Models\nStructured state space sequence models (S4) are a recent class of sequence models for deep learning that are\nbroadly related to RNNs, and CNNs, and classical state space models. They are inspired by a particular continuous\nsystem (1) that maps a 1-dimensional function or sequence \ud465(\ud461) \u2208 \u211d \u21a6 \ud466(\ud461) \u2208 \u211d through an implicit latent state\n\u210e(\ud461) \u2208 \u211d\ud441.\nConcretely, S4 models are de\ufb01ned with four parameters (\u2206, A, B, C), which de\ufb01ne a sequence-to-sequence trans-\nformation in two stages.\n\u210e\u2032(\ud461) = A\u210e(\ud461) + B\ud465(\ud461)\n(1a)\n\ud466(\ud461) = C\u210e(\ud461)\n(1b)\n\u210e\ud461 = A\u210e\ud461\u22121 + B\ud465\ud461\n(2a)\n\ud466\ud461 = C\u210e\ud461\n(2b)\n\ud472 = (C\ud469, C\ud468\ud469, \u2026 , C\ud468\n\ud458\n\ud469, \u2026 )\n(3a)\n\ud466 = \ud465 \u2217 \ud472\n(3b)\nDiscretization.\nThe \ufb01rst stage transforms the \u201ccontinuous parameters\u201d (\u2206, A, B) to \u201cdiscrete parameters\u201d (A, B)\nthrough \ufb01xed formulas A = \ud453\ud434(\u2206, A) and B = \ud453\ud435(\u2206, A, B), where the pair (\ud453\ud434, \ud453\ud435) is called a discretization rule.\nVarious rules can be used such as the zero-order hold (ZOH) de\ufb01ned in equation (4).\nA = exp(\u2206A)\nB = (\u2206A)\u22121(exp(\u2206A) \u2212 I) \u22c5 \u2206B\n(4)\nDiscretization has deep connections to continuous-time systems which can endow them with additional properties\nsuch as resolution invariance (Nguyen, Goel, et al. 2022) and automatically ensuring that the model is properly\nnormalized (Gu, Johnson, Timalsina, et al. 2023; Orvieto et al. 2023). It also has connections to gating mechanisms\nof RNNs (Gu, Gulcehre, et al. 2020; Tallec and Ollivier 2018) which we will revisit in Section 3.5. However, from\na mechanical point of view discretization can simply be viewed as the \ufb01rst step of the computation graph in the\nforward pass of an SSM. Alternate \ufb02avors of SSMs can bypass the discretization step and parameterize (A, B)\ndirectly instead (Zhang et al. 2023), which may be easier to reason about.\nComputation.\nAfter the parameters have been transformed from (\u2206, A, B, C) \u21a6 (A, B, C), the model can be\ncomputed in two ways, either as a linear recurrence (2) or a global convolution (3).\n3\nCommonly, the model uses the convolutional mode (3) for e\ufb03cient parallelizable training (where the whole input\nsequence is seen ahead of time), and switched into recurrent mode (2) for e\ufb03cient autoregressive inference (where\nthe inputs are seen one timestep at a time).\nLinear Time Invariance (LTI).\nAn important property of equations (1) to (3) is that the model\u2019s dynamics are\nconstant through time. In other words (\u2206, A, B, C), and consequently (A, B) as well, are \ufb01xed for all time-steps.\nThis property is called linear time invariance (LTI), which is deeply connected to recurrence and convolutions.\nInformally, we think of LTI SSMs as being equivalent to any linear recurrence (2a) or convolution (3b), and use\nLTI as an umbrella term for these classes of models.\nThus far, all structured SSMs have been LTI (e.g. computed as convolutions) because of fundamental e\ufb03ciency\nconstraints, discussed in Section 3.3. However, a core insight of this work is that LTI models have fundamental\nlimitations in modeling certain types of data, and our technical contributions involve removing the LTI constraint\nwhile overcoming the e\ufb03ciency bottlenecks.\nStructure and Dimensions.\nFinally, we note that structured SSMs are so named because computing them\ne\ufb03ciently also requires imposing structure on the A matrix. The most popular form of structure is diagonal\n(Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Smith, Warrington, and Linderman 2023), which we also\nuse.\nIn this case, the A \u2208 \u211d\ud441\u00d7\ud441, B \u2208 \u211d\ud441\u00d71, C \u2208 \u211d1\u00d7\ud441 matrices can all be represented by \ud441 numbers. To operate over\nan input sequence \ud465 of batch size \ud435 and length \ud43f with \ud437 channels, the SSM is applied independently to each\nchannel. Note that in this case, the total hidden state has dimension \ud437\ud441 per input, and computing it over the\nsequence length requires \ud442(\ud435\ud43f\ud437\ud441) time and memory; this is the root of the fundamental e\ufb03ciency bottleneck\naddressed in Section 3.3.\nGeneral State Space Models.\nWe note that the term state space model has a very broad meaning which simply\nrepresents the notion of any recurrent process with a latent state. It has been used to refer to many disparate\nconcepts in di\ufb00erent disciplines, including Markov decision processes (MDP) (reinforcement learning (Hafner\net al. 2020)), dynamic causal modeling (DCM) (computational neuroscience (Friston, Harrison, and Penny 2003)),\nKalman \ufb01lters (controls (Kalman 1960)), hidden Markov models (HMM) and linear dynamical systems (LDS)\n(machine learning), and recurrent (and sometimes convolutional) models at large (deep learning).\nThroughout this entire paper we use the term \u201cSSM\u201d to refer exclusively to the class of structured SSMs or S4\nmodels (Gu, Goel, and R\u00e9 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Hasani et al. 2023; Ma et al.\n2023; Smith, Warrington, and Linderman 2023) and use these terms interchangeably. For convenience we may also\ninclude derivatives of such models, such as those focusing on either the linear-recurrence or global-convolution\nviewpoints (Y. Li et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary.\nSSM Architectures.\nSSMs are standalone sequence transformations that can be incorporated into end-to-end\nneural network architectures.\n(We also sometimes call SSM architectures SSNNs, which are to SSM layers as\nCNNs are to linear convolution layers.)\nWe discuss some of the most well-known SSM architectures, many of\nwhich will also serve as our primary baselines.\n\u2022 Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which\ncan be viewed as a degenerate linear SSM.\n\u2022 H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with\nan SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which\nthey frame as a shift-SSM, before the main SSM layer.\n\u2022 Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized\nglobal convolution (Romero et al. 2021).\n\u2022 RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing\nan alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of\nconvolutions.\n4\n\u2022 RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention\napproximation (attention-free Transformer (S. Zhai et al. 2021)). Its main \u201cWKV\u201d mechanism involves LTI\nrecurrences and can be viewed as the ratio of two SSMs.\nOther closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We\nhighlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei\net al. 2017), which we view as the most closely related methods to our core selective SSM.\n3\nSelective State Space Models\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to\nincorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot\nuse convolutions, presenting a technical challenge of how to compute them e\ufb03ciently. We overcome this with\na hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then\ndescribe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some\nadditional properties of selection mechanisms (Section 3.5).\n3.1\nMotivation: Selection as a Means of Compression\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact,\nwe can view the tradeo\ufb00s of popular sequence models from this point of view. For example, attention is both\ne\ufb00ective and ine\ufb03cient because it explicitly does not compress context at all. This can be seen from the fact that\nautoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the\nslow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are\ne\ufb03cient because they have a \ufb01nite state, implying constant-time inference and linear-time training. However, their\ne\ufb00ectiveness is limited by how well this state has compressed the context.\nTo understand this principle, we focus on two running examples of synthetic tasks (Figure 2).\n\u2022 The Selective Copying task modi\ufb01es the popular Copying task (Arjovsky, Shah, and Bengio 2016) by varying\nthe position of the tokens to memorize. It requires content-aware reasoning to be able to memorize the relevant\ntokens (colored) and \ufb01lter out the irrelevant ones (white).\n\u2022 The Induction Heads task is a well-known mechanism hypothesized to explain the majority of in-context learning\nabilities of LLMs (Olsson et al. 2022). It requires context-aware reasoning to know when to produce the correct\noutput in the appropriate context (black).\nThese tasks reveal the failure mode of LTI models. From the recurrent view, their constant dynamics (e.g. the\n(A, B) transitions in (2)) cannot let them select the correct information from their context, or a\ufb00ect the hidden\nstate passed along the sequence an in input-dependent way. From the convolutional view, it is known that global\nconvolutions can solve the vanilla Copying task (Romero et al. 2021) because it only requires time-awareness,\nbut that they have di\ufb03culty with the Selective Copying task because of lack of content-awareness (Figure 2).\nMore concretely, the spacing between inputs-to-outputs is varying and cannot be modeled by static convolution\nkernels.\nIn summary, the e\ufb03ciency vs. e\ufb00ectiveness tradeo\ufb00 of sequence models is characterized by how well they compress\ntheir state: e\ufb03cient models must have a small state, while e\ufb00ective models must have a state that contains all\nnecessary information from the context. In turn, we propose that a fundamental principle for building sequence\nmodels is selectivity: or the context-aware ability to focus on or \ufb01lter out inputs into a sequential state. In\nparticular, a selection mechanism controls how information propagates or interacts along the sequence dimension\n(see Section 3.5 for more discussion).\n3.2\nImproving SSMs with Selection\nOne method of incorporating a selection mechanism into models is by letting their parameters that a\ufb00ect\ninteractions along the sequence (e.g. the recurrent dynamics of an RNN or the convolution kernel of a CNN) be\ninput-dependent.\n5\nInput\nOutput\n?\nOutput\nCopying\nSelective Copying\nInput\nInduction Heads\nSolution\nPerfectly solved by LTI (e.g. convolutional) models that do not need to look at the actual inputs\nFigure 2: (Left) The standard version of the Copying task involves constant spacing between input and output elements and is\neasily solved by time-invariant models such as linear recurrences and global convolutions. (Right Top) The Selective Copying task\nhas random spacing in between inputs and requires time-varying models that can selectively remember or ignore inputs depending\non their content. (Right Bottom) The Induction Heads task is an example of associative recall that requires retrieving an answer\nbased on context, a key ability for LLMs.\nAlgorithm 1 SSM (S4)\nInput:\n\ud465 \u2236 (\ud671, \ud67b, \ud673)\nOutput:\n\ud466 \u2236 (\ud671, \ud67b, \ud673)\n1: A \u2236 (\ud673, \ud67d) \u2190 \ud5af\ud5ba\ud5cb\ud5ba\ud5c6\ud5be\ud5cd\ud5be\ud5cb\n\u22b3 Represents structured \ud441 \u00d7 \ud441 matrix\n2: B \u2236 (\ud673, \ud67d) \u2190 \ud5af\ud5ba\ud5cb\ud5ba\ud5c6\ud5be\ud5cd\ud5be\ud5cb\n3: C \u2236 (\ud673, \ud67d) \u2190 \ud5af\ud5ba\ud5cb\ud5ba\ud5c6\ud5be\ud5cd\ud5be\ud5cb\n4: \u2206 \u2236 (\ud673) \u2190 \ud70f\u2206(\ud5af\ud5ba\ud5cb\ud5ba\ud5c6\ud5be\ud5cd\ud5be\ud5cb)\n5: A, B \u2236 (\ud673, \ud67d) \u2190 \ud5bd\ud5c2\ud5cc\ud5bc\ud5cb\ud5be\ud5cd\ud5c2\ud5d3\ud5be(\u2206, A, B)\n6: \ud466 \u2190 \ud5b2\ud5b2\ud5ac(A, B, C)(\ud465)\n\u22b3 Time-invariant: recurrence or convolution\n7: return \ud466\nAlgorithm 2 SSM + Selection (S6)\nInput:\n\ud465 \u2236 (\ud671, \ud67b, \ud673)\nOutput:\n\ud466 \u2236 (\ud671, \ud67b, \ud673)\n1: A \u2236 (\ud673, \ud67d) \u2190 \ud5af\ud5ba\ud5cb\ud5ba\ud5c6\ud5be\ud5cd\ud5be\ud5cb\n\u22b3 Represents structured \ud441 \u00d7 \ud441 matrix\n2: B \u2236 (\ud671, \ud67b, \ud67d) \u2190 \ud460\ud435(\ud465)\n3: C \u2236 (\ud671, \ud67b, \ud67d) \u2190 \ud460\ud436(\ud465)\n4: \u2206 \u2236 (\ud671, \ud67b, \ud673) \u2190 \ud70f\u2206(\ud5af\ud5ba\ud5cb\ud5ba\ud5c6\ud5be\ud5cd\ud5be\ud5cb+\ud460\u2206(\ud465))\n5: A, B \u2236 (\ud671, \ud67b, \ud673, \ud67d) \u2190 \ud5bd\ud5c2\ud5cc\ud5bc\ud5cb\ud5be\ud5cd\ud5c2\ud5d3\ud5be(\u2206, A, B)\n6: \ud466 \u2190 \ud5b2\ud5b2\ud5ac(A, B, C)(\ud465)\n\u22b3 Time-varying: recurrence (scan) only\n7: return \ud466\nAlgorithms 1 and 2 illustrates the main selection mechanism that we use. The main di\ufb00erence is simply making\nseveral parameters \u2206, B, C functions of the input, along with the associated changes to tensor shapes throughout.\nIn particular, we highlight that these parameters now have a length dimension \ud43f, meaning that the model has\nchanged from time-invariant to time-varying. (Note that shape annotations were described in Section 2). This\nloses the equivalence to convolutions (3) with implications for its e\ufb03ciency, discussed next.\nWe speci\ufb01cally choose \ud460\ud435(\ud465) = \ud5ab\ud5c2\ud5c7\ud5be\ud5ba\ud5cb\ud441(\ud465), \ud460\ud436(\ud465) = \ud5ab\ud5c2\ud5c7\ud5be\ud5ba\ud5cb\ud441(\ud465), \ud460\u2206(\ud465) = \ud5a1\ud5cb\ud5c8\ud5ba\ud5bd\ud5bc\ud5ba\ud5cc\ud5cd\ud437(\ud5ab\ud5c2\ud5c7\ud5be\ud5ba\ud5cb1(\ud465)), and \ud70f\u2206 = \ud5cc\ud5c8\ud5bf\ud5cd\ud5c9\ud5c5\ud5ce\ud5cc,\nwhere \ud5ab\ud5c2\ud5c7\ud5be\ud5ba\ud5cb\ud451 is a parameterized projection to dimension \ud451. The choice of \ud460\u2206 and \ud70f\u2206 is due to a connection to\nRNN gating mechanisms explained in Section 3.5.\n3.3\nEfcient Implementation of Selective SSMs\nHardware-friendly architectures such as convolutions (Krizhevsky, Sutskever, and Hinton 2012) and Transform-\ners (Vaswani et al. 2017) enjoy widespread application. Here we aim to make selective SSMs e\ufb03cient on modern\nhardware (GPU) as well. The selection mechanism is quite natural, and earlier works attempted to incorporate\nspecial cases of selection, such as letting \u2206 vary over time in recurrent SSMs (Gu, Dao, et al. 2020). However, as\npreviously mentioned a core limitation in the usage of SSMs is their computational e\ufb03ciency, which was why S4\nand all derivatives used LTI (non-selective) models, most commonly in the form of global convolutions.\n3.3.1\nMotivation of Prior Models\nWe \ufb01rst revisit this motivation and overview our approach to overcome limitations of prior methods.\n\u2022 At a high level, recurrent models such as SSMs always balance a tradeo\ufb00 between expressivity and speed: as\ndiscussed in Section 3.1, models with larger hidden state dimension should be more e\ufb00ective but slower. Thus\n6\nwe want to maximize hidden state dimension without paying speed and memory costs.\n\u2022 Note that the recurrent mode is more \ufb02exible than the convolution mode, since the latter (3) is derived from\nexpanding the former (2) (Gu, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021). However, this would require\ncomputing and materializing the latent state \u210e with shape (\ud671, \ud67b, \ud673, \ud67d), much larger (by a factor of \ud441, the SSM\nstate dimension) than the input \ud465 and output \ud466 of shape (\ud671, \ud67b, \ud673). Thus the more e\ufb03cient convolution mode was\nintroduced which could bypass the state computation and materializes a convolution kernel (3a) of only (\ud671, \ud67b, \ud673).\n\u2022 Prior LTI SSMs leverage the dual recurrent-convolutional forms to increase the e\ufb00ective state dimension by a\nfactor of \ud441 (\u2248 10 \u2212 100), much larger than traditional RNNs, without e\ufb03ciency penalties.\n3.3.2\nOverview of Selective Scan: Hardware-Aware State Expansion\nThe selection mechanism is designed to overcome the limitations of LTI models; at the same time, we therefore\nneed to revisit the computation problem of SSMs. We address this with three classical techniques: kernel fusion,\nparallel scan, and recomputation. We make two main observations:\n\u2022 The naive recurrent computation uses \ud442(\ud435\ud43f\ud437\ud441) FLOPs while the convolutional computation uses \ud442(\ud435\ud43f\ud437 log(\ud43f))\nFLOPs, and the former has a lower constant factor. Thus for long sequences and not-too-large state dimension\n\ud441, the recurrent mode can actually use fewer FLOPs.\n\u2022 The two challenges are the sequential nature of recurrence, and the large memory usage. To address the latter,\njust like the convolutional mode, we can attempt to not actually materialize the full state \u210e.\nThe main idea is to leverage properties of modern accelerators (GPUs) to materialize the state \u210e only in more\ne\ufb03cient levels of the memory hierarchy. In particular, most operations (except matrix multiplication) are bounded\nby memory bandwidth (Dao, Fu, Ermon, et al. 2022; Ivanov et al. 2021; Williams, Waterman, and Patterson\n2009). This includes our scan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to\na signi\ufb01cant speedup compared to a standard implementation.\nConcretely, instead of preparing the scan input (A, B) of size (\ud671, \ud67b, \ud673, \ud67d) in GPU HBM (high-bandwidth memory),\nwe load the SSM parameters (\u2206, A, B, C) directly from slow HBM to fast SRAM, perform the discretization and\nrecurrence in SRAM, and then write the \ufb01nal outputs of size (\ud671, \ud67b, \ud673) back to HBM.\nTo avoid the sequential recurrence, we observe that despite not being linear it can still be parallelized with a\nwork-e\ufb03cient parallel scan algorithm (Blelloch 1990; Martin and Cundy 2018; Smith, Warrington, and Linderman\n2023).\nFinally, we must also avoid saving the intermediate states, which are necessary for backpropagation. We carefully\napply the classic technique of recomputation to reduce the memory requirements: the intermediate states are not\nstored but recomputed in the backward pass when the inputs are loaded from HBM to SRAM. As a result, the\nfused selective scan layer has the same memory requirements as an optimized transformer implementation with\nFlashAttention.\nDetails of the fused kernel and recomputation are in Appendix D. The full Selective SSM layer and algorithm is\nillustrated in Figure 1.\n3.4\nA Simplifed SSM Architecture\nAs with structured SSMs, selective SSMs are standalone sequence transformations that can be \ufb02exibly incorporated\ninto neural networks. The H3 architecture is the basis for the most well-known SSM architectures (Section 2), which\nare generally comprised of a block inspired by linear attention interleaved with an MLP (multi-layer perceptron)\nblock. We simplify this architecture by combining these two components into one, which is stacked homogenously\n(Figure 3). This is inspired by the gated attention unit (GAU) (Hua et al. 2022), which did something similar for\nattention.\nThis architecture involves expanding the model dimension \ud437 by a controllable expansion factor \ud438. For each\nblock, most of the parameters (3\ud438\ud4372) are in the linear projections (2\ud438\ud4372 for input projections, \ud438\ud4372 for output\nprojection) while the inner SSM contributes less.\nThe number of SSM parameters (projections for \u2206, B, C, and\n7\nH3\nGated MLP\nMamba\nLinear \nprojection\nSequence \ntransformation\nNonlinearity \n(activation or \nmultiplication)\nX\nX\nX\n!\nX\nConv\nSSM\nX\n!\n!\nConv\nSSM\n\u2a02\nFigure 3: (Architecture.) Our simplifed block design combines the H3 block, which is the basis of most SSM architectures, with\nthe ubiquitous MLP block of modern neural networks. Instead of interleaving these two blocks, we simply repeat the Mamba block\nhomogenously. Compared to the H3 block, Mamba replaces the frst multiplicative gate with an activation function. Compared to\nthe MLP block, Mamba adds an SSM to the main branch. For \ud70e we use the SiLU / Swish activation (Hendrycks and Gimpel 2016;\nRamachandran, Zoph, and Quoc V Le 2017).\nthe matrix A) are much smaller in comparison.\nWe repeat this block, interleaved with standard normalization\nand residual connections, to form the Mamba architecture. We always \ufb01x to \ud438 = 2 in our experiments and use two\nstacks of the block to match the 12\ud4372 parameters of a Transformer\u2019s interleaved MHA (multi-head attention) and\nMLP blocks.\nWe use the SiLU / Swish activation function (Hendrycks and Gimpel 2016; Ramachandran, Zoph,\nand Quoc V Le 2017), motivated so that the Gated MLP becomes the popular \u201cSwiGLU\u201d variant (Chowdhery\net al. 2023; Shazeer 2020; Touvron et al. 2023).\nFinally, we additionally use an optional normalization layer (we\nchoose LayerNorm (J. L. Ba, Kiros, and Hinton 2016)), motivated by RetNet\u2019s usage of a normalization layer in a\nsimilar location (Y. Sun et al. 2023).\n3.5\nProperties of Selection Mechanisms\nThe selection mechanism is a broader concept that can be applied in di\ufb00erent ways, such as to more traditional\nRNNs or CNNs, to di\ufb00erent parameters (e.g. A in Algorithm 2), or using di\ufb00erent transformations \ud460(\ud465).\n3.5.1\nConnection to Gating Mechanisms\nWe highlight the most important connection: the classical gating mechanism of RNNs is an instance of our selection\nmechanism for SSMs. We note that the connection between RNN gating and the discretization of continuous-time\nsystems is well established (Funahashi and Nakamura 1993; Tallec and Ollivier 2018). In fact, Theorem 1 is\nan improvement of Gu, Johnson, Goel, et al. (2021, Lemma 3.1) generalizing to the ZOH discretization and\ninput-dependent gates (proof in Appendix C). More broadly, \u2206 in SSMs can be seen to play a generalized role\nof the RNN gating mechanism. In line with prior work, we adopt the view that discretization of SSMs is the\nprincipled foundation of heuristic gating mechanisms.\nTheorem 1. When \ud441 = 1, A = \u22121, B = 1, \ud460\u2206 = \ud5ab\ud5c2\ud5c7\ud5be\ud5ba\ud5cb(\ud465), and \ud70f\u2206 = \ud5cc\ud5c8\ud5bf\ud5cd\ud5c9\ud5c5\ud5ce\ud5cc, then the selective SSM recurrence\n(Algorithm 2) takes the form\n\ud454\ud461 = \ud70e(\ud5ab\ud5c2\ud5c7\ud5be\ud5ba\ud5cb(\ud465\ud461))\n\u210e\ud461 = (1 \u2212 \ud454\ud461)\u210e\ud461\u22121 + \ud454\ud461\ud465\ud461.\n(5)\nAs mentioned in Section 3.2, our speci\ufb01c choices of \ud460\u2206, \ud70f\u2206 is from this connection. In particular, note that if a\ngiven input \ud465\ud461 should be completely ignored (as necessary in the synthetic tasks), all \ud437 channels should ignore it,\nand so we project the input down to 1 dimension before repeating/broadcasting with \u2206.\n8\n3.5.2\nInterpretation of Selection Mechanisms\nWe elaborate on two particular mechanistic e\ufb00ects of selection.\nVariable Spacing.\nSelectivity allows \ufb01ltering out irrelevant noise tokens that may occur between inputs of\ninterest. This is exempli\ufb01ed by the Selective Copying task, but occurs ubiquitously in common data modalities,\nparticularly for discrete data \u2013 for example the presence of language \ufb01llers such as \u201cum\u201d. This property arises\nbecause the model can mechanistically \ufb01lter out any particular input \ud465\ud461, for example in the gated RNN case\n(Theorem 1) when \ud454\ud461 \u2192 0.\nFiltering Context.\nIt has been empirically observed that many sequence models do not improve with longer\ncontext (F. Shi et al. 2023), despite the principle that more context should lead to strictly better performance. An\nexplanation is that many sequence models cannot e\ufb00ectively ignore irrelevant context when necessary; an intuitive\nexample are global convolutions (and general LTI models). On the other hand, selective models can simply reset\ntheir state at any time to remove extraneous history, and thus their performance in principle improves monotonicly\nwith context length (e.g. Section 4.3.2).\nBoundary Resetting.\nIn settings where multiple independent sequences are stitched together, Transformers\ncan keep them separate by instantiating a particular attention mask, while LTI models will bleed information\nbetween the sequences. Selective SSMs can also reset their state at boundaries (e.g. \u2206\ud461 \u2192 \u221e or Theorem 1 when\n\ud454\ud461 \u2192 1). These settings may occur arti\ufb01cially (e.g. packing documents together to improve hardware utilization)\nor naturally (e.g. episode boundaries in reinforcement learning (Lu et al. 2023)).\nAdditionally, we elaborate on e\ufb00ects of each selective parameter.\nInterpretation of \u2206.\nIn general, \u2206 controls the balance between how much to focus or ignore the current input\n\ud465\ud461. It generalizes RNN gates (e.g. \ud454\ud461 in Theorem 1), mechanically, a large \u2206 resets the state \u210e and focuses on the\ncurrent input \ud465, while a small \u2206 persists the state and ignores the current input. SSMs (1)-(2) can be interpreted as\na continuous system discretized by a timestep \u2206, and in this context the intuition is that large \u2206 \u2192 \u221e represents\nthe system focusing on the current input for longer (thus \u201cselecting\u201d it and forgetting its current state) while a\nsmall \u2206 \u2192 0 represents a transient input that is ignored.\nInterpretation of A.\nWe remark that while the A parameter could also be selective, it ultimately a\ufb00ects the\nmodel only through its interaction with \u2206 via A = exp(\u2206A) (the discretization (4)). Thus selectivity in \u2206 is\nenough to ensure selectivity in (A, B), and is the main source of improvement. We hypothesize that making A\nselective in addition to (or instead of) \u2206 would have similar performance, and leave it out for simplicity.\nInterpretation of B and C.\nAs discussed in Section 3.1, the most important property of selectivity is \ufb01ltering\nout irrelevant information so that a sequence model\u2019s context can be compressed into an e\ufb03cient state. In an SSM,\nmodifying B and C to be selective allows \ufb01ner-grained control over whether to let an input \ud465\ud461 into the state \u210e\ud461 or\nthe state into the output \ud466\ud461. These can be interpreted as allowing the model to modulate the recurrent dynamics\nbased on content (input) and context (hidden states) respectively.\n3.6\nAdditional Model Details\nReal vs. Complex.\nMost prior SSMs use complex numbers in their state \u210e, which is necessary for strong\nperformance on many tasks (Gu, Goel, and R\u00e9 2022). However, it has been empirically observed that completely\nreal-valued SSMs seem to work \ufb01ne, and possibly even better, in some settings (Ma et al. 2023). We use real\nvalues as the default, which work well for all but one of our tasks; we hypothesize that the complex-real tradeo\ufb00 is\nrelated to the continuous-discrete spectrum in data modalities, where complex numbers are helpful for continuous\nmodalities (e.g. audio, video) but not discrete (e.g. text, DNA).\n9\nInitialization.\nMost prior SSMs also suggest special initializations, particularly in the complex-valued case,\nwhich can help in several settings such as low-data regimes. Our default initialization for the complex case is\nS4D-Lin and for the real case is S4D-Real (Gu, Gupta, et al. 2022), which is based on the HIPPO theory (Gu,\nDao, et al. 2020). These de\ufb01ne the \ud45b-th element of A as \u22121\u22152 + \ud45b\ud456 and \u2212(\ud45b + 1) respectively. However, we expect\nmany initializations to work \ufb01ne, particularly in the large-data and real-valued SSM regimes; some ablations are\nconsidered in Section 4.6.\nParameterization of \u2206.\nWe de\ufb01ned the selective adjustment to \u2206 as \ud460\u2206(\ud465) = \ud5a1\ud5cb\ud5c8\ud5ba\ud5bd\ud5bc\ud5ba\ud5cc\ud5cd\ud437(\ud5ab\ud5c2\ud5c7\ud5be\ud5ba\ud5cb1(\ud465)), which was\nmotivated by the mechanics of \u2206 (Section 3.5). We observe that it can be generalized from dimension 1 to a larger\ndimension \ud681. We set this to be a small fraction of \ud673, which uses a negligible number of parameters compared to\nthe main Linear projections in the block. We additionally note that the broadcasting operation can instead be\nviewed as another Linear projection, initialized to a speci\ufb01c pattern of 1\u2019s and 0\u2019s; if this projection is trainable,\nthis leads to the alternative \ud460\u2206(\ud465) = \ud5ab\ud5c2\ud5c7\ud5be\ud5ba\ud5cb\ud437(\ud5ab\ud5c2\ud5c7\ud5be\ud5ba\ud5cb\ud445(\ud465)), which can be viewed as a low-rank projection.\nIn our experiments, the \u2206 parameter (which can be viewed as a bias term) is initialized to \ud70f\u22121\n\u2206 (\ud5b4\ud5c7\ud5c2\ud5bf\ud5c8\ud5cb\ud5c6([0.001, 0.1])),\nfollowing prior work on SSMs (Gu, Johnson, Timalsina, et al. 2023).\nRemark 3.1. For brevity in our experimental results, we sometimes abbreviate selective SSMs as S6 models, because they\nare S4 models with a selection mechanism and computed with a scan.\n4\nEmpirical Evaluation\nIn Section 4.1 we test Mamba\u2019s ability to solve the two synthetic tasks motivated in Section 3.1. We then evaluate\non three domains, each evaluated on autoregressive pretraining as well as downstream tasks.\n\u2022 Section 4.2: language model pretraining (scaling laws), and zero-shot downstream evaluation.\n\u2022 Section 4.3: DNA sequence pretraining, and \ufb01ne-tuning on a long-sequence classi\ufb01cation task.\n\u2022 Section 4.4: audio waveform pretraining, and the quality of autoregressively generated speech clips.\nFinally, Section 4.5 shows Mamba\u2019s computational e\ufb03ciency at both training and inference time, and Section 4.6\nablates various components of the architecture and selective SSMs.\n4.1\nSynthetic Tasks\nFull experiment details for these tasks including task details and training protocol are in Appendix E.1.\n4.1.1\nSelective Copying\nThe Copying task is one of the most well-studied synthetic tasks for sequence modeling, originally designed to test\nthe memorization abilities of recurrent models. As discussed in Section 3.1, LTI SSMs (linear recurrences and\nglobal convolutions) can easily solve this task by only keeping track of time instead of reasoning about the data; for\nexample, by constructing a convolution kernel of exactly the right length (Figure 2). This was explicitly validated\nin earlier work on global convolutions (Romero et al. 2021). The Selective Copying task prevents this shortcut\nby randomizing the spacing between tokens. Note that this task has been introduced before as the Denoising\ntask (Jing et al. 2019).\nNote that many previous works argue that adding architecture gating (multiplicative interactions) can endow\nmodels with \u201cdata-dependence\u201d and solve related tasks (Dao, Fu, Saab, et al. 2023; Poli et al. 2023). However,\nwe \ufb01nd this explanation insu\ufb03cient intuitively because such gating does not interact along the sequence axis,\nand cannot a\ufb00ect the spacing between tokens. In particular architecture gating is not an instance of a selection\nmechanism (Appendix A).\nTable 1 con\ufb01rms that gated architectures such as H3 and Mamba only partially improve performance, while the\nselection mechanism (modifying S4 to S6) easily solves this task, particularly when combined with these more\npowerful architectures.\n10\nModel\nArch.\nLayer\nAcc.\nS4\nNo gate\nS4\n18.3\n-\nNo gate\nS6\n97.0\nH3\nH3\nS4\n57.0\nHyena\nH3\nHyena\n30.1\n-\nH3\nS6\n99.7\n-\nMamba\nS4\n56.4\n-\nMamba\nHyena\n28.4\nMamba\nMamba\nS6\n99.8\nTable 1: (Selective Copying.)\nAccuracy for combinations of architectures\nand inner sequence layers.\n102\n103\n104\n105\n106\nTest Sequence Length\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nInduction Heads Extrapolation\nMHA-Absolute\nMHA-RoPE\nMHA-xPos\nH3\nHyena\nMamba\nRandom\nTrain Length\nTable 2: (Induction Heads.) Models are trained on sequence length\n28 = 256, and tested on increasing sequence lengths of 26 = 64 up to\n220 = 1048576. Full numbers in Table 11.\n4.1.2\nInduction Heads\nInduction heads (Olsson et al. 2022) is a simple task from the mechanistic interpretability lens (Elhage et al. 2021)\nthat is surprisingly predictive of the in-context learning ability of LLMs. It requires models to perform associative\nrecall and copy: for example, if the model has seen a bigram such as \u201cHarry Potter\u201d in the sequence, then the\nnext time \u201cHarry\u201d appears in the same sequence, the model should be able to predict \u201cPotter\u201d by copying from\nhistory.\nDataset.\nWe train a 2-layer model on the induction heads task at sequence length 256, with a vocab size of\n16, which is comparable to prior work on this task (Dao, Fu, Saab, et al. 2023) but with longer sequences. We\nadditionally investigate generalization and extrapolation abilities by evaluating on a range of sequence lengths\nfrom 26 = 64 up to 220 = 1048576 at test time.\nModels.\nFollowing established work on induction heads, we use 2 layer models, which allows attention to\nmechanistically solve the induction heads task (Olsson et al. 2022). We test both multi-head attention (8 heads,\nwith various positional encodings) and SSM variants. We use a model dimension \ud437 of 64 for Mamba and 128 for\nthe other models.\nResults.\nTable 2 shows that Mamba\u2014or more precisely, its selective SSM layer\u2014has the ability to solve the\ntask perfectly because of its ability to selectively remember the relevant token while ignoring everything else in\nbetween. It generalizes perfectly to million-length sequences, or 4000\u00d7 longer than it saw during training, while no\nother method goes beyond 2\u00d7.\nOut of positional encoding variants for attention models, xPos (which was designed for length extrapolation)\nis slightly better than the others; also note that all attention models were only tested up to sequence length\n214 = 16384 due to memory limitations. Out of other SSMs, H3 and Hyena are similar, contrary to the \ufb01ndings in\nPoli et al. (2023).\n4.2\nLanguage Modeling\nWe evaluate the Mamba architecture on standard autoregressive language modeling against other architectures, on\nboth pretraining metrics (perplexity) and zero-shot evaluations. We set the model sizes (depth and width) to\nmirror GPT3 speci\ufb01cations. We use the Pile dataset (L. Gao, Biderman, et al. 2020), and follow the training\nrecipe described in Brown et al. (2020). All training details are in Appendix E.2.\n4.2.1\nScaling Laws\nFor baselines, we compare against the standard Transformer architecture (GPT3 architecture), as well as the\nstrongest Transformer recipe we know of (here referred to as Transformer++), based on the PaLM and LLaMa\n11\n1019\n1020\nFLOPs (log scale)\n101\n6 \u00d7 100\n2 \u00d7 101\nPerplexity (log scale)\nScaling Laws on The Pile (Sequence Length 2048)\nHyena\nRWKV\nTransformer\nRetNet\nH3++\nTransformer++\nMamba\n1019\n1020\nFLOPs (log scale)\n101\n6 \u00d7 100\n2 \u00d7 101\nPerplexity (log scale)\nScaling Laws on The Pile (Sequence Length 8192)\nHyena\nRWKV\nTransformer\nRetNet\nH3++\nTransformer++\nMamba\nFigure 4: (Scaling Laws.) Models of size \u2248 125\ud440 to \u2248 1.3\ud435 parameters, trained on the Pile. Mamba scales better than all other\nattention-free models and is the frst to match the performance of a very strong \u201cTransformer++\u201d recipe that has now become\nstandard, particularly as the sequence length grows.\narchitectures (e.g. rotary embedding, SwiGLU MLP, RMSNorm instead of LayerNorm, no linear bias, and higher\nlearning rates). We also compare against other recent subquadratic architectures (Figure 4). All model details are\nin Appendix E.2.\nFigure 4 shows scaling laws under the standard Chinchilla (Ho\ufb00mann et al. 2022) protocol, on models from\n\u2248 125\ud440 to \u2248 1.3\ud435 parameters. Mamba is the \ufb01rst attention-free model to match the performance of a very\nstrong Transformer recipe (Transformer++) that has now become standard, particularly as the sequence length\ngrows. We note that full results on context length 8k are missing for the RWKV and RetNet baselines, prior\nstrong recurrent models that can also be interpreted as SSMs, due to a lack of e\ufb03cient implementation leading to\nout-of-memory or unrealistic computation requirements.\n4.2.2\nDownstream Evaluations\nTable 3 shows the performance of Mamba on a range of popular downstream zero-shot evaluation tasks. We\ncompare against the most well-known open source models at these sizes, most importantly Pythia (Biderman et al.\n2023) and RWKV (B. Peng et al. 2023) which were trained with the same tokenizer, dataset, and training length\n(300B tokens) as our models. (Note that Mamba and Pythia are trained with context length 2048, while RWKV\nwas trained with context length 1024.)\n4.3\nDNA Modeling\nMotivated by the success of large language models, there has been recent exploration into using the foundation\nmodel paradigm for genomics. DNA has been likened to language in that it consists of sequences of discrete\ntokens with a \ufb01nite vocab. It is also known for requiring long-range dependencies to model (Avsec et al. 2021).\nWe investigate Mamba as a FM backbone for pretraining and \ufb01ne-tuning in the same setting as recent works on\nlong-sequence models for DNA (Nguyen, Poli, et al. 2023). In particular, we focus on two explorations of scaling\nlaws across model size and sequence length (Figure 5), and a di\ufb03cult downstream synthetic classi\ufb01cation task\nrequiring long context (Figure 6).\nFor pretraining, we largely follow a standard causal language modeling (next token prediction) setup for the training\nand model details (see also Appendix E.2). For the dataset, we largely follow the setup of HyenaDNA (Nguyen,\nPoli, et al. 2023), which uses the HG38 dataset for pretraining consisting of a single human genome with about 4.5\nbillion tokens (DNA base pairs) in the training split.\n4.3.1\nScaling: Model Size\nIn this experiment, we investigate the scaling properties of genomics foundation models with various model\nbackbones (Figure 5 Left).\nTraining.\nTo advantage the baselines, we train on a short sequence length of 1024; as shown in Section 4.3.2, we\nexpect results to favor Mamba even more at longer sequence lengths. We \ufb01x a global batch size of 1024, for a\n12\nTable 3: (Zero-shot Evaluations.) Best results for each size in bold. We compare against open source LMs with various tokenizers,\ntrained for up to 300B tokens. Pile refers to the validation split, comparing only against models trained on the same dataset and\ntokenizer (GPT-NeoX-20B). For each model size, Mamba is best-in-class on every single evaluation result, and generally matches\nbaselines at twice the model size.\nModel\nToken.\nPile\nLAMBADA\nLAMBADA\nHellaSwag\nPIQA\nArc-E\nArc-C\nWinoGrande\nAverage\nppl \u2193\nppl \u2193\nacc \u2191\nacc \u2191\nacc \u2191\nacc \u2191\nacc \u2191\nacc \u2191\nacc \u2191\nHybrid H3-130M\nGPT2\n\u2014\n89.48\n25.77\n31.7\n64.2\n44.4\n24.2\n50.6\n40.1\nPythia-160M\nNeoX\n29.64\n38.10\n33.0\n30.2\n61.4\n43.2\n24.1\n51.9\n40.6\nMamba-130M\nNeoX\n10.56\n16.07\n44.3\n35.3\n64.5\n48.0\n24.3\n51.9\n44.7\nHybrid H3-360M\nGPT2\n\u2014\n12.58\n48.0\n41.5\n68.1\n51.4\n24.7\n54.1\n48.0\nPythia-410M\nNeoX\n9.95\n10.84\n51.4\n40.6\n66.9\n52.1\n24.6\n53.8\n48.2\nMamba-370M\nNeoX\n8.28\n8.14\n55.6\n46.5\n69.5\n55.1\n28.0\n55.3\n50.0\nPythia-1B\nNeoX\n7.82\n7.92\n56.1\n47.2\n70.7\n57.0\n27.1\n53.5\n51.9\nMamba-790M\nNeoX\n7.33\n6.02\n62.7\n55.1\n72.1\n61.2\n29.5\n56.1\n57.1\nGPT-Neo 1.3B\nGPT2\n\u2014\n7.50\n57.2\n48.9\n71.1\n56.2\n25.9\n54.9\n52.4\nHybrid H3-1.3B\nGPT2\n\u2014\n11.25\n49.6\n52.6\n71.3\n59.2\n28.1\n56.9\n53.0\nOPT-1.3B\nOPT\n\u2014\n6.64\n58.0\n53.7\n72.4\n56.7\n29.6\n59.5\n55.0\nPythia-1.4B\nNeoX\n7.51\n6.08\n61.7\n52.1\n71.0\n60.5\n28.5\n57.2\n55.2\nRWKV-1.5B\nNeoX\n7.70\n7.04\n56.4\n52.5\n72.4\n60.5\n29.4\n54.6\n54.3\nMamba-1.4B\nNeoX\n6.80\n5.04\n64.9\n59.1\n74.2\n65.5\n32.8\n61.5\n59.7\nGPT-Neo 2.7B\nGPT2\n\u2014\n5.63\n62.2\n55.8\n72.1\n61.1\n30.2\n57.6\n56.5\nHybrid H3-2.7B\nGPT2\n\u2014\n7.92\n55.7\n59.7\n73.3\n65.6\n32.3\n61.4\n58.0\nOPT-2.7B\nOPT\n\u2014\n5.12\n63.6\n60.6\n74.8\n60.8\n31.3\n61.0\n58.7\nPythia-2.8B\nNeoX\n6.73\n5.04\n64.7\n59.3\n74.0\n64.1\n32.9\n59.7\n59.1\nRWKV-3B\nNeoX\n7.00\n5.24\n63.9\n59.6\n73.7\n67.8\n33.1\n59.6\n59.6\nMamba-2.8B\nNeoX\n6.22\n4.23\n69.2\n66.1\n75.2\n69.7\n36.3\n63.5\n63.3\nGPT-J-6B\nGPT2\n\u2013\n4.10\n68.3\n66.3\n75.4\n67.0\n36.6\n64.1\n63.0\nOPT-6.7B\nOPT\n\u2013\n4.25\n67.7\n67.2\n76.3\n65.6\n34.9\n65.5\n62.9\nPythia-6.9B\nNeoX\n6.51\n4.45\n67.1\n64.0\n75.2\n67.3\n35.5\n61.3\n61.7\nRWKV-7.4B\nNeoX\n6.31\n4.38\n67.2\n65.5\n76.1\n67.8\n37.5\n61.0\n62.5\ntotal of 220 \u2248 1\ud440 tokens per batch. Models were trained for 10\ud43e gradient steps for a total of 10\ud435 tokens.\nResults.\nFigure 5 (Left) shows that Mamba\u2019s pretraining perplexity improves smoothly with model size, and\nthat Mamba scales better than both HyenaDNA and Transformer++. For example, at the largest model size of\n\u2248 40\ud440 parameters, the curve shows that Mamba can match the Transformer++ and HyenaDNA models with\nroughly 3\u00d7 to 4\u00d7 fewer parameters.\n4.3.2\nScaling: Context Length\nIn the next DNA experiment, we investigate the scaling properties of models with respect to sequence length.\nWe only compare the HyenaDNA and Mamba models, as quadratic attention becomes prohibitively expensive at\nlonger sequence lengths. We pretrain models on sequence lengths 210 = 1024, 212 = 4096, 214 = 16384, 216 = 65536,\n218 = 262144, 220 = 1048576. We \ufb01x a model size of 6 layers by width 128 (about 1.3M-1.4M parameters). Models\nwere trained for 20\ud43e gradient steps for a total of \u2248 330\ud435 tokens. The longer sequence lengths used sequence length\nwarmup similar to (Nguyen, Poli, et al. 2023).\nResults.\nFigure 5 (Right) shows that Mamba is able to make use of longer context even up to extremely long\nsequences of length 1M, and its pretraining perplexity improves as the context increases. On the other hand,\nthe HyenaDNA model gets worse with sequence length. This is intuitive from the discussion in Section 3.5 on\nproperties of the selection mechanism. In particular, LTI models cannot selectively ignore information; from a\nconvolutional perspective, a very long convolution kernel is aggregating all information across a long sequence\n13\n106\n107\nParameters (log scale)\n2.7\n2.8\n2.9\n3.0\n3.1\nPerplexity\nScaling Laws on the Human Genome (HG38)\nHyenaDNA\nMamba\nTransformer++\n103\n104\n105\n106\nSequence Length\n2.75\n2.80\n2.85\n2.90\n2.95\n3.00\nPerplexity\nScaling Laws - Sequence Length (HG38)\nHyenaDNA 1.4M\nMamba 1.4M\nMamba 7M\nFigure 5: (DNA Scaling Laws.) Pretraining on the HG38 (human genome) dataset. (Left) Fixing short context length 210 = 1024\nand increasing size from \u2248 200\ud43e to \u2248 40\ud440 parameters, Mamba scales better than baselines. (Right) Fixing model size and increasing\nsequence lengths while keeping tokens/batch and total training tokens fxed. Unlike baselines, the selection mechanism of Mamba\nfacilitates better performance with increasing context length.\n103\n104\n105\n106\nSequence Length\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nAccuracy\nFinetuning Accuracy (Species DNA Classification)\nHyenaDNA 1.4M\nMamba 1.4M\nMamba 7M\nRandom\nFigure 6: (Great Apes DNA Classifcation.) Accuracy after\nfne-tuning on sequences of length 210 = 1024 up to 220 =\n1048576 using pretrained models of the same context length. Nu-\nmerical results in Table 13.\n104\n105\n106\nSequence Length\n1.300\n1.325\n1.350\n1.375\n1.400\n1.425\n1.450\n1.475\nBits Per Byte\nScaling Laws - Sequence Length (YouTubeMix)\nS4+FFN\nMamba\nFigure 7: (Audio Pretraining.) Mamba improves performance\nover prior state-of-the-art (Sashimi) in autoregressive audio mod-\neling, while improving up to minute-long context or million-\nlength sequences (controlling for computation).\nwhich may be very noisy. Note that while HyenaDNA claims to improve with longer context, their results do not\ncontrol for computation time.\n4.3.3\nSynthetic Species Classifcation\nWe evaluate models on a downstream task of classifying between 5 di\ufb00erent species by randomly sampling a contigu-\nous segment of their DNA. This task is adapted from HyenaDNA, which used the species {human, lemur, mouse, pig, hippo}.\nWe modify the task to be signi\ufb01cantly more challenging by classifying between the \ufb01ve great apes species\n{human, chimpanzee, gorilla, orangutan, bonobo}, which are known to share 99% of their DNA.\n4.4\nAudio Modeling and Generation\nFor the audio waveform modality, we compare primarily to the SaShiMi architecture and training protocols (Goel\net al. 2022).\nThis model comprises\n1. a U-Net backbone with two stages of pooling by a factor \ud45d that doubles the model dimension \ud437 per stage,\n2. alternating S4 and MLP blocks in each stage.\nWe consider replacing the S4+MLP blocks with Mamba blocks.\nExperiment details are in Appendix E.4.\n4.4.1\nLong-Context Autoregressive Pretraining\nWe evaluate pretraining quality (autoregressive next-sample prediction) on YouTubeMix (DeepSound 2017), a\nstandard piano music dataset used by prior work consisting of 4 hours of solo piano music, sampled at a rate of\n14\n16000 Hz Pretraining details largely follow the standard language modeling setup (Section 4.2). Figure 7 evaluates\nthe e\ufb00ect of increasing training sequence lengths from 213 = 8192 to 220 \u2248 106, while keeping computation \ufb01xed.\n(There are some slight edge cases to the way the data is curated, which may lead to kinks in the scaling curves.\nFor example, only minute-long clips were available so the maximum sequence length is actually bounded by\n60\ud460 \u22c5 16000\ud43b\ud467 = 960000.)\nBoth Mamba and the SaShiMi (S4+MLP) baseline improve consistently with longer context lengths; Mamba is\nbetter throughout, and the gap widens at longer lengths. The main metric is bits per byte (BPB), which is a\nconstant factor log(2) of the standard negative log-likelihood (NLL) loss for pretraining other modalities.\nWe note one important detail: this is the only experiment in this paper in which we switched from the real\nparameterization to complex (Section 3.6). We show additional ablations in Appendix E.4.\n4.4.2\nAutoregressive Speech Generation\nSC09 is a benchmark speech generation dataset (Donahue, McAuley, and Puckette 2019; Warden 2018), consisting\nof 1-second clips sampled at 16000 Hz of the digits \u201czero\u201d through \u201cnine\u201d with highly variable characteristics. We\nlargely follow the autoregressive training setup and generation protocol of Goel et al. (2022).\nTable 4 shows automated metrics of the Mamba-UNet model compared to a variety of baselines from Goel et al.\n(2022): WaveNet (Oord et al. 2016), SampleRNN (Mehri et al. 2017), WaveGAN (Donahue, McAuley, and Puckette\n2019), Di\ufb00Wave (Z. Kong et al. 2021), and SaShiMi. A small Mamba model outperforms the state-of-the-art\n(and much larger) GAN- and di\ufb00usion- based models. A larger model parameter-matched to the baselines further\nimproves on \ufb01delity metrics dramatically.\nTable 5 takes the small Mamba model and investigates combinations of di\ufb00erent architectures for the outer stages\nand center stage. It shows that Mamba is consistently better than S4+MLP in the outer blocks, and Mamba >\nS4+MLP > MHA+MLP in the center blocks.\nTable 4: (SC09) Automated metrics for unconditional generation\non a challenging dataset of fxed-length speech clips.\n(Top to\nBottom) Autoregressive baselines, non-autoregressive baselines,\nMamba, and dataset metrics.\nModel\nParams\nNLL \u2193\nFID \u2193\nIS \u2191\nmIS \u2191\nAM \u2193\nSampleRNN\n35.0M\n2.042\n8.96\n1.71\n3.02\n1.76\nWaveNet\n4.2M\n1.925\n5.08\n2.27\n5.80\n1.47\nSaShiMi\n5.8M\n1.873\n1.99\n5.13\n42.57\n0.74\nWaveGAN\n19.1M\n-\n2.03\n4.90\n36.10\n0.80\nDifWave\n24.1M\n-\n1.92\n5.26\n51.21\n0.68\n+ SaShiMi\n23.0M\n-\n1.42\n5.94\n69.17\n0.59\nMamba\n6.1M\n1.852\n0.94\n6.26\n88.54\n0.52\nMamba\n24.3M\n1.860\n0.67\n7.33\n144.9\n0.36\nTrain\n-\n-\n0.00\n8.56\n292.5\n0.16\nTest\n-\n-\n0.02\n8.33\n257.6\n0.19\nTable 5: (SC09 Model Ablations) Models with 6M parameters.\nIn SaShiMi\u2019s U-Net backbone, there are 8 center blocks operat-\ning on sequence length 1000, sandwiched on each side by 8 outer\nblocks on sequence length 4000, sandwiched by 8 outer blocks\non sequence length 16000 (40 blocks total). The architecture of\nthe 8 center blocks are ablated independently of the rest. Note\nthat Transformers (MHA+MLP) were not tested in the more im-\nportant outer blocks because of efciency constraints.\nOuter\nCenter\nNLL \u2193\nFID \u2193\nIS \u2191\nmIS \u2191\nAM \u2193\nS4+MLP\nMHA+MLP\n1.859\n1.45\n5.06\n47.03\n0.70\nS4+MLP\nS4+MLP\n1.867\n1.43\n5.42\n53.54\n0.65\nS4+MLP\nMamba\n1.859\n1.42\n5.71\n56.51\n0.64\nMamba\nMHA+MLP\n1.850\n1.37\n5.63\n58.23\n0.62\nMamba\nS4+MLP\n1.853\n1.07\n6.05\n73.34\n0.55\nMamba\nMamba\n1.852\n0.94\n6.26\n88.54\n0.52\n4.5\nSpeed and Memory Benchmarks\nWe benchmark the speed of the SSM scan operation (state expansion \ud441 = 16), as well as the end-to-end inference\nthroughput of Mamba, in Figure 8. Our e\ufb03cient SSM scan is faster than the best attention implementation that\nwe know of (FlashAttention-2 (Dao 2023)) beyond sequence length 2K, and up to 20-40\u00d7 faster than a standard\nscan implementation in PyTorch. Mamba achieves 4-5\u00d7 higher inference throughput than a Transformer of similar\nsize, since without the KV cache it can use much higher batch sizes. For example, a Mamba-6.9B (untrained)\nwould have higher inference throughput than a 5\u00d7 smaller Transformer-1.3B. Details in Appendix E.5, which\nadditionally includes a benchmark of memory consumption.\n15\n512\n1k\n2k\n4k\n8k\n16k\n32k\n64k\n128k\n256k\n512k\nSequence length\n0.1\n1\n10\n100\n1000\nTime (ms)\nScan vs Convolution vs Attention time (A100 80GB PCIe)\nFlashAttention-2\nConvolution\nScan (PyTorch)\nScan (ours)\nOOM\n1\n2\n4\n8\n16\n32\n64\n128\nBatch size\n500\n1000\n1500\nThroughput (tokens / s)\n140\n247\n441\n744\n1089\n1445\n1688\n1814\n79\n132\n199\n265\n323\n364\nOOM\nOOM\n58\n101\n172\n261\n364\n443\n490\n515\n46\n66\n91\n109\n120\nOOM\nOOM\nOOM\nInference throughput on A100 80GB (prompt length 2048)\nMamba 1.4B\nTransformer 1.3B\nMamba 6.9B\nTransformer 6.7B\nFigure 8: (Efciency Benchmarks.) (Left) Training: our efcient scan is 40\u00d7 faster than a standard implementation. (Right)\nInference: as a recurrent model, Mamba can achieve 5\u00d7 higher throughput than Transformers.\n4.6\nModel Ablations\nWe perform a series of detailed ablations on components of our model, focusing on the setting of language modeling\nwith size \u2248 350M models at Chinchilla token counts (same setting as Figure 4).\n4.6.1\nArchitecture\nTable 6 investigates the e\ufb00ects of the architecture (block) and its inner SSM layer (Figure 3). We \ufb01nd that\n\u2022 Among previous non-selective (LTI) SSMs, which are equivalent to global convolutions, performance is very\nsimilar.\n\u2022 Replacing the complex-valued S4 variant from previous work with a real-valued one does not a\ufb00ect performance\nmuch, suggesting that (at least for LM) real-valued SSMs may be a better choice when accounting for hardware\ne\ufb03ciency.\n\u2022 Replacing any of these with a selective SSM (S6) signi\ufb01cantly improves performance, validating the motivation\nof Section 3.\n\u2022 The Mamba architecture performs similarly to the H3 architecture (and seems slightly better when using a\nselective layer).\nWe also investigate interleaving the Mamba block with other blocks such as MLP (a traditional architecture) MHA\n(a hybrid attention architecture) in Appendix E.2.2.\n4.6.2\nSelective SSM\nTable 7 ablates the selective SSM layer by considering di\ufb00erent combinations of selective \u2206, B, and C param-\neters (Algorithm 2), showing that \u2206 is the most important parameter due to its connection to RNN gating\n(Theorem 1).\nTable 8 considers di\ufb00erent initializations of the SSM, which have been shown to make a large di\ufb00erence in some\ndata modalities and settings (Gu, Goel, and R\u00e9 2022; Gu, Gupta, et al. 2022). On language modeling, we \ufb01nd\nthat simpler real-valued diagonal initializations (S4D-Real, row 3) instead of more standard complex-valued\nparameterizations (S4D-Lin, row 1) perform better. Random initializations also work well, consistent with \ufb01ndings\nfrom prior work (Mehta et al. 2023).\nTable 9 and Table 10 consider varying the dimension of the \u2206 and (B, C) projections respectively. Changing\nthem from static to selective provides the most bene\ufb01t, while increasing the dimensions further generally improves\nperformance modestly with a small increase in parameter count.\nOf particular note is the dramatic improvement of the selective SSM when the state size \ud441 is increased, with over\na 1.0 perplexity improvement for a cost of only 1% additional parameters. This validates our core motivation in\nSections 3.1 and 3.3.\n16\nTable 6: (Ablations: Architecture and SSM layer.) The Mamba block performs similarly to H3 while being simpler. In the\ninner layer, there is little diference among diferent parameterizations of LTI models, while selective SSMs (S6) provide a large\nimprovement. More specifcally, the S4 (real) variant is S4D-Real and the S4 (complex) variant is S4D-Lin.\nModel\nArch.\nSSM Layer\nPerplexity\nHyena\nH3\nHyena\n10.24\nH3\nH3\nS4 (complex)\n10.30\n-\nH3\nS4 (real)\n10.34\n-\nH3\nS6\n8.95\nModel\nArch.\nSSM Layer\nPerplexity\n-\nMamba\nHyena\n10.75\n-\nMamba\nS4 (complex)\n10.54\n-\nMamba\nS4 (real)\n10.56\nMamba\nMamba\nS6\n8.69\nTable 7: (Ablations: Selective parameters.) \u2206 is the most im-\nportant parameter (Theorem 1), but using multiple selective pa-\nrameters together synergizes.\nSelective \u2206\nSelective B\nSelective C\nPerplexity\n\u0017\n\u0017\n\u0017\n10.93\n\u0017\n\u0013\n\u0017\n10.15\n\u0017\n\u0017\n\u0013\n9.98\n\u0013\n\u0017\n\u0017\n9.81\n\u0013\n\u0013\n\u0013\n8.71\nTable 8: (Ablations: Parameterization of A.) The more\nstandard initializations based on S4D-Lin (Gu, Gupta, et al.\n2022) perform worse than S4D-Real or a random initializa-\ntion, when the SSM is selective.\nA\ud45b Initialization\nField\nPerplexity\nA\ud45b = \u2212 1\n2 + \ud45b\ud456\nComplex\n9.16\nA\ud45b = \u22121\u22152\nReal\n8.85\nA\ud45b = \u2212(\ud45b + 1)\nReal\n8.71\nA\ud45b \u223c exp(\ud4a9(0, 1))\nReal\n8.71\nTable 9: (Ablations: Expressivity of \u2206.)\nThe selection mechanism of \u2206 constructs\nit with a projection of the input. Project-\ning it even to dim. 1 provides a large in-\ncrease in performance; increasing it fur-\nther provides further improvements at the\ncost of a modest increase in parameters.\nState size fxed to \ud441 = 16.\nSize of \u2206 proj.\nParams (M)\nPerplexity\n-\n358.9\n9.12\n1\n359.1\n8.97\n2\n359.3\n8.97\n4\n359.7\n8.91\n8\n360.5\n8.83\n16\n362.1\n8.84\n32\n365.2\n8.80\n64\n371.5\n8.71\nTable 10: (Ablations: SSM state dimension.) (Top) Constant B and C (Bottom)\nSelective B and C. Increasing the SSM state dimension \ud441, which can be viewed as\nan expansion factor on the dimension of the recurrent state, can signifcantly improve\nperformance for a negligible cost in parameters/FLOPs, but only when B and C are\nalso selective. Size of \u2206 projection fxed to 64.\nState dimension \ud441\nParams (M)\nPerplexity\n1\n367.1\n9.88\n2\n367.4\n9.86\n4\n368.0\n9.82\n8\n369.1\n9.82\n16\n371.5\n9.81\n1\n367.1\n9.73\n2\n367.4\n9.40\n4\n368.0\n9.09\n8\n369.1\n8.84\n16\n371.5\n8.71\n5\nDiscussion\nWe discuss related work, limitations, and some future directions.\nRelated Work.\nAppendix A discusses how the selection mechanism relates to similar concepts. Appendix B has\nan extended related work of SSMs and other related models.\nNo Free Lunch: Continuous-Discrete Spectrum.\nStructured SSMs were originally de\ufb01ned as discretizations\nof continuous systems (1), and have had a strong inductive bias toward continuous-time data modalities such as\nperceptual signals (e.g. audio, video). As discussed in Sections 3.1 and 3.5, the selection mechanism overcomes\ntheir weaknesses on discrete modalities such as text and DNA; but this conversely can impede their performance\n17\non data that LTI SSMs excel on. Our ablations on audio waveforms examine this tradeo\ufb00 in more detail.\nDownstream Afordances.\nTransformer-based foundation models (particularly LLMs) have a rich ecosystem of\nproperties and modes of interaction with pretrained models, such as \ufb01ne-tuning, adaptation, prompting, in-context\nlearning, instruction tuning, RLHF, quantization, and so on. We are particularly interested in whether Transformer\nalternatives such as SSMs have similar properties and a\ufb00ordances.\nScaling.\nOur empirical evaluation is limited to small model sizes, below the threshold of most strong open source\nLLMs (e.g. Llama (Touvron et al. 2023)) as well as other recurrent models such as RWKV (B. Peng et al. 2023)\nand RetNet (Y. Sun et al. 2023), which have been evaluated at the 7B parameter scale and beyond. It remains to\nassess whether Mamba still compares favorably at these larger sizes. We also note that scaling SSMs may involve\nfurther engineering challenges and adjustments to the model that are not discussed in this paper.\n6\nConclusion\nWe introduce a selection mechanism to structured state space models, allowing them to perform context-dependent\nreasoning while scaling linearly in sequence length. When incorporated into a simple attention-free architecture,\nMamba achieves state-of-the-art results on a diverse set of domains, where it matches or exceeds the performance\nof strong Transformer models. We are excited about the broad applications of selective state space models to\nbuild foundation models for di\ufb00erent domains, especially in emerging modalities requiring long context such as\ngenomics, audio, and video. Our results suggest that Mamba is a strong candidate to be a general sequence model\nbackbone.\nAcknowledgments\nWe thank Karan Goel, Arjun Desai, and Kush Bhatia for helpful feedback on the draft.\nReferences\n[1]\nMartin Arjovsky, Amar Shah, and Yoshua Bengio. \u201cUnitary Evolution Recurrent Neural Networks\u201d. In: The\nInternational Conference on Machine Learning (ICML). 2016, pp. 1120\u20131128.\n[2]\niga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R Taylor,\nYannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley. \u201cEfective Gene Expression Prediction from\nSequence by Integrating Long-range Interactions\u201d. In: Nature Methods 18.10 (2021), pp. 1196\u20131203.\n[3]\nJimmy Ba, Geofrey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. \u201cUsing Fast Weights to\nAttend to the Recent Past\u201d. In: Advances in Neural Information Processing Systems (NeurIPS) 29 (2016).\n[4]\nJimmy Lei Ba, Jamie Ryan Kiros, and Geofrey E Hinton. \u201cLayer Normalization\u201d. In: arXivpreprintarXiv:1607.06450\n(2016).\n[5]\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. \u201cNeural Machine Translation by Jointly Learning to\nAlign and Translate\u201d. In: The International Conference on Learning Representations (ICLR). 2015.\n[6]\nDavid Balduzzi and Muhammad Ghifary. \u201cStrongly-typed Recurrent Neural Networks\u201d. In: International Con-\nference on Machine Learning. PMLR. 2016, pp. 1292\u20131300.\n[7]\nStella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan,\nMohammad Afah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raf, et al. \u201cPythia: A Suite for\nAnalyzing Large Language Models across Training and Scaling\u201d. In: The International Conference on Machine\nLearning (ICML). PMLR. 2023, pp. 2397\u20132430.\n[8]\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. \u201cPIQA: Reasoning about Physical Commonsense\nin Natural Language\u201d. In: Proceedings of the AAAI conference on Artifcial Intelligence. Vol. 34. 05. 2020, pp. 7432\u2013\n7439.\n[9]\nGuy E Blelloch. \u201cPrefx Sums and Their Applications\u201d. In: (1990).\n[10]\nJames Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher. \u201cQuasi-recurrent Neural Networks\u201d. In:\narXiv preprint arXiv:1611.01576 (2016).\n18\n[11]\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-\nlakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. \u201cLanguage Models are Few-shot Learners\u201d. In:\nAdvances in Neural Information Processing Systems (NeurIPS) 33 (2020), pp. 1877\u20131901.\n[12]\nAydar Bulatov, Yuri Kuratov, and Mikhail S Burtsev. \u201cScaling Transformer to 1M tokens and Beyond with RMT\u201d.\nIn: arXiv preprint arXiv:2304.11062 (2023).\n[13]\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. \u201cGenerating Long Sequences with Sparse Trans-\nformers\u201d. In: arXiv preprint arXiv:1904.10509 (2019).\n[14]\nKrzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Pe-\nter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. \u201cRethinking Attention with Performers\u201d. In:\nThe International Conference on Learning Representations (ICLR). 2021.\n[15]\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. \u201cPaLM: Scaling Language Modeling\nwith Pathways\u201d. In: Journal of Machine Learning Research 24.240 (2023), pp. 1\u2013113. url: http://jmlr.org/\npapers/v24/22-1144.html.\n[16]\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. \u201cEmpirical Evaluation of Gated Re-\ncurrent Neural Networks on Sequence Modeling\u201d. In: arXiv preprint arXiv:1412.3555 (2014).\n[17]\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. \u201cThink you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge\u201d. In: arXiv\npreprint arXiv:1803.05457 (2018).\n[18]\nTri Dao. \u201cFlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning\u201d. In: (2023).\n[19]\nTri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. \u201cFlashAttention: Fast and Memory-\nEfcient Exact Attention with IO-Awareness\u201d. In: Advances in Neural Information Processing Systems (NeurIPS).\n2022.\n[20]\nTri Dao, Daniel Y Fu, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher R\u00e9. \u201cHungry Hungry\nHippos: Towards Language Modeling with State Space Models\u201d. In: The International Conference on Learning\nRepresentations (ICLR). 2023.\n[21]\nYann N Dauphin, Angela Fan, Michael Auli, and David Grangier. \u201cLanguage Modeling with Gated Convolu-\ntional Networks\u201d. In: The International Conference on Machine Learning (ICML). PMLR. 2017, pp. 933\u2013941.\n[22]\nDeepSound. SampleRNN. https://github.com/deepsound-project/samplernn-pytorch. 2017.\n[23]\nJiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, and Furu Wei. \u201cLongNet:\nScaling Transformers to 1,000,000,000 Tokens\u201d. In: arXiv preprint arXiv:2307.02486 (2023).\n[24]\nChris Donahue, Julian McAuley, and Miller Puckette. \u201cAdversarial Audio Synthesis\u201d. In: The International\nConference on Learning Representations (ICLR). 2019.\n[25]\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. \u201cAn Image is Worth 16x16 Words:\nTransformers for Image Recognition at Scale\u201d. In: The International Conference on Learning Representations\n(ICLR). 2020.\n[26]\nNelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell,\nYuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfeld-Dodds, Danny\nHernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack\nClark, Jared Kaplan, Sam McCandlish, and Chris Olah. \u201cA Mathematical Framework for Transformer Circuits\u201d.\nIn: Transformer Circuits Thread (2021). https://transformer-circuits.pub/2021/framework/index.html.\n[27]\nMahan Fathi, Jonathan Pilault, Pierre-Luc Bacon, Christopher Pal, Orhan Firat, and Ross Goroshin. \u201cBlock-\nState Transformer\u201d. In: arXiv preprint arXiv:2306.09539 (2023).\n[28]\nYassir Fathullah, Chunyang Wu, Yuan Shangguan, Junteng Jia, Wenhan Xiong, Jay Mahadeokar, Chunxi Liu,\nYangyang Shi, Ozlem Kalinli, Mike Seltzer, et al. \u201cMulti-Head State Space Model for Sequence Modeling\u201d. In:\nINTERSPEECH. 2023.\n[29]\nKarl J Friston, Lee Harrison, and Will Penny. \u201cDynamic Causal Modelling\u201d. In: Neuroimage 19.4 (2003), pp. 1273\u2013\n1302.\n[30]\nDaniel Y Fu, Elliot L Epstein, Eric Nguyen, Armin W Thomas, Michael Zhang, Tri Dao, Atri Rudra, and Christo-\npher R\u00e9. \u201cSimple Hardware-efcient Long Convolutions for Sequence Modeling\u201d. In: The International Confer-\nence on Machine Learning (ICML) (2023).\n[31]\nKen-ichi Funahashi and Yuichi Nakamura. \u201cApproximation of Dynamical Systems by Continuous Time Recur-\nrent Neural Networks\u201d. In: Neural Networks 6.6 (1993), pp. 801\u2013806.\n19\n[32]\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He,\nAnish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. \u201cThe Pile: An 800GB Dataset of Diverse Text\nfor Language Modeling\u201d. In: arXiv preprint arXiv:2101.00027 (2020).\n[33]\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPof, Charles Foster, Laurence Golding, Jefrey\nHsu, Kyle McDonell, Niklas Muennighof, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang,\nKevin Wang, and Andy Zou. A Framework for Few-shot Language Model Evaluation. Version v0.0.1. Sept. 2021.\ndoi: 10.5281/zenodo.5371628. url: https://doi.org/10.5281/zenodo.5371628.\n[34]\nKaran Goel, Albert Gu, Chris Donahue, and Christopher R\u00e9. \u201cIt\u2019s Raw! Audio Generation with State-Space\nModels\u201d. In: The International Conference on Machine Learning (ICML). 2022.\n[35]\nAlbert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. \u201cHIPPO: Recurrent Memory with Optimal\nPolynomial Projections\u201d. In: Advances in Neural Information Processing Systems (NeurIPS). 2020.\n[36]\nAlbert Gu, Karan Goel, and Christopher R\u00e9. \u201cEfciently Modeling Long Sequences with Structured State Spaces\u201d.\nIn: The International Conference on Learning Representations (ICLR). 2022.\n[37]\nAlbert Gu, Caglar Gulcehre, Tom Le Paine, Matt Hofman, and Razvan Pascanu. \u201cImproving the Gating Mech-\nanism of Recurrent Neural Networks\u201d. In: The International Conference on Machine Learning (ICML). 2020.\n[38]\nAlbert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. \u201cOn the Parameterization and Initialization of Diag-\nonal State Space Models\u201d. In: Advances in Neural Information Processing Systems (NeurIPS). 2022.\n[39]\nAlbert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. \u201cCombining Recur-\nrent, Convolutional, and Continuous-time Models with the Linear State Space Layer\u201d. In: Advances in Neural\nInformation Processing Systems (NeurIPS). 2021.\n[40]\nAlbert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher R\u00e9. \u201cHow to Train Your HIPPO: State\nSpace Models with Generalized Basis Projections\u201d. In: The International Conference on Learning Representations\n(ICLR). 2023.\n[41]\nAnkit Gupta, Albert Gu, and Jonathan Berant. \u201cDiagonal State Spaces are as Efective as Structured State\nSpaces\u201d. In: Advances in Neural Information Processing Systems 35 (2022), pp. 22982\u201322994.\n[42]\nDavid Ha, Andrew Dai, and Quoc V. Le. \u201cHyperNetworks\u201d. In: The International Conference on Learning Rep-\nresentations (ICLR). 2017.\n[43]\nDanijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. \u201cDream to Control: Learning Behav-\niors by Latent Imagination\u201d. In: The International Conference on Learning Representations (ICLR). 2020.\n[44]\nRamin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus.\n\u201cLiquid Structural State-Space Models\u201d. In: The International Conference on Learning Representations (ICLR).\n2023.\n[45]\nMikael Henaf, Arthur Szlam, and Yann LeCun. \u201cRecurrent Orthogonal Networks and Long-Memory Tasks\u201d.\nIn: The International Conference on Machine Learning (ICML). 2016.\n[46]\nDan Hendrycks and Kevin Gimpel. \u201cGaussian Error Linear Units (GELUs)\u201d. In: arXiv preprint arXiv:1606.08415\n(2016).\n[47]\nSepp Hochreiter and J\u00fcrgen Schmidhuber. \u201cLong Short-Term Memory\u201d. In: Neural Computation 9.8 (1997),\npp. 1735\u20131780.\n[48]\nJordan Hofmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego\nde Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. \u201cAn Empirical Analysis of Compute-\nOptimal Large Language Model Training\u201d. In: Advances in Neural Information Processing Systems (NeurIPS) 35\n(2022), pp. 30016\u201330030.\n[49]\nWeizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. \u201cTransformer Quality in Linear Time\u201d. In: The Interna-\ntional Conference on Machine Learning (ICML). PMLR. 2022, pp. 9099\u20139117.\n[50]\nHassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, and Pierre-Alain Muller. \u201cDeep\nLearning for Time Series Classifcation: A Review\u201d. In: Data Mining and Knowledge Discovery 33.4 (2019),\npp. 917\u2013963.\n[51]\nAndrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefer. \u201cData Movement is All You Need:\nA Case Study on Optimizing Transformers\u201d. In: Proceedings of Machine Learning and Systems 3 (2021), pp. 711\u2013\n732.\n[52]\nLi Jing, Caglar Gulcehre, John Peurifoy, Yichen Shen, Max Tegmark, Marin Soljacic, and Yoshua Bengio. \u201cGated\nOrthogonal Recurrent Units: On Learning to Forget\u201d. In: Neural Computation 31.4 (2019), pp. 765\u2013783.\n[53]\nRudolph Emil Kalman. \u201cA New Approach to Linear Filtering and Prediction Problems\u201d. In: (1960).\n20\n[54]\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. \u201cTransformers are RNNs: Fast\nAutoregressive Transformers with Linear Attention\u201d. In: International Conference on Machine Learning. PMLR.\n2020, pp. 5156\u20135165.\n[55]\nZhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. \u201cDifWave: A Versatile Difusion Model\nfor Audio Synthesis\u201d. In: International Conference on Learning Representations. 2021.\n[56]\nChrysoula Kosma, Giannis Nikolentzos, and Michalis Vazirgiannis. \u201cTime-Parameterized Convolutional Neu-\nral Networks for Irregularly Sampled Time Series\u201d. In: arXiv preprint arXiv:2308.03210 (2023).\n[57]\nAlex Krizhevsky, Ilya Sutskever, and Geofrey E Hinton. \u201cImageNet Classifcation with Deep Convolutional\nNeural Networks\u201d. In: Advances in Neural Information Processing Systems (NeurIPS) 25 (2012).\n[58]\nTao Lei. \u201cWhen Attention Meets Fast Recurrence: Training Language Models with Reduced Compute\u201d. In:\nProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021, pp. 7633\u20137648.\n[59]\nTao Lei, Yu Zhang, Sida I Wang, Hui Dai, and Yoav Artzi. \u201cSimple Recurrent Units for Highly Parallelizable\nRecurrence\u201d. In: arXiv preprint arXiv:1709.02755 (2017).\n[60]\nMario Lezcano-Casado and David Mart\u00ednez-Rubio. \u201cCheap Orthogonal Constraints in Neural Networks: A\nSimple Parametrization of the Orthogonal and Unitary Group\u201d. In: The International Conference on Machine\nLearning (ICML). 2019.\n[61]\nYuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. \u201cWhat Makes Convolutional Models\nGreat on Long Sequence Modeling?\u201d In: The International Conference on Learning Representations (ICLR). 2023.\n[62]\nVasileios Lioutas and Yuhong Guo. \u201cTime-aware Large Kernel Convolutions\u201d. In: The International Conference\non Machine Learning (ICML). PMLR. 2020, pp. 6172\u20136183.\n[63]\nChris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, and Feryal Behba-\nhani. \u201cStructured State Space Models for In-Context Reinforcement Learning\u201d. In: Advances in Neural Informa-\ntion Processing Systems (NeurIPS). 2023.\n[64]\nShahar Lutati, Itamar Zimerman, and Lior Wolf. \u201cFocus Your Attention (with Adaptive IIR Filters)\u201d. In: arXiv\npreprint arXiv:2305.14952 (2023).\n[65]\nXuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke\nZettlemoyer. \u201cMega: Moving Average Equipped Gated Attention\u201d. In: The International Conference on Learning\nRepresentations (ICLR). 2023.\n[66]\nEric Martin and Chris Cundy. \u201cParallelizing Linear Recurrent Neural Nets Over Sequence Length\u201d. In: The\nInternational Conference on Learning Representations (ICLR). 2018.\n[67]\nSoroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain, Jose Sotelo, Aaron Courville,\nand Yoshua Bengio. \u201cSampleRNN: An Unconditional End-to-End Neural Audio Generation Model\u201d. In: The\nInternational Conference on Learning Representations (ICLR). 2017.\n[68]\nHarsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. \u201cLong Range Language Modeling via\nGated State Spaces\u201d. In: The International Conference on Learning Representations (ICLR). 2023.\n[69]\nZakaria Mhammedi, Andrew Hellicar, Ashfaqur Rahman, and James Bailey. \u201cEfcient Orthogonal Parametri-\nsation of Recurrent Neural Networks using Householder Refections\u201d. In: International Conference on Machine\nLearning. PMLR. 2017, pp. 2401\u20132409.\n[70]\nEric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and Christopher R\u00e9.\n\u201cS4ND: Modeling Images and Videos as Multidimensional Signals with State Spaces\u201d. In: Advances in Neural\nInformation Processing Systems (NeurIPS). 2022.\n[71]\nEric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Pa-\ntel, Clayton Rabideau, Stefano Massaroli, Yoshua Bengio, et al. \u201cHyenaDNA: Long-range Genomic Sequence\nModeling at Single Nucleotide Resolution\u201d. In: Advances in Neural Information Processing Systems (NeurIPS).\n2023.\n[72]\nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann,\nAmanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfeld-Dodds, Danny\nHernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom\nBrown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. \u201cIn-context Learning and Induction Heads\u201d.\nIn: Transformer Circuits Thread (2022). https://transformer-circuits.pub/2022/in-context-learning-and-induction-\nheads/index.html.\n[73]\nAaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalch-\nbrenner, Andrew Senior, and Koray Kavukcuoglu. \u201cWaveNet: A Generative Model for Raw Audio\u201d. In: arXiv\npreprint arXiv:1609.03499 (2016).\n21\n[74]\nAntonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and So-\nham De. \u201cResurrecting Recurrent Neural Networks for Long Sequences\u201d. In: The International Conference on\nMachine Learning (ICML). 2023.\n[75]\nDenis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Ngoc-Quan Pham, Rafaella Bernardi, Sandro Pezzelle,\nMarco Baroni, Gemma Boleda, and Raquel Fern\u00e1ndez. \u201cThe LAMBADA Dataset: Word Prediction Requiring\na Broad Discourse Context\u201d. In: Proceedings of the 54th Annual Meeting of the Association for Computational\nLinguistics. 2016, pp. 1525\u20131534.\n[76]\nRazvan Pascanu, Tomas Mikolov, and Yoshua Bengio. \u201cOn the Difculty of Training Recurrent Neural Net-\nworks\u201d. In: International Conference on Machine Learning. 2013, pp. 1310\u20131318.\n[77]\nBo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael\nChung, Matteo Grella, Kranthi Kiran GV, et al. \u201cRWKV: Reinventing RNNs for the Transformer Era\u201d. In: arXiv\npreprint arXiv:2305.13048 (2023).\n[78]\nHao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng Kong. \u201cRandom\nFeature Attention\u201d. In: The International Conference on Learning Representations (ICLR). 2021.\n[79]\nMichael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano\nErmon, and Christopher R\u00e9. \u201cHyena Hierarchy: Towards Larger Convolutional Language Models\u201d. In: The\nInternational Conference on Machine Learning (ICML). 2023.\n[80]\nZhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and\nYiran Zhong. \u201cToeplitz Neural Network for Sequence Modeling\u201d. In: The International Conference on Learning\nRepresentations (ICLR). 2023.\n[81]\nZhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong. \u201cThe devil\nin linear transformer\u201d. In: arXiv preprint arXiv:2210.10340 (2022).\n[82]\nZhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and\nYiran Zhong. \u201cCosFormer: Rethinking Softmax in Attention\u201d. In: The International Conference on Learning\nRepresentations (ICLR). 2022.\n[83]\nAli Rahimi and Benjamin Recht. \u201cRandom features for large-scale kernel machines\u201d. In: Advances in neural\ninformation processing systems 20 (2007).\n[84]\nPrajit Ramachandran, Barret Zoph, and Quoc V Le. \u201cSwish: A Self-gated Activation Function\u201d. In: arXiv preprint\narXiv:1710.05941 7.1 (2017), p. 5.\n[85]\nDavid W Romero, Anna Kuzina, Erik J Bekkers, Jakub M Tomczak, and Mark Hoogendoorn. \u201cCKConv: Con-\ntinuous Kernel Convolution For Sequential Data\u201d. In: arXiv preprint arXiv:2102.02611 (2021).\n[86]\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. \u201cWinogrande: An Adversarial Wino-\ngrad Schema Challenge at Scale\u201d. In: Communications of the ACM 64.9 (2021), pp. 99\u2013106.\n[87]\nGeorge Saon, Ankit Gupta, and Xiaodong Cui. \u201cDiagonal State Space Augmented Transformers for Speech\nRecognition\u201d. In: ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE. 2023, pp. 1\u20135.\n[88]\nImanol Schlag, Kazuki Irie, and J\u00fcrgen Schmidhuber. \u201cLinear Transformers are Secretly Fast Weight Program-\nmers\u201d. In: The International Conference on Machine Learning (ICML). PMLR. 2021, pp. 9355\u20139366.\n[89]\nNoam Shazeer. \u201cGLU Variants Improve Transformer\u201d. In: arXiv preprint arXiv:2002.05202 (2020).\n[90]\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Sch\u00e4rli, and\nDenny Zhou. \u201cLarge Language Models can be Easily Distracted by Irrelevant Context\u201d. In: The International\nConference on Machine Learning (ICML). PMLR. 2023, pp. 31210\u201331227.\n[91]\nJiaxin Shi, Ke Alexander Wang, and Emily Fox. \u201cSequence Modeling with Multiresolution Convolutional Mem-\nory\u201d. In: The International Conference on Machine Learning (ICML). PMLR. 2023, pp. 31312\u201331327.\n[92]\nJimmy TH Smith, Andrew Warrington, and Scott W Linderman. \u201cSimplifed State Space Layers for Sequence\nModeling\u201d. In: The International Conference on Learning Representations (ICLR). 2023.\n[93]\nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. \u201cRoformer: Enhanced Trans-\nformer with Rotary Position Embedding\u201d. In: arXiv preprint arXiv:2104.09864 (2021).\n[94]\nYutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei.\n\u201cRetentive network: A successor to transformer for large language models\u201d. In: arXiv preprint arXiv:2307.08621\n(2023).\n[95]\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. \u201cSequence to Sequence Learning with Neural Networks\u201d. In:\nAdvances in Neural Information Processing Systems (NeurIPS) 27 (2014).\n22\n[96]\nCorentin Tallec and Yann Ollivier. \u201cCan Recurrent Neural Networks Warp Time?\u201d In: The International Con-\nference on Learning Representations (ICLR). 2018.\n[97]\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Se-\nbastian Ruder, and Donald Metzler. \u201cLong Range Arena: A Benchmark for Efcient Transformers\u201d. In: Inter-\nnational Conference on Learning Representations (ICLR). 2021.\n[98]\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. \u201cEfcient Transformers: A Survey\u201d. In: ACM Com-\nputing Surveys 55.6 (2022), pp. 1\u201328.\n[99]\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Bap-\ntiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. \u201cLlama: Open and Efcient Foundation Language\nModels\u201d. In: arXiv preprint arXiv:2302.13971 (2023).\n[100]\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\nand Illia Polosukhin. \u201cAttention Is All You Need\u201d. In: AdvancesinNeuralInformationProcessingSystems(NeurIPS).\n2017.\n[101]\nEugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. \u201cOn Orthogonality and Learning Recur-\nrent Networks with Long Term Dependencies\u201d. In: International Conference on Machine Learning. PMLR. 2017,\npp. 3570\u20133578.\n[102]\nJue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu, Mohamed Omar, and Rafay Hamid. \u201cSelective\nStructured State-Spaces for Long-form Video Understanding\u201d. In: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. 2023, pp. 6387\u20136397.\n[103]\nPete Warden. \u201cSpeech Commands: A Dataset for Limited-Vocabulary Speech Recognition\u201d. In: ArXiv abs/1804.03209\n(2018).\n[104]\nSamuel Williams, Andrew Waterman, and David Patterson. \u201cRoofine: An Insightful Visual Performance Model\nfor Multicore Architectures\u201d. In: Communications of the ACM 52.4 (2009), pp. 65\u201376.\n[105]\nBrandon Yang, Gabriel Bender, Quoc V Le, and Jiquan Ngiam. \u201cCondConv: Conditionally Parameterized Con-\nvolutions for Efcient Inference\u201d. In: Advances in Neural Information Processing Systems (NeurIPS) 32 (2019).\n[106]\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. \u201cHellaSwag: Can a Machine Really\nFinish Your Sentence?\u201d In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguis-\ntics. 2019.\n[107]\nShuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh Susskind.\n\u201cAn Attention Free Transformer\u201d. In: arXiv preprint arXiv:2105.14103 (2021).\n[108]\nMichael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher R\u00e9. \u201cEfectively Modeling\nTime Series with Simple Discrete State Spaces\u201d. In: The International Conference on Learning Representations\n(ICLR). 2023.\n[109]\nLin Zheng, Chong Wang, and Lingpeng Kong. \u201cLinear complexity randomized self-attention mechanism\u201d. In:\nInternational Conference on Machine Learning. PMLR. 2022, pp. 27011\u201327041.\n[110]\nSimiao Zuo, Xiaodong Liu, Jian Jiao, Denis Charles, Eren Manavoglu, Tuo Zhao, and Jianfeng Gao. \u201cEfcient\nLong Sequence Modeling via State Space Augmented Transformer\u201d. In: arXiv preprint arXiv:2212.08136 (2022).\n23\nA\nDiscussion: Selection Mechanism\nOur selection mechanism is inspired by and related to concepts such as gating, hypernetworks, and data-dependence.\nIt can also be viewed as related to \u201cfast weights\u201d (J. Ba et al. 2016), which connects classical RNNs with the\nmechanism of linear attention (Schlag, Irie, and Schmidhuber 2021). However, we believe that it is a distinct\nconcept that is worth clarifying.\nGating.\nGating originally referred to the gating mechanisms of RNNs such as the LSTM (Hochreiter and\nSchmidhuber 1997) and GRU (J. Chung et al. 2014), or the gated equation (5)n Theorem 1. This was interpreted\nas a particular mechanism for controlling whether to let an input into the hidden state of an RNN. In particular,\nthis a\ufb00ects the propagation of signal through time and causes inputs to interact along the sequence length\ndimension.\nHowever, the concept of gating has since been relaxed in popular usage to simply mean any multiplicative\ninteraction (often with an activation function). For example, elementwise multiplicative components of neural\nnetwork architectures (that do not interact along sequence length) are now commonly referred to as gated\narchitectures (Hua et al. 2022; Mehta et al. 2023), despite a very di\ufb00erent meaning than the original RNN sense.\nThus we believe the original concept of RNN gating versus the popular usage of multiplicative gating actually\nhave a very di\ufb00erent semantic meaning.\nHypernetworks.\nHypernetworks refer to neural networks whose parameters are themselves generated by smaller\nneural networks. The original idea (Ha, Dai, and Quoc V. Le 2017) used it in a narrow sense to de\ufb01ne a large\nRNN whose recurrent parameters are generated by a smaller RNN.\nData-dependence.\nSimilar to hypernetworks, data-dependence can refer to any notion where some parameters\nof the model depend on the data (Poli et al. 2023).\nExample: GLU Activation.\nTo illustrate the issues with these concepts, consider a simple diagonal linear\nlayer \ud466 = D\ud465, where D is a diagonal weight parameter. Now suppose that D is itself generated from a linear\ntransformation of \ud465, with an optional nonlinearity: D = \ud70e(W \ud465). Since it is diagonal, the multiplication becomes\nan elementwise product: \ud466 = \ud70e(W \ud465)\u25e6\ud465.\nThis is a rather trivial transformation, yet it technically satis\ufb01es the common meanings of gating (since it has a\nmultiplicative \u201cbranch\u201d), hypernetworks (since the parameter D is generated by another layer), and data-dependent\n(since D depends on the data \ud465). However, this in fact simply de\ufb01nes a GLU function, which is so simple that\nit is often considered just an activation function (Dauphin et al. 2017; Shazeer 2020) instead of a meaningful\nlayer.\nSelection.\nThus, while selection mechanisms could be considered a special case of ideas such as architectural\ngating, hypernetworks, or data-dependence, so can an enormous range of other constructions\u2014essentially anything\nwith a multiplication, including standard attention mechanisms (Bahdanau, Cho, and Bengio 2015; Vaswani et al.\n2017) as well\u2014and we \ufb01nd it uninformative to think of them as such.\nInstead, we view it as most closely related to the gating mechanism of traditional RNNs, which is a special case\n(Theorem 1) and also has a deeper history of connections to SSMs through variable (input-dependent) discretization\nof \u2206 (Funahashi and Nakamura 1993; Gu, Dao, et al. 2020; Tallec and Ollivier 2018). We also eschew the term\n\u201cgating\u201d in favor of selection to clarify the overloaded use of former. More narrowly, we use selection to refer to\nthe mechanistic action of a model to select or ignore inputs and facilitate data interaction along the sequence\nlength (Section 3.1). Beyond selective SSMs and gated RNNs, other examples may include input-dependent\nconvolutions (Kosma, Nikolentzos, and Vazirgiannis 2023; Lioutas and Guo 2020; Lutati, Zimerman, and Wolf\n2023; Yang et al. 2019) and even attention.\n24\nB\nRelated Work\nWe overview several prior works related to our methods. We mention that some of the most closely related models\ninclude recurrent layers such as S4, S5, and quasi-RNNs; as well as end-to-end architectures such as H3, RetNet,\nand RWKV.\nB.1\nS4 Variants and Derivatives\nWe describe a brief overview of some structured SSMs from past work, particularly those that have a relation to\nour method.\n\u2022 S4 (Gu, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021) introduced the \ufb01rst structured SSM, describing\ndiagonal structure and diagonal plus low-rank (DPLR). It focused on e\ufb03cient convolutional algorithms for\nDPLR SSMs due to a connection to continuous-time online memorization (HIPPO) (Gu, Dao, et al. 2020).\n\u2022 DSS (Gupta, Gu, and Berant 2022) \ufb01rst discovered the empirical e\ufb00ectiveness of diagonal structured SSMs by\napproximating the HIPPO initialization. This was expanded on theoretically in S4D (Gu, Gupta, et al. 2022).\n\u2022 S5 (Smith, Warrington, and Linderman 2023) independently discovered the diagonal SSM approximation, and\nis the \ufb01rst S4 model to be computed recurrently with the parallel scan. However, this required lowering the\ne\ufb00ective state dimension, which they accomplished by switching the SSM dimensions from a SISO (single-input\nsingle-output) to MIMO (multi-input multi-output) formulation. Our proposed S6 shares the scan, but di\ufb00ers\nby (i) keeping the SISO dimensions, which provides a larger e\ufb00ective recurrent state, (ii) using a hardware-aware\nalgorithm to overcome the computation issue, (iii) adding the selection mechanism.\nLu et al. (2023) applied S5 to meta-RL in order to handle resetting the SSM state between episode trajectories.\nTheir mechanism can be viewed as a particular hard-coded instance of a selection mechanism, where A is\nmanually set to 0, instead of our learnable mechanism that depends on the input. It would be interesting to\napply selective SSMs generically to this setting and probe if the model has learned to automatically reset its\nstate on episode boundaries.\n\u2022 Mega (Ma et al. 2023) introduced a simpli\ufb01cation of S4 to be real- instead of complex- valued, giving it an\ninterpretation of being an exponential moving average (EMA). They additionally make an interesting connection\nof the discretization step of SSMs to an EMA damping term. Contrary to \ufb01ndings in the original S4 papers, this\nwas the \ufb01rst model to show that real-valued SSMs are empirically e\ufb00ective in certain settings or when combined\nwith di\ufb00erent architectural components.\n\u2022 Liquid S4 (Hasani et al. 2023) is also motivated by augmenting S4 with an input-dependent state transition.\nFrom this perspective it shares similarity to selection mechanisms, although in a limited form which is still\ncomputed convolutionally and close to LTI.\n\u2022 SGConv (Y. Li et al. 2023), Hyena (Poli et al. 2023), LongConv (Fu et al. 2023), MultiresConv (J. Shi, K. A.\nWang, and Fox 2023), and Toeplitz Neural Network (Qin, Han, W. Sun, He, et al. 2023) all focus on the\nconvolutional representation of S4 and create global or long convolution kernels with di\ufb00erent parameterizations.\nHowever, these methods cannot do fast autoregressive inference directly.\nNotably, all of these methods, and all other structured SSMs that we are aware of, have been non-selective and\nusually strictly LTI (linear time invariant).\nB.2\nSSM Architectures\nWe use SSM architectures or state space neural networks (SSNN) to refer to deep neural network architectures\nincorporating one of the previous SSMs as a black box layer.\n\u2022 GSS (Mehta et al. 2023) was the \ufb01rst gated neural network architecture incorporating SSMs. It is motivated by\nthe gated attention unit (GAU) of Hua et al. (2022) and looks quite similar to our block, except with additional\nprojections. Most importantly, its projection contracts the model dimension to reduce the state size of the\nSSM, while ours expands the model dimension in order to increase the state size, based on the motivation in\nSection 3.1.\n25\n\u2022 Mega (Ma et al. 2023) combined the EMA simpli\ufb01cation of S4 described above into a hybrid architecture using\nan e\ufb03cient attention approximation.\n\u2022 H3 (Dao, Fu, Saab, et al. 2023) is motivated by combining S4 with linear attention (Katharopoulos et al. 2020).\nIt is the \ufb01rst to generalize this formulation of linear attention to more general recurrences, which is also the\nbasis of later architectures.\n\u2022 Selective S4 (J. Wang et al. 2023) incorporates S4 as a black box to generate a binary mask which is multiplied\non the input. While sharing the \u201cselection\u201d name, we consider this an architectural modi\ufb01cation that is closer to\narchitectural gating than a selection mechanism (Appendix A). For example, we hypothesize that it would not\nsolve the Selective Copying task because simply masking out the irrelevant inputs does not a\ufb00ect the spacing\nbetween the relevant ones (indeed, the Selective Copying task can even be viewed as coming pre-masked if the\nnoise tokens are embedded to 0).\n\u2022 RetNet (Y. Sun et al. 2023) is also based on Linear Attention and very similar to H3, but reduces the inner S4\nlayer to a special case where the state dimension is \ud441 = 1. Although not framed as such, its recurrence can be\nviewed as a special case of a linear SSM.\nIts primary source of improvement is using a linear attention with large head dimension, which can be viewed as\nanother method to perform input-dependent state expansion. Using a larger head dimension in the context\nof linear attention variants was \ufb01rst done by H3, but not extensively used since this requires a proportional\namount of extra computation. RetNet avoids this with an alternate way to parallelize the computation with a\nvariant of standard multi-head attention instead of convolutions, made feasible by their particular special case\nof SSMs which acts as a simple EMA.\n\u2022 RWKV (B. Peng et al. 2023) is another recent RNN designed for language modeling. It is based on AFT\n(attention-free Transformer (S. Zhai et al. 2021)), another variant of linear attention. Its main \u201cWKV\u201d mechanism\ninvolves LTI recurrences and can be seen as the ratio of two SSMs.\nWe also highlight the gated attention unit (GAU) from Hua et al. (2022), which was motivated by combining the\nTransformer\u2019s MHA and MLP blocks together and was an inspiration for our architecture (Section 3.4) combining\nthe H3 and MLP blocks.\nB.3\nRelationship to RNNs\nRNNs and SSMs are broadly related, as they both involve the concepts of recurrence on a latent state.\nSeveral older RNNs such as the strongly typed RNN (Balduzzi and Ghifary 2016), quasi-RNN (QRNN) (Bradbury\net al. 2016), and simple recurrent unit (SRU) (Lei 2021; Lei et al. 2017) involve forms of gated RNNs without\ntime-wise nonlinearities. Because of the connections of gating mechanisms and selection mechanisms, these can be\nviewed as cases of selective SSMs, and are thus more powerful in a sense than the family of LTI structured SSMs\nabove. The main di\ufb00erences are:\n\u2022 They do not use state expansion (\ud441 = 1) or selective B, C parameters, both of which are important for\nperformance (Section 4.6).\n\u2022 They use a heuristic gating mechanism, which we generalize as a consequence of the selection mechanism +\ndiscretization (Theorem 1). The connections to principled SSM theory provides better parameterizations\nand initializations (Section 3.6).\nAdditionally, older RNNs famously su\ufb00ered from e\ufb03ciency issues and the vanishing gradients problem (Pascanu,\nMikolov, and Bengio 2013), both caused by their sequential nature. The latter could be solved for some of the\nabove RNNs by leveraging the parallel scan (Martin and Cundy 2018), but the former was di\ufb03cult without theory\nlater developed for SSMs. For example, modern structured SSMs di\ufb00er in more careful parameterization of the\nrecurrent dynamics inspired by classical SSM theory (e.g. through discretization (Gu, Johnson, Goel, et al. 2021;\nGu, Johnson, Timalsina, et al. 2023)), or direct analysis (Orvieto et al. 2023)).\nWe also note that there is a long line of work on orthogonal RNNs (Arjovsky, Shah, and Bengio 2016; Hena\ufb00,\nSzlam, and LeCun 2016; Lezcano-Casado and Mart\u00ednez-Rubio 2019; Mhammedi et al. 2017; Vorontsov et al. 2017)\n26\nwhich are motivated by constraining the A transition matrix to be orthogonal or unitary, in order to control\nits eigenvalues and prevent the vanishing gradient problem. However, these had other limitations; we believe\nthat these stem from the fact that orthogonal/unitary RNNs are also LTI. For example, they are almost always\nevaluated on the Copying task which they can solve perfectly, but observed to struggle on the Selective Copying\ntask (Jing et al. 2019).\nB.4\nLinear Attention\nThe Linear Attention (LA) (Katharopoulos et al. 2020) framework is an important result popularizing kernel\nattention and showing how it relates to recurrent autoregressive models. Many variants have proposed alternative\nkernels and other modi\ufb01cations. Random Feature Attention (RFA) (H. Peng et al. 2021) chooses the kernel feature\nmap to approximate softmax attention (i.e. the exp feature map) using the random Fourier feature approximation\nof Gaussian kernels (Rahimi and Recht 2007). Performer (Choromanski et al. 2021) \ufb01nds an approximation\nto the exponential kernel involving only positive features, which also allows the softmax normalization term.\nTransNormer (Qin, Han, W. Sun, D. Li, et al. 2022) showed that the LA denominator term can be unstable\nand proposed replacing it with a LayerNorm. cosFormer (Qin, W. Sun, et al. 2022) augments RFA with a\ncosine reweighting mechanism that incorporates positional information to emphasize locality. Linear Randomized\nAttention (Zheng, C. Wang, and L. Kong 2022) generalize RFA from the perspective of importance sampling,\nand generalize it to provide better estimates of the full softmax kernel (rather than just the exp-transformed\nnumerator).\nAside from kernel attention, many other variants of e\ufb03cient attention exist; the survey Tay, Dehghani, Bahri,\net al. (2022) o\ufb00ers an extensive categorization of many of these.\nB.5\nLong Context Models\nLong context has become a popular subject, and several recent models have claimed to scale to longer and longer\nsequences. However, these are often from a computational standpoint and have not been extensively validated.\nThese include:\n\u2022 Recurrent Memory Transformer (Bulatov, Kuratov, and Burtsev 2023), a lightweight wrapper around a\nTransformer backbone. It showed ability to generalize up to 1M sequences but only on synthetic memorization\ntasks; their main result is similar to our Induction Heads extrapolation experiment (Table 2).\n\u2022 LongNet (Ding et al. 2023), which claimed to scale to 1B length but only evaluated on length < 100\ud43e for actual\ntasks.\n\u2022 Hyena and HyenaDNA (Nguyen, Poli, et al. 2023; Poli et al. 2023), which claimed to leverage up to 1M context.\nHowever, their experiments trained on proportionally more data at longer contexts, making it hard to conclude\nif quality improvements at 1M context are due to context length or due to more data and computation.\n\u2022 Sparse Transformer (Child et al. 2019) showed a proof-of-concept of using a strided sparse attention Transformer\nto model audio waveforms of length 220 = 1048576, although did not discuss performance tradeo\ufb00s when\ncontrolling for computation and model size.\nIn contrast, we believe this work presents one of the \ufb01rst approaches to meaningfully demonstrate increasing\nperformance with longer context.\nC\nMechanics of Selective SSMs\nProof of Theorem 1. Consider a selective SSM (Algorithm 2) with \ud441 = 1, A = \u22121, B = 1, \ud460\u2206 = \ud5ab\ud5c2\ud5c7\ud5be\ud5ba\ud5cb(\ud465), \ud70f\u2206 = \ud5cc\ud5c8\ud5bf\ud5cd\ud5c9\ud5c5\ud5ce\ud5cc.\nThe corresponding continuous-time SSM (1) is\n\u210e(\ud461) = \u2212\u210e(\ud461) + \ud465(\ud461)\nwhich is also called a leaky integrator.\n27\nThe discretization step size is\n\u2206\ud461 = \ud70f\u2206(\ud5af\ud5ba\ud5cb\ud5ba\ud5c6\ud5be\ud5cd\ud5be\ud5cb + \ud460\u2206(\ud465\ud461))\n= \ud5cc\ud5c8\ud5bf\ud5cd\ud5c9\ud5c5\ud5ce\ud5cc(\ud5af\ud5ba\ud5cb\ud5ba\ud5c6\ud5be\ud5cd\ud5be\ud5cb + \ud5ab\ud5c2\ud5c7\ud5be\ud5ba\ud5cb(\ud465\ud461))\n= \ud5cc\ud5c8\ud5bf\ud5cd\ud5c9\ud5c5\ud5ce\ud5cc(\ud5ab\ud5c2\ud5c7\ud5be\ud5ba\ud5cb(\ud465\ud461))\nwhere we observe that the parameter can be viewed as a learnable bias and folded into the linear projection.\nNow applying the zero-order hold (ZOH) discretization formulas:\nA\ud461 = exp(\u2206A) =\n1\n1 + exp(\ud5ab\ud5c2\ud5c7\ud5be\ud5ba\ud5cb(\ud465\ud461) = \ud70e(\u2212\ud5ab\ud5c2\ud5c7\ud5be\ud5ba\ud5cb(\ud465\ud461))\n= 1 \u2212 \ud70e(\ud5ab\ud5c2\ud5c7\ud5be\ud5ba\ud5cb(\ud465\ud461))\nB\ud461 = (\u2206A)\u22121(exp(\u2206A) \u2212 I) \u22c5 \u2206B = \u2212(exp(\u2206A) \u2212 I) = 1 \u2212 A\n= \ud70e(\ud5ab\ud5c2\ud5c7\ud5be\ud5ba\ud5cb(\ud465\ud461)).\nThus the fnal discrete recurrence (2a) is\n\ud454\ud461 = \ud70e(\ud5ab\ud5c2\ud5c7\ud5be\ud5ba\ud5cb(\ud465\ud461))\n\u210e\ud461 = (1 \u2212 \ud454\ud461)\u210e\ud461\u22121 + \ud454\ud461\ud465\ud461\nas desired.\nD\nHardware-aware Algorithm For Selective SSMs\nWithout input-dependent selectivity, SSMs can be e\ufb03ciently implemented as a convolution (Dao, Fu, Saab, et al.\n2023; Gu, Goel, and R\u00e9 2022), which leverages the fast Fourier transform (FFT) as primitive. With selectivity,\nSSMs are no-longer equivalent to convolution, but we leverage the parallel associative scan. While SSM scans\nare theoretically e\ufb03cient (\ud442(\ud435\ud43f\ud437\ud441) FLOPs, scaling linear in \ud43f), training foundation models with selective SSMs\nrequires them to be e\ufb03cient on modern hardware (GPUs) as well. We describe how we use kernel fusion and\nrecomputation to make SSM scan fast and memory-e\ufb03cient. We evaluate the speed of our scan implementation\ncompared to convolution and attention in Section 4.5, showing that it is up to 7\u00d7 times faster than attention at\nsequence length 32K, and is as memory-e\ufb03cient as the best attention implementation (FlashAttention).\nSpeed.\nOn modern hardware accelerators (GPUs) most operations (except matrix multiply) are bounded by\nmemory-bandwidth (Dao, Fu, Ermon, et al. 2022; Ivanov et al. 2021; Williams, Waterman, and Patterson 2009).\nThis the case with our scan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to\nsigni\ufb01cant speedup compared to a standard implementation.\nThe standard way to implement the scan algorithm in Section 3.2 is to prepare the scan input A, B of size\n(\ud435, \ud43f, \ud437, \ud441) in GPU HBM (high-bandwidth memory, commonly referred to as GPU memory), call a parallel\nassociative scan implementation to write the scan output of size (\ud435, \ud43f, \ud437, \ud441) to GPU HBM, then multiply that scan\noutput with C to produce an output of size (\ud435, \ud43f, \ud437). However, this requires the number of memory reads/writes\non the order of \ud442(\ud435\ud43f\ud437\ud441). We can instead fuse the discretization step, the scan, and the multiplication with C\ninto one kernel:\n1. We read in \ud442(\ud435\ud43f\ud437 + \ud437\ud441) bytes of memory (\u2206, A, B, C) from slow HBM to fast SRAM.\n2. We discretize to produce A, B of size (\ud435, \ud43f, \ud437, \ud441) in SRAM.\n3. We perform a parallel associative scan, yielding intermediate states of size (\ud435, \ud43f, \ud437, \ud441) in SRAM.\n4. We multiply and sum with C, producing outputs of size (\ud435, \ud43f, \ud437) and write it to HBM.\nThis way, we reduce IOs by a factor of \ud442(\ud441) (the state dimension), which in practice speeds up the operation by\n20-40 times (Section 4.5).\n28\nTable 11: (Induction heads.) Models are trained on sequence length 28 = 256, and tested on various sequence lengths of 26 = 64\nup to 220 = 1048576. \u0013 denotes perfect generalization accuracy, while \u0017 denotes out of memory.\nModel\nParams\nTest Accuracy (%) at Sequence Length\n26\n27\n28\n29\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\nMHA-Abs\n137K\n\u0013\n99.6\n100.0\n58.6\n26.6\n18.8\n9.8\n10.9\n7.8\n\u0017\n\u0017\n\u0017\n\u0017\n\u0017\n\u0017\nMHA-RoPE\n137K\n\u0013\n\u0013\n100.0\n83.6\n31.3\n18.4\n8.6\n9.0\n5.5\n\u0017\n\u0017\n\u0017\n\u0017\n\u0017\n\u0017\nMHA-xPos\n137K\n\u0013\n\u0013\n100.0\n99.6\n67.6\n25.4\n7.0\n9.0\n7.8\n\u0017\n\u0017\n\u0017\n\u0017\n\u0017\n\u0017\nH3\n153K\n\u0013\n\u0013\n100.0\n80.9\n39.5\n23.8\n14.8\n8.2\n5.9\n6.6\n8.2\n4.7\n8.2\n6.3\n7.4\nHyena\n69M\u2217\n97.7\n\u0013\n100.0\n\u0013\n44.1\n12.5\n6.6\n5.1\n7.0\n5.9\n6.6\n6.6\n5.9\n6.3\n9.8\nMamba\n74K\n\u0013\n\u0013\n100.0\n\u0013\n\u0013\n\u0013\n\u0013\n\u0013\n\u0013\n\u0013\n\u0013\n\u0013\n\u0013\n\u0013\n\u0013\n\u2217 Most of the parameters are in learnable positional encodings.\nFor sequence length \ud43f too long where we cannot \ufb01t the sequence in SRAM (which is much smaller than HBM), we\nsplit the sequences into chunks and perform the fused scan on each chunk. As long as we have the intermediate\nscan states, we can continue the scan with the next chunk.\nMemory.\nWe describe how we use the classical technique of recomputation to reduce the total amount of memory\nrequired to train selective SSM layers.\nFrom the way we fuse the forward pass, we do not save the intermediate states of size (\ud435, \ud43f, \ud437, \ud441) to avoid memory\nblowup. However, these intermediate states are necessary for the backward pass to compute gradients. We instead\nrecompute those intermediate states in the backward pass. Since the inputs \u2206, A, B, C and output gradient\nread from HBM to SRAM are of size \ud442(\ud435\ud43f\ud441 + \ud437\ud441), and the input gradients are also of size \ud442(\ud435\ud43f\ud441 + \ud437\ud441),\nrecomputation avoids the cost of reading \ud442(\ud435\ud43f\ud441\ud437) elements from HBM. This means that recomputation of the\nSSM states in the backward pass speeds up the computation compared to storing them and reading them from\nHBM.\nBeyond optimizing for the memory requirement of just the scan operation, we also use recomputation to optimize\nthe memory requirement of the entire selective SSM block (input projection, convolution, activation, scan, output\nprojection). In particular, we do not save intermediate activations that take a lot of memory but are fast to\nrecompute (e.g. output of activation function or short convolution). As a result, the selective SSM layer has the\nsame memory requirement as an optimized Transformer implementation with FlashAttention. In particular, each\nattention layer (FlashAttention) stores around 12 bytes of activations per token, an each MLP layer stores around\n20 bytes of activations per token, for a total of 32 bytes ((assuming mixed-precision training in FP16 or BF16)).\nEach selective SSM stores around 16 bytes of activations per token. Hence two layers of selective SSMs have\naround the same activation memory as an attention layer and an MLP layer.\nE\nExperimental Details and Additional Results\nE.1\nSynthetic Tasks\nSelective Copying.\nOur setting is on sequences of length 4096, with a vocab size of 16 possible tokens (including\nthe white \u201cnoise\u201d token from Figure 2) and requiring models to memorize 16 \u201cdata\u201d tokens. We use 2 layer models\nwith a model dimension of \ud437 = 64.\nModels are trained for 400K steps at a constant learning rate of 0.0001 with a batch size of 64.\nInduction Heads.\nTraining consists of randomly generating data every step, with a batch size of 8. We choose\nan \u201cepoch\u201d size of 8192 steps, and track the accuracy on \ufb01xed validation sets (also randomly generated) of\neach target sequence length. For the MHA-Abs and Mamba models, results are reported after the 25th epoch\n(8192 \u00d7 25 = 204800 steps). For the MHA-RoPE and MHA-xPos models, results are reported after the 50th epoch\n(8192 \u00d7 50 = 409600 steps). For the LTI H3 and Hyena models, results are reported after the 10th epoch (81920\nsteps) because they had converged by then and failed to improve further.\n29\nTable 12: (Scaling Law Model Sizes.) Our model sizes and hyperparameters for scaling experiments. (Model dimension and\nnumber of heads applies only to Transformer models.)\nParams\n\ud697_\ud695\ud68a\ud6a2\ud68e\ud69b\ud69c\n\ud68d_\ud696\ud698\ud68d\ud68e\ud695\n\ud697_\ud691\ud68e\ud68a\ud68d\ud69c / \ud68d_\ud691\ud68e\ud68a\ud68d\nTraining steps\nLearning Rate\nBatch Size\nTokens\n125M\n12\n768\n12 / 64\n4800\n6e-4\n0.5M tokens\n2.5B\n350M\n24\n1024\n16 / 64\n13500\n3e-4\n0.5M tokens\n7B\n760M\n24\n1536\n16 / 96\n29000\n2.5e-4\n0.5M tokens\n15B\n1.3B\n24\n2048\n32 / 64\n50000\n2e-4\n0.5M tokens\n26B\nWe use the Adam optimizer with no weight decay. All models are trained at constant learning rates 2\ud452 \u2212 4 and\n1\ud452 \u2212 3, and the better results are reported for each model (2\ud452 \u2212 4 for all models except Mamba). The attention\nand Hyena models did not learn at LR 1\ud452 \u2212 3. H3 learned at both LRs, but interestingly generalized better to\nshorter sequences at the smaller LR of 2\ud452 \u2212 4. Mamba learned at both LRs, but extrapolated better at the larger\nLR of 1\ud452 \u2212 3.\nE.2\nLanguage Modeling\nE.2.1\nScaling Law Details\nAll models were trained on the Pile.\nModel Sizes.\nTable 12 speci\ufb01es the model sizes we use for scaling laws. This is taken directly from the GPT3\nspeci\ufb01cations (Brown et al. 2020), with very minor modi\ufb01cations. First, we changed the batch size of the 1.3B\nmodel from 1M tokens to 0.5M tokens, since we did not use enough parallelization to require the larger batch\nsize. Second, we changed the number of training steps and total tokens to roughly match Chinchilla scaling\nlaws (Ho\ufb00mann et al. 2022), which specify that training tokens should increase proportionally to model size.\nTraining Recipes.\nAll models used the AdamW optimizer with\n\u2022 gradient clip value 1.0\n\u2022 weight decay 0.1\n\u2022 no dropout\n\u2022 linear learning rate warmup with cosine decay\nBy default, the peak learning rate is the GPT3 speci\ufb01cation.\nWe give several models an \u201cimproved recipe\u201d, inspired by changes adopted by popular large language models such\nas PaLM (Chowdhery et al. 2023) and LLaMa (Touvron et al. 2023). These include:\n\u2022 linear learning rate warmup with cosine decay to 1\ud452 \u2212 5, with a peak value of 5\u00d7 the GPT3 value\n\u2022 no linear bias terms\n\u2022 RMSNorm instead of LayerNorm\n\u2022 AdamW hyperparameter \ud6fd = (.9, .95) (the GPT3 value) instead of the PyTorch default of \ud6fd = (.9, .999)\nArchitecture and Training Details.\nOur models are:\n\u2022 Transformer: The standard Transformer based on GPT3 (Table 12).\n\u2022 Transformer++: A Transformer with an improved architecture, namely rotary positional encodings (Su et al.\n2021) and SwiGLU MLP (Shazeer 2020), and the improved training recipe above.\n\u2022 Hyena: Interleaving a Hyena block (the H3 block with S4 replaced by a global convolution parameterized by an\nMLP) with standard MLP blocks. The MLP blocks have expansion factor 2 instead of 4 and the number of\nlayers is correspondingly increased by 1.5\u00d7 to preserve parameter count.\n30\n\u2022 H3++: The H3 architecture with a few modi\ufb01cations, including (i) using the same \u201cthin\u201d Hyena dimensions\nabove (ii) the improved training recipe above (iii) a linear attention head dimension of 8.\n\u2022 RWKV: The default RWKV model from B. Peng et al. (2023), including its modi\ufb01ed MLP block. We also used\nas much of its speci\ufb01ed training recipe as possible, such as increasing the learning rates by 2\u00d7 or 3\u00d7 on certain\nparameters.\n\u2022 RetNet: The default RetNet model from Y. Sun et al. (2023). We also gave it the improved training recipe\nabove.\n\u2022 Mamba: The standard Mamba architecture, with the improved training recipe.\nE.2.2\nAdditional Scaling Law Ablations\nWe perform additional ablations on the architecture using the same protocol as the 2k context length scaling laws\nin Figure 4 (Left).\nMamba Architecture: Interleaving Blocks.\nWe test the e\ufb00ect of di\ufb00erent architectural blocks combined with\nthe Mamba block. We focus on the viewpoint that the Mamba block is simply the standard SwiGLU block with\nan extra \ud5bc\ud5c8\ud5c7\ud5cf \u2192 \ud5b2\ud5b2\ud5ac path added. This leads to two natural ablations:\n\u2022 What if the Mamba block is interleaved with a standard MLP block, instead of stacked homogenously? This\ncan also be interpreted as taking Mamba and removing half of the SSMs.\n\u2022 What if the Mamba block is interleaved with MHA (multi-head attention) blocks? This can also be interpreted\nas taking a Transformer with SwiGLU MLPs (i.e. what we call Transformer++) and simply adding SSMs to the\nMLP blocks.\nFigure 9 (Right) shows these variants compared to the original (homogenous) Mamba architecture. Interestingly,\nneither change matters too much. The Mamba-MLP architecture is only slightly worse, and still better than\nall models except Transformer++. The Mamba-MHA architecture is only slightly better, which is somewhat\nsurprising in light of the fact that many recent works have found that combining (LTI) SSMs with Attention can\nlead to substantial improvements (Dao, Fu, Saab, et al. 2023; Fathi et al. 2023; Fathullah et al. 2023; Saon, Gupta,\nand Cui 2023; Zuo et al. 2022).\nH3 Architecture: Training Recipes.\nNext we ablate di\ufb00erences between the Hyena and H3++ models, our\nweakest and strongest models outside of Transformer++ and Mamba, particularly to isolate the e\ufb00ect of training\nrecipes.\n\u2022 Hyena: The Hyena block with its original architecture and GPT3 training recipe (same as Figure 4).\n\u2022 Hyena+: The same architecture but with the improved training recipe described above.\n\u2022 H3+: The same architecture as Hyena+ but with the Hyena convolution kernel swapped out for S4D convolution\nkernel.\n\u2022 H3++: The same as H3+, but with a linear attention head dimension of 8. This increases computation inside\nthe SSM recurrence but does not increase parameters.\nOur general convention is that \u201cModel+\u201d represents the base model with the improved training recipe, and\n\u201cModel++\u201d also allows for architectural changes.\nFigure 9 (Right) shows that\n\u2022 A large improvement is achieved by the improved training recipe, which was used for many of the models in the\nmain Figure 4 (RetNet, H3++, Transformer++, Mamba).\n\u2022 The choice of the inner LTI SSM does not matter (e.g. Hyena vs. S4), consistent with \ufb01ndings throughout this\npaper.\n\u2022 The head dimension expansion improves performance, consistent with one of our main themes that expanded\nstate dimension improves performance for SSMs (Section 3).\n31\n1019\n1020\nFLOPs (log scale)\n101\n7 \u00d7 100\n8 \u00d7 100\n9 \u00d7 100\nPerplexity (log scale)\nScaling Laws on The Pile (Sequence Length 2048)\nMamba\nMamba-MLP\nMamba-MHA\n1019\n1020\nFLOPs (log scale)\n101\nPerplexity (log scale)\nScaling Laws on The Pile (Sequence Length 2048)\nHyena\nHyena+\nH3+\nH3++\nFigure 9: (Scaling laws: extra ablations.) (Left) Instead of (Right) Instead of\nE.2.3\nDownstream Evaluation Details\nThis pretraining procedure is the same as the scaling law protocol, but extended to 300B tokens. For the 1.3B\nmodel, we use a batch size of 1M tokens to be consistent with the GPT3 speci\ufb01cations. We report the perplexity\non the Pile validation set, and for this metric only compare to models trained on the same dataset and with the\nsame tokenizer, in particular Pythia and RWKV.\nFor downstream evaluation, we use the LM evaluation harness from EleutherAI (L. Gao, Tow, et al. 2021),\nas done by most work in this area. We evaluate on the following tasks/datasets that measure common sense\nreasoning:\n\u2022 LAMBADA (Paperno et al. 2016).\n\u2022 HellaSwag (Zellers et al. 2019).\n\u2022 PIQA (Bisk et al. 2020).\n\u2022 ARC-challenge (P. Clark et al. 2018).\n\u2022 ARC-easy: an easy subset of ARC-challenge.\n\u2022 WinoGrande (Sakaguchi et al. 2021).\nWe report accuracy for LAMBADA, WinoGrande, PIQA, and ARC-easy, and accuracy normalized by sequence\nlength for HellaSwag and ARC-challenge (since normalized accuracy is higher for almost all models for these\ntask).\nE.3\nDNA Modeling\nE.3.1\nPretraining Details\nWe describe the dataset and training procedure of the HG38 pretraining task in more detail.\nThe dataset follows the splits from the prior Enformer work on genomics (Avsec et al. 2021); the training split\ncontains a total of \ud446 = 34021 segments of length 217 = 131072 that cover the genome, for a total of approximately\n4.5 billion tokens (DNA base pairs). These segments are pairs of (chromosome number, starting index, ending\nindex), and can be extended if necessary (e.g. to get longer segments).\nWe deviate from HyenaDNA when the training sequence length is not 217. HyenaDNA always takes a \ufb01xed\nsub-segment (e.g. the beginning or middle of the prescribed segment), and thus for any training sequence length\neach epoch is \ufb01xed to 34021 samples and doesn\u2019t necessarily go through the whole genome. On the other hand, we\nuse the entire training data:\n\u2022 When the context length \ud43f is less than (or equal to) 217, we divide up each segment into non-overlapping\nsub-segments of length \ud43f, so that there are \ud446 \u00d7 217\n\ud43f total samples and \ud446 \u00d7 217 \u2248 4.5\ud435 tokens per epoch.\n\u2022 When the context length \ud43f is greater than 217, we turn each segment into two samples, one that begins with the\nprescribed segment and one that ends with the prescribed segment. Thus each epoch has 2\ud446 items and 2\ud446\ud43f\n32\ntokens per epoch. For example, at sequence length 218 = 262144 there are 4\u00d7 as many tokens as the default,\nand at sequence length 220 there are 16\u00d7 as many tokens.\nOther training details generally follow the same protocol as our language modeling experiments (Appendix E.2).\nFor example, we use the AdamW with (\ud6fd1, \ud6fd2) = (0.9, 0.95), no dropout, weight decay 0.1. We use a cosine learning\nrate scheduler with linear warmup for 10% of total steps.\nE.3.2\nScaling: Model Size Details\nModels.\nThe models we consider are:\n\u2022 Transformer++: a Transformer with improved architecture, notably the usage of RoPE positional encodings (Su\net al. 2021). Informally, we found these to be noticeably better than vanilla positional encodings from (Vaswani\net al. 2017).\n\u2022 HyenaDNA: the Hyena model from Nguyen, Poli, et al. (2023) and Poli et al. (2023), which is roughly a\nTransformer with the MHA block replaced by an H3 block using a global convolution parameterized by an MLP.\n\u2022 Mamba: the standard Mamba architecture.\nModel Sizes.\nWe use the following model sizes.\nBlocks\n4\n5\n6\n7\n8\n10\n12\nModel Dimension\n64\n96\n128\n192\n256\n384\n512\nParams (Approx.)\n250K\n700K\n1.4M\n3.5M\n7.0M\n19.3M\n40.7M\nNote that the number of blocks for Mamba is doubled, because one Transformer \u201clayer\u201d includes both the MHA and\nMLP blocks (and similarly for Hyena), which requires two Mamba blocks to match parameters (Section 3.4).\nTraining.\nFor each model (Transformer++, HyenaDNA, Mamba), we swept the learning rate across {1\ud452 \u2212 3, 2\ud452 \u2212\n3, 4\ud452 \u2212 3, 8\ud452 \u2212 3}. The optimal Transformer and HyenaDNA learning rates were 2e-3 across all sizes. The optimal\nMamba learning rate was 8e-3; note that Mamba performed better than baselines with matched learning rates\n(2e-3), but was more stable and improved even more at higher learning rates. (Furthermore, as this LR is on the\nupper range of the sweep, it is possible that our results are still suboptimal.)\nNote that, in contrast to standard LM scaling laws (Table 12), our LR held constant across model sizes for\nsimplicity. The optimal LR should go down for larger models, but we didn\u2019t \ufb01nd a noticeable e\ufb00ect at the small\nmodel sizes (at most a few million parameters) we considered.\nE.3.3\nScaling: Context Length Details\nWe use a total batch size of 224 \u2248 16\ud440 tokens per training step, for every sequence length (e.g. at length 220\nthere are 16 segments per batch and at length 210 there are 16384 segments per batch). This is a large batch size\nrelative to the model size by usual LM standards, but note that a batch size of 223 is the minimum possible on a\nmachine with 8 GPUs and sequence length of 220, and that HyenaDNA used much larger batches of 228.\nThe learning rate used was 0.008 for Mamba and 0.001 for HyenaDNA; we initially attempted to use the same\nlearning rate of 0.002 from the previous section for HyenaDNA, but found that it was unstable at the longest\ncontext length.\nSequence Length Warmup.\nFollowing (Nguyen, Poli, et al. 2023), we use sequence length warmup (SLW)\nduring pretraining. We choose a simple schedule of 2 epochs at each power-of-two sequence length starting from\n210 = 1024. (Note that because of how data is curated, at the longest sequence lengths more steps and tokens are\nspent proportionally. In particular, each stage up to length 217 processes the same number of tokens, but 4\u00d7 as\nmany tokens are processed at length 218, 8\u00d7 as many at length 219, and 16\u00d7 as many at length 220.)\nUnlike HyenaDNA, we always control for the number of tokens per gradient update, so the batch size is successively\nhalved as the sequence lengths are doubled in each stage.\n33\nTable 13: (Great Apes DNA Classifcation.) Accuracy after fne-tuning on sequences of length 210 = 1024 up to 220 = 1048576\nusing pretrained models of the same context length. Random guessing is 20%.\nModel\nParams\nAccuracy (%) at Sequence Length\n210\n212\n214\n216\n218\n220\nHyenaDNA\n1.4M\n28.04\n28.43\n41.17\n42.22\n31.10\n54.87\nMamba\n1.4M\n31.47\n27.50\n27.66\n40.72\n42.41\n71.67\nMamba\n7M\n30.00\n29.01\n31.48\n43.73\n56.60\n81.31\nRemark E.1. We also note that the schedule was not tuned, and we never experimented with turning of sequence length\nwarmup for these pretraining experiments. We later found that SLW did not help noticeably for audio pretraining at\nsimilar lengths (Section 4.4), and it is possible that it is not necessary for DNA pretraining either.\nE.3.4\nSpecies (Great Apes) Classifcation\nModels are causal and therefore only the last element (across the sequence length) of the model\u2019s output is used for\nthe classi\ufb01cation head. Note that we control for the total number of elements in the loss function per gradient step.\nThe pretraining objective includes all positions across the sequence length, so that \ud68b\ud68a\ud69d\ud68c\ud691_\ud69c\ud692\ud6a3\ud68e\u00d7\ud69c\ud68e\ud69a\ud69e\ud68e\ud697\ud68c\ud68e_\ud695\ud68e\ud697\ud690\ud69d\ud691\nis held constant; in other words, the batch size decreases as the sequence length increases. However, for a\nclassi\ufb01cation task, since only the last position enters the loss, the batch size itself is held constant. Note that this\nalso means that \ufb01ne-tuning models with longer sequence lengths is more computationally expensive.\nTraining consists of 10 epochs, each of which has 1024 gradient steps. Each gradient step uses batch size 64, which\nare all independently randomly drawn by uniformly picking a species, uniformly picking a chromosome, and then\nuniformly picking a contiguous segment of DNA.\nFollowing (Nguyen, Poli, et al. 2023), models with a maximum context length greater than 214 = 16384 use\nsequence length warmup with 1 epoch at length 214 = 16384, 1 epoch at length 215 = 32768, 1 epoch at length\n216 = 65536, and so on up to the maximum sequence length. For example, the model with 220 = 1048576 context\nundergoes 6 epochs of sequence length warmup before 4 more epochs at its maximum sequence length.\nThe learning rate for all Hyena models is \ud7fa\ud68e \u2212 \ud7fb, while the learning rate for all Mamba models is \ud7f7\ud68e \u2212 \ud7fa. These\nwere found by performing learning rate sweeps for each model among {1\ud452 \u2212 5, 2\ud452 \u2212 5, 4\ud452 \u2212 5, 1\ud452 \u2212 4, 2\ud452 \u2212 4} for\nthe smaller sequence lengths (210, 212, 214, 216), and these values were consistently found to be the best for each\nmodel. An abridged learning rate sweep was done at length 218, which agreed with these values, and a single run\nat length 220 was performed (as described above, the computational cost of these experiments is proportional to\nthe sequence length). The learning rate followed a cosine decay schedule with warmup with 5 epochs of linear\nwarmup to the maximum learning rate, and 5 epochs of cosine decay down to 1\ud452 \u2212 6. The unusually long learning\nrate warmup schedule was chosen because the sequence length warmup was also long (e.g. comprising 6 out of 10\nepochs for the model with context length 220); we did not experiment with this choice.\nResults for the Species classi\ufb01cation task are in Table 13.\nE.4\nAudio Details\nE.4.1\nYouTubeMix Audio Pretraining\nModel.\nWe use a model with 3 blocks per stage (3 \u00d7 5 = 15 total Mamba blocks), pooling factor \ud45d = 16, and\nouter dimension \ud437 = 64, for about 3.5M parameters.\nDataset.\nThe data is mu-law encoded at 8 bits, so the model is modeling discrete tokens with a vocab size of\n256.\nThe dataset consists of clips of up to 1 minute long, or length 960000, which is subsampled and divided into\nsegments of any desired sequence length. Since the architecture involves two stages of pooling by a factor of 16,\n34\nTable 14: YouTubeMix length scaling sequence lengths and batch sizes.\nSequence length\nBatch size\nTokens / batch\n468 \u00d7 2048 = 958464\n1\n958464\n234 \u00d7 2048 = 479232\n2\n958464\n117 \u00d7 2048 = 239616\n4\n958464\n59 \u00d7 2048 = 120832\n8\n966656\n30 \u00d7 2048 = 61440\n16\n983040\n15 \u00d7 2048 = 30720\n32\n983040\n8 \u00d7 2048 = 16384\n64\n1048576\n4 \u00d7 2048 = 8192\n128\n1048576\n104\n105\nSequence Length\n1.25\n1.30\n1.35\n1.40\n1.45\n1.50\nBits Per Byte\nAudio Waveforms - SSM Parameterization\nS4+MLP\nMamba (S6)\n+complex\n-selective B/C\n-selective \n   (Mamba-S4)\n104\n105\nSequence Length\n1.25\n1.30\n1.35\n1.40\n1.45\nBits Per Byte\nAudio Waveforms - SSM Parameterization\nMamba (S6)\n+complex\n-selective B/C\n-selective \n   (Mamba-S4)\nFigure 10: (Audio Pretraining (YouTubeMix) Ablations.) As a uniformly-sampled \u201ccontinuous\u201d signal modality, audio wave-\nforms actually beneft from LTI models which have matching inductive bias. (Left) Homogenous models (all blocks have the same\nparameterization) (Right) Only the center U-Net blocks are ablated; the outer blocks are Mamba-S4. Purple line is same as fgure\non left.\nand we want the resulting sequence length to be a a multiple of 8 for hardware e\ufb03ciency, the longest possible\nsequence is 468 \u00d7 2048 = 958464. The rest of our sequence lengths are de\ufb01ned by successively halving this and\nrounding up to the nearest multiple of 2048.\nTable 14 lists the speci\ufb01cations used in Figure 7. Beyond the varying batch sizes, the number of valid segments in\nthe training set varied between di\ufb00erent sequence lengths (e.g. the number of training steps per epoch was not\nconstant for di\ufb00erent points in the graph), which may have contributed to kinks in the scaling curves.\nTraining.\nModels were trained for 200\ud43e training steps with a maximum learning rate of 0.002, 20\ud43e (10%)\nwarmup steps, and weight decay 0.1 (similar to our general pretraining recipe across domains).\nAdditional Ablations: SSM Parameterizations.\nWe investigate SSM parameterizations on long-form audio\nwaveform pretraining in the setting of Figure 7. The setting is modi\ufb01ed slightly to use larger models (8 layers and\n\ud437 = 64 for 6M params, the SaShiMi default), shorter sequences (211 = 2048 to 218 = 262144 instead of 213 to 220),\nlower LR (0.001 from 0.002), and shorter training cycles (100K instead of 200K steps).\nFigure 10 shows that the change from S4 \u2192 S6 (i.e. the selection mechanism) is not always bene\ufb01cial. On long-form\naudio waveforms, it in fact signi\ufb01cantly hampers performance, which may be intuitive from the point of view\nthat audio is uniformly sampled and very smooth, and therefore bene\ufb01ts from continuous linear time-invariant\n(LTI) methods. After ablating away the selection mechanism, note that the resulting model is the S4 layer\ninside the Mamba block. To disambiguate, we call this Mamba-S4 as opposed the default Mamba architecture\nMamba-S6.\nHowever, on the right side, we keep the outer layers of the U-Net Mamba-S4 and ablate only the inner layers.\nThe performance di\ufb00erences shrink dramatically; this reinforces the hypothesis that layers closer to the raw audio\nsignal should be LTI, but once they are \u201ctokenized\u201d and compressed by the outer layers, the inner layers no longer\nneed to be LTI. In this setting however, the real-valued SSM still underperforms the complex-valued one.\n35\nE.4.2\nSC09 Speech Generation\nAutoregressive training largely followed the autoregressive language modeling protocol, such as\n\u2022 Weight decay 0.1\n\u2022 Learning rate warmup for 10% of total steps\n\u2022 AdamW optimizer with \ud6fd = (0.9, 0.95)\n\u2022 Gradient clip value 0.1\nWe used a learning rate of 0.002 and 200000 training steps at a batch size of 16.\nThe large Mamba model in Table 4 has 15 layers per stage with an outer dimension of \ud437 = 96 and pooling factor\n4. We note that this dataset is small (training went through 100 epochs) and for this large model, there was\nsigni\ufb01cant over\ufb01tting of the BPB or NLL. However, automated metrics of generated samples continually improving\nthroughout training.\nThe models in the architecture ablations in Table 5 all have 8 layers per stage with an outer dimension of \ud673 = 64\nand pooling factor 4. The S4+MLP block has roughly 2\ud4372 + 4\ud4372 parameters (expansion factor 2 in the MLP).\nThe Transformer block has 4\ud4372 + 2\ud4372 parameters (expansion factor 1 in the MLP). The Mamba block has the\nusual \u2248 6\ud4372 parameters. All models have roughly 6M total parameters.\nE.5\nEfciency Benchmark\nScan Operation.\nWe compare the core operation of selective SSMs, which is the parallel scan (Section 3.3),\nagainst convolution and attention, measured on an A100 80GB PCIe GPU. Note that these do not include the cost\nof other operations outside of this core operation, such as computing the convolutional kernel in global-convolution\nmodels, or computing the QKV projections in attention.\nAs a baseline, we implement a standard parallel scan in PyTorch with no kernel fusion. This requires materializing\nthe parameters A, B, C in HBM.\nOur scan implementation fuses the discretization step and the parallel scan, avoiding the cost of materializing all\nthe large parameters in HBM.\nFor convolution, we use the standard implementation in PyTorch, which separately performs FFTs on the inputs\nand the \ufb01lters, multiply them in frequency domain, then performs an inverse FFT to obtain the result. The\ntheoretical complexity is \ud442(\ud43f log(\ud43f)) for sequence length \ud43f.\nFor attention, we compare against the fastest implementation that we are aware of (FlashAttention-2 (Dao 2023)),\nwith causal mask. Note that FlashAttention-2 with causal mask is about 1.7\u00d7 faster than without causal mask,\nsince approximately only half of the attention entries are computed.\nWe use batch size of 1 and increase the sequence length from 29 = 512, 210 \u2248 1\ud43e, 211 \u2248 2\ud43e, up to 219 \u2248 500\ud43e\n(some of the baselines run out of memory before reaching 500K). We use a model dimension of \ud437 = 1024 and state\ndimension \ud441 = 16. We measure with BF16 inputs, which is the data type most commonly used for large scale\ntraining.\nEnd-to-end Inference.\nWe measure the inference throughput of a Mamba 1.4B model and an untrained Mamba\n6.9B model, against a standard Transformer (GPT3 architecture) at 1.3B and 6.7B size. We use the standard\nTransformer implementation in the Huggingface transformers library.\nWe set the prompt length to be 2048 and the generation length to be 128. We vary the batch size from 1, 2, 4, 8, 16,\n32, 64, to 128, and measure time time taken to generate 128 tokens. We then calculate the throughput (tokens/s)\nas batch size \u00d7 128\u2215time taken. We repeat the measurements 3 times and take the average. Measurements are\ndone on an A100 80GB PCIe GPU.\nMemory Benchmark.\nThe memory usage simply scales proportionally to the size of the activation tensors, as\nwith most deep sequence models. We report measurements of the training memory requirements of 125M models\n36\nTable 15: (Memory benchmark.) Mamba\u2019s memory footprint is comparable to the most optimized Transformer. Results for 125M\nmodels.\nBatch size\nTransformer (w/ FlashAttention-2)\nMamba\n1\n4.6GB\n4.8GB\n2\n5.2GB\n5.8GB\n4\n6.9GB\n7.3GB\n8\n11.5GB\n12.3GB\n16\n20.7GB\n23.1GB\n32\n34.5GB\n38.2GB\non 1 A100 80GB GPU. Each batch consists of sequences of length 2048. We compare to the most memory-e\ufb03cient\nTransformer implementation we are aware of (with kernel fusion from torch.compile and with FlashAttention-2).\nTable 15 shows that Mamba\u2019s memory requirement is comparable to a similar-sized Transformer with an extremely\noptimized implementation, and we expect further improvement in Mamba\u2019s memory footprint in the future.\n37\n"
  },
  {
    "title": "Merlin:Empowering Multimodal LLMs with Foresight Minds",
    "link": "https://arxiv.org/pdf/2312.00589.pdf",
    "upvote": "24",
    "text": "Merlin: Empowering Multimodal LLMs with Foresight Minds\nEn Yu1*\nLiang Zhao2\u2217\nYana Wei3\nJinrong Yang1\nDongming Wu4\nLingyu Kong5\nHaoran Wei2\nTiancai Wang2\nZheng Ge2\nXiangyu Zhang2\nWenbing Tao1,6\u2020\n1 Huazhong University of Science and Technology\n2 MEGVII Technology\n3 ShanghaiTech University\n4 Beijing Institute of Technology\n5University of Chinese Academy of Sciences\n6 Tuke Research\n\u2217 Equal Contribution\n\u2020 Corresponding Author\nhttps://ahnsun.github.io/merlin/\nObject Tracking\nPlease track this person and panda.\nVideo Referring\nWhat is this player doing in this video? \nThis player is participating in a \nsoccer match and is dribbling \nthe ball.\nVideo Relation\nWhich object does the person use to \nstir the detergent in the bowl?\nThe person uses the spoon to \nstir the detergrent in the bowl. \nFuture Reasoning\nWhat is this player in the first frame \ngoing to do next?Analyse based on \nthe trajectory. \nBased on the trajectory of this player,it \nis evident he is readying to make a \nbasketball 3-point shot next. \nWhat are the chances of the player \nmaking this 3-point shot?\nThere is a defender planning to \ninterfere the shot. Therefore, the \nshooting percentage might decrease.\nDoes the vehicle's movement entail \nany risks? The ego car's correct \ncourse of action is? \nThere is a pedestrian crossing the \nstreet in the middle of the crosswalk. \nThe ego car should be cautious and \nprepared to stop or slow down when \nnecessary to avoid colliding with the \npedestrian. The ego car's correct \ncourse of action is to be patient and \nwait for the pedestrian to cross the \nstreet before proceeding.\nFigure 1. Demo cases presentation of Merlin. Here we showcase several main capabilities of Merlin. Notably, in the dialogue, the words\nmarked with colors correspond to the trajectory outputs of the targets in the image. To save space, we highlight them using the same colors.\nAbstract\nHumans possess the remarkable ability to foresee the fu-\nture to a certain extent based on present observations, a\nskill we term as \u201cforesight minds\u201d. However, this capability\nremains largely under-explored within existing Multimodal\nLarge Language Models (MLLMs), hindering their capacity\nto learn the fundamental principles of how things operate\nand the intentions behind the observed subjects. To address\nthis issue, we introduce the integration of \u201cfuture model-\ning\u201d into the existing learning frameworks of MLLMs. By\nutilizing the subject\u2019s trajectory \u2014 a highly structured rep-\nresentation of a consecutive frame sequence \u2014 as a learn-\ning objective, we aim to bridge the gap between the past and\nthe future. We propose two innovative methods to empower\nMLLMs with \u201cforesight minds\u201d: Foresight Pre-Training\n(FPT) and Foresight Instruction-Tuning (FIT), which are\ninspired by the modern learning paradigm of LLMs. Specif-\nically, FPT jointly training various tasks centered on trajec-\ntories, enabling MLLMs to learn how to attend and predict\n1\narXiv:2312.00589v1  [cs.CV]  30 Nov 2023\nentire trajectories from a given initial observation. Then,\nFIT requires MLLMs to first predict trajectories of related\nobjects and then reason about potential future events based\non them. Aided by FPT and FIT, we build a novel and uni-\nfied MLLM named Merlin that supports multi-images input\nand analysis about potential actions of multiple objects for\nthe future reasoning. Experimental results show Merlin\u2019s\npowerful foresight minds with impressive performance on\nboth future reasoning and visual comprehension tasks.\n1. Introduction\nHuman beings can predict future events or outcomes based\non current observations, known in neuroscience theory as\npredictive processing [20]. In this paper, we refer to this\nability as foresight minds, which involves the use of past ex-\nperiences, knowledge, sensory information, and probabilis-\ntic reasoning to generate expectations about future events.\nIn the artificial intelligence (AI) domain, the capability to\npredict future events is an important topic towards the real-\nization of artificial general intelligence (AGI).\nRecent advancements in Multimodal Large Language\nModels (MLLMs), such as GPT-4V [49] and Bard [2], have\nshown significant potential in image understanding and log-\nical reasoning. Despite these achievements, these models\nstruggle to foresee future events based on current image ob-\nservations. Even provided with additional observations, like\nsequences of multiple frames, the current MLLM models\nstill struggle to adequately analyze and infer specific target\nbehaviors, such as predicting object movements or interac-\ntions (shown in Figure 2). On the contrary, human can rea-\nson the future to some extent based on the observed current\nstate [5, 55], which shows powerful foresight minds.\nTo mitigate this existing deficiency in MLLMs, we start\nfrom dividing human\u2019s process of foreseeing the future into\na two-stage system [31, 55]: (1) observing the dynamic\nclues of the subject and then (2) analyzing the behavior pat-\ntern and reasoning what might happen according to the ob-\nservation. For instance, while watching a basketball game,\npeople will first observe the moving players on the court,\nand then forecast the specific player\u2019s forthcoming actions,\ne.g., shooting, slam-dunking, or passing, by analyzing the\ncurrent states and movement patterns of the players. Com-\npare this system to current MLLMs, we find that MLLMs\ncan complete the second stage well, thanks to the powerful\nlogical reasoning ability of LLM [51, 71]. Therefore the key\nchallenge is the first stage. That is, how to make MLLM ac-\nquire correctly dynamic information from the multi-image\nobservation?\nExplicitly modeling next frames (e.g., reconstructing\nnext frames [11, 74]) can be a straightforward way. How-\never, it can be hard to directly extract dynamic clues\nfrom the redundant visual information [26], especially from\nFailure case of GPT-4V\nWhat is this player in the first frame going to \ndo next?Analyse based on the trajectory. \nBased on the player\u2019s position within the court, their body \nlanguage suggest they might be considering taking a shot.\nIs there any player trying to disturb this shot?\nIn the provided frames, there does not appear to be a \ndefender trying to contest the shot.\n\u221a\nFigure 2. Failure case of GPT-4V about future reasoning.\nvideo sequences.\nIt is necessary to construct a suitable\nlearning objective to assist MLLM in obtaining dynamic\nclues about the specific subjects. To this end, we point out\nthat trajectory, as a highly structured representation, is a\ngood learning objective which can link the temporal con-\ntexts between the past and the future.\nBased on this insight, we propose to model the future\nto empower existing MLLMs with \u201cforesight minds\u201d. Fol-\nlowing the modern learning paradigm of LLMs, our fu-\nture learning process includes two stages: (1) Foresight\nPre-Training (FPT), a paradigm that causally models the\ntemporal trajectories, which interleave with multi-frame\nimages. The model starts with the initial observation of\none or multiple subjects in the first frame as the query\nand then is required to predict the whole trajectory. No-\ntably, we introduce various tasks containing richly labeled\ndata [19, 27, 32, 60, 65, 78], including object detection, ob-\nject tracking, etc., to perform multitask learning. And sam-\nples from these tasks are properly formatted to ensure coor-\ndinated pre-training. (2) Future Instruction-Tuning (FIT),\nthen, considers the trajectory modeling bestowed by FPT as\na bridge in the logical chain of future reasoning. Simply\nput, when querying an MLLM, it must articulate its rea-\nsoning in conjunction with the trajectory for each object\nreferenced. This method, as a form of Trajectory Chain-\nof-Thought, effectively narrows the gap between trajectory\nperception and predictive future reasoning, thereby fully un-\nleashing model\u2019s foresight minds.\nAided by the above future modeling technologies, we\nprovide Merlin1, a novel and unified MLLM capable of\nhandling inputs and outputs of spatial coordinates or track-\n1Merlin is a legendary character in the tales of King Arthur, renowned\nas a powerful wizard and a wise counselor in the Arthurian legends. He is\nfrequently depicted as having the power to foresee future events and has a\ndeep understanding of fate and destiny.\n2\nlets from single image or multiple frames. Moreover, Mer-\nlin is adept at performing inductive reasoning about future\nevents based on current observational results. To demon-\nstrate this, we provide several real dialogues between users\nand Merlin, as displayed in the Figure 1. Unlike the pre-\nvious MLLMs [42, 82, 85] which only supported interac-\ntion with a single image, Merlin not only provides a richer\nmulti-image interaction, but also on this basis, is capable of\nexecuting unique and powerful future reasoning.\nWe construct a new future reason benchmark to evalu-\nate Merlin\u2019s logical reasoning and future prediction abil-\nities.\nThe results, which significantly surpass previous\nbaselines [9, 41, 42, 75], demonstrate Merlin\u2019s stunning\nperformance in future reasoning. We further reveal Mer-\nlin\u2019s exceptional performance in general visual understand-\ning. Through analysis in scenarios such as VQA (Visual\nQuestion Answering) [24, 29], comprehensive understand-\ning [43, 77], and hallucination [39], we unexpectedly dis-\ncovered that our proposed novel paradigm of future learning\naids MLLMs in gaining a deeper understanding of images.\nWe believe this brings new insights for the training of future\nMLLMs.\n2. Related Work\n2.1. Large Language Models\nLarge Language Models (LLMs) have gained significant at-\ntention due to their capabilities in language generation and\nlogical reasoning. Pioneering models like BERT [15], GPT-\n2 [52], and T5 [54] laid the groundwork, but GPT-3 [8],\nthe first model with a 175 billion parameter size, made no-\ntable strides, demonstrating strong zero-shot performance.\nAn emergent ability, wherein model size scaling results in\nsignificant language capability improvements, was also ob-\nserved in LLMs. This was further facilitated by Instruct-\nGPT [50] and ChatGPT [48] using Reinforcement Learn-\ning with Human Feedback (RLHF) on GPT-3. These ad-\nvancements led to what\u2019s called LLMs\u2019 \u201ciPhone moment\u201d.\nFollowing GPT\u2019s success, several open-source LLMs, in-\ncluding OPT [80], LLaMA [67], and GLM [79], have been\nproposed, showing similar performance to GPT-3. Models\nlike Alpaca [66] and Vicuna [10] illustrate the application\nof these LLMs, using a self-instruct framework to construct\nexcellent dialogue models.\n2.2. Multimodal Large Language Models\nThe advancements in LLMs [48, 67, 68] have projected a\npromising path towards artificial general intelligence (AGI).\nThis has incited interest in developing multi-modal ver-\nsions of these models.\nCurrent Multi-modal Large Lan-\nguage Models (MLLMs) harness the potential of cross-\nmodal transfer technologies.\nThese models consolidate\nmultiple modalities into a unified language semantic space,\nand then employ autoregressive language models as de-\ncoders for language outputs.\nModels like Flamingo [1]\nhave adopted a gated cross-attention mechanism, trained on\nbillions of image-text pairs, to align visual and linguistic\nmodalities, showing impressive performance on few-shot\nlearning tasks. Similarly, BLIP-2 [37] introduced the Q-\nFormer to align visual features more effectively with lan-\nguage space. The LLaVA series [41, 42] further enhanced\nthis process by using simply a MLP in place of the Q-\nFormer and designing a two-stage instruction-tuning pro-\ncedure. Apart from creating general MLLMs, techniques\nhave also been developed for visual-interactive multimodal\ncomprehension, involving the precise tuning of referring in-\nstructions [9, 81, 83]. Furthermore, another interesting di-\nrection in MLLM research involves integrating MLLMs for\ncross-modal generation [17, 21, 33] by using text-to-image\nmodels such as Stable Diffusion.\n3. Metholodgy\n3.1. Overall Architecture\nMerlin is designed to unlock the foresight minds based on\nobservations from single images and multi-frame videos.\nIn order to accomplish this, images and videos are com-\nprehensively represented through a series of visual tokens,\nwhich are then integrated into the language sequence that\ncan be comprehended by Large Language Models (LLMs)\nin a unified framework. Specifically, Merlin consists of an\nimage encoder, a decoder-only LLM, and a modality align-\nment block as illustrated in Figure 3.\nFollowing preva-\nlent practice [9, 41, 42, 85], we opt for the pre-trained\nCLIP [53] ViT-L/14 [18] as the visual encoder and Vicuna-\n7B v1.5 [10] as the large language decoder. For more de-\ntails, please refer to our supplementary materials.\nTo provide enough visual information and details, the in-\nput images are resized to a resolution of 448 \u00d7 448. At this\njuncture, the visual encoder iteratively attends to (448/14)2\nuniformly divided image patches, yielding 1024 encoded\ntokens. Considering the limited context length of LLMs and\naddressing the substantial computational challenges posed\nby high resolution and multi-frame context modeling, we\nsimply utilize a 2D convolution to achieve both dimension\nprojection and token aggregation [7, 56].\nWe choose 2D convolution over 1D linear layers [9, 41,\n42] or cross-attention layers [4, 37, 85] as connector for the\nfollowing reasons: (1) 2D convolution clusters local visual\ntokens on a spatial scale [23], effectively achieving a one-\nstep transformation from spatial to channel information; (2)\nThe good convergence properties [30, 62] of 2D convolu-\ntion compared with cross-attention lay a solid foundation\nfor foresight learning in a two-step training approach.\n3\nVision \nEncoder\nProjector\nMultiple Images Contexts\nInitial Observation\nLocation:Frame1:[20,46,145,191]\nAppearance: The red player\nAction: Running on the rugby field\nUser Prompt\nWhat will happen to this player?\nGiven\nvideo\nclip\na\ntell\nme\nwhat\nwill\nhappen\nto\nthis\nplayer\n.\nLarge Language Model \n...\na\nvideo\nclip\ntell\nme\nwhat\nwill\nhappen\nShow me the trajectory please.\nJudging\nby\nhis\n...\nJudging by his respective trajectories and movements: Track1<Id1>Frame1:[50,66,115,191];Frame2:[65,60,124,205];Frame3:[62,70,136,214]. This player will be tackled by \nanother player in a white jersey:Track2<Id2>Frame1: [35,58,119,203];Frame2:[13,52,113,200];Frame3:[1,72,101,205]</Id2>. And both players will fall to the ground. \nResponse:\nTrajectory Visualization\nFirst frame\nFigure 3. Overall pipeline of Merlin. The architecture of Merlin consists of three main components: (1) an image encoder, (2) a large\nlanguage model, and (3) a modality-align projector. Bottom: The diverse input format that supports multiple-image contexts, initial\nobservation and the specific user prompt. Top: The model response including the predicted trajectory and the feature reasoning.\n3.2. Foresight Pre-Training\nGenerative Pre-Training (GPT) [8, 48, 49] serves as the\ncornerstone of this generation\u2019s Language Models (LLMs).\nThrough learning to predict next token, the model effi-\nciently condenses data, thereby yielding emergent forms of\nintelligence [70]. In this context, a very natural approach to\nenhance the model\u2019s perception of the dynamic clues across\nmultiple frames is to explicitly model the next frame (or im-\nage). However, due to the high redundancy in multi-frame\nvisual information, the truly next-frame prediction remains\na significant challenge to date. A better approach at this\njuncture is to implicitly model high semantic information\nin the label space (such as categories, bounding boxes) on\na frame-by-frame basis.\nTemporally, this label informa-\ntion forms a trajectory, a highly structured representation.\nCausally modeling the trajectory in conjunction with each\nframe of image helps to connect the past and present in time,\nthus enabling the model to perceive the future.\nTo this end, we propose the Foresight Pre-Training, a\nway of causally modeling the trajectories interleaved with\nmulti-frame images, to empower the MLLM with the capac-\nity of perceive the dynamic clues, and ultimately achieving\nfuture reasoning. Specifically, given a video clip includ-\ning several frames, we first give the model the observation\nof the first frame, then we require the model to predict the\nwhole trajectory of the concerned subject in this video. No-\ntably, the observation of the first frame can be the descrip-\ntion or simple position of the concerned object. Formally,\nP(Y |X) \u223c P(Y |{X1, X2, ...}, Ofirst),\n(1)\nwhere Xi denotes the ith frame and Ofirst is the first frame\nobservation, Y refers to the trajectory of the subject in\nOfirst within the frame sequence.\nThe observation and\nthe raw frames will be regarded as the condition to prompt\nMLLM to predict the trajectory.\nData Construction. We first aggregate all valuable mul-\ntimodal information from diverse data resources and then\nproperly organize them for multi-task foresight pre-training.\nSpecifically, for each sample instance I, we first collect its\nmultimodal information including consecutive multi-frame\nimages {X1, X2, ...}, subject observations from the first\nframe Ofirst, and subject trajectory Y constructed from all\nframes. Formally,\nI = {{X1, X2, ...}, Ofirst, Y }.\n(2)\nWe categorize observations of one subject of the first frame\ninto three types: location description, appearance descrip-\ntion and action description. Then we randomly select one of\nthese observations of a particular subject in the first frame\nas the query object.\nTo better unleash the powerful generative modeling ca-\npacity of LLM, we construct this query process as a type\nof conversation. Here is an example of the constructed data\nshown in Figure 3. In this case, we want to query the sub-\nject \u2014 the panda on the right \u2014 with the randomly select\nobservation, and expect the answer with the movement tra-\njectory of this panda across multiple frames. To model this\nprocess, we convert the query to question and trajectory to\nanswer with proper natural language for embellishment.\nOverall, the aforementioned process of dialogization\nroughly follows these three principles: (1) Precise defini-\ntion of task prompts and answer formats. In particu-\nlar, we use a task prompt to tell MLLM what specific task\nto do (detect or track), and also specified the answer for-\n4\nAn Example  of FPT dialogue\nGiven a video clip including:\nframe1:\n\\n,frame2:\n\\n,frame3:\n\\n\nInterleaved Multiple Images:\nInitial Observation:\nAppearance: A panda on the right side.\nAction: A panda is lying on the ground.\nRandomly choice\nLocation: Frame1:[562, 342, 926, 561]\nDialogue:\nQuestion: Interleaved Multiple Images, can you tell me the \ntrajectory according to the initial observation ? To \nrespond correctly, utilize the specified <Idi>Frame \nt:[xmin,ymin,xmax,ymax]</Idi> format.\nAnswer:\nIts trajectory is <Id1>Frame1:[562,342,926,561]; Frame2: \n[576,334,960,568];Frame3:[632,366,979,589]</Id1>.\nFigure 4. One example to illustrate the multi-modality pre-\ntraining dataset. The top block shows the provided contexts in-\ncluding the multiple images contexts and initial observation (box,\nappearance and action) about the subject to prompt the LLM. The\nbottom block shows the dialogue including question and answer.\nmat with accurate descriptions in each question.\nIn this\nway, different types of tasks can be flexibly organized to-\ngether without compromising the general language ability.\n(2) Clear indication of multimodal information. Con-\ncretely, for each group of image tokens, we add a special\nframe indicator in front of then, i.e., frame1:<image> and\nframe2:<image>, so as to help MLLM better focus on the\ncorresponding image. (3) Interleaving of frames and ob-\nservations. For the same identity, we interleave the frames\nin which it appears with its positional observations, and en-\nclose them with two ID tokens (i.e. <Idi> and </Idi>)\nto construct a trajectory. We believe that this interleaved or-\nganization helps in generatively training to model causality\nwithin the trajectory, while the ID tokens ensures that the\nmodel can distinguish among different identity objects.\nTraining Details. Previous practices [4, 40, 42] typically\nconducting a separate modality alignment training phase\nfollowing a multi-task pre-training stage, which however,\ncomplicates the training process and data construction. In\nthis paper, we directly incorporate both of them into one\nstage, and unfreeze all modules during pre-training. This is\nbecause that we believe the MLLMs are sufficiently pow-\nerful to concurrently handle the learning of general mul-\ntimodal capabilities and multi-task specific abilities under\nproper guidance. Furthermore, we mix a large amount of\nimage-text pairs and rich-annotated conversation data (for-\nmatted according to the above method) from diverse data\nsources [19, 27, 32, 47, 61, 63, 65, 78] to conduct multi-\ntask learning. In doing so, not only endows the model with\nforesight minds but also ensures its multimodal alignment.\n3.3. Foresight Instruction Tuning\nAlthought Foresight Pre-Training equips the model with the\nability to observe dynamic clues across multiple frames, it\nstill falls short of true foresight minds. This is because mod-\nels typically struggle to effectively transform such observa-\ntions into successful future reasoning [69, 84].\nRecent work [51, 84] has highlighted that Chain-of-\nThought (CoT) [71] is crucial in bridge the gap between\nthe observations and actions of MLLMs with theory of\nmind [57, 69]. Meanwhile, several prior studies [9, 82] have\nalso demonstrated that prompts indicating position (such as\nbounding boxes or points) \u2014 a principle analogous to CoT\n\u2014 can concentrate an MLLM\u2019s attention on the relevant\narea, leading to more accurate dialogues and reducing the\nlikelihood of visual hallucination. Drawing inspiration from\nthese findings, we conduct the Foresight Instruction Train-\ning (FIT) building upon the foundation of FPT to further en-\nhance the model\u2019s future reasoning capability. In specific,\nbuilding on the trajectory generating powered by FPT, we\nfurther union the trajectories to generatively rationalize the\nforthcoming events. Mathematically,\nP(Z|X, Y ) \u223c P(Z|{X1, X2, ...}, Ofirst, Y ),\n(3)\nwhere Z refers to the future observation which is deduced\nfrom observations in each frame. It can be actions, events,\ntrends, or simply likelihoods. In this context, multi-frame\nimages, in conjunction with the first subject observation,\nand the trajectory of the same subject across all frames,\nserve as the union condition to prompt MLLM to causally\npredict the future. This way, akin to a Trajectory Chain-\nof-Thought, effectively bridges the gap between trajectory\nperception and predictive future reasoning, thereby fully un-\nleashing model\u2019s foresight minds.\nData Construction. The specific data construction method\nis similar to FPT, but on this basis, we also deduce a future\nobservation Z from the information across multiple frames\nand append it after the trajectory in the answer. Formally,\nI = {{X1, X2, ...}, Ofirst, Y, Z}.\n(4)\nPractically, in this paper, we constitute future observa-\ntions based on multi-frame, multi-target action descriptions\ncombined with human priors, and further process them with\nGPT-4 [49] to ultimately form reasonable future inferences.\nMore details are provided in the supplementary materials.\nFigure 4 provides an illustrative example of FIT, when\na user questions Merlin about the future of a player in red\nattire, Merlin initially presents the observed trajectory of\nthe concerned player, followed by the trajectory of another\nplayer in white. Using these trajectories, Merlin deduces\nthat the player in white is likely to tackle the one in red,\nresulting in both players falling to the ground.\n5\nMethod\nLLM Size\nFuture Reasoning (Dev.)\nFuture Reasoning (Test)\nAverage\nOL\nPPR\nFR\nIR\nFP\nAverage\nOL\nPPR\nFR\nIR\nFP\nInstructBLIP [13]\n13B\n42.0\n14.8\n30.7\n56.8\n88.9\n19.0\n44.4\n5.7\n24.0\n67.3\n92.7\n32.4\nMiniGPT-4 [85]\n13B\n43.3\n28.4\n30.7\n49.4\n86.7\n21.4\n48.9\n21.0\n35.0\n67.3\n90.2\n31.1\nOpenFlamingo [3]\n7B\n5.28\n2.5\n10.7\n8.6\n2.2\n2.4\n11.5\n2.9\n14.0\n9.3\n11.0\n20.3\nMMGPT [43]\n7B\n19.5\n1.2\n24.0\n9.9\n60.0\n2.4\n16.8\n3.8\n13.0\n12.1\n52.4\n2.7\nMiniGPT-4 [85]\n7B\n26.8\n7.4\n14.7\n19.8\n80.0\n11.9\n27.9\n8.6\n13.0\n29.9\n61.0\n27.0\nInstructBLIP [13]\n7B\n34.8\n6.2\n17.3\n51.9\n84.4\n14.3\n39.0\n2.9\n17.0\n52.3\n78.0\n44.6\nLLaVA [42]\n7B\n38.7\n8.6\n25.3\n53.1\n77.8\n28.6\n39.7\n13.3\n35.0\n48.6\n82.9\n18.9\nmPLUG-Owl [75]\n7B\n41.0\n18.5\n18.7\n66.7\n86.7\n14.3\n45.9\n16.2\n23.0\n59.8\n91.5\n39.2\nShikra [9]\n7B\n51.5\n32.1\n30.7\n63.0\n88.9\n42.9\n60.0\n27.6\n50.0\n70.1\n92.7\n59.5\nKosmos-2 [28]\n1.6B\n54.4\n38.3\n33.3\n56.8\n91.1\n52.4\n58.2\n40.4\n30.0\n65.4\n89.0\n66.2\nLLaVA-1.5 [41]\n7B\n59.6\n43.2\n52.0\n71.6\n93.3\n38.1\n-\n-\n-\n-\n-\n-\nMerlin (Ours)\n7B\n64.4\n42.0\n54.7\n72.8\n97.8\n54.8\n66.5\n41.3\n51.0\n83.0\n97.6\n59.7\nTable 1. The Effectiveness of Future Reasoning. We mainly select 5 metrics from MMBench develop and test set, respectively, including\nOL: Object localization, PPR: Physical property reasoning, FR: Function reasoning, IR: Identity reasoning, and FP: Future prediction.\nThe best and second-best performances are shown in bold font and underlined respectively.\nLaSOT\nGOT10k\nMethod\nSuccess\nPnorm\nP\nAO\nSR0.5\nSR0.75\nSpecialist Models\nSiamFC [6]\n33.6\n42.0\n33.9\n34.8\n35.3\n9.8\nATOM [14]\n51.5\n-\n-\n55.6\n63.4\n40.2\nSiamRPN++ [36]\n49.6\n56.9\n49.1\n51.8\n61.8\n32.5\nSiamFC++ [73]\n54.4\n62.3\n54.7\n59.5\n69.5\n47.9\nGeneralist Models\nMerlin (Ours)\n39.8\n40.2\n38.1\n51.4\n55.9\n42.8\nTable 2.\nComparison of Merlin with available tracking\nmethod. Notably, Merlin is only trained on a small sampled data\nfrom the relevant tracking datasets instead of the full one.\nTraining Details. We freeze the vision encoder and keep\nthe convolutional projector and the LLM unfreezed in this\nstage. On this basis, we primarily adopt the open-source in-\nstruction tuning datasets, e.g. LLaVA-665K [41] for build-\ning the basic ability for multi-round visual-language con-\nversation. For further unleash the foresight minds of model,\nwe first uniformly sample a certain number of multitask di-\nalogues in FPT, in order to maintain the model\u2019s capacity\nof modeling the dynamic clues across multi-frame images.\nIn addiction, we also sample data from three specific sce-\nnario datasets [38, 46, 72] and construct around 60K FIT\nconversations based on the aforementioned data construc-\ntion process.\n4. Experiment\n4.1. Experimental Settings\nDatasets.\nFor the foresight pre-training (FPT) stage,\nwe\nfirst\nuse\n10M\nimage-text\npairs\nsampled\nfrom\nLAION400M [58] to ensure multimodal alignment.\nOn\nthis basis, we gather various open-source datasets with rich\nannotations to conduct multi-task learning, including (1)\nobject detection datasets: Object365 [61] and OpenIm-\nage [34]; (2) tracking datasets: LaSOT [19], GOT10K [27],\nMOT17 [47], DanceTrack [65] and SOMPT22 [63]; (3)\ngrounding dataset: Ref-COCO [32]; (4) object relation\ndataset: VCR [78]. For these data, as described in Sec-\ntion 3.2), we apply strict task definitions and format spec-\nifications, and re-organize them in the form of interleaved\nframes and observations.\nUltimately, we obtain approx-\nimately 5M question-answer data, which are mixed with\n10M paired data for foresight pre-training.\nFor the foresight instruction-tuning (FIT) stage, we mix\napproximately 730K conversation data, including (1) open-\nsource instruction-tuning data LLaVA-665K [41], which in-\ntegrates a series of VQA datasets [64] and multi-round con-\nversation datasets [42]; (2) around 30K FIT multi-frame\nconversations constructed from three specific scenarios in-\ncluding MultiSports [38], TITAN [46] and STAR [72] based\non the data construction method described in Section 3.3;\n(3) nearly 40K randomly sampled FPT multi-task data. For\nmore details of the datasets, please refer to the supplemen-\ntary materials.\nImplementation Details. During the foresight pre-training,\nwe optimize all parameters of the model, setting the learn-\ning rate to 5e \u2212 5 and training for one epoch. In the instruc-\ntion tuning stage, we freeze the visual encoder and fine-tune\nthe parameters of the projector and LLM. In both stages, we\ntrain Merlin using the AdamW [45] optimizer and a cosine\nannealing scheduler [44] as the learning rate scheduler. Ad-\nditional implementation details can be found in the supple-\nmentary materials.\n4.2. Properties Evaluation of Foresight Minds\nIn this section, we mainly verify the foresight minds of Mer-\nlin from two aspects, i.e., future reasoning and identity as-\nsociation ability, where the former focuses on forecasting\nfuture events or behavior based on image observation, and\nthe latter focuses on the model\u2019s ability to establish sub-\nject identity associations across multiple frames to obtain\ndynamic clues.\nFuture Reasoning. To evaluate this ability, we probe this\n6\nMethod\nLLM Size\nRandom\nPopular\nAdversarial\nAccuracy\nF1-Score\nYes\nAccuracy\nF1-Score\nYes\nAccuracy\nF1-Score\nYes\nLLaVA [42]\n13B\n64.12\n73.38\n83.26\n63.90\n72.63\n81.93\n58.91\n69.95\n86.76\nMiniGPT-4 [85]\n13B\n79.67\n80.17\n52.53\n69.73\n73.02\n62.20\n65.17\n70.42\n67.77\nInstructBLIP [13]\n13B\n88.57\n89.27\n56.57\n82.77\n84.66\n62.37\n72.10\n77.32\n73.03\nShikra [9]\n13B\n86.90\n86.19\n43.26\n83.97\n83.16\n45.23\n83.10\n82.49\n46.50\nMultiModal-GPT [22]\n7B\n50.10\n66.71\n99.90\n50.00\n66.67\n100.00\n50.00\n66.67\n100.00\nmPLUG-Owl [75]\n7B\n53.97\n68.39\n95.63\n50.90\n66.94\n98.57\n50.67\n66.82\n98.67\nLLaVA [42]\n7B\n72.16\n78.22\n76.29\n61.37\n71.52\n85.63\n58.67\n70.12\n88.33\nMerlin\n7B\n91.58\n91.66\n49.38\n89.53\n89.56\n50.27\n84.10\n84.95\n55.63\nTable 3. Zero-shot object hallucination evaluation on the COCO validation set. \u201cYes\u201d represents the proportion of positive answers\nthat the model outputs.\nVQA Task\nGeneralist\nMethod\nGQA\nVisWiz\nMMBd\nMMBt\nMM-Vet\nBLIP-2 [37]\n41.0\n19.6\n-\n-\n22.4\nInstructBLIP [13]\n49.2\n34.5\n36.0\n33.9\n26.2\nShikra [9]\n-\n-\n58.8\n60.2\n-\nIDEFICS-9B [35]\n38.4\n35.5\n48.2\n45.3\n-\nIDEFICS-80B [35]\n45.2\n36.0\n54.5\n54.6\n-\nQwen-VL\u2020 [4]\n59.3\n35.2\n38.2\n32.2\n-\nQwen-VL-Chat\u2020 [4]\n57.5\n38.9\n60.6\n61.8\n-\nLLaVA-1.5 [41]\n62.0\n50.0\n64.3\n59.5\n30.5\nMerlin (Ours)\n60.5\n50.4\n66.2\n65.5\n34.9\nTable 4. Comparison with SOTA methods on main MLLM\nbenchmarks. For VQA tasks, we mainly choose GQA [29] and\nVisWiz [25] to evaluate the model; For general evaluation, we\nmainly choose MMBench [43] and MM-Vet [77].\n\u2020Includes us-\ning in-house data that is not publicly accessible.\nability based on the several sub-tasks of MMBench [43].\nMMBench provides a comprehensive evaluation system to\nassess various capabilities of MLLM, with some metrics\nfocusing on the model\u2019s prediction and reasoning capabil-\nities. To this end, we pick out these metrics to establish\nthis new future reasoning benchmark and compare Mer-\nlin with the existing SOTA models. As shown in Table 1,\nMerlin achieves the best overall performance (64.4 average\nscore on the development set and 66.5 average score on the\ntest set). Moreover, it obtains the best in 8/10 indicators\nand ranks second in all other indicators, which favorably\ndemonstrates Merlin\u2019s strong future reasoning ability.\nIdentity Association. We examine this ability by evalu-\nating the performance of object-tracking tasks, which can\ncomprehensively demonstrate object association and pre-\ndiction capabilities. To this end, we evaluate Merlin in ex-\nisting mainstream tracking benchmarks, i.e., LaSOT [19]\nand GOT10K [27]. It is worth noting that Merlin is the\nfirst MLLM that can also carry out tracking tasks. As\nshown in Table 2, Merlin achieves comparable performance\nwith expert models and even outperforms on some metrics.\nNotably, we only sample a small amount of tracking data\nto train Merlin instead of the full amount of data, which\nmeans LLM exhibits significant potential in handling tem-\nporal tasks, possibly because tracking, as a temporal task,\ncan be viewed as a casually frame-level autoregressive task.\n4.3. Object Hallucination\nHallucination presents a significant challenge in existing\nMLLMs.\nThis term describes the phenomenon where\nthe generated textual content exhibits inconsistencies when\ncompared to its corresponding image content. In this sec-\ntion, we present the experiments from the Polling-Based\nObject Probing Evaluation (POPE [39]). As demonstrated\nin Table 3, Merlin surpasses recent SOTA methods with\nclear margins. More specifically, Merlin achieves optimal\nperformance in all metrics across three scenarios: Ran-\ndom, Popular and Adversarial, with improvements of up\nto 5 points compared to the highly competitive baseline\nShikra [9]. Surprisingly, in multiple scenarios, the \u2019yes\u2019 rate\nof Merlin is quietly close to 50%, demonstrating its extraor-\ndinary visual perception capabilities.\nWe analyze this success largely owing to the proposed\nforesight learning (FPT and FIT). By enabling the model\nto learn the dynamic correspondence between trajectories\nacross multiple images, the model has gained a more precise\nability to attend to relevant object (trajectories) contexts in\nthe image, which helps to better avoid misidentification and\nmisalignment of irrelevant targets. We believe that this re-\nsult will provide new thinking about addressing the issue of\nhallucinations in MLLM.\n4.4. General Comprehension\nIn order to showcase the general multi-modal ability, we\nfurther benchmark Merlin on various VQA benchmarks and\nrecent benchmarks proposed for evaluating the comprehen-\nsive capabilities of MLLMs.\nVisual Question Answering (VQA). We first evaluate\nMerlin on several mainstream VQA benchmarks to reflect\nthe perceptual abilities of MLLMs in understanding image\ncontent. As shown in Table 4, Merlin achieves competi-\ntive performance compared with existing advanced MLLMs\nin the selected VQA benchmarks (VQA). The results indi-\ncate that Merlin possesses strong image understanding and\nquestion-answering capabilities.\n7\nThis player <Id1> Frame1: [461, 278, 651, 976]; Frame2: [458, 254, 609, 968]; Frame3: [442, 254, 607, 918] </Id1> is \ndribbling the ball while being pursued by the opponents.\nFigure 5. Attention map visualization. To facilitate the observation, we map the attention between the box responses and the visual\ntokens of each frame for visualization.\nPre-Training\nInst.-Tuning\nGOT10K\nFuture Rea.\nITP FPT-Data\nITD FIT-Data\nAO\nAveragedev\n!\n\u2717\n!\n\u2717\n-\n59.5\n!\n\u2717\n!\n!\n-\n60.7\n\u2717\n!\n!\n!\n15.5\n52.8\n!\n!\n!\n\u2717\n51.4\n61.2\n!\n!\n!\n!\n51.4\n64.4\nTable 5.\nAblation study of the proposed strategies in Mer-\nlin. (ITP: Image-text pair data, ITD: instruction-tuning data). We\nmainly report the AO score of GOT10k and the average score of\nfuture reasoning.\nSynthetica MLLM Benchmarks. Recently, several bench-\nmarks have been proposed to evaluate the comprehen-\nsive performance of MLLMs, encompassing diverse finer-\ngrained scenarios including visual perception, object recog-\nnition, optical character recognition (OCR), future reason-\ning, and so on. In this part, we select several mainstream\nMLLM benchmarks to evaluate Merlin. As shown in Ta-\nble 4, We present performance in accuracy on benchmarks\nincluding MM-Vet [77] and MMBench [43].\nOn MM-\nBench, we report results on the both development and test\nsets. The results show that Merlin significantly outperforms\ncomparative methods, even though many methods utilized a\nsubstantial amount of in-house data for pre-training, or em-\nployed several times more parameters. This implies that,\nwhile introducing foresight minds into MLLMs, we not\nonly preserved their original visual capabilities but even fur-\nther enhanced their overall level of visual perception.\n4.5. Ablative Analysis of FPT & FIT\nAs introduced in Section 3.2 and Section 3.3, FPT serves\nas the pre-training strategy to enable MLLM to encapsulate\ndynamic information across frames by predicting the tra-\njectory of the next frame. FIT is designed to activate the\nability of foresight minds in a way of trajectory CoT dur-\ning instruction fine-tuning. To further explore the effect of\nFPT and FIT, we conduct an ablation study based on the es-\ntablished future reasoning benchmark and tracking dataset\nGOT10K [27]. As shown in Table 5, we mainly report the\naverage overlap (AO) of GOT10K and the average score of\nfuture reasoning in the development set.\nThe results show that both FPT and FIT training strate-\ngies contribute to the improvement of the metrics. Combin-\ning both FPT and FIT, Merlin achieves the best performance\nwhich proves the effectiveness of the proposed strategies.\nFurthermore, we can also observe that the lack of image-\ntext pair data during the pre-training stage considerably\nhampers the model\u2019s general ability. This phenomenon sup-\nports our perspective that, during the comprehensive pre-\ntraining phase, the integration of image-text pair data is es-\nsential for maintaining modality alignment and preventing\na decline in combined capabilities.\n4.6. Visualization Analysis\nIn this subsection, we visualize the attention map of Merlin\nto further substantiate the effectiveness of utilizing the pro-\nposed strategies. As shown in Figure 5, we select the output\nattention map of the middle-level layers of LLM for visu-\nalization. We can observe that the word embedding of the\noutput trajectory coordinates can attend to the correspond-\ning object from different frames correctly. This visualiza-\ntion results further prove that the trajectory representation is\na good interface to enable MLLM to establish the alignment\nbetween the language description and the multi-images dy-\nnamic visual contexts.\nFurthermore, this effectively ex-\nplains why Merlin possesses a more powerful comprehen-\nsive visual capability and a greatly lower level of hallucina-\ntion compared to previous baselines. Indeed, the trajectory-\ndriven foresight learning allows the large language model\nto read images more profoundly!\n5. Limitation and Conclusion\nThis study highlighted an obvious deficiency in Multimodal\nLarge-Language Models (MLLMs), specifically their abil-\nity to predict future events or outcomes based on current ob-\nservations, referred as \u201cforesight minds\u201d. To address this,\nwe serve as the first to point out that trajectory, as a highly\nstructured representation, is a good learning objective to\nassist MLLM in obtaining dynamic information from the\nimage observations. Based on this insight, we introduced\na unique training method including Foresight Pre-Training\n8\n(FPT) and Foresight Instruction-Tuning (FIT). By synergiz-\ning FPT and FIT, we created Merlin, a unified MLLM that\neffectively understands and outputs spatial coordinates or\ntracklets from single images or multiple frames. Merlin ex-\ncels at a range of traditional vision-language tasks while\ndemonstrating powerful future reasoning capacities. De-\nspite the substantial advancements made by Merlin, there\nstill are some limitations, particularly in processing long se-\nquential videos and more comprehensive feature reasoning\nevaluation. We aspire for Merlin to guide the enhancement\nof more advanced MLLMs in the future.\nA. Appendix\nIn this appendix, we offer additional information about\nMerlin due to the paper\u2019s page limit of 9 pages. Specifi-\ncally, Section B provides in-depth insights into the dataset\nwe constructed, including its data sources and how it was\ncreated. Section C delves into the data formats of the pro-\nposed FPT and FIT, as explained in the main manuscript.\nSection D offers a more detailed explanation of the train-\ning approach. Section E expands on the discussion of the\nproposed method, including its limitations and future di-\nrections. Finally, Section F offers additional experimental\nresults and demo visualizations.\nB. Dataset Details\nIn Section 3 of the manuscript, we explained how we\ncreated the Foresight Pre-Training (FPT) and Foresight\nInstruction-Tuning (FIT) datasets. Now, in this section, we\ngo into greater detail about how we collected and built the\ndataset. To start, we provide an overview of our collected\ndata in Table 6, and then dive into the step-by-step process\nof how it was constructed.\nImage-text pairs.\nWe mainly collect image-text pairs\ndataset from the open-sourced dataset, LAION-400M [58].\nLAION-400M is a high-quality dataset with CLIP-filtered\n400 million image-text pairs. In this paper, we random sam-\nple 10M image-text pairs of LAION-400M for the Foresight\nPre-Training.\nDetection datasets. Detection datasets are crucial for im-\nproving the model\u2019s ability to understand space during its\ninitial training phase.\nIn our research, we used datasets\nfrom various publicly available sources like Object365 [60],\nOpenImage [34], and CrowdHuman [59]. We processed\nthese datasets in the following ways:\n(1) Extract all objects in each image along with their cate-\ngories and bounding boxes.\n(2) Remove too small objects (smaller than 1/32 of the im-\nage size).\n(3) Randomly select 15 categories if image has more cate-\ngories than a set limit (15).\nTask\nData\nSize\nTask description example\nCap.\nLAION\n10M\n\u2013\nRef.\nRefCOCO\n200K\nRefer to the supplied image and box coor-\ndinates for a short response to the question.\nMeViS\n180K\nProvide the trajectory coordinate of the tar-\nget according to the sentence describes.\nDet.\nObject365\n3.2M\nWhen submitting your answer, maintain the\ncategory:[xmin,ymin,xmax,ymax] structure\nconsistently.\nOpenImage\n800K\nCrowdHuman 20K\nTrack\nGOT10K\n100K\nFor the trajectories included in the answer,\nplease use the format <Idi>Frame t:\n[xmin,ymin,xmax,ymax]</Idi>.\nLaSOT\n15K\nMOT17\n10K\nSompt\n5K\nDanceTrack\n25K\nSportsMOT\n20K\nBDD100K\n120K\nRea.\nVCR\n250K\nIn response, account for any relevant object\nlocations, denoted by [x0,y0,x1,y1].\nDia.*\nLLaVA-Ins\n665K\n\u2013\nMerlin-chat\n30K\nTable 6.\nAll training data.\nCap.:Captioning, Ref.:Referring\n(including REC, REG and Referring Tracking), Det.: Detection,\nTrack:Tracking (including single object tracking (SOT) and mul-\ntiple object tracking (MOT)), Rea.:Reasoning, Dia.:Dialogue. *\nmeans that the data is only used in the SFT stage.\n(4) Compose the detection data format as the following\ncat1:<box>,<box>,<box>;cat2:<box>;....\nAs shown in Table 6, we also created special output\nprompts that guide the model to respond in a precise for-\nmat, as detailed in our study.\nTracking datasets. Building tracking data is a key part\nof our Foresight Pre-Training (FPT) method, as we\u2019ve dis-\ncussed in our main paper.\nFPT is designed to causally\nmodeling the trajectory in conjunction with each frame\nof the image to empower the model to acquire foresight\nminds. The tracking data naturally includes trajectory in-\nformation about movement paths, making it ideal for our\nFPT task. We gathered a variety of open-source tracking\ndata, including Single Object Tracking (SOT) and Multi-\nObject Tracking (MOT) datasets like GOT10K [27], La-\nSOT [19], MOT17 [47], Sompt [63], DanceTrack [65],\nSportsMOT [12], BDD100K [76], MeViS [16]), etc. We\nfollow the steps below to pre-process the tracking data.\n(1) Random sample 3 to 5 frames at a certain frame interval\n(gap = 3,4 or 5) to form a video clip each time.\n(2) Extract all trajectories with their category, identity and\nbounding boxes in each video clip.\n(3) Remove the trajectory containing too small objects\n(smaller than 1/32 of the image size).\n(4) Select the initial observation (location, appearance or\naction in the first frame) as the trajectory query.\n(5) Compose the tracking data format as the following\nquery,cat1<Idi>Frame1:<box>;Frame2:<box>;\n...</Idi>.\n9\nExamples of Data Formats\nImage-Text Pairs\n<im_start>LAION400M/Key-094857437.jpg<im_end>\\nGo Compare Car Insurance Groups by Moneysupermarket On The Forbes Innovative Growth.<eos>\nGeneral Detection \n<im_start>Object365/v2/patch16/objects365_v2_00900001.jpg<im_end>\\ndetect all \nobjects.Maintain strict adherence to the format category:[xmin,ymin,xmax,ymax] when \npresenting your answer. Answer:Person:[0,374,472,1000],[559,382,902,766];Necklace: \n[148,593,341,651];Mirror:[387,119,1000,836].<eos>\nOpen-Vocabulary Detection\n<im_start>Object365/v1/patch8/objects365_v1_00420917.jpg<im_end>\\ndetect parking \nmeter,shelf and bakset.When composing your answer, be sure to consistently utilize the \ncategory:[xmin,ymin, xmax,ymax] structure. Answer:Shelf:[190,247,242,392],[266,293, \n318,401]; Bakset:[4,213,10,254] ,[9,210,146,251].<eos>\nReferring Expression Comprehension \n<im_start>COCO_train2014_000000310289.jpg<im_end>\\ntell me the position of the \ngiant doughnut with white icing and red , white , and blue sprinkles. Utilize the \n[xmin,ymin, xmax,ymax] format in your answer. Answer:[523,621,816,937].<eos>\nReferring Expression Generation\n<im_start>COCO_train2014_000000106755.jpg<im_end>\\ntell me what can you see in \nthis region[131,238,675,578]? Craft a concise reply using the image and box coordinates \nyou have at hand. Answer:I can see a guy with his back turned to us.<eos>\nGeneral Tracking \nGiven frame1:<im_start>dancetrack0004/img1/00000001.jpg<im_end>\\n,frame2: \n<im_start>dancetrack0004/img1/00000006.jpg<im_end>\\n,track person<Id1>Frame1: \n[497,401,655,918]</Id1>,person<Id2>Frame1:[325,351,433,933]</Id2>.Keep your \nresponse consistent with the class<Idi>Frame t:[xmin,ymin,xmax,ymax]</Idi> \nformat.Answer:person<Id1>Frame1:[497,401, 655,918];Frame2:[506,491,671,912]</\nId1>,person<Id2>Frame1: [325,351,433,933];Frame2: [295,409,445,975]</Id2>.\nReferring Tracking \nGiven frame1:<im_start>MeViS/6084240e75fa/00055.jpg<im_end>\\n,frame2:<im_start> \nMeViS/6084240e75fa/00060.jpg<im_end>\\n and frame3:<im_start>MeViS/6084240e75fa/\n00060.jpg<im_end>\\n,point out the trajectory of the panda rolling around.Your trajectory \ndetails should strictly align with the Tracki<Idi>Frame t:[xmin,ymin,xmax,ymax]</Idi> \nformat in your submission. Answer:Track1<Id1>Frame1:[483,658,275,340]; Frame2: \n[498,714, 225,283];Frame3:[481,725,223,269]</Id1>.\nGiven frame1:<im_start>MeViS/8e78a5a2978a/00014.jpg<im_end>\\n,frame2:<im_start> \nMeViS/8e78a5a2978a/00015.jpg<im_end>\\n and frame3:<im_start>MeViS/8e78a5a2978a/\n00020.jpg<im_end>\\n,tell me what this object<Id1>frame:1:[340,347,407,391];frame:2: \n[362,329,428,365];frame:3:[412,310,503,389]</Id1> is doing?Given the image frames and \ntrajectory coordinates, please respond succinctly to the question. Answer: This is a person \nleaning towards the left while riding motorized-bicycle\nVisual Reasoning\n<im_start>VCR/lsmdc_3038_ITS_COMPLICATED/3038_ITS_COMPLICATED_00.08.14.616-00.08.17.313@0.jpg<im_end>\\nKindly review the image and provide a thorough answer to \nmy question.Why is [337,5,452,928] smiling at [0,9,434,889]? Answer: Because [337,5,452,928] has spinach in her teeth .<eos>\nMerlin-Chat\nGiven a video clip including frame1:<img_start>MultiSports/basketball/v_4r8QL_wglzQ_c009/frame81.jpg<img_end>/n,frame2:<img_start>MultiSports/basketball/v_4r8QL_wglzQ_c009/\nframe84.jpg<img_end>/n and frame3:<img_start>MultiSports/basketball/v_4r8QL_wglzQ_c009/frame87.jpg<img_end>/n,what observations can you make from the video clip?\nIn this video clip, a captivating basketball game is visible, where athletes are fully engrossed in a highly intense contest. This competition, marked by skillful plays and fervent energy, is \nactively unfolding right before our eyes.\nCould you tell me what this player<Id1>Frame1:[401, 506, 479, 662]</Id1>, might be contemplating as their next move?Maintain consistency in your answer by using the object<Idi>Frame \nt:[xmin,ymin,xmax,ymax]</Idi> format for trajectories.\nGiven the trajectory and action of this player<Id1>Frame1:[401, 506, 479, 662];Frame2:[374, 495, 457, 669];Frame3:[353, 493, 414, 680];Frame4:[316, 484, 405, 675]</Id1>, it is evident he \nis readying to make a basketball pick-and-roll defensive next.\nWhich player is he planning to defense?\nThis player is planning to avoid the dribble of the player<Id2>Frame1:[265, 491, 316, 675];Frame2:[257, 480, 312, 659]</Id2>.\nFigure 6. Data format visualization. In our training, we use datasets that involve multiple tasks. To illustrate how these datasets are\nstructured, we\u2019ve chosen an example from each one. It\u2019s important to mention that all information about boxes has been adjusted to a\nstandard range of 1000. In the examples, questions are shown in black text, answers in blue, and negative samples in red.\nSimilar to constructing detection data, we also adopt out-\nput format prompts to guide the model to provide answers\nin the desired trajectory format.\nVisual Reasoning datasets.\nVisual reasoning dataset is\nconstructed to enhance the common sense reasoning abil-\nity of the model. In this work, we mainly collect the VCR\ndataset and adopt the same processing method as Shikra [9].\nReferring datasets. For referring dataset, we mainly col-\nlect from RefCOCO [32] and MeViS [16] to construct im-\nage referring and video referring datasets, respectively. For\nimage referring, we simply extract the description and the\ncorresponding bounding box from each image. For video\nreferring, we primarily extract pairs of trajectories and tra-\njectory descriptions from MeViS.\nDialogue datasets.\nTo facilitate the model\u2019s ability to\nachieve long conversations and follow user instructions, we\nutilized the open-sourced LLaVA-665K instruction tuning\ndataset [41]. Additionally, we created a smaller instruction\ntuning dataset (30K entries) using our Trajectory Chain-of-\nThought (T-CoT) method within Foresight Instruction Tun-\ning. This dataset focuses on three specific contexts: Multi-\nSports [38], TITAN [46], and STAR [72]. MultiSports cat-\nalogs multi-person sports actions with spatial and temporal\ndetails. TITAN encompasses street scenes with varied la-\nbels like vehicle states, pedestrian age groups, and specific\npedestrian actions. STAR offers real-world video data with\nsymbolic descriptions and logic-based analysis.\nTo create the T-CoT conversation data, we leveraged\nGPT-4 to generate rich feature reasoning dialogues using\nthe trajectories and actions from these datasets. We guided\nGPT-4 to interpret video clips through these trajectories and\ntheir associated descriptions or actions, then instructed it to\n10\nConfiguration\nPre-training\nSupervised Fine-tuning\nViT init.\nOpenAI-CLIP-L/14\nMerlin FPT\nLLM init.\nVicuna-7B-v1.5\nMerlin FPT\nProjection init.\nrandom\nMerlin FPT\nImage resolution\n4482\n4482\nViT sequence length\n2048\n2048\nLLM sequence length\n2048\n2048\nOptimizer\nAdamW\nOptimizer hyperparameter\n\u03b22 = 0.95, eps = 1e\u22128\nPeak learning rate\n5e\u22125\nMinimum learning rate\n0\nViT learning rate decay\n0.9\n0\nViT Drop path rate\n0\nLearning rate schedule\ncosine decay\nWeight decay\n0.05\nGradient clip\n1.0\nTraining steps\n7k\n3k\nWarm-up steps\n70\n90\nGlobal batch size\n2048\n256\nGradient Acc.\n8\n1\nNumerical precision\nbfloat16\nOptimizer sharding\n\u2713\nActivation checkpointing\n\u2717\nModel parallelism\n\u2717\nPipeline parallelism\n\u2717\nTable 7. Training hyperparameters of Merlin. The hyperparam-\neter placed in the middle indicates that this hyperparameter is used\nin both stages.\nSetting\nMetrics\nPrecise Task description\nGOT10K Future Rea\n1\nw\n51.4\n64.4\n2\nw/o\n28.4\n62.4\nTable 8. Comparisions between Merlin with (w) and without\n(w/o) the precise task description. We minaly report the AO\nscore of GOT10K and the average score of Future Reasoning.\nformulate Q&A pairs. These questions were designed to\nderive answers from available information. This process\nresulted in 30K QA pairs, incorporating trajectory coordi-\nnates in both questions and answers. We plan to expand this\ndataset, which we refer to as Merlin-chat, in the future.\nC. Data Format Details\nTo assist readers in comprehending the specific nature of the\ndata we\u2019ve constructed, this section includes visualizations\nof the training data format. As illustrated in Figure 6, we\npresent an exhaustive list of all data formats used, encom-\npassing both pretraining and instruction tuning phases. No-\ntably, we have incorporated negative samples (highlighted\nin bold red) into our question-answer pairs. This addition\nis designed to teach the model the skill of responding with\na negative answer when appropriate, thereby bolstering its\nability to resist producing hallucinatory responses.\nD. Training Details\nIn this section, we report the detailed training hyperparam-\neter settings of Merlin in Table 7.\nE. More Discussions\nLimitations and Future Work.\nWhile Merlin demon-\nstrates exceptional foresight capabilities, it is not without\nits limitations in its current form. A significant constraint\nis the inability to support long-range video sequences (ex-\nceeding 8 frames), which hinders its capacity to model ex-\ntended motion information. This is primarily due to Mer-\nlin\u2019s reliance on an image encoder rather than a video en-\ncoder, resulting in an excessive number of vision tokens for\nthe LLM to process. Addressing this, the development of\na more efficient tokenizer for long-range videos emerges as\na crucial area for future research. Moreover, this paper in-\ntroduces a novel benchmark for future reasoning, building\nupon the existing MMBench framework. Currently, there\nis no comprehensive and accurate benchmark to evaluate\nfuture reasoning abilities thoroughly. Therefore, exploring\nthe creation of a more robust and comprehensive future rea-\nsoning benchmark represents another significant avenue for\nfuture investigation.\nF. More Experiments\nIn this section, we aim to provide more experimental results\nincluding ablation studies (shown in Table 8) and qualitative\nanalysis (shown in Figure 7, 8 and 9).\nEffect of Precise Task Description. In the method section\nof main paper, we emphasize the importance of precise task\ndescriptions, which can prevent conflicts between multiple\ntask learning and damage to general language abilities. The\nperformance corresponding to the models with and without\nthe precise task description is reported in Table 8. We can\nobserve that the model without the precise task description\nbehaves significantly poorer, especically the tracking per-\nformance (51.4% - 28.4%). The results further prove that\nprecise task description is an important option for multi-\ntask learning in existing MLLM.\nQualitative analysis. We further provide more conversa-\ntion results between use and Merlin as shown in Figure 7,\nFigure 8 and Figure 9. It can be seen that Merlin is not\nonly capable of successfully completing tasks that existing\nMLLMs can do (such as REC, REG, etc.), but also can un-\ndertake some brand new tasks (like tracking, video refer-\nring, etc.), while demonstrating powerful future reasoning\nabilities.\nThese results further showcase Merlin\u2019s strong\ninstruction-following ability and foresight minds.\n11\nGeneral Detection\nPlease detect all objects.Ensure your response adheres strictly to the format \ncategory:[xmin,ymin,xmax,ymax].\nPerson:[259, 601, 343, 1000];Giraffe:[460, 001, 836, 985],[672, 306, 822, 755], \n[845, 359, 1000, 812].\nOV-Detection\nDetect helmet,gloves and hockey stick.When submitting your answer, maintain \nthe category:[xmin,ymin,xmax,ymax] structure consistently.\nHelmet:[416, 022, 520, 225],[556, 066, 628, 233];gloves:[275, 335, 385, 547],[453, \n445, 530, 560],[595, 348, 658, 435];hockey stick:[443, 445, 965, 898],[000, 698, \n093, 745],[453, 258, 925, 445].\nREC\nA man sitting on the right bottom corner.Provide the bounding box coordinate \nof the region this sentence describes.\n[610, 400, 1000, 820]\nREG\nWhat can you see in this region [158, 623, 400, 999]? Refer to the supplied \nimage and box coordinates for a short response to the question.\nIn this region, I can see a bird flying in front of a castle.\nImage Relation\nWhat is the difference between this girl[180, 643, 266, 944] and this \ngirl[387, 662, 468, 946]? \nThe main difference between the two girls is their position in the image. The girl on the \nleft, [180, 643, 266, 944], is standing on the left side of the image, while the girl on the \nright, [387, 662, 468, 946], is standing on the right side. This difference in positioning \nmight be due to the girls' roles in the performance or the composition of the image.\nFigure 7. More conversation visualization using Merlin in image-level tasks. We showcase additional examples where Merlin adeptly\nhandles various image-level tasks including general detection, open vocabulary detection (OV-Detection), referring expression comprehen-\nsion (REC), referring expression generation (REG), and relation reasoning.\n12\nGeneral Tracking\nGiven frame1:<image>,frame2:<image>, frame3:<image>,track person<Id1>Frame1:[329, 238, 420, 633]</Id1> and person<Id2> \nFrame1:[491, 235, 570, 628]</Id2>.To respond correctly, utilize the specified class<Idi>Frame t:[xmin,ymin,xmax,ymax]</Idi> format.\nperson<Id1>Frame1:[329, 238, 420, 633];Frame2:[310, 212, 400, 645];Frame3:[271, 202, 378, 648]</Id1>, \nperson<Id2>Frame1:[491, 235, 570, 628];Frame2:[495, 247, 572, 637];Frame3:[481, 258, 553, 635]</Id2>.\nVideo REC\nGiven frame1:<image>,frame2:<image> and frame3:<image>,track the black cat in this video clip.Use the specified \nTracki<Idi>Frame t:[xmin,ymin,xmax,ymax]</Idi> format for all trajectories in your reply.\nThere is one black cat.Track1<Id1>frame:1:[098, 181, 600, 662];frame:2:[196, 000, 642, 800];frame:3:[068, \n115, 623, 786]</Id1>.\nVideo REG\nGiven frame1:<image>,frame2:<image> and frame3:<image>,what is the hulkbuster<Id1>Frame1:[000,001,682,999];Frame2:[500, \n275, 999, 999];Frame3:[167, 001, 698, 999]</Id1> doing in this video clip?\nThe Hulkbuster<Id1>Frame1:[000,001,682,999];Frame2:[500, 275, 999, 999];Frame3:[167, 001, 698, 999]</Id1> is a robotic suit \nthat is part of the Marvel Cinematic Universe. In the video clip, the Hulkbuster is engaged in a fight with another character, possibly \nthe Hulk or another opponent. The suit is designed to be a powerful and versatile weapon, capable of dealing significant damage to \nits enemies. The Hulkbuster's appearance and actions in the clip suggest that it is in the midst of a battle, using its advanced \ntechnology and strength to take on its opponent.\nFigure 8. More conversation visualization using Merlin in video-level tasks. We showcase additional examples where Merlin adeptly\nhandles various video-level tasks including general tracking, video referring expression comprehension (Video REC) and video referring\nexpression generation (Video REG).\n13\nFuture Reasoning\nAccording to frame1<image>, frame2<image> and frame3<image>, track person and human-used objects present in the video along \nwith a caption about what the person is doing. When detailing person and human-used objects\u2019 trajectories in your response, adhere \nto the class<Idi>Frame t:[xmin,ymin,xmax,ymax]</Idi> format.\nThere is one person<Id1>frame:1:[392, 001, 827, 998];frame:2:[392, 001, 827, 998];frame:3:[342, 001, 742, 998]</Id1> \nand one trash can<Id2>frame:1:[332, 608, 485, 983];frame:2:[332, 608, 485, 983];frame:3:[228, 163, 419, 536]</Id2>. \nThe person is holding a trash can and is standing in a room.\nWhat might this person do next immediately?\nThe man is standing in a room and appears to be emptying the trash can. The man might next put the trash can on the \nfloor or a designated trash receptacle.\nAccording to frame1<image>, frame2<image> and frame3<image>, Explain Cyclist<Id1>Frame1:[280, 436, 346, 721]</Id1>'s route \nand include a short caption. For the trajectories included in the answer, please use the format class<Idi>Frame t:[xmin,ymin,xmax, \nymax]</Idi>.\nThere is one Cyclist<Id1>Frame1:[280, 436, 346, 721];Frame2:[183, 425, 300, 771];Frame3:[071, 402, 235, 829]</Id1>. \nCyclist's route: From the left side of the road, the cyclist is riding a bicycle down the road.\nIf there is an ego car on the road, this picture is the one seen inside the ego car. How should the ego car have?\nIf there is an ego car on the road. The ego car should be prepared to stop or slow down if necessary, and be cautious of \nthe cyclist's movements.\nFigure 9. More future reasoning conversation visualization using Merlin.\n14\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Men-\nsch, Katherine Millican, Malcolm Reynolds, Roman Ring,\nEliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong,\nSina Samangooei, Marianne Monteiro, Jacob L. Menick,\nSebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sa-\nhand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira,\nOriol Vinyals, Andrew Zisserman, and Kar\u00b4en Simonyan.\nFlamingo: a visual language model for few-shot learning.\nIn NeurIPS, 2022. 3\n[2] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri,\nEmanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2\ntechnical report. arXiv preprint arXiv:2305.10403, 2023. 2\n[3] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf\nHanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton,\nSamir Gadre, Shiori Sagawa, et al. Openflamingo: An open-\nsource framework for training large autoregressive vision-\nlanguage models. arXiv preprint arXiv:2308.01390, 2023.\n6\n[4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-vl: A frontier large vision-language model with\nversatile abilities. arXiv preprint arXiv:2308.12966, 2023. 3,\n5, 7\n[5] Christopher Bates, Peter W Battaglia, Ilker Yildirim, and\nJoshua B Tenenbaum. Humans predict liquid dynamics us-\ning probabilistic simulation. In CogSci, 2015. 2\n[6] Luca Bertinetto, Jack Valmadre, Joao F Henriques, Andrea\nVedaldi, and Philip HS Torr. Fully-convolutional siamese\nnetworks for object tracking.\nIn ECCV, pages 850\u2013865,\n2016. 6\n[7] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao\nZhang, Christoph Feichtenhofer, and Judy Hoffman.\nTo-\nken merging: Your vit but faster. In ICLR. OpenReview.net,\n2023. 3\n[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. Advances in neural in-\nformation processing systems, 33:1877\u20131901, 2020. 3, 4\n[9] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,\nFeng Zhu, and Rui Zhao.\nShikra:\nUnleashing multi-\nmodal llm\u2019s referential dialogue magic.\narXiv preprint\narXiv:2306.15195, 2023. 3, 5, 6, 7, 10\n[10] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-\nhao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yong-\nhao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P.\nXing.\nVicuna: An open-source chatbot impressing gpt-\n4 with 90%* chatgpt quality.\nhttps://lmsys.org/\nblog/2023-03-30-vicuna/, 2023. 3\n[11] Radostin Cholakov and Todor Kolev. Transformers predict-\ning the future. applying attention in next-frame and time se-\nries forecasting. arXiv preprint arXiv:2108.08224, 2021. 2\n[12] Yutao Cui, Chenkai Zeng, Xiaoyu Zhao, Yichun Yang,\nGangshan Wu, and Limin Wang. Sportsmot: A large multi-\nobject tracking dataset in multiple sports scenes.\narXiv\npreprint arXiv:2304.05170, 2023. 9\n[13] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\nFung, and Steven Hoi.\nInstructblip:\nTowards general-\npurpose vision-language models with instruction tuning.\narXiv preprint arXiv:2305.06500, 2023. 6, 7\n[14] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and\nMichael Felsberg. Atom: Accurate tracking by overlap max-\nimization. In CVPR, pages 4660\u20134669, 2019. 6\n[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova.\nBert:\nPre-training of deep bidirectional\ntransformers for language understanding.\narXiv preprint\narXiv:1810.04805, 2018. 3\n[16] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, and\nChen Change Loy.\nMevis: A large-scale benchmark for\nvideo segmentation with motion expressions. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 2694\u20132703, 2023. 9, 10\n[17] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng\nGe, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou,\nHaoran Wei, et al. Dreamllm: Synergistic multimodal com-\nprehension and creation. arXiv preprint arXiv:2309.11499,\n2023. 3\n[18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In ICLR. OpenReview.net, 2021. 3\n[19] Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia\nYu, Hexin Bai, Yong Xu, Chunyuan Liao, and Haibin Ling.\nLasot: A high-quality benchmark for large-scale single ob-\nject tracking. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 5374\u20135383,\n2019. 2, 5, 6, 7, 9\n[20] Karl Friston. The free-energy principle: a unified brain the-\nory? Nature reviews neuroscience, 11(2):127\u2013138, 2010. 2\n[21] Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying\nShan.\nPlanting a seed of vision in large language model.\narXiv preprint arXiv:2307.08041, 2023. 3\n[22] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang,\nMiao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping\nLuo, and Kai Chen.\nMultimodal-gpt: A vision and lan-\nguage model for dialogue with humans.\narXiv preprint\narXiv:2305.04790, 2023. 7\n[23] Anirudh Goyal and Yoshua Bengio. Inductive biases for deep\nlearning of higher-level cognition. CoRR, abs/2011.15091,\n2020. 3\n[24] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi\nLin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham.\nVizwiz grand challenge: Answering visual questions from\nblind people.\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 3608\u20133617,\n2018. 3\n[25] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi\nLin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham.\n15\nVizwiz grand challenge: Answering visual questions from\nblind people.\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 3608\u20133617,\n2018. 7\n[26] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll\u00b4ar, and Ross Girshick. Masked autoencoders are scalable\nvision learners. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 16000\u2013\n16009, 2022. 2\n[27] Lianghua Huang, Xin Zhao, and Kaiqi Huang. Got-10k: A\nlarge high-diversity benchmark for generic object tracking in\nthe wild. IEEE transactions on pattern analysis and machine\nintelligence, 43(5):1562\u20131577, 2019. 2, 5, 6, 7, 8, 9\n[28] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,\nSaksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui,\nOwais Khan Mohammed, Qiang Liu, et al.\nLanguage is\nnot all you need: Aligning perception with language mod-\nels. arXiv preprint arXiv:2302.14045, 2023. 6\n[29] Drew A Hudson and Christopher D Manning. Gqa: A new\ndataset for real-world visual reasoning and compositional\nquestion answering. In CVPR, 2019. 3, 7\n[30] Sergey Ioffe and Christian Szegedy. Batch normalization:\nAccelerating deep network training by reducing internal co-\nvariate shift. In ICML, pages 448\u2013456. JMLR.org, 2015. 3\n[31] Kendrick N Kay, Thomas Naselaris, Ryan J Prenger, and\nJack L Gallant. Identifying natural images from human brain\nactivity. Nature, 452(7185):352\u2013355, 2008. 2\n[32] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and\nTamara Berg.\nReferitgame: Referring to objects in pho-\ntographs of natural scenes. In Proceedings of the 2014 con-\nference on empirical methods in natural language processing\n(EMNLP), pages 787\u2013798, 2014. 2, 5, 6, 10\n[33] Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Gen-\nerating images with multimodal language models.\narXiv\npreprint arXiv:2305.17216, 2023. 3\n[34] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-\njlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan\nPopov, Matteo Malloci, Alexander Kolesnikov, et al. The\nopen images dataset v4: Unified image classification, object\ndetection, and visual relationship detection at scale. Interna-\ntional Journal of Computer Vision, 128(7):1956\u20131981, 2020.\n6, 9\n[35] Hugo Laurenc\u00b8on, Lucile Saulnier, L\u00b4eo Tronchon, Stas Bek-\nman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Sid-\ndharth Karamcheti, Alexander M Rush, Douwe Kiela, et al.\nObelics: An open web-scale filtered dataset of interleaved\nimage-text documents.\nIn Thirty-seventh Conference on\nNeural Information Processing Systems Datasets and Bench-\nmarks Track, 2023. 7\n[36] Bo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing,\nand Junjie Yan. SiamRPN++: Evolution of siamese visual\ntracking with very deep networks. In CVPR, pages 4282\u2013\n4291, 2019. 6\n[37] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models.\narXiv\npreprint arXiv:2301.12597, 2023. 3, 7\n[38] Yixuan Li, Lei Chen, Runyu He, Zhenzhi Wang, Gang-\nshan Wu, and Limin Wang.\nMultisports: A multi-person\nvideo dataset of spatio-temporally localized sports actions.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 13536\u201313545, 2021. 6, 10\n[39] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin\nZhao, and Ji-Rong Wen.\nEvaluating object hallucina-\ntion in large vision-language models.\narXiv preprint\narXiv:2305.10355, 2023. 3, 7\n[40] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning.\narXiv\npreprint arXiv:2310.03744, 2023. 5\n[41] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning.\narXiv\npreprint arXiv:2310.03744, 2023. 3, 6, 7, 10\n[42] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning, 2023. 3, 5, 6, 7\n[43] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang\nZhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He,\nZiwei Liu, et al. Mmbench: Is your multi-modal model an\nall-around player? arXiv preprint arXiv:2307.06281, 2023.\n3, 6, 7, 8\n[44] Ilya Loshchilov and Frank Hutter.\nSgdr:\nStochas-\ntic gradient descent with warm restarts.\narXiv preprint\narXiv:1608.03983, 2016. 6\n[45] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. In ICLR, 2019. 6\n[46] Srikanth Malla, Behzad Dariush, and Chiho Choi.\nTi-\ntan: Future forecast using action priors. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 11186\u201311196, 2020. 6, 10\n[47] Anton Milan, Laura Leal-Taix\u00b4e, Ian Reid, Stefan Roth, and\nKonrad Schindler.\nMot16: A benchmark for multi-object\ntracking. arXiv preprint arXiv:1603.00831, 2016. 5, 6, 9\n[48] OpenAI.\nChatgpt.\nhttps://openai.com/blog/\nchatgpt/, 2023. 3, 4\n[49] OpenAI. Gpt-4 technical report, 2023. 2, 4, 5\n[50] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, John Schulman, Jacob\nHilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda\nAskell, Peter Welinder, Paul F. Christiano, Jan Leike, and\nRyan Lowe. Training language models to follow instructions\nwith human feedback. In NeurIPS, 2022. 3\n[51] Renjie Pi, Jiahui Gao, Shizhe Diao, Rui Pan, Hanze Dong,\nJipeng Zhang, Lewei Yao, Jianhua Han, Hang Xu, and Ling-\npeng Kong Tong Zhang. Detgpt: Detect what you need via\nreasoning. arXiv preprint arXiv:2305.14167, 2023. 2, 5\n[52] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, Ilya Sutskever, et al. Language models are unsu-\npervised multitask learners. OpenAI blog, 1(8):9, 2019. 3\n[53] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 3\n16\n[54] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J Liu. Exploring the limits of transfer learning with\na unified text-to-text transformer. The Journal of Machine\nLearning Research, 21(1):5485\u20135551, 2020. 3\n[55] Narender Ramnani and R Christopher Miall. A system in\nthe human brain for predicting the actions of others. Nature\nneuroscience, 7(1):85\u201390, 2004. 2\n[56] Michael S. Ryoo, A. J. Piergiovanni, Anurag Arnab, Mostafa\nDehghani, and Anelia Angelova. Tokenlearner: What can\n8 learned tokens do for images and videos?\nCoRR,\nabs/2106.11297, 2021. 3\n[57] Maarten Sap, Ronan Le Bras, Daniel Fried, and Yejin Choi.\nNeural theory-of-mind? on the limits of social intelligence\nin large lms. In EMNLP, pages 3762\u20133780. Association for\nComputational Linguistics, 2022. 5\n[58] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:\nOpen dataset of clip-filtered 400 million image-text pairs.\narXiv preprint arXiv:2111.02114, 2021. 6, 9\n[59] Shuai Shao, Zijian Zhao, Boxun Li, Tete Xiao, Gang Yu,\nXiangyu Zhang, and Jian Sun.\nCrowdhuman: A bench-\nmark for detecting human in a crowd.\narXiv preprint\narXiv:1805.00123, 2018. 9\n[60] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang\nYu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A\nlarge-scale, high-quality dataset for object detection. In Pro-\nceedings of the IEEE/CVF international conference on com-\nputer vision, pages 8430\u20138439, 2019. 2, 9\n[61] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang\nYu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A\nlarge-scale, high-quality dataset for object detection. In Pro-\nceedings of the IEEE/CVF international conference on com-\nputer vision, pages 8430\u20138439, 2019. 5, 6\n[62] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. In ICLR,\n2015. 3\n[63] Fatih Emre Simsek, Cevahir Cigla, and Koray Kayabol.\nSompt22: A surveillance oriented multi-pedestrian tracking\ndataset. In European Conference on Computer Vision, pages\n659\u2013675. Springer, 2022. 5, 6, 9\n[64] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang,\nXinlei Chen, Dhruv Batra, Devi Parikh, and Marcus\nRohrbach. Towards vqa models that can read. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 8317\u20138326, 2019. 6\n[65] Peize Sun, Jinkun Cao, Yi Jiang, Zehuan Yuan, Song Bai,\nKris Kitani, and Ping Luo. Dancetrack: Multi-object track-\ning in uniform appearance and diverse motion. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 20993\u201321002, 2022. 2, 5, 6, 9\n[66] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,\nXuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B.\nHashimoto.\nStanford alpaca:\nAn instruction-following\nllama model. https://github.com/tatsu-lab/\nstanford_alpaca, 2023. 3\n[67] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste\nRozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aure-\nlien Rodriguez, Armand Joulin, Edouard Grave, and Guil-\nlaume Lample. Llama: Open and efficient foundation lan-\nguage models. arXiv preprint arXiv:2302.13971, 2023. 3\n[68] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\nLlama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288, 2023. 3\n[69] Tomer D. Ullman. Large language models fail on trivial al-\nterations to theory-of-mind tasks. CoRR, abs/2302.08399,\n2023. 5\n[70] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Bar-\nret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten\nBosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tat-\nsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean,\nand William Fedus.\nEmergent abilities of large language\nmodels. Trans. Mach. Learn. Res., 2022, 2022. 4\n[71] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.\nChain-of-thought prompting elicits reasoning in large lan-\nguage models. Advances in Neural Information Processing\nSystems, 35:24824\u201324837, 2022. 2, 5\n[72] Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua B Tenenbaum,\nand Chuang Gan. Star: A benchmark for situated reasoning\nin real-world videos. In Thirty-fifth Conference on Neural\nInformation Processing Systems Datasets and Benchmarks\nTrack (Round 2), 2021. 6, 10\n[73] Yinda Xu et al.\nSiamfc++: Towards robust and accurate\nvisual tracking with target estimation guidelines. In AAAI,\npages 140\u2013148, 2020. 6\n[74] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind\nSrinivas. Videogpt: Video generation using vq-vae and trans-\nformers. arXiv preprint arXiv:2104.10157, 2021. 2\n[75] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,\nYiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,\nYaya Shi, et al.\nmplug-owl: Modularization empowers\nlarge language models with multimodality. arXiv preprint\narXiv:2304.14178, 2023. 3, 6, 7\n[76] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying\nChen, Fangchen Liu, Vashisht Madhavan, and Trevor Dar-\nrell. Bdd100k: A diverse driving dataset for heterogeneous\nmultitask learning. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n2636\u20132645, 2020. 9\n[77] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,\nKevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.\nMm-vet: Evaluating large multimodal models for integrated\ncapabilities. arXiv preprint arXiv:2308.02490, 2023. 3, 7, 8\n[78] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi.\nFrom recognition to cognition: Visual commonsense rea-\nsoning.\nIn Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 6720\u20136731,\n2019. 2, 5, 6\n[79] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu\nLai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao\n17\nXia, et al. Glm-130b: An open bilingual pre-trained model.\narXiv preprint arXiv:2210.02414, 2022. 3\n[80] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,\nMoya Chen, Shuohui Chen, Christopher Dewan, Mona Diab,\nXian Li, Xi Victoria Lin, et al. Opt: Open pre-trained trans-\nformer language models. arXiv preprint arXiv:2205.01068,\n2022. 3\n[81] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi\nShao, Wenwei Zhang, Kai Chen, and Ping Luo. Gpt4roi: In-\nstruction tuning large language model on region-of-interest.\narXiv preprint arXiv:2307.03601, 2023. 3\n[82] Liang Zhao, En Yu, Zheng Ge, Jinrong Yang, Haoran Wei,\nHongyu Zhou, Jianjian Sun, Yuang Peng, Runpei Dong,\nChunrui Han, et al.\nChatspot: Bootstrapping multimodal\nllms via precise referring instruction tuning. arXiv preprint\narXiv:2307.09474, 2023. 3, 5\n[83] Liang Zhao, En Yu, Zheng Ge, Jinrong Yang, Haoran Wei,\nHongyu Zhou, Jianjian Sun, Yuang Peng, Runpei Dong,\nChunrui Han, et al.\nChatspot: Bootstrapping multimodal\nllms via precise referring instruction tuning. arXiv preprint\narXiv:2307.09474, 2023. 3\n[84] Pei Zhou, Aman Madaan, Srividya Pranavi Potharaju, Aditya\nGupta, Kevin R. McKee, Ari Holtzman, Jay Pujara, Xiang\nRen, Swaroop Mishra, Aida Nematzadeh, Shyam Upadhyay,\nand Manaal Faruqui.\nHow far are large language models\nfrom agents with theory-of-mind? CoRR, abs/2310.03051,\n2023. 5\n[85] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\nhamed Elhoseiny.\nMinigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv\npreprint arXiv:2304.10592, 2023. 3, 6, 7\n18\n"
  },
  {
    "title": "SeaLLMs -- Large Language Models for Southeast Asia",
    "link": "https://arxiv.org/pdf/2312.00738.pdf",
    "upvote": "23",
    "text": "SeaLLMs - Large Language Models for Southeast Asia\nXuan-Phi Nguyen\u2217, Wenxuan Zhang\u2217, Xin Li\u2217, Mahani Aljunied\u2217, Qingyu Tan, Liying Cheng,\nGuanzheng Chen, Yue Deng, Sen Yang, Chaoqun Liu, Hang Zhang, Lidong Bing\u2020\nDAMO Academy, Alibaba Group\nHupan Lab, 310023, Hangzhou, China\nhttps://github.com/DAMO-NLP-SG/SeaLLMs\nAbstract\nDespite the remarkable achievements of large language models (LLMs) in various\ntasks, there remains a linguistic bias that favors high-resource languages, such as\nEnglish, often at the expense of low-resource and regional languages. To address\nthis imbalance, we introduce SeaLLMs, an innovative series of language models\nthat specifically focuses on Southeast Asian (SEA) languages. SeaLLMs are built\nupon the Llama-2 model and further advanced through continued pre-training with\nan extended vocabulary, specialized instruction and alignment tuning to better\ncapture the intricacies of regional languages. This allows them to respect and\nreflect local cultural norms, customs, stylistic preferences, and legal considerations.\nOur comprehensive evaluation demonstrates that SeaLLM-13b models exhibit\nsuperior performance across a wide spectrum of linguistic tasks and assistant-\nstyle instruction-following capabilities relative to comparable open-source models.\nMoreover, they outperform ChatGPT-3.5 in non-Latin languages, such as Thai,\nKhmer, Lao, and Burmese, by large margins while remaining lightweight and\ncost-effective to operate.\n1\nIntroduction\nThe advent of large-scale language models (LLMs) has radically transformed the field of natural\nlanguage processing, demonstrating remarkable abilities in text generation, comprehension, and\ndecision-making tasks [5, 21, 22, 32, 33, 31, 11, 36, 3]. While the proficiencies of these models\nare extraordinary, the majority of existing LLMs embody a linguistic hierarchy overwhelmingly\ndominated by English [1, 13, 37]. This dominance undermines the multilingual capability of such\nmodels, with particularly prejudicial outcomes for lower-resource and regional languages, where\ndata scarcity and tokenization challenges lead to disproportionately poor model performance. This\nlinguistic disparity not only impedes access to state-of-the-art AI technologies for non-English-\nspeaking populations but also risks cultural homogenization and the loss of linguistic diversity.\nWhile hyper-polyglot models exist [26, 18, 36], they may pay high cost at high-resource language\nperformance and lack multilingual instruction-following abilities.\nRecognizing the urgent need to democratize AI and empower linguistically diverse regions, we\nintroduce SeaLLMs, a suite of specialized language models optimized for Southeast Asian languages1.\nThese languages, while rich and diverse, often lack the extensive dataset support available for more\nwidely spoken languages, resulting in a stark performance gap in existing LLM applications.\n\u2217Equal contributions.\n\u2020Corresponding author: l.bing@alibaba-inc.com\n1English (Eng), Chinese (Zho), Indonesian (Ind), Vietnamese (Vie), Thai (Tha), Khmer (Khm), Lao, Malay\n(Msa), Burmese (Mya) and Tagalog (Tgl)\nTechnical Report.\narXiv:2312.00738v1  [cs.CL]  1 Dec 2023\nFigure 1: Sea-bench (\u00a74.2) scores as evaluated by GPT-4 [38] for different models.Each radar chart\ncompares scores as averaged across 5 categories (left) and 9 languages (right). Detailed breakdown\nby each category and language is given in Figure 7 in the Appendix.\nOur SeaLLMs-13B models eclipse the performance of most available open-source LLMs in a\ncomprehensive array of tasks including world knowledge assessments, language comprehension, and\ngenerative capabilities in SEA languages. Compared to the Llama-2-13B model, SeaLLMs does not\nonly preserve, but also demonstrate enhanced performance in tasks involving existing languages,\nsuch as English, that were part of the original model\u2019s training set.\nFurthermore, our SeaLLM-Chat models emerge as formidable contenders to the widely reputed\nChatGPT [21], rivaling it as an AI assistant across multiple interactive challenges, as corroborated\nby evaluations conducted with GPT-4 as a judge [38]. This is despite the fact that GPT-4 evaluator\nfavors ChatGPT more often than humans do [12]. Notably, as shown Figure 1, our SeaLLM-13B\nchat model outperforms ChatGPT-3.5 by large margins in less-represented languages such as Khmer,\nLao or Burmese.\nFigure 2 illustrates the four-stage training process of SeaLLMs. In the first stage, detailed in \u00a72.3,\nwe conduct continuous pre-training from the foundational Llama-2 model [33] with an extended\nvocabulary tailored for SEA languages. Next, we fine-tune the model in a novel hybrid paradigm\nwith a mixture of multilingual pre-training data and English-dominant instruction fine-tuning data\n(\u00a73.2). The following stage (\u00a73.3) subsequently fine-tunes the model on a balanced and custom-built\nmultilingual SFT dataset. Finally, we conduct self-preferencing alignment optimization using the\nSeaLLM model itself, without relying on human annotators or more powerful LLMs [22].\nLlama-2\nContinual\nPre-training\nPre-train & SFT\nhybrid\nSFT\nSelf-Preferencing\nOptimization\nFigure 2: Complete Training Process of SeaLLMs. It begins with continual pre-training Llama-2\nwith more data of regional languages. Then the models undergo specialized fine-tuning process with\nmultilingual SFT data, before finally being tuned with self-preferencing alignment.\n2\nPre-training\n2.1\nPre-training Data\nThe pre-training data comprises a heterogeneous assortment of documents sourced from several\npublicly accessible repositories [29, 25, 6, 9]. Specifically, during the creation of the pre-training\n2\nFigure 3: Language composition of Pre-training and SFT data for SeaLLMs. The pre-training data\nhas more balanced language ratios. However, the SFT data is dominantly English because of the\nabundance of open-source English data and the difficulty in acquiring SFT data in other languages.\ndata, we include web-based corpora such as Common Crawl2, journalistic content such as CC-News,\ntext corpora with expertly-curated knowledge such as Wikipedia [9], and some scholarly publications.\nAfter collecting the data, we employ the FastText language identifier [4] to retain the documents for\nthe major languages in Southeast Asia, namely Thai, Vietnamese, Indonesian, Chinese, Khmer, Lao,\nMalay, Burmese, and Tagalog, and discard the remaining ones.\nSubsequent stages of data refinement involve the deployment of a bespoke pipeline configured with\nmultiple modules dedicated to data cleansing and content filtration. These modules are meticulously\nengineered to filter content that is deemed harmful or otherwise inappropriate, thereby ensuring that\nthe integrity of the dataset is upheld.\nFigure 3 (left) illustrates the composition of the pre-training data for SeaLLMs. Even though the\namount of available open-source English unlabeled data is virtually unlimited in this scenario, we use\nonly the highest quality English data, sourced from a subset of RedPajama [6], in a more balanced\nblend with the data from the SEA languages. The rationale for this is that most of the English and\nWestern-centric knowledge has already been learnt during Llama-2 pre-training, and the English\ndata used for SeaLLMs are mainly to preserve such knowledge. This is vastly different from other\npolyglot models [26, 36] which were pre-trained from scratch with data distributions that are highly\nskewed towards English and high-resource languages.\nThe pre-training corpus is also heuristically categorized into high-quality and low-quality data\ngroups. High-quality data comes from sources that comprise clean, regular, and formal texts, such as\nWikipedia [9]. Meanwhile, the low-quality data group includes sources with less formal and dirtier\ncontent, such as web-based mC4 [25], and thus is an order of magnitude larger than the high-quality\ndata group. As we intend to train the model to lean more towards high-quality data distribution,\nwe adopt a novel pre-training data streaming process that makes use of this data categorization, as\nexplained later in \u00a72.3.\n2.2\nVocabulary Expansion\nTable 1 describes how expensive it is to process an under-represented non-Latin language. For\nexample, encoding a single sentence in Thai requires roughly 4.3 times more tokens than its English\nequivalent. The reason for this is that most English/Latin-based language models employ a byte\npair encoding (BPE) tokenizer [27] that often inefficiently segments texts from non-European and\nnon-Latin scripts into disproportionately lengthy byte sequences. These sequences inadequately\nrepresent the underlying semantic content, resulting in diminished model performance [20]. Such\ninefficiency hampers the model\u2019s ability to handle tasks that depend on processing extended contexts,\nsuch as reading comprehension, due to limitations imposed by maximum context lengths.\n2https://commoncrawl.org\n3\nTable 1: Averaged compression ratios between the tokenized length of texts of each language produced\nby different tokenizers versus the baseline tokenized length of same-meaning English equivalents\nproduced by ChatGPT tokenizer. For example, it costs 15.6x more tokens to encode the same English\nsentence in Khmer with ChatGPT tokenizer, while it only takes 2.67x more tokens with our tokenizer.\nLanguage\nChatGPT\u2019s ratio\nLlama\u2019s ratio\nSeaLLM\u2019s ratio\nPhase 1: Vie, Zho, Tha, Ind (+11K tokens)\nVie\n4.41\n3.46\n1.48\nZho\n2.80\n2.36\n1.40\nTha\n9.09\n5.10\n1.87\nInd\n2.00\n2.09\n1.36\nPhase-2: Khm, Lao, Msa, Mya, Tgl (+5K tokens)\nKhm\n15.56\n12.14\n2.67\nLao\n13.29\n13.50\n2.07\nMsa\n2.07\n2.16\n1.50\nMya\n17.11\n9.85\n1.93\nTgl\n2.28\n2.22\n1.91\nEng\n1.00 (baseline)\n1.19\n1.19\nTo that end, we propose a novel vocabulary expansion technique, as formally described in Algorithm 1\nin the Appendix. This technique involves recursively merging whole-word and sub-word token pieces\nof a new language from a highly multilingual target tokenizer, which in this case we use the NLLB\ntokenizer [7], to the existing Llama-2 tokenizer. This new set of retrieved tokens are then pruned to\nremove rarely appearing and low-quality tokens before being added to the final SeaLLM tokenizer.\nThis frequency-based pruning process ensures the new language is sufficiently and efficiently encoded\nwithout introducing tokens from other existing languages (e.g., English), which may disrupt the\nlearned knowledge during the Llama-2 pre-training stage. After this extension process, we obtained\n16,512 new tokens to represent Southeast Asian languages.\nTable 1 demonstrates the efficiency of the new vocabulary. The compression ratio for Thai text has\nmarkedly improved from 4.29 to 1.57, signifying a 2.7-fold increase in the length of Thai text that\ncan be encoded within the same context constraints. At the same time, the compression of English\ntext has experienced a negligible reduction of 0.3%, thus maintaining its tokenization effectiveness.\n2.3\nPre-training Process\nWe organize our pre-training dataset based on the language of the content and the quality of the data,\nas mentioned in \u00a72.1. For each language, a separate stream of data, which we denote as FLi, is\nestablished. When we need a document for pre-training, we select a language, Li, according to a\npre-defined probability, and then pull a document from the corresponding stream FLi. This selection\nis done at each step when the model\u2019s parameters are updated.\nWe aggregate documents in possibly different languages into a single training sequence. This helps\nthe model learn a more language-agnostic representation of texts and better cross-lingual knowledge\ntransfer, while also maximizing the training efficiency. This training sequence is designed to be\nas long as the maximum context length. If the last document of the sequence is truncated with a\nsufficiently long leftover, we move the cursor of the leftover to the next new line to transform it into\nan uncorrupted document and then roll it over to the next batch.\nOur pre-training approach consists of two distinct stages. In the initial stage, each language-specific\nstream FLi feeds the model with exclusively high-quality documents. This strategy helps guide the\ninitial learning process toward a preference for high-quality information. Once we have used up all\nthe high-quality data, the streams begin to supply a blend of both high- and low-quality documents.\nIn other words, streams for low-resource languages may exhaust the high-quality data and begin with\nthe high and low mixture while other high-resource streams are still processing high-quality data.\nDuring the second phase, we continue to pre-train the model for a few thousand more steps, but we\nrevert to using only high-quality documents. This is designed to readjust the model\u2019s learning focus\nback towards the high-quality data, improving the model\u2019s performance.\n4\n3\nSupervised Fine-tuning (SFT)\n3.1\nSupervised Fine-tuning Data\nOur supervised finetuning (SFT) data consists of many categories, including text understanding and\nprocessing, math and logical reasoning, user-centric instruction-following, and natural dialog data.\nAs most public and open-source SFT data are English-only [16, 15, 19, 14], various techniques were\nimplemented to enhance the multilingual aspect of the model. These include sourcing natural data\nfrom local websites in natural settings, selectively translating from English data, employing self-\ninstruction, and using advanced prompting techniques [35, 17, 20]. As those synthetically generated\ndata may remain incorrect or low-quality, native speakers were then engaged to further verify, filter,\nand edit such synthetic responses to finalize the SFT dataset. We find that engaging the annotators\nto verify and modify model-generated responses is more efficient than having them write responses\nfrom scratch.\nSafety-related data also played a crucial role in fine-tuning SeaLLMs. This included both universally\napplicable general safety practices and specific safety protocols and norms relevant to different\ncountries. We manually collected and prepared country-relevant safety data, which covered a broad\nrange of culturally and legally sensitive topics in each of these countries. This was necessary as such\ntopics are often overlooked or may even conflict with open-source English-centric safety data [8].\nThese measures ensured that SeaLLMs gained a comprehensive understanding of safety practices\nacross different regions, enabling it to provide reliable responses in a multitude of SEA languages.\nFigure 3 (right) displays the composition of supervised fine-tuning data for SeaLLMs. As can be\nseen, the significant majority of the data (96.7%) is English and open-source, while the sizes of the\nSEA languages are minuscule. As a result, a trivial implementation of supervised fine-tuning [23, 30]\nwould have overshadowed the multilingual dataset for SEA languages and caused the model only\nto be able to respond in English. To address this issue, we propose a pre-training and SFT hybrid\nfine-tuning stage as explained in the next section \u00a73.2.\n3.2\nPre-training and Supervised Fine-tuning Hybrid\nIt is common practice to conduct supervised fine-tuning on instruction and conversational datasets\nto enable a pre-trained model to obtain instruction-following abilities [23, 33, 11, 22]. For the\ndevelopment of SeaLLMs, however, we propose incorporating an additional step prior to complete\nfine-tuning, namely Pre-train & SFT Hybrid. In this step, the model is further trained on a combination\nof three types of data: the pre-training corpus, labeled data from traditional NLP tasks, and significant\nquantities of open-source instruction-following data. The rationale for this approach is that the\nopen-source English instruction data is available in dominantly larger quantities compared to our\nSEA-language instruction data. This abundance can overshadow the smaller SEA datasets during\nthe SFT process. Thus, we reserve the majority of English SFT data for this stage, while leaving a\nrelatively small and balanced amount of it to the next supervised fine-tuning process (\u00a73.3).\nDuring this intermediate, or hybrid, training stage, the model processes batches that mix these\ndifferent data types into singular sequences. In other words, sequences may consist of both general\npre-training content and instruction-following examples. This blending differs from the initial pre-\ntraining stage in that we avoid truncating unsupervised texts to preserve their integrity. Additionally,\nwe mask the source side of the instruction or supervised data to prevent the model from overfitting to\nthe training examples and to reduce the risk of it simply memorizing the input data instead of learning\nthe more generalized ability to follow instructions.\n3.3\nSupervised Fine-tuning Process\nSimilar to Llama-2 [33], we initially conduct supervised fine-tuning by compiling instructions from a\nvariety of sources explained in \u00a73.1, combining them at random into a single, consolidated sequence.\nThis technique is designed to optimize the efficiency of the fine-tuning process. To further enhance\nthe usefulness and safety, we systematically integrate extensive and carefully designed system\n5\nprompts3 into every instruction sample. Our initial research found that incorporating such detailed,\ncomprehensive and aligned system prompts is effective in establishing a safe and useful prior within\nthe model, which contributes to the overall improvement of the model\u2019s performance, as well as to\nincrease its adherence to safety protocols.\nTo enhance the multi-turn conversation capability, in the later stage of fine-tuning, we further\nartificially create multi-turn conversations by randomly joining several single-turn instructions\ntogether. Our preliminary experiments suggest that this synthetic tuning approach significantly\nimproves the models\u2019 conversational capability.\n3.4\nSelf-Preferencing Optimization\nAlignment from human feedback preference has been key to the success of many AI-assistant\nlanguage models [28, 33, 24, 23]. To save the cost of human preference annotation work, some has\nsought to use powerful LLMs like GPT-4 [22] to play as a preference data generator [34]. However,\nthat may not even be feasible for low-resource non-Latin languages because of the unfavorable\ntokenization of ChatGPT as explained in \u00a72.2. In other words, even short prompts would exceed their\ncontext-length and the API-call costs would explode by up to 17 times.\nTherefore, we use our own SeaLLM SFT models to generate preference data using a proprietary\nprompting strategy, which we later use to employ direct preference optimization [24] to significantly\nimprove the model abilities as an AI agent. As such, unlike other works [19, 34], our models are\nfree from relying on powerful close-sourced models like GPT-4 to improve the performance in\nlow-resource languages.\n4\nEvaluation\n4.1\nModel Variants\nWe trained multiple variants of SeaLLMs, as specified in the following.\n\u2022 SeaLLM-7b: A 7B-parameter model trained to support all 10 languages. It is more\nlightweight and fast.\n\u2022 SeaLLM-13b-5L: A 13B-parameter model trained in the first phase to support English\n(Eng), Vietnamese (Vie), Indonesian (Ind), Thai (Tha) and Chinese (Zho). This model has\nthe best capability in these languages, with a performance in Thai that rivals ChatGPT-3.5..\n\u2022 SeaLLM-13b-10L or SeaLLM-13b for short: A 13B-parameter model continually trained\nfrom SeaLLM-13b-5L to support the remaining 5 official languages: Khmer (Khm), Lao\n(Lao), Malay (Msa), Burmese (Mya) and Tagalog (Tgl). The pre-training data for these\nlanguages is order of magnitude smaller than the other corpora. This model outperforms\nChatGPT-3.5 in most non-Latin SEA languages (Khm, Lao, Mya and Tha) by large margins.\n4.2\nSea-bench Peer Comparison\nWhile there are popular benchmarks to evaluate LLMs as a helpful assistant, such as MT-bench [38],\nthey are only English-based and apparently not suitable to evaluate performances in low-resource\nlanguages. Due to such a lack of multilingual benchmarks for assistant-style models, we engaged\nnative linguists to build a multilingual test set with instructions that cover SEA languages, called\nSea-bench. The linguists sourced such data by translating open-source English test sets, collecting\nreal user questions from local forums and websites, collecting real math and reasoning questions\nfrom reputable sources, as well as writing test instructions themselves.\nOur Sea-Bench consists of diverse categories of instructions to evaluate the models, as follow:\n\u2022 Task-solving: This type of data comprises various text understanding and processing\ntasks that test the ability of the language model to perform certain NLP tasks such as\nsummarization, translation, etc.\n3e.g., \u201cYou are a helpful, respectful and honest assistant. Your answers should not include any harmful,\nunethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially\nunbiased and positive in nature. If you don\u2019t know the answer to a question, please don\u2019t share false information.\u201d\n6\n(a) By language, averaged over 5 task categories\n(b) By category, averaged over 9 languages\nFigure 4: Sea-bench peer comparison between SeaLLM-13b v.s. ChatGPT as rated by GPT-4 for\ndifferent models across 9 languages and 5 categories.\n\u2022 Math-reasoning: This includes math problems and logical reasoning tasks.\n\u2022 General-instruction data: This type of data consists of general user-centric instructions,\nwhich evaluate the model\u2019s ability in general knowledge and writing. Examples for this type\ncan be requests for recommendations, such as \"Suggest three popular books,\" or instructions\nthat require the model to generate creative outputs, like \"Write a short story about a dragon.\"\n\u2022 NaturalQA: This consists of queries posted by real users, often in local forums, about local\ncontexts or scenarios. The aim is to test the model\u2019s capacity to understand and respond\ncoherently to colloquial language, idioms, and locally contextual references.\n\u2022 Safety: This includes both general safety and local context-related safety instructions. The\ninstructions could involve testing the model\u2019s understanding of safe practices, its ability\nto advise on safety rules, or its capacity to respond to safety-related queries. While most\ngeneral safety questions are translated from open sources, other local context-related safety\ninstructions are written by linguists of each language. Safety data only covers Vietnamese,\nIndonesian, and Thai.\nAs inspired by MT-bench [38], we evaluate and compare SeaLLMs with well-known and state-of-\nthe-art models using GPT-4 as a judge in a score-based grading metrics and a peer comparison (or\npairwise comparison) manner.\nFigure 1 compares our SeaLLM-13b chat model with Qwen-14b [3], PolyLM-13b [36] and the\nwidely reputed ChatGPT-3.5 [21]. As shown in the figure, as aggregated by task category (left radar\nchart), our SeaLLM-13b model performs on-par or surpasses ChatGPT-3.5 across many linguistic and\nwriting tasks. This is despite the fact that GPT-4 evaluator favors ChatGPT more often than humans\ndo [12]. Our model do perform worse in terms of math reasoning, probably due to the lack of math\nor code text sources, such as StackExchange or GitHub, in the pre-training data of both SeaLLM and\noriginally Llama-2 [33].\nMeanwhile, the right radar chart of Figure 1 compares instruction-following capabilities of models in\nthe angle of different SEA languages. As shown, SeaLLM-13b outperforms ChatGPT-3.5 by large\nmargins in most non-Latin languages, such as Burmese (Mya), Lao, Khmer and Thai. In combination\nwith the fact that SeaLLM can encode these languages with up to 9 times fewer tokens, our models are\nnot only superior but also cheaper to operate in these languages than ChatGPT. This helps democratize\nthe benefits of large language models to under-represented and potentially developing communities.\nOn the other hand, Figure 4 evaluates Sea-bench as peer comparison between SeaLLM-13b and\nChatGPT-3.5. As it can be seen, despite potentially large model size difference, SeaLLM-13b is\nequal or better than ChatGPT for 40% of the times for Latin-based languages (Eng, Vie, Ind, Msa). In\ncontrast, for non-Latin languages, our SeaLLM-13b surpasses ChatGPT by up to 90%. The results for\npeer comparison also appear to be consistent with score-based grading results as shown in Figure 1.\n7\n4.3\nWorld Knowledge\nIn this section, we evaluate our models and reputable chat baselines [33, 36, 21] in terms of world\nknowledge. For knowledge across languages, we use the M3Exam benchmark [37], which consists\nof real questions from human exam papers with various degrees of difficulty, ranging from primary\nschool to high school examinations. We evaluate M3Exam with 3-shot native-instruction prompts\nacross English, Chinese, Vietnamese, Indonesian and Thai. We also evaluate our models with the\nwell-known English-centric MMLU benchmark [10].\nTable 2: Multilingual world knowledge accuracy evaluation across multiple languages and various\nmodels of different sizes.\nModel\nM3Exam\nMMLU\nEng\nZho\nVie\nInd\nTha\nEng\nChatGPT-3.5\n75.46\n60.20\n58.64\n49.27\n37.41\n70.00\nLlama-2-7b\n49.58\n37.58\n29.82\n28.93\n19.89\n45.62\nLlama-2-13b\n61.17\n43.29\n39.97\n35.50\n23.74\n53.50\nPolylm-13b\n32.23\n29.26\n29.01\n25.36\n18.08\n22.94\nSeaLLM-7b\n54.89\n39.30\n38.74\n32.95\n25.09\n47.16\nSeaLLM-13b-5L\n63.20\n45.13\n49.13\n40.04\n36.85\n55.23\nSeaLLM-13b-10L\n62.69\n44.50\n46.45\n39.28\n36.39\n52.68\nTable 2 details the evaluations of world knowledge across multiple languages and models of different\nsizes. Our SeaLLM-13b-5L model, which supports the 5 languages as specified in \u00a74.1, performs\nthe best across the 13B-parameter models. Meanwhile, the SeaLLM-13b-10L model shows slight\ndegradations as trade-offs for support for the remaining low-resource languages.\n4.4\nReading Comprehension\nWe also evaluate the reading comprehension capability of our SeaLLMs in different SEA languages.\nSpecifically, we utilize the evaluation sets from XQuAD [2] for English, Chinese, Vietnamese, and\nThai, and the evaluation set from IndoQA4 for Indonesian. We compare our SeaLLMs with vanilla\nLlama-2 of the same size under a 1-shot setting and list the results of ChatGPT-3.5 as well for\nreference. As shown in Table 3, SeaLLM models exhibit better performance for Vietnamese and\nespecially Thai. Meanwhile, ChatGPT\u2019s scores may not reflect its true ability for this task as it fails to\ncomply with the shot format and instruction and produces verbose words that decrease the F1 score.\nTable 3: Reading comprehension F1 scores of different models across multiple languages. XQuAD [2]\nis used for Eng, Zho, Vie, and Thai, while IndoQA is used for Ind.\nModel\nEng\nZho\nVie\nInd\nTha\nChatGPT-3.5*\n69.18\n69.66\n59.25\n52.82\n40.75\nLlama-2-7b\n80.28\n71.92\n64.23\n50.86\n26.18\nSeaLLM-7b\n65.79\n64.98\n66.43\n44.72\n44.43\nLlama-2-13b\n80.46\n70.54\n62.87\n63.05\n25.73\nSeaLLM-13b-10L\n74.10\n73.24\n73.83\n56.82\n59.50\n4.5\nMachine Translation\nTo benchmark the machine translation performance of our SeaLLMs, we retrieve the test sets\nfrom Flores-200 [7] as the evaluation data. In the experiments, we adopt two evaluation protocols:\n(1) translation between English and SEA languages and (2) translation between SEA languages\ndirectly. We report 4-shot chrF++ scores for both SeaLLMs and the compared methods in Figure 5\n4https://huggingface.co/datasets/jakartaresearch/indoqa\n8\nFigure 5: Translation chrF++ scores of various models for both SEA languages to English and English\nto SEA languages directions.\nFigure 6: Direct translation between SEA languages. Scores are indicated as the different between the\nrespective chrF++ score of SeaLLM-13b minus that of ChatGPT-3.5. Red colors suggests SeaLLM-\n13b is better, while blue colors indicates ChatGPT is better.\nand Figure 6. As can be seen, in terms of En\u2194X translation, our SeaLLM-13B consistently\nsurpasses the performance of LLaMA-2-13B with reasonable margins. Besides, SeaLLM-13B\nexhibits clear superiority over ChatGPT-3.5 in low-resource languages, such as Lao and Khmer,\nwhile maintaining comparable performance with ChatGPT-3.5 in most high-resource languages (e.g.,\nVietnamese and Indonesian). For direct translation between SEA languages, our SeaLLM-13B still\nachieves higher chrF++ scores than ChatGPT-3.5 in most cases, especially when the translation pairs\ninvolve low-resource languages. Overall, compared to ChatGPT-3.5, our SeaLLM-13B demonstrates\nremarkable capability in the translation from/to low-resource languages and suffers very few or even\nno performance drops in the remaining use cases. We believe our SeaLLMs will play a key role in\nfacilitating communication and cultural exchange across communities in Southeast Asia.\n9\n5\nConclusion\nIn conclusion, our research presents a substantial advance in the development of equitable and\nculturally aware AI with the creation of SeaLLMs, a specialized suite of language models attuned to\nthe linguistic and cultural landscapes of Southeast Asia. Through rigorous pre-training enhancements\nand culturally tailored fine-tuning processes, SeaLLMs have demonstrated exceptional proficiency\nin language understanding and generation tasks, challenging the performance of dominant players\nsuch as ChatGPT-3.5, particularly in SEA languages. The models\u2019 attunement to local norms and\nlegal stipulations\u2014validated by human evaluations\u2014establishes SeaLLMs as not only a technical\nbreakthrough but a socially responsive innovation, poised to democratize access to high-quality AI\nlanguage tools across linguistically diverse regions. This work lays a foundation for further research\ninto language models that respect and uphold the rich tapestry of human languages and cultures,\nultimately driving the AI community towards a more inclusive future.\nReferences\n[1] Kabir Ahuja, Rishav Hada, Millicent Ochieng, Prachi Jain, Harshita Diddee, Samuel Maina,\nTanuja Ganu, Sameer Segal, Maxamed Axmed, Kalika Bali, and Sunayana Sitaram. MEGA:\nmultilingual evaluation of generative AI. CoRR, abs/2303.12528, 2023.\n[2] Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. On the cross-lingual transferability of\nmonolingual representations. arXiv preprint arXiv:1910.11856, 2019.\n[3] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge,\nYu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.\n[4] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors\nwith subword information. Transactions of the Association for Computational Linguistics, 5:\n135\u2013146, 2017. ISSN 2307-387X.\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\n[6] Together Computer. Redpajama: An open source recipe to reproduce llama training dataset,\n2023. URL https://github.com/togethercomputer/RedPajama-Data.\n[7] Marta R Costa-juss\u00e0, James Cross, Onur \u00c7elebi, Maha Elbayad, Kenneth Heafield, Kevin\nHeffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. No language left\nbehind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672, 2022.\n[8] Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, and Lidong Bing.\nMultilingual jailbreak\nchallenges in large language models. arXiv preprint arXiv:2310.06474, 2023.\n[9] Wikimedia Foundation. Wikimedia downloads. URL https://dumps.wikimedia.org.\n[10] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the\nInternational Conference on Learning Representations (ICLR), 2021.\n[11] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\nSaulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.\n[12] Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung Kim, and Dongyeop Kang.\nBenchmarking cognitive biases in large language models as evaluators. September 2023.\n[13] Viet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Veyseh, Hieu Man, Franck Dernoncourt,\nTrung Bui, and Thien Huu Nguyen. Chatgpt beyond english: Towards a comprehensive\nevaluation of large language models in multilingual learning. CoRR, abs/2304.05613, 2023.\n[14] Ariel N. Lee, Cole J. Hunter, and Nataniel Ruiz. Platypus: Quick, cheap, and powerful\nrefinement of llms. 2023.\n10\n[15] Wing Lian, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and \"Teknium\".\nOpenorca: An open dataset of gpt augmented flan reasoning traces. https://https:\n//huggingface.co/Open-Orca/OpenOrca, 2023.\n[16] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou,\nQuoc V. Le, Barret Zoph, Jason Wei, and Adam Roberts. The flan collection: Designing data\nand methods for effective instruction tuning, 2023.\n[17] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement\nwith self-feedback. arXiv preprint arXiv:2303.17651, 2023.\n[18] Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman,\nTeven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al.\nCrosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786,\n2022.\n[19] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and\nAhmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4, 2023.\n[20] Xuan-Phi Nguyen, Sharifah Mahani Aljunied, Shafiq Joty, and Lidong Bing. Democratizing\nllms for low-resource languages by leveraging their english dominant abilities with linguistically-\ndiverse prompts. arXiv preprint arXiv:2306.11372, 2023.\n[21] OpenAI. Chatgpt (june 2023 version, 2023.\n[22] OpenAI. Gpt-4 technical report. arXiv preprint, 2023.\n[23] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to\nfollow instructions with human feedback. Advances in Neural Information Processing Systems,\n35:27730\u201327744, 2022.\n[24] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and\nChelsea Finn. Direct preference optimization: Your language model is secretly a reward model.\narXiv preprint arXiv:2305.18290, 2023.\n[25] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. arXiv e-prints, 2019.\n[26] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow,\nRoman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. Bloom: A\n176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100,\n2022.\n[27] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare\nwords with subword units. In Proceedings of the 54th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pages 1715\u20131725. Association for\nComputational Linguistics, 2016. doi: 10.18653/v1/P16-1162. URL http://www.aclweb.\norg/anthology/P16-1162.\n[28] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec\nRadford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback.\nAdvances in Neural Information Processing Systems, 33:3008\u20133021, 2020.\n[29] Pedro Javier Ortiz Su\u00e1rez, Beno\u00eet Sagot, and Laurent Romary. Asynchronous pipeline for\nprocessing huge corpora on medium to low resource infrastructures. In 7th Workshop on the\nChallenges in the Management of Large Corpora (CMLC-7). Leibniz-Institut f\u00fcr Deutsche\nSprache, 2019.\n[30] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.\nhttps://github.com/tatsu-lab/stanford_alpaca, 2023.\n11\n[31] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-\nTze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for\ndialog applications. arXiv preprint arXiv:2201.08239, 2022.\n[32] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,\nTimoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien\nRodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient\nfoundation language models. arXiv preprint arXiv:2302.13971, 2023.\n[33] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open\nfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n[34] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes\nBelkada, Shengyi Huang, Leandro von Werra, Cl\u00e9mentine Fourrier, Nathan Habib, Nathan\nSarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf. Zephyr: Direct distillation\nof lm alignment, 2023.\n[35] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi,\nand Hannaneh Hajishirzi.\nSelf-instruct: Aligning language model with self generated\ninstructions. arXiv preprint arXiv:2212.10560, 2022.\n[36] Xiangpeng Wei, Haoran Wei, Huan Lin, Tianhao Li, Pei Zhang, Xingzhang Ren, Mei Li,\nYu Wan, Zhiwei Cao, Binbin Xie, et al. Polylm: An open source polyglot large language model.\narXiv preprint arXiv:2307.06018, 2023.\n[37] Wenxuan Zhang, Sharifah Mahani Aljunied, Chang Gao, Yew Ken Chia, and Lidong Bing.\nM3exam: A multilingual, multimodal, multilevel benchmark for examining large language\nmodels. CoRR, abs/2306.05179, 2023.\n[38] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and\nchatbot arena. arXiv preprint arXiv:2306.05685, 2023.\n12\nA\nVocabulary Expansion\nAlgorithm 1 explains in details how we perform selective and recursive merger of tokens from target\nNLLB vocabulary into the original Llama vocabulary to enrich the linguistic coverage for new and\nlow-resource languages. Specifically, given a small seed unlabeled dataset of a given new language,\nthe algorithm first tokenizes a document with the current Llama tokenizer. The resulting tokens\nare then exhaustively merged into longer tokens that are supported by the target NLLB vocabulary.\nDuring this merger process, any intermediate sub-word is also added to the Llama tokenizer as long\nas they exist in the rich NLLB vocabulary.\nThe new set of collected tokens are then pruned to remove rarely appearing and low-quality tokens\nbefore being added to the final SeaLLM tokenizer. This frequency-based pruning process ensures the\nnew language is sufficiently and efficiently encoded without introducing tokens from other existing\nlanguages (e.g., English), which may disrupt the learned knowledge during the Llama-2 pre-training\nstage.\nAlgorithm 1 Vocabulary Extension algorithm: Vi is Llama vocabulary, Vt is target NLLB vocabulary,\nD is unlabeled data and m is minimum frequency.\n1: function EXHAUSTIVEMERGE(Vi, Vt, tV )\n2:\nTnew \u2190 empty set \u2205\n3:\nrepeat\n4:\nfor each consecutive token pair (prev, next) in tV do\n5:\ntmerged \u2190 \u27e8prev\u27e9\u27e8next\u27e9\n\u25b7 Form a new token\n6:\nif tmerged exists in Vt then\n7:\nReplace (prev, next) with tmerged in tV\n\u25b7 Update tV with new token\n8:\nTnew \u2190 Tnew \u222a tmerged\n9:\nbreak\n10:\nuntil no new token added to Tnew\n11:\nreturn Tnew\n12: function VOCABEXTEND(Vi, Vt, D, m)\n13:\nV \u2190 Vi\n14:\nF \u2190 empty set \u2205\n15:\nT \u2190 empty set \u2205\n16:\nfor document d in D do\n17:\ntV \u2190 tokenize(V, d)\n\u25b7 tokenize the document\n18:\nTnew \u2190 EXHAUSTIVEMERGE(Vi, Vt, tV )\n\u25b7 obtain new words from Vt based on d\n19:\nV \u2190 V \u222a Tnew\n\u25b7 update V with new words Tnew\n20:\nT \u2190 T \u222a Tnew\n21:\nF \u2190 Update frequencies of Tnew to F\n\u25b7 update appearance frequencies of Tnew\n22:\nT \u2190 Prune ti \u2208 T with corresponding ft \u2208 F where ft < m\n\u25b7 Remove rare words\n23:\nVfinal \u2190 Vi \u222a T\n24:\nreturn Vfinal\nB\nSea-bench Evaluation Details\nFigure 7 breaks down the GPT-4 rated Sea-bench score-based evaluations of SeaLLM-13b and\nother baselines by both language and task category. As shown, our SeaLLM-13b model far exceeds\nChatGPT-3.5 in most non-Latin languages, such as Burmese (Mya), Lao and Khmer, though it trails\nbehind this formidable competitor in Latin-based languages, mostly in math reasoning skills.\n13\nFigure 7: Sea-bench scores as evaluated by GPT-4 for different models across 9 languages and 5\ncategories.\n14\n"
  },
  {
    "title": "VideoBooth: Diffusion-based Video Generation with Image Prompts",
    "link": "https://arxiv.org/pdf/2312.00777.pdf",
    "upvote": "19",
    "text": "VideoBooth: Diffusion-based Video Generation with Image Prompts\nYuming Jiang1\nTianxing Wu1\nShuai Yang1\nChenyang Si1\nDahua Lin2\nYu Qiao2\nChen Change Loy1\nZiwei Liu1B\n1S-Lab, Nanyang Technological University\n2Shanghai AI Laboratory\nhttps://vchitect.github.io/VideoBooth-project/\nImage Prompt\nPortrait of a dog, looks out the car window.\nImage Prompt\nCat is looking at a laptop.\nA horse eating grass.\nImage Prompt\nImage Prompt\nDog walking in the green farm 4k\nElephant walk in the yellow grass of \nsavannah\nElephant\ndrinking\nwater\nin\nmasai\nmara\nreserve, kenya\nClose up of cat on top of a vintage chair\nHorse grazes in snowy meadow\nFigure 1. Videos synthesized by image prompts. Our VideoBooth generates videos with the subjects specified in the image prompts.\nAbstract\nText-driven video generation witnesses rapid progress.\nHowever, merely using text prompts is not enough to depict\nthe desired subject appearance that accurately aligns with\nusers\u2019 intents, especially for customized content creation.\nIn this paper, we study the task of video generation with im-\nage prompts, which provide more accurate and direct con-\ntent control beyond the text prompts. Specifically, we pro-\npose a feed-forward framework VideoBooth, with two ded-\nicated designs: 1) We propose to embed image prompts in a\ncoarse-to-fine manner. Coarse visual embeddings from im-\nage encoder provide high-level encodings of image prompts,\nwhile fine visual embeddings from the proposed attention\ninjection module provide multi-scale and detailed encod-\ning of image prompts. These two complementary embed-\ndings can faithfully capture the desired appearance. 2) In\nthe attention injection module at fine level, multi-scale im-\nage prompts are fed into different cross-frame attention lay-\ners as additional keys and values. This extra spatial infor-\nmation refines the details in the first frame and then it is\npropagated to the remaining frames, which maintains tem-\nporal consistency. Extensive experiments demonstrate that\nVideoBooth achieves state-of-the-art performance in gener-\nating customized high-quality videos with subjects specified\nin image prompts. Notably, VideoBooth is a generalizable\nframework where a single model works for a wide range of\nimage prompts with feed-forward pass.\n1. Introduction\nText-to-image models [16, 17, 20, 26, 31, 37, 41, 57, 62\u2013\n64, 68, 70] have attracted substantial attention. With Stable\nDiffusion [64], we can now easily generate images using\n1\narXiv:2312.00777v1  [cs.CV]  1 Dec 2023\ntexts. Recently, the focus has been shifted to text-to-video\nmodels [6, 18, 30, 42, 44, 55, 71, 74, 78, 91] to generate\nvideos by taking text descriptions as inputs. However, in\nsome user cases, texts alone are not expressive enough to\ndefine the specific appearance of subjects [21, 66]. For ex-\nample, as shown in Fig. 2, if we want to generate a video\nclip containing the dog as the third row, we need to use\nseveral attributive adjuncts to define the appearance of the\ndog in text prompts. Even with these extensive attributive\nadjuncts, models still cannot generate the desired appear-\nance. Defining the appearance of the desired object with\ntexts alone has the following flaws: 1) It is hard to enumer-\nate all desired attributes, and 2) The model cannot capture\nall attributes accurately with a long text. Compared to using\ntexts, a more straightforward way to define the appearance\nis to provide reference images, termed image prompts. The\nimage prompts are complementary to text prompts and en-\nrich the details that are hard to be depicted by text prompts.\nThere are several attempts to introduce image prompts\ninto text-to-image models, which can be roughly divided\ninto two groups. One is to fine-tune parts of parameters us-\ning few-shot reference images [15, 21, 28, 47, 66], which\ncontain the same objects captured under different circum-\nstances. However, the requirement for the number of refer-\nence images is demanding as sometimes it is not practical\nto obtain multiple images of the same object. The other cat-\negory [11, 40, 48, 77, 80, 81], aiming to address this limita-\ntion, proposes to embed image prompts into text-to-image\nmodels and the inference is tuning-free. Both of these two\ntypes of attempts achieve plausible results in generating im-\nages containing objects specified in image prompts.\nIn this paper, we study a more challenging task, i.e.,\ntext-to-video generation with image prompts. The task has\ntwo main challenges: 1) Similar to text-to-image genera-\ntion, the attributes of image prompts should be accurately\ncaptured and then reflected in the generated videos; 2) Dif-\nferent from text-to-image generation, we aim for the dy-\nnamic movement of the object rather than a static one. Di-\nrectly adapting these methods to video domain results in\nmismatched appearance or unnatural degraded movements.\nTo address these challenges, we proposed VideoBooth with\nelaborately designed coarse-to-fine visual embedding com-\nponents: 1) Coarse visual embeddings via image encoder:\nAn image encoder is trained to inject the image prompts\ninto text embeddings; 2) Fine visual embeddings via at-\ntention injection: The image prompts are mapped to multi-\nscale latent representations to control the generation process\nthrough cross-frame attentions of text-to-video models.\nSpecifically, inspired by early attempts [40, 77] in text-\nto-image models with image prompts, we extract the CLIP\nimage features of the provided image prompts using the pre-\ntrained CLIP model [61]. Then the extracted features are\nmapped into the text embedding space, which are inserted\nA dog with a\nbrown and white\ncoat\nand\na\ndistinctive collie\nappearance\nA dog\n<Dog> eating snack inside big iron cage at home.\nsimple text\nlong text\nimage prompt\nFigure 2. The Use of Image Prompts. We generate three video\nclips using different types of prompts: simple text prompt, long\ntext prompt, and image prompt. We use the LLaVa model [51]\nto generate a text prompt describing the appearance of the image\nprompt. Using text prompts alone cannot fully capture the visual\ncharacteristics of the image prompt.\nto replace parts of the original text embeddings. The well-\ntrained encoder embeds the coarse appearance information\nof the given image prompts. However, coarse visual embed-\nding is a universal embedding: 1) It only contains high-level\nsemantic information, and 2) It is shared across all blocks\nwith the same scale. As a result, some visual details are\nmissing in the coarse visual embeddings.\nTo further refine the generated details as well as main-\ntain temporal consistency, different from the highly com-\npact coarse visual embeddings, multi-scale image prompts\nare injected into cross-frame attention modules in different\nlayers. The image prompts provide spatial information as\nwell as details with different granularities. On the one hand,\nkeeping spatial information of the image prompts can retain\nmore details. On the other hand, different cross-frame atten-\ntion modules need detailed information at different scales.\nSpecifically, the latent representations of image prompts are\nappended as additional keys and values to refine the details\nin the first generated frame. To propagate the refined first\nframe to the following frames to maintain temporal consis-\ntency, we then use the updated values of the first frame as\nvalues for the remaining frames.\nWe set up a dedicated VideoBooth dataset to support the\nstudy of the new task.\nWith each video, we provide an\nimage prompt and a text prompt. Extensive experiments\ndemonstrate the effectiveness of our proposed VideoBooth\nto generate videos with subjects specified in image prompts.\nAs shown in Fig. 1, videos generated by VideoBooth better\nkeep the visual attributes of image prompts. Besides, our\nproposed VideoBooth is tuning-free at inference time and\n2\nvideos can be generated with feed-forward pass only. The\ncontributions are summarised as follows:\n\u2022 To the best of our knowledge, we are the first to explore\nthe task of video generation using image prompts with-\nout finetuning at inference time. We propose a dedicated\ndataset to support the task. Our proposed VideoBooth\nframework can generate consistent videos containing the\nsubjects specified in image prompts.\n\u2022 We introduce a new coarse-to-fine visual embedding\nstrategy by image encoder and attention injection, which\nbetter captures the characteristics of the image prompts.\n\u2022 We propose a novel attention injection method, using the\nmulti-scale image prompts with spatial information to re-\nfine the generated details.\n2. Related Work\nText-to-Video Models take the text descriptions as inputs\nand generate clips of videos. Early explorations [34, 74]\non text-to-video models are based on the idea of VQVAE.\nMake-A-Video [71] proposes to add temporal attention to\nthe architecture of DALLE2 model [63].\nRecently, the\nemergence of diffusion models [32, 65] boosts research on\ntext-to-video models [1, 23, 30, 33, 55, 76, 91].\nVideo\nLDM [6] proposes to train the text-to-video models on\nStable Diffusion with temporal attention and 3D convolu-\ntion introduced to handle the temporal generation. Gen-\n1 [18] introduces depth maps to handle the temporal consis-\ntency of text-to-video models. Some methods [27, 89] re-\nsort to training separate modules for synthesizing motions.\nAll of the methods initialize their models with pre-trained\ntext-to-image models. Another paradigm of using text-to-\nimage models is to directly apply Stable Diffusion [64] to\nfew-shot or zero-shot settings. Tune-A-Video [78] adapts\nthe self-attention into cross-frame attention and then fine-\ntunes the stable diffusion model on a video clip.\nMod-\nels trained in this way have the capability to transfer mo-\ntions from original videos. Text2Video-Zero [44] proposes\nto generate videos by using correlated noise maps to im-\nprove consistency.\nApart from video generation, diffu-\nsion models have been applied to video-to-video genera-\ntions [8, 9, 19, 24, 36, 39, 50, 53, 59, 60, 69, 75, 83, 84, 88].\nCustomized Content Creation aims at generating images\nand videos using reference images [15]. For customized\ntext-to-image generation, optimization-based methods [28,\n35, 47, 49] are proposed to optimize the weights of the dif-\nfusion model. For example, Textual Inversion [21] opti-\nmizes the word embeddings, while DreamBooth [66] pro-\nposes to finetune the weights of Stable Diffusion as well.\nOptimization-based methods require several reference im-\nages with the same subject to avoid the overfitting of the\nmodel, which is demanding in real-world applications. The\ncost of finetuning hampers the practical usage of these\nmethods. To address these limitations, encoder-based meth-\nods [56, 81, 85, 92] are proposed to learn a mapping net-\nwork to embed the reference images.\nELITE [77] pro-\nposes to learn a global mapping network and local mapping\nnetwork to encode the images into word embeddings. Jia\net al. [40] propose to use an additional cross-attention to\nembed the image features. With the trained encoder, the\npersonalized generation can be achieved in a feed-forward\npass.\nSome recent works [2, 11, 22, 25] combine the\nencoder-based model and finetuning-based model to im-\nprove the performance. BLIP-Diffusion [48] proposes to\npretrain a multimodal encoder in a large-scale dataset and\nthen finetune the model on the specific subject for infer-\nence. Customized image generation is also applied to place\nthe objects into the user-specified scenes [4, 13, 46, 72, 87].\nAlso, some efforts [12, 14, 38, 67, 73, 79, 80, 86] have been\nmade to personalized face generation. He et al. [29] pro-\npose to improve the performance from the data perspective.\nSome works [3, 43, 54] focus on composing multiple sub-\njects in one image. Apart from works on image generation,\nthere are some early attempts at personalized video manip-\nulation. Make-A-Protagonist [90] edits an existing video in\na personalized way using Stable Diffusion 2.1 to embed the\nimage prompts. The motion of the original video is learned\nfrom Tune-A-Video [78]. VideoDreamer [10] proposes to\ngenerate personalized videos by generating the first frames\nusing a finetuning-based method and then generating the\nvideo clip using the Text2Video-Zero [44]. Different from\nexisting works, our proposed VideoBooth does not need to\nfinetune any weights at the inference time.\n3. VideoBooth\nOur proposed VideoBooth aims at generating videos from\nan image prompt I and a text prompt T. The image prompt\nspecifies the appearance of the subject. An overview of\nour proposed VideoBooth is illustrated in Fig. 3. The im-\nage prompt is fed into VideoBooth in two levels. At the\ncoarse level, it is fed into a pretrained CLIP Image encoder\nto extract visual features. An encoder, composed of sev-\neral MLP layers, is trained to map visual features into the\nspace of text embeddings. The obtained embedding fI will\nbe inserted into text embedding, which is extracted by feed-\ning text prompt T into CLIP text encoder. To further refine\nthe synthesized details, we propose to inject image prompt\nI into the cross-frame attention module in the pretrained\nvideo diffusion model. Specifically, we append latent rep-\nresentation xt\nI of image prompt I into the cross-frame atten-\ntion. In this way, multi-scale visual details with spatial in-\nformation are involved in the calculation of attention maps\nso that visual characteristics can be better preserved. Two\nways of feeding image prompt corporate with each other in\na coarse-to-fine manner. The encoder provides coarse vi-\nsual embeddings of the image prompt, while the attention\ninjection provides fine visual embeddings.\n3\ncelebrates birthday with \ngifts, balloons and soap bubbles stock footage video\nCLIP Image \nEncoder\nMLPs\nlearned embeddings\nCross Attention\nK \nProjection\nV \nProjection\nQ \nProjection\nCross-Frame Attention\nK \nProjection\nV \nProjection\nV img\nProjection\nQ \nProjection\nTemporal Attention\nBlocks in U-Net\nimage prompt\nK img\nProjection\n!\n!\n!\n!\n!\n!\n!\n\u2744\n\u2744\n\u2744\nconcated keys\nconcated values\n!!\n!!\"#\n$\ncoarse visual embeddings\n\ufb01ne visual embeddings\n%!\n!!\n\"\n!!\n\"\nFigure 3. Overview of VideoBooth. VideoBooth generates videos by taking image prompts I and text prompts T as inputs. The image\nprompt is fed into the CLIP image encoder, followed by MLP layers. The obtained coarse visual embedding fI is then inserted into the text\nembeddings. The composed embeddings serve as the input for cross attention. The embedding extracted by the encoder provides a coarse\nencoding of the visual appearance of the image prompt. To further refine the details in the generated videos, at the fine level, we append\nthe latent representation of the image prompt to the cross-frame attention as additional keys and values. Different cross-frame attention\nlayers receive latent representations with different scales. The multi-scale features with spatial details refine the synthesized details.\n3.1. Preliminary: Pretrained Text-to-Video Model\nOur proposed VideoBooth is developed based on the pre-\ntrained text-to-video model [30, 76]. In this section, we will\nbriefly introduce the framework of text-to-video model.\nInflated 2D Conv. To handle video data and capture the\ntemporal correlation, 2D conv in the Stable Diffusion model\nis inflated to 3D conv. In this way, the U-Net can encode 3D\nfeatures containing the temporal dimension.\nCross-Frame Attention Module.\nStable Diffusion has\na self-attention module, where the features are enhanced\nby attending to themselves. To improve temporal consis-\ntency, the self-attention is modified into cross-frame atten-\ntion. Specifically, in cross-frame attention, the feature of\neach frame is enhanced by attending and referencing to the\nfirst frame and the previous frame.\nThe cross-frame at-\ntention operates on both the spatial domain and temporal\ndomain, thus the temporal consistency of the synthesized\nframes is improved.\nTemporal Attention Module. Apart from cross-frame at-\ntention, a temporal attention module is introduced to further\nimprove temporal consistency. Temporal attention operates\non temporal domain and attends to all frames.\n3.2. Coarse Visual Embeddings via Image Encoder\nGiven an image prompt I and text prompt T, the generated\nvideo is supposed to be consistent with visual elements and\ntextual elements. Inspired by previous attempts at image-\nbased customization methods [40, 77], we propose to en-\ncode visual information of image prompts by an image en-\ncoder. The image prompt and text prompt complement each\nother. The image prompt provides visual characteristics of\nthe desired subject in the video, and the text prompt pro-\nvides other orthogonal information. The extracted visual\nembeddings are combined with text embeddings as the final\nembeddings for the cross-attention module. Specifically,\nthe CLIP image encoder is employed to extract the visual\nfeatures fV of image prompt I. Since the discrepancy exists\nbetween the CLIP image and text embeddings, fV is then\nfed into MLP layers F(\u00b7) to map fV to the spaces of text\nembeddings. The final embedding fI for the image prompt\nis obtained as follows:\nfV = CLIPI(I), fI = F(fV ).\n(1)\nAs for the text prompt T, we feed it into the CLIP text en-\ncoder to extract the text embedding fT :\nfT = [f 0\nT , f 1\nT , ..., f k\nT , ...],\n(2)\nwhere f k\nT is the k-th word embedding in the text prompt.\nTo make the diffusion model generate videos condition-\ning on both text prompts and image prompts, we need to\nintegrate these two embeddings, i.e., fI and fT . The idea is\nto replace the word embedding of the target subject with fI.\nMathematically, fI and ft are fused to obtain the final text\ncondition ct as follows:\ncT = [f 0\nT , f 1\nT , ..., f k\u22121\nT\n, fI, f k+n\nT\n, ...],\n(3)\n4\nQ0\nQi\nimage prompt\nframe #0\nKI\nK0\n[          ; \n]\nVI\n[          ; \n]\nV0\nK\nV\nV0new\nK0\nQ0\n[          ; \n]\nframe #(i-1)\nKi-1\n[          ; \n]\nVi-1\nQi\nVi\nFigure 4. Fine Visual Embedding Refinement. We propose to\ninject the latent representation of image prompt (here we use the\nimage for illustration purpose) directly into the cross-frame atten-\ntion module. We use the keys and values from the image prompt\nto update the values of the first frame firstly. Then, the updated\nvalues of the first frame are used to update the remaining frames.\nInjecting the image prompt in the cross-frame attention helps to\ntransfer the detailed visual characteristics of the image prompts to\nthe synthesized frames. We perform the refinement in different\ncross-attention layers with different scales.\nwhere k is the token index of the target subject in the text\nembedding, and n is the length of the text tokens for the\ntarget subject. For example, an image of a papillon dog\nis provided as an image prompt. To fuse the information\nfrom the image prompt and the text prompt \u201cPapillon dog\ncelebrates birthday with gifts\u201d, the word embeddings of the\n\u201cpapillon dog\u201d will be replaced with fI before they are fed\ninto the cross-attention module of the diffusion models.\nDuring the training of the coarse stage, we fix the param-\neters of the CLIP image encoder, and train the MLP layers.\nTo make the diffusion model accommodate with the com-\nposed text embeddings cT , we also finetune K and V pro-\njections (linear layers to map the input feature to the corre-\nsponding keys and values) in the cross-attention module.\n3.3. Fine Visual Embeddings via Attention Injection\nThe well-trained image encoder embeds the coarse visual\nembeddings for image prompts and thus the synthesized\nvideos contain the subjects specified in image prompts.\nHowever, the image encoder projects the image prompt into\na flattened high-level representation, resulting in the loss of\nits detailed visual cues. Thus, some detailed visual charac-\nteristics in the image prompts may not be well preserved.\nTo address this problem, a more effective way to preserve\nthese details is to provide the model with the image prompts\nwith spatial resolutions.\nTo further refine the synthesized details, we propose to\ninject image prompts into the cross-frame attention of the\ntext-to-video models. By injecting image prompts into the\ncross-frame attention, the image prompts are involved in the\nupdates of the synthesized frames so that the model can di-\nrectly borrow some visual cues from image prompts.\nSince text-to-video diffusion models operate in the la-\ntent space, we first feed the image prompt into the VAE of\nStable Diffusion and get its latent representation xI. More-\nover, since the sampling of the videos starts from the noise\nmap, the latent in the intermediate timesteps contains the\nnoises. If we append the clean latent xI of the image prompt\nto the cross-frame attention, the domain discrepancy exists.\nTherefore, we follow the diffusion forward process to add\ncorresponding noises to xI:\nxI\nt = \u221a\u03b1txI\n0 +\n\u221a\n1 \u2212 \u03b1t\u03f5,\n(4)\nwhere \u03b1 is a hyperparameter determined by the denoising\nschedule and \u03f5 \u223c N(0, I).\nThe cross-frame attention is used to improve the tempo-\nral consistency of the generated frames. For each frame,\nthe key and value are the concatenation of the features of\nthe first frame and the previous frame. Here, we introduce\nthe image prompts as the additional keys and values for the\nframes. As shown in Fig. 4, we propose to update the val-\nues of the first frame firstly using the keys and values of the\nimage prompts and the frame itself. Mathematically, the\noperation can be expressed as follows:\nV new\n0\n= softmax(KQT\n0\n\u221a\nd\n) \u00b7 V,\nK = [KI, K0], V = [VI, V0],\n(5)\nwhere KI and VI are the keys and values obtained from the\nimage prompts. The query, key, and value of the first frame\nare denoted Q0, K0, and V0, respectively. It should be noted\nthat we use a separately trained K and V projection for la-\ntent representations xI\nt of image prompts because the image\nprompts have clean backgrounds, which are different from\nother frames. The parameters of the newly added K and V\nprojections are initialized by original K and V projections.\nThen the updated first frame is used to refine the remain-\ning frames. When updating the remaining frames, the keys\nused for calculating the attention maps are the original keys,\nwhile the values are the updated ones. The update is ex-\npressed as follows:\nV new\ni\n= softmax(KQT\ni\n\u221a\nd\n) \u00b7 V,\nK = [K0, Ki\u22121], V = [V new\n0\n, Vi\u22121].\n(6)\nTo sum up, in the attention injection, we update the val-\nues of the first frame using the image prompts first, and then\nuse the updated first frame to update the other frames. In\nthis way, the visual cues from the image prompts can be\nconsistently propagated to all the frames.\nIt should be noted that the diffusion model has multiple\ncross-frame attention layers with different scales. To inject\n5\nmulti-scale visual cues for better detail refinement, in differ-\nent cross-frame attention layers, we feed latent representa-\ntions of the image prompts with corresponding resolutions,\nwhich are obtained from different stages of the U-Net.\n3.4. Coarse-to-Fine Training Strategy\nThe visual details of the image prompts are embedded into\nthe final synthesized results in two stages: coarse visual em-\nbeddings using an image encoder and fine visual embedding\nby attention injection. We propose to train these two mod-\nules in a coarse-to-fine manner. In other words, we train\nthe coarse image encoder and tune the parameters in the\ncross-attention first. After the model has the capability of\ngenerating videos containing the subjects specified in im-\nage prompts, we then train the attention injection module\nto embed image prompts into cross-frame attention layers.\nAs we will show in the ablation study (Sec. 5.5), if these\ntwo modules are trained together, the fine attention injection\nmodule leaks the strong visual cues and the coarse encoder\nlearns meaningless representations. As a result, in sampling\nphase, the image encoder for the coarse visual embedding\ncannot provide the coarse information and then the fine at-\ntention module cannot refine the details. Therefore, it is\nnecessary to train VideoBooth in a coarse-to-fine manner.\n4. VideoBooth Dataset\nWe establish the VideoBooth dataset to support the task of\nvideo generation using image prompts. We start from the\nWebVid dataset [5], a well-known open-source dataset for\ntext-to-video generation. In the WebVid dataset, there is a\ntext prompt with each video. In this paper, we study the\ntask of generating a video clip from one text prompt and\none image prompt. Hence, in addition to the original text\nprompt, we need to provide an image prompt for each video.\nWe propose to segment the subjects from the first frame\nof the video using the Grounded-SAM (Grounded Segment\nAnything) [45, 52], and the segmented subjects are image\nprompts. The Grounded-SAM receives word prompts as\ninputs and generates segmentation masks for the target sub-\njects specified in word prompts. To obtain the word prompt\nfor the input to Grounded-SAM, we use the spaCy library to\nparse the noun chunks from the original text prompts, which\nare used as the word prompts. After the segmentation, we\nperform data filtering to ensure the data quality. We fil-\nter out small objects and large objects (those are almost the\nsame size as the original video) according to the ratio of the\nobject to the whole video. Also, since we focus on gen-\nerating video clips containing moving objects, we further\nfilter the videos containing moving objects. The keywords\nwe used for filtering are dog, cat, bear, car, panda, tiger,\nhorse, elephant, and lion. In the current version, we have\nprocessed 2.5M subset of the WebVid dataset. After data\nfiltering, we have 48,724 video data pairs for training. We\nwill process the full set of the WebVid dataset and include\nthe filtered data in our VideoBooth dataset.\nTo evaluate the performance, we also set up a test bench-\nmark. The test benchmark consists of 650 test pairs. For\neach pair, an image prompt and a text prompt are provided.\nThe test pairs are selected from the rest of the WebVid-10M\ndataset, which does not overlap with the training set.\n5. Experiments\n5.1. Comparison Methods\nTextual Inversion [21] is a method for customized text-to-\nimage generation. The appearance of the target subjects is\nembedded into the text embeddings. Concretely, a text to-\nken S\u2217 is optimized to represent the subject. We adapt it to\nthe task of text-to-video generation by replacing the image\nmodel with the video model.\nDreamBooth [66] is also proposed for customized text-to-\nimage generation. It injects the target subject into the text\ntokens as well as model weights. During the training, both\nthe model weights and the special token S\u2217 are optimized.\nELITE [77] is an encoder-based method for personalized\ngeneration. An encoder is trained to embed the images into\nthe text embeddings. Local mapping and global mapping\nare employed to transform the CLIP embedding of image\nprompts into the features, which are injected into the cross-\nattention module. We adapt and retrain the method using\nthe same pretrained video model we use.\n5.2. Evaluation Metrics\nWe use three metrics to evaluate the performance [77]. To\nmeasure the alignment of the generated videos and given\ntext prompts, we use the CLIP-Text metric. The metric is\ncalculated using the cosine similarity of the CLIP text em-\nbeddings of text prompts and CLIP image embeddings of\nthe generated frames. For each video, the value is obtained\nby averaging values of the all frames. As for the evalua-\ntion of the similarity between the given image prompts and\nthe generated videos, we adopt two metrics: CLIP-Image\nand DINO [7, 58]. The CLIP-Image metric is calculated by\nthe cosine similarity between the CLIP image embedding\nof image prompts and generated frames. Since the CLIP\nmodel is trained to align image embeddings and text embed-\ndings, we follow the practice in previous methods [13, 66]\nand use the DINO similarity as another indicator. DINO\nis trained to differentiate the differences between objects of\nthe same classes. We use the ViT-S/16 model to extract the\nfeatures of the image prompts and generated frames. The\nfinal score is obtained by averaging over all frames.\n5.3. Quantitative Comparisons\nWe report quantitative results in Table 1. As shown in Ta-\nble 1, our proposed VideoBooth achieves state-of-the-art\n6\nELITE\nVideoBooth (Ours)\nTextual Inversion\nDreamBooth\ndog laying on ground\nclose-up of cat playing\nwith a yellow ball in the\nwhite interior\nELITE\nVideoBooth (Ours)\nTextual Inversion\nDreamBooth\nELITE\nVideoBooth (Ours)\nTextual Inversion\nDreamBooth\ncar in the bush\nFigure 5. Qualitative Comparison. VideoBooth effectively preserves the fidelity of image prompts and achieves better visual quality.\nTable 1. Quantitative Comparisons. VideoBooth achieves the\nbest image alignment and comparable text alignment performance.\nMethod\nCLIP-Text \u2191\nCLIP-Image \u2191\nDINO \u2191\nTextual Inversion [21]\n29.9749\n69.7995\n45.3143\nDreamBooth [66]\n30.6877\n71.2078\n52.9661\nELITE [77]\n30.0881\n73.7518\n58.9522\nVideoBooth (Ours)\n30.0967\n74.7971\n65.0979\nimage alignment performance compared to baseline meth-\nods. As for the alignment with the text prompts, our pro-\nposed VideoBooth has comparable performance with the\nbaseline models.\nIt should be noted that the CLIP-Text\nscore of DreamBooth is significantly higher than the other\nmethods. The reason lies in that the optimized token S\u2217\nin DreamBooth is inserted into the text embeddings, rather\nthan replacing the original word embeddings like other\nmethods.\nThis would result in the generated videos of\nDreamBooth being highly related to the text prompts but\nhaving no correlation to the image prompts in some cases.\nWe also conducted a user study, in which 25 users par-\nticipated.\nEach user is presented with twelve groups of\nvideos, and each group contains four videos generated by\nfour methods. For each group, users are asked to make three\nchoices: 1) which one has the best image alignment? 2)\nwhich one has the best text alignment? 3) which one has the\nbest overall quality. Figure 6 summarizes the results. Our\nresults are preferred by most users in all three dimensions.\n5.4. Qualitative Comparisons\nWe show three visual comparisons on the generated video\nframes of our proposed VideoBooth and baseline methods\n0\n0.2\n0.4\n0.6\n0.8\n1\nImage Alignment\nText Alignment\nOverall Quality\nTextual Inversion\nDreamBooth\nELITE\nOurs\n0\n20\n40\n60\n80\n100\nImage Alignment\nText Alignment\nOverall Quality\nTextual Inversion\nDreamBooth\nELITE\nOurs\nImage Alignment\nText Alignment\nOverall Quality\nTextual Inversion\nDreamBooth\nELITE\nOurs\n(%)\nFigure 6. User Study. Our proposed VideoBooth achieves the\nhighest user preference ratios on all three dimensions.\nin Fig. 5. In the first example, the model is supposed to gen-\nerate videos containing the dog specified in the image laying\non the ground. Textual Inversion cannot correctly embed\nthe appearance of the dog and results in generating another\ntotally different dog. DreamBooth and ELITE can embed\nthe coarse appearance of the dog but the generated details\nvary from the image prompt. Our proposed VideoBooth\nsuccessfully embeds the details of the image prompts into\nthe synthesized videos. In the second example, the condi-\ntion is to generate a cat playing with the yellow ball. All the\nmethods can generate a yellow ball and a cat but only our\nproposed VideoBooth can accurately generate the cat hav-\ning the appearance from the image prompt. As for the last\nexample, Textual Inversion fails to generate a car. Dream-\nBooth generates a distorted car in the bush. ELITE model\ncan generate a car in the bush, but the color differs from the\nimage prompt. By contrast, our model can generate videos\nhaving the same car in the bush.\n7\nDog as a chef at the table\n(a) Only Coarse Embeddings with Image Encoder\n(b) Only Fine Embeddings with Attention Injection\n(c) Unified Training for Image Encoder and Attention Injection\n(d) Full Model\nFigure 7. Ablation Study. (a) With only coarse embeddings from\nimage encoder, generated patterns in the body of the dog are dif-\nferent from the image prompt. (b) With only fine embeddings from\nattention injection, there lacks the coarse encodings of the dog for\nthe attention injection module to refine and thus the generated dog\nis distorted at later synthesized frames. (c) The unified training\ndegrades the capability of image encoder and thus the dog is also\ndistorted. (d) The full model better keeps the all visual details of\nthe image prompt.\n5.5. Ablation Study\nTo evaluate the effectiveness of the proposed components,\nwe perform three ablation studies. Due to the computational\nresources, we train these models on the subset of our train-\ning set. The quantitative metrics are shown in Table 2.\nOnly Coarse Embeddings. This ablation model injects the\nimage prompts with only coarse embeddings via Image En-\ncoder. In the example shown in Fig. 7(a), the ablation model\nonly encodes the coarse appearance of the image prompts.\nThe pattern in the legs of the synthesized dog is different\nfrom that in the image prompt. By contrast, the results of\nour full model show that our proposed model can transfer all\nthe details in the image prompts to the synthesized videos.\nOnly Fine Embeddings. In this ablation model, we only\nhave fine embeddings of the image prompt in cross-frame\nattention layers. The main purpose of using fine embed-\nding is to refine the coarse encoding of image prompts from\nthe Image Encoder. Without coarse embeddings, the model\nTable 2. Ablation Study. The full model has the best scores.\nVariants\nCLIP-Image \u2191\nDINO \u2191\n(a) Coarse Embeddings only\n75.4366\n64.9568\n(b) Fine Embeddings only\n75.5553\n66.0378\n(c) Unified Training\n75.8254\n67.4201\n(d) Full Model\n76.1631\n69.7374\nwith fine embeddings only cannot refine the details. As\nshown in Fig. 7(b), the first frame contains the exact ap-\npearance as the image prompt, but the temporal consistency\ncannot be guaranteed. The generated dog is distorted in the\nfollowing frames. The reason is that the model trained in\nthis way overfits the image prompt. In the first frame, the\nmodel can copy the information from image prompts. In the\nfollowing frames, without the coarse embeddings, the gen-\neration of the appearance only relies on the propagation of\nthe appearance from the first frame.\nThe Necessity of Coarse-to-Fine Training.\nIn Video-\nBooth, we propose the coarse-to-fine training strategy, i.e.,\ntrain the coarse embeddings first and then train the attention\ninjection module. In this ablation model, we train these two\nmodules within one stage. The unified training makes the\nmodel rely heavily on the strong guidance provided in at-\ntention injection. In this way, the trained image encoder has\nlimited capability. Thus, the model trained in this way has\na similar behavior as the model with only fine embeddings.\nThe model also overfits the image prompts from the atten-\ntion injection. As shown in the first example in Fig. 7(c), the\nappearance in the first frame is correct, but the generated\ndog in the following frames is distorted. Due to the im-\nage encoder having limited capability to provide the correct\ncoarse encoding of image prompts, the attention injection\ncannot refine the details. By contrast, the full model can\ngenerate consistent frames with all details well preserved.\n6. Discussion\nIn this paper, we propose a novel framework VideoBooth to\ngenerate videos using image prompts and text prompts. The\nimage prompts specify the appearance of the subjects. We\ninject the image prompts into the model in two modules:\nCoarse Embeddings via Image Encoder and Fine Embed-\ndings via Attention Injection. The Image Encoder provides\nthe coarse embeddings of image prompts for the refinement\nof the Attention Injection module. These two modules co-\noperate with each other and they are trained in a coarse-to-\nfine manner. Our proposed VideoBooth generates consis-\ntent videos containing the desired subjects.\nPotential Negative Societal Impacts. Our model can be\nused to synthesize videos. The model may be applied to\ngenerate fake videos, which can be potentially avoided by\nusing more advanced fake video detection methods.\n8\nReferences\n[1] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin\nHuang, Jiebo Luo, and Xi Yin. Latent-shift: Latent diffu-\nsion with temporal shift for efficient text-to-video genera-\ntion. arXiv preprint arXiv:2304.08477, 2023. 3\n[2] Moab Arar, Rinon Gal, Yuval Atzmon, Gal Chechik, Daniel\nCohen-Or, Ariel Shamir, and Amit H Bermano. Domain-\nagnostic tuning-encoder for fast personalization of text-to-\nimage models. arXiv preprint arXiv:2307.06925, 2023. 3\n[3] Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-\nOr, and Dani Lischinski.\nBreak-a-scene:\nExtracting\nmultiple concepts from a single image.\narXiv preprint\narXiv:2305.16311, 2023. 3\n[4] Jinbin Bai, Zhen Dong, Aosong Feng, Xiao Zhang, Tian\nYe, Kaicheng Zhou, and Mike Zheng Shou.\nIntegrat-\ning view conditions for image synthesis.\narXiv preprint\narXiv:2310.16002, 2023. 3\n[5] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisser-\nman. Frozen in time: A joint video and image encoder for\nend-to-end retrieval. In IEEE International Conference on\nComputer Vision, 2021. 6, 13\n[6] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n22563\u201322575, 2023. 2, 3\n[7] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. In Pro-\nceedings of the IEEE/CVF international conference on com-\nputer vision, pages 9650\u20139660, 2021. 6\n[8] Duygu Ceylan,\nChun-Hao Huang,\nand Niloy J. Mi-\ntra.\nPix2video:\nVideo editing using image diffusion.\narXiv:2303.12688, 2023. 3\n[9] Wenhao Chai, Xun Guo, Gaoang Wang, and Yan Lu. Stable-\nvideo: Text-driven consistency-aware diffusion video edit-\ning. arXiv preprint arXiv:2308.09592, 2023. 3\n[10] Hong Chen, Xin Wang, Guanning Zeng, Yipeng Zhang,\nYuwei Zhou, Feilin Han, and Wenwu Zhu.\nVideo-\ndreamer: Customized multi-subject text-to-video generation\nwith disen-mix finetuning. arXiv preprint arXiv:2311.00990,\n2023. 3\n[11] Hong Chen, Yipeng Zhang, Xin Wang, Xuguang Duan,\nYuwei Zhou, and Wenwu Zhu. Disenbooth: Disentangled\nparameter-efficient tuning for subject-driven text-to-image\ngeneration. arXiv preprint arXiv:2305.03374, 2023. 2, 3\n[12] Li Chen,\nMengyi Zhao,\nYiheng Liu,\nMingxu Ding,\nYangyang Song, Shizun Wang, Xu Wang, Hao Yang, Jing\nLiu, Kang Du, et al.\nPhotoverse:\nTuning-free image\ncustomization with text-to-image diffusion models.\narXiv\npreprint arXiv:2309.05793, 2023. 3\n[13] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao,\nand Hengshuang Zhao. Anydoor: Zero-shot object-level im-\nage customization. arXiv preprint arXiv:2307.09481, 2023.\n3, 6\n[14] Zhuowei Chen, Shancheng Fang, Wei Liu, Qian He, Mengqi\nHuang, Yongdong Zhang, and Zhendong Mao. Dreamiden-\ntity: Improved editability for efficient face-identity preserved\nimage generation. arXiv preprint arXiv:2307.00300, 2023. 3\n[15] Jooyoung Choi, Yunjey Choi, Yunji Kim, Junho Kim, and\nSungroh Yoon.\nCustom-edit:\nText-guided image edit-\ning with customized diffusion models.\narXiv preprint\narXiv:2305.15779, 2023. 2, 3\n[16] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng,\nChang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao,\nHongxia Yang, et al. Cogview: Mastering text-to-image gen-\neration via transformers. Advances in Neural Information\nProcessing Systems, 34:19822\u201319835, 2021. 1\n[17] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang.\nCogview2: Faster and better text-to-image generation via hi-\nerarchical transformers. arXiv preprint arXiv:2204.14217,\n2022. 1\n[18] Patrick Esser,\nJohnathan Chiu,\nParmida Atighehchian,\nJonathan Granskog, and Anastasis Germanidis.\nStructure\nand content-guided video synthesis with diffusion models.\narXiv preprint arXiv:2302.03011, 2023. 2, 3\n[19] Ruoyu Feng, Wenming Weng, Yanhui Wang, Yuhui Yuan,\nJianmin Bao, Chong Luo, Zhibo Chen, and Baining Guo.\nCcedit: Creative and controllable video editing via diffusion\nmodels. arXiv preprint arXiv:2309.16496, 2023. 3\n[20] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin,\nDevi Parikh, and Yaniv Taigman.\nMake-a-scene: Scene-\nbased text-to-image generation with human priors.\narXiv\npreprint arXiv:2203.13131, 2022. 1\n[21] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\nAmit H. Bermano, Gal Chechik, and Daniel Cohen-Or. An\nimage is worth one word: Personalizing text-to-image gen-\neration using textual inversion, 2022. 2, 3, 6, 7, 13\n[22] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano,\nGal Chechik, and Daniel Cohen-Or. Encoder-based domain\ntuning for fast personalization of text-to-image models. ACM\nTransactions on Graphics (TOG), 42(4):1\u201313, 2023. 3\n[23] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew\nTao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-\nYu Liu, and Yogesh Balaji. Preserve your own correlation:\nA noise prior for video diffusion models. In ICCV, 2023. 3\n[24] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel.\nTokenflow: Consistent diffusion features for consistent video\nediting. arXiv preprint arXiv:2307.10373, 2023. 3\n[25] Yuan Gong, Youxin Pang, Xiaodong Cun, Menghan Xia,\nHaoxin Chen, Longyue Wang, Yong Zhang, Xintao Wang,\nYing Shan, and Yujiu Yang.\nTalecrafter:\nInteractive\nstory visualization with multiple characters. arXiv preprint\narXiv:2305.18247, 2023. 3\n[26] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo\nZhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vec-\ntor quantized diffusion model for text-to-image synthesis. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 10696\u201310706, 2022. 1\n[27] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\npersonalized text-to-image diffusion models without specific\ntuning. arXiv preprint arXiv:2307.04725, 2023. 3\n9\n[28] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar,\nDimitris Metaxas, and Feng Yang.\nSvdiff: Compact pa-\nrameter space for diffusion fine-tuning.\narXiv preprint\narXiv:2303.11305, 2023. 2, 3\n[29] Xingzhe He, Zhiwen Cao, Nicholas Kolkin, Lantao Yu,\nHelge Rhodin, and Ratheesh Kalarot. A data perspective on\nenhanced identity preservation for diffusion personalization.\narXiv preprint arXiv:2311.04315, 2023. 3\n[30] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and\nQifeng Chen. Latent video diffusion models for high-fidelity\nvideo generation with arbitrary lengths.\narXiv preprint\narXiv:2211.13221, 2022. 2, 3, 4\n[31] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-\nage editing with cross attention control.\narXiv preprint\narXiv:2208.01626, 2022. 1\n[32] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in Neural Information\nProcessing Systems, 33:6840\u20136851, 2020. 3\n[33] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022. 3\n[34] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu,\nand Jie Tang.\nCogvideo:\nLarge-scale pretraining for\ntext-to-video generation via transformers.\narXiv preprint\narXiv:2205.15868, 2022. 3\n[35] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLoRA: Low-rank adaptation of large language models. In In-\nternational Conference on Learning Representations, 2022.\n3\n[36] Zhihao Hu and Dong Xu. Videocontrolnet: A motion-guided\nvideo-to-video translation framework by using diffusion\nmodel with controlnet.\narXiv preprint arXiv:2307.14073,\n2023. 3\n[37] Ziqi Huang, Tianxing Wu, Yuming Jiang, Kelvin CK Chan,\nand Ziwei Liu. Reversion: Diffusion-based relation inversion\nfrom images. arXiv preprint arXiv:2303.13495, 2023. 1\n[38] Junha Hyung, Jaeyo Shin, and Jaegul Choo. Magicapture:\nHigh-resolution multi-concept portrait customization. arXiv\npreprint arXiv:2309.06895, 2023. 3\n[39] Hyeonho Jeong and Jong Chul Ye. Ground-a-video: Zero-\nshot grounded video editing using text-to-image diffusion\nmodels. arXiv preprint arXiv:2310.01107, 2023. 3\n[40] Xuhui Jia, Yang Zhao, Kelvin CK Chan, Yandong Li, Han\nZhang, Boqing Gong, Tingbo Hou, Huisheng Wang, and\nYu-Chuan Su. Taming encoder for zero fine-tuning image\ncustomization with text-to-image diffusion models.\narXiv\npreprint arXiv:2304.02642, 2023. 2, 3, 4\n[41] Yuming Jiang, Shuai Yang, Haonan Qju, Wayne Wu,\nChen Change Loy, and Ziwei Liu. Text2human: Text-driven\ncontrollable human image generation. ACM Transactions on\nGraphics (TOG), 41(4):1\u201311, 2022. 1\n[42] Yuming Jiang, Shuai Yang, Tong Liang Koh, Wayne Wu,\nChen Change Loy, and Ziwei Liu. Text2performer: Text-\ndriven human video generation.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\n2023. 2\n[43] Chen Jin, Ryutaro Tanno, Amrutha Saseendran, Tom Diethe,\nand Philip Teare. An image is worth multiple words: Learn-\ning object level concepts using multi-concept prompt learn-\ning. arXiv preprint arXiv:2310.12274, 2023. 3\n[44] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nvosyan,\nRoberto\nHenschel,\nZhangyang\nWang,\nShant\nNavasardyan, and Humphrey Shi. Text2video-zero: Text-to-\nimage diffusion models are zero-shot video generators. arXiv\npreprint arXiv:2303.13439, 2023. 2, 3\n[45] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00b4ar, and\nRoss Girshick. Segment anything. arXiv:2304.02643, 2023.\n6\n[46] Sumith Kulal, Tim Brooks, Alex Aiken, Jiajun Wu, Jimei\nYang, Jingwan Lu, Alexei A Efros, and Krishna Kumar\nSingh. Putting people in their place: Affordance-aware hu-\nman insertion into scenes. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 17089\u201317099, 2023. 3\n[47] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli\nShechtman, and Jun-Yan Zhu. Multi-concept customization\nof text-to-image diffusion. 2023. 2, 3\n[48] Dongxu Li, Junnan Li, and Steven CH Hoi.\nBlip-\ndiffusion:\nPre-trained subject representation for control-\nlable text-to-image generation and editing. arXiv preprint\narXiv:2305.14720, 2023. 2, 3\n[49] Yuheng Li, Haotian Liu, Yangming Wen, and Yong Jae Lee.\nGenerate anything anywhere in any scene. arXiv preprint\narXiv:2306.17154, 2023. 3\n[50] Jun Hao Liew, Hanshu Yan, Jianfeng Zhang, Zhongcong Xu,\nand Jiashi Feng. Magicedit: High-fidelity and temporally\ncoherent video editing. In arXiv, 2023. 3\n[51] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. In NeurIPS, 2023. 2\n[52] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\nZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun\nZhu, et al. Grounding dino: Marrying dino with grounded\npre-training for open-set object detection.\narXiv preprint\narXiv:2303.05499, 2023. 6\n[53] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya\nJia. Video-p2p: Video editing with cross-attention control.\narXiv preprint arXiv:2303.04761, 2023. 3\n[54] Zhiheng Liu, Yifei Zhang, Yujun Shen, Kecheng Zheng, Kai\nZhu, Ruili Feng, Yu Liu, Deli Zhao, Jingren Zhou, and Yang\nCao. Cones 2: Customizable image synthesis with multiple\nsubjects. arXiv preprint arXiv:2305.19327, 2023. 3\n[55] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang,\nLiang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and\nTieniu Tan.\nVideofusion:\nDecomposed diffusion mod-\nels for high-quality video generation.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10209\u201310218, 2023. 2, 3\n[56] Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu.\nSubject-diffusion: Open domain personalized text-to-image\n10\ngeneration without test-time fine-tuning.\narXiv preprint\narXiv:2307.11410, 2023. 3\n[57] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch,\nand Daniel Cohen-Or. Null-text inversion for editing real\nimages using guided diffusion models.\narXiv preprint\narXiv:2211.09794, 2022. 1\n[58] Maxime Oquab, Timoth\u00b4ee Darcet, Th\u00b4eo Moutakanni, Huy\nVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.\nDinov2: Learning robust visual features without supervision.\narXiv preprint arXiv:2304.07193, 2023. 6\n[59] Hao Ouyang, Qiuyu Wang, Yuxi Xiao, Qingyan Bai, Jun-\ntao Zhang, Kecheng Zheng, Xiaowei Zhou, Qifeng Chen,\nand Yujun Shen.\nCodef: Content deformation fields for\ntemporally consistent video processing.\narXiv preprint\narXiv:2308.07926, 2023. 3\n[60] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,\nXintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fus-\ning attentions for zero-shot text-based video editing. arXiv\npreprint arXiv:2303.09535, 2023. 3\n[61] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In International Conference on Machine Learning,\npages 8748\u20138763. PMLR, 2021. 2\n[62] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In International Confer-\nence on Machine Learning, pages 8821\u20138831. PMLR, 2021.\n1\n[63] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022. 3\n[64] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models, 2021. 1, 3\n[65] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10684\u201310695, 2022. 3\n[66] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. 2022. 2, 3, 6, 7, 13\n[67] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei,\nTingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein,\nand Kfir Aberman. Hyperdreambooth: Hypernetworks for\nfast personalization of text-to-image models. arXiv preprint\narXiv:2307.06949, 2023. 3\n[68] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour,\nBurcu Karagol Ayan,\nS Sara Mahdavi,\nRapha Gontijo Lopes, et al.\nPhotorealistic text-to-image\ndiffusion models with deep language understanding. arXiv\npreprint arXiv:2205.11487, 2022. 1\n[69] Chaehun Shin, Heeseung Kim, Che Hyun Lee, Sang-gil Lee,\nand Sungroh Yoon. Edit-a-video: Single video editing with\nobject-aware consistency. arXiv preprint arXiv:2303.07945,\n2023. 3\n[70] Chenyang Si, Ziqi Huang, Yuming Jiang, and Ziwei Liu.\nFreeu:\nFree lunch in diffusion u-net.\narXiv preprint\narXiv:2309.11497, 2023. 1\n[71] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint arXiv:2209.14792,\n2022. 2, 3\n[72] Yizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian Price,\nJianming Zhang, Soo Ye Kim, and Daniel Aliaga. Object-\nstitch: Object compositing with diffusion model.\nIn Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 18310\u201318319, 2023. 3\n[73] Dani Valevski, Danny Wasserman, Yossi Matias, and Yaniv\nLeviathan. Face0: Instantaneously conditioning a text-to-\nimage model on a face. arXiv preprint arXiv:2306.06638,\n2023. 3\n[74] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kin-\ndermans, Hernan Moraldo, Han Zhang, Mohammad Taghi\nSaffar, Santiago Castro, Julius Kunze, and Dumitru Erhan.\nPhenaki: Variable length video generation from open domain\ntextual description. arXiv preprint arXiv:2210.02399, 2022.\n2, 3\n[75] Wen Wang, kangyang Xie, Zide Liu, Hao Chen, Yue Cao,\nXinlong Wang, and Chunhua Shen. Zero-shot video editing\nusing off-the-shelf image diffusion models. arXiv preprint\narXiv:2303.17599, 2023. 3\n[76] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou,\nZiqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo\nYu, Peiqing Yang, et al. Lavie: High-quality video gener-\nation with cascaded latent diffusion models. arXiv preprint\narXiv:2309.15103, 2023. 3, 4\n[77] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei\nZhang, and Wangmeng Zuo. Elite: Encoding visual con-\ncepts into textual embeddings for customized text-to-image\ngeneration. arXiv preprint arXiv:2302.13848, 2023. 2, 3, 4,\n6, 7, 13\n[78] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei,\nYuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and\nMike Zheng Shou. Tune-a-video: One-shot tuning of image\ndiffusion models for text-to-video generation. arXiv preprint\narXiv:2212.11565, 2022. 2, 3\n[79] Zijie Wu, Chaohui Yu, Zhen Zhu, Fan Wang, and Xiang Bai.\nSingleinsert: Inserting new concepts from a single image\ninto text-to-image models for flexible editing. arXiv preprint\narXiv:2310.08094, 2023. 3\n[80] Guangxuan Xiao, Tianwei Yin, William T Freeman, Fr\u00b4edo\nDurand, and Song Han. Fastcomposer: Tuning-free multi-\nsubject image generation with localized attention.\narXiv\npreprint arXiv:2305.10431, 2023. 2, 3\n[81] Xingqian Xu, Jiayi Guo, Zhangyang Wang, Gao Huang, Ir-\nfan Essa, and Humphrey Shi. Prompt-free diffusion: Taking\n\u201dtext\u201d out of text-to-image diffusion models. arXiv preprint\narXiv:2305.16223, 2023. 2, 3\n11\n[82] Tianfan Xue, Baian Chen, Jiajun Wu, Donglai Wei, and\nWilliam T Freeman. Video enhancement with task-oriented\nflow. International Journal of Computer Vision, 127:1106\u2013\n1125, 2019. 13, 14\n[83] Hanshu Yan, Jun Hao Liew, Long Mai, Shanchuan Lin,\nand Jiashi Feng. Magicprop: Diffusion-based video editing\nvia motion-aware appearance propagation.\narXiv preprint\narXiv:2309.00908, 2023. 3\n[84] Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change\nLoy. Rerender a video: Zero-shot text-guided video-to-video\ntranslation. arXiv preprint arXiv:2306.07954, 2023. 3\n[85] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-\nadapter: Text compatible image prompt adapter for text-to-\nimage diffusion models. arXiv preprint arXiv:2308.06721,\n2023. 3\n[86] Ge Yuan,\nXiaodong Cun,\nYong Zhang,\nMaomao Li,\nChenyang Qi, Xintao Wang, Ying Shan, and Huicheng\nZheng. Inserting anybody in diffusion models via celeb ba-\nsis. arXiv preprint arXiv:2306.00926, 2023. 3\n[87] Ziyang Yuan, Mingdeng Cao, Xintao Wang, Zhongang Qi,\nChun Yuan, and Ying Shan. Customnet: Zero-shot object\ncustomization with variable-viewpoints in text-to-image dif-\nfusion models. arXiv preprint arXiv:2310.19784, 2023. 3\n[88] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng\nZhang, Wangmeng Zuo, and Qi Tian.\nControlvideo:\nTraining-free controllable text-to-video generation.\narXiv\npreprint arXiv:2305.13077, 2023. 3\n[89] Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao\nZhang, Jiawei Liu, Weijia Wu, Jussi Keppo, and Mike Zheng\nShou.\nMotiondirector: Motion customization of text-to-\nvideo diffusion models. arXiv preprint arXiv:2310.08465,\n2023. 3\n[90] Yuyang Zhao, Enze Xie, Lanqing Hong, Zhenguo Li,\nand Gim Hee Lee.\nMake-a-protagonist:\nGeneric video\nediting with an ensemble of experts.\narXiv preprint\narXiv:2305.08850, 2023. 3\n[91] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models.\narXiv preprint\narXiv:2211.11018, 2022. 2, 3\n[92] Yufan Zhou, Ruiyi Zhang, Tong Sun, and Jinhui Xu. Enhanc-\ning detail preservation for customized text-to-image gen-\neration:\nA regularization-free approach.\narXiv preprint\narXiv:2305.13579, 2023. 3\n12\nSupplementary\nA. Comparison methods\nTexutal Inversion. In Textual Inversion [21], the appear-\nance of target subjects is embedded into the text embed-\ndings. A text token S\u2217 is optimized to represent one spe-\ncific subject. When applied to text-to-image models, multi-\nple images containing the same object are required to opti-\nmize the text token S\u2217. In the setting of video generation,\nwe directly use multiple video clips split from the original\nlong video to optimize the text token. Once optimized, the\ntext token S\u2217 is used to replace the word embeddings of the\ntarget subject in the sentence to sample new videos.\nDreamBooth. In DreamBooth [66], target subjects are in-\njected into text tokens and model weights simultaneously.\nDuring the training, both model weights and a special token\nS\u2217 are optimized. Similar to Textual Inversion, we use the\noriginal video and text description to train the model. Mul-\ntiple video clips sampled from the long video are employed\nto optimize weights and text token S\u2217. Once trained, the\ntext token S\u2217 is inserted before the word embeddings of the\ntarget object to sample new videos.\nELITE. Different from Textual Inversion and DreamBooth,\nELITE [77] is an encoder-based method for fast customized\ngeneration.\nAn encoder is trained to transform the im-\nages into embeddings. Local mapping and global mapping\nare employed to transform the CLIP embedding of image\nprompts into the features, which are fed into the cross-\nattention module. We adapt the ELITE to video generation.\nWe train the model using the same data and the same base\nvideo model.\nB. More Discussions on Ablation Study\nIn this section, we show one more visual example of the\nablation study. In Fig. 7 of the main paper, we show that\nthe model with only coarse embeddings results in impre-\ncise encoding of appearance. Both the model trained with\nonly fine embeddings and the model trained using the uni-\nfied training strategy overfit to image prompts. In the two\nexamples shown in Fig. 7 of the main paper, the first frames\ncan take the image prompt, but the generated appearance is\ndistorted along the frames. In Fig. A9, we discuss another\ncase, which exhibits a different behaviour.\nOnly Coarse Embeddings. This ablation model injects the\nimage prompts with only coarse embeddings via Image En-\ncoder. As shown in Fig. A9(a), the coarse embeddings pro-\nvide coarse but not precise guidance. The face of the dog\nand the shape of the head are not accurately captured. Our\nfull model shown in Fig. A9(d) can capture the visual de-\ntails correctly.\nOnly Fine Embeddings. In this ablation model, we only\nhave fine embeddings of image prompts in cross-frame at-\ntention layers. Recall that the purpose of fine embeddings\nis to refine the encoding from coarse levels. In the example\nshown in Fig. A9(b), the first frame does not successfully\nembed the image prompt. Without coarse embeddings, the\ngeneration of the appearance of the dog relies purely on the\npropagation from the first frame. The failure in encoding\nthe image prompt into the first frame results in the follow-\ning frames having random appearances for the dog.\nThe Necessity of Coarse-to-Fine Training.\nIn Video-\nBooth, we propose the coarse-to-fine training strategy, i.e.,\ntrain the coarse embeddings first and then train the attention\ninjection module. This ablation model is trained within one\nstage. In the example shown in Fig. A9(c), the first frame\nsuccessfully takes the appearance of the image prompt. The\nmodel generates a consistent appearance in all frames, but\nthe motion of this generated clip is small and not aligned\nwith the text prompt. We found that in the case of generat-\ning small motions or static frames, the coarse-to-fine train-\ning strategy can work well. However, when it comes to gen-\nerating large motions as shown in Fig. 7 of the main paper,\nthe appearance will be distorted along frames.\nC. WaterMark Removal Module\nSince the videos in WebVid dataset [5] have a watermark,\nthe model trained using this dataset generates videos with\na watermark in nature. To generate videos without water-\nmark for better visual quality, we finetune the model with\nan additional module using the Vimeo dataset [82].\nWe\nonly use text prompts and original videos to finetune the\nmodel. As shown in Fig. A8, we add six blocks before\nthe last conv out layer of the base video model. The added\nsix blocks can be regarded as a small UNet. After the first\nblock, we downsample features by two times. Then after\nthe second block, features are downsampled by two times.\nThen features are enhanced by two blocks. Finally, fea-\ntures are upscaled with two consecutive blocks. Each block\nupsamples features by two times. Inside each block, there\nare two ResNet blocks. Skip connections are adopted be-\ntween downsampling blocks and upsampling blocks. After\nall blocks, we feed the model to one conv layer, which is\ninitialized with zero. The motivation for zero initialization\nis to avoid the newly added blocks affecting the model. We\nadd the obtained features to the original features as residues.\nThe added features are fed into the final layer (i.e. Conv Out\nlayer) of the base video generation model. The newly added\nmodules and the last layer are optimized during the finetun-\ning. After finetuning, the watermark can be removed with-\nout influencing the generative capability of VideoBooth. It\nshould be noted that we use the model without watermark\nremoval module when comparing with baselines.\n13\n\u2026\nUNet\nWaterMark Removal Module\nCross-Frame Attention\nCross Attention\nTemporal Attention\nResidual Block\nConv Layer\nConv Out\n\u2744\n\ud83d\udd25\n\ud83d\udd25\n\u2744\n\u2744\n\u2744\nFigure A8. Illustration of Watermark Removal Module. We add a WaterMark Removal Module before the conv out layer. The output\nof the watermark removal module is added as a residue to the original features. We finetune the newly added module and the conv out layer\nusing the video data [82] without watermarks.\nDog is running\n(b) Only Fine Embeddings with Attention Injection\n(a) Only Coarse Embeddings with Image Encoder\n(c) Unified Training for Image Encoder and Attention Injection\n(d) Full Model\nFigure A9. More Visual Analysis on Ablation Study.\n14\n"
  },
  {
    "title": "Beyond ChatBots: ExploreLLM for Structured Thoughts and Personalized Model Responses",
    "link": "https://arxiv.org/pdf/2312.00763.pdf",
    "upvote": "18",
    "text": "Beyond ChatBots: ExploreLLM for Structured Thoughts and\nPersonalized Model Responses\nXIAO MA\u2217, Google, USA\nSWAROOP MISHRA\u2020, Google Deepmind, USA\nARIEL LIU, Google, USA\nSOPHIE SU, Google, USA\nJILIN CHEN, Google, USA\nCHINMAY KULKARNI, Emory University, USA\nHENG-TZE CHENG, Google Deepmind, USA\nQUOC LE, Google Deepmind, USA\nED CHI, Google Deepmind, USA\nFig. 1. ExploreLLM introduces a new interaction pattern with large language models (LLMs) by automatically\ndecomposing complex tasks into sub-tasks, and allowing users greater task control and personalization.\nLarge language model (LLM) powered chatbots are primarily text-based today, and impose a large interactional\ncognitive load, especially for exploratory or sensemaking tasks such as planning a trip or learning about a\nnew city. Because the interaction is textual, users have little scaffolding in the way of structure, informational\n\u201cscent\u201d, or ability to specify high-level preferences or goals. We introduce ExploreLLM that allows users to\nstructure thoughts, help explore different options, navigate through the choices and recommendations, and\nto more easily steer models to generate more personalized responses. We conduct a user study and show\nthat users find it helpful to use ExploreLLM for exploratory or planning tasks, because it provides a useful\n\u2217Author list is by descending order of contribution.\n\u2020Developed initial idea of prompt decomposition and tree-based design and interaction.\nAuthors\u2019 addresses: Xiao Ma, xmaa@google.com, Google, USA; Swaroop Mishra, swaroopmishra@google.com, Google\nDeepmind, USA; Ariel Liu, arielliu@google.com, Google, USA; Sophie Su, sophiesu@google.com, Google, USA; Jilin\nChen, jilinc@google.com, Google, USA; Chinmay Kulkarni, chinmay.kulkarni@emory.edu, Emory University, USA; Heng-\nTze Cheng, hengtze@google.com, Google Deepmind, USA; Quoc Le, qvl@google.com, Google Deepmind, USA; Ed Chi,\nedchi@google.com, Google Deepmind, USA.\narXiv:2312.00763v1  [cs.HC]  1 Dec 2023\n2\nMa et al.\nschema-like structure to the task, and guides users in planning. The study also suggests that users can more\neasily personalize responses with high-level preferences with ExploreLLM. Together, ExploreLLM points\nto a future where users interact with LLMs beyond the form of chatbots, and instead designed to support\ncomplex user tasks with a tighter integration between natural language and graphical user interfaces.\nCCS Concepts: \u2022 Human-centered computing \u2192 Graphical user interfaces; Natural language inter-\nfaces; Collaborative interaction; Empirical studies in interaction design.\nAdditional Key Words and Phrases: Chatbots, Artificial Intelligence, Large Language Models, Natural Language\nInterfaces, Task Decomposition, Graphical User Interfaces, Interaction, Schema, Prompting, Learning from\nInstruction.\n1\nINTRODUCTION\nLarge language model (LLMs) powered chatbots have dramatically improved the user adoption of\nAI systems but have limited interaction patterns that are linear ands text-heavy. Users can only\ncarry out a single-stream conversation with existing chatbots such as Google Bard or OpenAI\nChatGPT. These chatbots, for the large part, respond with text1. Such textual responses, which are\noften verbose, impose a significant cognitive load on users to understand and act upon, especially\nfor complex tasks. Often, users have to engage in multi-turn conversations, both to probe what\nthe chatbots can understand, and to communicate their intent and preferences. Similarly, because\nusers can only respond with text, conversational repair [1] is effortful.\nWhile there have been significant advances in prompt-based methods that unlock the reasoning\nand planning abilities of LLMs [29, 47, 50, 53, 56], the interaction pattern between users and LLM-\nbased assistants has largely remained the same. This remains the case despite increasing evidence\nthat users struggle to communicate complex tasks to assistants [4, 16], much like conversational\nassistants that preceded them [57]. Just as non-AI-experts use ad-hoc repair strategies to improve\nprompts for LLMs [54], non-expert users similarly use ad-hoc tactics like adding details to their\nrequest, pointing out assistant errors in how the request was interpreted, or simply giving up on\ntheir original task and deviating to a related, simpler task [16]. These tactics sometimes act as a\n\u201cband-aid\u201d, but still leave a majority of users unsatisfied [16]. More importantly, they prevent users\nfrom fully leveraging the potential of AI assistance to complete their tasks.\nIn this work, we introduce a new interaction pattern between users and LLM-powered assistants,\nby combining a prompt-based task decomposition method with a new schema-like graphical user\ninterface (UI). The new system, ExploreLLM, decomposes tasks into sub-tasks automatically using\na prompt-based decomposition method. Building on the LLM reasoning literature [47, 50, 53, 56],\nwe custom-designed prompts to generate related and easier-to-solve sub-tasks related to the\noriginal user query. Then, inspired by theories of schema in cognitive science [26] and distributed\nsensemaking [8] in human-computer interaction, we render the generated sub-tasks for the users\nin a structured and interactive UI. As a concrete example, for a complex task such as \u201cI want to\nplan a trip to Tokyo\", ExploreLLM will organize the query into sub-tasks such as deciding on\ndates and duration, making hotel and flight arrangements, checking travel documents, etc. Such\norganizational structures, or schema, allow people to not only learn what aspects are important in\nthe given task, but also act as a cue to express their own preferences. Further, we design a dedicated\nuser preference solicitation UI and a recommender-like user interaction within sub-tasks, aiming\nat improving personalization and the ability of users to easily steer model responses.\nWe conducted a user study with eight participants where we asked users to compare ChatGPT\nand ExploreLLM on a planning task. Our user study results show that ExploreLLM is helpful\nin providing structured task breakdown which helps users to think and structure their thoughts.\n1As of Nov 2023, the output of chatbots are becoming increasingly multimedia, but single-stream and text-heavy nonetheless.\nBeyond ChatBots: ExploreLLM for Structured Thoughts and Personalized Model Responses\n3\nUsers find the structured guidance that ExploreLLM provides useful for planning. Users mention\nthat ExploreLLM is easier to personalize with their own preferences, in contrast to text-based\nchatbots.\nThe rest of the manuscript is organized as follows. We first give an overview of the ExploreLLM\ndesign, providing details on the key components and implementation details. Through a qualitative\nuser evaluation, we describe how it helps users complete complex tasks. In later discussion, we\noutline future work on integrating tool use and further opportunities for automation. Finally, we\ndiscuss the limitations of ExploreLLM, some of which stem from foundational limitations of LLMs\n(such as hallucination), and others from the limited functionality that we developed. We plan to\nopen-source ExploreLLM following the publication of this manuscript.\n2\nBACKGROUND\nExploreLLM builds on recent work in LLM reasoning, theories from human cognition, and prior\nwork on natural language and graphical user interfaces in human-computer interaction (HCI).\n2.1\nPrompting elicits reasoning and planning in LLMs\nIn-context learning [2] and its evolution via various prompting methods have unlocked the rea-\nsoning and planning abilities of LLMs. Once instruction-tuned [30, 35, 49], LLMs can follow\nspecific instructions from end-users in natural language. Leveraging the instruction following\nabilities of LLMs, researchers show that carefully designed prompts can improve LLM performance\nacross a variety of reasoning and planning tasks, through intermediate representation in the form\nof thoughts [32, 50], decomposition [15, 37, 39, 56], search-decomposition mix [41, 53], struc-\nture [5, 9, 28], abstraction [55] and optimization [52]. Prompting based solutions have significantly\nimproved model capabilities across a diverse range of tasks such as program synthesis [19], dia-\nlogue [11], biomedical applications [36], style transfer [40], multilingual [43], moral reasoning [22]\nand a large spectrum of natural language understanding tasks [24, 48]. However, most of these\nadvancements are targeted at improving LLMs\u2019 performance on some benchmark tasks, rather than\nbenefiting end users (non-expert users in particular [54]). In this work, we explore the possibility\nof leveraging prompt-based methods to better support end users in their complex exploratory or\nplanning tasks.\n2.2\nSchemata support thinking and problem solving\nInterestingly, some methods for eliciting the reasoning ability in LLMs have roots in psychology and\ncognitive science \u2013 particularly the concept of schema. A schema is a framework, outline, or plan\nfor solving a problem [26]. Prior work shows that schema is an effective tool in supporting human\nproblem solving [23, 38]. One way to create schemata is through task decomposition. The intuition\nof breaking tasks down also aligns well with the distributed sensemaking work, which indicates\nthat in solving complex problems, it is useful for people to have a starting problem structure,\nthat they can later customize to their own goals [8]. Further, although originally developed by\ncognitive scientists to describe human learning [46], the concept of schema has inspired subsequent\nframeworks of machine intelligence, such as work by Minsky on frame and frame-systems [27].\nWhen faced with a new situation, humans select from memory a structure called a frame, a data-\nstructure representing a stereotyped situation. Collections of related frames are linked together\ninto frame-systems that can be manipulated while allowing information coordination [27]. In this\nwork, we leverage LLMs\u2019 reasoning ability to assist humans in creating schemata for problem\nsolving using prompt-based task decomposition automatically. In addition, we draw inspirations\nfrom frames and frame-systems when designing system interfaces. Users can interact with each\n4\nMa et al.\nsub-task as connected components via a UI for further customization and the ExploreLLM system\nkeeps track of user contexts and preferences for coordinated decision making.\n2.3\nWhy natural language alone is not enough\nNatural language user interfaces (NLUIs) and graphical user interfaces (GUIs) are two major ways\nfor humans to interact with machines. As early as 1972, Winograd developed SHRDLU, a natural-\nlanguage interface that manipulates blocks in a virtual \u201cblocks world\u201d [51]. The effectiveness of\nNLUIs are limited by the capability of underlying AI systems. The invention of GUIs in the 1970s\nwas largely a response to the lack of the natural language understanding and generation abilities\nof machines. GUIs played a major role in the wide adoption of personal computing [13]. With the\nrecent advancements in LLMs, NLUIs received renewed attention in the form of chatbots.\nAt the same time, there is compelling evidence that natural language interfaces alone are not\nenough: decades of work in cognitive science suggests that thinking is intimately tied to doing,\nnot just speaking. Put differently, thinking is a process that is not limited to what happens in our\nbrain, but instead it is \u201cdistributed\u201d throughout our environment [12]. For example, people find it\nmuch easier to move lettered tiles into various arrangements for playing Scrabble [25] (rather than\nmerely talk through alternatives), and professionals frequently draw rough diagrams to \u201chelp them\nthink\u201d [23].\nWhile natural language is flexible, it limits users mostly to single-stream and text-heavy inter-\naction patterns. GUIs have unique advantages that are more compatible with human cognition\nand sensemaking. Consistent with the schemata concept, graphical user interfaces are particularly\nhelpful in adding structure to a task, and allowing users to notice aspects of the task that matter the\nmost [10, 45]. For instance, a table comparing alternatives across the most important dimensions\nhelps programmers choose between competing technical approaches [21], and a \u201cmind map\u201d devel-\noped by other users can help users learn important aspects of a complex problem [8]. Over time, if\ninterfaces are well-structured and predictable, users may develop tool-specific expertise in using\nthem that extends beyond their conceptual task understanding [18]. For example, photo-editing\nexperts develop expertise with specific tools like Adobe Photoshop that goes beyond a conceptual\nunderstanding of editing photos. Unfortunately, current AI chatbot interfaces do not take advantage\nof GUIs, and instead generate responses to each query linearly. As a result, users struggle to develop\na strong mental model of such interactions, which are especially important for complex tasks.\nIn this work, we introduce ExploreLLM to automatically induce a structure that highlights\nsalient dimensions for exploratory tasks, by combining the best of natural language and graphical\nuser interfaces. Some recent work has started exploring the better design of graphical interfaces for\nLLMs [14, 44]. For example, Sensecape is an interactive system built with LLMs to support flexible\ninformation foraging and sensemaking [44]. Graphologue converts the text-only response from\nLLMs to interactive diagrams to facilitate information-seeking and question-answering tasks [14].\nExploreLLM builds on these initial explorations to create predictable, structured interfaces for\ncomplex exploratory tasks, focusing on task decomposition. Together, we demonstrate a promis-\ning direction of \u201chybrid\u201d user interfaces, where a tighter integration is drawn between natural\nlanguage and graphical user interfaces: language allows for expression of complex and abstract\ngoals and preferences, and graphical representations allow for more structured understanding and\nexploration [17, 18].\n3\nMETHODS\nIn this section, we first outline the key design components of the ExploreLLM system and imple-\nmentation details. Then we describe the setup of the user study.\nBeyond ChatBots: ExploreLLM for Structured Thoughts and Personalized Model Responses\n5\n3.1\nExploreLLM Overall Design\nWe first provide an overview of the system, and then delve into details and the design rationales\nof each component. In this work, we design a standalone prototype for simplicity. However, this\nmode of interaction can potentially be invoked from the current chatbot interfaces with magic\ncommands such as \u201c/explore\u201d.\n3.1.1\nSystem Overview.\n(1) Node: The ExploreLLM system has an underlying tree-like data structure. Unlike traditional\nchatbots, we create an abstraction of a node that can be nested. A node is a unit of interaction\nand can represent different forms of interactions (e.g., multi-turn natural language chats or\nUI interfaces). By default, a new node is created when the user starts interacting with the\nsystem. When needed, the system automatically creates children nodes for users to explore\nthrough task decomposition. All nodes form a tree-like hierarchical structure that holds all\nthe context of the user\u2019s exploration journey. For this work, we limit the nodes to depth = 2\nfor simplicity (root user query with one layer of children nodes). In the future this can be\nextended to more layers or graphs.\n(2) Personal Preferences: At any given node, the users can provide free-form context that\nthe system should be aware of for better personalization. The personal preference context\nis shared globally across all nodes.\n(3) Options: In each node, the system will take personal preferences into consideration and\ndispatch a backend call to present some options for the user to choose from. Users can\ninteract with different options via a checkbox UI to indicate their preferences.\n(4) Summarize: After sufficient exploration, users may want to tie everything back together\nand get a summary of their journey so far. Therefore, the system has a \u201csummarize\u201d function\nthat is available on each page. Users can click on the button to exit to the root node and get\na text summary of their entire interaction across the system.\n3.1.2\nNode. One of the most important design goals of the ExploreLLM system is to better support\ncomplex and under-specified tasks that require exploration. We address the challenge that complex\ntasks require high cognitive load by creating a tree-like abstraction. Through reasoning literature,\nespecially on decompositions [15, 37, 56], we know that LLMs are capable of decomposing a complex\nproblem into a list of easier subproblems. In this work, we leverage task decomposition abilities\nof LLMs for users\u2019 benefit, rather than as a method to improve LLM\u2019s task accuracy. Users can\ntype into a generic query box and the system will first create a root node for this query, and then\nautomatically calls the LLM task decomposition endpoint to create a list of easier sub-tasks.\nWe use the prompt in Figure 2 for automatic task decomposition. The original user query and\nany additional personalization cues are passed to the LLM through this prompt (Figure 2). The\noutput of the prompt is a list of sub-tasks that, together with the original task, forms a hierarchical\ntree-like structure. We display each sub-task to the users as \u201ccards\u201d that they can interact with\n(see Figure 3), mimicking a schema-like structure. When users hover over one of the card, a \u201csee\nmore\u201d button appears. Like a discussion thread in online forums, users now can focus on each task\nseparately while the system keeps track of the logical structure, thus reducing user\u2019s mental load.\n3.1.3\nPersonal Preferences. The second design goal of the system is to support better personaliza-\ntion. Complex tasks usually have important personal contexts or constraints that are important\nto the user, and are incredibly overwhelming to elicit in one go. LLMs are trained to be \u201cgeneral\npurpose\u201d, which dictates that the responses are often tailored to an \u201caverage user\u201d. This \u201cregression\ntoward the mean\u201d causes the LLMs response to be generic and not personalized. While it is possible\nfor LLMs to offer more personalized responses once the users clearly specifies their preferences,\n6\nMa et al.\nI\nwant to\naccomplish\nthe main\ngoal\nof: { t e x t }\nTo\nb e t t e r\na s s i s t me, p l e a s e\nbreak down the\nproblem\ni n t o\nsub -problems.\nEach sub -problem\nshould\nhelp me to\nsolve\nthe\no r i g i n a l\nproblem.\nMake\ni t\nso\nt h a t\neach sub -problem\ni s\nnot\nt r i v i a l\nand can be\nh e l p f u l .\nTake my context\nand\np e r s o n a l i z a t i o n\ncues\nto\np e r s o n a l i z e\nthe\nsub -problems.\nMake sure\neach sub -problem\ni s\nconcise\nand\nl e s s\nthan 15 words.\nP e r s o n a l i z a t i o n\nCue: { s e l e c t e d _ o p t i o n s }\nMy Context: {user_context}\nOutput\nformat (make sure\nonly\noutput a\nv a l i d\nJSON\no b j e c t\nt h a t\ncan be\nparsed\nwith\nj a v a s c r i p t\nf u nc t io n JSON.\nparse).\nDo not\ni n c l u d e\nany '``` or\njson '.\n{{\n\"sub_problems\": A l i s t\nof\ns t r i n g s (max 8), each\nas a\nv a l i d\nsub -query\n}}\nOutput:\nFig. 2. Prompt used in ExploreLLM to break a complex task down into structured sub-tasks.\nFig. 3. User starts interacting with the ExploreLLM system by typing a query. The system automatically\nbreaks down the original user query into sub-tasks using a custom prompt, and then create nodes that\nrepresent each sub-task. Users can see the sub-tasks rendered as \u201ccards\u201d, and interact with each one. A\ndedicated UI prompts the users to specify personal contexts and preferences for personalization. Once user is\nsatisfied with the exploration, they can click the summarize button at the bottom and ExploreLLM generates\na summary of the user journey.\nprior work in recommender systems show that users themselves often are unaware of their needs\nand often struggle to express them, especially in the beginning of a planning process [31].\nTo more prominently elicit user preferences, we include a dedicated UI that is always available\nfor users to update their preferences. We prompt the user to \u201ctell us about yourself and your\npreferences to get better recommendations\u201d. Importantly, the personalized context field is always\navailable regardless which node the user is exploring, so that they can update the context if the\ntask at hand reminds them of some preferences. The personalized context is passed in the prompt\nto the LLMs in all subsequent backend calls for better personalization.\nBeyond ChatBots: ExploreLLM for Structured Thoughts and Personalized Model Responses\n7\n3.1.4\nOptions. Another design solution for reducing mental load while increasing personalization\nis providing users options to choose from. Again, we draw inspirations from recommender systems.\nPrior work notes that the cognitive load for users to provide accurate preferences and ratings for\nitems is much greater than providing implicit feedback (e.g., selecting an option they prefer) [33].\nWe leverage this insight and construct a LLM options generation endpoint with the prompt in\nFigure 4.\nUser: { t e x t }\n== I n s t r u c t i o n s ==\nThe user\nwants\nto: {context}\nHere\ni s\none\nof\nthe\nsub -query\nto\nhelp\nanswer\nthe main query.\nGo i n t o\nd e t a i l s\nto\nhelp me with\nthe\nsub -query.\nShow me some options\nto\np e r s o n a l i z e\nand choose from.\nBe\nconcrete\nand make sure\nthe\noptions\nare\nv a l i d\nchoices\nto\nf i n i s h\nthe\ntask\nin\nsub -query.\nP e r s o n a l i z a t i o n\nCue: { s e l e c t e d _ o p t i o n s }\nMy Context: {user_context}\nWhen coming up with\noptions , make sure\nthey\nare\nd i v e r s e\nand\nr e p r e s e n t a t i v e\nof\nm u l t i p l e\ndemographics , c u l t u r e s\n, and view\np o i n t s .\nOutput\nformat (make sure\nonly\noutput a\nv a l i d\nJSON\no b j e c t ):\nDo not\ni n c l u d e\nany '``` or\njson '.\n{{\n\"recommended\": Your recommendation. ,\n\"options\": A l i s t\nof\noptions (at\nl e a s t 5) f o r me to\nchoose from. Each\noption\ni s\na\ns i n g l e\ns t r i n g . Provide\nh e l p f u l\nd e t a i l s . Don't\ni n c l u d e\nnumbers or\nb u l l e t\np o i n t s .\n}}\n== End of\nI n s t r u c t i o n ==\nUser: { t e x t }\nOutput:\nFig. 4. Prompt used in ExploreLLM to provide options for users to choose from.\nWe included the request for \u201cdiverse and representative\u201d inspirationally to make sure the options\ngenerated are inclusive [20]. However, due to the small sample size of our user studies, we did not\nformally evaluate whether such a prompt is effective in increasing the options diversity or swaying\nuser preferences. We discuss this in more detail in limitations and future work.\nOnce the users click on a node to \u201csee more\u201d, the system redirects them to a whole page screen\nto focus on the sub-task at hand. In the backend, the system dispatches the call to the LLM options\ngeneration endpoint. One the results come back, we display the generated options with a checkbox\nUI (see Figure 5). The checkbox UI is designed to make it as easy as possible for users to provide\nimplicit feedback through selection for better recommendations. The system keeps track of user\nselections, and any subsequent prompts to the backend will include all user selections for better\npersonalization. The design intends to resemble a more passive \u201cbrowsing\u201d experience to minimize\nthe mental load for users.\n3.1.5\nSummarize. After sufficient exploration in each sub-task, users may want to come back to\nthe main task and get a summary of their journey. Therefore, we include an UI for summarization.\nOnce the user clicks on \u201csummarize\u201d, the system gathers all user interaction signals and passes it\nto the LLM summarization endpoint with the prompt in Figure 6. The system redirects the user\nback to the root node and produces a textual summary of the main task.\n3.2\nImplementation\nWe implemented the above design with Next.js 13 for the front-end, and OpenAI GPT-4 API and\nFastAPI as backend. It is important to note that the interaction design is agnostic to the LLM API of\n8\nMa et al.\nFig. 5. ExploreLLM generates personalized options for users to choose from.\nUser: { t e x t }\nYour Response:\nHere\ni s\nsome information\nh e l p f u l\nto know about\nthe\nuser\nto\np e r s o n a l i z e\nresponse.\nP e r s o n a l i z a t i o n : { s e l e c t e d _ o p t i o n s }\nContext: {user_context}\nAnswer the\no r i g i n a l\nuser\nquery. When helpful , p e r s o n a l i z e\nthe\nresponse.\nFig. 6. Prompt used in ExploreLLM to generate personalized summary across different sub-tasks.\nchoice, as long as the LLM has sufficient instruction following and reasoning capabilities. User data\nis stored in a secured MongoDB database and only selected authors have access to the database. The\nservices are hosted on Google Cloud Run and Vercel. We plan to open-source the implementation\nof ExploreLLM following the publication of this manuscript.\n3.3\nUser Studies\nWe conducted a qualitative study to evaluate ExploreLLM. We used convenience sampling for\nrecruiting participants. There was no restriction on age, gender, and the language was limited to\nEnglish. We asked a few screening questions about prior experiences and attitudes towards AI\nchatbots, such as Bard by Google, ChatGPT by OpenAI and Claude by Anthropic.\nThen, participants were instructed to do a two-part unmoderated user study. We asked the\nparticipants to record themselves during the study, while presenting their computer screens and\nthinking aloud. Participants were informed of their rights and signed a standard consent form before\nthe study. We issued a small token of appreciation as incentive for the study without monetary\npayments. For two of the first studies (P1 and P6), one of the authors of the paper was on the same\ncall as an observer with the participant, and provided light clarification assistance when needed.\nWe made minor changes to the instructions to ensure subsequent unmoderated studies can proceed\nsmoothly. All studies were conducted during Nov 6 - 10, 2023.\nThe study was within-subject and counterbalanced. For each part of the study, participants were\ninstructed to do a travel planning task to a destination of their choice, one using ChatGPT, the other\nusing ExploreLLM. We always instruct the participants to plan the trip to the same destination\nBeyond ChatBots: ExploreLLM for Structured Thoughts and Personalized Model Responses\n9\nin the second task as the previous one. We provided a login with ChatGPT plus subscription\nand participants were instructed to use ChatGPT-42. We randomized the order of the system\nthe participants use. In the end, we had five participants using ChatGPT first (P1-5), and three\nparticipants using ExploreLLM first (P6-8).\nWe collected age and gender as demographic information in the exit survey. We transcribed the\nuser study videos and two authors of the paper conducted qualitative coding on the transcripts to\nidentify common themes, first independently, and then discussed and agreed on the final findings.\n3.3.1\nMain Task. We provided the following instructions for the main task. It is important to note\nthat our system design is not limited to travel planning. We chose to evaluate trip planning as an\nexample task because it is general, typically complex, and allows for individual user constraints\nand preferences.\nKey study instructions are as follows:\nImagine you are planning a trip to a destination of your choice using [system name].\n\u2022 You have less than 5 min to complete this task.\n\u2022 You can have as many interactions as you like using this system.\n\u2022 Please make sure this trip is personalized to you (e.g., cost, time, location).\n\u2022 Don\u2019t forget to think aloud as you try to complete the task!\nOnce you have completed the task, answer these following questions verbally:\n\u2022 Tell us in detail, what do you find most helpful and unhelpful from this result?\n\u2022 If at all, which part of the result do you find personalized to you?\n\u2022 If at all, how much does this system make you feel more or less confident about planning a\ntrip?\n\u2022 Is there anything that you would like to comment about this task?\n3.3.2\nParticipant Demographics. Eight participants took part in the study, out of which six identified\nas male, one female, and one preferred not to say. Seven out of eight participants are within the\n25-34 years old, with one participant 35-44 years old. Seven participants are located in the U.S., with\none in the U.K.. We did not collect other demographic information for privacy due to the limited\nsample size. We acknowledge that the lack of diversity of participants is a significant limitation,\nand discuss future work to address this problem in later sections.\n4\nRESULTS\nOverall, participants confirmed our hypotheses that the current chatbot system provides generic and\nverbose responses, and that they liked ExploreLLM\u2019s ability to provide structured task breakdown\nand personalization, despite some usability issues. Importantly, we also found that hallucination is\na major hurdle in building trust in the system. Participants often pointed out where the information\nprovided in the system is wrong, or that they don\u2019t trust the information and need to conduct their\nown research for additional verification. Finally, participants expressed wishes for more control of\nthe system, richer content and tool use, which we discuss in future work.\nBelow we report in more detail each of the findings. We refer to a particular participant as P#.\n4.1\nLimitations of the Chat-Only UI\nParticipants pointed out two major limitations of the current chat-only UI for exploratory tasks.\nAll but one participant mentioned that ChatGPT responses are verbose and generic. Multiple\n2Note during the study period the ChatGPT system was changed from ChatGPT-4 to turbo. We do not know the details of\nthe change but in ChatGPT-4 turbo, the system also has web browsing capability, which resulted in some of the participants\ncommenting about transparency of the system.\n10\nMa et al.\nparticipants used the \u201cstop generating\u201d button to interrupt ChatGPT mid-response (while the\nresponse is streaming), commenting \u201cthis is really verbose\u201d (P5). P3 stated, \u201cI didn\u2019t need this much\ntext to start with.\u201d P7 said, \u201ctoo much, stop it\u201d. Often, people interrupt the generation when they\nnotice the response being streamed back is off track. After the interruption, people often ask follow\nup questions to steer the conversation to another direction. For example, P5 asked \u201cCan you tell me\nwhen is the best time to go to [destination]?\u201d after noticing the ChatGPT was going on and on about\nthe cost of the trip after the initial prompt.\nThe second major limitation of the chat-only interface is that responses are generic. P1, P2, P5\nand P6 commented that ChatGPT gives generic answers. P1 stated, \u201cIt was giving me kind of generic\nstuff. I had to remember to tell it stuff about me.\u201d P6 mentioned that \u201cIt was not really personalized\nbecause this is just giving me information.\u201d Even when the user provides more personal preferences,\nsometimes ChatGPT fails to take it into consideration when generating responses. For example,\nP5 specifically mentioned something that they are not interested in but the system still generates\nresponses containing those options, revealing a lack of steerability.\n4.2\nStructured UI and Guided Task Flow\nCompared to the chat-only UI of ChatGPT, participants liked the structured UI of ExploreLLM\nand the guided task flow.\nIn particular, participants found that the broken down list of sub-tasks was useful and helped\nthem think, plan and navigate. P5 stated, \u201cHaving these different aspects of the trip surface upfront is\nhelpful [...] where was the best time, how do we get there, what are some things to do there? It\u2019s great\nthat the system surfaces these up front. So I don\u2019t have to like consider all the different aspects because\nI feel like I will miss things.\u201d P2 stated, \u201cthe logical flow was very clear.\u201d P3 said, \u201cthe whole nature of\nguiding people through their problem is a big one.\u201d\nPeople also liked the fact that the list of sub-tasks persists as a collection of artifacts to reference\nback on. Compared to the chat-only interface, P1 stated, \u201cIt\u2019s just nice not have a wall of text to have\nto look back through.\u201d In contrast to the wall of text of ChatGPT, people found ExploreLLM\u2019s UI\nglanceable and that it keeps a better record of the task state. P2 commented on the list of sub-tasks,\nsaying that \u201cI really like the list [...] I can see those lists so I felt more confident.\u201d This is in contrast\nto the linear chat-only UI, where the users end up with a long chat history and have to \u201cdig back\nthrough to find the interesting parts\u201d (P1).\nAt the same time, users indicated their wish for more control. P1 indicated that \u201cI wish that I\ncould [...] X out some of these things. [...] these seem pretty similar so I could like close one of those so I\ncan remember [..] like keep the high priority ones open.\u201d\n4.3\nPersonalization\nContrary to the generic responses in chat-only interactions, the personalization aspect of Ex-\nploreLLM stood out to participants.\nParticipants found that it was easier to get personal preferences into the system because of the\ndedicated UI (P1, P2, P3, P6). P3 said, \u201cI do think this is more personalized because it was easier to get\nthat out of me.\u201d P2 also noticed that the personalized context carries over globally throughout the\nentire interaction. \u201cIn the middle of the tasks, I specified clean and budget and it carried over to other\ntasks, and it helped me to narrow down those options and prioritize.\u201d\nNonetheless, participants expect ExploreLLM to be more proactive in eliciting their personal\npreferences, by doing \u201cmore cognitive lift\u201d. When commenting on the options generated by the\nExploreLLM system for flights, P3 added, \u201cIt should have probably asked me [...] where do I have\nairline miles [...] or am I a member of a thing? Do I have any preferences? I hate having to remember\nBeyond ChatBots: ExploreLLM for Structured Thoughts and Personalized Model Responses\n11\nwhat to tell it. If I talk to a travel agent, I don\u2019t get that. [...] They know what to ask to start. So that\u2019s\na pretty big difference.\u201d\n4.4\nOptions as Responses\nFive out of eight participants (P1, P2, P5, P6, P8) indicated liking having a list of options to choose\nfrom in ExploreLLM. For some participants, the benefit mainly comes from reducing mental load.\nP8 said, \u201cI think the idea of specifying sort of vaguely what you\u2019re interested in, getting some options,\npicking one, and then having the other follow-up responses take those options into account make\nsense.\u201d P2 added, \u201cIt removed all the burdens and it\u2019ll is just basically gave me options that I just need\nto choose from so it was super helpful.\u201d For others, such as P6, the benefit comes more from the\nability to personalize. P6 saw different layover options for the flight in one of the sub-tasks, and\npointed out that having different stopover options is important because sometimes there are visa\nlimitations and they prefer to go through one of the options than other ones. Finally, people also\nliked the explainability of the options, indicating that the explanations associated with each option\nare helpful.\nHowever, issues arise when there are similar or too many options. P5 observes that for the\nsubtask \u201cCheck the best time to visit [destination]\u201d, \u201cthe recommended and other options are pretty\nmuch the same\u201d. (The options were all in autumn, including late August, September, and early\nOctober.)\nNotably, many participants expressed the desire for richer content (P5, P7) and tool use (P1, P2,\nP6). P5 suggested wanting to see the list of options on a map to visualize their relationships to\neach other and how close things are. \u201cSome richer content like images and Maps should help and if I\ncould [...] click on this to [...] get more details about the champagne houses or click on the locations to\nunderstand more about [...] the scenery around it, that would be helpful.\u201d P5 also switched tabs to\nsearch for some suggested destinations on Google maps mid-task. P7 also suggested that they want\nto see the full itinerary on a map.\nP2 suggested connecting to external tools would be more helpful. When the system was loading,\nP6 speculated that the system was using some tools to search for flight options (the system was\nnot using any tools yet and we discuss this in future work). Participants also expressed the desire\nfor the system to take actions rather than providing information alone. P1 mentioned that even\nthough the system provided information, \u201cI still have to do the annoying nitty-grittys like actually\nbook the tickets and figure out calendar dates\u201d.\n4.5\nHallucination\nAcross both ChatGPT and ExploreLLM, participants noted hallucination as a major limitation and\nexpressed reservations in trusting the results fully.\nParticipants noted multiple times that some information generated by ChatGPT and ExploreLLM\nis wrong. P2 requested travel planning for the NeurIPS 2023 conference in ExploreLLM. Although\nthe system correctly breaks down the task into first looking into the location of NeurIPS conference,\nit tried to look up the location for 2022 rather than the latest. Further, the system hallucinates and\nthinks that the NeurIPS 2022 was in Vancouver, Canada. P2 was confused and looked up NeurIPS\n2022\u2019s location, which is New Orleans, USA. Factual errors like this get propagated to downstream\ntasks such as hotel booking and is very disruptive to the entire experience. In another example, P6\nnoticed that one of the suggested airports for flight options did not exist. The ExploreLLM system\nsuggested one option to go from London to India as \u201cEmirates EK63 flight departs from London\n3Fact checking shows that Emirates EK6 leaves from London Heathrow to Dubai.\n12\nMa et al.\nGatwick and arrives at Kolkata Bhawanipur with one stop at Dubai\u201d. P6 commented that \u201cThis is\nwrong, there is no airport at Bhawanipur.\u201d\nSuch hallucination has significant safety implications and also dramatically limits the system\u2019s\nusefulness and the extent to which users trust the system to take actions autonomously. P1 com-\nmented on one of the ChatGPT responses, \u201cI don\u2019t know if I would trust it even it\u2019s telling me [...] be\ncareful about the transit options being canceled because of snow, I think I need to do some research\ninto exactly what that means\u201d. We discuss this further in the limitations and future work section,\nincluding opportunities for better grounding, tool use, and integration with other systems.\n4.6\nUsability Issues\nAs a research prototype, some usability issues have emerged in ExploreLLM. Most notably, all\nparticipants noticed that the system has high latency. Almost all participants noted that the system is\nslow when generating different options. Because the response back from the prompts are structured\njson data, it is not very suitable for streaming, which could have reduced the perception of latency.\nAs the underlying LLMs become faster, the latency issue can be mitigated to a large extent.\nProviding more transparency about the process to the users can be another design solution\nfor handling latency. When the system is loading, currently ExploreLLM indicates \u201cthinking\u201d.\nP3 wondered, \u201cWhat is it thinking about? ChatGPT with browsing is a little bit clearer because it\u2019s\ntelling me what tools it\u2019s using and how it\u2019s thinking about stuff.\u201d As more tools and functionalities\nget integrated into the system, revealing the inner workings of the system is important for user\ntransparency and trust.\n5\nDISCUSSION\nIn this work, we introduced the ExploreLLM system designed to provide more structured task\nguidance and easier personalization. Echoing findings in recent work [14, 44], our user studies\nsupport the motivating hypothesis that current chatbots\u2019 responses can be verbose and generic.\nParticipants liked having the structure of sub-tasks and the ability to personalize, wishing for even\nmore control.\nOne of the most important findings of our work is that much of the prompt engineering work\nand the \u201cthoughts\u201d of LLMs can have direct user benefits when appropriately exposed to the end\nusers. This intuitively makes sense as many underlying structures of LLM reasoning methods are\ncompatible with how humans think and solve problems. We show that prompt-based methods can\neffectively aid humans in creating schemata (or \u201cmental models\u201d) for problem solving. With the\nscaffold of a logical task structure, each sub-tasks can be explored separately to reduce cognitive\nload, while loosely coupled together for effective information coordination across the system.\nIn addition, our findings show that task decomposition is promising for better tool use. LLM\nassistants do not exist in isolation, and users wish for a tighter integration with existing data and\ntools. Tool use is especially important given that hallucination presented itself as a major hurdle in\ngaining user trust. Through task decomposition, we can break down a much more complex task\ninto concrete sub-tasks that have readily made tools suitable for the sub-task (e.g., checking the\nweather of a particular location; searching for a particular type of place on maps).\nWe see that participants intuitively begin to expect or wish for better tool use once they start\nexploring sub-tasks. The tree-like nature of ExploreLLM tasks system makes it highly compatible\nfor deeper decomposition and integration with external tools. For example, one of the sub-tasks\ngenerated by the ExploreLLM system for travel planning is flight booking. This sub-task can\nbe further broken down into sub-sub-tasks, such as deciding on dates and specifying departure\nlocations. Once the user interacts with the sub-sub-tasks, we can map user input to parameters\nwhen invoking specialized tools such as flight search engines to provide accurate and personalized\nBeyond ChatBots: ExploreLLM for Structured Thoughts and Personalized Model Responses\n13\nresults. For complex open-ended tasks that require extensive planning, a related open problem\nis how to decide when to use tools autonomously to solve sub-problems, and when to elicit user\nfeedback.\nMore generally, our work shows the promise of re-imagining the relationship between natural\nlanguage user interfaces (NLUIs) and graphical user interfaces (GUIs) [13]. As much as people are\nexcited about the giant leap in NLUIs, there lies an opportunity to re-imagine the design of GUIs\nunder this new interaction paradigm. ExploreLLM offers one exploration of the possibilities in\ncreating new \u201chybrid\u201d systems that integrate the best parts of both natural language and graphical\nuser interfaces.\n6\nLIMITATIONS AND FUTURE WORK\nOne of the most significant limitations of our work is the lack of diversity in participants. In\naddition to convenience sampling, our participants have limited representation in terms of age and\ngender, and are biased towards experienced users of AI chatbots. Five of our participants reported\ninteracting with AI chatbots a few times a week, and the rest three reported using it every day. We\nplan to expand participant diversity in a follow up work.\nSecondly, we only explored one layer of task decomposition without tool use or data integration.\nFuture work can extend to more layers of task decomposition and integrate existing tools to sub-\ntasks, or even explore leveraging the tool making abilities of LLMs itself [3, 42]. Our system only\nelicits user context through input text, and future work can explore pulling user contexts directly\nfrom user data and external apps, such as emails and calendar (e.g., what reservations users already\nmade, what events are on the calendar, what are the dates of the travel). In addition, we only\nexplored a checkbox UI and displayed options in sub-tasks as a list. Future work can explore richer\nUI as participants mentioned, such as maps and diagrams.\nThe prompt we used for task decomposition and options generation endpoints can be further\ntuned for quality and diversity. For example, we can tune both prompts to avoid repetition or\noptions that are too similar. It is also important to consider fairness in options generation given\nprior work on algorithm fairness [6] and the impact on user behaviors by social media ranking\nalgorithms [7] and to guard against overreliance [34]. While we included the prompt to generate\n\u201cdiverse and representative\u201d options, we did not formally evaluate the diversity of options generated\ndue to the limitation of sample size. Future work could formally evaluate the impact of prompts on\nthe diversity and quality of options generated, and their downstream impacts on user behavior.\nFinally, as a research prototype, our implementation still has usability issues. We plan to open-\nsource our implementation following the publication of the manuscript to allow the community to\nbuild and iterate on ExploreLLM.\n7\nACKNOWLEDGEMENT\nWe thank Jim Maddock for designing the initial version of the user evaluation plan and literature\nreview. We also thank Melvin Johnson and Varun Godbole for their insightful feedback during the\nearly phase of this project.\n8\nAUTHOR CONTRIBUTION STATEMENTS\nXiao led the overall project, co-designed the personalization, options generation, and summarization\ninteractions, implemented the system, and conducted user studies and analysis. Swaroop developed\nthe original idea of tree-based interaction, came up with the initial design, demo video and write\nup. Ariel contributed to user studies and interaction design. Sophie iterated on the initial demo and\ndesigned the system UI and interaction. Jilin contributed to the paper framing and user studies\ndesign. Chinmay participated in early discussions about the LLM sub-task decomposition method,\n14\nMa et al.\nand helped with theoretically grounding work. Heng-Tze and Quoc provided continuous advice\nand feedback on the research of prompt decomposition and tree-based user-LLM interactions. Ed\nprovided feedback on paper framing and connection to prior literature. All authors reviewed the\nmanuscript.\nREFERENCES\n[1] Saul Albert and Jan P De Ruiter. 2018. Repair: the interface between interaction and cognition. Topics in cognitive\nscience 10, 2 (2018), 279\u2013313.\n[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural\ninformation processing systems 33 (2020), 1877\u20131901.\n[3] Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. 2023. Large language models as tool makers.\narXiv preprint arXiv:2305.17126 (2023).\n[4] Minsuk Chang, Stefania Druga, Alexander J Fiannaca, Pedro Vergani, Chinmay Kulkarni, Carrie J Cai, and Michael\nTerry. 2023. The Prompt Artists. In Proceedings of the 15th Conference on Creativity and Cognition. 75\u201387.\n[5] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. 2022. Program of thoughts prompting: Disentangling\ncomputation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588 (2022).\n[6] Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. 2017. Algorithmic decision making and\nthe cost of fairness. In Proceedings of the 23rd acm sigkdd international conference on knowledge discovery and data\nmining. 797\u2013806.\n[7] Motahhare Eslami, Aimee Rickman, Kristen Vaccaro, Amirhossein Aleyasen, Andy Vuong, Karrie Karahalios, Kevin\nHamilton, and Christian Sandvig. 2015. \" I always assumed that I wasn\u2019t really that close to [her]\" Reasoning about\nInvisible Algorithms in News Feeds. In Proceedings of the 33rd annual ACM conference on human factors in computing\nsystems. 153\u2013162.\n[8] Kristie Fisher, Scott Counts, and Aniket Kittur. 2012. Distributed sensemaking: improving sensemaking by leveraging\nthe efforts of previous users. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 247\u2013256.\n[9] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023.\nPal: Program-aided language models. In International Conference on Machine Learning. PMLR, 10764\u201310799.\n[10] Andreas Gegenfurtner, Erno Lehtinen, Laura Helle, Markus Nivala, Erkki Svedstr\u00f6m, and Roger S\u00e4lj\u00f6. 2019. Learning\nto see like an expert: On the practices of professional vision and visual expertise. International Journal of Educational\nResearch 98 (2019), 280\u2013291.\n[11] Prakhar Gupta, Cathy Jiao, Yi-Ting Yeh, Shikib Mehri, Maxine Eskenazi, and Jeffrey P Bigham. 2022. InstructDial:\nimproving zero and few-shot generalization in dialogue through instruction tuning. In Proceedings of the 2022 Conference\non Empirical Methods in Natural Language Processing. 505\u2013525.\n[12] James Hollan, Edwin Hutchins, and David Kirsh. 2000. Distributed cognition: toward a new foundation for human-\ncomputer interaction research. ACM Transactions on Computer-Human Interaction (TOCHI) 7, 2 (2000), 174\u2013196.\n[13] Bernard J Jansen. 1998. The graphical user interface. ACM SIGCHI Bulletin 30, 2 (1998), 22\u201326.\n[14] Peiling Jiang, Jude Rayan, Steven P Dow, and Haijun Xia. 2023. Graphologue: Exploring Large Language Model\nResponses with Interactive Diagrams. arXiv preprint arXiv:2305.11473 (2023).\n[15] Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2022.\nDecomposed prompting: A modular approach for solving complex tasks. arXiv preprint arXiv:2210.02406 (2022).\n[16] Yoonsu Kim, Jueon Lee, Seoyoung Kim, Jaehyuk Park, and Juho Kim. 2023. Understanding Users\u2019 Dissatisfaction with\nChatGPT Responses: Types, Resolving Tactics, and the Effect of Knowledge Level. arXiv preprint arXiv:2311.07434\n(2023).\n[17] David Kirsh. 1995. Complementary strategies: Why we use our hands when we think. (1995).\n[18] Scott R Klemmer, Bj\u00f6rn Hartmann, and Leila Takayama. 2006. How bodies matter: five themes for interaction design.\nIn Proceedings of the 6th conference on Designing Interactive systems. 140\u2013149.\n[19] Kirby Kuznia, Swaroop Mishra, Mihir Parmar, and Chitta Baral. 2022. Less is More: Summary of Long Instructions\nis Better for Program Synthesis. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language\nProcessing. 4532\u20134552.\n[20] Preethi Lahoti, Nicholas Blumm, Xiao Ma, Raghavendra Kotikalapudi, Sahitya Potluri, Qijun Tan, Hansa Srinivasan,\nBen Packer, Ahmad Beirami, Alex Beutel, et al. 2023. Improving Diversity of Demographic Representation in Large\nLanguage Models via Collective-Critiques and Self-Voting. arXiv preprint arXiv:2310.16523 (2023).\n[21] Michael Xieyang Liu, Jane Hsieh, Nathan Hahn, Angelina Zhou, Emily Deng, Shaun Burley, Cynthia Taylor, Aniket\nKittur, and Brad A Myers. 2019. Unakite: Scaffolding developers\u2019 decision-making using the web. In Proceedings of the\n32nd Annual ACM Symposium on User Interface Software and Technology. 67\u201380.\nBeyond ChatBots: ExploreLLM for Structured Thoughts and Personalized Model Responses\n15\n[22] Xiao Ma, Swaroop Mishra, Ahmad Beirami, Alex Beutel, and Jilin Chen. 2023. Let\u2019s Do a Thought Experiment: Using\nCounterfactuals to Improve Moral Reasoning. arXiv preprint arXiv:2306.14308 (2023).\n[23] Dor Ma\u2019ayan, Wode Ni, Katherine Ye, Chinmay Kulkarni, and Joshua Sunshine. 2020. How domain experts create\nconceptual diagrams and implications for tool design. In Proceedings of the 2020 CHI Conference on Human Factors in\nComputing Systems. 1\u201314.\n[24] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri,\nShrimai Prabhumoye, Yiming Yang, et al. 2023. Self-refine: Iterative refinement with self-feedback. arXiv preprint\narXiv:2303.17651 (2023).\n[25] Paul P Maglio, Teenie Matlock, Dorth Raphaely, Brian Chernicky, and David Kirsh. 2020. Interactive skill in Scrabble.\nIn Proceedings of the twenty-first annual conference of the cognitive science society. Psychology Press, 326\u2013330.\n[26] Sandra P Marshall. 1995. Schemas in problem solving. Cambridge University Press.\n[27] Marvin Minsky. 1974. A framework for representing knowledge.\n[28] Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind\nTafjord, Ashish Sabharwal, Peter Clark, et al. 2022. LILA: A Unified Benchmark for Mathematical Reasoning. In\nProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 5807\u20135832.\n[29] Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh Hajishirzi. 2022. Reframing Instructional\nPrompts to GPTk\u2019s Language. In Findings of the Association for Computational Linguistics: ACL 2022. 589\u2013612.\n[30] Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022. Cross-Task Generalization via Natural\nLanguage Crowdsourcing Instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers). 3470\u20133487.\n[31] Julia Neidhardt, Rainer Schuster, Leonhard Seyfang, and Hannes Werthner. 2014. Eliciting the users\u2019 unknown\npreferences. In Proceedings of the 8th ACM Conference on Recommender systems. 309\u2013312.\n[32] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David\nDohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2022. Show Your Work: Scratchpads for Intermediate\nComputation with Language Models. In Deep Learning for Code Workshop.\n[33] Douglas W Oard and Jinmook Kim. 1998. Implicit feedback for recommender systems. (1998).\n[34] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]\n[35] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback.\nAdvances in Neural Information Processing Systems 35 (2022), 27730\u201327744.\n[36] Mihir Parmar, Swaroop Mishra, Mirali Purohit, Man Luo, Murad Mohammad, and Chitta Baral. 2022. In-BoXBART:\nGet Instructions into Biomedical Multi-Task Learning. In Findings of the Association for Computational Linguistics:\nNAACL 2022. 112\u2013128.\n[37] Pruthvi Patel, Swaroop Mishra, Mihir Parmar, and Chitta Baral. 2022. Is a Question Decomposition Unit All We Need?.\nIn Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 4553\u20134569.\n[38] Sarah R Powell. 2011. Solving word problems using schemas: A review of the literature. Learning Disabilities Research\n& Practice 26, 2 (2011), 94\u2013108.\n[39] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. 2022. Measuring and narrowing\nthe compositionality gap in language models. arXiv preprint arXiv:2210.03350 (2022).\n[40] Emily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris Callison-Burch, and Jason Wei. 2022. A Recipe for\nArbitrary Text Style Transfer with Large Language Models. In Proceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Papers). 837\u2013848.\n[41] Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, and Xian Li. 2023. Branch-Solve-Merge\nImproves Large Language Model Evaluation and Generation. arXiv preprint arXiv:2310.15123 (2023).\n[42] Timo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and\nThomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761\n(2023).\n[43] Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi\nTay, Sebastian Ruder, Denny Zhou, et al. 2022. Language models are multilingual chain-of-thought reasoners. In The\nEleventh International Conference on Learning Representations.\n[44] Sangho Suh, Bryan Min, Srishti Palani, and Haijun Xia. 2023. Sensecape: Enabling Multilevel Exploration and\nSensemaking with Large Language Models. arXiv preprint arXiv:2305.11483 (2023).\n[45] Masaki Suwa, Barbara Tversky, John Gero, and Terry Purcell. 2001. Seeing into sketches: Regrouping parts encourages\nnew interpretations. In Visual and spatial reasoning in design. 207\u2013219.\n[46] Perry W Thorndyke and Barbara Hayes-Roth. 1979. The use of schemata in the acquisition and transfer of knowledge.\nCognitive psychology 11, 1 (1979), 82\u2013106.\n16\nMa et al.\n[47] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny\nZhou. 2023. Self-Consistency Improves Chain of Thought Reasoning in Language Models. arXiv:2203.11171 [cs.CL]\n[48] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi.\n2022. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560 (2022).\n[49] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V\nLe. 2021. Finetuned Language Models are Zero-Shot Learners. In International Conference on Learning Representations.\n[50] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022.\nChain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing\nSystems 35 (2022), 24824\u201324837.\n[51] Terry Winograd. 1971. Procedures as a representation for data in a computer program for understanding natural\nlanguage. (1971).\n[52] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun Chen. 2023. Large\nlanguage models as optimizers. arXiv preprint arXiv:2309.03409 (2023).\n[53] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree\nof thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601 (2023).\n[54] JD Zamfirescu-Pereira, Richmond Y Wong, Bjoern Hartmann, and Qian Yang. 2023. Why Johnny can\u2019t prompt: how\nnon-AI experts try (and fail) to design LLM prompts. In Proceedings of the 2023 CHI Conference on Human Factors in\nComputing Systems. 1\u201321.\n[55] Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed H Chi, Quoc V Le, and Denny Zhou.\n2023. Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models. arXiv preprint arXiv:2310.06117\n(2023).\n[56] Denny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier\nBousquet, Quoc Le, et al. 2022. Least-to-most prompting enables complex reasoning in large language models. arXiv\npreprint arXiv:2205.10625 (2022).\n[57] Tamara Zubatiy, Niharika Mathur, Larry Heck, Kayci L Vickers, Agata Rozga, and Elizabeth D Mynatt. 2023. \" I don\u2019t\nknow how to help with that\"-Learning from Limitations of Modern Conversational Agent Systems in Caregiving\nNetworks. Proceedings of the ACM on Human-Computer Interaction 7, CSCW2 (2023), 1\u201328.\nBeyond ChatBots: ExploreLLM for Structured Thoughts and Personalized Model Responses\n17\nA\nAPPENDIX\nWe provide some examples of interaction results with Google Bard (Figure 7), OpenAI ChatGPT\n(Figure 8) and ExploreLLM (Figure 9, 10 and 11) for reference.\nFig. 7. Bard results for the query \u201cI want to book a flight to Tokyo\u201d\n18\nMa et al.\nFig. 8. GPT-4 results for the query\u201cI want to book a flight to Tokyo\u201d\nFig. 9. ExploreLLM results for the query \u201cI want to book a flight to Tokyo\u201d. Users can further interact with\neach of the sub-tasks.\nBeyond ChatBots: ExploreLLM for Structured Thoughts and Personalized Model Responses\n19\nFig. 10. ExploreLLM regenerates and refreshes the sub-tasks for the query \u201cI want to book a flight to Tokyo\u201d\nwhen the user specifies \u201cI am traveling with a toddler\u201d.\nFig. 11. When the user interacts with a subtask such as \u201cinvestigate child-friendly facilities on potential\nflights\u201d, ExploreLLM generates a list of options (may contain hallucination).\n"
  },
  {
    "title": "MoMask: Generative Masked Modeling of 3D Human Motions",
    "link": "https://arxiv.org/pdf/2312.00063.pdf",
    "upvote": "15",
    "text": "MoMask: Generative Masked Modeling of 3D Human Motions\nChuan Guo\u2217\nYuxuan Mu\u2217\nMuhammad Gohar Javed\u2217\nSen Wang\nLi Cheng\nUniversity of Alberta\n{cguo2, ymu3, javed4, lcheng5}@ualberta.ca\nhttps://ericguo5513.github.io/momask/\nWalking forward and steps over an object, \nand then continue walking.\nTaking two strides forward, pivot swiftly on \nleft foot, and then walk the other way.\nA person performs a standing back kick.\nFigure 1. Our MoMask, when provided with a text input, generates high-quality 3D human motion with diversity and precise control over\nsubtleties such as \u201dtwo strides forward\u201d, \u201dpivot on left foot\u201d, and \u201dpivot swiftly\u201d.\nAbstract\nWe introduce MoMask, a novel masked modeling frame-\nwork for text-driven 3D human motion generation. In Mo-\nMask, a hierarchical quantization scheme is employed to\nrepresent human motion as multi-layer discrete motion to-\nkens with high-fidelity details. Starting at the base layer,\nwith a sequence of motion tokens obtained by vector quan-\ntization, the residual tokens of increasing orders are de-\nrived and stored at the subsequent layers of the hierar-\nchy.\nThis is consequently followed by two distinct bidi-\nrectional transformers. For the base-layer motion tokens,\na Masked Transformer is designated to predict randomly\nmasked motion tokens conditioned on text input at train-\ning stage. During generation (i.e. inference) stage, start-\ning from an empty sequence, our Masked Transformer iter-\natively fills up the missing tokens; Subsequently, a Residual\nTransformer learns to progressively predict the next-layer\ntokens based on the results from current layer. Extensive ex-\nperiments demonstrate that MoMask outperforms the state-\n*These authors contributed equally to this work.\nof-art methods on the text-to-motion generation task, with\nan FID of 0.045 (vs e.g. 0.141 of T2M-GPT) on the Hu-\nmanML3D dataset, and 0.228 (vs 0.514) on KIT-ML, re-\nspectively. MoMask can also be seamlessly applied in re-\nlated tasks without further model fine-tuning, such as text-\nguided temporal inpainting.\n1. Introduction\nGenerating 3D human motions from textual descriptions,\naka text-to-motion generation, is a relatively new task that\nmay play an important role in a broad range of applications\nsuch as video games, metaverse, and virtual reality & aug-\nmented reality. In the past few years, it has generated in-\ntensive research interests [9, 15, 16, 21, 23, 36, 42, 49, 50].\nAmong them, it has become popular to engage generative\ntransformers in modeling human motions [13, 16, 21, 49].\nIn this pipeline, motions are transformed into discrete to-\nkens through vector quantization (VQ), then fed into e.g.\nan autoregressive model to generate the sequence of motion\ntokens in an unidirectional order. Though achieving impres-\n1\narXiv:2312.00063v1  [cs.CV]  29 Nov 2023\nsive results, these methods shares two innate drawbacks. To\nbegin with, the VQ process inevitably introduces approxi-\nmation errors, which imposes undesired limit to the motion\ngeneration quality. Moreover, the unidirectional decoding\nmay unnecessarily hinder the expressiveness of the genera-\ntive models. For instance, consider the following scenario:\nat each time step, the motion content is generated by only\nconsidering the preceding (rather than global) context; fur-\nthermore, errors will often accumulate over the generation\nprocess. Though several recent efforts using discrete diffu-\nsion models [23, 30] have considered to decode the motion\ntokens bidirectionally, by relying on a cumbersome discrete\ndiffusion process, they typically require hundreds of itera-\ntions to produce a motion sequence.\nMotivated by these observations, we propose a novel\nframework, MoMask, for high-quality and efficient text-to-\nmotion generation by leveraging the residual vector quanti-\nzation (RVQ) techniques [4, 34, 48] and the recent genera-\ntive masked transformers [7, 8, 24, 47]. Our approach builds\non the following three components. First, an RVQ-VAE is\nlearned to establish precise mappings between 3D motions\nand the corresponding sequences of discrete motion tokens.\nUnlike previous motion VQ tokenizers [13, 16, 49] that typ-\nically quantize latent codes in a single pass, our hierarchi-\ncal RVQ employs iterative rounds of residual quantization\nto progressively reduce quantization errors. This results in\nmulti-layer motion tokens, with the base layer serving to\nperform standard motion quantization, and the rest layers\nin the hierarchy capturing the residual coding errors of their\nrespective orders, layer by layer. Our quantization-based hi-\nerarchical design is further facilitated by two distinct trans-\nformers, the Masked Transformer (i.e. M-Transformer) and\nResidual Transformer (R-Transformer), that are dedicated\nto generating motion tokens for the base VQ layer and the\nrest residual layers, respectively.\nThe M-Transformer, based on BERT [10], is trained to\npredict the randomly masked tokens at the base layer, con-\nditioned on textual input. The ratio of masking, instead of\nbeing fixed [10, 18], is a scheduled variable that ranges from\n0 to 1. During generation, starting from all tokens being\nmasked out, M-Transformer produces a complete sequence\nof motion tokens within a small number of iterations. At\neach iteration, all masked tokens are predicted simultane-\nously. Predicted tokens with the highest confidence will re-\nmain unchanged, while the others are masked again and re-\npredicted in the next iteration. Once the base-layer tokens\nare generated, the R-Transformer ensues to progressively\npredict the residual tokens of the subsequent layer given the\ntoken sequence at current layer. Overall, the entire set of\nlayered motion tokens can be efficiently generated within\nmerely 15 iterations, regardless of the motion\u2019s length.\nOur main contributions can be summarized as follows:\nFirst, our MoMask is the first generative masked modeling\nframework for the problem of text-to-motion generation. It\ncomprises of a hierarchical quantization generative model\nand the dedicated mechanism for precise residual quantiza-\ntion, base token generation and residual token prediction.\nSecond, our MoMask pipeline produces precise and effi-\ncient text-to-motion generation.\nEmpirically, it achieves\nnew state-of-the-art performance on text-to-motion gener-\nation task with an FID of 0.045 (vs. 0.141 in [49]) on Hu-\nmanML3D and 0.204 (vs. 0.514 in [49]) on KIT-ML. Third,\nour MoMask also works well for related tasks, such as text-\nguided motion inpainting.\n2. Related Work\nHuman Motion Generation. Recently, we have witnessed\nthe surge of works for neural motion generation, with con-\nditioning on various domains such as motion prefix [29, 33],\naction class [6, 14, 31, 35], audio [13, 40, 43, 53], texts [9,\n15, 16, 36, 42]. Early works [1, 12, 20, 27, 38] commonly\nmodel motion generation deterministically, resulting in av-\neraged and blurry motion results. This is properly addressed\nby stochastic models. GAN modeling is adopted in [5, 46]\nfor action-conditioned motion generation. Meanwhile, tem-\nporal VAE framework and transformer architecture are ex-\nploited in the works of [17, 35]. In terms of text-to-motion\ngeneration, T2M [15] extended the temporal VAE to learn\nthe probabilistic mapping between texts and motions. Simi-\nlarly, TEMOS [36] takes advantage of Transformer VAE to\noptimize a joint variational space between natural language\nand motions, which is extended by TEACH [3] for long\nmotion compositions. MotionCLIP [41] and ohMG [28]\nmodel text-to-motion in an unsupervised manner using the\nlarge pretrained CLIP [39] model.\nThe emerging diffu-\nsion models and autoregressive models have significantly\nchanged the field of motion generation. In diffusion meth-\nods, a network is learned to gradually denoise the mo-\ntion sequence, supervised by a scheduled diffusion pro-\ncess [9, 22, 23, 30, 42, 43, 50]. Regarding autoregressive\nmodels [13, 16, 21, 49, 52], motions are firstly discretized\nas tokens via vector quantization [44], which are then mod-\neled by the expressive transformers as in language model.\nGenerative Masked Modeling.\nBERT [10] introduces\nmasked modeling for language tasks that word tokens are\nrandomly masked out with a fixed ratio, and then the bi-\ndirectional transformer learns to predict the masked tokens.\nDespite being a decent pre-trained text encoder, BERT can-\nnot synthesize novel samples. In this regard, [7] proposes\nto mask the tokens with a variable and traceable rate that\nis controlled by a scheduling function.\nTherefore, new\nsamples can be synthesized iteratively following the sched-\nuled masking. MAGE [24] unifies representation learning\nand image synthesis using the masked generative encoder.\nMuse [8] extends this paradigm for text-to-image genera-\ntion and editing. Magvit [47] suggests a versatile masking\n2\n&0\n&5\n&2\nMask\nCLIP\nMasked Transformer\n(a) Motion Residual VQ-VAE\n(b) Masked Transformer\nPredicted Tokens *)5 \nE\nVQ 0\nVQ 1\nVQ V\nResidual\nD\n\u2026\nMotion Tokens )5\nResidual Transformer\nMotion Tokens \n)5:4/0\nVQ ID%\nPredicted Tokens *)4\n(c) Residual Transformer\n\u201cA person walks \nin a circle.\u201d\nCodebook Indexing\nMotion Tokens )5\n\u201cA person walks \nin a circle.\u201d\nCLIP\nMasked token\nBase-layer token\nResidual\u2013layer token\nResidual-layer VQ code\nBase-layer VQ code\nFigure 2. Approach overview. (a) Motion sequence is tokenized through vector quantization (VQ), also referred to as the base quantization\nlayer, as well as a hierarchy of multiple layers for residual quantization. (b) Parallel prediction by the Masked Transformer: the tokens in\nthe base layer t0 are randomly masked out with a variable rate, and then a text-conditioned masked transformer is trained to predict the\nmasked tokens in the sequence simultaneously. (c) Layer-by-layer progressive prediction by the Residual Transformer. A text-conditioned\nresidual transformer learns to progressively predict the residual tokens tj>0 from the tokens in previous layers, t0:j\u22121.\nstrategy for multi-task video generation. Inspired by these\nsuccesses, we first introduce generative masked modeling\nfor human motion synthesis in this paper.\nDeep Motion Quantization and RVQ. [2] learns seman-\ntically meaningful discrete motif words leveraging triplet\ncontrastive learning.\nTM2T [16] starts applying vector\nquantized-VAE [44] to learn the mutual mapping between\nhuman motions and discrete tokens, where the autoencod-\ning latent codes are replaced with the selected entries from\na codebook. T2M-GPT [49] further enhances the perfor-\nmance using EMA and code reset techniques. Nevertheless,\nthe quantization process inevitably introduces errors, lead-\ning to suboptimal motion reconstruction. In this work, we\nadapt residual quantization [4, 34, 48], a technique used in\nneural network compression [11, 25, 26] and audio quan-\ntization [4, 45] which iteratively quantizes a vector and its\nresiduals. This approach represents the vector as a stack of\ncodes, enabling high-precision motion discretization.\n3. Approach\nOur goal is to generate a 3D human pose sequence m1:N of\nlength N guided by a textual description c, where mi \u2208 RD\nwith D denoting the dimension of pose features. As illus-\ntrated in Fig. 2, our approach consists of three principle\ncomponents: a residual-based quantizer that tokenizes mo-\ntion sequence into multi-layer discrete tokens (Sec. 3.1),\na masked transformer that generates motion tokens in the\nbase layer (Sec. 3.2), and a residual transformer (Sec. 3.3)\nthat predicts the tokens in the subsequent residual layers.\nThe inference process of generation is detailed in Sec. 3.4.\n3.1. Training: Motion Residual VQ-VAE\nConventional motion VQ-VAEs [16, 21, 49, 52] transform\na motion sequence into one tuple of discrete motion tokens.\nSpecifically, the motion sequence m1:N \u2208 RN\u00d7D is firstly\nencoded into a latent vector sequence \u02dcb1:n \u2208 Rn\u00d7d with\ndownsampling ratio of n/N and latent dimension d, using\n1D convolutional encoder E; each vector is subsequently\nreplaced with its nearest code entry in a preset codebook\nC = {ck}K\nk=1 \u2282 Rd, known as quantization Q(\u00b7). Then the\nquantized code sequence b1:n = Q(\u02dcb1:n) \u2208 Rn\u00d7d is pro-\njected back to motion space for reconstructing the motion\n\u02c6m = D(b). After all, the indices of the selected codebook\nentries (namely motion tokens) are used as the alternative\ndiscrete representation of input motion. Though effective,\nthe quantization operation Q(\u00b7) inevitably leads to informa-\ntion loss, which further limits the quality of reconstruction.\nTo address this issue, we introduce residual quantization\n(RQ) as described in Fig. 2(a). In particular, RQ repre-\nsents a motion latent sequence \u02dcb as V + 1 ordered code\nsequences, using V + 1 quantization layers. Formally, this\nis defined as RQ(\u02dcb1:n) = [bv\n1:n]V\nv=0, with bv\n1:n \u2208 Rn\u00d7d\ndenoting the code sequence at the v-th quantization layer.\nConcretely, starting from 0-th residual r0 = \u02dcb, RQ recur-\nsively calculates bv as the approximation of residual rv, and\nthen the next residual rv+1 as\nbv = Q(rv),\nrv+1 = rv \u2212 bv,\n(1)\n3\nM-Transformer\nRemask\n\u2026\nM-Transformer\nR-Transformer\n\u2026\nR-Transformer\nD\n6*(8)\n6*(:)\n6*\n+ \u2212 - \nsteps\n6+\n. \u2212 - \nsteps\n6*:-\nText embedding of \u201dA person walks in a circle\u201d\nGenerating the base-layer tokens.\nPredicting the residual-layer tokens.\nDecoding\nMasked token\nBase-layer token\nResidual\u2013layer token\nFigure 3. Inference process. Starting from an empty sequence t0(0), the M-Transformer generates the base-layer token sequence t0 in L\niterations. Following this, the R-Transformer progressively predicts the rest-layer token sequences t2:V within V \u2212 1 steps.\nfor v = 0, ..., V . After RQ, the final approximation of latent\nsequence \u02dcb is the sum of all quantized sequences PV\nv=0 bv,\nwhich is then fed into decoder D for motion reconstruction.\nOverall, the residual VQ-VAE is trained via a motion re-\nconstruction loss combined with a latent embedding loss at\neach quantization layer:\nLrvq = \u2225m \u2212 \u02c6m\u22251 + \u03b2\nV\nX\nv=1\n\u2225rv \u2212 sg[bv]\u22252\n2,\n(2)\nwhere sg[\u00b7] denotes the stop-gradient operation, and \u03b2 a\nweighting factor for embedding constraint. This framework\nis optimized with straight-though gradient estimator [44],\nand our codebooks are updated via exponential moving av-\nerage and codebook reset following T2M-GPT [49].\nQuantization Dropout. Ideally, the early quantization lay-\ners are expected to restore the input motion as much as pos-\nsible; then the later layers add up the missing finer details.\nTo exploit the capacity of each quantizer, we adopt a quan-\ntization dropout strategy, which randomly disables the last\n0 to V layers with probability q \u2208 [0, 1] during training.\nAfter training, each motion sequence m can be repre-\nsented as V + 1 discrete motion token sequences T =\n[tv\n1:n]V\nv=0 where each token sequence tv\n1:n \u2208 {1, ..., |Cv|}n is\nthe ordered codebook-indices of quantized embedding vec-\ntors bv\n1:n, such that bv\ni = Cv\ntv\ni for i \u2208 [1, n]. Among these\nV + 1 sequences, the first (i.e. base) sequence possesses\nthe most prominent information, while the impact of subse-\nquent layers gradually diminishes.\n3.2. Training: Masked Transformer\nOur bidirectional masked transformer is designed to model\nthe base-layer motion tokens t0\n1:n \u2208 Rn, as illustrated\nin Figure 2(b). We first randomly masked out a varying\nfraction of sequence elements, by replacing the tokens with\na special [MASK] token. With \u02dct0 denoting the sequence af-\nter masking, the goal is to predict the masked tokens given\ntext c and \u02dct0. We use CLIP [39] for extracting text features.\nMathematically, our masked transformer p\u03b8 is optimized to\nminimize the negative log-likelihood of target predictions:\nLmask =\nX\n\u02dct0\nk=[MASK]\n\u2212 log p\u03b8(t0\nk|\u02dct0, c).\n(3)\nMask Ratio Schedule. We adopt a cosine function \u03b3(\u00b7) for\nscheduling the masking ratio following [7, 8]. Practically,\nthe mask ratio is obtained by \u03b3(\u03c4) = cos( \u03c0\u03c4\n2 ) \u2208 [0, 1],\nwhere \u03c4 \u2208 [0, 1] that \u03c4 = 0 means the sequence is com-\npletely corrupted. During training, the \u03c4 \u223c U(0, 1) is ran-\ndomly sampled, and then m = \u2308\u03b3(\u03c4) \u00b7 n\u2309 sequence entries\nare uniformly selected to be masked with n denoting the\nlength of sequence.\nReplacing and Remasking. To enhance the contextual rea-\nsoning of the masked transformer, we adopt the remasking\nstrategy used in BERT pretraining [10]. If a token is se-\nlected for masking, we replace this token with (1) [MASK]\ntoken 80% of the time; (2) a random token 10% of the time;\nand (3) an unchanged token 10% of the time.\n3.3. Training: Residual Transformer\nWe learn a single residual transformer to model the tokens\nfrom the other V residual quantization layers. The residual\ntransformer has a similar architecture to the masked trans-\nformer (Sec. 3.2), except that it contains V separate embed-\nding layers. During training, we randomly select a quan-\ntizer layer j \u2208 [1, V ] to learn. All the tokens in the pre-\nceding layers t0:j\u22121 are embedded and summed up as the\ntoken embedding input. Taking the token embedding, text\nembedding, and RQ layer indicator j as input, the residual\ntransformer p\u03d5 is trained to predict the j-th layer tokens in\nparallel. Overall, the training objective is:\nLres =\nV\nX\nj=1\nn\nX\ni=1\n\u2212 log p\u03d5(tj\ni|t1:j\u22121\ni\n, c, j).\n(4)\nWe also share the parameters of the j-th prediction layer\nand the (j + 1)-th motion token embedding layer for more\nefficient learning.\n4\nDatasets\nMethods\nR Precision\u2191\nFID\u2193\nMultiModal Dist\u2193\nMultiModality\u2191\nTop 1\nTop 2\nTop 3\nHuman\nML3D\nTM2T [16]\n0.424\u00b1.003\n0.618\u00b1.003\n0.729\u00b1.002\n1.501\u00b1.017\n3.467\u00b1.011\n2.424\u00b1.093\nT2M [15]\n0.455\u00b1.003\n0.636\u00b1.003\n0.736\u00b1.002\n1.087\u00b1.021\n3.347\u00b1.008\n2.219\u00b1.074\nMDM [42]\n-\n-\n0.611\u00b1.007\n0.544\u00b1.044\n5.566\u00b1.027\n2.799\u00b1.072\nMLD [9]\n0.481\u00b1.003\n0.673\u00b1.003\n0.772\u00b1.002\n0.473\u00b1.013\n3.196\u00b1.010\n2.413\u00b1.079\nMotionDiffuse [50]\n0.491\u00b1.001\n0.681\u00b1.001\n0.782\u00b1.001\n0.630\u00b1.001\n3.113\u00b1.001\n1.553\u00b1.042\nT2M-GPT [49]\n0.492\u00b1.003\n0.679\u00b1.002\n0.775\u00b1.002\n0.141\u00b1.005\n3.121\u00b1.009\n1.831\u00b1.048\nReMoDiffuse [51]\n0.510\u00b1.005\n0.698\u00b1.006\n0.795\u00b1.004\n0.103\u00b1.004\n2.974\u00b1.016\n1.795\u00b1.043\nMoMask (base)\n0.504\u00b1.004\n0.699\u00b1.006\n0.797\u00b1.004\n0.082\u00b1.008\n3.050\u00b1.013\n1.050\u00b1.061\nMoMask\n0.521\u00b1.002\n0.713\u00b1.002\n0.807\u00b1.002\n0.045\u00b1.002\n2.958\u00b1.008\n1.241\u00b1.040\nKIT-\nML\nTM2T [16]\n0.280\u00b1.005\n0.463\u00b1.006\n0.587\u00b1.005\n3.599\u00b1.153\n4.591\u00b1.026\n3.292\u00b1.081\nT2M [15]\n0.361\u00b1.005\n0.559\u00b1.007\n0.681\u00b1.007\n3.022\u00b1.107\n3.488\u00b1028\n2.052\u00b1.107\nMDM [42]\n-\n-\n0.396\u00b1.004\n0.497\u00b1.021\n9.191\u00b1.022\n1.907\u00b1.214\nMLD [9]\n0.390\u00b1.008\n0.609\u00b1.008\n0.734\u00b1.007\n0.404\u00b1.027\n3.204\u00b1.027\n2.192\u00b1.071\nMotionDiffuse [50]\n0.417\u00b1.004\n0.621\u00b1.004\n0.739\u00b1.004\n1.954\u00b1.062\n2.958\u00b1.005\n0.730\u00b1.013\nT2M-GPT [49]\n0.416\u00b1.006\n0.627\u00b1.006\n0.745\u00b1.006\n0.514\u00b1.029\n3.007\u00b1.023\n1.570\u00b1.039\nReMoDiffuse [51]\n0.427\u00b1.014\n0.641\u00b1.004\n0.765\u00b1.055\n0.155\u00b1.006\n2.814\u00b1.012\n1.239\u00b1.028\nMoMask (base)\n0.415\u00b1.010\n0.634\u00b1.011\n0.760\u00b1.005\n0.372\u00b1.020\n2.931\u00b1.041\n1.097\u00b1.054\nMoMask\n0.433\u00b1.007\n0.656\u00b1.005\n0.781\u00b1.005\n0.204\u00b1.011\n2.779\u00b1.022\n1.131\u00b1.043\nTable 1. Quantitative evaluation on the HumanML3D and KIT-ML test set. \u00b1 indicates a 95% confidence interval. MoMask (base)\nmeans that MoMask only uses base-layer tokens. Bold face indicates the best result, while underscore refers to the second best.\n3.4. Inference\nAs presented in Figure 3, there are three stages in in-\nference.\nFirstly, starting from an empty sequence t0(0)\nthat all tokens are masked out, we expect to generate the\nbase-layer token sequence t0 of length n in L iterations.\nGiven the masked token sequence at l-th iteration t0(l), M-\nTransformer first predicts the probability distribution of to-\nkens at the masked locations, and samples motion tokens\nwith the probability. Then the sampled tokens with the low-\nest \u2308\u03b3( l\nL) \u00b7 n\u2309 confidences are masked again, and the other\ntokens will remain unchanged for the rest iterations. This\nnew token sequence t0(l+1) is used to predict the token se-\nquence at the next iteration until l reaches L. Once the base-\nlayer tokens are completely generated, the R-Transformer\nprogressively predicts the token sequence in the rest quanti-\nzation layers. Finally, all tokens are decoded and projected\nback to motion sequences through the RVQ-VAE decoder.\nClassifier Free Guidance.\nWe adopt classifier-free\nguidance (CFG) [8, 19] for the prediction of both M-\nTransformer and R-Transformer. During training, we train\nthe transformers unconditionally c = \u2205 with probability of\n10%. During inference, CFG takes place at the final lin-\near projection layer before softmax, where the final logits\n\u03c9g are computed by moving the conditional logits \u03c9c away\nfrom the unconditional logits \u03c9u with guidance scale s:\n\u03c9g = (1 + s) \u00b7 \u03c9c \u2212 s \u00b7 \u03c9u.\n(5)\n4. Experiments\nEmpirical evaluations are conducted on two widely used\nmotion-language benchmarks, HumanML3D [15] and KIT-\nML [37]. HumanML3D dataset collects 14,616 motions\nfrom AMASS [32] and HumanAct12 [14] datasets, with\neach motion described by 3 textual scripts, totaling 44,970\ndescriptions. This diverse motion-language dataset contains\na variety of actions, including exercising, dancing, and ac-\nrobatics. KIT-ML dataset consists of 3,911 motions and\n6,278 text descriptions, offering an small-scale evaluation\nbenchmark. For both motion datasets, we adopt the pose\nrepresentation from the work of T2M [15]. The datasets are\naugmented by mirroring, and divided into training, testing,\nand validation sets with the ratio of 0.8:0.15:0.05.\nEvaluation metrics from T2M [15] are also adopted\nthroughout our experiments including: (1) Frechet Incep-\ntion Distance (FID), which evaluates the overall motion\nquality by measuring the distributional difference between\nthe high-level features of the generated motions and those\nof real motions; (2) R-Precision and multimodal distance,\nwhich gauge the semantic alignment between input text and\ngenerated motions; and (3) Multimodality for assessing the\ndiversity of motions generated from the same text.\nThough multimodality is indeed important, we stress its\nrole as a secondary metric that should be assessed in the\nconjunction with primary performance metrics such as FID\nand RPrecision. Emphasizing multimodality without con-\nsidering the overall quality of generated results could lead\nto optimization of models that produce random outputs for\n5\nMoMask (Ours)\nMLD\nT2M-GPT\nMDM\nTime axes\nA person sneaks away while walking sideways.\nThis person stumbles left and right while moving forward.\nA person walks forwards, sits.\nFigure 4. Visual comparisons between the different methods given three distinct text descriptions from HumanML3D testset. Only key\nframes are displayed. Compared to previous methods, MoMask generates motions with higher quality and better understanding of the\nsubtle language concepts such as \u201dstumble\u201d, \u201dsneak\u201d, \u201dwalk sideways\u201d. Please refer to the demo video for complete motion clips.\nany given input.\nImplementation Details. Our models are implemented us-\ning PyTorch. For the motion residual VQ-VAE, we employ\nresblocks for both the encoder and decoder, with a down-\nscale factor of 4. The RVQ consists of 6 quantization layers,\nwhere each layer\u2019s codebook contains 512 512-dimensional\ncodes. The quantization dropout ratio q is set to 0.2. Both\nthe masked transformer and residual transformer are com-\nposed of 6 transformer layers, with 6 heads and a latent di-\nmension of 384, applied to the HumanML3D and KIT-ML\ndatasets. The learning rate reaches 2e-4 after 2000 itera-\ntions with a linear warm-up schedule for the training of all\nmodels. The mini-batch size is uniformly set to 512 for\ntraining RVQ-VAE and 64, 32 for training transformers on\nHumanML3D and KIT-ML, respectively. During inference,\nwe use the CFG scale of 4 and 5 for M-Transformer and\nR-Transformer on HumanML3D, and (2, 5) on KIT-ML.\nMeanwhile, L is set to 10 on both datasets.\n4.1. Comparison to state-of-the-art approaches\nWe compare our approach to a set of existing state-of-the-\nart works ranging from VAE [15], diffusion-based mod-\nels [9, 42, 51], to autoregressive models [16, 49].\nQuantitative Comparisons.\nFollowing previous prac-\ntices [15, 42], each experiment is repeated 20 times, and\nthe reported metric values represent the mean with a 95%\nstatistical confidence interval. Additionally, we conduct ex-\nperiments with MoMask exclusively generating the base-\nlayer motion tokens, denoted as MoMask (base). Quantita-\ntive results for the HumanML3D and KIT-ML datasets are\npresented in Table 1.\nOverall, MoMask attains state-of-the-art performance on\nboth datasets, demonstrating substantial improvements in\nmetrics such as FID, R-Precision, and multimodal distance.\nFor the suboptimal performance on KIT-ML dataset, we\nwould like to point out that the leading model, ReMoDif-\nfuse [51], involves more intricate data retrieval from a large\n6\n(a) Comparisons on FID and Inference Cost\n(b) HumanML3D User Study\nFigure 5. (a) Comparison of inference time costs. All tests are conducted on the same Nvidia2080Ti. The closer the model is to the\norigin, the better. (b) User study results on the HumanML3D dataset. Each bar represents the preference rate of MoMask over the\ncompared model. Overall, MoMask is preferred over the other models most of the time. The dashed line marks 50%.\ndatabase to achieve high-quality motion generation. Ad-\nditionally, we observe that MoMask, even with the base-\nlayer tokens alone, already achieves competitive perfor-\nmance compared to baselines, and the inclusion of residual\ntokens further elevates the results to a higher level.\nIn Figure 5(a), we evaluate the efficiency and quality of\nmotion generation using various methods. The inference\ncost is calculated as the average inference time over 100\nsamples on one Nvidia2080Ti device. Comparing to base-\nline methods, MoMask positions itself more favorably be-\ntween generation quality and efficiency.\nUser Study. We further conduct a user study on Amazon\nMechanical Turk to validate our previous conclusions. This\nuser study involves 42 AMT users with master recognition,\nwith the side-by-side comparisons between MoMask and\neach of the state-of-the-art methods including MDM [42],\nMLD [9] and T2M-GPT [49].\nWe generate the 50 mo-\ntions for each method using the same text pool from Hu-\nmanML3D test set, and collect feedback from 3 distinct\nusers for each comparison. As shown in Fig. 5(b), MoMask\nis preferred by users in most of the time, and even earns\n42% of preference on par with ground truth motions.\nQualitative Comparisons.\nFigure 4 displays qualitative\ncomparisons of our approach and MDM[42], MLD [9],\nand T2M-GPT [49]. MDM [42] usually generates overall\nsemantically correct motions but fails to capture nuanced\nconcepts such as \u201dsneak\u201d and \u201dsideways\u201d. Though T2M-\nGPT [49] and MLD [9] have improved performance in this\naspect, they still find it difficult to generate motions accu-\nrately aligned with the textual description. For example, in\nthe bottom row, the motions from these two methods either\nforget to walk sideways (T2M-GPT [49]) or to sneak away\n(MLD [9]). Moreover, MLD [9] sometimes produces life-\nless motions where the character slides around, as shown in\nthe top row. In comparison, our method is able to generate\nhigh-quality motions faithful to the input texts. Please refer\nto supplementary videos for dynamic visualizations.\nMethods\nReconstruction\nGeneration\nFID\u2193\nMPJPE\u2193\nFID\u2193\nMM-Dist\u2193\nEvaluation on KIT-ML dataset\nM2DM [23]\n0.413\u00b1.009\n-\n0.515\u00b1.029\n3.015\u00b1.017\nT2M-GPT [49]\n0.472\u00b1.011\n-\n0.514\u00b1.029\n3.007\u00b1.023\nMoMask\n0.112\u00b1.002\n37.2\n0.228\u00b1.011\n2.774\u00b1.022\nEvaluation on HumanML3D dataset\nTM2T [16]\n0.307\u00b1.002\n230.1\n1.501\u00b1.017\n3.467\u00b1.011\nM2DM [23]\n0.063\u00b1.001\n-\n0.352\u00b1.005\n3.116\u00b1.008\nT2M-GPT [49]\n0.070\u00b1.001\n58.0\n0.141\u00b1.005\n3.121\u00b1.009\nMoMask\n0.019\u00b1.001\n29.5\n0.051\u00b1.002\n2.957\u00b1.008\nw/o RQ\n0.091\u00b1.001\n58.7\n0.093\u00b1.004\n3.031\u00b1.009\nw/o QDropout\n0.077\u00b1.000\n39.3\n0.091\u00b1.003\n2.959\u00b1.008\nw/o RRemask\n-\n-\n0.063\u00b1.003\n3.049\u00b1.006\nMoMask (V , 0)\n0.091\u00b1.001\n58.7\n0.093\u00b1.004\n3.031\u00b1.009\nMoMask (V , 1)\n0.069\u00b1.001\n54.6\n0.073\u00b1.003\n3.031\u00b1.008\nMoMask (V , 2)\n0.049\u00b1.002\n46.0\n0.072\u00b1.003\n2.978\u00b1.006\nMoMask (V , 3)\n0.037\u00b1.001\n42.5\n0.064\u00b1.003\n2.970\u00b1.007\nMoMask (V , 4)\n0.027\u00b1.001\n35.3\n0.069\u00b1.003\n2.987\u00b1.007\nMoMask (V , 5)\n0.019\u00b1.001\n29.5\n0.051\u00b1.002\n2.962\u00b1.008\nMoMask (V , 6)\n0.014\u00b1.001\n26.7\n0.076\u00b1.003\n2.994\u00b1.007\nMoMask (V , 7)\n0.014\u00b1.000\n25.3\n0.084\u00b1.004\n2.968\u00b1.007\nMoMask (q, 0)\n0.077\u00b1.000\n39.3\n0.091\u00b1.003\n2.959\u00b1.008\nMoMask (q, 0.2)\n0.019\u00b1.001\n29.5\n0.051\u00b1.002\n2.957\u00b1.008\nMoMask (q, 0.4)\n0.021\u00b1.000\n30.2\n0.082\u00b1.003\n3.006\u00b1.007\nMoMask (q, 0.6)\n0.024\u00b1.000\n33.2\n0.053\u00b1.003\n2.946\u00b1.006\nMoMask (q, 0.8)\n0.023\u00b1.000\n33.4\n0.083\u00b1.004\n3.002\u00b1.008\nTable 2. Comparison of our RVQ design vs. motion VQs from pre-\nvious works [16, 23, 49], and further analysis on residual quantiza-\ntion (RQ), quantization dropout (QDropout), and replacing & re-\nmasking (RRmask). V and q are the number of RQ and QDropout\nratio, respectively. MPJPE is measured in millimeters.\n4.2. Component Analysis\nIn Table 2, we comprehensively evaluate the impact of dif-\nferent design components in MoMask through various com-\nparisons, showcasing the performance in both motion re-\nconstruction and generation. Initially, we compare our ap-\nproach with previous VQ-based motion generation meth-\nods [16, 23, 49] on the HumanML3D and KIT-ML datasets.\nNotably, M2DM [23] incorporates orthogonality constraints\n7\nInput region\nGenerated region\nEffect range: 3s - 6s\nA person falls down and gets \nback up quickly.\nEffect range: last 2s\nA person bows.\nEffect range: 3s - 5s\nA person kicks something.\nEffect range: first 2s\nA person is doing \nwarm up.\nFigure 6. Examples of temporal inpainting. Dark dash line indicates the range(s) where the motion content(s) is given by the reference\nsequence. Orange dash line indicates the range of motion content generated by MoMask, conditioned on the text prompt below.\n2\n4\n6\n8\n10\n12\n14\n16\n18\nGuidance scale\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nFID\n3.0\n3.2\n3.4\n3.6\n3.8\nMM-Dist\n0\n5\n10\n15\n20\n25\n30\nNumber of iterations\n0.1\n0.2\n0.3\n0.4\nFID\n3.0\n3.1\n3.2\n3.3\nMM-Dist\nFigure 7. Evaluation sweep over guidance scale s (top) and itera-\ntion numbers L (bottom) in inference. We find a accuracy-fidelity\nsweep spot around s = 4, meanwhile 10 iterations (L = 10) for\nmasked decoding yield sufficiently good results.\namong all codebook entries to enhance VQ performance.\nOur residual design shows clearly superior performance\nwhen comparing with these single VQ-based approaches.\nAblation.\nIn the ablation experiments, we observe that\nboth residual quantization (RQ) and quantization dropout\n(QDropout) effectively contribute to the enhancement of\nmotion quality in terms of both reconstruction and gener-\nation. Additionally, replacing-and-remasking strategy, as\nwell as RQ, facilitates more faithful motion generation.\nNumber of Residual Layers (V ). In Tab. 2, we inves-\ntigate RVQ with different numbers of quantization layers.\nGenerally, more residual VQ layers result in more precise\nreconstruction, but they also increase the burden on the R-\nTransformer for residual token generation. We particularly\nobserve that the generation performance starts to degrade\nwith more than 5 residual layers. This finding emphasizes\nthe importance of striking a balance in the number of resid-\nual layers for optimal performance.\nQuantization Dropout (q). We also analyze the impact\nof quantization dropout ratio q in Tab. 2. As we increase\ndropout probability from 0.2, the performance gains be-\ncome marginal, or even converse. We speculate that fre-\nquent disabling quantization layers may disturb the learning\nof quantization models.\nInference Hyper-parameters. The CFG scale s and the\nnumber of iterations L are two crucial hyperparameters dur-\ning the inference of masked modeling. In Fig. 7, we present\nthe performance curves of FID and multimodality distance\nby sweeping over different values of s and L. Several key\nobservations emerge. Firstly, an optimal guidance scale s\nfor M-Transformer inference is identified around s = 4.\nOver-guided decoding may even inversely deteriorate the\nperformance. Secondly, more iterations are not necessarily\nbetter. As L increases, the FID and multimodality distance\nconverge to the minima quickly, typically within around 10\niterations. Beyond 10 iterations, there are no further perfor-\nmance gains in both FID and multimodal distance. In this\nregard, our MoMask requires fewer inference steps com-\npared to most autoregressive and diffusion models.\n4.3. Application: Temporal Inpainting\nIn Fig. 6, we showcase the capability of MoMask in tem-\nporally inpainting a specific region in a motion sequence.\nThe region can be freely located in the middle, suffix, or\nprefix. Specifically, we mask out all the tokens in the re-\ngion of interest and then follow the same inference proce-\ndure described in Sec. 3.4. For both tasks, our approach\ngenerates smooth motions in coherence with the given text\ndescriptions.\nAdditionally, we conduct a user study to\nquantitatively compare our inpainting results with those of\nMDM [42]. In this study, 40 samples are generated from\nboth methods using the same motion and text input, and\npresented to users side-by-side. With 6 users involved, 68%\nof the results from MoMask are preferred over MDM.\n5. Discussion and Conclusion\nLimitations. We acknowledge certain limitations of Mo-\nMask. Firstly, while MoMask excels in fidelity and faith-\n8\nfulness for text-to-motion synthesis, its diversity is rela-\ntively limited. We plan to delve into the underlying causes\nof this limitation in future work. Secondly, MoMask re-\nquires the target length as input for motion generation. This\ncould be properly addressed by applying the text2length\nsampling [15] beforehand. Thirdly, akin to most VQ-based\nmethods, MoMask may face challenges when generating\nmotions with fast-changing root motions, such as spinning.\nExemplar cases are presented in the supplementary videos.\nIn conclusion, we introduce MoMask, a novel generative\nmasked modeling framework for text-driven 3D human\nmotion generation.\nMoMask features three advanced\ntechniques:\nresidual quantization for precise motion\nquantization,\nmasked transformer and residual trans-\nformer for high-quality and faithful motion generation.\nMoMask is efficient and flexible, achieving superior per-\nformance without extra inference burden, and effortlessly\nsupporting temporal motion inpainting in multiple contexts.\nReferences\n[1] Chaitanya Ahuja and Louis-Philippe Morency.\nLan-\nguage2pose: Natural language grounded pose forecasting.\nIn 2019 International Conference on 3D Vision (3DV), pages\n719\u2013728. IEEE, 2019. 2\n[2] Andreas Aristidou, Daniel Cohen-Or, Jessica K Hodgins,\nYiorgos Chrysanthou, and Ariel Shamir. Deep motifs and\nmotion signatures. ACM Transactions on Graphics (TOG),\n37(6):1\u201313, 2018. 3\n[3] Nikos Athanasiou, Mathis Petrovich, Michael J Black, and\nG\u00a8ul Varol. Teach: Temporal action composition for 3d hu-\nmans. In 2022 International Conference on 3D Vision (3DV),\npages 414\u2013423. IEEE, 2022. 2\n[4] Zal\u00b4an Borsos, Rapha\u00a8el Marinier, Damien Vincent, Eugene\nKharitonov, Olivier Pietquin, Matt Sharifi, Dominik Roblek,\nOlivier Teboul, David Grangier, Marco Tagliasacchi, et al.\nAudiolm: a language modeling approach to audio genera-\ntion. IEEE/ACM Transactions on Audio, Speech, and Lan-\nguage Processing, 2023. 2, 3\n[5] Haoye Cai, Chunyan Bai, Yu-Wing Tai, and Chi-Keung\nTang. Deep video generation, prediction and completion of\nhuman action sequences. In Proceedings of the European\nConference on Computer Vision (ECCV), pages 366\u2013382,\n2018. 2\n[6] Pablo Cervantes, Yusuke Sekikawa, Ikuro Sato, and Koichi\nShinoda. Implicit neural representations for variable length\nhuman motion generation. In European Conference on Com-\nputer Vision, pages 356\u2013372. Springer, 2022. 2\n[7] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T\nFreeman. Maskgit: Masked generative image transformer.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 11315\u201311325, 2022.\n2, 4\n[8] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot,\nJose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Mur-\nphy, William T Freeman, Michael Rubinstein, et al. Muse:\nText-to-image generation via masked generative transform-\ners. arXiv preprint arXiv:2301.00704, 2023. 2, 4, 5\n[9] Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao\nChen, and Gang Yu. Executing your commands via motion\ndiffusion in latent space. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 18000\u201318010, 2023. 1, 2, 5, 6, 7\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova.\nBert:\nPre-training of deep bidirectional\ntransformers for language understanding.\narXiv preprint\narXiv:1810.04805, 2018. 2, 4\n[11] Sohrab Ferdowsi,\nSlava Voloshynovskiy,\nand Dimche\nKostadinov.\nRegularized residual quantization: a multi-\nlayer sparse dictionary learning approach.\narXiv preprint\narXiv:1705.00522, 2017. 3\n[12] Anindita Ghosh, Noshaba Cheema, Cennet Oguz, Christian\nTheobalt, and Philipp Slusallek. Synthesis of compositional\nanimations from textual descriptions. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 1396\u20131406, 2021. 2\n[13] Kehong Gong, Dongze Lian, Heng Chang, Chuan Guo,\nXinxin Zuo, Zihang Jiang, and Xinchao Wang. Tm2d: Bi-\nmodality driven 3d dance generation via music-text integra-\ntion. arXiv preprint arXiv:2304.02419, 2023. 1, 2\n[14] Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao\nSun, Annan Deng, Minglun Gong, and Li Cheng.\nAc-\ntion2motion: Conditioned generation of 3d human motions.\nIn Proceedings of the 28th ACM International Conference on\nMultimedia, pages 2021\u20132029, 2020. 2, 5\n[15] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji,\nXingyu Li, and Li Cheng. Generating diverse and natural 3d\nhuman motions from text. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 5152\u20135161, 2022. 1, 2, 5, 6, 9\n[16] Chuan Guo, Xinxin Zuo, Sen Wang, and Li Cheng. Tm2t:\nStochastic and tokenized modeling for the reciprocal genera-\ntion of 3d human motions and texts. In European Conference\non Computer Vision, pages 580\u2013597. Springer, 2022. 1, 2, 3,\n5, 6, 7\n[17] Chuan Guo, Xinxin Zuo, Sen Wang, Xinshuang Liu, Shihao\nZou, Minglun Gong, and Li Cheng. Action2video: Gener-\nating videos of human 3d actions. International Journal of\nComputer Vision, 130(2):285\u2013315, 2022. 2\n[18] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll\u00b4ar, and Ross Girshick. Masked autoencoders are scalable\nvision learners. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 16000\u2013\n16009, 2022. 2\n[19] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 5\n[20] Ruozi Huang, Huang Hu, Wei Wu, Kei Sawada, Mi Zhang,\nand Daxin Jiang. Dance revolution: Long-term dance gen-\neration with music via curriculum learning. arXiv preprint\narXiv:2006.06119, 2020. 2\n[21] Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and\nTao Chen. Motiongpt: Human motion as a foreign language.\narXiv preprint arXiv:2306.14795, 2023. 1, 2, 3\n9\n[22] Jihoon Kim, Jiseob Kim, and Sungjoon Choi. Flame: Free-\nform language-based motion synthesis & editing. In Pro-\nceedings of the AAAI Conference on Artificial Intelligence,\npages 8255\u20138263, 2023. 2\n[23] Hanyang Kong, Kehong Gong, Dongze Lian, Michael Bi Mi,\nand Xinchao Wang. Priority-centric human motion genera-\ntion in discrete latent space. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 14806\u2013\n14816, 2023. 1, 2, 7\n[24] Tianhong Li, Huiwen Chang, Shlok Mishra, Han Zhang,\nDina Katabi, and Dilip Krishnan. Mage: Masked generative\nencoder to unify representation learning and image synthe-\nsis. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 2142\u20132152,\n2023. 2\n[25] Yue Li, Wenrui Ding, Chunlei Liu, Baochang Zhang, and\nGuodong Guo. Trq: Ternary neural networks with residual\nquantization. In Proceedings of the AAAI Conference on Ar-\ntificial Intelligence, pages 8538\u20138546, 2021. 3\n[26] Zefan Li, Bingbing Ni, Wenjun Zhang, Xiaokang Yang, and\nWen Gao. Performance guaranteed network acceleration via\nhigh-order residual quantization. In Proceedings of the IEEE\nInternational Conference on Computer Vision, pages 2584\u2013\n2592, 2017. 3\n[27] Angela S Lin, Lemeng Wu, Rodolfo Corona, Kevin Tai, Qix-\ning Huang, and Raymond J Mooney. Generating animated\nvideos of human activities from natural language descrip-\ntions. Learning, 2018(1), 2018. 2\n[28] Junfan Lin, Jianlong Chang, Lingbo Liu, Guanbin Li, Liang\nLin, Qi Tian, and Chang-wen Chen.\nBeing comes from\nnot-being: Open-vocabulary text-to-motion generation with\nwordless training. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n23222\u201323231, 2023. 2\n[29] Zhenguang Liu, Shuang Wu, Shuyuan Jin, Shouling Ji, Qi\nLiu, Shijian Lu, and Li Cheng. Investigating pose represen-\ntations and motion contexts modeling for 3d motion predic-\ntion. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 45(1):681\u2013697, 2022. 2\n[30] Yunhong Lou, Linchao Zhu, Yaxiong Wang, Xiaohan Wang,\nand Yi Yang.\nDiversemotion:\nTowards diverse human\nmotion generation via discrete diffusion.\narXiv preprint\narXiv:2309.01372, 2023. 2\n[31] Thomas Lucas, Fabien Baradel, Philippe Weinzaepfel, and\nGr\u00b4egory Rogez. Posegpt: Quantization-based 3d human mo-\ntion generation and forecasting. In European Conference on\nComputer Vision, pages 417\u2013435. Springer, 2022. 2\n[32] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Ger-\nard Pons-Moll, and Michael J Black. Amass: Archive of\nmotion capture as surface shapes.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 5442\u20135451, 2019. 5\n[33] Wei Mao, Miaomiao Liu, Mathieu Salzmann, and Hongdong\nLi. Learning trajectory dependencies for human motion pre-\ndiction. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 9489\u20139497, 2019. 2\n[34] Julieta Martinez, Holger H Hoos, and James J Little. Stacked\nquantizers for compositional vector compression.\narXiv\npreprint arXiv:1411.2173, 2014. 2, 3\n[35] Mathis Petrovich, Michael J Black, and G\u00a8ul Varol. Action-\nconditioned 3d human motion synthesis with transformer\nvae. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 10985\u201310995, 2021. 2\n[36] Mathis Petrovich, Michael J Black, and G\u00a8ul Varol. Temos:\nGenerating diverse human motions from textual descriptions.\nIn European Conference on Computer Vision, pages 480\u2013\n497. Springer, 2022. 1, 2\n[37] Matthias Plappert, Christian Mandery, and Tamim Asfour.\nThe kit motion-language dataset. Big data, 4(4):236\u2013252,\n2016. 5\n[38] Matthias Plappert, Christian Mandery, and Tamim Asfour.\nLearning a bidirectional mapping between human whole-\nbody motion and natural language using deep recurrent neu-\nral networks. Robotics and Autonomous Systems, 109:13\u201326,\n2018. 2\n[39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In International Conference on Machine Learning,\npages 8748\u20138763. PMLR, 2021. 2, 4\n[40] Li Siyao, Weijiang Yu, Tianpei Gu, Chunze Lin, Quan Wang,\nChen Qian, Chen Change Loy, and Ziwei Liu. Bailando:\n3d dance generation by actor-critic gpt with choreographic\nmemory. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 11050\u2013\n11059, 2022. 2\n[41] Guy Tevet, Brian Gordon, Amir Hertz, Amit H Bermano,\nand Daniel Cohen-Or. Motionclip: Exposing human motion\ngeneration to clip space. In European Conference on Com-\nputer Vision, pages 358\u2013374. Springer, 2022. 2\n[42] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir,\nDaniel Cohen-Or, and Amit H Bermano. Human motion dif-\nfusion model. arXiv preprint arXiv:2209.14916, 2022. 1, 2,\n5, 6, 7, 8\n[43] Jonathan Tseng, Rodrigo Castellon, and Karen Liu. Edge:\nEditable dance generation from music. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 448\u2013458, 2023. 2\n[44] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete\nrepresentation learning.\nAdvances in Neural Information\nProcessing Systems, 30, 2017. 2, 3, 4\n[45] Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang,\nLong Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huam-\ning Wang, Jinyu Li, et al. Neural codec language models\nare zero-shot text to speech synthesizers.\narXiv preprint\narXiv:2301.02111, 2023. 3\n[46] Zhenyi Wang, Ping Yu, Yang Zhao, Ruiyi Zhang, Yufan\nZhou, Junsong Yuan, and Changyou Chen. Learning diverse\nstochastic human-action generators by learning smooth la-\ntent transitions. In Proceedings of the AAAI Conference on\nArtificial Intelligence, pages 12281\u201312288, 2020. 2\n[47] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jos\u00b4e Lezama, Han\nZhang, Huiwen Chang, Alexander G Hauptmann, Ming-\nHsuan Yang, Yuan Hao, Irfan Essa, et al.\nMagvit:\n10\nMasked generative video transformer.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10459\u201310469, 2023. 2\n[48] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan\nSkoglund, and Marco Tagliasacchi. Soundstream: An end-\nto-end neural audio codec. IEEE/ACM Transactions on Au-\ndio, Speech, and Language Processing, 30:495\u2013507, 2021.\n2, 3\n[49] Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli\nHuang, Yong Zhang, Hongwei Zhao, Hongtao Lu, and Xi\nShen.\nT2m-gpt: Generating human motion from textual\ndescriptions with discrete representations.\narXiv preprint\narXiv:2301.06052, 2023. 1, 2, 3, 4, 5, 6, 7\n[50] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou\nHong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondif-\nfuse: Text-driven human motion generation with diffusion\nmodel. arXiv preprint arXiv:2208.15001, 2022. 1, 2, 5\n[51] Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai,\nFangzhou Hong, Huirong Li, Lei Yang, and Ziwei Liu.\nRemodiffuse: Retrieval-augmented motion diffusion model.\narXiv preprint arXiv:2304.01116, 2023. 5, 6\n[52] Yaqi Zhang, Di Huang, Bin Liu, Shixiang Tang, Yan Lu, Lu\nChen, Lei Bai, Qi Chu, Nenghai Yu, and Wanli Ouyang. Mo-\ntiongpt: Finetuned llms are general-purpose motion genera-\ntors. arXiv preprint arXiv:2306.10900, 2023. 2, 3\n[53] Zixiang Zhou and Baoyuan Wang.\nUde: A unified driv-\ning engine for human motion generation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5632\u20135641, 2023. 2\n11\n"
  },
  {
    "title": "DREAM: Diffusion Rectification and Estimation-Adaptive Models",
    "link": "https://arxiv.org/pdf/2312.00210.pdf",
    "upvote": "14",
    "text": "DREAM: Diffusion Rectification and Estimation-Adaptive Models\nJinxin Zhou1\u2217\nTianyu Ding2\u2217\u2020\nTianyi Chen2\nJiachen Jiang1\nIlya Zharkov2\nZhihui Zhu1\nLuming Liang2\u2020\n1Ohio State University\n2Microsoft\n{zhou.3820,jiang.2880,zhu.3440}@osu.edu, {tianyuding,tiachen,zharkov,lulian}@microsoft.com\n10k\n20k\n50k\n100k\n150k\n200k\n400k\n800k\nStandard\nDREAM\nTraining iterations\nTurning the top to the bottom by adding only three lines of code (line 4-6 in Algorithm 3).\nFigure 1. Comparative training of conditional diffusion models for super-resolution. Top: standard conditional DDPM [44]. Bottom:\nenhancing the same model training with just three additional lines of code, leaving the sampling process unchanged. DREAM facilitates\nnotably faster and more stable training convergence, significantly surpassing baseline models in key metrics of perception and distortion.\nAbstract\nWe present DREAM, a novel training framework rep-\nresenting Diffusion Rectification and Estimation-Adaptive\nModels, requiring minimal code changes (just three lines)\nyet significantly enhancing the alignment of training with\nsampling in diffusion models. DREAM features two compo-\nnents: diffusion rectification, which adjusts training to re-\nflect the sampling process, and estimation adaptation, which\nbalances perception against distortion. When applied to im-\nage super-resolution (SR), DREAM adeptly navigates the\ntradeoff between minimizing distortion and preserving high\nimage quality. Experiments demonstrate DREAM\u2019s superi-\nority over standard diffusion-based SR methods, showing a\n2 to 3\u00d7 faster training convergence and a 10 to 20\u00d7 reduc-\ntion in necessary sampling steps to achieve comparable or\nsuperior results. We hope DREAM will inspire a rethinking\nof diffusion model training paradigms. [Project page]\n1. Introduction\nSingle-image super-resolution (SISR) [3, 12, 50, 59] in-\nvolves generating high-resolution (HR) images from low-\n\u2217Equal contribution. \u2020Corresponding author.\nresolution (LR) counterparts, a process crucial in various\napplications including video surveillance, medical diagno-\nsis, and photography. SISR is challenging due to the diverse\nreal-world degradation patterns and the inherent ill-posed\nnature of the task, where different HR images can corre-\nspond to the same LR image.\nSISR methods are generally categorized into regression-\nbased and generation-based approaches. Regression-based\nmethods [7, 31, 34, 69] focus on minimizing pixel-level dis-\ncrepancies, i.e., distortion, between SR predictions and HR\nreferences. However, this approach often fails to capture the\nperceptual quality of images. To address this, generation-\nbased methods employ deep generative models, includ-\ning autoregressive models [40, 41], variational autoen-\ncoders (VAEs) [27, 53], normalizing flows (NFs) [11, 26],\nand generative adversarial networks (GANs) [16, 24, 33,\n42], aiming to improve the perceptual aspects of SR images.\nRecently, Diffusion Probabilistic Models (DPMs) [19,\n48], a novel class of generative models, have attracted in-\ncreased interest for their impressive generative abilities, es-\npecially in the SISR task [14, 20, 43, 44, 62]. Nonethe-\nless, DPM-based methods face challenges due to their de-\npendence on a long sampling chain, which can lead to error\naccumulation and reduce training and sampling efficiency.\nA further issue is the discrepancy between training and sam-\n1\narXiv:2312.00210v1  [cs.CV]  30 Nov 2023\npling [39, 61]: training typically involves denoising noisy\nimages conditioned on ground truth samples, whereas test-\ning (or sampling) conditions on previously self-generated\nresults. This disparity, inherent in the multi-step sampling\nprocess, tends to magnify with each step, thereby constrain-\ning the full potential of DPMs in practice.\nTo bridge the gap between training and sampling in\ndiffusion models, we introduce DREAM, an end-to-end\ntraining framework denoting Diffusion Rectification and\nEstimation-Adaptive Models. DREAM consists of two key\nelements: diffusion rectification and estimation adaptation.\nDiffusion rectification extends traditional diffusion training\nwith an extra forward pass, enabling the model to utilize its\nown predictions. This approach accounts for the discrep-\nancy between training (using ground-truth data) and sam-\npling (using model-generated estimates). However, solely\nrelying on this self-alignment can compromise perceptual\nquality for the sake of reducing distortion. To counter this,\nour estimation adaptation strategy balances standard diffu-\nsion and diffusion rectification by adaptively incorporating\nground-truth information. This approach smoothly transi-\ntions focus between the two by adaptively injecting ground-\ntruth information. This integration harmonizes the advan-\ntages of both approaches, effectively reducing the training-\nsampling discrepancy, as demonstrated in Figure 3.\nThe DREAM framework excels in its simplicity, easily\nintegrating into existing diffusion-based models with only\nthree lines of code and requiring no alterations to the net-\nwork architecture or sampling process. When applied to the\nSR task, DREAM has notably improved generation qual-\nity across various diffusion-based SR methods and datasets.\nFor example, on the 8\u00d7 CeleA-HQ dataset, it boosts the\nSR3 [44] method\u2019s PSNR from 23.85 dB to 24.63 dB while\nreducing the FID score from 61.98 to 56.01.\nAddition-\nally, DREAM accelerates training convergence by 2 to 3\ntimes and improves sampling efficiency, requiring 10 to 20\ntimes fewer steps for comparable or superior results.\nIt\nalso demonstrates enhanced out-of-distribution (OOD) SR\nresults compared to baseline methods.\nOur contributions are summarized as follows:\n\u2022 We introduce DREAM, a simple yet effective framework\nto alleviate the training-sampling discrepancy in standard\ndiffusion models, requiring minimal code modifications.\n\u2022 We demonstrate the application of DREAM to various\ndiffusion-based SR methods, resulting in significant im-\nprovements in distortion and perception metrics.\n\u2022 The proposed DREAM also notably speeds up training\nconvergence, enhances sampling efficiency, and delivers\nsuperior out-of-distribution (OOD) results.\n2. Related work\nSuper-resolution. In single-image super-resolution, sub-\nstantial efforts [2, 9, 10, 15, 22, 28, 33, 47, 63, 64, 68, 69]\nhave been devoted to two primary categories: regression-\nbased and generation-based.\nRegression-based methods,\nsuch as EDSR [34], RRDB [57], and SWinIR [31], focus\non a direct mapping from LR to HR images, employing\npixel-wise loss to minimize differences between SR im-\nages and their HR references.\nWhile effective in reduc-\ning distortion, these methods often yield overly smooth,\nblurry images.\nGeneration-based methods, on the other\nhand, aim to produce more realistic SR images.\nGAN-\nbased models, like SRGAN [28], combine adversarial and\nperceptual losses [65] to enhance visual quality.\nMeth-\nods of this line include SFTGAN [56] and GLEAN [5],\nwhich integrate semantic information to improve texture re-\nalism. ESRGAN [57] further refines SRGAN\u2019s architec-\nture and loss function. However, GAN-based methods of-\nten face challenges like complex regularization and opti-\nmization to avoid instability. Autoregressive models (e.g.,\nPixel-CNN [54], Pixel-RNN [41], VQVAE [55], and LAR-\nSR [17]) are computationally intensive and less practical for\nHR image generation. Normalizing Flows (NFs) [11, 26]\nand VAEs [27, 53] also contribute to the field, but these\nmethods sometimes struggle to produce satisfactory results.\nDiffusion model. Inspired by non-equilibrium statisti-\ncal physics, [48] first proposes Diffusion Probabilistic Mod-\nels (DPMs) to learn complex distributions. These models\nhave since advanced significantly [8, 19, 37, 49], achiev-\ning state-of-the-art results in image synthesis. Beyond gen-\neral image generation, diffusion models have shown re-\nmarkable utility in low-level vision tasks, particularly in\nSR. Notable examples include SR3 [44], which excels in\nimage super-resolution through iterative refinement, and\nIDM [14], which blends DPMs with explicit image repre-\nsentations to enable flexible generation across various res-\nolutions.\nSRDiff [29] uniquely focuses on learning the\nresidual distribution between HR and LR images through\ndiffusion processes. LDM [43] deviates from traditional\npixel space approaches, employing cross-attention condi-\ntioning for diffusion in latent space. Building upon LDM,\nResShift [62] employs a refined transition kernel for se-\nquentially transitioning the residual from LR embeddings\nto their HR counterparts.\nTraining-sampling discrepancy.\n[39] first analyzes\nthe training-sampling discrepancy in unconditional diffu-\nsion models, proposing to represent estimation errors with\na Gaussian distribution for improved DPM training. This\ndiscrepancy was later attributed by [61] to a constant train-\ning weight strategy, suggesting a reweighted objective func-\ntion based on the signal-to-noise ratio at different diffusion\nsteps. In addition, [30] adjusts the distribution during the\nsampling process by choosing the optimal step within a pre-\ndefined windows for denoising at each stage. [38] applies\na predefined linear function to adjust noise variance during\nsampling, and [13] recommends starting the sampling from\n2\nAlgorithm 1 Conditional DDPM Training\n1: repeat\n2:\n(x0, y0) \u223c p(x0, y0), t \u223c U(1, T), \u03f5t \u223c N(0, I)\n3:\nCompute yt = \u221a\u00af\u03b1ty0 + \u221a1 \u2212 \u00af\u03b1t\u03f5t\n4:\nUpdate \u03b8 with gradient \u2207\u03b8||\u03f5t \u2212 \u03f5\u03b8(x0, yt, t)||1\n5: until converged\nan approximate distribution that mirrors the training process\nin terms of frequency and pixel space.\nOur approach, distinct from previous unconditional\nmethods, addresses discrepancies based on predictions rel-\native to the conditional input data, ensuring a tailored\nand accurate solution for complex visual prediction tasks\nlike SISR. Our method also draws inspiration from step-\nunrolling techniques in depth estimation [21, 46] and text\ngeneration [45], leveraging the model\u2019s own predictions\nfor error estimation. However, we uniquely integrate self-\nestimation with adaptive incorporation of ground-truth data.\nThis integration, guided by the pattern of estimation errors,\neffectively balances perceptual quality and distortion, en-\nhancing generated image qualities.\n3. Method\n3.1. Preliminaries\nThe goal of SR is to recover a high-resolution (HR) image\nfrom its low-resolution (LR) counterpart. This task is rec-\nognized as ill-posed due to its one-to-many nature [44, 62],\nand is further complicated by various degradation models\nin real-world scenarios. Notably, diffusion models [19, 48]\nhave emerged as powerful generative models, showcas-\ning strong capabilities in image generation tasks. Follow-\ning [44], we address the SR challenge by adapting a con-\nditional denoising diffusion probabilistic (DDPM) model.\nThis adaptation, conditioned on the LR image, sets it apart\nfrom traditional, unconditional models which are primarily\ndesigned for unconstrained image generation.\nWe denote the LR and HR image pair as (x0, y0). A\nconditional DDPM model involves a Markov chain, encom-\npassing a forward process that traverses the chain, adding\nnoise to y0, and a reverse process, which conducts reverse\nsampling from the chain for denosing from pure Gaussain\nnoise to the HR image y0, conditioned on the LR image x0.\nForward process. The forward process, also referred to\nas the diffusion process, takes a sample y0 and simulates\nthe non-equilibrium thermodynamic diffusion process [48].\nIt gradually adds Gaussian noise to y0 via a fixed Markov\nchain of length T:\nq(yt|yt\u22121) = N(yt;\np\n1 \u2212 \u03b2tyt\u22121, \u03b2tI),\n(1)\nq(y1:T |y0) =\nT\nY\nt=1\nq(yt|yt\u22121),\n(2)\nAlgorithm 2 Conditional DDPM Sampling\n1: yT \u223c N(0, I)\n2: for t = T \u00b7 \u00b7 \u00b7 1 do\n3:\nz \u223c N(0, I) if t > 1 else z = 0\n4:\nyt\u22121 =\n1\n\u221a\u03b1t (yt \u2212\n1\u2212\u03b1t\n\u221a1\u2212\u00af\u03b1t \u03f5\u03b8(x0, yt, t)) + \u03c3tz\n5: end for\n6: return y0\nwhere {\u03b2t \u2208 (0, 1)}T\nt=1 is the variance scheduler. As the\nstep t increases, the signal y0 gradually loses its distinguish-\nable features. Ultimately, as t \u2192 \u221e, yt converges to an\nisotropic Gaussian distribution. Moreover, we can derive\nthe distribution for sampling at arbitrary step t from y0:\nq (yt|y0) = N\n\u0000yt; \u221a\u00af\u03b1ty0, (1 \u2212 \u00af\u03b1t) I\n\u0001\n.\n(3)\nwhere \u00af\u03b1t = Qt\ni=1 \u03b1i and \u03b1t = 1 \u2212 \u03b2t.\nReverse process. The reverse process, also referred to\nas the denosing process, learns the conditional distributions\np\u03b8(yt\u22121|yt, x0) for denoising from Gaussian noise to y0\nconditioned on x0, through a reverse Markovian process:\np\u03b8(yt\u22121|yt, x0) = N(yt\u22121; \u00b5\u03b8(x0, yt, t), \u03c32\nt I),\n(4)\np\u03b8(y0:T |x0) = p(yT )\nT\nY\nt=1\np\u03b8(yt\u22121|yt, x0),\n(5)\nwhere \u03c3t is a predetermined term related to \u03b2t [19].\nTraining. We train a denoising network \u03f5\u03b8(x0, yt, t) to\npredict the noise vector \u03f5t added at step t. Following [19,\n44], the training objective can be expressed as:\nL(\u03b8) = E(x0,y0),\u03f5t,t \u2225\u03f5t \u2212 \u03f5\u03b8(x0, yt, t)\u22251 .\n(6)\nWith Eq. (3), we parameterize yt = \u221a\u00af\u03b1ty0 + \u221a1 \u2212 \u00af\u03b1t\u03f5t,\nand summarize the training process in Algorithm 1.\nSampling. In essence, the training minimizes the di-\nvergence between the forward posterior q (yt\u22121|yt, y0) and\np\u03b8 (yt\u22121|yt, x0), and the mean \u00b5\u03b8(x0, yt, t) in Eq. (4) is\nparameterized [44] to match the mean of q (yt\u22121|yt, y0):\n\u00b5\u03b8(x0, yt, t) =\n1\n\u221a\u03b1t (yt \u2212 1 \u2212 \u03b1t\n\u221a1 \u2212 \u00af\u03b1t\n\u03f5\u03b8(x0, yt, t)).\n(7)\nTo sample y0 \u223c p\u03b8(y0|x0), starting from yT \u223c N(0, I),\nwe reverse the Markovian process by iteratively sampling\nyt\u22121 \u223c p(yt\u22121|yt, x0) based on Eqs. (4) and (7), which\ncompletes the sampling process, as shown in Algorithm 2.\n3.2. Challenge: training-sampling discrepancy\nTraining diffusion models for SR presents a critical chal-\nlenge, stemming from a discrepancy between the training\nand inference phases, which we term as training-sampling\ndiscrepancy. During the training phase, the model oper-\nates on actual data, wherein the noisy image yt at diffu-\nsion step t is derived from the ground-truth HR image y0\nas per line 3 in Algorithm 1. However, during the inference\n3\nHR\nnoisy image\ncondition LR\nadd \nnoise\nnetwork\ndenoised estimated HR\n\ud835\udc61\n1 \u2212 \ud835\udf06!\n\ud835\udf06!\n+\nblending\nDiffusion Rectification\nAdaptive Estimation\nFigure 2. Overview of the DREAM framework. Starting with ground-truth HR images, a standard diffusion process with a frozen\ndenoiser network generates denoised HR estimates. The Adaptive Estimation merges these estimated HR images with the original HR\nimages, guided by the pattern of estimation errors. The Diffusion Rectification constructs the noisy images from this merged HR images,\nwhich are then fed into the denoiser network (now unfrozen). Similar to DDPM [19], the denoiser network is trained to eliminate both the\nintroduced Gaussian noise and errors arising from the training-sampling discrepancy, as detailed in Eq. (14).\nphase, the ground truth y0 is unavailable. As outlined in\nline 4 in Algorithm 2, the model now operates on predicted\ndata, where yt is obtained from the preceding sampling step\nt + 1. Due to the estimation error, the noisy image yt con-\nstructed in these two processes usually differs, giving rise\nto the training-sampling discrepancy.\nTo better illustrate the discrepancy, we conduct an exper-\niment utilizing a pre-trained SR3 model [44], denoted by\n\u03f5\u03b8, adhering to the standard diffusion training framework.\nThe goal is to understand the implications for HR signal y0\nreconstruction under two distinct scenarios:\n\u2022 \u201cTraining\u201d. Simulating the training process, we assume\naccess to the ground-truth y0, and construct the noisy im-\nage at time step t as per line 3 in Algorithm 1, denoting\nthe image as ytrain\nt\n.\n\u2022 \u201cSampling\u201d. Simulating the sampling process, we assume\nno access to y0 and iteratively construct the noisy image\nat each time step t by sampling from the previous step, as\nper line 4 in Algorithm 2. The noisy image thus obtained\nis denoted by ysample\nt\n.\nTo retrieve the HR image y0 from the noisy image in\nboth scenarios, we utilize Eq. (3) and the pre-trained net-\nwork \u03f5\u03b8 to compute the predicted HR signal as follows:\ney0 =\n1\n\u221a\u00af\u03b1t\n\u0000yt \u2212\n\u221a\n1 \u2212 \u00af\u03b1t\u03f5\u03b8 (x0, yt, t)\n\u0001\n=: h\u03b8(yt).\n(8)\nFollowing this, we compute eytrain\n0\n=\nh\u03b8(ytrain\nt\n) and\neysample\n0\n= h\u03b8(ysample\nt\n) as the predicted HR images in the\n\u201ctraining\u201d and \u201csampling\u201d scenarios, respectively. For per-\nformance evaluation, we take 100 samples from FFHQ [25]\nand calculate the averaged MSE and LPIPS [65] metrics\nbetween the predicted HR images and the ground-truth y0\nacross various time step t under the defined settings.\nWe present the findings in Figure 3a, where both MSE\nand LPIPS exhibit a decline with a smaller t, as expected,\nsince the network can reconstruct more accurate HR sig-\n0\n500\n1000\n1500\n2000\nTime step t\n0.0\n0.2\n0.4\n0.6\n0.8\nMSE - \"Training\"\nMSE - \"Sampling\"\nLPIPS - \"Training\"\nLPIPS - \"Sampling\"\n(a) Standard diffusion\n0\n500\n1000\n1500\n2000\nTime step t\n0.0\n0.2\n0.4\n0.6\n0.8\nMSE - \"Training\"\nMSE - \"Sampling\"\nLPIPS - \"Training\"\nLPIPS - \"Sampling\"\n(b) DREAM\nFigure 3. Evaluation of training-sampling discrepancy and its al-\nleviation through our DREAM framework. The mean curve over\n100 samples at each time step t is plotted, with the shaded area rep-\nresenting the standard deviation of each metric. Here, T = 2000.\nnal from less noisy input. Importantly, discernible dispari-\nties are observed between the curves representing the \u201ctrain-\ning\u201d and \u201csampling\u201d settings\u2014the \u201ctraining\u201d curves consis-\ntently exhibit lower error compared to the \u201csampling\u201d ones,\nsuggesting the advantage of having access to the ground-\ntruth y0 for improved prediction accuracy. In contrast, Fig-\nure 3b illuminates a remarkable alleviation in this discrep-\nancy when employing our DREAM framework to train the\nidentical SR3 architecture: the \u201csampling\u201d curve closely\naligns with the \u201ctraining\u201d curve, despite the lack of ac-\ncess to the ground-truth y0, across both MSE and LPIPS\nmetrics. This underscores the efficacy of our approach in\nbridging the training-sampling discrepancy and thereby fa-\ncilitating more accurate predictions.\n3.3. The DREAM framework\nWe now present our DREAM framework (see Figure 2), an\nend-to-end training strategy designed to bridge the gap be-\ntween training and sampling in diffusion models. It consists\nof two core components: diffusion rectification and estima-\ntion adaptation, which we elaborate as follows.\n4\n(a) LR\n(b) Standard\n(c) DRM\n(e) HR\n(d) DREAM\nPSNR: 23.85\nFID: 61.98\nPSNR: 25.04\nFID: 77.51\nPSNR: 24.63\nFID: 56.01\n16 \u00d7 16\n128 \u00d7 128\nFigure 4. 8\u00d7 SR on the CelebA-HQ dataset [23].\nDiffusion rectification. The goal of diffusion rectifica-\ntion is to modify the behavior of the diffusion training to\naccount for the training-sampling discrepancy, which arises\nfrom the manner in which we construct the intermediate\nsignals\u2014either from the ground-truth or from the model\u2019s\nown estimation. Hence, we extend the diffusion training\nframework to align more closely with the sampling process,\nenabling the model to utilize its own output for prediction.\nSpecifically, during training, upon acquiring ytrain\nt\nas per\nline 3 in Algorithm 1, we refrain from directly minimizing\nL(\u03b8). Instead, we construct our own prediction of the HR\nimage as eytrain\n0\naccording to Eq. (8), formulated as:\neytrain\n0\n=\n1\n\u221a\u00af\u03b1t\n\u0000ytrain\nt\n\u2212\n\u221a\n1 \u2212 \u00af\u03b1t\u03f5\u03b8(x0, ytrain\nt\n, t)\n\u0001\n=\n1\n\u221a\u00af\u03b1t\n\u0000\u221a\u00af\u03b1ty0 +\n\u221a\n1 \u2212 \u00af\u03b1t\u03f5t\n\u25b7 line 3\n\u2212\n\u221a\n1 \u2212 \u00af\u03b1t\u03f5\u03b8(x0, ytrain\nt\n, t)\n\u0001\n= y0 +\np\n(1 \u2212 \u00af\u03b1t)/\u00af\u03b1t\u2206\u03f5t,\u03b8\n(9)\nwhere \u2206\u03f5t,\u03b8 = \u03f5t \u2212 \u03f5\u03b8(x0, ytrain\nt\n, t). Utilizing this self-\nestimated HR image eytrain\n0\n, we generate the noisy image\neytrain\nt\nto serve as input1 to the network \u03f5\u03b8 once more:\neytrain\nt\n= \u221a\u00af\u03b1t eytrain\n0\n+\n\u221a\n1 \u2212 \u00af\u03b1t\u03f5\u2032\nt\n= \u221a\u00af\u03b1ty0 +\n\u221a\n1 \u2212 \u00af\u03b1t(\u03f5\u2032\nt + \u2206\u03f5t,\u03b8),\n(10)\nwhere \u03f5\u2032\nt \u223c N(0, I). Then, the training objective for this\ndiffusion rectification model (DRM) can be expressed as:\nLDRM(\u03b8) = E(x0,y0),\u03f5t,\u03f5\u2032\nt,t\n\r\r\r\n\u0000\u03f5\u2032\nt + \u2206\u03f5t,\u03b8\n\u0001\n\u2212 \u03f5\u03b8(x0, eytrain\nt\n, t)\n\r\r\r\n1 .\n(11)\nEssentially, Eq. (11) suggests that this DRM approach\nstrives not only to eliminate the sampled noise \u03f5\u2032\nt but also\nto address the error term \u2206\u03f5t,\u03b8 arising from the discrepancy\nbetween the imperfect estimation eytrain\n0\nand the ground-truth\ny0, as seen in Eq. (9); hence the term \u201crectification\u201d. No-\ntably, leveraging the model\u2019s own prediction during training\nas in Eq. (10) mirrors the sampling process of DDIM [49]\nwith a particular choice of \u03c3t, thereby imposing enhanced\n1To match the actual sampling process, there might be a desire to re-\nconstruct eytrain\nt\u22121, yet this could notably complicate the entire procedure.\nNonetheless, we have observed similar performance by simply using eytrain\nt\n.\nAlgorithm 3 Conditional DREAM Training\n1: repeat\n2:\n(x0, y0) \u223c p(x0, y0), t \u223c U(1, T), \u03f5t \u223c N(0, I)\n3:\nCompute yt = \u221a\u00af\u03b1ty0 + \u221a1 \u2212 \u00af\u03b1t\u03f5t\n4:\nCompute \u2206\u03f5t,\u03b8 = \u03f5t \u2212 StopGradient(\u03f5\u03b8(x0, yt, t))\n5:\nCompute byt = yt + \u221a1 \u2212 \u00af\u03b1t\u03bbt\u2206\u03f5t,\u03b8\n6:\nUpdate \u03b8 with gradient \u2207\u03b8||\u03f5t+\u03bbt\u2206\u03f5t,\u03b8 \u2212\u03f5\u03b8(x0, byt, t)||1\n7: until converged\nsupervision. We remark that DRM is closely related to the\napproaches in [21, 45, 46] where they perform similar step-\nunrolling techniques for perceptual vision tasks or text gen-\neration tasks. However, we are the first to tailor it to low-\nlevel vision tasks and provide a clear analysis.\nEstimation adaptation. While DRM incorporates addi-\ntional rectification supervision to account for the sampling\nprocess, its naive application to the SR task might not de-\nliver satisfactory results. As shown in Figure 4, a distortion-\nperception tradeoff [4] is observed in the generated SR im-\nages. Despite achieving a state-of-the-art PSNR (less dis-\ntortion), the images produced by DRM tend to be smoother\nand lack fine details, reflecting a high FID score (poor per-\nception).\nThis is particularly evident when compared to\nthe standard conditional diffusion model, namely SR3 [44].\nThis limitation could be traced back to DRM\u2019s static self-\nalignment mechanism, which may inappropriately guide the\ngenerated images to regress towards the mean.\nTo address the issue, and inspired by the powerful gener-\native capability of the standard diffusion model, we propose\nan estimation adaptation strategy. This aims to harness both\nthe superior quality of standard diffusion and the reduced\ndistortion offered by diffusion rectification.\nSpecifically,\nrather than naively using our own prediction eytrain\n0\ncomputed\nin Eq. (9), we adaptively inject ground-truth information y0\nby blending it with eytrain\n0\nas follows:\nby0 = \u03bbt eytrain\n0\n+ (1 \u2212 \u03bbt)y0,\n(12)\nwhere \u03bbt \u2208 (0, 1) is an increasing function such that by0\nemphasizes more on y0 at smaller t, aligning with the net-\nwork\u2019s tendency to achieve more accurate predictions, as\nobserved in Figure 3. Intuitively, as t decreases, by0 closely\napproximates the ground-truth, making it more beneficial\nto resemble the standard diffusion, yielding images with re-\nalistic details. Conversely, as t increases and the predic-\ntion leans towards random noise, it is advantageous to fo-\ncus more on the estimation itself, effectively aligning the\ntraining and sampling processes through the rectification.\nFollowing the adaptive estimation by0 in Eq. (12), we\nconstruct the new noisy image byt similarly as before:\nbyt = \u221a\u00af\u03b1t by0 +\n\u221a\n1 \u2212 \u00af\u03b1t\u03f5\u2032\nt\n= \u221a\u00af\u03b1ty0 +\n\u221a\n1 \u2212 \u00af\u03b1t(\u03f5\u2032\nt + \u03bbt\u2206\u03f5t,\u03b8).\n(13)\n5\nTable 1. Comparison on face and general scene datasets against three baselines for various p values, with best and second-best colorized.\np\nCelebA-HQ [23]\nDIV2K [1]\nSR3 [44]\nIDM [14]\nSR3 [44]\nResShift [62]\nPSNR\u2191 SSIM\u2191 LPIPS\u2193 FID\u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193 FID\u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193 FID\u2193 PSNR\u2191 SSIM\u2191 LPIPS\u2193 FID\u2193\n0 (DRM)\n25.04\n0.76\n0.204\n77.51\n25.06\n0.76\n0.188\n67.46\n28.67\n0.81\n0.189\n16.72\n29.98\n0.83\n0.233\n17.76\n1 (DREAM)\n24.63\n0.74\n0.177\n56.01\n24.50\n0.73\n0.167\n53.22\n28.10\n0.79\n0.121\n14.32\n29.24\n0.80\n0.158\n16.23\n2 (DREAM)\n24.62\n0.74\n0.180\n61.72\n24.32\n0.72\n0.169\n55.38\n28.06\n0.79\n0.140\n15.54\n28.77\n0.79\n0.134\n15.72\n3 (DREAM)\n24.15\n0.71\n0.182\n58.89\n24.09\n0.72\n0.172\n54.04\n27.88\n0.79\n0.123\n14.83\n28.44\n0.79\n0.124\n15.67\n\u221e (standard)\n23.85\n0.71\n0.184\n61.98\n24.01\n0.71\n0.172\n56.01\n27.02\n0.76\n0.121\n16.72\n25.30\n0.68\n0.211\n25.91\nFinally, the training objective for our full Diffusion Recti-\nfication and Estimation-Adaptive Model (DREAM) can be\nexpressed as:\nLDREAM(\u03b8) = E(x0,y0),\u03f5t,\u03f5\u2032\nt,t\n\r\r\u0000\u03f5\u2032\nt + \u03bbt\u2206\u03f5t,\u03b8\n\u0001\n\u2212 \u03f5\u03b8(x0, byt, t)\n\r\r\n1 .\n(14)\nChoice of \u03bbt. Comparing Eq. (14) with Eq. (11), the key\ndifference lies in the introduction of \u03bbt for adaptively mod-\nulating the intensity of the rectification term \u2206\u03f5t,\u03b8. Note\nthat we only need \u03bbt \u2208 (0, 1) to be increasing to leverage\nthe benefits of both standard diffusion and rectification. In\npractice, we set \u03bbt = (\u221a1 \u2212 \u00af\u03b1t)p, where p adds an extra\nlayer of flexibility: at p = 0, \u03bbt remains at 1, reverting\nthe method to DRM with consistent static rectification; as\np \u2192 \u221e, \u03bbt \u2192 0, transitioning our approach towards the\nstandard diffusion model. As shown in Figure 4, the images\nproduced by DERAM with p = 1 achieve a superior bal-\nance between perception and distortion, significantly out-\nperforming the standard SR3 [44] across both metrics.\nTraining details. It\u2019s important to highlight that while\nthe same network \u03f5\u03b8 is utilized for calculating both the rec-\ntification term \u2206\u03f5t,\u03b8 and the predicted noise \u03f5\u03b8(x0, byt, t)\nin Eq. (14), a key distinction exists: we refrain from prop-\nagating the gradient when computing \u2206\u03f5t,\u03b8, and thus, it is\nderived from the frozen network. The actual supervision\nis imposed following its adaptive adjustment. Moreover,\nwe empirically observe that using the same Gaussian noise\n(i.e., \u03f5t \u2261 \u03f5\u2032\nt) in DREAM yields superior performance, fur-\nther simplifying Eq. (13) to:\nbyt = ytrain\nt\n+\n\u221a\n1 \u2212 \u00af\u03b1t\u03bbt\u2206\u03f5t,\u03b8.\n(15)\nWe summarize our DREAM framework in Algorithm 3, tai-\nlored for enhanced diffusion training, while Algorithm 2 re-\nmains applicable for sampling purposes.\n4. Experiments\n4.1. Implementation details\nBaselines and datasets.\nOur experiments involve three\ndiffusion-based SR methods as baselines, spanning datasets\nfor faces, general scenes, and natural images. For face im-\nTable 2. Quantitative comparison for 16\u00d716 to 128\u00d7128 face\nsuper-resolution on CelebA-HQ [23]. Consistency measures the\nMSE (\u00d710\u22125) between LR and downsampled SR images.\nMethod\nPSNR\u2191\nSSIM\u2191\nConsistency\u2193\nPULSE [36]\n16.88\n0.44\n161.1\nFSRGAN [6]\n23.85\n0.71\n33.8\nRegression [44]\n23.96\n0.69\n2.71\nSR3 [44]\n23.85\n0.71\n2.33\nIDM [14]\n24.01\n0.71\n2.14\nSR3 [44]+DREAM\n24.63\n0.74\n2.12\nIDM [14]+DREAM\n24.50\n0.73\n1.26\nage datasets, we adopt SR32 [44] and IDM [14] as base-\nlines, with training conducted on FFHQ [25] and evalua-\ntions on CelebA-HQ [23]. For general scenes, we use the\nDIV2K dataset [1], employing SR3 [44] and ResShift [62]\nas baseline models. Notably, SR3 and IDM operate in pixel\nspace, whereas ResShift3 conducts diffusion process in la-\ntent space. In addition, to assess out-of-distribution (OOD)\nperformance, we train SR3 as baseline on the DIV2K\ndataset and evaluate on CAT [66] and LSUN datasets [60].\n4.2. Results and analysis\nEffect of p in \u03bbt.\nIn DREAM implementation, we set\n\u03bbt = (\u221a1 \u2212 \u00af\u03b1t)p, where p manages the balance between\nground-truth and self-estimation data as in Eq. (12). We\nconduct experiments with three baselines (SR3, IDM and\nResShift) for 8\u00d7 face SR on CelebA-HQ and 4\u00d7 gen-\neral scene SR on DIV2K at various p settings, as shown\nin Table 1. Baselines use the standard diffusion process\n(p \u2192 \u221e). For p = 0 (\u03bbt \u2261 1), corresponding to the DRM\nmodel in Eq. (11), there is a notable reduction in distortion\n(higher PSNR and SSIM), but at the cost of perceptual qual-\nity (lower LPIPS and FID), confirming our findings in Fig-\nure 4. Increasing p to 1 (our full DREAM approach) leads\nto a slight decrease in distortion but significantly improves\nthe balance between distortion and perception. Further in-\ncrease in p shows continual distortion degradation, while\nperceptual quality initially improves then declines. DREAM\n2Due to the unavailability of official code, we use a widely-recognized\nimplementation [link].\n3To ensure consistency across baselines, we standardize the transition\nkernel to align with DDPM\u2019s approach for noise prediction.\n6\n(a) LR\n(b) Standard\n(c) DREAM\n(d) HR\nFigure 5. Qualitative comparison for 8\u00d7 SR using IDM [14] on\nthe CelebA-HQ dataset [23]. Results highlight DREAM\u2019s supe-\nrior fidelity and enhanced identity preservation, leading to more\nrealistic detail generation in features like hair, eyes, and rings.\ndemonstrates clear advantages over baseline models across\nall metrics. We found p = 1 yields the best overall perfor-\nmance compared to other p values and baselines, making it\nour choice for subsequent experiments.\nFace super-resolution. Figures 4 and 5 show qualita-\ntive comparisons for face super-resolution from 16 \u00d7 16\nto 128 \u00d7 128, applying our DREAM approach to state-of-\nthe-art diffusion-based methods, SR3 and IDM. While SR3\nand IDM generally have decent image qualities, they of-\nten miss intricate facial details like hair and eyes, resulting\nin somewhat unrealistic appearance, and even omit acces-\nsories like rings. In contrast, our DREAM approach op-\nerated on the these baseline more faithfully preserves fa-\ncial identity and details. Table 2 shows a quantitative com-\nparison of our DREAM approach applied to SR3 and IDM\nagainst other methods, using metrics such as PSNR, SSIM,\nand consistency [44]. While GAN-based models are known\nfor their fidelity to human perception at higher SR scales,\ntheir lower consistency scores suggest a notable deviation\nfrom the original LR images. Applying DREAM to SR3\nand IDM, we observe considerable enhancements across\nall metrics. Notably, the simpler SR3, a pure conditional\nDDPM, when augmented with DREAM, outperforms the\nmore complex IDM, underscoring DREAM\u2019s effectiveness.\nGeneral scene super-resolution. Figure 6 shows a vi-\nsual comparison of 4\u00d7 SR results on the DIV2K dataset [1],\nusing our DREAM approach against standard diffusion\nmethods, with SR3 and ResShift as baselines.\nStandard\ntraining tends to produce images with blurred details and\ncompromised realism, evident in unclear window outlines\nand distorted shirt textures.\nIn contrast, DREAM main-\ntains structural integrity and delivers more realistic tex-\ntures. Following [17], we conduct a comprehensive com-\n(a) Standard\n(b) DREAM\nFigure 6. Qualitative comparison for 4\u00d7 SR on DIV2K [1]. Top\nwith SR3 [44] the baseline; bottom with ResShift [62] the baseline.\nTable 3. Quantitative comparison for 4\u00d7 SR on DIV2K. All mod-\nels are trained on DIV2K plus Flickr2K [52].\nThe best and\nsecond-best results among generative models are colorized.\nMethod\nPSNR\u2191 SSIM\u2191 LPIPS\u2193\nBicubic\n26.7\n0.77\n0.409\nReg.-based EDSR [34]\n28.98\n0.83\n0.270\nRRDB [57]\n29.44\n0.84\n0.253\nGAN-based ESRGAN [57]\n26.22\n0.75\n0.124\nRankSRGAN [67]\n26.55\n0.75\n0.128\nFlow-based SRFlow [35]\n27.09\n0.76\n0.121\nHCFlow [32]\n27.02\n0.76\n0.124\nFlow+GAN HCFlow++ [32]\n26.61\n0.74\n0.110\nDiffusion\nSR3 [44]\n27.02\n0.76\n0.121\nSR3 [44]+DREAM\n28.10\n0.79\n0.121\nResShift [62]\n25.30\n0.68\n0.211\nResShift [62]+DREAM\n28.44\n0.79\n0.124\nparison with various regression-based and generative meth-\nods on the DIV2K dataset. The results, detailed in Table 3\nand benchmarked against models from [32], demonstrate\nDREAM\u2019s effectiveness. Notably, DREAM has led to an\nincrease of 1.08dB and 3.14dB in PSNR, and improvements\nof 0.03 and 0.11 in SSIM for SR3 and ResShift, respec-\ntively, outperforming other generative methods. Moreover,\nthese methods demonstrate comparable or superior perfor-\nmance in perceptual quality metrics, marked by a 0.087 re-\nduction in LPIPS for ResShift. Although LPIPS scores are\nnot as favorable as those obtained by HCFlow++, even with\nDREAM applied, further improvements in image quality\ncould be achieved through advanced network designs and\nincorporating GAN loss, as in HCFlow++. However, such\napproaches are orthogonal to DREAM, and we leave these\nexplorations for future work.\n4.3. Training and sampling acceleration\nThe DREAM strategy not only improves SR image qual-\nity but also accelerates the training.\nAs shown in Fig-\nure 1, DREAM reaches convergence at around 100k to\n7\n0\n200\n400\n600\n800\n1000\nTraining iterations (x 1k)\n10\n15\n20\n25\n30\nPSNR\nPSNR (Standard)\nPSNR (DREAM)\n0.4\n0.6\n0.8\n1.0\nSSIM\nSSIM (Standard)\nSSIM (DREAM)\n(a) Distortion\n0\n200\n400\n600\n800\n1000\nTraining iterations (x 1k)\n0\n40\n80\n120\nFID\nFID (Standard)\nFID (DREAM)\n0.05\n0.10\n0.15\n0.20\n0.25\nLPIPS\nLPIPS (Standard)\nLPIPS (DREAM)\n(b) Perception\nFigure 7. Evolution of distortion metrics (left) and perceptual met-\nrics (right) using SR3 as a baseline on the DIV2K dataset.\n20\n50\n100\n200\n500\n1000 2000\nSampling steps\n22\n24\n26\n28\nPSNR\nPSNR (Standard)\nPSNR (DREAM)\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nSSIM\nSSIM (Standard)\nSSIM (DREAM)\n(a) Distortion\n20\n50\n100\n200\n500\n1000 2000\nSampling steps\n40\n60\n80\n100\n120\nFID\nFID (Standard)\nFID (DREAM)\n0.15\n0.20\n0.25\n0.30\n0.35\nLPIPS\nLPIPS (Standard)\nLPIPS (DREAM)\n(b) Perception\nFigure 8. Comparison of distortion metrics (left) and perception\nmetrics (right) with varying sampling steps, using SR3 as a base-\nline on the CelebA-HQ dataset.\n150k iterations, a significant improvement over the stan-\ndard diffusion-based SR3\u2019s 400k iterations. Moreover, Fig-\nure 7 illustrates the evolution of training in terms of dis-\ntortion metrics (PSNR and SSIM) and perception metrics\n(LPIPS and FID) using SR3 as the baseline on the DIV2K\ndataset. DREAM not only converges faster but also sur-\npasses SR3\u2019s final results before its own convergence. For\nexample, DREAM achieves a PSNR of 28.07 and FID of\n14.72 at just 470k iterations, while the baseline SR3 with\nstandard diffusion reaches PSNR 27.02 and FID 16.72 after\nfull convergence at 980k iterations, indicating a 2\u00d7 speedup\nin training. Additional experiments with different baselines\nand datasets can be found in the appendix.\nMoreover, DREAM considerably accelerates the sam-\npling process, outperforming standard diffusion training\nwith fewer sampling steps. Figure 8 demonstrates this us-\ning SR3 on the CelebA-HQ dataset, comparing SR images\ngenerated with varying sampling steps in terms of both dis-\ntortion and perception metrics. While the standard baseline\ntypically requires an entire 2000 sampling steps, DREAM\nachieves improved distortion metrics (0.73 v.s.\n0.71 in\nSSIM) and comparable perceptual quality (0.189 v.s. 0.184\nin LPIPS) with only 100 steps. This marks a substantial\n20\u00d7 speedup in sampling.\nMore details and results are\navailable in the appendix.\n4.4. Out-of-distribution (OOD) evaluations\nTo evaluate our approach\u2019s OOD performance, we train\nthe SR3 model on DIV2K for 4\u00d7 SR scaling, then evalu-\nate its performance on various natural image datasets from\nthe CAT [66] and LSUN [60] benchmarks, covering mul-\ntiple SR scales. This OOD evaluation encompasses both\n(a) 2\u00d7\n(b) 3\u00d7\n(c) 4\u00d7\n(d) 5\u00d7\nFigure 9. Visual comparison of OOD SR. We use SR3 as a base-\nline, pretrain it on DIV2K and evaluate on CAT and LSUN, across\nvarious scales. The top row is obtained using standard training for\nSR3; the bottom row is generated using DREAM on SR3.\nTable 4. Quantitative comparison of OOD SR on CAT and LSUN\nBedroom and Tower validation sets at various scales.\nScale\nCats\nTowers\nBedrooms\n2\u00d7\nStandard 19.72/0.398 18.82/0.333 20.20/0.314\nDREAM 22.50/0.337 20.89/0.288 22.15/0.278\n3\u00d7\nStandard 22.48/0.281 18.42/0.266 20.14/0.235\nDREAM 23.90/0.265 19.35/0.252 20.65/0.231\n4\u00d7\nStandard 26.49/0.257 24.03/0.217 26.89/0.187\nDREAM 27.19/0.246 24.94/0.212 27.53/0.183\n5\u00d7\nStandard 24.52/0.381 21.79/0.331 23.18/0.313\nDREAM 24.58/0.373 21.84/0.324 23.19/0.310\ndataset diversity and scaling differences. As shown in Fig-\nure 9, our DREAM training approach significantly enhances\nmodel robustness, producing more realistic and clearer im-\nages across different scales. For instance, it captures finer\ndetails such as the beard of cats at 2\u00d7 and 5\u00d7 scales, the\nstructural integrity of a tower at 3\u00d7 scale, and the intri-\ncate wrinkles on a bed at 4\u00d7 scale. Following [14], Table 4\npresents the average PSNR and LPIPS metrics for 100 se-\nlected images from these validation datasets. Our findings\nshow that the DREAM training framework consistently im-\nproves baseline model across diverse datasets and scales.\n5. Conclusion\nThis paper introduces DREAM, a novel training framework\ndesigned to address the training-sampling discrepancy in\nconditional diffusion models with minimal code modifica-\ntions. DREAM comprises two key components: diffusion\nrectification and estimation adaptation. Diffusion rectifica-\ntion extends the existing training framework for diffusion\nmodels by aligning training more closely with sampling\nthrough self-estimation. Estimation adaptation optimizes\nthe balance between accuracy and fidelity by adaptively\nincorporating ground-truth information. When applied to\nSISR tasks, DREAM effectively bridges the gap between\ntraining and sampling. Extensive experiments demonstrate\n8\nthat DREAM enhances distortion and perception metrics\nacross various diffusion-based SR baselines. It also speeds\nup training, improves sampling efficiency, and achieves ro-\nbust OOD performance across diverse datasets and scales.\nWhile DREAM is mainly utilized for SR in this work, its\ncapabilities are applicable to a range of dense visual predic-\ntion tasks. Future research may investigate its use in both\nlow-level vision tasks, such as inpainting and deblurring,\nand high-level vision tasks like semantic segmentation and\ndepth estimation. Additionally, exploring DREAM\u2019s appli-\ncation in both unconditional and conditional image genera-\ntion presents an intriguing direction for future work.\nReferences\n[1] Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge\non single image super-resolution: Dataset and study. In Pro-\nceedings of the IEEE conference on computer vision and pat-\ntern recognition workshops, pages 126\u2013135, 2017. 6, 7, 1, 2,\n3, 4, 5\n[2] Saeed Anwar and Nick Barnes. Densely residual laplacian\nsuper-resolution. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 44(3):1192\u20131204, 2020. 2\n[3] Marco Bevilacqua, Aline Roumy, Christine Guillemot, and\nMarie Line Alberi-Morel.\nLow-complexity single-image\nsuper-resolution based on nonnegative neighbor embedding.\n2012. 1\n[4] Yochai Blau and Tomer Michaeli. The perception-distortion\ntradeoff. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 6228\u20136237, 2018. 5\n[5] Kelvin CK Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu,\nand Chen Change Loy.\nGlean:\nGenerative latent bank\nfor large-factor image super-resolution. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 14245\u201314254, 2021. 2\n[6] Yu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen, and Jian\nYang. Fsrnet: End-to-end learning face super-resolution with\nfacial priors.\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 2492\u20132501,\n2018. 6\n[7] Yinbo Chen, Sifei Liu, and Xiaolong Wang.\nLearning\ncontinuous image representation with local implicit image\nfunction. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 8628\u20138638,\n2021. 1\n[8] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural informa-\ntion processing systems, 34:8780\u20138794, 2021. 2\n[9] Tianyu Ding, Luming Liang, Zhihui Zhu, and Ilya Zharkov.\nCdfi: Compression-driven network design for frame inter-\npolation.\nIn Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 8001\u20138011,\n2021. 2\n[10] Tianyu Ding, Luming Liang, Zhihui Zhu, Tianyi Chen, and\nIlya Zharkov. Sparsity-guided network design for frame in-\nterpolation. arXiv preprint arXiv:2209.04551, 2022. 2\n[11] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio.\nDensity estimation using real NVP.\narXiv:1605.08803,\n2016. 1, 2\n[12] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou\nTang.\nLearning a deep convolutional network for image\nsuper-resolution.\nIn Computer Vision\u2013ECCV 2014: 13th\nEuropean Conference, Zurich, Switzerland, September 6-12,\n2014, Proceedings, Part IV 13, pages 184\u2013199. Springer,\n2014. 1\n[13] Martin Nicolas Everaert, Athanasios Fitsios, Marco Bocchio,\nSami Arpa, Sabine S\u00a8usstrunk, and Radhakrishna Achanta.\nExploiting the signal-leak bias in diffusion models. arXiv\npreprint arXiv:2309.15842, 2023. 2\n[14] Sicheng Gao, Xuhui Liu, Bohan Zeng, Sheng Xu, Yan-\njing Li, Xiaoyan Luo, Jianzhuang Liu, Xiantong Zhen, and\nBaochang Zhang. Implicit diffusion models for continuous\nsuper-resolution. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pages\n10021\u201310030, 2023. 1, 2, 6, 7, 8, 3\n[15] Zhicheng Geng, Luming Liang, Tianyu Ding, and Ilya\nZharkov. Rstt: Real-time spatial temporal transformer for\nspace-time video super-resolution.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 17441\u201317451, 2022. 2\n[16] Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative Adversarial Networks. NIPS,\n2014. 1\n[17] Baisong Guo, Xiaoyun Zhang, Haoning Wu, Yu Wang, Ya\nZhang, and Yan-Feng Wang.\nLar-sr: A local autoregres-\nsive model for image super-resolution.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 1909\u20131918, 2022. 2, 7\n[18] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. Advances in neural information processing systems,\n30, 2017. 1\n[19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 1, 2, 3, 4\n[20] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet,\nMohammad Norouzi, and Tim Salimans. Cascaded diffusion\nmodels for high fidelity image generation. The Journal of\nMachine Learning Research, 23(1):2249\u20132281, 2022. 1\n[21] Yuanfeng Ji, Zhe Chen, Enze Xie, Lanqing Hong, Xihui Liu,\nZhaoqiang Liu, Tong Lu, Zhenguo Li, and Ping Luo. Ddp:\nDiffusion model for dense visual prediction. arXiv preprint\narXiv:2303.17559, 2023. 3, 5\n[22] Younghyun Jo, Seoung Wug Oh, Peter Vajda, and Seon Joo\nKim. Tackling the ill-posedness of super-resolution through\nadaptive target generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 16236\u201316245, 2021. 2\n[23] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.\nProgressive growing of gans for improved quality, stability,\nand variation. arXiv preprint arXiv:1710.10196, 2017. 5, 6,\n7, 1, 3\n9\n[24] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.\nProgressive growing of gans for improved quality, stability,\nand variation. In ICLR, 2018. 1\n[25] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks.\nIn Proceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 4401\u20134410, 2019. 4, 6\n[26] Diederik P. Kingma and Prafulla Dhariwal. Glow: Genera-\ntive Flow with Invertible 1x1 Convolutions. In NIPS, 2018.\n1, 2\n[27] Diederik P Kingma and Max Welling. Auto-Encoding Vari-\national Bayes. In ICLR, 2013. 1, 2\n[28] Christian Ledig, Lucas Theis, Ferenc Husz\u00b4ar, Jose Caballero,\nAndrew Cunningham, Alejandro Acosta, Andrew Aitken,\nAlykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-\nrealistic single image super-resolution using a generative ad-\nversarial network. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 4681\u20134690,\n2017. 2\n[29] Haoying Li, Yifan Yang, Meng Chang, Shiqi Chen, Huajun\nFeng, Zhihai Xu, Qi Li, and Yueting Chen. Srdiff: Single\nimage super-resolution with diffusion probabilistic models.\nNeurocomputing, 479:47\u201359, 2022. 2, 1\n[30] Mingxiao Li, Tingyu Qu, Wei Sun, and Marie-Francine\nMoens.\nAlleviating exposure bias in diffusion models\nthrough sampling with shifted time steps.\narXiv preprint\narXiv:2305.15583, 2023. 2\n[31] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc\nVan Gool, and Radu Timofte. Swinir: Image restoration us-\ning swin transformer. In Proceedings of the IEEE/CVF inter-\nnational conference on computer vision, pages 1833\u20131844,\n2021. 1, 2\n[32] Jingyun Liang, Andreas Lugmayr, Kai Zhang, Martin\nDanelljan, Luc Van Gool, and Radu Timofte.\nHierarchi-\ncal conditional flow: A unified framework for image super-\nresolution and image rescaling.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 4076\u20134085, 2021. 7\n[33] Jie Liang, Hui Zeng, and Lei Zhang. Details or artifacts:\nA locally discriminative learning approach to realistic im-\nage super-resolution. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n5657\u20135666, 2022. 1, 2\n[34] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and\nKyoung Mu Lee. Enhanced deep residual networks for single\nimage super-resolution. In Proceedings of the IEEE confer-\nence on computer vision and pattern recognition workshops,\npages 136\u2013144, 2017. 1, 2, 7\n[35] Andreas Lugmayr, Martin Danelljan, Luc Van Gool, and\nRadu Timofte. Srflow: Learning the super-resolution space\nwith normalizing flow.\nIn Computer Vision\u2013ECCV 2020:\n16th European Conference, Glasgow, UK, August 23\u201328,\n2020, Proceedings, Part V 16, pages 715\u2013732. Springer,\n2020. 7\n[36] Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi,\nand Cynthia Rudin. Pulse: Self-supervised photo upsam-\npling via latent space exploration of generative models. In\nProceedings of the ieee/cvf conference on computer vision\nand pattern recognition, pages 2437\u20132445, 2020. 6, 1\n[37] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\ndenoising diffusion probabilistic models.\nIn International\nConference on Machine Learning, pages 8162\u20138171. PMLR,\n2021. 2\n[38] Mang Ning, Mingxiao Li, Jianlin Su, Albert Ali Salah, and\nItir Onal Ertugrul. Elucidating the exposure bias in diffusion\nmodels. arXiv preprint arXiv:2308.15321, 2023. 2\n[39] Mang Ning, Enver Sangineto, Angelo Porrello, Simone\nCalderara, and Rita Cucchiara.\nInput perturbation re-\nduces exposure bias in diffusion models.\narXiv preprint\narXiv:2301.11706, 2023. 2\n[40] A\u00a8aron van den Oord, Sander Dieleman, Heiga Zen, Karen\nSimonyan, Oriol Vinyals, Alex Graves, Nal Kalchbren-\nner, Andrew Senior, and Koray Kavukcuoglu.\nWaveNet:\nA Generative Model for Raw Audio.\narXiv preprint\narXiv:1609.03499, 2016. 1\n[41] A\u00a8aron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse\nEspeholt, Alex Graves, and Koray Kavukcuoglu.\nCondi-\ntional Image Generation with PixelCNN Decoders. In NIPS,\n2016. 1, 2\n[42] Alec Radford, Luke Metz, and Soumith Chintala.\nUn-\nsupervised representation learning with deep convolu-\ntional generative adversarial networks.\narXiv preprint\narXiv:1511.06434, 2015. 1\n[43] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 1, 2\n[44] Chitwan Saharia, Jonathan Ho, William Chan, Tim Sal-\nimans, David J Fleet, and Mohammad Norouzi.\nImage\nsuper-resolution via iterative refinement. IEEE Transactions\non Pattern Analysis and Machine Intelligence, 45(4):4713\u2013\n4726, 2022. 1, 2, 3, 4, 5, 6, 7\n[45] Nikolay Savinov, Junyoung Chung, Mikolaj Binkowski,\nErich Elsen, and Aaron van den Oord.\nStep-unrolled de-\nnoising autoencoders for text generation. In International\nConference on Learning Representations, 2022. 3, 5\n[46] Saurabh Saxena, Abhishek Kar, Mohammad Norouzi, and\nDavid J Fleet. Monocular depth estimation using diffusion\nmodels. arXiv preprint arXiv:2302.14816, 2023. 3, 5\n[47] Jae Woong Soh, Gu Yong Park, Junho Jo, and Nam Ik Cho.\nNatural and realistic single image super-resolution with ex-\nplicit natural manifold discrimination.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 8122\u20138131, 2019. 2\n[48] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics.\nIn International confer-\nence on machine learning, pages 2256\u20132265. PMLR, 2015.\n1, 2, 3\n[49] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. In International Conference\non Learning Representations, 2021. 2, 5\n[50] Jian Sun, Zongben Xu, and Heung-Yeung Shum. Gradient\nprofile prior and its applications in image super-resolution\n10\nand enhancement. IEEE Transactions on Image Processing,\n20(6):1529\u20131542, 2010. 1\n[51] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\nShlens, and Zbigniew Wojna. Rethinking the inception archi-\ntecture for computer vision. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pages\n2818\u20132826, 2016. 1\n[52] Radu Timofte, Eirikur Agustsson, Luc Van Gool, Ming-\nHsuan Yang, and Lei Zhang. Ntire 2017 challenge on single\nimage super-resolution: Methods and results. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition workshops, pages 114\u2013125, 2017. 7, 1, 2\n[53] Arash Vahdat and Jan Kautz. NVAE: A deep hierarchical\nvariational autoencoder. In NeurIPS, 2020. 1, 2\n[54] A\u00a8aron Van Den Oord,\nNal Kalchbrenner,\nand Koray\nKavukcuoglu. Pixel recurrent neural networks. In Interna-\ntional conference on machine learning, pages 1747\u20131756.\nPMLR, 2016. 2\n[55] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete\nrepresentation learning. Advances in neural information pro-\ncessing systems, 30, 2017. 2\n[56] Xintao Wang, Ke Yu, Chao Dong, and Chen Change Loy.\nRecovering realistic texture in image super-resolution by\ndeep spatial feature transform. In Proceedings of the IEEE\nconference on computer vision and pattern recognition,\npages 606\u2013615, 2018. 2\n[57] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu,\nChao Dong, Yu Qiao, and Chen Change Loy. Esrgan: En-\nhanced super-resolution generative adversarial networks. In\nProceedings of the European conference on computer vision\n(ECCV) workshops, pages 0\u20130, 2018. 2, 7\n[58] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-\nmoncelli. Image quality assessment: from error visibility to\nstructural similarity. IEEE transactions on image processing,\n13(4):600\u2013612, 2004. 1\n[59] Qing Yan, Yi Xu, Xiaokang Yang, and Truong Q Nguyen.\nSingle image superresolution based on gradient profile\nsharpness. IEEE Transactions on Image Processing, 24(10):\n3187\u20133202, 2015. 1\n[60] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas\nFunkhouser, and Jianxiong Xiao. Lsun: Construction of a\nlarge-scale image dataset using deep learning with humans\nin the loop. arXiv preprint arXiv:1506.03365, 2015. 6, 8\n[61] Hu Yu, Li Shen, Jie Huang, Man Zhou, Hongsheng Li, and\nFeng Zhao. Debias the training of diffusion models. arXiv\npreprint arXiv:2310.08442, 2023. 2\n[62] Zongsheng Yue, Jianyi Wang, and Chen Change Loy.\nResshift:\nEfficient diffusion model for image super-\nresolution by residual shifting. In Thirty-seventh Conference\non Neural Information Processing Systems, 2023. 1, 2, 3, 6,\n7, 5\n[63] Kai Zhang, Luc Van Gool, and Radu Timofte. Deep unfold-\ning network for image super-resolution. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 3217\u20133226, 2020. 2\n[64] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timo-\nfte. Designing a practical degradation model for deep blind\nimage super-resolution.\nIn Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 4791\u2013\n4800, 2021. 2\n[65] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 586\u2013595, 2018. 2, 4\n[66] Weiwei Zhang, Jian Sun, and Xiaoou Tang.\nCat head\ndetection-how to effectively exploit shape and texture fea-\ntures. In Computer Vision\u2013ECCV 2008: 10th European Con-\nference on Computer Vision, Marseille, France, October 12-\n18, 2008, Proceedings, Part IV 10, pages 802\u2013816. Springer,\n2008. 6, 8\n[67] Wenlong Zhang, Yihao Liu, Chao Dong, and Yu Qiao.\nRanksrgan: Generative adversarial networks with ranker for\nimage super-resolution.\nIn Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 3096\u2013\n3105, 2019. 7\n[68] Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng\nZhong, and Yun Fu.\nImage super-resolution using very\ndeep residual channel attention networks. In Proceedings of\nthe European conference on computer vision (ECCV), pages\n286\u2013301, 2018. 2\n[69] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and\nYun Fu. Residual dense network for image super-resolution.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 2472\u20132481, 2018. 1, 2\n11\nDREAM: Diffusion Rectification and Estimation-Adaptive Models\nSupplementary Material\nIn this supplementary material, we begin by describ-\ning more details of the evaluation metrics and experiment\nsetup in Section 6. In following Section 7, we present more\nquantitative comparisons and visualization results on vari-\nous baselines and datasets, which further demonstrates the\neffectiveness of our DREAM strategy. We conclude with a\ndiscussion of the ethical implications in Section 8.\n6. Metrics and setups\nWe provide a more comprehensive explanation of the met-\nrics and the experiment settings employed in the main text\nof the paper.\n6.1. Metrics\nIn this section, we will detail the metrics applied to mea-\nsure image distortion and perception quality. The distor-\ntion metrics encompass Peak Signal-to-Noise Ratio (PSNR)\nand Structural Similarity Index Measure (SSIM), as well\nas Consistency the the perception measurement include the\nLearned Perceptual Image Patch Similarity (LPIPS) and the\nFr\u00b4echet Inception Distance (FID).\nPeak Signal-to-Noise Ratio (PSNR). PSNR is an indi-\ncator of image reconstruction quality. However, its value\nin decibels (dB) presents certain constraints when assessing\nsuper-resolution tasks [36]. Thus, it acts merely as a ref-\nerential metric of image quality, comparing the maximum\npossible signal to the level of background noise. Generally,\na higher PSNR suggests a lower degree of image distortion.\nStructure Similarity Index Measure (SSIM). Building\non the image distortion modeling framework [58], the SSIM\napplies the principles of structural similarity, mirroring the\nfunctionality of the human visual system. It is adept at de-\ntecting local structural alterations within an image. SSIM\nmeasures image attributes such as luminance, contrast, and\nstructure by employing the mean for luminance assessment,\nvariance for contrast evaluation, and covariance to gauge\nstructural integrity.\nConsistency.\nConsistency is measured by calculating\nthe MSE (\u00d710\u22125) between the low-resolution inputs and\ntheir corresponding downsampled super-resolution outputs.\nLearned Perceptual Image Patch Similarity (LPIPS).\nLPIPS evaluates the perceptual resemblance between gen-\nerated images and their authentic counterparts by analyzing\ndeep feature representations.\nFr\u00b4echet Inception Distance score (FID). FID [18] as-\nsesses image quality by emulating human judgment of im-\nage resemblance. This is achieved by utilizing a pre-trained\nInception-V3 network [51] to contrast the distribution pat-\nterns of the generated images against the distributions of the\noriginal, ground-truth images.\n6.2. Setups\nIn this section, we will provide detailed descriptions of the\nconfigurations for various baseline models as well as the\ndatasets utilized in our experiments.\nSR3 model on face dataset.\nWe train the SR3 [44]\nmodel on an upscaled 8\u00d7 FFHQ dataset for 1M iterations\nand evaluate on 100 images from the CelebA [23] valida-\ntion dataset. During training, the LR images are consis-\ntently resized to 16 \u00d7 16 pixels, while the HR counterparts\nare scaled to 128\u00d7128 pixels. For the SR image generation,\nthe LR images are first upscaled to 128 \u00d7 128 pixels using\nbicubic interpolation and serve as the conditioning input. In\nalignment with the DDPM [19], the Adam optimizer is uti-\nlized with a fixed learning rate of 1e-4 through the training\nphase. The training employs a batch size of 4, incorporates\na dropout rate of 0.2, and utilizes a linear beta scheduler\nover 2000 steps with a starting value of 10\u22126 and a final\nvalue of 10\u22122. A single 24GB NVIDIA RTX A5000 GPU\nis used under this situation.\nIDM model on face dataset. Adhering to the offical im-\nplementation of the IDM [14], the model is trained on a 8\u00d7\nFFHQ dataset for 1M iterations and evaluated on 100 im-\nages from the CelebA [23] validation dataset. Specifically,\nthroughout training, LR images are consistently resized to\n16 \u00d7 16 pixels, while their HR counterparts are scaled to\n128 \u00d7 128 pixels. These LR images are then processed\nthrough a specialized LR conditioning network, which is\nstacked with a series of convolutional layers, bilinear down-\nsampling filtering, and leaky ReLU activation to extract a\nhierarchy of multi-resolution features. These features are\nthen employed as the conditioning input for the denoising\nnetwork. The training employs the Adam optimizer with\na constant learning rate of 10\u22124, a batch size of 32, and\na dropout rate of 0.2. We implement a linear beta sched-\nuler that advances over 2000 steps, starting from 10\u22126 and\nescalating to 10\u22122. This setup is supported by two 24GB\nNVIDIA RTX A5000.\nSR3 model on general scene dataset.\nWe train the\nSR3 [44] model on upscaled 4\u00d7 the training dataset com-\nparising DIV2K [1] and Flicker2K [52] for 1M iterations.\nConsistent with the SRDiff [29], each image is cropped into\npatches of 160 \u00d7 160 as the HR ground truths. To pro-\nduce the corresponding LR image patches of 40\u00d740 pixels,\nthe HR image patches are downscaled using a bicubic ker-\nnel. These LR image patches are then resized back to the\nHR dimensions using bicubic interpolation and are used as\n1\nTable 5. Comparison of training time by using different baselines\nand datasets. The value denotes the ratio of DREAM/standard.\nFace\nDIV2K\nRatio\nSR3 [44]\nIDM [14]\nSR3 [44]\nResShift [62]\n1.38\n1.21\n1.24\n1.08\nthe conditioning input for the super-resolution process. For\nevaluation, the entire DIV2K validation set, consisting of\n100 images, is utilized. The HR images are downsampled\nusing a bicubic kernel to generate LR images, which are\nthen cropped into 40\u00d740 pixel patches with a 5-pixel over-\nlap between adjacent patches. The SR3 model is applied to\nthese LR patches to yield the SR predictions which are sub-\nsequently merged to form the final SR images. The model\u2019s\ntraining utilizes the Adam optimizer with a steady learning\nrate of 10\u22124, a batch size of 32 patches, and a dropout rate\nof 0.2. A linear beta scheduler is applied over 1000 steps,\ninitiating at 10\u22126 and culminating at 10\u22122. This configura-\ntion is executed on two 24GB NVIDIA RTX A5000 GPUs.\nResShift on general scene datatset.\nTraining the\nResShift model [62]uses a 4\u00d7 dataset, combining the train-\ning sets from DIV2K [1] and Flickr2K [52] over 0.5M iter-\nations. Similar as data process in the previous SR3 setting,\neach image is partitioned into patches of 256x256 pixels to\nserve as HR ground truths. The LR image patches, resized\nto 64x64 pixels, are derived by downscaling the HR patches\nwith a bicubic kernel. The VQGAN encoder, pre-trained on\nthe ImageNet dataset, processes these LR patches to distill\nsalient features, furnishing the necessary conditioning input\nfor the following latent denoiser network. For performance\nevaluation, we use the entire DIV2K validation set, which\ncomprises 100 images. The HR images are downsampled\nto LR with a bicubic kernel, and then segmented into 64x64\npixel patches, maintaining an 8-pixel overlap between ad-\njacent patches. The latent denoiser model is applied to the\nLR patches to generate the corresponding SR latent codes.\nThese latent codes are subsequently processed by the VQ-\nGAN decoder to reconstruct the SR patches, thereby pro-\nducing the final high-resolution super-resolution images.\nThe training regimen employs the Adam optimizer with a\nconsistent learning rate of 5 \u00d7 10\u22125 and a batch size of 32\npatches. A linear beta scheduler is utilized over 50 steps,\nselected evenly from a linearly spaced 2000-steps schedule\nbeginning at 10\u22126 and increasing to 10\u22122. The training is\nconducted using two 24GB NVIDIA RTX A5000.\n7. Additional experimental results\nIn this section, we begin by providing additional results\non the acceleration of training and sampling across various\nbaselines and datasets in Section 7.1. Lastly, in Section 7.2,\nwe offer a more comprehensive visual comparison on the\ngeneral scene dataset, using the SR3 [44] and ResShift [62]\nmodels as baselines.\n0\n200\n400\n600\n800\n1000\nTraining iterations (x 1k)\n10\n15\n20\n25\nPSNR\nPSNR (Standard)\nPSNR (DREAM)\n0.4\n0.6\n0.8\nSSIM\nSSIM (Standard)\nSSIM (DREAM)\n(a) Distortion\n0\n200\n400\n600\n800\n1000\nTraining iterations (x 1k)\n50\n75\n100\n125\n150\nFID\nFID (Standard)\nFID (DREAM)\n0.1\n0.2\n0.3\nLPIPS\nLPIPS (Standard)\nLPIPS (DREAM)\n(b) Perception\nFigure 10. Evolution of distortion metrics (left) and perceptual\nmetrics (right) using SR3 as a baseline on the face dataset.\n0\n200\n400\n600\n800\n1000\nTraining iterations (x 1k)\n10\n15\n20\n25\nPSNR\nPSNR (Standard)\nPSNR (DREAM)\n0.4\n0.6\n0.8\nSSIM\nSSIM (Standard)\nSSIM (DREAM)\n(a) Distortion\n0\n200\n400\n600\n800\n1000\nTraining iterations (x 1k)\n50\n75\n100\n125\n150\nFID\nFID (Standard)\nFID (DREAM)\n0.1\n0.2\n0.3\nLPIPS\nLPIPS (Standard)\nLPIPS (DREAM)\n(b) Perception\nFigure 11. Evolution of distortion metrics (left) and perceptual\nmetrics (right) using IDM as a baseline on the face dataset.\n0\n100\n200\n300\n400\n500\nTraining iterations (x 1k)\n24\n26\n28\n30\nPSNR\nPSNR (Standard)\nPSNR (DREAM)\n0.6\n0.7\n0.8\n0.9\nSSIM\nSSIM (Standard)\nSSIM (DREAM)\n(a) Distortion\n0\n100\n200\n300\n400\n500\nTraining iterations (x 1k)\n10\n20\n30\n40\n50\nFID\nFID (Standard)\nFID (DREAM)\n0.1\n0.2\n0.3\n0.4\n0.5\nLPIPS\nLPIPS (Standard)\nLPIPS (DREAM)\n(b) Perception\nFigure 12. Evolution of distortion metrics (left) and perceptual\nmetrics (right) using ResShift as a baseline on the DIV2K dataset.\n7.1. Training and sampling acceleration\nTraining efficiency.\nIn Table 5, we detail the training\ntime ratio between our DREAM methodology and stan-\ndard training approaches across a variety of baselines and\ndatasets. Our DREAM method, which includes only a sin-\ngle additional forward computation, results in a marginal in-\ncrease in training time. However, it offers a considerable ad-\nvantage by significantly accelerating training convergence.\nWe further illustrate the evolution of training through dis-\ntortion metrics, namely PSNR and SSIM, as well as per-\nception metrics such as LPIPS and FID. Utilizing SR3 and\nIDM as baselines for the face dataset, the improvements are\nevident in Figure 10 and Figure 11. The ResShift model,\nused as a baseline for the DIV2K dataset, demonstrates sim-\nilar enhancements in Figure 12. Notably, DREAM not only\nfacilitates quicker convergence but also outperforms the fi-\nnal outcomes of several baselines after they fully converge.\nFor example, with the face dataset, the SR3 model using\nDREAM achieves a PSNR of 24.49 and an FID of 61.02 in\njust 490k iterations, whereas the standard diffusion baseline\nreaches a PSNR of 23.85 and an FID of 61.98 after 880k\niterations. This underlines a substantial training speedup by\n2\n20\n50\n100\n200\n500\n1000\nSampling steps\n20\n25\n30\nPSNR\nPSNR (Standard)\nPSNR (DREAM)\n0.5\n0.6\n0.7\n0.8\npsnr\nSSIM (Standard)\nSSIM (DREAM)\n(a) Distortion\n20\n50\n100\n200\n500\n1000\n2000\nSampling steps\n50\n75\n100\nFID\nFID (Standard)\nFID (DREAM)\n0.16\n0.20\n0.24\nLPIPS\nLPIPS (Standard)\nLPIPS (DREAM)\n(b) Perception\nFigure 13. Comparison of distortion metrics (left) and perception\nmetrics (right) with varying sampling steps, using IDM as a base-\nline on the CelebA-HQ dataset.\n20\n50\n100\n200\n500\n1000\n2000\nSampling steps\n23.0\n23.5\n24.0\n24.5\n25.0\nPSNR\nPSNR (Standard)\nPSNR (DREAM)\n0.60\n0.65\n0.70\n0.75\npsnr\nSSIM (Standard)\nSSIM (DREAM)\n(a) Distortion\n20\n50\n100\n200\n500\n1000\nSampling steps\n10\n20\n30\n40\n50\nFID\nFID (Standard)\nFID (DREAM)\n0.12\n0.14\n0.16\n0.18\n0.20\nLPIPS\nLPIPS (Standard)\nLPIPS (DREAM)\n(b) Perception\nFigure 14. Comparison of distortion metrics (left) and perception\nmetrics (right) with varying sampling steps, using SR3 as a base-\nline on the DIV2K dataset.\nroughly 2\u00d7 with DREAM. Similarly, the IDM model with\nDREAM reaches a PSNR of 23.54 and an FID of 55.81 in\nonly 330k iterations, compared to the baseline achieving a\nPSNR of 23.85 and an FID of 61.98 after 760k iterations,\nreinforcing the significant efficiency of DREAM.\nSampling acceleration.\nFurthermore, DREAM signifi-\ncantly enhances the efficiency of the sampling process, sur-\npassing the performance of standard diffusion training with\na reduced number of sampling steps. Figure 13 showcases\nthe capabilities of DREAM using the IDM model on the\nCelebA-HQ dataset. It compares super-resolution images\ngenerated with different numbers of sampling steps, evalu-\nating them against both distortion and perception metrics.\nWhile the conventional baseline necessitates up to 2000\nsampling steps, DREAM attains superior distortion met-\nrics (an SSIM of 0.73 compared to 0.71) and comparable\nperceptual quality (an LPIPS of 0.179 versus 0.172) with\nmerely 100 steps, leading to an impressive 20\u00d7 increase\nin sampling efficiency. In a similar vein, Figure 14a illus-\ntrates the impact of DREAM using the SR3 model on the\nDIV2K dataset. Here, the images produced with varying\nsampling steps are again evaluated using both sets of met-\nrics. Standard baselines typically require 1000 sampling\nsteps, but with DREAM, improved distortion metrics (an\nSSIM of 0.79 versus 0.76) and similar perceptual quality\n(an LPIPS of 0.127 versus 0.121) are achieved with just 100\nsteps, resulting in a substantial 10\u00d7 sampling speedup.\n7.2. Visualization\nFace dataset.\nIn Figure 15 and Figure 16, we pro-\nvide more representative examples from CelebA-HQ\n(a) LR\n(b) Standard\n(c) DREAM\n(d) HR\nFigure 15. Qualitative comparison for 8\u00d7 SR using SR3 [44] on\nthe CelebA-HQ dataset [23]. Results highlight DREAM\u2019s supe-\nrior fidelity and enhanced identity preservation, leading to more\nrealistic details, such as eye and teeth.\n(a) LR\n(b) Standard\n(c) DREAM\n(d) HR\nFigure 16. Qualitative comparison for 8\u00d7 SR using IDM [14] on\nthe CelebA-HQ dataset [23]. Results highlight DREAM\u2019s supe-\nrior fidelity and enhanced identity preservation, leading to more\nrealistic detail generation in features like nose, and wrinkles.\ndataset [23], employing SR3 and IDM as baselines, respec-\ntively. These results again validate the remarkable ability of\nDREAM strategy in synthesizing high-fidelity face images.\nGeneral scene dataset. To further illustrate the broad ef-\nfectiveness of our DREAM strategy, we present a selection\nof examples from the DIV2K [1]. These examples show-\ncase complex image elements such as intricate textures, re-\npeated symbols, and distinct objects. We conduct a compar-\native visualization of our DREAM strategy against standard\ntraining practices, employing the SR3 model as a baseline\nin Figure 17, Figure 18 and Figure 19. Similarly, we use the\nResShift model as a baseline in Figure 20, Figure 21 and\nFigure 22. The comparisons unequivocally demonstrate the\nsuperior performance of our DREAM strategy.\n8. Ethic impact\nThis research is applicable to the task of enhancing human\nfacial resolution, a frequent requirement in mobile photog-\nraphy. It does not inherently contribute to negative social\nconsequences. However, given concerns surrounding per-\nsonal security, it is crucial to safeguard against its potential\nmisuse for harmful intentions.\n3\n(a) Standard\n(b) DREAM\nFigure 17. Qualitative comparison for 4\u00d7 SR on DIV2K [1] using SR3 [44] model as baseline. Left Image: standard training; Right\nImage: DREAM training. The model trained under DREAM framework exhibits enhanced fine-grained details and rendering more realistic\nresults, as indicated by the magnified section of the synthesized SR images.\n(a) Standard\n(b) DREAM\nFigure 18. Qualitative comparison for 4\u00d7 SR on DIV2K [1] using SR3 [44] model as baseline. Left Image: standard training; Right\nImage: DREAM training. The model trained under DREAM framework exhibits enhanced fine-grained details and rendering more realistic\nresults, as indicated by the magnified section of the synthesized SR images.\n(a) Standard\n(b) DREAM\nFigure 19. Qualitative comparison for 4\u00d7 SR on DIV2K [1] using SR3 [44] model as baseline. Left Image: standard training; Right\nImage: DREAM training. The model trained under DREAM framework exhibits enhanced fine-grained details and rendering more realistic\nresults, as indicated by the magnified section of the synthesized SR images.\n4\n(a) Standard\n(b) DREAM\nFigure 20. Qualitative comparison for 4\u00d7 SR on DIV2K [1] using ResShift [62] model as baseline. Left Image: standard training;\nRight Image: DREAM training. The model trained under DREAM framework exhibits enhanced fine-grained details and rendering more\nrealistic results, as indicated by the magnified section of the synthesized SR images.\n(a) Standard\n(b) DREAM\nFigure 21. Qualitative comparison for 4\u00d7 SR on DIV2K [1] using ResShift [62] model as baseline. Left Image: standard training;\nRight Image: DREAM training. The model trained under DREAM framework exhibits enhanced fine-grained details and rendering more\nrealistic results, as indicated by the magnified section of the synthesized SR images.\n(a) Standard\n(b) DREAM\nFigure 22. Qualitative comparison for 4\u00d7 SR on DIV2K [1] using ResShift [62] model as baseline. Left Image: standard training;\nRight Image: DREAM training. The model trained under DREAM framework exhibits enhanced fine-grained details and rendering more\nrealistic results, as indicated by the magnified section of the synthesized SR images.\n5\n"
  },
  {
    "title": "GraphDreamer: Compositional 3D Scene Synthesis from Scene Graphs",
    "link": "https://arxiv.org/pdf/2312.00093.pdf",
    "upvote": "14",
    "text": "GraphDreamer: Compositional 3D Scene Synthesis from Scene Graphs\nGege Gao1,2,3,4 Weiyang Liu1,5,* Anpei Chen2,3,4 Andreas Geiger3,4,\u2020 Bernhard Sch\u00a8olkopf1,2,4,\u2020\n1Max Planck Institute for Intelligent Systems \u2013 T\u00a8ubingen\n2ETH Z\u00a8urich\n3University of T\u00a8ubingen\n4T\u00a8ubingen AI Center\n5University of Cambridge\n*Directional lead\n\u2020Shared last author\ngraphdreamer.github.io\nWizard\nDesk\nBall\nStack of Spell Books\nBearded\nRobed\nWise-looking\nCrystal\nSmall\nGlowing\nTransparent\nMystical\nWooden\nSturdy\nCarved\nAged\nAncient\nLeather-bound\nWeathered\nRune-etched\nStanding in front of \nPlaced on\nGazing into\nSitting on\nNext to\nScene Graph\nUser Text Input\nPlease follow the examples in the Visual Genome dataset and generate a \nscene graph that best describes the following text: \n\u201cA Wizard standing in front of a Wooden Desk, gazing into a Crystal \nBall placed on the Wooden Desk, with a Stack of Ancient Spell Books \nsitting on the Wooden Desk and next to the crystal ball.\u201d\nNodes (Objects):\n  - Wizard\n  - Wooden Desk\n  - Crystal Ball\n  - Stack of Ancient Spell Books\nEdges (Relationships):\n  - Wizard -> standing in front of -> Wooden Desk\n  - Crystal Ball -> placed on -> Wooden Desk\n  - Wizard -> gazing into -> Crystal Ball\n  - Stack of Ancient Spell Books -> sitting on -> Wooden Desk\n  - Stack of Ancient Spell Books -> next to -> Crystal Ball\nAttributes:\n  - Wizard: bearded, robed, wise-looking\n  - Crystal Ball: small, glowing, transparent, mystical\n  - Stack of Ancient Spell Books: leather-bound, weathered, rune-etched\n  - Wooden Desk: sturdy, carved, aged\nUser\nChatGPT\nAttribute\nRelationship\nObject\nUser can also directly input / edit the scene graph\nOptimization via score distillation sampling\nCompositional 3D Scene\nWizard\nDesk\nScene\nBall\nBooks\nFigure 1. GraphDreamer takes a scene graph as input and generates a compositional 3D scene where each object is fully disentangled. To save the effort of\nbuilding a scene graph from scratch, the scene graph can be generated by a language model (e.g., ChatGPT) from a user text input (left box).\nAbstract\nAs pretrained text-to-image diffusion models become in-\ncreasingly powerful, recent efforts have been made to distill\nknowledge from these text-to-image pretrained models for\noptimizing a text-guided 3D model. Most of the existing\nmethods generate a holistic 3D model from a plain text\ninput. This can be problematic when the text describes a\ncomplex scene with multiple objects, because the vectorized\ntext embeddings are inherently unable to capture a complex\ndescription with multiple entities and relationships. Holistic\n3D modeling of the entire scene further prevents accurate\ngrounding of text entities and concepts. To address this lim-\nitation, we propose GraphDreamer, a novel framework to\ngenerate compositional 3D scenes from scene graphs, where\nobjects are represented as nodes and their interactions as\nedges. By exploiting node and edge information in scene\ngraphs, our method makes better use of the pretrained text-\nto-image diffusion model and is able to fully disentangle\ndifferent objects without image-level supervision. To facil-\nitate modeling of object-wise relationships, we use signed\ndistance fields as representation and impose a constraint to\navoid inter-penetration of objects. To avoid manual scene\ngraph creation, we design a text prompt for ChatGPT to\ngenerate scene graphs based on text inputs. We conduct\nboth qualitative and quantitative experiments to validate the\neffectiveness of GraphDreamer in generating high-fidelity\ncompositional 3D scenes with disentangled object entities.\n1. Introduction\nRecent years have witnessed substantial progresses in text-to-\n3D generation [20, 28, 38], largely due to the rapid develop-\nment made in text-to-image models [31, 32] and text-image\nembeddings [30]. This emerging field has attracted consider-\nable attention due to its significant potential to revolutionize\nthe way artists and designers work.\nThe central idea of current text-to-3D pipelines is to lever-\nage knowledge of a large pretrained text-to-image generative\nmodel to optimize each randomly sampled 2D view of a\n3D object such that these views resemble what the input\ntext describes. The 3D consistency of these 2D views is\ntypically guaranteed by a proper 3D representation (e.g.,\nneural radiance fields (NeRF) [22] in DreamFusion [28]).\nDespite being popular, current text-to-3D pipelines still suf-\nfer from attribute confusion and guidance collapse. Attribute\nconfusion is a fundamental problem caused by text-image\nembeddings (e.g., CLIP [30]). For example, models often\nfail at distinguishing the difference between \u201ca black cat\n1\narXiv:2312.00093v1  [cs.CV]  30 Nov 2023\non a pink carpet\u201d and \u201ca pink cat on a black carpet\u201d. This\nproblem may prevent current text-to-3D generation methods\nfrom accurately grounding all attributes to corresponding\nobjects. As the text prompt becomes even more complex, in-\nvolving multiple objects, attribute confusion becomes more\nsignificant. Guidance collapse refers to the cases where the\ntext prompt is (partially) ignored or misinterpreted by the\nmodel. This typically also happens as the text prompt gets\nmore complex. For example, \u201ca teddy bear pushing a shop-\nping cart and holding baloons\u201d, with \u201cteddy bear\u201d being\nignored. These problems largely limit the practical utility of\ntext-to-3D generation techniques.\nA straightforward solution is to model the multi-object\n3D scene in a compositional way. Following this insight,\nrecent methods [17, 27] condition on additional context in-\nformation such as 3D layout which provides the size and\nlocation of each object in the form of non-overlapping 3D\nbounding boxes. While a non-overlapping 3D layout can cer-\ntainly help to produce a compositional 3D scene with each\nobject present, it injects a strong prior and greatly limits\nthe diversity of generated scenes. The non-overlapping 3D\nbox assumption can easily break when objects are irregular\n(non-cubic) and obscuring each other. For example, the text\nprompt \u201can astronaut riding a horse\u201d can not be represented\nby two non-overlapping bounding boxes. To avoid these\nlimitations while still achieving object decomposition, we\npropose GraphDreamer, which takes a scene graph (e.g.,\n[12]) as input and generates a compositional 3D scene. Un-\nlike 3D bounding boxes, scene graphs are spatially more\nrelaxed and can model complex object interaction. While\nscene graphs are generally easier to specify than spatial 3D\nlayouts, we also design a text prompt to query ChatGPT\nthat enables the automatic generation of a scene graph from\nunstructured text. See Figure 1 for an illustrative example.\nGraphDreamer is guided by the insight that a scene graph\ncan be decomposed into a separate and semantically un-\nambiguous text description of every node and edge1. The\ndecomposition of a scene graph into multiple textual de-\nscriptions makes it possible to distill knowledge from text-\nto-image diffusion models, similar to common text-to-3D\nmethods. Specifically, to allow each object to be disentan-\ngled from the other objects in the scene, we use separate\nidentity-aware positional encoder networks (i.e., object fea-\nture fields) to encode object-level semantic information and\na shared Signed Distance Field (SDF) network to decode\nthe SDF value from identity-aware positional features. The\ncolor value is decoded in a way similar to the SDF value.\nScene-level rendering is performed by integrating objects\nbased on the smallest SDF value at each sampled point in 3D\nspace. More importantly, with both SDF and color values of\neach object, we propose an identity-aware object rendering\nthat, in addition to a global rendering of the entire 3D scene,\n1Cf. the assumption of independent causal mechanisms [33]\nrenders different objects separately. Our local identity-aware\nrendering allows the gradient from the text-dependent dis-\ntillation loss (e.g., score distillation sampling [28]) to be\nback-propagated selectively to corresponding objects with-\nout affecting the other objects. The overall 3D scene will be\nsimultaneously optimized with the global text description to\nmatch the scene semantics to the global text. In summary,\nwe make the following contributions:\n\u2022 To the best of our knowledge, GraphDreamer is the first\n3D generation method that can synthesize compositional\n3D scenes from either scene graphs or unstructured text\ndescriptions. No 3D bounding boxes are required as input.\n\u2022 GraphDreamer uses scene graphs to construct a disentan-\ngled representation where each object is optimized via its\nrelated text description, avoiding object-level ambiguity.\n\u2022 GraphDreamer is able to produce high-fidelity complex 3D\nscenes with disentangled objects, outperforming both state-\nof-the-art text-to-3D methods and existing 3D-bounding-\nbox-based compositional text-to-3D methods.\n\u2022 In Appendix C, we envision a new paradigm of semantic\n3D reconstruction \u2013 Inverse Semantics, where a vision-\nlanguage model (e.g., GPT4-V) is used to extract a scene\ngraph from an input image (i.e., scene graph encoder) and\nGraphDreamer is used to generate a compositional 3D\nscene from the scene graph (i.e., scene graph decoder).\n2. Related Work\nText-to-2D generation. Driven by large-scale image-text\naligned datasets [34], text-to-image generation models [1, 31,\n32] have made great progress in producing highly realistic\nimages. Among these models, generative diffusion models\nlearn to gradually transform a noisy latent z with noise \u03f5\ntypically from a Gaussian distribution, towards image data x\nthat reproduce the semantics of a given text prompt y. This\ngenerative process slowly adds structure to the noise, based\non a weighted denoising score matching objective [11, 26].\n2D-lifting for 3D generation. In contrast to existing text-to-\nimage generation models, text-guided 3D generative mod-\nels [9, 20, 21, 23, 25, 28, 37, 38] usually optimize a 3D\nmodel by guiding its randomly rendered 2D view based on\nthe pretraining knowledge of some text-to-image genera-\ntion model, because of the shortage of text-3D paired assets.\nDreamFusion [28] and subsequent work [4, 16, 36, 41, 43]\npropose to optimize the 3D model by distilling a pretrained\ndiffusion model [31, 32] via score distillation sampling [38].\nGenerate objects with SDF. In text-to-3D generation, re-\ncent works [9, 16, 18, 20, 28, 36, 38, 39, 41] parameterized\nthe 3D scene as a NeRF [2, 22] or a hybrid pipeline com-\nbining NeRFs with a mesh refiner [10, 16, 35, 37]. In our\napproach, we use a signed distance field (SDF) as the ge-\nometry representation of the scene, as we aim at modeling\nmulti-object scenes in a compositional way. In multi-object\n2\nscenes, objects may be coupled in various ways. SDFs ap-\nproximate the implicit surfaces of objects, which, unlike\nNeRFs, prevents unexpected intersections between objects.\nHybrid 3D representation for disentanglement. Another\nline of work uses hybrid representations to learn disentan-\ngled 3D objects [14, 19, 42]. The works [5, 6] put forward\na hybrid approach that represents the face/body as meshes\nand the hair/clothing as NeRFs, enabling a disentangled re-\nconstruction of avatars. [46] adopts this representation and\nproposes a text-to-3D method that generates compositional\nhead avatars. However, the use of a parametric head model\nlimits this method to human head generation. Their disen-\ntanglement only applies to two objects (e.g., face and hair),\nand in contrast, ours can be used for multiple objects.\n3. Preliminaries\nScore distillation sampling (SDS). SDS [28] is a technique\nthat optimizes a 3D model by distilling a pretrained text-to-\nimage diffusion model. Given a noisy image zt rendered\nfrom a 3D model parameterized by \u0398, and a pretrained text-\nto-image diffusion model with a learned noise prediction\nnetwork \u03f5\u03d5(\u00b7), SDS uses \u03f5\u03d5(\u00b7) as a score function that pre-\ndicts the sampled noise \u03f5 contained in xt at noise level t as\n\u02c6\u03f5\u03d5(y, t) = \u03f5\u03d5(zt; y, t), where y is a given conditional text\nembedding. The score is then used to warp the noisy zt\ntowards real image distributions, by guiding the direction of\nthe gradients that update the parameters \u0398 of the 3D model:\n\u2207\u0398L(z; y) = Et,\u03f5\n\u0014\nw(t)\n\u0010\n\u02c6\u03f5\u03d5(y, t) \u2212 \u03f5\n\u0011 \u2202z\n\u2202\u0398\n\u0015\n(1)\nwhere w(t) is a weighting function that depends on t and \u03f5\nis the sampled isotropic Gaussian noise, \u03f5 \u223c N(0, I).\nSDF volume rendering.\nTo render a pixel color of a\ntarget camera, we cast a ray r from the camera center o\nalong its viewing direction d, then sample a series of points\np = o+td in between the near and far intervals [tn, tf]. Fol-\nlowing NeRF [22], the ray color C(r) can be approximated\nby integrating the point samples,\nC(r) =\nN\nX\ni=1\nwici =\nN\nX\ni=1\nTi\u03b1ici\n(2)\nwhere wi is the color weighting function, Ti represents\nthe cumulative transmittance, which is calculated as Ti =\nQi\u22121\nj=1(1 \u2212 \u03b1j), and \u03b1i denotes the piece-wise constant opac-\nity value for the i-th sub-interval, with \u03b1i \u2208 [0, 1].\nUnlike NeRFs that directly predict the density for a given\nposition p, methods based on implicit surface representation\nlearn to map p to a signed distance value with a trainable\nnetwork and extract the density [45] or opacity [40] from\nthe SDF with a deterministic transformation. We extract the\nopacity following NeuS [40]. NeuS formulates the transfor-\nmation function based on an unbiased weighting function,\nwhich ensures that the pixel color is dominated by the inter-\nsection point of camera ray with the zero-level set of SDF,\n\u03b1i = max\n \n\u03a6\u03b2(ui) \u2212 \u03a6\u03b2(ui+1)\n\u03a6\u03b2(ui)\n, 0\n!\n(3)\nwhere \u03a6\u03b2(\u00b7) is the Sigmoid function with a trainable steep-\nness \u03b2, and ui is the SDF value of the sampled position.\n4. Method\nConsider generating a scene of M objects, O = {oi}M\ni=1\nfrom a global text prompt yg. When the scene is complex\nor has many attributes and inter-object relationships to spec-\nify, yg will become very long, and the generation will be\naccompanied by guidance collapse [3, 15]. We thus propose\nto first generate a scene graph G(O) from yg following the\nsetting of [12], which precisely describes object attributes\nand inter-object relationships. We provide an example of a\nfour-object scene in Figure 1 for better illustration.\n4.1. Leveraging Scene Graphs for Text Grounding\nGiven user text input yg, objects {oi}M\ni=1 in the text (which\ncan be detected either manually or automatically, e.g., using\nChatGPT2) form the nodes in graph G(O), as shown in\nFigure 1. To provide more details to an object oi, the user\ncan add additional descriptions, such as \u201cWise-looking\u201d and\n\u201cLeather-bound\u201d, which become the attributes attached to\noi in G(O). Combining oi with all its attributes simply by\ncommas, we get an object prompt y(i) for oi that can be\nprocessed by text encoders.\nThe relationship between each pair of objects oi and oj is\ntransformed into edge ei,j in G(O). For instance, the edge\nbetween node \u201cWizard\u201d and \u201cDesk\u201d is \u201cStanding in front of\u201d.\nFor a graph with M nodes, there are possibly CM\n2\nedges.\nBy combining oi, ei,j, and oj, we obtain edge prompt y(i,j)\nthat exactly defines the pairwise relationship, e.g., \u201cWizard\nstanding in front of Wooden Desk\u201d. Note that there might\nbe no edge between two nodes, e.g., between \u201cWizard\u201d and\n\u201cStack of Ancient Spell Books\u201d. We denote the number of\nexisting edges in G(O) as K, with K \u2264 CM\n2 .\nFrom this example, we also see that using graph G(O)\nis a better way to customize a scene compared to a pure\ntext description yg, in terms of both flexibility in attaching\nattributes to objects and accuracy in defining relationships.\nBy processing the input scene graph, we now obtain a set of\n(1 + M + K) prompts Y\n\u0000G(O)\n\u0001\nas:\nY\n\u0000G(O)\n\u0001\n=\n\b\nyg, y(i), y(i,j) | oi \u2208 O, ei,j \u2208 G(O)\n\t\n(4)\n2ChatGPT4, https://chat.openai.com\n3\nf   (p)\n(1)\nf   (p)\n(2)\nf   (p)\n(3)\nf   (p)\n(4)\n...\n...\n...\n...\nGlobal rendering\nLocal node-wise rendering\nWizard\nDesk\nBall\nStack of Spell Books\nBearded\nRobed\nWise-looking\nCrystal\nSmall\nGlowing\nTransparent\nMystical\nWooden\nSturdy\nCarved\nAged\nAncient\nLeather-bound\nWeathered\nRune-etched\nStanding in front of \nPlaced on\nGazing into\nSitting on\nNext to\nLocal edge-wise rendering\nBearded\nRobed\nWise-looking\nNode text description\nWizard\nSDS Loss\nWizard\nDesk\nStanding in front of \nEdge text description\nRay r\nGlobal text description\nSDS Loss\nSDS Loss\nSDF decoder\nColor decoder\nObject fields\nVolume\nrendering\nA position p\nin 3D space\n p\nSDF-based representation\nIdentity-aware\nvolume rendering\nFigure 2. The overall pipeline of GraphDreamer. Specifically, GraphDreamer first decomposes the scene graph into global, node-wise and edge-wise text\ndescription, and then optimizes the SDF-based objects in the 3D scene using their corresponding text description.\nwhich are used to guide scene generation from the perspec-\ntive of both individual objects and pairwise relationships.\nGraphDreamer consists of three learnable modules: a\npositional feature encoder F\u03b8(\u00b7), a signed distance network\nu\u03d51(\u00b7), and a radiance network c\u03d52(\u00b7). The entire model is\nparameterized by \u0398 = {\u03b8, \u03d51, \u03d52}. There are two goals\nin optimizing GraphDreamer: (i) to model the complete\ngeometry and appearance of each object, and (ii) to ensure\nthat object attributes and interrelationships are in accordance\nwith the scene graph G(O). The overall training process is\nillustrated in Figure 2.\n4.2. Disentangled Modeling of Objects\nPositional encodings are useful for networks to identify the\nlocation it is currently processing. To achieve the first goal\nof making objects separable, we need to additionally identify\nwhich object a position belongs to. Therefore, instead of\none positional feature embedding, we encode a position p\ninto multiple feature embeddings by introducing a set of\npositional hash feature encoders, each parameterized by \u03b8i,\ncorresponding to the number of objects,\nF\u03b8(\u00b7) = {F\u03b8i(\u00b7)}M\ni=1\n\u03b8 = {\u03b8i}M\ni=1\n(5)\nThese feature encoders then form different object fields, i.e.,\none field per object across the same scene space \u2126 \u2282 R3.\nIndividualized object fields. Given a position p \u2208 \u2126, the\nfeature that forms the field of object oi is obtained as:\nf (i)(p) = F\u03b8i(p) \u2208 RF\ni \u2208 {1, \u00b7 \u00b7 \u00b7 , M}\n(6)\nwhere F is the number of feature dimensions, the same for all\nF\u03b8i(\u00b7). Here, for each F\u03b8i(\u00b7) we adopt the multi-resolution\nhash grid encoding from Instant NGP [24] following [16, 36,\n41] to reduce computational cost.\nThese identity-aware feature embeddings are then passed\nto the shared shallow MLPs for SDF and color prediction,\ne.g., the SDF u(i)(p) \u2208 R and color c(i)(p) \u2208 R3 values for\nobject oi\u2019s field are predicted as:\nu(i)(p) = u\u03d51\n\u0000f (i)(p)\n\u0001\nc(i)(p) = c\u03d52\n\u0000f (i)(p)\n\u0001\n(7)\nwhere u(i)(p) indicates the signed distance value from po-\nsition p to the closest surface of object oi, with negative\nvalues inside oi and positive values outside, and c(i)(p) the\ncolor value in oi\u2019s field where only object oi is considered.\nHere, we follow prior work [44] to initialize the SDF approx-\nimately as a sphere. We transform u(i)(p) into opacity with\nEq. (3) as \u03b3(i)(p) for the volume rendering of object fields.\nIn scenes where mutual-obscuring relationships are in-\nvolved, to generate the complete geometry and appearance\nfor each object, we need to make hidden object surfaces visi-\nble. Therefore, the scene needs to be properly decomposed\nbefore rendering the objects.\nScene space decomposition. Intuitively, a position p \u2208 \u2126\ncan be occupied by at most one object. Since the SDF de-\ntermines the boundary of an object, p can thus be identified\nas belonging to object oi if its SDF values {u(i)}M\ni=1 are\nminimized at index i. Based on this, we define an one-hot\nidentity (column) vector \u03bb(p) for each position p as:\n\u03bb(p) = argmax\ni=1,\u00b7\u00b7\u00b7 ,M\n\b\n\u2212u(i)(p)\n\t\n\u2208 {0, 1}M\n(8)\nBased on \u03bb(p), we can decompose the scene into identity-\naware sub-spaces and render each object individually with\nall other objects removed.\nIdentity-aware object rendering. To render object oi, given\na position p, we multiply its opacity \u03b3(i) in oi\u2019s field with\n\u03bb(i)(p) to obtain the opacity for only object oi as:\n\u03b1(i)(p) = \u03bb(i)(p) \u00b7 \u03b3(i) \u2208 [0, +\u221e)\n(9)\nwhere \u03bb(i)(p) is the i-th element in vector \u03bb(p).\nIf\n\u03bb(i)(p) = 1, which means p is identified as most likely\n4\nto be occupied by object oi, we have opacity \u03b1(i)(p) \u2265 0 in\nobject o\u2019s field only, while in all other object fields p will be\nempty. Based on this identity-aware opacity \u03b1(i)(p), we can\nobtain the ray color with object oi present only as:\nC(i)\nr\n=\nX\np\n\u03b1(i)(p)T (i)(p) \u00b7 c(i)(p)\n(10)\nwhere the cumulative transmittance T (i)(pj) is defined fol-\nlowing Eq. (2). By aggregating all rendered pixels, an object\nimage C(i) is obtained, which contains object oi only. With\nC(i) and the object prompt y(i) from the scene graph G(O),\nwe can thus define the object SDS loss following Eq. (1) as:\n\u2207\u0398L(i)\u0000C(i); y(i)\u0001\noi \u2208 O\n(11)\nto supervise GraphDreamer at the object level.\n4.3. Pairwise Modeling of Object Relationships\nBy building up a set of identity-aware object fields, we are\nnow able to render objects in G(O) individually to match\ny(i). To make two related objects oi and oj respect the\nrelationship defined in edge prompt y(i,j), we need to render\noi and oj jointly. Therefore, a combination of two object\nfields is required.\nEdge rendering. Given an edge ei,j connecting nodes oi\nand oj, we can accumulate an edge-wise opacity \u03b1(i,j)(p)\nat position p from opacity values of oi and oj based on the\none-hot identity vector \u03bb(p) as:\n\u03b1(i,j)(p) =\nX\nk\u2208{i,j}\n\u03bb(k)(p) \u00b7 \u03b3(k)(p)\n(12)\nand similarly, an edge-wise color c(i,j)(p) at position p as:\nc(i,j)(p) =\nX\nk\u2208{i,j}\n\u03bb(k)(p) \u00b7 c(k)(p)\n(13)\nfrom which we can render a ray color across two object\nfields of oi and oj following the same integration process as\nEq. (2), and obtain an edge image C(i,j) in with both objects\ninvolved. We can thus define an edge SDS loss as:\n\u2207\u0398L(i,j)\u0000C(i,j); y(i,j)\u0001\nei,j \u2208 G(O)\n(14)\nto match the edge prompt y(i,j).\nScene rendering. To provide global scene graph G(O) con-\nstraints, we further render the whole scene globally by com-\nbining all object fields together. Similarly as for the edge\nrendering, we use \u03bb(p) to accumulated the global opacity\n\u03b1g(p) and a global color c(p) at position p over all object\nfields as:\n\u03b1g(p) = \u03bbT (p) \u00b7 \u03b3(p)\ncg(p) = \u03bbT (p) \u00b7 c(p)\n(15)\nwith \u03b3(p) and c(p) denote the column vectors of opacity\nand color values at p over all object fields. Through the\nintegration process, we can render the entire scene into a\nscene image Cg that represents the entire scene graph. We\ndefine a scene-level SDS loss, \u2207\u0398Lg\u0000Cg; yg\u0001\n, to globally\noptimize GraphDreamer to match the scene prompt yg.\nEfficient SDS guidance. So far, a number of (M + K + 1)\nSDS losses are introduced, corresponding to the number of\nprompts we obtained in Eq. (4). However, optimizing all\nconstraints jointly is intractable. Instead, in each training\nstep, we include two SDS losses only: (i) an object SDS\nloss Eq. (11) for only one object oi, with each step choosing\na different object oi looping through O; (ii) an edge SDS\nloss Eq. (14) of one ei,j connected to oi, looping through\nall existing edges connected to oi. The scene SDS loss is\nincluded only in the training step after each traversal of O,\nand is used solely in that step without any other SDS loss.\nThus, the total SDS loss for optimizing GraphDreamer is:\nLSDS =\n(\n\u2207\u0398L(i) + \u2207\u0398Le,\ni = s % (M + 1)\n\u2207\u0398Lg,\nothers\n(16)\nwhere s indexes the current training step, and e refers to one\nof the edges connected with oi.\n4.4. Training Objectives\nApart from the SDS guidance, to further optimize the pre-\ndiction of unobserved positions, i.e., inside objects and on\nhidden surfaces at object intersections, we include two geom-\netry constraints for physically plausible shape completion.\nPenetration constraint. Since each point p \u2208 \u2126 can be\ninside or on the surface of at most one object o in the set\nof objects O, and for regions outside the actual objects,\nu(p) \u2208 (0, +\u221e) with p \u2208 \u2126 \\ O, we define a penetration\nmeasurement at point p:\nN \u2212(p) =\nM\nX\ni=1\nmax\nn\nsgn\n\u0000\u2212u(i)(p)\n\u0001\n, 0\no\n(17)\nwhere sgn(x) is the sign function. Intuitively, N \u2212(p) mea-\nsures the number of objects inside which the point p is\nlocated, according to the predicted u(i)(p). Using this mea-\nsurement, we propose to implement a penetration constraint,\nLpenet(p) = max\n\b\n0, N \u2212(p) \u2212 1\n\t\n(18)\nto constrains the penetration number N \u2212(p) not to exceed\n1, which is averaged over all sampled points during training.\nEikonal constraint. At each sampled position p, we adopt\nthe Eikonal loss [7] on SDF values u(i)(p) from all object\nfields, formulated as\nLeknl =\n1\nNM\nX\ni,p\n\u0012\r\r\r\u2207u(i)(p)\n\r\r\r\n2 \u2212 1\n\u0013\n(19)\nwhere N is the size of the sample set Pr on ray r.\n5\nA milkshake\nSharing\nTwo macaw \nparrots\nWith two \nstraws\nA cake\nEating\nA badger\nDecorated\nWearing a \nparty hat\nLayered\nWith birthday \ncandles\n\"a hippo biting through a watermelon\"\nA watermelon\nBiting \nthrough\nA hippo\nLarge\nJuicy\n\"two macaw parrots sharing a \nmilkshake with two straws\"\nA shopping cart\nPushing \nWith right hand\nA teddy bear\nFloating\nPushing\nFull of \nvegetables\nA handful of \nballoons\nHolding \nIn left hand\nColorful\n\u201ca Teddy Bear pushing a Shopping Cart full \nof vegetables with right hand and holding a \nHandful Of Balloons in left hand\u201d\n\u201ca badger wearing a party hat and blowing \nout birthday candles on a cake\u201d\nThe piano\nPlaying\nAn octopus\nAn eggshell \nBroken in two\nStanding next to\nA chick\nNewborn\nTwo halves\nYellow\nCracked\nFluffy\n\u201ca DSLR photo of an Eggshell broken in two \nwith an adorable Chick standing next to it\u201d\n\"an octopus playing the piano\"\n\u201ca corgi riding a rocket\u201d\nA rocket\nRiding\nA corgi\n\u201ca Wizard standing in front of a Wooden Desk, \ngazing into a Crystal Ball perched atop the \nWooden Desk, with a Stack of Ancient Spell \nBooks perched atop the Wooden Desk\u201d\n(Scene Graph can be found in Figure 1)\nMVDream\nGraphDreamer\nGraphDreamer \n(w/o graph)\nMagic3D\nFigure 3. Qualitative comparison with baseline approaches and the ablated configuration (w/o graph). GraphDreamer generates scenes with all composing\nobjects being separable. Moreover, with accurate guidance from scene graphs, object attributes and inter-object relationships produced by GraphDreamer\nmatch the given prompts better. We recommend to zoom-in for details.\n6\nCLIP\nScore Magic3D [16] MVDream [36] GraphDreamer\n(w/o graph)\nGraphDreamer\nmean\n0.3267\n0.3102\n0.3019\n0.3326\nstd.\n0.0362\n0.0061\n0.0254\n0.0252\nTable 1. Quantitative results. The mean and standard deviation (std.) values\nare summarized from CLIP scores of 30 multi-object scenes, with the\nnumber of objects in the scene \u2265 2. For better comparison, we provide the\nresult of GraphDreamer (w/o graph), the configuration with the scene graph\nG(O) dropped from GraphDreamer, and thus the conditioning text for all\nrenderings collapse to a single yg.\nTotal training loss. Our final loss function for training\nGraphDreamer thus consists of four terms:\nL\u0398 = \u03b21LSDS + \u03b22Lpenet + \u03b23Leknl\n(20)\nwhere {\u03b21, \u03b22, \u03b23} are hyperparameters.\n5. Experiments and Results\nImplementation details. We adopt a two-stage coarse-to-\nfine optimization strategy following previous work [4, 16,\n28, 36]. In the first stage of 10K denoising steps, we render\nimages of 64\u00d764 resolution only for faster convergence and\nuse DeepFloyd-IF3 [13, 32] as our guidance model, which is\nalso trained to generate 64px images. In the second stage of\n10K steps, the model is refined by rendering 256px images\nand uses Stable Diffusion4 [31] as guidance. Both stages of\noptimization use 1 Nvidia Quadro RTX 6000 GPU.\nBaseline approaches. We report results of two state-of-the-\nart approaches, Magic3D [16] and MVDream [36]. Both\napproaches use a frozen guidance model without additional\nlearnable module [41] and do not have special initialization\nrequirements [4] for geometry. We use the same guidance\nmodels and strategy to train Magic3D, while for MVDream,\nsince it proposes to use a fine-tuned multi-view diffusion\nmodel5, we follow its official training protocol and use its\nreleased diffusion model as guidance. The experimental\nresults of GraphDreamer and the baselines are all obtained\nafter training for 20K steps in total.\nEvaluation criteria. We report the CLIP Score [29] in\nquantitative comparison with baseline models and evaluation\non object decomposition. The metric is defined as:\nCLIPScore(C, y) = cos\n\nEC(C), EY (y)\n\u000b\n(21)\nwhich measures the similarity between a prompt text y for\nan image C and the actual content of the image, with EC(C)\nthe visual CLIP embedding and EY (y) the textual CLIP\nembedding, both encoded by the same CLIP model6.\n3DeepFloyd-IF, huggingface.co/DeepFloyd/IF-I-XL-v1.0\n4Stable diffusion, huggingface.co/stabilityai/stable-diffusion-2-1-base\n5MVDream-sd-v2.1-base-4view, huggingface.co/MVDream/MVDream\n6CLIP B/32, huggingface.co/openai/clip-vit-base-patch32\nCLIP Score\nw. Self Prompt \u2191\nw. Other Prompts \u2193\nmean\nstd.\nmean\nstd.\nGraphDreamer\n0.308\n0.012\n0.201\n0.009\nTable 2. The CLIP scores of individual object images C(i). Metric w. Self\nPrompt refers to scores calculated between C(i) and its own prompt y(i),\nand w. Other Prompts between C(i) and prompts of all other objects in\nthe same scene {y(j), j \u0338= i, oj \u2208 O}. Detailed experimental settings and\nanalysis on these figures as well as the on the chart showing in Figure 4,\ncan be found in Subsection 5.3.\nObject(s)\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nCLIP Score\nmean \u00b1 std.\nmean \u00b1 2 std.\nmean \u00b1 3 std.\nw. Self Prompt\nw. Other Prompts\nFigure 4. Error bands of object CLIP scores.\n5.1. Comparison to Other Methods\nWe report quantitative results in Table 1 and qualitative re-\nsults in Figure 3. The figures in Table 1 are summarized\nfrom CLIP scores of 30 multi-object scenes, with the num-\nber of objects \u2265 2. GraphDreamer (full model) achieves the\nhighest CLIP score. Qualitative results shown in Figure 3\nalso suggest that GraphDreamer is generally more applica-\nble in producing multiple objects in a scene with various\ninter-object relationships. As can be observed from baseline\nresults, semantic mismatches are more commonly found,\nsuch as, in the example of \u201ctwo macaw parrots sharing a\nmilkshake with two straws\u201d, Magic3D generates two \u201cmilk-\nshakes\u201d and MVDream produces one \u201cparrot\u201d only, which\nmixed up the number attribute of \u201cmilkshake\u201d and \u201cparrots\u201d,\nand in the example of \u201da hippo biting through a watermelon\u201d,\ntwo objects \u201chippo\u201d and \u201cwatermelon\u201d are blended into one\nobject; GraphDreamer, on the other hand, models both in-\ndividual attributes and inter-object relationships correctly\nbased on the guidance of the input scene graph.\n5.2. Ablation Study\nTo evaluate the effect of introducing scene graph G(O) as\nguidance on preventing guidance collapse, we consider to\ndrop G(O) from GraphDreamer, termed w/o graph, and\ncompare the results in Table 1 and Figure 3. By removing\nG(O), the conditioning text for the scene collapses from the\nset of prompts defined in Eq. (4) to the global prompt yg only.\nAs reported in the fourth column of Table 1, the mean CLIP\nscore of the ablated configuration decreases by more than 3\nstd. compared to the full model, indicating that the ability\nof this configuration to generate 3D scenes that match given\nprompts is significantly reduced. The results in Figure 3 also\n7\nA stone egg\nHatching \nout of\nA baby \ndragon\n\u201ca baby dragon hatching \nout of a stone egg\u201d\n\u201ca baby bunny sitting on \ntop of a stack of pancakes\u201d\nA large basket of \nrainbow macarons\nStanding on\nA blue jay\nA stack of \npancakes\nSitting on top of\nA baby bunny\n\u201ca blue jay standing on a large \nbasket of rainbow macarons\u201d\n\u201can astronaut riding \na brown horse\u201d\nRiding\nAn \nastronaut\nBrown\nA horse\nMermaid\nTreasure \nchest\nCoral \nthrone\nSitting \non \nGuarding\nLocated next to \nShimmering tail\nEthereal\nLong \nflowing hair \nBarnacle-\nencrusted\nAncient\nOverflowing \nwith coins\nColorful\nIntricate\nLiving coral\n\u201ca fantasy scenario of a Mermaid sitting \non a Coral Throne, with a Treasure \nChest located next to the Coral Throne\u201d\n\u201ca gummy bear playing \nthe saxophone\u201d\nThe saxophone\nPlaying\nA gummy bear\nFigure 5. More qualitative compositional examples from GraphDreamer.\ncorroborate such a decline, given that the problems such as\nmissing objects and attribute confusion arise again. Both\nevaluations suggest the need for the scene graph G(O) in\nGraphDreamer for combating guidance collapse problems.\n5.3. Decomposition Analysis\nWe consider scenes with the number of objects \u2265 2. To\nfurther quantitatively evaluate whether objects in a scene are\nwell separated and rendered into object images individually,\nwe calculate two CLIP metrics for each object image C(i):\n(i) w. Self Prompt (abbr., wSP) refers to the CLIP score be-\ntween C(i) and its own object prompt y(i), and (ii) w. Other\nPrompts (abbr., wOP) refers to the CLIP scores between\nC(i) and all other object prompts {y(j), j \u0338= i, oj \u2208 O} in\nthe same scene. Intuitively, if a scene is well decomposed,\neach object image C(i) should contain one object oi only\nwithout any part of other objects, and thus the scores wSP\nshould be much higher than the scores wOP. Table 2 re-\nports a statistical summary on the metrics of 64 objects, for\neach object, we render images from 4 orthogonal views C(i)\nv\n(v = 1, 2, 3, 4), and thus we get 4 wSP scores and multiple\nwOP scores per object. We calculate the mean and standard\ndeviation (std.) of these wSP and wOP scores separately\nover the view images. The figures reported in the table are\naveraged over all 64 objects. The mean and std. values are\nalso presented in an error band graph in Figure 4, where\nthe x-axis is the index of objects. From this graph it can be\nobserved more obviously that the wSP CLIP score is signif-\nicantly higher than the wOP CLIP score, without overlap\nbetween the mean wSP scores and the mean \u00b1 3 std. error\nband of the wOP scores, which shows that the scenes are\nproperly decomposed into individual objects. More compo-\nsitional examples can be found in Figure 5.\n6. Concluding Remarks\nThis paper proposes GraphDreamer, which generates compo-\nsitional 3D scenes from scene graphs or text (by leveraging\nGPT4-V to produce a scene graph from the text). Graph-\nDreamer first decomposes the scene graph into global, node-\nwise and edge-wise text descriptions and then optimizes the\nSDF-based objects with the SDS loss from their correspond-\ning text descriptions. We conduct extensive experiments to\nshow that GraphDreamer is able to prevent attribute confu-\nsion and guidance collapse, generating disentangled objects.\n8\nAcknowledgment\nThe authors would like to thank Yao Feng, Zhen Liu, Zeju\nQiu, Yandong Wen and Yuliang Xiu for proofreading the\ndraft and providing many useful suggestions. Weiyang Liu\nand Bernhard Sch\u00a8olkopf was supported by the German Fed-\neral Ministry of Education and Research (BMBF): T\u00a8ubingen\nAI Center, FKZ: 01IS18039B, and by the Machine Learn-\ning Cluster of Excellence, the German Research Foundation\n(DFG): SFB 1233, Robust Vision: Inference Principles and\nNeural Mechanisms, TP XX, project number: 276693517.\nAndreas Geiger and Anpei Chen were supported by the ERC\nStarting Grant LEGO-3D (850533) and the DFG EXC num-\nber 2064/1 - project number 390727645.\nReferences\n[1] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,\nJiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala,\nTimo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, and\nMing-Yu Liu. ediff-i: Text-to-image diffusion models with\nan ensemble of expert denoisers, 2023.\n[2] Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter\nHedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan.\nMip-nerf: A multiscale representation for anti-aliasing neural\nradiance fields. In Proc. of the IEEE International Conf. on\nComputer Vision (ICCV), 2021.\n[3] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel\nCohen-Or. Attend-and-excite: Attention-based semantic guid-\nance for text-to-image diffusion models. ACM Trans. on\nGraphics, 2023.\n[4] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fan-\ntasia3D: Disentangling geometry and appearance for high-\nquality text-to-3D content creation. In Proc. of the IEEE\nInternational Conf. on Computer Vision (ICCV), 2023.\n[5] Yao Feng, Jinlong Yang, Marc Pollefeys, Michael J Black,\nand Timo Bolkart. Capturing and animation of body and\nclothing from monocular video. In SIGGRAPH Asia, 2022.\n[6] Yao Feng, Weiyang Liu, Timo Bolkart, Jinlong Yang, Marc\nPollefeys, and Michael J Black. Learning disentangled avatars\nwith hybrid 3D representations. arXiv.org, 2309.06441, 2023.\n[7] Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and\nYaron Lipman. Implicit geometric regularization for learn-\ning shapes. In Proc. of the International Conf. on Machine\nlearning (ICML), 2020.\n[8] Jonathan Ho and Tim Salimans. Classifier-free diffusion\nguidance. In Advances in Neural Information Processing\nSystems (NIPS), 2021.\n[9] Ajay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter Abbeel,\nand Ben Poole. Zero-shot text-guided object generation with\ndream fields. Proc. IEEE Conf. on Computer Vision and\nPattern Recognition (CVPR), 2022.\n[10] Ajay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter Abbeel,\nand Ben Poole. Zero-shot text-guided object generation with\ndream fields, 2022.\n[11] Diederik P. Kingma, Tim Salimans, Ben Poole, and Jonathan\nHo. Variational diffusion models. arXiv.org, 2107.00630,\n2023.\n[12] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and\nFei-Fei Li. Visual genome: Connecting language and vision\nusing crowdsourced dense image annotations. International\nJournal of Computer Vision (IJCV), 2017.\n[13] DeepFloyd Lab. DeepFloyd, 2023.\n[14] Gengyan Li, Abhimitra Meka, Franziska Mueller, Marcel C\nBuehler, Otmar Hilliges, and Thabo Beeler. Eyenerf: a hy-\nbrid representation for photorealistic synthesis, animation and\nrelighting of human eyes. ACM Trans. on Graphics, 2022.\n[15] Yumeng Li, Margret Keuper, Dan Zhang, and Anna Khoreva.\nDivide & bind your attention for improved generative seman-\ntic nursing. arXiv.org, 2307.10864, 2023.\n[16] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-\nYu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-\n3d content creation. Proc. IEEE Conf. on Computer Vision\nand Pattern Recognition (CVPR), 2023.\n[17] Yiqi Lin, Haotian Bai, Sijia Li, Haonan Lu, Xiaodong Lin,\nHui Xiong, and Lin Wang. Componerf: Text-guided multi-\nobject compositional nerf with editable 3D scene layout.\narXiv.org, 2303.13843, 2023.\n[18] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov,\nSergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot\none image to 3d object. In ICCV, 2023.\n[19] Weiyang Liu, Zhen Liu, Liam Paull, Adrian Weller, and Bern-\nhard Sch\u00a8olkopf. Structural causal 3d reconstruction. In Proc.\nof the European Conf. on Computer Vision (ECCV), 2022.\n[20] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and\nDaniel Cohen-Or. Latent-nerf for shape-guided generation of\n3D shapes and textures. In Proc. IEEE Conf. on Computer\nVision and Pattern Recognition (CVPR), 2023.\n[21] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and\nRana Hanocka. Text2mesh: Text-driven neural stylization for\nmeshes. Proc. IEEE Conf. on Computer Vision and Pattern\nRecognition (CVPR), 2022.\n[22] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In Proc. of the European Conf. on Computer Vision\n(ECCV), 2020.\n[23] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky,\nand Tiberiu Popa. Clip-mesh: Generating textured meshes\nfrom text using pretrained image-text models. In SIGGRAPH\nAsia, 2022.\n[24] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a multires-\nolution hash encoding. ACM Trans. on Graphics, 2022.\n[25] Gimin Nam, Mariem Khlifi, Andrew Rodriguez, Alberto\nTono, Linqi Zhou, and Paul Guerrero. 3d-ldm: Neural implicit\n3d shape generation with latent diffusion models, 2022.\n[26] Alex Nichol and Prafulla Dhariwal. Improved denoising\ndiffusion probabilistic models. arXiv.org, 2021.\n[27] Ryan Po and Gordon Wetzstein. Compositional 3D scene\ngeneration using locally conditioned diffusion. arXiv.org,\n2303.12218, 2023.\n9\n[28] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Milden-\nhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv.org,\n2209.14988, 2022.\n[29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervision.\narXiv.org, 2103.00020, 2021.\n[30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In Proc. of the International Conf. on Machine learning\n(ICML), 2021.\n[31] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proc. IEEE Conf.\non Computer Vision and Pattern Recognition (CVPR), 2022.\n[32] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay\nWhang, Emily Denton, Seyed Kamyar Seyed Ghasemipour,\nBurcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes,\nTim Salimans, Jonathan Ho, David J Fleet, and Mohammad\nNorouzi. Photorealistic text-to-image diffusion models with\ndeep language understanding. In Advances in Neural Infor-\nmation Processing Systems (NIPS), 2022.\n[33] B. Sch\u00a8olkopf*, F. Locatello*, S. Bauer, R. Nan Ke, N. Kalch-\nbrenner, A. Goyal, and Y. Bengio. Towards causal representa-\ntion learning. Proceedings of the IEEE, 2021.\n[34] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes,\nAarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick\nSchramowski, Srivatsa Kundurthy, Katherine Crowson, Lud-\nwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion-\n5b: An open large-scale dataset for training next generation\nimage-text models. In Advances in Neural Information Pro-\ncessing Systems (NIPS), 2022.\n[35] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and\nSanja Fidler. Deep marching tetrahedra: a hybrid representa-\ntion for high-resolution 3d shape synthesis. In Advances in\nNeural Information Processing Systems (NIPS), 2021.\n[36] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li,\nand Xiao Yang.\nMvdream: Multi-view diffusion for 3D\ngeneration. arXiv.org, 2308.16512, 2023.\n[37] Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni,\nMichael Niemeyer, and Federico Tombari. Textmesh: Gener-\nation of realistic 3D meshes from text prompts, 2023.\n[38] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh,\nand Greg Shakhnarovich. Score jacobian chaining: Lifting\npretrained 2d diffusion models for 3D generation. In Proc.\nIEEE Conf. on Computer Vision and Pattern Recognition\n(CVPR), 2023.\n[39] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,\nand Greg Shakhnarovich. Score jacobian chaining: Lifting\npretrained 2d diffusion models for 3d generation. In CVPR,\n2023.\n[40] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\nKomura, and Wenping Wang. Neus: Learning neural im-\nplicit surfaces by volume rendering for multi-view reconstruc-\ntion. In Advances in Neural Information Processing Systems\n(NeurIPS), 2021.\n[41] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and\ndiverse text-to-3D generation with variational score distilla-\ntion. arXiv.org, 2305.16213, 2023.\n[42] Jiamin Xu, Zihan Zhu, Hujun Bao, and Weiwei Xu. Hybrid\nmesh-neural representation for 3D transparent object recon-\nstruction. arXiv.org, 2203.12613, 2022.\n[43] Jiale Xu, Xintao Wang, Weihao Cheng, Yan-Pei Cao, Ying\nShan, Xiaohu Qie, and Shenghua Gao. Dream3d: Zero-shot\ntext-to-3d synthesis using 3d shape prior and text-to-image\ndiffusion models. In CVPR, 2023.\n[44] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan\nAtzmon, Basri Ronen, and Yaron Lipman. Multiview neural\nsurface reconstruction by disentangling geometry and appear-\nance. In Advances in Neural Information Processing Systems\n(NIPS), 2020.\n[45] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Vol-\nume rendering of neural implicit surfaces. In Advances in\nNeural Information Processing Systems (NeurIPS), 2021.\n[46] Hao Zhang, Yao Feng, Peter Kulits, Yandong Wen, Justus\nThies, and Michael J Black. Text-guided generation and\nediting of compositional 3D avatars. arXiv.org, 2309.07125,\n2023.\n10\nAppendix\nTable of Contents\nA. Experimental Settings\n12\nA.1. Additional Implementation Details\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\nA.2. Hyperparameter Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\nB. Additional Experiments and Results\n13\nB.1. Ablation: Penetration Constraint . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\nB.2. User Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\nB.3. More Qualitative Comparison to Other Text-to-3D Methods\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n15\nC. Inverse Semantics: A Paradigm of Semantic Reconstruction with GPT4V-guided GraphDreamer\n16\nD. Failure Cases and Limitations\n18\n11\nA. Experimental Settings\nA.1. Additional Implementation Details\nPoint-wise identity vector \u03bb(p).\nAs defined in Eq. (8), identity vector \u03bb(p) at a given position p is defined as an one-hot\nvector for a better disentanglement in the forward rendering process. To combine \u03bb(p) into differentiate training, we customize\nthe gradient of \u03bb(p) in back-propagation by combining with a Softmax operation:\n\u03bb+(p) = \u03bb(p) +\nn\ns(p) \u2212 sg\n\u0002\ns(p)\n\u0003o\ns(p) = Softmax\n\u0000\u2212u(p)\n\u0001\n(22)\nwhere second term s(p) \u2212 sg[s(p)] is zero in value, with sg(\u00b7) standing for the stop-gradient (e.g., .detach() in PyTorch)\noperation, and thus contributes only the gradient for updating GraphDreamer.\nPenetration Constraint Lpenet.\nIn Eq. (18), we introduced a rather intuitive definition for the penetration constraint, which\nis, however, not continuous over object identities i \u2208 {1, \u00b7 \u00b7 \u00b7 , M}. To further refine this constraint, we implement Lpenet as:\nLpenet =\n1\n(M \u2212 1)N\nX\nj\u0338=i,p\nn\nReLU\n\u0002\nd(p) \u2212 u(p)\n\u0003o2\nd(p) = ReLU\n\u0002\n\u03bb+(p) \u00b7 \u2212u(p)\n\u0003\n(23)\nwhere N is the size of sampled positions on the current ray, \u03bb+(p) is the differentiable identity vector defined in Eq. (22),\nand term d(p) is an one-hot vector. If p is inside or on the surface of object o(i), and u(i)(p) gets the minimum among other\nobject SDFs, the only non-negative element of d(p) equals the absolute value of u(i)(p). Then, Lpenet prevents all other SDF\nvalues (j \u0338= i) from being negative. Or else, if p is outside of all objects, Lpenet has not impact on u(p). An ablation study for\nthis constraint can be found in Section B.\nA.2. Hyperparameter Settings\nTraining loss coefficients. We set coefficients {\u03b21, \u03b22, \u03b23} defined in Eq. (20) based on the magnitude of each loss term, as\n\u03b21 = 1 for LSDS, \u03b22 = 100 for Lpenet, and \u03b23 = 10 for Leknl.\nClassifier free guidance. Classifier-free guidance [8] weight (CFG-w) is a hyperparameter that trades off image quality and\ndiversity for guiding the 2D view. We schedule CFG-w for the training of GraphDreamer roughly based on the number of\nobjects M as below, considering that the larger the number M, the more difficult it is for mode seeking.\nMethod\nGraphDreamer\nMagic3D\nMVDream\n(Nmber of Objects)\nM = 2\nM \u2265 3\nCoarse Stage (<10K steps)\n50 (IF)\n100 (IF)\n100 (IF)\n50 (MVSD)\nFine Stage (10K\u223c20K steps)\n50 (SD)\n50 (SD)\n50 (SD)\nTable 3. CFG-w settings for GraphDreamer and baseline approaches. IF stands for DeepFloyd-IF model, SD for Stable diffusion, and MVSD for MVDream\nStable diffusion, as detailed in Section 5 (para. Baseline approaches).\n12\nB. Additional Experiments and Results\nB.1. Ablation: Penetration Constraint\nAs introduced in Section 4.4 and defined practically in Eq. (23), the purpose of using the penetration constraint Lpenet is to\nprevent unexpected penetrations between the implicit surfaces of objects, represented by SDF u(i)(p), in a multi-object scene.\nTo verify the necessity of this constraint, we ablate Lpenet in training GraphDreamer and report the quantitative and qualitative\nresults of this ablated configuration, denoted as GraphDreamer w/o Lpenet, in Table 4 and Figure 6.\n\u201ca baby bunny sitting on \ntop of a stack of pancakes\u201d\nA stack of \npancakes\nSitting on top of\nA baby bunny\n\"a silver knight galloping \na chestnut horse\"\na horse\ngalloping\na silver knight\nchestnut\n\"a fox working on a jigsaw \npuzzle, cute, cartoon\"\na jigsaw puzzle\nworking on\na fox\ncute, cartoon\nA milkshake\nSharing\nTwo macaw \nparrots\nWith two \nstraws\n\"two macaw parrots sharing a \nmilkshake with two straws\"\nGraphDreamer\nGraphDreamer \n(w/o \u2112\ud835\udc91\ud835\udc86\ud835\udc8f\ud835\udc86\ud835\udc95)\nGraphDreamer\nGraphDreamer \n(w/o \u2112\ud835\udc91\ud835\udc86\ud835\udc8f\ud835\udc86\ud835\udc95)\nFigure 6. Ablation study: GraphDreamer (w/o Lpenet) stands for the configuration that trains GraphDreamer without penetration constraint Lpenet(p).\nAs shown in Figure 6, objects generated without Lpenet are more likely to inter-penetrate; as a result, it is much harder to\nidentify clean segmentation boundaries between objects. The CLIP scores shown in Table 4 also suggest a degradation in the\nperformance of this ablated configuration in modeling individual objects, as the mean similarities of both object image C(i)\nwith its own prompt y(i) (w. Self Prompt) and scene image Cg with global prompt yg (Global) drop down significantly, while\nthe mean similarity of C(i) with other object prompts (w. Other Prompts) increases slightly.\nCLIP Score\nw. Self Prompt\nw. Other Prompts\nGlobal\nmean (\u2191)\nstd.\nmean (\u2193)\nstd.\nmean (\u2191)\nstd.\nGraphDreamer (w/o Lpenet)\n0.2665\n0.0091\n0.2070\n0.0087\n0.3064\n0.0210\nGraphDreamer\n0.3077\n0.0121\n0.2006\n0.0085\n0.3326\n0.0252\nTable 4. Ablation study of the penetration constraint (with or without Lpenet).\nB.2. User Study\nMethods\nMVDream\nMagic3D\nGraphDreamer\nSelected (%)\n23.12\n14.62\n62.26\nTable 5. User study: selecting one from three generated results that best aligns with given text prompts. The results are collected from 31 users and\nsummarized over 30 multi-object prompts.\nTo further evaluate the performance of GraphDreamer in generating guidance-compliant multi-object scenes, we conduct a\nsurvey of the results generated by the baseline approaches and GraphDreamer over 30 multi-object text prompts. We invite\n31 raters for this study, and for each prompt, the raters are asked to select one of the three results that semantically best fits\n13\nA milkshake\nSharing\nTwo macaw \nparrots\nWith two \nstraws\nA cake\nEating\nA badger\nDecorated\nWearing a \nparty hat\nLayered\nWith birthday \ncandles\n\"a hippo biting through a watermelon\"\nA watermelon\nBiting \nthrough\nA hippo\nLarge\nJuicy\nA shopping cart\nPushing \nWith right hand\nA teddy bear\nFloating\nPushing\nFull of \nvegetables\nA handful of \nballoons\nHolding \nIn left hand\nColorful\n\"two macaw parrots sharing a \nmilkshake with two straws\"\n\u201ca Teddy Bear pushing a Shopping Cart full of vegetables with \nright hand and holding a Handful Of Balloons in left hand\u201d\n\u201ca badger wearing a party hat and \nblowing out birthday candles on a cake\u201d\nThe piano\nPlaying\nAn octopus\nAn eggshell \nBroken in two\nStanding next to\nA chick\nNewborn\nTwo halves\nYellow\nCracked\nFluffy\n\u201ca DSLR photo of an Eggshell broken in two \nwith an adorable Chick standing next to it\u201d\n\"an octopus playing the piano\"\n\u201ca corgi riding a rocket\u201d\nA rocket\nRiding\nA corgi\n\u201ca Wizard standing in front of a Wooden Desk, \ngazing into a Crystal Ball perched atop the \nWooden Desk, with a Stack of Ancient Spell \nBooks perched atop the Wooden Desk\u201d\n(Scene Graph can be found in Figure 1)\nFantasia3D\nGraphDreamer\nDreamFusion\nCase failed in texture training stage.\nFigure 7. Extended qualitative comparison with more baseline approaches. Note that Fantasia3D relies on the SDF initialization, which may achieve better\nperformance if fine-grind initial shapes are provided. Here, we report results using default sphere initialization.\n14\nthe given prompt (if all/none of the results fit well, select the one that has the highest fidelity). The answers are summarized\nin Table 5. In general, more raters (62.26%) selected the results produced by GraphDreamer over the other two baseline\napproaches, considering our results to be more consistent with the given prompts.\nB.3. More Qualitative Comparison to Other Text-to-3D Methods\nTo provide a more comprehensive comparison on the performance of recent TT3D methods in generating multi-object\nscenes, we extend the qualitative comparison reported in Figure 3 with two more baseline approaches, DreamFusion [28] and\nFantasia3D [4]. Both baseline results are obtained after 20K training steps. For DreamFusion, we adopt the same two-stage\ntraining protocol as GraphDreamer and Magic3D, with details provided in Section 5. As shown in Figure 7, problems related\nto guidance collapse occur in both methods, while Fantasia3D failed in some cases. Note the performance of the Fantasia3D\nmethodology may vary depending on how the shape of the SDF surface is initialized, whereas here we have only performed the\ndefault sphere initialization (with a radius of 0.5). We have also conducted generation experiments with ProlificDreamer [41],\nwhich adopts a three-stage optimization strategy, and yet it still failed to generated any content in these multi-object cases after\n20K steps of training in the first stage (both setting CFG-w to 100 or 200), and the results are thus not included.\nVideos of Figure 3 and Figure 5 can be found in our project page.\n15\nC. Inverse Semantics: A Paradigm of Semantic Reconstruction with GPT4V-guided GraphDreamer\nIn this section, we envision a new paradigm, called inverse semantics, which first reconstructs a scene graph from an input\nimage and then produces a compositional 3D scene based on this scene graph. We call it inverse semantics, because it\nresembles the idea of inverse graphics in a high-level sense. Inverse semantics differs from inverse graphics in the aspect of\nreconstruction that is emphasized; it focuses on interpreting semantic meaning rather than reconstructing visual details. The\ncomparison between inverse graphics and inverse semantics is given as follows:\nInverse Semantics\n(a) The traditional analysis-by-synthesis reconstruction pipeline\nPhysical parameters\n(Albedo, Geometry, Lighting, Viewpoint, etc.)\nImage\nImage\nEncoding\nRendering\n3D representation\n(b) The semantic reconstruction pipeline\nScene Graph\nImage\nImage\nEncoding\nRendering\nInverse Graphics\nObject-centric 3D representation\nDecoding\nPixel-level reconstruction\nSemantics-level reconstruction\nFigure 8. A paradigm comparison between inverse graphics and inverse semantics\nSpecifically, we can implement the inverse semantics paradigm with a GPT4V-guided GraphDreamer. We first use GPT4V\nto obtain the scene graph from an input image, and then apply GraphDreamer to generate a compositional 3D scene based on\nthis scene graph. Enhanced by GPT4V\u2019s powerful image understanding capabilities, we can obtain a detailed scene graph from\nthe input image and generate a 3D scene from the graph that semantically inverse the given image. An qualitative example of\nour inverse semantics paradigm is provided in Figure 9.\nNodes (Objects):\n- Man\n- Hat\n- Shotgun\nEdges (Relationships):\n- Man -> wearing (on head) -> Hat\n- Man -> holding with both hands -> Shotgun\nAttributes:\n- Man: elderly, grey bearded, long grey hair\n- Hat: wide-brimmed, pointed top, grey\n- Shotgun: automatic, curved magazine, metallic\nChatGPT\nPlease generate a scene graph that best \ndescribes the image in the style of the \nVisual Genome dataset, restricting to \nthe most salient 3 nodes.\nUser\nMan\nHat\nelderly\nShotgun\ngrey \nbearded\nlong\ngrey hair\nwide-brimmed\npointed\ntop\ngrey\nautomatic\nmetallic\nwearing\non head\nholding\nwith both hands\ncurved \nmagazine\nFigure 9. A qualitative example of the inverse semantics paradigm. We generate a scene graph directly from an input image with GPT4V and restrict the\nnodes present to the most salient ones. GPT4V identifies the objects and provides with proper attributes and edges for each object. This makes it possible to\ninverse modeling the semantics of a given scene image and extends the potential applications of GraphDreamer.\n16\nMoreover, it is also possible to extract rough estimates of the center coordinates of the objects in the graph from the image\nwith GPT4V. The coordinate estimates are in 2D (the up-right plane) only, since the image is in 2D. For the third coordinate\n(on the front axis), GPT4V can offer a relative order by layering objects from the viewer\u2019s perspective. Then, with the SDF\nrepresentation of 3D objects, we can use these estimates as the sphere centers when initializing object SDFs u(i)(p) to specify\nmore accurately the spatial relationships of the objects, and thus to make the generated 3D scene better match the given\nimage. We expect that the inverse semantics paradigm could be of general interest to the community, and our GPT4V-guided\nGraphDreamer serves as an important stepping stone towards this direction.\n\u2026\nNodes (Objects):\n- Kirby (Character)\n- Table\n- Lamp\n- Bookshelf\n- Cushion (Soft Surface)\nEdges (Relationships):\n- Kirby -> resting on -> Cushion\n- Lamp -> place on -> Table\n- Lamp -> casting light on -> Kirby\n- Bookshelf -> standing beside -> Table\n- Cushion -> positioned in front of -> Table\nAttributes:\n- Kirby (Character): pink, sleeping, round\n- Table: wooden, sturdy, low\n- Lamp: yellow light, illuminating, bright\n- Bookshelf: organized, stocked with books, holding plant\n- Cushion (Soft Surface): soft, plush, comfortable\nChatGPT\nPlease generate a scene graph that best describes \nthe image, in the style of the Visual Genome \ndataset, restricting to the most salient 5 nodes.\nUser\nUser\nCan you try to estimate the center coordinates of the objects in \nthe scene graph? Try your best to give me a rough estimate.\nKirby\nCushion\npink\nLamp\ncasting\nlight on\nsleeping\nround\nlow\nresting\non\nyellow\nlight\ncomfortable\nplush\nBookshelf\nTable\npositioned\nin front of\nplaced\non\nstanding\nbeside\nsturdy\nwooden\norganized\nsoft\nholding plant\nbright\nilluminating\nstocked with books\nFigure 10. An example of more complex semantics represented with more nodes. ChatGPT structures the semantics in this complex image into a scene graph\nusing the most salient node. To inverse such semantics and generated the scene in 3D, one may further ask GPT to provide with center coordinates for each\nnode, which can make the semantic inverse more precise in terms of spatial relations.\n17\nD. Failure Cases and Limitations\nThe piano\nPlaying\nA piglet\n\"an piglet playing the piano\"\nThe violin\nPlaying\nAn astronaut\n\"an astronaut playing the violin\"\n\u201cA Hero Character confronting a Dragon that \nprotecting a Crystal, fantasy game scenario\u201d\nDragon\nconfronting\nHero\nCharacter \nGlowing\nDetermined\nLarge\nCrystal\nseeking\nMystical\nTerrifying\nPower\nsource\nprotecting\n(a)\n(b)\n(c)\nFigure 11. Examples of some failure cases. (a) In some cases, results of GraphDreamer may come with the Janus problem. (b) GraphDreamer failed to\nseparate the violin from the astronaut\u2019s left hand, which is closely held by that hand. (c) One leg of the Hero is missing, potentially due to the obscuring of the\nCrystal and, deeper down, to the lack of priors for human shape.\nGraphDreamer still has some limitations. First, the generation quality of a single object is still largely limited by the SDS\noptimization. The commonly observed multi-head problems (i.e., Janus problem) may also appear in GraphDreamer. See\nFigure 11(a) for an example. The piglet in the generated results exhibit multiple heads from different viewpoints. Therefore,\nseeking a better distillation loss from 2D pretrained diffusion model is of great importance.\nSecond, the decomposition of different objects may sometimes fail. See Figure 11(b) for an example. This may be caused\nby the semantic dominance of one object over another. We can observe that the disentangled astronaut still looks like a\nreasonable astronaut, but the disentangled violin is affected by some parts of the astronaut. We suspect that the semantic\nmeaning of an astronaut is more dominant than the violin. This may cause the whole optimization process to favor more on the\nsemantic meaning of the astronaut node.\nThird, the semantic meaning of some object in the scene may be incomplete. See Figure 11(c) for an example. The hero\ncharacter lacks an leg in the generated result. This phenomenon is caused by the lack of explicit human geometric priors.\nWithout the knowledge of an explicit human shape, the generated 3D human may be affected by severe occlusion. Therefore,\nwhen generating a 3D scene that involves humans, it can be more beneficial to consider human body priors.\n18\n"
  },
  {
    "title": "HiFi Tuner: High-Fidelity Subject-Driven Fine-Tuning for Diffusion Models",
    "link": "https://arxiv.org/pdf/2312.00079.pdf",
    "upvote": "13",
    "text": "HiFi Tuner:\nHigh-Fidelity Subject-Driven Fine-Tuning for Diffusion Models\nZhonghao Wang1,2, Wei Wei4, Yang Zhao1, Zhisheng Xiao1,\nMark Hasegawa-Johnson2, Humphrey Shi2,3, Tingbo Hou1\n1Google, 2UIUC, 3Georgia Tech, 4Accenture\nAbstract\nThis paper explores advancements in high-fidelity per-\nsonalized image generation through the utilization of pre-\ntrained text-to-image diffusion models. While previous ap-\nproaches have made significant strides in generating ver-\nsatile scenes based on text descriptions and a few input\nimages, challenges persist in maintaining the subject fi-\ndelity within the generated images. In this work, we in-\ntroduce an innovative algorithm named HiFi Tuner to en-\nhance the appearance preservation of objects during per-\nsonalized image generation. Our proposed method employs\na parameter-efficient fine-tuning framework, comprising a\ndenoising process and a pivotal inversion process.\nKey\nenhancements include the utilization of mask guidance, a\nnovel parameter regularization technique, and the incor-\nporation of step-wise subject representations to elevate the\nsample fidelity. Additionally, we propose a reference-guided\ngeneration approach that leverages the pivotal inversion\nof a reference image to mitigate unwanted subject varia-\ntions and artifacts.\nWe further extend our method to a\nnovel image editing task: substituting the subject in an im-\nage through textual manipulations. Experimental evalua-\ntions conducted on the DreamBooth dataset using the Stable\nDiffusion model showcase promising results. Fine-tuning\nsolely on textual embeddings improves CLIP-T score by 3.6\npoints and improves DINO score by 9.6 points over Tex-\ntual Inversion. When fine-tuning all parameters, HiFi Tuner\nimproves CLIP-T score by 1.2 points and improves DINO\nscore by 1.2 points over DreamBooth, establishing a new\nstate of the art.\n1. Introduction\nDiffusion models [14, 37] have demonstrated a remark-\nable success in producing realistic and diverse images.\nThe advent of large-scale text-to-image diffusion mod-\nels [29, 30, 33], leveraging expansive web-scale training\ndatasets [7, 34], has enabled the generation of high-quality\nsubject \nrepresentation \nlearning\nDDIM \ninversion\nreference-\nguided \ngeneration\n\u2026\n\u2026\n\u2026\nstep-wise subject \nrepresentations\nsource images & masks\ngenerated image reference image\n\u201cA can with a \nmountain in the \nbackground.\u201d\nFigure 1. Illustration of HiFi Tuner. We first learn the step-wise\nsubject representations with subject source images and masks.\nThen we select and transform the reference image, and use DDIM\ninversion to obtain its noise latent trajectory. Finally, we generate\nan image controlled by the prompt, the step-wise subject represen-\ntations and the reference subject guidance.\nimages that align closely with textual guidance. Despite this\nachievement, the training data remains inherently limited in\nits coverage of all possible subjects. Consequently, it be-\ncomes infeasible for diffusion models to accurately gener-\nate images of specific, unseen subjects based solely on tex-\ntual descriptions. As a result, personalized generation has\nemerged as a pivotal research problem. This approach seeks\nto fine-tune the model with minimal additional costs, aim-\ning to generate images of user-specified subjects that seam-\nlessly align with the provided text descriptions.\nWe identify three drawbacks of existing popular meth-\nods for subject-driven fine-tuning [9, 15, 31, 32]. Firstly, a\nnotable imbalance exists between sample quality and pa-\nrameter efficiency in the fine-tuning process. For exam-\nple, Textual Inversion optimizes only a few parameters in\nthe text embedding space, resulting in poor sample fidelity.\nConversely, DreamBooth achieves commendable sample fi-\ndelity but at the cost of optimizing a substantial number of\nparameters. Ideally, there should be a parameter-efficient\nmethod that facilitates the generation of images with satis-\nfactory sample fidelity while remaining lightweight for im-\n1\narXiv:2312.00079v1  [cs.CV]  30 Nov 2023\nproved portability. Secondly, achieving a equilibrium be-\ntween sample fidelity and the flexibility to render objects in\ndiverse scenes poses a significant challenge. Typically, as\nfine-tuning iterations increase, the sample fidelity improves,\nbut the flexibility of the scene coverage diminishes. Thirdly,\ncurrent methods struggle to accurately preserve the appear-\nance of the input object. Due to the extraction of subject\nrepresentations from limited data, these representations of-\nfer weak constraints to the diffusion model. Consequently,\nunwanted variations and artifacts may appear in the gener-\nated subject.\nIn this study, we introduce a novel framework named\nHiFi Tuner for subject fine-tuning that prioritizes the pa-\nrameter efficiency, thereby enhancing sample fidelity, pre-\nserving the scene coverage, and mitigating undesired sub-\nject variations and artifacts. Our denoising process incor-\nporates a mask guidance to reduce the influence of the im-\nage background on subject representations. Additionally,\nwe introduce a novel parameter regularization method to\nsustain the model\u2019s scene coverage capability and design\na step-wise subject representation mechanism that adapts to\nparameter functions at different denoising steps. We further\npropose a reference-guided generation method that lever-\nages pivotal inversion of a reference image. By integrating\nguiding information into the step-wise denoising process,\nwe effectively address issues related to unwanted variations\nand artifacts in the generated subjects. Notably, our frame-\nwork demonstrates versatility by extending its application\nto a novel image editing task: substituting the subject in an\nimage with a user-specified subject through textual manip-\nulations.\nWe summarize the contributions of our work as follows.\nFirstly, we identify and leverage three effective techniques\nto enhance the subject representation capability of textual\nembeddings. This improvement significantly aids the diffu-\nsion model in generating samples with heightened fidelity.\nSecondly, we introduce a novel reference-guided generation\nprocess that successfully addresses unwanted subject vari-\nations and artifacts in the generated images. Thirdly, we\nextend the application of our methodology to a new subject-\ndriven image editing task, showcasing its versatility and ap-\nplicability in diverse scenarios. Finally, we demonstrate the\ngeneric nature of HiFi Tuner by showcasing its effectiveness\nin enhancing the performance of both the Textual Inversion\nand the DreamBooth.\n2. Related Works\nSubject-driven text-to-image generation. This task re-\nquires the generative models generate the subject provided\nby users in accordance with the textual prompt description.\nPioneer works [4, 26] utilize Generative Adversarial Net-\nworks (GAN) [10] to synthesize images of a particular in-\nstance. Later works benefit from the success of diffusion\nmodels [30, 33] to achieve a superior faithfulness in the per-\nsonalized generation. Some works [6, 35] rely on retrieval-\naugmented architecture to generate rare subjects. However,\nthey use weakly-supervised data which results in an unsat-\nisfying faithfullness for the generated images. There are\nencoder-based methods [5, 16, 36] that encode the refer-\nence subjects as a guidance for the diffusion process. How-\never, these methods consume a huge amount of time and\nresources to train the encoder and does not perform well for\nout-of-domain subjects. Other works [9, 31] fine-tune the\ncomponents of diffusion models with the provided subject\nimages. Our method follows this line of works as our mod-\nels are faithful and generic in generating rare and unseen\nsubjects.\nText-guided image editing.\nThis task requires the\nmodel to edit an input image according to the modifica-\ntions described by the text. Early works [9, 27] based on\ndiffusion models [30, 33] prove the effectiveness of manip-\nulating textual inputs for editing an image. Further works\n[1, 24] propose to blend noise with the input image for the\ngeneration process to maintain the layout of the input im-\nage. Prompt-to-Prompt [12, 25] manipulates the cross at-\ntention maps from the image latent to the textual embedding\nto edit an image and maintain its layout. InstructPix2Pix [2]\ndistills the diffusion model with image editing pairs synthe-\nsized by Prompt-to-Prompt to implement the image editing\nbased on instructions.\n3. Methods\nIn this section, we elaborate HiFi Tuner in details. We use\nthe denoising process to generate subjects with appearance\nvariations and the inversion process to preserve the details\nof subjects. In section 3.1, we present some necessary back-\ngrounds for our work. In section 3.2, we introduce the three\nproposed techniques that help preserving the subject iden-\ntity. In section 3.3, we introduce the reference-guided gen-\neration technique, which merits the image inversion process\nto further preserve subject details. In section 3.4, we intro-\nduce an extension of our work on a novel image editing ap-\nplication \u2013 personalized subject replacement with only tex-\ntual prompt edition.\n3.1. Backgrounds\nStable diffusion [30] is a widely adopted framework in\nthe realm of text-to-image diffusion models. Unlike other\nmethods [29, 33], Stable diffusion is a latent diffusion\nmodel, where the diffusion model is trained within the latent\nspace of a Variational Autoencoder (VAE). To accomplish\ntext-to-image generation, a text prompt undergoes encoding\ninto textual embeddings c using a CLIP text encoder[28].\nSubsequently, a random Gaussian noise latent xT is initial-\nized. The process then recursively denoises noisy latent xt\nthrough a noise predictor network \u03f5\u03b8 with the conditioning\n2\nSRL\n\u0305\ud835\udc61 = \ud835\udc5f\ud835\udc4e\ud835\udc5b\ud835\udc51(1, \ud835\udc47)\nSRL\n\u0305\ud835\udc61 = \ud835\udc47\n\u00d7\ud835\udc41\n\u00d7\ud835\udc3c\nSRL\n\u0305\ud835\udc61 = \ud835\udc61\n\u00d7\ud835\udc3c\nSRL\n\u0305\ud835\udc61 = 1\n\u00d7\ud835\udc3c\n...\n...\n\ud835\udc50!\n\ud835\udf19!\n\ud835\udc50!\"\n\ud835\udc50!#$%\n\ud835\udc50!#\n\ud835\udc50!&\n\ud835\udc50!%\nDDIM\n\u0305\ud835\udc61 = \ud835\udc47\n...\nDDIM\n\u0305\ud835\udc61 = 1\n[c, \ud835\udc50!\"]\n[c, \ud835\udc50!%]\nSAM\ncosine similarity\nTransform\nSAM\nDDIM\n\u0305\ud835\udc61 = \ud835\udc47\nDDIM-1\n\u0305\ud835\udc61 = 1\nDDIM-1\n\u0305\ud835\udc61 = \ud835\udc61'\nDDIM-1\n\u0305\ud835\udc61 = \ud835\udc61!\n...\nRGG\n\u0305\ud835\udc61 = \ud835\udc61!\n...\nRGG\n\u0305\ud835\udc61 = \ud835\udc61'\n...\nDDIM\n\u0305\ud835\udc61 =1\n...\n...\n\ud835\udc50(\n\ud835\udc50(\n\ud835\udc50(\n[c, \ud835\udc50!\"]\n[c, \ud835\udc50!\n#!]\n[c, \ud835\udc50!\n#\"]\n[c, \ud835\udc50!%]\n[                        ,                ]\nDM\n\ud835\udc50\n\ud835\udc50!\nSubject Representation Learning (SRL)\n\u0305\ud835\udc61\nDM\n[c, \ud835\udc50!#]\n\ud835\udf19)\n\u0305\ud835\udc61\n\u2a02\n\u2a02\n\u2a01\n1 \u2212 \ud835\udc64\n\ud835\udc64\nDM\n\u2112!\"#\n\u2112!\"#\nDM\n\ud835\udf19\n\u2a02\n\u2a01\n1 \u2212 \ud835\udc64\n\u0305\ud835\udc61\nReference-Guided Generation (RGG)\nsubject representation learning process\nreference image selection and transformation\nreference-guided generation process\n\ud835\udc3c*%\n\ud835\udc65\"\n\ud835\udc3c*&\n\ud835\udc3c(\n\ud835\udc65#!\n\u2217\n\ud835\udc65#\"\n\u2217\n\ud835\udc65#\u2217\n\ud835\udc65#$%\n\ud835\udc65#\nFigure 2. The framework of HiFi Tuner. The grey arrows stand for the data flow direction. The red arrows stand for the gradient back\npropagation direction. SAM stands for the Segment Anything [18] model. DM stands for the Stable Diffusion [30] model. DDIM and\nDDIM \u22121 stands for the DDIM denoising step and inversion step respectively.\nof c. Finally, the VAE decoder is employed to project the\ndenoised latent x0 onto an image. During the sampling pro-\ncess, a commonly applied mechanism involves classifier-\nfree guidance [13] to enhance sample quality. Additionally,\ndeterministic samplers, such as DDIM [38], are employed\nto improve sampling efficiency. The denoising process can\nbe expressed as\nxt\u22121 = F (t)(xt, c, \u03d5)\n= \u03b2txt \u2212 \u03b3t(w\u03f5\u03b8(xt, c) + (1 \u2212 w)\u03f5\u03b8(xt, \u03d5)).\n(1)\nwhere \u03b2t and \u03b3t are time-dependent constants; w is the\nclassifier-free guidance weight; \u03d5 is the CLIP embedding\nfor a null string.\nTextual inversion [9]. As a pioneer work in person-\nalized generation, Textual Inversion introduced the novel\nconcept that a singular learnable textual token is adequate\nto represent a subject for the personalization. Specifically,\nthe method keeps all the parameters of the diffusion model\nfrozen, exclusively training a word embedding vector cs us-\ning the diffusion objective:\nLs(cs) = min\ncs \u2225\u03f5\u03b8(xt, [c, cs]) \u2212 \u03f5\u22252\n2,\n(2)\nwhere [c, cs] represents replacing the object-related word\nembedding in the embedding sequence of the training cap-\ntion (e.g. \u201ca photo of A\u201d) with the learnable embedding cs.\nAfter cs is optimized, this work applies F (t)(xt, [c, cs], \u03d5)\nfor generating personalized images from prompts.\nNull-text\ninversion\n[25]\nmethod\nintroduces\nan\ninversion-based approach to image editing,\nentailing\nthe initial inversion of an image input to the latent space,\nfollowed by denoising with a user-provided prompt.\nThis method comprises two crucial processes: a pivotal\ninversion process and a null-text optimization process.\nThe pivotal inversion involves the reversal of the latent\nrepresentation of an input image, denoted as x0, back\nto a noise latent representation, xT , achieved through\nthe application of reverse DDIM. This process can be\nformulated as reparameterizing Eqn. (1) with w = 1:\nxt+1 = F \u22121(t)(xt, c) = \u03b2txt + \u03b3t\u03f5\u03b8(xt, c)\n(3)\nWe denote the latent trajectory attained from the pivotal in-\nversion as [x\u2217\n0, ..., x\u2217\nT ]. However, naively applying Eqn. (1)\nfor x\u2217\nT will not restore x\u2217\n0, because \u03f5\u03b8(xt, c) \u0338= \u03f5\u03b8(x\u2217\nt\u22121, c).\nTo recover the original image, Null-text inversion trains a\nnull-text embedding \u03d5t for each timestep t force the the\ndenoising trajectory to stay close to the forward trajectory\n[x\u2217\n0, ..., x\u2217\nT ]. The learning objective is\nL(t)\nh (\u03d5t) = min\n\u03d5t \u2225x\u2217\nt\u22121 \u2212 F (t)(xt, c, \u03d5t)\u22252\n2.\n(4)\nAfter training, image editing techniques such as the prompt-\nto-prompt [12] can be applied with the learned null-text em-\nbeddings {\u03d5\u2217\nt } to allow manipulations of the input image.\n3\n3.2. Learning subject representations\nWe introduce three techniques for improved learning of the\nrepresentations that better capture the given object.\nMask guidance One evident issue we observed in Tex-\ntual Inversion is the susceptibility of the learned textual em-\nbedding, cs, to significant influence from the backgrounds\nof training images. This influence often imposes constraints\non the style and scene of generated samples and makes iden-\ntity preservation more challenging due to the limited capac-\nity of the textual embedding, which is spent on unwanted\nbackground details. We present a failure analysis of Tex-\ntual Inversion in the Appendix A. To address this issue,\nwe propose a solution involving the use of subject masks\nto confine the loss during the learning process of cs. This\napproach ensures that the training of cs predominantly fo-\ncuses on subject regions within the source images. Specifi-\ncally, binary masks of the subjects in the source images are\nobtained using Segment Anything (SAM) [18], an off-the-\nshelf instance segmentation model. The Eqn. (2) is updated\nto a masked loss:\nLs(cs) = min\ncs \u2225M \u2299 (\u03f5\u03b8(xt, [c, cs]) \u2212 \u03f5)\u22252\n2,\n(5)\nwhere \u2299 stands for element-wise product, and M stands for\na binary mask of the subject. This simple technique miti-\ngates the adverse impact of background influences and en-\nhancing the specificity of the learned textual embeddings.\nParameter regularization We aim for the learned em-\nbedding, cs, to obtain equilibrium between identity preser-\nvation and the ability to generate diverse scenes. To achieve\nthis balance, we suggest initializing cs with a portion of the\nnull-text embedding, \u03d5s, and introducing an L2 regulariza-\ntion term. This regularization term is designed to incen-\ntivize the optimized cs to closely align with \u03d5s:\nLs(cs) = min\ncs \u2225M \u2299(\u03f5\u03b8(xt, [c, cs])\u2212\u03f5)\u22252\n2+ws\u2225cs\u2212\u03d5s\u22252\n2. (6)\nHere, cs \u2208 Rn\u00d7d where n is the number of tokens and d is\nthe embedding dimension, and ws is a regularization hyper-\nparameter. We define \u03d5s as the last n embeddings of \u03d5 and\nsubstitute the last n embeddings in c with cs, forming [c, cs].\nIt is noteworthy that [c, cs] = c if cs is not optimized, given\nthat \u03d5 constitutes the padding part of the embedding. This\nregularization serves two primary purposes. Firstly, the sta-\nble diffusion model is trained with a 10% caption drop, sim-\nplifying the conditioning to \u03d5 and facilitating classifier-free\nguidance [13]. Consequently, \u03d5 is adept at guiding the dif-\nfusion model to generate a diverse array of scenes, making it\nan ideal anchor point for the learned embedding. Secondly,\ndue to the limited data used for training the embedding, un-\nconstrained parameters may lead to overfitting with erratic\nscales. This overfitting poses a risk of generating severely\nout-of-distribution textual embeddings.\nDDIM \nstep\n50\n25\n1\nFigure 3. Step-wise function analysis of cs. We generate an image\nfrom a noise latent with DDIM and an optimized cs representing a\nsubject dog. The text prompt is \u201dA sitting dog\u201d. The top image is\nthe result generated image. We follow [12] to obtain the attention\nmaps with respect to the 5 token embeddings of cs as shown in the\nbelow images. The numbers to the left refer to the corresponding\nDDIM denoising steps. In time step 50, the 5 token embeddings of\ncs are attended homogeneously across the latent vectors. In time\nstep 1, these token embeddings are attended mostly by the subject\ndetailed regions such as the forehead, the eyes, the ears, etc.\nStep-wise subject representations We observe that the\nlearned textual embedding, cs, plays distinct roles across\nvarious denoising time steps. It is widely acknowledged that\nduring the sampling process. In early time steps where t is\nlarge, the primary focus is on generating high-level image\nstructures, while at smaller values of t, the denoising pro-\ncess shifts its emphasis toward refining finer details. Anal-\nogous functional distinctions exist for the role of cs. Our\nanalysis of cs across time steps, presented in Fig. 3, un-\nderscores these variations. Motivated by this observation,\nwe propose introducing time-dependent embeddings, ct\ns, at\neach time step instead of a single cs to represent the subject.\nThis leads to a set of embeddings, [c1\ns, ..., cT\ns ], working col-\nlectively to generate images. To ensure smooth transitions\nbetween time-dependent embeddings, we initially train a\nsingle cs across all time steps. Subsequently, we recursively\noptimize ct\ns following DDIM time steps, as illustrated in Al-\ngorithm 1. This approach ensures that ct\ns is proximate to\nct+1\ns\nby initializing it with ct+1\ns\nand optimizing it for a few\nsteps. After training, we apply\nxt\u22121 = F (t)(xt, [c, ct\ns], \u03d5)\n(7)\nwith the optimized [c1\ns, ..., cT\ns ] to generate images.\n3.3. Reference-guided generation\nShown in Figure 2, we perform our reference-guided gen-\neration in three steps. First, we determine the initial latent\n4\nAlgorithm 1: Optimization algorithm for ct\ns.\nT is\nDDIM time steps. I is the optimization steps per DDIM\ntime step. X0 is the set of encoded latents of the source\nimages. Ns(\u00b7) is the DDIM noise scheduler. Ls(\u00b7) refers\nto the loss function in Eqn. (6).\nResult: Cs\nCs = {}, cT +1\ns\n= cs\nfor t = [T, ..., 1] do\nct\ns = ct+1\ns\nfor i = [1, ..., I] do\n\u03f5 \u223c N(0, 1), x0 \u2208 X0, xt = Ns(x0, \u03f5, t)\nct\ns = ct\ns \u2212 \u03b7\u2207ctsLs(ct\ns)\nCs = Cs \u222a {ct\ns}\nxT and follow the DDIM denoising process to generate an\nimage. Thus, we can determine the subject regions of {xt}\nrequiring guiding information and the corresponding refer-\nence image. Second, we transform the reference image and\ninverse the latent of the transformed image to obtain a refer-\nence latent trajectory, [x\u2217\n0, ..., x\u2217\nT ]. Third, we start a new de-\nnoising process from xT and apply the guiding information\nfrom [x\u2217\n0, ..., x\u2217\nT ] to the guided regions of {xt}. Thereby,\nwe get a reference-guided generated image.\nGuided regions and reference image. First, we deter-\nmine the subject regions of xt that need the guiding infor-\nmation. Notice that xt \u2208 RH\u00d7W \u00d7C, where H, W and\nC are the height, width and channels of the latent xt re-\nspectively. Following the instance segmentation methods\n[11, 22], we aim to find a subject binary mask Mg to deter-\nmine the subset xs\nt \u2208 Rm\u00d7C corresponding to the subject\nregions. Because DDIM [38] is a deterministic denoising\nprocess as shown in Eqn. (1), once xT , c and \u03d5 are de-\ntermined, the image to be generated is already determined.\nTherefore, we random initialize xT with Gaussian noise;\nthen, we follow Eqn. (7) and apply the decoder of the stable\ndiffusion model to obtain a generated image, Ig1; by ap-\nplying Grounding SAM [18, 21] with the subject name to\nIg1 and resizing the result to H \u00d7 W, we obtain the sub-\nject binary mask Mg. Second, we determine the reference\nimage by choosing the source image with the closest sub-\nject appearance to the subject in Ig1, since the reference-\nguided generation should modify {xt} as small as possible\nto preserve the image structure. As pointed out by Dream-\nBooth [31], DINO [3] score is a better metric than CLIP-I\n[28] score in measuring the subject similarity between two\nimages. Hence, we use ViT-S/16 DINO model [3] to extract\nthe embedding of Ig1 and all source images. We choose the\nsource image whose DINO embedding have the highest co-\nsine similarity to the DINO embedding of Ig1 as the refer-\nence image, Ir. We use Grounding SAM [18, 21] to obtain\nthe subject binary mask Mr of Ir.\nReference image transformation and inversion. First,\nwe discuss the transformation of Ir. Because the subject in\nIg1 and the subject in Ir are spatially correlated with each\nother, we need to transform Ir to let the subject better align\nwith the subject in Ig1. As the generated subject is prone\nto have large appearance variations, it is noneffective to use\nimage registration algorithms, e.g. RANSAC [8], based on\nlocal feature alignment. We propose to optimize a transfor-\nmation matrix\nT\u03b8 =\n\uf8ee\n\uf8f0\n\u03b81\n0\n0\n0\n\u03b81\n0\n0\n0\n1\n\uf8f9\n\uf8fb\n\uf8ee\n\uf8f0\ncos(\u03b82)\n\u2212 sin \u03b82\n0\nsin \u03b82\ncos(\u03b82)\n0\n0\n0\n1\n\uf8f9\n\uf8fb\n\uf8ee\n\uf8f0\n1\n0\n\u03b83\n0\n1\n\u03b84\n0\n0\n1\n\uf8f9\n\uf8fb\n(8)\ncomposed of scaling, rotation and translation such that\nT\u03b8(Mr) best aligns with Mg. Here, {\u03b8i} are learnable pa-\nrameters, and T\u03b8(\u00b7) is the function of applying the transfor-\nmation to an image. T\u03b8 can be optimized with\nLt = min\n\u03b8\n\u2225T\u03b8(Mr) \u2212 Mg\u22251\n1.\n(9)\nPlease refer to the Appendix B for a specific algorithm opti-\nmizing T\u03b8. We denote the optimized T\u03b8 as T \u2217\n\u03b8 and the result\nof T \u2217\n\u03b8 (Mr) as M \u2217\nr . Thereafter, we can transform Ir with\nT \u2217\n\u03b8 (Ir) to align the subject with the subject in Ig1. Notice\nthat the subject in T \u2217\n\u03b8 (Ir) usually does not perfectly align\nwith the subject in Ig1. A rough spatial location for placing\nthe reference subject should suffice for the reference guid-\ning purpose in our case. Second, we discuss the inversion\nof T \u2217\n\u03b8 (Ir). We use BLIP-2 model [19] to caption Ir and use\na CLIP text encoder to encode the caption to cr. Then, we\nencode T \u2217\n\u03b8 (Ir) into x\u2217\n0 with a Stable Diffusion image en-\ncoder. Finally, we recursively apply Eqn. (3) to obtain the\nreference latent trajectory, [x\u2217\n0, ..., x\u2217\nT ].\nGeneration process. There are two problems with the\nreference-guided generation: 1) the image structure needs\nto be preserved; 2) the subject generated needs to conform\nwith the context of the image. We reuse xT in step 1 as\nthe initial latent. If we follow Eqn. (7) for the denoising\nprocess, we will obtain Ig1. We aim to add guiding infor-\nmation to the denoising process and obtain a new image Ig2\nsuch that the subject in Ig2 has better fidelity and the image\nstructure is similar to Ig1. Please refer to Algorithm 2 for\nthe specific reference-guided generation process. As dis-\ncussed in Section 3.2, the stable diffusion model focuses on\nthe image structure formation at early denoising steps and\nthe detail polishing at later steps. If we incur the guiding\ninformation in early steps, Ig2 is subject to have structural\nchange such that M \u2217\nr cannot accurately indicate the subject\nregions. It is harmful to enforce the guiding information at\nlater steps either, because the denoising at this stage gathers\nuseful information mostly from the current latent. There-\nfore, we start and end the guiding process at middle time\nsteps ts and te respectively. At time step ts, we substitute\nthe latent variables corresponding to the subject region in\nxt with those in x\u2217\nt . We do this for three reasons: 1) the\nsubstitution enables the denoising process to assimilate the\n5\nAlgorithm 2: Reference-guided generation algorithm.\nJ is the number of optimization steps for \u03d5h per denois-\ning step. L(t)\nh (\u00b7) refers to the loss function in Eqn. (10).\nResult: x0\nInputs: ts, te, xT , M \u2217\nr , c, \u03d5, [c1\ns, ..., cT\ns ], [x\u2217\n0, ..., x\u2217\nT ]\nfor t = [T, ..., 1] do\nif t == ts then\n\u03d5h = \u03d5\nxt[M \u2217\nr ] = x\u2217\nt [M \u2217\nr ]\nxt\u22121 = F (t)(xt, [c, ct\ns], \u03d5)\nif t \u2a7d ts and t \u2a7e te then\nfor j = [1, ..., J] do\n\u03d5h = \u03d5h \u2212 \u03b7\u2207\u03d5hL(t)\nh (\u03d5h)\nxt\u22121[M \u2217\nr ] = F (t)(xt, [c, ct\ns], \u03d5h)[M \u2217\nr ]\nsubject to be generated to the reference subject; 2) the la-\ntent variables at time step ts are close to the noise space\nso that they are largely influenced by the textual guidance\nas well; 3) the substitution does not drastically change the\nimage structure because latent variables have small global\neffect at middle denoising steps. We modify Eqn. (4) to\nEqn. (10) for guiding the subject generation.\nL(t)\nh (\u03d5h) = min\n\u03d5h \u2225x\u2217\nt\u22121[M \u2217\nr ] \u2212 F (t)(xt, [c, ct\ns], \u03d5h)[M \u2217\nr ]\u22252\n2\n(10)\nHere, xt[M] refers to latent variables in xt indicated by the\nmask M. Because \u03d5h is optimized with a few steps per\ndenoising time step, the latent variables corresponding to\nthe subject regions change mildly within the denoising time\nstep. Therefore, at the next denoising time step, the stable\ndiffusion model can adapt the latent variables correspond-\ning to non-subject regions to conform with the change of the\nlatent variables corresponding to the subject regions. Fur-\nthermore, we can adjust the optimization steps for \u03d5h to\ndetermine the weight of the reference guidance. More ref-\nerence guidance will lead to a higher resemblance to the\nreference subject while less reference guidance will result\nin more variations for the generated subject.\n3.4. Personalized subject replacement\nWe aim to use the learned subject textual representations to\nreplace the subject in an image with the user-specified sub-\nject. Although there are methods [20, 23, 39, 40] inpainting\nthe image area with a user-specified subject, our method has\ntwo advantages over them. First, we do not specify the in-\npainting area of the image; instead, our method utilize the\ncorrelation between the textual embeddings and the latent\nvariables to identify the subject area. Second, our method\ncan generate a subject with various pose and appearance\nsuch that the added subject better conforms to the image\ncontext.\nAlgorithm 3: Personalized subject replacement algo-\nrithm. F \u22121(t) refers to Eqn. (3). K is the optimization\nsteps for null-text optimization. L(t)\nh (\u00b7) refers to Eqn. (4)\nResult: xg\n0\nInputs: xr\n0, cr, cg, [c1\ns, ..., cT\ns ]\nxr\n0\n\u2217 = xr\n0\nfor t = [0, ..., T \u2212 1] do\nxr\nt+1\n\u2217 = F \u22121(t)(xr\nt\n\u2217, cr)\nxr\nT = xr\nT\n\u2217, \u03d5T = \u03d5\nfor t = [T, ..., 1] do\nfor k = [1, ..., K] do\n\u03d5t = \u03d5t \u2212 \u03b7\u2207\u03d5tL(t)\nh (\u03d5t)\nxr\nt\u22121, ar\nt\n\u2217 = A(t)(xr\nt, cr, \u03d5t)\n\u03d5t\u22121 = \u03d5\u2217\nt = \u03d5t\nxg\nT = xr\nT\n\u2217\nfor t = [T, ..., 1] do\nxg\nt\u22121 = \u02dcF (t)\n[cts,wg](xg\nt , [cg, ct\ns], \u03d5\u2217\nt , ar\nt\n\u2217)\nWe first follow the fine-tuning method in Section 3.2 to\nobtain the step-wise subject representations [c1\ns, ..., cT\ns ]. We\nencode the original image Ir to xr\n0 with the Stable Diffusion\nimage encoder; then we use BLIP-2 model [19] to caption\nIr and encode the caption into cr with the Stable Diffusion\nlanguage encoder. We identify the original subject word\nembedding in cr and substitute that with the new subject\nword embedding wg to attain a cg (e.g. \u2018cat\u2019 \u2192 \u2018dog\u2019 in\nthe sentence \u2018a photo of siting cat\u2019). Then we follow Al-\ngorithm 3 to generate the image with the subject replaced.\nReferring to the prompt-to-prompt paper [12], we store the\nstep-wise cross attention weights with regard to the word\nembeddings in cr to ar\nt\n\u2217. A(t)(\u00b7, \u00b7, \u00b7) performs the same op-\nerations as F (t)(\u00b7, \u00b7, \u00b7) in Eqn. (1) but returns xt\u22121 and ar\nt\n\u2217.\nWe also modify F (t)(\u00b7, \u00b7, \u00b7) to \u02dcF (t)\n[cts,wg](\u00b7, \u00b7, \u00b7, ar\nt\n\u2217) such that\nall token embeddings use fixed cross attention weights ar\nt\n\u2217\nexcept that [ct\ns, wg] use the cross attention weights of the\nnew denoising process.\n4. Experiments\nDataset. We use the DreamBooth [31] dataset for evalua-\ntion. It contains 30 subjects: 21 of them are rigid objects\nand 9 of them are live animals subject to large appearance\nvariations. The dataset provides 25 prompt templates for\ngenerating images. Following DreamBooth, we fine-tune\nour framework for each subject and generate 4 images for\neach prompt template, totaling 3,000 images.\nSettings. We adopt the pretrained Stable Diffusion [30]\nversion 1.4 as the text-to-image framework. We use DDIM\nwith 50 steps for the generation process. For HiFi Tuner\nbased on Textual Inversion, we implement both the learning\nof subject textual embeddings described in Section 3.2 and\nthe reference-guided generation described in Section 3.3.\n6\n\u201cA backpack on top of \ngreen grass with \nsunflowers around it.\u201d\n\u201cA backpack on top of the \nsidewalk in a crowded \nstreet.\u201d\n\u201cA can with a tree and \nautumn leaves in the \nbackground.\u201d\n\u201cA candle in the jungle.\u201d\n\u201cA race car toy in the \nsnow.\u201d\n\u201cA robot toy sits on top of \na wooden table.\u201d\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nsource image\ntextual inversion\nours (TI)\nDreamBooth\nours (DB)\nFigure 4. Qualitative comparison. We implement our fine-tuning method based on both Textual Inversion (TI) and DreamBooth (DB). A\nvisible improvement is made by comparing the images in the third column with those in the second column and comparing the images in\nthe fifth column and those in the forth column.\nWe use 5 tokens for cs and adopts an ADAM [17] optimizer\nwith a learning rate 5e\u22123 to optimize it. We first optimize\ncs for 1000 steps and then recursively optimize ct\ns for 10\nsteps per denoising step. We set ts = 40 and te = 10\nand use an ADAM [17] optimizer with a learning rate 1e\u22122\nto optimize \u03d5h. We optimize \u03d5h for 10 steps per DDIM\ndenoising step. For HiFi Tuner based on DreamBooth, we\nfollow the original subject representation learning process\nand implement the reference-guided generation described\nin Section 3.3. We use the same optimization schedule to\noptimize \u03d5h as mentioned above. For the reference-guided\ngeneration, we only apply HiFi Tuner to the 21 rigid objects,\nbecause their appearances vary little and have strong need\nfor the detail preservation.\nEvaluation metrics. Following DreamBooth [31], we\nuse DINO score and CLIP-I score to measure the subject\nfidelity and use CLIP-T score the measure the prompt fi-\ndelity. CLIP-I score is the average pairwise cosine similar-\n7\nsource images\ninput image\nresult image\nA cat sits on a table next to a vase of tulips \nA dog sits on a table next to a vase of tulips \n\ud835\udc50!\nFigure 5. Results for personalized subject replacement.\nity between CLIP [28] embeddings of generated images and\nreal images, while DINO score calculates the same cosine\nsimilarity but uses DINO [3] embeddings instead of CLIP\nembeddings. As pointed out in the DreamBooth paper [31],\nDINO score is a better means than CLIP-I score in mea-\nsuring the subject detail preservation. CLIP-T score is the\naverage cosine similarity between CLIP [28] embeddings of\nthe pairwise prompts and generated images.\nQualitative comparison.\nFig. 4 shows the qualita-\ntive comparison between HiFi Tuner and other fine-tuning\nframeworks. HiFi Tuner possesses three advantages com-\npared to other methods. First, HiFi Tuner is able to di-\nminish the unwanted style change for the generated sub-\njects. As shown in Fig. 4 (a) & (b), DreamBooth blends\nsun flowers with the backpack, and both DreamBooth and\nTextual Inversion generate backpacks with incorrect col-\nors; HiFi Tuner maintains the styles of the two backpacks.\nSecond, HiFi Tuner can better preserve details of the sub-\njects. In Fig. 4 (c), Textual Inversion cannot generate the\nwhale on the can while DreamBooth generate the yellow\npart above the whale differently compared to the original\nimage; In Fig. 4 (d), DreamBooth generates a candle with\na white candle wick but the candle wick is brown in the\noriginal image. Our method outperforms Textual Inversion\nand DreamBooth in preserving these details. Third, HiFi\nTuner can better preserve the structure of the subjects. In\nFig. 4 (e) & (f), the toy car and the toy robot both have\ncomplex structures to preserve, and Textual Inversion and\nDreamBooth generate subjects with apparent structural dif-\nferences. HiFi Tuner makes improvements on the model\u2019s\nstructural preservation capability.\nQuantitative comparison.\nWe show the quantitative\nimprovements HiFi Tuner makes in Table 1. HiFi Tuner im-\nproves Textual Inversion for 9.6 points in DINO score and\n3.6 points in CLIP-T score, and improves DreamBooth for\n1.2 points in DINO score and 1.2 points in CLIP-T score.\nAblation studies. We present the quantitative improve-\nments of adding our proposed techniques in Table 2. We\nobserve that fine-tuning either DreamBooth or Textual In-\nversion with more steps leads to a worse prompt fidelity.\nTherefore, we fine-tune the networks with fewer steps than\nTable 1. Quantitative comparison.\nMethod\nDINO \u2191\nCLIP-I \u2191\nCLIP-T \u2191\nReal images\n0.774\n0.885\nN/A\nStable Diffusion\n0.393\n0.706\n0.337\nTextual Inversion [9]\n0.569\n0.780\n0.255\nOurs (Textual Inversion)\n0.665\n0.807\n0.291\nDreamBooth [31]\n0.668\n0.803\n0.305\nOurs (DreamBooth)\n0.680\n0.809\n0.317\nTable 2. Ablation study.\nMethod\nDINO \u2191\nCLIP-I \u2191\nCLIP-T \u2191\nBaseline (Textual Inversion)\n0.567\n0.786\n0.293\n+ mask\n0.606\n0.788\n0.292\n+ regularization\n0.612\n0.789\n0.294\n+ step-wise representations\n0.626\n0.790\n0.292\n+ reference guidance\n0.665\n0.807\n0.291\nBaseline (DreamBooth)\n0.662\n0.803\n0.315\n+ reference guidance\n0.680\n0.809\n0.317\nthe original implementations, which results in higher CLIP-\nT scores but lower DINO scores for the baselines. There-\nafter, we can use our techniques to improve the subject fi-\ndelity so that both DINO scores and CLIP-T scores can sur-\npass the original implementations. For HiFi Tuner based\non Textual Inversion, we fine-tune the textual embeddings\nwith 1000 steps. The four proposed techniques make steady\nimprovements over the baseline in DINO score while main-\ntain CLIP-T score. The method utilizing all of our proposed\ntechniques makes a remarkable 9.8-point improvement in\nDINO score over the baseline. For HiFi Tuner based on\nDreamBooth, we fine-tune all the diffusion model weights\nwith 400 steps. By utilizing the reference-guided genera-\ntion, HiFi Tuner achieves a 1.8-point improvement over the\nbaseline in DINO score.\nResults for personalized subject replacement.\nWe\nshow the qualitative results in Figure 5. More results can\nbe found in the Appendix C.\n5. Conclusions\nIn this work, we introduce a parameter-efficient fine-tuning\nmethod that can boost the sample fidelity and the prompt fi-\ndelity based on either Textual Inversion or DreamBooth. We\npropose to use a mask guidance, a novel parameter regular-\nization technique and step-wise subject representations to\nimprove the sample fidelity. We invents a reference-guided\ngeneration technique to mitigate the unwanted variations\nand artifacts for the generated subjects. We also exemplify\nthat our method can be extended to substitute a subject in\nan image with personalized item by textual manipulations.\n8\nReferences\n[1] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended\ndiffusion for text-driven editing of natural images. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 18208\u201318218, 2022.\n2\n[2] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 18392\u201318402, 2023.\n2\n[3] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. In Pro-\nceedings of the IEEE/CVF international conference on com-\nputer vision (ICCV), pages 9650\u20139660, 2021. 5, 8\n[4] Arantxa Casanova, Marlene Careil, Jakob Verbeek, Michal\nDrozdzal,\nand Adriana Romero Soriano.\nInstance-\nconditioned gan. Advances in Neural Information Process-\ning Systems, 34:27517\u201327529, 2021. 2\n[5] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Rui, Xuhui\nJia, Ming-Wei Chang, and William W Cohen. Subject-driven\ntext-to-image generation via apprenticeship learning. arXiv\npreprint arXiv:2304.00186, 2023. 2\n[6] Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W.\nCohen. Re-imagen: Retrieval-augmented text-to-image gen-\nerator. In The Eleventh International Conference on Learn-\ning Representations (ICLR), 2023. 2\n[7] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergio-\nvanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman,\nAdam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A\njointly-scaled multilingual language-image model. In The\nEleventh International Conference on Learning Representa-\ntions (ICLR), 2022. 1\n[8] Martin A Fischler and Robert C Bolles.\nRandom sample\nconsensus: a paradigm for model fitting with applications to\nimage analysis and automated cartography. Communications\nof the ACM, 24(6):381\u2013395, 1981. 5\n[9] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\nAmit Haim Bermano, Gal Chechik, and Daniel Cohen-or.\nAn image is worth one word: Personalizing text-to-image\ngeneration using textual inversion. In International Confer-\nence on Learning Representations (ICLR), 2023. 1, 2, 3, 8,\n11\n[10] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial networks. Commu-\nnications of the ACM, 63(11):139\u2013144, 2020. 2\n[11] Kaiming He, Georgia Gkioxari, Piotr Doll\u00b4ar, and Ross Gir-\nshick. Mask r-cnn. In Proceedings of the IEEE/CVF interna-\ntional conference on computer vision (ICCV), pages 2961\u2013\n2969, 2017. 5\n[12] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-or. Prompt-to-prompt image\nediting with cross-attention control. In International Con-\nference on Learning Representations (ICLR), 2023. 2, 3, 4,\n6\n[13] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 3, 4\n[14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems (NeurIPS), 33:6840\u20136851, 2020. 1\n[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLoRA: Low-rank adaptation of large language models. In In-\nternational Conference on Learning Representations (ICLR),\n2022. 1\n[16] Xuhui Jia, Yang Zhao, Kelvin CK Chan, Yandong Li, Han\nZhang, Boqing Gong, Tingbo Hou, Huisheng Wang, and\nYu-Chuan Su. Taming encoder for zero fine-tuning image\ncustomization with text-to-image diffusion models.\narXiv\npreprint arXiv:2304.02642, 2023. 2\n[17] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014. 7\n[18] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00b4ar, and\nRoss Girshick. Segment anything. arXiv:2304.02643, 2023.\n3, 4, 5\n[19] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBLIP-2:\nbootstrapping language-image pre-training with\nfrozen image encoders and large language models. In In-\nternational conference on machine learning (ICML), 2023.\n5, 6\n[20] Tianle Li,\nMax Ku,\nCong Wei,\nand Wenhu Chen.\nDreamedit: Subject-driven image editing.\narXiv preprint\narXiv:2306.12624, 2023. 6\n[21] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\nZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun\nZhu, et al. Grounding dino: Marrying dino with grounded\npre-training for open-set object detection.\narXiv preprint\narXiv:2303.05499, 2023. 5\n[22] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nProceedings of the IEEE/CVF international conference on\ncomputer vision (ICCV), pages 10012\u201310022, 2021. 5\n[23] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher\nYu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting\nusing denoising diffusion probabilistic models. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), pages 11461\u201311471, 2022. 6\n[24] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-\njun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided\nimage synthesis and editing with stochastic differential equa-\ntions. In International Conference on Learning Representa-\ntions (ICLR), 2022. 2\n[25] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and\nDaniel Cohen-Or.\nNull-text inversion for editing real im-\nages using guided diffusion models. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), pages 6038\u20136047, 2023. 2, 3\n[26] Yotam Nitzan, Kfir Aberman, Qiurui He, Orly Liba, Michal\nYarom, Yossi Gandelsman, Inbar Mosseri, Yael Pritch, and\n9\nDaniel Cohen-Or. Mystyle: A personalized generative prior.\nACM Transactions on Graphics (TOG), 41(6):1\u201310, 2022. 2\n[27] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or,\nand Dani Lischinski.\nStyleclip: Text-driven manipulation\nof stylegan imagery. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision (ICCV), pages\n2085\u20132094, 2021. 2\n[28] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision.\nIn International conference on machine learning\n(ICML), pages 8748\u20138763. PMLR, 2021. 2, 5, 8\n[29] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022. 1, 2\n[30] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition (CVPR), pages 10684\u201310695, 2022. 1, 2, 3, 6\n[31] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n22500\u201322510, 2023. 1, 2, 5, 6, 7, 8\n[32] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei,\nTingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein,\nand Kfir Aberman. Hyperdreambooth: Hypernetworks for\nfast personalization of text-to-image models, 2023. 1\n[33] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems (NeurIPS), 35:36479\u201336494, 2022. 1,\n2\n[34] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. Advances in Neural In-\nformation Processing Systems (NeurIPS), 35:25278\u201325294,\n2022. 1\n[35] Shelly Sheynin, Oron Ashual, Adam Polyak, Uriel Singer,\nOran Gafni, Eliya Nachmani, and Yaniv Taigman.\nKnn-\ndiffusion: Image generation via large-scale retrieval. arXiv\npreprint arXiv:2204.02849, 2022. 2\n[36] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instant-\nbooth: Personalized text-to-image generation without test-\ntime finetuning. arXiv preprint arXiv:2304.03411, 2023. 2\n[37] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics.\nIn International confer-\nence on machine learning, pages 2256\u20132265. PMLR, 2015.\n1\n[38] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. In International Conference\non Learning Representations (ICLR), 2020. 3, 5\n[39] Shaoan Xie, Zhifei Zhang, Zhe Lin, Tobias Hinz, and Kun\nZhang. Smartbrush: Text and shape guided object inpaint-\ning with diffusion model. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 22428\u201322437, 2023. 6\n[40] Xin Zhang, Jiaxian Guo, Paul Yoo, Yutaka Matsuo, and\nYusuke Iwasawa. Paste, inpaint and harmonize via denois-\ning: Subject-driven image editing with pre-trained diffusion\nmodel. arXiv preprint arXiv:2306.07596, 2023. 6\n10\nA. Failure analysis of Textual Inversion\nFigure 6. A failure analysis of Textual Inversion [9] method.\nWe present a failure analysis of Textual Inversion as\nshown in Figure 6. The source image is a dog sitting on\na meadow. For a prompt \u201ca sitting dog\u201d, the generated im-\nages mostly contain a dog sitting on a meadow and the dog\u2019s\nappearance is not well preserved.\nB. Algorithm of optimizing T\u03b8\nPlease refer to Algorithm 4 for optimizing T\u03b8.\nAlgorithm 4: Algorithm of optimizing T\u03b8. P(M) \u2208\nRN\u00d73 returns the coordinates where M == 1 and ap-\npends 1\u2019s after the coordinates.\nResult: T \u2217\n\u03b8\nInputs: Mr, Mg\nPr = P(Mr), Pg = P(Mg)\nfor l = [1, ..., L] do\ns = 0\nPt = T\u03b8(Pr)\nfor pt \u2208 Pt do\nm = MAX FLOAT\nfor pg \u2208 Pg do\nx = \u2225pt \u2212 pg\u22252\n2\nif x < m then\nm = x\ns = s + m\n\u03b8 = \u03b8 \u2212 \u03b7\u2207\u03b8s\nT \u2217\n\u03b8 = T\u03b8\nC. Results for personalized subject replace-\nment\nWe show more results for the personalized subject replace-\nment in Figure 7.\nsource images\ninput image\nresult image\nA robot toy sits on the ground with trees in the background\nA fancy boot sits on the ground with trees in the background\n\ud835\udc50!\nsource images\ninput image\nresult image\nA toy duck sits on a rock with ocean in the background\nA candle sits on a rock with ocean in the background\n\ud835\udc50!\nsource images\ninput image\nresult image\nA cat sits in a jungle with grass around it\nA dog sits in a jungle with grass around it\n\ud835\udc50!\nFigure 7. Results for personalized subject replacement.\n11\n"
  },
  {
    "title": "Dolphins: Multimodal Language Model for Driving",
    "link": "https://arxiv.org/pdf/2312.00438.pdf",
    "upvote": "12",
    "text": "DOLPHINS: MULTIMODAL LANGUAGE MODEL FOR DRIVING\nYingzi Ma1\nYulong Cao2\nJiachen Sun3\nMarco Pavone2,4\nChaowei Xiao1,2\n1University of Wisconsin-Madison\n2NVIDIA\n3University of Michigan\n4Stanford University\nABSTRACT\nThe quest for fully autonomous vehicles (AVs) capable of navigating complex real-world scenarios\nwith human-like understanding and responsiveness. In this paper, we introduce Dolphins, a novel\nvision-language model architected to imbibe human-like abilities as a conversational driving assistant.\nDolphins is adept at processing multimodal inputs comprising video (or image) data, text instructions,\nand historical control signals to generate informed outputs corresponding to the provided instructions.\nBuilding upon the open-sourced pretrained Vision-Language Model, OpenFlamingo, we first enhance\nDolphins\u2019s reasoning capabilities through an innovative Grounded Chain of Thought (GCoT) process.\nThen we tailored Dolphins to the driving domain by constructing driving-specific instruction data\nand conducting instruction tuning. Through the utilization of the BDD-X dataset, we designed and\nconsolidated four distinct AV tasks into Dolphins to foster a holistic understanding of intricate\ndriving scenarios. As a result, the distinctive features of Dolphins are characterised into two\ndimensions: (1) the ability to provide a comprehensive understanding of complex and long-tailed\nopen-world driving scenarios and solve a spectrum of AV tasks, and (2) the emergence of human-like\ncapabilities including gradient-free instant adaptation via in-context learning and error recovery\nvia reflection. See the project page for demo, examples, and request pre-trained models: https:\n//vlm-driver.github.io/.\nKeywords Autonomous Driving \u00b7 Vision Language Model\n1\nIntroduction\nThe odyssey toward achieving full autonomy in vehicular systems has been a crucible of innovation, melding insights\nfrom artificial intelligence [1], robotics [2], and automotive engineering [3]. The essential aspiration is to design\nautonomous vehicles (AVs) capable of maneuvering through complex real-world driving situations with human-like\nunderstanding and responsiveness.\nCurrent autonomous driving systems (ADS) [4] are data-driven and typically modular, dividing tasks like perception,\nprediction, planning and control [5]. However, these systems struggle with integration and performance in varied\nsituations. End-to-end (E2E) designs offer a direct sensory input to control output mapping, but they lack interpretability,\nposing challenges in safety and regulatory compliance [6, 7, 8].\nMoreover, existing ADS exhibit many limitations when compared with human drivers including: (1) Holistic Under-\nstanding and Interpretation: existing data-driven Autonomous Driving Systems (ADS) often fall short in holistically\nunderstanding and interpreting dynamic and complex scenarios, especially those within the long-tail distribution of\nopen-world driving environments [9, 10]. For instance, considering a scenario where a ball bounces onto the road,\nfollowed by a child running after it, a human driver could immediately deduce the potential danger and act accordingly\nto prevent any mishap, leveraging a blend of common sense, past experiences, and a fundamental understanding of\nhuman behaviors. In contrast, existing ADS might struggle to interpret this scenario accurately without prior exposure\nto a large amount of similar data. This lack of holistic understanding limits the system\u2019s ability to generalize well\nacross unexpected scenarios that may be located in the long tail of the data distribution [11, 12]. (2) Instant Learning\nand Adaptation: unlike human drivers who can instantly learn and adapt to new scenarios with just a few examples,\nexisting ADS requires extensive training with large amounts of data to handle new situations. For example, a human\narXiv:2312.00438v1  [cs.CV]  1 Dec 2023\ndriver can quickly learn to navigate around a new type of road obstacle after encountering it once or twice, whereas an\nADS might require exposure to many similar scenarios to learn the same lesson. (3) Reflection and Error Recovery:\nexisting ADS typically employ feedforward processing during operation, lacking the capability for real-time correction\nbased on feedback and guidance. In contrast, human drivers can correct their driving behavior in real time based on\nfeedback. For instance, if a human driver takes a wrong turn, they can quickly adjust their decision based on the error\nfeedback, whereas an ADS might struggle to quickly recover from the error feedback [13, 14].\nThese limitations underline the need for an intermediate framework that can bridge the gap between the current state of\nAV systems and human-like driving. Recent advancements in (multimodal) large language models (LLMs) [15, 16, 17]\nwith emergent abilities offer a hopeful path toward addressing these challenges. These models are endowed with a\nrich repository of human knowledge, laying the foundation for valuable insights that could significantly improve ADS.\nHowever, these model are mainly trained on general vision and language data, which restricts their efficacy in the\nspecialized driving domain. Moreover, current model designs can only digest static image and text data to generate\nzero-shot decisions, lacking in handling temporal video input and in-context learning.\nIn this paper, we propose Dolphins (shown in Figure 1), a vision language model (VLM) specifically tailored for AVs,\nas a conversational driving assistant to help reduce the gap between existing ADS and human-like driving.\nBuilt upon OpenFlamingo [18], Dolphins is adapted to the driving domain through a series of specialized instruction\ndatasets and targeted instruction tuning. We first build an image instruction-following dataset with grounded CoT\nresponses based on some public VQA datasets [19, 20, 21, 22], visual instruction datasets [15, 23], and ChatGPT, to\nground the fine-grained reasoning capability into OpenFlamingo models. Then, we utilize BDD-X [24] to establish\nour instruction dataset, focusing on four key AV tasks: behavior comprehension, control signal forecasting, behavior\nanalysis, and in-depth conversation.\nDolphins demonstrates an advanced understanding of complex driving scenarios and human-like abilities such as\ninstant learning, adaptation, reflection, and reasoning, which significantly reduces the gap between existing ADS and\nhuman-like driving. Notably, Dolphins showcases broad task applicability across perception, prediction, and planning,\nthanks to its comprehensive scenario understanding. It interprets static and dynamic scenarios, integrates environmental\nfactors, and handles downstream prediction and planning tasks effectively.\nFurthermore, Dolphins\u2019s in-context learning ability allows it to quickly adapt to new driving conditions, a significant\nadvancement over existing models. Its error recovery mechanism enhances model accuracy and reliability, making it a\nvaluable tool for real-world driving scenarios. Importantly, Dolphins offers interpretability, a crucial factor in building\ntrust and ensuring transparency in ADS operations.\nWe summarize our contribution as three folds:\n\u2022 We propose a VLM-based conversational driving assistant, Dolphins, that plans high-level behaviors like\nhumans complementary to ADS.\n\u2022 We have devised a Grounded Chain of Thought (GCoT) process to initially endow Dolphins with the\ncapability of Chain of Thought reasoning. Following this, we align the model with AV tasks, facilitating its\nunderstanding of the AV context despite the limited scope of the available dataset. This approach not only\ncompensates for dataset constraints but also enables Dolphins to effectively decompose complex tasks and\nlearn the underlying subtasks.\n\u2022 We demonstrate the prominent capability of Dolphins spanning scene understanding and reasoning, instant\nlearning and adaptation, and reflection and error recovery, with both quantitative metrics and qualitative\ndemonstrations.\n2\nRelated Work\nAutonomous Driving with LLMs The recent wave of research focuses on utilizing Large Language Models (LLMs)\nas the driving agents to address autonomous driving-related tasks, such as perception, reasoning, planning, and other\nrelated tasks. For instance, DriveLikeHuman [25] designs a new paradigm to mimic the process of human learning to\ndrive based on LLMs while GPT-Driver [26] leverages GPT-3.5 to assist autonomous driving in dependable motion\nplanning. In a parallel vein, SurrealDriver [27] uses the CARLA simulator for building a LLM-based DriverAgent\nwith memory modules, including short-term memory, long-term guidelines, and safety criteria, which can simulate\nhuman driving behavior to understand driving scenarios, decision-making, and executing safe actions. DriveLM [28]\nand NuPrompt [29] introduce innovative driving tasks based on the NuScenes dataset [30]. Specifically, DriveLM\nleverages the idea of graph-of-thought (GoT) to connect graph-style QA pairs for making decisions and ensuring\nexplainable planning using the powerful reasoning capabilities of LLMs for autonomous driving. NuPrompt employs\n2\n\ud83d\udc2c\nInstruction \nTuning with \nGrounded \nChain of \nThought\n(GCoT)\nScene Understanding\nInstant Adaptation\nError Recovery\n\ud83e\uddd1: Human Driver\n\ud83d\udc2c: Police car in front with siren on, \nslow down and pass with cautious.\n\ud83d\udc2c: <in-context example> Drive slowly \nand avoid snow on the road.\n<\ud83e\uddd1: Left tail light is on.>\n\ud83d\udc2c: The silver car is turning left.\nDolphins\n\ud83e\udda9\nFlamingo\n  Prediction and Planning\nMulti-modal \nIn-context \nInstruction \nTuning in AV\n(MIMIC-IT-AV)\nConversation\n\ud83d\udc2c: It is raining.\n\ud83e\uddd1: What is the weather like?\nFigure 1: Dolphins overview. Demonstrations in Section 5 show that Dolphins\u2019s capabilities on a group of subtasks\nbelonging to the two dimensions of holistic understanding and reasoning, and human-like capabilities. The former\nencompasses autonomous driving-related capabilities such as scene understanding and prediction and planning for the\nego car\u2019s behavior. The latter analyzes three human-level abilities: rapid learning and adaptation, reflection and error\nrecovery, and interactive conversation.\nLLMs to formulate a new prompt-based driving task that focuses on object tracking. However, these works only\naccept linguistic input and lack the incorporation of rich visual features. In contrast, Dolphins excels as a cohesive\nlarge vision-language model, not only possessing the reasoning and planning capabilities of LLMs but also exhibiting\nproficiency in understanding diverse visual features.\nLarge Vision-Language Models (LVLMs). Progress has been witnessed in employing the powerful capabilities\nof large language models like LLaMAs [31, 32], Vicuna [33], and MPT [34] to enhance Large Vision Language\nModels (LVLMs), such as Flamingo [35] and BLIP-2 [36]. Recently, to unlock the capabilities of LVLMs to align with\nhuman preferences, LLaVA [15] and MiniGPT-4 [37] pioneers visual instruction tuning, with subsequent efforts like\nOtter [38], InstructBLIP [39], and Mplug-owl [40] following suit. Building upon these significant contributions, the\npotential of LVLMs has been progressively realized, leading to their swift adaptation across diverse domains, including\nvideo chatting [41, 42, 43, 44], embodied AI [45, 46, 47, 48], 3D-world understanding [49, 50], medical healthcare [51,\n17, 52], marine sector [53], etc. Inspired by these models, we propose Dolphins, the vision-language model designed\nto facilitate autonomous driving-related interaction. This field has already witnessed some related research efforts,\nsuch as HiLM-D [54], DriveGPT4 [55], LINGO-11. However, DriveGPT4 and HiLM-D exhibit a deficiency in task\ndiversity and only accept one video as input, which can significantly curtail the LVLMs\u2019 ability to generalize to\nunseen instructions. To mitigate this issue, we propose Dolphins, which is extended from OpenFlamingo [18] with\nstrong in-context learning capabilities. Furthermore, we employ in-context instruction tuning [56] to enhance few-shot\nadaptations of our model. Consequently, Dolphins is proficient in handling diverse video inputs and exhibits the\ncapacity for rapid adaptation to unseen instructions through in-context learning.\nMultimodal In-context Learning. Flamingo [35] is the pioneering work to support in-context learning in the multi-\nmodal domain by constructing MultiModal MassiveWeb(M2W) and employing the upstream training. Following this\nline of thought, the other works [38, 57] focus on constructing text-image interleaved instruction datasets by adding\nrelated in-context exemplars, thus enhancing the instruction comprehension ability of MLLMs while preserving the\nin-context learning capacity.\n3\nMethod\nTo equip VLMs with a comprehensive understanding and human-like capabilities, we need to ground them within\nthe autonomous vehicle (AV) context to support a variety of tasks. However, limited task-specific labeled data in\nAV has posed a challenge for such grounding. To address this, we initially foster comprehensive reasoning in VLMs\nby utilizing chain-of-thought (CoT) principles [46], applied to a custom VQA dataset. Specifically, we designed a\nvideo-text interleaved dataset by enriching existing datasets, covering all functionalities at a coarse level. Tuning the\nVLM on this dataset enables it to develop capabilities for handling tasks with finer granularity.\nWe introduce our methodology in this section. First, we describe our grounding method for CoT in \u00a7 3.1. Next, we\nelaborate on the creation of our video-text interleaved dataset for autonomous driving in \u00a7 3.2 along with our devised\ntasks. Finally, we detail the multi-modal in-context instruction tuning for AD in \u00a7 3.3.\n1https://wayve.ai/thinking/lingo-natural-language-autonomous-driving/\n3\n3.1\nGCoT Instruction tuning\nReasoning abilities based on fine-grained understanding are essential in AD. This is because the model needs to\nperceive the spatial information of objects in the perceived visual input to infer their relationships and interactions with\nthe ego vehicles. To the best of our knowledge, most VLMs in the literature lack fine-grained multimodal understanding\nof the visual modality (e.g., image and video), primarily due to their coarse-grained alignment in vision-language\npre-training [58, 59]. Although HiLM-D [54] delivers a fine-grained understanding capabilities of VLMs by feeding\nhigh-resolution images and adding a detection module in autonomous driving (AD), it is restricted by the quality of\nthe existing datasets. To further improve the fine-grained understanding of VLMs, we devise grounded CoT (GCoT)\ninstruction tuning and develop a dataset that grounds this ability.\nFigure 2: The process of generating GCoT response for\nVQA tasks to enhance the fined-grained reasoning capability\nof VLMs. ChatGPT is prompted to generate GCoT step by\nstep from text input.\nIdeally, GCoT capability should naturally occur within\nthe autonomous vehicle (AV) context by utilizing datasets\ncomprised of massive driving videos paired with relevant\nquestion and answer sets. However, the availability of\nsuch datasets in the AV domain is markedly limited and\nit is hard to capture the spatial information of a driving\nvideo. We, therefore, design a new method to circumvent\nthis limitation.\nSpecifically, we initially ground the\nGCoT capability in a general image dataset. Recognizing\nthe proficiency of ChatGPT in demonstrating reasoning\nability through detailed step-by-step reasoning, we define\na general pipeline for generating GCoT response using\nChatGPT to enrich the current VQA datasets. As shown\nin Figure 2, this process is divided into three steps: (1)\nbriefly describe the content of the image. (2) identify the\nobject in the question and describe its spatial position. (3)\nif the question requires reasoning, provide the reasoning\nprocess in this step. Finally, we combine the sentences\ngenerated by ChatGPT in these three steps and append\n\u201cSo the answer is {answer}\u201d at the end to form a complete\nGCoT response. This approach involves training the\nmodel on diverse visual data with GCoT response,\nwhere it learns to articulate its reasoning process in a\nstep-by-step manner for various scenarios and objects\nthat might not be specific to driving but are crucial for building foundational reasoning skills. Detailed information\ncan be found in Appendix A.\nSubsequently, we transition this grounded capability to the AV context. This transfer involves aligning the model with\nAV-specific scenarios, where it applies the generalized reasoning ability to the nuanced and dynamic environment\nof autonomous driving. The transfer process includes fine-tuning the model on AV-specific datasets, which, although\nlimited, contain critical driving scenarios, road conditions, and interactions. This stage focuses on adapting the general\nreasoning skills to the specialized requirements of AV scenarios, ensuring that the VLM can apply its fined-grained\nreasoning capability to real-world driving situations effectively.\nIn summary, the development of the fine-grained capability in our VLM is a multi-stage process. It begins with\ngrounding the model in a general image dataset with GCoT responses generated by ChatGPT, followed by a careful\ntransfer and fine-tuning of this skill in the specific context of AD. The use of both real and synthetic AV datasets\nensures a comprehensive and robust training regime, preparing the VLM to handle the intricate and varied challenges\nof autonomous vehicular navigation with nuanced, step-by-step reasoning. nstructions with just a handful of annotated\nexamples in autonomous driving-related tasks.\n3.2\nDevised Instruction Tasks for Autonomous Driving\nFor autonomous driving-related video understanding, we include four tasks critical for perception, prediction and\nplanning as shown in Figure 3: (1) Behavior Understanding. For predicting action description labels in the BDD-X\ndataset, we employ the same instructions for description (noted as Qa) from DriveGPT4 [55] to guide the model in\nlearning the ego car behavior in videos. (2) Behavior Reasoning. Similar to the Behavior Understanding task, we\nalso utilize instructions of justification (noted as Qj) from DriveGPT4 to enable the model to interpret the behavior\nof the ego car. (3) Prediction with Control Signals. In the BDD-X dataset, the time durations of different video\nsegments vary. Hence, in this task, the number of historical control signals provided depends on the duration of the\n4\nPlanning with control signals\nBehavior Understanding\nBehavior Reasoning\nScene Understanding\nDescribe the weather conditions while \ndriving. (from image-instruction pairs)\nWhat is the current behavior of this ego \ncar? (from BDD-X)\nDetailed Conversation\nPredict the speed and turning angle of the ego \ncar in the next second? (from BDD-X)\nWhat is the reason behind this ego car\u2019s \nbehavior? (from BDD-X)\nWhat is the behavior of the ego car? Describe \nthe reasons in detail. (from ChatGPT)\nFigure 3: Overview of our proposed dataset. Compared with the previous datasets, we employ RICES (Retrieval-based\nIn-Context Example Selection) [60] approach to choose in-context examples for each sample. Additionally, We\nintroduce the \"Detailed Conversation\" task to train our model to generate detailed responses that align closely with\nhuman-preferred responses. This instruction is aimed at unlocking the latent potential of the foundation model, which\nhas instruction fine-tuned on the dataset consisting of image-instruction-response triplets.\nvideo segments. VLMs are required to predict the ego car\u2019s speed and turn angle for the next second based on these\ncontrol signals (e.g., speed, accelerator, and turn angle). (4) Detailed Conversation. The three tasks above tend\nto lean towards traditional vision-language tasks (short answer). Consequently, we aim to introduce more detailed\nconversations to enhance instruction generalization ability for human-preferred responses (long answer). Specifically,\nwe rely on the in-context learning ability of ChatGPT [61] to enrich the action description and reasoning labels for\ngenerating human-preferred responses in terms of traffic rules, potential risks of the behavior, driving precautions, etc.\nTo construct a dataset suitable for end-to-end autonomous driving systems, we collect video segments and labels sourced\nfrom the BDD-X dataset [24]. The BDD-X dataset comprises roughly 7,000 videos, with each video being subdivided\ninto multiple segments, each of which conveys distinct behaviors of the ego car along with corresponding textual\nannotations. There are approximately 25,000 examples in total, with annotations including action descriptions (e.g.,\n\"the car stops\") and action reasoning (e.g., \"because the traffic light is red\"). Following the previous work [55], we\nleverage the BDD-X dataset to develop our visual instruction-following dataset for autonomous driving, consisting\nof four distinct autonomous driving-related tasks and their corresponding instructions. However, due to limitations in\nthe diversity of tasks and instructions, the VLM trained on this dataset exhibits a significant deficiency in its ability\nof zero-shot generalization to unseen tasks. Thus, we leverage multi-modal in-context instruction tuning [38] to\nassist our model in the rapid adaptation to new instructions with just a handful of annotated examples in autonomous\ndriving-related tasks.\nIntegrated with our devised tasks, our proposed dataset comprises 32k video-instruction-answer triplets, with 11k\nof them belonging to the detailed conversation task generated by ChatGPT. The remaining three tasks collectively\ncontain 21k triplets from labels of the BDD-X dataset. Noticed that the proposed tasks for constructing the dataset are a\ncoarse-grained set that can be resolved better by a CoT process. As a result, the model grounded on CoT will be forced\nto emerge diverse capabilities beyond such tasks in order to achieve good results on the constructed dataset during the\ninstruction tuning process.\n3.3\nMulti-modal In-context Instruction Tuning in Autonomous Driving\nIn the NLP community, training models with in-context examples is widely considered beneficial for facilitating the\nmodel\u2019s capacity to learn new tasks from several input-output examples, known as few-shot prompting [57, 62, 63, 64,\n65].In terms of visual instruction tuning, Otter [38] has introduced in-context instruction tuning to preserve the VLM\u2019s\nfew-shot, in-context learning capabilities. Motivated by these notable works, we introduce in-context instruction tuning\nto the autonomous driving domain. This field currently faces a severe shortage of diverse instruction-following datasets.\n5\nVision Encoder\nSelf-Attention\nFeed-Forward\nAdd & Norm\nAdd & Norm\nLoRA\nLoRA\nn-th GATED XATTN-DENSE\nn-th LLM Block (e.g., LLaMA, MPT)\n1st GATED XATTN-DENSE\n1st LLM Block (e.g., LLaMA, MPT)\nPerceiver Resamplar\n\u2026\n\u2026\n\u2026\nTime embeds\nMedia embeds\nT frames\nN examples (in-context examples + current instance)\n\u2026\nVideo 1\nVideo 2\nVideo n-1\nVideo\n\u2026\n\u2026\nToken 1\nToken 2\nToken t-1\nToken t\n\u2026\nToken t+1\nToken t+2\nToken n-1\nToken n\n+\nFigure 4: Dolphins\u2019s model architecture.\nWe aim to enhance VLMs\u2019 in-context learning capabilities to facilitate the generalization of models across a spectrum\nof autonomous driving-related tasks.\nIn pursuit of the aforementioned objective, we employ OpenFlamingo [18] as our foundational VLM. OpenFlamingo, a\nreimplementation of Flamingo [35], is trained on the integration of image-text interleaved Lion-2B [66] and MMC4 [67]\ndatasets to enhance its in-context learning capabilities. Our autonomous driving-related instruction dataset, as de-\nscribed in Section 3.2, adopts a format comprising video-instruction-answer triplets. Consequently, we employ\na retrieval approach to select in-context examples for each triplet. Specifically, we utilize VideoMAE [68] and\ntext-embedding-ada-002 2 as the image encoder EImage and text encoder EText, which map a video segment\nXv or a text (instruction-answer pairs) instance Xt to a d-dimensional latent space. Then, we subsequently retrieve\nin-context examples based on the cosine similarity of their representations for each sample Zi = (Xi\nv, Xi\nt). We denote\nthis retrieval pipeline as R:\nR(Zi) = {Top k\nXv\n\u0000cos(EImage(Xi\nv), EImage(Xv))\n\u0001\n,\n(1)\nTop k\nXt\n\u0000cos(EText(Xi\nt), EText(Xt))\n\u0001\n}\n(2)\n= {\u02c6Z1, . . . , \u02c6Z2k}.\n(3)\nWhere k represents that we respectively search k nearest samples in both text-encoded and image-encoded latent space.\nIn essence, examples featuring behaviors akin to those of the ego car within the video are more likely to be selected. In\nour previous research endeavors [69], we observed that the retrieval of in-context examples based on textual similarity\nproved more effective in preserving the VLM\u2019s in-context learning ability compared to using image features. We posit\nthat this conclusion is equally applicable to video-text pairs. Therefore, we only utilize in-context examples retrieved by\ntext embedding similarity and constrain the provision of in-context examples to a maximum of k = 3 per triplet during\nthe training stage.\n4\nTraining\n4.1\nModel Architecture\nOur model is based on OpenFlamingo architecture, named Dolphins. The model consists of a vision encoder from\nCLIP [70], a perceiver resampler to receive the visual features from the vision encoder, and a text encoder from large\nlanguage models (e.g., LLaMA [16], MPT [34]) equipped with gated cross-attention layers for image-text interactions.\nHowever, unlike Flamingo, OpenFlamingo lacks the capability to support video inputs. Therefore, to mitigate the\nvanishing of global temporal features resulting from the aggregation of spatial features, we introduce a set of learned\nlatent vectors as temporal position embeddings. Similarly, another set of learned latent vectors is incorporated to\nfunction as media position embeddings, introducing essential ordering information within the few-shot prompt. The\ninclusion of these embeddings has led to a noteworthy enhancement in the model\u2019s ability in video understanding. To\npreserve the pretraining knowledge and reduce computing consumption, We freeze both the encoders and only finetune\nthe perceiver resampler module, gated cross-attention layers, and LoRA [71] module added to the text encoder, as\nshown in Figure 4.\n2https://platform.openai.com/docs/guides/embeddings/what-are-embeddings\n6\nDefinition: [Task Definition] (four autonomous driving-related tasks)\n# in-context exemplars\nUser: <image>is a driving video.\n[instruction]\nGPT: <answer> [answer] <endofchunk>\n\u00b7 \u00b7 \u00b7\nUser: <image> is a driving video.\n[instruction]\nGPT: <answer> [answer] <endofchunk>\n# current instance\nUser: <image> is a driving video.\n[instruction]\nGPT: <answer> [answer] <endofchunk>\nTable 1: <image> and <endofchunk> tokens are originally from the OpenFlamingo training paradigm, and we follow\nOtter to include a new token <answer> for intercepting the target answer of the model output more easily. Note that\nonly green sequence/tokens are used to compute the loss and we train our model using a cross-entropy loss.\n4.2\nImplementation Details\nInspired by Otter, we employ a similar format to prepare our instruction-tuning data. Additionally, we also introduce a\nspecific task definition at the beginning of each task as a task-level instruction, which aids the model in comprehending\nthe broader context of autonomous driving-related video-instruction pairs of the same type. The training data is\nstructured as shown in Table 1.\nIn contrast to the existing video-related VLMs [72, 73, 74, 75, 55], which typically employ a two-stage training\nframework involving a first stage for aligning video-text features followed by a second stage for visual instruction\ntuning, we remove video alignment stage on general video-text pairs datasets and instead fine-tune initially on\nimage instruction-following datasets that we collect, equipped with grounded CoT templates to enhance fine-grained\nunderstanding and reasoning abilities. Subsequently, we further fine-tuned using our proposed autonomous driving\ninstruction dataset to transfer the model\u2019s capabilities from images to autonomous vehicles (AVs).\nTo optimize Dolphins, We utilize DeepSpeed [76] for optimization during the training process. An AdamW [77]\noptimizer is used, with \u03b21 = 0.9, \u03b22 = 0.999, and a weight decay of 0.01. All training runs on 4 NVIDIA A100 GPUs,\nwith a total batch size of 128, a learning rate of 2 \u00d7 10\u22125 for the second stage. The maximum sequence length is fixed\nat 1024 and BF16 precision is used for both training and inference.\n5\nDemonstration\nIn this section, we show various examples of demonstrations of Dolphins from two dimensions (shown in Figure ??):\nholistic understanding and human-like capabilities, on both zero- and few-shot settings. We will first summarize\nkey desiderata of AV tasks tailored for the VLM setup (\u00a7 5.1). Then we will show that Dolphins has a holistic\nunderstanding with emerged capabilities accomplishing these diverse tasks spanning perception (\u00a7 5.1.1), prediction,\nand planning (\u00a7 5.1.2) even for unseen instructions. Also, we will show the human-like capabilities of Dolphins in (1)\nrapid learning and adaptation through in-context learning (\u00a7 5.2.1); (2) error recovering through reflection (\u00a7 5.2.3); and\n(3) communicating with human through interactive conversation (\u00a7 5.2.2).\n5.1\nHolistic Understanding and Reasoning\nWe transform the traditional perception, prediction, and planning design into a group of subtasks tailored to the\nadvantages of VLM in terms of open-vocabulary detection and comprehensive semantic reasoning.\n\u2022 Attributed road agents/traffic elements. Compared to bounding boxes and tracked history for road agents\ncategorized in a close set of labels, Dolphins should be able to understand road agents and traffic elements\nwith comprehensive semantic attributes including: an open vocabulary semantic type (e.g., a police vehicle,\na kid pedestrian, etc.); a semantic status (e.g., with right turn light on, with green light on, etc.); a behavior\ndescription if it is a dynamic road agent (e.g., turning right in slow speed, parallel parking, etc.). These\ncomprehensive attributes are crucial for understanding the rationale behind the scene with VLM (e.g., giving\nroad to a police vehicle with siren on, right turn light on infering a right turn behavior, etc.).\n\u2022 Operational design domain (ODD). Dolphins should be able to extract ODDs including weather, time of\nday, and geolocation. ODDs provide a high-level driving concept that supervises the downstream prediction\nand planning strategy (e.g., driving slowly and keeping longer safety distance on snowy days).\n7\n\u2022 Ego agent behavior. Dolphins should be able to understand the behavior of the ego agent (e.g., the ego\nvehicle is turning right). With an ego-centric video as the input, it is crucial for the model to understand the\nego agent behavior first to condition other road agents behavior.\n\u2022 Predicted road agent behavior. Compared to prediction-based on trajectory, Dolphins should be able to\nprovide a (probalistical) behavior prediction of road agents\u2019 behaviors to cover different modes in the future\n(e.g., the vehicle on the front right side could follow the lane or change lane to the front of the ego). This is\ncrucial for the ego vehicle to understand the intentions of other road agents and plan its corresponding reactive\nbehavior in advance.\n\u2022 Ego agent future plan. Dolphins should be able to reason on top of the perceived scene and provide a\ninstruction (e.g., as the vehicle on the front right could change lanes to the front of the ego, the ego should\ndrive with caution and be ready to yield to it). By featuring a reasoning of things not to do and contingency\nplanning, Dolphins is capable of planning for safe and flexible actions foreseeing different modes of other\nroad agents.\nIn the following section, we will demonstrate Dolphins\u2019s capability on such subtasks spanning over perception,\nprediction, and planning through holistic understanding and reasoning.\n5.1.1\nPerception\nIn evaluating Dolphins\u2019s holistic understanding on perception tasks, we focus on the understanding of scenario and\nbehavior. These competencies are pivotal for autonomous systems, necessitating acute recognition and comprehension\nof environmental and situational nuances. Our demonstrations reveal Dolphins\u2019s capacity to interpret driving-related\nvisual content, spanning subtasks described in \u00a7 5.1:\n\u2022 Semantic attributes of road agents & traffic elements. Dolphins is able to capture various types of road\nagents and traffic elements with attributes (e.g., black car, red traffic light, evident in Figures 5, 7, 8 and 9);\n\u2022 ODDs. Dolphins is able to understand different ODDs such as weather conditions (as depicted in Figures 7\nand 8), and illumination (as shown in Figures 5 and 10);\n\u2022 Traffic conditions. Dolphins is proficient in pinpointing the precise driving location (as observed in Figure\n5), and overall traffic status (as observed in Figure 5, 7, and 8).\n\u2022 Behavior of road agents. Dolphins is able to understand behaviors of road agents (as shown in Figure 6, 9,\nand 10).\n\u2022 Ego agent behavior. Dolphins is able to understand the ego agent behavior by inferring from the ego-centric\nvideo (as detailed in Figures 8, 6, 9, and 10).\nThis comprehensive perceptual insight allows for a high-quality and fluent natural language response from the system,\nencompassing a wide spectrum of capabilities crucial for autonomous navigation.\n5.1.2\nPrediction and Planning\nFollowing DriveLM [28], We also evaluate Dolphins\u2019s prediction and planning capabilities, which involve utilizing\nthe reasoning ability of VLMs to assist the driver in making decisions and ensuring explainable planning. In Figures 12,\n13, 19 and 15, we showcase our model\u2019s multimodal ability to predict the behavior of other vehicles in the future and\ndetermine whether these vehicles affect the ego agent\u2019s trajectory. Figures 14, 18, 16, and 17 present some examples\nthat demonstrate Dolphins can generate comprehensive plans for the ego car based on current traffic conditions.\nFurthermore, Figures 19 and 15 demonstrate Dolphins\u2019s capabilities to make reasonable and safe plans based on the\ncontingent behavior of other agents, which is considered crucial in real-world driving scenarios. However, due to\nthe lack of relevant instructions during training, we currently recommend using in-context learning ability, ongoing\ndialogue, and control signals to assist Dolphins in completing these two tasks. We believe this is still an under-explored\nfacet and we are working on it.\n5.2\nHuman-like Capabilities\nIn this set of demonstrations, we will show the human-like capabilities of Dolphins in (1) rapid learning and adaptation\nthrough in-context learning (\u00a7 5.2.1); (2) error recovering through reflection (\u00a7 5.2.3); and (3) interpretability through\ninteractive conversation (\u00a7 5.2.2).\n8\n5.2.1\nRapid Learning and Adaptation\nIn this demonstration, we document the agility of Dolphins to rapidly assimilate and adapt to new driving conditions\u2014a\nprocess akin to human learning. This facet is examined by presenting Dolphins with a series of unforeseen scenarios\nand monitoring its response efficiency and accuracy after exposure to a limited set of examples. The tasks are designed\nto test the model\u2019s in-context learning capabilities by progressively introducing more complex and previously unseen\ndriving scenarios. Through this, Dolphins demonstrates its ability to leverage prior knowledge and quickly adapt,\nsuggesting that even in the absence of extensive pre-training on certain tasks, it can still formulate accurate predictions\nand actions using a sparse sampling of in-context examples. Specifically, Figure 20 shows our model\u2019s ability to learn\ncertain common-sense knowledge from in-context examples, such as \u201cYou cannot determine the current time by the\nlight in a tunnel.\u201d. Besides, Dolphins demonstrates the excellent ability to respond to unseen instructions in various\nstyles, such as \u201cWhat if\u201d and \u201cWhat are you doing\u201d, as shown in Figures 20 and 21.\n5.2.2\nInteractive Conversation\nIn this demonstration, we subject Dolphins to an evaluation of its conversational skills through multi-turn dialogues,\ngauging its competence in engaging with human drivers under varying conditions. Utilizing a set of instructions\nprimarily derived from LINGO-1 3, we present Dolphins with a spectrum of queries reflective of real-world driving\ninteractions. As shown in Figures 22, 23, and 24, the conversations are constructed to assess Dolphins\u2019s ability\nto comprehend and respond to nuanced language, maintain context over multiple exchanges, and offer informative\nand contextually relevant responses spanning from potential hazards in the scene to ego planning and the reasoning\nbehind the scene. The results from these interactions indicate that Dolphins possesses a robust conversational ability,\ndistinguishing itself significantly from other contemporary driving-related Vision Language Models in terms of linguistic\nflexibility and contextual understanding. In the future, this could be a foundation for a human interface that builds up\ntrust between AV and road users or its passengers.\n5.2.3\nReflection and Error Recovering\nThis demonstration is devoted to showcasing Dolphins\u2019s self-assessment and error correction mechanisms. We present\ninstances where the model first produces a suboptimal response to a given driving scenario and is subsequently provided\nwith feedback. The focus here is on how Dolphins reflects on this feedback to identify and correct its mistakes. We\nevaluate the effectiveness of these mechanisms through a series of iterative interactions, illustrating the model\u2019s capacity\nfor reflection, error identification, and the implementation of corrective measures. The results (evidenced in Figure 25,\n26 and 27) underscore the model\u2019s ability to not just detect and recover from errors in a manner that mirrors human\ncognitive processes, but to also refine its subsequent responses, thereby enhancing overall performance and reliability.\nFor example, Figure 25 presents that Dolphins is able to reflect on the tail-light signals and predict the correct right\nturning behavior of the black car; and Figure 26 shows that Dolphins can reflect and revise the reasoning for the\nunnecessary yielding decision when encountering a car in a narrow street.\n6\nConclusion and Future Directions\nAs we conclude our exploration into Dolphins, a novel vision-language model designed for enhancing autonomous\nvehicles (AVs), we reflect on the significant strides made and the challenges ahead. Dolphins has demonstrated\na remarkable capacity for holistic understanding and human-like reasoning in complex driving scenarios, marking\na substantial advancement in the realm of autonomous driving technology. By leveraging multimodal inputs and\nemploying the innovative Grounded Chain of Thought (GCoT) process, Dolphins has shown its proficiency as a\nconversational driving assistant, capable of addressing a wide spectrum of AV tasks with enhanced interpretability and\nrapid adaptation capabilities.\nHowever, our journey towards fully optimizing Dolphins for real-world application in AVs encounters notable\nchallenges, particularly in computational overhead and feasibility. Our assessment of Dolphins\u2019s performance on\nthe DriveLM dataset, a realistic benchmark for real-world driving scenarios, revealed an average inference time of\n1.34 seconds on an NVIDIA A100, indicating a potential limitation in achieving high frame rates on edge devices.\nAdditionally, the power consumption associated with running such sophisticated models in vehicles presents a significant\nhurdle for deployment. These findings underscore the necessity of further advancements in model efficiency. Looking\nforward, the development of customized and distilled versions of these models, as suggested by emerging research\n[78], appears to be a promising direction. These streamlined models are anticipated to be more feasible for deployment\non edge devices, balancing computational demands with power efficiency. We believe that continued exploration and\n3https://wayve.ai/thinking/lingo-natural-language-autonomous-driving/\n9\ninnovation in this domain are vital for realizing the full potential of AVs equipped with advanced AI capabilities like\nthose offered by Dolphins.\nFigure 5: An example showcasing Dolphins\u2019s capability in scenario understanding \u00a7 5.1.1. The video features an\nego car driving in a tunnel. Dolphins can identify the environment in which the ego car is situated and accurately\ndetermine the color of the front vehicle as well as infer the current time.\nFigure 6: An example showcasing Dolphins\u2019s capability in scenario understanding and behavior understand-\ning \u00a7 5.1.1. The video features an ego car making a right turn. Dolphins can identify these contents. Words means\nhallucination.\n10\nFigure 7: An example showcasing Dolphins\u2019s capability in scenario understanding \u00a7 5.1.1. The video features an\nego car driving on a snowy street. Dolphins can identify the environment in which the ego car is situated, the presence\nof the traffic light, and accurately determine the color of the passing vehicle.\nFigure 8: An example showcasing Dolphins\u2019s capability in scenario understanding and behavior understand-\ning \u00a7 5.1.1. The video features an ego car stopping at an intersection on a rainy day. Dolphins comprehensively\ndescribes the environment in which the ego car is situated, the behavior of the ego car, and can infer the reasons for its\nbehavior.\n11\nFigure 9: An example showcasing Dolphins\u2019s capability in scenario understanding and behavior understand-\ning \u00a7 5.1.1. The video shows an ego car following a taxi and going through an intersection. Words means hallucination.\nFigure 10: An example showcasing Dolphins\u2019s capability in scenario understanding and behavior understand-\ning \u00a7 5.1.1. The video shows an ego car driving slowly on a busy road at night. Dolphins can identify the ego car\ntraveling at a slow speed and infer that the reason is that the speed of the vehicle ahead is restricting the ego car\u2019s speed.\n12\nFigure 11: An example showcasing Dolphins\u2019s capability in scenario understanding \u00a7 5.1.1. The video shows an\nego car stopped at a busy interaction with a police car next to it. Dolphins can identify the police officer and police car\nwith its emergency lights on.\nFigure 12: An example showcasing Dolphins\u2019s capability in prediction \u00a7 5.1.2. The video features an ego car driving\non a snowy street. Dolphins can predict the trajectory of the white car passing by it. Since both the white car and the\nego car are traveling in the same direction on a one-way road, the trajectories of both cars temporarily overlap.\n13\nFigure 13: An example showcasing Dolphins\u2019s capability in prediction \u00a7 5.1.2. The video shows an ego car making\na U-turn. Dolphins can predict the trajectory of the black car parked in front of it. Since the black car in front is\ncurrently parked on the side of the road, Our model predicts that the car will remain there in the future and will not be\npositioned in the ego car\u2019s trajectory of turning.\nFigure 14: An example showcasing Dolphins\u2019s capability in planning \u00a7 5.1.2 with control signals. Dolphins can also\nunderstand control signals and employ them to predict the speed and turn angle of the ego car next second. Additionally,\nWe can enable Dolphins to formulate plans for the ego car, thereby assisting the driver during driving. However, due\nto the lack of sufficient information, such as the map, driving destination, etc., the plan is still limited to a brief period\nin the future.\n14\nFigure 15: An example showcasing Dolphins\u2019s capability in prediction and contingency planning \u00a7 5.1.2. The video\nfeatures an ego car driving on a snowy city street on a rainy day. Dolphins can predict the future behavior of the white\nvan in front of it while making reasonable contingency plans for the ego car in case the white decelerates or turns right.\nFigure 16: An example showcasing Dolphins\u2019s capability in planning \u00a7 5.1.2. The video features an ego car driving\non a highway and a white car is parked in front of it. Dolphins, by assessing that the white car in front has come to a\nstop, plans for the future behavior of the ego car, which should involve changing lanes to the left to avoid a collision\nwith the stationary white car.\n15\nFigure 17: An example showcasing Dolphins\u2019s capability in planning \u00a7 5.1.2. The video features an ego car driving\non a highway on a rainy day. Dolphins is asked about the information regarding the black car to the right of the ego\ncar, and it accurately determined that the black car\u2019s lane change would not affect the ego car\u2019s trajectory because they\nare separated by two lanes.\nFigure 18: An example showcasing Dolphins\u2019s capability in planning \u00a7 5.1.2. The video features an ego car driving\non a dark city street. Dolphins can recognize that the ego car is approaching an intersection with a red traffic light, so\nit plans the future behavior for the ego car, which should be to come to a stop and wait for the traffic light to turn green\nto pass through the intersection safely.\n16\nFigure 19: An example showcasing Dolphins\u2019s capability in prediction and contingency planning \u00a7 5.1.2. The video\nshows an ego car following a taxi and going through an intersection. On one hand, Dolphins can predict the future\nbehavior of the yellow taxi for a certain period. On the other hand, Dolphins can make reasonable contingency plans\nfor the ego car in case the yellow taxi in front suddenly accelerates or comes to a stop.\nFigure 20: Three examples show our model enables rapid adaptation to unseen instructions through in-context\nlearning \u00a7 5.2.1. In the first two examples, Dolphins learns to play the role of the driver through in-context examples\nand can accurately describe its behavior, despite not having been trained on such instructions. The third example shows\nthat Dolphins can learn common sense from in-context examples, such as not being able to judge the current time\nbased on the light when inside a tunnel.\n17\nFigure 21: An example showcasing Dolphins\u2019s capability in prediction through in-context learning \u00a7 5.2.1.\nDolphins can reason for future events that have not yet happened. Through in-context examples, our model can learn\nto answer \"What If\"-style questions, to imagine the behavior of the ego car in the future by language.\nFigure 22: An example showcasing Dolphins\u2019s capability in interactive conversation \u00a7 5.2.2. The video features an\nego car stopping at an intersection on a rainy day, where Dolphins can answer various questions about the weather, the\ntraffic lights, and their impact on driving.\n18\nFigure 23: An example showcasing Dolphins\u2019s capability in interactive conversation \u00a7 5.2.2. This video shows\nan ego car stopped at an intersection waiting for a red light and a pedestrian crossing a zebra crossing. We can ask\nDolphins some questions about what it\u2019s paying attention to at this intersection.\nFigure 24: An example showcasing Dolphins\u2019s capability in interactive conversation \u00a7 5.2.2. This video shows an\nego car driving behind a bicyclist. Through conversation with Dolphins, we can understand how to drive safely in the\npresense of cyclists. And our model can also create a tagline for this scene.\n19\nFigure 25: An example showcasing Dolphins\u2019s capability in reflection \u00a7 5.2.3. The video features an ego car driving\nthrough an interaction on a city street. The black vehicle in front of the ego car appears to want to turn right as its right\ntaillight is flashing. Initially, Dolphins predicts that the black car will continue to drive forward. However, after being\nprovided with information about the \u2019slashing rear lights indicate a turn or lane change.\u2019 traffic rule, Dolphins reflects\nand revises the wrong prediction.\nFigure 26: An example showcasing Dolphins\u2019s capability in reflection \u00a7 5.2.3. The video shows an ego car driving\nthrough an interaction with a white car coming from the opposite direction. Initially, Dolphins plans for the ego car\nto slow down and let the white car proceed. However, after being provided with information about the human driver\ndecision, Dolphins reflects and revises the driving plan.\n20\nFigure 27: An example showcasing Dolphins\u2019s capability in reflection \u00a7 5.2.3. The video shows an ego car driving on\na highway. Initially, Dolphins predicts that the white car in front of the ego car will continue to drive forward. However,\nafter being provided with information about the human driver decision, Dolphins revises the wrong prediction and\nrealizes that the ego car should have also stopped to avoid the white car that is about to stop and merge into the left lane.\nFigure 28: An example showcasing Dolphins\u2019s capability in reflection \u00a7 5.2.3. The video shows an ego car stopping\nbehind a black car. Initially, Dolphins plans for the ego car to remain stopped and wait for the traffic. However, after\nbeing provided with information about the human driver decision (driving more aggressively), Dolphins reflects and\nplans for the ego car to change lanes to the left to continue moving forward when careful attention is paid to cars\ntraveling the left lane, such as a white van.\n21\nReferences\n[1] Michael I Jordan and Tom M Mitchell. Machine learning: Trends, perspectives, and prospects.\nScience,\n349(6245):255\u2013260, 2015.\n[2] Mohsen Soori, Behrooz Arezoo, and Roza Dastres. Artificial intelligence, machine learning and deep learning in\nadvanced robotics, a review. Cognitive Robotics, 2023.\n[3] Gijs Mom. The evolution of automotive technology: a handbook. SAE International, 2023.\n[4] Wei Li, CW Pan, Rong Zhang, JP Ren, YX Ma, Jin Fang, FL Yan, QC Geng, XY Huang, HJ Gong, et al. Aads:\nAugmented autonomous driving simulation using data-driven algorithms. Science robotics, 4(28):eaaw0863,\n2019.\n[5] Ardi Tampuu, Tambet Matiisen, Maksym Semikin, Dmytro Fishman, and Naveed Muhammad. A survey of\nend-to-end driving: Architectures and training methods. IEEE Transactions on Neural Networks and Learning\nSystems, 33(4):1364\u20131384, 2020.\n[6] Daniel Coelho and Miguel Oliveira. A review of end-to-end autonomous driving in urban environments. IEEE\nAccess, 10:75296\u201375311, 2022.\n[7] Li Chen, Penghao Wu, Kashyap Chitta, Bernhard Jaeger, Andreas Geiger, and Hongyang Li. End-to-end\nautonomous driving: Challenges and frontiers. arXiv preprint arXiv:2306.16927, 2023.\n[8] Liangkai Liu, Sidi Lu, Ren Zhong, Baofu Wu, Yongtao Yao, Qingyang Zhang, and Weisong Shi. Computing\nsystems for autonomous driving: State of the art and challenges. IEEE Internet of Things Journal, 8(8):6469\u20136486,\n2020.\n[9] Ashesh Jain, Luca Del Pero, Hugo Grimmett, and Peter Ondruska. Autonomy 2.0: Why is self-driving always 5\nyears away? arXiv preprint arXiv:2107.08142, 2021.\n[10] Kelvin Wong, Yanlei Gu, and Shunsuke Kamijo. Mapping for autonomous driving: Opportunities and challenges.\nIEEE Intelligent Transportation Systems Magazine, 13(1):91\u2013106, 2020.\n[11] Felipe Codevilla, Eder Santana, Antonio M L\u00f3pez, and Adrien Gaidon. Exploring the limitations of behavior\ncloning for autonomous driving. In Proceedings of the IEEE/CVF International Conference on Computer Vision,\npages 9329\u20139338, 2019.\n[12] Lucas de Paula Veronese, Fernando Auat-Cheein, Filipe Mutz, Thiago Oliveira-Santos, Jos\u00e9 E Guivant, Edilson\nDe Aguiar, Claudine Badue, and Alberto Ferreira De Souza. Evaluating the limits of a lidar for an autonomous\ndriving localization. IEEE Transactions on Intelligent Transportation Systems, 22(3):1449\u20131458, 2020.\n[13] Jesse Levinson, Jake Askeland, Jan Becker, Jennifer Dolson, David Held, Soeren Kammel, J Zico Kolter, Dirk\nLanger, Oliver Pink, Vaughan Pratt, et al. Towards fully autonomous driving: Systems and algorithms. In 2011\nIEEE intelligent vehicles symposium (IV), pages 163\u2013168. IEEE, 2011.\n[14] Ekim Yurtsever, Jacob Lambert, Alexander Carballo, and Kazuya Takeda. A survey of autonomous driving:\nCommon practices and emerging technologies. IEEE access, 8:58443\u201358469, 2020.\n[15] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. ArXiv, abs/2304.08485,\n2023.\n[16] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language\nmodels. arXiv preprint arXiv:2302.13971, 2023.\n[17] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung\nPoon, and Jianfeng Gao. Llava-med: Training a large language-and-vision assistant for biomedicine in one day.\narXiv preprint arXiv:2306.00890, 2023.\n[18] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bit-\nton, Samir Yitzhak Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell\nWortsman, and Ludwig Schmidt. Openflamingo: An open-source framework for training large autoregressive\nvision-language models. ArXiv, abs/2308.01390, 2023.\n[19] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter:\nElevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 6904\u20136913, 2017.\n[20] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question answering\nbenchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and\npattern recognition, pages 3195\u20133204, 2019.\n22\n[21] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and com-\npositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 6700\u20136709, 2019.\n[22] Kushal Kafle and Christopher Kanan. An analysis of visual question answering algorithms. In Proceedings of the\nIEEE international conference on computer vision, pages 1965\u20131973, 2017.\n[23] Bo Zhao, Boya Wu, and Tiejun Huang. Svit: Scaling up visual instruction tuning. arXiv preprint arXiv:2307.04087,\n2023.\n[24] Jinkyu Kim, Anna Rohrbach, Trevor Darrell, John F. Canny, and Zeynep Akata. Textual explanations for\nself-driving vehicles. In European Conference on Computer Vision, 2018.\n[25] Daocheng Fu, Xin Li, Licheng Wen, Min Dou, Pinlong Cai, Botian Shi, and Yu Qiao. Drive like a human:\nRethinking autonomous driving with large language models. arXiv preprint arXiv:2307.07162, 2023.\n[26] Jiageng Mao, Yuxi Qian, Hang Zhao, and Yue Wang.\nGpt-driver: Learning to drive with gpt.\nArXiv,\nabs/2310.01415, 2023.\n[27] Ye Jin, Xiaoxi Shen, Huiling Peng, Xiaoan Liu, Jingli Qin, Jiayang Li, Jintao Xie, Peizhong Gao, Guyue Zhou,\nand Jiangtao Gong. Surrealdriver: Designing generative driver agent simulation framework in urban contexts\nbased on large language model. arXiv preprint arXiv:2309.13193, 2023.\n[28] DriveLM Contributors. Drivelm: Drive on language. https://github.com/OpenDriveLab/DriveLM, 2023.\n[29] Dongming Wu, Wencheng Han, Tiancai Wang, Ying-Hao Liu, Xiangyu Zhang, and Jianbing Shen. Language\nprompt for autonomous driving. ArXiv, abs/2309.04379, 2023.\n[30] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan,\nYu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving. arXiv\npreprint arXiv:1903.11027, 2019.\n[31] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave,\nand Guillaume Lample. Llama: Open and efficient foundation language models. ArXiv, abs/2302.13971, 2023.\n[32] Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Cant\u00f3n Ferrer, Moya\nChen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao,\nVedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas,\nViktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux,\nThibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar\nMishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan\nSchelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams,\nJian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan\nNarang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation\nand fine-tuned chat models. ArXiv, abs/2307.09288, 2023.\n[33] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang,\nYonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing\ngpt-4 with 90%* chatgpt quality, March 2023.\n[34] MosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023.\nAccessed: 2023-05-05.\n[35] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur\nMensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao\nGong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh,\nSahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Si-\nmonyan. Flamingo: a visual language model for few-shot learning. ArXiv, abs/2204.14198, 2022.\n[36] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. Blip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models. ArXiv, abs/2301.12597, 2023.\n[37] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. ArXiv, abs/2304.10592, 2023.\n[38] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal\nmodel with in-context instruction tuning. ArXiv, abs/2305.03726, 2023.\n23\n[39] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Albert\nLi, Pascale Fung, and Steven C. H. Hoi. Instructblip: Towards general-purpose vision-language models with\ninstruction tuning. ArXiv, abs/2305.06500, 2023.\n[40] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yi Zhou, Junyan Wang, Anwen Hu, Pengcheng Shi,\nYaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qiang Qi, Ji Zhang, and Feiyan Huang.\nmplug-owl: Modularization empowers large language models with multimodality. ArXiv, abs/2304.14178, 2023.\n[41] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao.\nVideochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023.\n[42] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video\nunderstanding. arXiv preprint arXiv:2306.02858, 2023.\n[43] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed\nvideo understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023.\n[44] Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Minghui Qiu, Pengcheng Lu, Tao Wang, and Zhongyu Wei.\nValley: Video assistant with large language model enhanced ability. arXiv preprint arXiv:2306.07207, 2023.\n[45] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. arXiv\npreprint arXiv:2303.03378, 2023.\n[46] Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao,\nand Ping Luo. Embodiedgpt: Vision-language pre-training via embodied chain of thought. arXiv preprint\narXiv:2305.15021, 2023.\n[47] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding,\nDanny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge\nto robotic control. arXiv preprint arXiv:2307.15818, 2023.\n[48] Jingkang Yang, Yuhao Dong, Shuai Liu, Bo Li, Ziyue Wang, Chencheng Jiang, Haoran Tan, Jiamu Kang, Yuanhan\nZhang, Kaiyang Zhou, et al. Octopus: Embodied vision-language programmer from environmental feedback.\narXiv preprint arXiv:2310.08588, 2023.\n[49] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm:\nInjecting the 3d world into large language models. arXiv preprint arXiv:2307.12981, 2023.\n[50] Runsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen, Jiangmiao Pang, and Dahua Lin. Pointllm: Empowering\nlarge language models to understand point clouds. arXiv preprint arXiv:2308.16911, 2023.\n[51] Tianyu Han, Lisa C Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexander L\u00f6ser,\nDaniel Truhn, and Keno K Bressem. Medalpaca\u2013an open-source collection of medical conversational ai models\nand training data. arXiv preprint arXiv:2304.08247, 2023.\n[52] Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Cyril Zakka, Yash Dalmia, Eduardo Pontes Reis,\nPranav Rajpurkar, and Jure Leskovec. Med-flamingo: a multimodal medical few-shot learner. arXiv preprint\narXiv:2307.15189, 2023.\n[53] Ziqiang Zheng, Jipeng Zhang, Tuan-Anh Vu, Shizhe Diao, Yue Him Wong Tim, and Sai-Kit Yeung. Marinegpt:\nUnlocking secrets of ocean to the public. arXiv preprint arXiv:2310.13596, 2023.\n[54] Xinpeng Ding, Jianhua Han, Hang Xu, Wei Zhang, and Xiaomeng Li.\nHilm-d: Towards high-resolution\nunderstanding in multimodal large language models for autonomous driving. arXiv preprint arXiv:2309.05186,\n2023.\n[55] Zhenhua Xu, Yujia Zhang, Enze Xie, Zhen Zhao, Yong Guo, Kenneth K.Y. Wong, Zhenguo Li, and Hengshuang\nZhao. Drivegpt4: Interpretable end-to-end autonomous driving via large language model. ArXiv, abs/2310.01412,\n2023.\n[56] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, C. Li, and Ziwei Liu. Mimic-it:\nMulti-modal in-context instruction tuning. ArXiv, abs/2306.05425, 2023.\n[57] Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. Metaicl: Learning to learn in context.\nArXiv, abs/2110.15943, 2021.\n[58] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, and Ping Luo.\nGpt4roi: Instruction tuning large language model on region-of-interest. arXiv preprint arXiv:2307.03601, 2023.\n[59] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal\nllm\u2019s referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023.\n24\n[60] Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context learning. arXiv\npreprint arXiv:2112.08633, 2021.\n[61] Openai chat. https://chat.openai.com. Accessed: 2023-10-20.\n[62] Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He He. Meta-learning via language model in-context\ntuning. ArXiv, abs/2110.07814, 2021.\n[63] Srinivas Iyer, Xiaojuan Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu\nWang, Qing Liu, Punit Singh Koura, Xian Li, Brian O\u2019Horo, Gabriel Pereyra, Jeff Wang, Christopher Dewan,\nAsli Celikyilmaz, Luke Zettlemoyer, and Veselin Stoyanov. Opt-iml: Scaling language model instruction meta\nlearning through the lens of generalization. ArXiv, abs/2212.12017, 2022.\n[64] S. Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le, Barret Zoph,\nJason Wei, and Adam Roberts. The flan collection: Designing data and methods for effective instruction tuning.\nIn International Conference on Machine Learning, 2023.\n[65] Julian Coda-Forno, Marcel Binz, Zeynep Akata, Matthew M. Botvinick, Jane X. Wang, and Eric Schulz. Meta-in-\ncontext learning in large language models. ArXiv, abs/2305.12907, 2023.\n[66] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine\nCrowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion-5b: An open large-scale dataset for\ntraining next generation image-text models. ArXiv, abs/2210.08402, 2022.\n[67] Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig\nSchmidt, William Yang Wang, and Yejin Choi. Multimodal c4: An open, billion-scale corpus of images interleaved\nwith text. ArXiv, abs/2304.06939, 2023.\n[68] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. VideoMAE: Masked autoencoders are data-efficient\nlearners for self-supervised video pre-training. In Advances in Neural Information Processing Systems, 2022.\n[69] Anonymous. Understanding multimodal instruction format for in-context learning. In Submitted to The Twelfth\nInternational Conference on Learning Representations, 2023. under review.\n[70] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language\nsupervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.\n[71] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.\n[72] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video\nunderstanding. ArXiv, abs/2306.02858, 2023.\n[73] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wen Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao.\nVideochat: Chat-centric video understanding. ArXiv, abs/2305.06355, 2023.\n[74] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed\nvideo understanding via large vision and language models. ArXiv, abs/2306.05424, 2023.\n[75] Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Ming-Hui Qiu, Pengcheng Lu, Tao Wang, and Zhongyu Wei.\nValley: Video assistant with large language model enhanced ability. ArXiv, abs/2306.07207, 2023.\n[76] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable\ntraining deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD\nInternational Conference on Knowledge Discovery & Data Mining, pages 3505\u20133506, 2020.\n[77] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on\nLearning Representations, 2018.\n[78] Tinychat: Large language model on the edge. https://hanlab.mit.edu/blog/tinychat. Accessed: 2023-\n10-20.\n[79] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and\nC Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th\nEuropean Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\u2013755.\nSpringer, 2014.\n[80] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis\nKalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced\ndense image annotations. International journal of computer vision, 123:32\u201373, 2017.\n25\nA\nData\nImage instruction-following dataset enriched with GCoT response.\nThe majority of vision-language tasks can be\ngenerally viewed as Visual Question Answering (VQA) tasks, requiring the model to provide answers to queries related\nto the image. Therefore, we collect 4 VQA datasets to generate GCoT response by ChatGPT, including VQAv2 [19],\nOK-VQA [20], GQA [21], and TDIUC [22]. Except for GQA, the image source for these tasks is MSCOCO [79],\nwhich contains many images, but each image has fewer annotations of caption and object. This may result in the object\nfrom the question not having corresponding position information during step (2), making it difficult for ChatGPT to\nprovide an accurate reasoning process. Therefore, we use the Visual Genome dataset [80] as a supplement, as it has\nricher annotations and intersects with MSCOCO. The GQA task provides detailed object annotations but lacks captions,\nwhich presents a challenge to ChatGPT in comprehending the overall content of the image. So we organize the objects,\nattributes, and their relationships in the annotations into sentences, which are used to describe the relationships between\ntwo objects in the image in place of captions. After preparation, we prompt ChatGPT to follow the aforementioned\nthree steps to generate GCoT templates step by step. The prompts can be found in Table 2. In addition, we also include\nLLaVA-instruct-80k [15] and SVIT [23] datasets to enhance the model\u2019s instruction-following capability. In summary, in\nthe first stage, Dolphins is trained on an image instruction-following dataset comprising approximately 10.7k examples.\nWithin this dataset, there are 9,645 VQA examples accompanied by GCoT responses, which are generated by ChatGPT.\nVideo instruction-following dataset based on BDD-X.\nTo transition the model\u2019s powerful scene understanding\nand reasoning ability, which has been fine-tuned on the image instruction-following datasets, to the driving video\ndomain, we construct an autonomous driving-related instruction-following dataset based on BDD-X, and at the same\ntime retrieve in-context examples to generate few-shot templates for training to enhance model\u2019s in-context learning\nability by retrieve method.\nB\nPrompts\nThe prompts used to instruct ChatGPT to generate the grounded CoT process with three thinking steps for VQA tasks\nare shown in Table 2.\n26\nSystem Message\nGive you some captions, each describing the image you are observing and specific object locations within\nthe image are given, along with detailed coordinates. These coordinates are in the form of bounding boxes,\nrepresented as (x1, y1, x2, y2) with floating numbers ranging from 0 to 1. These values correspond to the\ntop left x, top left y, bottom right x, and bottom right y. Besides, you will receive a question about the image\nand the reference answer to this question.\nI want you to create a paragraph to explain how to obtain the reference answer for the question. To create\nthe paragraph, follow these steps:\n1. Create a short sentence to describe the image by using captions.\n2. Find the question object contained in the question. If the question object exists in the provided objects,\ncreate a sentence describing the object and its bounding box; Instead, find the most relevant one with the\nquestion object from the provided objects and create a sentence describing the object and its bounding box.\n3. If the answer to the question requires reasoning, create a sentence to explain the question. For instance,\nthe question is \"When was this piece of sporting equipment invented?\" and if in step 3 you have determined\nthat the \"sports equipment\" is a surfboard, based on your background knowledge, the sentence you need\nto create is: \"In 1926 an American surfer named Tom Blake (1902 - 1994) invented the very first, hollow\nsurfboard.\"; If the answer to the question does not require reasoning and can be derived directly by looking\nat the image, for example, the question \"What color of the bear is?\", then in this step, you do not need to\ncreate any sentences to explain it.\nNote: You need to determine the position between the objects based on their bounding box. For example\nobject A: [x1, y1, x2, y2], object B: [a1, b1, a2, b2]. If x2 > a2, it means that object A is to the right of\nobject B, and vice versa. If y2 > b2, it means that object A is on top of object B, and vice versa.\nUser\nCaptions:\nMan in all black swimsuit walking down a beach with his surfboard.\nA man in a wetsuit carrying a surfboard to the water.\nA person with a surfboard walking on a beach.\nA person with a surfboard walks to the water.\nA man carrying a surfboard across a sandy beach.\nObjects:\nbird: [0.095, 0.797, 0.355, 0.849], surfboard: [0.388, 0.418, 1.254, 0.977], person: [0.431, 0.222, 0.941,\n1.362]\nQuestion: When was this piece of sporting equipment invented?\nReference Answer: 1926\nOutput\n1. The picture shows a man carrying a surfboard across a sandy beach.\n2. So the sporting equipment in question should refer to the surfboard.\n3. In 1926 an American surfer named Tom Blake (1902 - 1994) invented the very first, hollow surfboard.\nTable 2: One example to prompt ChatGPT for generating GCoT to enhance the VLM\u2019s capabilities of fine-grained\nmultimodal understanding and reasoning, which are considered the important proficiency in ADs.\n27\n"
  },
  {
    "title": "Instruction-tuning Aligns LLMs to the Human Brain",
    "link": "https://arxiv.org/pdf/2312.00575.pdf",
    "upvote": "10",
    "text": "Preprint.\nINSTRUCTION-TUNING ALIGNS LLMS TO THE\nHUMAN BRAIN\nKhai Loong Aw, Syrielle Montariol\u2217, Badr AlKhamissi\u2217, Martin Schrimpf\u2020, Antoine Bosselut\u2020\nEPFL\nABSTRACT\nInstruction-tuning is a widely-adopted method of finetuning that enables large lan-\nguage models (LLMs) to generate output that more closely resembles human re-\nsponses to natural language queries, in many cases leading to human-level per-\nformance on diverse testbeds. However, it remains unclear whether instruction-\ntuning truly makes LLMs more similar to how humans process language. We\ninvestigate the effect of instruction-tuning on LLM-human similarity in two ways:\n(1) brain alignment, the similarity of LLM internal representations to neural ac-\ntivity in the human language system, and (2) behavioral alignment, the similar-\nity of LLM and human behavior on a reading task. We assess 25 vanilla and\ninstruction-tuned LLMs across three datasets involving humans reading naturalis-\ntic stories and sentences. We discover that instruction-tuning generally enhances\nbrain alignment by an average of 6%, but does not have a similar effect on be-\nhavioral alignment. To identify the factors underlying LLM-brain alignment, we\ncompute correlations between the brain alignment of LLMs and various model\nproperties, such as model size, various problem-solving abilities, and performance\non tasks requiring world knowledge spanning various domains. Notably, we find\na strong positive correlation between brain alignment and model size (r = 0.95),\nas well as performance on tasks requiring world knowledge (r = 0.81). Our results\ndemonstrate that instruction-tuning LLMs improves both world knowledge rep-\nresentations and brain alignment, suggesting that mechanisms that encode world\nknowledge in LLMs also improve representational alignment to the human brain.\n1\nINTRODUCTION\nInstruction-tuning is a widely adopted method for finetuning large language models (LLMs) on\ndatasets containing task-specific instructions. This approach enhances their ability to generalize ef-\nfectively to previously unseen tasks by learning to follow provided instructions (Wang et al., 2022c).\nInstruction-tuning often costs only a small fraction of compute relative to pretraining (Chung et al.,\n2022), yet propels pretrained LLMs to incredible performance leaps on reasoning and problem-\nsolving benchmarks. This transformation has enabled LLMs to approach human performance on\nmany tasks, despite using only few (or zero) training examples, as well as to tackle open-world\nreasoning tasks previously only achievable by humans (Zhang et al., 2023).\nIn addition to teaching LLMs to understand and follow human instructions, instruction-tuning also\nimproves the ability of LLMs to mimic the ground-truth outputs (often human-written) of the train-\ning data. This property allows them to produce more controllable and predictable output that is\ndeemed (1) more desirable by human evaluators on various metrics (Zhang et al., 2023; Chung\net al., 2022; Wang et al., 2022b), (2) more aligned to human values (Chia et al., 2023), and (3) more\nstylistically similar to human outputs (Dasgupta et al., 2022; Safdari et al., 2023).\nConsequently, instruction-tuning yields LLMs more similar to humans in both capability and output\nrememblance. From a neuroscience perspective, this begs the question: Does instruction-tuning\nmake LLMs more similar to the human language system? Previous work has shown that models\nwith higher task performance are more aligned to the human language system (Schrimpf et al., 2021;\n\u2217Equal contribution\n\u2020Equal supervision / senior authors\n1\narXiv:2312.00575v1  [cs.CL]  1 Dec 2023\nPreprint.\nGoldstein et al., 2022; Caucheteux & King, 2022), and, on some datasets, even hit the estimated\nnoise ceiling.1 However, there has been no similar study on how instruction-tuning, the method that\nhas enabled powerful LLMs such as ChatGPT, affects alignment to the human language system.\nIn this work, we explore the impact of instruction-tuning on the alignment between LLMs and the\nhuman language system, considering two aspects: (1) brain alignment, which assesses how closely\nLLMs\u2019 internal representations match neural activity patterns in the human language system, and (2)\nbehavioral alignment, which evaluates the similarity between LLM behavior and human behavior.\nTo conduct this study, both LLMs and human participants are presented with the same language\nstimuli comprised of naturalistic stories and sentences. For LLMs, we analyze their internal rep-\nresentations and per-word perplexity, while for humans, we use previously collected brain activity\ndata from functional magnetic resonance imaging (fMRI) experiments and per-word reading times.\nTo measure brain alignment, we use the Brain-Score (Schrimpf et al., 2018) linear predictivity met-\nric, assessing how well LLM representations predict human brain activity in response to the same\nlanguage stimuli (Jain & Huth, 2018; Toneva & Wehbe, 2019; Schrimpf et al., 2021; Oota et al.,\n2023), using data from three neural datasets: Pereira et al. (2018), Blank et al. (2014), and Wehbe\net al. (2014). To evaluate behavioral alignment, we use a benchmark in Brain-Score which calculates\nthe Pearson correlation between LLM per-word perplexity and human per-word reading times from\nthe Futrell et al. (2018) dataset. Perplexity for LLMs and reading times for humans offer insights\ninto comprehension difficulty (Ehrlich & Rayner, 1981; Hale, 2001; Smith & Levy, 2013), allowing\nus to examine whether LLMs and humans share similarities in terms of which words and sentences\nthey find challenging or surprising. Because models vary in their brain and behavioral alignment\nacross different architectures and training objectives (Schrimpf et al., 2021), we estimate the effect\nof instruction-tuning across 17 instruction-tuned LLMs and 8 vanilla LLMs, and report a significant\nincrease in brain alignment by instruction-tuned models compared to vanilla ones.\nTo investigate why instruction-tuning increases alignment to human brain activity, we then esti-\nmate the contribution of various LLM properties towards brain alignment. Specifically, we compute\nPearson correlations between an LLM\u2019s brain alignment and its properties, including next-word\nprediction (NWP) ability, model size, a range of problem-solving abilities, and world knowledge\nspanning different domains. The evaluation of the latter two properties is based on the Big-Bench\nHard benchmark (BBH) (Suzgun et al., 2022) and the Massive Multi-task Language Understanding\nbenchmark (MMLU) (Hendrycks et al., 2021), respectively.\nWe report three major findings:\n1. Instruction-tuning generally improves the alignment of LLM representations to brain ac-\ntivity, increasing brain alignment by 6.2% on average for the LLMs and neural datasets we\ntested (Figure 1).\n2. Investigating the factors underlying LLM-brain alignment, we find that world knowl-\nedge and model size are strongly correlated with brain alignment (r = 0.81 and 0.95 for\ninstruction-tuned models, respectively; Figure 2).\n3. Surprisingly, our results generally indicate that instruction-tuning LLMs does not enhance\nbehavioral alignment with human reading times. Furthermore, behavioral alignment on this\ndataset demonstrates poor correlations with all other measures we investigate, including\ntask performance and model size (Figure 3).\n2\nBACKGROUND & RELATED WORK\nEffect of Instruction-tuning on LLMs.\nInstruction-tuning is an effective method for enhancing\nLLM capability and controllability. It trains LLMs using pairs of human instructions and desired\noutputs. The benefits of instruction-tuning can be categorized into three key aspects (Zhang et al.,\n2023): (1) it bridges the disparity between the pretraining objective of LLMs (next-word predic-\ntion) and the goal of accurately following human instructions, (2) it achieves greater control and\npredictability of model behavior compared to standard LLMs, allowing researchers to make them\nmore similar to humans in both capability and output similarity (Chia et al., 2023; Dasgupta et al.,\n1In fMRI recordings, an upper limit of representational similarity can be computed by sampling from the\nsame participant twice, deducing a threshold defined by the noise level of the data gathering process.\n2\nPreprint.\nrepresentations\n1. Train linear regression\n2. Test on held-out data\nfMRI activity\nLanguage stimuli\nLanguage model\nHuman\nPereira2018\n\"Beekeeping encourages the conservation\nof local habitats. It is in every beekeeper's ...\"\n\"If you were to journey to the North of England,\nyou would come to a valley that ...\"\nBlank2014\nWehbe2014\n\"Harry had never believed he would meet a\nboy he hated more than Dudley, but ...\"\n0.10 0.12 0.14 0.16 0.18 0.20 0.22 0.24\nVanilla LM\n0.10\n0.12\n0.14\n0.16\n0.18\n0.20\n0.22\n0.24\nInstruction-tuned LM\nT5\nLlama\nAverage Brain alignment (Pearson corr.)\nB\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n# Instruction-tuning epochs\n0.29\n0.30\n0.31\n0.32\n0.33\n0.34\nBBH Overall score\nInstruction\nNo instruction\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n# Instruction-tuning epochs\n0.225\n0.230\n0.235\nBrain alignment (r)\nInstruction\nNo instruction\nA\n0\n2\n4\n6\n8\n10\n12\n% change in brain alignment\nPereira2018\nBlank2014\nWehbe2014\nNeural dataset\n+6.9%\n+8.0%\n+3.8%\nC\nD\nE\naverage improvement: 6.2%\nBrain alignment\nLM properties\nFigure 1: Instruction-tuning aligns LLM representations to human brain activity. (A) We eval-\nuate brain alignment as the similarity of an LLM\u2019s internal representations to human brain activity,\nusing a linear predictivity metric in Brain-Score. We evaluate 25 vanilla and instruction-tuned LLMs\nwith sizes between 77M and 33B parameters. We use 3 neural datasets of humans reading naturalis-\ntic stories and sentences: PEREIRA2018, BLANK2014, and WEHBE2014, and compute the average\nacross the three datasets. (B) Instruction-tuning improves average brain alignment by 6.2% on av-\nerage. Each point above the identity line represents an instruction-tuned LLM that has greater brain\nalignment than its vanilla counterpart. Error bars (here and elsewhere) represent median absolute\ndeviation over human participants. (C) Instruction-tuning generally improves brain alignment on all\nthree neural datasets. (D) We instruction-tune LLaMA-7B using the Alpaca dataset (\u201cInstruction\u201d\nmodel). We also train an ablation model with the same process and training data, but remove the\ninstruction portion from each training sample (\u201cNo Instruction\u201d model). This experiment shows that\nbrain alignment improvements are due to both (1) training data (present in both models) and (2) the\nprocess of training LLMs to understand and follow instructions (present only in original model).\n2022; Safdari et al., 2023), and (3) it often costs only a small fraction of compute relative to pre-\ntraining, enabling LLMs to swiftly adapt to target domains (Chung et al., 2022). We contribute to\nthis research area from a neuroscience perspective, by studying whether instruction-tuning makes\nLLMs more aligned to the human language system in terms of brain and behavioral alignment.\nEffect of Finetuning on Brain alignment.\nPrior works have studied how finetuning affects LMs\u2019\nalignment to human brain activity. These include finetuning on a wide range of downstream NLP\ntasks (Oota et al., 2022), finetuning to summarize narratives (Aw & Toneva, 2023), and finetun-\ning to directly predict brain activity recordings (Schwartz et al., 2019). These studies aim to use\nbrain alignment to study how finetuning affects LMs and their representations. Our work builds on\nthis line of research by demonstrating that instruction-tuning aligns LLM representations to human\nbrain activity. We also investigate why instruction-tuned LLMs align to brain activity by testing the\ncorrelation of brain alignment with various world knowledge domains and problem-solving abilities.\nLM properties linked to Brain alignment.\nThere is a growing body of work disentangling the\ncontribution of various LM properties towards brain alignment. These include studying how brain\nalignment is driven by next-word prediction ability (Schrimpf et al., 2021; Caucheteux & King,\n2022), multi-word semantics (Merlin & Toneva, 2022), performance on various NLP tasks (Oota\n3\nPreprint.\net al., 2022), and model size (Antonello et al., 2023). To disentangle the contribution of various\nLM properties toward brain alignment, we use instruction-tuned LLMs. They are especially useful\nas they have been trained to respond to a standard question-answer format, allowing us to evaluate\nLLMs on a wide array of tasks and in a more fine-grained manner. We expand this area of research\nby identifying that world knowledge is a key property underlying LLM-brain alignment.\n3\nLANGUAGE MODELS\nWe evaluate the brain alignment of 25 large language models (LLMs) from two model families: T5\n(Raffel et al., 2020) and LLaMa (Touvron et al., 2023). T5 models are encoder-decoder LLMs pre-\ntrained on the Colossal Common Crawl Corpus (C4), a corpus of 356 billion tokens, using a masked\ninfilling objective, and then further finetuned on multi-task mixture of unsupervised and supervised\ntasks converted into a text-to-text format. In our study, we use all five T5 models with sizes between\n77M to 11B parameters. LLaMA models (Touvron et al., 2023) are decoder-only LLMs trained\non 1.6 trillion tokens from a mixture of corpora including C4, English CommonCrawl, Wikipedia,\nGithub, and more. For LLaMA, we use the 7B, 13B, and 33B parameter versions in our study.\nFor the instruction-tuned variants of T5 models, we utilize a variety of models finetuned on the\nFLAN suite (15M examples for 1,836 different tasks accompanied by instructions, Chung et al.,\n2022), Alpaca (52K instruction-following examples generated through methods inspired by Self-\nInstruct, Wang et al. (2022a), Taori et al., 2023), and GPT4ALL (437K instruction-following exam-\nples generated with GPT-3.5-turbo, Anand et al., 2023) datasets. As for the LLaMa model family,\nwe employ Vicuna\u2019s 7B, 13B, and 33B models (Chiang et al., 2023), which are finetuned on user-\nshared conversations. Additionally, we incorporate the StableVicuna-13B model, which further\nrefines the Vicuna-13B model using reinforcement learning from human feedback (RLHF) (Ouyang\net al., 2022) on a range of conversational and instructional datasets. We also use the 7B version of\nAlpaca (Taori et al., 2023). Additional details about these LLMs can be found in Appendix A.\nTable 1: Example of instruction-tuning training data format: (Instruction, Input, Output) from\nthe Alpaca dataset (Taori et al., 2023). The input field is optional for certain types of instructions.\nInstruction\nInput\nOutput\n\u201cWrite a short paragraph\n\u201cThe importance of\n\u201cThe use of renewable energy is growing\nabout the given topic.\u201d\nusing renewable energy.\u201d\nrapidly in relevance and ...\u201d\n4\nBRAIN ALIGNMENT\nBrain alignment refers to the method of evaluating the similarity between LLM representations and\nhuman brain activity (Figure 1). This relies on fMRI recordings of human subjects while they read\nlanguage stimuli on potentially any topic (here: Pereira et al., 2018; Blank et al., 2014; Wehbe et al.,\n2014). The same language stimuli from prior brain recordings are provided as input to LLMs,\nwhose intermediate layer activations are extracted as their representations of the language stimuli.\nWe follow a general approach previously used in several works (Schrimpf et al., 2018; 2021; Jain\n& Huth, 2018; Toneva & Wehbe, 2019; Oota et al., 2023; Aw & Toneva, 2023). Specifically, we\nuse the linear predictivity metric implemented in Brain-Score (Schrimpf et al., 2020, Figure 1), first\ntraining a linear function to predict fMRI voxels associated with the human language system using\nLLM representations as input features. We then apply this linear function to held-out brain activity\ndata from the original corpus of recordings, and evaluate the brain alignment of the LLM as the\nPearson correlation between the predicted and actual brain activity data. For each LLM, we evaluate\nits brain alignment for every layer (e.g., LLaMA-7B has 32 layers), and use the highest value as the\nLLM\u2019s brain alignment value, following Schrimpf et al. (2018).\nDatasets\nWe use three fMRI datasets to measure the brain alignment of LLMs. Each dataset\ninvolves a different set of human participants and uses a different set of language stimuli.\nPEREIRA2018 (experiments 2 and 3 from Pereira et al., 2018): In experiment 2, nine participants\nread 384 sentences organized into 96 text passages. In experiment 3, six participants read 243\nsentences in 72 text passages. Each sentence was displayed for four seconds on a screen.\n4\nPreprint.\nBLANK2014 (Blank et al., 2014): The data consists of fMRI recordings of 5 human participants\nlistening to naturalistic stories from the Natural Stories Corpus (Futrell et al., 2018).\nWEHBE2014 (Wehbe et al., 2014): The data includes fMRI recordings of 8 human participants read-\ning chapter 9 of the book Harry Potter and the Sorceror\u2019s Stone (Rowling et al., 1998). Participants\nread the chapter at a fixed interval of one word every 0.5 seconds.\n4.1\nINSTRUCTION-TUNING ALIGNS LLM REPRESENTATIONS TO HUMAN BRAIN ACTIVITY\nFirst, we study the effect of instruction-tuning on LLM brain alignment. We compute each LLM\u2019s\naverage brain alignment as the mean of its brain alignment on the 3 neural datasets. We find that\ninstruction-tuning improves alignment by an average of 6.2% across all tested LLMs (Figure 1B).\nThis holds across all three neural datasets, with average improvements of +6.9% on PEREIRA2018,\n+8.0% improvement on BLANK2014, and +3.8% on WEHBE2014 (Figure 1C). This provides a\nstrong signal that instruction-tuning improves brain alignment. Moreover, a smaller instruction-\ntuned model can attain higher brain alignment than a larger model from the same family that has not\nbeen instruction-tuned (e.g., Alpaca-7B v.s. LLaMa-13B, see detailed results in Appendix E).\nTo longitudinally study how instruction-tuning aligns LLM representations to brain activity, we sep-\narately instruction-tune a LLaMA-7B model on the Stanford Alpaca instruction dataset (Taori et al.,\n2023) for 3 epochs. By evaluating checkpoints regularly during training, we find that instruction-\ntuning progressively improves brain alignment (Figure 1D). We also perform an ablation study to\ndisambiguate the effect on brain alignment of (1) the instruction-following ability provided by the\ninstruction-tuning step and (2) added training data. We fine-tune LLaMA-7B with the same process\nand data, but remove the instruction portion from each training sample. We observe that brain align-\nment of this ablated model increases during fine-tuning but stays lower than its instruction-following\ncounterpart (Figure 1D). This shows that brain alignment improvements are due to both factors.\n9.0\n9.5\n10.0\n10.5\n11.0\n11.5\nlog10(# model parameters)\n0.14\n0.16\n0.18\n0.20\n0.22\n0.24\nBrain alignment (r)\nflan-t5-small\nflan-t5-base\nflan-alpaca-base\nflan-t5-large\nflan-alpaca-large\nflan-t5-xl\nflan-alpaca-xl\nflan-gpt4all-xl\nflan-sharegpt-xl\nflan-alpaca-gpt4-xl\nflan-t5-xxl\nflan-alpaca-xxl\nalpaca-7b\nvicuna-7b\nvicuna-13b\nstable-vicuna-13b\nvicuna-33b\nT5\nLlama\nr = 0.95, p \u226a 0.001\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\nBBH world knowledge\n0.14\n0.16\n0.18\n0.20\n0.22\n0.24\nBrain alignment (r)\nflan-t5-small\nflan-t5-base\nflan-alpaca-base\nflan-t5-large\nflan-alpaca-large\nflan-t5-xl\nflan-alpaca-xl\nflan-gpt4all-xl\nflan-sharegpt-xl\nflan-alpaca-gpt4-xl\nflan-t5-xxl\nflan-alpaca-xxl\nalpaca-7b\nvicuna-7b\nvicuna-13b\nstable-vicuna-13b\nvicuna-33b\nT5\nLlama\nr = 0.68, p < 0.005\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\nMMLU Overall score\n0.14\n0.16\n0.18\n0.20\n0.22\n0.24\nBrain alignment (r)\nflan-t5-small\nflan-t5-base\nflan-alpaca-base\nflan-t5-large\nflan-alpaca-large\nflan-t5-xl\nflan-alpaca-xl\nflan-gpt4all-xl\nflan-sharegpt-xl\nflan-alpaca-gpt4-xl\nflan-t5-xxl\nflan-alpaca-xxl\nalpaca-7b\nvicuna-7b\nvicuna-13b\nstable-vicuna-13b\nvicuna-33b\nT5\nLlama\nr = 0.81, p \u226a 0.001\nA\nB\nC\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPearson correlation\n Pereira2018\n Blank2014\n Wehbe2014\n***\n*\nn.s.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPearson correlation\n Pereira2018\n Blank2014\n Wehbe2014\n**\n*\nn.s.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPearson correlation\n Pereira2018\n Blank2014\n Wehbe2014\n******\n**\nn.s.\nBrain alignment\nLM properties\nFigure 2: World knowledge and model size are important factors underlying LLM-brain align-\nment. To identify factors underlying brain alignment, we test Pearson correlations between brain\nalignment and various LLM properties, such as model size, world knowledge in various domains\n(MMLU benchmark), and various types of problem-solving abilities (BBH benchmark). Insets dis-\nplay results on individual datasets, with stars reflecting statistical significance (n.s. = p > 0.05, *\n= p < 0.05, ** = p < 0.005, etc.) (A) Brain alignment is significantly and strongly correlated with\nworld knowledge as evaluated by the MMLU Overall score (r = 0.81), which reports the mean per-\nformance across all MMLU subjects. (B) Brain alignment is significantly and strongly correlated\nwith performance on the world knowledge task category in BBH (r = 0.68). (C) Brain alignment is\nsignificantly and strongly correlated with model size (logarithm of number of parameters) (r = 0.95).\n4.2\nFACTORS UNDERLYING LLM-BRAIN ALIGNMENT\nTo identify factors underlying the representational similarity between LLMs and human brains, we\ncompute the Pearson correlation between LLM brain alignment and various properties of LLMs:\nperformance on a benchmark involving different reasoning abilities (BBH benchmark; Suzgun\net al., 2022), performance on a benchmark requiring domain-specific world knowledge (MMLU;\nHendrycks et al., 2021), language modeling ability, and model size.\n5\nPreprint.\nWorld Knowledge and Reasoning abilities MMLU is designed to measure the world knowledge\nof LLMs across many subject domains. It contains 57 tasks, categorized by the subject domain of\nworld knowledge tested: STEM, Humanities, Social Sciences, and Others (a broad category rang-\ning from finance to marketing to professional medicine). BBH contains 23 tasks, grouped into four\ncategories based on the type of problem-solving ability tested: Algorithmic and Multi-Step Arith-\nmetic Reasoning; Natural Language Understanding; Use of World Knowledge; and Multilingual\nKnowledge and Reasoning. For both benchmarks, we use category classifications from the original\npapers. We measure the performance of LLMs on BBH and MMLU using the instruct-eval\nrepository2 with default settings (3-shots, 5-shots respectively) and preset prompts. We measure\nthe Pearson correlation (and its p-value) between LLM brain alignment and performance on each\ncategory of the MMLU and BBH benchmarks. We obtain p-values after performing false discovery\nrate correction, ensuring our results are significant and not due to random chance.\nWe find that brain alignment is significantly and strongly correlated with world knowledge. On the\nMMLU benchmark, we observe a high correlation between brain alignment scores and the MMLU\nOverall score (r = 0.81), which reports the mean performance across all world knowledge subject\ndomains on MMLU (Figure 2A). Similarly, brain alignment is also significantly and strongly cor-\nrelated with the mean performance on tasks included in the world knowledge category of the BBH\nbenchmark (r = 0.68; Figure 2B). Interestingly, we do not find strong correlations with other dimen-\nsions of the BBH benchmark (e.g., Algorithmic reasoning and Multilingual reasoning, see Table 2),\nthough this could also be due to limitations of the tested models (most are primarily pretrained on\nEnglish language data), as indicated by their low raw performance scores on some tasks. Overall,\nour results provide a strong signal that more accessible representations of world knowledge are a\nkey factor in aligning LLM representations to human brain activity.\nLanguage Modeling Ability\nPrior works have shown correlations between brain alignment and\nnext-word prediction (NWP) ability (Caucheteux & King, 2022; Schrimpf et al., 2021). We find\nsimilar results for correlation between brain alignment and NWP loss (r = -0.54, Appendix G).\nInterestingly, the strength of correlation between brain alignment and world knowledge performance\n(r = 0.81) is greater than that between brain alignment and NWP loss (r = -0.54). This result suggests\nthat world knowledge understanding is a better predictor of brain alignment than NWP ability.\nModel Size Finally, we find that brain alignment is significantly and strongly correlated with model\nsize (r = 0.95), as measured by the logarithm of the number of model parameters (Figure 2C).\nSchrimpf et al. (2021) observe such a pattern for language models, and we find the pattern holds for\ninstruction-tuned models, and models trained at a larger scale than their study (7B+ parameters).\nHowever, model size alone does not determine brain alignment. Our results show that smaller\ninstruction-tuned LLMs can have greater brain alignment than larger vanilla models. For exam-\nple, LLaMA-13B obtains brain alignment of 0.220, Vicuna-13B obtains 0.229, LLaMA-33B ob-\ntains 0.227, and Vicuna-33B obtains 0.232. Hence, Vicuna-13B has greater brain alignment than\nLLaMA-33B despite being less than 40% its size. We observe a similar trend in another four models:\nT5-base, Flan-T5-base, T5-large, Flan-T5-large. Also, prior works have shown that large random\nmodels achieve poor brain alignment (Schrimpf et al., 2021). These results demonstrate that there\nare LLM properties aside from model size that contribute significantly to brain alignment.\nDatasets\nWe observe significant correlations between brain alignment and world knowledge and\nmodel size on PEREIRA2018 and WEHBE2014, but surprisingly not BLANK2014. We hypothesize\nthat it may be because BLANK2014 has a small participant size (N = 5) that leads to greater noise\nin the results, compared to PEREIRA2018 and WEHBE2014.\n5\nBEHAVIORAL ALIGNMENT\nIn the previous section, we show that instruction-tuning aligns the internal representations of LLMs\nto human brain recordings (Section 4.1). In this section, we explore whether instruction-tuning also\naligns LLM behavior to human behavior.\nFollowing the approach previously proposed by Schrimpf et al. (2021) and implemented in the\nBrain-Score package (Schrimpf et al., 2020), we measure behavioral alignment by evaluating the\n2https://github.com/declare-lab/instruct-eval\n6\nPreprint.\nTable 2: Brain alignment strongly correlates with world knowledge across all subject domains\nin MMLU, as well as the world knowledge category in BBH. At the same time, brain alignment is\nnot significantly correlated with all other types of problem-solving abilities in BBH (e.g., algorithmic\nor multilingual reasoning). We obtain p-values after performing false discovery rate correction,\nensuring our results are significant and not due to random chance.\nTask category\nBrain Alignment\ncorrected\nNumber\nAverage Model\nCorrelation (r)\np-value\nof tasks\nPerformance\nMMLU \u2013 Overall Score\n0.809\n0.000329\n57\n0.36\nMMLU \u2013 STEM\n0.792\n0.000343\n18\n0.28\nMMLU \u2013 Humanities\n0.791\n0.000343\n13\n0.34\nMMLU \u2013 Social Sciences\n0.807\n0.000329\n12\n0.41\nMMLU \u2013 Others\n0.809\n0.000329\n14\n0.40\nBBH \u2013 Overall score\n0.384\n0.177\n23\n0.28\nBBH \u2013 Algorithmic reasoning\n0.194\n0.558\n8\n0.22\nBBH \u2013 Language understanding\n0.163\n0.585\n3\n0.43\nBBH \u2013 World knowledge\n0.679\n0.005\n5\n0.36\nBBH \u2013 Multilingual reasoning\n-0.035\n0.895\n1\n0.19\nBBH \u2013 Others\n0.478\n0.083\n6\n0.27\nsimilarity between LLM per-word perplexity and human per-word reading times, given the same\nlanguage stimuli (Figure 3A). We use the self-paced reading times dataset from Futrell et al. (2018),\nconsisting of the reading times of 179 human participants recorded while they were visually pre-\nsented with 10 naturalistic stories. We provide language stimuli from this data as input to LLMs\nand measure their per-word perplexity. Finally, we evaluate behavioral alignment by computing the\nPearson correlation between per-word LLM perplexity and per-word human reading times.\nUsing the same set of models as in the previous section, we compare the behavioral alignment\nof each instruction-tuned LLM against its vanilla counterpart. Our results generally indicate that\ninstruction-tuning LLMs does not improve behavioral alignment to human reading times (Fig-\nure 3B). For half of the LLMs tested, it results in no change or reduced behavioral alignment.\nThen, we test the correlation between LLM behavioral alignment and model size, next-word pre-\ndiction ability, various reasoning abilities (as measured by performance on the BBH benchmark),\nand world knowledge across various domains (as measured by performance on the MMLU bench-\nmark). Contrary to our findings on the correlation between brain alignment and model size and\nworld knowledge (Section 4.2), we do not find that these factors are correlated with the behavioral\nalignment of models: world knowledge in Figure 3C (r = 0.08, p = 0.76), model size in Figure 3D\n(r = 0.26, p = 0.31), next-word prediction loss for T5 models in Figure 3E (r = -0.2, p = 0.54), and\nnext-word prediction loss for LLaMA models in Figure 3F (r = 0.68, p = 0.21). We discuss our\ninterpretations of these results and possible explanations in Section 6.2.\n6\nDISCUSSION\n6.1\nIMPLICATIONS FOR NLP: BUILDING LLMS\nUsing brain alignment to interpret LLMs. Representations in the human brain support a wide\nrange of downstream tasks that current models still struggle on. Prior works have used brain activity\nresults to interpret neural networks (Dong & Toneva, 2023) and build more performant models\n(Dapello et al., 2020; 2022; Safarani et al., 2021). Instruction-tuning has emerged as a breakthrough\ntechnique to improve LLM abilities, quality of outputs, and allow LLMs to adapt to new tasks with\nminimal task-specific training. However, the manner in which instruction-tuning alters the internal\nrepresentations of LLMs to achieve these improvements remains an open question. Brain activity\ndata provides a neuroscientific angle to investigate this question. Our results show that instruction-\ntuning LLMs improves both world knowledge representations and human brain alignment. They\nsuggest that instruction-tuning improves the ability of an LLM to retrieve world knowledge stored\nin its model weights and that the mechanisms that encode world knowledge in LLMs also improve\n7\nPreprint.\n4.0\n4.2\n4.4\n4.6\nNWP loss\n0.17\n0.18\n0.19\n0.20\nBehavioral alignment (r)\nalpaca-7b\nvicuna-7b\nvicuna-13b\nstable-vicuna-13b\nvicuna-33b\nLlama\nn.s. (p=0.21)\n0.6\n0.8\n1.0\n1.2\nNWP loss\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nBehavioral alignment (r)\nflan-t5-small\nflan-t5-base\nflan-alpaca-base\nflan-t5-large\nflan-alpaca-large\nflan-t5-xl\nflan-alpaca-xl\nflan-gpt4all-xl\nflan-sharegpt-xl\nflan-alpaca-gpt4-xl\nflan-t5-xxl\nflan-alpaca-xxl\nT5\nn.s. (p=0.54)\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\nMMLU Overall score\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nBehavioral alignment (r)\nflan-t5-small\nflan-t5-base\nflan-alpaca-base\nflan-t5-large\nflan-alpaca-large\nflan-t5-xl\nflan-alpaca-xl\nflan-gpt4all-xl\nflan-sharegpt-xl\nflan-alpaca-gpt4-xl\nflan-t5-xxl\nflan-alpaca-xxl\nalpaca-7bvicuna-7b\nvicuna-13b\nstable-vicuna-13b\nvicuna-33b\nT5\nLlama\nn.s. (p=0.76)\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nVanilla LM\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nInstruction-tuned LM\nT5\nLlama\nAverage Behavioral alignment (r)\n9.0\n9.5\n10.0\n10.5\n11.0\n11.5\nlog10(# model parameters)\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nBehavioral alignment (r)\nflan-t5-small\nflan-t5-base\nflan-alpaca-base\nflan-t5-large\nflan-alpaca-large\nflan-t5-xl\nflan-alpaca-xl\nflan-gpt4all-xl\nflan-sharegpt-xl\nflan-alpaca-gpt4-xl\nflan-t5-xxl\nflan-alpaca-xxl\nalpaca-7b\nvicuna-7b\nvicuna-13b\nstable-vicuna-13b\nvicuna-33b\nT5\nLlama\nA\nperplexity\nfor each word\nreading times\nfor each word\nCompute\nPearson correlation\nLanguage stimuli\nLanguage model\nHuman\n\"If you were to journey to the North of England,\nyou would come to a valley that ...\"\nFutrell2018\nBehavioral alignment\nLM properties\nB\nE\nF\nC\nD\nn.s. (p=0.31)\nFigure 3: Instruction-tuning LLMs generally does not improve behavioral alignment to hu-\nman reading times. Furthermore, behavioral alignment correlates poorly with all other tested\nmeasures: world knowledge, model size, and next-word prediction (NWP) ability. (A) We present\nthe same language stimuli to LLMs and human participants, using the Futrell2018 benchmark in\nBrain-Score, which contains naturalistic stories. We compute the behavioral alignment as the Pear-\nson correlation between LLM perplexity for each word and human reading times for each word.\n(B) Instruction-tuning does not generally improve behavioral alignment. Furthermore, behavioral\nalignment is poorly and not significantly correlated with all other measures: (C) world knowledge\non MMLU (r = 0.08, p = 0.76), (D) model size (r = 0.26, p = 0.31), (E) NWP loss for T5 models (r\n= -0.2, p = 0.54), and (F) NWP loss for LLaMA models (r = 0.68, p = 0.21).\nrepresentational alignment to the human brain. Overall, our results highlight the importance of\nintegrating world knowledge in the development of future LLMs.\n6.2\nIMPLICATIONS FOR NEUROSCIENCE: STUDYING LLM-HUMAN ALIGNMENT\nInstruction-tuned LLMs are useful for studying LLM properties underlying brain and behav-\nioral alignment.\nTo identify why LLM and human brains exhibit representational similarities,\nprior work has mostly focused on high-level properties such as model size (Antonello et al., 2023),\nand external behaviors such as predicting missing words Schrimpf et al. (2021); Caucheteux & King\n(2022). However, a key to understanding these similarities is to identify lower-level or internal\nproperties of LLMs that underlie brain alignment. This includes the amount of knowledge LLMs\ncontain, e.g., factual (AlKhamissi et al., 2022) and commonsense (Sap et al., 2020; Bosselut et al.,\n2019). Our work shows that we can harness instruction-tuned LLMs for this purpose as they have\nbeen trained to respond to a general question format, allowing us to evaluate LLMs in a more fine-\ngrained manner. This allows the study of both internal (e.g., knowledge) and external (e.g., behavior)\nproperties of LLMs, and how they correlate with brain and behavioral alignment.\n8\nPreprint.\nExamining more dimensions of behavior.\nTo evaluate behavioral alignment, our work and many\nprior works compare LM and human next-word surprisal on reading tasks (Wilcox et al., 2020;\nSchrimpf et al., 2021; Eghbal A. Hosseini et al., 2023). This evaluates only a single dimension\nof LM and human behavior (per-word perplexity and reading times). On the models we test here,\nbehavioral alignment is not significantly correlated with model size, world knowledge, or next-word\nprediction ability. While next-word prediction performance correlates with alignment to human\nreading times across a broad band of models (Schrimpf et al., 2021), this trend does not hold up\nin recent Transformer-based LMs (Oh & Schuler, 2023), having a surprising negative correlation\nwith parameter count (Oh et al., 2022). Our results highlight the need to create more benchmarks\nto expand the dimensions of behavior examined for both LLMs and humans, in order to holistically\nevaluate LLM behavior, as well as LLM-human behavioral alignment.\nBrain alignment datasets with humans performing diverse tasks.\nOur work studies brain align-\nment using neural datasets that are limited to humans reading naturalistic stories and sentences in\nEnglish. Unfortunately, there does not exist brain activity data for human participants attempting the\nBBH and MMLU benchmarks. This may explain why brain alignment is not significantly correlated\nwith many categories of problem-solving on BBH, e.g., language understanding. In the future, we\nhope to study brain alignment with human participants performing more diverse sets of tasks, e.g.,\nreading computer program code (Ivanova et al., 2020). This can identify more factors underlying\nLLM-brain alignment, and provide insights into how brain activity and the human language system\nmay be shaped by various forms of problem-solving. Furthermore, for the neural datasets in our\nwork, many of the larger models exceed the noise ceiling estimates (Appendix E), highlighting the\nneed for more neural datasets (with better ways of computing noise ceiling estimates).\nWorld knowledge shapes brain activity.\nOur results show that world knowledge is a key factor\nin aligning LLM representations to human brain activity. LLMs with greater world knowledge\nacross all tested subject domains produce representations that align more closely to human brain\nactivity. Analogously, this suggests that world knowledge influences human brain activity, shaping\nthe language comprehension systems in the brain.\n6.3\nLIMITATIONS AND FUTURE WORK\nLimited model families.\nA model\u2019s performance on MMLU and BBH is affected not only by the\nquantity of knowledge it possesses, but also by its ability to follow each benchmark\u2019s instruction\nformat. Hence, we restricted our study to two model families (T5 and LLaMA). This attempts to\nensure that the models are trained on a similar instruction format, so they have similar ability to\nfollow the instruction format of MMLU and BBH. Thus, any differences in their MMLU or BBH\nperformance would point to differences in the quantity of knowledge they possess. However, we\nhope to add a greater variety of LLMs in future work to strengthen the generalizability of our claims.\nComputational cost.\nWe investigate 25 LLMs, with the largest models having 33B parameters,\non many datasets: brain alignment (PEREIRA2018, BLANK2014, WEHBE2014), behavioral align-\nment (FUTRELL2018), next-word prediction (WikiText-2) and other evaluations (BBH, MMLU).\nUnfortunately, this makes it computationally expensive to replicate our experiments. In the hopes of\naddressing this concern, we provide our full results in Appendices E, F, G, H, and I.\n7\nCONCLUSION\nWe investigate whether instruction-tuning improves the alignment of LLMs to the human language\nsystem. We evaluate 25 LLMs with parameter sizes ranging from 77 million to 33 billion, across\nthree neural datasets of humans reading naturalistic stories and sentences. We find that instruction-\ntuning generally improves the alignment of LLM representations to brain activity. Delving into the\nfactors underlying LLM-brain alignment, we discover that world knowledge and model size are key\ndeterminants of brain alignment. This suggests that world knowledge helps shape representations in\nthe human language system, and highlights the significance of integrating world knowledge in the\ndevelopment of future LLMs.\n9\nPreprint.\nREPRODUCIBILITY STATEMENT\nAll models and code repositories utilized in this study are open-source and their corresponding\nlinks are provided in Appendix B and D for full transparency and reproducibility. For the calcula-\ntion of brain and behavioral alignment, we relied on the Brain-Score repository (www.github.\ncom/brain-score/language), which serves as a publicly accessible resource for conduct-\ning these assessments. We encourage researchers interested in replicating our findings to refer to\nthe provided links and consult the Brain-Score repository for further details on datasets and the\nevaluation process. For measuring the performance of LLMs on BBH and MMLU benchmarks,\nwe use the broadly-used instruct-eval repository (https://github.com/declare-lab/\ninstruct-eval) with default settings.\nREFERENCES\nBadr AlKhamissi, Millicent Li, Asli Celikyilmaz, Mona Diab, and Marjan Ghazvininejad. A Review\non Language Models as Knowledge Bases, April 2022. URL http://arxiv.org/abs/\n2204.06031. arXiv:2204.06031 [cs].\nYuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar.\nGpt4all: Training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo.\nhttps://github.com/nomic-ai/gpt4all, 2023.\nRichard Antonello, Aditya Vaidya, and Alexander G. Huth.\nScaling laws for language en-\ncoding models in fMRI, May 2023.\nURL http://arxiv.org/abs/2305.11863.\narXiv:2305.11863 [cs].\nKhai Loong Aw and Mariya Toneva.\nTraining language models to summarize narratives im-\nproves brain alignment, February 2023.\nURL http://arxiv.org/abs/2212.10898.\narXiv:2212.10898 [cs, q-bio].\nIdan Blank, Nancy Kanwisher, and Evelina Fedorenko.\nA functional dissociation between lan-\nguage and multiple-demand systems revealed in patterns of BOLD signal fluctuations. Jour-\nnal of Neurophysiology, 112(5):1105\u20131118, September 2014.\nISSN 0022-3077, 1522-1598.\ndoi: 10.1152/jn.00884.2013.\nURL https://www.physiology.org/doi/10.1152/\njn.00884.2013.\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, and\nYejin Choi. Comet: Commonsense transformers for automatic knowledge graph construction.\nIn Annual Meeting of the Association for Computational Linguistics, 2019.\nURL https:\n//api.semanticscholar.org/CorpusID:189762527.\nCharlotte Caucheteux and Jean-R\u00b4emi King.\nBrains and algorithms partially converge in natu-\nral language processing.\nCommunications Biology, 5(1):134, February 2022.\nISSN 2399-\n3642. doi: 10.1038/s42003-022-03036-1. URL https://www.nature.com/articles/\ns42003-022-03036-1.\nYew Ken Chia, Pengfei Hong, Lidong Bing, and Soujanya Poria.\nINSTRUCTEVAL: Towards\nHolistic Evaluation of Instruction-Tuned Large Language Models, June 2023.\nURL http:\n//arxiv.org/abs/2306.04757. arXiv:2306.04757 [cs].\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An\nopen-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https:\n//lmsys.org/blog/2023-03-30-vicuna/.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan\nLi, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu,\nZhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie\nPellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent\nZhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-\ncob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei.\nScaling Instruction-\nFinetuned Language Models, December 2022.\nURL http://arxiv.org/abs/2210.\n11416. arXiv:2210.11416 [cs].\n10\nPreprint.\nJoel Dapello, Tiago Marques, Martin Schrimpf, Franziska Geiger, David D. Cox, and James J.\nDiCarlo.\nSimulating a Primary Visual Cortex at the Front of CNNs Improves Robustness to\nImage Perturbations.\nIn Neural Information Processing Systems (NeurIPS), June 2020.\ndoi:\n10.1101/2020.06.16.154542.\nJoel Dapello, Kohitij Kar, Martin Schrimpf, Robert Geary, Michael Ferguson, David D. Cox, and\nJames J. DiCarlo. Aligning Model and Macaque Inferior Temporal Cortex Representations Im-\nproves Model-to-Human Behavioral Alignment and Adversarial Robustness. preprint, Neuro-\nscience, July 2022.\nURL http://biorxiv.org/lookup/doi/10.1101/2022.07.\n01.498495.\nIshita Dasgupta, Andrew K. Lampinen, Stephanie C. Y. Chan, Antonia Creswell, Dharshan Ku-\nmaran, James L. McClelland, and Felix Hill. Language models show human-like content effects\non reasoning, July 2022. URL http://arxiv.org/abs/2207.07051. arXiv:2207.07051\n[cs].\nDota Tianai Dong and Mariya Toneva.\nInterpreting multimodal video transformers using brain\nrecordings. In ICLR 2023 Workshop on Multimodal Representation Learning: Perks and Pitfalls,\n2023. URL https://openreview.net/forum?id=p-vL3rmYoqh.\nEghbal A. Hosseini, Martin Schrimpf, Yian Zhang, Samuel Bowman, Noga Zaslavsky, and Evelina\nFedorenko. Artificial neural network language models predict human brain responses to lan-\nguage even after a developmentally realistic amount of training. bioRxiv, pp. 2022.10.04.510681,\nJanuary 2023. doi: 10.1101/2022.10.04.510681. URL http://biorxiv.org/content/\nearly/2023/09/19/2022.10.04.510681.abstract.\nSusan F. Ehrlich and Keith Rayner. Contextual effects on word perception and eye movements\nduring reading.\nJournal of Verbal Learning and Verbal Behavior, 20(6):641\u2013655, December\n1981. ISSN 00225371. doi: 10.1016/S0022-5371(81)90220-6. URL https://linkinghub.\nelsevier.com/retrieve/pii/S0022537181902206.\nRichard Futrell, Edward Gibson, Harry J. Tily, Idan Blank, Anastasia Vishnevetsky, Steven Pianta-\ndosi, and Evelina Fedorenko. The natural stories corpus. In Proceedings of the Eleventh Interna-\ntional Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan, May\n2018. European Language Resources Association (ELRA). URL https://aclanthology.\norg/L18-1012.\nAriel Goldstein, Zaid Zada, Eliav Buchnik, Mariano Schain, Amy Price, Bobbi Aubrey, Samuel A.\nNastase, Amir Feder, Dotan Emanuel, Alon Cohen, Aren Jansen, Harshvardhan Gazula, Gina\nChoe, Aditi Rao, Catherine Kim, Colton Casto, Lora Fanda, Werner Doyle, Daniel Friedman,\nPatricia Dugan, Lucia Melloni, Roi Reichart, Sasha Devore, Adeen Flinker, Liat Hasenfratz, Omer\nLevy, Avinatan Hassidim, Michael Brenner, Yossi Matias, Kenneth A. Norman, Orrin Devinsky,\nand Uri Hasson. Shared computational principles for language processing in humans and deep\nlanguage models. Nature Neuroscience, 25(3):369\u2013380, March 2022. ISSN 1097-6256, 1546-\n1726. doi: 10.1038/s41593-022-01026-4. URL https://www.nature.com/articles/\ns41593-022-01026-4.\nJohn Hale. A probabilistic earley parser as a psycholinguistic model. In Second meeting of the North\nAmerican Chapter of the Association for Computational Linguistics on Language technologies\n2001 - NAACL \u201901, pp. 1\u20138, Pittsburgh, Pennsylvania, 2001. Association for Computational Lin-\nguistics. doi: 10.3115/1073336.1073357. URL http://portal.acm.org/citation.\ncfm?doid=1073336.1073357.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. Measuring Massive Multitask Language Understanding, January 2021. URL http:\n//arxiv.org/abs/2009.03300. arXiv:2009.03300 [cs].\nAnna A Ivanova, Shashank Srikant, Yotaro Sueoka, Hope H Kean, Riva Dhamala, Una-May\nO\u2019Reilly, Marina U Bers, and Evelina Fedorenko. Comprehension of computer code relies pri-\nmarily on domain-general executive brain regions. eLife, 9:e58906, December 2020. ISSN 2050-\n084X.\ndoi: 10.7554/eLife.58906.\nURL https://elifesciences.org/articles/\n58906.\n11\nPreprint.\nShailee Jain and Alexander G Huth. Incorporating Context into Language Encoding Models for\nfMRI. preprint, Neuroscience, May 2018. URL http://biorxiv.org/lookup/doi/\n10.1101/327601.\nGabriele Merlin and Mariya Toneva. Language models and brain alignment: beyond word-level\nsemantics and prediction, December 2022. URL http://arxiv.org/abs/2212.00596.\narXiv:2212.00596 [cs, q-bio].\nByung-Doh Oh and William Schuler. Why Does Surprisal From Larger Transformer-Based Lan-\nguage Models Provide a Poorer Fit to Human Reading Times?\nTransactions of the Asso-\nciation for Computational Linguistics, 11:336\u2013350, March 2023.\nISSN 2307-387X.\ndoi:\n10.1162/tacl a 00548. URL https://doi.org/10.1162/tacl_a_00548.\nByung-Doh Oh, Christian Clark, and William Schuler. Comparison of Structural Parsers and Neu-\nral Language Models as Surprisal Estimators.\nFrontiers in Artificial Intelligence, 5:777963,\nMarch 2022.\nISSN 2624-8212.\ndoi:\n10.3389/frai.2022.777963.\nURL https://www.\nfrontiersin.org/articles/10.3389/frai.2022.777963/full.\nSubba Reddy Oota, Jashn Arora, Veeral Agarwal, Mounika Marreddy, Manish Gupta, and Bapi\nSurampudi. Neural Language Taskonomy: Which NLP Tasks are the most Predictive of fMRI\nBrain Activity? In Proceedings of the 2022 Conference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Language Technologies, pp. 3220\u20133237, Seat-\ntle, United States, 2022. Association for Computational Linguistics.\ndoi: 10.18653/v1/2022.\nnaacl-main.235. URL https://aclanthology.org/2022.naacl-main.235.\nSubba Reddy Oota, Manish Gupta, Raju S. Bapi, Gael Jobard, Frederic Alexandre, and Xavier\nHinaut. Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding (Survey),\nJuly 2023. URL http://arxiv.org/abs/2307.10246. arXiv:2307.10246 [cs, q-bio].\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kel-\nton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano,\nJan Leike, and Ryan J. Lowe.\nTraining language models to follow instructions with human\nfeedback. ArXiv, abs/2203.02155, 2022. URL https://api.semanticscholar.org/\nCorpusID:246426909.\nFrancisco Pereira, Bin Lou, Brianna Pritchett, Samuel Ritter, Samuel J. Gershman, Nancy Kan-\nwisher, Matthew Botvinick, and Evelina Fedorenko. Toward a universal decoder of linguistic\nmeaning from brain activation. Nature Communications, 9(1):963, March 2018. ISSN 2041-\n1723. doi: 10.1038/s41467-018-03068-4. URL https://www.nature.com/articles/\ns41467-018-03068-4.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the Limits of Transfer Learning with a Uni-\nfied Text-to-Text Transformer, July 2020. URL http://arxiv.org/abs/1910.10683.\narXiv:1910.10683 [cs, stat].\nJ.K. Rowling, M. GrandPre, M. GrandPr\u00b4e, T. Taylor, Arthur A. Levine Books, and Scholastic\nInc. Harry Potter and the Sorcerer\u2019s Stone. Harry Potter. A.A. Levine Books, 1998. ISBN\n9780590353403. URL https://books.google.de/books?id=zXgTdQagLGkC.\nShahd Safarani, Arne Nix, Konstantin Willeke, Santiago Cadena, Kelli Restivo, George Denfield,\nAndreas Tolias, and Fabian Sinz. Towards robust vision by multi-task learning on monkey visual\ncortex. Advances in Neural Information Processing Systems, 34:739\u2013751, 2021.\nMustafa Safdari, Greg Serapio-Garc\u00b4\u0131a, Cl\u00b4ement Crepy, Stephen Fitz, Peter Romero, Luning Sun,\nMarwa Abdulhai, Aleksandra Faust, and Maja Matari\u00b4c. Personality Traits in Large Language\nModels, June 2023. URL http://arxiv.org/abs/2307.00184. arXiv:2307.00184 [cs].\nMaarten Sap, Vered Shwartz, Antoine Bosselut, Yejin Choi, and Dan Roth. Commonsense Rea-\nsoning for Natural Language Processing.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics: Tutorial Abstracts, pp. 27\u201333, Online, 2020. As-\nsociation for Computational Linguistics. doi: 10.18653/v1/2020.acl-tutorials.7. URL https:\n//www.aclweb.org/anthology/2020.acl-tutorials.7.\n12\nPreprint.\nMartin Schrimpf, Jonas Kubilius, Ha Hong, Najib J. Majaj, Rishi Rajalingham, Elias B. Issa, Ko-\nhitij Kar, Pouya Bashivan, Jonathan Prescott-Roy, Franziska Geiger, Kailyn Schmidt, Daniel\nL. K. Yamins, and James J. DiCarlo. Brain-Score: Which Artificial Neural Network for Ob-\nject Recognition is most Brain-Like?\npreprint, Neuroscience, September 2018. URL http:\n//biorxiv.org/lookup/doi/10.1101/407007.\nMartin Schrimpf, Jonas Kubilius, Michael J Lee, N. Apurva Ratan Murty, Robert Ajemian, and\nJames J. DiCarlo. Integrative Benchmarking to Advance Neurally Mechanistic Models of Human\nIntelligence. Neuron, 2020. ISSN 0896-6273. doi: 10.1016/j.neuron.2020.07.040.\nMartin Schrimpf, Idan Asher Blank, Greta Tuckute, Carina Kauf, Eghbal A. Hosseini, Nancy\nKanwisher, Joshua B. Tenenbaum, and Evelina Fedorenko.\nThe neural architecture of lan-\nguage: Integrative modeling converges on predictive processing. Proceedings of the National\nAcademy of Sciences, 118(45):e2105646118, November 2021.\nISSN 0027-8424, 1091-6490.\ndoi: 10.1073/pnas.2105646118. URL https://pnas.org/doi/full/10.1073/pnas.\n2105646118.\nDan Schwartz, Mariya Toneva, and Leila Wehbe.\nInducing brain-relevant bias in natural lan-\nguage processing models, October 2019. URL http://arxiv.org/abs/1911.03268.\narXiv:1911.03268 [cs, q-bio].\nNathaniel J. Smith and Roger Levy. The effect of word predictability on reading time is logarithmic.\nCognition, 128(3):302\u2013319, 2013. ISSN 0010-0277. doi: https://doi.org/10.1016/j.cognition.\n2013.02.013.\nURL https://www.sciencedirect.com/science/article/pii/\nS0010027713000413.\nMirac Suzgun, Nathan Scales, Nathanael Sch\u00a8arli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Challenging BIG-\nBench Tasks and Whether Chain-of-Thought Can Solve Them, October 2022.\nURL http:\n//arxiv.org/abs/2210.09261. arXiv:2210.09261 [cs].\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.\nhttps://github.com/tatsu-lab/stanford_alpaca, 2023.\nMariya Toneva and Leila Wehbe.\nInterpreting and improving natural-language processing (in\nmachines) with natural language-processing (in the brain), November 2019.\nURL http:\n//arxiv.org/abs/1905.11833. arXiv:1905.11833 [cs, q-bio].\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Ar-\nmand Joulin, Edouard Grave, and Guillaume Lample.\nLLaMA: Open and Efficient Founda-\ntion Language Models, February 2023.\nURL http://arxiv.org/abs/2302.13971.\narXiv:2302.13971 [cs].\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and\nHannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions,\n2022a.\nYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, An-\njana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, Eshaan\nPathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson,\nKirby Kuznia, Krima Doshi, Maitreya Patel, Kuntal Kumar Pal, Mehrad Moradshahi, Mihir Par-\nmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri,\nRushang Karia, Shailaja Keyur Sampat, Savan Doshi, Siddhartha Mishra, Sujan Reddy, Sumanta\nPatro, Tanay Dixit, Xudong Shen, Chitta Baral, Yejin Choi, Noah A. Smith, Hannaneh Ha-\njishirzi, and Daniel Khashabi. Super-NaturalInstructions: Generalization via Declarative Instruc-\ntions on 1600+ NLP Tasks, October 2022b. URL http://arxiv.org/abs/2204.07705.\narXiv:2204.07705 [cs].\nYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei,\nAtharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan\n13\nPreprint.\nPathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby\nKuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mi-\nrali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang\nKaria, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Pa-\ntro, Tanay Dixit, and Xudong Shen. Super-NaturalInstructions: Generalization via declarative\ninstructions on 1600+ NLP tasks. In Proceedings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pp. 5085\u20135109, Abu Dhabi, United Arab Emirates, Decem-\nber 2022c. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.340.\nURL https://aclanthology.org/2022.emnlp-main.340.\nLeila Wehbe, Brian Murphy, Partha Talukdar, Alona Fyshe, Aaditya Ramdas, and Tom Mitchell. Si-\nmultaneously Uncovering the Patterns of Brain Regions Involved in Different Story Reading Sub-\nprocesses. PLoS ONE, 9(11):e112575, November 2014. ISSN 1932-6203. doi: 10.1371/journal.\npone.0112575. URL https://dx.plos.org/10.1371/journal.pone.0112575.\nEthan Gotlieb Wilcox, Jon Gauthier, Jennifer Hu, Peng Qian, and Roger Levy. On the Predictive\nPower of Neural Language Models for Human Real-Time Comprehension Behavior, June 2020.\nURL http://arxiv.org/abs/2006.01912. arXiv:2006.01912 [cs].\nShengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi\nHu, Tianwei Zhang, Fei Wu, and Guoyin Wang. Instruction Tuning for Large Language Models:\nA Survey, August 2023. URL http://arxiv.org/abs/2308.10792. arXiv:2308.10792\n[cs].\n14\nPreprint.\nA\nLANGUAGE MODELS: PARAMETER COUNT AND NUMBER OF LAYERS\nTable 3: Parameter count and number of layers for all 25 vanilla and instruction-tuned LLMs.\nThe upper part contains encoder-decoder models of the T5 family, the lower parts decoder-only\nmodels of the LLaMA family. For the parameter count, \u201cM\u201d refers to million and \u201cB\u201d refers to\nbillion. The number of layers for T5 models is a sum of the number of encoder and decoder layers.\nModel\nParameter Count\nNumber of Layers\nt5-small\n77 M\n16\nflan-t5-small\n77 M\n16\nt5-base\n250 M\n24\nflan-t5-base\n250 M\n24\nflan-alpaca-base\n250 M\n24\nt5-large\n800 M\n48\nflan-t5-large\n800 M\n48\nflan-alpaca-large\n800 M\n48\nt5-xl\n3 B\n48\nflan-t5-xl\n3 B\n48\nflan-alpaca-xl\n3 B\n48\nflan-gpt4all-xl\n3 B\n48\nflan-sharegpt-xl\n3 B\n48\nflan-alpaca-gpt4-xl\n3 B\n48\nt5-xxl\n11 B\n48\nflan-t5-xxl\n11 B\n48\nflan-alpaca-xxl\n11 B\n48\nllama-7b\n7 B\n32\nalpaca-7b\n7 B\n32\nvicuna-7b\n7 B\n32\nllama-13b\n13 B\n40\nvicuna-13b\n13 B\n40\nstable-vicuna-13b\n13 B\n40\nllama-33b\n33 B\n60\nvicuna-33b\n33 B\n60\n15\nPreprint.\nB\nLANGUAGE MODELS: LINKS TO MODELS WEIGHTS\nTable 4: Link to model weights for all 25 vanilla and instruction-tuned LLMs. The upper\npart contains encoder-decoder models of the T5 family, the lower parts decoder-only models of the\nLLaMA family. We provide these links for reproducibility purposes.\nModel\nLink to model weights\nt5-small\nwww.huggingface.co/google/t5-v1_1-small\nflan-t5-small\nwww.huggingface.co/google/flan-t5-small\nt5-base\nwww.huggingface.co/google/t5-v1_1-base\nflan-t5-base\nwww.huggingface.co/google/flan-t5-base\nflan-alpaca-base\nwww.huggingface.co/declare-lab/flan-alpaca-base\nt5-large\nwww.huggingface.co/google/t5-v1_1-large\nflan-t5-large\nwww.huggingface.co/google/flan-t5-large\nflan-alpaca-large\nwww.huggingface.co/declare-lab/flan-alpaca-large\nt5-xl\nwww.huggingface.co/google/t5-v1_1-xl\nflan-t5-xl\nwww.huggingface.co/google/flan-t5-xl\nflan-alpaca-xl\nwww.huggingface.co/declare-lab/flan-alpaca-xl\nflan-gpt4all-xl\nwww.huggingface.co/declare-lab/flan-gpt4all-xl\nflan-sharegpt-xl\nwww.huggingface.co/declare-lab/flan-sharegpt-xl\nflan-alpaca-gpt4-xl\nwww.huggingface.co/declare-lab/flan-alpaca-gpt4-xl\nt5-xxl\nwww.huggingface.co/google/t5-v1_1-xxl\nflan-t5-xxl\nwww.huggingface.co/google/flan-t5-xxl\nflan-alpaca-xxl\nwww.huggingface.co/declare-lab/flan-alpaca-xxl\nllama-7b\nwww.github.com/facebookresearch/llama\nalpaca-7b\nwww.github.com/tatsu-lab/stanford_alpaca\nvicuna-7b\nwww.huggingface.co/lmsys/vicuna-7b-v1.3\nllama-13b\nwww.github.com/facebookresearch/llama\nvicuna-13b\nwww.huggingface.co/lmsys/vicuna-13b-v1.3\nstable-vicuna-13b\nwww.huggingface.co/CarperAI/stable-vicuna-13b-delta\nllama-33b\nwww.github.com/facebookresearch/llama\nvicuna-33b\nwww.huggingface.co/lmsys/vicuna-33b-v1.3\n16\nPreprint.\nC\nMMLU AND BBH BENCHMARKS\nMMLU is designed to measure knowledge from many domains Hendrycks et al. (2021). It contains\n57 tasks, categorized by the subject domain of world knowledge tested: STEM, Humanities, So-\ncial Sciences, and Others. The STEM category includes questions on computer science, physics,\nmathematics, etc. The Humanities category includes questions on philosophy, law, history, etc. The\nSocial Sciences category includes questions on politics, sociology, economics, geography, etc. The\nOthers category includes questions on business topics such as finance, accounting, as well as general\nknowledge of global facts.\nBBH is designed to evaluate various problem-solving and reasoning abilities of LLMs (Suzgun\net al., 2022). BBH contains 23 tasks, categorized by the type of problem-solving ability tested:\n(1) Algorithmic and Multi-Step Arithmetic Reasoning, (2) Natural Language Understanding, (3)\nUse of World Knowledge, and (4) Multilingual Knowledge and Reasoning. The world knowledge\ncategory of BBH contains tasks that test for factual and general knowledge. Tasks requiring fac-\ntual knowledge include: \u201cSports Understanding\u201d and \u201cMovie Recommendation\u201d. Tasks requiring\ngeneral knowledge include: \u201cCausal Judgement\u201d, which tests knowledge about causal-reasoning\nsuppositions, and \u201cRuin Names\u201d, which requires knowledge about human perception and usage of\nhumor in the English language.\nFor both benchmarks, we adopt the same category classification as used in their original papers. We\nmeasure the performance of LLMs on BBH and MMLU using the instruct-eval repository3\nwith default settings (3-shots, 5-shots respectively) and preset prompts.\nD\nCODE REPOSITORIES\nWe use the Brain-Score repository to evaluate brain alignment for the PEREIRA2018 and\nBLANK2014 datasets, as well as behavioral alignment for the FUTRELL2018 dataset.\nLink:\nwww.github.com/brain-score/language.\nWe use an open-source repository to evaluate brain alignment for the WEHBE2014 dataset.\nLink: www.github.com/awwkl/brain_language_summarization, which builds on\nwww.github.com/mtoneva/brain_language_nlp.\nWe use Instruct-Eval repository to evaluate MMLU and BBH scores. Link: www.github.com/\ndeclare-lab/instruct-eval.\nWe use Stanford Alpaca repository for instruction-tuning.\nLink:\nwww.github.com/\ntatsu-lab/stanford_alpaca).\n3https://github.com/declare-lab/instruct-eval\n17\nPreprint.\nE\nRESULTS FOR BRAIN ALIGNMENT\nTable 5: Brain alignment results for all 25 vanilla and instruction-tuned LLMs. We provide\nthese results for reproducibility purposes.\nPEREIRA2018\nBLANK2014\nWEHBE2014\nAverage\nt5-small\n0.166\n0.168\n0.071\n0.135\nflan-t5-small\n0.202\n0.178\n0.079\n0.153\nt5-base\n0.222\n0.188\n0.074\n0.162\nflan-t5-base\n0.234\n0.178\n0.076\n0.163\nflan-alpaca-base\n0.227\n0.179\n0.076\n0.161\nt5-large\n0.270\n0.082\n0.071\n0.141\nflan-t5-large\n0.311\n0.104\n0.080\n0.165\nflan-alpaca-large\n0.322\n0.126\n0.082\n0.177\nt5-xl\n0.285\n0.192\n0.072\n0.183\nflan-t5-xl\n0.314\n0.215\n0.072\n0.200\nflan-alpaca-xl\n0.312\n0.209\n0.075\n0.199\nflan-gpt4all-xl\n0.300\n0.206\n0.078\n0.195\nflan-sharegpt-xl\n0.323\n0.211\n0.070\n0.201\nflan-alpaca-gpt4-xl\n0.302\n0.205\n0.073\n0.193\nt5-xxl\n0.343\n0.297\n0.096\n0.246\nflan-t5-xxl\n0.350\n0.268\n0.103\n0.240\nflan-alpaca-xxl\n0.346\n0.268\n0.102\n0.239\nllama-7b\n0.405\n0.154\n0.118\n0.226\nalpaca-7b\n0.420\n0.167\n0.118\n0.235\nvicuna-7b\n0.399\n0.152\n0.119\n0.223\nllama-13b\n0.412\n0.133\n0.115\n0.220\nvicuna-13b\n0.423\n0.148\n0.116\n0.229\nstable-vicuna-13b\n0.415\n0.144\n0.115\n0.225\nllama-33b\n0.426\n0.145\n0.109\n0.227\nvicuna-33b\n0.436\n0.156\n0.105\n0.232\nTable 6: Noise ceiling estimates for all 3 neural datasets. For PEREIRA2018 and BLANK2014,\nnoise ceiling estimates are computed using the Brain-Score repository, with details provided in\nSchrimpf et al. (2021). For WEHBE2014, noise ceiling estimates are also computed using a similar\nprocedure.\nPEREIRA2018\nBLANK2014\nWEHBE2014\nAverage\nNoise ceiling\n0.359\n0.210\n0.104\n0.224\n18\nPreprint.\nF\nRESULTS FOR NEXT-WORD PREDICTION, MMLU, BBH\nTable 7:\nWikiText-2 NWP loss, MMLU Overall Score, and BBH Overall Score for all\ninstruction-tuned LLMs. Results for vanilla LLMs are not shown as they are not adapted for the\nquestion formats in the MMLU and BBH benchmarks. We provide these results for reproducibility\npurposes.\nWikiText-2 NWP Loss\nMMLU Overall Score\nBBH Overall Score\nflan-t5-small\n0.851\n0.294\n0.287\nflan-t5-base\n1.235\n0.341\n0.308\nflan-alpaca-base\n1.074\n0.304\n0.266\nflan-t5-large\n0.625\n0.419\n0.370\nflan-alpaca-large\n0.648\n0.397\n0.276\nflan-t5-xl\n0.650\n0.493\n0.402\nflan-alpaca-xl\n0.604\n0.466\n0.270\nflan-gpt4all-xl\n0.625\n0.337\n0.212\nflan-sharegpt-xl\n0.664\n0.446\n0.363\nflan-alpaca-gpt4-xl\n0.593\n0.456\n0.348\nflan-t5-xxl\n0.638\n0.545\n0.443\nflan-alpaca-xxl\n0.607\n0.508\n0.229\nalpaca-7b\n4.201\n0.404\n0.328\nvicuna-7b\n4.387\n0.472\n0.331\nvicuna-13b\n4.130\n0.521\n0.387\nstable-vicuna-13b\n4.623\n0.495\n0.380\nvicuna-33b\n3.940\n0.590\n0.426\nNotes on comparing next-word prediction (NWP) loss across model families.\nThe T5 and\nLLaMA models belong to separate model families. We wish to caution that comparing next-word\nprediction loss across different model families may not be meaningful. This is due to several rea-\nsons related to architectural differences, training methodologies, and objectives. (1) Architecture:\nT5 models have an encoder-decoder architecture while LLaMA models have a decoder-only archi-\ntecture. (2) Training Objectives: The T5 models were trained on supervised and unsupervised tasks,\nwhile the LLaMA models were trained only on unsupervised text 3. (3) Loss computation: The loss\nfunctions for both model families are computed differently, making it inappropriate to directly com-\npare their loss values. (4) Evaluation Metrics: Next-word prediction loss is just one metric, and it\nmay not capture the overall language understanding capabilities of a model. Hence, we additionally\nevaluate these LLMs\u2019 alignment to human brain activity, as well as their performance on problem-\nsolving abilities (BBH) and tasks requiring world knowledge (MMLU). In summary, while NWP\nloss is a valuable metric for evaluating language models within the same family or architecture,\ncomparing across different model families may not be meaningful.\n19\nPreprint.\nG\nRESULTS FOR CORRELATIONS OF BRAIN ALIGNMENT WITH LLM\nPROPERTIES\n5\n10\n15\nNWP loss\n0.14\n0.16\n0.18\n0.20\n0.22\n0.24\nBrain alignment (r)\nflan-t5-small\nflan-t5-base\nflan-alpaca-base\nflan-t5-large\nflan-alpaca-large\nflan-t5-xl\nflan-alpaca-xl\nflan-gpt4all-xl\nflan-sharegpt-xl\nflan-alpaca-gpt4-xl\nflan-t5-xxl\nflan-alpaca-xxl\nalpaca-7b\nvicuna-7b\nvicuna-13b\nstable-vicuna-13b\nvicuna-33b\nT5\nLlama\nr = -0.54, p = 2.68e-02\n9.0\n9.5\n10.0\n10.5\n11.0\n11.5\nlog10(# model parameters)\n0.14\n0.16\n0.18\n0.20\n0.22\n0.24\nBrain alignment (r)\nflan-t5-small\nflan-t5-base\nflan-alpaca-base\nflan-t5-large\nflan-alpaca-large\nflan-t5-xl\nflan-alpaca-xl\nflan-gpt4all-xl\nflan-sharegpt-xl\nflan-alpaca-gpt4-xl\nflan-t5-xxl\nflan-alpaca-xxl\nalpaca-7b\nvicuna-7b\nvicuna-13b\nstable-vicuna-13b\nvicuna-33b\nT5\nLlama\nr = 0.95, p = 9.43e-09\nC\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPearson correlation\n Pereira2018\n Blank2014\n Wehbe2014\n******\n**\nn.s.\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\nBBH world knowledge\n0.14\n0.16\n0.18\n0.20\n0.22\n0.24\nBrain alignment (r)\nflan-t5-small\nflan-t5-base\nflan-alpaca-base\nflan-t5-large\nflan-alpaca-large\nflan-t5-xl\nflan-alpaca-xl\nflan-gpt4all-xl\nflan-sharegpt-xl\nflan-alpaca-gpt4-xl\nflan-t5-xxl\nflan-alpaca-xxl\nalpaca-7b\nvicuna-7b\nvicuna-13b\nstable-vicuna-13b\nvicuna-33b\nT5\nLlama\nr = 0.68, p = 2.72e-03\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\nMMLU Overall score\n0.14\n0.16\n0.18\n0.20\n0.22\n0.24\nBrain alignment (r)\nflan-t5-small\nflan-t5-base\nflan-alpaca-base\nflan-t5-large\nflan-alpaca-large\nflan-t5-xl\nflan-alpaca-xl\nflan-gpt4all-xl\nflan-sharegpt-xl\nflan-alpaca-gpt4-xl\nflan-t5-xxl\nflan-alpaca-xxl\nalpaca-7b\nvicuna-7b\nvicuna-13b\nstable-vicuna-13b\nvicuna-33b\nT5\nLlama\nr = 0.81, p = 8.52e-05\nA\nB\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPearson correlation\n Pereira2018\n Blank2014\n Wehbe2014\n***\n*\nn.s.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPearson correlation\n Pereira2018\n Blank2014\n Wehbe2014\n**\n*\nn.s.\nBrain alignment\nLM properties\nStatistical significance: p > 0.05 (n.s.), p < 0.05 (*), p < 0.005 (**), p < 0.0005 (***), etc.\nD\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\nPearson correlation\n Pereira2018\n Blank2014\n Wehbe2014\nn.s.\nn.s.\n*\nFigure 4: Correlation between brain alignment and various LLM properties: (A) MMLU\nbenchmark global score, (B) BBH benchmark score with only world knowledge tasks, (C) num-\nber of parameters of the model, and (D) Next word prediction (NWP) performance.\nA\nB\nBrain alignment\nLM properties\nStatistical significance: p > 0.05 (n.s.), p < 0.05 (*), p < 0.005 (**), p < 0.0005 (***), etc.\n9.0\n9.5\n10.0\n10.5\n11.0\n11.5\nlog10(# model parameters)\n0.12\n0.14\n0.16\n0.18\n0.20\n0.22\n0.24\n0.26\nBrain alignment (r)\nt5-small\nflan-t5-small\nt5-base\nflan-t5-base\nflan-alpaca-base\nt5-large\nflan-t5-large\nflan-alpaca-large\nt5-xl\nflan-t5-xl\nflan-alpaca-xl\nflan-gpt4all-xl\nflan-sharegpt-xl\nflan-alpaca-gpt4-xl\nt5-xxl\nflan-t5-xxl\nflan-alpaca-xxl\nllama-7b\nalpaca-7b\nvicuna-7b\nllama-13b\nvicuna-13b\nstable-vicuna-13b\nllama-33b\nvicuna-33b\nT5\nLlama\nr = 0.92, p = 7.19e-11\n5\n10\n15\nNWP loss\n0.12\n0.14\n0.16\n0.18\n0.20\n0.22\n0.24\n0.26\nBrain alignment (r)\nt5-small\nflan-t5-small\nt5-base\nflan-t5-base\nflan-alpaca-base\nt5-large\nflan-t5-large\nflan-alpaca-large\nt5-xl\nflan-t5-xl\nflan-alpaca-xl\nflan-gpt4all-xl\nflan-sharegpt-xl\nflan-alpaca-gpt4-xl\nt5-xxl\nflan-t5-xxl\nflan-alpaca-xxl\nllama-7b\nalpaca-7b\nvicuna-7b\nllama-13b\nvicuna-13b\nstable-vicuna-13b\nllama-33b\nvicuna-33b\nT5\nLlama\nr = -0.54, p = 4.94e-03\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\nPearson correlation\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPearson correlation\n Pereira2018\n Blank2014\n Wehbe2014\n*********\n**\nn.s.\n Pereira2018\n Blank2014\n Wehbe2014\nn.s.\nn.s.\n**\nFigure 5: Correlation between brain alignment and various LLM properties for all 25 LLMs:\n(A) number of parameters of the model, and (B) Next word prediction (NWP) performance.\n20\nPreprint.\nH\nRESULTS FOR BEHAVIORAL ALIGNMENT\nTable 8: Behavioral alignment results for all 25 vanilla and instruction-tuned LLMs. We pro-\nvide these results for reproducibility purposes.\nFUTRELL2018\nt5-small\n0.229\nflan-t5-small\n0.054\nt5-base\n0.333\nflan-t5-base\n0.152\nflan-alpaca-base\n0.290\nt5-large\n0.303\nflan-t5-large\n0.145\nflan-alpaca-large\n0.291\nt5-xl\n0.225\nflan-t5-xl\n0.113\nflan-alpaca-xl\n0.181\nflan-gpt4all-xl\n0.251\nflan-sharegpt-xl\n0.285\nflan-alpaca-gpt4-xl\n0.250\nt5-xxl\n0.260\nflan-t5-xxl\n0.274\nflan-alpaca-xxl\n0.267\nllama-7b\n0.204\nalpaca-7b\n0.206\nvicuna-7b\n0.205\nllama-13b\n0.184\nvicuna-13b\n0.184\nstable-vicuna-13b\n0.196\nllama-33b\n0.158\nvicuna-33b\n0.164\nTable 9: Noise ceiling estimates for the FUTRELL2018 reading-times dataset. Noise ceiling\nestimates are computed using the Brain-Score repository, with details provided in Schrimpf et al.\n(2021).\nFUTRELL2018\nNoise ceiling\n0.858\n21\nPreprint.\nI\nRESULTS FOR INSTRUCTION-TUNING LLAMA-7B ON ALPACA DATASET\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n# Instruction-tuning epochs\n0.225\n0.230\n0.235\nBrain alignment (r)\nInstruction\nNo instruction\nA\nB\nC\nBrain alignment\nLM properties\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n# Instruction-tuning epochs\n0.29\n0.30\n0.31\n0.32\n0.33\n0.34\nBBH Overall score\nInstruction\nNo instruction\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n# Instruction-tuning epochs\n0.26\n0.28\n0.30\n0.32\n0.34\n0.36\n0.38\nMMLU Overall score\nInstruction\nNo instruction\nFigure 6: Improvements in brain alignment from instruction-tuning are due to both additional\ntraining data, as well as training to understand and follow instructions.\nInstruction model\nWe instruction-tune LLaMA-7B on the Stanford Alpaca dataset (Taori et al.,\n2023) using the default training process, following the code in www.github.com/tatsu-lab/\nstanford_alpaca. In particular, the model is instruction-tuned using 52K instruction-following\nexamples generated through methods inspired by Self-Instruct (Wang et al., 2022a)). This model is\nlabeled \u201cInstruction\u201d in Figure 6.\nNo instruction model (Ablation)\nWe also train an ablation model with the same process and\ntraining data as the default instruction-tuning, but remove the instruction portion from each training\nsample. This ablation model is labeled \u201cNo instruction\u201d in Figure 6. This ablation experiment\ndisentangles: (1) training data (present in both Instruction and No instruction), from (2) training\nLLMs to understand and follow instructions (present only in Instruction).\nWe use all provided training samples from the Alpaca dataset, thus ensuring that the models are\ntrained on the same amount of data. We observe that brain alignment of the \u201cNo Instruction\u201d ablation\nmodel increases during fine-tuning but stays lower than its \u201cInstruction\u201d counterpart.\n22\n"
  },
  {
    "title": "Scaffold-GS: Structured 3D Gaussians for View-Adaptive Rendering",
    "link": "https://arxiv.org/pdf/2312.00109.pdf",
    "upvote": "9",
    "text": "Scaffold-GS: Structured 3D Gaussians for View-Adaptive Rendering\nTao Lu 1,3*\nMulin Yu1*\nLinning Xu2\nYuanbo Xiangli4\nLimin Wang1,3\nDahua Lin1,2\nBo Dai1\n1Shanghai Artificial Intelligence Laboratory, 2The Chinese University of Hong Kong,\n3Nanjing University, 4Cornell University\nPSNR: 28.04dB \nPSNR: 28.57dB\nGT\n(3D-GS) 17.16dB / 242MB / 127FPS\n(Ours) 20.41 dB / 66MB / 110FPS\n(3D-GS) 34.60dB / 204MB / 113 FPS\n(Ours) 35.41dB / 48MB / 88 FPS\n(3D-GS) 29.93 dB / 288MB / 109 FPS\n(Ours) 31.13 dB / 133MB / 128FPS\n3D-GS RGB\nOurs RGB\nInitial points\n3D Gaussians\n (3D-GS, Ours-anchor)\nScaffold-GS\n3D-GS Opacity\nOurs Opacity\nFigure 1. Scaffold-GS represents the scene using a set of 3D Gaussians structured in a dual-layered hierarchy. Anchored on a sparse\ngrid of initial points, a modest set of neural Gaussians are spawned from each anchor to dynamically adapt to various viewing angles\nand distances. Our method achieves rendering quality and speed comparable to 3D-GS with a more compact model (last row metrics:\nPSNR/storage size/FPS). Across multiple datasets, Scaffold-GS demonstrates more robustness in large outdoor scenes and intricate indoor\nenvironments with challenging observing views e.g. transparency, specularity, reflection, texture-less regions and fine-scale details.\nAbstract\nNeural rendering methods have significantly advanced\nphoto-realistic 3D scene rendering in various academic and\nindustrial applications. The recent 3D Gaussian Splatting\nmethod has achieved the state-of-the-art rendering quality\nand speed combining the benefits of both primitive-based\nrepresentations and volumetric representations. However,\nit often leads to heavily redundant Gaussians that try to\nfit every training view, neglecting the underlying scene ge-\nometry.\nConsequently, the resulting model becomes less\nrobust to significant view changes, texture-less area and\nlighting effects. We introduce Scaffold-GS, which uses an-\nchor points to distribute local 3D Gaussians, and predicts\ntheir attributes on-the-fly based on viewing direction and\ndistance within the view frustum.\nAnchor growing and\npruning strategies are developed based on the importance\nof neural Gaussians to reliably improve the scene cover-\nage. We show that our method effectively reduces redun-\ndant Gaussians while delivering high-quality rendering. We\nalso demonstrates an enhanced capability to accommodate\nscenes with varying levels-of-detail and view-dependent ob-\n* denotes equal contribution.\nservations, without sacrificing the rendering speed. Project\npage: https://city-super.github.io/scaffold-gs/.\n1. Introduction\nPhoto-realistic and real-time rendering of 3D scenes has al-\nways been a pivotal interest in both academic research and\nindustrial domains, with applications spanning virtual real-\nity [51], media generation [36], and large-scale scene vi-\nsualization [43, 45, 49]. Traditional primitive-based repre-\nsentations like meshes and points [6, 26, 32, 55] are faster\ndue to the use of rasterization techniques tailored for mod-\nern GPUs. However, they often yield low-quality render-\nings, exhibiting discontinuity and blurry artifacts. In con-\ntrast, volumetric representations and neural radiance fields\nutilize learning-based parametric models [3, 5, 30], hence\ncan produce continuous rendering results with more details\npreserved. Nevertheless, they come with the cost of time-\nconsuming stochastic sampling, leading to slower perfor-\nmance and potential noise.\nIn recent times, 3D Gaussian Splatting (3D-GS) [22] has\n1\narXiv:2312.00109v1  [cs.CV]  30 Nov 2023\nachieved state-of-the-art rendering quality and speed. Ini-\ntialized from point clouds derived from Structure from Mo-\ntion (SfM) [42], this method optimizes a set of 3D Gaus-\nsians to represent the scene. It preserves the inherent con-\ntinuity found in volumetric representations, whilst facilitat-\ning rapid rasterization by splatting 3D Gaussians onto 2D\nimage planes.\nWhile this approach offers several advantages, it tends\nto excessively expand Gaussian balls to accommodate ev-\nery training view, thereby neglecting scene structure. This\nresults in significant redundancy and limits its scalability,\nparticularly in the context of complex large-scale scenes.\nFurthermore, view-dependent effects are baked into indi-\nvidual Gaussian parameters with little interpolation capa-\nbilities, making it less robust to substantial view changes\nand lighting effects.\nWe present Scaffold-GS, a Gaussian-based approach that\nutilizes anchor points to establish a hierarchical and region-\naware 3D scene representation. We construct a sparse grid\nof anchor points initiated from SfM points. Each of these\nanchors tethers a set of neural Gaussians with learnable off-\nsets, whose attributes (i.e. opacity, color, rotation, scale) are\ndynamically predicted based on the anchor feature and the\nviewing position. Unlike the vanilla 3D-GS which allows\n3D Gaussians to freely drift and split, our strategy exploits\nscene structure to guide and constrain the distribution of 3D\nGaussians, whilst allowing them to locally adaptive to vary-\ning viewing angles and distances. We further develop the\ncorresponding growing and pruning operations for anchors\nto enhance the scene coverage.\nThrough extensive experiments, we show that our\nmethod delivers rendering quality on par with or even sur-\npassing the original 3D-GS. At inference time, we limit\nthe prediction of neural Gaussians to anchors within the\nview frustum, and filter out trivial neural Gaussians based\non their opacity with a filtering step (i.e. learnable selec-\ntor). As a result, our approach can render at a similar speed\n(around 100 FPS at 1K resolution) as the original 3D-GS\nwith little computational overhead. Moreover, our storage\nrequirements are significantly reduced as we only need to\nstore anchor points and MLP predictors for each scene.\nIn conclusion, our contributions are: 1) Leveraging scene\nstructure, we initiate anchor points from a sparse voxel\ngrid to guide the distribution of local 3D Gaussians, form-\ning a hierarchical and region-aware scene representation; 2)\nWithin the view frustum, we predict neural Gaussians from\neach anchor on-the-fly to accommodate diverse viewing di-\nrections and distances, resulting in more robust novel view\nsynthesis; 3) We develop a more reliable anchor growing\nand pruning strategy utilizing the predicted neural Gaus-\nsians for better scene coverage.\n2. Related work\nMLP-based Neural Fields and Rendering.\nEarly neu-\nral fields typically adopt a multi-layer perceptron (MLP) as\nthe global approximator of 3D scene geometry and appear-\nance. They directly use spatial coordinates (and viewing\ndirection) as input to the MLP and predict point-wise at-\ntribute, e.g. signed distance to scene surface (SDF) [33, 34,\n46, 54], or density and color of that point [2, 30, 49]. Be-\ncause of its volumetric nature and inductive bias of MLPs,\nthis stream of methods achieves the SOTA performance in\nnovel view synthesis. The major challenge of this scene rep-\nresentation is that the MLP need to be evaluated on a large\nnumber of sampled points along each camera ray. Con-\nsequently, rendering becomes extremely slow, with limited\nscalability towards complex and large-scale scenes. Despite\nseveral works have been proposed to accelerate or mitigate\nthe intensive volumetric ray-marching, e.g. using proposal\nnetwork [4], baking technique [11, 19], and surface render-\ning [41]. They either incorporated more MLPs or traded\nrendering quality for speed.\nGrid-based Neural Fields and Rendering.\nThis type of\nscene representations are usually based on a dense uniform\ngrid of voxels. They have been greatly used in 3D shape\nand geometry modeling [12, 15, 21, 29, 35, 44, 57]. Some\nrecent methods have also focused on faster training and in-\nference of radiance field by exploiting spatial data struc-\nture to store scene features, which were interpolated and\nqueried by sampled points during ray-marching. For in-\nstance, Plenoxel [13] adopted a sparse voxel grid to inter-\npolate a continuous density field, and represented view-\ndependent visual effects with Spherical Harmonics. The\nidea of tensor factorization has been studied in multiple\nworks [9, 10, 50, 52] to further reduce data redundancy and\nspeed-up rendering. K-planes [14] used neural planes to\nparameterize a 3D scene, optionally with an additional tem-\nporal plane to accommodate dynamics. Several generative\nworks [8, 40] also capitalized on triplane structure to model\na 3D latent space for better geometry consistency. Instant-\nNGP [31] used a hash grid and achieved drastically faster\nfeature query, enabling real-time rendering of neural radi-\nance field. Although these approaches can produce high-\nquality results and are more efficient than global MLP rep-\nresentation, they still need to query many samples to render\na pixel, and struggle to represent empty space effectively.\nPoint-based Neural Fields and Rendering.\nPoint-based\nrepresentations utilize the geometric primitive (i.e. point\nclouds) for scene rendering. A typical procedure is to ras-\nterize an unstructured set of points using a fixed size, and\nexploits specialized modules on GPU and graphics APIs for\nrendering [7, 37, 38]. In spite of its fast speed and flexibil-\n2\nity to solve topological changes, they usually suffer from\nholes and outliers that lead to artifacts in rendering. To alle-\nviate the discontinuity issue, differentiable point-based ren-\ndering has been extensively studied to model objects geom-\netry [16, 20, 27, 48, 55]. In particular, [48, 55] used dif-\nferentiable surface splatting that treats point primitives as\ndiscs, ellipsoids or surfels that are larger than a pixel. [1, 24]\naugmented points with neural features and rendered using\n2D CNNs. As a comparison, Point-NeRF [53] achieved\nhigh-quality novel view synthesis utilizing 3D volume ren-\ndering, along with region growing and point pruning dur-\ning optimization. However, they resorted to volumetric ray-\nmarching, hence hindered display rate. Notably, the recent\nwork 3D-GS [22] employed anisotropic 3D Gaussians ini-\ntialized from structure from motion (SfM) to represent 3D\nscenes, where a 3D Gaussian was optimized as a volume\nand projected to 2D to be rasterized as a primitive. Since\nit integrated pixel color using \u03b1-blender, 3D-GS produced\nhigh-quality results with fine-scale detail, and rendered at\nreal-time frame rate.\n3. Methods\nThe original 3D-GS [22] optimizes Gaussians to reconstruct\nevery training view, with heuristic splitting and pruning op-\nerations but in general neglects the underlying scene struc-\nture. This often leads to highly redundant Gaussians and\nmakes the model less robust to novel viewing angles and\ndistances. To address this issue, we propose a hierarchical\n3D Gaussian scene representation that respects the scene\ngeometric structure, with anchor points initialized from\nSfM to encode local scene information and spawn local neu-\nral Gaussians. The physical properties of neural Gaussians\nare decoded from the learned anchor features in a view-\ndependent manner on-the-fly. Fig. 2 illustrates our frame-\nwork. We start with a brief background of 3D-GS then un-\nfold our proposed method in details. Sec. 3.2.1 introduces\nhow to initialize the scene with a regular sparse grid of an-\nchor points from the irregular SfM point clouds. Sec. 3.2.2\nexplains how we predict neural Gaussians properties based\non anchor points and view-dependent information. To make\nour method more robust to the noisy initialization, Sec. 3.3\nintroduces a neural Gaussian based \u201cgrowing\u201d and \u201cprun-\ning\u201d operations to refine the anchor points. Sec. 3.4 elabo-\nrates training details.\n3.1. Preliminaries\n3D-GS [22] represents the scene with a set of anisotropic\n3D Gaussians that inherit the differential properties of vol-\numetric representation while be efficiently rendered via a\ntile-based rasterization.\nStarting from a set of Structure-from-Motion (SfM)\npoints, each point is designated as the position (mean) \u00b5\nof a 3D Gaussian:\nG(x) = e\u2212 1\n2 (x\u2212\u00b5)T \u03a3\u22121(x\u2212\u00b5),\n(1)\nwhere x is an arbitrary position within the 3D scene and\n\u03a3 denotes the covariance matrix of the 3D Gaussian. \u03a3 is\nformulated using a scaling matrix S and rotation matrix R\nto maintain its positive semi-definite:\n\u03a3 = RSST RT ,\n(2)\nIn addition to color c modeled by Spherical harmonics, each\n3D Gaussian is associated with an opacity \u03b1 which is mul-\ntiplied by G(x) during the blending process.\nDistinct from conventional volumetric representations,\n3D-GS efficiently renders the scene via tile-based rasteri-\nzation instead of resource-intensive ray-marching. The 3D\nGaussian G(x) are first transformed to 2D Gaussians G\u2032(x)\non the image plane following the projection process as de-\nscribed in [58]. Then a tile-based rasterizer is designed to\nefficiently sort the 2D Gaussians and employ \u03b1-blending:\nC(x\u2032) =\nX\ni\u2208N\nci\u03c3i\ni\u22121\nY\nj=1\n(1 \u2212 \u03c3j),\n\u03c3i = \u03b1iG\u2032\ni(x\u2032),\n(3)\nwhere x\u2032 is the queried pixel position and N denotes the\nnumber of sorted 2D Gaussians associated with the queried\npixel. Leveraging the differentiable rasterizer, all attributes\nof the 3D Gaussians are learnable and directly optimized\nend-to-end via training view reconstruction.\n3.2. Scaffold-GS\n3.2.1\nAnchor Point Initialization\nConsistent with existing methods [22, 53], we use the sparse\npoint cloud from COLMAP [39] as our initial input. We\nthen voxelize the scene from this point cloud P \u2208 RM\u00d73\nas:\nV =\n\u001a\u0016P\n\u03f5\n\u0019\u001b\n\u00b7 \u03f5,\n(4)\nwhere V \u2208 RN\u00d73 denotes voxel centers, and \u03f5 is the voxel\nsize. We then remove duplicate entries, denoted by {\u00b7} to\nreduce the redundancy and irregularity in P.\nThe center of each voxel v \u2208 V is treated as an anchor\npoint, equipped with a local context feature fv \u2208 R32, a\nscaling factor lv \u2208 R3, and k learnable offsets Ov \u2208 Rk\u00d73.\nIn a slight abuse of terminology, we will denote the anchor\npoint as v in the following context. We further enhance fv\nto be multi-resolution and view-dependent. For each anchor\nv, we 1) create a features bank {fv, fv\u21931 , fv\u21932 }, where \u2193n\ndenotes fv being down-sampled by 2n factors; 2) blend the\nfeature bank with view-dependent weights to form an inte-\ngrated anchor feature \u02c6fv. Specifically, Given a camera at\n3\n(a) Sparse Voxel from SfM Points\n(b) Neural Gaussian Derivation (k=4)\n(c) Neural Gaussian Splatting\n & \ud835\udfaa-blending\nRendered RGB\nGT\nL1, LSSIM, Lvol\nEach Voxel\nS(fa)k\nF\ud835\udfaa -> opac.\nFc -> rgb \nFs  -> scale\nFq -> quatrn.\nVisible Voxels\nPosition & Opacity\nColor, Scale & Quaternion\nanchor\nvoxel \nneural Gaussian\nlearnable offset\nO1\nO2\nO3\nO4\n\ud835\udefc < \ud835\udf0f\ud835\udefc\nFigure 2. Overview of Scaffold-GS. (a) We start by forming a sparse voxel grid from SfM-derived points. An anchor associated with\na learnable scale is placed at the center of each voxel, roughly sculpturing the scene occupancy. (b) Within a view frustum, k neural\nGaussians are spawned from each visible anchor with offsets {Ok}. Their attributes, i.e. opacity, color, scale and quaternion are then\ndecoded from the anchor feature, relative camera-anchor viewing direction and distance using F\u03b1, Fc, Fs, Fq respectively. (c) Note that\nto alleviate redundancy and improve efficiency, only non-trivial neural Gussians (i.e. \u03b1 \u2265 \u03c4\u03b1) are rasterized following [22]. The rendered\nimage is supervised via reconstruction (L1), structural similarity (LSSIM) and a volume regularization (Lvol).\nposition xc and an anchor at position xv, we calculate their\nrelative distance and viewing direction with:\n\u03b4vc = \u2225xv \u2212 xc\u22252, \u20d7dvc =\nxv \u2212 xc\n\u2225xv \u2212 xc\u22252\n,\n(5)\nthen weighted sum the feature bank with weights predicted\nfrom a tiny MLP Fw:\n{w, w1, w2} = Softmax(Fw(\u03b4vc, \u20d7dvc)),\n(6)\n\u02c6fv = w \u00b7 fv + w1 \u00b7 fv\u21931 + w2 \u00b7 fv\u21932 ,\n(7)\n3.2.2\nNeural Gaussian Derivation\nIn this section, we elaborate on how our approach derives\nneural Gaussians from anchor points. Unless specified oth-\nerwise, F\u2217 represents a particular MLP throughout the sec-\ntion.\nMoreover, we introduce two efficient pre-filtering\nstrategies to reduce MLP overhead.\nWe parameterize a neural Gaussian with its position\n\u00b5 \u2208 R3, opacity \u03b1 \u2208 R, covariance-related quaternion\nq \u2208 R4 and scaling s \u2208 R3, and color c \u2208 R3. As shown\nin Fig. 2(b), for each visible anchor point within the view-\ning frustum, we spawn k neural Gaussians and predict their\nattributes. Specifically, given an anchor point located at xv,\nthe positions of its neural Gaussians are calculated as:\n{\u00b50, ..., \u00b5k\u22121} = xv + {O0, . . . , Ok\u22121} \u00b7 lv,\n(8)\nwhere {O0, O1, ..., Ok\u22121} \u2208 Rk\u00d73 are the learnable offsets\nand lv is the scaling factor associated with that anchor, as\ndescribed in Sec. 3.2.1. The attributes of k neural Gaussians\nare directly decoded from the anchor feature \u02c6fv, the relative\nviewing distance \u03b4vc and direction \u20d7dvc between the cam-\nera and the anchor point (Eq. 5) through individual MLPs,\ndenoted as F\u03b1, Fc, Fq and Fs. Note that attributes are de-\ncoded in one-pass. For example, opacity values of neural\nGaussians spawned from an anchor point are given by:\n{\u03b10, ..., \u03b1k\u22121} = F\u03b1( \u02c6fv, \u03b4vc, \u20d7dvc),\n(9)\ntheir colors {ci}, quaternions {qi} and scales {si} are simi-\nlarly derived. Implementation details are in supplementary.\nNote that the prediction of neural Gaussian attributes\nare on-the-fly, meaning that only anchors visible within\nthe frustum are activated to spawn neural Gaussians. To\nmake the rasterization more efficient, we only keep neural\nGaussians whose opacity values are larger than a predefined\nthreshold \u03c4\u03b1. This substantially cuts down the computa-\ntional load and helps our method maintain a high rendering\nspeed on-par with the original 3D-GS.\n3.3. Anchor Points Refinement\nGrowing Operation.\nSince neural Gaussians are closely\ntied to their anchor points which are initialized from SfM\npoints, their modeling power is limited to a local region, as\nhas been pointed out in [22, 53]. This poses challenges to\nthe initial placement of anchor points, especially in texture-\nless and less observed areas. We therefore propose an error-\nbased anchor growing policy that grows new anchors where\nneural Gaussians find significant. To determine a significant\narea, we first spatially quantize the neural Gaussians by con-\nstructing voxels of size \u03f5g. For each voxel, we compute the\naveraged gradients of the included neural Gaussians over\nN training iterations, denoted as \u2207g. Then, voxels with\n\u2207g > \u03c4g is deemed as significant, where \u03c4g is a pre-defined\nthreshold; and a new anchor point is thereby deployed at\nthe center of that voxel if there was no anchor point estab-\nlished. Fig. 3 illustrates this growing operation. In practice,\nwe quantize the space into multi-resolution voxel grid to al-\n4\nGradient (small to large)\nMulti-res voxel \n(colored for new anchor)\nFigure 3. Growing operation. We develop an anchor growing\npolicy guided by the gradients of the neural Gaussians.\nFrom\nleft to right, we spatially quantize neural Gaussians into multi-\nresolution voxels (m \u2208 {1, 2, 3}) of size {\u03f5(m)\ng\n}. New anchors\nare added to voxels with aggregated gradients larger than {\u03c4 (m)\ng\n}.\nlow new anchors to be added at different granularity, where\n\u03f5(m)\ng\n= \u03f5g/4m\u22121,\n\u03c4 (m)\ng\n= \u03c4g \u2217 2m\u22121,\n(10)\nwhere m denotes the level of quantization. To further regu-\nlate the addition of new anchors, we apply a random elimi-\nnation to these candidates. This cautious approach to adding\npoints effectively curbs the rapid expansion of anchors.\nPruning Operation\nTo eliminate trivial anchors, we ac-\ncumulate the opacity values of their associated neural Gaus-\nsians over N training iterations. If an anchor fails to pro-\nduce neural Gaussians with a satisfactory level of opacity,\nwe then remove it from the scene.\n3.4. Losses Design\nWe optimize the learnable parameters and MLPs with re-\nspect to the L1 loss over rendered pixel colors, with SSIM\nterm [47] LSSIM and volume regularization [28] Lvol. The\ntotal supervision is given by:\nL = L1 + \u03bbSSIMLSSIM + \u03bbvolLvol,\n(11)\nwhere the volume regularization Lvol is:\nLvol =\nNng\nX\ni=1\nProd(si).\n(12)\nHere, Nng denotes the number of neural Gaussians in the\nscene and Prod(\u00b7) is the product of the values of a vector,\ne.g., in our case the scale si of each neural Gaussian. The\nvolume regularization term encourages the neural Gaus-\nsians to be small with minimal overlapping.\n4. Experiments\n4.1. Experimental Setup\nDataset and Metrics.\nWe conducted a comprehen-\nsive evaluation across 27 scenes from publicly avail-\nable datasets.\nSpecifically, we tested our approach on\nall available scenes tested in the 3D-GS [22], including\nseven scenes from Mip-NeRF360 [4], two scenes from\nTanks&Temples [23], two scenes from DeepBlending [18]\nand synthetic Blender dataset [30]. We additionally evalu-\nated on datasets with contents captured at multiple LODs\nto demonstrate our advantages in view-adaptive rendering.\nSix scenes from BungeeNeRF [49] and two scenes from\nVR-NeRF [51] are selected. The former provides multi-\nscale outdoor observations and the latter captures intricate\nindoor environments. Apart from the commonly used met-\nrics (PSNR, SSIM [47], and LPIPS [56]), we additionally\nreport the storage size (MB) and the rendering speed (FPS)\nfor model compactness and performance efficiency.\nWe\nprovide the averaged metrics over all scenes of each dataset\nin the main paper and leave the full quantitative results on\neach scene in the supplementary.\nBaseline and Implementation.\n3D-GS [22] is selected as\nour main baseline for its established SOTA performance in\nnovel view synthesis. Both 3D-GS and our method were\ntrained for 30k iterations. We also record the results of Mip-\nNeRF360 [4], iNGP [31] and Plenoxels [13] as in [22].\nFor our method, we set k = 10 for all experiments. All\nthe MLPs employed in our approach are 2-layer MLPs with\nReLU activation; the dimensions of the hidden units are all\n32. For anchor points refinement, we average gradients over\nN = 100 iterations, and by default use \u03c4g = 64\u03f5. On intri-\ncate scenes and the ones with dominant texture-less regions,\nwe use \u03c4g = 16\u03f5. An anchor is pruned if the accumulated\nopacity of its neural Gaussians is less than 0.5 at each round\nof refinement. The two loss weights \u03bbSSIM and \u03bbvol are set\nto 0.2 and 0.001 in our experiments. Please check the sup-\nplementary material for more details.\n4.2. Results Analysis\nOur evaluation was conducted on diverse datasets, ranging\nfrom synthetic object-level scenes, indoor and outdoor envi-\nronments, to large-scale urban scenes and landscapes. A va-\nriety of improvements can be observed especially on chal-\nlenging cases, such as texture-less area, insufficient obser-\nvations, fine-scale details and view-dependent light effects.\nSee Fig. 1 and Fig. 4 for examples.\nComparisons.\nIn assessing the quality of our approach,\nwe compared with 3D-GS [22],\nMip-NeRF360 [4],\niNGP [31] and Plenoxels [13] on real-world datasets. Qual-\nitative results are presented in Tab. 1.\nThe quality met-\nrics for Mip-NeRF360, iNGP and Plenoxels align with\nthose reported in the 3D-GS study. It can be noticed that\nour approach achieves comparable results with the SOTA\nalgorithms on Mip-NeRF360 dataset, and surpassed the\nSOTA on Tanks&Temples and DeepBlending, which cap-\ntures more challenging environments with the presence\n5\nTable 1. Quantitative comparison to previous methods on real-world datasets. Competing metrics are extracted from respective papers.\nDataset\nMip-NeRF360\nTanks&Temples\nDeep Blending\nMethod\nMetrics\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\n3D-GS [22]\n28.69\n0.870\n0.182\n23.14\n0.841\n0.183\n29.41\n0.903\n0.243\nMip-NeRF360 [4]\n29.23\n0.844\n0.207\n22.22\n0.759\n0.257\n29.40\n0.901\n0.245\niNPG [31]\n26.43\n0.725\n0.339\n21.72\n0.723\n0.330\n23.62\n0.797\n0.423\nPlenoxels [13]\n23.62\n0.670\n0.443\n21.08\n0.719\n0.379\n23.06\n0.795\n0.510\nOurs\n28.84\n0.848\n0.220\n23.96\n0.853\n0.177\n30.21\n0.906\n0.254\n3D-GS (frame PSNR / avg PSNR)\nOurs (frame PSNR / avg PSNR)\nGT (scene name)\nCloser view\nCloser view\nCloser view\n3D-GS (frame PSNR / avg PSNR)\nOurs (frame PSNR / avg PSNR) \nGT (scene name)\n29.88 / 31.93\n27.76 / 31.52\n33.46 / 31.93\n32.32 / 31.52\n31.51 / 29.34\n30.15 / 28.88\n30.12 / 29.34\n29.10 / 28.88\n23.52 / 22.15\n22.56 / 21.90 \n30.17 / 29.8\n28.48 / 28.95 \n26.76 / 25.77\n26.00 / 25.23 \n34.14 / 30.62\n32.04 / 29.80 \n26.97 / 29.61\n21.80 / 29.40 \n27.04 / 28.87\n22.76 / 28.48 \n30.88\n26.24\nMip360-Room(a)\nMip360-Room(b)\nMip360-Counter(a)\nMip360-Counter(b)\nTandT-Train \nTandT-Truck\nDB-DrJohnson \nDB-Playroom \nVR-Kitchen\nVR-Apartment\nFigure 4. Qualitative comparison of Scaffold-GS and 3D-GS [22] across diverse datasets [4, 17, 23, 51]. Patches that highlight the\nvisual differences are emphasized with arrows and green & yellow insets for clearer visibility. Our approach constantly outperforms 3D-GS\non these scenes, with evident advantages in challenging scenarios, e.g. thin geometry and fine-scale details (MIP360-ROOM(a), MIP360-\nCOUNTER(a)), texture-less regions (DB-DRJOHNSON, DB-PLAYROOM), light effects (MIP360-COUNTER(b), DB-DRJOHNSON), in-\nsufficient observations (TANDT-TRAIN, VR-KITCHEN). It can also be observed (e.g. VR-APARTMENT) that our model is superior in\nrepresenting contents at varying scales and viewing distances.\nTable 2. Performance comparison. Rendering FPS and storage\nsize are reported. Storage size reduction ratio is indicated by (\u2193).\nRendering speed of both methods are measured on our machine.\nDataset\nMip-NeRF360\nTanks&Temples\nDeep Blending\nFPS\nMem (MB)\nFPS\nMem (MB)\nFPS\nMem (MB)\n3D-GS\n97\n693\n123\n411\n109\n676\nOurs\n102\n156 (4.4\u00d7 \u2193)\n110\n87 (4.7\u00d7 \u2193)\n139\n66 (10.2\u00d7 \u2193)\nof e.g. changing lighting, texture-less regions and reflec-\ntions. In terms of efficiency, we evaluated rendering speed\nand storage size of our method and 3D-GS, as shown in\nTab. 2. Our method achieved real-time rendering while us-\ning less storage, indicating that our model is more com-\npact than 3D-GS without sacrificing rendering quality and\nspeed. Additionally, akin to prior grid-based methods, our\napproach converged faster than 3D-GS. See supplementary\nmaterial for more analysis.\nWe also examined our method on the synthetic Blender\ndataset, which provides an exhaustive set of views capturing\nobjects at 360\u25e6. A good set of initial SfM points is not read-\nily available in this dataset, thus we start from 100k grid\npoints and learn to grow and prune points with our anchor\nrefinement operations. After 30k iterations, we used the re-\n6\nTable 3. Qualitative comparison. Our method is able to handle\nlarge-scale scenes (e.g. BUNGEENERF) with light-weight repre-\nsentation. Our method shows consistent compactness and effec-\ntiveness in complex lighting conditions and synthetic scenes.\nDataset\nBungeeNeRF\nVR-NeRF\nSynthetic Blender\nPSNR\nMem (MB)\nPSNR\nMem (MB)\nPSNR\nMem (MB)\n3D-GS\n24.89\n1606\n28.94\n263\n33.32\n53\nOurs\n27.01\n203 (7.9\u00d7 \u2193)\n29.24\n69 (3.8\u00d7 \u2193)\n33.68\n14 (3.8\u00d7 \u2193)\nFigure 5. Comparison on multi-scale scenes (w/ zoom-in cases).\nWe showcase the rendering outcomes at an unsceen closer scale on\nthe AMSTERDAM scene from BungeeNeRF. Our method smoothly\nextrapolates to new viewing distances using refined neural Gaus-\nsian properties, remedying the needle-like artifacts of original 3D-\nGS caused by fixed Gaussian scaling values.\nmained points as initialized anchors and re-run our frame-\nwork. The PSNR score and storage size compared with 3D-\nGS are presented in Tab. 3. Fig. 1 also demonstrates that our\nmethod can achieve better visual quality with more reliable\ngeometry and texture details.\nMulti-scale Scene Contents.\nWe examined our model\u2019s\ncapability in handling multi-scale scene details on the\nBungeeNeRF and VR-NeRF datasets. As shown in Tab. 3,\nour method achieved superior quality whilst using fewer\nstorage size to store the model compared to 3D-GS [22].\nAs illustrated in Fig. 4 and Fig. 5, our method was supe-\nrior in accommodating varying levels of detail in the scene.\nIn contrast, images rendered from 3D-GS often suffered\nfrom noticeable blurry and needle-shaped artifacts. This\nis likely because that Gaussian attributes are optimized to\noverfit multi-scale training views, creating excessive Gaus-\nsians that work for each observing distance. However, it can\neasily lead to ambiguity and uncertainty when synthesizing\nnovel views, since it lacks the ability to reason about view-\ning angle and distance. On contrary, our method efficiently\nencoded local structures into compact neural features, en-\nhancing both rendering quality and convergence speed. De-\ntails are provided in the supplementary material.\nFigure 6. Anchor feature clustering. We cluster anchor features\n(DB-PLAYROOM) into 3 clusters using K-means [25] and visu-\nalize the result. The clustered features show clues of scene con-\ntents, e.g. the banister, stroller, desk and monitor can be clearly\nidentified. Anchors on the wall and floor are also respectively\ngrouped together. This shows that our approach improves the in-\nterpretability of 3D-GS model, and has the potential to be scaled-\nup on much larger scenes exploiting reusable features.\nFeature Analysis.\nWe further perform an analysis of the\nlearnable anchor features and the selector mechanism. As\ndepicted in Fig. 6, the clustered pattern suggests that the\ncompact anchor feature spaces adeptly capture regions with\nsimilar visual attributes and geometries, as evidenced by\ntheir proximity in the encoded feature space.\nView Adaptability.\nTo support that our neural Gaussians\nare view-adaptive, we explore how the values of attributes\nchange when the same Gaussian is observed from differ-\nent positions. Fig. 7 demonstrates a varying distribution\nof attributes intensity at different viewing positions, while\nmaintaining a degree of local continuity. This accounts for\nthe superior view adaptability of our method compared to\nthe static attributes of 3D-GS, as well as its enhanced gen-\neralizability to novel views.\nSelection Process by Opacity.\nWe examine the decoded\nopacity from neural Gaussians and our opacity-based selec-\ntion process (Fig. 2(b)) from two aspects. First, without the\nanchor point refinement module, we filter neural Gaussian\nusing their decoded opacity values to extract geometry from\na random point cloud. Fig. 8 demonstrates that the remained\nneural Gaussians effectively reconstruct the coarse structure\nof the bulldozer model from random points, highlighting\nits capability for implicit geometry modeling under mainly\nrendering-based supervision. We found this is conceptually\nsimilar to the proposal network utilized in [4], serving as\nthe geometry proxy estimator for efficient sampling.\nSecond, we apply different k values in our method.\n7\nFigure 7. View-adaptive neural Gaussian attributes. We visu-\nalize the decoded attributes of a single neural Gaussian observed\nat different positions. Each point corresponds to a viewpoint in\nspace. The color of the point denotes the intensity of attributes de-\ncoded for this view (left: Fs \u2192 si; right: F\u03b1 \u2192 \u03b1i). This pattern\nindicates that attributes of a neural Gaussian adapt to viewpoint\nchanging, while exhibiting a certain degree of local continuity.\nAnchor points\n\ud835\udfaa > \ud835\udf49\nSelected Neural Gaussians in a test view\nselector\nFigure 8. Geometry culling via selector. (Left) Anchor points\nfrom randomly initialized points; (Right) Activated neural Gaus-\nsians derived from each anchor under the current view. In synthetic\nBlender scenes, with all 3D Gaussians visible in the viewing frus-\ntum, our opacity filtering functions similar to a geometry proxy\nestimator, excluding unoccupied regions before rasterization.\nTraining iterations\nActivated neural Gaussians\nFigure 9. Learning with different k values. Despite varying\ninitial k values under different hyper-parameter settings, they con-\nverge to activate a similar number of neural Gaussians with com-\nparable rendering fidelity.\nFig. 9 shows that regardless of the k value, the final number\nof activated neural Gaussians converges to a similar amount\nthrough the training, indicating Scaffold-GS\u2019s preference to\nselect a collection of non-redundant Gaussians that are suf-\nficient to represent the scene.\nTable 4. Effects of filtering. FILTER 1 refers to selecting an-\nchors by view frustum and FILTER 2 refers to the opacity-based\nselection process. The filtering method has no notable impact on\nfidelity, but greatly affects inference speed.\nScene\nDB-PLAYROOM\nDB-DRJOHNSON\nPSNR\nFPS\nPSNR\nFPS\nNO FILTERS\n30.4\n84\n29.7\n79\nFILTER 1\n30.3\n118\n29.6\n100\nFILTER 2\n30.6\n109\n29.7\n104\nFULL\n30.62\n150\n29.8\n129\nTable 5. Anchor refinement. The growing operation is essential\nfor fidelity since it improves the poor initialization. The pruning\noperation controls the increasing of storage size and optimizes the\nquality of remained anchors.\nScene\nDB-PLAYROOM\nDB-DRJOHNSON\nPSNR\nMem (MB)\nPSNR\nMem (MB)\nNONE\n28.45\n24\n28.81\n12\nW/ PRUNING\n29.12\n23\n28.51\n12\nW/ GROWING\n30.54\n71\n29.75\n76\nFULL\n30.62\n63\n29.80\n68\n4.3. Ablation Studies\nEfficacy of Filtering Strategies.\nWe evaluated our fil-\ntering strategies (Sec. 3.2.2), which we found crucial for\nspeeding up our method.\nAs Tab. 4 shows, while these\nstrategies had no notable effect on fidelity, they significantly\nenhanced inference speed. However, there was a risk of\nmasking pertinent neural Gaussians, which we aim to ad-\ndress in future works.\nEfficacy of Anchor Points Refinement Policy.\nWe eval-\nuated our growing and pruning operations described in\nSec. 3.3. Tab. 5 shows the results of disabling each opera-\ntion in isolation and maintaining the rest of the method. We\nfound that the addition operation is crucial for accurately re-\nconstructing details and texture-less areas, while the prun-\ning operation plays an important role in eliminating trivial\nGaussians and maintaining the efficiency of our approach.\n4.4. Discussions and Limitations\nThrough our experiments, we found that the initial points\nplay a crucial role for high-fidelity results. Initializing our\nframework from SfM point clouds is a swift and viable\nsolution, considering these point clouds usually arise as a\nbyproduct of image calibration processes. However, this ap-\nproach may be suboptimal for scenarios dominated by large\ntexture-less regions. Despite our anchor point refinement\nstrategy can remedy this issue to some extent, it still suffers\nfrom extremely sparse points. We expect that our algorithm\nwill progressively improve as the field advances, yielding\n8\nmore accurate results. Further details are discussed in the\nsupplementary material.\n5. Conclusion\nIn this work, we introduce Scaffold-GS, a novel 3D neural\nscene representation for efficient view-adaptive rendering.\nThe core of Scaffold-GS lies in its structural arrangement of\n3D Gaussians guided by anchor points from SfM, whose at-\ntributes are on-the-fly decoded from view-dependent MLPs.\nWe show that our approach leverages a much more com-\npact set of Gaussians to achieve comparable or even bet-\nter results than the SOTA algorithms. The advantage of\nour view-adaptive neural Gaussians is particularly evident\nin challenging cases where 3D-GS usually fails. We fur-\nther show that our anchor points encode local features in\na meaningful way that exhibits semantic patterns to some\ndegree, suggesting its potential applicability in a range of\nversatile tasks such as large-scale modeling, manipulation\nand interpretation in the future.\nReferences\n[1] Kara-Ali Aliev, Dmitry Ulyanov, and Victor S. Lempitsky.\nNeural point-based graphics.\nIn European Conference on\nComputer Vision, 2019. 3\n[2] Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter\nHedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan.\nMip-nerf: A multiscale representation for anti-aliasing neu-\nral radiance fields. 2021 IEEE/CVF International Confer-\nence on Computer Vision (ICCV), pages 5835\u20135844, 2021.\n2\n[3] Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter\nHedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan.\nMip-nerf: A multiscale representation for anti-aliasing neu-\nral radiance fields. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision (ICCV), pages 5855\u2013\n5864, 2021. 1\n[4] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P.\nSrinivasan, and Peter Hedman. Mip-nerf 360: Unbounded\nanti-aliased neural radiance fields. CVPR, 2022. 2, 5, 6, 7, 3\n[5] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P.\nSrinivasan, and Peter Hedman.\nZip-nerf:\nAnti-aliased\ngrid-based neural radiance fields.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision\n(ICCV), pages 19697\u201319705, 2023. 1\n[6] Mario Botsch, Alexander Hornung, Matthias Zwicker, and\nLeif Kobbelt.\nHigh-quality surface splatting on today\u2019s\ngpus. In Proceedings Eurographics/IEEE VGTC Symposium\nPoint-Based Graphics, 2005., pages 17\u2013141. IEEE, 2005. 1\n[7] Mario\nBotsch,\nAlexander\nSorkine-Hornung,\nMatthias\nZwicker, and Leif P. Kobbelt. High-quality surface splatting\non today\u2019s gpus.\nProceedings Eurographics/IEEE VGTC\nSymposium Point-Based Graphics, 2005., pages 17\u2013141,\n2005. 2\n[8] Eric Chan, Connor Z. Lin, Matthew Chan, Koki Nagano,\nBoxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J.\nGuibas, Jonathan Tremblay, S. Khamis, Tero Karras, and\nGordon Wetzstein. Efficient geometry-aware 3d generative\nadversarial networks. 2022 IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 16102\u2013\n16112, 2021. 2\n[9] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\nHao Su.\nTensorf:\nTensorial radiance fields.\nArXiv,\nabs/2203.09517, 2022. 2\n[10] Anpei Chen, Zexiang Xu, Xinyue Wei, Siyu Tang, Hao Su,\nand Andreas Geiger. Factor fields: A unified framework for\nneural fields and beyond. ArXiv, abs/2302.01226, 2023. 2\n[11] Zhiqin Chen, Thomas Funkhouser, Peter Hedman, and An-\ndrea Tagliasacchi. Mobilenerf: Exploiting the polygon ras-\nterization pipeline for efficient neural field rendering on mo-\nbile architectures. In The Conference on Computer Vision\nand Pattern Recognition (CVPR), 2023. 2\n[12] Christopher Bongsoo Choy, Danfei Xu, JunYoung Gwak,\nKevin Chen, and Silvio Savarese. 3d-r2n2: A unified ap-\nproach for single and multi-view 3d object reconstruction.\nArXiv, abs/1604.00449, 2016. 2\n[13] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong\nChen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:\nRadiance fields without neural networks. In CVPR, 2022. 2,\n5, 6, 3\n[14] Sara Fridovich-Keil, Giacomo Meanti, Frederik Warburg,\nBenjamin Recht, and Angjoo Kanazawa.\nK-planes: Ex-\nplicit radiance fields in space, time, and appearance. 2023\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 12479\u201312488, 2023. 2\n[15] Kyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna,\nand Thomas A. Funkhouser. Local deep implicit functions\nfor 3d shape.\n2020 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pages 4856\u20134865,\n2019. 2\n[16] Markus Gross and Hanspeter Pfister. Point-based graphics.\nElsevier, 2011. 3\n[17] Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm,\nGeorge Drettakis, and Gabriel Brostow. Deep blending for\nfree-viewpoint image-based rendering. 37(6):257:1\u2013257:15,\n2018. 6, 3\n[18] Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm,\nGeorge Drettakis, and Gabriel Brostow. Deep blending for\nfree-viewpoint image-based rendering. ACM Transactions\non Graphics (ToG), 37(6):1\u201315, 2018. 5, 2, 3\n[19] Peter Hedman,\nPratul P. Srinivasan,\nBen Mildenhall,\nJonathan T. Barron, and Paul E. Debevec.\nBaking neural\nradiance fields for real-time view synthesis. 2021 IEEE/CVF\nInternational Conference on Computer Vision (ICCV), pages\n5855\u20135864, 2021. 2\n[20] Eldar Insafutdinov and Alexey Dosovitskiy. Unsupervised\nlearning of shape and pose with differentiable point clouds.\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2018. 3\n[21] Abhishek Kar, Christian H\u00a8ane, and Jitendra Malik. Learning\na multi-view stereo machine. ArXiv, abs/1708.05375, 2017.\n2\n[22] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler,\nand George Drettakis.\n3d gaussian splatting for real-time\n9\nradiance field rendering. ACM Transactions on Graphics, 42\n(4), 2023. 1, 3, 4, 5, 6, 7, 2\n[23] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen\nKoltun. Tanks and temples: Benchmarking large-scale scene\nreconstruction. ACM Transactions on Graphics, 36(4), 2017.\n5, 6, 2, 3\n[24] Georgios Kopanas, Julien Philip, Thomas Leimk\u00a8uhler, and\nGeorge Drettakis.\nPoint-based neural rendering with per-\nview optimization.\nComputer Graphics Forum, 40, 2021.\n3\n[25] K Krishna and M Narasimha Murty. Genetic k-means algo-\nrithm. IEEE Transactions on Systems, Man, and Cybernet-\nics, Part B (Cybernetics), 29(3):433\u2013439, 1999. 7\n[26] Christoph Lassner and Michael Zollhofer.\nPulsar: Effi-\ncient sphere-based neural rendering.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 1440\u20131449, 2021. 1\n[27] Chen-Hsuan Lin, Chen Kong, and Simon Lucey.\nLearn-\ning efficient point cloud generation for dense 3d object re-\nconstruction. In AAAI Conference on Artificial Intelligence\n(AAAI), 2018. 3\n[28] Stephen Lombardi,\nTomas Simon,\nGabriel Schwartz,\nMichael Zollhoefer, Yaser Sheikh, and Jason Saragih. Mix-\nture of volumetric primitives for efficient neural rendering.\nACM Transactions on Graphics (ToG), 40(4):1\u201313, 2021. 5\n[29] Lars M. Mescheder, Michael Oechsle, Michael Niemeyer,\nSebastian Nowozin, and Andreas Geiger. Occupancy net-\nworks: Learning 3d reconstruction in function space. 2019\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 4455\u20134465, 2018. 2\n[30] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. Communications of the ACM, 65(1):99\u2013106, 2021. 1,\n2, 5, 3\n[31] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a mul-\ntiresolution hash encoding. ACM Transactions on Graphics\n(ToG), 41(4):1\u201315, 2022. 2, 5, 6, 3\n[32] Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao,\nWenzheng Chen, Alex Evans, Thomas M\u00a8uller, and Sanja Fi-\ndler. Extracting Triangular 3D Models, Materials, and Light-\ning From Images. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\npages 8280\u20138290, 2022. 1\n[33] Michael Oechsle, Songyou Peng, and Andreas Geiger.\nUnisurf:\nUnifying neural implicit surfaces and radiance\nfields for multi-view reconstruction.\n2021 IEEE/CVF In-\nternational Conference on Computer Vision (ICCV), pages\n5569\u20135579, 2021. 2\n[34] Jeong Joon Park,\nPeter R. Florence,\nJulian Straub,\nRichard A. Newcombe, and S. Lovegrove. Deepsdf: Learn-\ning continuous signed distance functions for shape represen-\ntation. 2019 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), pages 165\u2013174, 2019. 2\n[35] Songyou Peng, Michael Niemeyer, Lars M. Mescheder,\nMarc Pollefeys, and Andreas Geiger. Convolutional occu-\npancy networks. ArXiv, abs/2003.04618, 2020. 2\n[36] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Milden-\nhall. Dreamfusion: Text-to-3d using 2d diffusion. In The\nEleventh International Conference on Learning Representa-\ntions, ICLR 2023, Kigali, Rwanda, May 1-5, 2023, 2023. 1\n[37] Liu Ren, Hanspeter Pfister, and Matthias Zwicker. Object\nspace ewa surface splatting: A hardware accelerated ap-\nproach to high quality point rendering. Computer Graphics\nForum, 21, 2002. 2\n[38] Miguel Sainz and Renato Pajarola. Point-based rendering\ntechniques. Computers & Graphics, 28(6):869\u2013879, 2004. 2\n[39] Johannes\nLutz\nSch\u00a8onberger\nand\nJan-Michael\nFrahm.\nStructure-from-motion revisited.\nIn Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2016. 3, 2\n[40] Jessica Shue, Eric Chan, Ryan Po, Zachary Ankner, Jiajun\nWu, and Gordon Wetzstein. 3d neural field generation us-\ning triplane diffusion. 2023 IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 20875\u2013\n20886, 2022. 2\n[41] Vincent Sitzmann, Semon Rezchikov, William T. Freeman,\nJoshua B. Tenenbaum, and Fr\u00b4edo Durand. Light field net-\nworks: Neural scene representations with single-evaluation\nrendering. In Neural Information Processing Systems, 2021.\n2\n[42] Noah Snavely, Steven M. Seitz, and Richard Szeliski. Photo\nTourism: Exploring Photo Collections in 3D. Association\nfor Computing Machinery, New York, NY, USA, 1 edition,\n2023. 2\n[43] Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Prad-\nhan, Ben Mildenhall, Pratul P Srinivasan, Jonathan T Barron,\nand Henrik Kretzschmar. Block-nerf: Scalable large scene\nneural view synthesis. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n8248\u20138258, 2022. 1\n[44] Shubham Tulsiani, Tinghui Zhou, Alyosha A. Efros, and Ji-\ntendra Malik.\nMulti-view supervision for single-view re-\nconstruction via differentiable ray consistency. 2017 IEEE\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 209\u2013217, 2017. 2\n[45] Haithem Turki,\nDeva Ramanan,\nand Mahadev Satya-\nnarayanan.\nMega-nerf:\nScalable construction of large-\nscale nerfs for virtual fly-throughs.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 12922\u201312931, 2022. 1\n[46] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\nKomura, and Wenping Wang. Neus: Learning neural implicit\nsurfaces by volume rendering for multi-view reconstruction.\narXiv preprint arXiv:2106.10689, 2021. 2\n[47] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli.\nImage quality assessment: from error visibility to structural\nsimilarity. IEEE Transactions on Image Processing, 13(4):\n600\u2013612, 2004. 5\n[48] Olivia Wiles, Georgia Gkioxari, Richard Szeliski, and Justin\nJohnson. Synsin: End-to-end view synthesis from a single\nimage. 2020 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), pages 7465\u20137475, 2019. 3\n[49] Yuanbo Xiangli, Linning Xu, Xingang Pan, Nanxuan Zhao,\nAnyi Rao, Christian Theobalt, Bo Dai, and Dahua Lin.\n10\nBungeenerf: Progressive neural radiance field for extreme\nmulti-scale scene rendering.\nIn Computer Vision - ECCV\n2022 - 17th European Conference, Tel Aviv, Israel, Octo-\nber 23-27, 2022, Proceedings, Part XXXII, pages 106\u2013122.\nSpringer, 2022. 1, 2, 5, 3\n[50] Yuanbo Xiangli, Linning Xu, Xingang Pan, Nanxuan Zhao,\nBo Dai, and Dahua Lin. Assetfield: Assets mining and re-\nconfiguration in ground feature plane representation. ArXiv,\nabs/2303.13953, 2023. 2\n[51] Linning Xu, Vasu Agrawal, William Laney, Tony Garcia,\nAayush Bansal, Changil Kim, Samuel Rota Bul`o, Lorenzo\nPorzi, Peter Kontschieder, Alja\u02c7z Bo\u02c7zi\u02c7c, Dahua Lin, Michael\nZollh\u00a8ofer, and Christian Richardt. VR-NeRF: High-fidelity\nvirtualized walkable spaces. In SIGGRAPH Asia Conference\nProceedings, 2023. 1, 5, 6, 2, 3\n[52] Linning Xu, Yuanbo Xiangli, Sida Peng, Xingang Pan,\nNanxuan Zhao, Christian Theobalt, Bo Dai, and Dahua Lin.\nGrid-guided neural radiance fields for large urban scenes. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 8296\u20138306, 2023. 2\n[53] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin\nShu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf:\nPoint-based neural radiance fields.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5438\u20135448, 2022. 3, 4\n[54] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman.\nVolume rendering of neural implicit surfaces.\nIn Thirty-\nFifth Conference on Neural Information Processing Systems,\n2021. 2\n[55] Wang Yifan, Felice Serena, Shihao Wu, Cengiz \u00a8Oztireli,\nand Olga Sorkine-Hornung. Differentiable surface splatting\nfor point-based geometry processing. ACM Transactions on\nGraphics (TOG), 38(6):1\u201314, 2019. 1, 3\n[56] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2018. 5\n[57] Xi Zhao, Ruizhen Hu, Haisong Liu, Taku Komura, and\nXinyu Yang. Localization and completion for 3d object inter-\nactions. IEEE Transactions on Visualization and Computer\nGraphics, 26(8):2634\u20132644, 2019. 2\n[58] Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and\nMarkus Gross. Ewa volume splatting. In Proceedings Visu-\nalization, 2001. VIS\u201901., pages 29\u2013538. IEEE, 2001. 3\n11\nScaffold-GS: Structured 3D Gaussians for View-Adaptive Rendering\nSupplementary Material\n6. Overview\nThis supplementary is organized as follows: (1) In the\nfirst section, we elaborate implementation details of our\nScaffold-GS, including anchor point feature enhancement\n(Sec.3.2.1), structure of MLPs (Sec.3.2.2) and anchor point\nrefinement strategies (Sec.3.3); (2) The second part de-\nscribes our dataset preparation steps. We then show addi-\ntional experimental results and analysis based on our train-\ning observations.\n7. Implementation details.\nFeature Bank.\nTo enhance the view-adaptability, we up-\ndate the anchor feature through a view-dependent encoding.\nFollowing calculating the relative distance \u03b4vc and view-\ning direction \u20d7dvc of a camera and an anchor, we predict\na weight vector w \u2208 R3 as follows:\n(w, w1, w2) = Softmax(Fw(\u03b4vc, \u20d7dvc)),\n(13)\nwhere Fw is a tiny MLP that serves as a view encoding func-\ntion. We then encode the view direction information to the\nanchor feature fv by compositing a feature bank containing\ninformation with different resolutions as follows:\n\u02c6fv = w \u00b7 fv + w1 \u00b7 fv\u21931 + w2 \u00b7 fv\u21932 ,\n(14)\nIn practice, we implement the feature bank via slicing and\nrepeating, as illustrated in Fig. 10. We found this slicing and\nmixture operation improves Scaffold-GS\u2019s ability to cap-\nture different scene granularity. The distribution of feature\nbank\u2019s weights is illustrated in Fig. 11.\nfv\u2193\nrepeat\nrepeat\nrepeat\nrepeat\nrepeat\nrepeat\nfv\n1\nfv\u21932\nFigure 10. Generation of Feature Bank. We expand the anchor\nfeature f into a set of multi-resolution features {fv, fv\u21931 , fv\u21932 }\nvia slicing and repeating. This operation improves Scaffold-GS\u2019s\nability to capture different scene granularity.\nMLPs as feature decoders.\nThe core MLPs include the\nopacity MLP F\u03b1, the color MLP Fc and the covariance\nMLP Fs and Fq. All of these F\u2217 are implemented in a LIN-\nEAR \u2192 RELU \u2192 LINEAR style with the hidden dimension\nof 32, as illustrated in Fig. 12. Each branch\u2019s output is acti-\nvated with a head layer.\n(a)\n(b)\n(c)\nFigure 11.\nView-based feature bank\u2019s weight distribution.\n(a), (b) and (c) denote the predicted weights {w2, w1, w} for\n{fv\u21932 , fv\u21931 , fv} from a group of uniformally distributed view-\npoints. Light color denotes larger weights. For this anchor, finer\nfeatures are more activated at center view positions. The patterns\nexhibit the ability to capture different scene granularities based on\nview direction and distance.\n36x32\n32\u00d71\u00d7(\ud835\udc58)\nReLU\n36x32\n32\u00d73\u00d7(\ud835\udc58)\nReLU\n36x32\n32\u00d77\u00d7(\ud835\udc58)\nReLU\nInput: N\u00d7(32+3+1)\n3\n\u2026\n3\n\u2026\n3\n1 1 1 1\n1 1\nOpacity:  N\u00d71\u00d7(\ud835\udc58)\nColor: N\u00d73\u00d7(\ud835\udc58)\nTanh\nSigmoid\n3\n3\n\u2026\n3\n4\n4\nfv^\ndvc\n\u2192\n\ud835\udeffvc\n\u2026\n4\nScale: N\u00d73\u00d7(\ud835\udc58)\nQuaternion: N\u00d74\u00d7(\ud835\udc58)\nSigmoid*sv\nNormalization\nF\ud835\udefc\nFc\nFs & Fq \nFigure 12. MLP Structures. For each anchor point, we use small\nMLPs (F\u03b1, Fc, Fs, Fq) to predict attributes (opacity, color, scale\nand quaternion) of k neural Gaussians. The input to MLPs are\nanchor feature \u02c6fv, relative viewing direction \u20d7dvc and distance \u03b4vc\nbetween the camera and anchor point.\n\u2022 For opacity, the output is activated by Tanh, where value\n0 serves as a natural threshold for selecting valid samples\nand the final valid values can cover the full range of [0,1).\n\u2022 For color, we activate the output with Sigmoid function:\n{c0, ..., ck\u22121} = Sigmoid(Fc),\n(15)\nwhich constrains the color into a range of (0,1).\n\u2022 For rotation, we follow 3D-GS [22] and activate it with a\nnormalization to obtain a valid quaternion.\n\u2022 For scaling, we adjust the base scaling sv of each anchor\nwith the MLP output as follows:\n{s0, ..., sk\u22121} = Sigmoid(Fs) \u00b7 sv,\n(16)\nVoxel Size.\nThe voxel size \u03f5 sets the finest anchor reso-\nlution. We employ two strategies: 1) Use the median of\nthe nearest-neighbor distances among all initial points: \u03f5\n1\nzoom\nout\nInitial anchors \nRe\ufb01ned anchors \nzoom\nout\nExample scene\nFigure 13. Anchor Refinement. We visualize the initial and re-\nfined anchor points on the truck scene [23]. The truck is high-\nlighted by the circle. Note that the refined points effectively cov-\ners surrounding regions and fine-scale structures, leaning to more\ncomplete and detailed scene renderings.\nis adapted to point cloud density, yielding denser anchors\nwith enhanced rendering quality but might introduce more\ncomputational overhead; 2) Set \u03f5 manually to either 0.005\nor 0.01: this is effective in most scenarios but might lead to\nmissing details in texture-less regions. We found these two\nstrategies adequately accommodate various scene complex-\nities in our experiments.\nAnchor Refinement.\nAs briefly discussed in the main pa-\nper, the voxelization process suggests that our method may\nbehave sensitive to initial SfM results. We illustrate the ef-\nfect of the anchor refinement process in Fig. 13, where new\nanchors enhance scene details and fill gaps in large texture-\nless regions and less observed areas.\nTable 6. SSIM scores for Mip-NeRF360 [4] scenes.\nMethod\nScenes\nbicycle\ngarden\nstump\nroom\ncounter\nkitchen\nbonsai\n3D-GS [22]\n0.771\n0.868\n0.775\n0.914\n0.905\n0.922\n0.938\nMip-NeRF360 [4]\n0.685\n0.813\n0.744\n0.913\n0.894\n0.920\n0.941\niNPG [31]\n0.491\n0.649\n0.574\n0.855\n0.798\n0.818\n0.890\nPlenoxels [13]\n0.496\n0.6063\n0.523\n0.8417\n0.759\n0.648\n0.814\nOurs\n0.705\n0.842\n0.784\n0.925\n0.914\n0.928\n0.946\nTable 7. PSNR scores for Mip-NeRF360 [4] scenes.\nMethod\nScenes\nbicycle\ngarden\nstump\nroom\ncounter\nkitchen\nbonsai\n3D-GS [22]\n25.25\n27.41\n26.55\n30.63\n28.70\n30.32\n31.98\nMip-NeRF360 [4]\n24.37\n26.98\n26.40\n31.63\n29.55\n32.23\n33.46\niNPG [31]\n22.19\n24.60\n23.63\n29.27\n26.44\n28.55\n30.34\nPlenoxels [13]\n21.91\n23.49\n20.66\n27.59\n23.62\n23.42\n24.67\nOurs\n24.50\n27.17\n26.27\n31.93\n29.34\n31.30\n32.70\nTable 8. LPIPS scores for Mip-NeRF360 [4] scenes.\nMethod\nScenes\nbicycle\ngarden\nstump\nroom\ncounter\nkitchen\nbonsai\n3D-GS [22]\n0.205\n0.103\n0.210\n0.220\n0.204\n0.129\n0.205\nMip-NeRF360 [4]\n0.301\n0.170\n0.261\n0.211\n0.204\n0.127\n0.176\niNPG [31]\n0.487\n0.312\n0.450\n0.301\n0.342\n0.254\n0.227\nPlenoxels [13]\n0.506\n0.3864\n0.503\n0.4186\n0.441\n0.447\n0.398\nOurs\n0.306\n0.146\n0.284\n0.202\n0.191\n0.126\n0.185\nTable 9. Storage size (MB) for Mip-NeRF360 [4] scenes.\nMethod\nScenes\nbicycle\ngarden\nstump\nroom\ncounter\nkitchen\nbonsai\n3D-GS [22]\n1291\n1268\n1034\n327\n261\n414\n281\nOurs\n248\n271\n493\n133\n194\n173\n258\nTable 10.\nSSIM scores for Tanks&Temples [23] and Deep\nBlending [18] scenes.\nMethod\nScenes\nTruck\nTrain\nDr Johnson\nPlayroom\n3D-GS [22]\n0.879\n0.802\n0.899\n0.906\nMip-NeRF360 [4]\n0.857\n0.660\n0.901\n0.900\niNPG [31]\n0.779\n0.666\n0.839\n0.754\nPlenoxels [13]\n0.774\n0.663\n0.787\n0.802\nOurs\n0.883\n0.822\n0.907\n0.904\n8. Experiments and Results\nAdditional\nData\nPreprocessing.\nWe\nused\nCOLMAP [39] to estimate camera poses and generate\nSfM points for VR-NeRF [51] and BungeeNeRF [49]\ndatasets.\nBoth two datasets are challenging in terms of\nvarying levels of details presented in the captures.\nThe\nVR-NeRF dataset was tested using its eye-level subset with\n3 cameras. For all other datasets, we adhered to the original\n3D-GS [22] method, sourcing them from public resources.\nPer-scene Results.\nHere we list the error metrics used in\nour evaluation in Sec.4 across all considered methods and\nscenes, as shown in Tab. 6-17.\nTraining Process Analysis.\nFigure 14 illustrates the vari-\nations in PSNR during the training process for both train-\ning and testing views. Our method demonstrates quicker\n2\nTraining views\nTesting views\nAmsterdam (BungeeNeRF)\nPompidou (BungeeNeRF)\nTrain (Tanks&Temples)\nCounter (Mip-NeRF360)\n3D-GS\nOurs\nFigure 14. PSNR curve of Scaffold-GS and 3D-GS [22] across diverse datasets [4, 17, 49]. We illustrate the variations in PSNR during\nthe training process for both training and testing views. The orange curve represents Scaffold-GS, while the blue curve corresponds to\n3D-GS. Our method not only achieves rapid convergence but also exhibits superior performance, marked by a significant rise in training\nPSNR and consistently higher testing PSNR, in contrast to 3D-GS.\nTable 11.\nPSNR scores for Tanks&Temples [23] and Deep\nBlending [18] scenes.\nMethod\nScenes\nTruck\nTrain\nDr Johnson\nPlayroom\n3D-GS [22]\n25.19\n21.10\n28.77\n30.04\nMip-NeRF360 [4]\n24.91\n19.52\n29.14\n29.66\niNPG [31]\n23.26\n20.17\n27.75\n19.48\nPlenoxels [13]\n23.22\n18.93\n23.14\n22.98\nOurs\n25.77\n22.15\n29.80\n30.62\nTable 12.\nLPIPS scores for Tanks&Temples [23] and Deep\nBlending [18] scenes.\nMethod\nScenes\nTruck\nTrain\nDr Johnson\nPlayroom\n3D-GS [22]\n0.148\n0.218\n0.244\n0.241\nMip-NeRF360 [4]\n0.159\n0.354\n0.237\n0.252\niNPG [31]\n0.274\n0.386\n0.381\n0.465\nPlenoxels [13]\n0.335\n0.422\n0.521\n0.499\nOurs\n0.147\n0.206\n0.250\n0.258\nTable 13. Storage size (MB) for Tanks&Temples [23] and Deep\nBlending [18] scenes.\nMethod\nScenes\nTruck\nTrain\nDr Johnson\nPlayroom\n3D-GS [22]\n578\n240\n715\n515\nOurs\n107\n66\n69\n63\nTable 14. PSNR scores for Synthetic Blender [30] scenes.\nMethod\nScenes\nMic\nChair\nShip\nMaterials\nLego\nDrums\nFicus\nHotdog\n3D-GS [22]\n35.36\n35.83\n30.80\n30.00\n35.78\n26.15\n34.87\n37.72\nOurs\n37.25\n35.28\n31.17\n30.65\n35.69\n26.44\n35.21\n37.73\nconvergence, enhanced robustness, and better generaliza-\ntion compared to 3D-GS, as evidenced by the rapid increase\nin training PSNR and higher testing PSNR. Specifically, for\nTable 15. Storage size (MB) for Synthetic Blender [30] scenes.\nMethod\nScenes\nMic\nChair\nShip\nMaterials\nLego\nDrums\nFicus\nHotdog\n3D-GS [22]\n50\n116\n63\n35\n78\n93\n59\n44\nOurs\n12\n13\n16\n18\n13\n35\n11\n8\nTable 16.\nPSNR scores for BungeeNeRF [49] and VR-\nNeRF [51] scenes.\nMethod\nScenes\nAmsterdam\nBilbao\nPompidou\nQuebec\nRome\nHollywood\nApartment\nKitchen\n3D-GS [22]\n25.74\n26.35\n21.20\n28.79\n23.54\n23.25\n28.48\n29.40\nOurs\n27.10\n27.66\n25.34\n30.51\n26.50\n24.97\n28.87\n29.61\nTable 17. Storage size (MB) for BungeeNeRF [49] and VR-\nNeRF [51] scenes.\nMethod\nScenes\nAmsterdam\nBilbao\nPompidou\nQuebec\nRome\nHollywood\nApartment\nKitchen\n3D-GS [22]\n1453\n1337\n2129\n1438\n1626\n1642\n202\n323\nOurs\n243\n197\n230\n166\n200\n182\n48\n90\nthe Amsterdam and Pompidou scenes in BungeeNeRF, we\ntrained them with images at three coarser scales and eval-\nuated them at a novel finer scale.\nThe fact that 3D-GS\nachieved higher training PSNR but lower testing PSNR in-\ndicates its tendency to overfit at training scales.\n3\n"
  },
  {
    "title": "StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style Adapter",
    "link": "https://arxiv.org/pdf/2312.00330.pdf",
    "upvote": "9",
    "text": "StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style Adapter\nGongye Liu1\nMenghan Xia2\u2217\nYong Zhang2\nHaoxin Chen2\nJinbo Xing3\nXintao Wang2\nYujiu Yang1* Ying Shan2\n1Tsinghua University\n2Tencent AI Lab\n3CUHK\nProject page:\nhttps://gongyeliu.github.io/StyleCrafter.github.io/\nAbstract\nText-to-video (T2V) models have shown remarkable ca-\npabilities in generating diverse videos. However, they strug-\ngle to produce user-desired stylized videos due to (i) text\u2019s\ninherent clumsiness in expressing specific styles and (ii) the\ngenerally degraded style fidelity.\nTo address these chal-\nlenges, we introduce StyleCrafter, a generic method that en-\nhances pre-trained T2V models with a style control adapter,\nenabling video generation in any style by providing a ref-\nerence image. Considering the scarcity of stylized video\ndatasets, we propose to first train a style control adapter\nusing style-rich image datasets, then transfer the learned\nstylization ability to video generation through a tailor-made\nfinetuning paradigm. To promote content-style disentangle-\nment, we remove style descriptions from the text prompt and\nextract style information solely from the reference image us-\ning a decoupling learning strategy. Additionally, we design\na scale-adaptive fusion module to balance the influences\nof text-based content features and image-based style fea-\ntures, which helps generalization across various text and\nstyle combinations. StyleCrafter efficiently generates high-\nquality stylized videos that align with the content of the texts\nand resemble the style of the reference images. Experiments\ndemonstrate that our approach is more flexible and efficient\nthan existing competitors.\n1. Introduction\nThe popularity of powerful diffusion models has led to re-\nmarkable progress in the field of content generation. For\ninstance, text-to-image (T2I) models are capable of gener-\nating diverse and vivid images from text prompts, encom-\npassing various visual concepts. This great success can be\nattributed not only to the advancement of models but also to\nthe availability of various image data over the Internet. Dif-\nferently, text-to-video (T2V) models fall short of the data\ncategories especially in styles, since existing videos pre-\n*Corresponding author\nStyle\u00a0image\n(a)\u00a0T2I\n(b)\u00a0T2V\n(c)\u00a0T2V\u00a0+\u00a0Style\u00a0adapter\nFigure 1. Effect of adding style adapter to T2V models. (a) and (b)\nare results of Stable Diffusion [33] and VideoCrafter [4]. (c) is the\nresult of VideoCrafter equipped with a style adapter. The content\ntext prompt is \u201dA knight riding a horse through the field\u201d. For (a)\nand (b), the style prompt is generated from the style image using\nGPT4V [29].\ndominantly feature photorealism. While the strategies, like\ninitializing weights from well-trained T2I models or joint\ntraining with image and video datasets, can help mitigate\nthis issue, the generated stylized videos generally suffer\nfrom degraded style fidelity. An example is illustrated in\nFigure 1(b). AnimateDiff [12] can make impressive styl-\nized videos by combining LoRA-tuned T2I models [20] (i.e.\nStable Diffusion) with pre-trained temporal blocks, how-\never, each style requires LoRA finetuning on a small set\nof examples, which is inefficient and unable to support any\nstyle.\nTo address these problems, we propose StyleCrafter, a\ngeneric method that enhances pre-trained T2V models with\na style control adapter, enabling video generation in any\nstyle by providing a reference image. The advantages are\ntwofold: (i) a style image offers stylistic feature guidance,\ncomplementing the stylization capabilities of T2V models\nin a zero-shot fashion; (ii) a reference image delivers a\nmore accurate portrayal of the desired style compared to\ntext prompts. This is evidenced by Figure 1(c). Anyhow,\narXiv:2312.00330v1  [cs.CV]  1 Dec 2023\nit is non-trivial to achieve this goal. On the one hand, the\nstyle control adapter requires to extract accurate style fea-\nture from the reference image in a content-decoupled man-\nner. On the other, the scarcity of stylized videos challenges\nthe adaptation training of the T2V models.\nConsidering the scarcity of stylized video datasets, we\npropose to first train a style adapter to extract desired style\nfeature from images on image datasets, and then transfer\nthe learned stylization ability to video generation through a\ntailor-made finetuning paradigm. To promote content-style\ndisentanglement, we remove style descriptions from the text\nprompt and extract style information solely from the refer-\nence image using a decoupling learning strategy. Particu-\nlarly, we design a scale-adaptive fusion module to balance\nthe influences of text-based content features and image-\nbased style features, which helps generalization across var-\nious text and style combinations. StyleCrafter efficiently\ngenerates high-quality stylized videos that align with the\ncontent of the texts and resemble the style of the reference\nimages. Comprehensive experiments are conducted to as-\nsess our proposed approach, demonstrating that it signifi-\ncantly outperforms existing single-reference based competi-\ntors and even achieves performance comparable to style-\nfinetuning based methods (e.g. Dreambooth [35] and An-\nimateDiff [12]). Furthermore, ablation studies offer a thor-\nough analysis of the technical decisions made in developing\nthe complete method, which provides valuable insights for\nthe community. Our contributions are summarized as fol-\nlows:\n\u2022 We propose the concept of improving stylized generation\nfor pre-trained T2V models by adding a style adapter,\nwhich attains notable advantages over existing baselines.\n\u2022 We explore an efficient style adapter architecture among\nmultiple technical choices, which facilitates the content-\nstyle disentangled generation from text and image inputs.\n\u2022 We propose a training paradigm for generic T2V style\nadapter without requiring large-scale stylized videos.\n2. Related Works\n2.1. Text to Video Synthesis\nText-to-video synthesis(T2V) is a highly challenging task\nwith significant application value, aiming to generate cor-\nresponding videos from text descriptions.\nVarious ap-\nproaches have been proposed, including autoregressive\ntransformer [42] models and diffusion models [15, 28, 40,\n41].\nN\u00a8uwa [47] introduces a 3D transformer encoder-\ndecoder framework to address various text-guided visual\ntasks including T2V generation. Phenaki [43] presents a\nbidirectional masked transformer for compressing videos\ninto discrete tokens, thereby enabling video generation.\nVideo Diffusion Model [17] employs a space-time factor-\nized U-Net to execute the diffusion process in pixel space.\nImagen Video [16] proposes a cascade diffusion model and\nv-parameterization to enhance VDM. Another branch of\ntechniques make good use of pre-trained T2I models and\nfurther introduce some temporal blocks for video genera-\ntion extension. CogVideo [18] builds upon CogView2 [7]\nand employs multi-frame-rate hierarchical training strategy\nto transition from T2I to T2V. Similarly, Make-a-video [38],\nMagicVideo [54] and LVDM [13] inherit pretrained T2I dif-\nfusion models and extends them to T2V generation by in-\ncorporating temporal attention modules.\n2.2. Stylized Image Generation\nStylized image generation aims to create images that ex-\nhibit a specific artistic style. Early research primarily con-\ncentrated on image style transfer, a technique that involves\nthe transfer of one images\u2019s style onto the content of an-\nother, requiring a source image to provide content. Tra-\nditional style transfer methods [44, 51] employ low-level,\nhand-crafted features to align patches between content im-\nages and style images. Since Gatys et al. [10] discovered\nthat the feature maps in CNNs capture style patterns effec-\ntively, a number of studies [2, 6, 22, 26, 27, 52] have been\ndenoted to utilize neural networks to achieve arbitrary style\ntransfer.\nAs the field of generation models progressed, researchers\nbegan exploring stylized image generation for T2I models.\nAlthough T2I models can generate various artistic images\nfrom corresponding text prompts, words are often limited\nto accurately convey the stylistic elements in artistic works.\nConsequently, recent works have shifted towards example-\nguided artistic image generation. Inspired by Textural In-\nversion(TI) [8], InST [53] employs diffusion models for\nboth style transfer and stylized image generation by invert-\ning style images into CLIP spaces. Wang et al. [46] train a\nstyle adapter based on pretrained Stable Diffusion to adapt\ndiffusion models to style conditions. DreamStyler [1] pro-\npose to optimize a multi-stage textual embedding with a\ncontext-aware text prompt.\n2.3. Stylized Video Generation\nBuilding upon the foundation of stylized image genera-\ntion, researchers have extended the concept to video style\ntransfer and stylized video generation. Due to the scarcity\nof large-scale stylized video data, a common approach for\nvideo stylization involves applying image stylization tech-\nniques on a frame-by-frame basis. Early methods of video\nstyle transfer [3, 9, 34] apply style transfer frame by frame\nin video sequences, generating stable stylized video se-\nquences through the use of optical flow constraints. Ad-\nditionally, Some video editing methods [11, 21, 23, 31, 48,\n49] based on pretrained T2I models also support text-guided\nvideo style transfer. Although these methods effectively im-\nprove temporal consistency, they often fail to handle frames\nwith a large action span. The reliance on a source video\nalso undermines the flexibility of stylized video genera-\ntion. VideoComposer [45] focuses on controllable video\ngeneration, allowing multiple conditional input to govern\nthe video generation, including structure, motion, style, etc.\nAnimateDiff [12] employs a T2I model as a base genera-\ntor and adds a motion module to learn motion dynamics,\nwhich enables extending the success of personalized T2I\nmodels(e.g., LoRA [19], Dreambooth [35]) to video ani-\nmation. However, the dependence on a personalized model\nrestricts its ability to generate videos with arbitrary styles.\n3. Method\nWe propose a method to equip pre-trained Text-to-Video\n(T2V) models with a style adapter, allowing for the gen-\neration of stylized videos based on both a text prompt and a\nstyle reference image. The overview diagram is illustrated\nin Figure 2. In this framework, the textual description dic-\ntates the video content, while the style image governs the\nvisual style, ensuring a disentangled control over the video\ngeneration process. Given the limited availability of styl-\nized videos, we employ a two-stage training strategy. Ini-\ntially, we utilize an image dataset abundant in artistic styles\nto learn reference-based style modulation. Subsequently,\nadaptation finetuning on a mixed dataset of style images\nand realistic videos is conducted to improve the temporal\nquality of the generated videos.\n3.1. Reference-Based Style Modulation\nOur style adapter serves to extract style features from the\ninput reference image and infuse them into the backbone\nfeatures of the denoising U-Net. As mainstream T2V mod-\nels are generally trained with image and video datasets, they\nsupport not only text-to-video generation but also text-to-\nimage generation.\nTo overcome the scarcity of stylized\nvideos, we propose to train the style adapter based on a\npre-trained T2V model (i.e. the open-source video diffu-\nsion model VideoCrafter [4]) for stylized image generation\nunder the supervision of a stylistic image dataset.\nContent-Style Decoupled Data Augmentation.\nWe use\nthe stylistic images from two publicly available dataset, i.e.\nWikiArt [30] and a subset of Laion-Aethetics [36] (aethetics\nscore above 6.5). In the original image-caption pairs, we ob-\nserve that the captions generally contains both content and\nstyle descriptions, and some of them do not match the im-\nage content well. To promote the content-style decoupling,\nwe use BLIP-2 [25] to regenerate captions for the images\nand remove certain forms of style description (e.g., a paint-\ning of) with regular expressions. In addition, as an image\ncontains both style and content information, it is necessary\nto construct a decoupling supervision strategy to guarantee\n\ud835\udc33\u0bcd\n\ud835\udc33\u0b34\nDecoder\nCLIP\u00a0Text\u00a0\nEncoder\n\ud835\udc33\u0bcd\u0b3f\u0b35\nText\u00a0CA\n\ud835\udc05\u0bdc\u0be1\n\u0bdc\n\ud835\udc60\u0bdc\n\ud835\udc05\u0be6\n\ud835\udc05\u0be7\n\ud835\udc05\u0be2\u0be8\u0be7\n\u0bdc\nDenoising\u00a0U\u2010Net\n\u22ef\n\u22ef\n\u22ef\nCLIP\u00a0Image\u00a0\nEncoder\nQuery\nTransformer\n\u201ca\u00a0chef\u00a0is\u00a0cooking\u201d\nStyle\u00a0Query\nStyle\u00a0Image\nSampling\nContext\u2010aware Scale Factor\u00a0Prediction\nQuery\nTransformer\nMLP\n\u22ef\n\u22ef\n\ud835\udc05\u0be7\n\ud835\udc05\u0be6\n\ud835\udc05\u0be6\n\ud835\udc05\u0be7\nScale\u00a0Query\nDual\u00a0Cross\u2010Attention\n\ud835\udc94\nStyle\u00a0CA\nLN\nFigure 2. Overview of our proposed style adapter. It consists of\nthree components, i.e. style feature extractor, dual cross-attention\nmodule, and context-aware scale factor predictor.\nthe extracted style feature free of content features. We re-\ngard that every local regions of a stylistic image share the\nsame style representation, which not only reflects on texture\nand color theme but also on the structure and perceptual se-\nmantics. Based on this insight, we process each stylistic\nimage to obtain the target image and style image through\ndifferent strategies: for target image, we scale the shorter\nside of the image to 512 and then crop the target content\nfrom the central area; for style image, we scale the shorter\nside of the image to 800 and randomly crop a local patch.\nThis approach reduces the overlap between the style refer-\nence and generation target, while still preserves the global\nstyle semantics complete and consistent.\nStyle Embedding Extraction.\nCLIP [32] has demon-\nstrated remarkable capability in extract high-level semantics\nfrom open-domain images. In order to capitalize on this ad-\nvantage, we employ a pre-trained CLIP image encoder as\na feature extractor. Specifically, we utilize both the global\nsemantic token and the full 256 local tokens (i.e., from the\nfinal layer of the Transformer) since our desired style em-\nbedding should not only serve as an accurate style trigger\nfor the T2V model, but also provide auxiliary feature ref-\nerences. As the image tokens encompass both style and\ncontent information, we further employ a trainable Query\nTransformer (Q-Former) [25] to extract the style embedding\nFs. We create N learnable style query embeddings as in-\nput for the Q-Former, which interact with image features\nthrough self-attention layers. Note that, this is a commonly\nadopted architecture for visual condition extraction [37, 50].\nBut, it is the style-content fusion mechanism that makes our\nStyle\u00a0ID\nPrompt\u00a0ID\n0\n1\n2\n3\n4\n5\n6\n7\nFigure 3. Illustration of content-style fusion scale factors across\nmultiple input pairs. Four short prompts(less than 5 words) with\nprompt id \u2208 [0, 3] and four long prompts(more than 8 words) with\nprompt id \u2208 [4, 7] are randomly selected. Results indicate that\nshorter prompts and images with richer style-semantics tend to\nhave relatively higher scale factors.\nproposed design novel and insightful for style modulation,\nas detailed below.\nAdaptive Style-Content Fusion.\nWith the extracted style\nembedding, there are two ways to combine the style and\ntext conditions, including (i) attach-to-text: attach the style\nembedding to the text embedding and then interact with\nthe backbone feature via the originally text-based cross-\nattention as a whole; (ii) dual cross-attention: adding a new\ncross-attention module for the style embedding and then\nfuse the text-conditioned feature and style-conditioned fea-\nture. According to our experiment (see Sec. 4.4), solution\n(ii) surpasses solution (i) in disentangling the roles of text\nand style conditions, helpful to making the generated video\nalign with the content of the texts and resemble the style of\nthe reference image. The formula can be written as:\nFi\nout = TCA(Fi\nin, Ft) + si \u2217 LN(SCA(Fi\nin, Fs)),\n(1)\nwhere Fi\nin denotes the backbone feature of layer i, LN de-\nnotes layer normalization, and TCA and SCA denote text-\nbased cross attention and style-based cross attention respec-\ntively. si is a scale factor learned by a context-aware scale\nfactor prediction network, to balance the magnitudes of\ntext-based feature and style-based feature. The motivation\nis that different stylistic genres may have different emphasis\non content expression. For example, the abstract styles tend\nto diminish the concreteness of the content, while realism\nstyles tend to highlight the accuracy and specificity of the\ncontent. So, we propose a context-aware scale factor pre-\ndiction network to predict the fusion scale factors according\nto the text and style image. Specifically, we create a learn-\nable factor query, it interact with textual features Ft and\nstyle feature Fs to generate scale features via a Q-Former\nand then is projected into layer-wise scale factors s \u2208 R16.\nFigure 3 illustrates the learned scale factors across multiple\ncontext-style inputs. It shows that the adaptive scale factors\nhave a strong correlation with style genres while also de-\npend on the text prompts. Style references with rich style-\nsemantics(i.e., ukiyo-e style) typically yield higher scale\nfactors to emphasize style; while complex prompts tend to\nproduce lower scale factors to enhance content control. This\nis consistent with our hypothesis to motivate our design.\n3.2. Temporal Adaptation to Stylized Features\nGiven a pre-trained T2V model, the style adapter trained\non image dataset works well for stylized image generation.\nHowever, it still struggles to generate satisfactory stylized\nvideos, which is vulnerable to temporal jittering and visual\nartifacts. The possible causes are that the cross-frame oper-\nations, i.e. temporal self-attention, do not involve in the pro-\ncess of stylized image generation and thus induce incompat-\nible issues. So, it is necessary to finetune the temporal self-\nattention with the style adapter incorporated. Following the\npractice of T2V image and video joint training, the finetun-\ning is performed on the mixed datasets of stylistic images\nand photorealistic videos. This is an adaptation training of\ntemporal blocks and the model converges efficiently.\nClassifier-Free Guidance for Multiple Conditions.\nUn-\nlike T2I models, video models exhibit higher sensitivity to\nstyle guidance due to their limited stylized generation capa-\nbilities. Using a unified \u03bb for both style and context guid-\nance may lead to undesirable generation results. Regard-\ning this, we adopt a more flexible mechanism for multi-\nple conditions classifier-free guidance. Building upon the\nvanilla text-guided classifier-free guidance, which controls\ncontext alignment by contrasting textual-conditioned distri-\nbution \u03f5(zt, ct) with unconditional distribution \u03f5(zt, \u2205), we\nintroduce style guidance with \u03bbs by emphasizing the differ-\nence between the text-style-guided distribution \u03f5(zt, ct, cs)\nand the text-guided distribution \u03f5(zt, ct). The complete for-\nmulation is as below:\n\u02c6\u03f5(zt, ct, cs) = \u03f5(zt, \u2205) + \u03bbs(\u03f5(zt, ct, cs) \u2212 \u03f5(zt, ct))\n+ \u03bbt(\u03f5(zt, ct) \u2212 \u03f5(zt, \u2205)),\n(2)\nwhere ct and cs denote textual and style condition respec-\ntively. \u2205 denotes using no text or style conditions. In our\nexperiment, we follow the recommended configuration of\ntext guidance in VideoCrafter[4], setting \u03bbt = 15.0, while\nthe style guidance is configured with \u03bbs = 7.5 empirically.\n4. Experimental Results\n4.1. Experimental settings\nImplementation Details.\nWe adopt the open-sourced\nVideoCrafter [4] as our base T2V model. We first train the\nstyle modulation on image dataset, i.e. WikiArt [30] and\nLaion-Aethetics-6.5+ [36] for 40k steps with a batch size of\nStyle\u00a0Reference\n(a)\u00a0DreamBooth\n(b)\u00a0CustomDiffusion\n(c)\u00a0InST\n(d)\u00a0SD*\n(e)\u00a0Ours\nFigure 4. Visual comparison on style-guided T2I generation. All the methods are tested with four styles and three content texts: (i) A\nperson jogging along a scenic trail; (ii) A colorful butterfly resting on a flower; (iii) An ancient temple surrounded by lush vegetation.\n256. In the second stage, we froze the style modulation part\nand only train temporal blocks of VideoCrafter, we jointly\ntrain image datasets and video datasets(WebVid-10M) for\n20k steps with a batch size of 8 on video data and 128 on\nimage data, sampling image batches with a ratio of 20%.\nTesting Datasets.\nTo evaluate the effectiveness and gen-\neralizability of our method, we construct a test set com-\nprising content prompts and style references. For content\nprompts, we use GPT-4 [29] to generate recognizable tex-\ntual descriptions from four meta-categories(human, animal,\nobject, and landscape). We manually filter out low-quality\nprompts, retaining 20 image prompts and 12 video prompts.\nFor style references, we collect 20 diverse single-reference\nstylized images and 8 sets of style images with multi-\nreference(each contains 5 to 7 images in similar styles) from\nthe Internet. In total, the test set contains 400 pairs for styl-\nized image generation, and 300 pairs for stylized video gen-\neration (240 single-reference pairs and 60 multi-reference\npairs). Details are available in the supplementary materials.\nEvaluation Metrics.\nFollowing the practice of previous\nworks [39, 46, 53], we employ CLIP-based scores to mea-\nsure the (i) text alignment and (ii) style conformity. Addi-\ntionally, we measure the (iii) temporal consistency of video\ngeneration by calcuating clip scores between contiguous\nframes. Note that these metrics are not perfect. For ex-\nample, one can easily achieve a close-to-1 style score by\nentirely replicating the style reference. Similarly, stylized\nMethods\nDreambooth [35]\nCustomDiffsion [24]\nInST [53]\nSD* [33]\nOurs\nText \u2191\n0.3047\n0.3119\n0.3004\n0.2766\n0.2729\nStyle \u2191\n0.3459\n0.3663\n0.3708\n0.4183\n0.5171\nTable 1. Quantitative comparison on single-reference style-guided\nT2I generation.\nWe evaluate the CLIP scores for text align-\nment(Text) and style Conformity(Style) on a test set of 400\npairs. Bold: Best.\nresults may yield inferior text scores compared to realistic\nresults, even though both accurately represent the content\ndescribed in the text. As a supplement, we conduct the user\npreference study to make a subjective assessment based on\ncontent alignment, style conformity, and temporal quality.\nDetails can be found in the supplementary materials.\n4.2. Style-Guided Text-to-Image Generation\nAs mentioned in Sec. 3.1, our proposed method also sup-\nports pre-trained T2V models to generate stylized images.\nWe are interested to evaluate our method against state-\nof-the-art style-guided T2I synthesis methods, which is\nbetter-established than its video counterpart.\nThe com-\npetitors includes DreamBooth [35], CustomDiffusion [24],\nInST [53], and SD*(Stable Diffusion [33] equipped with\nGPT-4V [29]). The evaluation is conducted on the test set\ncomprising 400 text-style pairs as introduced in Sec. 4.1.\nFor each style, DreamBooth and CustomDiffusion are op-\ntimized with the provided single reference image to learn\nthe customized concept of style. The quantitative compar-\nison is tabulated in Table 1, showing that our method sur-\n\u201cA chef\u00a0preparing\u00a0meals\u00a0in\u00a0kitchen\u201d\nVideoCrafter*\nOurs\n\u201cA\u00a0wolf\u00a0walking\u00a0stealthily\u00a0through\u00a0the\u00a0forest\u201d\nGen\u20102*\nVideoComposer\nVideoCrafter*\nOurs\nGen\u20102*\nVideoComposer\nFigure 5. Qualitative comparison of single-reference style-guided T2V generation on various styles and text prompts.\npasses all the competitors in style conformity. As discussed\nin Sec. 4.1, the text alignment is measured by the similar-\nity between content text embedding and stylized image em-\nbedding, the stylistic appearance actually hinders the metric\nin some extents, which makes those methods (like Dream-\nBooth and CustomDiffusion) with weak stylistic effects\n(i.e. close to photorealism) achieve superior scores. This\ncould be observed from the qualitative comparison illus-\ntrated in Figure 4. Dreambooth[35], CustomDiffusion[24],\nand InST[53] struggle to accurately capture the style from\nvarious style references and are exhibit low style confor-\nmity. SD* demonstrates good stylistic ability but still fails\nto reproduce the style of the reference image, possibly be-\ncause the text\u2019s inherent clumsiness in expressing specific\nstyles despite utilizing the powerful GPT4V for visual style\nunderstanding. In contrast, our method efficiently generates\nhigh-quality stylized images that align with the content of\nthe texts and resemble the style of the reference image. Our\nmethod demonstrates stable stylized generation capabilities\nwhen dealing with different types of prompts.\n4.3. Style-Guided Text-to-Video Generation\nExisiting approaches for style-guided video generation can\nbe divided into two categories: one is the single-reference\nbased methods that are usually tuning-free, e.g. VideoCom-\nposer [45]; the other is the multi-reference based methods\nthat generally requires multiple images of the target style\nfor fine-tuning, e.g. AnimateDiff [12]. We make compari-\nson with these methods respectively. Apart from the qual-\nity metrics, we further conduct a user study to evaluate the\nstylized video results, including text alignment, style con-\nformity and the temporal quality.\nSingle-Reference\nbased\nGuidance.\nVideoCom-\nposer [45] is a controllable video generation model\nMethods\nCLIP scores\nUser Preferences\nVid.Com. [45]\nVid.Craf.* [4]\nOurs\nVid.Craf.* [4]\nGen-2* [5]\nOurs\nText \u2191\n0.0468\n0.2209\n0.2726\n39.1%\n74.7%\n84.4%\nStyle \u2191\n0.7306\n0.3124\n0.4531\n8.0%\n23.1%\n68.9%\nTemporal \u2191\n0.9853\n0.9757\n0.9892\n4.4%\n51.1%\n44.4%\nTable 2. Quantitative Comparison on style-guided T2V genera-\ntion. We evaluate the CLIP scores and user preferences for image-\ntext alignment(Text), style conformity(Style), and temporal\nquality(Temporal) on a test set of 240 pairs. Vid.Com.: Video-\nComposer, Vid.Craf.: VideoCrafter. Bold: best.\nthat allows multiple conditional input including style\nreference image. It is a natural competitor of our method.\nBesides, we further construct two additional comparative\nmethods, i.e.\nVideoCrafter* and GEN2*, which extend\nVideoCrafter [4] and GEN2 [5], the state-of-the-art T2V\nmodels in open-source and close-source channels respec-\ntively, to make use of style reference images by utilizing\nGPT-4V[29] to generates style prompts from them. The\nevaluation is conducted on 240 text-style pairs, as intro-\nduced in Sec. 4.1. The quantitative comparison is tabulated\nin Table 2. Several typical visual examples are illustrated\nin Figure 5. We can observe that VideoComposer tends\nto copy content from style references and struggles to\ngenerate text-aligned content, which is possibly because\nof the invalid decoupling learning.\nConsequently, its\nresults exhibit abnormally high style conformity and\nvery low text alignment.\nIn comparison, our method\nstrikes a better balance between text alignment and style\nconformity, enabling more desirable generation results. In\naddition, VideoCrafter* exhibits limited stylized generation\ncapabilities, producing videos with diminished style and\ndisjointed movements.\nGen-2* demonstrates superior\nstylized generation capabilities. However, GEN-2 is still\nlimited by the inadequate representation of style in textual\n\u201cA wooden\u00a0sailboat\u00a0docked\u00a0in\u00a0a\u00a0harbor\u201d\nAnimateDiff\nOurs(S\u2010R)\n\u201cA\u00a0student\u00a0walking\u00a0to\u00a0school\u00a0with\u00a0backpack\u201d\nOurs(M\u2010R)\nAnimateDiff\nOurs(S\u2010R)\nOurs(M\u2010R)\nFigure 6. Qualitative comparison of multi-reference style-guided T2V generation on various styles and text prompts.\nMethods\nCLIP scores\nUser Preferences\nAnimateDiff\nOurs (S-R)\nOurs (M-R)\nAnimateDiff\nOurs(M-R)\nText \u2191\n0.2867\n0.2661\n0.2634\n64.7%\n90.7%\nStyle \u2191\n0.3528\n0.4803\n0.4997\n10.0%\n90.0%\nTemporal \u2191\n0.8903\n0.9851\n0.9852\n19.3%\n80.7%\nTable 3. Quantitative comparison on multi-reference style-guided\nT2V generation. We evaluate the CLIP scores and user prefer-\nences for image-text alignment(Text), style conformity(Style),\nand temporal quality(Temporal) on a test set of 60 pairs. S-R:\nSingle-Reference, M-R: Multi-Reference, Bold: best\ndescriptions, and is more prone to sudden changes in color\nand luminance. In comparison, our method captures styles\nmore effectively and reproduces them in the generated\nresults.\nMulti-Reference based Guidance.\nAnimateDiff [12] de-\nnotes a paradigm to turn personalized-SD (i.e. SD fine-\ntuned on specific-domain images via LoRA [19] or Dream-\nbooth [35]) for video generation, namely combined with\npre-trained temporal blocks of T2V models. It can generate\nvery impressive results if the personalized-SD is carefully\nprepared, however, we find it struggles to achieve as satis-\nfactory results if only a handful style reference images are\navailable for training. We conduct evaluation on 60 text-\nstyle pairs with multi-references, as presented in Sec.4.1.\nWe train Dreambooth [35] models for each style and incor-\nporate them into AnimateDiff based on their released code-\nbase. Thanks to the flexibility of Q-Former, our method also\nsupports multiple reference images in a tuning-free fashion,\ni.e. computing the image embeddings of each reference im-\nage and concatenating all embeddings as input to the Q-\nFormer. The quantitative and qualitative results are com-\npared in Table 3 and Figure 6 respectively. According to the\nMethods\nContent Alignment \u2191\nStyle Conformity \u2191\nOurs\n0.2729\n0.5171\nw/o Data Augmentation\n0.2967\n0.4297\nw/o Dual Cross-Attention\n0.0983\n0.7332\nw/o Adaptive Fusion\n0.2612\n0.5263\nTable 4. Ablation studies on style modulation designs. The per-\nformance is evaluated based on the style-guided T2I generation.\nresults, AnimateDiff shows weakness in two aspects: (i) it\nstruggles to achieve high-fidelity stylistic appearance while\ntends to generate close-to-realism results despite the style\nreferences are typical artistic styles; (ii) it is vulnerable to\ntemporal artifacts. As the trained Personalized-SD can gen-\nerate decent stylistic images (provided in the supplementary\nmaterials), we conjecture that the performance degradation\nis caused by the incompatibility from the pre-trained tempo-\nral blocks and independently trained personalized-SD mod-\nels, which not only interrupts temporal consistency but also\nweakens the stylistic effect. In contrast, our method can\ngenerate temporal consistent videos with high style confor-\nmity to the reference images and accurate content alignment\nwith the text prompts. Furthermore, using multiple refer-\nences can further promoter the performance of our method,\nwhich offers additional advantages in practical applications.\n4.4. Ablation Study\nWe make ablation studies on some important designs of\nour method, including data augmentation, module architec-\ntures, and training strategies, to validate their effectiveness.\nData Augmentation.\nWe first study the effectiveness of\ncontent-style decoupled data augmentation. As depicted in\nTable 4, training with the original image-caption pairs re-\nstricts the model\u2019s ability to extract style representations,\nw/o\u00a0Data\u00a0Aug\nOurs\n(i)\u00a0\n(ii)\u00a0\n(iii)\u00a0\nw/o\u00a0Dual\u00a0CA\n(iii)\u00a0\u00a0\u201cA\u00a0bouquet of\u00a0flowers\u00a0\nin\u00a0a\u00a0vase\u201d\n(i)\u00a0\u00a0\u201cA woman\u00a0reading\u00a0a\u00a0\nbook\u00a0in\u00a0a\u00a0park\u201d\n(ii)\u00a0\u00a0\u201cA\u00a0bear\u00a0fishing\u00a0in\u00a0a\u00a0river\u201d\nFigure 7. Visual Comparison on the effects of dual cross-attention\nand data augmentation. Removing dual cross-attention tends to\nfuse the content from the reference image while using no data aug-\nmentation fails to capture the \u201d3D render\u201d style characteristics.\nOurs\u00a0w/o\u00a0Adaptive\u00a0Fusion\nOurs\n(i)\n(ii)\n(i)\n(ii)\nFigure 8. Visual inspection on the effect of adaptive content-style\nfusion. It shows superiority in generalization to extreme cases of\ninput, e.g. long text description. Two text prompts are used: (i)\nA little girl; (ii) A little girl reading a book in the park, with a\ntelescope nearby pointed at the sky.\nleading to lower style conformity. For example, as shown\nin Figure 7, method without data augmentation fails to cap-\nture the \u201d3D render\u201d style from the reference image.\nDual Cross-Attention.\nAs discussed in Sec. 3.1, there are\ntwo mechanism to combine the newly added style condition\nwith the native textual condition, i.e. attach-to-text and dual\ncross-attention. Here we make a comparison to study their\neffects, and the results are presented in Table 4 and Figure 7.\nIt shows that attach-to-text tends to directly fuse the content\nfrom the reference image and the text prompts rather than\ncombining the text-based content and image-based style.\nThis indicates the effectiveness of dual cross-attention in\nfacilitating content-style decoupling.\nAdaptive Style-Content Fusion.\nAs previously dis-\ncussed in Figure 3, our proposed adaptive style-content fu-\nsion module demonstrates effectiveness in adaptively pro-\ncessing various conditional context. It benefits the gener-\nalization ability of model to deal with diverse combination\nof content text and style image. Figure 8 reveals that al-\nMethods\nContent Alignment \u2191\nStyle Conformity \u2191\nTemporal consistency \u2191\nStyle adapter training only\n0.2691\n0.3923\n0.9612\nJoint training\n0.3138\n0.2226\n0.9741\nTwo-stage (ours)\n0.2726\n0.4531\n0.9892\nTable 5. Ablation study on our two-stage training scheme.\n\u201cA\u00a0Student\u00a0walking\u00a0\nto\u00a0school\u00a0with\u00a0\nbackpack\u201d\nOurs\nJoint\u00a0training\nStyle\u00a0adapter\u00a0training\u00a0only\nFigure 9. Comparison on the effects of different training schemes.\nthough the baseline cam handle short prompt inputs such\nas \u201dA little girl\u201d, it struggles to accurately generate all ob-\njects described in longer prompts. In contrast, the adaptive\nfusion module can achieve decent text alignment for long\ntext descriptions thanks to its flexibility to adaptive balance\nbetween text prompt and style reference.\nTwo-Stage Training Scheme.\nOur proposed training\nscheme consists of two stages, i.e. style adapter training\nand temporal adaption. To show its necesscity, we build\ntwo baselines: (i) style adapter training only: that we train\na style adapter on image data and apply it directly to styl-\nized video generation without any additional finetuning; (ii)\njoint training: that we conduct style adapter training and\ntemporal blocks finetuning on image-video dataset simulta-\nneously. As depicted in Figure 9 and Table 9, baseline (i)\nexhibits inferior temporal consistency when applied directly\nto video, and sometimes undermines the content alignment\nand style conformity. As for baseline (ii), the learning of\nstyle embedding extraction seems to be interfered by the\njoint finetuning of temporal blocks, which impedes it to\ngenerate desirable stylized videos.\n5. Conclusion and Limitations\nWe have presented StyleCrafter, a generic method enabling\npre-trained T2V model for video generation in any style\nby providing a reference image. To achieve this, we made\nexploration in three aspects, including the architecture of\nstyle adapter, the content and style feature fusion mecha-\nnism, and some tailor-made strategies for data augmenta-\ntion and training stylized video generation without stylis-\ntic video data. All of these components allows our method\nto generate high-quality stylized videos that align with text\nprompts and conform to style references. Extensive exper-\niments have evidenced the effectiveness of our proposed\ndesigns and comparisons with existing competitors demon-\nstrate the superiority of our method in visual quality, effi-\nciency and flexibility. Anyway, our method also has certain\nlimitations, e.g., unable to generate desirable results when\nthe reference image can not represent the target style suffi-\nciently or the presented style is extremely unseen. Further\nexplorations are demanded to address those issues.\nReferences\n[1] Namhyuk Ahn, Junsoo Lee, Chunggi Lee, Kunhee Kim,\nDaesik Kim, Seung-Hun Nam, and Kibeom Hong. Dream-\nstyler: Paint by style inversion with text-to-image diffusion\nmodels. arXiv preprint:2309.06933, 2023. 2\n[2] Jie An, Siyu Huang, Yibing Song, Dejing Dou, Wei Liu, and\nJiebo Luo. Artflow: Unbiased image style transfer via re-\nversible neural flows. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n862\u2013871, 2021. 2\n[3] Dongdong Chen, Jing Liao, Lu Yuan, Nenghai Yu, and Gang\nHua. Coherent online video style transfer. In Proceedings\nof the IEEE International Conference on Computer Vision,\npages 1105\u20131114, 2017. 2\n[4] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang,\nXiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu,\nQifeng Chen, Xintao Wang, Chao Weng, and Ying Shan.\nVideocrafter1: Open diffusion models for high-quality video\ngeneration. preprint arXiv:2310.19512, 2023. 1, 3, 4, 6\n[5] Gen-2 contributors.\nGen-2.\nGen-2. Accessed Nov. 1,\n2023 [Online] https://research.runwayml.com/\ngen2. 6, 1, 4\n[6] Yingying Deng, Fan Tang, Weiming Dong, Chongyang Ma,\nXingjia Pan, Lei Wang, and Changsheng Xu. Stytr2: Im-\nage style transfer with transformers.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 11326\u201311336, 2022. 2\n[7] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang.\nCogview2: Faster and better text-to-image generation via\nhierarchical transformers. Advances in Neural Information\nProcessing Systems, 35:16890\u201316902, 2022. 2\n[8] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\nAmit H Bermano, Gal Chechik, and Daniel Cohen-Or. An\nimage is worth one word: Personalizing text-to-image gen-\neration using textual inversion. arXiv preprint:2208.01618,\n2022. 2\n[9] Wei Gao, Yijun Li, Yihang Yin, and Ming-Hsuan Yang. Fast\nvideo multi-style transfer. In Proceedings of the IEEE/CVF\nwinter conference on applications of computer vision, pages\n3222\u20133230, 2020. 2\n[10] Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Im-\nage style transfer using convolutional neural networks. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 2414\u20132423, 2016. 2\n[11] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel.\nTokenflow: Consistent diffusion features for consistent video\nediting. arXiv preprint:2307.10373, 2023. 2\n[12] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\npersonalized text-to-image diffusion models without specific\ntuning. arXiv preprint:2307.04725, 2023. 1, 2, 3, 6, 7, 4\n[13] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and\nQifeng Chen.\nLatent video diffusion models for high-\nfidelity video generation with arbitrary lengths.\narXiv\npreprint:2211.13221, 2022. 2\n[14] Yingqing He, Menghan Xia, Haoxin Chen, Xiaodong Cun,\nYuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao\nWeng, Ying Shan, et al.\nAnimate-a-story:\nStorytelling\nwith retrieval-augmented video generation. arXiv preprint\narXiv:2307.06940, 2023. 6\n[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. In Advances in Neural Informa-\ntion Processing Systems (NeurIPS), pages 6840\u20136851, 2020.\n2\n[16] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint:2210.02303, 2022. 2\n[17] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William\nChan, Mohammad Norouzi, and David J Fleet. Video dif-\nfusion models. arXiv:2204.03458, 2022. 2\n[18] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and\nJie Tang. Cogvideo: Large-scale pretraining for text-to-video\ngeneration via transformers.\narXiv preprint:2205.15868,\n2022. 2\n[19] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint:2106.09685, 2021. 3, 7, 1\n[20] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. In In-\nternational Conference on Learning Representations (ICLR),\n2022. 1\n[21] Nisha Huang, Yuxin Zhang, and Weiming Dong. Style-a-\nvideo: Agile diffusion for arbitrary text-based video style\ntransfer. arXiv preprint:2305.05464, 2023. 2\n[22] Xun Huang and Serge Belongie. Arbitrary style transfer in\nreal-time with adaptive instance normalization. In Proceed-\nings of the IEEE international conference on computer vi-\nsion, pages 1501\u20131510, 2017. 2\n[23] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nvosyan,\nRoberto\nHenschel,\nZhangyang\nWang,\nShant\nNavasardyan, and Humphrey Shi. Text2video-zero: Text-to-\nimage diffusion models are zero-shot video generators. arXiv\npreprint:2303.13439, 2023. 2\n[24] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli\nShechtman, and Jun-Yan Zhu. Multi-concept customization\nof text-to-image diffusion. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 1931\u20131941, 2023. 5, 6, 1\n[25] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models.\narXiv\npreprint arXiv:2301.12597, 2023. 3, 1\n[26] Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu,\nand Ming-Hsuan Yang. Universal style transfer via feature\ntransforms. Advances in neural information processing sys-\ntems, 30, 2017. 2\n[27] Songhua Liu, Tianwei Lin, Dongliang He, Fu Li, Meiling\nWang, Xin Li, Zhengxing Sun, Qian Li, and Errui Ding.\nAdaattn: Revisit attention mechanism in arbitrary neural\nstyle transfer. In Proceedings of the IEEE/CVF international\nconference on computer vision, pages 6649\u20136658, 2021. 2\n[28] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\ndenoising diffusion probabilistic models.\nIn International\nConference on Machine Learning (ICML), pages 8162\u2013\n8171. PMLR, 2021. 2\n[29] OpenAI. Gpt-4v(ision) system card. Technical report, 2023.\n1, 5, 6, 3\n[30] Fred Phillips and Brandy Mackintosh. Wiki art gallery, inc.:\nA case for critical thinking. Issues in Accounting Education,\n26(3):593\u2013608, 2011. 3, 4\n[31] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,\nXintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fus-\ning attentions for zero-shot text-based video editing. arXiv\npreprint:2303.09535, 2023. 2\n[32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In International Conference on Machine Learning\n(ICML), 2021. 3\n[33] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, 2022. 1, 5\n[34] Manuel Ruder, Alexey Dosovitskiy, and Thomas Brox.\nArtistic style transfer for videos.\nIn Pattern Recognition:\n38th German Conference, GCPR 2016, Hannover, Ger-\nmany, September 12-15, 2016, Proceedings 38, pages 26\u201336.\nSpringer, 2016. 2\n[35] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. arXiv preprint:2208.12242, 2022. 2, 3, 5, 6,\n7, 1\n[36] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. Advances in Neural In-\nformation Processing Systems, 35:25278\u201325294, 2022. 3,\n4\n[37] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instant-\nbooth: Personalized text-to-image generation without test-\ntime finetuning. arXiv preprint arXiv:2304.03411, 2023. 3\n[38] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint:2209.14792, 2022. 2\n[39] Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro\nChin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang,\nGlenn Entis, Yuanzhen Li, et al. Styledrop: Text-to-image\ngeneration in any style. arXiv preprint arXiv:2306.00983,\n2023. 5, 4\n[40] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. In In International Conference\non Learning Representations (ICLR), 2021. 2\n[41] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. In In International Conference on Learning Repre-\nsentations (ICLR), 2021. 2\n[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 2\n[43] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kin-\ndermans, Hernan Moraldo, Han Zhang, Mohammad Taghi\nSaffar, Santiago Castro, Julius Kunze, and Dumitru Erhan.\nPhenaki: Variable length video generation from open domain\ntextual description. arXiv preprint:2210.02399, 2022. 2\n[44] Bin Wang, Wenping Wang, Huaiping Yang, and Jiaguang\nSun. Efficient example-based painting and synthesis of 2d\ndirectional texture. IEEE Transactions on Visualization and\nComputer Graphics, 10(3):266\u2013277, 2004. 2\n[45] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Ji-\nuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jin-\ngren Zhou. Videocomposer: Compositional video synthe-\nsis with motion controllability. arXiv preprint:2306.02018,\n2023. 3, 6, 4\n[46] Zhouxia Wang, Xintao Wang, Liangbin Xie, Zhongang Qi,\nYing Shan, Wenping Wang, and Ping Luo.\nStyleadapter:\nA single-pass lora-free model for stylized image generation.\narXiv preprint:2309.01770, 2023. 2, 5\n[47] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang,\nDaxin Jiang, and Nan Duan. N\u00a8uwa: Visual synthesis pre-\ntraining for neural visual world creation. In European con-\nference on computer vision, pages 720\u2013736. Springer, 2022.\n2\n[48] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian\nLei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou. Tune-a-video: One-shot tuning\nof image diffusion models for text-to-video generation. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 7623\u20137633, 2023. 2\n[49] Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change\nLoy. Rerender a video: Zero-shot text-guided video-to-video\ntranslation. arXiv preprint:2306.07954, 2023. 2\n[50] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-\nadapter: Text compatible image prompt adapter for text-to-\nimage diffusion models. arXiv preprint arXiv:2308.06721,\n2023. 3\n[51] Wei Zhang, Chen Cao, Shifeng Chen, Jianzhuang Liu, and\nXiaoou Tang. Style transfer via image component analysis.\nIEEE Transactions on multimedia, 15(7):1594\u20131601, 2013.\n2\n[52] Yuxin Zhang, Fan Tang, Weiming Dong, Haibin Huang,\nChongyang Ma, Tong-Yee Lee, and Changsheng Xu. Do-\nmain enhanced arbitrary image style transfer via contrastive\nlearning. In ACM SIGGRAPH 2022 Conference Proceed-\nings, pages 1\u20138, 2022. 2\n[53] Yuxin Zhang, Nisha Huang, Fan Tang, Haibin Huang,\nChongyang Ma, Weiming Dong, and Changsheng Xu.\nInversion-based style transfer with diffusion models. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 10146\u201310156, 2023. 2, 5, 6,\n1\n[54] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng.\nMagicvideo:\nEfficient\nvideo generation with latent diffusion models.\narXiv\npreprint:2211.11018, 2022. 2\nStyleCrafter: Enhancing Stylized Text-to-Video Generation with Style Adapter\nSupplementary Material\nOur Supplementary Material consists of 5 sections:\n\u2022 Section A provides a detailed statement of our exper-\niments, including the implementation details of com-\nparison methods, details of our test set, and details of\nthe user study.\n\u2022 Section B adds more comparison experiments, includ-\ning the comparison with StyleDrop, and comparison in\nmulti-reference stylized image generation.\n\u2022 Section C explores the extended application of Style-\nCrafter, including the collaboration with depth control.\n\u2022 Section D demonstrates more results of our methods.\n\u2022 Section E discusses the limitations.\nA. Implementation Details\nA.1. Comparison methods\nFor comparison methods, we follow the instructions from\nthe official papers and open-source implementations. De-\ntails are provided as follows:\nDreambooth\nDreambooth [35] aims to generate images\nof a specific concept (e.g., style) by finetuning the entire\ntext-to-image model on one or serveral images. We train\nDreambooth based on Stable Diffusion 1.5. The training\nprompts are obtained from BLIP-2 [25], and we manually\nadd a style postfix using the rare token \u201dsks\u201d.\nFor ex-\nample, \u201dtwo slices of watermelon on a red surface in sks\nstyle\u201d is used for the first style reference in Table S3. We\ntrain the model for 500 steps for single-reference styles and\n1500 steps for multi-reference styles, with learning rates of\n5 \u00d7 10\u22126 and a batch size of 1. The training steps are care-\nfully selected to achieve the balance between text alignment\nand style conformity.\nCustomDiffusion\nCustomDiffusion [24] propose an effi-\ncient method for fast tuning text-to-image models for cer-\ntain styles or concepts. We train CustomDiffusion based\non Stable Diffusion 1.5. Similar to Dreambooth, we ob-\ntained training prompts from BLIP-2 [25] and we manually\nadd postfix like \u201din \u00a1new1\u00bf style\u201d. We generate a set of\n200 regularization images from mannually designed instant\nprompts for each style. We train the model for 500 steps for\nsingle-reference styles and 1500 steps for multi-reference\nstyles, with learning rates of 1 \u00d7 10\u22125 and a batch size of 2.\nInST\nInST [53] propose a inversion-based method to\nachieve style-guided text-to-image generation through\nlearning a textual description from style reference. We train\nInST for 1000 steps with learning rates of 1 \u00d7 10\u22124 and a\nbatch size of 1.\nStableDiffusion\nWe extend Stable Diffusion to style-\nguided text-to-video gerneration by utilizing GPT-4v to\ngenerate style descriptions from style reference.\nDetails\nabout style descriptions can be found in Table S3\nVideoCrafter and Gen-2\nSimilar to SD*,\nWe use\nVideoCrafter [4] 320 \u00d7 512 Text2Video Model and Gen-\n2 [5] equipped with GPT-4v to generate stylized videos\nfrom style references and text prompts.\nAnimateDiff\nAnimateDiff [12] aims to extend personal-\nized T2I model(i.e., Dreambooth or LoRA [19]) for video\ngeneration. To compare with AnimateDiff, we first train\npersonalized dreambooth models for each group of multi-\nreference style images, then we incorporate them into An-\nimateDiff based on their released codebase. We did not\nuse lora because we observed that AnimateDiff fails to turn\nlora-SD for video generation in most cases.\nA.2. Testing Datasets\nWe provide a detailed description of the testing datasets.\nContent Prompts\nWe use GPT-4 to generate recogniz-\nable textual descriptions from four meta-categories(human,\nanimal, object, and landscape). We manually filter out low-\nquality prompts, retaining 20 image prompts and 12 video\nprompts. The final prompts in testset are provided in Table\nS1 and Table S2.\nStyle References\nWe collect 20 diverse single-reference\nstylized images and 8 sets of style images with multi-\nreference(each contains 5 to 7 images in similar styles) from\nthe Internet1. Besides, for the comparison with the Text-to-\nImage model including Stable Diffusion and the Text-to-\nVideo model including VideoCrafter and Gen-2, we extend\nthem to stylized generation by equipped them with GPT-\n4v to generate textual style descriptions from style refer-\nence. We provide style references and corresponding style\ndescriptions in Table S3 and Figure S1.\n1The\nstyle\nreferences\nare\ncollected\nfrom\nhttps : / /\nunsplash . com/,\nhttps : / / unsplash . com/,\nhttps :\n//en.m.wikipedia.org/wiki/, https://civitai.com/,\nhttps://clipdrop.co/\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure S1. Multiple references in the testset\nPrompt\nMeta Category\nPrompt\nMeta Category\nA man playing the guitar on a city street.\nHuman\nA flock of birds flying gracefully in the sky.\nAnimal\nA woman reading a book in a park.\nHuman\nA colorful butterfly resting on a flower.\nAnimal\nA couple dancing gracefully together.\nHuman\nA bear fishing in a river.\nAnimal\nA person sitting on a bench, feeding birds.\nHuman\nA dog running in front of a house.\nAnimal\nA person jogging along a scenic trail.\nHuman\nA rabbit nibbling on a carrot.\nAnimal\nA bouquet of flowers in a vase.\nObject\nA cobblestone street lined with shops and cafes.\nLandscape\nA telescope pointed at the stars.\nObject\nA modern cityscape with towering skyscrapers.\nLandscape\nA rowboat docked on a peaceful lake.\nObject\nA winding path through a tranquil garden.\nLandscape\nA lighthouse standing tall on a rocky coast.\nObject\nAn ancient temple surrounded by lush vegetation.\nLandscape\nA rustic windmill in a field.\nObject\nA serene mountain landscape with a river flowing\nthrough it.\nLandscape\nTable S1. Text prompts used in the testset for image generation\nPrompt\nMeta Category\nPrompt\nMeta Category\nA street performer playing the guitar.\nHuman\nA bear catching fish in a river.\nAnimal\nA chef preparing meals in kitchen.\nHuman\nA knight riding a horse through a field.\nAnimal\nA student walking to school with backpack.\nHuman\nA wolf walking stealthily through the forest.\nAnimal\nA campfire surrounded by tents.\nObject\nA river flowing gently under a bridge.\nLandscape\nA hot air balloon floating in the sky.\nObject\nA field of sunflowers on a sunny day.\nLandscape\nA rocketship heading towards the moon.\nObject\nA wooden sailboat docked in a harbor.\nLandscape\nTable S2. Text prompts used in the testset for video generation\nStyle Reference\nStyle Descriptions\nStyle Reference\nStyle Descriptions\n3D Digital Art, {prompt}, whimsical and\nmodern, smooth and polished surfaces, bold\nand contrasting colors, soft shading and\nlighting, surreal representation.\nDigital Painting, {prompt}, detailed render-\ning, vibrant color palette, smooth gradients,\nrealistic light and reflection, immersive nat-\nural landscape scene.\nManga-inspired digital art, {prompt}, dy-\nnamic composition, exaggerated propor-\ntions,\nsharp\nlines,\ncel-shading,\nhigh-\ncontrast colors with a focus on sepia tones\nand blues.\nChildlike watercolor,\n{prompt},\nsimple\nbrush strokes, primary and secondary col-\nors, bold outlines, flat washes, playful,\nspontaneous, and expressive.\nComic book illustration, {prompt}, digital\nmedium, clean inking, cell shading, satu-\nrated colors with a natural palette, and a de-\ntailed, textured background.\nPixel art illustration,\n{prompt},\ndigital\nmedium, detailed sprite work, vibrant color\npalette, smooth shading, and a nostalgic,\nretro video game aesthetic.\nInk and watercolor on paper, {prompt},\nurban sketching style, detailed line work,\nwashed colors, realistic shading, and a vin-\ntage feel.\nFlat Vector Illustration, {prompt}, simpli-\nfied shapes, uniform color fills, minimal\nshading, absence of texture, clean and mod-\nern aesthetic.\nWatercolor and ink illustration, {prompt},\ntraditional comic style, muted earthy color\npalette, detailed with a sense of movement,\nsoft shading, and a historic ambiance.\nLow Poly Digital Art, {prompt}, geomet-\nric shapes, vibrant colors, flat texture, sharp\nedges, gradient shading, modern graphic\nstyle.\nChinese ink wash painting, {prompt}, min-\nimalistic color use, calligraphic brushwork,\nemphasis on flow and balance, with poetic\ninscription.\nChinese Ink Wash Painting,\n{prompt},\nmonochromatic palette,\ndynamic brush-\nstrokes, calligraphic lines, with a focus on\nnegative space and movement.\nManga Style, {prompt}, black and white\ndigital inking, high contrast, detailed line\nwork, cross-hatching for shadows, clean, no\ncolor.\nLine Drawing, {prompt}, simple and clean\nlines, monochrome palette, smooth texture,\nminimalist and cartoonish representation .\nVan Gogh\u2019s \u201dStarry Night\u201d style, {prompt},\nwith expressive, swirling brushstrokes, rich\nblue and yellow palette, and bold, impasto\ntexture.\nWatercolor Painting, {prompt}, fluid brush-\nstrokes, transparent washes, color blending,\nvisible paper texture, impressionistic style.\nVan Gogh-inspired pen sketch, {prompt},\ndynamic and swirling line work, monochro-\nmatic sepia tones, textured with a sense of\nmovement and energy.\nUkiyo-e Woodblock Print, {prompt}, gra-\ndation, limited color palette, flat areas of\ncolor, expressive line work, stylized wave\nforms, traditional Japanese art.\nWatercolor\nPainting,\n{prompt},\nfluid\nwashes of color, wet-on-wet technique,\nvibrant hues, soft texture, impressionistic\nportrayal.\nVictorian watercolor, {prompt}, fine detail,\nsoft pastel hues, gentle lighting, clear tex-\nture, with a quaint, realistic portrayal of ev-\neryday life.\nTable S3. Style references in the testset and corresponding style descriptions generated from GPT-4v[29].\nA.3. User Study\nIn this section, we provide a detailed introduction about\nour user study. We randomly selected 15 single-reference\nstyle-text pairs to compare the generated results among\nVideoCrafter [4], Gen-2 [5], and our proposed method.\nGiven that videocomposer [45] directly replicates the style\nreference and is minimally influenced by the prompt in most\ncases, we excluded it from the comparison in the user study.\nAdditionally, we randomly chose 10 multi-reference style-\ntext pairs for the comparison between AnimateDiff [12]\n(multiple style-specific models) and our method (a generic\nmodel). To ensure a blind comparison, we randomized the\norder of options for each question and masked the possible\nmodel watermark in the lower right corner.\nThe designed user preference interface is illustrated in\nFigure S10. We invited 15 users of normal eyesight to eval-\nuate the generated results in three aspects: text alignment,\nstyle conformity, and temporal quality. The instructions and\nquestions are provided as below. Consequently, a total of\n1125 votes are collected.\nInstructions.\n\u2022 Task: Watch the following video results generated\nfrom the style reference and text description, with 3\nsub-questions for each set of comparisons (please sep-\narately review the generated results from the follow-\ning three perspectives:\n\u2013 Text Alignment (multiple choice, means that the\ncontent of the generated video is aligned with the\ntext description(prompt), and the content of the text\ndescription should appear in the generated result);\n\u2013 Style Conformity (single choice, means that the\nstyle of the generated video is consistent with the\nstyle of the reference image, where the style in-\ncludes both the color tone, texture, brush strokes,\netc., as well as the painting style, emotion, and\nmood);\n\u2013 Temporal Quality (single choice, consists of two\naspects: First, the generated video should include\ncertain action or camera movement, and should be\nin line with the picture context; Second, the content\nof the picture should be coherent, without abrupt\nchanges or flickering);\n\u2022 Please ignore the watermark effect and the missing\narea in the bottom right corner of the result.\nQuestions.\n\u2022 Which one is aligned with text description? [Multiple\nchoice]\n\u2022 Which one performs best in Style Conformity? [Single\nchoice]\n\u2022 Which one performs best in Temporal Quality? [Sin-\ngle choice]\nB. Extended Comparison\nB.1. Comparison with StyleDrop\nIn this section, we present a supplementary comparison\nwith StyleDrop[39]. StyleDrop proposes a versatile method\nfor synthesizing images that faithfully follow a specific style\nusing a text-to-image model. Owing to the absence of an\nofficial StyleDrop implementation, we have excluded the\ncomparison with StyleDrop from the main text. Instead,\nwe include a comparison with an unofficial StyleDrop im-\nplementation2 in the appendix as an alternative. We train\nStyleDrop based on Muse for 1000 steps with a batch size\nof 8 and a learning rate of 3 \u00d7 10\u22124.\nThe quantitative and qualitative results are presented in\nTable S4 and Figure S2 respectively.\nResults show that\ncompared to StyleDrop, our proposed method more effec-\ntively captures the visual characteristics of a user-provided\nstyle and combines them with various prompts in a flexible\nmanner.\nMethods\nStyleDrop\nOurs\nText \u2191\n0.2389\n0.2729\nStyle \u2191\n0.3962\n0.5171\nTable S4. Quantitative comparison between StyleDrop and our\nproposed method.\nStyle Reference\n(a) StyleDrop\n(b) Ours\nFigure S2. Visual comparison between StyleDrop and our pro-\nposed method. Testing prompts: (i) A woman reading a book in\na park.; (ii) A person jogging along a scenic trail.; (iii) A colorful\nbutterfly resting on a flower.; (iv) A rabbit nibbling on a carrot.;\n(v) A telescope pointed at the stars.\nStyle Reference\n(a) DreamBooth\n(b) CustomDiffusion\n(c) Ours\nFigure S3. Visual comparison on mulit-reference stylized T2I generation. Testing prompts: (i) A rustic windmill in a field.; (ii) A person\njogging along a scenic trail.; (iii) A flock of birds flying gracefully in the sky.; (iv) A rowboat docked on a peaceful lake.; (v) An ancient\ntemple surrounded by lush vegetation.\nB.2. Multi-reference Stylized Image Generation\nWe further conduct the comparison of multi-reference styl-\nized image generation with Dreambooth [35] and Cus-\ntomDiffusion [24], both of which support generating im-\nages in specific styles by finetuning on the reference im-\nages. Figure S1 and Table S5 present the visual and quan-\ntitative results respectively, demonstrating that our method\nsurpasses all competitors in terms of style conformity for\nmulti-reference stylized generation. Although Dreambooth\nand CustomDiffusion exhibit competitive performance in\ncertain cases, their stylized generation abilities tend to vary\nwith different prompts, i.e. struggling to maintain consis-\ntent visual styles across arbitrary prompts. It is possibly be-\ncause the several images are insufficient to allow the model\nthe disentangle the contents and styles and thus harms the\n2https://github.com/aim-uofa/StyleDrop-PyTorch\ngeneralization performance. Besides, the requirement for\nfinetuning during the testing process also undermines their\nflexibility.\nIn contrast, our method efficiently generates\nhigh-quality stylized images that align with the prompts and\nconform the style of the reference images without additional\nfinetuning costs.\nMethods\nDreambooth [35]\nCustomDiffsion [24]\nOurs\nText \u2191\n0.2868\n0.2986\n0.2641\nStyle \u2191\n0.4270\n0.4441\n0.5639\nTable S5.\nQuantitative comparison on Multi-reference style-\nguided T2I generation. Bold: Best.\nDepth\n\u201cA car turning around on the road.\u201d\n\u201cA car turning around on the  road.\u201d\n\u201cA tiger walks in the forest.\u201d\n\u201cA tiger walks in the forest.\u201d\nInput\nInput\nVid.Comp.\nVid.Comp.\nOurs\nOurs\nFigure S4. Visual comparison on stylized video generation with additional depth guidance. Vid.Comp.: VideoComposer\nC. Application Extension\nIn this section, we further explore the compatibility with\nadditional controllable conditions, e.t., depth. Following\nthe approach of structure control in Animate-A-Story[14],\nwe introduce video structure control by integrating a well-\ntrained depth adapter into the base T2V model. Note that\nStyleCrafter and depth-adapter are trained independently,\nthe only operation we take is to combine the both during\nthe inference stage. Instead of employing DDIM Inversion\nto ensure consistency, we generate the videos from random\nnoise. The visual comparison with VideoComposer[45] is\npresent in Figure S4. VideoComposer struggles to produce\nresults faithful to text descriptions when faced with artis-\ntic styles, such as the \u201dboat\u201d mentioned in the prompt. In\ncontrast, our method not only supports collaboration with\ndepth guidance, but also generates videos with controllable\ncontent, style, and structure.\nD. More Results\nWe provide additional qualitative comparison on single-\nreference stylized video generation and multi-reference\nstylized video generation, as illustrated in Figure S6 and\nFigure S7, respectively. Our proposed method outperforms\nprevious methods in style conformity, and achieves satisfac-\ntory text alignment and temporal consistency. Furthermore,\nadditional results are available in Figure S8 and Figure S9,\nwhich indicate that our method can effectively handle di-\nverse combinations of style and prompts.\nE. Limitations\nWhile our proposed method effectively handles most com-\nmon styles, it does have certain limitations. Firstly, since\nStyleCrafter is developed based on an existing T2V model,\nsuch as VideoCrafter, it unavoidably inherits part of the\nbase model\u2019s shortcomings. For example, our method fails\nto generate high-definition faces in certain cases as shown\nin Figure S5. Besides, visual styles are considerably more\ncomplex than what we explore in our paper, our model may\n\u201cA street performer playing the guitar.\u201d\n\u201cA student walking to school with backpack.\u201d\nFigure S5. Failure cases of our methods\nproduce just passable results when confronted with refer-\nence images possessing highly stylized semantics. For ex-\nample, as depicted in Figure S5, although our model suc-\ncessfully reproduces ink strokes, there are still discrepan-\ncies with reference images in the aesthetic level, such as the\nlack of \u201dblank-leaving\u201d in the generation results. Addition-\nally, considering the absence of stylized video data, our styl-\nized video generation results are somewhat less satisfactory\nthan stylized image generation in visual style expression. A\npossible solution is to collect sufficient stylized video data\nfor training, which we leave for further work.\n\u201cA field of sunflowers on a sunny day.\u201d\nVid.Craf.*\nOurs\n\u201cA rocketship heading towards the moon.\u201d\nGen-2*\nVid.Comp.\nOurs\nGen-2*\nVid.Comp.\nVid.Craf.*\n\u201cA river flowing gently under a bridge.\u201d\nVid.Craf.*\nOurs\n\u201cA street performer playing the guitar.\u201d\nGen-2*\nVid.Comp.\nOurs\nGen-2*\nVid.Comp.\nVid.Craf.*\n\u201cA bear catching fish in a river.\u201d\nVid.Craf.*\nOurs\n\u201cA knight riding a horse through a field.\u201d\nGen-2*\nVid.Comp.\nOurs\nGen-2*\nVid.Comp.\nVid.Craf.*\nFigure S6. More Visual Comparison on Sinlge-Reference Stylized T2V Generation. Vid.Comp.: VideoComposer; Vid.Craf.: VideoCrafter.\n\u201cA street performer playing the guitar.\u201d\nAnimateDiff\nOurs(S-R)\n\u201cA wolf walking stealthily through the forest.\u201d\nOurs(M-R)\nAnimateDiff\nOurs(S-R)\nOurs(M-R)\n\u201cA student walking to school with backpack.\u201d\nAnimateDiff\nOurs(S-R)\n\u201cA knight riding a horse through a field.\u201d\nOurs(M-R)\nAnimateDiff\nOurs(S-R)\nOurs(M-R)\n\u201cA chef preparing meals in kitchen.\u201d\nAnimateDiff\nOurs(S-R)\n\u201cA rocketship heading towards the moon.\u201d\nOurs(M-R)\nAnimateDiff\nOurs(S-R)\nOurs(M-R)\nFigure S7. More Visual Comparison on Multi-Reference Stylized T2V Generation\n(a) Style Reference\n(b) Stylized Videos Generated from StyleCrafter\nPrompt:  \u201cA bear catching fish in a river.\u201d\n(a) Style Reference\n(b) Stylized Videos Generated from StyleCrafter\nPrompt:  \u201cA wooden sailboat docked in a harbor.\u201d\nFigure S8. More Results of StyleCrafter on Style-Guided Text-to-Video Generation\n(a) Style Reference\n(b) Stylized Videos Generated from StyleCrafter\nPrompt:  \u201cA chef preparing meals in kitchen.\u201d\n(a) Style Reference\n(b) Stylized Videos Generated from StyleCrafter\nPrompt:  \u201cA campfire surrounded by tents.\u201d\nFigure S9. More Results of StyleCrafter on Style-Guided Text-to-Video Generation\nFigure S10. User Preference Study Interface\n"
  },
  {
    "title": "FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting",
    "link": "https://arxiv.org/pdf/2312.00451.pdf",
    "upvote": "8",
    "text": "FSGS: Real-Time Few-shot View Synthesis using Gaussian Splatting\nZehao Zhu1*\nZhiwen Fan1\u2217\nYifan Jiang1\nZhangyang Wang1\n1University of Texas at Austin\nAbstract\nNovel view synthesis from limited observations remains\nan important and persistent task. However, high efficiency\nin existing NeRF-based few-shot view synthesis is often\ncompromised to obtain an accurate 3D representation. To\naddress this challenge, we propose a Few-Shot view syn-\nthesis framework based on 3D Gaussian Splatting that en-\nables real-time and photo-realistic view synthesis with as\nfew as three training views. The proposed method, dubbed\nFSGS, handles the extremely sparse initialized SfM points\nwith a thoughtfully designed Gaussian Unpooling process.\nOur method iteratively distributes new Gaussians around\nthe most representative locations, subsequently infilling lo-\ncal details in vacant areas. We also integrate a large-scale\npre-trained monocular depth estimator within the Gaus-\nsians optimization process, leveraging online augmented\nviews to guide the geometric optimization towards an op-\ntimal solution. Starting from sparse points observed from\nlimited input viewpoints, our FSGS can accurately grow\ninto unseen regions, comprehensively covering the scene\nand boosting the rendering quality of novel views. Over-\nall, FSGS achieves state-of-the-art performance in both ac-\ncuracy and rendering efficiency across diverse datasets, in-\ncluding LLFF, Mip-NeRF360, and Blender. Project web-\nsite: https://zehaozhu.github.io/FSGS/.\n1. Introduction\nNovel view synthesis (NVS) from a set of view collec-\ntions, as demonstrated by recent works [16, 35, 46], has\nplayed a critical role in the domain of 3D vision and is piv-\notal in many applications, e.g., VR/AR and autonomous\ndriving.\nDespite its effectiveness in photo-realistic ren-\ndering, the requirement of dense support views has hin-\ndered its practical usages [33]. Previous studies have fo-\ncused on reducing the view requirements by leveraging\nNeural Radiance Field (NeRF)[30], a powerful implicit\n3D representation that captures scene details, combined\nwith volume rendering techniques[13]. Depth regulariza-\n*Equal contribution\n\u2026\nSparse input views\nFew SfM points\n3D Gaussians\n203FPS\nFPS vs. SSIM\nFigure 1.\nReal-Time Few-shot Novel View Synthesis.\nWe\npresent a point-based framework that is initialized from extremely\nsparse SfM points, achieving a significantly faster rendering speed\n(2900\u00d7) while enhancing the visual quality (from 0.684 to 0.745,\nin SSIM) compared to the previous SparseNeRF [50].\ntions [11, 33, 47, 54] within the density field, additional\nsupervision from 2D pre-trained models [23, 50], large-\nscale pre-training [8, 59], and frequency annealings [55]\nhave been proposed and adopted to address the challenge\nof few-shot view synthesis. However, NeRF-based methods\nincur high training and rendering costs to achieve satisfac-\ntory rendering quality. Though some NeRF follow-ups have\nreduced the training time in real-world scenes from days to\nhours [40, 43, 51, 58], or even minutes [31], a significant\ngap remains between achieving real-time rendering speed\nand photo-realistic high-resolution rendering quality.\n3D\nGaussian\nSplatting\n(3D-GS)[26]\nhas\nrecently\nemerged as an efficient representation to model the 3D\nscenes from a dense collection of camera viewpoints. It\nrepresents the scene as a combination of 3D Gaussians\nwith attributes for modeling intricate shape and appear-\nances, and 2D images are rendered via splatting-based\nrasterization[57]. By replacing volume rendering with the\nefficient differentiable splatting, 3D Gaussian Splatting at-\ntains real-time rendering speed while maintaining the abil-\n1\narXiv:2312.00451v1  [cs.CV]  1 Dec 2023\nSparse input\nSfM\nInitialization\nProximity-guided\nGaussian Unpooling\nProjection + Rasterization\nGT\nDepth(DPT)\nRGB\nDepth (Pred)\nLoss\nLoss\nTraining view\nSynthesized view\nGaussian\nGraph\nExisting\nGaussian\nUnpooled\nGaussian\nOptimized Gaussians\nRendered\nPaths\nReal-time\nRendering\nL1 + SSIM losses\nDepth correlation\n,\nFigure 2. FSGS Pipeline. 3D Gaussians are initialized from SfM, with a few images (black cameras). For the sparsely placed Gaussians,\nwe propose densifying new Gaussians to enhance scene coverage by unpooling existing Gaussians into new ones, with properly initialized\nGaussian attributes. Monocular depth priors, enhanced by sampling unobserved views (red cameras), guide the optimization of grown\nGaussians towards a reasonable geometry. The final loss consists of a photometric loss term, and a geometric regularization term calculated\nas depth correlation.\nity to render photo-realistic images from any new view-\npoint. However, 3D-GS is initialized from Structure-from-\nMotion (SfM) points, and its performance strongly relies on\nboth the quantity and accuracy of the initialized points. Al-\nthough the subsequent Gaussian densification [26] can in-\ncrease the number of Gaussians in both under-reconstructed\nand over-reconstructed regions, this straightforward strat-\negy falls short in few-shot settings: it suffers from inade-\nquate initialization, leading to oversmoothed outcomes and\na tendency to overfit on training views.\nOur goal is to construct a compact and efficient repre-\nsentation while preserving the visual fidelity from limited\nobservations.\nTo this end, we present FSGS, to model\nlarge-scale scenes from Sparse viewpoint inputs using 3D\nGaussians as representation. To address the challenge of\nmitigating the gap in dense coverage from sparse input, the\nfirst challenge is how to move the locations of Gaussians\neffectively to cover and represent the 3D scene.\nThere-\nfore, we propose Proximity-guided Gaussian Unpooling,\nwhere it grows new Gaussians by measuring the proxim-\nity of existing Gaussians with their neighbors. By plac-\ning new Gaussians in representative locations and initial-\nizing them with the information from existing Gaussians,\nit effectively increases the Gaussian count for comprehen-\nsive scene coverage. The next challenge is how to ensure\nthe grown Gaussians correctly represent the geometry of\nscenes, even when there are not enough multi-view cues?\nThus it becomes essential to leverage extra priors to con-\ntrol and regularize the optimization of the new Gaussians.\nSpecifically, we propose to utilize rich depth priors with\npre-trained monocular depth estimators on both training and\nonline augmented pseudo views, which guide the Gaussian\nUnpooling to converge to a reasonable solution and en-\nsure the geometric smoothness of the scenes. The back-\npropagation of the integrated depth prior is achieved by\nimplementing a differentiable depth rasterizer. Validations\non few-shot NVS benchmarks: scene-level LLFF, Mip-\nNeRF360, and object-level Blender datasets, demonstrates\nthat FSGS achieves the state-of-the-art rendering quality,\nwhile can run at 203 FPS, a practical speed for real-world\napplications. Our contributions are outlined below:\n\u2022 We propose a novel point-based framework for few-shot\nview synthesis, featuring Proximity-guided Gaussian Un-\npooling to densify Gaussians for comprehensive scene\ncoverage.\n\u2022 Our framework integrates monocular depth priors, en-\nhanced by virtually sampled training views, to guide the\nGaussian optimization toward an optimal solution.\n\u2022 Equipped with an enhanced training paradigm, FSGS not\nonly achieves real-time rendering speed (200+FPS) but\nalso improves visual quality, paving the way for practical\nusage in real-world scenarios.\n2. Related Works\n2.1. Neural Representations for 3D Reconstruction\nThe recent advancement of neural rendering techniques,\nsuch as Neural Radiance Fields (NeRFs) [30], has shown\nencouraging progress for novel view synthesis.\nNeRF\nlearns an implicit neural scene representation that utilizes\na MLP to map 3D coordinates (x, y, z) and view depen-\ndency (\u03b8, \u03d5) to color and density through a volume render-\ning function. Tremendous works focus on improving its ef-\nficiency [7, 15, 17, 26, 31, 38, 43], quality [1, 3, 4, 9, 20, 42,\n48, 52], generalizing to unseen scenes [8, 10, 24, 44, 53, 59],\napplying artistic effects [14, 22, 49, 60] and 3D genera-\n2\ntion [5, 6, 18, 21, 25, 27, 28, 34, 41, 45]. In particular,\nReiser et al. [38] accelerate NeRF\u2019s training by splitting\na big MLP into thousands of tiny MLPs. MVSNeRF [8]\nconstructs a 3D cost volume [19, 56] and renders high-\nquality images from novel viewpoints.\nMoreover, Mip-\nNeRF [1] adopts conical frustum rather than a single ray\nin order to mitigate aliasing. Mip-NeRF 360 [2] further\nextends it to the unbounded scenes. While these NeRF-\nlike models present strong performance on various bench-\nmarks, they generally require several hours of training time.\nMuller et al. [31] adopt a multiresolution hash encoding\ntechnique that reduces the training time to 5 seconds. Kerbl\net al. [26] propose to use a 3D Gaussian Splatting pipeline\nthat achieves real-time rendering for either objects or un-\nbounded scenes. The proposed FSGS approach is based on\nthe 3D Gaussian Splatting framework but largely reduces\nthe required training views.\n2.2. Novel-View Synthesis Using Sparse Views\nThe original neural radiance field takes more than one hun-\ndred images as training views, which largely prohibits its\npractical usage. To tackle this issue, several works have at-\ntempted to reduce the number of training views. Specif-\nically, Deng et al. [12] applies additional depth supervi-\nsion to improve the rendering quality. Niemeyer et al. [32]\nproposes a depth smoothness loss as geometry regulariza-\ntion to stabilize training. DietNeRF [23] adds supervision\non high-dimensional semantic space, the CLIP embedding\nspace [36], to constraint the rendered unseen views. Pixel-\nNeRF [59] trains a convolution encoder to capture context\ninformation and learns to predict 3D representation from\na single input.\nMore recently, FreeNeRF [55] proposes\na dynamic frequency controlling module that successfully\ntrains a NeRF using sparse views with minimal modifica-\ntion. SparseNeRF [50] proposes a new spatial continuity\nloss to distill spatial coherence from monocular depth esti-\nmators. Compared to the aforementioned approaches, our\nmethod achieves real time rendering in novel views, and\nsignificantly improves the rendering quality.\n3. Method\nAn overview of the FSGS framework is provided in Fig. 2.\nFSGS inputs with a limited number of images which are\ncaptured within a static scene.\nThe camera poses and\nsparse point clouds are calculated from Structure-from-\nMotion(SfM) [39].\nThe initial 3D Gaussians used for\nfurther training are initialized from SfM points, with at-\ntributes of color, position and shape. The challenge in ex-\ntremely sparse SfM points and insufficient observations is\naddressed by adopting Proximity-guided Gaussian Unpool-\ning to densify Gaussians and fill the empty space by mea-\nsuring the proximity between existing Gaussians and strate-\ngically placing new ones to the most representative loca-\nFull-view SfM points\nThree-view SfM points\nFigure 3. Sparse SfM Points vs. Synthesized Quality. The SfM\npoints from COLMAP using 3-views (Bottom Left) is significantly\nsparse than full-view(Top Left). 3D-GS with sparse SfM points\nwill decrease its quality when the training view number decreases.\ntion, to increase the capacity of handling details. To guar-\nantee that the densified Gaussians can be optimized toward\nthe correct scene geometry, we leverage the priors from 2D\nmonocular depth estimator, enhanced by pseudo view gen-\neration which prevents our model from overfitting to sparse\ninput viewpoints.\n3.1. Preliminary and Problem Formulation\n3D Gaussian Splatting (3D-GS), as delineated in Kerbl et\nal. [26], represents an 3D scene explicitly through a col-\nlection of 3D Gaussians, with attributes: a position vector\n\u00b5 \u2208 R3 and a covariance matrix \u03a3 \u2208 R3\u00d73. Each Gaussian\ninfluences a point x in 3D space following the 3D Gaussian\ndistribution:\nG(x) =\n1\n(2\u03c0)3/2 |\u03a3|1/2 e\u2212 1\n2 (x\u2212\u00b5)T \u03a3\u22121(x\u2212\u00b5)\n(1)\nTo ensure that \u03a3 is positive semi-definite and holds prac-\ntical physical significance, \u03a3 is decomposed into two learn-\nable components by \u03a3 = RSST RT , where R is a quater-\nnion matrix representing rotation and S is a scaling matrix\nIn addition, each Gaussians store an opacity logit o \u2208\nR and the appearance feature represented by n spherical\nharmonic (SH) coefficients\n\b\nci \u2208 R3|i = 1, 2, ..., n\n\t\nwhere\nn = D2 is the number of coefficients of SH with degree D.\nTo render the 2D image, 3D-GS orders all the Gaussians\nthat contributes to a pixel and blends the ordered Gaussians\noverlapping the pixels using the following function:\nc =\nn\nX\ni=1\nci\u03b1i\ni\u22121\nY\nj=1\n(1 \u2212 \u03b1j)\n(2)\nwhere ci is the color computed from the SH coefficients\nof the ith Gaussian. \u03b1i is given by evaluating a 2D Gaus-\nsian with covariance \u03a3\u2032 \u2208 R2\u00d72 multiplied by the opac-\nity. The 2D covariance matrix \u03a3\u2032 is calculated by \u03a3\u2032 =\nJW\u03a3W T JT , projecting the 3D covariance \u03a3 to the cam-\nera coordinates. Here, J denotes the Jacobian of the affine\napproximation of the projective transformation, W is the\nview transformation matrix.\n3\nA heuristic Gaussian densification scheme is introduced\nin 3D-GS [26], where Gaussians are densified based on an\naverage magnitude of view-space position gradients which\nexceed a threshold. Although this method is effective when\ninitialized with comprehensive SfM points, it is insufficient\nfor fully covering the entire scene with an extremely sparse\npoint cloud, from sparse-view input images. Additionally,\nsome Gaussians tend to grow towards extremely large vol-\numes, leading to results that overfit the training views and\ngeneralize badly to novel viewpoints (See Fig. 3).\n3.2. Proximity-guided Gaussian Unpooling\nThe granularity of the modeled scene depends heavily on\nthe quality of the 3D Gaussians representing the scene;\ntherefore, addressing the limited 3D scene coverage is cru-\ncial for effective sparse-view modeling.\nProximity Score and Graph Construction\nDuring\nGaussian optimization, we construct a directed graph, re-\nferred to as the proximity graph, to connect each existing\nGaussian with its nearest K neighbors by computing the\nEuclidean distance. We denote the originating Gaussian at\nthe head as the \u201csource\u201d Gaussian, while the one at the tail\nas the \u201cdestination\u201d Gaussian, which is one of the source\u2019s\nK neighbors. The proximity score assigned to each Gaus-\nsian is calculated as the average distance to its K nearest\nneighbors. The proximity graph is updated following the\ndensification or pruning process during optimization. We\nset K to 3 in practice.\nGaussian Unpooling\nInspired by the vertex-adding strat-\negy of the mesh subdivision algorithm [61] which is widely\nused in computer graphics, we propose unpooling Gaus-\nsians based on the proximity graph and the proximity score\nof each Gaussian. Specifically, if the proximity score of a\nGaussian exceeds the threshold tprox, our method will grow\na new Gaussian at the center of each edge, connecting the\n\u201csource\u201d and \u201cdestination\u201d Gaussians, as shown in Fig. 4.\nThe attributes of scale and opacity in the newly created\nGaussians are set to match those of the \u201cdestination\u201d Gaus-\nsians. Meanwhile, other attributes such as rotation and SH\ncoefficients are initialized to zero. The Gaussian unpool-\ning strategy encourages the newly densified Gaussians to\nbe distributed around the representative locations and pro-\ngressively fill observation gaps during optimization.\n3.3. Geometry Guidance for Gaussian Optimization\nHaving achieved dense coverage by unpooling Gaussians, a\nphotometric loss with multi-view clues is applied for opti-\nmizing Gaussians. However, the insufficient observations\nin the sparse-view setting limit the capacity for learning\ncoherent geometry, leading to a high risk of overfitting on\ntraining views and poor generalization to novel views. This\nnecessitates the incorporation of additional regularization\nGT Shape\nExisting\nGaussians\nNew\nGaussians\nPositional\nGradient\nEdge with\nProximity Score\n1D Depth\nExample\n3D-GS\nGaussian Unpool.\nGaussian Unpool.\n+ Depth Guidance\nMonocular\nDepth Guidance\nViewing\nCamera\nFigure 4.\nGaussian Unpooling Illustration.\nWe show a 2D\ntoy case for visualizing Gaussian Unpooling with depth guidance,\nwhere the example 1D depth can provide priors on the relative\ndistance of the Gaussians from the viewing direction, guide the\nGaussian deformation toward a better solution.\nand priors to guide the Gaussian optimization. Specifically,\nwe seek help from depth priors produced by a well-trained\nmonocular depth estimator to guide the geometry of Gaus-\nsians towards a reasonable solution.\nInjecting Geometry Coherence from Monocular Depth\nWe generate the monocular Dest depth maps at training\nviews by using the pre-trained Dense Prediction Trans-\nformer (DPT), trained with 1.4 million image-depth pairs\nas a handy yet effective choice. To mitigate the scale ambi-\nguity between the true scene scale and the estimated depth,\nwe introduce a relaxed relative loss, Pearson correlation, on\nthe estimated and rendered depth maps. It measures the dis-\ntribution difference between 2D depth maps and follows the\nbelow function:\nCorr( \u02c6Dras, \u02c6Dest) =\nCov( \u02c6Dras, \u02c6Dest)\nq\nVar( \u02c6Dras) Var( \u02c6Dest)\n(3)\nThis soften constraint allows for the alignment of depth\nstructure without being hindered by the inconsistencies in\nabsolute depth values.\nDifferentiable Depth Rasterization\nTo enable the back-\npropogation from depth prior to guide Gaussian training,\nwe implement a differentiable depth rasterizer, allowing for\nreceiving the error signal between the rendered depth Dras\nand the estimated depth Dest. Specifically, we utilize the\nalpha-blending rendering in 3D-GS for depth rasterization,\nwhere the z-buffer from the ordered Gaussians contributing\nto a pixel is accumulated for producing the depth value:\nd =\nn\nX\ni=1\ndi\u03b1i\ni\u22121\nY\nj=1\n(1 \u2212 \u03b1j)\n(4)\nHere di represents the z-buffer of the ith Gaussians and \u03b1 is\nidentical to that in Eq. 2. The fully differentiable implemen-\ntation enables the depth correlation loss, further improving\nthe similarity between the rendered and estimated depths.\n4\nSynthesize Pseudo Views\nTo address the inherent issue\nof overfitting to sparse training views, we employ unob-\nserved (pseudo) view augmentation to incorporate more\nprior knowledge within the scene derived from a 2D prior\nmodel. The synthesized view is sampled from the two clos-\nest training views in Euclidean space, calculating the av-\neraged camera orientation and interpolating a virtual one\nbetween them. A random noise is applied to the 3 degrees-\nof-freedom (3DoF) camera location as shown in Eq. 5, and\nthen images are rendered.\nP \u2032 = (t + \u03b5, q),\n\u03b5 \u223c N(0, \u03b4)\n(5)\nHere, t \u2208 P denotes camera location, while q is a quater-\nnion representing the rotation averaged from the two cam-\neras. This approach of synthesizing online pseudo-views\nenables dynamic geometry updates, as the 3D Gaussians\nwill update progressively, reducing the risk of overfitting.\n3.4. Optimization\nCombining all together, we can summarize the training loss:\nL(G, C) = \u03bb1\n\r\r\rC \u2212 \u02c6C\n\r\r\r\n1\n|\n{z\n}\nL1\n+\u03bb2 D-SSIM(C, \u02c6C)\n|\n{z\n}\nLssim\n+ \u03bb3 \u2225Corr(Dras, Dest)\u22251\n|\n{z\n}\nLprior\n(6)\nwhere L1, and Lssim stands for the photometric loss term be-\ntween predicted image \u02c6C and ground-truth image C. Lprior\nrepresents the geometric regularization term on the training\nviews and synthesized pseudo views. We set \u03bb1, \u03bb2, \u03bb3\nas 0.8, 0.2, 0.05 respectively by grid search. The pseudo\nviews sampling is enabled after 2,000 iterations to ensure\nthe Gaussians can roughly represent the scene.\n4. Experiments\n4.1. Experimental Settings\nLLFF Datasets [29] consist of eight forward-facing real-\nworld scenes. Following RegNeRF [33], we select every\neighth image as the test set, and evenly sample sparse views\nfrom the remaining images for training purposes. We use 3\nviews to train all the methods, and evaluate them at resolu-\ntions of 1008\u00d7756 and 504\u00d7378.\nMip-NeRF360 Datasets [2] consist of nine scenes, each\nfeaturing a complex central object or area against a detailed\nbackground. We utilize the published seven scenes for com-\nparison, using 24 training views with images downsampled\nto 4\u00d7 and 8\u00d7. Test images are selected following the same\nconvention with LLFF Datasets. To the best of our knowl-\nedge, we are the first to attempt few-shot novel view syn-\nthesis in unbounded scenes like Mip-NeRF360. We aim to\nestablish this simple baseline for the challenge of few-shot\nnovel view synthesis in complex large-scale scenarios.\nBlender datasets [30] have eight objects with realistic im-\nages synthesized by Blender. We align with DietNeRF [23],\nwhere we use 8 images for training and 25 for testing. We\nevalute all methods at resolution of 400\u00d7400.\nBaselines\nWe compare FSGS with several few-shot NVS\nmethods on these three dataset, including DietNeRF [23],\nRegNeRF [33], FreeNeRF [55], and SparseNeRF [47].\nAdditionally, we include comparisons with the high-\nperforming MLP-based NeRF, Mip-NeRF [3], which is pri-\nmarily designed for dense-view training, and point-based\n3D-GS, following its original dense-view training recipe.\nFollowing [26, 33, 50], we report the average PSNR, SSIM,\nLPIPS scores and FPS for all the methods.\nImplementation Details\nWe implemented FSGS using\nthe PyTorch framework, with initial camera poses and point\nclouds computed from SfM, based on the specified num-\nber of training views. During optimization, we densify the\nGaussians every 100 iterations and perform densification af-\nter 500 iterations. The total optimization steps are set at\n10,000 for all datasets, requiring approximately 9.5 min-\nutes on Blender and LLFF datasets, and \u223c24 minutes on\nMip-NeRF360 datasets. We set proximity threshold tprox to\n10, and the pseudo views are sampled after 2,000 iterations,\nwith \u03c3 set as 0.1. We utilize the pre-trained Dense Predic-\ntion Transformer (DPT) model [37] for zero-shot monoc-\nular depth estimation.\nAll results are obtained using an\nNVIDIA A6000 GPU.\n4.2. Comparisons to other Few-shot Methods\nComparisons on LLFF Datasets\nAs shown in Tab. 1,\nour method FSGS, despite trained from sparse SfM point\nclouds, provides the best quantitative results and effectively\naddresses the insufficient scene coverage in the initializa-\ntion. Our method surpasses SparseNeRF by 0.57 and 0.64\nin PSNR at both test resolutions, while operating 2180 times\nfaster. FSGS also outperforms 3D-GS by 2.60 in PSNR\nand boost the FPS from 385 to 458, demonstrating that our\nrefined Gaussians are more compact and precise for scene\nrepresentation from sparse views. The real-time rendering\nspeed makes FSGS a viable choice for practical usages.\nThe qualitative analysis, as presented in Fig. 5, demon-\nstrates that Mip-NeRF and 3D-GS struggle with the extreme\nsparse view problem; Mip-NeRF leads to degraded geomet-\nric modeling, and 3D-GS tends to produce blurred results in\nareas of complex geometry. The geometry field regulariza-\ntion in RegNeRF and frequency annealing in FreeNeRF do\nimprove the quality to some extent, but still exhibit insuf-\nficient visual quality. SparseNeRF, which also utilizes the\nmonocular depth maps for optimization, manifests as less\ntextural details. In contrast, our proposed Gaussian unpool-\ning, pulls more Gaussians to the unobserved regions and\nthus recovers more structural details in novel views.\n5\nMip-NeRF360\n3D-GS\nSparseNeRF\nOurs\nGT\nFern\nLeaves\nFlower\nFigure 5. Qualitative Results on LLFF Datasets. We demonstrate novel view results produced by 3D-GS [26], Mip-NeRF360 [2],\nSparseNeRF [50] and our approach for comparison. We can observe that NeRF-based methods generate floaters (Scene: Flower) and show\naliasing results (Scene: Leaves) due to limited observation. 3D-GS produces oversmoothed results, caused by overfitting on training views.\nOur method produces pleasing appearances while demonstrating detailed thin structures.\nMethods\n503 \u00d7 381 Resolution\n1006 \u00d7 762 Resolution\nFPS\u2191\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nFPS\u2191\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nMip-NeRF\n0.21\n16.11\n0.401\n0.460\n0.14\n15.22\n0.351\n0.540\n3D-GS\n385\n17.83\n0.582\n0.321\n312\n16.94\n0.488\n0.402\nDietNeRF\n0.14\n14.94\n0.370\n0.496\n0.08\n13.86\n0.305\n0.578\nRegNeRF\n0.21\n19.08\n0.587\n0.336\n0.14\n18.66\n0.535\n0.411\nFreeNeRF\n0.21\n19.63\n0.612\n0.308\n0.14\n19.13\n0.562\n0.384\nSparseNeRF\n0.21\n19.86\n0.624\n0.328\n0.14\n19.07\n0.564\n0.392\nOurs\n458\n20.43\n0.682\n0.248\n351\n19.71\n0.642\n0.283\nTable 1. Quantitative Comparison in LLFF Datasets, with 3 Training Views. FSGS achieves the best performance in terms of rendering\naccuracy and inference speed across all resolutions. Significantly, FSGS runs 2,180\u00d7 faster than the previous best, SparseNeRF, while\nimproving the SSIM from 0.624 to 0.682, at the resolution of 503 \u00d7 381. We color each cell as best , second best , and third best .\nComparisons on Mip-NeRF360 Datasets\nAs shown in\nTab. 2, methods requiring dense view coverage (Mip-\nNeRF360, 3D-GS) are outperformed by ours in terms of\nrendering speed and metrics, across the two resolutions.\nMethods employing regularizations from their respective\ngeometry and appearance fields (DietNeRF, RegNeRF,\nFreeNeRF) still suffer from rendering quality.\nSparseN-\neRF, by incorporating depth ranking information into NeRF,\nenhances quality to a degree, but remains far from achiev-\ning real-time speed. Our FSGS significantly outperforms\nNeRF-based approaches, boosting PNSR by 0.85 and im-\nproving FPS from 0.07 to 290 at 1/8 resolution. We pro-\nvide a qualitative comparison in Fig. 9, where we observe\nthat Mip-NeRF360 and SparseNeRF fail to capture the in-\ntricate details of scenes and tend to overfit on sparse train-\ning views, most notably in areas far away from cameras. In\ncomparison, FSGS recovers the fine-grained details such as\nthe leaves on the ground (Scene: Stump) and the piano keys\n(Scene: Bonsai), aligning well with the ground truth.\nComparisons on Blender Datasets\nTab. 3 presents the\nquantitative results on the Blender datasets.\nHere, our\nmethod significantly outperforms the baselines on object-\nlevel datasets, with an improvement of 0.40 in PSNR com-\npared to FreeNeRF, although primarily designed for scene-\nlevel scenarios with complex geometry. Fig. 7 visualizes\nthe rendered image. We find that DietNeRF hallucinates\ngeometric details, and FreeNeRF exhibits noticeable alias-\ning effects leading to worse geometry. 3D-GS suffers from\nexcessive blurriness and distorts the edges of the objects. In\ncontrast, our model not only captures the precise geometry\nof objects but also accurately simulates the shading effects.\n6\nMip-NeRF360\n3D-GS\nSparseNeRF\nOurs\nGT\nGarden\nRoom\nBonsai\nStump\nFigure 6. Qualitative Results on Mip-NeRF360 Datasets. Comparisons were conducted with 3D-GS [26], Mip-NeRF360 [2], and\nSparseNeRF [50]. Our method continues to produce visually pleasing results with sharper details than other methods in large-scale scenes.\nMethods\n1/8 Resolution\n1/4 Resolution\nFPS\u2191\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nFPS\u2191\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nMip-NeRF360\n0.12\n21.23\n0.613\n0.351\n0.07\n19.78\n0.530\n0.431\n3D-GS\n223\n20.89\n0.633\n0.317\n145\n19.93\n0.588\n0.401\nDietNeRF\n0.05\n20.21\n0.557\n0.387\n0.03\n19.11\n0.482\n0.452\nRegNeRF\n0.07\n22.19\n0.643\n0.335\n0.04\n20.55\n0.546\n0.398\nFreeNeRF\n0.07\n22.78\n0.689\n0.323\n0.04\n21.39\n0.587\n0.377\nSparseNeRF\n0.07\n22.85\n0.693\n0.315\n0.04\n21.43\n0.604\n0.389\nOurs\n290\n23.70\n0.745\n0.230\n203\n22.52\n0.673\n0.313\nTable 2. Quantitative Comparison in Mip-NeRF360 Datasets, with 24 Training Views. Our FSGS shows obvious advantages over\nNeRF-based methods, with an improvement of more than 0.05 in SSIM and running 4,142\u00d7 faster. Additionally, our method not only\nperforms better than 3D-GS in rendering metrics but also shows improvement in FPS (from 223 to 290), thanks to the Gaussian unpooling\nwhich motivates Gaussians to expand to unseen regions more accurately.\n4.3. Ablation Studies\nIn Tab. 4, we ablate our design choices on the the LLFF\ndataset under the 3-view setting.\nEffectiveness of Gaussian Unpooling As shown in the sec-\nond row of Tab. 4, our Gaussian Unpooling expands the\nscene geometry caused by limited training views, resulting\nin a PSNR improvement of 0.81 compared to 3D-GS. We\nalso visualize its visual effects in Fig. 8 (2nd row). The\nheuristic Gaussian densification leads to blurring results,\nparticularly noticeable in areas like bush and grass, our ap-\nproach enriches structural and visual details.\nImpact of Monocular Depth Prior Tab. 4 (3rd row)\ndemonstrates the improvement by introducing depth priors,\nguiding the Gaussian unpooling towards more plausible ge-\nometry. In Fig. 8, we observe that the depth regularization\neffectively eliminates the artifacts in grassy regions, and en-\nforces more consistent and solid surfaces with geometric\ncoherence. We also display the rendered depth map, where\ndepth regularization leads to depths aligning better with the\nactual geometric structures.\nPseudo-view Matters in Few-shot Modeling Tab. 4 (4th\nrow) validates the impact of synthesizing more unseen\nviews during training, which anchors the Gaussians to a\nplausible geometry and further enhances the modeling qual-\nity when the geometry in densification is not accurate.\n7\nDietNeRF\n3D-GS\nFreeNeRF\nOurs\nGT\nFicus\nHotdog\nFigure 7. Qualitative Results on Blender Datasets. Our method consistently outperforms other baselines in the task of novel view\nsynthesis for object-centric datasets.\nMethod\nFPS\u2193\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nMip-NeRF\n0.22\n20.89\n0.830\n0.168\n3D-GS\n332\n21.56\n0.847\n0.130\nDietNeRF\n0.14\n22.50\n0.823\n0.124\nRegNeRF\n0.22\n23.86\n0.852\n0.105\nFreeNeRF\n0.22\n24.26\n0.883\n0.098\nSparseNeRF\n0.22\n24.04\n0.876\n0.113\nOurs\n467\n24.64\n0.895\n0.095\nTable 3. Quantitative Comparison in Blender Datasets, with\n8 Training Views. FSGS outperforms existing few-shot methods\nand 3D-GS across all metrics, validating the generalization of the\nproposed techniques to handheld object-level 3D modeling.\nGaussian\nGeometry\nPseudo\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nUnpooling\nGuidance\nViews\n\u2717\n\u2717\n\u2717\n17.83\n0.582\n0.321\n\u2713\n\u2717\n\u2717\n18.64\n0.603\n0.311\n\u2713\n\u2713\n\u2717\n19.93\n0.644\n0.283\n\u2713\n\u2713\n\u2713\n20.43\n0.682\n0.248\nTable 4. Ablation Study. Starting from 3D-GS [26] (1st row),\nwe find that our proposed Gaussian Unpooling (2nd row) is more\neffective than the densification scheme in 3D-GS for few-shot\nview synthesis. Applying additional supervision from a monoc-\nular depth estimator further regularizes the Gaussian optimiza-\ntion towards a better solution (3rd row). Introducing pseudo-view\naugmentation to apply additional regularization when optimizing\nGaussians further enhances the results in a few-shot scenario.\n5. Conclusion\nIn this work, we present a real-time few-shot framework,\nFSGS, for novel views synthesis within an insufficiently\nview overlapping.\nStarting from extremely sparse SfM\npoint cloud, FSGS adopts the point-based representation\nRendered RGB (Baseline)\n+ Gaussian Unpool. + Depth Corr\n+ Gaussian Unpool.\nRendered Depth (Baseline)\n+Gaussian Unpool.\n+ Gaussian Unpool. + Depth Corr\nFigure 8. Ablation Study by Visualization. 3D-GS [26] (1st row)\nshows that the baseline method is significantly degraded when the\nview coverage is insufficient. Gaussian Unpooling provides ex-\ntra capacity to 3D Gaussians to model the scene, but the learned\ngeometry may not be accurate. Adding Depth Correlation regu-\nlarization (3rd row) can further improve the modeled details.\nand proposes an effective Gaussian unpooling method by\nmeasuring the proximity of each Gaussian to its neighbor.\nThe adoption of monocular depth priors has proven effec-\ntive to guide the expanded scene geometry toward a better\nsolution. FSGS is capable of generating photo-realistic im-\nages with as few as three images, and perform inference at\nmore than 200FPS, offering new avenues for real-time ren-\ndering and more cost-effective capture methods.\n8\nA. Appendix Part I: More Technical Details\nA.1. Initialization\nSimilar to 3D Gaussian Splatting [26], we start our pipeline\nfrom unstructured multi-view images, and calibrate the im-\nages using Structure-from-Motion [39]. Next, we will con-\ntinue the dense stereo matching under COLMAP with the\nfunction \u201cpatch match stereo \u201d and utilize the fused stereo\npoint cloud from \u201cstereo fusion \u201d. We then initialize the SH\ncoefficients at degree 0 and the positions of the 3D Gaus-\nsians based on the fused point cloud. Additionally, we re-\nmain the rest coefficients and rotation to 0. We also initial-\nize the opacity to 0.1 and set the scale to match the average\ndistance between points.\nA.2. Training\nDuring training, we start with a SH degree of 0 for a basic\nlighting representation, incrementing by 1 every 500 itera-\ntions up to a degree of 4 to increase complexity over time.\nWe set the learning rate of position, SH coefficients, opac-\nity, scaling, and rotation to 0.00016, 0.0025, 0.05, 0.005,\nand 0.001 respectively. At iterations 2000, 5000, and 7000,\nthe opacity for all Gaussians is reset to 0.05 to eliminate the\nlow-opacity floaters. In the Blender dataset [30], the Pear-\nson correlation is only computed in pixels where the depth\nvalues are greater than 0. Additionally, we utilize an open-\nsource code1 to estimate the inverse depth map for both the\ninput images and the rendered images from pseudo views.\nWe detail the procedures of our proposed FSGS in Algo-\nrithm 1.\nB. Appendix Part II: More Experiments\nB.1. Effects of Training Views\nWe demonstrate the quantitative results of FSGS on LLFF\ndatasets under 3, 6, 9 views in Tab. 5. We can observe\nthat more views consistently exhibit higher visual quality.\nIn Fig.\n9, we present a qualitative comparison. We can\nobserve that training with fewer views often leads to over-\nfitting on sparse training data. In contrast, training with 9\nviews continues to capture the fine-grained structures of the\nscenes. More views provide a more comprehensive cover-\nage of the scene, capturing more details of the scenes. This\nrichness in data boosts supervision signals during optimiza-\ntion, leading to more detailed and structural texture. Across\nall the settings, FSGS delivers superior performance com-\npared to all the baselines, affirming the effectiveness of our\nproposed depth prior.\n1https://pytorch.org/hub/intelisl_midas_v2/\nAlgorithm 1 The training pipeline of FSGS\n1: Training view images I = {Ii \u2208 RH\u00d7W \u00d73}N\ni=1 and\ntheir associated camera poses P = {\u03d5i \u2208 R3\u00d74}N\ni=1.\n2: Run SfM with the input images and camera poses and\nobtain an initial point cloud P, used to define 3D Gaus-\nsians function G = {Gi(\u00b5i, \u03c3i, ci, \u03b1i)}K\ni=1.\n3: Leverage pretrained depth estimator E to predict the\ndepth map Di = E(Ii).\n4: Synthesize pseudo views P\u2020 = {\u03d5\u2020\ni \u2208 R3\u00d74}M\ni=1 from\ninput camera poses P.\n5: while until convergence do\n6:\nRandomly sample an image Ii \u2208 I and the corre-\nsponding camera pose \u03d5i\n7:\nRasterize the rgb image \u02c6Ii and the depth map \u02c6Di\nwith camera pose \u03d5i\n8:\nL\n=\n\u03bb1\u2225Ii \u2212 \u02c6Ii\u22251 + \u03bb2 D-SSIM(Ii, \u02c6Ii) +\n\u03bb3 Pearson(Di, \u02c6Di)\n9:\nif iteration > titer then\n10:\nSample a pseudo camera pose \u03d5\u2020\nj \u2208 P\u2020.\n11:\nRasterize the rgb image \u02c6I\u2020j and the depth \u02c6\nD\u2020i\n12:\nCompute the estimated depth as D\u2020j = E( \u02c6I\u2020j).\n13:\nL = L + \u03bb4 Pearson(D\u2020j, \u02c6\nD\u2020i)\n14:\nend if\n15:\nif IsRefinement(iteration) then\n16:\nfor Gi(\u00b5i, \u03c3i, ci, \u03b1i) \u2208 G do\n17:\nif \u03b1i > \u03b5 or IsTooLarge(\u00b5i, \u03c3i) then\n18:\nRemoveGaussian()\n19:\nend if\n20:\nif \u2207pL > tpos then\n21:\nGaussianDensify()\n22:\nend if\n23:\nif NoProximity(G) then\n24:\nGaussianUnpooling()\n25:\nend if\n26:\nend for\n27:\nend if\n28:\nUpdate Gaussians parameter G via \u2207GL.\n29: end while\nB.2. FSGS on Mobile Phones Data\nTo validate the generalization capability of FSGS in vari-\nous real-world settings, we created a new dataset using only\na consumer smartphone, the iPhone 15 Pro. This dataset\ncontains three scenes, comprising two indoor scenes and\none outdoor scene. Each scene consists of a collection of\nRGB images under 5712\u00d74284 resolution, with the view-\npoint number ranging from 20 to 40. Our data calibration\npipeline follows the same process procedures as the LLFF\ndatasets [29], and we also select every 8-th image as the\nnovel views for evaluation. For training, we evenly sam-\n9\nFortress\nTrex\nFlower\nLeaves\nHorns\n3 Views\nGT\n6 Views\n9 Views\nFern\nOrchids\nFigure 9. Qualitative Results on the Effects of Training Views. We visualize the rendered images produced by all methods with training\nviews ranging from three to nine.\nMethods\nPSNR\nSSIM\nLPIPS\n3-view\n6-view\n9-view\n3-view\n6-view\n9-view\n3-view\n6-view\n9-view\nMip-NeRF\n16.10\n22.91\n24.88\n0.401\n0.756\n0.826\n0.460\n0.213\n0.170\n3D-GS\n17.83\n22.87\n24.65\n0.582\n0.732\n0.813\n0.321\n0.204\n0.159\nDietNeRF\n14.94\n21.75\n24.28\n0.370\n0.717\n0.801\n0.496\n0.248\n0.183\nRegNeRF\n19.08\n23.10\n24.86\n0.587\n0.760\n0.820\n0.336\n0.206\n0.161\nFreeNeRF\n19.63\n23.73\n25.13\n0.612\n0.779\n0.827\n0.308\n0.195\n0.160\nSparseNeRF\n19.86\n23.64\n24.97\n0.624\n0.784\n0.834\n0.328\n0.202\n0.158\nOurs\n20.31\n24.20\n25.32\n0.682\n0.811\n0.856\n0.248\n0.173\n0.136\nTable 5. Quantitative Analysis on the Effects of Training Views. We conduct experiments by using different training views (from 3 to\n9) to test the adopted baseline methods. Our FSGS consistently outperforms other methods across all metrics.\nple 3 images from the remaining views. These images are\nthen downsampled to 4\u00d7 and 8\u00d7 for both training and eval-\nuation. We utilize the pretrained monocular depth estimator\nto predict the depth for input images, and the poses are com-\nputed via COLMAP. We compare our method with FreeN-\neRF [55], SparseNeRF [50], and 3D-GS [26].\nTab.6 presents the quantitative results, where FSGS out-\nperforms SparseNeRF with over 0.75 higher PSNR and runs\n3,757\u00d7 faster, a significant leap that underscores its poten-\ntial for practical, real-world applications where speed is cru-\ncial and the environment is intricate. We also visualize the\nqualitative results in Fig.10, where FSGS significantly im-\nproves the visual quality of the scenes over 3D-GS, partic-\nularly in the realm of geometry reconstruction. FreeNeRF\nand SparseNeRF are constrained by geometric continuity\nfrom unseen perspectives and do not fully capitalize on the\navailable depth information.\nB.3. Visual Comparisons of the Rendered Depth\nWe demonstrate the qualitative results of the predicted\ndepth for each methods, as shown in Fig. 11. We compare\nour method with 3DGS [26], FreeNeRF [55] and SparseN-\neRF [50]. In the left we visualize the ground truth of the\nimages. FSGS significantly outperforms the three baselines\nin terms of depth quality and details. The depth maps pro-\nduced by FSGS are not only more accurate but also exhibit\na higher level of detail, showcasing its robustness in re-\nconstructing complex scenes. In contrast, both FreeNeRF\nand SparseNeRF exhibit limitations in geometric modeling\nand struggle to accurately learn complex geometries, lead-\n10\nFreeNeRF\n3D-GS\nSparseNeRF\nOurs\nGT\nOurdoor-1\nIndoor-1\nIndoor-2\nFigure 10. Qualitative Results on Datasets Collected by Mobile Phones. We test the generalization capacity of all methods on self-\ncaptured iPhone images, with a calibration process from COLMAP. Our method reveals the majority of scene details despite only adopting\nthree views in training.\nMethods\n1/8 Resolution\n1/4 Resolution\nFPS\u2191\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nFPS\u2191\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nMip-NeRF\n0.07\n14.74\n0.337\n0.602\n0.14\n13.84\n0.284\n0.631\n3D-GS\n225\n16.29\n0.408\n0.439\n127\n15.83\n0.413\n0.530\nDietNeRF\n0.05\n13.62\n0.263\n0.598\n0.08\n12.57\n0.228\n0.722\nRegNeRF\n0.07\n17.41\n0.517\n0.440\n0.14\n16.44\n0.443\n0.504\nFreeNeRF\n0.07\n18.07\n0.497\n0.426\n0.14\n17.14\n0.462\n0.509\nSparseNeRF\n0.07\n18.79\n0.539\n0.441\n0.04\n17.82\n0.472\n0.524\nOurs\n263\n19.54\n0.539\n0.403\n190\n18.41\n0.493\n0.471\nTable 6. Quantitative Comparison in Mobile Phone Datasets, with 3 Training Views. Our method continues to achieve the best\nperformance in the challenging mobile phone dataset.\ning to a distorted scene representation. Although SparseN-\neRF leverage depth prior, it still does not fully capture the\nfine-grained structures in real-world structures, resulting in\na noticeable drop in quality compared to FSGS. 3D-GS, on\nthe other hand, tends to lose fine details in the areas from\naway the camera, leading to a diminished overall quality in\nthe depth and texture of distant objects.\n11\nDepth (FreeNeRF)\nDepth (3D-GS)\nDepth (SparseNeRF)\nDepth (Ours)\nRGB (GT)\nFigure 11. Visual Comparisons of Predicted Depth. We visualize the estimated scene depth from all baselines. Noticeable NeRF alias\nartifacts are found in SparseNeRF and FreeNeRF. 3D-GS produces an oversmoothed scene geometry, while our method demonstrates\nvisually pleasing geometric details.\n12\nReferences\n[1] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter\nHedman, Ricardo Martin-Brualla, and Pratul P Srinivasan.\nMip-nerf: A multiscale representation for anti-aliasing neu-\nral radiance fields. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, pages 5855\u2013\n5864, 2021. 2, 3\n[2] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P.\nSrinivasan, and Peter Hedman.\nMip-NeRF 360:\nUn-\nbounded Anti-Aliased Neural Radiance Fields.\n2022\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 5460\u20135469, 2022. 3, 5, 6, 7\n[3] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P\nSrinivasan, and Peter Hedman. Mip-nerf 360: Unbounded\nanti-aliased neural radiance fields. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5470\u20135479, 2022. 2, 5\n[4] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P.\nSrinivasan, and Peter Hedman. Zip-nerf: Anti-aliased grid-\nbased neural radiance fields. ICCV, 2023. 2\n[5] Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, and Kwan-\nYee K Wong. Dreamavatar: Text-and-shape guided 3d hu-\nman avatar generation via diffusion models. arXiv preprint\narXiv:2304.00916, 2023. 3\n[6] Eric Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu,\nand Gordon Wetzstein. pi-gan: Periodic implicit genera-\ntive adversarial networks for 3d-aware image synthesis. In\narXiv, 2020. 3\n[7] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\nHao Su. Tensorf: Tensorial radiance fields. In European\nConference on Computer Vision, pages 333\u2013350. Springer,\n2022. 2\n[8] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang,\nFanbo Xiang, Jingyi Yu, and Hao Su.\nMvsnerf: Fast\ngeneralizable radiance field reconstruction from multi-view\nstereo. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 14124\u201314133, 2021. 1,\n2, 3\n[9] Tianlong Chen, Peihao Wang, Zhiwen Fan, and Zhangyang\nWang. Aug-nerf: Training stronger neural radiance fields\nwith triple-level physically-grounded augmentations.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 15191\u201315202, 2022.\n2\n[10] Julian Chibane, Aayush Bansal, Verica Lazova, and Gerard\nPons-Moll. Stereo radiance fields (srf): Learning view syn-\nthesis from sparse views of novel scenes. In IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR).\nIEEE, jun 2021. 2\n[11] Congyue Deng, Chiyu Jiang, Charles R Qi, Xinchen Yan,\nYin Zhou, Leonidas Guibas, Dragomir Anguelov, et al.\nNerdi: Single-view nerf synthesis with language-guided\ndiffusion as general image priors. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 20637\u201320647, 2023. 1\n[12] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ra-\nmanan.\nDepth-supervised nerf: Fewer views and faster\ntraining for free. arXiv preprint arXiv:2107.02791, 2021.\n3\n[13] Robert A Drebin, Loren Carpenter, and Pat Hanrahan.\nVolume rendering.\nACM Siggraph Computer Graphics,\n22(4):65\u201374, 1988. 1\n[14] Zhiwen Fan, Yifan Jiang, Peihao Wang, Xinyu Gong, Dejia\nXu, and Zhangyang Wang. Unified implicit neural styliza-\ntion. In European Conference on Computer Vision, pages\n636\u2013654. Springer, 2022. 2\n[15] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong\nChen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:\nRadiance fields without neural networks. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5501\u20135510, 2022. 2\n[16] Kyle Gao, Yina Gao, Hongjie He, Dening Lu, Linlin Xu,\nand Jonathan Li. Nerf: Neural radiance field in 3d vision, a\ncomprehensive review, 2023. 1\n[17] Stephan J Garbin, Marek Kowalski, Matthew Johnson,\nJamie Shotton, and Julien Valentin.\nFastnerf:\nHigh-\nfidelity neural rendering at 200fps.\narXiv preprint\narXiv:2103.10380, 2021. 2\n[18] Jiatao Gu, Lingjie Liu, Peng Wang, and Christian Theobalt.\nStylenerf:\nA style-based 3d aware generator for high-\nresolution image synthesis. In International Conference on\nLearning Representations, 2022. 3\n[19] Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai,\nFeitong Tan, and Ping Tan. Cascade cost volume for high-\nresolution multi-view stereo and stereo matching. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 2495\u20132504, 2020. 3\n[20] Yuan-Chen Guo, Di Kang, Linchao Bao, Yu He, and Song-\nHai Zhang. Nerfren: Neural radiance fields with reflections.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 18409\u201318418, 2022.\n2\n[21] Lukas H\u00a8ollein, Ang Cao, Andrew Owens, Justin Johnson,\nand Matthias Nie\u00dfner. Text2room: Extracting textured 3d\nmeshes from 2d text-to-image models. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion (ICCV), pages 7909\u20137920, October 2023. 3\n[22] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter\nAbbeel, and Ben Poole. Zero-shot text-guided object gen-\neration with dream fields. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 867\u2013876, 2022. 2\n[23] Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting nerf\non a diet: Semantically consistent few-shot view synthesis.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 5885\u20135894, 2021. 1, 3, 5\n[24] M. M. Johari, Y. Lepoittevin, and F. Fleuret.\nGeonerf:\nGeneralizing nerf with geometry priors.\nProceedings of\nthe IEEE international conference on Computer Vision and\nPattern Recognition (CVPR), 2022. 2\n[25] Animesh Karnewar, Andrea Vedaldi, David Novotny, and\nNiloy Mitra. Holodiffusion: Training a 3D diffusion model\nusing 2D images. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, 2023.\n3\n13\n[26] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler,\nand George Drettakis. 3d gaussian splatting for real-time\nradiance field rendering. ACM Transactions on Graphics\n(ToG), 42(4):1\u201314, 2023. 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n[27] Chen-Hsuan\nLin,\nJun\nGao,\nLuming\nTang,\nTowaki\nTakikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja\nFidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-\nresolution text-to-3d content creation.\nIn IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\n2023. 3\n[28] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot one image to 3d object, 2023. 3\n[29] Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon,\nNima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and\nAbhishek Kar. Local light field fusion: Practical view syn-\nthesis with prescriptive sampling guidelines. ACM Trans-\nactions on Graphics (TOG), 38(4):1\u201314, 2019. 5, 9\n[30] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting Scenes As Neural Radiance Fields for View\nSynthesis.\nCommunications of the ACM, 65(1):99\u2013106,\n2021. 1, 2, 5, 9\n[31] Thomas M\u00a8uller,\nAlex Evans,\nChristoph Schied,\nand\nAlexander Keller. Instant neural graphics primitives with\na multiresolution hash encoding.\nACM Transactions on\nGraphics (ToG), 41(4):1\u201315, 2022. 1, 2, 3\n[32] Michael Niemeyer, Jonathan T Barron, Ben Mildenhall,\nMehdi SM Sajjadi, Andreas Geiger, and Noha Radwan.\nRegnerf: Regularizing neural radiance fields for view syn-\nthesis from sparse inputs. arXiv preprint arXiv:2112.00724,\n2021. 3\n[33] Michael Niemeyer, Jonathan T Barron, Ben Mildenhall,\nMehdi SM Sajjadi, Andreas Geiger, and Noha Radwan.\nRegnerf: Regularizing neural radiance fields for view syn-\nthesis from sparse inputs. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 5480\u20135490, 2022. 1, 5\n[34] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv\npreprint arXiv:2209.14988, 2022. 3\n[35] AKM Shahariar Azad Rabby and Chengcui Zhang.\nBe-\nyondpixels: A comprehensive review of the evolution of\nneural radiance fields, 2023. 1\n[36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In International Conference on Machine Learning,\npages 8748\u20138763. PMLR, 2021. 3\n[37] Ren\u00b4e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vi-\nsion transformers for dense prediction. In Proceedings of\nthe IEEE/CVF international conference on computer vi-\nsion, pages 12179\u201312188, 2021. 5\n[38] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas\nGeiger. Kilonerf: Speeding up neural radiance fields with\nthousands of tiny mlps. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, pages 14335\u2013\n14345, 2021. 2, 3\n[39] Johannes\nL\nSchonberger\nand\nJan-Michael\nFrahm.\nStructure-from-motion revisited.\nIn Proceedings of\nthe IEEE conference on computer vision and pattern\nrecognition, pages 4104\u20134113, 2016. 3, 9\n[40] Katja Schwarz, Axel Sauer, Michael Niemeyer, Yiyi Liao,\nand Andreas Geiger.\nVoxgraf:\nFast 3d-Aware Image\nSynthesis With Sparse Voxel Grids.\nArXiv Preprint\nArXiv:2206.07695, 2022. 1\n[41] Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon\nKo, Hyeonsu Kim, Junho Kim, Jin-Hwa Kim, Jiyoung Lee,\nand Seungryong Kim. Let 2d diffusion model know 3d-\nconsistency for robust text-to-3d generation. arXiv preprint\narXiv:2303.07937, 2023. 3\n[42] Mohammed Suhail, Carlos Esteves, Leonid Sigal, and\nAmeesh Makadia. Light field neural rendering. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 8269\u20138279, 2022. 2\n[43] Cheng Sun, Min Sun, and Hwann-Tzong Chen.\nDi-\nrect Voxel Grid Optimization: Super-Fast Convergence for\nRadiance Fields Reconstruction.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5459\u20135469, 2022. 1, 2\n[44] Mukund Varma T, Peihao Wang, Xuxi Chen, Tianlong\nChen, Subhashini Venugopalan, and Zhangyang Wang. Is\nattention all that neRF needs?\nIn The Eleventh Interna-\ntional Conference on Learning Representations, 2023. 2\n[45] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran\nYi, Lizhuang Ma, and Dong Chen.\nMake-it-3d: High-\nfidelity 3d creation from a single image with diffusion prior,\n2023. 3\n[46] Ayush Tewari, Justus Thies, Ben Mildenhall, Pratul Srini-\nvasan, Edgar Tretschk, Yifan Wang, Christoph Lassner,\nVincent Sitzmann, Ricardo Martin-Brualla, Stephen Lom-\nbardi, Tomas Simon, Christian Theobalt, Matthias Niess-\nner, Jonathan T. Barron, Gordon Wetzstein, Michael Zoll-\nhoefer, and Vladislav Golyanik. Advances in neural render-\ning, 2022. 1\n[47] Prune Truong, Marie-Julie Rakotosaona, Fabian Manhardt,\nand Federico Tombari. Sparf: Neural radiance fields from\nsparse and noisy poses. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 4190\u20134200, 2023. 1, 5\n[48] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zick-\nler, Jonathan T Barron, and Pratul P Srinivasan. Ref-nerf:\nStructured view-dependent appearance for neural radiance\nfields. arXiv preprint arXiv:2112.03907, 2021. 2\n[49] Can Wang, Menglei Chai, Mingming He, Dongdong Chen,\nand Jing Liao. Clip-nerf: Text-and-image driven manip-\nulation of neural radiance fields.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 3835\u20133844, 2022. 2\n[50] Guangcong Wang, Zhaoxi Chen, Chen Change Loy, and Zi-\nwei Liu. Sparsenerf: Distilling depth ranking for few-shot\nnovel view synthesis.\narXiv preprint arXiv:2303.16196,\n2023. 1, 3, 5, 6, 7, 10\n14\n[51] Liao Wang, Jiakai Zhang, Xinhang Liu, Fuqiang Zhao,\nYanshun Zhang, Yingliang Zhang, Minye Wu, Jingyi Yu,\nand Lan Xu.\nFourier PlenOctrees for Dynamic Radi-\nance Field Rendering in Real-Time.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 13524\u201313534, 2022. 1\n[52] Peng Wang, Yuan Liu, Zhaoxi Chen, Lingjie Liu, Ziwei\nLiu, Taku Komura, Christian Theobalt, and Wenping Wang.\nF2-nerf: Fast neural radiance field training with free camera\ntrajectories. CVPR, 2023. 2\n[53] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P\nSrinivasan, Howard Zhou, Jonathan T Barron, Ricardo\nMartin-Brualla, Noah Snavely, and Thomas Funkhouser.\nIbrnet: Learning multi-view image-based rendering.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 4690\u20134699, 2021. 2\n[54] Dejia Xu,\nYifan Jiang,\nPeihao Wang,\nZhiwen Fan,\nHumphrey Shi, and Zhangyang Wang. Sinnerf: Training\nneural radiance fields on complex scenes from a single im-\nage. In European Conference on Computer Vision, pages\n736\u2013753. Springer, 2022. 1\n[55] Jiawei Yang, Marco Pavone, and Yue Wang. Freenerf: Im-\nproving few-shot neural rendering with free frequency reg-\nularization. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 8254\u2013\n8263, 2023. 1, 3, 5, 10\n[56] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long\nQuan.\nMvsnet: Depth inference for unstructured multi-\nview stereo. In Proceedings of the European conference\non computer vision (ECCV), pages 767\u2013783, 2018. 3\n[57] Wang Yifan, Felice Serena, Shihao Wu, Cengiz \u00a8Oztireli,\nand Olga Sorkine-Hornung. Differentiable surface splatting\nfor point-based geometry processing. ACM Transactions on\nGraphics (TOG), 38(6):1\u201314, 2019. 1\n[58] Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong\nChen, Benjamin Recht, and Angjoo Kanazawa.\nPlenox-\nels:\nRadiance Fields Without Neural Networks.\n2022\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 5491\u20135500, 2022. 1\n[59] Alex Yu,\nVickie Ye,\nMatthew Tancik,\nand Angjoo\nKanazawa. pixelnerf: Neural radiance fields from one or\nfew images. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 4578\u2013\n4587, 2021. 1, 2, 3\n[60] Kai Zhang, Nick Kolkin, Sai Bi, Fujun Luan, Zexiang Xu,\nEli Shechtman, and Noah Snavely. Arf: Artistic radiance\nfields, 2022. 2\n[61] Denis Zorin, Peter Schr\u00a8oder, and Wim Sweldens. Interpo-\nlating subdivision for meshes with arbitrary topology. In\nProceedings of the 23rd annual conference on Computer\ngraphics and interactive techniques, pages 189\u2013192, 1996.\n4\n15\n"
  },
  {
    "title": "PyNeRF: Pyramidal Neural Radiance Fields",
    "link": "https://arxiv.org/pdf/2312.00252.pdf",
    "upvote": "8",
    "text": "PyNeRF: Pyramidal Neural Radiance Fields\nHaithem Turki\nCarnegie Mellon University\nhturki@cs.cmu.edu\nMichael Zollh\u00f6fer\nMeta Reality Labs Research\nzollhoefer@meta.com\nChristian Richardt\nMeta Reality Labs Research\ncrichardt@meta.com\nDeva Ramanan\nCarnegie Mellon University\ndeva@cs.cmu.edu\nAbstract\nNeural Radiance Fields (NeRFs) can be dramatically accelerated by spatial grid\nrepresentations [6, 9, 20, 25]. However, they do not explicitly reason about scale\nand so introduce aliasing artifacts when reconstructing scenes captured at different\ncamera distances. Mip-NeRF and its extensions propose scale-aware renderers that\nproject volumetric frustums rather than point samples but such approaches rely\non positional encodings that are not readily compatible with grid methods. We\npropose a simple modification to grid-based models by training model heads at\ndifferent spatial grid resolutions. At render time, we simply use coarser grids to\nrender samples that cover larger volumes. Our method can be easily applied to\nexisting accelerated NeRF methods and significantly improves rendering quality\n(reducing error rates by 20\u201390% across synthetic and unbounded real-world scenes)\nwhile incurring minimal performance overhead (as each model head is quick to\nevaluate). Compared to Mip-NeRF, we reduce error rates by 20% while training\nover 60\u00d7 faster.\n1\nIntroduction\nRecent advances in neural volumetric rendering techniques, most notably around Neural Radiance\nFields [19] (NeRFs), have lead to significant progress towards photo-realistic novel view synthesis.\nHowever, although NeRF provides state-of-the-art rendering quality, it is notoriously slow to train\nand render due in part to its internal MLP representation. It further assumes that scene content is\nequidistant from the camera and rendering quality degrades due to aliasing and excessive blurring\nwhen that assumption is violated.\nRecent methods [6, 9, 20, 25] accelerate NeRF training and rendering significantly through the use of\ngrid representations. Others, such as Mip-NeRF [2], address aliasing by projecting camera frustum\nvolumes instead of point-sampling rays. However, these anti-aliasing methods rely on the base NeRF\nMLP representation (and are thus slow) and are incompatible with grid representations due to their\nreliance on non-grid-based inputs.\nInspired by divide-and-conquer NeRF extensions [22, 23, 27, 30] and classical approaches such as\nGaussian pyramids [1] and mipmaps [34], we propose a simple approach that can easily be applied\nto any existing accelerated NeRF implementation. We train a pyramid of models at different scales,\nsample along camera rays (as in the original NeRF), and simply query coarser levels of the pyramid\nfor samples that cover larger volumes (similar to voxel cone tracing [8]). Our method is simple\nto implement and significantly improves the rendering quality of fast rendering approaches with\nminimal performance overhead.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2312.00252v1  [cs.CV]  30 Nov 2023\n(a) NeRF\n(b) Mip-NeRF\n(c) Grid Methods (eg: iNGP)\n(d) PyNeRF\nFigure 1: Comparison of methods. (a) NeRF traces a ray from the camera\u2019s center of projection\nthrough each pixel and samples points x along each ray. Sample locations are then encoded with a\npositional encoding to produce a feature \u03b3(x) that is fed into an MLP. (b) Mip-NeRF instead reasons\nabout volumes by defining a 3D conical frustum per camera pixel. It splits the frustum into sampled\nvolumes, approximates them as multivariate Gaussians, and computes the integral of the positional\nencodings of the coordinates contained within the Gaussians. Similar to NeRF, these features are then\nfed into an MLP. (c) Accelerated grid methods, such as iNGP, sample points as in NeRF, but do not\nuse positional encoding and instead featurize each point by interpolating between vertices in a feature\ngrid. These features are then passed into a much smaller MLP, which greatly accelerates training\nand rendering. (d) PyNeRF also uses feature grids, but reasons about volumes by training separate\nmodels at different scales (similar to a mipmap). It calculates the area covered by each sample in\nworld coordinates, queries the models at the closest corresponding resolutions, and interpolates their\noutputs.\nContribution: Our primary contribution is a partitioning method that can be easily adapted to any\nexisting grid-rendering approach. We present state-of-the-art reconstruction results against a wide\nrange of datasets, including on novel scenes we designed that explicitly target common aliasing\npatterns. We evaluate different posssible architectures and demonstrate that our design choices\nprovide a high level of visual fidelity while maintaining the rendering speed of fast NeRF approaches.\n2\nRelated Work\nThe now-seminal Neural Radiance Fields (NeRF) paper [19] inspired a vast corpus of follow-up\nwork. We discuss a non-exhaustive list of such approaches along axes relevant to our work.\nGrid-based methods. The original NeRF took 1\u20132 days to train, with extensions for unbounded\nscenes [3, 40] taking longer. Once trained, rendering takes seconds per frame and is far below\ninteractive thresholds. NSVF [17] combines NeRF\u2019s implicit representation with a voxel octree\nthat allows for empty-space skipping and improves inference speeds by 10\u00d7. Follow-up works [10,\n11, 39] further improve rendering to interactive speeds by storing precomputed model outputs into\nauxiliary grid structures that bypass the need to query the original model altogether at render time.\nPlenoxels [25] and DVGO [26] accelerate both training and rendering by directly optimizing a voxel\ngrid instead of an MLP to train in minutes or even seconds. TensoRF [6] and K-Planes [9] instead\nuse the product of low-rank tensors to approximate the voxel grid and reduce memory usage, while\nInstant-NGP [20] (iNGP) encodes features into a multi-resolution hash table. The main goal of our\nwork is to combine the speed benefits of grid-based methods with an approach that maintains quality\nacross different rendering scales.\n2\nLevel 1\nLevel 2\nLevel 3\nLevel 4\nFigure 2: We visualize renderings from a pyramid of spatial grid-based NeRFs trained for different\nvoxel resolutions. Models at finer pyramid levels tend to capture finer content.\nDivide-and-conquer. Several works note the diminishing returns in using large networks to represent\nscene content, and instead render the area of interest with multiple smaller models. DeRF [22] and\nKiloNeRF [23] focus on inference speed while Mega-NeRF [30], Block-NeRF [27], and SUDS [31]\nuse scene decomposition to efficiently train city-scale neural representations. Our method is similar\nin philosophy, although we partition across different resolutions instead of geographical area.\nAliasing. The original NeRF assumes that scene content is captured at roughly equidistant camera\ndistances and emits blurry renderings when the assumption is violated. Mip-NeRF [2] reasons about\nthe volume covered by each camera ray and proposes an integrated positional encoding that alleviates\naliasing. Mip-NeRF 360 [3] extends the base method to unbounded scenes. Exact-NeRF [14] derives\na more precise integration formula that better reconstructs far-away scene content. Bungee-NeRF [36]\nleverages Mip-NeRF and further adopts a coarse-to-fine training approach with residual blocks to\ntrain on large-scale scenes with viewpoint variation. LIRF [37] proposes a multiscale image-based\nrepresentation that can generalize across scenes. The methods all build upon the original NeRF MLP\nmodel and do not readily translate to accelerated grid-based methods.\nConcurrent work. Several contemporary efforts explore the intersection of anti-aliasing and fast\nrendering. Zip-NeRF [4] combines a hash table representation with a multi-sampling method that\napproximates the true integral of features contained within each camera ray\u2019s view frustum. Although\nit trains faster than Mip-NeRF, it is explicitly not designed for fast rendering as the multi-sampling\nadds significant overhead. Mip-VoG [12] downsamples and blurs a voxel grid according to the volume\nof each sample in world coordinates. We compare their reported numbers to ours in Section 4.2.\nTri-MipRF [13] uses a similar prefiltering approach, but with triplanes instead of a 3D voxel grid.\nClassical methods. Similar to PyNeRF, classic image processing methods, such as Gaussian [1] and\nLaplacian [5] hierarchy, maintain a coarse-to-fine pyramid of different images at different resolutions.\nCompared to Mip-NeRF, which attempts to learn a single MLP model across all scales, one could\nargue that our work demonstrates that the classic pyramid approach can be efficiently adapted to neural\nvolumetric models. In addition, our ray sampling method is similar to Crassin et al.\u2019s approach [8],\nwhich approximates cone tracing by sampling along camera rays and querying different mipmap\nlevels according the spatial footprint of each sample (stored as a voxel octree in their approach and as\na NeRF model in ours).\n3\n(a) Point Sampling\n(c8, \u03c38) = f8(x, d)\n(c9, \u03c39) = f9(x, d)\n(b) Model Evaluation\nc = 0.4c8 + 0.6c9\n\u03c3 = 0.4\u03c38 + 0.6\u03c39\n(c) Interpolation\nFigure 3: Overview. (a) We sample frustums along the camera ray corresponding to each pixel and\nderive the scale of each sample according to its width in world coordinates. (b) We query the model\nheads closest to the scale of each sample. (c) We derive a single color and density value for each\nsample by interpolating between model outputs according to scale.\n3\nApproach\n3.1\nPreliminaries\nNeRF. NeRF [19] represents a scene within a continuous volumetric radiance field that captures\ngeometry and view-dependent appearance. It encodes the scene within the weights of a multi-\nlayer perceptron (MLP). At render time, NeRF casts a camera ray r for each image pixel. NeRF\nsamples multiple positions xi along each ray and queries the MLP at each position (along with\nthe ray viewing direction d) to obtain density and color values \u03c3i and ci. To better capture\nhigh-frequency details, NeRF maps xi and d through an L-dimensional positional encoding (PE)\n\u03b3(x) = [sin(20\u03c0x), cos(20\u03c0x), . . . , sin(2L\u03c0x), cos(2L\u03c0x)] instead of directly using them as MLP\ninputs. It then composites a single color prediction \u02c6C(r) per ray using numerical quadrature\nPN\u22121\ni=0 Ti(1 \u2212 exp(\u2212\u03c3i\u03b4i)) ci, where Ti = exp(\u2212 Pi\u22121\nj=0 \u03c3j\u03b4j) and \u03b4i is the distance between\nsamples. The training process optimizes the model by sampling batches R of image pixels and\nminimizing the loss P\nr\u2208R\n\r\r\rC(r) \u2212 \u02c6C(r)\n\r\r\r\n2\n. We refer the reader to Mildenhall et al. [19] for details.\nAnti-aliasing. The original NeRF suffers from aliasing artifacts when reconstructing scene content\nobserved at different distances or resolutions due to its reliance on point-sampled features. As these\nfeatures ignore the volume viewed by each ray, different cameras viewing the same position from\ndifferent distances may produce the same ambiguous feature. Mip-NeRF [2] and variants instead\nreason about volumes by defining a 3D conical frustum per camera pixel. It featurizes intervals\nwithin the frustum with a integrated positional encoding (IPE) that approximates each frustum as a\nmultivariate Gaussian to estimate the integral E[\u03b3(x)] over the PEs of the coordinates within it.\nGrid-based acceleration. Various methods [6, 9, 20, 25, 26] eschew NeRF\u2019s positional encoding\nand instead store learned features into a grid-based structure, e.g. implemented as an explicit voxel\ngrid, hash table, or a collection of low-rank tensors. The features are interpolated based on the\nposition of each sample and then passed into a hard-coded function or much smaller MLP to produce\ndensity and color, thereby accelerating training and rendering by orders of magnitude. However,\nthese approaches all use the same volume-insensitive point sampling of the original NeRF and do not\nhave a straightforward analogy to Mip-NeRF\u2019s IPE as they no longer use positional encoding.\n3.2\nMultiscale sampling\nAssume that each sample x (where we drop the i index to reduce notational clutter) is associated with\nan integration volume. Intuitively, samples close to a camera correspond to small volumes, while\nsamples far away from a camera correspond to large volumes (Figure 3). Our crucial insight for\nenabling multiscale sampling with grid-based approaches is remarkably simple: we train separate\nNeRFs at different voxel resolutions and simply use coarser NeRFs for samples covering larger\nvolumes. Specifically, we define a hierarchy of L resolutions that divide the world into voxels of\nlength 1/N0, ..., 1/NL\u22121, where Nl+1 = sNl and s is a constant scaling factor. We also define a\nfunction fl(x, d) at each level that maps from sample location x and viewing direction d to color\nc and density \u03c3. fl can be implemented by any grid-based NeRF; in our experiments, we use a\n4\nAlgorithm 1 PyNeRF rendering function\nInput: m rays r, L pyramid levels, hierarchy mapping function M, base resolution N0, scaling\nfactor s\nOutput: m estimated colors c\nx, d, P(x) \u2190 sample(r)\n\u25b7 Sample points x along each ray with direction d and area P(x)\nM(P(x)) \u2190 logs(P(x)/N0)\n\u25b7 Equation 1\nl \u2190 min(L \u2212 1, max(0, \u2308M(P(x))\u2309))\n\u25b7 Equation 2\nw \u2190 l \u2212 M(P(x))\n\u25b7 Equation 5\nmodel_out \u2190 zeros(len(x))\n\u25b7 Zero-initialize model outputs for each sample x\nfor i in unique(l) do\n\u25b7 Iterate over sample levels\nmodel_out[l = i] += w[l = i]fi(x[l = i], d[l = i])\nmodel_out[l = i] += (1 \u2212 w)[l = i]fi\u22121(x[l = i], d[l = i])\nend for\nc \u2190 composite(model_out)\n\u25b7 Composite model outputs into per-ray color c\nreturn c\nhash table followed by small density and color MLPs, similar to iNGP. We further define a mapping\nfunction M that assigns the integration volume of sample x to the hierarchy level l. We explore\ndifferent alternatives, but find that selecting the level whose voxels project to the 2D pixel area P(x)\nused to define the integration volume works well:\nM(P(x)) = logs(P(x)/N0)\n(1)\nl = min(L \u2212 1, max(0, \u2308M(P(x))\u2309))\n(2)\n\u03c3, c = fl(x, d),\n[GaussPyNeRF] (3)\nwhere \u2308\u00b7\u2309 is the ceiling function. Such a model can be seen as a (Gaussian) pyramid of spatial\ngrid-based NeRFs (Fig. 2). If the final density and color were obtained by summing across different\npyramid levels, the resulting levels would learn to specialize to residual or \u201cband-pass\u201d frequencies\n(as in a 3D Laplacian pyramid [5]):\n\u03c3, c =\nl\nX\ni=0\nfi(x, d).\n[LaplacianPyNeRF] (4)\nOur experiments show that such a representation is performant, but expensive since it requires l\nmodel evaluations per sample. Instead, we find a good tradeoff is to linearly interpolate between two\nmodel evaluations at the levels just larger than and smaller than the target integration volume:\n\u03c3, c = wfl(x, d) + (1 \u2212 w)fl\u22121(x, d),\nwhere\nw = l \u2212 M(P(x)).\n(Default) [PyNeRF] (5)\nThis adds the cost of only a single additional evaluation (increasing the overall rendering time from\n0.0045 to 0.005 ms per pixel) while maintaining rendering quality (see Section 4.6). Our algorithm is\nsummarized in Algorithm 1.\nMatching areas vs volumes. One might suspect it may be better to select the voxel level l whose\nvolume best matches the sample\u2019s 3D integration volume. We experimented with this, but found it\nmore effective to match the projected 2D pixel area rather than volumes. Note that both approaches\nwould produce identical results if the 3D volume was always a cube, but volumes may be elongated\nalong the ray depending on the sampling pattern. Matching areas is preferable because most visible\n3D scenes consist of empty space and surfaces, implying that when computing the composite color for\na ray r, most of the contribution will come from a few samples x lying near the surface of intersection.\nWhen considering the target 3D integration volume associated with x, most of the contribution to the\nfinal composite color will come from integrating along the 2D surface (since the rest of the 3D volume\nis either empty or hidden). This loosely suggests we should select levels of the voxel hierarchy based\non (projected) area rather than volume.\nHierarchical grid structures. Our method can be applied to any accelerated grid method irrespective\nof the underyling storage. However, a drawback of this approach is an increased on-disk serialization\nfootprint due to training a hierarchy of spatial grid NeRFs. A possible solution is to exploit hierarchical\ngrid structures that already exist within the base NeRF. Note that multi-resolution grids such as those\nused by iNGP [20] or K-Planes [9] already define a scale hierarchy that is a natural fit for PyNeRF.\n5\nTable 1: Synthetic results. PyNeRF outperforms all baselines and trains over 60\u00d7 faster than Mip-\nNeRF. Both PyNeRF and Mip-NeRF properly reconstruct the brick wall in the Blender-A dataset, but\nMip-NeRF fails to accurately reconstruct checkerboard patterns.\nMultiscale Blender [2]\nBlender-A\n\u2191PSNR\n\u2191SSIM\n\u2193LPIPS\n\u2193Avg Error\n\u2193Train Time (h)\n\u2191PSNR\n\u2191SSIM\n\u2193LPIPS\n\u2193Avg Error\n\u2193 Train Time (h)\nPlenoxels [25]\n24.98\n0.843\n0.161\n0.080\n0:28\n18.13\n0.511\n0.523\n0.190\n0:40\nK-Planes [9]\n29.88\n0.946\n0.058\n0.022\n0:32\n21.17\n0.593\n0.641\n0.405\n1:22\nTensoRF [6]\n30.04\n0.948\n0.056\n0.021\n0:27\n27.01\n0.785\n0.197\n0.054\n1:20\niNGP [20]\n30.21\n0.958\n0.040\n0.022\n0:20\n20.85\n0.767\n0.244\n0.089\n0:56\nNerfacto [28]\n29.56\n0.947\n0.051\n0.022\n0:25\n27.46\n0.796\n0.195\n0.053\n1:07\nMip-VoG [12]\n30.42\n0.954\n0.053\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\nMip-NeRF [2]\n34.50\n0.974\n0.017\n0.009\n29:49\n31.33\n0.894\n0.098\n0.063\n30:12\nPyNeRF\n34.78\n0.976\n0.015\n0.008\n0:25\n41.99\n0.986\n0.007\n0.004\n1:10\nRather than learning a separate feature grid for each model in our pyramid, we can reuse the same\nmulti-resolution features across levels (while still training different MLP heads).\nMulti-resolution pixel input. One added benefit of the above is that one can train with multiscale\ntraining data, which is particularly helpful for learning large, city-scale NeRFs [27, 30, 31, 36, 38]. For\nsuch scenarios, even storing high-resolution pixel imagery may be cumbersome. In our formulation,\none can store low-resolution images and quickly train a coarse scene representation. The benefits are\nmultiple. Firstly, divide-and-conquer approaches such as Mega-NeRF [31] partition large scenes into\nsmaller cells and train using different training pixel/ray subsets for each (to avoid training on irrelevant\ndata). However, in the absence of depth sensors or a priori 3D scene knowledge, Mega-NeRF is\nlimited in its ability to prune irrelevant pixels/rays (due to intervening occluders) which empirically\nbloat the size of each training partition by 2\u00d7 [30]. With our approach, we can learn a coarse 3D\nknowledge of the scene on downsampled images and then filter higher-resolution data partitions\nmore efficiently. Once trained, lower-resolution levels can also serve as an efficient initialization for\nfiner layers. In addition, many contemporary NeRF methods use occupancy grids [20] or proposal\nnetworks [3] to generate refined samples near surfaces. We can quickly train these along with our\ninitial low-resolution model and then use them to train higher-resolution levels in a sample-efficient\nmanner. We show in our experiments that such course-to-fine multiscale training can speed up\nconvergence (Section 4.5).\nUnsupervised levels. A naive implementation of our method will degrade when zooming in and out\nof areas that have not been seen at training time. Our implementation mitigates this by maintaining\nan auxiliary data structure (similar to an occupancy grid [20]) that tracks the coarsest and finest levels\nqueried in each region during training. We then use the structure at inference time to only query\nlevels that were supervised during training.\n4\nExperiments\nWe first evaluate PyNeRF\u2019s performance by measuring its reconstruction quality on bounded synthetic\n(Section 4.2) and unbounded real-world (Section 4.3) scenes. We demonstrate PyNeRF\u2019s generaliz-\nability by evaluating it on additional NeRF backbones (Section 4.4) and then explore the convergence\nbenefits of using multiscale training data in city-scale reconstruction scenarios (Section 4.5). We\nablate our design decisions in Section 4.6.\n4.1\nExperimental Setup\nTraining. We implement PyNeRF on top of the Nerfstudio library [28] and train on each scene\nwith 8,192 rays per batch by default for 20,000 iterations on the Multiscale Blender and Mip-NeRF\n360 datasets, and 50,000 iterations on the Boat dataset and Blender-A. We train a hierarchy of 8\nPyNeRF levels backed by a single multi-resolution hash table similar to that used by iNGP [20] in\nSection 4.2 and Section 4.3 before evaluating additional backbones in Section 4.4. We use 4 features\nper level with a hash table size of 220 by default, which we found to give the best quality-performance\ntrade-off on the A100 GPUs we use in our experiments. Each PyNeRF uses a 64-channel density\nMLP with one hidden layer followed by a 128-channel color MLP with two hidden layers. We\nuse similar model capacities in our baselines for fairness. We sample rays using an occupancy\ngrid [20] on the Multiscale Blender dataset, and with a proposal network [3] on all others. We use\n6\nPlenoxels, 30 FPS\nMic\nK-Planes, 25 FPS\niNGP, 40 FPS\nMip-NeRF, 0.1 FPS PyNeRF (Ours), 33 FPS\nGround Truth\nShip\nBrick\nCheckerboard\nFigure 4: Synthetic results. PyNeRF and Mip-NeRF provide comparable results on the first three\nscenes that are crisper than those of the other fast renderers. Mip-NeRF does not accurately render\nthe tiles in the last row while PyNeRF recreates them near-perfectly.\ngradient scaling [21] to improve training stability in scenes with that capture content at close distance\n(Blender-A and Boat). We parameterize unbounded scenes with Mip-NeRF 360\u2019s contraction method.\nMetrics. We report quantitative results based on PSNR, SSIM [33], and the AlexNet implementation\nof LPIPS [41], along with the training time in hours as measured on a single A100 GPU. For ease of\ncomparison, we also report the \u201caverage\u201d error metric proposed by Mip-NeRF [2] composed of the\ngeometric mean of MSE = 10\u2212PSNR/10,\n\u221a\n1 \u2212 SSIM, and LPIPS.\n4.2\nSynthetic Reconstruction\nDatasets. We evaluate PyNeRF on the Multiscale Blender dataset proposed by Mip-NeRF along with\nour own Blender scenes (which we name \u201cBlender-A\u201d) intended to further probe the anti-aliasing\nability of our approach (by reconstructing a slanted checkerboard and zooming into a brick wall).\nBaselines. We compare PyNeRF to several fast-rendering approaches, namely Instant-NGP [20]\nand Nerfacto [28], which store features within a multi-resolution hash table, Plenoxels [25] which\noptimizes an explicit voxel grid, and TensoRF [6] and K-Planes [9], which rely on low-rank tensor\ndecomposition. We also compare our Multiscale Blender results to those reported by Mip-VoG [12],\na contemporary fast anti-aliasing approach, and to Mip-NeRF [2] on both datasets.\nResults. We summarize our results in Table 1 and show qualitative examples in Figure 4. PyNeRF\noutperforms all fast rendering approaches as well as Mip-VoG by a wide margin and is slightly\nbetter than Mip-NeRF on Multiscale Blender while training over 60\u00d7 faster. Both PyNeRF and\nMip-NeRF properly reconstruct the brick wall in the Blender-A dataset, but Mip-NeRF fails to\naccurately reconstruct checkerboard patterns.\n4.3\nReal-World Reconstruction\nDatasets. We evaluate PyNeRF on the Boat scene of the ADOP [24] dataset, which to our knowledge\nis one of the only publicly available unbounded real-world captures that captures its primary object of\n7\nPlenoxels, 3 FPS\nBicycle\nK-Planes, 3 FPS\niNGP, 4 FPS\nMip-NeRF 360, 0.04 FPS PyNeRF (Ours), 4 FPS\nGround Truth\nBoat\nFigure 5: Real-world results. PyNeRF reconstructs higher-fidelity details (such as the spokes on the\nbicycle and the lettering within the boat) than other methods.\nTable 2: Real-world results. PyNeRF outperforms all baselines in PSNR and average error, and\ntrains 40\u00d7 faster than Mip-NeRF 360 and 100\u00d7 faster than Exact-NeRF (the next best methods).\nBoat [24]\nMip-NeRF 360 [3]\n\u2191PSNR\n\u2191SSIM\n\u2193LPIPS\n\u2193Avg Error\n\u2193Train Time (h)\n\u2191PSNR\n\u2191SSIM\n\u2193LPIPS\n\u2193Avg Error\n\u2193 Train Time (h)\nPlenoxels [25]\n17.05\n0.505\n0.617\n0.185\n2:14\n21.88\n0.606\n0.524\n0.117\n1:00\nK-Planes [9]\n18.00\n0.501\n0.590\n0.168\n2:41\n21.53\n0.577\n0.500\n0.120\n1:08\nTensoRF [6]\n14.75\n0.398\n0.630\n0.234\n2:30\n18.07\n0.439\n0.677\n0.181\n1:07\niNGP [20]\n15.34\n0.433\n0.646\n0.222\n1:42\n21.14\n0.568\n0.521\n0.126\n0:40\nNerfacto [28]\n19.27\n0.570\n0.425\n0.135\n2:12\n22.47\n0.616\n0.431\n0.105\n1:02\nMip-NeRF 360 w/ GLO [3]\n20.03\n0.595\n0.416\n0.124\n37:28\n22.76\n0.664\n0.342\n0.095\n37:35\nMip-NeRF 360 w/o GLO [3]\n15.92\n0.480\n0.501\n0.194\n37:10\n22.70\n0.664\n0.342\n0.095\n37:22\nExact-NeRF w/ GLO [14]\n20.21\n0.601\n0.425\n0.123\n109:11\n21.40\n0.619\n0.416\n0.121\n110:06\nExact-NeRF w/o GLO [14]\n16.33\n0.489\n0.510\n0.187\n107:52\n22.56\n0.619\n0.410\n0.121\n108:11\nPyNeRF\n20.43\n0.601\n0.422\n0.121\n2:12\n23.09\n0.654\n0.358\n0.094\n1:00\ninterest from different camera distances. For further comparison, we construct a multiscale version of\nthe outdoor scenes in the Mip-NeRF 360 [3] dataset using the same protocol as Multiscale Blender [2].\nBaselines. We compare PyNeRF to the same fast-rendering approaches as in Section 4.2, along with\ntwo unbounded Mip-NeRF variants: Mip-NeRF 360 [3] and Exact-NeRF [14]. We report numbers\non each variant with and without generative latent optimization [18] to account for lighting changes.\nResults. We summarize our results in Table 2 along with qualitative results in Figure 5. Once\nagain, PyNeRF outperforms all baselines, trains 40\u00d7 faster than Mip-NeRF 360, and 100\u00d7 faster than\nExact-NeRF (the next best alternatives).\n4.4\nAdditional Backbones\nMethods. We demonstrate how PyNeRF can be applied to any grid-based NeRF method by evaluating\nit with K-Planes [9] and TensoRF [6] in addition to our default iNGP-based implementatino. We\ntake advantage of the inherent multi-resolution structure of iNGP and K-Planes by reusing the same\nfeature grid across PyNeRF levels and train a separate feature grid per level in our TensoRF variant.\nResults. We train the PyNeRF variants along with their backbones across the datasets described in\nSection 4.2 and Section 4.3, and summarize the results in Table 3. All PyNeRF variants show clear\nimprovements over their base methods.\n4.5\nCity-Scale Convergence\nDataset. We evaluate PyNeRF\u2019s convergence properties on the the Argoverse 2 [35] Sensor dataset\n(to our knowledge, the largest city-scale dataset publicly available). We select the largest overlapping\nsubset of logs and filter out moving objects through a pretrained segmentation model [7]. The\nresulting training set contains 400 billion rays across 150K video frames.\nMethods. We use SUDS [31] as the backbone model in our experiments. We begin training our\nmethod on 8\u00d7 downsampled images (containing 64\u00d7 fewer rays) for 5,000 iterations and then on\n8\nTable 3: Additional backbones. We train the PyNeRF variants along with their backbones across the\ndatasets described in Section 4.2 and Section 4.3 All PyNeRF variants outperform their baselines by\na wide margin.\nSynthetic\nReal-World\n\u2191PSNR\n\u2191SSIM\n\u2193LPIPS\n\u2193Avg Error\n\u2191PSNR\n\u2191SSIM\n\u2193LPIPS\n\u2193Avg Error\niNGP [20]\n28.86\n0.916\n0.087\n0.032\n19.94\n0.541\n0.537\n0.146\nK-Planes [9]\n27.90\n0.865\n0.131\n0.047\n20.54\n0.553\n0.520\n0.136\nTensoRF [6]\n29.12\n0.902\n0.100\n0.042\n17.21\n0.421\n0.696\n0.200\nPyNeRF\n36.22\n0.979\n0.013\n0.004\n22.65\n0.645\n0.369\n0.098\nPyNeRF-K-Planes\n35.42\n0.975\n0.014\n0.005\n22.00\n0.622\n0.405\n0.108\nPyNeRF-TensoRF\n35.67\n0.976\n0.015\n0.005\n21.35\n0.568\n0.482\n0.122\nTable 4: City-scale convergence. We track rendering quality over the first four hours of training.\nPyNeRF achieves the same rendering quality as SUDS 2\u00d7 faster.\n\u2191 PSNR\nTime (h)\n1:00\n2:00\n3:00\n4:00\nSUDS [31]\n16.01\n17.41\n18.08\n18.53\nPyNeRF\n17.17\n18.44\n18.59\n18.73\n\u2191 SSIM\nTime (h)\n1:00\n2:00\n3:00\n4:00\nSUDS [31]\n0.570\n0.600\n0.602\n0.606\nPyNeRF\n0.614\n0.618\n0.619\n0.621\n\u2193 LPIPS\nTime (h)\n1:00\n2:00\n3:00\n4:00\nSUDS [31]\n0.531\n0.496\n0.470\n0.466\nPyNeRF\n0.521\n0.485\n0.469\n0.465\n\u2193 Avg Error\nTime (h)\n1:00\n2:00\n3:00\n4:00\nSUDS [31]\n0.182\n0.160\n0.150\n0.145\nPyNeRF\n0.165\n0.146\n0.144\n0.142\nprogressively higher resolutions (downsampled to 4\u00d7, 2\u00d7, and 1\u00d7) every 5,000 iterations hereafter.\nWe compare to the original SUDS method as a baseline.\nMetrics. We report the evolution of the quality metrics used in Section 4.2 and Section 4.3 over the\nfirst four hours of the training process.\nResults. We summarize our results in Table 4. PyNeRF converges more rapidly than the SUDS\nbaseline, achieving the same rendering quality at 2 hours as SUDS after 4.\n4.6\nDiagnostics\nMethods. We validate our design decisions by testing several variants. We ablate our MLP-level\ninterpolation described in Equation 5 and compare it to the GausssPyNeRF and LaplacianPyNeRF\nvariants described in Section 3.2 along with another that instead interpolates the learned grid feature\nvectors (which avoids the need for an additional MLP evaluation per sample). As increased storage\nfootprint is a potential drawback method, we compare our default strategy of sharing the same\nmulti-resolution feature grid across PyNeRF levels to the naive implementation that trains a separate\ngrid per level. We also explore using 3D sample volumes instead of projected 2D pixel areas to\ndetermine voxel levels l.\nResults. We train our method and variants as described in Section 4.2 and Section 4.3, and summarize\nthe results (averaged across datasets) in Table 5. Our proposed interpolation method strikes a good\nbalance \u2014 its performance is near-identical to the full LaplacianPyNeRF approach while training\n3\u00d7 faster (and is significantly better than the other interpolation methods). Our strategy of reusing\nthe same feature grid across levels performs comparably to the naive implementation while training\nfaster due to fewer feature grid lookups. Using 2D pixel areas instead of 3D volumes to determine\nvoxel level l provides an improvement.\n9\nTable 5: Diagnostics. The rendering quality of our interpolation method is near-identical to the full\nresidual approach while training 3\u00d7 faster, and is significantly better than other alternatives. Reusing\nthe same feature grid across levels performs comparably to storing separate hash tables per level\nwhile training faster.\nMethod\nOur\nInterp.\nShared\nFeatures\n2D\nArea\n\u2191PSNR\n\u2191SSIM\n\u2193LPIPS\n\u2193 Avg\nError\n\u2193 Train\nTime (h)\nGaussPyNeRF (Eq. 3)\n\u2717\n\u2713\n\u2713\n28.72\n0.803\n0.201\n0.056\n0:43\nLaplacianPyNeRF (Eq. 4)\n\u2717\n\u2713\n\u2713\n29.48\n0.813\n0.190\n0.052\n2:44\nFeature grid interpolation\n\u2717\n\u2717\n\u2713\n28.45\n0.767\n0.244\n0.070\n0:46\nSeparate hash tables\n\u2713\n\u2717\n\u2713\n29.41\n0.813\n0.196\n0.054\n0:52\nLevels w/ 3D Volumes\n\u2713\n\u2713\n\u2717\n29.19\n0.811\n0.184\n0.054\n0:48\nPyNeRF\n\u2713\n\u2713\n\u2713\n29.44\n0.812\n0.191\n0.053\n0:48\n5\nLimitations\nAlthough our method generalizes to any grid-based method (Section 4.4), it requires a larger on-disk\nserialization footprint due to training a hierarchy of spatial grid NeRFs. This can be mitigated\nby reusing the same feature grid when the underlying backbone uses a multi-resolution feature\ngrid [9, 20], but this is not true of all methods [6, 25].\n6\nSocietal Impact\nOur method facilitates the rapid construction of high-quality neural representations in a resource\nefficient manner. As such, the risks inherent to our work is similar to those of other neural rendering\npapers, namely privacy and security concerns related to the intentional or inadvertent capture or\nprivacy-sensitive information such as human faces and vehicle license plate numbers. While we did\nnot apply our approach to data with privacy or security concerns, there is a risk, similar to other neural\nrendering approaches, that such data could end up in the trained model if the employed datasets\nare not properly filtered before use. Many recent approaches [15, 16, 29, 31, 42] distill semantics\ninto NeRF\u2019s representation, which may be used to filter out sensitive information at render time.\nHowever this information would still reside in the model itself. This could in turn be mitigated by\npreprocessing the input data used to train the model [32].\n7\nConclusion\nWe propose a method that significantly improves the anti-aliasing properties of fast volumetric\nrenderers. Our approach can be easily applied to any existing grid-based NeRF, and although simple,\nprovides state-of-the-art reconstruction results against a wide variety of datasets (while training\n60\u2013100\u00d7 faster than existing anti-aliasing methods). We propose several synthetic scenes that model\ncommon aliasing patterns as few existing NeRF datasets cover these scenarios in practice. Creating\nand sharing additional real-world captures would likely facilitate further research.\nAcknowledgements. HT and DR were supported in part by the Intelligence Advanced Research\nProjects Activity (IARPA) via Department of Interior/ Interior Business Center (DOI/IBC) contract\nnumber 140D0423C0074. The U.S. Government is authorized to reproduce and distribute reprints for\nGovernmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views\nand conclusions contained herein are those of the authors and should not be interpreted as necessarily\nrepresenting the official policies or endorsements, either expressed or implied, of IARPA, DOI/IBC,\nor the U.S. Government.\n10\nReferences\n[1] E. Adelson, C. Anderson, J. Bergen, P. Burt, and J. Ogden. Pyramid methods in image processing. RCA\nEng., 29, 11 1983.\n[2] J. T. Barron, B. Mildenhall, M. Tancik, P. Hedman, R. Martin-Brualla, and P. P. Srinivasan. Mip-NeRF: A\nmultiscale representation for anti-aliasing neural radiance fields. In ICCV, 2021.\n[3] J. T. Barron, B. Mildenhall, D. Verbin, P. P. Srinivasan, and P. Hedman. Mip-NeRF 360: Unbounded\nanti-aliased neural radiance fields. In CVPR, 2022.\n[4] J. T. Barron, B. Mildenhall, D. Verbin, P. P. Srinivasan, and P. Hedman. Zip-NeRF: Anti-aliased grid-based\nneural radiance fields. In ICCV, 2023.\n[5] P. Burt and E. Adelson.\nThe laplacian pyramid as a compact image code.\nIEEE Transactions on\nCommunications, 31(4):532\u2013540, 1983. doi: 10.1109/TCOM.1983.1095851.\n[6] A. Chen, Z. Xu, A. Geiger, J. Yu, and H. Su. TensoRF: Tensorial radiance fields. In ECCV, 2022.\n[7] B. Cheng, M. D. Collins, Y. Zhu, T. Liu, T. S. Huang, H. Adam, and L.-C. Chen. Panoptic-deeplab: A\nsimple, strong, and fast baseline for bottom-up panoptic segmentation. In CVPR, 2020.\n[8] C. Crassin, F. Neyret, M. Sainz, S. Green, and E. Eisemann. Interactive indirect illumination using voxel\ncone tracing: A preview. In Symposium on Interactive 3D Graphics and Games, 2011.\n[9] S. Fridovich-Keil, G. Meanti, F. R. Warburg, B. Recht, and A. Kanazawa. K-planes: Explicit radiance\nfields in space, time, and appearance. In CVPR, 2023.\n[10] S. J. Garbin, M. Kowalski, M. Johnson, J. Shotton, and J. Valentin. FastNeRF: High-fidelity neural\nrendering at 200fps. In ICCV, 2021.\n[11] P. Hedman, P. P. Srinivasan, B. Mildenhall, J. T. Barron, and P. Debevec. Baking neural radiance fields for\nreal-time view synthesis. In ICCV, 2021.\n[12] D. Hu, Z. Zhang, T. Hou, T. Liu, H. Fu, and M. Gong. Multiscale representation for real-time anti-aliasing\nneural rendering. arxiv:2304.10075, 2023.\n[13] W. Hu, Y. Wang, L. Ma, B. Yang, L. Gao, X. Liu, and Y. Ma. Tri-miprf: Tri-mip representation for efficient\nanti-aliasing neural radiance fields. In ICCV, 2023.\n[14] B. K. S. Isaac-Medina, C. G. Willcocks, and T. P. Breckon. Exact-NeRF: An exploration of a precise\nvolumetric parameterization for neural radiance fields. CVPR, 2023.\n[15] J. Kerr, C. M. Kim, K. Goldberg, A. Kanazawa, and M. Tancik. LERF: Language embedded radiance\nfields. In ICCV, 2023.\n[16] S. Kobayashi, E. Matsumoto, and V. Sitzmann. Decomposing nerf for editing via feature field distillation.\nIn Advances in Neural Information Processing Systems, volume 35, 2022.\n[17] L. Liu, J. Gu, K. Z. Lin, T.-S. Chua, and C. Theobalt. Neural sparse voxel fields. NIPS, 2020.\n[18] R. Martin-Brualla, N. Radwan, M. S. M. Sajjadi, J. T. Barron, A. Dosovitskiy, and D. Duckworth. NeRF in\nthe wild: Neural radiance fields for unconstrained photo collections. In CVPR, 2021.\n[19] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. NeRF: Representing\nscenes as neural radiance fields for view synthesis. In ECCV, 2020.\n[20] T. M\u00fcller, A. Evans, C. Schied, and A. Keller. Instant neural graphics primitives with a multiresolution\nhash encoding. ACM Trans. Graph., 41(4):102:1\u201315, July 2022. doi: 10.1145/3528223.3530127.\n[21] J. Philip and V. Deschaintre. Floaters No More: Radiance Field Gradient Scaling for Improved Near-\nCamera Training. In T. Ritschel and A. Weidlich, editors, Eurographics Symposium on Rendering. The\nEurographics Association, 2023. ISBN 978-3-03868-229-5. doi: 10.2312/sr.20231122.\n[22] D. Rebain, W. Jiang, S. Yazdani, K. Li, K. Yi, and A. Tagliasacchi. DeRF: Decomposed radiance fields. In\nCVPR, 2021.\n[23] C. Reiser, S. Peng, Y. Liao, and A. Geiger. KiloNeRF: Speeding up neural radiance fields with thousands\nof tiny MLPs. In ICCV, 2021.\n11\n[24] D. R\u00fcckert, L. Franke, and M. Stamminger. Adop: Approximate differentiable one-pixel point rendering.\nACM Trans. Graph., 41(4):99:1\u201314, July 2022. ISSN 0730-0301.\n[25] Sara Fridovich-Keil and Alex Yu, M. Tancik, Q. Chen, B. Recht, and A. Kanazawa. Plenoxels: Radiance\nfields without neural networks. In CVPR, 2022.\n[26] C. Sun, M. Sun, and H. Chen. Direct voxel grid optimization: Super-fast convergence for radiance fields\nreconstruction. In CVPR, 2022.\n[27] M. Tancik, V. Casser, X. Yan, S. Pradhan, B. Mildenhall, P. P. Srinivasan, J. T. Barron, and H. Kretzschmar.\nBlock-NeRF: Scalable large scene neural view synthesis. In CVPR, pages 8248\u20138258, June 2022.\n[28] M. Tancik, E. Weber, E. Ng, R. Li, B. Yi, J. Kerr, T. Wang, A. Kristoffersen, J. Austin, K. Salahi, A. Ahuja,\nD. McAllister, and A. Kanazawa. Nerfstudio: A modular framework for neural radiance field development.\nIn ACM SIGGRAPH 2023 Conference Proceedings, SIGGRAPH \u201923, 2023.\n[29] V. Tschernezki, I. Laina, D. Larlus, and A. Vedaldi. Neural Feature Fusion Fields: 3D distillation of\nself-supervised 2D image representation. In International Conference on 3D Vision (3DV), 2022.\n[30] H. Turki, D. Ramanan, and M. Satyanarayanan. Mega-NERF: Scalable construction of large-scale NeRFs\nfor virtual fly-throughs. In CVPR, pages 12922\u201312931, June 2022.\n[31] H. Turki, J. Y. Zhang, F. Ferroni, and D. Ramanan. SUDS: Scalable urban dynamic scenes. In CVPR,\n2023.\n[32] J. Wang, B. Amos, A. Das, P. Pillai, N. Sadeh, and M. Satyanarayanan. A scalable and privacy-aware IoT\nservice for live video analytics. In Multimedia Systems Conference, pages 38\u201349, 2017.\n[33] Z. Wang, A. Bovik, H. Sheikh, and E. Simoncelli. Image quality assessment: from error visibility to\nstructural similarity. IEEE Transactions on Image Processing, 13(4):600\u2013612, 2004.\n[34] L. Williams. Pyramidal parametrics. Computer Graphics, 17:1\u201311, July 1983.\n[35] B. Wilson, W. Qi, T. Agarwal, J. Lambert, J. Singh, S. Khandelwal, B. Pan, R. Kumar, A. Hartnett, J. K.\nPontes, D. Ramanan, P. Carr, and J. Hays. Argoverse 2: Next generation datasets for self-driving perception\nand forecasting. In NeurIPS Datasets and Benchmarks, 2021.\n[36] Y. Xiangli, L. Xu, X. Pan, N. Zhao, A. Rao, C. Theobalt, B. Dai, and D. Lin. BungeeNeRF: Progressive\nneural radiance field for extreme multi-scale scene rendering. In ECCV, 2022.\n[37] H. Xin, Z. Qi, F. Ying, L. Xiaoyu, W. Xuan, and W. Qing. Local implicit ray function for generalizable\nradiance field representation. In CVPR, 2023.\n[38] L. Xu, Y. Xiangli, S. Peng, X. Pan, N. Zhao, C. Theobalt, B. Dai, and D. Lin. Grid-guided neural radiance\nfields for large urban scenes. In CVPR, 2023.\n[39] A. Yu, R. Li, M. Tancik, H. Li, R. Ng, and A. Kanazawa. PlenOctrees for real-time rendering of neural\nradiance fields. In ICCV, 2021.\n[40] K. Zhang, G. Riegler, N. Snavely, and V. Koltun. NeRF++: Analyzing and improving neural radiance\nfields. arXiv:2010.07492, 2020.\n[41] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In CVPR, 2018.\n[42] S. Zhi, T. Laidlow, S. Leutenegger, and A. Davison. In-place scene labelling and understanding with\nimplicit scene representation. In ICCV, 2021.\n12\nSupplemental Materials\nA\nSingle-scale datasets\nAlthough PyNeRF is designed for scenarios that capture scene content at different distances, we also\nevaluate it on the original Synthetic-NeRF [19] dataset where the camera distance remains constant.\nIn this scenario, PyNeRF performs similarly to existing SOTA, as shown in Table 6.\nTable 6: Single-scale results. We evaluate PyNeRF on single-scale Blender [19]. PyNeRF performs\ncomparably to existing state-of-the-art.\nPSNR\nLego\nMic\nMaterials\nChair\nHotdog\nFicus\nDrums\nShip\nMean\nK-Planes [9]\n35.38\n33.27\n29.57\n33.88\n36.19\n30.81\n25.62\n30.16\n31.86\nTensoRF [6]\n35.14\n25.70\n33.69\n37.03\n36.04\n29.77\n24.64\n30.12\n31.52\niNGP [20]\n35.67\n36.85\n29.60\n35.71\n37.37\n33.95\n25.44\n30.29\n33.11\nNerfacto [28]\n34.84\n33.58\n26.50\n34.48\n37.07\n30.66\n23.63\n30.95\n31.46\nPyNeRF\n36.63\n36.39\n29.92\n35.76\n37.64\n34.29\n25.80\n30.64\n33.38\nB\nAdditional results\nWe list results for each individual downsampling level in Table 7 and Table 8 to supplement those\nshown in Table 1 and Table 2.\nTable 7: Synthetic results. We average results across Multiscale Blender [2] and Blender-A and list\nmetrics for each downsampling level. All PyNeRF variants outperform their baselines by a wide\nmargin.\n\u2191PSNR\n\u2191SSIM\n\u2193LPIPS\nFull Res.\n1/2 Res.\n1/4 Res.\n1/8 Res.\nFull Res.\n1/2 Res.\n1/4 Res.\n1/8 Res.\nFull Res.\n1/2 Res.\n1/4 Res.\n1/8 Res.\n\u2193Avg Error\nPlenoxels [25]\n22.61\n23.68\n24.54\n23.62\n0.767\n0.768\n0.784\n0.789\n0.307\n0.265\n0.200\n0.161\n0.102\nK-Planes [9]\n25.14\n27.03\n30.26\n30.75\n0.807\n0.840\n0.896\n0.925\n0.225\n0.163\n0.085\n0.053\n0.046\nTensoRF [6]\n25.93\n28.12\n31.46\n30.97\n0.865\n0.893\n0.921\n0.930\n0.169\n0.112\n0.064\n0.056\n0.042\niNGP [20]\n26.90\n29.14\n30.89\n28.49\n0.865\n0.905\n0.947\n0.947\n0.152\n0.095\n0.047\n0.054\n0.032\nNerfacto [28]\n25.35\n27.26\n29.78\n29.09\n0.809\n0.840\n0.893\n0.917\n0.214\n0.158\n0.094\n0.068\n0.049\nMip-NeRF [2]\n32.07\n33.65\n34.76\n35.00\n0.952\n0.959\n0.961\n0.960\n0.048\n0.036\n0.028\n0.021\n0.020\nPyNeRF\n33.18\n35.83\n37.59\n38.29\n0.964\n0.977\n0.984\n0.989\n0.030\n0.013\n0.007\n0.004\n0.008\nPyNeRF-K-Planes\n33.12\n35.18\n36.45\n36.94\n0.963\n0.973\n0.980\n0.985\n0.028\n0.014\n0.009\n0.005\n0.008\nPyNeRF-TensoRF\n32.94\n35.34\n36.92\n37.46\n0.959\n0.974\n0.982\n0.987\n0.033\n0.014\n0.008\n0.005\n0.008\nTable 8: Real-world results. We average results across Boat [24] and Mip-NeRF 360 [3]. As in\nTable 7, all PyNeRF variants improve significantly upon their baselines.\n\u2191PSNR\n\u2191SSIM\n\u2193LPIPS\nFull Res.\n1/2 Res.\n1/4 Res.\n1/8 Res.\nFull Res.\n1/2 Res.\n1/4 Res.\n1/8 Res.\nFull Res.\n1/2 Res.\n1/4 Res.\n1/8 Res.\n\u2193Avg Error\nPlenoxels [25]\n20.69\n20.70\n20.98\n21.93\n0.627\n0.543\n0.547\n0.640\n0.661\n0.607\n0.525\n0.364\n0.128\nK-Planes [9]\n20.53\n20.55\n20.84\n21.85\n0.618\n0.525\n0.512\n0.602\n0.655\n0.587\n0.488\n0.328\n0.128\nTensoRF [6]\n17.31\n17.33\n17.49\n17.96\n0.548\n0.431\n0.367\n0.384\n0.748\n0.714\n0.662\n0.552\n0.190\niNGP [20]\n19.53\n19.83\n16.06\n20.86\n0.598\n0.504\n0.489\n0.574\n0.670\n0.610\n0.517\n0.351\n0.146\nNerfacto [28]\n21.37\n21.42\n21.81\n23.15\n0.629\n0.558\n0.575\n0.688\n0.594\n0.512\n0.389\n0.226\n0.110\nMip-NeRF 360 w/ GLO [3]\n21.73\n21.72\n22.13\n23.65\n0.650\n0.597\n0.628\n0.736\n0.518\n0.427\n0.309\n0.165\n0.100\nMip-NeRF 360 w/o GLO [3]\n21.01\n21.00\n21.39\n22.88\n0.634\n0.580\n0.610\n0.718\n0.529\n0.441\n0.323\n0.179\n0.111\nExact-NeRF w/ GLO [14]\n20.72\n20.73\n21.04\n22.34\n0.637\n0.571\n0.583\n0.674\n0.559\n0.478\n0.378\n0.237\n0.121\nExact-NeRF w/o GLO [14]\n20.98\n20.97\n21.34\n22.80\n0.635\n0.578\n0.604\n0.710\n0.548\n0.451\n0.339\n0.192\n0.113\nPyNeRF\n22.05\n22.16\n22.56\n23.84\n0.645\n0.591\n0.620\n0.725\n0.535\n0.441\n0.316\n0.184\n0.098\nPyNeRF-K-Planes\n21.47\n21.49\n21.87\n23.18\n0.633\n0.570\n0.591\n0.694\n0.563\n0.478\n0.362\n0.217\n0.108\nPyNeRF-TensoRF\n20.82\n20.89\n21.25\n22.48\n0.594\n0.521\n0.528\n0.630\n0.648\n0.558\n0.438\n0.284\n0.122\n13\n"
  },
  {
    "title": "Text-Guided 3D Face Synthesis -- From Generation to Editing",
    "link": "https://arxiv.org/pdf/2312.00375.pdf",
    "upvote": "8",
    "text": "Text-Guided 3D Face Synthesis - From Generation to Editing\nYunjie Wu\u20201\nYapeng Meng\u20201,2\nZhipeng Hu\u20201\nLincheng Li\u22171\nHaoqian Wu1\nKun Zhou3\nWeiwei Xu3\nXin Yu4\n1Netease Fuxi AI Lab\n2Tsinghua University\n3Zhejiang University\n4University of Queensland\n\u201cGive him mustache\u201d\nAnimation\nAnimation\nRelighting\nRelighting\n\u201cMark Zuckerberg\u201d\n\u201cLet him wear a \npurple Zorro mask\u201d\n\u201cMake his lips black\u201d\n\u201cElon Musk\u201d\n\u201cTurn him Asian\u201d\n\u201cMake him bald\u201d\n\u201cMake his eyes big\u201d\n(a) High-fidelity generation and flexible editing\n(b) Semlessly integrated to existing CG pipeline\nFigure 1. (a) Our approach enables the high-fidelity generation and flexible editing of 3D faces from textual input. It facilitates sequential\nediting for creating customized details in 3D faces. (b) The produced 3D faces can be seamlessly integrated into existing CG pipelines.\nAbstract\nText-guided 3D face synthesis has achieved remarkable\nresults by leveraging text-to-image (T2I) diffusion models.\nHowever, most existing works focus solely on the direct gen-\neration, ignoring the editing, restricting them from synthe-\nsizing customized 3D faces through iterative adjustments.\nIn this paper, we propose a unified text-guided framework\nfrom face generation to editing. In the generation stage, we\npropose a geometry-texture decoupled generation to miti-\ngate the loss of geometric details caused by coupling. Be-\nsides, decoupling enables us to utilize the generated geom-\netry as a condition for texture generation, yielding highly\ngeometry-texture aligned results. We further employ a fine-\ntuned texture diffusion model to enhance texture quality in\nboth RGB and YUV space. In the editing stage, we first em-\nploy a pre-trained diffusion model to update facial geometry\nor texture based on the texts. To enable sequential editing,\nwe introduce a UV domain consistency preservation reg-\nularization, preventing unintentional changes to irrelevant\nfacial attributes. Besides, we propose a self-guided consis-\ntency weight strategy to improve editing efficacy while pre-\nserving consistency. Through comprehensive experiments,\nwe showcase our method\u2019s superiority in face synthesis.\nProject page: https://faceg2e.github.io/.\n\u2020Equal contribution\n*Corresponding author\n1. Introduction\nModeling 3D faces serves as a fundamental pillar for var-\nious emerging applications such as film making, video\ngames, and AR/VR. Traditionally, the creation of detailed\nand intricate 3D human faces requires extensive time from\nhighly skilled artists. With the development of deep learn-\ning, existing works [8, 10, 47, 56] attempted to produce 3D\nfaces from photos or videos with generative models. How-\never, the diversity of the generation remains constrained pri-\nmarily due to the limited scale of training data. Fortunately,\nrecent large-scale vision-language models (e.g., CLIP [33],\nStable Diffusion [35]) pave the way for generating diverse\n3D content. Through the integration of these models, nu-\nmerous text-to-3D works [23, 28, 29, 50, 52] can create 3D\ncontent in a zero-shot manner.\nMany studies have been conducted on text-to-3D face\nsynthesis. They either utilize CLIP or employ score dis-\ntillation sampling (SDS) on text-to-image (T2I) models to\nguide the 3D face synthesis. Some methods [46, 53] employ\nneural fields to generate visually appealing but low-quality\ngeometric 3D faces. Recently, Dreamface [54] has demon-\nstrated the potential for generating high-quality 3D face tex-\ntures by leveraging SDS on facial textures, but their geom-\netry is not fidelitous enough and they overlooked the sub-\nsequent face editing. A few works [2, 12, 27] enable text-\nguided face editing, allowing coarse-grained editing (e.g.\n1\narXiv:2312.00375v1  [cs.CV]  1 Dec 2023\noverall style), but not fine-grained adjustments (e.g., lips\ncolor). Besides, the lack of design in precise editing control\nleads to unintended changes in their editing, preventing the\nsynthesis of customized faces through sequential editing.\nTo address the aforementioned challenges, we present\ntext-guided 3D face synthesis - from generation to editing,\ndubbed FaceG2E. We propose a progressive framework to\ngenerate the facial geometry and textures, and then perform\naccurate face editing sequentially controlled by text. To the\nbest of our knowledge, this is the first attempt to edit a 3D\nface in a sequential manner. We propose two core compo-\nnents: (1) Geometry-texture decoupled generation and (2)\nSelf-guided consistency preserved editing.\nTo be specific, our proposed Geometry-texture decou-\npled generation generates the facial geometry and texture\nin two separate phases. By incorporating texture-less ren-\ndering in conjunction with SDS, we induce the T2I model\nto provide geometric-related priors, inciting details (e.g.,\nwrinkles, lip shape) in the generated geometry. Building\nupon the generated geometry, we leverage ControlNet to\nforce the SDS to be aware of the geometry, ensuring pre-\ncise geometry-texture alignment. Additionally, we fine-tune\na texture diffusion model that incorporates both RGB and\nYUV color spaces to compute SDS in the texture domain,\nenhancing the quality of the generated textures.\nThe newly developed Self-guided consistency preserved\nediting enables one to follow the texts, performing efficient\nediting in specific facial attributes without causing other\nunintended changes. Here, we first employ a pre-trained\nimage-edit diffusion model to update the facial geometry\nor texture. Then we introduce a UV domain consistency\npreservation regularization to prevent unexpected changes\nin faces, enabling sequential editing. To avoid the degra-\ndation of editing effects caused by the regularization, we\nfurther propose a self-guided consistency weighting strat-\negy. It adaptively determines the regularization weight for\neach facial region by projecting the cross-attention scores of\nthe T2I model to the UV domain. As shown in Fig. 1, our\nmethod can generate high-fidelity 3D facial geometry and\ntextures while allowing fine-grained face editing. With the\nproposed components, we achieve better visual and quanti-\ntative results compared to other SOTA methods, as demon-\nstrated in Sec. 4. In summary, our contributions are:\n\u2022 We propose FaceG2E, facilitating a full pipeline of text-\nguided 3D face synthesis, from generation to editing.\nUser surveys confirm that our synthesized 3D faces are\nsignificantly preferable than other SOTA methods.\n\u2022 We propose the geometry-texture decoupled generation,\nproducing faces with high-fidelity geometry and texture.\n\u2022 We design the self-guided consistency preservation, en-\nabling the accurate editing of 3D faces. Leveraging pre-\ncise editing control, our method showcases some novel\nediting applications, such as sequential and geometry-\ntexture separate editing.\n2. Related Work\nText-to-Image generation.\nRecent advancements in\nvisual-language models [33] and diffusion models [9, 14,\n43] have greatly improved text-to-image generation [4, 34,\n35, 38]. These methods, trained on large-scale image-text\ndatasets [41, 42], can synthesize realistic and complex im-\nages from text descriptions. Subsequent studies have made\nfurther efforts to introduce additional generation process\ncontrols [17, 49, 55], fine-tuning the pre-trained models for\nspecific scenarios [11, 16, 36], and enabling image editing\ncapabilities [6, 13, 24]. However, generating high-quality\nand faithful 3D assets, such as 3D human faces, from tex-\ntual input still poses an open and challenging problem.\nText-to-3D generation. With the success of text-to-image\ngeneration in recent years, text-to-3D generation has at-\ntracted significant attention from the community. Early ap-\nproaches [15, 21, 31, 39, 51] utilize mesh or implicit neural\nfields to represent 3D content, and optimized the CLIP met-\nrics between the 2D rendering and text prompts. However,\nthe quality of generated 3D contents is relatively low.\nRecently, DreamFusion [32] has achieved impressive re-\nsults by using a score distillation sampling (SDS) within the\npowerful text-to-image diffusion model [38]. Subsequent\nworks further enhance DreamFusion by reducing genera-\ntion time [28], improving surface material representation\n[7], and introducing refined sampling strategies [19]. How-\never, the text-guided generation of high-fidelity and intri-\ncate 3D faces remains challenging. Building upon Dream-\nFusion, we carefully design the form of score distillation by\nexploiting various diffusion models at each stage, resulting\nin high-fidelity and editable 3D faces.\nText-to-3D face synthesis. Recently, there have been at-\ntempts to generate 3D faces from text. Describe3D [48]\nand Rodin [46] propose to learn the mapping from text to\n3D faces on pairs of text-face data. They solely employ\nthe mapping network trained on appearance descriptions to\ngenerate faces, and thus fail to generalize to out-of-domain\ntexts (e.g., celebrities or characters). On the contrary, our\nmethod can generalize well to these texts and synthesize\nvarious 3D faces.\nOther works [12, 18, 22, 27, 54] employ SDS on the pre-\ntrained T2I models. Dreamface [54] utilizes CLIP to select\nfacial geometry from candidates. Then they perform the\nSDS with a texture diffusion network to generate facial tex-\ntures. Headsculpt [12] employs Stable Diffusion [35] and\nInstructPix2Pix [6] for computing the SDS, and relies on\nthe mixture of SDS gradients for constraining the editing\nprocess. These approaches can perform not only generation\nbut also simple editing. However, they still lack the design\nin precise editing control, and unintended changes in the\nediting results often occur. This prevents them from synthe-\n2\n\"a DSLR photo of Scarlett Johansson\"\n3DMM-based mesh\ntexture \ud835\udc85\n(1) Geometry Phase \ngeometry \ud835\udc88\n\"let her wear a batman eyemask\"\ngenerated face\nedited face\n,\n\ud835\udc85\ud835\udc90\n\ud835\udc85\ud835\udc86\nSD/ControlNet/\nInsP2P/\nTexDiffusion\nDescription/Instruction/Keyword\ncondition\n(optional)\nrendered\nnoise\n\ud835\udc3f\u0bcc\u0bbd\u0bcc\nupdate\ndepth\n\ud835\udc85\ud835\udc90\n\ud835\udc85\ud835\udc86\n\ud835\udc91\ud835\udc86\n\ud835\udc91\ud835\udc90\n\"facial texture\"\ndepth\ncontrolnet\nSDS\nTex\ndiffusion\nSDS\nstable\ndiffusion\nSDS\nnormal\ngrey\n\ud835\udc70\ncondition\n(2) Texture Phase \n(a) Geometry-Texture Decoupled Generation\nshared\nview\n\ud835\udc70\n\ud835\udc70\u2032\nshared\nview\nInstruct\npix2pix\nSDS\ncondition\n\u2299\nSelf-guided\nConsistency\nRegularization\n=\nUV unwrap\n(b) Self-guided Consistency Preserved Editing\n(c) Score Distillation Sampling (SDS)\nnorm(att(\ud835\udc70\u11f1, \"mask\" ))\n\ud835\udc6a\u0bdc\nUV unwrap\na (1)\na (2)\nb\nFigure 2. Overview of FaceG2E. (a) Geometry-texture decoupled generation, including a geometry phase and a texture phase. (b) Self-\nguided consistency preserved editing, in which we utilize the built-in cross-attention to obtain the editing-relevant regions and unwrap them\nto UV space. Then we penalize inconsistencies in the irrelevant regions. (c) Our method exploits multiple score distillation sampling.\nsizing highly customized 3D faces via sequential editing.\nOn the contrary, our approach facilitates accurate editing of\n3D faces, supporting sequential editing.\n3. Methodology\nFaceG2E is a progressive text-to-3D approach that first gen-\nerates a high-fidelity 3D human face and then performs fine-\ngrained face editing. As illustrated in Fig. 2, our method\nhas two main stages: (a) Geometry-texture decoupled gen-\neration, and (b) Self-guided consistency preserved editing.\nIn Sec. 3.1, we introduce some preliminaries that form the\nfundamental basis of our approach. In Sec. 3.2 and Sec.\n3.3, we present the generation and editing stages.\n3.1. Preliminaries\nScore distillation sampling has been proposed in Dream-\nFusion [32] for text-to-3D generation.\nIt utilizes a pre-\ntrained 2D diffusion model \u03d5 with a denoising function\n\u03f5\u03d5 (zt; y, t) to optimize 3D parameters \u03b8. SDS renders an\nimage I = R(\u03b8) and embeds I with an encoder E(\u00b7),\nachieving image latent z. Then it injects a noise \u03f5 into z,\nresulting in a noisy latent code zt. It takes the difference\nbetween the predicted and added noise as the gradient:\n\u2207\u03b8LSDS(I) = Et,\u03f5\n\u0014\nw(t) (\u03f5\u03d5 (zt; y, t) \u2212 \u03f5) \u2202z\n\u2202I\n\u2202I\n\u2202\u03b8\n\u0015\n, (1)\nwhere w(t) is a time-dependent weight function and y is the\nembedding of input text.\nFacial Geometry and Texture is represented with param-\neters \u03b8 = (\u03b2, u) in FaceG2E. \u03b2 denotes the identity coeffi-\ncient from the parametric 3D face model HIFI3D [5], and u\ndenotes a image latent code for facial texture. The geometry\ng can be achieved by the blendshape function M(\u00b7):\ng = M(\u03b2) = T +\nX\ni\n\u03b2iSi,\n(2)\nwhere T is the mean face and S is the vertices offset basis.\nAs to the texture, the facial texture map d is synthesized\nwith a decoder: d = D(u). We take the decoder from VAE\nof Stable Diffusion [35] as D(\u00b7).\n3.2. Geometry-Texture Decoupled Generation\nThe first stage of FaceG2E is the geometry-texture decou-\npled generation, which generates facial geometry and tex-\nture from the textual input. Many existing works have at-\ntempted to generate geometry and texture simultaneously\nin a single optimization process, while we instead decouple\nthe generation into two distinct phases: the geometry phase\nand the texture phase. The decoupling provides two advan-\ntages: 1) It helps enhance geometric details in the gener-\nated faces. 2) It improves geometry-texture alignment by\nexploiting the generated geometry to guide the texture gen-\neration.\nGeometry Phase. An ideal generated geometry should pos-\nsess both high quality (e.g., no surface distortions) and a\n3\ngood alignment with the input text. The employed facial\n3D morphable model provides strong priors to ensure the\nquality of generated geometry. As to the alignment with\nthe input text, we utilize SDS on the network \u03d5sd of Stable\nDiffusion [35] to guide the geometry generation.\nPrevious works [22, 27, 53] optimize geometry and tex-\nture simultaneously.\nWe observe this could lead to the\nloss of geometric details, as certain geometric informa-\ntion may be encapsulated within the texture representation.\nTherefore, we aim to enhance the SDS to provide more\ngeometry-centric information in the geometry phase. To\nthis end, we render the geometry g with texture-less ren-\ndering \u02dcI = \u02dcR(g), e.g., surface normal shading or diffuse\nshading with constant grey color. The texture-less shading\nattributes all image details solely to geometry, thereby al-\nlowing the SDS to focus on geometry-centric information.\nThe geometry-centric SDS loss is defined as:\n\u2207\u03b2Lgeo =Et,\u03f5\n\"\nw(t) (\u03f5\u03d5sd (zt; y, t) \u2212 \u03f5) \u2202zt\n\u2202 \u02dcI\n\u2202 \u02dcI\n\u2202g\n\u2202g\n\u2202\u03b2\n#\n. (3)\nTexture Phase. Many works [27, 54] demonstrate that tex-\nture can be generated by minimizing the SDS loss. How-\never, directly optimizing the standard SDS loss could lead to\ngeometry-texture misalignment issues, as shown in Fig .9.\nTo address this problem, we propose the geometry-aware\ntexture content SDS (GaSDS). We resort to the ControlNet\n[55] to endow the SDS with awareness of generated geom-\netry, thereby inducing it to uphold geometry-texture align-\nment. Specifically, we render g into a depth map e. Then\nwe equip the SDS with the depth-ControlNet \u03d5dc, and take\ne as a condition, formulating the GaSDS:\n\u2207uLga\ntex = Et,\u03f5\n\u0014\nw(t) (\u03f5\u03d5dc (zt; e, y, t) \u2212 \u03f5) \u2202zt\n\u2202I\n\u2202I\n\u2202d\n\u2202d\n\u2202u\n\u0015\n.\n(4)\nWith the proposed GaSDS, the issue of geometric mis-\nalignment is addressed. However, artifacts such as local\ncolor distortion or uneven brightness persist in the textures.\nThis is because the T2I model lacks priors of textures,\nwhich hinders the synthesis of high-quality texture details.\nHence we propose texture prior SDS to introduce such\npriors of textures. Inspired by DreamFace [54], we train\na diffusion model \u03d5td1 on texture data to estimate the tex-\nture distribution for providing the prior. Our training dataset\ncontains 500 textures, including processed scanning data\nand selected synthesized data [3]. Different from Dream-\nFace, which uses labeled text in training, we employ a fixed\ntext keyword (e.g., \u2018facial texture\u2019) for all textures. Because\nthe objective of \u03d5td1 is to model the distribution of textures\nas a prior, the texture-text alignment is not necessary. We\nadditionally train another \u03d5td2 on the YUV color spaces to\npromote uniform brightness, as shown in Fig 3. We fine-\ntune both \u03d5td1 and \u03d5td2 on Stable Diffusion. The texture\ntexutre\ndiffusion1\na face UV texture\nFace texture datasets\ntexutre\ndiffusion2\nYUV color space\nnoise\nnoise\nFigure 3. Training the texture diffusion model is performed on the\ncollected facial textures in both RGB and YUV color space.\nprior SDS is formulated with the trained \u03d5td1 and \u03d5td2 as:\n\u2207uLpr\ntex = Lrgb\ntex + \u03bbyuvLyuv\ntex ,\nLrgb\ntex = Et,\u03f5\n\u0014\nw(t)\n\u0000\u03f5\u03d5td1\n\u0000zd\nt ; y\u2217, t\n\u0001\n\u2212 \u03f5\n\u0001 \u2202zd\nt\n\u2202d\n\u2202d\n\u2202u\n\u0015\n,\nLyuv\ntex = Et,\u03f5\n\"\nw(t)\n\u0010\n\u03f5\u03d5td2\n\u0010\nzd\u2032\nt ; y\u2217, t\n\u0011\n\u2212 \u03f5\n\u0011 \u2202zd\u2032\nt\n\u2202d\n\u2202d\n\u2202u\n#\n,\n(5)\nwhere zd\nt and zd\u2032\nt denote the noisy latent codes of the texture\nd and the converted YUV texture d\u2032. The y\u2217 is the text\nembedding of the fixed text keyword. We combine the Lga\ntex\nand Lpr\ntex as our final texture generation loss:\nLtex = Lga\ntex + \u03bbprLpr\ntex,\n(6)\nwhere \u03bbpr is a weight to balance the gradient from Lpr\ntex.\n3.3. Self-guided Consistency Preserved Editing\nTo attain the capability of following editing instructions in-\nstead of generation prompts, a simple idea is to take the\ntext-guided image editing model InstructPix2Pix [6] \u03d5ip2p\nas a substitute for Stable Diffusion to form the SDS:\n\u2207\u03b2,uLedit = Et,\u03f5\n\u0014\nw(t)\n\u0000\u03f5\u03d5ip2p (z\u2032\nt; zt, y\u2217, t) \u2212 \u03f5\n\u0001\n\u2202z\u2032\nt\n\u2202\u03b2, \u2202u\n\u0015\n,\n(7)\nwhere z\u2032\nt denotes the latent for the rendering of the edited\nface, and the original face is embedded to zt as an extra\nconditional input, following the setting of InstructPix2Pix.\nNote that our geometry and texture are represented by\nseparate parameters \u03b2 and u, so it is possible to indepen-\ndently optimize one of them, enabling separate editing of\ngeometry and texture. Besides, when editing the texture,\nwe integrate the Lpr\ntex to maintain the structural rationality\nof textures.\nSelf-guided Consistency Weight. The editing SDS in Eq.\n7 enables effective facial editing, while fine-grained edit-\ning control still remains challenging, e.g., unpredictable and\nundesired variations may occur in the results, shown as Fig.\n4\nIteration\ncross-attention score\nconsistency weight \ud835\udc6a\ud835\udc8a\nedited face\nFigure 4. Visualization of the edited face, the cross-attention score\nfor token \u201cmask\u201d and the consistency weight Ci during iterations\nin editing. Note the viewpoints vary due to random sampling in\niterations.\n10. This hinders sequential editing, as earlier edits can be\nunintentionally disrupted by subsequent ones. Therefore,\nconsistency between the faces before and after the editing\nshould be encouraged.\nHowever, the consistency between faces during editing\nand the noticeability of editing effects, are somewhat con-\ntradictory. Imagine a specific pixel in texture, encouraging\nconsistency inclines the pixel towards being the same as the\noriginal pixel, while the editing may require it to take on a\ncompletely different value to achieve the desired effect.\nA key observation in addressing this issue is that the\nweight of consistency should vary in different regions: For\nregions associated with editing instructions, a lower level of\nconsistency should be maintained as we prioritize the edit-\ning effects. Conversely, for irrelevant regions, a higher level\nof consistency should be ensured. For instance, given the\ninstruction \u201clet her wear a Batman eyemask\u201d, we desire the\neyemask effect near the eyes region while keeping the rest\nof the face unchanged.\nTo locate the relevant region for editing instructions, we\npropose a self-guided consistency weight strategy in the UV\ndomain. We utilize the built-in cross-attention of the In-\nstructPix2Pix model itself. The attention scores introduce\nthe association between different image regions and spe-\ncific textual tokens. An example of the consistency weight\nis shown in Fig 4. We first select a region-indicating token\nT \u2217 in the instruction, such as \u201cmask\u201d. At each iteration i,\nwe extract the attention scores between the rendered image\nI of the editing and the token T \u2217. The scores are normal-\nized and unwrapped to the UV domain based on the cur-\nrent viewpoint, and then we compute temporal consistency\nweight \u02dcCi from the unwrapped scores:\n\u02dcCi = 1 \u2212 (proj (norm (att (I\u2032, T \u2217))))2 ,\n(8)\nwhere att(\u00b7, \u00b7) denotes the cross-attention operation to pre-\ndict the attention scores, the norm(\u00b7) denotes the normal-\nization operation, and the proj denotes the unwrapping pro-\njection from image to UV domain. As \u02dcCi is related to the\nviewpoint, we establish a unified consistency weight Ci to\nfuse \u02dcCi from different viewpoints. The initial state of Ci is\na matrix of all \u2018one\u2019, indicating the highest level of consis-\ntency applied to all regions. The updating of Ci at each step\nis informed by the \u02dcCi. Specifically, we select the regions\nwhere the values in \u02dcCi are lower than Ci to be updated.\nThen we employ a moving average strategy to get the Ci:\nCi = Ci\u22121 \u2217 w + \u02dcCi \u2217 (1 \u2212 w),\n(9)\nwhere w is a fixed moving average factor. We take the Ci\nas a weight to perform region-specific consistency.\nConsistency Preservation Regularization. With the con-\nsistency weight Ci in hand, we propose a region-specific\nconsistency preservation regularization in the UV domain to\nencourage consistency between faces before and after edit-\ning in both texture and geometry:\nLtex\nreg = \u2225(do \u2212 de) \u2299 Ci\u22252\n2 ,\nLgeo\nreg = \u2225(po \u2212 pe) \u2299 Ci\u22252\n2 ,\n(10)\nwhere do, de denote the texture before and after the editing,\npo, pe denote the vertices position map unwrapped from the\nfacial geometry before and after the editing, and \u2299 denotes\nthe Hadamard product.\nWith the consistency preservation regularization, we\npropose the final loss for our self-guided consistency pre-\nserved editing as:\nLfinalEdit = Ledit + \u03bbregLreg,\n(11)\nwhere \u03bbreg is the balance weight.\n4. Experiments\n4.1. Implementation Details\nOur implementation is built upon Huggingface Diffusers\n[45]. We use stable-diffusion [37] checkpoint for geom-\netry generation, and sd-controlnet-depth [30] for texture\ngeneration. We utilize the official instruct-pix2pix [44] in\nface editing. The RGB and YUV texture diffusion models\nare both fine-tuned on the stable-diffusion checkpoint. We\nutilize NVdiffrast [26] for differentiable rendering. Adam\n[25] optimizer with a fixed learning rate of 0.05 is em-\nployed. The generation and editing for geometry/texture\nrequire 200/400 iterations, respectively. It takes about 4\nminutes to generate or edit a face on a single NVIDIA A30\nGPU. We refer readers to the supplementary material for\nmore implementation details.\n5\nThe Joker\nSpider-man\nScarlett Johansson\nBenedict Cumberbatch\nAnne Hathaway\nTurn her a man\nMake his eyes big\nMake him chubby\nJason Statham\nTurn him a woman\nElderly Asian man, gray \nbeard, deep wrinkles\nCaucasian teenager girl, thin \neyebrows, pink lipstick\nMake his eyemask blue\nTom Cruise\nNeteyam in Avatar\nThanos in the Marvel\nTurn him a \ncropper-made robot\nMake his lips red\nCaptain America\nHulk\nMake him young\nTurn him female\nLet her wear Batman \neyemask\nKratos in God of War\nGeometry Editing\nTexture Editing\nFigure 5. FaceG2E enables the generation of highly realistic and diverse 3D faces (on the left), as well as provides flexible editing\ncapabilities for these faces (on the right). Through sequential editing, FaceG2E achieves the synthesis of highly customized 3D faces, such\nas \u2018A female child Hulk wearing a Batman mask\u2019. Additionally, independent editing is available for geometry and texture modification.\n4.2. Synthesis Results\nWe showcase some synthesized 3D faces in Fig. 1 and Fig.\n5. As depicted in the figures, FaceG2E demonstrates ex-\nceptional capabilities in generating a wide range of visu-\nally diverse and remarkably lifelike faces, including notable\ncelebrities and iconic film characters. Furthermore, it en-\nables flexible editing operations, such as independent ma-\nnipulation of geometry and texture, as well as sequential\nediting. Notably, our synthesized faces can be integrated\ninto existing CG pipelines, enabling animation and relight-\ning applications, as exemplified in Fig. 1. More animation\nand relighting results are in the supplementary material.\n4.3. Comparison with the state-of-the-art\nWe compare some state-of-the-art methods for text-guided\n3D face generation and editing, including Describe3D [48],\nDreamFace [54] and TADA [27]. Comparisons with some\nother methods are contained in the supplementary material.\n4.3.1\nQualitative Comparison\nThe qualitative results are presented in Fig. 6. We can ob-\nserve that: (1) Describe3D struggles to generate 3D faces\n\u201cEmma Watson\u201d\n\u201cWill Smith\u201d\n\u201cDeadpool\u201d\n\u201cLet him wear a \nblack glasses\u201d\n\u201cLet her wear a \nGeisha makeup\u201d\n\u201cMake his eyemask\nmade of gold\u201d\nDescribe3D\nDreamface\nTADA\nOurs\nFigure 6. The comparison on text-guided 3D face synthesis. We\npresent both the generation and editing results of each method.\nfollowing provided texts due to its limited training data and\ninability to generalize beyond the training set. (2) TADA\nproduces visually acceptable results but exhibits shortcom-\nings in (i) generating high-quality geometry (e.g., evident\n6\nDescribe3D\nDreamface\nTADA\nOurs\n\u201cScatlett Johansson\u201d\n\u201cMake her \nbig eyes\u201d\n\u201cTurn her \nskin dark\u201d\n\u201cMake her \nlips purple\u201d\n\u2192\n\u2192\n\u2192\nFigure 7. The comparison on sequential face editing.\nMethod\nGeneration\nEditing\nScore \u2191\nRanking-1 \u2191\nScore \u2191\nRanking-1 \u2191\nDescribe3D [48]\n29.81\n0%\n28.83\n0%\nDreamface [54]\n33.22\n10%\n33.14\n10%\nTADA [27]\n34.85\n10%\n33.73\n20%\nOurs\n36.95\n80%\n35.50\n70%\nTable 1. The CLIP evaluation results on the synthesized 3D faces.\ngeometric distortion in its outputs), and (ii) accurately\nfollowing editing instructions (e.g., erroneously changing\nblack glasses to blue in case 2). (3) Dreamface can generate\nrealistic faces but lacks editing capabilities. Moreover, its\ngeometry fidelity is insufficient, hindering the correlation\nbetween the text and texture-less geometry. In comparison,\nour method is superior in both generated geometry and tex-\nture and allows for accurate and flexible face editing.\nWe further provide a comparison of sequential editing\nin Fig. 7. Clearly, the editing outcomes of Describe3D\nand Dreamface in each round lack prominence. Although\nTADA performs well with single-round editing instructions,\nit struggles in sequence editing due to unintended changes\nthat impact the preceding editing effects influenced by sub-\nsequent edits. For instance, in the last round, TADA mistak-\nenly turns the skin purple. In contrast, our FaceG2E benefits\nfrom the proposed self-guided consistency preservation, al-\nlowing for precise sequence editing.\n4.3.2\nQuantitative Comparison\nWe quantitatively compare the fidelity of synthesized faces\nto text descriptions using the CLIP evaluation. We provide\na total of 20 prompts, evenly split between generation and\nFigure 8. Quantitative results of user study. Our results are more\nfavored by the participants compared to the other methods.\nediting tasks, to all methods for face synthesis. All results\nare rendered with the same pipeline, except DreamFace,\nwhich takes its own rendering in the web demo [20]. A\nfixed prefix \u2018a realistic 3D face model of \u2019 is employed for\nall methods when calculating the CLIP score. We report the\nCLIP Score [40] and Ranking-1 in Tab. 1. CLIP Ranking-\n1 calculates the ratio of a method\u2019s created faces ranked as\ntop-1 among all methods. The results validate the superior\nperformance of our method over other SOTA methods.\n4.3.3\nUser Study\nWe perform a comparative user study involving 100 par-\nticipants to evaluate our method against state-of-the-art\n(SOTA) approaches. Participants are presented with 10 face\ngeneration examples and 10 face editing examples, and are\nasked to select the best method for each example based on\nspecific criteria. The results, depicted in Fig. 8, unequivo-\ncally show that our method surpasses all others in terms of\nboth geometry and texture preference.\n4.4. Ablation Study\nHere we present some ablation studies. Extra studies based\non user surveys are provided in the supplementary material.\n4.4.1\nEffectiveness of GDG\nTo evaluate the effectiveness of geometry-texture decoupled\ngeneration (GDG), we conduct the following studies.\nGeometry-centric SDS (GcSDS). In Fig. 9(a), we con-\nduct an ablation study to assess the impact of the proposed\nGcSDS. We propose a variation that takes standard textured\nrendering as input for SDS and simultaneously optimizes\nboth geometry and texture variables. The results reveal that\nwithout employing the GcSDS, there is a tendency to gen-\nerate relatively planar meshes, which lack geometric details\nsuch as facial wrinkles. We attribute this deficiency to the\nmisrepresentation of geometric details by textures.\nGeometry-aligned texture content SDS (GaSDS). In\nColumns 3 and 4 of Fig. 9(b), we evaluate the effective-\nness of GaSDS. We replace the depth-ControlNet in GaSDS\n7\n(a) Geometry Generation\n(b) Texture Generation\nOurs\nw/o GcSDS\nOurs\nw/o \ud835\udcdb\ud835\udc95\ud835\udc86\ud835\udc99\n\ud835\udc88\ud835\udc82 & \ud835\udcdb\ud835\udc95\ud835\udc86\ud835\udc99\n\ud835\udc91\ud835\udc93\nw/o \ud835\udcdb\ud835\udc95\ud835\udc86\ud835\udc99\n\ud835\udc9a\ud835\udc96\ud835\udc97\nw/o \ud835\udcdb\ud835\udc95\ud835\udc86\ud835\udc99\n\ud835\udc91\ud835\udc93\nFigure 9. The ablation study of our geometry-texture decoupled generation. The input texts are \u2018Scarlett Johansson\u2019 and \u2018Will Smith\u2019.\nOriginal face\nw/o Reg\nOurs full\nw/o SC-weight\nScarlett Johansson\nHulk\n\u2192\nMake her lips purple\nLet him wear Batman eyemask\n\u2192\nFigure 10.\nAnalysis of the proposed self-guided consistency\npreservation (SCP) in 3D face editing.\nwith the standard Stable-Diffusion model to compute Lga\ntex.\nThe results demonstrate a significant problem of geometry-\ntexture misalignment. This issue arises because the stan-\ndard Stable Diffusion model only utilizes text as a condi-\ntional input and lacks perception of geometry, therefore fail-\ning to provide geometry-aligned texture guidance.\nTexture prior SDS. To assess the efficacy of our texture\nprior SDS, we compared it with two variants: one that solely\nrelies on geometry-aware texture content SDS, denoted as\nw/o Lpr\ntex, and another that excludes the use of Lyuv\ntex , de-\nnoted as w/o Lyuv\ntex . As shown in Columns 1,2 and 3 of Fig.\n9(b), the results demonstrate that the w/o Lpr\ntex pipeline gen-\nerates textures with significant noise and artifacts. The w/o\nLyuv\ntex pipeline produces textures that generally adhere to\nthe distribution of facial textures, but may exhibit brightness\nirregularities. The complete Lpr\ntex yields the best results.\n4.4.2\nEffectiveness of SCP\nTo evaluate the effectiveness of the proposed self-guided\nconsistency preservation (SCP) in editing, we conduct the\nfollowing ablation study. We make two variants: One vari-\nant, denoted as w/o Reg, solely relies on Ledit for editing\nwithout employing consistency regularization. The other\nvariant, denoted as w/o SC-weight, computes the consis-\ntency preservation regularization without using the self-\nguided consistency weight.\nThe results are shown in Fig. 10. While w/o Reg shows\nnoticeable editings following the instructions, unexpected\nalterations occur, such as the skin and hair of Scarlett turn-\ning purple, and Hulk\u2019s skin turning yellow.\nThis inade-\nquacy can be attributed to the absence of consistency con-\nstraints. On the other hand, w/o SC-weight prevents unde-\nsirable changes in the results but hampers the effectiveness\nof editing, making it difficult to observe significant editing\neffects. In contrast, the full version of SCP achieves evident\nediting effects while preserving consistency in unaffected\nregions, thereby ensuring desirable editing outcomes.\n5. Conclusion\nWe propose FaceG2E, a novel approach for generating di-\nverse and high-quality 3D faces and performing facial edit-\ning using texts. With the proposed geometry-texture decou-\npled generation, high-fidelity facial geometry and texture\ncan be produced. The designed self-guided consistency pre-\nserved editing enabling us to perform flexible editing, e.g.,\nsequential editing. Extensive evaluations demonstrate that\nFaceG2E outperforms SOTA methods in 3D face synthesis.\nDespite achieving new state-of-the-art results, we notice\nsome limitations in FaceG2E. (1) The geometric represen-\ntation restricts us from generating shapes beyond the facial\nskin, such as hair and accessories. (2) Sequential editing\nenables the synthesis of customized faces, but it also leads\nto a significant increase in time consumption. Each round\nof editing requires additional time.\n8\nReferences\n[1] Stable-dreamfusion.\nhttps : / / github . com /\nashawkey/stable-dreamfusion, 2022. 12\n[2] Shivangi Aneja, Justus Thies, Angela Dai, and Matthias\nNie\u00dfner. Clipface: Text-guided editing of textured 3d mor-\nphable models. arXiv preprint arXiv:2212.01406, 2022. 1\n[3] Haoran Bai, Di Kang, Haoxian Zhang, Jinshan Pan, and Lin-\nchao Bao. Ffhq-uv: Normalized facial uv-texture dataset for\n3d face reconstruction.\narXiv preprint arXiv:2211.13874,\n2022. 4\n[4] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,\nJiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,\nSamuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image\ndiffusion models with an ensemble of expert denoisers. arXiv\npreprint arXiv:2211.01324, 2022. 2\n[5] Linchao Bao, Xiangkai Lin, Yajing Chen, Haoxian Zhang,\nSheng Wang, Xuefei Zhe, Di Kang, Haozhi Huang, Xinwei\nJiang, Jue Wang, Dong Yu, and Zhengyou Zhang.\nHigh-\nfidelity 3d digital human head creation from rgb-d selfies.\nACM Transactions on Graphics, 2021. 3\n[6] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nIn CVPR, pages 18392\u201318402, 2023. 2, 4\n[7] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia.\nFantasia3d:\nDisentangling geometry and appearance for\nhigh-quality text-to-3d content creation.\narXiv preprint\narXiv:2303.13873, 2023. 2\n[8] Rahul Dey and Vishnu Naresh Boddeti. Generating diverse\n3d reconstructions from a single occluded face image. In\nCVPR, pages 1547\u20131557, 2022. 1\n[9] Prafulla Dhariwal and Alexander Nichol.\nDiffusion mod-\nels beat gans on image synthesis. NeurIPS, 34:8780\u20138794,\n2021. 2\n[10] Abdallah Dib, Junghyun Ahn, Cedric Thebault, Philippe-\nHenri Gosselin, and Louis Chevallier. S2f2: Self-supervised\nhigh fidelity face reconstruction from monocular image.\nIn 2023 IEEE 17th International Conference on Automatic\nFace and Gesture Recognition (FG), pages 1\u20138. IEEE, 2023.\n1\n[11] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-\nnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-\nOr.\nAn image is worth one word: Personalizing text-to-\nimage generation using textual inversion.\narXiv preprint\narXiv:2208.01618, 2022. 2\n[12] Xiao Han, Yukang Cao, Kai Han, Xiatian Zhu, Jiankang\nDeng, Yi-Zhe Song, Tao Xiang, and Kwan-Yee K. Wong.\nHeadsculpt:\nCrafting 3d head avatars with text.\narXiv\npreprint arXiv:2306.03038, 2023. 1, 2\n[13] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-\nage editing with cross attention control.\narXiv preprint\narXiv:2208.01626, 2022. 2\n[14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. NeurIPS, 33:6840\u20136851, 2020.\n2\n[15] Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang\nCai, Lei Yang, and Ziwei Liu. Avatarclip: zero-shot text-\ndriven generation and animation of 3d avatars. ACM TOG,\n41(4):1\u201319, 2022. 2\n[16] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 2\n[17] Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao,\nand Jingren Zhou. Composer: Creative and controllable im-\nage synthesis with composable conditions. arXiv preprint\narXiv:2302.09778, 2023. 2\n[18] Xin Huang, Ruizhi Shao, Qi Zhang, Hongwen Zhang, Ying\nFeng, Yebin Liu, and Qing Wang. Humannorm: Learning\nnormal diffusion model for high-quality and realistic 3d hu-\nman generation. arXiv preprint arXiv:2310.01406, 2023. 2\n[19] Yukun Huang, Jianan Wang, Yukai Shi, Xianbiao Qi, Zheng-\nJun Zha, and Lei Zhang. Dreamtime: An improved optimiza-\ntion strategy for text-to-3d content creation. arXiv preprint\narXiv:2306.12422, 2023. 2\n[20] Deemos. Inc.\ndreamface web demo.\nhttps : / /\nhyperhuman.deemos.com/, 2023. 7\n[21] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter\nAbbeel, and Ben Poole. Zero-shot text-guided object gen-\neration with dream fields. In CVPR, pages 867\u2013876, 2022.\n2\n[22] Ruixiang Jiang, Can Wang, Jingbo Zhang, Menglei Chai,\nMingming He, Dongdong Chen, and Jing Liao.\nAvatar-\ncraft: Transforming text into neural human avatars with\nparameterized shape and pose control.\narXiv preprint\narXiv:2303.17606, 2023. 2, 4, 12\n[23] Zutao Jiang, Guansong Lu, Xiaodan Liang, Jihua Zhu, Wei\nZhang, Xiaojun Chang, and Hang Xu. 3d-togo: Towards\ntext-guided cross-category 3d object generation. In AAAI,\npages 1051\u20131059, 2023. 1\n[24] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen\nChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:\nText-based real image editing with diffusion models.\nIn\nCVPR, pages 6007\u20136017, 2023. 2\n[25] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014. 5\n[26] Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol,\nJaakko Lehtinen, and Timo Aila.\nModular primitives for\nhigh-performance differentiable rendering. ACM Transac-\ntions on Graphics, 39(6), 2020. 5\n[27] Tingting Liao, Hongwei Yi, Yuliang Xiu, Jiaxiang Tang,\nYangyi Huang, Justus Thies, and Michael J. Black. TADA!\nText to Animatable Digital Avatars. In International Confer-\nence on 3D Vision (3DV), 2024. 1, 2, 4, 6, 7\n[28] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\ntext-to-3d content creation. In CVPR, pages 300\u2013309, 2023.\n1, 2\n[29] Zhengzhe Liu, Yi Wang, Xiaojuan Qi, and Chi-Wing Fu. To-\nwards implicit text-guided 3d shape generation. In CVPR,\npages 17896\u201317906, 2022. 1\n9\n[30] lllyasviel. Controlnet. https://huggingface.co/\nrunwayml/lllyasviel/sd-controlnet-depth,\n2023. 5\n[31] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and\nRana Hanocka. Text2mesh: Text-driven neural stylization\nfor meshes. In CVPR, pages 13492\u201313502, 2022. 2\n[32] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall.\nDreamfusion: Text-to-3d using 2d diffusion.\narXiv\npreprint arXiv:2209.14988, 2022. 2, 3, 11\n[33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 1, 2\n[34] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv preprint arXiv:2204.06125, 1\n(2):3, 2022. 2\n[35] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, pages 10684\u2013\n10695, 2022. 1, 2, 3, 4\n[36] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. In CVPR, pages 22500\u201322510, 2023. 2\n[37] RunwayML.\nStable diffusion v1.5.\nhttps : / /\nhuggingface.co/runwayml/stablediffusion-\nv1-5, 2022. 5\n[38] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding. NeurIPS, 35:36479\u201336494, 2022. 2\n[39] Aditya Sanghi, Hang Chu, Joseph G Lambourne, Ye Wang,\nChin-Yi Cheng, Marco Fumero, and Kamal Rahimi Malek-\nshan. Clip-forge: Towards zero-shot text-to-shape genera-\ntion. In CVPR, pages 18603\u201318613, 2022. 2\n[40] Aditya Sanghi, Rao Fu, Vivian Liu, Karl DD Willis, Hooman\nShayani, Amir H Khasahmadi, Srinath Sridhar, and Daniel\nRitchie. Clip-sculptor: Zero-shot generation of high-fidelity\nand diverse shapes from natural language. In CVPR, pages\n18339\u201318348, 2023. 7\n[41] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:\nOpen dataset of clip-filtered 400 million image-text pairs.\narXiv preprint arXiv:2111.02114, 2021. 2\n[42] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for train-\ning next generation image-text models. NeurIPS, 35:25278\u2013\n25294, 2022. 2\n[43] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. In International Conference\non Learning Representations, 2020. 2\n[44] timbrooks. Instructpix2pix. https://huggingface.\nco/runwayml/timbrooks/instruct- pix2pix,\n2023. 5\n[45] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro\nCuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj,\nand Thomas Wolf. Diffusers: State-of-the-art diffusion mod-\nels, 2022. 5\n[46] Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin\nBao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang\nWen, Qifeng Chen, et al. Rodin: A generative model for\nsculpting 3d digital avatars using diffusion. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 4563\u20134573, 2023. 1, 2\n[47] Erroll Wood, Tadas Baltru\u02c7saitis, Charlie Hewitt, Matthew\nJohnson, Jingjing Shen, Nikola Milosavljevi\u00b4c, Daniel Wilde,\nStephan Garbin, Toby Sharp, Ivan Stojiljkovi\u00b4c, et al. 3d face\nreconstruction with dense landmarks. In ECCV, pages 160\u2013\n177. Springer, 2022. 1\n[48] Menghua Wu, Hao Zhu, Linjia Huang, Yiyu Zhuang,\nYuanxun Lu, and Xun Cao. High-fidelity 3d face genera-\ntion from natural language descriptions. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 4521\u20134530, 2023. 2, 6, 7\n[49] Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wen-\ntian Zhang, Yefeng Zheng, and Mike Zheng Shou. Boxdiff:\nText-to-image synthesis with training-free box-constrained\ndiffusion. pages 7452\u20137461, 2023. 2\n[50] Jiale Xu, Xintao Wang, Weihao Cheng, Yan-Pei Cao, Ying\nShan, Xiaohu Qie, and Shenghua Gao. Dream3d: Zero-shot\ntext-to-3d synthesis using 3d shape prior and text-to-image\ndiffusion models. In CVPR, pages 20908\u201320918, 2023. 1\n[51] Jiale Xu, Xintao Wang, Weihao Cheng, Yan-Pei Cao, Ying\nShan, Xiaohu Qie, and Shenghua Gao. Dream3d: Zero-shot\ntext-to-3d synthesis using 3d shape prior and text-to-image\ndiffusion models. In CVPR, pages 20908\u201320918, 2023. 2\n[52] Kim Youwang, Kim Ji-Yeon, and Tae-Hyun Oh. Clip-actor:\nText-driven recommendation and stylization for animating\nhuman meshes. In ECCV, pages 173\u2013191. Springer, 2022.\n1\n[53] Huichao Zhang, Bowen Chen, Hao Yang, Liao Qu, Xu\nWang, Li Chen, Chao Long, Feida Zhu, Kang Du, and Min\nZheng. Avatarverse: High-quality & stable 3d avatar cre-\nation from text and pose. arXiv preprint arXiv:2308.03610,\n2023. 1, 4\n[54] Longwen Zhang, Qiwei Qiu, Hongyang Lin, Qixuan Zhang,\nCheng Shi, Wei Yang, Ye Shi, Sibei Yang, Lan Xu, and\nJingyi Yu.\nDreamface:\nProgressive generation of ani-\nmatable 3d faces under text guidance.\narXiv preprint\narXiv:2304.03117, 2023. 1, 2, 4, 6, 7\n[55] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models, 2023.\n2, 4\n[56] Wojciech Zielonka, Timo Bolkart, and Justus Thies. Towards\nmetrical reconstruction of human faces.\nIn ECCV, pages\n250\u2013269. Springer, 2022. 1\n10\nA. Appendix\nA.1. Implementation Details\nCamera settings. During the optimization, We employ a\ncamera with fixed intrinsic parameters: near=0.1, far=10,\nfov=12.59, rendering image size=224. For the camera ex-\ntrinsics, we defined a set of optional viewing angles and ran-\ndomly selected one of these angles as the rendering view-\npoint for optimization in each iteration. The elevation angle\nx \u2208 0, 10, 30, the azimuth angle y \u2208 {0, 30, 60, 300, 330},\nand the camera distance d \u2208 {1.5, 3}. We set these extrin-\nsics to ensure that the rendering always includes the facial\nregion.\nLight settings. We utilize spherical harmonic (SH) to rep-\nresent lighting. We pre-define 16 sets of spherical harmonic\n3-band coefficients. In each iteration of rendering, we ran-\ndomly select one set from these coefficients to represent the\ncurrent lighting.\nPrompt engineering. In the generation stage, for the face\ndescription prompt of a celebrity or a character, we add the\nprefix \u2018a zoomed out DSLR photo of \u2019. We also utilize the\nview-dependent prompt enhancement. For the azimuth in\n(0,45) and (315,360), we add a suffix \u2018 from the front view\u2019,\nfor the azimuth in (45,135) and (225,315), we add a suffix \u2018\nfrom the side view\u2019.\nSDS Time schedule. Following the Dreamfusion [32], we\nset the range of t to be between 0.98 and 0.02 in the SDS\ncomputation process. Besides, we utilize the linearly de-\ncreasing schedule for t, which is crucial for the stability of\nsynthesis. As the iteration progresses from 0 to the final\n(e.g. iteration 400), our t value linearly decreases from 0.98\nto 0.02.\nA.2. User survey as ablation\nWe conduct a user survey as ablation to further validate the\neffectiveness of our key design. A total of 100 volunteers\nparticipated in the experiment. We presented the results of\nour method and different degradation versions, alongside\nthe text prompts. Then we invited the volunteers to rate the\nfacial generation and editing. The ratings ranged from 1\nto 5, with higher scores indicating higher satisfaction. The\nuser rating results are shown in Tab. 2 and Tab. 3. The\nresults indicate that removing any of our key designs during\nthe face generation or face editing leads to a decrease in user\nratings. This suggests that our key designs are necessary for\nsynthesizing high-quality faces.\nGeneration\nours\nw/o Lyuv\ntex\nw/o Lpr\ntex\nw/o Lga\ntex & Lpr\ntex\n3.82\n3.77\n2.59\n1.78\nTable 2. Ablation study of face generation based on user ratings.\nEditing\nours\nw/o SC-weight\nw/o Reg\n3.95\n2.55\n2.28\nTable 3. Ablation study of face editing based on user ratings.\nFigure 11. Relighting of our synthesized 3D faces.\nGeneration + Sequential editing\nGeneration with \ncomposed prompt\nJason Statham\nMake his eyes big\nTurn him a \ncropper-made robot\nMake his lips red\nMake his eyes big\nLet him wear a\npurple Zorro mask Make his lips black\nMark Zuckerberg\nFigure 12. Generation with composed prompt leads to the loss of\nconcepts in prompts.\nA.3. More Relighting Results\nWe present some more relighting results in Fig 11.\nWe\nrecommend referring to the supplementary video or project\npage, where the video results can better demonstrate our an-\nimation and relighting effects.\nA.4. Generation with composed prompt\nOur sequential editing can synthesize complex 3D faces, an\nalternative approach is to combine all editing prompts into\na composed prompt and generate the face in one step.\nIn Fig.12, we showcase the results generated from a\ncomposed prompt with our generation stage. It can be ob-\nserved that directly generating with the composed prompt\nleads to the loss of certain concepts and details present in\nthe prompts (e.g., the cropped-made effect in row 1, or the\nblack lips in row 2). This underscores the necessity of the\nediting technique we propose for synthesizing customized\nfaces.\n11\nA.5. More Comparison Results\nWe conduct more comparisons with more baseline meth-\nods. We add two baselines: a public implementation [1]\nfor the Dreamfusion, and AvatarCraft [22], a SOTA text-to-\n3D avatar method that utilizes the implicit neutral field rep-\nresentation. We compare text-guided 3D face generation,\nsingle-round 3D face editing, and sequential 3D face edit-\ning. Note that baseline methods are not capable of directly\nediting 3D faces with text instruction (e.g., \u2018make her old\u2019),\nso we let them perform the editing by generating a face with\nthe composed prompt. For example, \u2018an old Emma Watson\u2019\nis the composed prompt of \u2018Emma Watson\u2019 and \u2018Make her\nold\u2019.\nWe present the 3D face generation results in Fig 13 and\nFig 14. The 3D face editing results are contained in Fig\n15 and Fig 16. The comparisons on sequential editing are\npresented in Fig 17 and Fig 18. It should be noted that\nDreamfusion [1] and Avatarcraft [22] occasionally fail to\nproduce meaningful 3D shapes and instead output a white\nbackground for some prompts. This issue could potentially\nbe addressed by resetting the random seed, however, due to\ntime constraints, we did not attempt repeated trials. We have\nlabeled these examples as \u2018Blank Result\u2019 in the figures.\n12\nDescribe3D\nDreamface\nTADA\nOurs\nAvatarcraft\nDreamfusion\nA teenager \nboy, Asian, \nyellow skin, \nround face\nElderly Asian \nman, gray \nbeard, deep \nwrinkles\nCaucasian \nteenager girl, \nthin eyebrows, \npink lipstick\nTaylor \nSwift\nTom\nCruise\nBarack \nHussein \nObama\nJake \nGyllenhaal\nCate \nBlanchett\nFigure 13. Comparison on text-guided 3D face generation.\n13\nDescribe3D\nDreamface\nTADA\nOurs\nAvatarcraft\nDreamfusion\nKratos in \nthe God \nof War\nThanos in \nthe Marvel\nCaptain \nAmerica\nNeteyam in \nAvatar\nThe Flash\nVoldemort\nRed Skull\nHellboy in \nHellboy\nseries\nFigure 14. Comparison on text-guided 3D face generation.\n14\nOriginal:\nEmma \nWatson\nEditing:\nMake her \nold\nOriginal:\nScarlett \nJohansson\nEditing:\nTurn her into \nHarley Quinn\nOriginal:\nScarlett \nJohansson\nEditing:\nLet her wear a\nGeisha makeup\nOriginal:\nBrad Pitt\nEditing:\nMake him \nmade of wood\nBlank\nResult\nBlank\nResult\nBlank\nResult\nDescribe3D\nDreamface\nTADA\nOurs\nAvatarCraft\nDreamfusion\nFigure 15. Comparison on text-guided single-round 3D face editing.\n15\nDescribe3D\nDreamface\nTADA\nOurs\nAvatarCraft\nDreamfusion\nOriginal:\nBenedict \nCumberbatch\nEditing:\nTurn him \na girl\nOriginal:\nAnne\nHathaway\nEditing:\nTurn her \na man\nOriginal:\nSpider-man\nEditing:\nMake him \nchubby\nOriginal:\nThe Joker\nEditing:\nMake his \neyemask blue\nBlank\nResult\nFigure 16. Comparison on text-guided single-round 3D face editing.\n16\nOriginal:\nJason \nStatham\nEditing step 1:\nMake his \neyes big\nEditing step 2:\nTurn him a \ncropper-made \nrobot\nEditing step 3:\nMake his \nlips red\nOriginal:\nHulk\nEditing step 1:\nMake him \nyoung\nEditing step 2:\nTurn him \nfemale\nEditing step 3:\nLet her wear \nBatman \neyemask\nDescribe3D\nDreamface\nTADA\nOurs\nAvatarCraft\nDreamfusion\nEditing\nstep 1\nOriginal\nEditing\nstep 2\nEditing\nstep 3\nEditing\nstep 1\nOriginal\nEditing\nstep 2\nEditing\nstep 3\nFigure 17. Comparison on text-guided sequential 3D face editing.\n17\nOriginal:\nMark \nZuckerberg\nEditing step 1:\nMake his \neyes big\nEditing step 2:\nLet him wear \na purple \nZorro mask\nEditing step 3:\nMake his \nlips black\nEditing step 1:\nTurn him \nAsian\nEditing step 2:\nGive him\nthick mustache\nEditing step 3:\nMake him\nbald\nOriginal:\nElon \nMusk\nBlank\nResult\nDescribe3D\nDreamface\nTADA\nOurs\nAvatarCraft\nDreamfusion\nEditing\nstep 1\nOriginal\nEditing\nstep 2\nEditing\nstep 3\nEditing\nstep 1\nOriginal\nEditing\nstep 2\nEditing\nstep 3\nFigure 18. Comparison on text-guided sequential 3D face editing.\n18\n"
  },
  {
    "title": "Towards Accurate Differential Diagnosis with Large Language Models",
    "link": "https://arxiv.org/pdf/2312.00164.pdf",
    "upvote": "8",
    "text": "Towards Accurate Differential\nDiagnosis with Large Language Models\nDaniel McDuff\u2217, \u2021, 1, Mike Schaekermann\u2217, \u2021, 1, Tao Tu\u2217, 1, Anil Palepu\u2217, 1,\nAmy Wang1, Jake Garrison1, Karan Singhal1, Yash Sharma1, Shekoofeh Azizi2,\nKavita Kulkarni1, Le Hou1, Yong Cheng2, Yun Liu1,\nS Sara Mahdavi1, Sushant Prakash1, Anupam Pathak1, Christopher Semturs1,\nShwetak Patel1, Dale R Webster1, Ewa Dominowska1,\nJuraj Gottweis1, Joelle Barral2, Katherine Chou1, Greg S Corrado1, Yossi Matias1,\nJake Sunshine\u2020, \u2021,1, Alan Karthikesalingam\u2020, \u2021,1 and Vivek Natarajan\u2020, \u2021,1\n1Google Research, 2Google DeepMind\nAn accurate differential diagnosis (DDx) is a cornerstone of medical care, often reached through an iterative\nprocess of interpretation that combines clinical history, physical examination, investigations and procedures.\nInteractive interfaces powered by Large Language Models (LLMs) present new opportunities to both\nassist and automate aspects of this process. In this study, we introduce an LLM optimized for diagnostic\nreasoning, and evaluate its ability to generate a DDx alone or as an aid to clinicians. 20 clinicians evaluated\n302 challenging, real-world medical cases sourced from the New England Journal of Medicine (NEJM)\ncase reports. Each case report was read by two clinicians, who were randomized to one of two assistive\nconditions: either assistance from search engines and standard medical resources, or LLM assistance in\naddition to these tools. All clinicians provided a baseline, unassisted DDx prior to using the respective\nassistive tools. Our LLM for DDx exhibited standalone performance that exceeded that of unassisted\nclinicians (top-10 accuracy 59.1% vs 33.6%, [p = 0.04]). Comparing the two assisted study arms, the DDx\nquality score was higher for clinicians assisted by our LLM (top-10 accuracy 51.7%) compared to clinicians\nwithout its assistance (36.1%) (McNemar\u2019s Test: 45.7, p < 0.01) and clinicians with search (44.4%) (4.75,\np = 0.03). Further, clinicians assisted by our LLM arrived at more comprehensive differential lists than\nthose without its assistance. Our study suggests that our LLM for DDx has potential to improve clinicians\u2019\ndiagnostic reasoning and accuracy in challenging cases, meriting further real-world evaluation for its ability\nto empower physicians and widen patients\u2019 access to specialist-level expertise.\n1 Introduction\nAn accurate diagnosis is a critical component of effective medical care. Building AI systems capable of\nperforming or assisting clinicians in this important task has been a long-standing grand challenge [1]. While\nprior focus has been on evaluating a machine\u2019s ability to accurately output a diagnosis [2\u20135], real-world clinical\npractice involves an iterative and interactive process of reasoning about a differential diagnosis (DDx), weighing\nmultiple diagnostic possibilities in the light of increasing amounts of clinical information over time (ranging\nfrom clinical history and examination to investigations and procedures). Deep learning has been applied to\npromising effect for generating DDx in a number of specialties including radiology [3], ophthalmology [4]\nand dermatology [2], but such systems lack the interactive capabilities to fluently assist a user through\ncommunication in natural language.\nThe emergence of Large Language Models (LLMs) present an opportunity to design novel interactive tools and\ninterfaces to aid in differential diagnosis. Such LLMs trained on vast corpora of text, can recognize, summarize,\npredict, and generate new text based on knowledge gained during the learning process and task specification\nvia a prompt. These models have demonstrated the ability to perform complex language comprehension and\nreasoning tasks, generating coherent text and thereby enabling a large variety of real-world applications [6\u20139].\nBoth general-purpose LLMs (GPT-4) and medical domain-specialized LLMs (Med-PaLM 2) have demonstrated\n\u2217 Equal contributions. \u2020 Equal leadership.\n\u2021 Corresponding authors: {dmcduff, mikeshake, jakesunshine, alankarthi, natviv}@google.com\narXiv:2312.00164v1  [cs.CY]  30 Nov 2023\nstrong performance in standardized and multiple-choice medical benchmarks [10, 11]. Such evaluations\nrepresent a natural starting point for probing the medical knowledge and capabilities but fail to measure\nutility in real-world scenarios for care delivery, for example in challenging medical cases faced by trained\nphysicians. It is also not obvious how these models might actively assist clinicians in the development of\na DDx. Recent work has begun to assess the standalone performance of these models on challenging case\nreports that involve complex deduction [5, 12, 13], but has stopped short of evaluating how they can assist\nclinicians and augment performance and empower them to provide better care.\nIn this work, we introduced and investigated the ability of an LLM optimised for clinical diagnostic reasoning,\nto generate a DDx in challenging, real-world medical cases. Beyond measuring standalone performance like\nprior work [5], we integrated this model into an interactive interface to measure how well our LLM could\nassist clinicians in developing a DDx. Using a set of challenging real-world cases from the New England\nJournal of Medicine (NEJM) case reports, we compared clinicians\u2019 ability to form a DDx with the assistance\nof our LLM, versus with access to traditional information retrieval tools (e.g., Internet search and books).\nThe LLM achieved impressive performance in both generating DDx lists that contained the correct diagnosis\n(i.e., top-10 accuracy) and in identifying the correct final diagnosis as the most likely in the list (i.e., top-1\naccuracy). Under automated model based evaluation, the quality and the accuracy of the DDx list produced\nby our LLM was found to be significantly better than the state-of-the-art GPT-4 model [5].\nPerhaps, more importantly, the LLM also improved the diagnostic capability of clinicians as measured by the\nquality of their DDx lists for the evaluated cases. LLMs optimized for the safety-critical medical domain such\nas ours present a novel paradigm for assisting clinicians because of the potential for variation in the ways in\nwhich a given individual may converse with the system and utilise it in collaborative reasoning. We used\nsemi-structured qualitative interviews to gather information from participating clinicians on their experiences\nof using the tool, their views of the potential role and risks of LLMs in medical diagnosis and in aiding the\ndifferential diagnosis process. These interviews highlighted the potential for LLMs to increase the diversity of\nDDx lists and speed up the process of arriving at a comprehensive DDx for challenging cases. The clinicians\nalso highlighted that the most appropriate application at the present time would be in learning and education.\nOur key contributions can be summarized as:\n\u2022 Introducing an LLM for DDx, a model optimized for differential diagnosis, alongside a user interface\nallowing clinicians to interact with the model for improving clinical diagnostic reasoning.\n\u2022 Evaluating the performance of the LLM on challenging diagnostic cases from the NEJM Case Reports.\n\u2022 Showing that the LLM outperforms the prior state of the art, GPT-4, in both top-1 and top-10 accuracy\non this benchmark under automated evaluation.\n\u2022 Evaluating the impact of the LLM as an assistive tool for clinicians in differential diagnosis, with\nrandomized comparison to the usual practice in which clinicians are assisted by Search and their usual\nclinical resources.\n2 NEJM Clinicopathological Conference Case Reports\nThe Case Records of the Massachusetts General Hospital (MGH) are published (lightly edited) transcriptions\nof the clinicopathological conferences of the MGH (Boston, MA). In the clinicopathological conference, a\npatient case presentation is described and then an expert physician is asked to provide a DDx and a final\ndiagnosis, along with their diagnostic reasoning, based only on the patient\u2019s provided medical history and\npreliminary test results. The published cases, organized generally as diagnostic puzzles culminating in a\ndefinitive, pathology-confirmed diagnosis, are published regularly in the NEJM. We leverage these case reports,\nlicensed from the NEJM, to evaluate the LLM\u2019s capability to generate a DDx alone and, separately, to aid\nclinicians in generation of their own differential. For this latter task, we developed a user interface for clinicians\nto interact with the LLM.\nA set of 326 case texts from the NEJM Clinicopathological Conference (CPC) series were considered. These\ncase reports were published over a 10 year period between June 13th 2013 and August 10th 2023. Of these, 23\n(7%) were excluded on the grounds that they discussed case management and were not primarily focused\non diagnosis. The articles were distributed over the years between 2013-2023 as follows: 2013 N=22, 2014\n|2\nN=34, 2015 N=36, 2016 N=35, 2017 N=36, 2018 N=16, 2020 N=23, 2021 N=36, 2022 N=39, 2023 N=26.\nThe supplementary material includes the full set of case numbers. The 302 cases include the 70 cases used by\nKanjee et al. [5].\nThese case reports cover a range of medical specialties. The largest proportion are from internal medicine\n(N=159), followed by neurology (N=42), pediatrics (N=33) and psychiatry (N=10). The text corresponding\nto the history of the present illness (HPI) was manually extracted from each article as input to the LLM.\nThe average (median) word count of these sections of the case reports is 1,031 words (mean: 1,044, SD: 296,\nrange: 378-2,428). The average (median) character count is 6,619 characters (mean: 6,760, SD: 1,983, range:\n2,426-15,196).\nA modified version of the article, inclusive of the provided HPI, admission imaging and admission labs (if\navailable in the case) was created for the human clinicians (see Fig. 1). This version had redacted the final\ndiagnosis, expert discussion of the DDx and any subsequent imaging or biopsy results (which are typical\nelements of the conclusion of the case challenges). Given the LLM is a text-only AI model, the admission\nimages and lab tables were not fed into the model. However, text-based descriptions of specific lab values or\nimaging findings were sometimes included in the case description.\n3 Training a Large Language Model for DDx\nOur study introduces an LLM for DDx, a model which uses a transformer architecture (PaLM 2 [7]), fine-tuned\non medical domain data; alongside an interface for enabling its use as an interactive assistant for clinicians.\nAs with Med-PaLM 2 [10], our LLM builds upon PaLM 2, an iteration of Google\u2019s LLM with substantial\nperformance improvements on multiple LLM benchmark tasks. For the purposes of this analysis the large (L)\nPaLM 2 model was used.\nThe LLM was fine-tuned with long context length on a task mixture consisting of medical question answering\n(multiple-choice and long-form questions), medical dialogue generation and electronic health record (EHR)\nnote summarization. The datasets used included the training splits of MultiMedQA (MedQA, MedMCQA,\nHealthSearchQA, LiveQA and MedicationQA) [10], a proprietary dataset of medical conversations, and expert\nhandcrafted EHR note summaries from MIMIC-III [14]. The capability to process long context input enables\nthe LLM to handle tasks that require long-range reasoning and comprehension.\nZero-Shot Prompting. We evaluated the LLM on each of the NEJM case studies with the following prompt:\n\u201cYou are a helpful medical assistant. You will be provided and asked about a complicated clinical case; read it\ncarefully and then provide a diverse and thorough DDx\".\nExample Input:\nCase: [Case Text]\nWhat are the top 10 most likely diagnoses? Be precise, listing one diagnosis per line, and try to cover\nmany unique possibilities (at least 10). The top 10 diagnoses are:\nExample Output:\n1. S\u00e9zary syndrome\n2. Adult T-cell leukemia/lymphoma\n3. Mycosis fungoides\n4. Atopic dermatitis\n5. Psoriasis\n6. Drug reaction with eosinophilia and systemic symptoms (DRESS)\n7. Graft-versus-host disease (GVHD)\n8. Cutaneous T-cell lymphoma, not otherwise specified\n9. Hypereosinophilic syndrome\n10. Systemic lupus erythematosus (SLE)\n|3\n4 The LLM for DDx User Interface\nThe interface associated with our LLM, depicted in Fig. 2, enables users to interact with the underlying model\nvia text-based chat in the context of a given case description. In our study, the interface was pre-populated\nwith a text-only representation of the history of present illness (HPI) for a given case. Clinicians were asked\nto initiate the interaction by querying the LLM using a suggested prompt. Following this initial prompt and\nthe LLM\u2019s response, clinicians were free to query the model using any additional follow-up questions, though\nclinicians were cautioned to avoid asking questions about information that had not already been presented in\nthe case. A pilot study indicated that without such a warning, clinicians may ask questions about specific lab\nvalues or imaging leading to confabulations.\nFor a given question, the interface generated the response by querying the LLM using prompt template:\nRead the case below and answer the question provided after the case.\nFormat your response in markdown syntax to create paragraphs and bullet points. Use \u2018<br><br>\u2019 to\nstart a new paragraph. Each paragraph should be 100 words or less. Use bullet points to list multiple\noptions. Use \u2018<br>* \u2019 to start a new bullet point. Emphasize important phrases like headlines. Use\n\u2018**\u2019 right before and right after a phrase to emphasize it. There must be NO space in between \u2018**\u2019 and\nthe phrase you try to emphasize.\nCase: [Case Text]\nQuestion (suggested initial question is \u201cWhat are the top 10 most likely diagnoses and\nwhy (be precise)?\u201d): [Question]\nAnswer:\n5 Methods\n5.1 Experimental Design\nIn order to comparatively evaluate the LLM\u2019s ability to generate a DDx alone and aid clinicians with their\nDDx generation we designed a two-stage reader study illustrated in Fig. 3). Our study was designed to\nevaluate the assistive effect of the LLM for generalist clinicians (not specialists) who only have access to the\ncase presentation and not the full case information (which would include the expert commentary on the DDx).\nThe first stage of the study had a counterbalanced design with two conditions. Clinicians generated DDx\nlists first without assistance and then a second time with assistance, where the type of assistance varied by\ncondition.\nStage 1. Clinicians generate DDx with and without assistance\nTwenty U.S. board-certified internal medicine physicians (median years of experience: 9, mean: 11.5, SD: 7.24,\nrange: 3-32) viewed the redacted case report, with access to the case presentation and associated figures and\ntables. They did this task in one of two conditions, based on random assignment.\nCondition I - Search. The clinicians were first instructed to provide a list of up to ten diagnoses, with a\nminimum of three, based solely on review of the case presentation without using any reference materials (e.g.,\nbooks) or tools (e.g., Internet search). Following this, the clinicians were instructed to use Internet Search or\nother resources as desired (but not given access to the LLM) and asked to re-perform their DDx.\nCondition II - LLM for DDx. As with condition I, the clinicians were first instructed to provide a list of\nup to 10 diagnoses, with a minimum of three, based solely on review of the case presentation without using\nany reference materials (e.g., books) or tools (e.g., Internet search). Following this the clinicians were given\naccess to the LLM and asked to re-perform their DDx. In addition to the LLM, clinicians could choose to use\nInternet search or other resources if they wished.\nStage 2. Specialists with full case information extract gold DDx and evaluate Stage 1 DDx\nNineteen U.S. board-certified specialist clinicians (median years of experience: 14, mean: 13.7, SD: 7.82, range:\n4-38) were recruited from internal medicine (N=10), neurology (N=3), pediatrics (N=2), psychiatry (N=1),\n|4\ndermatology (N=1), obstetrics (N=1), and emergency medicine (N=1). Their mean years of experience was\n13.7 (SD: 7.82, range: 4-38). These specialists were aligned with the specialty of the respective CPC case,\nviewed the full case report and were asked to list at least 5 and up to 10 differential diagnoses. Following this,\nthey were asked to evaluate the five DDx lists generated in Stage 1, including two DDx lists from condition 1\n(DDx without Assistance and DDx with Search Assistance), two DDx lists from condition 2 (DDx without\nAssistance and DDx with LLM Assistance) and the standalone LLM DDx list.\nThe specialists answered the following questions to evaluate the DDx lists:\nThe quality score developed by Bond et al. [15] and used by Kanjee et al. [5] is a differential score based\non an ordinal 5-point scale: \u201cHow close did the differential diagnoses (DDx) come to including the final\ndiagnosis?\u201d. The options were: 5. DDx includes the correct diagnosis, 4. DDx contains something that is very\nclose, but not an exact match to the correct diagnosis, 3. DDx contains something that is closely related and\nmight have been helpful in determining the correct diagnosis, 2. DDx contains something that is related, but\nunlikely to be helpful in determining the correct diagnosis, 1. Nothing in the DDx is related to the correct\ndiagnosis.\nAn appropriateness score: \u201cHow appropriate was each of the differential diagnosis lists from the different\nmedical experts compared the differential list that you just produced?\u201d. The options to respond were on a\nLikert scale of 5 (very appropriate) to 1 (very inappropriate).\nA comprehensiveness score: \u201cUsing your differential diagnosis list as a benchmark/gold standard, how\ncomprehensive are the differential lists from each of the experts?\u201d. The options to respond were: 4. The DDx\ncontains all candidates that are reasonable, 3. The DDx contains most of the candidates but some are missing,\n2. The DDx contains some of the candidates but a number are missing, 1. The DDx has major candidates\nmissing.\nFinally, specialists were asked to specify in which position of the DDx list the correct diagnosis was matched,\nin case it was included in the DDx at all.\nAutomated Evaluation. In addition to comparing against ground-truth diagnosis and expert evaluation\nfrom clinicians, we also created an automated evaluation of the performance of the five DDxs using a language-\nmodel based metric. Such automated metrics are useful as human evaluation is time and cost-prohibitive\nfor many experiments. We first extracted the (up to ten) individual diagnoses listed in each DDx. We\nleveraged minor text-processing steps via regular expressions to separate the outputs by newlines and strip\nany numbering before the diagnoses. Then we asked a medically fine-tuned language model, Med-PaLM 2 [10],\nwhether or not each of these diagnoses was the same as the ground-truth diagnosis using the following prompt:\nIs our predicted diagnosis correct (y/n)? Predicted diagnosis: [diagnosis], True diagnosis: [label]\nAnswer [y/n].\nA diagnosis was marked as correct if the language model output \u2019y\u2019.\n5.2 Qualitative Interviews\nFollowing the study we performed a semi-structured 30-minute interviews with five of the generalist clinicians\nwho participated in Stage 1. Semi-structured interviews explored the following questions:\n1. How did you find the task of generating a DDx from the case report text?\n2. Think about how you used Internet search or other resources. How were these tools helpful or unhelpful?\n3. Think about how you used the LLM for DDx. How was it helpful or unhelpful?\n4. Were there cases where you trusted the output of the search queries? Tell us more about the experience\nif so, such as types of cases, types of search results.\n5. Were there cases where you trusted the output of the LLM queries? Tell us more about the experience\nif so, such as types of cases, types of search results.\n6. Think about the reasoning provided by the LLM\u2019s interface? Where were they helpful? Where were\nthey unhelpful?\n7. What follow-up questions did you find most helpful to ask the LLM?\n|5\n8. How much time does it take to get used to the LLM? How was it intuitive? How was it unintuitive?\n6 Results\nIn evaluating the quality of the DDx lists we used several criteria, inspired by the approach taken in [5]\nand extended to draw additional insight from the clinicians. First, we measured whether the final diagnosis\nmatched an entry in the DDx list and in which position (top-N accuracy). Second, we used Bond et al.\u2019s [15]\nquality score and the appropriateness and comprehensiveness scales that we created. Combined these measures\nassess overall DDx quality, appropriateness and comprehensiveness.\nWhen using the LLM for assistance, clinicians asked, on average (mean), 2.92 questions in the interface\n(median, 2 and IQR, 1-4). On average (mean), clinician questions consisted of 9.39 words (median, 10 and\nIQR, 6-12) and 54.31 characters (median, 61 and IQR, 39-63). The LLM responses, on average (mean),\nconsisted of 237.60 words (median, 198 and IQR, 127-332) and 1540.81 characters (median, 1276 and IQR,\n815-2210).\nIn the Search condition the most popular tools were UpToDate (used in 34% of tasks), Google Search (30%)\nand PubMed (22%). While clinicians were allowed to use additional tools in the LLM condition, this was far\nless frequent (<5% of tasks).\n6.1 Performance of the Language Model on Differential Diagnosis\nQuality, Appropriateness and Comprehensiveness.\nOur language model\u2019s DDx lists achieved strong quality, appropriateness and comprehensiveness scores (see\nFig. 4). The median quality score was 5 (\u201cDDx includes the correct diagnosis\u201d) with 54% of DDx lists achieving\nthat score. The number of cases that scored 5 (i.e., the DDx included the top diagnosis) was statistically\nsignificantly higher for the LLM compared to clinicians without assistance (McNemar\u2019s Test: 64.4, p < 0.01).\nThe mean appropriateness score of 4.43 out of five (SD: 0.92). The median comprehensiveness score was 4 (=\n\u201cThe DDx contains all candidates that are reasonable\u201d) with 55% of the DDx lists achieving that score.\nThe mean appropriateness score of the LLM (4.34) was significantly higher than that for unassisted clinicians\n(3.74) (paired t-test 8.52,p < 0.001) and assisted clinicians in either the Search (3.80) (paired t-test 7.23, p <\n0.001) or LLM (4.06) (paired t-test 4.98, p < 0.001) conditions.\nTop-N Accuracy. For computing top-N accuracy, if any of the first N diagnoses in an individual DDx\nwere marked correct by the language model, the differential was considered to be correct. We computed the\nproportion of correct DDx lists across all cases to compute the top-N accuracy (for N from 1 to 10) for each\nDDx. The LLM reliably generated DDx lists that perform well against the ground-truth diagnosis. Fig. 5\nshows the top-N accuracy for the LLM. The LLM provided the correct diagnosis in 177 (59%) of the DDx\nlists and in 89 (29%) of the lists it was at the top of the list. These scores are above the scores the clinicians\nachieved in any of the conditions. The top-10 accuracy of the LLM (59.1%) was significantly higher than the\ntop-10 accuracy for the unassisted clinicians (33.6%) (p = 0.04).\nFig. 5 shows the top-N accuracy based on human and the automated metric. The results are broadly\ncomparable, illustrating that despite the final diagnoses often being complex and nuanced, the automated\nmetric faithfully captures the distinction between a DDx list that includes the correct diagnosis and one that\ndoes not.\n6.2 LLM for DDx as an Assistant for Differential Diagnosis\nQuality, Appropriateness and Comprehensiveness. Of the DDx lists created before assistance 37%\n(Search condition) and 29% (LLM for DDx condition) achieved a quality score of 5 (Fig. 4). In comparison\n49% of those created with assistance from the LLM scored 5.\nThe number of cases that scored 5 (i.e., the DDx included the top diagnosis) was statistically higher for\nclinicians assisted by the LLM compared to clinicians without assistance (McNemar\u2019s Test: 48.3, p < 0.01)\nand clinicians with Search assistance (5.45, p = 0.02).\nFor comprehensiveness, the number of cases that scored 4 (i.e., The DDx contains all candidates that are\n|6\nreasonable) was statistically higher for clinicians assisted by the LLM compared to clinicians without assistance\n(McNemar\u2019s Test: 185.8, p < 0.01) and clinicians with Search assistance (185.8, p < 0.01).\nThe mean appropriateness score after assistance with the LLM (4.06) was significantly higher than after\nassistance with Search (3.80) (paired t-test 3.32, p = 0.001) and the baseline (3.74) (paired t-test 4.79, p <\n0.001).\nTo summarize, with the support of the LLM, the quality, appropriateness and comprehensiveness scores for\nthe DDx lists were greater than for the lists prior to assistance (see Fig. 4).\nTop-N Accuracy. The top-N accuracy of the clinicians increased with assistance from the LLM compared\nto without (see Fig. 5). A Sankey diagram illustrates the impact of the two forms of assistance (Search and\nLLM for DDx) on top-10 accuracy (Fig. 6). In the LLM condition 73 cases that did not feature the final\ndiagnosis prior to using the tool included it after assistance from the LLM. This result is in contrast to only 37\ncases in the Search condition. Comparing the two assisted study arms, the DDx quality score was higher for\nclinicians assisted by our LLM (top-10 accuracy 51.7%) compared to clinicians without its assistance (36.1%)\n(McNemar\u2019s Test: 45.7, p < 0.01) and clinicians with search (44.4%) (4.75, p = 0.03).\n6.3 Duration of DDx Tasks with the LLM for DDx and Search\nThe time taken to generate updated DDx lists in the Search conditions vs the LLM condition were similar\n(Search: [7.19 minutes, SD = 5.33], LLM for DDx: [7.29 minutes, SD=6.41]). These were not significantly\ndifferent (paired t-test p = 0.807), which is surprising as the clinicians all had experience using Internet\nsearch and other information retrieval tools, yet they were using the LLM interface for the first time. We\nhypothesized that they would take longer using the LLM due to the initial learning curve.\n6.4 Length of DDx Lists Using the LLM for DDx and Search\nWhen unassisted, the median length of the DDx lists was 6 (IQR, 5-9); the mean was 6.41 (SD, 2.39). With\nsearch the median DDx list length was 7 (IQR, 5-10); the mean was 6.92 (SD, 2.52). With the LLM the\nmedian DDx list length was 8 (IQR, 6-10); the mean was 7.58 (SD, 2.33). With assistance from the LLM, the\nlength of the DDx lists was longer than without assistance (paired t-test: 7.13, p < 0.001) and longer than\nthe DDx lists with assistance from search (paired t-test: 3.15, p = 0.002).\n6.5 Contamination Analysis\nWe trained the LLM by building on an model pretrained on large-scale data and fine-tuning on medical\ndata. While we did not include NEJM case reports in the fine-tuning data for the model, it is possible that\npretraining data for the model contained partial mentions or full NEJM case reports, whether from the original\nsource (NEJM) or reproduced by other websites. To better understand the possibility of overlap between\nthe training corpus and the test data, we performed a contamination analysis using the pretraining corpora.\nWe looked for overlap between character sequences in the test articles and training corpora using a sliding\nwindow, searching for all instances of 512-character overlap. A case report is considered to have overlap if at\nat least one document from the pretraining corpora has an overlap. We identified that there was no overlap\nfor case reports beginning in 2022 and 2023. Some overlap existed for case reports published prior to 2022.\nWe calculated the top-N accuracy for the LLM on both of these sets of case reports, prior to 2022 (N = 238)\nand 2022 to date (N = 65), and did not observe a substantial difference in results. Across all years, 16.9% (51\nout of 302) of case reports displayed at least one instance of overlap.\n6.6 LLM for DDx Comparison with GPT-4\nAs we did not have the same set of human raters who evaluated the differentials produced by GPT-4 [5] and\nour LLM, we can not compare top-10 accuracy numbers directly. Therefore, in our study design, we evaluate\nperformance on that 70 cases subset (reported in [5]) using the automated metric (which is shown above to\nbe relatively consistent with human evaluation). Our LLM for DDx performs better with regard to top-N\naccuracy for N>1, with the gap being most prominent N>2 (Fig. 7). This suggests potentially significant\nimprovements in quality and comprehensiveness of the differentials produced by our LLM.\n|7\nTable 1 | Top-1 and Top-10 Accuracy. The percentage of DDx lists with the final diagnosis.\nModel-Only\nHuman\nLLM for DDx\nBefore Assistance After Search Assistance After LLM for DDx Assistance\nMetrics\nTop-1\u2191 Top-10\u2191 Top-1\u2191\nTop-10\u2191\nTop-1\u2191\nTop-10\u2191\nTop-1\u2191\nTop-10\u2191\nFull Set (302 Cases)\n29.2%\n59.1%\n15.9%\n33.6%\n24.3%\n44.5%\n25.2%\n51.8%\nSet without Overlap (56 Cases)\n35.4%\n55.4%\n13.8%\n34.6%\n29.2%\n46.2%\n24.6%\n52.3%\nDifference\n+6.2%\n-3.7%\n-2.1%\n+1.0%\n+4.9%\n+1.7%\n-0.6%\n+0.5%\n6.7 Qualitative Analysis\nWe describe our qualitative results that provide insight into how the clinicians viewed the LLM. We identified\nseveral key themes in the clinicians\u2019 responses and present several illustrative quotes below.\nComparing Search and the LLM for DDx. One clinician contrasted the use of Search to the LLM in this\nway: \u201cSearch was adequate when I had a good idea of what the differential ought to be to start with, but there\nwere some cases where I was only able to generate 3 or 4 because I really wasn\u2019t sure. If I put in \u2018infectious\ncauses of headache\u2019 [to the search engine] those were not very helpful.\u201d, whereas \u201c[the LLM] was required to\npull some additional diagnoses that may not have been the final diagnosis but would be important to think\nabout.\u201d.\nUse Cases.\nC3: \u201cI think if [the LLM] is intended for medical students or for medical education then it could be very very\npowerful tool.\u201d.\nConfabulations.\nC1: \u201cI walked into it thinking I could ask what ever I want, but if it was something that could not be inferred\nfrom the case I might get a response that isn\u2019t real or appropriate. C3: \u201cThe biggest thing I think that I had a\nconcern about was inaccuracy for someone who does not have a clinical background\u201d. But the clinicians found\nways to work around these limitations by leveraging their expertise or other resources to validate responses.\nEase of Use.\nC2: \u201cFor me it was very intuitive, it was a very easy tool to use, I wish I had it every single day.\u201d). These\ncomments highlight that natural language interfaces have a very low barrier to use.\n7 Discussion\nWe used a popular series of complex diagnostic challenges to evaluate an LLM optimized for clinical reasoning\nand diagnosis (LLM for DDx); both in a standalone capacity and under randomized comparisons as an assistive\ntool for physicians. In standalone performance, the LLM generated more appropriate and comprehensive\nDDx lists than physicians when they were unassisted, with its DDx lists more likely to include the final\ndiagnosis than DDx lists from a board-certified internal medicine physician, no matter which position in the\nDDx list was considered (i.e., top-N accuracy with N ranging from 1 to 10). Clinicians using the LLM as\nan assistant produced a DDx with higher top-N accuracy, and DDx with greater quality, appropriateness\nand comprehensiveness; compared to the status quo for clinical practice (use of Internet search and other\nresources).\nThe NEJM clinicopathological conferences (CPCs) examined here are well-known for being unique and\nchallenging clinical conundrums. Within this distinctive setting, the proposed LLM outperformed an unassisted\nboard-certified physician in both top-1 and top-n performance. While the CPCs have long been used as\nbenchmarks for difficult diagnosis, it is also well-known that performance in CPCs in no way reflects a broader\nmeasure of competence in a physician\u2019s duties [16]. Furthermore, the act of DDx comprises many other steps\nthat were not scrutinized in this study, including the goal-directed acquisition of information under uncertainty\n(known to be challenging for AI systems despite recent technical progress in this direction [17\u201319]). While\n|8\nbased on real-world cases, the clinical pathology case presentation format and input into the model does differ\nin important ways from how a clinician would evaluate a patient and generate their DDx at the outset of a\nclinical encounter. For example, while the case reports are created as \u201cpuzzles\u201d with enough clues that should\nenable a specialist to reason towards the final diagnosis, it would be challenging to create such a concise,\ncomplete and coherent case report at the beginning of a real clinical encounter.\nWe are therefore very cautious to extrapolate our findings toward any implications about the LLM\u2019s utility as\na standalone diagnostic tool. Nevertheless, our controlled evaluation mirrored the findings of other recent\nworks exploring the performance of both LLMs and pre-LLM \u201cDDx generator\u201d in smaller subsets of the NEJM\nCPCs, which have shown the potential for automated technology to reach the correct DDx in these challenging\ncases with superior performance to standalone physicians [5, 12, 13, 20]. While this represents a step beyond\nhistorical attempts at automating DDx in NEJM CPCs, where computerized approaches were deemed overtly\nunreliable for practical use [21], such studies also undertook limited consideration of the quality of DDx\ngenerated by these automated systems or their role as assistive tools.\nOur work extends prior observations by showing not only that the LLM was more likely to arrive at a correct\nanswer or provide the correct answer in a list, but that its DDx were determined by an independent rater to\nbe of higher appropriateness and comprehensiveness than those produced by board certified physicians with\naccess to references and search.\nIn our study clinicians had access to both images and tabular data in redacted case reports, while the LLM\nwas only provided with the main body of the text. Though the LLM outperformed the clinicians despite\nthis limitation, it is unknown whether and how much this gap would widen if the LLM had access to the\nfigures and tables. Early evidence suggests the effect might be case/context dependent as other studies have\nfound image access by models to not always improve performance in CPCs [13]. Furthermore, the integration\nof multimodal inputs by LLMs is an area of novel research [22, 23], with a large potential number of data\nmodalities to consider, and little precedent for how information from multiple modalities should be integrated\nover time for a single case by AI systems.\nThe repeated examination of NEJM CPCs by automated systems highlights its promise as a \u201cbenchmark\u201d for\nLLM evaluation and development. Benchmarking enables comparisons of models against one another and the\nability to evaluate a model\u2019s performance improvements or degradation over time. However, consistency in\nusing CPCs as a scalable benchmark is challenging if reliant upon using human judgement to establish whether\na candidate differential matches the ground truth. We utilized an automated approach for comparing our LLM\nfor DDx to a baseline LLM performance (GPT-4). Our estimates varied from that recently published in other\nstudies, despite using the same subset of cases [5]. Direct comparisons of different technologies would ideally\nbe conducted by more extensive and blinded human evaluation, including work to ensure reproducibility of\nthe human evaluation protocol, analysis of inter-rater reliability, and the use of metrics reflecting the quality,\nappropriateness and comprehensiveness of LLM differentials in addition to estimations of accuracy. Our\nestimates of top-1 and top-10 accuracy, while impressive at close to 30% and 60% respectively, highlight\nnoteworthy room for improvement for LLMs, especially for complex cases that are non-pathognomonic (i.e.,\ncases that do not have a sign or symptom that defines a diagnosis). However, as noted above, the CPCs\nrepresent \u201cdiagnostic puzzles\u201d rather than real-world examples of common clinical workflows; and it is therefore\nimportant to consider more realistic settings in which LLMs might prove of practical value in medicine.\nOne such example is the potential for LLMs to assist clinicians in complex diagnoses. Deep learning tools\nhave shown considerable promise in many areas of medicine, but are overwhelmingly used as assistive rather\nthan autonomous tools [24], given the safety-critical nature of medical practice and the many issues of\nrobustness [25] and fairness [26\u201328] seen in deployment. Furthermore, observations of standalone diagnostic\naccuracy often do not guarantee that an AI tool will improve performance in real-world settings as an assistive\ntool, and it remains unclear how to optimally integrate AI and human decision-making in medicine [29]. For\nLLMs in particular, the known incidence of hallucination/confabulation [30] might mislead clinicians into\ninaccurate diagnosis, replicating or even extending findings in other clinical settings that AI systems might\nactually degrade the performance of clinicians rather than necessarily improving outcomes.\nThis highlights the importance of focused study of LLMs in assistive scenarios. We explored this specifically in\nNEJM CPCs and found that the proposed LLM for DDx, increased the number of appropriate DDx produced\n|9\nby a clinician when used as an assistive tool in addition to overall top-N accuracy, suggesting that the LLM\u2019s\nprimary assistive potential may be due to making the scope of DDx more complete. Given the potential for\nmisleading information to arise from AI systems, including in convincing dialogue, clinicians must appreciate\nthe fundamental limitations of these models and not lose sight of their primacy in the provider-patient\nrelationship and their ultimate authority and responsibility for the diagnostic and therapeutic management of\ntheir patients. Such thoughtful and effective LLM use should not be unintuitive to most clinicians. Aiding\nthe diagnostic process could reasonably occur in an emergency room upon presentation (during potentially\ntime-sensitive moments), upon admission to the medical ward, or by a consulting service after a patient has\nbeen admitted or in outpatient clinics. Our findings suggest that onward research should more rigorously\nexplore how LLMs augment clinicians\u2019 DDx in many such specific scenarios, where the risks and benefits\nmight vary.\nDespite being a novel tool, the use of the LLM did not seem to add inefficiency or increase the amount of time\nspent on solving each CPC compared to the use of Search or other conventional information. This suggests\nthat the conversational interface was unobtrusive and intuitive. Consistent with this, the interviewed clinicians\nall described it as \u201ceasy\u201d to use, and were positive about the use and implications of the LLM interface.\nEnhancing efficiency while maintaining or improving quality are generally accepted goals of improving health\ncare delivery, alongside improving provider experience [31], and our study showed significant potential in this\nregard, as clinicians also reported feeling more confident in their DDx lists after using the model. \u201cThat is\nwhere the search really became difficult, I didn\u2019t know what to put in to narrow down my search that is going\nto help me narrow down my differential.\u201d However, there are numerous human factors, social elements, and\nother complex considerations in these use cases, and it is critical to ensure efforts are made to avoid inequities\nin access to avoid exacerbating existing health disparities.\nClinicians frequently expressed excitement about using the LLM but were also aware of the shortcomings\nof language models and had concerns about confabulations in particular if used by individuals not trained\nor instructed to avoid such questions. However, our work did not explore many other important aspects of\nhuman-AI interaction, which require further study in safety-critical settings such as this. For example, we\ndid not explore the extent to which clinicians trusted the outputs of the model or their understanding of\nits training and limitations, or undertake focused \u201conboarding\u201d or training in its use, which are all known\nto be important modifiers of the benefits derived by clinicians from AI assistants [32]. The CPC challenges\nthemselves do not enable a rigorous exploration of the possible impacts of AI assistance on health equity and\nfairness; a further study of how these aspects of clinicians\u2019 DDx is impacted by LLM assistance is needed.\nWhile AI systems are known to be able to express uncertainty [33] and defer appropriately to clinicians [34],\nwhich might significantly improve the balance between trust and skepticism needed for effective AI assistance\nin medicine. Qualitative feedback suggested that there remains room for targeted improvement of LLMs\nas assistive diagnostic tools, with one clinician noting that \u201cIt was most helpful for simpler cases that were\nspecific keywords or pathognomonic signs.\u201d (C3) but for more complex cases it still tended to draw conclusions\nfrom isolated symptoms rather than viewing the case holistically. The assistive effect of these LLMs could\npotentially \u2018upskill\u2019 clinical providers, particularly in enabling them to broaden and enhance the quality of\ntheir DDx. As corroborated via our clinician interviews after their experience with the LLM, such upskilling\ncould be relevant for education or training purposes to support providers across a skill continuum ranging\nfrom trainees to attending providers. The upskilling capabilities could also extend to locations where specialist\nmedical training is less common (e.g., in lower and middle income countries [LMIC]). However, our findings\nmay not generalise to these scenarios, given that we utilized a pool of twenty clinicians with a mean experience\nof 11.5 years. This may not adequately represent the diverse set of users seeking to benefit from LLMs as a\ndiagnostic aid. Further studies are warranted in an array of more realistic clinical scenarios, with a wider\nrange of clinical users that might range from medical students to allied health professionals. The underlying\nmechanism of assistance also merits further study and might be an important factor for optimising the assistive\neffect of LLMs, particularly given their conversational and interactive nature. For example, our study did not\nexplore the impact of LLMs on the clinical reasoning process.\n|10\n8 Limitations\nThere are limitations to this evaluation.\nWhile based on real-world cases, the clinical pathology case\npresentation format and input into the model does differ in important ways from how a clinician would\nevaluate a patient and generate their differential diagnosis at the outset of a clinical encounter. The case\nreports are created as \u201cpuzzles\u201d with enough clues that should enable a specialist to reason towards the final\ndiagnosis. At the beginning of a clinician encounter, it would be challenging to create such a concise, complete\nand coherent case report. Case reports in the NEJM style would not be available when at intake. Similarly,\nthese cases were selected to represent challenging cases instead of common conditions (i.e., \u2018zebras\u2019 as opposed\nto \u2018horses\u2019 in clinical parlance). As such, our evaluation does not directly indicate the results of or suggest\nthat clinicians should leverage the assistive capabilities of an LLM for typical cases seen on a daily basis.\nIn terms of modalities, the case reports include both images and tables. The clinicians had access to these in\nthe redacted case reports. However, the LLM only had access to the main body of the text. Though the LLM\nfor DDx outperformed the clinicians despite this limitation, it is unknown whether and how much this gap\nwould widen if the LLM had access to the figures and tables. Early evidence suggests the effect might be\ncase/context dependent [13].\nThe study highlighted some weaknesses of the existing LLM. Specifically, one clinician highlighted that \u201cIt\nwas most helpful for simpler cases that were specific keywords or pathognomonic signs.\u201d (C3) but that for\nmore complex cases it still tended to draw conclusions from isolated symptoms rather than viewing the case\nholistically. Considering the significance of assessing challenging cases, the NEJM CPC case reports will likely\nserve as a useful dataset for continued benchmarking as LLMs improve in performance.\n9 Conclusion\nGenerating a DDx is a critical step in clinical case management, and the capabilities of LLMs present new\nopportunities for assistive tooling to help with this task. Our randomized study showed that the LLM for\nDDx was a helpful AI tool for DDx generation for generalist clinicians. Clinician participants indicated utility\nfor learning and education, and additional work is needed to understand suitability for clinical settings.\nAcknowledgments\nThis project was an extensive collaboration between many teams at Google Research and Google DeepMind.\nWe thank Ayush Jain, Rory Sayres, Sami Lachgar, Lauren Winer, Maggie Shiels, Brett Harfield, Si Wai\nMan, Preeti Singh, Annisah Um\u2019rani, Bradley Green, and Philip Mansfield for their valuable insights and\nfeedback during our research. We are also grateful to Micheal Howell, Meredith Morris, Celeste Grade, Karen\nDeSalvo, Zoubin Ghahramani, James Manyika, and Jeff Dean for their support during the course of this\nproject. Finally, we extend our sincere thanks to the Massachusetts Medical Society group for the support\nand partnership.\nCompeting interests\nThis study was funded by Alphabet Inc and/or a subsidiary thereof (\u2018Alphabet\u2019). All authors are employees\nof Alphabet and may own stock as part of the standard compensation.\n|11\nReferences\n1.\nSzolovits, P. & Pauker, S. G. Categorical and probabilistic reasoning in medical diagnosis. Artificial Intelligence 11, 115\u2013144\n(1978).\n2.\nLiu, Y., Jain, A., Eng, C., Way, D. H., Lee, K., Bui, P., Kanada, K., de Oliveira Marinho, G., Gallegos, J., Gabriele, S.,\net al. A deep learning system for differential diagnosis of skin diseases. Nature medicine 26, 900\u2013908 (2020).\n3.\nRauschecker, A. M., Rudie, J. D., Xie, L., Wang, J., Duong, M. T., Botzolakis, E. J., Kovalovich, A. M., Egan, J., Cook,\nT. C., Bryan, R. N., et al. Artificial intelligence system approaching neuroradiologist-level differential diagnosis accuracy at\nbrain MRI. Radiology 295, 626\u2013637 (2020).\n4.\nBalas, M. & Ing, E. B. Conversational ai models for ophthalmic diagnosis: Comparison of chatgpt and the isabel pro\ndifferential diagnosis generator. JFO Open Ophthalmology 1, 100005 (2023).\n5.\nKanjee, Z., Crowe, B. & Rodman, A. Accuracy of a Generative Artificial Intelligence Model in a Complex Diagnostic\nChallenge. JAMA (2023).\n6.\nOpenAI. GPT-4 Technical Report 2023. arXiv: 2303.08774 [cs.CL].\n7.\nAnil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen, Z., et al.\nPalm 2 technical report. arXiv preprint arXiv:2305.10403 (2023).\n8.\nScao, T. L., Fan, A., Akiki, C., Pavlick, E., Ili\u0107, S., Hesslow, D., Castagn\u00e9, R., Luccioni, A. S., Yvon, F., Gall\u00e9, M., et al.\nBloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100 (2022).\n9.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S.,\net al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023).\n10.\nSinghal, K., Azizi, S., Tu, T., Mahdavi, S. S., Wei, J., Chung, H. W., Scales, N., Tanwani, A., Cole-Lewis, H., Pfohl, S.,\net al. Large language models encode clinical knowledge. Nature, 1\u20139 (2023).\n11.\nNori, H., King, N., McKinney, S. M., Carignan, D. & Horvitz, E. Capabilities of gpt-4 on medical challenge problems. arXiv\npreprint arXiv:2303.13375 (2023).\n12.\nEriksen, A. V., Moller, S. & Ryg. Use of GPT-4 to Diagnose Complex Clinical Cases. NEJM AI (2023).\n13.\nBuckley, T., Diao, J. A., Rodman, A. & Manrai, A. K. Accuracy of a Vision-Language Model on Challenging Medical\nCases 2023. arXiv: 2311.05591 [cs.CV].\n14.\nJohnson, A. E., Pollard, T. J., Shen, L., Lehman, L.-w. H., Feng, M., Ghassemi, M., Moody, B., Szolovits, P., Anthony Celi,\nL. & Mark, R. G. MIMIC-III, a freely accessible critical care database. Scientific data 3, 1\u20139 (2016).\n15.\nBond, W. F., Schwartz, L. M., Weaver, K. R., Levick, D., Giuliano, M. & Graber, M. L. Differential diagnosis generators:\nan evaluation of currently available computer programs. Journal of general internal medicine 27, 213\u2013219 (2012).\n16.\nLedley, R. S. & Lusted, L. B. Reasoning foundations of medical diagnosis: symbolic logic, probability, and value theory aid\nour understanding of how physicians reason. Science 130, 9\u201321 (1959).\n17.\nHong, J., Levine, S. & Dragan, A. Zero-Shot Goal-Directed Dialogue via RL on Imagined Conversations. arXiv preprint\narXiv:2311.05584 (2023).\n18.\nKossen, J., Cangea, C., V\u00e9rtes, E., Jaegle, A., Patraucean, V., Ktena, I., Tomasev, N. & Belgrave, D. Active Acquisition for\nMultimodal Temporal Data: A Challenging Decision-Making Task. arXiv preprint arXiv:2211.05039 (2022).\n19.\nMackie, I., Chatterjee, S. & Dalton, J. Generative Relevance Feedback with Large Language Models. arXiv preprint\narXiv:2304.13157 (2023).\n20.\nFritz, P., Kleinhans, A., Raoufi, R., Sediqi, A., Schmid, N., Schricker, S., Schanz, M., Fritz-Kuisle, C., Dalquen, P., Firooz,\nH., et al. Evaluation of medical decision support systems (DDX generators) using real medical cases of varying complexity\nand origin. BMC Medical Informatics and Decision Making 22, 254 (2022).\n21.\nMiller, R. A., Pople Jr, H. E. & Myers, J. D. in Computer-assisted medical decision making 139\u2013158 (Springer, 1985).\n22.\nLi, C., Wong, C., Zhang, S., Usuyama, N., Liu, H., Yang, J., Naumann, T., Poon, H. & Gao, J. Llava-med: Training a large\nlanguage-and-vision assistant for biomedicine in one day. arXiv preprint arXiv:2306.00890 (2023).\n23.\nTu, T., Azizi, S., Driess, D., Schaekermann, M., Amin, M., Chang, P.-C., Carroll, A., Lau, C., Tanno, R., Ktena, I., et al.\nTowards generalist biomedical ai. arXiv preprint arXiv:2307.14334 (2023).\n24.\nMuehlematter, U. J., Daniore, P. & Vokinger, K. N. Approval of artificial intelligence and machine learning-based medical\ndevices in the USA and Europe (2015\u201320): a comparative analysis. The Lancet Digital Health 3, e195\u2013e203 (2021).\n25.\nRoschewitz, M., Khara, G., Yearsley, J., Sharma, N., James, J. J., Ambr\u00f3zay, \u00c9., Heroux, A., Kecskemethy, P., Rijken, T.\n& Glocker, B. Automatic correction of performance drift under acquisition shift in medical image classification. Nature\nCommunications 14, 6608 (2023).\n26.\nObermeyer, Z., Powers, B., Vogeli, C. & Mullainathan, S. Dissecting racial bias in an algorithm used to manage the health\nof populations. Science 366, 447\u2013453 (2019).\n27.\nSeyyed-Kalantari, L., Zhang, H., McDermott, M. B., Chen, I. Y. & Ghassemi, M. Underdiagnosis bias of artificial intelligence\nalgorithms applied to chest radiographs in under-served patient populations. Nature medicine 27, 2176\u20132182 (2021).\n28.\nSamorani, M., Harris, S. L., Blount, L. G., Lu, H. & Santoro, M. A. Overbooked and overlooked: machine learning and\nracial bias in medical appointment scheduling. Manufacturing & Service Operations Management 24, 2825\u20132842 (2022).\n29.\nGaube, S., Suresh, H., Raue, M., Merritt, A., Berkowitz, S. J., Lermer, E., Coughlin, J. F., Guttag, J. V., Colak, E. &\nGhassemi, M. Do as AI say: susceptibility in deployment of clinical decision-aids. NPJ digital medicine 4, 31 (2021).\n30.\nUmapathi, L. K., Pal, A. & Sankarasubbu, M. Med-halt: Medical domain hallucination test for large language models.\narXiv preprint arXiv:2307.15343 (2023).\n31.\nSikka, R., Morath, J. M. & Leape, L. The quadruple aim: care, health, cost and meaning in work 2015.\n32.\nCai, C. J., Winter, S., Steiner, D., Wilcox, L. & Terry, M. \" Hello AI\": uncovering the onboarding needs of medical\npractitioners for human-AI collaborative decision-making. Proceedings of the ACM on Human-computer Interaction 3,\n1\u201324 (2019).\n33.\nYin, Z., Sun, Q., Guo, Q., Wu, J., Qiu, X. & Huang, X. Do Large Language Models Know What They Don\u2019t Know? arXiv\npreprint arXiv:2305.18153 (2023).\n|12\n34.\nDvijotham, K., Winkens, J., Barsbey, M., Ghaisas, S., Stanforth, R., Pawlowski, N., Strachan, P., Ahmed, Z., Azizi, S.,\nBachrach, Y., et al. Enhancing the reliability and accuracy of AI-enabled diagnosis via complementarity-driven deferral to\nclinicians. Nature Medicine 29, 1814\u20131820 (2023).\n|13\nFigure 1 | NEJM Clinicopathological Conference Case Reports. History of Present Illness, Admission Labs and\nAdmission Imaging sections were included in the redacted version presented to generalist clinicians for producing a DDx. The\nLLM had access to only the History of Present Illness. Specialist clinicians evaluating the quality of the DDx had access to the\nfull (unredacted) case report including the expert differential discussion.\n|14\nA 72-Year-Old Man with Hemiparesis, \nConfusion, Difficulty Speaking\nA 72 year old male patient presents to ER \nafter waking up 18 hours ago with sudden \nonset right hemiparesis, confusion, and \ndifficulty speaking, which is not improving. \nYou are able to find out directly from his \ndaughter who found him at home and \nbrought him in.\n\nPast medical history is notable for a 20 \npack-year smoking history before quitting \na few years ago. The patient takes \natorvastatin for dyslipidemia and an ACE \ninhibitor for hypertension, but is otherwise \nhealthy. He has been retired for the last\u2028\n6 years.\nHi, I am here to assist you in developing a differential \ndiagnosis for this case. Ask me anything.\nWhat are the top 10 most likely diagnoses and why?\nThe sudden onset of right hemiparesis, confusion, and \ndifficulty speaking in this patient is concerning for a stroke. \nThe differential diagnosis for stroke includes\u009b\n\u00ad\u00aa Ischemic stroke: This is the most common type of \nstroke, and is caused by a blockage of blood flow to the \nbrain. Risk factors for ischemic stroke include age, \nhypertension, dyslipidemia, smoking, and diabetes\u00a1\n\u0088\u00aa Hemorrhagic stroke: This type of stroke is caused by \nbleeding in the brain. It can be caused by a ruptured ...\nAsk a question or try a suggested prompt\nSend\nSubmit session for research\nExperimental\nQuestions to try\nQuestions to try\nDifferential diagnosis\nAsk a question or try a suggested prompt\nSend\nWhat will help distinguish between stroke and TIA?\nTIAs are transient ischemic attacks that resolve within 24 \nhours. Strokes are permanent neurologic deficits that last \nlonger than 24 hours.\n\nIn this case, the patient's symptoms have been present for \nseveral hours and are still ongoing, so it is likely that he is \nhaving a stroke. However, further evaluation is needed to \nconfirm the diagnosis and determine the cause of the \nstroke.\nA\nB\nC\nFigure 2 | The LLM for DDx User Interface. The history of the present illness (text only) was pre-populated in the user\ninterface (A) with an initial suggested prompt to query the LLM (B). Following this prompt and response, the user was free to\nenter any additional follow-up questions (C). The case shown in this figure is a mock case selected for illustrative purposes only.\nMerlin DDx\nBaseline DDx\nDDx with \nSearch\n(Condition I)\nStage 1\nStage 2\nBaseline \nDDx\n(Condition I)\nBaseline \nDDx\n(Condition II)\nDDx with \nLLM\n(Condition II)\nDDx w/ Search\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nThe clinician generated a \ngold-standard DDx from \nthe expert discussion.\nCondition I\nGold DDx\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nBaseline DDx\nDDx w/ LWM\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nCondition II\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nLLM for \nDDx\n(Condition II)\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nCondition III\nClinician A\n(Generalist)\nCase Presentation \n(Redacted Report)\n+\nClinician B\n(Generalist)\nCase Presentation \n(Redacted Report)\n+\nMerlin\n(LLM)\nCase Presentation \n(Redacted Report - \nText Only)\n+\nFull Case Report\n+\nClinician C\nClinician A\nClinician B\nCondition I:\nCondition II:\nCase Presentation \n(Redacted Report)\n+\nCase Presentation \n(Redacted Report)\n+\nBaseline DDx\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nInternet \nsearch\nDDx w/ Search\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nBaseline DDx\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nDDx w/ LLM\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nLLM for DDx\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nStage 1\nClinicians who were \nrandomized to 1 of 2 \nconditions, reviewed \nthe redacted case \nreports and generated \nDDx lists.\n(Language Model)\nStage 2\nA clinician with the \nnecessary training and \naccess to the expert \ndiscussion evaluated \nthe quality of the DDx \nlists.\nThen performed a \npairwise comparison with \nthe DDx candidates.\nFigure 3 | Experimental Design. To evaluate the LLM\u2019s ability to generate DDx lists and aid clinicians with their DDx\ngeneration, we designed a two-stage reader study. First, clinicians with access only to the case presentation completed DDx lists\nwithout using any assistive tools. Second, the clinicians completed DDx lists with access either to Search engines and other\nresources, or to LLM in addition to these tools. Randomization was employed such that every case was reviewed by two clinicians,\none with LLM assistance and one without. These DDx lists were then evaluated by a specialist who had access to the full case\nand expert commentary on the differential diagnosis, but who was blinded to whether and what assistive tool was used.\n|15\n5\n4\n3\n2\n1\nScore:\nThe correct \ndiagnosis\n Something\n very close to the\n correct diagnosis\nSomething\nthat might have\nbeen helpful\nSomething that\nis related, but \nunlikely to be helpful\nNothing related \nto the correct\ndiagnosis\nDDx contains:\n4\n3\n2\n1\nThe DDx contains all \ncandidates that \nare reasonable\nThe DDx contains most \nof the candidates but \nsome are missing\nThe DDx contains some \nof the candidates but \na number are missing\nThe DDx has \nmajor candidates\n missing\nQuality Score: Inclusion of the Final Diagnosis\nComprehensiveness Score\nAppropriateness Score\n5\n4\n3\n2\n1\nVery\nappropriate\nVery\nInappropriate\n(c)\n(a)\n(b)\nLLM for DDx Only\nClinician Assisted\nby LLM for DDx\nClinician Assisted\nby Search\nClinician Unassisted\n(LLM for DDx Condition)\nClinician Unassisted\nSearch Condition)\nLLM for DDx Only\nClinician Assisted\nby LLM for DDx\nClinician Assisted\nby Search\nClinician Unassisted\n(LLM for DDx Condition)\nClinician Unassisted\n(Search Condition)\nFigure 4 | Evaluation of the quality of generalists\u2019 DDx lists. (a) DDx Quality Score based on the question: \u201cHow close\ndid the differential diagnoses (DDx) come to including the final diagnosis?\u201d (b) DDx Comprehensiveness Score based on the\nquestion: \u201cUsing your differential diagnosis list as a bench mark/gold standard, how comprehensive are the differential lists\nfrom each of the experts?\u201d (c) DDx Appropriateness Score based on the question: \u201cHow appropriate was each of the differential\ndiagnosis lists from the different medical experts compared the differential list that you just produced?\u201d In all cases, the LLM and\nclinicians assisted by the LLM scored the highest overall. Numbers reflect the number of cases (out of 302). Note: The clinicians\ncould answer \u201cI am not sure\u201d in response to these questions, in a very small number (< 1%) of cases they used that option.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nTop-n\n10\n20\n30\n40\n50\n60\nAccuracy (%)\nLLM for DDx only\nClinician Assisted by LLM for DDx\nClinician Assisted by Search\nClinician Unassisted\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nTop-n\n10\n20\n30\n40\n50\n60\nAccuracy (%)\nLLM for DDx only\nClinician Assisted by LLM for DDx\nClinician Assisted by Search\nClinician Unassisted\nFigure 5 | Top-n Accuracy. (left) The percentage of DDx lists with the final diagnosis through human evaluation. (right) The\npercentage of DDx lists with the final diagnosis through automated evaluation.\n|16\nLLM for DDx Arm\nAfter\nAssistance\nBefore\nAssistance\nNot in DDx List\nIn DDx List\nSearch Arm\nAfter\nAssistance\nBefore\nAssistance\n(a)\n(b)\nN = 94\nN = 83\nN = 73\nN = 135\nN = 11\nN = 208\nN = 146\nN = 156\nN = 109\nN = 97\nN = 37\nN = 156\nN = 12\nN = 193\nN = 168\nN = 134\nFigure 6 | Sankey diagram showing effect of assistance. (a) in the LLM arm, 73 cases had the final diagnosis in the DDx\nlist after assistance that did not contain it before, (b) in the Search arm this was 37 cases. In both cases, a small minority (LLM\nfor DDx arm = 11, Search arm = 12) a DDx list with the final diagnosis before assistance did not contain it afterwards.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0\n10\n20\n30\n40\n50\n60\n70\n80\nAccuracy (%)\nLLM for DDx\nGPT-4\nFigure 7 | Top-n Accuracy. Comparison of the percentage of DDx lists with the final diagnosis for our LLM for DDx vs\nGPT-4 for 70 cases.\n|17\n"
  },
  {
    "title": "X-Dreamer: Creating High-quality 3D Content by Bridging the Domain Gap Between Text-to-2D and Text-to-3D Generation",
    "link": "https://arxiv.org/pdf/2312.00085.pdf",
    "upvote": "6",
    "text": "X-Dreamer: Creating High-quality 3D Content by Bridging the Domain Gap\nBetween Text-to-2D and Text-to-3D Generation\nYiwei Ma1\u2021\nYijun Fan1\u2021\nJiayi Ji1\nHaowei Wang1\nXiaoshuai Sun1 *\nGuannan Jiang2\nAnnan Shu2\nRongrong Ji1\n1Key Laboratory of Multimedia Trusted Perception and Efficient Computing,\nMinistry of Education of China, Xiamen University, 361005, P.R. China.\n2Contemporary Amperex Technology Co., Limited (CATL), Fujian, China\nAbstract\nIn recent times, automatic text-to-3D content creation\nhas made significant progress, driven by the development of\npretrained 2D diffusion models. Existing text-to-3D meth-\nods typically optimize the 3D representation to ensure that\nthe rendered image aligns well with the given text, as eval-\nuated by the pretrained 2D diffusion model. Nevertheless,\na substantial domain gap exists between 2D images and 3D\nassets, primarily attributed to variations in camera-related\nattributes and the exclusive presence of foreground objects.\nConsequently, employing 2D diffusion models directly for\noptimizing 3D representations may lead to suboptimal out-\ncomes.\nTo address this issue, we present X-Dreamer, a\nnovel approach for high-quality text-to-3D content creation\nthat effectively bridges the gap between text-to-2D and text-\nto-3D synthesis. The key components of X-Dreamer are two\ninnovative designs: Camera-Guided Low-Rank Adaptation\n(CG-LoRA) and Attention-Mask Alignment (AMA) Loss.\nCG-LoRA dynamically incorporates camera information\ninto the pretrained diffusion models by employing camera-\ndependent generation for trainable parameters. This inte-\ngration enhances the alignment between the generated 3D\nassets and the camera\u2019s perspective. AMA loss guides the\nattention map of the pretrained diffusion model using the\nbinary mask of the 3D object, prioritizing the creation of\nthe foreground object. This module ensures that the model\nfocuses on generating accurate and detailed foreground\nobjects. Extensive evaluations demonstrate the effective-\nness of our proposed method compared to existing text-to-\n3D approaches. Our project webpage: https://xmu-\nxiaoma666.github.io/Projects/X-Dreamer.\n*Corresponding author; \u2021Equal contributions.\nText-to-2D generation\nText-to-3D generation\nConsider Camera \nParameters\nConcentrate on the \n3D Asset\nFocus on the \nEntire Image\nCamera-\nindependent\nCamera-\ndependent\nFocus on the \nForeground Object\nFigure 1. The outputs of the text-to-2D generation model (left)\nand the text-to-3D generation model (right) under the same text\nprompt, i.e., \u201cA statue of Leonardo DiCaprio\u2019s head\u201d.\n1. Introduction\nThe field of text-to-3D synthesis, which seeks to gener-\nate superior 3D content predicated on input textual descrip-\ntions, has shown significant potential to impact a diverse\nrange of applications. These applications extend beyond\ntraditional areas such as architecture, animation, and gam-\ning, and encompass contemporary domains like virtual and\naugmented reality.\nIn recent years, extensive research has demonstrated sig-\nnificant performance improvement in the text-to-2D gen-\neration task [2, 24\u201326] by leveraging pretrained diffusion\nmodels [6, 30, 31] on a large-scale text-image dataset [27].\nBuilding on these advancements, DreamFusion [22] intro-\nduces an effective approach that utilizes a pretrained 2D\ndiffusion model [26] to autonomously generate 3D assets\nfrom text, eliminating the need for a dedicated 3D asset\ndataset. A key innovation introduced by DreamFusion is\nthe Score Distillation Sampling (SDS) algorithm. This al-\ngorithm aims to optimize a single 3D representation, such as\narXiv:2312.00085v2  [cs.CV]  25 Dec 2023\nNeRF [18], to ensure that rendered images from any cam-\nera perspective maintain a high likelihood with the given\ntext, as evaluated by the pretrained 2D diffusion model. In-\nspired by the groundbreaking SDS algorithm, several recent\nworks [4, 13, 16, 35, 36] have emerged, envisioning the text-\nto-3D generation task through the application of pretrained\n2D diffusion models.\nWhile text-to-3D generation has made significant strides\nthrough the utilization of pretrained text-to-2D diffusion\nmodels, it is crucial to recognize and address the persistent\nand substantial domain gap that remains between text-to-2D\nand text-to-3D generation. This distinction is clearly illus-\ntrated in Fig. 1. To begin with, the text-to-2D model pro-\nduces camera-independent generation results, focusing on\ngenerating high-quality images from specific angles while\ndisregarding other angles. In contrast, 3D content creation\nis intricately tied to camera parameters such as position,\nshooting angle, and field of view. As a result, a text-to-3D\nmodel must generate high-quality results across all possible\ncamera parameters. This fundamental difference empha-\nsizes the necessity for innovative approaches that enable the\npretrained diffusion model to consider camera parameters.\nFurthermore, a text-to-2D generation model must simulta-\nneously generate both foreground and background elements\nwhile maintaining the overall coherence of the image. Con-\nversely, a text-to-3D generation model only needs to con-\ncentrate on creating the foreground object. This distinction\nallows text-to-3D models to allocate more resources and at-\ntention to precisely represent and generate the foreground\nobject. Consequently, the domain gap between text-to-2D\nand text-to-3D generation poses a significant performance\nobstacle when directly employing pretrained 2D diffusion\nmodels for 3D asset creation.\nIn this study, we present a pioneering framework, X-\nDreamer, designed to address the domain gap between text-\nto-2D and text-to-3D generation, thereby facilitating the\ncreation of high-quality text-to-3D content. Our framework\nincorporates two innovative designs that are specifically tai-\nlored to address the aforementioned challenges. Firstly, ex-\nisting approaches [4, 13, 16, 35] commonly employ 2D pre-\ntrained diffusion models [25, 26] for text-to-3D generation,\nwhich lack inherent linkage to camera parameters. To ad-\ndress this limitation and ensure that our text-to-3D model\nproduces results that are directly influenced by camera pa-\nrameters, we introduce Camera-Guided Low-Rank Adap-\ntation (CG-LoRA) to fine-tune the pretrained 2D diffusion\nmodel. Notably, the parameters of CG-LoRA are dynam-\nically generated based on the camera information during\neach iteration, establishing a robust relationship between\nthe text-to-3D model and camera parameters. Furthermore,\npretrained text-to-2D diffusion models allocate attention to\nboth foreground and background generation, whereas the\ncreation of 3D assets necessitates a stronger focus on ac-\ncurately generating foreground objects. To address this re-\nquirement, we introduce Attention-Mask Alignment (AMA)\nLoss, which leverages the rendered binary mask of the 3D\nobject to guide the attention map of the pretrained 2D sta-\nble diffusion model [25]. By incorporating this module, X-\nDreamer prioritizes the generation of foreground objects,\nresulting in a significant enhancement of the overall quality\nof the generated 3D content.\nWe present a compelling demonstration of the effective-\nness of X-Dreamer in synthesizing high-quality 3D assets\nbased on textual cues.\nBy incorporating CG-LoRA and\nAMA loss to address the domain gap between text-to-2D\nand text-to-3D generation, our proposed framework ex-\nhibits substantial advancements over prior methods in text-\nto-3D generation. In summary, our study contributes to the\nfield in three key aspects:\n\u2022 We propose a novel method, X-Dreamer, for high-quality\ntext-to-3D content creation, effectively bridging the do-\nmain gap between text-to-2D and text-to-3D generation.\n\u2022 To enhance the alignment between the generated re-\nsults and the camera perspective, we propose CG-LoRA,\nwhich leverages camera information to dynamically gen-\nerate specific parameters for 2D diffusion models.\n\u2022 To prioritize the creation of foreground objects in the text-\nto-3D model, we introduce AMA loss, which utilizes bi-\nnary masks of the foreground 3D object to guide the at-\ntention maps of the 2D diffusion model.\n2. Related Work\n2.1. Text-to-3D Content Creation\nIn recent years, there has been a significant surge in\ninterest surrounding the evolution of text-to-3D genera-\ntion [12, 17, 22]. This growing field has been propelled,\nin part, by advancements in pretrained vision-and-language\nmodels, such as CLIP [23], as well as diffusion models\nlike Stable Diffusion [25] and Imagen [26]. Contemporary\ntext-to-3D models can generally be classified into two dis-\ntinct categories: the CLIP-based text-to-3D approach and\nthe diffusion-based text-to-3D approach. The CLIP-based\ntext-to-3D approach [9, 11, 14, 17, 19, 37] employs CLIP\nencoders [23] to project textual descriptions and rendered\nimages derived from the 3D object into a modal-shared fea-\nture space. Subsequently, CLIP loss is harnessed to align\nfeatures from both modalities, optimizing the 3D represen-\ntation to conform to the textual description. Various schol-\nars have made significant contributions to this field. For in-\nstance, Michel et al. [17] are pioneers in proposing the use\nof CLIP loss to harmonize the text prompt with the rendered\nimages of the 3D object, thereby enhancing text-to-3D gen-\neration. Ma et al. [14] introduce dynamic textual guidance\nduring 3D object synthesis to improve convergence speed\nand generation performance. However, these approaches\nDMTET\nRender Image\nGuided Mesh\nDMTET\nDMTET\n\"An ice cream \nsundae, ... view.\" \nRender Mask\nGeometry Initialization\nGeometry Modeling\nStage1 :\nGeometry \nLearning\nAttention \nMap\nMesh\nStage2 : \nAppearance \nLearning\n\"An ice cream \nsundae, ... view.\" \nAttention \nMap\nPBR material\nCG-\nLoRA\nDifussion\nU-Net\nCG-\nLoRA\nRender\nImage\nRender Mask\nUpdate\nUpdate\nDifussion\nU-Net\nCamera Parameters\nCamera \nParameters\nText-to-3D \nResult\nFigure 2. Overview of the proposed X-Dreamer, which consists of geometry learning and appearance learning.\nhave inherent limitations, as they tend to generate 3D rep-\nresentations with a scarcity of geometry and appearance\ndetail. To overcome this shortcoming, the diffusion-based\ntext-to-3D approach [4, 8, 10, 13, 22, 32] leverages pre-\ntrained text-to-2D diffusion models [25, 26] to guide the\noptimization of 3D representations. Central to these models\nis the application of SDS loss [22] to align the rendered im-\nages stemming from a variety of camera perspectives with\nthe textual description. Specifically, given the target text\nprompt, Lin et al. [13] leverage a coarse-to-fine pipeline to\ngenerate high-resolution 3D content. Chen et al. [4] decou-\nple geometry modeling and appearance modeling to gen-\nerate realistic 3D assets. For specific purposes, some re-\nsearchers [28, 36] integrate trainable LoRA [7] branches\ninto pretrained diffusion models.\nFor instance, Seo et\nal. [28] put forth 3DFuse, a model that harnesses the power\nof LoRA to comprehend object semantics. Wang et al. [36]\nintroduce ProlificDreamer, where the role of LoRA is to\nevaluate the score of the variational distribution for 3D pa-\nrameters. However, the LoRA parameter begins its journey\nfrom random initialization and maintains its independence\nfrom the camera and text. To address these limitations, we\npresent two innovative modules: CG-LoRA and AMA loss.\nThese modules are designed to enhance the model\u2019s ability\nto consider important camera parameters and prioritize the\ngeneration of foreground objects throughout the text-to-3D\ncreation process.\n2.2. Low-Rank Adaptation (LoRA)\nLow-Rank Adaptation (LoRA) is a technique used to re-\nduce memory requirements when fine-tuning a large model.\nIt involves injecting only a small set of trainable parameters\ninto the pretrained model, while keeping the original param-\neters fixed. During the optimization process, gradients are\npassed through the fixed pretrained model weights to the\nLoRA adapter, which is then updated to optimize the loss\nfunction. LoRA has been applied in various fields, includ-\ning natural language processing [3, 7], image synthesis [38]\nand 3D generation [28, 36].\nTo achieve low-rank adap-\ntation, a linear projection with a pretrained weight matrix\nW0 \u2208 Rdin\u00d7dout is augmented with an additional low-rank\nfactorized projection. This augmentation is represented as\nW0 + \u2206W = W0 + AB, where A \u2208 Rdin\u00d7r, B \u2208\nRr\u00d7dout, and r \u226a min(din, dout). During training, W0\nremains fixed, while A and B are trainable. The modified\nforward pass, given the original forward pass Y = XW0,\ncan be formulated as follows:\nY = XW0 + XAB.\n(1)\nIn this paper, we introduce CG-LoRA, which involves the\ndynamic generation of trainable parameters for A based on\ncamera information. This technique allows for integrating\nperspective information, including camera parameters and\ndirection-aware descriptions, into the pretrained text-to-2D\ndiffusion model. As a result, our method significantly en-\nhances text-to-3D generation capabilities.\nC\n+\nCamera Parameters\n+ Element-wise Addition\nC Concatenation\nAn ice cream sundae, \nfront/side/back view.\nDirection-aware Text\nFigure 3. Illustration of Camera-Guided Low-Rank Adaptation.\n3. Approach\n3.1. Architecture\nIn this section, we present a comprehensive introduction\nto the proposed X-Dreamer, which consists of two main\nstages: geometry learning and appearance learning. For ge-\nometry learning, we employ DMTET [29] as the 3D repre-\nsentation. DMTET is an MLP parameterized with \u03a6dmt and\nis initialized with a 3D ellipsoid using the mean squared er-\nror (MSE) loss LMSE. Subsequently, we optimize DMTET\nand CG-LoRA using the SDS loss [22] LSDS and the\nproposed AMA loss LAMA to ensure the alignment be-\ntween the 3D representation and the input text prompt. For\nappearance learning, we leverage bidirectional reflectance\ndistribution function (BRDF) modeling [33] following the\nprevious approach [4].\nSpecifically, we utilize an MLP\nwith trainable parameters \u03a6mat to predict surface materials.\nSimilar to the geometry learning stage, we optimize \u03a6mat\nand CG-LoRA using the SDS loss LSDS and the AMA loss\nLAMA to achieve alignment between the 3D representation\nand the text prompt. Fig. 2 provides a detailed depiction of\nour proposed X-Dreamer.\nGeometry Learning. For geometry learning, an MLP net-\nwork \u03a6dmt is utilized to parameterize DMTET as a 3D rep-\nresentation. To enhance the stability of geometry model-\ning, we employ a 3D ellipsoid as the initial configuration\nfor DMTET \u03a6dmt. For each vertex vi \u2208 VT belonging to\nthe tetrahedral grid T, we train \u03a6dmt to predict two impor-\ntant values: the SDF value s(vi) and the deformation offset\n\u03b4(vi). To initialize \u03a6dmt with the 3D ellipsoid, we sample a\nset of N points {pi \u2208 R3}|N\ni=1 approximately distributed on\nthe surface of an ellipsoid and compute the corresponding\nSDF values {SDF(pi)}|N\ni=1. Subsequently, we optimize\n\u03a6dmt using MSE loss. This optimization process ensures\nthat \u03a6dmt effectively initializes DMTET to resemble the 3D\nellipsoid. The formulation of the MSE loss is given by:\nLMSE = 1\nN\nN\nX\ni=1\n(s(pi; \u03a6dmt) \u2212 SDF(pi))2 .\n(2)\nAfter initializing the geometry, our objective is to align the\ngeometry of DMTET with the input text prompt. Specifi-\ncally, we generate the normal map n and the object mask m\nfrom the initialized DMTET \u03a6dmt by employing a differen-\ntiable rendering technique [33], given a randomly sampled\ncamera pose c. Subsequently, we input the normal map n\ninto the frozen stable diffusion (SD) with a trainable CG-\nLoRA and update \u03a6dmt using the SDS loss, which is de-\nfined as follows:\n\u2207\u03a6dmtLSDS = Et,\u03f5\n\u0014\nw(t) (\u02c6\u03f5\u0398(nt; y, t) \u2212 \u03f5)\n\u2202n\n\u2202\u03a6dmt\n\u0015\n,\n(3)\nwhere \u0398 represents the parameter of SD, \u02c6\u03f5\u0398(nt; y, t) de-\nnotes the predicted noise of SD given the noise level t and\ntext embedding y. Additionally, nt = \u03b1tn + \u03c3t\u03f5, where\n\u03f5 \u223c N(0, I) represents noise sampled from a normal dis-\ntribution. The implementation of w(t), \u03b1t, and \u03c3t is based\non the DreamFusion [22].\nFurthermore, to focus SD on generating foreground ob-\njects, we introduce an additional AMA loss to align the ob-\nject mask m with the attention map of SD, given by:\nLAMA = 1\nL\nL\nX\ni=1\n|ai \u2212 \u03b7(m)|,\n(4)\nwhere L denotes the number of attention layers, and ai is\nthe attention map of i-th attention layer. The function \u03b7(\u00b7) is\nemployed to resize the rendered mask, ensuring its dimen-\nsions align with those of the attention maps.\nAppearance Learning. After obtaining the geometry of\nthe 3D object, our objective is to compute its appear-\nance using the Physically-Based Rendering (PBR) material\nmodel [15]. The material model comprises the diffuse term\nkd \u2208 R3, the roughness and metallic term krm \u2208 R2, and\nthe normal variation term kn \u2208 R3. For any point p \u2208 R3\non the surface of the geometry, we utilize an MLP param-\neterized by \u03a6mat to obtain the three material terms, which\ncan be expressed as follows:\n(kd, kn, krm) = MLP (P(p); \u03a6mat) ,\n(5)\nwhere P(\u00b7) represents the positional encoding using a hash-\ngrid technique [20]. Subsequently, each pixel of the ren-\ndered image can be computed as follows:\nV (p, \u03c9) =\nZ\n\u2126\nLi(p, \u03c9i)f(p, \u03c9i, \u03c9)(\u03c9i \u00b7 np)d\u03c9i,\n(6)\nwhere V (p, \u03c9) denotes the rendered pixel value from the di-\nrection \u03c9 for the surface point p. \u2126 denotes a hemisphere\ndefined by the set of incident directions \u03c9i satisfying the\ncondition \u03c9i \u00b7 np \u2265 0, where \u03c9i denotes the incident direc-\ntion, and np represents the surface normal at point p. Li(\u00b7)\ncorresponds to the incident light from an off-the-shelf en-\nvironment map, and f(\u00b7) is the Bidirectional Reflectance\nDistribution Function (BRDF) related to the material prop-\nerties (i.e., kd, kn, krm). By aggregating all rendered pixel\ncolors, we obtain a rendered image x = {V (p, \u03c9)}. Similar\nto the geometry modeling stage, we feed the rendered image\nx into SD. The optimization objective remains the same as\nEq. (3) and Eq. (4), where the rendered normal map n and\nthe parameters of DMTET \u03a6det are replaced with the ren-\ndered image x and the parameters of the material encoder\n\u03a6mat, respectively.\n3.2. Camera-Guided Low-Rank Adaptation\nThe domain gap between text-to-2D and text-to-3D gen-\neration presents a significant challenge, as discussed in\nSec. 1.\nIt has been observed that directly utilizing pre-\ntrained SD for text-to-3D generation can result in certain\nissues, such as the Janus Problem [1, 12]. To address these\nissues, we propose Camera-Guided Low-Rank Adaptation\n(CG-LoRA) as a solution to bridge the domain gap. As\ndepicted in Fig. 20, we leverage camera parameters and\ndirection-aware text to guide the generation of parameters\nin CG-LoRA, enabling X-Dreamer to effectively incorpo-\nrate camera perspective and direction information.\nSpecifically, given a text prompt T and camera parame-\nters C = {x, y, z, \u03d5yaw, \u03d5pit, \u03b8fov} 1, we initially project\nthese inputs into a feature space using the pretrained textual\nCLIP encoder Etxt(\u00b7) and a trainable MLP Epos(\u00b7):\nt = Etxt(T),\n(7)\nc = Epos(C),\n(8)\nwhere t \u2208 Rdtxt and c \u2208 Rdcam are textual features and\ncamera features. Subsequently, we employ two low-rank\nmatrices to project t and c into trainable dimensionality-\nreduction matrices within CG-LoRA:\nAtxt = Reshape(tW txt),\n(9)\nAcam = Reshape(cW cam),\n(10)\nwhere Atxt\n\u2208\nRd\u00d7 r\n2 and Acam\n\u2208\nRd\u00d7 r\n2 are two\ndimensionality-reduction matrices of CG-LoRA. The func-\ntion Reshape(\u00b7) is used to transform the shape of a ten-\nsor from Rd\u2217 r\n2 to Rd\u00d7 r\n2 .\n2\nW txt \u2208 Rdtxt\u00d7(d\u2217 r\n2) and\nW cam \u2208 Rdcam\u00d7(d\u2217 r\n2) are two low-rank matrices. Thus,\n1The variables x, y, z, \u03d5yaw, \u03d5pit, \u03b8fov represent the x, y, z coordi-\nnates, yaw angle, pitch angle of the camera, and field of view, respectively.\nThe roll angle \u03d5roll is intentionally set to 0 to ensure the stability of the\nobject in the rendered image.\n2Rd\u2217 r\n2 denotes a one-dimensional vector. Rd\u00d7 r\n2 represents a two-\ndimensional matrix.\nwe decompose them into the product of two matrices to re-\nduce the trainable parameters in our implementation, i.e.,\nW txt = U txtV txt and W cam = U camV cam, where\nU txt \u2208 Rdtxt\u00d7r\u2032, V txt \u2208 Rr\u2032\u00d7(d\u2217 r\n2 ), U cam \u2208 Rdcam\u00d7r\u2032,\nV cam \u2208 Rr\u2032\u00d7(d\u2217 r\n2 ), r\u2032 is a small number (i.e., 4). In ac-\ncordance with LoRA [7], we initialize the dimensionality-\nexpansion matrix B \u2208 Rr\u00d7d with zero values to ensure that\nthe model begins training from the pretrained parameters of\nSD. Thus, the feed-forward process of CG-LoRA is formu-\nlated as follows:\ny = xW + [xAtxt; xAcam]B,\n(11)\nwhere W \u2208 Rd\u00d7d represents the frozen parameters of the\npretrained SD model, and [\u00b7; \u00b7] is the concatenation opera-\ntion alone the channel dimension. In our implementation,\nwe integrate CG-LoRA into the linear embedding layers of\nthe attention modules in SD to effectively capture direction\nand camera information.\n3.3. Attention-Mask Alignment Loss\nAlthough SD is pretrained to generate 2D images that\nencompass both foreground and background elements, the\ntask of text-to-3D generation demands a stronger focus\non generating foreground objects.\nTo address this spe-\ncific requirement, we introduce Attention-Mask Alignment\n(AMA) Loss, which aims to align the attention map of SD\nwith the rendered mask image of the 3D object. Specif-\nically, for each attention layer in the pretrained SD, we\ncompute the attention map between the query image fea-\nture Q \u2208 RH\u00d7h\u00d7w\u00d7 d\nH and the key CLS token feature\nK \u2208 RH\u00d7 d\nH . The calculation is formulated as follows:\n\u00afa = Softmax(QK\u22a4\n\u221a\nd\n),\n(12)\nwhere H denotes the number of attention heads in SD, and\n\u00afa \u2208 RH\u00d7h\u00d7w represents the attention map. Subsequently,\nwe proceed to compute the overall attention map \u02c6a \u2208 Rh\u00d7w\nby averaging the attention values of \u00afa across all attention\nheads. Since the attention map values are normalized using\nthe softmax function, the activation values in the attention\nmap may become very small when the image feature reso-\nlution is high. However, considering that each element in\nthe rendered mask has a binary value of either 0 or 1, di-\nrectly aligning the attention map with the rendered mask is\nnot optimal. To address this, we propose a normalization\ntechnique that maps the values in the attention map from 0\nto 1. This normalization process is formulated as follows:\na =\n\u02c6a \u2212 min(\u02c6a)\nmax(\u02c6a) \u2212 min(\u02c6a) + \u03bd ,\n(13)\nwhere \u03bd represents a small constant value (e.g., 1e-6) that\nprevents division by zero in the denominator. Finally, we\nA sliced loaf of fresh bread.\nA rocket.\nA cabbage, highly detailed.\nA plate piled high with \nchocolate chip cookies.\nA strawberry.\nA chocolate cupcake, highly \ndetailed.\nA DSLR photo of a brown \ncowboy hat.\nA hamburger.\nFigure 4. Text-to-3D generation results from an ellipsoid.\nBarack Obama\u2019s head.\nA beautifully carved \nwooden queen chess \npiece.\nA beautifully carved wooden queen \nchess piece.\nA corgi, highly detailed.\nFigure 5. Text-to-3D generation results from coarse-grained guided meshes.\nalign the attention maps of all attention layers with the ren-\ndered mask of the 3D object using the AMA loss. The for-\nmulation of this alignment is presented in Eq. (4).\n4. Experiments\n4.1. Implementation Details.\nWe conduct the experiments using four Nvidia RTX 3090\nGPUs and the PyTorch library [21]. To calculate the SDS\nloss, we utilize the Stable Diffusion implemented by Hug-\ngingFace Diffusers [34]. For the DMTET \u03a6dmt and mate-\nrial encoder \u03a6mat, we implement them as a two-layer MLP\nand a single-layer MLP, respectively, with a hidden dimen-\nsion of 32. The values of dcam, dtxt, r, r\u2032, the batch size,\nthe SDS loss weight, the AMA loss weight, and the aspect\nratio of the perspective projection plane are set to 1024,\n1024, 4, 4, 4, 1, 0.1, and 1 respectively. We optimize X-\nDreamer for 2000 iterations for geometry learning and 1000\niterations for appearance learning. For each iteration, \u03d5pit,\n\u03d5yaw, and \u03b8fov are randomly sampled from (\u221215\u25e6, 45\u25e6),\n(\u2212180\u25e6, 180\u25e6), and (25\u25e6, 45\u25e6), respectively.\n4.2. Results of X-Dreamer\nText-to-3D generation from an ellipsoid. We present rep-\nresentative results of X-Dreamer for text-to-3D generation,\nutilizing an ellipsoid as the initial geometry, as shown in\nFig. 4. The results demonstrate the ability of X-Dreamer to\ngenerate high-quality and photo-realistic outputs that accu-\nrately correspond to the input text prompts.\nText-to-3D generation from coarse-grained meshes.\nWhile there is a wide availability of coarse-grained meshes\nfor download from the internet, directly utilizing these\nmeshes for 3D content creation often results in poor perfor-\nmance due to the lack of geometric details. However, when\ncompared to a 3D ellipsoid, these meshes may provide bet-\nter 3D shape prior information for X-Dreamer. Hence, in-\nProlificDreamer\nX-Dreamer\nDreamFusion\nMagic3D\nFantasia3D\nPrompt: An onion, highly detailed.\nPrompt: A statue of Leonardo DiCaprio's head.\nPrompt: A pumpkin,  highly detailed, 8K, HD.\nFigure 6. Comparison with State-of-the-Art (SOTA) methods. Our method yields results that exhibit enhanced fidelity and more details.\nstead of using ellipsoids, we can initialize DMTET with\ncoarse-grained guided meshes as well. As shown in Fig. 5,\nX-Dreamer can generate 3D assets with precise geomet-\nric details based on the given text, even when the provided\ncoarse-grained mesh lacks details. For instance, in the last\ncolumn of Fig. 5, X-Dreamer accurately transforms the ge-\nometry from a cow to a corgi based on the text prompt \u201cA\ncorgi, highly detailed.\u201d Therefore, X-Dreamer is also an ex-\nceptionally powerful tool for editing coarse-grained mesh\ngeometry using textual inputs.\nQualitative Comparison. To assess the effectiveness of X-\nDreamer, we compare it with four SOTA methods: Dream-\nFusion [22], Magic3D [13], Fantasia3D [4], and Prolific-\nDreamer [36], as depicted in Fig. 6. When compared to the\nSDS-based methods [13, 22, 36], X-Dreamer outperforms\nthem in generating superior-quality and realistic 3D assets.\nIn addition, when compared to the VSD-based method [36],\nX-Dreamer produces 3D content with comparable or even\nbetter visual effects, while requiring significantly less op-\ntimization time. Specifically, the geometry and appearance\nlearning process of X-Dreamer requires only approximately\n27 minutes, whereas ProlificDreamer exceeds 8 hours.\n4.3. Ablation Study\nAblation on the proposed modules. To gain insights into\nthe abilities of CG-LoRA and AMA loss, we perform abla-\ntion studies wherein each module is incorporated individu-\nally to assess its impact. As depicted in Fig. 7, the ablation\nresults demonstrate a notable decline in the geometry and\nappearance quality of the generated 3D objects when CG-\nLoRA is excluded from X-Dreamer. For instance, as shown\nin the second row of Fig. 7, the generated Batman lacks an\near on the top of its head in the absence of CG-LoRA. This\nobservation highlights the crucial role of CG-LoRA in in-\njecting camera-relevant information into the model, thereby\nenhancing the 3D consistency. Furthermore, the omission\nof AMA loss from X-Dreamer also has a deleterious effect\non the geometry and appearance fidelity of the generated 3D\nassets. Specifically, as illustrated in the first row of Fig. 7,\nX-Dreamer successfully generates a photorealistic texture\nfor the rocket, whereas the texture quality noticeably de-\nteriorates in the absence of AMA loss. This disparity can\nbe attributed to AMA loss, which directs the focus of the\nmodel towards foreground object generation, ensuring the\nrealistic representation of both geometry and appearance of\nforeground objects. These ablation studies provide valu-\nable insights into the individual contributions of CG-LoRA\nand AMA loss in enhancing the geometry, appearance, and\noverall quality of the generated 3D objects.\nAttention map comparisons w/ and w/o AMA loss. AMA\nloss is introduced with the aim of guiding attention during\nthe denoising process towards the foreground object. This\nobjective is achieved by aligning the attention map of SD\nwith the rendered mask of the 3D object. To evaluate the\neffectiveness of AMA loss in accomplishing this goal, we\nvisualize the attention maps of SD with and without AMA\nw/o AMA Loss\nw/o CG-LoRA\nX-Dreamer\nPrompt: A 3D rendering of Batman, highly detailed.\nPrompt: A DSLR photo of Lord Voldemort's head, highly detailed.\nPrompt: A rocket. \nFigure 7. Ablation studies of the proposed X-Dreamer.\nAttention Map\nRendered Mask\nRendered Image\nGeometry Learning\nAppearance Learning\nw/ AMA Loss\nw/o AMA Loss\nAttention Map\nRendered Mask\nRendered Image\nPrompt:  A DSLR photo of a cat, highly detailed.\nFigure 8. Visualization of Attention Map, Rendered Mask, and Rendered Image with and without AMA Loss. For clarity, we only visualize\nthe attention map of the first attention layer in SD.\nloss at both the geometry learning and appearance learning\nstages. As depicted in Fig. 8, it can be observed that incor-\nporating AMA loss not only results in improved geometry\nand appearance of the generated 3D asset, but also concen-\ntrates the attention of SD specifically on the foreground ob-\nject area. The visualizations confirm the efficacy of AMA\nloss in directing the attention of SD, resulting in improved\nquality and foreground object focus during geometry and\nappearance learning stages.\n5. Conclusion\nThis study introduces a groundbreaking framework\ncalled X-Dreamer, which is designed to enhance text-to-3D\nsynthesis by addressing the domain gap between text-to-2D\nand text-to-3D generation. To achieve this, we first pro-\npose CG-LoRA, a module that incorporates 3D-associated\ninformation, including direction-aware text and camera pa-\nrameters, into the pretrained Stable Diffusion (SD) model.\nBy doing so, we enable the effective capture of information\nrelevant to the 3D domain. Furthermore, we design AMA\nloss to align the attention map generated by SD with the\nrendered mask of the 3D object. The primary objective of\nAMA loss is to guide the focus of the text-to-3D model to-\nwards the generation of foreground objects. Through exten-\nsive experiments, we have thoroughly evaluated the efficacy\nof our proposed method, which has consistently demon-\nstrated its ability to synthesize high-quality and photoreal-\nistic 3D content from given text prompts.\nReferences\n[1] Mohammadreza\nArmandpour,\nHuangjie\nZheng,\nAli\nSadeghian, Amir Sadeghian, and Mingyuan Zhou.\nRe-\nimagine the negative prompt algorithm:\nTransform 2d\ndiffusion into 3d, alleviate janus problem and beyond. arXiv\npreprint arXiv:2304.04968, 2023. 5, 11\n[2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,\nJiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,\nSamuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image\ndiffusion models with an ensemble of expert denoisers. arXiv\npreprint arXiv:2211.01324, 2022. 1\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. Advances in neural in-\nformation processing systems, 33:1877\u20131901, 2020. 3\n[4] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia.\nFantasia3d:\nDisentangling geometry and appearance for\nhigh-quality text-to-3d content creation.\narXiv preprint\narXiv:2303.13873, 2023. 2, 3, 4, 7, 11\n[5] Yuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian\nLaforte, Vikram Voleti, Guan Luo, Chia-Hao Chen, Zi-\nXin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang.\nthreestudio: A unified framework for 3d content generation.\nhttps://github.com/threestudio-project/\nthreestudio, 2023. 11\n[6] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 1\n[7] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 3, 5\n[8] Yukun Huang, Jianan Wang, Ailing Zeng, He Cao, Xianbiao\nQi, Yukai Shi, Zheng-Jun Zha, and Lei Zhang. Dreamwaltz:\nMake a scene with complex 3d animatable avatars. arXiv\npreprint arXiv:2305.12529, 2023. 3\n[9] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter\nAbbeel, and Ben Poole. Zero-shot text-guided object genera-\ntion with dream fields. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n867\u2013876, 2022. 2\n[10] Ruixiang Jiang, Can Wang, Jingbo Zhang, Menglei Chai,\nMingming He, Dongdong Chen, and Jing Liao.\nAvatar-\ncraft: Transforming text into neural human avatars with\nparameterized shape and pose control.\narXiv preprint\narXiv:2303.17606, 2023. 3\n[11] Jiabao Lei, Yabin Zhang, Kui Jia, et al. Tango: Text-driven\nphotorealistic and robust 3d stylization via lighting decom-\nposition. Advances in Neural Information Processing Sys-\ntems, 35:30923\u201330936, 2022. 2\n[12] Weiyu Li, Rui Chen, Xuelin Chen, and Ping Tan. Sweet-\ndreamer: Aligning geometric priors in 2d diffusion for con-\nsistent text-to-3d. arXiv preprint arXiv:2310.02596, 2023.\n2, 5, 11\n[13] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\ntext-to-3d content creation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 300\u2013309, 2023. 2, 3, 7, 11\n[14] Yiwei Ma, Xiaoqing Zhang, Xiaoshuai Sun, Jiayi Ji, Haowei\nWang, Guannan Jiang, Weilin Zhuang, and Rongrong Ji.\nX-mesh: Towards fast and accurate text-driven 3d styliza-\ntion via dynamic textual guidance.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 2749\u20132760, 2023. 2\n[15] Stephen McAuley, Stephen Hill, Naty Hoffman, Yoshiharu\nGotanda, Brian Smits, Brent Burley, and Adam Martinez.\nPractical physically-based shading in film and game produc-\ntion. In ACM SIGGRAPH 2012 Courses, pages 1\u20137. 2012.\n4\n[16] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and\nDaniel Cohen-Or. Latent-nerf for shape-guided generation\nof 3d shapes and textures. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 12663\u201312673, 2023. 2\n[17] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and\nRana Hanocka. Text2mesh: Text-driven neural stylization\nfor meshes. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 13492\u2013\n13502, 2022. 2\n[18] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. Communications of the ACM, 65(1):99\u2013106, 2021.\n2\n[19] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky,\nand Tiberiu Popa. Clip-mesh: Generating textured meshes\nfrom text using pretrained image-text models. In SIGGRAPH\nAsia 2022 conference papers, pages 1\u20138, 2022. 2\n[20] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a mul-\ntiresolution hash encoding. ACM Transactions on Graphics\n(ToG), 41(4):1\u201315, 2022. 4\n[21] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-\nperative style, high-performance deep learning library. Ad-\nvances in neural information processing systems, 32, 2019.\n6\n[22] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall.\nDreamfusion: Text-to-3d using 2d diffusion.\narXiv\npreprint arXiv:2209.14988, 2022. 1, 2, 3, 4, 7, 11\n[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 2\n[24] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv preprint arXiv:2204.06125, 1\n(2):3, 2022. 1\n[25] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 2, 3\n[26] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022. 1, 2, 3\n[27] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. Advances in Neural In-\nformation Processing Systems, 35:25278\u201325294, 2022. 1\n[28] Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon\nKo, Hyeonsu Kim, Junho Kim, Jin-Hwa Kim, Jiyoung Lee,\nand Seungryong Kim.\nLet 2d diffusion model know 3d-\nconsistency for robust text-to-3d generation. arXiv preprint\narXiv:2303.07937, 2023. 3\n[29] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and\nSanja Fidler.\nDeep marching tetrahedra: a hybrid repre-\nsentation for high-resolution 3d shape synthesis. Advances\nin Neural Information Processing Systems, 34:6087\u20136101,\n2021. 4\n[30] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics.\nIn International confer-\nence on machine learning, pages 2256\u20132265. PMLR, 2015.\n1\n[31] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. arXiv preprint arXiv:2011.13456, 2020. 1\n[32] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi,\nLizhuang Ma, and Dong Chen. Make-it-3d: High-fidelity\n3d creation from a single image with diffusion prior. arXiv\npreprint arXiv:2303.14184, 2023. 3\n[33] Kenneth E Torrance and Ephraim M Sparrow. Theory for\noff-specular reflection from roughened surfaces.\nJosa, 57\n(9):1105\u20131114, 1967. 4\n[34] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro\nCuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj,\nand Thomas Wolf.\nDiffusers:\nState-of-the-art diffusion\nmodels.\nhttps://github.com/huggingface/\ndiffusers, 2022. 6\n[35] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,\nand Greg Shakhnarovich. Score jacobian chaining: Lifting\npretrained 2d diffusion models for 3d generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 12619\u201312629, 2023. 2\n[36] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongx-\nuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity\nand diverse text-to-3d generation with variational score dis-\ntillation. arXiv preprint arXiv:2305.16213, 2023. 2, 3, 7,\n11\n[37] Jiacheng Wei, Hao Wang, Jiashi Feng, Guosheng Lin, and\nKim-Hui Yap.\nTaps3d:\nText-guided 3d textured shape\ngeneration from pseudo supervision.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 16805\u201316815, 2023. 2\n[38] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 3836\u20133847, 2023. 3\nCamera Coordinates\nYaw Angle\nRoll Angle\nField of View \nPitch Angle\nwidth\nheight\nFigure 9. Illustration of camera parameters.\n6. Appendix\n6.1. Camera Parameters\nIn this section, we present a comprehensive elucidation\nof the camera parameters discussed in our paper, namely\nC = {x, y, z, \u03d5yaw, \u03d5pit, \u03d5roll, \u03b8fov}. These parameters\nplay a fundamental role in our research and are crucial for\nunderstanding the underlying principles of our methodol-\nogy. Fig. 9 visually depicts the camera parameters in rela-\ntion to our work. The variables x, y, and z correspond to the\ncamera\u2019s coordinates within the world coordinate system,\nproviding spatial information about its positioning. The ro-\ntation angles \u03d5yaw, \u03d5pit, and \u03d5roll describe the camera\u2019s\norientation along the x, y, and z axes, respectively. By con-\ntrolling these angles, we ensure that objects depicted in the\nrendered images maintain their desired orientation. Specifi-\ncally, to achieve a stable rendering process, we set \u03d5roll to a\nconstant value of 0\u25e6 throughout the training phase. Another\ncritical parameter is \u03b8fov, which denotes the camera\u2019s field\nof view. This parameter determines the angular range cap-\ntured in the rendered image. Moreover, we also consider\nthe aspect ratio, representing the proportional relationship\nbetween the width and height of the rendered area. In our\nimplementation, we set the aspect ratio to 1, ensuring a bal-\nanced rendering output.\n6.2. More Visualization\nIn this section, we present a comprehensive set of gen-\nerated results that offer compelling evidence to substantiate\nthe exceptional capabilities of our proposed X-Dreamer in\nproducing robust and top-tier 3D assets. The additional vi-\nsualization results, available in Fig. 10, Fig. 11, Fig. 12,\nFig. 13, and Fig. 14, not only serve to validate the effec-\ntiveness of our approach but also emphasize its significant\npotential for various applications in the field. These results\nare pivotal in establishing the credibility and superiority of\nour approach, bolstering the confidence in its practicality\nand effectiveness.\n6.3. More Comparison\nIn this section, we present an extensive visualization\ncomparison between our proposed X-Dreamer and four\nprominent baselines in the field, namely DreamFusion [22],\nMagic3D [13], Fantasia3D [4], and ProlificDreamer [36].\nBy incorporating these baselines into our analysis, we aim\nto provide a comprehensive assessment of the efficacy and\nsuperiority of our method. Since the codes of some meth-\nods [13, 22, 36] have not been publicly available, we present\nthe results obtained by implementing these methods in\nthreestudio [5]. As depicted in Fig. 17, our observations\nreveal a striking disparity between the generated results of\nX-Dreamer and those of the baselines. Notably, the out-\nputs produced by our proposed X-Dreamer exhibit a sig-\nnificantly higher degree of realism, unmistakably highlight-\ning the unparalleled effectiveness of our methodology. This\nstark contrast serves as a compelling visual testament to the\nexceptional quality and fidelity achieved by our approach.\nIn Fig. 18, we present a comprehensive comparison of our\nmethods with four baselines, using the images provided in\ntheir original papers. Our results clearly demonstrate the\nsuperiority of our proposed X-Dreamer in terms of real-\nism and optimization time compared to DreamFusion [22],\nMagic3D [13], and Fantasia3D [4]. When compared to Pro-\nlificDreamer [36], our X-Dreamer achieves comparable re-\nsults. It is worth noting that the learning process for geome-\ntry and appearance in X-Dreamer takes only approximately\n27 minutes, whereas ProlificDreamer requires over 8 hours.\nThe remarkable realism attained by X-Dreamer can be\npartly attributed to the incorporation of our novel CG-LoRA\ntechnique, which seamlessly integrates camera information\ninto the text-to-3D model. By leveraging this innovative\nstrategy, X-Dreamer ensures the generation of objects with\nconsistent perspectives, thereby eliminating the prevalent\nissue of inconsistent perspectives, commonly known as the\nJanus Problem [1, 12], encountered by other baselines. This\nsignificant advantage further consolidates the superiority of\nX-Dreamer over its competitors.\nFurthermore, our pro-\nposed AMA loss plays also a pivotal role in enhancing the\ngeneration process.\nBy emphasizing the foreground ob-\nject through the AMA loss, X-Dreamer effectively allocates\nmore attention to crucial details, resulting in the genera-\ntion of photorealistic 3D objects with exceptional geometry\nand appearance intricacies. This attention to detail sets X-\nDreamer apart from other baselines, revealing its capacity\nto produce outputs that surpass existing methods in terms\nof fidelity and visual appeal. By incorporating camera in-\nformation through CG-LoRA and leveraging the AMA loss\nto enhance foreground object generation, X-Dreamer estab-\nlishes itself as a cutting-edge solution capable of generating\nphotorealistic 3D objects with superior geometry and ap-\npearance details. These findings substantiate the efficacy\nand practical relevance of our methodology, thus contribut-\nA marble bust of an angel, 3D model, high resolution.\nA DSLR photo of David Beckham's head, highly detailed, 8K, HD.\nA DSLR photo of the Terracotta Army, 3D model, high resolution.\nA DSLR photo of a corn, highly detailed, 8K, HD.\n1\nFigure 10. More text-to-3D generation results of the proposed X-Dreamer.\ning to the advancement of the field and reinforcing the vital\nrole of X-Dreamer in pushing the boundaries of 3D object\ngeneration.\n6.4. Three PBR Material Terms\nThe Three PBR Material Terms in Physically Based Ren-\ndering (PBR) play a crucial role in describing material ap-\npearance and behavior. Firstly, the Diffuse Term kd \u2208 R3\naccurately represents how the material responds to diffuse\nlighting. This term is captured by a three-dimensional vec-\ntor that denotes the material\u2019s diffuse color. Secondly, the\nRoughness and Metallic Term krm \u2208 R2 effectively cap-\ntures the material\u2019s roughness and metallic properties. It is\nrepresented by a two-dimensional vector that conveys the\nvalues of roughness and metallicness. Roughness quanti-\nfies the surface smoothness, while metallicness indicates\nA DSLR photo of a macaron, 8K, HD, high resolution.\nA DSLR photo of a pear, highly detailed, 8K, HD.\nA DSLR photo of a red apple, highly detailed.\nA DSLR photo of a vase, highly detailed, 8K, HD.\nA parrot, 3d model, highly detailed, 8K, HD.\nFigure 11. More text-to-3D generation results of the proposed X-Dreamer.\nthe presence of metallic properties in the material. These\nterms collectively contribute to the overall visual realism\nof the rendered scene. Additionally, the Normal Variation\nTerm kn \u2208 R3 plays a crucial role in characterizing varia-\nA DSLR photo of a popcorn bucket.\nMessi's head, highly detailed, 8K, HD.\nA rabbit, animated movie character, high detail 3d model.\nA small saguaro cactus planted in a clay pot.\n3\nFigure 12. More text-to-3D generation results of the proposed X-Dreamer.\ntions in surface normals. This term, typically used in con-\njunction with a normal map, enables the simulation of fine\ndetails and textures on the material\u2019s surface. During the\nrendering process, these three PBR material terms interact\nto accurately simulate the optical properties exhibited by\nreal-world materials. This integration facilitates rendering\nengines in achieving high-quality rendering results and gen-\nerating realistic virtual scenes.\nTo demonstrate the effectiveness of these material terms,\nwe present visualizations in Fig. 19.\nThe generated re-\nA DSLR photo of a popcorn bucket.\nA DSLR photo of cheese, high resolution.\nA DSLR photo of a compass, highly detailed, 8K, HD.\nA DSLR photo of a blue and white porcelain vase, highly detailed, 8K, HD.\nA DSLR photo of hot air balloon, highly detailed, 8K, HD, 3D model.\n4\nFigure 13. More text-to-3D generation results of the proposed X-Dreamer.\nsults using X-Dreamer showcase accurate representations\nfor each of the three material terms. When combined, these\nterms collectively contribute to the creation of a realistic\nrendering.\nIn particular, the visualization of the Diffuse\nTerm kd \u2208 R3 exhibits similarities to the rendered im-\nage in certain cases.\nHowever, the rendered image dis-\nplays a higher level of detail compared to the Diffuse Term\nkd \u2208 R3. Zooming in on the images, as shown in the last\nrow of Fig. 19 reveals that the rendered image accurately\ndepicts the details of stomata generated during the bread\n5\nA birthday cake, 3D model, highly detailed.\nA DSLR photo of a fox, highly detailed.\nA DSLR photo of a lotus, highly detailed, 8K, HD.\nA DSLR photo of red rose, highly detailed, 8K, HD.\nA stack of pancakes covered in maple syrup.\nFigure 14. More text-to-3D generation results of the proposed X-Dreamer.\nfermentation process. In contrast, the stomata details in the\nDiffuse Term kd \u2208 R3 appear blurry. Thus, it is evident\nthat these three material terms play a crucial role in gener-\nating realistic photos, as they collectively contribute to the\naccurate representation of material properties and the fine\ndetails observed in the rendered images.\nPrompt: Green grapes, highly detailed, 8K, HD.\n Initialization Shape\nText-to-3D Result\nFigure 15. Text-to-3D results using the same textual prompt with\ndifferent initialization ellipsoids.\n6.5. Ablation Study of CG-LoRA\nIn our implementation, the parameters of CG-LoRA are\ndynamically generated based on two key factors related\nto camera information: camera parameters and direction-\naware text. To thoroughly investigate the impact of these\ncamera information terms on X-Dreamer, we conducted ab-\nlation experiments, dynamically generating CG-LoRA pa-\nrameters based solely on one of these terms. The results, as\nillustrated in Fig. 20, clearly demonstrate that when using\nonly one type of camera information term, the geometry and\nappearance quality of the generated results are significantly\ndiminished compared to those produced by X-Dreamer. For\nexample, as shown in the third row of Fig. 20, when uti-\nlizing the CG-LoRA solely related to direction-aware text,\nthe geometry quality of the generated cookies appears poor.\nSimilarly, as depicted in the last row of Fig. 20, employ-\ning the CG-LoRA solely based on camera parameters re-\nsults in diminished geometry and appearance quality for the\npumpkin. These findings highlight the importance of both\ncamera parameters and direction-aware text in generating\nA DSLR photo of a blue and white porcelain \nvase and an apple, highly detailed, 8K, HD.\nA sliced loaf of fresh bread and a cabbage.\nFigure 16. Limitations of X-Dreamer.\nhigh-quality 3D assets. The successful generation of param-\neters for CG-LoRA relies on the synergy between these two\ncamera information terms, emphasizing their crucial role in\nachieving superior geometry and appearance in the gener-\nated assets.\n6.6. Attention Map Visualization w/ and w/o the\nProposed Modules\nTo gain insights into the effectiveness of the proposed\nmodules on the attention map, we conducted visualizations\nof the attention map of X-Dreamer without CG-LoRA and\nAMA loss, as depicted in Fig. 21 and Fig. 22, respectively.\nOur observations reveal that both proposed modules effec-\ntively guide X-Dreamer to focus on the foreground object,\nenhancing its attention capabilities. In particular, Fig. 21\nand Fig. 22 showcase the attention maps of X-Dreamer dur-\ning the geometry learning stage and the appearance learning\nstage, respectively. From Fig. 21, it is evident that remov-\ning either CG-LoRA or AMA loss from X-Dreamer results\nin less-targeted attention maps. This outcome aligns with\nour expectations, as the AMA loss directly aligns the at-\ntention map with the rendered mask, enabling the model to\nconcentrate on the foreground objects. An intriguing find-\ning is that CG-LoRA also facilitates the model\u2019s ability to\nfocus on foreground objects. This can be attributed to the\nincorporation of camera information in X-Dreamer through\nCG-LoRA, which enhances the model\u2019s perception of the\nposition and shape of foreground objects. Consequently,\nA cat, highly detailed.\nGarlic with white skin, highly detailed,8K,HD.\nA Vintage Style LED Lantern.\nA statue of Leonardo DiCaprio's head.\nAn onion, highly detailed.\nDreamFusion\nMagic3D\nFantasia3D\nProlificDreamer\nOurs\nFigure 17. More comparison with State-of-the-Art (SOTA) methods.\nmore accurate attention maps are generated. Similar obser-\nvations can be made during the appearance modeling stage,\nas depicted in Fig. 22. The attention maps generated by\nX-Dreamer without CG-LoRA or AMA loss demonstrate a\nreduced focus on the foreground object. These findings pro-\nvide strong evidence for the effectiveness of the proposed\nmodules in guiding X-Dreamer to attend to the foreground\nobjects accurately.\nThe inclusion of camera information\nPrompt: A plate piled high with chocolate chip cookies.\nPrompt: A 3D model of an adorable cottage with a thatched roof.\nDreamFusion\nMagic3D\nFantasia3D\nProlificDreamer\nX-Dreamer\nPrompt: A marble bust of a mouse.\nFigure 18. More comparison with State-of-the-Art (SOTA) methods.\nthrough CG-LoRA proves to be a valuable addition, en-\nabling the model to generate attention maps that align with\nthe desired foreground objects throughout the geometry and\nappearance modeling stages.\n6.7. Different Initialization Ellipsoids\nIn this section, our objective is to investigate the impact\nof varying initialization ellipsoids under the same textual\nprompt. As illustrated in Fig. 15, when we refrain from\nspecifying a particular shape in the text prompt and opt\nfor initialization using ellipsoids of diverse shapes, the out-\ncomes of text-to-3D transformation demonstrate a notable\ncompatibility with mesh-related initialization procedures.\nThis observation is inherently logical. When the model un-\ndergoes optimization based on a given mesh, it promptly ac-\nquires a local optimal solution and tends to oscillate within\nits proximity during the optimization process. This behav-\nior makes it inherently challenging for the model to escape\nthis limited range and explore alternative local optimal so-\nlutions.\n6.8. Rendered Images under Different Environment\nMaps\nOur method, while employing a fixed environment for\ntraining the model, offers significant advantages and flexi-\nbility in the rendering of 3D assets generated by X-Dreamer.\nIt is worth noting that these assets are not limited to a single\nenvironment; instead, they possess the capability to be ren-\ndered in diverse environments, expanding their applicability\nand adaptability. The visual evidence presented in Fig. 23\nand Fig. 24 reinforces our claim. By utilizing different en-\nvironment maps, we can readily observe the striking varia-\ntions in the appearance of the rendered images of these 3D\nassets. This compelling demonstration showcases the inher-\nent versatility and potential of X-Dreamer\u2019s generated as-\nsets to seamlessly integrate into various rendering engines.\nThe ability to effectively address the challenges posed by\ndifferent lighting conditions is a crucial aspect of any ren-\ndering engine. By leveraging the capabilities of X-Dreamer,\nwe unlock the possibility of using its 3D assets in a multi-\ntude of rendering engines. This versatility ensures that the\nassets can adapt and thrive in diverse lighting scenarios, en-\nabling them to meet the demands of real-world applications\nin fields such as architecture, virtual reality, and computer\ngraphics.\n6.9. Limitation\nOne limitation of the proposed X-Dreamer is its inability\nto generate multiple objects simultaneously. When the input\ntext prompt contains two distinct objects, X-Dreamer may\nproduce a mixed object that combines properties of both\nobjects. A clearer understanding of this limitation can be\ngained by referring to Fig. 16. In the first row of Fig. 16,\nour intention is for X-Dreamer to generate \u201cA sliced loaf of\nA cabbage, highly detailed.\nA blue tulip.\nA sliced loaf of fresh bread.\nA plate piled high with chocolate chip cookies.\nRendered Image\nFigure 19. Visualization of Three Material Terms.\nfresh bread and a cabbage.\u201d However, the resulting output\nconsists of a bread with a few cabbage leaves, indicating the\nmodel\u2019s difficulty in generating separate objects accurately.\nSimilarly, in the second row of Fig. 16, we aim to generate\nA sliced loaf of fresh bread.\nA cabbage, highly detailed.\nW/ CG-LoRA\n (camera parameters)\nW/ CG-LoRA\n (direction-aware text)\nW/ CG-LoRA\nA plate piled high with chocolate chip cookies.\nA blue tulip.\nA statue of Leonardo DiCaprio's head.\nA pumpkin, highly detailed,8K,HD.\nFigure 20. Ablation Study of CG-LoRA.\nW/O AMA loss\nW/O CG-LoRA\nX-Dreamer\nA 3D rendering of Batman, highly detailed.\nA blue tulip.\nA sliced loaf of fresh bread.\nFigure 21. Attention map comparisons w/ and w/o the proposed modules for geometry learning.\n\u201cA blue and white porcelain vase and an apple.\u201d However,\nthe output depicts a blue and white porcelain vase with a\nshape resembling an apple. Nevertheless, it is important\nto note that this limitation does not significantly undermine\nthe value of our work. In applications such as games and\nmovies, each object functions as an independent entity, ca-\npable of creating complex scenes through interaction with\nother objects. Our primary objective is to generate high-\nquality 3D objects, and while the current limitation exists,\nit does not diminish the overall quality and utility of our\napproach.\nW/O AMA loss\nW/O CG-LoRA\nX-Dreamer\nA 3D rendering of Batman, highly detailed.\nA blue tulip.\nA sliced loaf of fresh bread.\nFigure 22. Attention map comparisons w/ and w/o the proposed modules for appearance learning.\nA 3D rendering of Batman, highly detailed.\nA pumpkin, highly detailed,8K,HD.\nA beautifully carved wooden queen chess piece.\nEnvironment \nMap\nA statue of Leonardo DiCaprio's head.\nA plate piled high with chocolate chip cookies.\nFigure 23. Rendered images of generated 3D assets under different environment maps.\nA cabbage, highly detailed.\n A marble bust of a mouse.\nA DSLR photo of Lord Voldemort's head, highly detailed.\nA DSLR photo of a brown cowboy hat.\nA sliced loaf of fresh bread.\nEnvironment \nMap\nFigure 24. Rendered images of generated 3D assets under different environment maps.\n"
  }
]