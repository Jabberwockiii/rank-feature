[
  {
    "title": "RWKV: Reinventing RNNs for the Transformer Era",
    "link": "https://arxiv.org/pdf/2305.13048.pdf",
    "upvote": "9",
    "text": "RWKV: Reinventing RNNs for the Transformer Era\nBo Peng1,2\u2217 Eric Alcaide2,3,4\u2217 Quentin Anthony2,5\u2217\nAlon Albalak2,6 Samuel Arcadinho2,7 Stella Biderman2,8 Huanqi Cao9 Xin Cheng10\nMichael Chung11 Xingjian Du1 Matteo Grella12 Kranthi Kiran GV2,13 Xuzheng He2\nHaowen Hou14 Jiaju Lin1 Przemys\u0142aw Kazienko15 Jan Koco\u00b4n15 Jiaming Kong16\nBart\u0142omiej Koptyra15 Hayden Lau2 Krishna Sri Ipsit Mantri17 Ferdinand Mom18,19\nAtsushi Saito2,20 Guangyu Song21 Xiangru Tang22 Bolun Wang23 Johan S. Wind24\nStanis\u0142aw Wo\u00b4zniak15 Ruichong Zhang9 Zhenyuan Zhang2 Qihang Zhao25,26\nPeng Zhou23 Qinghua Zhou5 Jian Zhu27 Rui-Jie Zhu28,29\n1Generative AI Commons 2EleutherAI 3U. of Barcelona 4Charm Therapeutics 5Ohio State U. 6U. of C., Santa Barbara\n7Zendesk 8Booz Allen Hamilton 9Tsinghua University 10Peking University 11Storyteller.io 12Crisis24 13New York U.\n14National U. of Singapore 15Wroclaw U. of Science and Technology 16Databaker Technology 17Purdue U. 18Criteo AI Lab\n19Epita 20Nextremer 21Moves 22Yale U. 23RuoxinTech 24U. of Oslo 25U. of Science and Technology of China\n26Kuaishou Technology 27U. of British Columbia 28U. of C., Santa Cruz 29U. of Electronic Science and Technology of China\nAbstract\nTransformers have revolutionized almost all\nnatural language processing (NLP) tasks but\nsuffer from memory and computational com-\nplexity that scales quadratically with sequence\nlength. In contrast, recurrent neural networks\n(RNNs) exhibit linear scaling in memory and\ncomputational requirements but struggle to\nmatch the same performance as Transformers\ndue to limitations in parallelization and scala-\nbility. We propose a novel model architecture,\nReceptance Weighted Key Value (RWKV), that\ncombines the efficient parallelizable training\nof transformers with the efficient inference of\nRNNs.\nOur approach leverages a linear attention mech-\nanism and allows us to formulate the model as\neither a Transformer or an RNN, thus paralleliz-\ning computations during training and maintains\nconstant computational and memory complex-\nity during inference. We scale our models as\nlarge as 14 billion parameters, by far the largest\ndense RNN ever trained, and find RWKV per-\nforms on par with similarly sized Transformers,\nsuggesting future work can leverage this archi-\ntecture to create more efficient models. This\nwork presents a significant step towards rec-\nonciling trade-offs between computational ef-\nficiency and model performance in sequence\nprocessing tasks. 1\n1\nIntroduction\nDeep learning has greatly advanced artificial intelli-\ngence, impacting a range of scientific and industrial\nuses. These often involve complex sequential data\n\u2217 Equal first authorship. Others listed alphabetically.\n1Code at: https://github.com/BlinkDL/RWKV-LM\nprocessing tasks such as natural language under-\nstanding, conversational AI, time-series analysis,\nand indirectly sequential formats like images and\ngraphs (Brown et al., 2020; Ismail Fawaz et al.,\n2019; Wu et al., 2020; Albalak et al., 2022). Pre-\ndominant among these techniques include RNNs\nand Transformers (Vaswani et al., 2017), each with\nspecific benefits and drawbacks. RNNs require less\nmemory, particularly for handling long sequences.\nHowever, they suffer from the vanishing gradient\nproblem and non-parallelizability in the time di-\nmension during training, limiting their scalability\n(Hochreiter, 1998; Le and Zuidema, 2016).\n102\n103\nCompute (exaFLOP)\n40\n45\n50\n55\nAccuracy\nBLOOM\nPythia\nOPT\nRWKV\nFigure 1: Average performance of RWKV models com-\npared to transformers across twelve NLP tasks. For\nfurther details, see section 5.\nTransformers emerged as a powerful alternative,\nadept at managing local and long-range dependen-\ncies and supporting parallelized training (Tay et al.,\n2022). Models such as GPT-3 (Brown et al., 2020),\nChatGPT (OpenAI, 2022; Koco\u00b4n et al., 2023),\narXiv:2305.13048v2  [cs.CL]  11 Dec 2023\nModel\nTime\nSpace\nTransformer\nO(T 2d)\nO(T 2 + Td)\nReformer\nO(T log Td)\nO(T log T + Td)\nPerformer\nO(Td2 log d) O(Td log d + d2 log d)\nLinear Transformers\nO(Td2)\nO(Td + d2)\nAFT-full\nO(T 2d)\nO(Td)\nAFT-local\nO(Tsd)\nO(Td)\nMEGA\nO(cTd)\nO(cd)\nRWKV (ours)\nO(Td)\nO(d)\nTable 1: Inference complexity comparison with different\nTransformers. Here T denotes the sequence length,\nd the feature dimension, c is MEGA\u2019s chunk size of\nquadratic attention, and s is the size of a local window\nfor AFT.\nLLaMA (Touvron et al., 2023), and Chinchilla\n(Hoffmann et al., 2022) showcase the potential of\nTransformers in NLP. However, the self-attention\nmechanism\u2019s quadratic complexity makes it compu-\ntationally and memory intensive for tasks involving\nlong sequences and constrained resources. This\nhas stimulated research to enhance Transformers\u2019\nscalability, sometimes sacrificing some of their ef-\nfectiveness (Wang et al., 2020; Zaheer et al., 2020;\nDao et al., 2022a).\nTo tackle these challenges, we introduce the Re-\nceptance Weighted Key Value (RWKV) model,\ncombining the strengths of RNNs and Transform-\ners while circumventing key drawbacks. RWKV\nalleviates memory bottleneck and quadratic scaling\nassociated with Transformers (Katharopoulos et al.,\n2020) with efficient linear scaling, while maintain-\ning the expressive properties of the Transformer,\nsuch as parallelized training and robust scalability.\nRWKV reformulates the attention mechanism with\na variant of linear attention, replacing traditional\ndot-product token interaction with more effective\nchannel-directed attention. This implementation,\nwithout approximation, offers the lowest computa-\ntional and memory complexity; see Table 1.\nThe motivation behind RWKV is to balance com-\nputational efficiency with expressive capacity in\nneural networks. It offers a solution for handling\nlarge-scale models with billions of parameters, ex-\nhibiting competitive performance at a reduced com-\nputational cost. Experiments suggest RWKV ad-\ndresses scaling and deployment challenges in AI,\nespecially for sequential data processing, pointing\ntowards more sustainable and efficient AI models.\nOur contributions in this paper are as follows:\n\u2022 The introduction of RWKV, a novel architec-\nture combining RNNs and Transformer advan-\ntages while mitigating their limitations.\n\u2022 Detailed experiments demonstrating RWKV\u2019s\nperformance and efficiency on benchmark\ndatasets for large-scale models.\n\u2022 The release of pretrained models, from 169\nmillion to 14 billion parameters, trained on\nthe Pile (Gao et al., 2020; Biderman et al.,\n2022).2\n2\nBackground\nHere we briefly review the fundamentals of RNNs\nand Transformers.\n2.1\nRecurrent Neural Networks (RNNs)\nPopular RNN architectures such as LSTM (Hochre-\niter and Schmidhuber, 1997) and GRU (Chung\net al., 2014) are characterized by the following for-\nmulation (shown for LSTM, others can be reasoned\nsimilarly):\nft = \u03c3g(Wfxt + Ufht\u22121 + bf),\n(1)\nit = \u03c3g(Wixt + Uiht\u22121 + bi),\n(2)\not = \u03c3g(Woxt + Uoht\u22121 + bo),\n(3)\n\u02dcct = \u03c3c(Wcxt + Ucht\u22121 + bc),\n(4)\nct = ft \u2299 ct\u22121 + it \u2299 \u02dcct,\n(5)\nht = ot \u2299 \u03c3h(ct).\n(6)\nAlthough RNNs can be factored into two linear\nblocks (W and U) and an RNN-specific block (1)\u2013\n(6), as noted by Bradbury et al. (2017), the data de-\npendency relying on previous time steps prohibits\nparallelizing these typical RNNs.\n2.2\nTransformers and AFT\nIntroduced by Vaswani et al. (2017), Transformers\nare a class of neural networks that have become\nthe dominant architecture for several NLP tasks.\nInstead of operating on sequences step-by-step like\nRNNs, Transformers rely on attention mechanisms\nto capture relationships between all input and all\noutput tokens:\nAttn(Q, K, V ) = softmax(QK\u22a4)V,\n(7)\nwhere the multi-headness and scaling factor\n1\n\u221adk is\nomitted for convenience. The core QK\u22a4 multipli-\ncation is an ensemble of pairwise attention scores\n2https://huggingface.co/RWKV\nbetween each token in a sequence, which can be\ndecomposed as vector operations:\nAttn(Q, K, V )t =\nPT\ni=1 eq\u22a4\nt ki \u2299 vi\nPT\ni=1 eq\u22a4\nt ki\n.\n(8)\nAFT (Zhai et al., 2021), alternately formulates\nAttn+(W, K, V )t =\nPt\ni=1 ewt,i+ki \u2299 vi\nPt\ni=1 ewt,i+ki\n,\n(9)\nwhere {wt,i} \u2208 RT\u00d7T is the learned pair-wise po-\nsition biases, and each wt,i is a scalar.\nInspired by AFT, RWKV takes a similar ap-\nproach. However, for simplicity, it modifies the\ninteraction weights so that it can be transformed\ninto an RNN. Each wt,i in RWKV is a channel-\nwise time decay vector multiplied by the relative\nposition and traced backward from current time as\nit decays:\nwt,i = \u2212(t \u2212 i)w,\n(10)\nwhere w \u2208 (R\u22650)d, with d the number of chan-\nnels. We require w to be non-negative to ensure\nthat ewt,i \u2264 1 and the per-channel weights decay\nbackwards in time.\n3\nRWKV\nThe RWKV model architecture is defined by four\nfundamental elements that are intrinsic to the time-\nmixing and channel-mixing blocks:\n\u2022 R: The Receptance vector acts as the receiver\nof past information.\n\u2022 W: The Weight signifies the positional\nweight decay vector, a trainable parameter\nwithin the model.\n\u2022 K: The Key vector performs a role analogous\nto K in traditional attention mechanisms.\n\u2022 V : The Value vector functions similarly to V\nin conventional attention processes.\nThese core elements interact multiplicatively at\neach timestep, as depicted in Figure 2.\n3.1\nArchitecture\nThe RWKV model is composed of stacked residual\nblocks. Each block consists of a time-mixing and\na channel-mixing sub-block, embodying recurrent\nstructures to leverage past information.\nThis model uses a unique attention-like score\nupdate process, which includes a time-dependent\nTime \nMixing\nLayerNorm\nLayerNorm\n?\nChannel \nMixing\n?\n?'\nOut\nWKV\nV\nK\nR\n?\nLayerNorm\nR'\nK'\nV'\nRWKV-LM Head\nInput \nEmbedding\nLayerNorm\nLayerNorm\nOutput Probabilities\nLayerNorm\nLayerNorm\nChannel Mixing\nTime Mixing\nOut\nSoftmax\nFigure 2: Elements within an RWKV block (left) and\nthe complete RWKV residual block, equipped with a\nfinal head for language modeling (right).\nsoftmax operation improving numerical stability\nand mitigating vanishing gradients (for rigorous\nproof, see Appendix H). It ensures that the gra-\ndient is propagated along the most relevant path.\nAdditionally, layer normalization (Ba et al., 2016)\nincorporated within the architecture aids in stabi-\nlizing the gradients, effectively addressing both\nvanishing and exploding gradient issues. These\ndesign elements not only enhance the training dy-\nnamics of deep neural networks but also facilitate\nthe stacking of multiple layers, leading to superior\nperformance over conventional RNN models by\ncapturing complex patterns across different levels\nof abstraction (see also Appendix I).\n3.1.1\nToken Shift\nIn this architecture, all linear projection vectors\n(R, K, V in time-mixing, and R\u2032, K\u2032 in channel-\nmixing) involved in computations are produced by\nlinear interpolation between current and previous\ntimestep inputs, facilitating a token shift.\nThe vectors for time-mixing computation are\nlinear projections of linear combinations of the\ncurrent and previous inputs of the block:\nrt = Wr \u00b7 (\u00b5r \u2299 xt + (1 \u2212 \u00b5r) \u2299 xt\u22121),\n(11)\nkt = Wk \u00b7 (\u00b5k \u2299 xt + (1 \u2212 \u00b5k) \u2299 xt\u22121),\n(12)\nvt = Wv \u00b7 (\u00b5v \u2299 xt + (1 \u2212 \u00b5v) \u2299 xt\u22121),\n(13)\nToken\nshift\nToken\nshift\nStates\nStates\nLayer Norm\nMy\nLayer Norm\nname\nLayer Norm\nTime Mix\nLM Head\nLayer Norm\nis\nLayer Norm\nBob\nLayer Norm\nChannel Mix\nTime Mix\nLM Head\nLayer Norm\nname\nLayer Norm\nis\nLayer Norm\nChannel Mix\nTime Mix\nLM Head\nToken\nshift\nToken\nshift\nChannel Mix\nFigure 3: RWKV architecture for language modeling.\nas are the channel-mixing inputs:\nr\u2032\nt = W \u2032\nr \u00b7 (\u00b5\u2032\nr \u2299 xt + (1 \u2212 \u00b5\u2032\nr) \u2299 xt\u22121),\n(14)\nk\u2032\nt = W \u2032\nk \u00b7 (\u00b5\u2032\nk \u2299 xt + (1 \u2212 \u00b5\u2032\nk) \u2299 xt\u22121).\n(15)\nThe token shift is implemented as a simple off-\nset in the temporal dimension at each block us-\ning the PyTorch (Paszke et al., 2019) library as\nnn.ZeroPad2d((0,0,1,-1)).\n3.1.2\nWKV Operator\nThe computation of the WKV operator in our\nmodel parallels the method used in Attention Free\nTransformer (AFT) (Zhai et al., 2021). However,\nunlike AFT where W is a pairwise matrix, our\nmodel treats W as a channel-wise vector that is\nmodified by relative position. In our model, this\nrecurrent behavior is defined by the time-dependent\nupdate of the WKV vectors, formalized in the fol-\nlowing equation:\nwkvt =\nPt\u22121\ni=1 e\u2212(t\u22121\u2212i)w+ki \u2299 vi + eu+kt \u2299 vt\nPt\u22121\ni=1 e\u2212(t\u22121\u2212i)w+ki + eu+kt\n.\n(16)\nTo circumvent any potential degradation of W,\nwe introduce a vector U that separately attends to\nthe current token. More information about this can\nbe found in Appendix I.\n3.1.3\nOutput Gating\nOutput gating is implemented in both time-mixing\nand channel-mixing blocks using the sigmoid of\nthe receptance, \u03c3(r). The output vector ot post the\nWKV operator is given by:\not = Wo \u00b7 (\u03c3(rt) \u2299 wkvt).\n(17)\nIn the channel-mixing block, a similar operation\nis performed:\no\u2032\nt = \u03c3(r\u2032\nt) \u2299 (W \u2032\nv \u00b7 max(k\u2032\nt, 0)2),\n(18)\nwhere we adopt the squared ReLU activation func-\ntion (So et al., 2021).\n3.2\nTransformer-like Training\nRWKV can be efficiently parallelized using a tech-\nnique called time-parallel mode, reminiscent of\nTransformers. The time complexity of processing\na batch of sequences in a single layer is O(BTd2),\nprimarily consisting of matrix multiplications W\u03bb,\nwhere \u03bb \u2208 {r, k, v, o} (assuming B sequences, T\nmaximum tokens, and d channels). In contrast, up-\ndating attention scores wkvt involves a serial scan\n(see Appendix D for more detail) and has complex-\nity O(BTd).\nThe matrix multiplications can be parallelized\nsimilarly to W\u03bb, where \u03bb \u2208 {Q, K, V, O} in con-\nventional Transformers. The element-wise WKV\ncomputation is time-dependent but can be readily\nparallelized along the other two dimensions (Lei\net al., 2018)3.\n3.3\nRNN-like Inference\nRecurrent networks commonly utilize the output\nat state t as input at state t + 1. This usage is\nalso observed in the autoregressive decoding infer-\nence of language models, where each token must\nbe computed before being passed to the next step.\nRWKV takes advantage of this RNN-like structure,\nknown as time-sequential mode. In this context,\nRWKV can be conveniently formulated recursively\nfor decoding during inference, as demonstrated in\nAppendix D.\n3.4\nAdditional Optimizations\nCustom Kernels\nTo address inefficiencies in the\nWKV computation arising from the sequential na-\nture of the task when using standard deep learning\nframeworks, we have developed a custom CUDA\n3For extremely long sequences, more sophisticated meth-\nods such as Martin and Cundy (2017) that parallelize over\nsequence length could be used.\nkernel. This kernel enables the execution of a sin-\ngle compute kernel on training accelerators, while\nall other parts of the model, such as matrix multi-\nplications and point-wise operations, are already\ninherently parallelizable and efficient.\nSmall Init Embedding\nDuring the initial stage\nof training a transformer model (Vaswani et al.,\n2017), we observe that the embedding matrix un-\ndergoes slow changes, presenting a challenge for\nthe model to move away from its initial noisy em-\nbedding state. To address this issue, we propose\nan approach that involves initializing the embed-\nding matrix with small values and subsequently\napplying an additional LayerNorm operation. This\naccelerates and stabilizes the training process, al-\nlowing for the training of deep architectures with\npost-LN components. The effectiveness of this\napproach is demonstrated in Figure 9, illustrating\nimproved convergence by enabling the model to\nquickly transition away from the initially small em-\nbedding. This is achieved through small changes\noccurring in a single step, which subsequently lead\nto substantial alterations in directions and further\nnotable changes after the LayerNorm operation.\nCustom Initialization\nBuilding on principles\nfrom previous works (He et al., 2016; Jumper et al.,\n2021), we adopt an initialization strategy where\nparameters are set to values resembling an identity\nmapping while breaking symmetry to establish a\nclear information flow. The majority of weights are\ninitialized to zero, and linear layers do not employ\nbiases. Detailed formulas are given in Appendix E.\nWe observe that the choice of initialization plays a\ncrucial role in both the speed and quality of conver-\ngence (refer to Appendix F for further details).\n3.5\nImplementation\nRWKV is implemented using the PyTorch Deep\nLearning Library (Paszke et al., 2019). We inte-\ngrate additional optimization strategies inspired by\nDeepSpeed (Rasley et al., 2020) into the system,\nimproving its efficiency and scalability.\nThe model begins with an embedding layer, as\ndetailed in Section 3.4. Following this are sev-\neral identical residual blocks arranged sequentially.\nThese are depicted in Figures 2 and 3 and adheres\nto the principles outlined in Section 3.1.1. After\nthe last block, a simple output projection head, con-\nsisting of a LayerNorm (Ba et al., 2016) and a\nlinear projection, is employed for logits generation\nfor next-token prediction and computation of the\ncross-entropy loss during training.\n4\nTrained Models and Computing Costs\nTo demonstrate the scalability of RWKV, we train\nsix models ranging from 169 million to 14 billion\nparameters as shown in Table 2. All models are\ntrained for one epoch (330 billion tokens) on the\nPile (Gao et al., 2020; Biderman et al., 2022).\nName\nLayers\nModel Dimension\nParameters\nFLOP per token\n169 M\n12\n768\n1.693 \u00d7 108\n2.613 \u00d7 108\n430 M\n24\n1024\n4.304 \u00d7 108\n7.573 \u00d7 108\n1.5 B\n24\n2048\n1.515 \u00d7 109\n2.823 \u00d7 109\n3 B\n32\n2560\n2.985 \u00d7 109\n5.710 \u00d7 109\n7 B\n32\n4096\n7.393 \u00d7 109\n1.437 \u00d7 1010\n14 B\n40\n5120\n1.415 \u00d7 1010\n2.778 \u00d7 1010\nTable 2: RWKV model architectures and FLOP counts.\nFurther details of these hyperparameters are elaborated\nupon in Appendix G.\nThe number of parameters for each model is\ncomputed using the formula:\n# parameters =\n2V D + 13D2L + D(11L + 4) where V = 50277\nis the vocabulary size, D represents the Model Di-\nmension and L corresponds to the number of lay-\ners. FLOPs is for a forward pass for one token. It\nwas calculated as 2(2V D + 13D2L), which is the\ntwice (add and multiply) the number of parameters\nin linear layers. The backwards pass FLOPs can\nbe approximated as twice that of the forward pass,\ngiving a total of 6(2V D + 13D2L) FLOP per to-\nken. Notably, this matches the standard formula\nfor FLOP calculations in transformers Kaplan et al.\n(2020): FLOP = 6 \u00b7 [# tokens] \u00b7 [# parameters].\n4.1\nAdditional Training Details\nFor training, we use the standard Adam optimizer\nwithout weight decay, use bfloat16 precision, and\ntrain with a context length of 1024 tokens. Fur-\nther details on hyperparameters are in Appendix G.\nDiverting from standard practice for transformers,\nwe apply exponential decay to our learning rate.\nWe also incorporate the auxiliary loss introduced\nby PaLM (Chowdhery et al., 2022), supplement-\ning the standard cross-entropy loss function. This\nauxiliary loss encourages the softmax normalizer\nto approximate zero closely. As for the learning\nrate schedule, it remains constant for the initial\niterations, and subsequently decays exponentially.\n4.2\nScaling Laws\nScaling laws (Kaplan et al., 2020; Henighan et al.,\n2020; Hoffmann et al., 2022; Muennighoff et al.,\n2023) in language models refer to the mathemati-\ncal relationships that describe how the performance\nof a language model changes with respect to var-\nious factors. These factors can include the model\nsize (N), dataset size (D), or the optimally allo-\ncated compute budget (Cmin). Scaling laws are\nimportant for two primary reasons: they allow us\nto make predictions and plans regarding the costs\nand performance of large models before they are\ntrained via interpolation and extrapolation (Black\net al., 2022; Le Scao et al., 2022) and the contexts\nin which they fail provides rich feedback on im-\nportant areas for future research (Wei et al., 2022a;\nBiderman et al., 2023a).\nPrevious work on scaling laws for RNNs has\nclaimed that LSTMs do not strictly follow the same\nlog-log linear scaling that transformers do (Kaplan\net al., 2020). We train 45 RWKV models for a\nvariety of pairs (dataset, parameters) and find that\nRWKV does follow the same general form of the\nscaling law that is well established for transformers.\nFigure 4 shows our results for loss as a function of\ncompute, with the linear fit to the Pareto optimal\npoints holding an r2 value of 0.994. Even when\nwe extrapolate our curve an additional order of\nmagnitude (blue), we find an extremely good fit\nwith an r2 of 0.875.\n10\n1\n100\n101\n102\n103\nCompute (exaFLOP)\n2 \u00d7 100\n3 \u00d7 100\nLoss\nNon-Optimal\nOptimal\nTest\nTrend Line\nFigure 4: Scaling laws curves for RWKV models\n5\nEvaluations\nHaving demonstrated the scalability of RWKV\nmodels in the previous section, we now turn our\nattention to their competitiveness with traditional\ntransformers. We focus on two questions:\nCompetitiveness\nIs RWKV competitive against\nquadratic transformer architectures with the same\namount of compute?\nLong Context\nDoes increasing the context\nlength of RWKV yield better language modeling\nloss when RWKV models are trained for context\nlengths that most open-sourced quadratic trans-\nformers cannot efficiently process?\n5.1\nNLP Evaluations\nTo demonstrate that RWKV is competitive with\ntraditional transformers at NLP tasks, we compare\nwith similarly sized models trained for a similar\nnumber of tokens (Pythia (Biderman et al., 2023b),\nOPT (Zhang et al., 2022) and BLOOM (Scao et al.,\n2022)). All RWKV models were trained for one\nepoch on the Pile (330B tokens), which is close but\nnot identical to the amount of tokens the Pythia,\nOPT, and BLOOM models were trained for. Con-\nsequently, we compare our models on a FLOP-\nmatched basis. We avoid comparing with model\ntrained in the Chinchilla-optimal regime (Hoff-\nmann et al., 2022) or the overtrained regime (Tou-\nvron et al., 2023) to ensure the most equitable com-\nparison.\nWe report results on ARC (both Easy and Chal-\nlenge) (Clark et al., 2018), BoolQ (Clark et al.,\n2019), COPA (Roemmele et al., 2018), HeadQA\n(Vilares and G\u00f3mez-Rodr\u00edguez, 2019), HellaSwag\n(Zellers et al., 2019), LAMBADA (Paperno et al.,\n2016), OpenBookQA\n(Mihaylov et al., 2018),\nPIQA (Bisk et al., 2020), ReCoRD (Zhang et al.,\n2018), SciQ (Johannes Welbl Nelson F. Liu, 2017),\nand Winogrande (Zellers et al., 2020). Figure 1\nshows the average results across all benchmarks.\nSome individual benchmarks are shown in Fig 5,\nwith the rest in Appendix J.\nAdditionally, we carried out comparative studies\non RWKV and ChatGPT / GPT-4, see Appendix\nL. They revealed that RWKV is very sensitive to\nprompt engineering. When the prompts were ad-\njusted (re-ordered) from the ones used for GPT to\nmore suitable for RWKV, the performance (F1) in-\ncreased even from 44.2% to 74.8%. For sarcasm\ndetection, RWKV outperformed ChatGPT, but was\nstill slightly worse than the SOTA solution.\n5.2\nExtended Context Finetuning\nUnlike transformers, RNNs do not have a pre-\ndefined sequences length when they are created.\nHowever in order to efficient make use of compute\n102\n103\nCompute (exaFLOP)\n17.5\n20.0\n22.5\n25.0\n27.5\n30.0\n32.5\n35.0\n37.5\nAccuracy\nBLOOM\nPythia\nOPT\nRWKV\n(a) ARC (Challenge)\n102\n103\nCompute (exaFLOP)\n30\n35\n40\n45\n50\nAccuracy\n(b) HellaSwag\n102\n103\nCompute (exaFLOP)\n20\n30\n40\n50\n60\n70\nAccuracy\nBLOOM\nPythia\nOPT\nRWKV\n(c) LAMBADA (OpenAI)\n102\n103\nCompute (exaFLOP)\n12.5\n15.0\n17.5\n20.0\n22.5\n25.0\n27.5\n30.0\nAccuracy\n(d) OpenBookQA\n102\n103\nCompute (exaFLOP)\n50\n55\n60\n65\n70\n75\n80\n85\n90\nAccuracy\nBLOOM\nPythia\nOPT\nRWKV\n(e) ReCoRD\n102\n103\nCompute (exaFLOP)\n50\n52\n54\n56\n58\n60\n62\n64\n66\nAccuracy\n(f) Winogrande\nFigure 5: Zero-Shot Performance of RWKV on common language modeling evaluation benchmarks. Additional\nplots can be found in Appendix J.\nwe nevertheless need to preprocess the training data\ninto contexts of the same length. We find that we\nare able to teach the model how to efficiently handle\nsubstantially larger batch sizes by finetuning with\nprogressively increasing sequence length. Specif-\nically, we first double the sequence length from\n1024 to 2048 and finetune for 10B tokens from the\noriginal pretraining corpus, then we double again\nto 4096 for 100B tokens from the same corpus, and\nfinally double to 8192 tokens for another 100B to-\nkens from the same corpus. In Fig. 6 we show that\nincreasing context length leads to lower test loss\non the Pile, an indication that RWKV can make\neffective use of long contextual information.\n21\n23\n25\n27\n29\n211\nContext Length\n21\n22\nPile test loss\n7B 8k\n14B 8k\nFigure 6: RWKV shows decreasing mean test loss as a\nfunction of context length on the Pile (Gao et al., 2020)\n5.3\nLong Context Benchmarks\nAdditionally, we evaluate our model\u2019s ability to\nhandle very long sequences by comparing to state-\nof-the-art long sequence models on the Long-\nRange Arena (LRA) benchmark (Tay et al., 2021).\nLRA is designed to assess the performance of mod-\nels in handling lengthy context situations. It in-\ncludes a collection of tasks with sequences rang-\ning from 1,000 to 16,000 tokens, covering various\ntypes of data like text, natural language, synthetic\nimages, and mathematical expressions. We apply\nRWKV on the LRA benchmark and the results are\nin Appendix J.2. The results show that RWKV per-\nforms second only to the S4 model in five datasets.\n6\nInference Experiments\nWe benchmark inference requirements according to\nsize and family. Specifically, we evaluate text gen-\neration speed and memory requirements on typical\ncompute platforms including CPU (x86) and GPU\n(NVIDIA A100 80 GB). For all of our inference\nexperiments we use float32 precision and the Hug-\ngingFace Transformers (Wolf et al., 2020). We in-\nclude all model parameters in the parameter count,\nincluding both embedding and non-embedding lay-\ners. Performance under different quantization se-\ntups is left to further work. See Appendix K for\nmore results.\nFigure 7: Cumulative time on text generation for LLMs.\nUnlike transformers, RWKV exhibits linear scaling.\n7\nFuture Work\nThere are several promising directions for future\nwork on the RWKV architecture. Work can be\ndone to increase model expressivity by enhancing\nthe time-decay formulations and exploring initial\nmodel states while maintaining efficiency.\nThe RWKV computational efficiency can be\nfurther improved by applying a parallel scan in\nthe wkvt step to reduce the computational cost to\nO(B log(T)d).\nThe mechanisms used in RWKV can be applied\nto encoder-decoder architectures, potentially re-\nplacing the cross-attention mechanism. This could\nbe applicable in seq2seq or multimodal settings,\nthereby enhancing efficiency during both training\nand inference.\nRWKV\u2019s state (or context) can be leveraged for\ninterpretability, predictability in sequence data, and\nsafety. Manipulating the hidden state could also\nguide behavior and allow greater customizability\nthrough prompt tuning.\nThe RWKV architecture is not perfect, and can\nbe improved via many aspects, such as modifying\nthe formulae or implementing larger internal states.\nLarger states can enhance the model\u2019s memory to\nprevious context and improve performance over\nvarious tasks.\n8\nConclusions\nWe introduced RWKV, a new approach to RNN\nmodels exploiting the potential of time-based mix-\ning components. RWKV introduces several key\nstrategies that allow it to capture locality and long-\nrange dependencies while addressing limitations of\ncurrent architectures by: (1) replacing the quadratic\nQK attention with a scalar formulation at linear\ncost, (2) reformulating recurrence and sequential\ninductive biases to enable efficient training paral-\nlelization and efficient inference, and (3) enhancing\ntraining dynamics using custom initializations.\nWe benchmark the proposed architecture in a\nwide variety of NLP tasks and show comparable\nperformance to SoTA with reduced cost. Further\nexperiments on expressivity, interpretability, and\nscaling showcase the model capabilities and draw\nparallels in behavior between RWKV and other\nLLMs.\nRWKV opens a new route for scalable and ef-\nficient architectures to model complex relation-\nships in sequential data. While many alternatives\nto Transformers have been proposed with similar\nclaims, ours is the first to back up those claims with\npretrained models with tens of billions of parame-\nters.\n9\nLimitations\nWhile our proposed RWKV model has demon-\nstrated promising results regarding training and\nmemory efficiency during inference, some limita-\ntions should be acknowledged and addressed in\nfuture work.\nFirst, the linear attention of RWKV leads to sig-\nnificant efficiency gains but still, it may also limit\nthe model\u2019s performance on tasks that require re-\ncalling minutiae information over very long con-\ntexts. This is due to the funneling of informa-\ntion through a single vector representation over\nmany time steps, compared with the full informa-\ntion maintained by the quadratic attention of stan-\ndard Transformers. In other words, the model\u2019s\nrecurrent architecture inherently limits its ability to\n\u201clook back\u201d at previous tokens, as opposed to tra-\nditional self-attention mechanisms. While learned\ntime decay helps prevent the loss of information,\nit is mechanistically limited compared to full self-\nattention.\nAnother limitation of this work is the increased\nimportance of prompt engineering in comparison to\nstandard Transformer models. The linear attention\nmechanism used in RWKV limits the information\nfrom the prompt that will be carried over to the\nmodel\u2019s continuation. As a result, carefully de-\nsigned prompts may be even more crucial for the\nmodel to perform well on tasks.\nThe above RWKV property was confirmed by\nstudies on prompt engineering presented in Ap-\npendix L. By changing the order of the information\npieces, we were even able to almost double the\nRWKV performance for some tasks.\n10\nEthics Statement\nIn this paper, we present a novel architecture for se-\nquential data processing and prove its effectiveness\nby building a series of LLMs trained on publicly re-\nleased pretraining data (Gao et al., 2020; Biderman\net al., 2022) and later fine-tuned on publicly avail-\nable instructions (Taori et al., 2023; Chaudhary,\n2023; Cheung, 2023; Anand et al., 2023; Anony-\nmous, 2023; Yang, 2023; Ji et al., 2023a,b).\nAs a novel architecture for sequential data,\nRWKV has the potential to improve sequence-\nbased models across different applications ranging\nfrom natural language processing to biomedical\ndata processing or climate modelling. Since the\ntraining code is released open source, RWKV con-\ntributes to the democratization of AI, levels the\nplaying field, and empowers members of the Open\nSource community to inspect, study, and finetune\nRWKV in particular tasks. Moreover, it contributes\nto advancing the understanding of LLMs capabili-\nties and limitations. A significant amount of work\nhas been devoted to increasing the efficiency of\nRWKV training so as to minimize its cost and pro-\nmote accessibility.\nAs LLMs trained on public data, RWKV\u2019s lower\ninference cost compared to Transformer alterna-\ntives makes it more suitable for deployment in con-\nsumer and edge hardware, which is a step towards\nthe democratization and distribution of LLMs to\nthe general public, creating better privacy and own-\nership incentives. It also lowers the resource bar-\nrier to Chat assistants and text generation for small\nand/or underrepresented communities. PreTrained\nmodel weights for different sizes ranging from 0.1B\nto 14B parameters trained on multiple languages\nare released to increase ease of adoption and allow\nfor the study of emergent phenomena.\nOn the other hand, with lower resource barri-\ners, the spreading of AI-generated text might be-\ncome more prevalent. Current RWKV LLMs may\nexhibit and/or reproduce biases and potentially\nharmful content present in the data used for train-\ning. Nonetheless, mitigation and finetuning strate-\ngies discussed for other, large Transformer models\nshould be applicable to RWKV as well.\nAcknowledgements\nWe thank StabilityAI for the compute used to train\nour models and for technical support in develop-\nment of RWKV. We also thank the members of the\nRWKV and EleutherAI Discord servers for their\nhelp and work on further extending the applicabil-\nity of RWKV to different domains.\nReferences\nMohammad Mahmudul Alam, Edward Raff, Stella Bi-\nderman, Tim Oates, and James Holt. 2023. Recasting\nself-attention with holographic reduced representa-\ntions. arXiv preprint arXiv:2305.19534.\nAlon Albalak, Yi-Lin Tuan, Pegah Jandaghi, Connor\nPryor, Luke Yoffe, Deepak Ramachandran, Lise\nGetoor, Jay Pujara, and William Yang Wang. 2022.\nFETA: A benchmark for few-sample task transfer\nin open-domain dialogue.\nIn Proceedings of the\n2022 Conference on Empirical Methods in Natu-\nral Language Processing, pages 10936\u201310953, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nYuvanesh Anand, Zach Nussbaum, Brandon Duder-\nstadt, Benjamin Schmidt, and Andriy Mulyar. 2023.\nGpt4all: Training an assistant-style chatbot with large\nscale data distillation from gpt-3.5-turbo. https:\n//github.com/nomic-ai/gpt4all.\nAnonymous. 2023. Sharegpt_vicuna_unfiltered.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hin-\nton. 2016. Layer normalization.\nFrancesco Barbieri, Jose Camacho-Collados, Luis Es-\npinosa Anke, and Leonardo Neves. 2020. TweetEval:\nUnified benchmark and comparative evaluation for\ntweet classification. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n1644\u20131650, Online. Association for Computational\nLinguistics.\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv:2004.05150.\nStella Biderman, Kieran Bicheno, and Leo Gao.\n2022.\nDatasheet for the pile.\narXiv preprint\narXiv:2201.07311.\nStella Biderman, USVSN Sai Prashanth, Lintang\nSutawika, Hailey Schoelkopf, Quentin Anthony,\nShivanshu Purohit, and Edward Raf. 2023a. Emer-\ngent and predictable memorization in large language\nmodels. arXiv preprint arXiv:2304.11158.\nStella Biderman, Hailey Schoelkopf, Quentin Anthony,\nHerbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mo-\nhammad Aflah Khan, Shivanshu Purohit, USVSN Sai\nPrashanth, Edward Raff, et al. 2023b. Pythia: A suite\nfor analyzing large language models across training\nand scaling. arXiv preprint arXiv:2304.01373.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng\nGao, and Yejin Choi. 2020. Piqa: Reasoning about\nphysical commonsense in natural language. In Thirty-\nFourth AAAI Conference on Artificial Intelligence.\nSid Black, Leo Gao, Phil Wang, Connor Leahy, and\nStella Biderman. 2021. Gpt-neo: Large scale autore-\ngressive language modeling with mesh-tensorflow.\nURL: https://doi. org/10.5281/zenodo, 5297715.\nSidney Black, Stella Biderman, Eric Hallahan, Quentin\nAnthony, Leo Gao, Laurence Golding, Horace He,\nConnor Leahy, Kyle McDonell, Jason Phang, et al.\n2022. Gpt-neox-20b: An open-source autoregres-\nsive language model. In Proceedings of BigScience\nEpisode\\# 5\u2013Workshop on Challenges & Perspec-\ntives in Creating Large Language Models, pages 95\u2013\n136.\nJames Bradbury, Stephen Merity, Caiming Xiong, and\nRichard Socher. 2017. Quasi-recurrent neural net-\nworks. In ICLR.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877\u20131901.\nAydar Bulatov, Yuri Kuratov, and Mikhail S. Burtsev.\n2023. Scaling transformer to 1m tokens and beyond\nwith rmt.\nAydar Bulatov, Yury Kuratov, and Mikhail Burtsev.\n2022. Recurrent memory transformer. Advances\nin Neural Information Processing Systems, 35:11079\u2013\n11091.\nSahil Chaudhary. 2023. Code alpaca: An instruction-\nfollowing llama model for code generation. https:\n//github.com/sahil280114/codealpaca.\nJoseph Cheung. 2023. Guanacodataset.\nKrzysztof Choromanski, Valerii Likhosherstov, David\nDohan, Xingyou Song, Andreea Gane, Tamas Sar-\nlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,\nLukasz Kaiser, David Belanger, Lucy Colwell, and\nAdrian Weller. 2020. Rethinking attention with per-\nformers.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways. CoRR, abs/2204.02311.\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho,\nand Yoshua Bengio. 2014. Empirical evaluation of\ngated recurrent neural networks on sequence model-\ning. In NIPS 2014 Deep Learning and Representa-\ntion Learning Workshop.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. Boolq: Exploring the surprising\ndifficulty of natural yes/no questions. arXiv preprint\narXiv:1905.10044.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the ai2 reasoning challenge. In\narXiv:1803.05457.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Ja-\ncob Hilton, Reiichiro Nakano, Christopher Hesse,\nand John Schulman. 2021.\nTraining verifiers to\nsolve math word problems.\nIn arXiv, volume\nabs/2110.14168.\nTri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, and\nChristopher Re. 2022a.\nFlashattention: Fast and\nmemory-efficient exact attention with IO-awareness.\nIn Advances in Neural Information Processing Sys-\ntems.\nTri Dao, Daniel Y Fu, Khaled K Saab, Armin W\nThomas, Atri Rudra, and Christopher R\u00e9. 2022b.\nHungry hungry hippos: Towards language mod-\neling with state space models.\narXiv preprint\narXiv:2212.14052.\nDorottya Demszky, Dana Movshovitz-Attias, Jeongwoo\nKo, Alan S. Cowen, Gaurav Nemade, and Sujith Ravi.\n2020. Goemotions: A dataset of fine-grained emo-\ntions. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, ACL\n2020, Online, July 5-10, 2020, pages 4040\u20134054.\nAssociation for Computational Linguistics.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\nAlbert Gu, Karan Goel, and Christopher R\u00e9. 2021. Effi-\nciently modeling long sequences with structured state\nspaces. arXiv preprint arXiv:2111.00396.\nAlbert Gu, Karan Goel, and Christopher R\u00e9. 2022. Effi-\nciently modeling long sequences with structured state\nspaces. In The International Conference on Learning\nRepresentations (ICLR).\nMandy Guo, Joshua Ainslie, David C Uthus, Santiago\nOntanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei\nYang. 2022.\nLongt5: Efficient text-to-text trans-\nformer for long sequences. In Findings of the Associ-\nation for Computational Linguistics: NAACL 2022,\npages 724\u2013736.\nAnkit Gupta, Albert Gu, and Jonathan Berant. 2022. Di-\nagonal state spaces are as effective as structured state\nspaces. Advances in Neural Information Processing\nSystems, 35:22982\u201322994.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Identity mappings in deep residual net-\nworks.\nTom Henighan, Jared Kaplan, Mor Katz, Mark Chen,\nChristopher Hesse, Jacob Jackson, Heewoo Jun,\nTom B Brown, Prafulla Dhariwal, Scott Gray, et al.\n2020.\nScaling laws for autoregressive generative\nmodeling. arXiv preprint arXiv:2010.14701.\nSepp Hochreiter. 1998. The vanishing gradient problem\nduring learning recurrent neural nets and problem so-\nlutions. International Journal of Uncertainty, Fuzzi-\nness and Knowledge-Based Systems, 6(02):107\u2013116.\nSepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long\nshort-term memory. Neural Computation, 9(8):1735\u2013\n1780.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland,\nKatie Millican, George van den Driessche, Bogdan\nDamoc, Aurelia Guy, Simon Osindero, Karen Si-\nmonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,\nand Laurent Sifre. 2022. Training compute-optimal\nlarge language models.\nHassan Ismail Fawaz, Germain Forestier, Jonathan We-\nber, Lhassane Idoumghar, and Pierre-Alain Muller.\n2019. Deep learning for time series classification:\na review. Data mining and knowledge discovery,\n33(4):917\u2013963.\nAndrew Jaegle, Felix Gimeno, Andy Brock, Oriol\nVinyals, Andrew Zisserman, and Joao Carreira. 2021.\nPerceiver: General perception with iterative atten-\ntion. In International conference on machine learn-\ning, pages 4651\u20134664. PMLR.\nHanhwi Jang, Joonsung Kim, Jae-Eon Jo, Jaewon Lee,\nand Jangwoo Kim. 2019. Mnnfast: A fast and scal-\nable system architecture for memory-augmented neu-\nral networks. In Proceedings of the 46th Interna-\ntional Symposium on Computer Architecture, pages\n250\u2013263.\nYunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang\nNiu, Baochang Ma, and Xiangang Li. 2023a. Belle:\nBe everyone\u2019s large language model engine. https:\n//github.com/LianjiaTech/BELLE.\nYunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang\nNiu, Lei Zhang, Baochang Ma, and Xiangang Li.\n2023b.\nExploring the impact of instruction data\nscaling on large language models: An empirical\nstudy on real-world use cases.\narXiv preprint\narXiv:2303.14742.\nMatt Gardner Johannes Welbl Nelson F. Liu. 2017.\nCrowdsourcing multiple choice science questions.\nIn DOI:10.18653/v1/W17-4413.\nJohn Jumper, Richard Evans, Alexander Pritzel, Tim\nGreen, Michael Figurnov, Olaf Ronneberger, Kathryn\nTunyasuvunakool, Russ Bates, Augustin \u017d\u00eddek, Anna\nPotapenko, and et al. 2021. Highly accurate pro-\ntein structure prediction with alphafold.\nNature,\n596(7873):583\u2013589.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models.\narXiv\npreprint arXiv:2001.08361.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-\npas, and Fran\u00e7ois Fleuret. 2020. Transformers are\nrnns: Fast autoregressive transformers with linear\nattention. In International Conference on Machine\nLearning, pages 5156\u20135165. PMLR.\nNikita Kitaev, L. Kaiser, and Anselm Levskaya.\n2020. Reformer: The efficient transformer. ArXiv,\nabs/2001.04451.\nJan Koco\u00b4n, Igor Cichecki, Oliwier Kaszyca, Mateusz\nKochanek, Dominika Szyd\u0142o, Joanna Baran, Julita\nBielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil\nKanclerz, Anna Koco\u00b4n, Bart\u0142omiej Koptyra, Wik-\ntoria Mieleszczenko-Kowszewicz, Piotr Mi\u0142kowski,\nMarcin Oleksy, Maciej Piasecki, \u0141ukasz Radli\u00b4nski,\nKonrad Wojtasik, Stanis\u0142aw Wo\u00b4zniak, and Prze-\nmys\u0142aw Kazienko. 2023. Chatgpt: Jack of all trades,\nmaster of none. Information Fusion, page 101861.\nJan Koco\u00b4n, Piotr Mi\u0142kowski, and Monika Za\u00b4sko-\nZieli\u00b4nska. 2019. Multi-level sentiment analysis of\npolemo 2.0: Extended corpus of multi-domain con-\nsumer reviews. In Proceedings of the 23rd Confer-\nence on Computational Natural Language Learning\n(CoNLL), pages 980\u2013991.\nPhong Le and Willem Zuidema. 2016. Quantifying\nthe vanishing gradient and long distance dependency\nproblem in recursive neural networks and recursive\nlstms. In Proceedings of the 1st Workshop on Repre-\nsentation Learning for NLP, pages 87\u201393.\nTeven Le Scao, Thomas Wang, Daniel Hesslow, Lucile\nSaulnier, Stas Bekman, M Saiful Bari, Stella Bider-\nman, Hady Elsahar, Jason Phang, Ofir Press, et al.\n2022. What language model to train if you have one\nmillion gpu hours? In Proceedings of BigScience\nEpisode #5\u2013Workshop on Challenges & Perspectives\nin Creating Large Language Models.\nTao Lei, Yu Zhang, Sida I. Wang, Hui Dai, and Yoav\nArtzi. 2018. Simple recurrent units for highly paral-\nlelizable recurrence. In Proceedings of the 2018 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 4470\u20134481, Brussels, Belgium.\nAssociation for Computational Linguistics.\nHanxiao Liu, Zihang Dai, David R. So, and Quoc V. Le.\n2021. Pay attention to mlps.\nXuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou,\nJonathan May, Hao Ma, and Luke Zettlemoyer. 2021.\nLuna: Linear unified nested attention.\nAdvances\nin Neural Information Processing Systems, 34:2441\u2013\n2453.\nXuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He,\nLiangke Gui, Graham Neubig, Jonathan May, and\nLuke Zettlemoyer. 2023. Mega: Moving average\nequipped gated attention. In ICLR.\nEric Martin and Chris Cundy. 2017. Parallelizing linear\nrecurrent neural nets over sequence length. ArXiv,\nabs/1709.04057.\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2022. Locating and editing factual asso-\nciations in GPT. Advances in Neural Information\nProcessing Systems, 36.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question answer-\ning. In EMNLP.\nNiklas Muennighoff, Alexander M Rush, Boaz Barak,\nTeven Le Scao, Aleksandra Piktus, Nouamane Tazi,\nSampo Pyysalo, Thomas Wolf, and Colin Raffel.\n2023.\nScaling data-constrained language models.\narXiv preprint arXiv:2305.16264.\nOpenAI. 2022. Introducing chatgpt. https://openai.\ncom/blog/chatgpt.\nAntonio Orvieto, Samuel L Smith, Albert Gu, Anushan\nFernando, Caglar Gulcehre, Razvan Pascanu, and\nSoham De. 2023.\nResurrecting recurrent neu-\nral networks for long sequences.\narXiv preprint\narXiv:2303.06349.\nDenis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazari-\ndou, Ngoc Quan Pham, Raffaella Bernardi, Sandro\nPezzelle, Marco Baroni, Gemma Boleda, and Raquel\nFernandez. 2016. The LAMBADA dataset: Word\nprediction requiring a broad discourse context. In\nProceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1525\u20131534, Berlin, Germany.\nAssociation for Computational Linguistics.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas K\u00f6pf, Edward\nYang, Zach DeVito, Martin Raison, Alykhan Tejani,\nSasank Chilamkurthy, Benoit Steiner, Lu Fang, Jun-\njie Bai, and Soumith Chintala. 2019. Pytorch: An\nimperative style, high-performance deep learning li-\nbrary.\nMichael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y\nFu, Tri Dao, Stephen Baccus, Yoshua Bengio, Ste-\nfano Ermon, and Christopher R\u00e9. 2023. Hyena hierar-\nchy: Towards larger convolutional language models.\narXiv preprint arXiv:2302.10866.\nIlan Price, Jordan Gifford-Moore, Jory Flemming, Saul\nMusker, Maayan Roichman, Guillaume Sylvain,\nNithum Thain, Lucas Dixon, and Jeffrey Sorensen.\n2020. Six attributes of unhealthy conversations. In\nProceedings of the Fourth Workshop on Online Abuse\nand Harms, pages 114\u2013124, Online. Association for\nComputational Linguistics.\nMarkus N. Rabe and Charles Staats. 2022.\nSelf-\nattention does not need o(n2) memory.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase,\nand Yuxiong He. 2020. Deepspeed: System opti-\nmizations enable training deep learning models with\nover 100 billion parameters. In Proceedings of the\n26th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining, KDD \u201920,\npage 3505\u20133506, New York, NY, USA. Association\nfor Computing Machinery.\nMelissa Roemmele, Cosmin Adrian Bejan, , and An-\ndrew S. Gordon. 2018. Choice of plausible alterna-\ntives: An evaluation of commonsense causal reason-\ning. In AAAI.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman\nCastagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon,\nMatthias Gall\u00e9, et al. 2022.\nBloom:\nA 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nRamsha Siddiqui. 2019. SARCASMANIA: Sarcasm\nExposed!\nhttp://www.kaggle.com/rmsharks4/\nsarcasmania-dataset.\n[Online; accessed 02-\nFebruary-2023].\nDavid R. So, Wojciech Manke, Hanxiao Liu, Zihang\nDai, Noam Shazeer, and Quoc V. Le. 2021. Primer:\nSearching for efficient transformers for language\nmodeling. CoRR, abs/2109.08668.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model.\nhttps://\ngithub.com/tatsu-lab/stanford_alpaca.\nYi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan,\nZhe Zhao, and Che Zheng. 2020. Synthesizer: Re-\nthinking self-attention in transformer models.\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen,\nDara Bahri, Philip Pham, Jinfeng Rao, Liu Yang,\nSebastian Ruder, and Donald Metzler. 2021. Long\nrange arena : A benchmark for efficient transformers.\nIn International Conference on Learning Representa-\ntions.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald Met-\nzler. 2022. Efficient transformers: A survey. ACM\nComputing Surveys, 55(6):1\u201328.\nIlya O. Tolstikhin, Neil Houlsby, Alexander Kolesnikov,\nLucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jes-\nsica Yung, Andreas Steiner, Daniel Keysers, Jakob\nUszkoreit, Mario Lucic, and Alexey Dosovitskiy.\n2021. Mlp-mixer: An all-mlp architecture for vi-\nsion. CoRR, abs/2105.01601.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nDavid Vilares and Carlos G\u00f3mez-Rodr\u00edguez. 2019.\nHead-qa: A healthcare dataset for complex reasoning.\nIn ACL.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. Superglue: A stickier\nbenchmark for general-purpose language understand-\ning systems.\nIn Advances in Neural Information\nProcessing Systems, volume 32. Curran Associates,\nInc.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. GLUE:\nA multi-task benchmark and analysis platform for nat-\nural language understanding. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, pages\n353\u2013355, Brussels, Belgium. Association for Com-\nputational Linguistics.\nSinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang,\nand Hao Ma. 2020. Linformer: Self-attention with\nlinear complexity.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raf-\nfel, Barret Zoph, Sebastian Borgeaud, Dani Yo-\ngatama, Maarten Bosma, Denny Zhou, Donald Met-\nzler, Ed Chi, Tatsunori Hashimoto, Oriol Vinyals,\nPercy Liang, Jeff Dean, and William Fedus. 2022a.\nEmergent abilities of large language models. ArXiv,\nabs/2206.07682.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Perric\nCistac, Clara Ma, Yacine Jernite, Julien Plu, Can-\nwen Xu, Teven Le Scao, Sylvain Gugger, Mariama\nDrame, Quentin Lhoest, and Alexander M. Rush.\n2020. Transformers: State-of-the-Art Natural Lan-\nguage Processing.\npages 38\u201345. Association for\nComputational Linguistics.\nZonghan Wu, Shirui Pan, Fengwen Chen, Guodong\nLong, Chengqi Zhang, and S Yu Philip. 2020. A com-\nprehensive survey on graph neural networks. IEEE\ntransactions on neural networks and learning sys-\ntems, 32(1):4\u201324.\nEllery Wulczyn, Nithum Thain, and Lucas Dixon. 2017.\nEx machina: Personal attacks seen at scale. In Pro-\nceedings of the 26th International Conference on\nWorld Wide Web, WWW 2017, Perth, Australia, April\n3-7, 2017, pages 1391\u20131399. ACM.\nJianxin Yang. 2023. Firefly. https://github.com/\nyangjianxin1/Firefly.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, et al. 2020. Big bird: Transformers for\nlonger sequences. Advances in Neural Information\nProcessing Systems, 33.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. Hellaswag: Can a\nmachine really finish your sentence? In ACL.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2020. Winogrande: An\nadversarial winograd schema challenge at scale. In\nACL.\nShuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen\nHuang, Hanlin Goh, Ruixiang Zhang, and Josh\nSusskind. 2021. An attention free transformer.\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng\nGao, Kevin Duh, and Benjamin Van Durme. 2018.\nRecord: Bridging the gap between human and ma-\nchine commonsense reading comprehension.\nIn\narXiv:1810.12885.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nA\nAuthor Contributions\nAll authors contributed to the drafting of this paper. Eric Alcaide and Quentin Anthony organized the\npaper and its experiments and were involved in all phases of the development process.\nModel Design and Development\nBo Peng (lead), Matteo Grella, Xuzheng He, Haowen Hou, Jiaming\nKong, Johan S. Wind\nModel Training\nBo Peng\nScaling Laws Analysis\nStella Biderman, Bo Peng\nBenchmark Evaluations\nStella Biderman (lead), Kranthi Kiran GV, Krishna Sri Ipsit Mantri, Atsushi\nSaito, Qihang Zhao, Peng Zhou, Rui-Jie Zhu\u00e5\u00e7\nLong Context Experiments\nXingjian Du, Rui-Jie Zhu, Bolun Wang, Ruichong Zhang, Jian Zhu,\nRui-Jie Zhu\nInference Speed Experiments\nSamuel Arcadinho, Przemys\u0142aw Kazienko, Qinghua Zhou\nInformation Flow Experiments\nHuanqi Cao, Michael Chung, Matteo Grella, Ferdinand Mom,\nZhenyuan Zhang\nChat Experiments\nJan Koco\u00b4n (lead), Przemys\u0142aw Kazienko, Bart\u0142omiej Koptyra, Hayden Lau, Xiangru\nTang, Stanis\u0142aw Wo\u00b4zniak, Zhenyuan Zhang\nEthics and Broader Impacts\nStella Biderman, Guangyu Song\nB\nAuthor Contributions\nBo Peng\nOriginal RWKV idea, original code, performance optimizations, original experiments, and\ntrained RWKV models from 0.1B to 14B.\nEric Alcaide\nManuscript (initial draft sections 1, C; sections 3, 7 and 8; revision and proofreading; final\nversion ). Figures (2, 3, 3, 8). Experiments section 6. Appendices E, K. Contributions to Appendix M.\nQuentin Anthony\nManuscript (organization, initial draft sections 1, C, 2; revision and proofreading;\nfinal version).\nAlon Albalak\nManuscript (abstract and sections 1, 9, and 7; proofreading and revision).\nSamuel Arcadinho\nContributions to Figures 7, 13, and 14. Contributions to Appendix K.\nStella Biderman\nPerformed the scaling laws analysis and evaluated competitor models on benchmark\ntasks.\nHuanqi Cao\nManuscript (contributions to 3.2 and 3.3; proofreading and revision). Experiments for\nAppendix I.\nXin Cheng\nManuscript (proofreading and revision). Contributions to Appendix M, J.\nMichael Chung\nManuscript (contributions to section I; proofreading and revision).\nXingjian Du\nEvaluation on Long Range Arena Benchmark (TBD until 5.31).\nMatteo Grella\nManuscript (sections H, I, 8; contributions to sections 1, 7 and 9; proofreading and\nrevision). Contributions to Appendix D.\nKranthi Kiran GV\nManuscript (sections C and 5; contributions to section 2; revision and proofreading).\nTables K and K. Appendix 4.\nXuzheng He\nManuscript (contributions to section 2; proofreading and revision). Contributions to\nFigure8. Appendix I. Contributions to appendix H.\nHaowen Hou\nFigure 9. Appendix F.\nJiaju Lin\nRWKV on LRA benchmarking\nPrzemys\u0142aw Kazienko\nManuscript (proofreading and revision). Contributions to Section 6, 9, and\nAppendix L.\nJan Kocon\nManuscript (Section 1; proofreading and revision). Contributions to Appendix L.\nJiaming Kong\nManuscript (revision and proofreading). Appendix H.\nBart\u0142omiej Koptyra\nManuscript (revision and proofreading) Contributions to Appendix L.\nHayden Lau\nManuscript (contributions to section 1 and 9; proofreading and revision). Contributions to\nAppendix M.\nKrishna Sri Ipsit Mantri\nFigure 12\nFerdinand Mom\nManuscript (contributions to section 1, C, 3.3, I; proofreading and revision). Contribu-\ntions to Appendix D.\nAtsushi Saito\nManuscript (sections 2 and 5; contributions to section C). Contributions to Appendix J\nGuangyu Song\nManuscript (rewrote section 3; final version). Initial draft Ethics Statement).\nXiangru Tang\nManuscript (sections C and 2; contributions to abstract; revision and proofreading).\nContributions to Appendix M.\nBolun Wang\nContributions to Tables 1.\nJohan S. Wind\nRWKV performance optimizations (CUDA), Contributions to Appendix 4.\nStanis\u0142aw Wo\u00b4zniak\nContributions to Appendix L.\nRuichong Zhang\nManuscript (proofreading and revision); Contributions to Figure 6 and Appendix M.\nZhenyuan Zhang\nManuscript (revision and proofreading). Figure 3. Experiments Appendix I. Contri-\nbutions to Appendices D and M.\nQihang Zhao\nManuscript (proofreading and revision). Contributions to Table 5.\nPeng Zhou\nContributions to Tables 1 and Table 5.\nQinghua Zhou\nManuscript (Proofreading and revision of section 3; Add missing citations in 3.3).\nRevision of Figures 2 and 12.\nJian Zhu\nManuscript (section C; proofreading and revision). Figures 3 and 6.\nRui-Jie Zhu\nTables 1 and 5. Experiments for table 5.\nC\nAdditional Related Work\nRecently, a number of techniques have been proposed to address the limitations of transformers.\nOptimizing Attention Mechanism\nMany transformer variants (\u201cx-formers\u201d) have been introduced to\nreduce the complexity of transformers (Tay et al., 2022), including sparse attention (Beltagy et al., 2020;\nKitaev et al., 2020; Guo et al., 2022), approximating the full attention matrix (Wang et al., 2020; Ma et al.,\n2021; Choromanski et al., 2020), combining chunked attention with gating (Ma et al., 2023) and other\nefficient methods (Katharopoulos et al., 2020; Jaegle et al., 2021).\nSome recent works like FlashAttention (Dao et al., 2022a) and others (Rabe and Staats, 2022; Jang et al.,\n2019) share similarities with RWKV\u2019s chunked computation scheme. Despite being memory-efficient,\ntheir time complexity remains quadratic or contains chunk size as a hidden factor. In contrast, RWKV\nachieves better space and time complexity during inference by formulating a linear attention as an RNN.\nAttention Free Models\nAnother line of research replaces the attention mechanism with other modules to\nscale to long sequences. MLP-Mixer and others (Tolstikhin et al., 2021; Liu et al., 2021) propose replacing\nattention by Multi-Layer Perceptrons (MLPs) in computer vision tasks. The Attention Free Transformer\n(AFT) (Zhai et al., 2021) and HrrFormer (Alam et al., 2023) replaces dot-product self-attention with a\ncomputationally efficient alternative. None of these models have been successfully scaled to the point\nwhere drawing comparisons with transformer-based large language models makes sense.\nThere has also been substantial research into state space models (SSM) (Gu et al., 2021) and its variants\n(Dao et al., 2022b; Gupta et al., 2022; Poli et al., 2023). In contrast to the preceding models, SSM and\nits successors have shown substantial progress towards efficient scaling. Simultaneously with this work,\nPoli et al. (2023) train SSM-based models with 125 million and 355 million parameters and show that the\nperformance is on-par with a transformer that uses a mix of local and global attention (Black et al., 2021).\nAdvances in RNNs\nInspired by the success of transformers, RNN-style (Hochreiter and Schmidhuber,\n1997; Chung et al., 2014) recursive components have also been modified to increase context length, such\nas the Recurrent Memory Transformer (Bulatov et al., 2022, 2023) and Linear Recurrent Units (Orvieto\net al., 2023). Most similar to our work, the Quasi-Recurrent neural network (QRNN) (Bradbury et al.,\n2017) uses both convolutional layers and recurrent pooling functions across timesteps and channels.\nWhile QRNN utilizes convolutional filters with fixed sizes, RWKV employs a time-mixing module as\nan attention mechanism with time-decaying factors. Different from the element-wise pooling in QRNN,\nRWKV includes a parametrized channel-mixing module that is parallelizable.\nD\nTime-Mixing Block as an RNN Cell\nAs stated in 3.3, the RWKV time-mixing block can be formulated as an RNN, as the WKV computation\ncan be written in such a recursive form:\na0, b0 = 0,\n(19)\nwkvt = at\u22121 + eu+kt \u2299 vt\nbt\u22121 + eu+kt\n,\n(20)\nat = e\u2212w \u2299 at\u22121 + ekt \u2299 vt,\n(21)\nbt = e\u2212w \u2299 bt\u22121 + ekt.\n(22)\n1\n2\ne\n3\nFigure 8: RWKV time-mixing block formulated as an\nRNN cell. Color codes: yellow (\u00b5) denotes the token\nshift, red (1) denotes the denominator, blue (2) denotes the\nnumerator, and pink (3) denotes the fraction computations\nin 16. h denotes the numerator-denominator tuple.\nThe dataflow of the RNN-like time-mixing is\nshown in Fig. 8, where the hidden states h is\nthe numerator-denominator tuple (a, b). To avoid\noverflow in calculating ekt, a numerical trick is\nused in the official implementation. Noticing\nthat a1 = ek1 \u2299 v1 and b1 = ek1, we set a\u2032\n1 =\nv1, b\u2032\n1 = 1, p1 = k1, where pt stores the shared\nexponents of at and bt. Now the above recursion\ncan be converted into a numerical safe version,\nfor each time step t > 1:\nq := max(pt\u22121, u + kt),\n(23)\nwkvt = ept\u22121\u2212q \u2299 a\u2032\nt\u22121 + eu+kt\u2212q \u2299 vt\nept\u22121\u2212q \u2299 b\u2032\nt\u22121 + eu+kt\u2212q\n. (24)\nThe update to a\u2032\nt, b\u2032\nt, and their shared exponent is\nalso carried out in a similar fashion:\nq\u2032 := max(pt\u22121 \u2212 w, kt),\n(25)\na\u2032\nt = ept\u22121\u2212w\u2212q\u2032 \u2299 a\u2032\nt\u22121 + ekt\u2212q\u2032 \u2299 vt,\n(26)\nb\u2032\nt = ept\u22121\u2212w\u2212q\u2032 \u2299 b\u2032\nt\u22121 + ekt\u2212q\u2032,\n(27)\npt = q\u2032.\n(28)\nThe RWKV model has an internal state that stores some previous information. In each layer, the internal\nstate consists five parts, each of which is a vector with D numbers, where D is the model dimension. The\nfive parts are:\n\u2022 The current input of the Time-mix block xt;\n\u2022 The current input of the Channel-mix block yt;\n\u2022 The numerator of the WKV value a\u2032\nt, as defined in equation (26);\n\u2022 The denominator of the WKV value b\u2032\nt, as defined in equation (27);\n\u2022 An auxiliary state pt in (28), which is used for WKV computation to maintain numerical precision.\nWhich yields a total size of 5DL parameters. It is worth noting that in an algebraic context with infinite\nprecision, the helper state pt can be ignored, and the WKV numerator and denominator can be computed\ndirectly using equations (21) and (22), reducing the size of the internal state to 4DL.\nE\nParameter initializations\nWe describe the specific parameter initializations below and motivate the design choices. Parameters\nbelonging to residual blocks are often adjusted by layer depth and total number of layers. Let # denote\nthe vocabulary size, s denote the embedding dimension, d denote the hidden size (we use d = 4s), L the\nnumber of layers, l the layer index (from 0 to L \u2212 1), we use the following initializations:\n\u2022 Embeddings are initialized to U (\u00b11 \u00d7 10\u22124) as explained in 3.4\n\u2022 For the time-mixing blocks (11, 12, 13), initializations are \u00b5ki = ( i\ns)1\u2212 l\nL , \u00b5vi = ( i\ns)1\u2212 l\nL + 0.3l\nL\u22121 and\n\u00b5ri = 1\n2 \u00b7 ( i\ns)1\u2212 l\nL\n\u2022 For the channel-mixing blocks (14, 15), \u00b5ki and \u00b5ri are initialized to ( i\ns)1\u2212 l\nL\n\u2022 wi (16), also known as \u201ctime decay\u201d, is initialized to \u22125 + 8 \u00b7 (\ni\nd\u22121)0.7+ 1.3l\nL\u22121 . Intuitively, it is the\ndiscount factor applied to previous tokens over time.\n\u2022 ui (16), also known as \u201cbonus\u201d, is set to 0.5 \u00b7 (((i + 1) mod 3) \u2212 1) + log 0.3. It is the special\nweighting applied to the current token in equation 16. The alternating zigzag pattern initially\ncreates subtle variations in the tensor elements, which are intended to help the model treat different\ndimensions of the embedding distinctively.\n\u2022 Wo (17) (time-mixing) and Wv (channel-mixing) are initialized to N(0,\nq\nd\ns = 2)\n\u2022 All other Wr, Wk, Wv weights are initialized to 0 so the model can start learning from the beginning\nwithout noisy signals.\n\u2022 All LayerNorm weights start from 1 and biases from 0.\nF\nSmall Init Embedding\nThis section presents the experimental validation of small initialization embedding. The experimental\nsetup is as follows. In the baseline configuration, the parameters are initialized using a normal distribution\nwith a mean of 0.0 and a standard deviation of 0.02, which is a commonly used initialization method\nin models like BERT and GPT. On the other hand, in the small initialization of the embedding (small\ninit emb) experiment, the parameters are initialized using a uniform distribution with a range of 1e-4,\nwhich is slightly different from RWKV where a normal distribution with a standard deviation of 1e-4\nis used. However, this difference is negligible and does not affect our conclusions. The experiments\nwere conducted with a batch size of 400. As depicted in Figure 9, the loss curve for the small init emb\nexhibits a faster rate of decrease and convergence compared to the traditional initialization using a normal\ndistribution.\n0\n10000\n20000\n30000\n40000\n50000\nStep\n4\n5\n6\n7\n8\n9\n10\n11\nLoss\nBaseline\nSmall Init Emb\nFigure 9: Effect of small initialization embedding.\nG\nHyperparameters\nTo train the models mentioned, we use \u03f5 = (0.9, 0.99) without weight decay for the Adam optimizer, and\nswitch batch size dynamically between 128 or 256 sequences, each of 1024 tokens. We further organize\nModel\n169M\n430M\n1.5B\n3B\n7B\n14B\nInit LR\n0.0006\n0.0004\n0.0003\n0.00015\n0.00015\n0.0001\nWarmup Mini-Epochs\n361\n411\n443\n451\n465\n544\nEnd LR\n0.00001\n0.00001\n0.00001\n0.00001\n0.00001\n0.000007\nTable 3: Hyperparameters for our learning rate (LR) schedule of the pretrained models.\nthe training into multiple mini-epochs, each of 40320 samples, to guide our learning rate schedule. The\ntraining process takes 8043 mini-epochs to make one pass over the Pile. The initial warming up mini-\nepochs have a constant learning rate of \u201cInit LR\u201d. After the warming up mini-epochs, the learning rate\nexponentially decays until in the last mini-epoch, in which the model finishes training on the entire Pile,\nthe learning rate arrives at the \u201cEnd LR\u201d. The related hyperparameters are shown in Table 3.\nH\nGradient Stability in RWKV\nIn this section, we present a mathematical description of the gradient stability property in RWKV, focusing\nspecifically on the time-mixing block. By gradient stability we mean that if the inputs xt are bounded and\nthe model parameters are fixed, then the gradients with respect to Wk and Wv are uniformly bounded for\nall T (thus not exploding). Consequently, we can control the amount each xt contributes to the gradient at\nT in a naturally decaying fashion by the weight decay mechanism w (thus not vanishing unless desired).\nFirst, we make the simplification that there are no token shifts, this will not affect the final conclusion.\nIn this scenario, wkvT can be written as\nwkvT =\nPT\nt=1 Ke\nt \u2299 vt\nPT\nt=1 Ke\nt\n= E(vt) = S(vt)\nS(1) ,\n(29)\nwhere\nvt = Wvxt,\n\u2202(vt)i\n\u2202(Wv)i,j\n= (xt)j,\nKe\nt = eWkxt+wT,t,\n\u2202(Ke\nt )i\n\u2202(Wk)i,j\n= (xt)j(Ke\nt )i,\nand S(\u00b7) and E(\u00b7) are shorthand for denoting sums and averages over weights Ke\nt .\nThe loss function at position T can be written as\nLT = l(f(wkvT ), yT ).\n(30)\nBecause wkvT relates to (Wk)i,j and (Wv)i,j only through the i-th channel (wkvT )i, we have\n\u2202LT\n\u2202(Wv)i,j\n=\n\u2202LT\n\u2202(wkvT )i\n\u2202(wkvT )i\n\u2202(Wv)i,j\n.\n(31)\nThe first part of the above equation contains trivial operations like output layers, and other layers of\ntime-mixing, which can be proven inductively. The second part of the above equation can be bounded as\n\f\f\f\f\n\u2202(wkvT )i\n\u2202(Wv)i,j\n\f\f\f\f =\n\f\f\f\f\n\u2202Ei[(vt)i]\n\u2202(Wv)i,j\n\f\f\f\f\n= |Ei[(xt)j]| \u2264 max\nt\n|(xt)j|,\n(32)\nwhich is irrelevant to T. Similarly,\n\u2202(wkvT )i\n\u2202(Wk)i,j\n= \u2202 Si[(vt)i]\nSi(1) /\u2202(Wk)i,j\n= Si[(xt)j(vt)i]\nSi(1)\n\u2212 Si[(xt)j]Si[(vt)i]\nSi(1)2\n= Ei[(xt)j(vt)i] \u2212 Ei[(xt)j]Ei[(vt)i]\n= covi((xt)j, (vt)i)\n(33)\ncan also be bounded. Note that wkv\u2019s softmax operation contains at least two non-zero terms (u and w),\nso the above \u201ccovariance\u201d will not degenerate into 0.\nI\nModel Behavior Visualization\n0\n100\n200\n300\n400\n500\n600\n700\n800\nChannel\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTime Decay\nTime decay (sorted along channel axis)\nLayer 1\nLayer 2\nLayer 3\nLayer 4\nLayer 5\nLayer 6\nLayer 7\nLayer 8\nLayer 9\nLayer 10\nLayer 11\nLayer 12\nFigure 10: Model behavior visualizations of RWKV.\nThe right plot illustrates the time decays (e\u2212w)\nin each layer of the RWKV-169M model, sorted\nalong the channel axis. Notably, several decays\nin the last layers are very close or equal to one,\nimplying that certain information is preserved\nand propagated throughout the model\u2019s temporal\ncontext. Meanwhile, many decays in the initial\nlayer are close to zero, which corresponds to lo-\ncal operations in wkv (16), likely to be associated\nwith tasks such as text parsing or lexical analy-\nsis. (Note that the local operations in wkv are\ndue to the extra parameter u, when e\u2212w is de-\ngenerated into 0.) These patterns of time decays\nare partly learned, but also come from parameter\ninitialization as it speeds up training.\nThe plot below shows the information retrieval\nand propagation path in the RWKV-430M model. The experiment follows the causal trace method\nintroduced by Meng et al. (2022), where we\n1. Run the model once, and record all states and activation of each layer during the computation;\n2. Corrupt the input embeddings of the subject using noise (\u201cThe Eiffel Tower\u201d in this example);\n3. Restore the states and activation of a certain layer at a certain token during the computation, and\nrecord the log-probability of the model outputting the correct answer (\u201cParis\u201d).\nUnlike transformers, RWKV relies on the recursive propagation of information in the time dimension.\nIn this case, the fact that the Eiffel Tower is located in Paris is retrieved in layer 4 just after the model\nsees \u201cThe Eiffel\u201d. It is then passed down to the subsequent layers. In layer 20, mostly, the information is\npropagated through time until reaching where it is needed. Finally, at the token \u201cof\u201d, it is passed down to\nthe last layer for outputting the answer.\nJ\nAdditional Evaluations\nJ.1\nFurther details on NLP tasks\nWe evaluate on the following tasks:\nARC (Clark et al., 2018)\nA dataset designed for multiple-choice question answering, encompassing\nscience exam questions ranging from third grade to ninth grade. It has Easy and Challenge subsets that\nwe report results on separately.\nBoolQ (Clark et al., 2019)\nA binary yes/no question answering benchmark.\n1\n6\n11\n16\n21\nLayer\nThe\n E\niff\nel\n Tower\n is\n located\n in\n the\n city\n of\nInformation propagation path\n7\n6\n5\n4\n3\n2\n1\nLog-probability of \"Paris\"\nFigure 11: Model behavior visualizations of the RWKV model.\nCOPA (Roemmele et al., 2018)\nA dataset to evaluate achievement in open-domain commonsense\ncausal reasoning.\nHeadQA (Vilares and G\u00f3mez-Rodr\u00edguez, 2019)\nA benchmark consisting of graduate-level questions\nencompassing various fields such as medicine, nursing, biology, chemistry, psychology, and pharmacology.\nHellaSwag\n(Zellers et al., 2019) A novel benchmark for commonsense Natural Language Inference\n(NLI) which is build by adversarial filtering against transformer models.\nLAMBADA (Paperno et al., 2016)\nA benchmark dataset that evaluates the model\u2019s contextual reasoning\nand language comprehension abilities by presenting context-target pairs, where the objective is to predict\nthe most probable target token. We follow standard practice and use the untokenized version created by\nOpenAI (Brown et al., 2020).\nOpenBookQA (Mihaylov et al., 2018)\nA QA dataset to evaluate human comprehension of a subject by\nincorporating open book facts, scientific knowledge, and perceptual common sense, drawing inspiration\nfrom open book exams.\nPIQA (Bisk et al., 2020)\nA benchmark for the task of physical common sense reasoning, which consists\nof a binary choice task that can be better understood as a set of two pairs, namely (Goal, Solution).\nReCoRD (Zhang et al., 2018)\nA benchmark for evaluating commonsense reasoning in reading\ncomprehension by generating queries from CNN/Daily Mail news articles and requiring text span answers\nfrom corresponding summarizing passages.\nSciQ (Johannes Welbl Nelson F. Liu, 2017)\nA multiple-choice QA dataset which was created using\nan innovative approach to gather well-crafted multiple-choice questions that are focused on a specific\ndomain.\nWinogrande (Zellers et al., 2020)\nA dataset designed to evaluate the acquisition of common sense\nreasoning by neural language models, aiming to determine whether we are accurately assessing the true\ncapabilities of machine common sense.\n102\n103\nCompute (exaFLOP)\n0\n20\n40\n60\n80\n100\nAccuracy\nBLOOM\nPythia\nOPT\nRWKV\n(a) ARC (Challenge)\n102\n103\nCompute (exaFLOP)\n0\n20\n40\n60\n80\n100\nAccuracy\n(b) ARC (Easy)\n102\n103\nCompute (exaFLOP)\n0\n20\n40\n60\n80\n100\nAccuracy\n(c) BoolQ\n102\n103\nCompute (exaFLOP)\n0\n20\n40\n60\n80\n100\nAccuracy\nBLOOM\nPythia\nOPT\nRWKV\n(d) COPA\n102\n103\nCompute (exaFLOP)\n0\n20\n40\n60\n80\n100\nAccuracy\n(e) HeadQA\n102\n103\nCompute (exaFLOP)\n0\n20\n40\n60\n80\n100\nAccuracy\n(f) HellaSwag\n102\n103\nCompute (exaFLOP)\n0\n20\n40\n60\n80\n100\nAccuracy\nBLOOM\nPythia\nOPT\nRWKV\n(g) LAMBADA (OpenAI)\n102\n103\nCompute (exaFLOP)\n0\n20\n40\n60\n80\n100\nAccuracy\n(h) OpenBookQA\n102\n103\nCompute (exaFLOP)\n0\n20\n40\n60\n80\n100\nAccuracy\n(i) PiQA\n102\n103\nCompute (exaFLOP)\n0\n20\n40\n60\n80\n100\nAccuracy\nBLOOM\nPythia\nOPT\nRWKV\n(j) ReCoRD\n102\n103\nCompute (exaFLOP)\n0\n20\n40\n60\n80\n100\nAccuracy\n(k) SciQ\n102\n103\nCompute (exaFLOP)\n0\n20\n40\n60\n80\n100\nAccuracy\n(l) Winogrande\nFigure 12: Zero-Shot Performance of RWKV on common language modeling evaluation benchmarks.\nJ.2\nEvaluation on Long Range Arena\nThe Long-Range Arena (LRA) benchmark (Tay et al., 2021) is designed to assess the performance of\nmodels in handling lengthy context situations. It includes a collection of tasks with sequences ranging\nfrom 1,000 to 16,000 tokens, covering various types of data like text, natural language, synthetic images,\nand mathematical expressions. We apply RWKV on the LRA benchmark and the report results are in\nTable 4. Other models\u2019 performances are directly cited from Gu et al. (2022); Alam et al. (2023).\nTable 4: Evaluation on Long Range Arena. Other models reported in the literature (Gu et al., 2022; Alam et al.,\n2023). Bolded values are the best.\nMODEL\nLISTOPS\nTEXT\nRETRIEVAL\nIMAGE\nPATHFINDER\nPATH-X\nAVG\nTransformer\n36.37\n64.27\n57.46\n42.44\n71.40\n\u2717\n53.66\nReformer\n37.27\n56.10\n53.40\n38.07\n68.50\n\u2717\n50.56\nBigBird\n36.05\n64.02\n59.29\n40.83\n74.87\n\u2717\n54.17\nLinear Trans.\n16.13\n65.90\n53.09\n42.34\n75.30\n\u2717\n50.46\nPerformer\n18.01\n65.40\n53.82\n42.77\n77.05\n\u2717\n51.18\nFNet\n35.33\n65.11\n59.61\n38.67\n77.80\n\u2717\n54.42\nNystr\u00f6mformer\n37.15\n65.52\n79.56\n41.58\n70.94\n\u2717\n57.46\nLuna-256\n37.25\n64.57\n79.29\n47.38\n77.72\n\u2717\n59.37\nHrrformer\n39.98\n65.38\n76.15\n50.45\n72.17\n\u2717\n60.83\nS4\n59.60\n86.82\n90.90\n88.65\n94.20\n96.35\n86.09\nRWKV\n55.88\n86.04\n88.34\n70.53\n58.42\n\u2717\n72.07\nThe results show that RWKV performs second only to the S4 model in five datasets. While RWKV\nsubstantially underpreforms S4 on Image, Pathfinder, and Path-X, on the problems related to natural\nlanguage and computer code processing RWKV performs on par with S4 or nearly so.\nJ.3\nEnwik8 Perplexity\nWe also evaluate our model in terms of perplexity on the Enwik8 dataset. Baseline comparisons are\nmade with Reformer (Kitaev et al., 2020), Synthesizer (Tay et al., 2020) (the best performing dense\nversion), Linear Transformer (Katharopoulos et al., 2020), Performer (Choromanski et al., 2020). L, d,\nand T denote the number of blocks (network depth), dimension of features, and sequence length, re-\nspectively. Both Linear Transformer and Performer are implemented with customized CUDA kernels\n(github.com/idiap/fast-transformers), and all other models are implemented in native Pytorch. 1 No weight\ndecay nor dropout was used. 2 Trained with AdamW and weight decay set to 0.1, dropout of 0.1, batch\nsize of 16, and initial learning rate of 6e-4.\nMethod\nL\nd\nT\nTrain bpc\nTest bpc\nTime Complexity\nSpace Complexity\nTransformer\n12\n512\n1024\n0.977\n1.137\nO(T 2d)\nO(T 2 + Td)\nTransformer\n24\n256\n1024\n1.039\n1.130\nO(T 2d)\nO(T 2 + Td)\nReformer\n12\n512\n1024\n1.040\n1.195\nO(T log Td)\nO(T log T + Td)\nSynthesizer\n12\n512\n1024\n0.994\n1.298\nO(T 2d)\nO(T 2 + Td)\nLinear Transformer\n12\n512\n1024\n0.981\n1.207\nO(Td2)\nO(Td + d2)\nPerformer\n12\n512\n1024\n1.002\n1.199\nO(Td2 log d)\nO(Td log d + d2 log d)\nAFT-simple\n12\n512\n1024\n1.046\n1.209\nO(Td)\nO(Td)\nRWKV-RNN1\n6\n512\n1024\n0.720\n-\nO(Td)\nO(d)\nRWKV-RNN2\n12\n512\n1024\n1.010\n1.178\nO(Td)\nO(d)\nTable 5: Enwik8 results, measured in bits per character (bpc).\nK\nInference results\nFigures 13 and 14 illustrate, respectively, the results on time (s) and memory (RAM, VRAM) requirements\nfor LLM inference in float32 precision. We benchmark the following model families and sizes:\n\u2022 RWKV: 169m, 430m, 1.4b, 3b, 7b, 14b\n\u2022 Bloom (Scao et al., 2022): 560m, 1b, 3b\n\u2022 OPT (Zhang et al., 2022): 125m, 350m, 1.3b, 2.7b, 6.7b, 13b\n\u2022 GPT-Neo (Black et al., 2021): 125m, 1.3b, 2.7b\n\u2022 Pythia (Biderman et al., 2023b): 160m, 410m, 1.4b, 2.8b, 6.7b, 12b\nFigure 13: Text generation inference memory (CPU RAM, GPU VRAM) for LLMs. Model parameters are not\naccounted.\nFigure 14: Text generation inference time for LLMs.\nTask Name\nMeasure\nChatGPT\nGPT-4\nRWKV-GPT\nRWKV-adapted\nSOTA\nRTE\nF1 Macro\n88.1\n91.3\n44.2\n74.8\n92.1\nWNLI\nAccuracy\n81.7\n91.6\n47.9\n49.3\n97.9\nGoEmotions\nF1 Macro\n25.6\n23.1\n7.9\n7.9\n52.8\nPolEmo2\nF1 Macro\n44.1\n41.0\n38.2\n40.9\n76.4\nTable 6: ChatGPT, GPT-4 and RWKV-4-Raven-14B reasoning performance comparison in RTE (Wang et al., 2019),\nWNLI (Wang et al., 2018), GoEmotions (Demszky et al., 2020), and PolEmo2 (Koco\u00b4n et al., 2019) benchmarks.\nRWKV GPT prompts were primarily used for ChatGPT in (Koco\u00b4n et al., 2023). SOTA is provided as a supplementary\nreference.\nTask Name\nMeasure\nChatGPT\nRWKV-adapted\nSOTA\nAggression\nF1 Macro\n69.10\n56.66\n74.45\nMathQA\nAccuracy\n71.40\n5.43\n83.20\nSarcasm\nF1 Macro\n49.88\n50.96\n53.57\nTweetSent\nF1 Macro\n63.32\n52.50\n72.07\nUnhealthy\nF1 Macro\n45.21\n43.30\n50.96\nTable 7: ChatGPT and RWKV-4-Raven-14B performance comparison in Aggresion (Wulczyn et al., 2017), Sarcasm\n(Siddiqui, 2019), Unhealthy (Price et al., 2020), MathQA (Cobbe et al., 2021), and TweetSent (Barbieri et al., 2020)\nbenchmarks. SOTA is provided as a supplementary reference.\nL\nImportance of prompt construction and comparison to GPT models\nInspired by Koco\u00b4n et al. (2023), we compared the zero-shot performance of the RWKV-4-Raven-14B with\nChatGPT (access in February 2023) and GPT-4 using several known NLP tasks, i.e., recognizing textual\nentailment (RTE), Winograd Natural Language Inference (WNLI), and recognizing emotions elicited\nin readers (GoEmotions and PolEmo2). Each model got the same prompts manually chosen to receive\nproper responses from the ChatGPT model. As shown in Tab. 6, RWKV performs significantly worse\nthan ChatGPT and GPT-4 in several specific tasks. We suspect that this disparity is likely caused by the\nchoice of prompts used to generate the answers since the prompts are written in natural language and do\nnot take into account that RWKV, as an RNN, is unable to look back inside an instruction.\nWhen the instruction style was adapted (re-ordered) to respect that RNNs are not capable of \"retro-\nspective processing\", the quality may significantly change, e.g., for RTE (Wang et al., 2019) F1 Macro\nincreased from 44.2% to 74.8%. We hypothesize that RWKV models are more sensitive to the position of\nthe components in the context, as RNN-based architectures cannot look back and readjust the weight of\nprevious information. For better performance, the desired information should be placed after the main\nquestion.\nAn example ChatGPT prompt for recognizing textual entailment (RTE)\nHaving premise <here is a premise> judge if the following hypothesis <here is a hypothesis> is\nlogically connected with the premise. Answer \"entailment\" if yes, or \"not_entailment\" if no.\nA re-ordered RWKV prompt for RTE taking into account the nature of the RNN\nCan you tell me if the hypothesis is entailment or is not entailment to the premise?\npremise: <here is a premise>\nhypothesis: <here is a hypothesis>\nWhile separating the instruction from the input is relatively easy to do, some other aspects of prompt\nengineering are harder to quantify. For that purpose, we also tested the approach of stating the input after\nthe question on multiple other tasks, i.e., aggression and sarcasm detection, classification of unhealthy\n(offensive) texts, mathematical Q&A, and sentiment analysis, see Tab. 7. The results suggest that better\nprompts might reduce the disparity between models. Raven achieves comparable results to ChatGPT on\nunhealthy conversation detection and even surpasses it on the sarcasm detection dataset. While such an\napproach to prompting looks necessary, it is not enough in itself to replace the capability of having free\naccess to the whole context. Therefore, prompt engineering seems to be significantly more important for\nthe RNN models rather than for standard transformers. It is entirely possible that good prompts to RNN\nmodels do not mean additional restrictions, but should simply be constructed using completely different\nguidelines. The authors of the aforementioned paper (Koco\u00b4n et al., 2023)4 perform chain-of-thought\nto improve results on the MathQA dataset. Even including this approach, the Raven model achieved a\nvery low accuracy of 5.43%. Without it, the model performed even worse, performing only very basic\nand simple calculations and achieving 4.13% accuracy. Raven struggled with questions that required\nintermediate results. It is likely that the order of information presented in the math questions inside the\ndataset poses a challenge for the RWKV model. It is yet to be seen if prompt engineering can address this\nissue. This further emphasizes the importance of the order of information the model receives.\nTemplate used to prompt the Raven model in MathQA with chain-of-thought\nWrite the reasoning and highlight the answer to the question at the end in the format: \"Answer: \".\nThe question is: <here is a question>\nTemplate used to prompt the Raven model in MathQA without chain-of-thought\nWrite the answer to the math question in the format: \"Answer: \".\nThe question is: <here is a question>\nM\nCases\nIn this part, we present a few instances of outputs produced by RWKV model using a Chat interface56.\nRWKV-4-Raven-14B\nTell me about ravens.\nRavens are large, black birds with a distinctive white head and neck. They are found in most parts\nof the world, including North America, Europe, Asia, and Australia. Ravens are known for their\nintelligence and problem-solving abilities. They are also considered to be symbols of death and\ntransformation in many cultures. Ravens are often associated with the afterlife or death because\nthey have been known to eat carrion or even other birds. In some cultures, ravens are also believed\nto be messengers of the gods or guardians of treasure.\n4This is in line with the idea discussed in (Wei et al., 2022b)\n5ttps://github.com/BlinkDL/ChatRWKV\n6https://huggingface.co/spaces/BlinkDL/ChatRWKV-gradio\nRWKV-4-Raven-14B\nExplain the concept of machine learning to a 10-year-old.\nMachine learning is like a super-smart computer that can learn on its own. It can recognize patterns\nand make decisions based on what it has learned. Just like how you might remember something\nfrom the past, the computer can remember what it\u2019s learned and use it to make predictions or\ndecisions in the future. It\u2019s like having a smart friend who always knows what to do!\nRWKV-4-Raven-14B\nWrite a short poem describing nature\u2019s beauty in springtime.\nNature\u2019s beauty in springtime is rare,\nA symphony of colors, a gentle surprise.\nThe flowers bloom, the birds sing and sing,\nAs the sun shines down, warming our skin.\nThe trees sway in the gentle breeze,\nAs we take in their beauty with ease.\nThe sky is blue, the clouds are white,\nA sight to behold, it\u2019s truly divine.\nRWKV-4-Raven-14B\nBelow is the numerical stable formula for updating the state variable in RWKV:\na\u2217\nt = ep\u2212qat\u22121 + eu+kt\u2212qvt\nb\u2217\nt = ep\u2212qbt\u22121 + eu+kt\u2212q\nwkvt = a\u2217\nt\nb\u2217\nt\nPlease write out the corresponding pytorch code to do so, where both at and bt are\ntorch.Tensor([n])\nHere is the PyTorch code to update the state variable in RWKV using the given formula:\nimport\nt o r c h\ndef\nu p d a t e _ s t a t e ( a ,\nb ,\nu ,\nv ,\np ,\nq ) :\na_t = t o r c h . exp ( p \u2212 q ) * a + t o r c h . exp ( u + k \u2212 q ) * v\nb_t = t o r c h . exp ( p \u2212 q ) * b + t o r c h . exp ( u + k \u2212 q )\nreturn\na_t ,\nb_t\na ,\nb = t o r c h . randn (2 ,\n3)\nu = t o r c h . randn ( 2 )\nv = t o r c h . randn ( 2 )\nwkv = t o r c h . randn ( 2 )\na_t ,\nb_t = u p d a t e _ s t a t e ( a ,\nb ,\nu ,\nv , wkv ,\nt o r c h . randn ( 2 ) )\nprint ( \" a_t : \" ,\na_t )\nprint ( \" b_t : \" ,\nb_t )\nThis code defines a function update_state that takes in the state variables a and b, the control\nvariables u and v, and the parameters wkv and torch.randn. It then uses the PyTorch torch.exp\nfunction to compute the exponential of the control variables and the log of the normalization factor.\nFinally, it uses these values to update the state.\nRWKV-4-Raven-14B\nExplain the following metaphor: Life is like cats.\nThe metaphor \"Life is like cats\" suggests that life can be unpredictable and full of unexpected\ntwists and turns, just like a cat\u2019s behavior. It implies that we cannot fully control or predict the\noutcome of our lives, just as we cannot control the movements of a cat. The metaphor also implies\nthat life is complex and multifaceted, with many different variables and factors at play. Just as a\ncat can have multiple personalities or moods, life can have many twists and turns that are difficult\nto predict or understand. Overall, the metaphor suggests that life is complex and unpredictable,\nmuch like a cat\u2019s behavior.\nRWKV-4-Raven-14B\nTest the convergence of P\u221e\ni=0 2\u2212i.\nThe given series is a geometric series with the first term a0 = 2 and common ratio r = 1\n2. We can\nfind the sum of this series using the formula for a geometric series:\n\u221e\nX\ni=0\nai =\na0\n1 \u2212 r =\n1\n2\n1 \u2212 1\n2\n=\n1\n1 \u2212 1\n2\n= 2\n1 = 2\nSo, the sum of the given series is 2 .\nRWKV-4-Raven-14B\nI would like to cook some Chinese food at home. Do you have any suggestions on the meal I\ncould choose?\nYes, of course! If you are not very experienced in cooking Chinese food, you could start with\nmaking noodles or rice, then make a dish using one of those. I also think making stir-fry with\nsome meat is an excellent idea.\nI want to make something more special. What about red braised pork belly?\nYes, red braised pork belly is also very popular and tasty.\nTell me the steps to cook.\nYou will need a few ingredients such as pork belly, star anise, sugar, ginger, scallions and wine.\nPut all of these in a pot, cook on medium heat and simmer until the meat is tender. After that,\nremove the meat from the pot and reserve the sauce. Slice the meat and add some ginger slices\ninto it, then boil until it\u2019s almost done.\nWhat do I need to be careful with the most during the cooking process?\nTo cook pork belly properly, it is very important to monitor the heat of the stove and be patient\nwith the cooking process. Overcooking it can cause the pork to be tough, dry and difficult to chew.\nYou can add a few ingredients like soy sauce or black beans to give the pork more flavor.\nTell me more about the proper heat and timing.\nIf the temperature of the water is too high, it can damage the collagen and connective tissues,\nresulting in tough pork belly. It is essential to control the temperature carefully, especially\nwhen braising the pork. Additionally, keep the water simmering but not boiling to maintain a\nlow simmer, as high heat will dry out the pork belly and result in tough and unpalatable meat.\nLastly, monitor the heat during cooking and make sure the heat is medium, as this is what is most\nappropriate for slow cooking pork belly.\nI see. Thank you for your advice.\nYou\u2019re welcome! It was a pleasure chatting with you. Let me know if you need any further\nassistance in the future.\nRWKV-4-Raven-14B\nIf all apples are fruits and all fruits are sweet, are all apples sweet? Please explain your\nanswer in detail.\nYes, all apples are fruits, and all fruits are sweet. Therefore, it is safe to assume that all apples are\nsweet.\nIf birds can fly. Penguins are birds. Can penguins fly? Please explain your answer in detail.\nNo, penguins cannot fly. Penguins are birds, but they are not capable of flight like other birds.\nThey have wings and can glide for short distances, but they cannot take off or fly long distances\nlike other birds.\nThe day before two days after the day before tomorrow is Saturday. What day is it today?\nPlease explain your answer in detail.\nToday is Friday. Two days after tomorrow is Sunday. Tomorrow is Saturday.\nIf five cats can catch five mice in five minutes, how long will it take one cat to catch one\nmouse? Please explain your answer in detail.\nIf five cats can catch five mice in five minutes, it means that each cat can catch one mouse in one\nminute. To find out how long it will take one cat to catch one mouse, we can use the concept of\nproportionality.\nLet\u2019s assume that it takes x minutes for a cat to catch a mouse. Then, the number of mice caught\nby a cat in one minute is equal to the number of cats multiplied by the number of mice caught by\neach cat: x = 5 * 1 = 5\nSo, it will take 5 minutes for one cat to catch one mouse.\nRWKV-4-Raven-14B\nSummarize the following paragraph.\n<The Abstract of this paper goes here>\nThe paper presents a novel architecture, Receptance Weighted Key Value (RWKV), that combines\nthe efficient parallelizable training of Transformers with the efficient inference of RNNs. The\nmodel is able to scale to tens of billions of parameters and exhibits linear computational complexity\nduring training and inference, making it a promising alternative to Transformers for sequence\nprocessing tasks.\nPlease further summarize it in one sentence.\nThe researchers have developed a new architecture that combines the efficient parallelizable\ntraining of Transformers with the efficient inference of RNNs, which allows for linear scaling in\nmemory and computational requirements.\n"
  },
  {
    "title": "ControlVideo: Training-free Controllable Text-to-Video Generation",
    "link": "https://arxiv.org/pdf/2305.13077.pdf",
    "upvote": "5",
    "text": "ControlVideo: Training-free Controllable\nText-to-Video Generation\nYabo Zhang1 Yuxiang Wei1 Dongsheng Jiang2 Xiaopeng Zhang2 Wangmeng Zuo1 (\f) Qi Tian2\n1Harbin Institute of Technology 2Huawei Cloud\nAbstract\nText-driven diffusion models have unlocked unprecedented abilities in image gener-\nation, whereas their video counterpart still lags behind due to the excessive training\ncost of temporal modeling. Besides the training burden, the generated videos also\nsuffer from appearance inconsistency and structural \ufb02ickers, especially in long\nvideo synthesis. To address these challenges, we design a training-free frame-\nwork called ControlVideo to enable natural and ef\ufb01cient text-to-video generation.\nControlVideo, adapted from ControlNet, leverages coarsely structural consistency\nfrom input motion sequences, and introduces three modules to improve video\ngeneration. Firstly, to ensure appearance coherence between frames, ControlVideo\nadds fully cross-frame interaction in self-attention modules. Secondly, to mitigate\nthe \ufb02icker effect, it introduces an interleaved-frame smoother that employs frame\ninterpolation on alternated frames. Finally, to produce long videos ef\ufb01ciently,\nit utilizes a hierarchical sampler that separately synthesizes each short clip with\nholistic coherency. Empowered with these modules, ControlVideo outperforms\nthe state-of-the-arts on extensive motion-prompt pairs quantitatively and qualita-\ntively. Notably, thanks to the ef\ufb01cient designs, it generates both short and long\nvideos within several minutes using one NVIDIA 2080Ti. Code is available at\nhttps://github.com/YBYBZhang/ControlVideo.\n1\nIntroduction\nLarge-scale diffusion models have made a tremendous breakthrough on text-to-image synthesis [1,\n22,26,29,32] and their creative applications [6,8,21,37]. Several works [5,9,11,12,34] attempt\nto replicate this success in the video counterpart, i.e., modeling higher-dimensional complex video\ndistributions in the wild world. However, training such a text-to-video model requires massive\namounts of high-quality videos and computational resources, which limits the further research and\napplications by relevant communities.\nTo reduce the excessive training requirements, we study a new and ef\ufb01cient form: controllable\ntext-to-video generation with text-to-image models. This task aims to produce a video conditioned\non both a textual description and motion sequences (e.g., depth or edge maps). As shown in Fig. 1,\ninstead of learning the video distribution from scratch, it could ef\ufb01ciently leverage the generation\ncapability of pre-trained text-to-image generative models [26,29] and coarsely temporal consistency\nof motion sequences to produce vivid videos.\nRecent studies [15, 40] have explored leveraging the structure controllability of ControlNet [43]\nor DDIM inversion [35] for video generation. Rather than synthesizing all frames independently,\n[15,40] enhance appearance coherence by replacing original self-attention with the sparser cross-\nframe attention. Nevertheless, their video quality is still far behind photo-realistic videos in terms\nof: (i) inconsistent appearance between some frames (see Fig. 4 (a)), (ii) visible artifacts in large\nmotion videos (see Fig. 4 (b)), and (iii) structural \ufb02ickers during inter-frame transitions. For (i) and\nPreprint. Under review.\narXiv:2305.13077v1  [cs.CV]  22 May 2023\nA man riding a sleek, black motorbike through the winding roads.\nA charming flamingo gracefully wanders in the calm water.\nControlNet\nControlVideo\nA swan \nmoving \nin a lake\nTemporal\nExtension\nJames bond does the moonwalk on the beach, animation style.\nA swan \nmoving \nin a lake\nFigure 1: Training-free controllable text-to-video generation. Left: ControlVideo adapts Con-\ntrolNet to the video counterpart by in\ufb02ating along the temporal axis, aiming to directly inherit its\nhigh-quality and consistent generation without any \ufb01netuning. Right: ControlVideo could synthesize\nphoto-realistic videos conditioned on various motion sequences, which are temporally consistent in\nboth structure and appearance. Results best seen at 500% zoom.\n(ii), their sparser cross-frame mechanisms increase the discrepancy between the query and key in\nself-attention modules, and hence impede inheriting high-quality and consistent generation from\npre-trained text-to-image models. For (iii), input motion sequences only provide the coarse-level\nstructure of videos, failing to smoothly transition between consecutive frames.\nIn this work, we propose a training-free ControlVideo for high-quality and consistent controllable\ntext-to-video generation, along with interleaved-frame smoother to enhance structural smoothness.\nControlVideo directly inherits the architecture and weights from ControlNet [43], while adapting it to\nthe video counterpart by extending self-attention with the fully cross-frame interaction. Different\nfrom prior works [15, 40], our fully cross-frame interaction concatenates all frames to become\na \u201clarger image\u201d, thus directly inheriting high-quality and consistent generation from ControlNet.\nInterleaved-frame smoother de\ufb02ickers the whole video via the interleaved interpolation at selected\nsequential timesteps. As illustrated in Fig. 3, the operation at each timestep smooths the interleaved\nthree-frame clips by interpolating middle frames, and the combination at two consecutive timesteps\nsmooths the entire video. Since the smoothing operation is only performed at a few timesteps, the\nquality and individuality of interpolated frames can be well retained by the following denoising steps.\nTo enable ef\ufb01cient long-video synthesis, we further introduce a hierarchical sampler to produce\nseparated short clips with long-term coherency. In speci\ufb01c, a long video is \ufb01rst split into multiple\nshort video clips with the selected key frames. Then, the key frames are pre-generated with fully\ncross-frame attention for long-range coherence. Conditioned on pairs of key frames, we sequentially\nsynthesize their corresponding intermediate short video clips with the global consistency.\nWe conduct the experiments on extensively collected motion-prompt pairs. The experimental results\nshow that our method outperforms alternative competitors qualitatively and quantitatively. Thanks to\nthe ef\ufb01cient designs, i.e., the xFormers [17] implementation and hierarchical sampler, ControlVideo\ncan produce both short and long videos within several minutes using one NVIDIA 2080Ti.\nIn summary, our contributions are presented as follows:\n\u2022 We propose a training-free ControlVideo for controllable text-to-video generation, which consists\nof the fully cross-frame interaction, interleaved-frame smoother, and hierarchical sampler.\n\u2022 The fully cross-attention demonstrates higher video quality and appearance consistency, while\ninterleaved-frame smoother further reduces structural \ufb02ickers throughout a whole video.\n\u2022 The hierarchical sampler enables ef\ufb01cient long-video generation in commodity GPUs.\n2\nBackground\nLatent diffusion model (LDM) [29] is an ef\ufb01cient variant of diffusion models [10] by applying the\ndiffusion process in the latent space rather than image space. LDM contains two main components.\n2\nor\nControlNet\nA swan moving \nin a lake\nInterleaved-\nFrame\nSmoother\n\u00d7 T steps \n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\nText\nCross-Attn\nFully\nCross-Frame\n\u00b7\u00b7\u00b7\nConv Block\nAttn Block\nTemporal\nInflation\nSelf-Attention\nFully Cross-Frame\nAttention\n\ud835\udc67!\n\ud835\udc67\"\n\ud835\udc67!\n\"\n\ud835\udc67!\n#\n\ud835\udc67!$%#\n\u00b7\u00b7\u00b7\n\ud835\udc67!\"\n\ud835\udc67!\n#\n\ud835\udc67!$%#\n\u00b7\u00b7\u00b7\n\ud835\udc67!\n\"\n\ud835\udc67!#\n\ud835\udc67!$%#\n\u00b7\u00b7\u00b7\n\ud835\udc67!\n\"\n\ud835\udc67!#\n\ud835\udc67!\n$%#\n\u00b7\u00b7\u00b7\nFigure 2: Overview of ControlVideo. For consistency in appearance, ControlVideo adapts Con-\ntrolNet to the video counterpart by adding fully cross-frame interaction into self-attention modules.\nConsidering the \ufb02ickers in structure, the interleaved-frame smoother is integrated to smooth all\ninter-frame transitions via the interleaved interpolation (see Fig. 3 for details).\nFirstly, it uses an encoder E to compress an image x into latent code z = E(x) and a decoder to\nreconstruct this image x \u2248 D(z), respectively. Secondly, it learns the distribution of image latent\ncodes z0 \u223c pdata(z0) in a DDPM formulation [10], including a forward and a backward process.\nThe forward diffusion process gradually adds gaussian noise at each timestep t to obtain zt:\nq(zt|zt\u22121) = N(zt;\np\n1 \u2212 \u03b2tzt\u22121, \u03b2tI),\n(1)\nwhere {\u03b2t}T\nt=1 are the scale of noises, and T denotes the number of diffusion timesteps. The backward\ndenoising process reverses the above diffusion process to predict less noisy zt\u22121:\np\u03b8(zt\u22121|zt) = N(zt\u22121; \u00b5\u03b8(zt, t), \u03a3\u03b8(zt, t)).\n(2)\nThe \u00b5\u03b8 and \u03a3\u03b8 are implemented with a denoising model \u03f5\u03b8 with learnable parameters \u03b8, which is\ntrained with a simple objective:\nLsimple := EE(z),\u03f5\u223cN (0,1),t\nh\n\u2225\u03f5 \u2212 \u03f5\u03b8(zt, t)\u22252\n2\ni\n.\n(3)\nWhen generating new samples, we start from zT \u223c N(0, 1) and employ DDIM sampling to predict\nzt\u22121 of previous timestep:\nzt\u22121 = \u221a\u03b1t\u22121\n\u0012zt \u2212 \u221a1 \u2212 \u03b1t\u03f5\u03b8(zt, t)\n\u221a\u03b1t\n\u0013\n|\n{z\n}\n\u201c predicted z0\u201d\n+\np\n| 1 \u2212 \u03b1t\u22121 \u00b7 \u03f5\u03b8(zt, t)\n{z\n}\n\u201cdirection pointing to zt\u201d\n,\n(4)\nwhere \u03b1t = Qt\ni=1(1 \u2212 \u03b2i). We use zt\u21920 to represent \u201cpredicted z0\u201d at timestep t for simplicity.\nNote that we use Stable Diffusion (SD) \u03f5\u03b8(zt, t, \u03c4) as our base model, which is an instantiation of\ntext-guided LDMs pre-trained on billions of image-text pairs. \u03c4 denotes the text prompt.\nControlNet [43] enables SD to support more controllable input conditions during text-to-image\nsynthesis, e.g., depth maps, poses, edges, etc. The ControlNet uses the same U-Net [30] architec-\nture as SD and \ufb01netunes its weights to support task-speci\ufb01c conditions, converting \u03f5\u03b8(zt, t, \u03c4) to\n\u03f5\u03b8(zt, t, c, \u03c4), where c denotes additional conditions. To distinguish the U-Net architectures of SD\nand ControlNet, we denote the former as the main U-Net while the latter as the auxiliary U-Net.\n3\nControlVideo\nControllable text-to-video generation aims to produce a video of length N conditioned on motion\nsequences c = {ci}N\u22121\ni=0\nand a text prompt \u03c4. As illustrated in Fig. 2, we propose a training-\nfree framework termed ControlVideo towards consistent and ef\ufb01cient video generation. Firstly,\n3\nControlVideo is adapted from ControlNet by employing fully cross-frame interaction, which ensures\nthe appearance consistency with less quality degradation. Secondly, the interleaved-frame smoother\nde\ufb02ickers the whole video by interpolating alternate frames at sequential timesteps. Finally, the\nhierarchical sampler separately produces short clips with the holistic coherency to enable long video\nsynthesis.\nFully cross-frame interaction.\nThe main challenge of adapting text-to-image models to the video\ncounterpart is to ensure temporal consistency. Leveraging the controllability of ControlNet, motion\nsequences could provide coarse-level consistency in structure. Nonetheless, even using the same\ninitial noise, individually producing all frames with ControlNet will lead to drastic inconsistency in\nappearance (see row 2 in Fig. 6). To keep the video appearance coherent, we concatenate all video\nframes to become a \u201clarge image\u201d, so that their content could be shared via inter-frame interaction.\nConsidering that self-attention in SD is driven by appearance similarities [40], we propose to enhance\nthe holistic coherency by adding attention-based fully cross-frame interaction.\nIn speci\ufb01c, ControlVideo in\ufb02ates the main U-Net from Stable Diffusion along the temporal axis,\nwhile keeping the auxiliary U-Net from ControlNet. Analogous to [11,15,40], it directly converts 2D\nconvolution layers to 3D counterpart by replacing 3 \u00d7 3 kernels with 1 \u00d7 3 \u00d7 3 kernels. In Fig. 2\n(right), it extends self-attention by adding interaction across all frames:\nAttention(Q, K, V ) = Softmax(QKT\n\u221a\nd\n) \u00b7 V , where Q = W Qzt, K = W Kzt, V = W V zt,\n(5)\nwhere zt = {zi\nt}N\u22121\ni=0 denotes all latent frames at timestep t, while W Q, W K, and W V project zt\ninto query, key, and value, respectively.\nPrevious works [15,40] usually replace self-attention with sparser cross-frame mechanisms, e.g., all\nframes attend to the \ufb01rst frame only. Yet, these mechanisms will increase the discrepancy between the\nquery and key in self-attention modules, resulting in the degradation of video quality and consistency.\nIn comparison, our fully cross-frame mechanism combines all frames into a \u201clarge image\u201d, and has\na less generation gap with text-to-image models (see comparisons in Fig. 6). Moreover, with the\nef\ufb01cient implementation, the fully cross-frame attention only brings little memory and acceptable\ncomputational burden in short-video generation (< 16 frames).\ni-2\ni-1\ni\ni+1\ni+2\ni-2\ni-1\ni\ni+1\ni+2\ni-3\ni+3\nStep t\ni-2\ni-1\ni\ni+1\ni+2\n\ud835\udc65!\u2192#\n\"\ud835\udc65!\u2192#\ni-2\ni-1\ni\ni+1\ni+2\n\"\ud835\udc65!$%\u2192#\n\ud835\udc65!$%\u2192#\nStep t-1\nInterpolate\nCopy\nDDIM Denoising\nUnsmooth\nSmooth\n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\nFigure 3: Illustration of interleaved-frame smoother.\nAt timestep t, predicted RGB frames xt\u21920 are\nsmoothed into \u02dcxt\u21920 via middle-frame interpolation.\nThe combination of two sequential timesteps reduces\nthe structural \ufb02ickers over the entire video.\nInterleaved-frame\nsmoother.\nAlbeit\nthe videos produced by fully cross-frame\ninteraction are promisingly consistent in\nappearance, they are still visibly \ufb02ickering\nin structure.\nInput motion sequences\nonly ensure the synthesized videos with\ncoarse-level structural consistency, but\nnot enough to keep the smooth transition\nbetween consecutive frames. Therefore,\nwe further propose an interleaved-frame\nsmoother to mitigate the \ufb02icker effect in\nstructure. As shown in Fig. 3, our key\nidea is to smooth each three-frame clip by\ninterpolating the middle frame, following\nby repeating it in an interleaved manner to\nsmooth the whole video.\nSpeci\ufb01cally,\nour\ninterleaved-frame\nsmoother is performed on predicted RGB\nframes at sequential timesteps.\nThe\noperation at each timestep interpolates\nthe even or odd frames to smooth their\ncorresponding three-frame clips. In this\nway, the smoothed three-frame clips from two consecutive timesteps are overlapped together to\nde\ufb02icker the entire video. Before applying our interleaved-frame smoother at timestep t, we \ufb01rst\npredict the clean video latent zt\u21920 according to zt:\nzt\u21920 = zt \u2212 \u221a1 \u2212 \u03b1t\u03f5\u03b8(zt, t, c, \u03c4)\n\u221a\u03b1t\n.\n(6)\n4\nA curious golden dog curiously wanders \non the rocky mountain trail.\nA daring man is scaling a treacherous and \njagged peak in the alpine wilderness.\nSource \nVideos\nStructure\nConditions\nTune-A-Video\nText2Video-Zero\nControlVideo\n(Ours)\n(a) Depth Maps\n(b) Canny Edges\nFigure 4: Qualitative comparisons conditioned on depth maps and canny edges. Our Con-\ntrolVideo produces videos with better (a) appearance consistency and (b) video quality than others. In\ncontrast, Tune-A-Video [40] fails to inherit structures from source videos, while Text2Video-Zero [15]\nbrings visible artifacts in large motion videos. Results best seen at 500% zoom.\nAfter projecting zt\u21920 into a RGB video xt\u21920 = D(zt\u21920), we convert it to a more smoothed video\n\u02dcxt\u21920 using our interleaved-frame smoother. Based on smoothed video latent \u02dczt\u21920 = E(\u02dcxt\u21920), we\ncompute the less noisy latent zt\u22121 following DDIM denoising in Eq. 4:\nzt\u22121 = \u221a\u03b1t\u22121 \u02dczt\u21920 +\np\n1 \u2212 \u03b1t\u22121 \u00b7 \u03f5\u03b8(zt, t, c, \u03c4).\n(7)\nNotably, the above process is only performed at the selected intermediate timesteps, which has two\nadvantages: (i) the newly computational burden can be negligible and (ii) the individuality and quality\nof interpolated frames are well retained by the following denoising steps.\nHierarchical sampler.\nSince video diffusion models need to maintain the temporal consistency\nwith inter-frame interaction, they often require substantial GPU memory and computational resources,\nespecially when producing longer videos. To facilitate ef\ufb01cient and consistent long-video synthesis,\nwe introduce a hierarchical sampler to produce long videos in a clip-by-clip manner. At each\ntimestep, a long video zt = {zi\nt}N\u22121\ni=0 is separated into multiple short video clips with the selected\nkey frames zkey\nt\n= {zkNc\nt\n}\nN\nNc\nk=0, where each clip is of length Nc \u2212 1 and the kth clip is denoted as\nbzk\nt = {zj\nt }(k+1)Nc\u22121\nj=kNc+1 . Then, we pre-generate the key frames with fully cross-frame attention for\nlong-range coherence, and their query, key, and value are computed as:\nQkey = W Qzkey\nt\n, Kkey = W Kzkey\nt\n, V key = W V zkey\nt\n.\n(8)\nConditioned on each pair of key frames, we sequentially synthesize their corresponding clips holding\nthe holistic consistency:\nb\nQk = W Qbzk\nt ,\nc\nKk = W K[zkNc\nt\n, z(k+1)Nc\nt\n],\nbV k = W V [zkNc\nt\n, z(k+1)Nc\nt\n].\n(9)\n5\nTable 1: Quantitative comparisons of ControlVideo with other methods. We evaluate them on 125\nmotion-prompt pairs in terms of consistency, and the best results are bolded.\nMethod\nStructure Condition\nFrame Consistency (%)\nPrompt Consistency (%)\nTune-A-Video [40]\nDDIM Inversion [35]\n94.53\n31.57\nText2Video-Zero [15]\nCanny Edge\n95.17\n30.74\nControlVideo\nCanny Edge\n96.83\n30.75\nText2Video-Zero [15]\nDepth Map\n95.99\n31.69\nControlVideo\nDepth Map\n97.22\n31.81\n4\nExperiments\n4.1\nExperimental Settings\nImplementation details.\nOur ControlVideo is adapted from ControlNet * [43] , and our interleaved-\nframe smoother employs a lightweight RIFE [13] to interpolate the middle frame of each three-frame\nclip. The synthesized short videos are of length 15, while the long videos usually contain about 100\nframes. Unless otherwise noted, their resolution is both 512 \u00d7 512. During sampling, we adopt\nDDIM sampling [35] with 50 timesteps, and interleaved-frame smoother is performed on predicted\nRGB frames at timesteps {30, 31} by default. With the ef\ufb01cient implementation of xFormers [17],\nour ControVideo could produce both short and long videos with one NVIDIA RTX 2080Ti in about 2\nand 10 minutes, respectively.\nDatasets.\nTo evaluate our ControlVideo, we collect 25 object-centric videos from DAVIS\ndataset [24] and manually annotate their source descriptions. Then, for each source description,\nChatGPT [23] is utilized to generate \ufb01ve editing prompts automatically, resulting in 125 video-prompt\npairs in total. Finally, we employ Canny and MiDaS DPT-Hybrid model [28] to estimate the edges\nand depth maps of source videos, and form 125 motion-prompt pairs as our evaluation dataset. More\ndetails are provided in the supplementary materials.\nMetrics.\nFollowing [5,40], we adopt CLIP [25] to evaluate the video quality from two perspectives.\n(i) Frame Consistency: the average cosine similarity between all pairs of consecutive frames, and (ii)\nPrompt Consistency: the average cosine similarity between input prompt and all video frames.\nBaselines.\nWe compare our ControlVideo with three publicly available methods: (i) Tune-A-\nVideo [40] extends Stable Diffusion to the video counterpart by \ufb01netuning it on a source video.\nDuring inference, it uses the DDIM inversion codes of source videos to provide structure guidance.\n(ii) Text2Video-Zero [15] is based on ControlNet, and employs the \ufb01rst-only cross-frame attention on\nStable Diffusion without \ufb01netuning. (iii) Follow-Your-Pose [18] is initialized with Stable Diffusion,\nand is \ufb01netuned on LAION-Pose [18] to support human pose conditions. After that, it is trained on\nmillions of videos [41] to enable temporally-consistent video generation.\n4.2\nQualitative and quantitative comparisons\nQualitative results.\nFig. 4 \ufb01rst illustrates the visual comparisons of synthesized videos conditioned\non both (a) depth maps and (b) canny edges. As shown in Fig. 4 (a), our ControlVideo demonstrates\nbetter consistency in both appearance and structure than alternative competitors. Tune-A-Video fails\nto keep the temporal consistency of both appearance and \ufb01ne-grained structure, e.g., the color of\ncoat and the structure of road. With the motion information from depth maps, Text2Video-\nZero achieves promising consistency in structure, but still struggles with incoherent appearance in\nvideos e.g., the color of coat. Besides, our ControlVideo also performs more robustly when\ndealing with large motion inputs. As illustrated in Fig. 4 (b), Tune-A-Video ignores the structure\ninformation from source videos. Text2Video-Zero adopts the \ufb01rst-only cross-frame mechanism to\ntrade off frame quality and appearance consistency, and generates later frames with visible artifacts.\nIn contrast, with the proposed fully cross-frame mechanism and interleaved-frame smoother, our\nControlVideo can handle large motion to generate high-quality and consistent videos.\n*https://huggingface.co/lllyasviel/ControlNet\n6\nTable 2: User preference study. The numbers denote the percentage of raters who favor the videos\nsynthesized by our ControlVideo over other methods.\nMethod Comparison\nVideo Quality\nTemporal Consistency\nText Alignment\nOurs vs. Tune-A-Video [40]\n73.6%\n83.2%\n68.0%\nOurs vs. Text2Video-Zero [15]\n76.0%\n81.6%\n65.6%\nSource \nVideos\nStructure\nConditions\nTune-A-Video\nControlVideo\n(Ours)\nText2Video-Zero\nFollow-Your-Pose\nIron man does the moonwalk \non the road.\nFigure 5: Qualitative comparisons on poses.\nTune-A-Video [40] only preserves original hu-\nman positions, while Text2Video-Zero [15] and\nFollow-Your-Pose [18] produce frames with ap-\npearance incoherence, e.g., changing faces of\niron man. Our ControlVideo achieves better con-\nsistency in both structure and appearance.\nCanny Edge\nIndividual\nFirst-only\nSparse-causal\nFully \nCross-frame\nFully \nCross-frame\n+ Smoother\nA mighty elephant marches steadily through \nthe rugged terrain.\nFigure 6:\nQualitative ablation studies on\ncross-frame mechanisms and interleaved-frame\nsmoother. Given canny edges in the \ufb01rst row,\nour fully cross-frame interaction produces video\nframes with higher quality and consistency than\nother mechanisms, and adding our smoother fur-\nther enhances the video smoothness.\nFig. 5 further shows the comparison conditioned on human poses. From Fig. 5, Tune-A-Video only\nmaintains the coarse structures of the source video, i.e., human position. Text2Video-Zero and\nFollow-Your-Pose produce video frames with inconsistent appearance, e.g., changing faces of\niron man (in row 4) or disappearing objects in the background (in row 5). In comparison,\nour ControlVideo performs more consistent video generation, demonstrating its superiority. More\nqualitative comparisons are provided in the supplementary materials.\nQuantitative results.\nWe have also compared our ControlVideo with existing methods quantita-\ntively on 125 video-prompt pairs. From Table 1, our ControlVideo conditioned on depth outperforms\nthe state-of-the-art methods in terms of frame consistency and prompt consistency, which is con-\nsistent with the qualitative results. In contrast, despite \ufb01netuning on a source video, Tune-A-Video\nstill struggles to produce temporally coherent videos. Although conditioned on the same structure\ninformation, Text2Video-Zero obtains worse frame consistency than our ControlVideo. For each\nmethod, the depth-conditioned models generate videos with higher temporal consistency and text\n\ufb01delity than the canny-condition counterpart, since depth maps provide smoother motion information.\n4.3\nUser study\nWe then perform the user study to compare our ControlVideo conditioned on depth maps with other\ncompeting methods. In speci\ufb01c, we provide each rater a structure sequence, a text prompt, and\nsynthesized videos from two different methods (in random order). Then we ask them to select the\nbetter synthesized videos for each of three measurements: (i) video quality, (ii) temporal consistency\n7\nTable 3: Quantitative ablation studies on cross-frame mechanisms and interleaved-frame smoother.\nThe results indicate that our fully cross-frame mechanism achieves better frame consistency than\nother mechanisms, and the interleaved-frame smoother signi\ufb01cantly improves the frame consistency.\nCross-Frame Mechanism\nFrame Consistency (%)\nPrompt Consistency (%)\nTime Cost (min)\nIndividual\n89.94\n30.79\n1.2\nFirst-only\n94.92\n30.54\n1.2\nSparse-Causal\n95.06\n30.59\n1.5\nFully\n95.36\n30.76\n3.0\nFully + Smoother\n96.83\n30.79\n3.5\nFrame 0\nFrame 11\nFrame 22\nFrame 33\nFrame 44\nFrame 55\nFrame 66\nFrame 77\nFrame 88\nFrame 99\nFrame 110\nFrame 121\na steamship on the ocean, at sunset, sketch style\nFigure 7: A long video produced with our hierarchical sampling. Motion sequences are shown\non the top left. Using the ef\ufb01cient sampler, our ControlVideo generates a high-quality long video\nwith the holistic consistency. Results best seen at 500% zoom.\nthroughout all frames, and (iii) text alignment between prompts and synthesized videos. The\nevaluation set consists of 125 representative structure-prompt pairs. Each pair is evaluated by 5\nraters, and we take a majority vote for the \ufb01nal result. From Table 2, the raters strongly favor our\nsynthesized videos from all three perspectives, especially in temporal consistency. On the other hand,\nTune-A-Video fails to generate consistent and high-quality videos with only DDIM inversion for\nstructural guidance, and Text2Video-Zero also produces videos with lower quality and coherency.\n4.4\nAblation study\nEffect of fully cross-frame interaction.\nTo demonstrate the effectiveness of the fully cross-frame\ninteraction, we conduct a comparison with the following variants: i) individual: no interaction\nbetween all frames, ii) \ufb01rst-only: all frames attend to the \ufb01rst one, iii) sparse-causal: each frame\nattends to the \ufb01rst and former frames, iv) fully: our fully cross-frame, refer to Sec. 3. Note that, all the\nabove models are extended from ControlNet without any \ufb01netuning. The qualitative and quantitative\nresults are shown in Fig. 6 and Table 3, respectively. From Fig. 6, the individual cross-frame\nmechanism suffers from severe temporal inconsistency, e.g., colorful and black-and-white\nframes. The \ufb01rst-only and sparse-causal mechanisms reduce some appearance inconsistency by\nadding cross-frame interaction. However, they still produce videos with structural inconsistency\nand visible artifacts, e.g., the orientation of the elephant and duplicate nose (row 3\nin Fig. 6). In contrast, due to less generation gap with ControlNet, our fully cross-frame interaction\nperforms better appearance coherency and video quality. Though the introduced interaction brings an\nextra 1 \u223c 2\u00d7 time cost, it is acceptable for a high-quality video generation.\nEffect of interleaved-frame smoother.\nWe further analyze the effect of the proposed interleaved-\nframe smoother. From Fig. 6 and Table 3, our interleaved-frame smoother greatly mitigates structural\n\ufb02ickers and improves the video smoothness.\n4.5\nExtension to long-video generation\nProducing a long video usually requires an advanced GPU with high memory. With the proposed\nhierarchical sampler, our ControlVideo achieves long video generation (more than 100 frames) in\na memory-ef\ufb01cient manner. As shown in Fig. 7, our ControlVideo can produce a long video with\nconsistently high quality. Notably, bene\ufb01ting from our ef\ufb01cient sampling, it only takes approximately\nten minutes to generate 100 frames with resolution 512 \u00d7 512 in one NVIDIA RTX 2080Ti. More\nvisualizations of long videos can be found in the supplementary materials.\n8\n5\nRelated work\nText-to-image synthesis.\nThrough pre-training on billions of image-text pairs, large-scale gen-\nerative models [1, 2, 3, 4, 14, 22, 26, 27, 29, 32, 33, 42] have made remarkable progress in creative\nand photo-realistic image generation. Various frameworks have been explored to enhance image\nquality, including GANs [7, 14, 33], autoregressive models [2, 3, 4, 22, 42], and diffusion mod-\nels [1,10,26,29,32]. Among these generative models, diffusion-based models are well open-sourced\nand popularly applied to several downstream tasks, such as image editing [8, 19] and customized\ngeneration [6, 16, 31, 37]. Besides text prompts, several works [20, 43] also introduce additional\nstructure conditions to pre-trained text-to-image diffusion models for controllable text-to-image\ngeneration. Our ControlVideo is implemented based on the controllable text-to-image models to\ninherit their ability of high-quality and consistent generation.\nText-to-video synthesis.\nLarge text-to-video generative models usually extend text-to-image mod-\nels by adding temporal consistency. Earlier works [12,36,38,39] adopt an autoregressive framework\nto synthesize videos according to given descriptions. Capitalizing on the success of diffusion models\nin image generation, recent works [9,11,34] propose to leverage their potential to produce high-quality\nvideos. Nevertheless, training such large-scale video generative models requires extensive video-text\npairs and computational resources.\nTo reduce the training burden, Gen-1 [5] and Follow-Your-Pose [18] provide coarse temporal infor-\nmation (e.g., motion sequences) for video generation, yet are still costly for most researchers and\nusers. By replacing self-attention with the sparser cross-frame mechanisms, Tune-A-Video [40] and\nText2Video-Zero [15] keep considerable consistency in appearance with little \ufb01netuning. Our Con-\ntrolVideo also adapts controllable text-to-image diffusion models without any training, but generates\nvideos with better coherency in both structure and appearance.\n6\nDiscussion\nIn this paper, we present a training-free framework, namely ControlVideo, towards consistent and\nef\ufb01cient controllable text-to-video generation. Particularly, ControlVideo is in\ufb02ated from ControlNet\nby adding fully cross-frame interaction to ensure appearance coherence without sacri\ufb01cing video\nquality. Besides, interleaved-frame smoother interpolates alternate frames at sequential timesteps to\neffectively reduce structural \ufb02ickers. With the further introduced hierarchical sampler and memory-\nef\ufb01cient designs, our ControlVideo can generate both short and long videos in several minutes\nwith commodity GPUs. Quantitative and qualitative experiments on extensive motion-prompt pairs\ndemonstrate that ControlVideo performs better than previous state-of-the-arts in terms of video\nquality and temporal consistency.\nLimitations.\nWhile our ControlVideo enables consistent and high-quality video generation, it still\nstruggles with producing videos beyond input motion sequences. For example, given sequential poses\nof Michael Jackson\u2019s moonwalk, it is dif\ufb01cult to generate a vivid video according to text prompts\nlike Iron man runs on the street. In the future, we will explore how motion sequences can be\nadapted to new ones based on input text prompts, so that users can create more diverse videos with\nour ControlVideo.\nBroader impact.\nLarge-scale diffusion models have made tremendous progress in text-to-video\nsynthesis, yet these models are costly and unavailable to the public. Our ControlVideo focuses\non training-free controllable text-to-video generation, and takes an essential step in ef\ufb01cient video\ncreation. Concretely, ControlVideo could synthesize high-quality videos with commodity hardware,\nhence, being accessible to most researchers and users. For example, artists may leverage our approach\nto create fascinating videos with less time. Moreover, ControlVideo provides insights into the tasks\ninvolved in videoss, e.g., video rendering, video editing, and video-to-video translation. On the \ufb02ip\nside, albeit we do not intend to use our model for harmful purposes, it might be misused and bring\nsome potential negative impacts, such as producing deceptive, harmful, or explicit videos. Despite\nthe above concerns, we believe that they could be well minimized with some steps. For example,\nan NSFW \ufb01lter can be employed to \ufb01lter out unhealthy and violent content. Also, we hope that the\ngovernment could establish and improve relevant regulations to restrict the abuse of video creation.\n9\nReferences\n[1] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala,\nTimo Aila, Samuli Laine, Bryan Catanzaro, et al. edif\ufb01: Text-to-image diffusion models with an ensemble\nof expert denoisers. arXiv preprint arXiv:2211.01324, 2022. 1, 9\n[2] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang,\nKevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked\ngenerative transformers. arXiv preprint arXiv:2301.00704, 2023. 9\n[3] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou\nShao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. NeurIPS, 2021.\n9\n[4] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-to-image generation\nvia hierarchical transformers. arXiv preprint arXiv:2204.14217, 2022. 9\n[5] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis.\nStructure and content-guided video synthesis with diffusion models. arXiv preprint arXiv:2302.03011,\n2023. 1, 6, 9\n[6] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel\nCohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion.\narXiv preprint arXiv:2208.01618, 2022. 1, 9\n[7] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 2020. 9\n[8] Amir Hertz, Ron Mokady, Jay Tenenbaum, K\ufb01r Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-\nprompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. 1, 9\n[9] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P\nKingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High de\ufb01nition video\ngeneration with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 1, 9\n[10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 2020. 2, 3,\n9\n[11] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet.\nVideo diffusion models. arXiv preprint arXiv:2204.03458, 2022. 1, 4, 9\n[12] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for\ntext-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 1, 9\n[13] Zhewei Huang, Tianyuan Zhang, Wen Heng, Boxin Shi, and Shuchang Zhou. Real-time intermediate \ufb02ow\nestimation for video frame interpolation. In ECCV, 2022. 6\n[14] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park.\nScaling up gans for text-to-image synthesis. In CVPR, 2023. 9\n[15] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang,\nShant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models are zero-shot\nvideo generators. arXiv preprint arXiv:2303.13439, 2023. 1, 2, 4, 5, 6, 7, 9\n[16] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept\ncustomization of text-to-image diffusion. arXiv preprint arXiv:2212.04488, 2022. 9\n[17] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren,\nMin Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel Haziza. xformers: A modular\nand hackable transformer modelling library. https://github.com/facebookresearch/xformers,\n2022. 2, 6\n[18] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Ying Shan, Xiu Li, and Qifeng Chen. Follow your\npose: Pose-guided text-to-video generation using pose-free videos. arXiv preprint arXiv:2304.01186, 2023.\n6, 7, 9\n[19] Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Image\nsynthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. 9\n[20] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-\nadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv\npreprint arXiv:2302.08453, 2023. 9\n[21] Minheng Ni, Zitong Huang, Kailai Feng, and Wangmeng Zuo. Imaginarynet: Learning object detectors\nwithout real images and annotations. arXiv preprint arXiv:2210.06886, 2022. 1\n[22] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya\nSutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided\ndiffusion models. arXiv preprint arXiv:2112.10741, 2021. 1, 9\n[23] TB OpenAI. Chatgpt: Optimizing language models for dialogue. OpenAI, 2022. 6\n[24] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbel\u00e1ez, Alex Sorkine-Hornung, and Luc\nVan Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675,\n2017. 6\n10\n[25] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In ICML, 2021. 6\n[26] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional\nimage generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 1, 9\n[27] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and\nIlya Sutskever. Zero-shot text-to-image generation. In ICML, 2021. 9\n[28] Ren\u00e9 Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust\nmonocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. TPAMI, 2020. 6\n[29] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution\nimage synthesis with latent diffusion models. In CVPR, 2022. 1, 2, 9\n[30] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical\nimage segmentation. In MICCAI, 2015. 3\n[31] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and K\ufb01r Aberman.\nDreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint\narXiv:2208.12242, 2022. 9\n[32] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-\nimage diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022. 1,\n9\n[33] Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila. Stylegan-t: Unlocking the power\nof gans for fast large-scale text-to-image synthesis. arXiv preprint arXiv:2301.09515, 2023. 9\n[34] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang,\nOron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv\npreprint arXiv:2209.14792, 2022. 1, 9\n[35] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint\narXiv:2010.02502, 2020. 1, 6\n[36] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Moham-\nmad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video\ngeneration from open domain textual description. arXiv preprint arXiv:2210.02399, 2022. 9\n[37] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encod-\ning visual concepts into textual embeddings for customized text-to-image generation. arXiv preprint\narXiv:2302.13848, 2023. 1, 9\n[38] Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, and Nan Duan.\nGodiva: Generating open-domain videos from natural descriptions. arXiv preprint arXiv:2104.14806,\n2021. 9\n[39] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, and Nan Duan. N\u00fcwa: Visual\nsynthesis pre-training for neural visual world creation. In ECCV, 2022. 9\n[40] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video\ngeneration. arXiv preprint arXiv:2212.11565, 2022. 1, 2, 4, 5, 6, 7, 9\n[41] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and Baining\nGuo. Advancing high-resolution video-language representation with large-scale video transcriptions. In\nCVPR, 2022. 6\n[42] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,\nAlexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich\ntext-to-image generation. arXiv preprint arXiv:2206.10789, 2022. 9\n[43] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. arXiv\npreprint arXiv:2302.05543, 2023. 1, 2, 3, 6, 9\n11\n"
  },
  {
    "title": "Training Diffusion Models with Reinforcement Learning",
    "link": "https://arxiv.org/pdf/2305.13301.pdf",
    "upvote": "2",
    "text": "TRAINING DIFFUSION MODELS\nWITH REINFORCEMENT LEARNING\nKevin Black\u2217 1\nMichael Janner\u2217 1\nYilun Du2\nIlya Kostrikov1\nSergey Levine1\n1 University of California, Berkeley\n2 Massachusetts Institute of Technology\n{kvablack, janner, kostrikov, sergey.levine}@berkeley.edu\nyilundu@mit.edu\nABSTRACT\nDiffusion models are a class of flexible generative models trained with an\napproximation to the log-likelihood objective. However, most use cases of diffusion\nmodels are not concerned with likelihoods, but instead with downstream objectives\nsuch as human-perceived image quality or drug effectiveness. In this paper, we\ninvestigate reinforcement learning methods for directly optimizing diffusion models\nfor such objectives. We describe how posing denoising as a multi-step decision-\nmaking problem enables a class of policy gradient algorithms, which we refer\nto as denoising diffusion policy optimization (DDPO), that are more effective\nthan alternative reward-weighted likelihood approaches. Empirically, DDPO can\nadapt text-to-image diffusion models to objectives that are difficult to express via\nprompting, such as image compressibility, and those derived from human feedback,\nsuch as aesthetic quality. Finally, we show that DDPO can improve prompt-image\nalignment using feedback from a vision-language model without the need for\nadditional data collection or human annotation. The project\u2019s website can be found\nat http://rl-diffusion.github.io.\n1\nINTRODUCTION\nDiffusion probabilistic models (Sohl-Dickstein et al., 2015) have recently emerged as the de facto\nstandard for generative modeling in continuous domains. Their flexibility in representing complex,\nhigh-dimensional distributions has led to the adoption of diffusion models in applications including\nimage and video synthesis (Ramesh et al., 2021; Saharia et al., 2022; Ho et al., 2022), drug and\nmaterial design (Xu et al., 2021; Xie et al., 2021; Schneuing et al., 2022), and continuous control\n(Janner et al., 2022; Wang et al., 2022; Hansen-Estruch et al., 2023). The key idea behind diffusion\nmodels is to iteratively transform a simple prior distribution into a target distribution by applying a\nsequential denoising process. This procedure is conventionally motivated as a maximum likelihood\nestimation problem, with the objective derived as a variational lower bound on the log-likelihood of\nthe training data.\nHowever, most use cases of diffusion models are not directly concerned with likelihoods, but instead\nwith downstream objective such as human-perceived image quality or drug effectiveness. In this paper,\nwe consider the problem of training diffusion models to satisfy such objectives directly, as opposed to\nmatching a data distribution. This problem is challenging because exact likelihood computation with\ndiffusion models is intractable, making it difficult to apply many conventional reinforcement learning\n(RL) algorithms. We instead propose to frame denoising as a multi-step decision-making task, using\nthe exact likelihoods at each denoising step in place of the approximate likelihoods induced by a full\ndenoising process. We present a policy gradient algorithm, which we refer to as denoising diffusion\npolicy optimization (DDPO), that can optimize a diffusion model for downstream tasks using only a\nblack-box reward function.\nWe apply our algorithm to the finetuning of large text-to-image diffusion models. Our initial evaluation\nfocuses on tasks that are difficult to specify via prompting, such as image compressibility, and those\nderived from human feedback, such as aesthetic quality. However, because many reward functions\nof interest are difficult to specify programmatically, finetuning procedures often rely on large-scale\nhuman labeling efforts to obtain a reward signal (Ouyang et al., 2022). In the case of text-to-image\ndiffusion, we propose a method for replacing such labeling with feedback from a vision-language\nmodel (VLM). Similar to RLAIF finetuning for language models (Bai et al., 2022b), the resulting\nprocedure allows for diffusion models to be adapted to reward functions that would otherwise require\n1\narXiv:2305.13301v4  [cs.LG]  4 Jan 2024\nCompressibility: llama\nAesthetic Quality: rabbit\nPrompt-Image Alignment: a raccoon washing dishes\nFigure 1 (Reinforcement learning for diffusion models) We propose a reinforcement learning\nalgorithm, DDPO, for optimizing diffusion models on downstream objectives such as compressibility,\naesthetic quality, and prompt-image alignment as determined by vision-language models. Each row\nshows a progression of samples for the same prompt and random seed over the course of training.\nadditional human annotations. We use this procedure to improve prompt-image alignment for unusual\nsubject-setting compositions.\nOur contributions are as follows. We first present the derivation and conceptual motivation of DDPO.\nWe then document the design of various reward functions for text-to-image generation, ranging\nfrom simple computations to workflows involving large VLMs, and demonstrate the effectiveness of\nDDPO compared to alternative reward-weighted likelihood methods in these settings. Finally, we\ndemonstrate the generalization ability of our finetuning procedure to unseen prompts.\n2\nRELATED WORK\nDiffusion probabilistic models. Denoising diffusion models (Sohl-Dickstein et al., 2015; Ho\net al., 2020) have emerged as an effective class of generative models for modalities including\nimages (Ramesh et al., 2021; Saharia et al., 2022), videos (Ho et al., 2022; Singer et al., 2022), 3D\nshapes (Zhou et al., 2021; Zeng et al., 2022), and robotic trajectories (Janner et al., 2022; Ajay et al.,\n2022; Chi et al., 2023). While the denoising objective is conventionally derived as an approximation\nto likelihood, the training of diffusion models typically departs from maximum likelihood in several\nways (Ho et al., 2020). Modifying the objective to more strictly optimize likelihood (Nichol &\nDhariwal, 2021; Kingma et al., 2021) often leads to worsened image quality, as likelihood is not\na faithful proxy for visual quality. In this paper, we show how diffusion models can be optimized\ndirectly for downstream objectives.\nControllable generation with diffusion models. Recent progress in text-to-image diffusion mod-\nels (Ramesh et al., 2021; Saharia et al., 2022) has enabled fine-grained high-resolution image synthesis.\nTo further improve the controllability and quality of diffusion models, recent approaches have in-\nvestigated finetuning on limited user-provided data (Ruiz et al., 2022), optimizing text embeddings\nfor new concepts (Gal et al., 2022), composing models (Du et al., 2023; Liu et al., 2022), adapters\nfor additional input constraints (Zhang & Agrawala, 2023), and inference-time techniques such as\nclassifier (Dhariwal & Nichol, 2021) and classifier-free (Ho & Salimans, 2021) guidance.\n2\nReinforcement learning from human feedback. A number of works have studied using human\nfeedback to optimize models in settings such as simulated robotic control (Christiano et al., 2017),\ngame-playing (Knox & Stone, 2008), machine translation (Nguyen et al., 2017), citation retrieval\n(Menick et al., 2022), browsing-based question-answering (Nakano et al., 2021), summarization\n(Stiennon et al., 2020; Ziegler et al., 2019), instruction-following (Ouyang et al., 2022), and alignment\nwith specifications (Bai et al., 2022a). Recently, Lee et al. (2023) studied the alignment of text-to-\nimage diffusion models to human preferences using a method based on reward-weighted likelihood\nmaximization. In our comparisons, their method corresponds to one iteration of the reward-weighted\nregresion (RWR) method. Our results demonstrate that DDPO significantly outperforms even multiple\niterations of weighted likelihood maximization (RWR-style) optimization.\nDiffusion models as sequential decision-making processes. Although predating diffusion models,\nBachman & Precup (2015) similarly posed data generation as a sequential decision-making problem\nand used the resulting framework to apply reinforcement learning methods to image generation.\nMore recently, Fan & Lee (2023) introduced a policy gradient method for training diffusion models.\nHowever, this paper aimed to improve data distribution matching rather than optimizing downstream\nobjectives, and therefore the only reward function considered was a GAN-like discriminator. In\nconcurrent work to ours, DPOK (Fan et al., 2023) built upon Fan & Lee (2023) and Lee et al. (2023)\nto better align text-to-image diffusion models to human preferences using a policy gradient algorithm.\nLike Lee et al. (2023), DPOK only considers a single preference-based reward function (Xu et al.,\n2023); additionally, their work studies KL-regularization and primarily focuses on training a different\ndiffusion model for each prompt. In contrast, we train on many prompts at once (up to 398) and\ndemonstrate generalization to many more prompts outside of the training set. Furthermore, we study\nhow DDPO can be applied to multiple reward functions beyond those based on human feedback,\nincluding how rewards derived automatically from VLMs can improve prompt-image alignment. We\nprovide a direct comparison to DPOK in Appendix C.\n3\nPRELIMINARIES\nIn this section, we provide a brief background on diffusion models and the RL problem formulation.\n3.1\nDIFFUSION MODELS\nIn this work, we consider conditional diffusion probabilistic models (Sohl-Dickstein et al., 2015; Ho\net al., 2020), which represent a distribution p(x0|c) over a dataset of samples x0 and corresponding\ncontexts c. The distribution is modeled as the reverse of a Markovian forward process q(xt | xt\u22121),\nwhich iteratively adds noise to the data. Reversing the forward process can be accomplished by\ntraining a neural network \u00b5\u03b8(xt, c, t) with the following objective:\nLDDPM(\u03b8) = E(x0,c)\u223cp(x0,c), t\u223cU{0,T }, xt\u223cq(xt|x0)\n\u0002\n\u2225\u02dc\u00b5(x0, t) \u2212 \u00b5\u03b8(xt, c, t)\u22252\u0003\n(1)\nwhere \u02dc\u00b5 is the posterior mean of the forward process, a weighted average of x0 and xt. This objective\nis justified as maximizing a variational lower bound on the log-likelihood of the data (Ho et al., 2020).\nSampling from a diffusion model begins with drawing a random xT \u223c N(0, I) and following the\nreverse process p\u03b8(xt\u22121 | xt, c) to produce a trajectory {xT , xT \u22121, . . . , x0} ending with a sample\nx0. The sampling process depends not only on the predictor \u00b5\u03b8 but also the choice of sampler. Most\npopular samplers (Ho et al., 2020; Song et al., 2021) use an isotropic Gaussian reverse process with a\nfixed timestep-dependent variance:\np\u03b8(xt\u22121 | xt, c) = N(xt\u22121 | \u00b5\u03b8 (xt, c, t) , \u03c32\nt I).\n(2)\n3.2\nMARKOV DECISION PROCESSES AND REINFORCEMENT LEARNING\nA Markov decision process (MDP) is a formalization of sequential decision-making problems. An\nMDP is defined by a tuple (S, A, \u03c10, P, R), in which S is the state space, A is the action space, \u03c10\nis the distribution of initial states, P is the transition kernel, and R is the reward function. At each\ntimestep t, the agent observes a state st \u2208 S, takes an action at \u2208 A, receives a reward R(st, at),\nand transitions to a new state st+1 \u223c P(st+1 | st, at). An agent acts according to a policy \u03c0(a | s).\n3\nAs the agent acts in the MDP, it produces trajectories, which are sequences of states and actions\n\u03c4 = (s0, a0, s1, a1, . . . , sT , aT ). The reinforcement learning (RL) objective is for the agent to\nmaximize JRL(\u03c0), the expected cumulative reward over trajectories sampled from its policy:\nJRL(\u03c0) = E\u03c4\u223cp(\u03c4|\u03c0)\nh PT\nt=0 R(st, at)\ni\n.\n4\nREINFORCEMENT LEARNING TRAINING OF DIFFUSION MODELS\nWe now describe how RL algorithms can be used to train diffusion models. We present two classes\nof methods and show that each corresponds to a different mapping of the denoising process to the\nMDP framework.\n4.1\nPROBLEM STATEMENT\nWe assume a pre-existing diffusion model, which may be pretrained or randomly initialized. Assuming\na fixed sampler, the diffusion model induces a sample distribution p\u03b8(x0 | c). The denoising diffusion\nRL objective is to maximize a reward signal r defined on the samples and contexts:\nJDDRL(\u03b8) = Ec\u223cp(c), x0\u223cp\u03b8(x0|c) [r(x0, c)]\nfor some context distribution p(c) of our choosing.\n4.2\nREWARD-WEIGHTED REGRESSION\nTo optimize JDDRL with minimal changes to standard diffusion model training, we can use the\ndenoising loss LDDPM (Equation 1), but with training data x0 \u223c p\u03b8(x0 | c) and an added weighting\nthat depends on the reward r(x0, c). Lee et al. (2023) describe a single-round version of this procedure\nfor diffusion models, but in general this approach can be performed for multiple rounds of alternating\nsampling and training, leading to an online RL method. We refer to this general class of algorithms\nas reward-weighted regression (RWR) (Peters & Schaal, 2007).\nA standard weighting scheme uses exponentiated rewards to ensure nonnegativity,\nwRWR(x0, c) = 1\nZ exp\n\u0000\u03b2r(x0, c)\n\u0001\n,\nwhere \u03b2 is an inverse temperature and Z is a normalization constant. We also consider a simplified\nweighting scheme that uses binary weights,\nwsparse(x0, c) = 1\n\u0002\nr(x0, c) \u2265 C\n\u0003\n,\nwhere C is a reward threshold determining which samples are used for training. In supervised learning\nterms, this is equivalent to repeated filtered finetuning on training data coming from the model.\nWithin the RL formalism, the RWR procedure corresponds to the following one-step MDP:\ns \u225c c\na \u225c x0\n\u03c0(a | s) \u225c p\u03b8(x0 | c)\n\u03c10(s) \u225c p(c)\nR(s, a) \u225c r(x0, c)\nwith a transition kernel P that immediately leads to an absorbing termination state. Therefore,\nmaximizing JDDRL(\u03b8) is equivalent to maximizing the RL objective JRL(\u03c0) in this MDP.\nFrom RL literature, weighting a log-likelihood objective by wRWR is known to approximately max-\nimize JRL(\u03c0) subject to a KL divergence constraint on \u03c0 (Nair et al., 2020). However, LDDPM\n(Equation 1) does not involve an exact log-likelihood \u2014 it is instead derived as a variational bound on\nlog p\u03b8(x0 | c). Therefore, the RWR procedure applied to diffusion model training is not theoretically\njustified and only optimizes JDDRL very approximately.\n4.3\nDENOISING DIFFUSION POLICY OPTIMIZATION\nRWR relies on an approximate log-likelihood because it ignores the sequential nature of the denoising\nprocess, only using the final samples x0. In this section, we show how the denoising process can\nbe reframed as a multi-step MDP, allowing us to directly optimize JDDRL using policy gradient\n4\nestimators. This follows the derivation in Fan & Lee (2023), who prove an equivalence between their\nmethod and a policy gradient algorithm where the reward is a GAN-like discriminator. We present a\ngeneral framework with an arbitrary reward function, motivated by our desire to optimize arbitrary\ndownstream objectives (Section 5). We refer to this class of algorithms as denoising diffusion policy\noptimization (DDPO) and present two variants based on specific gradient estimators.\nDenoising as a multi-step MDP. We map the iterative denoising procedure to the following MDP:\nst \u225c (c, t, xt)\n\u03c0(at | st) \u225c p\u03b8(xt\u22121 | xt, c)\nP(st+1 | st, at) \u225c\n\u0000\u03b4c, \u03b4t\u22121, \u03b4xt\u22121\n\u0001\nat \u225c xt\u22121\n\u03c10(s0) \u225c\n\u0000p(c), \u03b4T , N(0, I)\n\u0001\nR(st, at) \u225c\n\u001ar(x0, c)\nif t = 0\n0\notherwise\nin which \u03b4y is the Dirac delta distribution with nonzero density only at y. Trajectories consist of T\ntimesteps, after which P leads to a termination state. The cumulative reward of each trajectory is\nequal to r(x0, c), so maximizing JDDRL(\u03b8) is equivalent to maximizing JRL(\u03c0) in this MDP.\nThe benefit of this formulation is that if we use a standard sampler with p\u03b8(xt\u22121 | xt, c) parameterized\nas in Equation 2, the policy \u03c0 becomes an isotropic Gaussian as opposed to the arbitrarily complicated\ndistribution p\u03b8(x0 | c) as it is in the RWR formulation. This simplification allows for the evaluation\nof exact log-likelihoods and their gradients with respect to the diffusion model parameters.\nPolicy gradient estimation. With access to likelihoods and likelihood gradients, we can make direct\nMonte Carlo estimates of \u2207\u03b8JDDRL. Like RWR, DDPO alternates collecting denoising trajectories\n{xT , xT \u22121, . . . , x0} via sampling and updating parameters via gradient descent.\nThe first variant of DDPO, which we call DDPOSF, uses the score function policy gradient estimator,\nalso known as the likelihood ratio method or REINFORCE (Williams, 1992; Mohamed et al., 2020):\n\u2207\u03b8JDDRL = E\n\"\nT\nX\nt=0\n\u2207\u03b8 log p\u03b8(xt\u22121 | xt, c) r(x0, c)\n#\n(DDPOSF)\nwhere the expectation is taken over denoising trajectories generated by the current parameters \u03b8.\nHowever, this estimator only allows for one step of optimization per round of data collection, as the\ngradient must be computed using data generated by the current parameters. To perform multiple steps\nof optimization, we may use an importance sampling estimator (Kakade & Langford, 2002):\n\u2207\u03b8JDDRL = E\n\"\nT\nX\nt=0\np\u03b8(xt\u22121 | xt, c)\np\u03b8old(xt\u22121 | xt, c) \u2207\u03b8 log p\u03b8(xt\u22121 | xt, c) r(x0, c)\n#\n(DDPOIS)\nwhere the expectation is taken over denoising trajectories generated by the parameters \u03b8old. This\nestimator becomes inaccurate if p\u03b8 deviates too far from p\u03b8old, which can be addressed using trust\nregions (Schulman et al., 2015) to constrain the size of the update. In practice, we implement the\ntrust region via clipping, as in proximal policy optimization (Schulman et al., 2017).\n5\nREWARD FUNCTIONS FOR TEXT-TO-IMAGE DIFFUSION\nIn this work, we evaluate our methods on text-to-image diffusion. Text-to-image diffusion serves\nas a valuable test environment for reinforcement learning due to the availability of large pretrained\nmodels and the versatility of using diverse and visually interesting reward functions. In this section,\nwe outline our selection of reward functions. We study a spectrum of reward functions of varying\ncomplexity, ranging from those that are straightforward to specify and evaluate to those that capture\nthe depth of real-world downstream tasks.\n5.1\nCOMPRESSIBILITY AND INCOMPRESSIBILITY\nThe capabilities of text-to-image diffusion models are limited by the co-occurrences of text and\nimages in their training distribution. For instance, images are rarely captioned with their file size,\nmaking it impossible to specify a desired file size via prompting. This limitation makes reward\nfunctions based on file size a convenient case study: they are simple to compute, but not controllable\nthrough the conventional methods of likelihood maximization and prompt engineering.\n5\n\u201ca monkey is...\u201d\nBERTScore\n\u201ca monkey washing dishes...\u201d\n\u201cwhat is happening \nin this image?\u201d\nLLaVA\nDiffusion\nModel\nsimilarity-based \nreward\nFigure 2 (VLM reward function) Illustration of the VLM-based reward function for prompt-image\nalignment. LLaVA (Liu et al., 2023) provides a short description of a generated image; the reward is\nthe similarity between this description and the original prompt as measured by BERTScore (Zhang\net al., 2020).\nWe fix the resolution of diffusion model samples at 512x512, such that the file size is determined\nsolely by the compressibility of the image. We define two tasks based on file size: compressibility,\nin which the file size of the image after JPEG compression is minimized, and incompressibility, in\nwhich the same measure is maximized.\n5.2\nAESTHETIC QUALITY\nTo capture a reward function that would be useful to a human user, we define a task based on\nperceived aesthetic quality. We use the LAION aesthetics predictor (Schuhmann, 2022), which\nis trained on 176,000 human image ratings. The predictor is implemented as a linear model on\ntop of CLIP embeddings (Radford et al., 2021). Annotations range between 1 and 10, with the\nhighest-rated images mostly containing artwork. Since the aesthetic quality predictor is trained on\nhuman judgments, this task constitutes reinforcement learning from human feedback (Ouyang et al.,\n2022; Christiano et al., 2017; Ziegler et al., 2019).\n5.3\nAUTOMATED PROMPT ALIGNMENT WITH VISION-LANGUAGE MODELS\nA very general-purpose reward function for training a text-to-image model is prompt-image alignment.\nHowever, specifying a reward that captures generic prompt alignment is difficult, conventionally\nrequiring large-scale human labeling efforts. We propose using an existing VLM to replace additional\nhuman annotation. This design is inspired by recent work on RLAIF (Bai et al., 2022b), in which\nlanguage models are improved using feedback from themselves.\nWe use LLaVA (Liu et al., 2023), a state-of-the-art VLM, to describe an image. The finetuning\nreward is the BERTScore (Zhang et al., 2020) recall metric, a measure of semantic similarity, using\nthe prompt as the reference sentence and the VLM description as the candidate sentence. Samples\nthat more faithfully include all of the details of the prompt receive higher rewards, to the extent that\nthose visual details are legible to the VLM.\nIn Figure 2, we show one simple question: \u201cwhat is happening in this image?\u201d. While this captures\nthe general task of prompt-image alignment, in principle any question could be used to specify\ncomplex or hard-to-define reward functions for a particular use case. One could even employ a\nlanguage model to automatically generate candidate questions and evaluate responses based on the\nprompt. This framework provides a flexible interface where the complexity of the reward function is\nonly limited by the capabilities of the vision and language models involved.\n6\nEXPERIMENTAL EVALUATION\nThe purpose of our experiments is to evaluate the effectiveness of RL algorithms for finetuning\ndiffusion models to align with a variety of user-specified objectives. After examining the viability of\nthe general approach, we focus on the following questions:\n1. How do variants of DDPO compare to RWR and to each other?\n2. Can VLMs allow for optimizing rewards that are difficult to specify manually?\n3. Do the effects of RL finetuning generalize to prompts not seen during finetuning?\n6\nPretrained\nAesthetic Quality\nCompressibility\nIncompressibility\nFigure 3 (DDPO samples) Qualitative depiction of the effects of RL finetuning on different reward\nfunctions. DDPO transforms naturalistic images into stylized artwork to maximize aesthetic quality,\nremoves background content and applies foreground smoothing to maximize compressibility, and\nadds high-frequency noise to maximize incompressibility.\nDDPOIS\nDDPOSF\nRWR\nRWRsparse\nFigure 4 (Finetuning effectiveness) The relative effectiveness of different RL algorithms on three\nreward functions. We find that the policy gradient variants, denoted DDPO, are more effective\noptimizers than both RWR variants.\n6.1\nALGORITHM COMPARISONS\nWe begin by evaluating all methods on the compressibility, incompressibility, and aesthetic quality\ntasks, as these tasks isolate the effectiveness of the RL approach from considerations relating to the\nVLM reward function. We use Stable Diffusion v1.4 (Rombach et al., 2022) as the base model for all\nexperiments. Compressibility and incompressibility prompts are sampled uniformly from all 398\nanimals in the ImageNet-1000 (Deng et al., 2009) categories. Aesthetic quality prompts are sampled\nuniformly from a smaller set of 45 common animals.\nAs shown qualitatively in Figure 3, DDPO is able to effectively adapt a pretrained model with only\nthe specification of a reward function and without any further data curation. The strategies found to\noptimize each reward are nontrivial; for example, to maximize LAION-predicted aesthetic quality,\nDDPO transforms a model that produces naturalistic images into one that produces artistic drawings.\nTo maximize compressibility, DDPO removes backgrounds and applies smoothing to what remains.\nTo maximize incompressibility, DDPO finds artifacts that are difficult for the JPEG compression\nalgorithm to encode, such as high-frequency noise and sharp edges. Samples from RWR are provided\nin Appendix G for comparison.\n7\na dolphin riding a bike\nan ant playing chess\na bear washing dishes\n10k\n20k\n30k\n40k\n50k\nReward Queries\n0.69\n0.72\n0.75\n0.78\n0.81\n0.84\nBERTScore\nPrompt Alignment\n. . . riding a bike\n. . . playing chess\n. . . washing dishes\nFigure 5 (Prompt alignment) (L) Progression of samples for the same prompt and random seed over\nthe course of training. The images become significantly more faithful to the prompt. The samples also\nadopt a cartoon-like style, which we hypothesize is because the prompts are more likely depicted as\nillustrations than realistic photographs in the pretraining distribution. (R) Quantitative improvement\nof prompt alignment. Each thick line is the average score for an activity, while the faint lines show\naverage scores for a few randomly selected individual prompts.\nWe provide a quantitative comparison of all methods in Figure 4. We plot the attained reward\nas a function of the number of queries to the reward function, as reward evaluation becomes the\nlimiting factor in many practical applications. DDPO shows a clear advantage over RWR on all\ntasks, demonstrating that formulating the denoising process as a multi-step MDP and estimating the\npolicy gradient directly is more effective than optimizing a reward-weighted variational bound on\nlog-likelihood. Within the DDPO class, the importance sampling estimator slightly outperforms the\nscore function estimator, likely due to the increased number of optimization steps. Within the RWR\nclass, the performance of weighting schemes is comparable, making the sparse weighting scheme\npreferable on these tasks due to its simplicity and reduced resource requirements.\n6.2\nAUTOMATED PROMPT ALIGNMENT\nWe next evaluate the ability of VLMs, in conjunction with DDPO, to automatically improve the\nimage-prompt alignment of the pretrained model without additional human labels. We focus on\nDDPOIS for this experiment, as we found it to be the most effective algorithm in Section 6.1. The\nprompts for this task all have the form \u201ca(n) [animal] [activity] \u201d, where the animal comes from\nthe same list of 45 common animals used in Section 6.1 and the activity is chosen from a list of 3\nactivities: \u201criding a bike\u201d, \u201cplaying chess\u201d, and \u201cwashing dishes\u201d.\nThe progression of finetuning is depicted in Figure 5. Qualitatively, the samples come to depict the\nprompts much more faithfully throughout the course of training. This trend is also reflected quanti-\ntatively, though is less salient as small changes in BERTScore can correspond to large differences\nin relevance (Zhang et al., 2020). It is important to note that some of the prompts in the finetuning\nset, such as \u201ca dolphin riding a bike\u201d, had zero success rate from the pretrained model; if trained in\nisolation, this prompt would be unlikely to ever improve because there would be no reward signal. It\nwas only via transferrable learning across prompts that these difficult prompts could improve.\nNearly all of the samples become more cartoon-like or artistic during finetuning. This was not\noptimized for directly. We hypothesize that this may be a function of the pretraining distribution (one\nwould expect depictions of animals doing everyday activities to be more commonly cartoon-like than\nphotorealistic) or of the reward function (perhaps LLaVA has an easier time recognizing the content\nof simple cartoon-like images).\n8\nPretrained\nAesthetic Quality (New Animals)\nAesthetic Quality (Non-Animals)\nPrompt Alignment (New Scenarios)\nFinetuned\nFigure 6 (Generalization) Finetuning on a limited set of animals generalizes to both new animals\nand non-animal everyday objects. The prompts for the rightmost two columns are \u201ca capybara\nwashing dishes\u201d and \u201ca duck taking an exam\u201d. A quantitative analysis is provided in Appendix F,\nand more samples are provided in Appendix G.\n6.3\nGENERALIZATION\nRL finetuning on large language models has been shown to produce interesting generalization\nproperties; for example, instruction finetuning almost entirely in English has been shown to improve\ncapabilities in other languages (Ouyang et al., 2022). It is difficult to reconcile this phenomenon\nwith our current understanding of generalization; it would a priori seem more likely for finetuning\nto have an effect only on the finetuning prompt set or distribution. In order to investigate the\nsame phenomenon with diffusion models, Figure 6 shows a set of DDPO-finetuned model samples\ncorresponding to prompts that were not seen during finetuning. In concordance with instruction-\nfollowing transfer in language modeling, we find that the effects of finetuning do generalize, even\nwith prompt distributions as narrow as 45 animals and 3 activities. We find evidence of generalization\nto animals outside of the training distribution, to non-animal everyday objects, and in the case of\nprompt-image alignment, even to novel activities such as \u201ctaking an exam\u201d.\n7\nDISCUSSION AND LIMITATIONS\nWe presented an RL-based framework for training denoising diffusion models to directly optimize\na variety of reward functions. By posing the iterative denoising procedure as a multi-step decision-\nmaking problem, we were able to design a class of policy gradient algorithms that are highly effective\nat training diffusion models. We found that DDPO was an effective optimizer for tasks that are difficult\nto specify via prompts, such as image compressibility, and difficult to evaluate programmatically,\nsuch as semantic alignment with prompts. To provide an automated way to derive rewards, we also\nproposed a method for using VLMs to provide feedback on the quality of generated images. While our\nevaluation considers a variety of prompts, the full range of images in our experiments was constrained\n(e.g., animals performing activities). Future iterations could expand both the questions posed to the\nVLM, possibly using language models to propose relevant questions based on the prompt, as well as\nthe diversity of the prompt distribution. We also chose not to study the problem of overoptimization,\na common issue with RL finetuning in which the model diverges too far from the original distribution\nto be useful (see Appendix A); we highlight this as an important area for future work. We hope\nthat this work will provide a step toward more targeted training of large generative models, where\noptimization via RL can produce models that are effective at achieving user-specified goals rather\nthan simply matching an entire data distribution.\nBroader Impacts. Generative models can be valuable productivity aids, but may also pose harm\nwhen used for disinformation, impersonation, or phishing. Our work aims to make diffusion models\nmore useful by enabling them to optimize user-specified objectives. This adaptation has beneficial\napplications, such as the generation of more understandable educational material, but may also be\nused maliciously, in ways that we do not outline here. Work on the reliable detection of synthetic\ncontent remains important to mitigate such harms from generative models.\n9\n8\nACKNOWLEDGEMENTS\nThis work was partially supported by the Office of Naval Research and computational resource\ndonations from Google via the TPU Research Cloud (TRC). Michael Janner was supported by a\nfellowship from the Open Philanthropy Project. Yilun Du and Kevin Black were supported by\nfellowships from the National Science Foundation.\nCODE REFERENCES\nWe used the following open-source libraries for this work: NumPy (Harris et al., 2020), JAX\n(Bradbury et al., 2018), Flax (Heek et al., 2023), optax (Babuschkin et al., 2020), h5py (Collette,\n2013), transformers (Wolf et al., 2020), and diffusers (von Platen et al., 2022).\nREFERENCES\nAnurag Ajay, Yilun Du, Abhi Gupta, Joshua Tenenbaum, Tommi Jaakkola, and Pulkit Agrawal. Is con-\nditional generative modeling all you need for decision-making? arXiv preprint arXiv:2211.15657,\n2022.\nIgor Babuschkin, Kate Baumli, Alison Bell, Surya Bhupatiraju, Jake Bruce, Peter Buchlovsky,\nDavid Budden, Trevor Cai, Aidan Clark, Ivo Danihelka, Antoine Dedieu, Claudio Fantacci,\nJonathan Godwin, Chris Jones, Ross Hemsley, Tom Hennigan, Matteo Hessel, Shaobo Hou, Steven\nKapturowski, Thomas Keck, Iurii Kemaev, Michael King, Markus Kunesch, Lena Martens, Hamza\nMerzic, Vladimir Mikulik, Tamara Norman, George Papamakarios, John Quan, Roman Ring,\nFrancisco Ruiz, Alvaro Sanchez, Rosalia Schneider, Eren Sezener, Stephen Spencer, Srivatsan\nSrinivasan, Wojciech Stokowiec, Luyu Wang, Guangyao Zhou, and Fabio Viola. The DeepMind\nJAX Ecosystem, 2020. URL http://github.com/deepmind.\nPhilip Bachman and Doina Precup. Data generation as sequential decision making. Advances in\nNeural Information Processing Systems, 28, 2015.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn\nDrain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson\nKernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez,\nTristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario\nAmodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan.\nTraining a helpful and harmless assistant with reinforcement learning from human feedback. arXiv\npreprint arXiv:2204.05862, 2022a.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,\nAnna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson,\nChristopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson,\nEthan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile\nLukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova\nDasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El\nShowk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan,\nTristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas\nJoseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional AI: Harmlessness from\nAI feedback. arXiv preprint arXiv:2212.08073, 2022b.\nArpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Jonas\nGeiping, and Tom Goldstein. Universal guidance for diffusion models. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 843\u2013852, 2023.\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal\nMaclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and\nQiao Zhang.\nJAX: composable transformations of Python+NumPy programs, 2018.\nURL\nhttp://github.com/google/jax.\n10\nCheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran\nSong. Diffusion Policy: Visuomotor Policy Learning via Action Diffusion. arXiv preprint\narXiv:2303.04137, 2023.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep\nreinforcement learning from human preferences. In Neural Information Processing Systems, 2017.\nAndrew Collette. Python and HDF5. O\u2019Reilly, 2013.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale\nhierarchical image database. In Conference on Computer Vision and Pattern Recognition, 2009.\nPrafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat GANs on image synthesis. In\nAdvances in Neural Information Processing Systems, 2021.\nYilun Du, Conor Durkan, Robin Strudel, Joshua B Tenenbaum, Sander Dieleman, Rob Fergus, Jascha\nSohl-Dickstein, Arnaud Doucet, and Will Grathwohl. Reduce, reuse, recycle: Compositional\ngeneration with energy-based diffusion models and mcmc. arXiv preprint arXiv:2302.11552, 2023.\nYan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep\nreinforcement learning for continuous control. In International conference on machine learning,\npp. 1329\u20131338. PMLR, 2016.\nYing Fan and Kangwook Lee. Optimizing ddpm sampling with shortcut fine-tuning. arXiv preprint\narXiv:2301.13362, 2023.\nYing Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel,\nMohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for\nfine-tuning text-to-image diffusion models. arXiv preprint arXiv:2305.16381, 2023.\nRinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel\nCohen-Or. An image is worth one word: Personalizing text-to-image generation using textual\ninversion. arXiv preprint arXiv:2208.01618, 2022.\nLeo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. arXiv\npreprint arXiv:2210.10760, 2022.\nGabriel Goh, Nick Cammarata \u2020, Chelsea Voss \u2020, Shan Carter, Michael Petrov, Ludwig Schubert,\nAlec Radford, and Chris Olah. Multimodal neurons in artificial neural networks. Distill, 2021.\nhttps://distill.pub/2021/multimodal-neurons.\nPhilippe Hansen-Estruch, Ilya Kostrikov, Michael Janner, Jakub Grudzien Kuba, and Sergey Levine.\nIDQL: Implicit q-learning as an actor-critic method with diffusion policies.\narXiv preprint\narXiv:2304.10573, 2023.\nCharles R. Harris, K. Jarrod Millman, St\u00e9fan J. van der Walt, Ralf Gommers, Pauli Virtanen, David\nCournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti\nPicus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fern\u00e1ndez\ndel R\u00edo, Mark Wiebe, Pearu Peterson, Pierre G\u00e9rard-Marchant, Kevin Sheppard, Tyler Reddy,\nWarren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming\nwith NumPy. Nature, 585(7825):357\u2013362, 2020.\nJonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas\nSteiner, and Marc van Zee. Flax: A neural network library and ecosystem for JAX, 2023. URL\nhttp://github.com/google/flax.\nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on\nDeep Generative Models and Downstream Applications, 2021.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in\nNeural Information Processing Systems, 2020.\nJonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P.\nKingma, Ben Poole, Mohammad Norouzi, David J. Fleet, and Tim Salimans. Imagen video: High\ndefinition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022.\n11\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen.\nLora: Low-rank adaptation of large language models.\narXiv preprint\narXiv:2106.09685, 2021.\nMichael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine. Planning with diffusion for\nflexible behavior synthesis. In International Conference on Machine Learning, 2022.\nSham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In\nProceedings of the Nineteenth International Conference on Machine Learning, pp. 267\u2013274, 2002.\nDiederik P Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. In\nNeural Information Processing Systems, 2021.\nW. Bradley Knox and Peter Stone. TAMER: Training an Agent Manually via Evaluative Reinforce-\nment. In International Conference on Development and Learning, 2008.\nKimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel,\nMohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human\nfeedback. arXiv preprint arXiv:2302.12192, 2023.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. 2023.\nNan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B Tenenbaum. Compositional visual\ngeneration with composable diffusion models. arXiv preprint arXiv:2206.01714, 2022.\nJacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia\nGlaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, and Nat McAleese. Teach-\ning language models to support answers with verified quotes. arXiv preprint arXiv:2203.11147,\n2022.\nShakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. Monte carlo gradient\nestimation in machine learning. The Journal of Machine Learning Research, 21(1):5183\u20135244,\n2020.\nAshvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online reinforcement\nlearning with offline datasets. arXiv preprint arXiv:2006.09359, 2020.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher\nHesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou,\nGretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt:\nBrowser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332,\n2021.\nKhanh Nguyen, Hal Daum\u00e9 III, and Jordan Boyd-Graber. Reinforcement learning for bandit neural\nmachine translation with simulated human feedback. In Empirical Methods in Natural Language\nProcessing, 2017.\nAlexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models.\nIn International Conference on Machine Learning, 2021.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton,\nLuke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and\nRyan Lowe. Training language models to follow instructions with human feedback. arXiv preprint\narXiv:2203.02155, 2022.\nXue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:\nSimple and scalable off-policy reinforcement learning. CoRR, abs/1910.00177, 2019. URL\nhttps://arxiv.org/abs/1910.00177.\nJan Peters and Stefan Schaal. Reinforcement learning by reward-weighted regression for operational\nspace control. In International Conference on Machine learning, 2007.\n12\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-\nwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya\nSutskever. Learning transferable visual models from natural language supervision. arXiv preprint\narXiv:2103.00020, 2021.\nAditya Ramesh, Mikhail Pavlov, Scott Gray Gabriel Goh, Chelsea Voss, Alec Radford, Mark Chen,\nand Ilya Sutskever. Zero-shot text-to-image generation. arXiv preprint arXiv:2102.12092, 2021.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-\nresolution image synthesis with latent diffusion models. In IEEE Conference on Computer Vision\nand Pattern Recognition, 2022.\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\nDreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv\npreprint arXiv:2208.12242, 2022.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed\nKamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim\nSalimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image\ndiffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022.\nArne Schneuing, Yuanqi Du, Arian Jamasb Charles Harris, Ilia Igashov, Weitao Du, Tom Blundell,\nPietro Li\u00f3, Carla Gomes, Michael Bronstein Max Welling, and Bruno Correia. Structure-based\ndrug design with equivariant diffusion models. arXiv preprint arXiv:2210.02303, 2022.\nChrisoph Schuhmann.\nLaion aesthetics, Aug 2022.\nURL\nhttps://laion.ai/blog/\nlaion-aesthetics/.\nJohn Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region\npolicy optimization. In International Conference on Machine Learning, 2015.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\nUriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry\nYang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video\ndata. arXiv preprint arXiv:2209.14792, 2022.\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. In International Conference on Machine Learning,\n2015.\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In Interna-\ntional Conference on Learning Representations, 2021. URL https://openreview.net/forum?\nid=St1giarCHLP.\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,\nDario Amodei, and Paul F Christiano. Learning to summarize with human feedback. In Neural\nInformation Processing Systems, 2020.\nRichard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour.\nPolicy gradi-\nent methods for reinforcement learning with function approximation.\nIn S. Solla, T. Leen,\nand K. M\u00fcller (eds.), Advances in Neural Information Processing Systems, volume 12. MIT\nPress, 1999.\nURL https://proceedings.neurips.cc/paper_files/paper/1999/file/\n464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf.\nPatrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul,\nMishig Davaadorj, and Thomas Wolf. Diffusers: State-of-the-art diffusion models.\nhttps:\n//github.com/huggingface/diffusers, 2022.\nZhendong Wang, Jonathan J Hunt, and Mingyuan Zhou. Diffusion policies as an expressive policy\nclass for offline reinforcement learning. arXiv preprint arXiv:2208.06193, 2022.\n13\nRonald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\nlearning. Reinforcement learning, pp. 5\u201332, 1992.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,\nPierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick\nvon Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger,\nMariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural\nlanguage processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing: System Demonstrations, pp. 38\u201345, Online, October 2020. Association for\nComputational Linguistics. URL https://www.aclweb.org/anthology/2020.emnlp-demos.\n6.\nTian Xie, Xiang Fu, Octavian-Eugen Ganea, Regina Barzilay, and Tommi S Jaakkola. Crystal\ndiffusion variational autoencoder for periodic material generation. In International Conference on\nLearning Representations, 2021.\nJiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong.\nImagereward: Learning and evaluating human preferences for text-to-image generation. arXiv\npreprint arXiv:2304.05977, 2023.\nMinkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, , and Jian Tang. GeoDiff: A\ngeometric diffusion model for molecular conformation generation. In International Conference on\nLearning Representations, 2021.\nXiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, and Karsten Kreis.\nLion: Latent point diffusion models for 3d shape generation. arXiv preprint arXiv:2210.06978,\n2022.\nLvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models,\n2023.\nTianyi Zhang, Varsha Kishore*, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. BERTScore:\nEvaluating text generation with BERT. In International Conference on Learning Representations,\n2020.\nLinqi Zhou, Yilun Du, and Jiajun Wu. 3d shape generation and completion through point-voxel\ndiffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.\n5826\u20135835, 2021.\nDaniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul\nChristiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv\npreprint arXiv:1909.08593, 2019.\n14\nAPPENDIX A\nOVEROPTIMIZATION\nIncompressibility\nDDPO\nDDPO\nRWR\nRWR\nCounting Animals\nFigure 7 (Reward model overoptimization) Examples of RL overoptimizing reward functions.\n(L) The diffusion model eventually loses all recognizable semantic content and produces noise when\noptimizing for incompressibility. (R) When optimized for prompts of the form \u201cn animals\u201d, the\ndiffusion model exploits the VLM with a typographic attack (Goh et al., 2021), writing text that is\ninterpreted as the specified number n instead of generating the correct number of animals.\nSection 6.1 highlights the optimization problem: given a reward function, how well can an RL\nalgorithm maximize that reward? However, finetuning on a reward function, especially a learned one,\nhas been observed to lead to reward overoptimization or exploitation (Gao et al., 2022) in which the\nmodel achieves high reward while moving too far away from the pretraining distribution to be useful.\nOur setting is no exception, and we provide two examples of reward exploitation in Figure 7.\nWhen optimizing the incompressibility objective, the model eventually stops producing semantically\nmeaningful content, degenerating into high-frequency noise. Similarly, we observed that LLaVA is\nsusceptible to typographic attacks (Goh et al., 2021). When optimizing for alignment with respect to\nprompts of the form \u201cn animals\u201d, DDPO exploited deficiencies in the VLM by instead generating\ntext loosely resembling the specified number: for example, \u201csixx ttutttas\u201d above a picture of eight\nturtles.\nThere is currently no general-purpose method for preventing overoptimization. One common strategy\nis to add a KL-regularization term to the reward (Ouyang et al., 2022; Stiennon et al., 2020); we refer\nthe reader to the concurrent work of Fan et al. (2023) for a study of KL-regularization in the context of\nfinetuning text-to-image diffusion models. However, Gao et al. (2022) suggest that existing solutions,\nincluding KL-regularization, may be empirically equivalent to early stopping. As a result, in this\nwork, we manually identified the last checkpoint before a model began to deteriorate for each method\nand used that as the reference for qualitative results. We highlight this problem as an important area\nfor future work.\nAPPENDIX B\nCOMPARISON TO CLASSIFIER GUIDANCE\nClassifier guidance (Dhariwal & Nichol, 2021) was originally introduced as a way to improve sample\nquality for conditional generation using the gradients from an image classifier. For a differentiable\nreward function such as the LAION aesthetics predictor (Schuhmann, 2022), one could naturally\nimagine an extension to classifier guidance that uses gradients from such a predictor to improve\naesthetic score. The issue is that classifier guidance uses gradients with respect to the noisy images in\nthe intermediate stages of the denoising process, which requires retraining the guidance network on\nMethod\nAesthetic Score\nBase model\n5.95 \u00b1 0.03\nUniversal guidance\n6.14 \u00b1 0.05\nDDPOIS @ 20k reward queries\n6.63 \u00b1 0.03\nTable 1 Comparison of DDPO with universal guidance using the LAION aesthetic predictor. We\nreport the mean and one standard error over 50 samples for the prompt \u201cwolf\u201d.\n15\nnoisy images. Universal guidance (Bansal et al., 2023) sidesteps this issue by applying the guidance\nnetwork to the fully denoised image predicted by the diffusion model at each step.\nWe compare DDPO with universal guidance in Table 1. We used the official implementation of\nuniversal guidance1 with the recommended hyperparameters for style transfer, substituting the\nguidance network with the LAION aesthetics predictor. While universal guidance is able to produce\na statistically significant improvement in aesthetic score, the change is small compared to DDPO.\nWe only report results averaged over 50 samples for a single prompt, since universal guidance is\nvery slow; on an NVIDIA A100 GPU, it takes almost 2 minutes to generate a single image, whereas\nstandard generation (e.g., from a DDPO-finetuned model) takes 4 seconds.\nAPPENDIX C\nCOMPARISON TO DPOK\nHere we directly compare our implementation of DDPO to the results reported in the DPOK paper\n(Fan et al., 2023), which was developed concurrently with this work. The key similarities and\ndifferences between our experimental setups are summarized below:\n\u2022 For this experiment only, we use Stable Diffusion v1-5 as the base model and train the UNet\nwith low-rank adaptation (LoRA; Hu et al. (2021)) in order to match DPOK.\n\u2022 Rather than matching the hyperparameters in DPOK, we use the same hyperparameters as\nin our other experiments (Appendix D.5) except for the learning rate which we increase to\n3e-4. We found that when using LoRA, a higher learning rate is necessary to get comparable\nperformance to full finetuning.\n\u2022 Like DPOK, we train on four prompts: \u201ca green colored rabbit\u201d (color), \u201cfour wolves in the\npark\u201d (count), \u201ca dog and a cat\u201d (composition), and \u201ca dog on the moon\u201d (location). Unlike\nDPOK, we train a single model for all four prompts.\n\u2022 Like DPOK, we train the model using ImageReward (Xu et al., 2023) as the reward function.\nWe evaluate the model using ImageReward and the LAION aesthetics predictor (Schuhmann,\n2022).\n\u2022 Unlike DPOK, we do not employ KL regularization.\n0\n5k\n10k\n15k\n20k\n25k\nReward Queries\n0.0\n0.5\n1.0\n1.5\n2.0\nImageReward Score\nImageReward\nColor\nCount\nComposition\nLocation\n0\n5k\n10k\n15k\n20k\n25k\nReward Queries\n5.2\n5.4\n5.6\n5.8\n6.0\n6.2\nLAION Aesthetic Score\nAesthetic Quality\nDDPOIS\nDPOK\nFigure 8 Comparison of DDPOIS with DPOK. We take the DPOK numbers directly from the paper,\nwhich only reports scores at one point in training (after 20k reward queries). Like in DPOK, scores\nare averaged over 50 samples for each prompt.\n1https://github.com/arpitbansal297/Universal-Guided-Diffusion\n16\nPretrained\nColor\nCount\nComposition\nLocation\nFinetuned\nFigure 9 Qualtitative examples of the results of ImageReward training on the DPOK prompts: \u201ca\ngreen colored rabbit\u201d (color), \u201cfour wolves in the park\u201d (count), \u201ca dog and a cat\u201d (composition), and\n\u201ca dog on the moon\u201d (location). The finetuned images are generated from a model trained for 20k\nreward queries.\nThe results are presented in Figure 8. Our implementation of DDPOIS outperforms DPOK accross\nthe board, without using KL regularization. Figure 8 also doubles as a quantitative study of overop-\ntimization (Appendix A), since the model is trained with one reward function (ImageReward) and\nevaluated with another (LAION aesthetic score). We find that significant overoptimization does\nbegin to happen within 25k reward queries for one of the prompts (count: \u201cfour wolves in the park\u201d),\nwhich is reflected by a drop in LAION aesthetic score. However, the overoptimization is not severe\nor unreasonably fast. We provide qualitative samples in Figure 9 showing that the model is able to\nproduce high-quality images at 20k reward queries.\nAPPENDIX D\nIMPLEMENTATION DETAILS\nFor all experiments, we use Stable Diffusion v1.4 (Rombach et al., 2022) as the base model and\nfinetune only the UNet weights while keeping the text encoder and autoencoder weights frozen.\nD.1\nDDPO IMPLEMENTATION\nWe collect 256 samples per training iteration. For DDPOSF, we accumulate gradients across all 256\nsamples and perform one gradient update. For DDPOIS, we split the samples into 4 minibatches and\nperform 4 gradient updates. Gradients are always accumulated across all denoising timesteps for\na single sample. For DDPOIS, we use the same clipped surrogate objective as in proximal policy\noptimization (Schulman et al., 2017), but find that we need to use a very small clip range compared\nto standard RL tasks. We use a clip range of 1e-4 for all experiments.\nD.2\nRWR IMPLEMENTATION\nWe compute the weights for a training iteration using the entire dataset of samples collected for that\ntraining iteration. For wRWR, the weights are computed using the softmax function. For wsparse, we\nuse a percentile-based threshold, meaning C is dynamically selected such that the bottom p% of a\ngiven pool of samples are discarded and the rest are used for training.\nD.3\nREWARD NORMALIZATION\nIn practice, rewards are rarely used as-is, but instead are normalized to have zero mean and unit\nvariance. Furthermore, this normalization can depend on the current state; in the policy gradient\ncontext, this is analogous to a value function baseline (Sutton et al., 1999), and in the RWR context,\nthis is analogous to advantage-weighted regression (Peng et al., 2019). In our experiments, we\nnormalize the rewards on a per-context basis. For DDPO, this is implemented as normalization by a\nrunning mean and standard deviation that is tracked for each prompt independently. For RWR, this is\nimplemented by computing the softmax over rewards for each prompt independently. For RWRsparse,\nthis is implemented by computing the percentile-based threshold C for each prompt independently.\n17\nD.4\nRESOURCE DETAILS\nRWR experiments were conducted on a v3-128 TPU pod, and took approximately 4 hours to reach\n50k samples. DDPO experiments were conducted on a v4-64 TPU pod, and took approximately 4\nhours to reach 50k samples. For the VLM-based reward function, LLaVA inference was conducted\non a DGX machine with 8 80Gb A100 GPUs.\nD.5\nFULL HYPERPARAMETERS\nDDPOIS\nDDPOSF\nRWR\nRWRsparse\nDiffusion\nDenoising steps (T)\n50\n50\n50\n50\nGuidance weight (w)\n5.0\n5.0\n5.0\n5.0\nOptimization\nOptimizer\nAdamW\nAdamW\nAdamW\nAdamW\nLearning rate\n1e-5\n1e-5\n1e-5\n1e-5\nWeight decay\n1e-4\n1e-4\n1e-4\n1e-4\n\u03b21\n0.9\n0.9\n0.9\n0.9\n\u03b22\n0.999\n0.999\n0.999\n0.999\n\u03f5\n1e-8\n1e-8\n1e-8\n1e-8\nGradient clip norm\n1.0\n1.0\n1.0\n1.0\nRWR\nInverse temperature (\u03b2)\n-\n-\n0.2\n-\nPercentile\n-\n-\n-\n0.9\nBatch size\n-\n-\n128\n128\nGradient updates per iteration\n-\n-\n400\n400\nSamples per iteration\n-\n-\n10k\n10k\nDDPO\nBatch size\n64\n256\n-\n-\nSamples per iteration\n256\n256\n-\n-\nGradient updates per iteration\n4\n1\n-\n-\nClip range\n1e-4\n-\n-\n-\nD.6\nLIST OF 45 COMMON ANIMALS\nThis list was used for experiments with the aesthetic quality reward function and the VLM-based\nreward function.\ncat\ndog\nhorse\nmonkey\nrabbit\nzebra\nspider\nbird\nsheep\ndeer\ncow\ngoat\nlion\ntiger\nbear\nraccoon\nfox\nwolf\nlizard\nbeetle\nant\nbutterfly\nfish\nshark\nwhale\ndolphin\nsquirrel\nmouse\nrat\nsnake\nturtle\nfrog\nchicken\nduck\ngoose\nbee\npig\nturkey\nfly\nllama\ncamel\nbat\ngorilla\nhedgehog\nkangaroo\nAPPENDIX E\nADDITIONAL DESIGN DECISIONS\nE.1\nCFG TRAINING\nRecent text-to-image diffusion models rely critically on classifier-free guidance (CFG) (Ho &\nSalimans, 2021) to produce perceptually high-quality results. CFG involves jointly training the\ndiffusion model on conditional and unconditional objectives by randomly masking out the context c\nduring training. The conditional and unconditional predictions are then mixed at sampling time using\na guidance weight w:\n\u02dc\u03f5\u03b8(xt, t, c) = w\u03f5\u03b8(xt, t, c) + (1 \u2212 w)\u03f5\u03b8(xt, t)\n(3)\nwhere \u03f5\u03b8 is the \u03f5-prediction parameterization of the diffusion model (Ho et al., 2020) and \u02dc\u03f5\u03b8 is the\nguided \u03f5-prediction that is used to compute the next denoised sample.\nFor reinforcement learning, it does not make sense to train on the unconditional objective since the\nreward may depend on the context. However, we found that when only training on the conditional\nobjective, performance rapidly deteriorated after the first round of finetuning. We hypothesized that\n18\nthis is due to the guidance weight becoming miscalibrated each time the model is updated, leading to\ndegraded samples, which in turn impair the next round of finetuning, and so on. Our solution was to\nchoose a fixed guidance weight and use the guided \u03f5-prediction during training as well as sampling.\nWe call this procedure CFG training. Figure 10 shows the effect of CFG training on RWRsparse; it has\nno effect after a single round of finetuning, but becomes essential for subsequent rounds.\n0\n10k\n20k\n30k\n40k\nReward Queries\n\u2212200\n\u2212150\n\u2212100\n\u221250\n0\nNegative Filesize (kb)\nJPEG Compressibility\nwith CFG training\nwithout CFG training\nFigure 10 (CFG training) We run the RWRsparse algorithm while optimizing only the conditional \u03f5-\nprediction (without CFG training), and while optimizing the guided \u03f5-prediction (with CFG training).\nEach point denotes a diffusion model update. We find that CFG training is essential for methods that\ndo more than one round of interleaved sampling and training.\nE.2\nINTERLEAVING\nThere are two main differences between DDPO and RWR, as compared in Section 6.1: the objective\n(DDPO uses the policy gradient) and the data distribution (DDPO is significantly more on-policy,\ncollecting 256 samples per iteration as opposed to 10,000 for RWR). This choice is motivated by\nstandard RL practice, in which policy gradient methods specifically require on-policy data (Sutton\net al., 1999), whereas RWR is designed to work in on off-policy data (Nair et al., 2020) and is known\nto underperform other algorithms in more online settings (Duan et al., 2016).\nHowever, we can isolate the effect of the data distribution by varying how interleaved the sampling\nand training are in RWR. At one extreme is a single-round algorithm (Lee et al., 2023), in which N\nsamples are collected from the pretrained model and used for finetuning. It is also possible to run\nk rounds of finetuning each on N\nk samples collected from the most up-to-date model. In Figure 11,\nwe evaluate this hyperparameter and find that increased interleaving does help up to a point, after\nwhich it causes performance degradation. However, RWR is still unable to match the asymptotic\nperformance of DDPO at any level of interleaving.\nAPPENDIX F\nQUANTITATIVE RESULTS FOR GENERALIZATION\nIn Section 6.3, we presented qualitative evidence of both the aesthetic quality model and the image-\nprompt alignment model generalizing to prompts that were unseen during finetuning. In Figure 12, we\nprovide an additional quantitative analysis of generalization with the aesthetic quality model, where\nwe measure the average reward throughout training for several prompt distributions. In accordance\nwith the qualitative evidence, we see that the model generalizes very well to unseen animals, and\neveryday objects to a lesser degree.\n19\n0\n10k\n20k\n30k\nReward Queries\n4.2\n4.4\n4.6\n4.8\n5.0\n5.2\nLAION Aesthetic Score\nAesthetic Quality\n0\n10k\n20k\n30k\nReward Queries\n\u2212150\n\u2212100\n\u221250\n0\nNegative Filesize (kb)\nJPEG Compressibility\nSamples per Iteration (RWR)\n256\n512\n1,024\n2,048\n4,096\n8,192\n16,384\nSamples per Iteration (DDPOIS)\n256\nFigure 11 (RWR interleaving ablation) Ablation over the number of samples collected per iteration\nfor RWR. The number of gradient updates per iteration remains the same throughout. We find that\nmore frequent interleaving is beneficial up to a point, after which it causes performance degradation.\nHowever, RWR is still unable to match the asymptotic performance of DDPO at any level of\ninterleaving.\n0\n10k\n20k\n30k\n40k\n50k\nReward Queries\n4.2\n4.4\n4.6\n4.8\n5.0\n5.2\nReward\nAesthetic Quality Quantitative Generalization\nFinetuning Prompts (Animals Only)\nUnseen Animals\nOrdinary Objects\nFigure 12 (Quantitative generalization) Reward curves demonstrating the generalization of the\naesthetic quality objective to prompts not seen during finetuning. The finetuning prompts are a list of\n45 common animals, \u201cunseen animals\u201d is a list of 38 additional animals, and \u201cordinary objects\u201d is a\nlist of 50 objects (e.g. toaster, chair, coffee cup, etc.).\n20\nPretrained\nAesthetic Quality\nCompressibility\nIncompressibility\nFigure 13 (RWR samples)\na hedgehog riding a bike\na dog riding a bike\na lizard riding a bike\na shark washing dishes\na frog washing dishes\na monkey washing dishes\nFigure 14 (More image-prompt alignment samples)\nAPPENDIX G\nMORE SAMPLES\nFigure 13 shows qualitative samples from the baseline RWR method. Figure 14 shows more samples\non seen prompts from DDPO finetuning with the image-prompt alignment reward function. Figure 15\nshows more examples of generalization to unseen animals and everyday objects with the aesthetic\nquality reward function. Figure 16 shows more examples of generalization to unseen subjects and\nactivities with the image-prompt alignment reward function.\n21\nPretrained (New Animals)\nAesthetic Quality (New Animals)\nPretrained (Non-Animals)\nAesthetic Quality (Non-Animals)\nFigure 15 (Aesthetic quality generalization)\n22\na capybara washing dishes\na snail playing chess\na dog doing laundry\na giraffe playing basketball\na parrot driving a car\na duck taking an exam\na robot fishing in a lake\na horse typing on a keyboard\na rabbit sewing clothes\na tree riding a bike\na car eating a sandwich\nan apple playing soccer\nFigure 16 (Image-prompt alignment generalization)\n23\n"
  },
  {
    "title": "AudioToken: Adaptation of Text-Conditioned Diffusion Models for Audio-to-Image Generation",
    "link": "https://arxiv.org/pdf/2305.13050.pdf",
    "upvote": "2",
    "text": "AUDIOTOKEN: Adaptation of Text-Conditioned Diffusion Models\nfor Audio-to-Image Generation\nGuy Yariv \u2665,\u2663, Itai Gat \u2666, Lior Wolf \u2660, Yossi Adi \u2665,\u2217, Idan Schwartz \u2660,\u2663,\u2217\n\u2665The Hebrew University of Jerusalem , \u2666Technion - Israel Institute of Technology\n\u2660Tel-Aviv University, \u2663NetApp\nAbstract\nIn recent years, image generation has shown a great leap in\nperformance, where diffusion models play a central role. Al-\nthough generating high-quality images, such models are mainly\nconditioned on textual descriptions.\nThis begs the question:\nhow can we adopt such models to be conditioned on other\nmodalities?. In this paper, we propose a novel method utiliz-\ning latent diffusion models trained for text-to-image-generation\nto generate images conditioned on audio recordings. Using a\npre-trained audio encoding model, the proposed method en-\ncodes audio into a new token, which can be considered as\nan adaptation layer between the audio and text representa-\ntions.\nSuch a modeling paradigm requires a small number\nof trainable parameters, making the proposed approach ap-\npealing for lightweight optimization. Results suggest the pro-\nposed method is superior to the evaluated baseline methods,\nconsidering objective and subjective metrics. Code and sam-\nples are available at: https://pages.cs.huji.ac.il/\nadiyoss-lab/AudioToken.\nIndex Terms: Diffusion models, Audio-to-image.\n1. Introduction\nNeural generative models have changed the way we consume\ndigital content. From generating high-quality images [1, 2, 3],\nthough coherence of long spans of text [4, 5, 6], up to speech\nand audio [7, 8, 9, 10]. In recent years, diffusion-based gener-\native models have emerged as the preferred approach, showing\npromising results on various tasks [11].\nDuring the diffusion process, the model learns to map a pre-\nde\ufb01ned noise distribution to the target data distribution. In every\nstep of the diffusion process, the model learns to predict the\nnoise at a given step to \ufb01nally generate the signal from the target\ndistribution [12, 13, 14]. Diffusion models operate on different\nforms of data representations, e.g., raw input [15, 12], latent\nrepresentations [16], etc.\nWhen considering controllable generative models, the com-\nmon practice these days is to condition the generation on a tex-\ntual description of the input data; this is especially noticeable\nin image generation [1, 17, 18]. Recently, several methods pro-\nposed using different modalities to condition the generative pro-\ncess such as image-to-audio [19, 20], image-to-speech [21, 22],\nimage-to-text [23, 24], or audio-to-audio [25, 26]. However,\nsuch research direction is less explored by the community.\nIn this work, we focus on the task of audio-to-image gen-\neration. Given an audio sample contains an arbitrary sound, we\naim to generate a high-quality image representing the acous-\ntic scene. We propose leveraging a pre-trained text-to-image\n*Equal Contribution.\nFigure 1: Generated images (right) and input spectrograms\n(left) from the proposed method. The model gets as input an\naudio recording, extracts a representation, and projects into a\ntextual latent space which will be fed into a pre-trained text-\nconditioned diffusion generative model.\ngeneration model together with a pre-trained audio representa-\ntion model to learn an adaptation layer mapping between their\noutputs and inputs. Speci\ufb01cally, inspired by recent work on\ntextual-inversions [27], we propose to learn a dedicated audio-\ntoken that maps the audio representations into an embedding\nvector. Such a vector is then forwarded into the network as a\ncontinuous representation, re\ufb02ecting a new word embedding.\nSeveral methods for generating audio from image inputs\nwere proposed in prior work. The authors in [28, 29] proposed\nto generate images based on audio recordings using a Gener-\native Adversarial Network (GAN) based method. Unlike the\nproposed method, in [28], the authors present results for gener-\nating MNIST digits only and did not generalize to general audio\nsounds. In [29], the authors did generate images from general\naudio. However, this turned into low-quality images. The most\nrelevant related work to ours is Wav2Clip [30], in which the\nauthors \ufb01rst learn a Contrastive Language-Image Pre-Training\n(CLIP) [31] like a model for audio-image pairs. Then, later on,\nsuch representation can be used to generate images using VQ-\nGAN [32] under the VQ-GAN CLIP [33] framework.\nWhy use audio signals as a conditioning to image gener-\nation rather than text? Although text-based generative mod-\nels can generate impressive images, textual descriptions are not\nnaturally paired with the image, i.e., textual descriptions are\noften added manually. On the other hand, when considering\nvideos, audio, and images capture and represent the same scene,\narXiv:2305.13050v1  [cs.SD]  22 May 2023\n\"A photo of a\"\nPre-trained\nAudio-Encoder\nText-Tokenizer\nText-Encoder\nGenerative\nModel\nMLP\nAtten-Pooling\nEmbedder\nFigure 2: Architecture overview: We forward an audio recording through a pre-trained audio encoder and then through an Embedder\nnetwork. A pre-trained text encoder extracts tokens created by a tokenizer and the audio token. Finally, the generative model is fed with\nthe concatenated tensor of representations. It is important to note that only the Embedder parameters are trained during this process.\nhence are naturally paired. Moreover, audio signals can repre-\nsent complex scenes and objects such as different types of the\nsame instrument (e.g., classic guitar, electric guitar, etc.), or dif-\nferent scenes of the same object (e.g., classic guitar recorded in\nstudio vs. live show). Annotating such \ufb01ne-grained details of\nthe different objects is labor-intensive, hence hard to scale.\nIn summary, our contributions are: We propose a novel\nmethod AUDIOTOKEN for audio-to-image generation by lever-\naging a pre-trained text-to-image diffusion model together with\na pre-trained audio encoder; We propose a set of evaluation\nmetrics speci\ufb01cally dedicated for the task of an audio-to-image\ngeneration. Through extensive experiments, we show that our\nmethod is able to generate high-quality and diverse set of im-\nages based on audio-scenes.\n2. Adaptation of text-conditioned models\nDiffusion models are a family of models that are prone to learn\nthe underlying probabilistic model of the data distribution p(x).\nThis is done by learning the reverse Markov process of length\nT. Given a timestamp t \u2208 [0, 1], the denoising function \u03f5\u03b8 :\nRd \u2192 Rd learns to predict a clean version of the perturbed xt\nfrom the training distribution S = {x1, .., xm}:\nLDM \u225c Ex\u223cS,t\u223cU(0,1),\u03f5\u223cN (0,I)\n\u0002\n\u2225\u03f5 \u2212 \u03f5\u03b8(xt, t)\u22252\n2\n\u0003\n.\n(1)\nEmpirical results showed that learning diffusion models on top\nof latent spaces of autoencoders can produce results in a higher\nquality than those that are trained on the raw input [16]. In-\ntuitively, this process can be done on a latent representation of\nencoder-decoder architecture. Latent diffusion operates on top\nof a representation given by an encoder f:\nLLDM \u225c Ex\u223cS,t\u223cU(0,1),\u03f5\u223cN (0,I)\n\u0002\n\u2225\u03f5 \u2212 \u03f5\u03b8(f(xt), t)\u22252\n2\n\u0003\n. (2)\nThe output of the diffusion can later be forwarded through the\ndecoder to obtain the raw result (e.g., audio, image, text).\nAn important component of modern generative models is\nconditioning. This allows the generative process to be condi-\ntioned on a given input, i.e., modeling p(x|y) where y is a data\nentry. For example, in a text-based visual generation, the gen-\nerative process is conditioned on text. There are many types of\nconditioning, such as text, time, style, etc. [16]. Usually, the\nconditioning component is done by an injection of a condition\nrepresentation from an encoder \u03c4 to the attention mechanism\nof \u03f5\u03b8. Conditioning the diffusion process yields the following\ndiffusion process, LCLDM \u225c\nE(x,y)\u223cS,t\u223cU(0,1),\u03f5\u223cN (0,I)\n\u0002\n\u2225\u03f5 \u2212 \u03f5\u03b8(f(xt), t, \u03c4(y))\u22252\n2\n\u0003\n.\n(3)\nIn the following, we propose a method that leverages a con-\nditional generative model to produce high-quality and diverse\nimages that are based on audio-scenes.\n2.1. AUDIOTOKEN\nAudio signals contain information that can help us imagine the\nscene that produced them. This makes it tempting to use a gen-\nerative model that is conditioned on audio recordings to gener-\nate a scene. However, models that generate high-quality images\ncommonly rely on large-scale text-image pairs to generate im-\nages using text. We thus propose a method named AUDIOTO-\nKEN that effectively projects audio signals into a textual space,\nenabling us to leverage existing text-conditioned models to gen-\nerate images based on audio-based tokens.\nOur objective is to investigate the feasibility of directly en-\ncoding any audio signals into a dedicated representation that\nwill \ufb01t as an additional token for text-conditioning. By doing\nso, we can leverage existing pre-trained models and not learn a\nnew generative model with audio-visual pairs. Furthermore, we\nare not required to learn a new token for each individual class\nof audio or type of scene (as opposed to textual inversion-based\nmethods [27]). Instead, we develop an audio-to-image genera-\ntor capable of handling a wide range of diverse concepts.\nThe input to our method is a short video input (i, a), where\ni represents a frame from the video and a represents its corre-\nsponding audio recording. We are aiming to create a generative\nprocess that is audio-conditioned, i.e., p(i|a). To achieve this,\nwe utilize a text-conditioned generative model. Thus, we need\nto associate the audio signal a with the text conditioning.\nThe process begins with a transformer model that encodes\nthe initial prompt \u201cA photo of a\u201d into a representation etext \u2208\nR4\u00d7da, where da is the embedding dimension of the text input.\nAfterward, we concatenate to etext, an extra latent representation\nof the audio signal, denoted as eaudio \u2208 Rda. We utilize an\nEmbedder, which is composed of a pre-trained audio encoding\nnetwork and a small projection network. This results in:\neaudio = Embedder(a).\n(4)\nNext, we describe the Embedder network and the optimization\nprocess of our method.\nFigure 3: Qualitative results for Wav2Clip (\ufb01rst row), Image-\nBind (second row), AUDIOTOKEN (third row), and the original\nreference images (last row).\nAudio encoding: The Embedder leverages a pre-trained audio\nclassi\ufb01cation network \u03c6 to represent the audio. The discrimina-\ntive network\u2019s last layer is typically used for classi\ufb01cation, and\nthus it tends to diminish important audio information which is\nirrelevant to the discriminative task. Thus, we take a concate-\nnation of earlier layers and the last hidden layer (speci\ufb01cally\nselecting the fourth, eighth, and twelfth layers out of a total\nof twelve). This results in a temporal embedding of the audio\n\u03c6(a) \u2208 R\n\u02c6\nd\u00d7na, where na is the temporal audio dimension.\nThen, to learn a projection into the textual embedding space,\nwe forward \u03c6(a) in two linear layers with a GELU function\nbetween them:\n\u00afeaudio = W2\u03c3(W1\u03c6(a)),\n(5)\nwhere W1 \u2208 R\n\u02c6\nd\u00d7 \u02c6\nd, W1 \u2208 R\n\u02c6\nd\u00d7daudio, and \u03c3 is a GELU non-\nlinearity [34]. Finally, we apply an attentive pooling layer [35],\nreducing the temporal dimension of the audio signal, i.e.,\neaudio = Atten-Pooling(\u00afeaudio).\n(6)\nOptimization: During training, we update only the weights of\nthe linear and attentive pooling layers within the Embedder net-\nwork during the optimization process. The pre-trained audio\nnetwork and the generative network remain frozen. We adopt\nthe loss function employed by the original model LLDM (Equa-\ntion 2), maintaining consistency in the training scheme. Fur-\nthermore, we introduce an additional loss function that comple-\nments the original one, which involves encoding the label of the\nvideo, denoted by l \u2208 Rnl\u00d7da, where nl represents the label\u2019s\nlength (e.g., the size of the \u2018acoustic guitar\u2019 label is two). The\nlabel is encoded using the generative model\u2019s textual encoder,\nand then the spatial dimension is reduced using average pool-\ning, i.e., \u02c6l = Avg-Pooling(l). The classi\ufb01cation loss is de\ufb01ned\nas follows:\nLCL =\n \n1 \u2212 \u27e8eaudio, \u02c6l\u27e9\n\u2225eaudio\u2225\u2225\u02c6l\u2225\n!2\n.\n(7)\nIntuitively, this term ensures that the audio embedding remains\nclose to the video\u2019s concept, facilitating faster and more stable\nconvergence. Finally, we also add an \u21131 regularization to the\nencoded audio token, which encourages the audio token to be\nmore evenly distributed. The overall loss that is optimized for\nAUDIOTOKEN is given by\nL = LLDM + \u03bb\u21131\u2225eaudio\u22251.\n(8)\nThe overall loss that is optimized for AUDIOTOKEN with clas-\nsi\ufb01cation loss is given by\nL = LLDM + \u03bb\u21131\u2225eaudio\u22251 + \u03bbCLLCL.\n(9)\n2.2. Evaluation functions\nThe evaluation of a visual generation from audio-scene is yet\nopened.\nSuch evaluation setup is challenging since a well-\nperformed model is excepted to generate images that will (i)\ncapture the most prominent object in the audio recording; (ii)\nbe semantically correlate with the input audio; and (iii) be se-\nmantically similar to the \u201cground truth\u201d / target image. Lastly,\nwe require evaluating the general quality of the generated im-\nage. To mitigate that, we propose to use the following evalua-\ntion functions.\nAudio-Image Similarity (AIS) ideally measures the similarity\nbetween the semantic input audio and generated image features.\nWe employ the Wav2CLIP model [30]. The Wav2CLIP model\nenables to measure of the similarity between representations of\nan audio and image pair. This allows us to quantify to which\nextent the generated image describes the audio. Quantifying\nonly the correlation score is not telling the whole story since\nthe score scale may vary. Thus, it is unclear what is a good\nscore. Instead, we compare the similarity between a generated\nimage and its input audio and the similarity between the gener-\nated image and arbitrary audio from the data. The AIS score is\nthen averaged over all data entries in the validation set.\nImage-Image Similarity (IIS) measures the semantic similar-\nity between the generated image and the \u201cground truth\u201d one.\nThis information is crucial since it allows quantifying the se-\nmantic similarity to a \u201cground truth\u201d scene.\nWe employ the same reference-based method as in the AIS\nmetric. Thus, we measure the CLIP [31] score between the (i)\ngenerated image and its \u201cground truth\u201d and (ii) generated image\nand an arbitrary image from the data. The IIS score is then\naveraged over all data entries in the validation set.\nAudio-Image Content (AIC). To account for the image con-\ntent, we measure the level of agreement between the predicted\nclass of an image classi\ufb01er and the ground-truth audio label.\nHowever, since there might not be a complete correlation be-\ntween the image classi\ufb01er classes and the audio labels, an ad-\nditional CLIP-based score is employed to determine agreement.\nIf the CLIP-based matching score exceeds a threshold of 0.75,\nthe image and audio class are considered in agreement.\nFr\u00b4echet Inception Distance (FID). In order to evaluate the\nquality of the generated images, we adopt the standard FID\nscore [36]. Such reference-free metric compares the distribu-\ntion of the generated images against the original images using\nTable 1: We report AIC, FID, AIS, and IIS for AUDIOTO-\nKEN (with and without Classi\ufb01cation Loss (CL)), together with\nWav2Clip. For reference, we additionally report results for the\noriginal images (reference) and images generated by Stable Dif-\nfusion (SD) with text labels.\nMethod\nMetric\nAIC \u2191\nFID \u2193\nAIS \u2191\nIIS \u2191\nReference\n54.66\n-\n-\n-\nSD (Text)\n71.28\n52.85\n-\n-\nWav2Clip [30]\n29.32\n99.89\n47.76\n51.11\nImageBind [37]\n39.15\n67.42\n67.48\n75.50\nAUDIOTOKEN with CL\n48.01\n66.08\n62.28\n76.40\nAUDIOTOKEN\n45.48\n56.65\n68.23\n76.66\nan internal representation obtained from a pre-trained model. In\nthis work, we use the Inception model.\nHuman Evaluation. Lastly, we run a subjective test to evaluate\nthe adherence of the generated images to their labels. For each\nmethod, annotators were shown a generated image and asked to\nrate its relevance to a given label on a scale of 1-5.\n3. Results\nIn the following, we study our method from objective and sub-\njective points of view. We begin by describing details regarding\nthe experimental setup. Then, we report results for our method\nand baselines using the evaluation framework proposed in Sec-\ntion 2.2. We show that our method outperforms the current\nbaselines. We \ufb01nally subjectively evaluate and \ufb01nd that annota-\ntors agree that our method describes the audio the best.\nBaselines.\nWav2Clip [30] employs a CLIP-based loss for\naudio-text pairs.\nThen, they use this representation to gen-\nerate an image from a text that is highly correlated with the\naudio using VQ-GAN [32]. ImageBind [37] combines infor-\nmation from six different modalities (text, image/video, audio,\ndepth, thermal, and inertial measurement units (IMU)) into a\nsingle representation space. We used ImageBind\u2019s uni\ufb01ed la-\ntent with stable-diffusion-2-1-unclip 1 to generate images from\naudio samples.\nData. We use the VGGSound [38] dataset, which is derived\nfrom a collection of YouTube videos with corresponding audio-\nvisual data. The dataset contains 200, 000, each in the length of\nten seconds. The dataset is also annotated with 309 classes.\nHyperparameters. The Embedder network comprises 3 lay-\ners, with attention pooling applied to a sequence of 248. For\nthe generative model, we use Stable Diffusion [16]. This results\nin an 8, 853, 507 parameters model. During training, we ran-\ndomly crop \ufb01ve-second audio clips and select the frame with\nthe highest CLIP score corresponding to the VGGSound label.\nWe also \ufb01lter out frames with inconsistent classi\ufb01cations from\nboth the image and audio classi\ufb01ers. We train the model for\n60, 000 steps with a learning rate of 8e-5 and batch size of 8 on\nNvidia A6000 GPU.\nObjective evaluation.\nWe start by comparing the proposed\nmethod, with and without the Classi\ufb01cation Loss (CL), against\nWav2Clip and ImageBind, considering FID, AIS, AIC, and IIS.\nFor reference, we additionally include a topline of results of\ngenerating images directly from textual description (text labels)\nusing Stable Diffusion (SD). Results are reported in Table 1.\n1https://github.com/Zeqiang-Lai/\nAnything2Image\nFigure 4: Qualitative results of speaker generation for AU-\nDIOTOKEN (\ufb01rst row), and reference images (second row).\nResults suggest that AUDIOTOKEN is superior to Wav2Clip\nand ImageBind, considering all evaluation metrics. Interest-\ningly, AUDIOTOKEN also performs better when considering\nthe AIS metric, which leverages the Wav2Clip and ImageBind\nmodels to obtain the similarity score. This result demonstrates\naccurate audio detail identi\ufb01cation (e.g., distinguishing vari-\nous guitars) and considers multiple entities (e.g., multiple \ufb02y-\ning planes or a single plane). As expected, using textual labels\nreaches a higher accuracy and pushes the model toward learning\nrepresentation which is more discriminative but less correlated\nwith the target video. Generated images from all methods can\nbe seen in Figure 3.\nSubjective evaluation. We compare AUDIOTOKEN\nagainst\nWav2Clip, and SD using textual descriptions. We randomly\nsample 15 images from the test set and ask human annotators\nto rank their relevance to their textual labels on a scale between\n1 and 5. We enforce at least 17 annotations for each of the eval-\nuated images and compute the mean score together with its stan-\ndard deviations. AUDIOTOKEN outperforms Wav2Clip (4.07 \u00b1\n0.83 vs. 1.85 \u00b1 0.46). When considering comparison to SD\nusing text labels, AUDIOTOKEN is reaches comparable perfor-\nmance and yields slightly worse subjective scores (4.07 \u00b1 0.83\nvs. 4.58 \u00b1 0.60). These \ufb01ndings are especially encouraging, as\nthese suggest users found the images generated by AUDIOTO-\nKENto capture the main objects in the audio scene similarly to\nusing textual labels, which serves as a topline.\nSpeaker image generation.\nWe investigate its potential to\ncreate visuals of various speakers.\nWe gathered samples\nfrom two 30-minute videos per person that showcased Barack\nObama, Donald Trump, Emma Watson, and David Beckham to\nachieve this goal and extracted the audio representation from X-\nVector [39]. Our results in Fig. 4 indicate that our approach ac-\ncurately represents Barack Obama and Donald Trump. We pos-\ntulate that this could be due to their distinct voices. However,\nwith Emma Watson and David Beckham, the method mainly\ncaptures their gender.\n4. Conclusions\nIn this paper, we present a method for leveraging text-\nconditioned generative models for audio-based conditioning.\nOur method produces high-quality images which describe a\nscene from the audio recording. In addition, we propose a com-\nprehensive evaluation framework that takes into account the se-\nmantics of the images generated. Our method presents a \ufb01rst\nstep toward audio-conditioned image generation. The hidden\ninformation in the audio is richer than the observed one in the\ntext. Hence, we think that this problem is interesting and should\nget more focus from the community.\n5. References\n[1] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. Denton,\nS. K. S. Ghasemipour, B. K. Ayan, S. S. Mahdavi, R. G. Lopes\net al., \u201cPhotorealistic text-to-image diffusion models with deep\nlanguage understanding,\u201d in NeurIPS.\n[2] O. Gafni, A. Polyak, O. Ashual, S. Sheynin, D. Parikh, and\nY. Taigman, \u201cMake-a-scene: Scene-based text-to-image genera-\ntion with human priors,\u201d in ECCV, 2022.\n[3] H. Chang, H. Zhang, J. Barber, A. Maschinot, J. Lezama, L. Jiang,\nM.-H. Yang, K. Murphy, W. T. Freeman, M. Rubinstein et al.,\n\u201cMuse: Text-to-image generation via masked generative trans-\nformers,\u201d arXiv preprint arXiv:2301.00704, 2023.\n[4] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\nT. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar et al.,\n\u201cLlama: Open and ef\ufb01cient foundation language models,\u201d arXiv\npreprint arXiv:2302.13971, 2023.\n[5] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili\u00b4c, D. Hesslow,\nR. Castagn\u00b4e, A. S. Luccioni, F. Yvon, M. Gall\u00b4e et al., \u201cBloom: A\n176b-parameter open-access multilingual language model,\u201d arXiv\npreprint arXiv:2211.05100, 2022.\n[6] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhari-\nwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., \u201cLan-\nguage models are few-shot learners,\u201d Advances in neural informa-\ntion processing systems, vol. 33, pp. 1877\u20131901, 2020.\n[7] F. Kreuk, G. Synnaeve, A. Polyak, U. Singer, A. D\u00b4efossez,\nJ. Copet, D. Parikh, Y. Taigman, and Y. Adi, \u201cAudiogen: Textu-\nally guided audio generation,\u201d arXiv preprint arXiv:2209.15352,\n2022.\n[8] F. Kreuk, Y. Taigman, A. Polyak, J. Copet, G. Synnaeve,\nA. D\u00b4efossez,\nand Y. Adi,\n\u201cAudio language modeling us-\ning perceptually-guided discrete representations,\u201d arXiv preprint\narXiv:2211.01223, 2022.\n[9] K. Lakhotia, E. Kharitonov, W.-N. Hsu, Y. Adi, A. Polyak,\nB. Bolte, T.-A. Nguyen, J. Copet, A. Baevski, A. Mohamed\net al., \u201cOn generative spoken language modeling from raw au-\ndio,\u201d Transactions of the Association for Computational Linguis-\ntics, vol. 9, pp. 1336\u20131354, 2021.\n[10] C. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen,\nY. Liu, H. Wang, J. Li et al., \u201cNeural codec language mod-\nels are zero-shot text to speech synthesizers,\u201d arXiv preprint\narXiv:2301.02111, 2023.\n[11] H. Cao, C. Tan, Z. Gao, G. Chen, P.-A. Heng, and S. Z.\nLi, \u201cA survey on generative diffusion model,\u201d arXiv preprint\narXiv:2209.02646, 2022.\n[12] J. Ho, A. Jain, and P. Abbeel, \u201cDenoising diffusion probabilistic\nmodels,\u201d NeurIPS, 2020.\n[13] P. Dhariwal and A. Nichol, \u201cDiffusion models beat gans on image\nsynthesis,\u201d Advances in Neural Information Processing Systems,\nvol. 34, pp. 8780\u20138794, 2021.\n[14] A. Q. Nichol and P. Dhariwal, \u201cImproved denoising diffusion\nprobabilistic models,\u201d in International Conference on Machine\nLearning.\nPMLR, 2021, pp. 8162\u20138171.\n[15] Z. Kong, W. Ping, J. Huang, K. Zhao, and B. Catanzaro, \u201cDif-\nfwave: A versatile diffusion model for audio synthesis,\u201d arXiv\npreprint arXiv:2009.09761, 2020.\n[16] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer,\n\u201cHigh-resolution image synthesis with latent diffusion models,\u201d\nin CVPR, 2022.\n[17] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, \u201cHi-\nerarchical text-conditional image generation with clip latents,\u201d\narXiv preprint arXiv:2204.06125, 2022.\n[18] A. Q. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin,\nB. Mcgrew, I. Sutskever, and M. Chen, \u201cGlide: Towards pho-\ntorealistic image generation and editing with text-guided diffu-\nsion models,\u201d in International Conference on Machine Learning.\nPMLR, 2022, pp. 16 784\u201316 804.\n[19] R. Sheffer and Y. Adi, \u201cI hear your true colors: Image guided\naudio generation,\u201d arXiv preprint arXiv:2211.03089, 2022.\n[20] V. Iashin and E. Rahtu, \u201cTaming visually guided sound genera-\ntion,\u201d in British Machine Vision Conference (BMVC), 2021.\n[21] R. Gao and K. Grauman, \u201cVisualvoice: Audio-visual speech sep-\naration with cross-modal consistency,\u201d in 2021 IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition (CVPR).\nIEEE, 2021, pp. 15 490\u201315 500.\n[22] W.-N. Hsu, T. Remez, B. Shi, J. Donley, and Y. Adi, \u201cRe-\nvise: Self-supervised speech resynthesis with visual input for\nuniversal and generalized speech enhancement,\u201d arXiv preprint\narXiv:2212.11377, 2022.\n[23] Y. Tewel, Y. Shalev, I. Schwartz, and L. Wolf, \u201cZerocap: Zero-\nshot image-to-text generation for visual-semantic arithmetic,\u201d in\nCVPR, 2022.\n[24] J. Li, D. Li, S. Savarese, and S. Hoi, \u201cBlip-2: Bootstrapping\nlanguage-image pre-training with frozen image encoders and\nlarge language models,\u201d arXiv preprint arXiv:2301.12597, 2023.\n[25] F. Kreuk, A. Polyak, J. Copet, E. Kharitonov, T. A. Nguyen,\nM. Rivi`ere, W.-N. Hsu, A. Mohamed, E. Dupoux, and Y. Adi,\n\u201cTextless speech emotion conversion using discrete & decom-\nposed representations,\u201d in EMNLP, 2022.\n[26] G. Maimon and Y. Adi, \u201cSpeaking style conversion with discrete\nself-supervised units,\u201d arXiv preprint arXiv:2212.09730, 2022.\n[27] R. Gal, Y. Alaluf, Y. Atzmon, O. Patashnik, A. H. Bermano,\nG. Chechik, and D. Cohen-Or, \u201cAn image is worth one word:\nPersonalizing text-to-image generation using textual inversion,\u201d\narXiv preprint arXiv:2208.01618, 2022.\n[28] M. \u02d9Zelaszczyk and J. Ma\u00b4ndziuk, \u201cAudio-to-image cross-modal\ngeneration,\u201d in IJCNN, 2022.\n[29] C.-H. Wan, S.-P. Chuang, and H.-Y. Lee, \u201cTowards audio to scene\nimage synthesis using generative adversarial network,\u201d in ICASSP\n2019-2019 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP).\nIEEE, 2019, pp. 496\u2013500.\n[30] H.-H. Wu, P. Seetharaman, K. Kumar, and J. P. Bello, \u201cWav2clip:\nLearning robust audio representations from clip,\u201d in ICASSP,\n2022.\n[31] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agar-\nwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., \u201cLearning\ntransferable visual models from natural language supervision,\u201d in\nICML, 2021.\n[32] P. Esser, R. Rombach, and B. Ommer, \u201cTaming transform-\ners for high-resolution image synthesis,\u201d in Proceedings of the\nIEEE/CVF conference on computer vision and pattern recogni-\ntion, 2021, pp. 12 873\u201312 883.\n[33] K. Crowson, S. Biderman, D. Kornis, D. Stander, E. Hallahan,\nL. Castricato, and E. Raff, \u201cVqgan-clip: Open domain image gen-\neration and editing with natural language guidance,\u201d in Computer\nVision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel,\nOctober 23\u201327, 2022, Proceedings, Part XXXVII.\nSpringer,\n2022, pp. 88\u2013105.\n[34] D. Hendrycks and K. Gimpel, \u201cGaussian error linear units\n(gelus),\u201d arXiv preprint arXiv:1606.08415, 2016.\n[35] I. Schwartz, S. Yu, T. Hazan, and A. G. Schwing, \u201cFactor graph\nattention,\u201d in CVPR, 2019.\n[36] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and\nS. Hochreiter, \u201cGans trained by a two time-scale update rule con-\nverge to a local nash equilibrium,\u201d NeurIPS, 2017.\n[37] R. Girdhar, A. El-Nouby, Z. Liu, M. Singh, K. V. Alwala,\nA. Joulin, and I. Misra, \u201cImagebind: One embedding space to\nbind them all,\u201d in CVPR, 2023.\n[38] H. Chen, W. Xie, A. Vedaldi, and A. Zisserman, \u201cVggsound: A\nlarge-scale audio-visual dataset,\u201d in ICASSP, 2020.\n[39] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudan-\npur, \u201cX-vectors: Robust dnn embeddings for speaker recognition,\u201d\nin 2018 IEEE international conference on acoustics, speech and\nsignal processing (ICASSP).\nIEEE, 2018, pp. 5329\u20135333.\n"
  },
  {
    "title": "Textually Pretrained Speech Language Models",
    "link": "https://arxiv.org/pdf/2305.13009.pdf",
    "upvote": "1",
    "text": "Textually Pretrained Speech Language Models\nMichael Hassid\u2661,\u2660,\u2217\nTal Remez\u2660,\u2217\nTu Anh Nguyen\u2660\nItai Gat\u2660\nAlexis Conneau\u2662\nFelix Kreuk\u2660\nJade Copet\u2660\nAlexandre Defossez\u2660\nGabriel Synnaeve\u2660\nEmmanuel Dupoux\u2660\nRoy Schwartz\u2661,\u2217\nYossi Adi\u2661,\u2660,\u2217\n\u2660FAIR Team, Meta\n\u2662OpenAI\n\u2661The Hebrew University of Jerusalem\nmichael.hassid@mail.huji.ac.il\nAbstract\nSpeech language models (SpeechLMs) process and generate acoustic data only,\nwithout textual supervision. In this work, we propose TWIST, a method for\ntraining SpeechLMs using a warm-start from a pretrained textual language models.\nWe show using both automatic and human evaluations that TWIST outperforms\na cold-start SpeechLM across the board. We empirically analyze the effect of\ndifferent model design choices such as the speech tokenizer, the pretrained textual\nmodel, and the dataset size. We find that model and dataset scale both play\nan important role in constructing better-performing SpeechLMs. Based on our\nobservations, we present the largest (to the best of our knowledge) SpeechLM both\nin terms of number of parameters and training data. We additionally introduce two\nspoken versions of the StoryCloze textual benchmark to further improve model\nevaluation and advance future research in the field. We make speech samples, code\nand models publicly available.2\n1\nIntroduction\nSpeech is the earliest form of human language. Although it contains more than just textual content\n(e.g., intonation, non-verbal vocalizations), most spoken language understanding systems are limited\nto its textual form [Wen et al., 2015, Bastianelli et al., 2020, Gupta, 2022]; see Qin et al. [2021]\nfor a recent survey. Recent progress in speech language modeling [Touvron et al., 2023], speech\nsynthesis [Kong et al., 2020], and acoustic unit discovery [Hsu et al., 2021] provided the ability to\nbuild purely speech-based language models, henceforth, SpeechLMs [Lakhotia et al., 2021]. However,\ndespite the fast-growing presence of speech and audio content,3 text is still by far the most dominant\nlanguage modality on the web. This limits the ability of constructing large SpeechLMs, in contrast to\nthe great success of textual Language Model (LM)s [Devlin et al., 2019, Raffel et al., 2020, Brown\net al., 2020, Chowdhery et al., 2022]. The development of large textual LMs trained on massive text\ncorpora allows such models to effectively perform various tasks based on either few examples or\ntextual instructions [Brown et al., 2020, Touvron et al., 2023]. Such LMs often serve as a foundational\n\u2217Core Contributor. \u2662 Work done while working at Meta.\n2https://pages.cs.huji.ac.il/adiyoss-lab/twist/\n3E.g., podcasts, local radio, and video games. See https://www.insiderintelligence.com/content/\nlook-us-digital-audio-market-2022-how-big-who-s-listening-what-they-listening.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2305.13009v3  [cs.CL]  30 Jan 2024\nTWIST\nSpeech Tokenizer\nQuantizer\nSpeechLM\nSpeech \nLanguage Modeling\nResynthesis\nSpeech \nEncoder\nToken to \nSpeech\nPretrained Text \nLM\nWeight Init.\nFigure 1: Generative Spoken Language Modeling: the pipeline is composed of three main components\n(i) Speech tokenizer; (ii) SpeechLM; and (iii) Token-to-speech. This paper introduces TWIST,which\ninitializes the weights of the SpeechLM from a pretrained text LM.\nmodel to be fine-tuned on other tasks such as text classification [Howard and Ruder, 2018], textual\ninstructions [Ouyang et al., 2022], or code generation [Le et al., 2022, Nijkamp et al., 2022].\nThis success raises the question whether textual LMs can be leveraged to improve SpeechLMs.\nOn the one hand, as both modalities operate on completely different granularity levels (pseudo\nphoneme-states level vs. sub-word level), it is unclear whether such transfer will bring any benefit.\nOn the other hand, speech and text are closely connected, and thus it is natural to consider transferring\nmodels across these modalities. Indeed\u2014previous work was able to train speech and text LMs jointly,\nfocusing on improving speech translation [Bapna et al., 2021, Cheng et al., 2022, Bapna et al., 2022]\nor transcription tasks [Ao et al., 2021, Chen et al., 2023].\nIn this work, we show that textual LMs can benefit SpeechLMs. We propose TWIST, Textually\nWarm Initialized Speech Transformer Language Models, a method for initializing SpeechLMs from\na pretrained textual LMs (see Fig. 1). Our empirical findings suggest that such a simple approach\nis highly effective and provides a consistent improvement across all the evaluated metrics, both\nautomatic and human evaluations. We provide an extensive empirical analysis studying the effect of\nmodel and data scale, model architecture, and speech tokenizer on the overall model performance.\nBased on our observations, we present the largest SpeechLM to date (to the best of our knowledge),\nboth in terms of size (13B parameters) and training data (\u223c150k speech hours). Finally, to better\nevaluate SpeechLMs capabilities to model long contextual spoken sentences, we generate two spoken\nversions of the StoryCloze benchmark [Mostafazadeh et al., 2016]. We synthesize all the stories from\nthe original StoryCloze test set using either the original distractor or a randomly chosen one. Each of\nthe versions evaluates different properties of SpeechLMs; the former captures fine-grained temporal\ncommonsense relation, while the latter captures coarse global sentence coherence.\nOur contributions: (i) We introduce TWIST, a textually warm initialized speech transformer\nlanguage model. We empirically show how text-based LMs can be adapted into SpeechLMs, yielding\nconsistent improvements across all evaluated metrics, including both automatic and human ones; (ii)\nWe provide extensive analysis considering the different components of SpeechLMs. Our analysis\nsheds light on the different model design choices and how they affect model performance; (iii) We\nleverage our finding and train the largest SpeechLM to-date in terms of the number of parameters\nand the amount of training data; (iv) We provide two spoken versions of the StoryCloze dataset\ncapturing different aspects of the spoken content. We hope such datasets will be helpful for the\nresearch community to better evaluate SpeechLMs under different setups.\n2\nUsing Textual LMs to Improve SpeechLMs\nWe formally describe the proposed method. We start (Section 2.1) by presenting the relevant\nbackground, including the Generative Spoken Language Modeling (GSLM) pipeline, which we build\non, and relevant LM formulation. We then present TWIST, our proposed method (Section 2.2).\n2.1\nBackground\nIn this work we follow the GSLM framework [Lakhotia et al., 2021]. The general GSLM pipeline is\ncomposed of three main modules, each trained separately: (i) a speech tokenizer, (ii) a SpeechLM,\n2\nand (iii) a vocoder module (i.e., Token-to-speech). Speech resynthesis can be achieved while ignoring\nthe language model and directly feeding the quantized tokens into the vocoder module [Polyak et al.,\n2021]. In the following, we provide background for each of the components mentioned above. See\nFig. 1 for a visual description.\nSpeech tokenizers encode raw speech into a discrete representation. The common approach is to first\nencode the speech into a continuous representation and then quantize the representation to achieve a\nsequence of discrete tokens [Tjandra et al., 2019, 2020, Lakhotia et al., 2021, Borsos et al., 2022].\nFormally, denote the domain of audio samples by X \u2282 R. The representation for a raw signal is\ntherefore a sequence of samples x = (x1, . . . , xT ), where xt \u2208 X for all 1 \u2264 t \u2264 T.\nConsider an encoder network, f, which gets as input the speech utterance and outputs a sequence of\nspectral representations sampled at a low frequency as follows f(x) = (v1, . . . , vT \u2032), where T \u2032 is\ndetermined by the frame rate of the encoder. Note that we do not assume anything about the structure\nof the encoder network f. Lakhotia et al. [2021] evaluated several speech encoders, namely, Mel-\nspectrogram, Contrastive Predictive Coding [Oord et al., 2018, CPC], wav2vec2 [Baevski et al., 2020],\nand HuBERT [Hsu et al., 2021]. They found that HuBERT provides the best overall performance,\nand hence we follow the same setup.\nSince the representations learned by such models are usually continuous, a k-means algorithm\n[MacQueen, 1967] is applied over the models\u2019 outputs to generate discrete tokens, denoted as\nz = (z1, . . . , zT \u2032). Each element zi in z is a positive integer, zi \u2208 {1, . . . , K} for 1 \u2264 i \u2264 T \u2032, where\nK is the number of discrete tokens of the vocabulary Z = {1, . . . , K}.\nLanguage models aim to learn the underlying joint probability of token sequences p(w1, . . . , wn).\nEach token wi belongs to a vocabulary W, defined by a tokenizer.4 Using the chain rule, the joint\nprobability of a sequence can be computed as a product of its conditional probabilities:\np(w1, . . . , wn) =\nn\nY\ni=1\np(wi|wi\u22121, . . . , w1).\nNeural LMs, parameterized by \u03b8, aim to model the probability p\u03b8(wi|c(wi\u22121, . . . , w1)), where c is\na representation of the previous tokens. The network parameters \u03b8 are learned by minimizing the\nnegative log likelihood loss between the predicted and true probability distributions, such that\n\u2113(\u03b8, w) = \u2212\nn\nX\ni=1\nlog p\u03b8(wi|c(wi\u22121, . . . , w1)).\nThe network parameters \u03b8 are typically initialized with values sampled from a predefined distribution,\ne.g., a uniform or a centered Gaussian distributions [Glorot and Bengio, 2010].\nSpeech Language Models (SpeechLMs) are trained on the extracted discrete speech tokens, z, using\na speech tokenizer. When operating on z, SpeechLMs enable directly modeling spoken data without\naccessing textual transcriptions. Such a modeling framework additionally allows for capturing and\nmodeling prosodic features [Kharitonov et al., 2021], as well as speaker identity [Borsos et al., 2022],\nor even natural dialogues [Nguyen et al., 2022]. This is in contrast to using textual features, which do\nnot encode such information.\nToken-to-speech modules convert the speech discrete tokens to a raw waveform. Lakhotia et al.\n[2021] used a Tacotron2.0 [Shen et al., 2018] based model followed by WaveGlow [Prenger et al.,\n2019] vocoder. Later, Polyak et al. [2021] proposed a token-based vocoder based on the HiFi-GAN\narchitecture to convert tokens to speech directly. Such a paradigm seems to provide high-quality\ngenerations with better efficiency, as it uses only one model rather than two. As such, we use their\napproach in this work for our token-to-speech models.\n2.2\nTextually Warm-Initialized Speech Transformer Language Models\nWe propose TWIST, a method for training SpeechLMs initialized from a pretrained text LM, such\nas OPT [Zhang et al., 2022], and LLaMA [Touvron et al., 2023]. TWIST first replaces the original\ntext vocabulary W with Z, the set of speech tokens (see Section 2.1), and sets the tokenizer to be\n4Typically for text LMs, the tokenizer considers words or sub-words [Sennrich et al., 2016].\n3\nTable 1: Speech datasets statistics. We report both the number of hours in the training data of each\ndataset together with the number of speech tokens used to train the speech language model.\nLIBRISPEECH\nLIBRILIGHT\nSPOTIFY\nPEOPLE\nVOXPOPULI\nOVERALL\nHours\n960\n53k\n59k\n17k\n20k\n150k\nTokens-50Hz\n86M\n4.79B\n5.34B\n1.54B\n1.76B\n13.52B\nTokens-25Hz\n58M\n3.19B\n3.56B\n1.02B\n1.17B\n9.00B\na speech based tokenizer. We then replace the text lookup table with a new randomly initialized\nembedding table for the speech tokens. The rest of the body of the network remains unchanged\nduring initialization time. Finally, TWIST continues training the entire SpeechLM using speech data.\nA visual description can be found in Fig. 1.\nOne might wonder whether initializing a speech model with a textual one makes sense, as speech\ntokens operate on 20-40ms windows while text tokenizers span longer concepts (e.g., sub-words).5\nNonetheless, previous work showed that speech and text LMs can be trained jointly to improve either\nmachine translation [Bapna et al., 2021, Cheng et al., 2022, Bapna et al., 2022], or transcription based\nspeech related tasks [Ao et al., 2021, Chen et al., 2023]. We next show that SpeechLMs can benefit\nfrom textual LM initialization. We recognize that more advanced methods for converting speech\ntokens to word tokens probably exist, and hope this study will motivate researchers to explore them.\n3\nExperimental Setup\nWe follow the same setup as described in Section 2.1. Similarly to Lakhotia et al. [2021], we consider\nHuBERT [Hsu et al., 2021] followed by a k-means quantizer as the speech tokenizer. We use a token\nbased HiFi-GAN neural vocoder as our Token-to-speech model, with duration predictor as in Polyak\net al. [2021], Lee et al. [2021]. Then we optimize, compare and analyze the performance of various\nSpeechLMs considering different setups.\n3.1\nDatasets\nAll SpeechLMs are optimized using a collection of publicly available academic speech datasets:\nLibriSpeech (LS) [Panayotov et al., 2015], LibriLight (LL) [Kahn et al., 2020], Spotify pod-\ncasts [Clifton et al., 2020], People dataset [Galvez et al., 2021], and VoxPopuli [Wang et al., 2021a].\nWe filter non-English data using the provided meta-data. This results in \u223c150k hours, which translate\nto 13.5B tokens using a token frequency of 50Hz and 9B tokens using a token frequency of 25Hz.\nSee Table 1 for detailed descriptions of the datasets. To the best of our knowledge, this is the first\nwork that uses this scale of data to train SpeechLMs.\nIn cases where no pre-defined validation and test sets are available, we randomly sample 2% of the\ndata serving as the validation set and an additional 2% for the test set. Unless stated otherwise, in all\nsetups reported results are the average across all different test sets.\n3.2\nModel & Hyperparameters\nOur main family of textual LMs is OPT [Zhang et al., 2022]. We examine three model sizes: OPT-\n125M, OPT-350M and OPT-1.3B, corresponding to 12/24/24 transformer blocks and 768/1024/2048\nhidden dimension, respectively.6 We also consider other pretraining approaches, e.g., different\npretraining corpora, and experiment with BLOOM [Scao et al., 2022] and Pythia [Biderman et al.,\n2023], both with equivalent sizes to OPT-350M/1.3B. For each model size, we compare two variants:\nTWIST, i.e., a warm initialization from a textual LM, and a cold (randomly initialized) model\nfollowing the original GSLM approach (henceforth COLD-INIT). We use 8 GPUs for training, except\n5Considering a speech rate of \u223c120 words per minute (https://virtualspeech.com/blog/average-\nspeaking-rate-words-per-minute) an average word duration is 500ms, which means that SpeechLMs\nrepresent a single word using 12\u201325 tokens.\n6The actual number of parameters of the SpeechLMs is about 10\u201325% lower than the text ones since we\nreplace the text embedding layer with a smaller speech embedding layer, due to the smaller speech vocabulary.\n4\nTable 2: Zero-shot modeling results for different number of tokens and downsampling factors\n(Frequency), with and without TWIST. We report PPL over speech tokens, sWUGGY, sBLIMP.\nBold indicates the best model for each tokenizer, and we underline the best performing model overall\nfor sWUGGY and sBLIMP (PPL results are incomparable across tokenizers).\nTWIST\n# TOKENS\nFREQ.\nPPL\u2193\nSWUGGY\u2191\nSBLIMP\u2191\n\u2717\n100\n50Hz\n5.26\n68.91\n53.80\n\u2713\n5.03\n71.30\n55.96\n\u2717\n200\n50Hz\n5.61\n69.85\n53.48\n\u2713\n5.29\n72.92\n55.91\n\u2717\n500\n50Hz\n6.36\n66.65\n50.79\n\u2713\n5.85\n70.69\n52.71\n\u2717\n500\n25Hz\n6.65\n79.44\n54.84\n\u2713\n6.25\n81.42\n56.20\nfor the 1.3B models, which use 32 GPUs. In all cases, we choose the best checkpoint by the averaged\nspeech perplexity of the model over the validation sets.\nFor the speech tokenizers, we quantize HuBERT representations, which operate at 50Hz, with 100,\n200, and 500 clusters using the k-means algorithm. HuBERT is trained on LS 960h, while k-means\nare trained over the \u2018clean-100h\u2019 part only from the LS corpus. Models and quantizers are obtained\nfrom the textless-lib [Kharitonov et al., 2022]. Inspired by Chung et al. [2021] and Borsos et al.\n[2022], we also trained a new HuBERT tokenizer with a lower sampling rate. We trained the HuBERT\nmodel on a varied mixture of datasets: Multilingual LS [Pratap et al., 2020], Vox Populi [Wang et al.,\n2021a], Common Voice [Ardila et al., 2019], Spotify [Clifton et al., 2020], and Fisher [Cieri et al.,\n2004]. More details regarding model setups can be found in Appendix A.1.\n3.3\nEvaluation\nEvaluating such a complex pipeline comprised of several different components is a challenging\ntask. We follow Lakhotia et al. [2021], who were the first to propose an evaluation setup for this\npipeline. We also introduce a novel evaluation framework based on the StoryCloze textual benchmark\n[Mostafazadeh et al., 2016]. Overall, we consider three different evaluation setups: (i) Zero-shot\nmodeling; (ii) Human evaluation; and (iii) Spoken StoryCloze.\nZero-shot modeling metrics. We use the sWUGGY and sBLIMP [Nguyen et al., 2020] metrics to\nevaluate lexical and syntactic modeling of the SpeechLMs. In sWUGGY, the network is presented\nwith a pair of utterances, an existing word and a matching non-word, and evaluated on their capacity\nto attribute a higher probability to the existing word. Unless stated otherwise, we follow the common\napproach [Lakhotia et al., 2021] and report the \u201cin-vocab\u201d split results. As for sBLIMP, similarly\nto sWUGGY, the network is presented with a matched pair of speech segments, grammatical and\nungrammatical sentences. The task is to decide which of the two is grammatical based on the\nprobability of the sentence. For both measures, we compare the geometric mean over the models\u2019\nsequence probabilities assigned to each utterance within a pair. We additionally report speech tokens\nPerplexity (PPL), averaged across all test sets. We note that while PPL is not comparable across\ndifferent tokenizers, it can be helpful to compare TWIST against COLD-INIT.\nHuman evaluation. To better assess the quality of the generated outputs, we follow the same setup as\nthe Meaningfulness MOS [MMOS; Lakhotia et al., 2021]. In this setup, human raters are presented\nwith several speech continuations (\u223c10 seconds) following the same speech prompt (\u223c3 seconds),\nand are instructed to evaluate how natural (considering grammar, meaning, and diversity) a given\nsample is, on a scale between 1\u20135 with increments of 0.5. For speech prompts, we randomly sample\na collection of 50 examples from the test sets of three different datasets: LL, LS-clean and LS-other\nto reach overall 150 samples per evaluated method. We enforce 10 raters per sample and use the\nCrowdMOS [Ribeiro et al., 2011] package with the recommended recipes for detecting and discarding\ninaccurate scores.\n5\n125M\n350M\n1.3B\n# Parameters\n6.0\n6.5\n7.0\n7.5\nSpeech Perplexity\n1% Data\n10% Data\n100% Data\nCold-Init.\nTWIST (ours)\n(a)\n100k\n200k\n300k\n400k\nTraining steps\n6\n7\n8\nValid Speech Perplexity\n110k\n400k\nCold-Init.\nTWIST (ours)\n(b)\nFigure 2: (a) PPL as a function of training set and model sizes. (b) Validation PPL as a function of\ntraining steps. TWIST is both more sample-efficient and converges faster than COLD-INIT.\nWe note that Lakhotia et al. [2021] also proposed computing PPL and auto-BLEU over the tran-\nscription of the generated speech to assess both quality and diversity of the generations. While this\nmetrics are automatic, they are highly influenced by numerous factors such as speech vocoder quality,\nAutomatic Speech Recognition (ASR) transcription errors, sampling, etc. In preliminary results, we\nobserve a high variance when computing this metric, hence we focus on a human study, and report\nthese metrics for our main family of models in Appendix A.2.\nSpoken StoryCloze.\nFinally, to better evaluate the capabilities of SpeechLMs in capturing\nfine-grained textual nuances and continuation coherence, we provide two spoken versions of\nthe StoryCloze textual benchmark [Mostafazadeh et al., 2016], denoted by Spoken StoryCloze\n(SSTORYCLOZE) and Topic StoryClose (TSTORYCLOZE).7 The textual StoryCloze benchmark\ncontains 4k five-sentence commonsense stories (split to validation and test sets). For each story, there\nis an additional negative sample, composed of the first four sentences followed by a fifth, adversarial\nsentence. The goal is to distinguish the original fifth sentence from the negative one. To generate the\nspoken benchmark, we synthesize the stories from the test set using a single speaker TTS system\nas provided by Wang et al. [2021b], comprised of a FastSpeech2.0 model [Ren et al., 2020] and a\nHiFi-GAN vocoder [Kong et al., 2020].8\nFor SSTORYCLOZE, we follow the original StoryCloze negative samples. With this benchmark, we\nevaluate models\u2019 capabilities to capture fine-grained causal and temporal commonsense relations. For\nTSTORYCLOZE, we randomly sample the negative ending sentence from the dataset. The premise\nbehind TSTORYCLOZE is to evaluate continuation coherence given a spoken prompt. This version is\nfar easier, but our experiments show that it is still quite challenging for modern SpeechLMs. Similar\nto sWUGGY and sBLIMP, we feed both speech segments to the SpeechLM, measure the probability\nof each spoken sentence, and report the percentage of examples where the probability of the positive\nsample is higher than the negative one.\nWe also conduct human evaluation on both datasets in order to measure human performance over the\nspeech benchmarks (which may differ from their text equivalents). We introduce human raters with\nboth options and let them rate each one according to MMOS [Lakhotia et al., 2021], in a scale of 1\u20135\nwith 0.5 increments. The human score for this benchmarks is the proportion of samples where the\nscore given by the rater is higher for the correct utterance. We use 10 raters for each pair of utterances\nand use the CrowdMOS [Ribeiro et al., 2011] package with the recommended recipes for detecting\nand discarding inaccurate scores.\n7Both datasets are available at https://github.com/slp-rl/SpokenStoryCloze\n8We introduce a 100ms of silence between segments to generate naturally spoken sentence.\n6\nTable 3: Comparison of TWIST-7B/13B models against prior work. We report sWUGGY, using\nboth \u2018in-vocab\u2019 words and \u2018all\u2019 words, along with sBLIMP.\nMETHOD\nPPL\u2193\nSWUGGY\u2191\nSBLIMP\u2191\nall\nin-vocab\nvan Niekerk et al. [2021]\n\u2013\n64.3\n72.3\n54.0\nLakhotia et al. [2021]\n\u2013\n\u2013\n68.7\n57.1\nBorsos et al. [2022]\n\u2013\n71.5\n83.7\n64.7\nCOLD-INIT-1.3B\n6.20\n72.2\n81.9\n56.5\nTWIST-1.3B\n5.93\n72.7\n82.5\n57.0\nTWIST-7B\n5.47\n73.9\n83.6\n59.0\nTWIST-13B\n5.34\n74.5\n84.1\n59.2\n4\nResults\nSpeechLMs benefit from warm initialization using text LMs. We start by evaluating the effect\nof the warm initialization on SpeechLMs. We compare two versions of OPT-350M, with warm\n(using TWIST) and cold initialization (COLD-INIT), for different frequencies and different number\nof tokens. Results are reported in Table 2. We observe a consistent improvement across all metrics\nfollowing the TWIST approach. Interestingly, using speech tokens with larger downsampling factor\n(i.e., smaller frequency) leads to substantially better sWUGGY and sBLIMP results. These findings\nare consistent with Borsos et al. [2022], and also reflected by the speech resynthesis results, see\nAppendix A.3. For the rest of the paper we report results using the speech tokenizer with 500 tokens\nat 25Hz, which gets the best sWUGGY and sBLIMP results. For compatibility with prior work\n[Lakhotia et al., 2021], results for the speech tokenizer with 200 tokens at 50Hz are in the Appendix.\nScaling improves SpeechLMs. As prior work mainly considered relatively small SpeechLMs (e.g.,\n\u223c100M parameters in Lakhotia et al. [2021], Kharitonov et al. [2021]) and relatively small training\ndata (e.g., Borsos et al. [2022] use LL only), in the following we evaluate the effect of model and\ndataset scaling on the overall performance.\nWe train three different SpeechLMs using the OPT family. For each model size, we increase the\nmagnitudes of training data, considering 1%, 10%, and 100% of the training data with both TWIST\nand COLD-INIT. Our results (Fig. 2a and Fig. 5; Appendix A.4) first highlight that, as in our\nprevious experiments, SpeechLMs initialized using TWIST perform consistently better than those\nwith COLD-INIT. We next note that, as expected, increasing the model size and the magnitude of\nthe dataset improves model performance in almost all cases. Further, for all model scales, following\nthe TWIST approach with only 10% of the data yields comparable or superior performance than\nthe corresponding COLD-INIT approach using 100% of the data. This is consistent with previous\nfindings that pretraining leads to higher sample efficiency for downstream tasks [Peters et al., 2018].\nThe full set of results for the corresponding models can be found on Table 7 in Appendix A.4.\nTWIST converges faster. Next, we analyze how textual pretraining affects model convergence.\nFig. 2b presents the validation loss training curves for OPT-350M. We observe that using TWIST, the\nmodel reaches the same PPL in about one quarter of the training updates compared to COLD-INIT.\nNot all warm initializations are equally important. We have so far seen that warm initialization\nfrom OPT models is consistently beneficial for SpeechLMs. Are OPT models unique in this sense, or\ncould other pre-trained models similarly benefit SpeechLMs?\nTo address this question, we start by reporting results for different pre-trained text LMs. We consider\ndifferent textual pre-training approaches\u2014BLOOM [Scao et al., 2022] and Pythia [Biderman et al.,\n2023]\u2014both of similar size to OPT-350M/1.3B. Tables 8 and 9 in Appendix A.5 shows similar trends\nto our OPT experiments, in which using textually pretrained LMs yields consistently better results.\nWe next ask whether initialization of SpeechLMs from pre-trained text LMs is particularly important,\nor would initialization from other pre-trained modalities similarly benefit SpeechLMs. We consider\nImageGPT [Chen et al., 2020], an image-generation pre-trained model. We find (Table 10; Ap-\npendix A.6) that unlike the textual initialization, this model not only does not outperform COLD-INIT,\nit substantially underperforms it.\n7\nOriginal\nResynth.\nCold-Init.\n1.3B\nTWIST \n1.3B\nTWIST \n7B\n3.0\n3.5\n4.0\n4.5\nMeaningfulness MOS\n(a) LS-clean.\nOriginal\nResynth.\nCold-Init.\n1.3B\nTWIST \n1.3B\nTWIST \n7B\n3.0\n3.5\n4.0\n4.5\nMeaningfulness MOS\n(b) LS-other.\nOriginal\nResynth.\nCold-Init.\n1.3B\nTWIST \n1.3B\nTWIST \n7B\n3.0\n3.5\n4.0\n4.5\nMeaningfulness MOS\n(c) LL.\nFigure 3: Human evaluation (MMOS) for speech generation, with different models and datasets.\nTWIST outperforms the COLD-INIT, and the TWIST-7B performs better than smaller models. Full\nresults presented in Appendix A.7.\nSpeech Large Language Models. Equipped with previous findings, we train the largest SpeechLMs\nto date (to the best of our knowledge), a 7B/13B parameter SpeechLMs initialized from the LLaMA-\n7B/13B models [Touvron et al., 2023], denoted as TWIST-7B/13B. We use the same configuration\nas in Section 3.2 with specific exceptions detailed in Appendix A.1.\nTable 3 shows our results for the TWIST-7B/13B models along with several baseline methods. We\nalso include COLD-INIT and TWIST models of size 1.3B for a reference. Here we report sWUGGY\nscores both for \u2018in-vocab\u2019 samples (words found in the LS corpus) along with \u2018all\u2019 samples (including\nalso words which do not appear in the LS corpus). As expected, scaling benefits SpeechLMs.\nInitialization from LLaMA-7B/13B leads to additional performance improvement over TWIST-1.3B,\n\u223c8/10% relative improvement in PPL and \u223c1.7/2.5% relative improvement in sWUGGY. When\ncomparing to prior work, TWIST-13B outperforms all evaluated methods on both sWUGGY setups.9\nAs to sBLIMP, TWIST-13B\u2019s results are lower than Borsos et al. [2022], although the proposed\nmethod is orthogonal to their method.\nTable 4: Accuracy (%) results for\nspoken SSTORYCLOZE (SSC) and\nTSTORYCLOZE (TSC) benchmarks.\nMODEL\nSSC\u2191\nTSC\u2191\nHuman\n79.9\n90.2\nTWIST-1.3B\n52.4\n70.6\nTWIST-7B\n55.3\n74.1\nTWIST-13B\n55.4\n76.4\nSpoken StoryCloze. To better asses the contextual under-\nstanding of the SpeechLMs we experiment with our collected\nSpoken StoryCloze benchmark (see Section 3.3).\nTable 4 shows the results for TWIST-1.3B/7B/13B mod-\nels alongside human performance.\nAs expected, both\nSpeechLMs and humans exhibit superior performance in\nterms of continuation coherence (TSTORYCLOZE) compared\nto more fine-grained relations (SSTORYCLOZE). Interest-\ningly, despite humans achieving high scores on the textual\nStoryCloze benchmark, their performance on spoken bench-\nmarks is not flawless, suggesting that spoken language under-\nstanding tasks are more complicated for humans compared to\ntheir written equivalents. The results of SpeechLMs indicate\nreasonable performance on the TSTORYCLOZE benchmark, with a \u223c15% gap compared to human\nperformance. However, this gap widens substantially on the SSTORYCLOZE benchmark. This\nhighlights the opportunities for SpeechLMs\u2019 improvements both in continuation coherence and, more\nsignificantly, in causal and temporal commonsense relations. Lastly, consistent with our previous\nresults, scaling SpeechLMs yields performance improvements on both benchmarks.\nHuman evaluation. We conclude with a human evaluation for speech generation. As stated in\nSection 3.3, for each model and sample we generate speech continuation of \u223c10 seconds (using a\n\u223c3 seconds prompt). We evaluate the following models: 1.3B parameters model with and without\nTWIST, and TWIST-7B. We also compare to the original audio and the resynthesized reference,\nwhich serve as top-line estimates.\nOur result (Fig. 3) suggest that TWIST-7B is superior to TWIST-1.3B, while COLD-INIT-1.3B\nperforms consistently worse. Interestingly, we do not observe noticeable differences between the\n9In fact, under the \u2018all\u2019 setup even TWIST-1.3B outperforms all evaluated methods.\n8\nPrompt: Can we please go swimming at the beach\nCold-Init (1.3B): Can we please go swimming at the beach? Asked Tom. I should \nlike to go swimming he said...\nTWIST-1.3B: Can we please go swimming at the beach? Asked Clara. No, replied \nDora, who had been swimming...\nTWIST-7B: Can we please go swimming at the beach with mom on April 1st? Oh my \nGod, I would love to go somewhere with my kids on April 1st...\nFigure 4: Generation samples using 1.3B models, with and without TWIST, along with TWIST-7B.\nevaluated datasets. We also provide an example of generated sentences given a pre-defined prompt in\nFig. 4. This example illustrates that COLD-INIT-1.3B makes grammatical errors, both COLD-INIT-\n1.3B and TWIST-1.3B remain on topic, and TWIST-7B provides semantically richer continuations.\nAudio samples can be found in the project website.\n5\nRelated Work\nText Language Models have been long studied under various setups [Bahl et al., 1983, Chiang et al.,\n2022]. While Brants et al. [2007] showed the benefit of optimizing LLMs back in 2007, recently with\nthe help of neural networks such models became foundational models, serving as a base model for\ndifferent downstream tasks [Shoeybi et al., 2019, Radford et al., 2019, Brown et al., 2020, Lieber\net al., 2021, Zhang et al., 2022, Hoffmann et al., 2022, Chowdhery et al., 2022].\nThe success of pretrained language models as zero-shot or few-shot learners gave rise to an extensive\nline of work to apply similar technique to other input modalities. Chen et al. [2020] proposed\ngenerating natural images while optimizing a language model using masked prediction loss over the\npixel space of natural images. Other works proposed converting natural images to discrete tokens\nspace and optimizing textually conditioned language model to perform text-to-image generation [Yu\net al., 2022, Ramesh et al., 2021, Chang et al., 2023].\nIn the context of speech and audio, Lakhotia et al. [2021] first demonstrated how raw and uncurated\nspeech data can be leveraged into building a GSLM system. Next, Kharitonov et al. [2021] proposed\na multi-stream SpeechLM to jointly process \u201cpseudo-text\u201d tokens together with quantized prosodic\nfeatures (i.e., duration and F0). Polyak et al. [2021] evaluated the robustness and disentanglement\nproperties of speech-to-tokens models and demonstrated the ability to perform voice conversion\nas well as a lightweight speech codec. Kreuk et al. [2021] proposed to cast the task of speech\nemotion conversion as a translation task, hence translating between one emotion to the other in\nthe discrete space, while Maimon and Adi [2022] proposed a similar approach for speaking style\nconversion. Nguyen et al. [2022] proposed training two SpeechLMs jointly to mimic natural spoken\ndialogues. Recently, Borsos et al. [2022] proposed cascading several LMs, in which one LM\noperates over semantic speech tokens [Sicherman and Adi, 2023] while the others operate on acoustic\ntokens [Zeghidour et al., 2021, D\u00e9fossez et al., 2022]. Such modeling framework allows generating\nnatural speech while keeping the identity of the speaker and acoustic conditions unchanged.\nAnother line of relevant related work, demonstrated that sound and music can be generated following\na similar modeling paradigm. Kreuk et al. [2022] first proposed optimizing language models over\ndiscrete audio representations to construct a text-to-audio generation model. Similarly Agostinelli\net al. [2023] proposed optimizing three language models (following the same modeling approach as\nby Borsos et al. [2022]), operating at different granularity of the input representation for the task of\ntext-to-music generation. Donahue et al. [2023] proposed a similar modeling approach for the task\nof singing to accompaniment generation. Lee et al. [2021, 2022], Popuri et al. [2022] followed a\nsimilar modeling mechanism using a different speech tokenizer and proposed a textless approach for\nspeech-to-speech translation. Hsu et al. [2022] proposed a jointly modeling visual discrete tokens\ntogether with speech tokens to perform various speech resynthesis tasks including: silent video to\nspeech generation, speech enhancement, and recovering packet loss.\nFinally, perhaps the line of research most related to this work is jointly training speech and text\nmodels. Bapna et al. [2021], Cheng et al. [2022] and Bapna et al. [2022] considered speech as another\nlanguage in multilingual setups, and showed that involving speech and text as part of training data\n9\nimproves results for speech translation and multilingual text tasks. Chen et al. [2023] and Ao et al.\n[2021] used the joint training to improve transcriptions tasks as ASR and Text-to-Speech (TTS).\nNone of these prior work focus on SpeechLMs and its modeling and generation capabilities.\n6\nConclusion\nIn this work, we studied the effect of textual pretraining on speech-based language models. We\nempirically demonstrated how such simple model initialization can improve modeling performance\nconsidering both automatic and human-generated measures. We conducted extensive evaluation of\ndifferent speech tokenizers, model and dataset sizes, model architecture, and modality pretraining.\nOur analysis sheds light on the impact of specific modeling design choices on the overall performance\nof the system. Equipped with these findings, we presented the largest SpeechLMs to date (7B and\n13B parameters). Finally, we generated two spoken versions of the StoryCloze textual benchmark\nto measure contextual understanding abilities of SpeechLMs. We hope such empirical findings and\nbenchmark releases will be found useful for future research in the field.\n7\nLimitations and Broader Impact\nLimitations. The biggest limitation of SpeechLMs at large is the lack of semantic understanding,\nwhich might lead to ungrammatical, off-topic, or even inaccurate responses. Although we provided a\nbetter initialization point for optimizing SpeechLM, we did not observe major semantic knowledge\ntransfer following the TWIST approach. This limitation is a general critique of SpeechLMs, which\nshould be better studied and addressed in future research. Another limitation of the proposed approach\nis the granularity of the speech tokenizer, such small input resolution (50Hz / 25Hz) yields a relatively\nlong sequence (750 / 500 tokens) for relatively short speech segments (\u223c30 seconds). This translates\ninto long inference time and harder optimization.\nBroader impact. SpeechLMs share the same potential benefits for society as text-based LMs as they\ngive access, in the audio modality, to the same downstream applications (search, language generation,\nsummarization, translation, chatbots, etc.). Thereby increasing their reach to more use cases and\nmore languages, including \u2019unwritten\u2019 (or sparsely written) languages [Lakhotia et al., 2021]. As\na result, they also share the same potential risks regarding intentionally harmful applications (e.g.\nfake news, spamming, election manipulation) or unintentionally harmful ones (e.g., unfair or biased\nresults, toxic, regurgitated or untrustful generations).\n8\nAcknowledgements\nWe thank Miri Varshavsky Hassid for the great feedback and moral support.\nAuthors note.\nThis paper was submitted in the wake of the tragic terror attack perpetrated by\nHamas on October 7, 2023, which has left the Israeli nation profoundly devastated. The assault\ngreatly impacted us at a personal level, thereby significantly impacting the course of this research\nwork. This paper was finalized while we grieve and mourn our friends and family, under great stress,\nwith scientific considerations being the last thing on our minds. It may contain subtle errors.\nIn memory of the countless lives shattered by Hamas actions.\nReferences\nTsung-Hsien Wen, Milica Gasic, Nikola Mrk\u0161i\u00b4c, Pei-Hao Su, David Vandyke, and Steve Young.\nSemantically conditioned lstm-based natural language generation for spoken dialogue systems.\nIn Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,\npages 1711\u20131721, 2015.\nEmanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and Verena Rieser. Slurp: A spoken\nlanguage understanding resource package. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), pages 7252\u20137262, 2020.\n10\nAkshat Gupta. On building spoken language understanding systems for low resourced languages.\nIn Proceedings of the 19th SIGMORPHON Workshop on Computational Research in Phonetics,\nPhonology, and Morphology, pages 1\u201311, 2022.\nLibo Qin, Tianbao Xie, Wanxiang Che, and Ting Liu. A survey on spoken language understanding:\nRecent advances and new frontiers. arXiv preprint arXiv:2103.03095, 2021.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e\nLacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\nJungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hifi-gan: Generative adversarial networks for\nefficient and high fidelity speech synthesis. Advances in Neural Information Processing Systems,\n33:17022\u201317033, 2020.\nWei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov,\nand Abdelrahman Mohamed. Hubert: Self-supervised speech representation learning by masked\nprediction of hidden units. IEEE/ACM Transactions on Audio, Speech, and Language Processing,\n2021.\nKushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte,\nTu-Anh Nguyen, Jade Copet, Alexei Baevski, Abdelrahman Mohamed, and Emmanuel Dupoux.\nOn Generative Spoken Language Modeling from Raw Audio. TACL, 2021.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota,\nJune 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https:\n//aclanthology.org/N19-1423.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367, 2020. URL\nhttp://jmlr.org/papers/v21/20-074.html.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\nJeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification.\narXiv preprint arXiv:1801.06146, 2018.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback. Advances in Neural Information Processing Systems, 35:\n27730\u201327744, 2022.\nHung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi. Coderl:\nMastering code generation through pretrained models and deep reinforcement learning. Advances\nin Neural Information Processing Systems, 35:21314\u201321328, 2022.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese,\nand Caiming Xiong. A conversational paradigm for program synthesis. arXiv e-prints, pages\narXiv\u20132203, 2022.\nAnkur Bapna, Yu-an Chung, Nan Wu, Anmol Gulati, Ye Jia, Jonathan H Clark, Melvin Johnson,\nJason Riesa, Alexis Conneau, and Yu Zhang. Slam: A unified encoder for speech and language\nmodeling via speech-text joint pre-training. arXiv preprint arXiv:2110.10329, 2021.\n11\nYong Cheng, Yu Zhang, Melvin Johnson, Wolfgang Macherey, and Ankur Bapna. Mu2 slam:\nMultitask, multilingual speech and language models. arXiv preprint arXiv:2212.09553, 2022.\nAnkur Bapna, Colin Cherry, Yu Zhang, Ye Jia, Melvin Johnson, Yong Cheng, Simran Khanuja, Jason\nRiesa, and Alexis Conneau. mslam: Massively multilingual joint pre-training for speech and text.\narXiv preprint arXiv:2202.01374, 2022.\nJunyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li,\nYu Zhang, Zhihua Wei, Yao Qian, Jinyu Li, and Furu Wei. Speecht5: Unified-modal encoder-\ndecoder pre-training for spoken language processing. 2021.\nZhehuai Chen, Ankur Bapna, Andrew Rosenberg, Yu Zhang, Bhuvana Ramabhadran, Pedro Moreno,\nand Nanxin Chen. Maestro-u: Leveraging joint speech-text representation learning for zero\nsupervised speech asr. In 2022 IEEE Spoken Language Technology Workshop (SLT), pages 68\u201375.\nIEEE, 2023.\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Van-\nderwende, Pushmeet Kohli, and James Allen. A corpus and evaluation framework for deeper\nunderstanding of commonsense stories. arXiv preprint arXiv:1604.01696, 2016.\nAdam Polyak, Yossi Adi, Jade Copet, Eugene Kharitonov, Kushal Lakhotia, Wei-Ning Hsu, Ab-\ndelrahman Mohamed, and Emmanuel Dupoux. Speech resynthesis from discrete disentangled\nself-supervised representations. arXiv preprint arXiv:2104.00355, 2021.\nAndros Tjandra, Berrak Sisman, Mingyang Zhang, Sakriani Sakti, Haizhou Li, and Satoshi Nakamura.\nVqvae unsupervised unit discovery and multi-scale code2spec inverter for zerospeech challenge\n2019. In Interspeech, 2019.\nAndros Tjandra, Sakriani Sakti, and Satoshi Nakamura. Transformer vq-vae for unsupervised unit\ndiscovery and speech synthesis: Zerospeech 2020 challenge. In Interspeech, 2020.\nZal\u00e1n Borsos, Rapha\u00ebl Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi,\nOlivier Teboul, David Grangier, Marco Tagliasacchi, and Neil Zeghidour. Audiolm: a language\nmodeling approach to audio generation. arXiv preprint arXiv:2209.03143, 2022.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive\ncoding. arXiv preprint arXiv:1807.03748, 2018.\nAlexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework\nfor self-supervised learning of speech representations. NeurIPS, 2020.\nJ MacQueen. Classification and analysis of multivariate observations. In 5th Berkeley Symp. Math.\nStatist. Probability, pages 281\u2013297. University of California Los Angeles LA USA, 1967.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers), pages 1715\u20131725, Berlin, Germany, Au-\ngust 2016. Association for Computational Linguistics.\ndoi: 10.18653/v1/P16-1162.\nURL\nhttps://aclanthology.org/P16-1162.\nXavier Glorot and Yoshua Bengio.\nUnderstanding the difficulty of training deep feedforward\nneural networks. In Yee Whye Teh and Mike Titterington, editors, Proceedings of the Thirteenth\nInternational Conference on Artificial Intelligence and Statistics, volume 9 of Proceedings of\nMachine Learning Research, pages 249\u2013256, Chia Laguna Resort, Sardinia, Italy, 13\u201315 May\n2010. PMLR. URL https://proceedings.mlr.press/v9/glorot10a.html.\nEugene Kharitonov, Ann Lee, Adam Polyak, Yossi Adi, Jade Copet, Kushal Lakhotia, Tu-Anh\nNguyen, Morgane Rivi\u00e8re, Abdelrahman Mohamed, Emmanuel Dupoux, et al. Text-free prosody-\naware generative spoken language modeling. arXiv preprint arXiv:2109.03264, 2021.\nTu Anh Nguyen, Eugene Kharitonov, Jade Copet, Yossi Adi, Wei-Ning Hsu, Ali Elkahky, Paden\nTomasello, Robin Algayres, Benoit Sagot, Abdelrahman Mohamed, et al. Generative spoken\ndialogue language modeling. arXiv preprint arXiv:2203.16502, 2022.\n12\nJonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng\nChen, Yu Zhang, Yuxuan Wang, Rj Skerrv-Ryan, et al. Natural tts synthesis by conditioning\nwavenet on mel spectrogram predictions. In ICASSP, 2018.\nRyan Prenger, Rafael Valle, and Bryan Catanzaro. Waveglow: A flow-based generative network for\nspeech synthesis. In ICASSP, 2019.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language\nmodels. arXiv preprint arXiv:2205.01068, 2022.\nAnn Lee, Peng-Jen Chen, Changhan Wang, Jiatao Gu, Xutai Ma, Adam Polyak, Yossi Adi, Qing He,\nYun Tang, Juan Pino, et al. Direct speech-to-speech translation with discrete units. arXiv preprint\narXiv:2107.05604, 2021.\nVassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus\nbased on public domain audio books. In ICASSP, 2015.\nJacob Kahn, Morgane Riviere, Weiyi Zheng, Evgeny Kharitonov, Qiantong Xu, Pierre-Emmanuel\nMazar\u00e9, Julien Karadayi, Vitaliy Liptchinsky, Ronan Collobert, Christian Fuegen, et al. Libri-light:\nA benchmark for asr with limited or no supervision. In ICASSP 2020-2020 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), pages 7669\u20137673. IEEE, 2020.\nAnn Clifton, Aasish Pappu, Sravana Reddy, Yongze Yu, Jussi Karlgren, Ben Carterette, and Rosie\nJones. The spotify podcast dataset. arXiv preprint arXiv:2004.04270, 2020.\nDaniel Galvez, Greg Diamos, Juan Ciro, Juan Felipe Cer\u00f3n, Keith Achorn, Anjali Gopi, David Kanter,\nMaximilian Lam, Mark Mazumder, and Vijay Janapa Reddi. The people\u2019s speech: A large-scale\ndiverse english speech recognition dataset for commercial usage. arXiv preprint arXiv:2111.09344,\n2021.\nChanghan Wang, Morgane Riviere, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary\nWilliamson, Juan Pino, and Emmanuel Dupoux. Voxpopuli: A large-scale multilingual speech cor-\npus for representation learning, semi-supervised learning and interpretation. In Proceedings of the\n59th Annual Meeting of the Association for Computational Linguistics and the 11th International\nJoint Conference on Natural Language Processing (Volume 1: Long Papers), pages 993\u20131003,\n2021a.\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman\nCastagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. BLOOM: A 176b-\nparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.\nStella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hal-\nlahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al.\nPythia: A suite for analyzing large language models across training and scaling. arXiv preprint\narXiv:2304.01373, 2023.\nEugene Kharitonov, Jade Copet, Kushal Lakhotia, Tu Anh Nguyen, Paden Tomasello, Ann Lee, Ali\nElkahky, Wei-Ning Hsu, Abdelrahman Mohamed, Emmanuel Dupoux, et al. textless-lib: a library\nfor textless spoken language processing. arXiv preprint arXiv:2202.07359, 2022.\nYu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, and Yonghui\nWu. W2v-bert: Combining contrastive learning and masked language modeling for self-supervised\nspeech pre-training. In 2021 IEEE Automatic Speech Recognition and Understanding Workshop\n(ASRU), pages 244\u2013250. IEEE, 2021.\nVineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert. Mls: A\nlarge-scale multilingual dataset for speech research. arXiv preprint arXiv:2012.03411, 2020.\nRosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler, Josh Meyer, Reuben\nMorais, Lindsay Saunders, Francis M Tyers, and Gregor Weber. Common voice: A massively-\nmultilingual speech corpus. arXiv preprint arXiv:1912.06670, 2019.\n13\nChristopher Cieri, David Graff, Owen Kimball, Dave Miller, and Kevin Walker. Fisher english\ntraining speech part 1 transcripts. Philadelphia: Linguistic Data Consortium, 2004.\nTu Anh Nguyen, Maureen de Seyssel, Patricia Roz\u00e9, Morgane Rivi\u00e8re, Evgeny Kharitonov, Alexei\nBaevski, Ewan Dunbar, and Emmanuel Dupoux. The zero resource speech benchmark 2021:\nMetrics and baselines for unsupervised spoken language modeling. In NeurIPS \u2013 Self-Supervised\nLearning for Speech and Audio Processing Workshop, 2020.\nFl\u00e1vio Ribeiro, Dinei Flor\u00eancio, Cha Zhang, and Michael Seltzer. Crowdmos: An approach for\ncrowdsourcing mean opinion score studies. In 2011 IEEE international conference on acoustics,\nspeech and signal processing (ICASSP), pages 2416\u20132419. IEEE, 2011.\nChanghan Wang, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Ann Lee, Peng-Jen Chen, Jiatao Gu, and\nJuan Pino. fairseq s^2: A scalable and integrable speech synthesis toolkit. In Proceedings of the\n2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,\npages 143\u2013152, 2021b.\nYi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. Fastspeech 2: Fast\nand high-quality end-to-end text to speech. arXiv preprint arXiv:2006.04558, 2020.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\nLuke Zettlemoyer. Deep contextualized word representations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers), pages 2227\u20132237, New Orleans, Louisiana,\nJune 2018. Association for Computational Linguistics.\ndoi: 10.18653/v1/N18-1202.\nURL\nhttps://aclanthology.org/N18-1202.\nMark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.\nGenerative pretraining from pixels. In International conference on machine learning, pages\n1691\u20131703. PMLR, 2020.\nBenjamin van Niekerk, Leanne Nortje, Matthew Baas, and Herman Kamper. Analyzing speaker\ninformation in self-supervised models to improve zero-resource speech processing. arXiv preprint\narXiv:2108.00917, 2021.\nLalit R Bahl, Frederick Jelinek, and Robert L Mercer. A maximum likelihood approach to continuous\nspeech recognition. IEEE transactions on pattern analysis and machine intelligence, (2):179\u2013190,\n1983.\nCheng-Han Chiang, Yung-Sung Chuang, and Hung-Yi Lee. Recent advances in pre-trained language\nmodels: Why do they work and how do they work. In Proceedings of the 2nd Conference of the\nAsia-Pacific Chapter of the Association for Computational Linguistics and the 12th International\nJoint Conference on Natural Language Processing: Tutorial Abstracts, pages 8\u201315, 2022.\nThorsten Brants, Ashok Popat, Peng Xu, Franz Josef Och, and Jeffrey Dean. Large language models\nin machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in\nNatural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),\npages 858\u2013867, 2007.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism.\narXiv preprint arXiv:1909.08053, 2019.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nOpher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. Jurassic-1: Technical details and evaluation.\nWhite Paper. AI21 Labs, 1, 2021.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.\nTraining compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.\n14\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,\nAlexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-\nrich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,\nand Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine\nLearning, pages 8821\u20138831. PMLR, 2021.\nHuiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang,\nKevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation\nvia masked generative transformers. arXiv preprint arXiv:2301.00704, 2023.\nFelix Kreuk, Adam Polyak, Jade Copet, Eugene Kharitonov, Tu-Anh Nguyen, Morgane Rivi\u00e8re, Wei-\nNing Hsu, Abdelrahman Mohamed, Emmanuel Dupoux, and Yossi Adi. Textless speech emotion\nconversion using decomposed and discrete representations. arXiv preprint arXiv:2111.07402,\n2021.\nGallil Maimon and Yossi Adi. Speaking style conversion with discrete self-supervised units. arXiv\npreprint arXiv:2212.09730, 2022.\nAmitay Sicherman and Yossi Adi. Analysing discrete self supervised speech representation for\nspoken language modeling. arXiv preprint arXiv:2301.00591, 2023.\nNeil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. Sound-\nstream: An end-to-end neural audio codec. IEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing, 30:495\u2013507, 2021.\nAlexandre D\u00e9fossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio\ncompression. arXiv preprint arXiv:2210.13438, 2022.\nFelix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre D\u00e9fossez, Jade Copet, Devi\nParikh, Yaniv Taigman, and Yossi Adi. Audiogen: Textually guided audio generation. arXiv\npreprint arXiv:2209.15352, 2022.\nAndrea Agostinelli, Timo I Denk, Zal\u00e1n Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon,\nQingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, et al. Musiclm: Generating\nmusic from text. arXiv preprint arXiv:2301.11325, 2023.\nChris Donahue, Antoine Caillon, Adam Roberts, Ethan Manilow, Philippe Esling, Andrea Agostinelli,\nMauro Verzetti, Ian Simon, Olivier Pietquin, Neil Zeghidour, et al. Singsong: Generating musical\naccompaniments from singing. arXiv preprint arXiv:2301.12662, 2023.\nAnn Lee, Hongyu Gong, Paul-Ambroise Duquenne, Holger Schwenk, Peng-Jen Chen, Changhan\nWang, Sravya Popuri, Yossi Adi, Juan Pino, Jiatao Gu, and Wei-Ning Hsu. Textless speech-to-\nspeech translation on real data. In NAACL, 2022.\nSravya Popuri, Peng-Jen Chen, Changhan Wang, Juan Pino, Yossi Adi, Jiatao Gu, Wei-Ning Hsu,\nand Ann Lee. Enhanced direct speech-to-speech translation using self-supervised pre-training and\ndata augmentation. arXiv preprint arXiv:2204.02967, 2022.\nWei-Ning Hsu, Tal Remez, Bowen Shi, Jacob Donley, and Yossi Adi. Revise: Self-supervised speech\nresynthesis with visual input for universal and generalized speech enhancement. arXiv preprint\narXiv:2212.11377, 2022.\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever.\nRobust speech recognition via large-scale weak supervision. arXiv preprint arXiv:2212.04356,\n2022.\n15\nA\nSupplemental materials\nA.1\nModel & hyperparameters\nAll LM models are trained with a batch size of 64, where each sample is bounded for 25 seconds and\n704 tokens. The models are trained for 400k steps (\u223c1.2 epochs), using an inverse-sqrt scheduler,\n100 warmup steps and wADAM as the optimization algorithm. We also tune the learning rate per\nscenario, i.e: using/not-using pretrained LM, we end up with a maximal learning rate of 4e-4/8e-5 and\nfinal learning rate of 8e-5/2.5e-5, respectively. As for the LLaMA-7B/13B model, we use the same\nconfiguration except the following: cosine learning rate schedule, 500 warmup steps, a maximum\nlearning rate of 1e-4, a final rate of 1e-5, batch size of 1024 over 32 GPUs for 75k steps (\u223c4 epochs).\nThe new the frequency HuBERT speech tokenizer (now available in textless-lib Kharitonov et al.\n[2022]), is trained for 3 iterations with the default 50Hz features rate. For the 4-th iteration, we add\nan additional convolutional layer at the CNN Encoder with the strides 2/3/4, resulting in features of\n25Hz/16.6Hz/12.5Hz, respectively. Our early ablations show that 25Hz features with 500 tokens give\nthe best results in terms of language modeling, we thus train our models on these new tokens and\ncompare them with the rest of the tokens.\nA.2\nAutomatic evaluation of speech generation\nAs stated, PPL scores over the transcribed speech generations (text-PPL) are sensitive to modification\nin the sampling parameter, vocoder quality and ASR errors [Lakhotia et al., 2021]. Furthermore,\ntext-PPL tells only one side of the story, we also need to count for diversity in the generation, as\nsuggested by Lakhotia et al. [2021]. For instance, repeated speech gets good text-PPL score, at the\ncost of bad diversity measures.\nTo mitigate that, when computing the automatic metrics for generated speech, we threshold the\ntemperature parameter using the ASR model confidence. Next, as there can be trade-offs in terms of\nsentence coherence and diversity, we calibrated the temperature so that the generated speech matches\nthe transcription of the original utterance (as much as possible) in terms of generation diversity\n(auto-BLEU from Lakhotia et al. [2021]).\nFor speech prompts, we randomly sample a collection of 1024 examples from the test sets of three\ndifferent datasets: LS-clean, LS-other and LL. Each speech prompt is \u223c3 seconds long, while\nthe continuation is \u223c10 seconds. For the calibration procedure, we used randomly selected 128\nexamples from the corresponding validation datasets. Text transcriptions extracted using Whisper\n\u201csmall\u201d [Radford et al., 2022], while text-PPL measures were computed using a LLaMA-7B (text\nmodel) [Touvron et al., 2023]. Table 5 reports both text-PPL and auto-BLEU results for the original\ncontinuations, and the generations that produced by our main family of models (OPT architecture\ncoupled with the 25Hz tokenizer).\nTable 5: text-PPL and auto-BLEU results, with and without TWIST.\nTWIST\n# PARAM.\nTEXT-PPL\u2193\nAUTO-BLEU\u2193\n\u2014\nOriginal Cont.\n36.35\n0.23\n\u2717\n125M\n88.17\n0.305\n\u2713\n74.95\n0.29\n\u2717\n350M\n117.17\n0.27\n\u2713\n92.69\n0.25\n\u2717\n1.3B\n67.68\n0.29\n\u2713\n47.8\n0.29\nAs can be seen, while the diversity metric is comparable across models (due to calibration), TWIST\nmodels outperform the COLD-INIT ones in terms of text-PPL. These measures further support our\nclaims: TWIST models outperform COLD-INIT ones, and models generally improve with scale.\nNotice, the text-PPL for the 350M models are worse than the 125M models. However, as stated, the\ndiversity of the generations (as computed using auto-BLEU) is better for the 350M models.\n16\n125M\n350M\n1.3B\n# Parameters\n5.0\n5.5\n6.0\n6.5\nSpeech Perplexity\n1% Data\n10% Data\n100% Data\nCold-Init.\nTWIST (ours)\nFigure 5: PPL as a function of training set and model size, for models trained with the 200 tokens at\n50Hz tokenizer.\nA.3\nSpeech resynthesis results\nResynthesis can be considered as an upper bound for our language modeling setup. It does not involve\nSpeechLMs, and measures our ability to fully recover the speech content after tokenization [Polyak\net al., 2021]. As we additionally evaluate several speech tokenizers, we provide resynthesis metrics\nin the form of Word Error Rate (WER). We use Whisper \u201csmall\u201d [Radford et al., 2022] as our ASR\nmodel.\nIn Table 6, we evaluate the effect of the tokenizer on the resynthesis performance, and can better\nevaluate the impact of the tokenization process on the generated audio. As can be seen, all tokenizers\nincur a loss in WER. Using 500 tokens at 25Hz provides the best performance.\nTable 6: Speech Resynthesis. Results are reported for different number of tokens and downsampling\nfactors (Frequency).\n# TOKENS\nFREQUENCY\nWER\u2193\n100\n50Hz\n0.23\n200\n50Hz\n0.18\n500\n50Hz\n0.17\n500\n25Hz\n0.16\nA.4\nModel and data scaling results\nThe full set of results, i.e., PPL, sWUGGY and sBLIMP from Section 4 for model and dataset scaling\nis presented in Table 7. The equivalent of Fig. 2a using 200 tokens at 50Hz tokenizer can be found in\nFig. 5.\nA.5\nThe effect of LM architecture\nTo further validate our findings holds for other LM architectures other than OPT. In Table 8, we\nprovide results for the BLOOM [Scao et al., 2022] and Pythia [Biderman et al., 2023] of similar size\nto OPT-350M, with both 25Hz and 50Hz tokenizers. In Table 9 we provide results for BLOOM and\nPythia of similar size to OPT-1.3B with the 25Hz tokenizer.\nAs before, we observe similar patterns in terms of using a pretrained text LM. SpeechLMs initialize\nfrom text reach better performance across all metrics.\nA.6\nThe effect of different modality pretraining\nAlthough having completely different granularity, results suggest training SpeechLMs with model\ninitialization from a text based LMs brings a consistent performance improvement. As a result, a\n17\nTable 7: Model and Data Scaling. Results are reported for different models on various size using\ndifferent magnitude of data, with and without TWIST. We report PPL/ sWUGGY / sBLIMP.\nTWIST\n# PARAM.\n# TOKENS\nFREQ.\n1% OF DATA\n10% OF DATA\n100% OF DATA\n\u2717\n125M\n200\n50Hz\n6.08 / 66.90 / 52.45\n5.79 / 68.16 / 52.71\n5.79 / 68.26 / 53.02\n\u2713\n5.94 / 69.48 / 52.87\n5.51 / 70.67 / 54.34\n5.49 / 70.75 / 53.92\n\u2717\n350M\n200\n50Hz\n6.14 / 66.49 / 51.97\n5.71 / 68.85 / 53.13\n5.61 / 68.95 / 53.48\n\u2713\n5.93 / 68.49 / 53.13\n5.37 / 72.64 / 55.63\n5.29 / 72.92 / 55.91\n\u2717\n1.3B\n200\n50Hz\n6.29 / 64.91 / 52.18\n5.51 / 71.39 / 54.58\n5.32 / 72.83 / 55.12\n\u2713\n6.05 / 67.89 / 52.83\n5.37 / 72.45 / 55.65\n5.23 / 73.39 / 55.91\n\u2717\n125M\n500\n25Hz\n7.22 / 77.58 / 53.74\n6.82 / 78.12 / 54.00\n6.81 / 77.74 / 54.27\n\u2713\n7.06 / 78.99 / 54.12\n6.52 / 80.08 / 55.45\n6.50 / 80.57 / 55.43\n\u2717\n350M\n500\n25Hz\n7.37 / 76.96 / 53.07\n6.79 / 77.93 / 54.71\n6.65 / 79.44 / 54.84\n\u2713\n7.26 / 78.67 / 53.95\n6.41 / 81.23 / 56.08\n6.26 / 79.44 / 56.20\n\u2717\n1.3B\n500\n25Hz\n7.49 / 75.44 / 52.96\n6.40 / 80.80 / 55.95\n6.20 / 81.94 / 56.52\n\u2713\n7.19 / 78.00 / 53.90\n6.19 / 82.28 / 56.81\n5.93 / 82.49 / 57.05\nTable 8: LM Model Architecture. Results are reported for both Bloom and Pythia model architectures\n(\u223c350M parameters), with and without TWIST. We report PPL, sWUGGY and sBLIMP.\nTWIST\nARCH.\n# TOKENS\nFREQ.\nPPL\u2193\nSWUGGY\u2191\nSBLIMP\u2191\n\u2717\nBLOOM\n200\n50Hz\n5.63\n69.38\n53.03\n\u2713\n5.21\n72.48\n55.79\n\u2717\nBLOOM\n500\n25Hz\n6.45\n79.46\n55.60\n\u2713\n6.06\n82.01\n57.22\n\u2717\nPythia\n200\n50Hz\n5.62\n70.00\n53.07\n\u2713\n5.23\n72.20\n56.00\n\u2717\nPythia\n500\n25Hz\n6.45\n79.82\n55.45\n\u2713\n6.12\n81.41\n56.70\nTable 9: LM Model Architecture. Results are reported for both Bloom and Pythia model architectures\n(\u223c1.3B parameters), with and without TWIST. We report PPL, sWUGGY and sBLIMP.\nTWIST\nARCH.\nPPL\u2193\nSWUGGY\u2191\nSBLIMP\u2191\n\u2717\nBLOOM\n6.09\n80.47\n56.02\n\u2713\n5.80\n82.63\n57.43\n\u2717\nPythia\n6.05\n81.33\n56.34\n\u2713\n5.81\n81.77\n57.02\nnatural question would be do speech and text tokens have special connection or LMs are just general\nnext token prediction mechanisms?\nTo evaluate such a hypothesis, we consider a language model trained on a different modality. Specifi-\ncally, we train ImageGPT [Chen et al., 2020] (\u201cmedium\u201d size) models, one from scratch and another\none pretrained using next pixel prediction using a transformer language model. For the pretrained\nmodel we use the official pre-trained model.10 Table 10 summarizes the results.\nInterestingly, ImageGPT pre-trained models perform much worse than models pretrained on text. For\na reference, models trained from scratch achieve comparable performance to previously reported\nmodels.\n10https://huggingface.co/docs/transformers/model_doc/imagegpt\n18\nTable 10: Results for the ImageGPT model (image pretraining), with and without TWIST. We report\nPPL, sWUGGY and sBLIMP. Unlike textual pretraining, image pretraining not only does not benefit\nSpeechLMs, but substantially hurts their performance.\nTWIST\n# TOKENS\nFREQ.\nPPL\u2193\nSWUGGY\u2191\nSBLIMP\u2191\n\u2717\n200\n50Hz\n5.22\n72.22\n55.70\n\u2713\n8.21\n54.26\n51.90\n\u2717\n500\n25Hz\n6.20\n81.85\n56.39\n\u2713\n7.85\n72.50\n51.84\nA.7\nFull human evaluation (MMOS) results\nWe include the full Human evaluation (MMOS) results, corresponds to Fig. 3 in Table 11.\nTable 11: Full human evaluation (MMOS) results. We report MMOS score as: mean (95% confidence-\ninterval).\nMETHOD\nLS-CLEAN\nLS-OTHER\nLL-OTHER\nOriginal\n4.11(\u00b10.04)\n4.23(\u00b10.04)\n4.05(\u00b10.05)\nResynthesis\n3.95(\u00b10.07)\n3.87(\u00b10.06)\n3.96(\u00b10.06)\nCOLD-INIT-1.3B\n3.34(\u00b10.08)\n3.37(\u00b10.06)\n3.31(\u00b10.07)\nTWIST-1.3B\n3.51(\u00b10.07)\n3.67(\u00b10.07)\n3.65(\u00b10.06)\nTWIST-7B\n3.79(\u00b10.06)\n3.85(\u00b10.07)\n3.81(\u00b10.06)\n19\n"
  },
  {
    "title": "Augmenting Autotelic Agents with Large Language Models",
    "link": "https://arxiv.org/pdf/2305.12487.pdf",
    "upvote": "1",
    "text": "Preprint.\nAUGMENTING AUTOTELIC AGENTS WITH\nLARGE LANGUAGE MODELS\nC\u00b4edric Colas\nMIT, Inria\nccolas@mit.edu\nLaetitia Teodorescu\nInria\nlaetitia.teodorescu@inria.fr\nPierre-Yves Oudeyer\nInria\nXingdi Yuan\nMicrosoft Research\nMarc-Alexandre C\u02c6ot\u00b4e\nMicrosoft Research\nABSTRACT\nHumans learn to master open-ended repertoires of skills by imagining and practicing their own\ngoals. This autotelic learning process, literally the pursuit of self-generated (auto) goals (telos),\nbecomes more and more open-ended as the goals become more diverse, abstract and creative. The\nresulting exploration of the space of possible skills is supported by an inter-individual exploration:\ngoal representations are culturally evolved and transmitted across individuals, in particular using\nlanguage. Current arti\ufb01cial agents mostly rely on prede\ufb01ned goal representations corresponding\nto goal spaces that are either bounded (e.g. list of instructions), or unbounded (e.g. the space of\npossible visual inputs) but are rarely endowed with the ability to reshape their goal representations, to\nform new abstractions or to imagine creative goals. In this paper, we introduce a language model\naugmented autotelic agent (LMA3) that leverages a pretrained language model (LM) to support the\nrepresentation, generation and learning of diverse, abstract, human-relevant goals. The LM is used\nas an imperfect model of human cultural transmission; an attempt to capture aspects of humans\u2019\ncommon-sense, intuitive physics and overall interests. Speci\ufb01cally, it supports three key components\nof the autotelic architecture: 1) a relabeler that describes the goals achieved in the agent\u2019s trajectories,\n2) a goal generator that suggests new high-level goals along with their decomposition into subgoals\nthe agent already masters, and 3) reward functions for each of these goals. Without relying on any\nhand-coded goal representations, reward functions or curriculum, we show that LMA3 agents learn\nto master a large diversity of skills in a task-agnostic text-based environment.\n1\nINTRODUCTION\nEach human learns an open-ended set of skills across their life: from throwing objects, building Lego structures\nand drawing stick \ufb01gures, to perhaps playing tennis professionally, building bridges or conveying emotions through\npaintings. These skills are not the direct product of evolution but the result of goal-directed learning processes. Although\nmost living creatures pursue goals that directly impact their survival, humans seem to spend most of their time pursuing\nfrivolous goals: e.g. watching movies, playing video games, or taking photographs (Chu & Schulz, 2020).\nThe \ufb01eld of developmental AI models these evolved tendencies with intrinsic motivations (IM), internal reward systems\nthat drive agents to experience interesting situations and explore their environment (Singh et al., 2010; Oudeyer &\nKaplan, 2007). While knowledge-based IMs drive agents to learn about the world (Aubret et al., 2019; Linke et al.,\n2020), competence-based IMs drive agents to learn to control their environment (Oudeyer & Kaplan, 2007; Colas\net al., 2022b). Agents endowed with these intrinsic motivations are autotelic; they are intrinsically driven (auto) to\nlearn to represent, generate, pursue and master their own goals (telos) (Colas et al., 2022b). Open-ended learning\nprocesses require the joint training of a problem generator (e.g. environment dynamics, opponents, goals) and a problem\nsolver: the former challenging the latter in more and more complex scenarios, providing a never-ending curriculum\nfor the problem solver (Schmidhuber, 2013; Wang et al., 2020; Ecoffet et al., 2021; Jiang et al., 2021; Team et al.,\n2023). Autotelic agents are speci\ufb01cally designed for open-ended skill learning by jointly training a goal generator and a\ngoal-conditioned policy, see a review in Colas et al. (2022b).\nHuman skill learning is a cultural process. Most of the goals we care about are in\ufb02uenced by the goals of others: we clap\nhands to give encouragements, we strive to \ufb01nish college, or learn to play the piano. For this reason, we cannot expect\nautotelic agents learning in isolation to learn to represent goals that we care about, nor to bootstrap an open-ended skill\n1\narXiv:2305.12487v1  [cs.AI]  21 May 2023\nPreprint.\n\ud83e\udd16\n\ud83c\udf0d\nYou are playing a video game, here is \nwhat you did previously:\n<trajectory>\nExercise: find a sequence of 2, 3, or 4 \ninstructions that will help me achieve a \nnew, interesting or creative goal in this \ngame.\nInstructions:\n<previous goal 1>\n<previous goal 2>\n...\nFirst describe the new goal starting \nwith an imperative verb; then list the \ninstructions.\nAnswer:\nGoal: prepare cilantro dish\nInstructions: open fridge; \npick up cilantro; pick up knife; \ncut cilantro; cook cilantro\n\ud83e\udd16\n\ud83c\udf0d\nStep 0: open fridge\nStep 0: you open the fridge.\nStep 1: pick up cilantro\nStep 1: you pick up the cilantro.\nStep 2: close fridge\nStep 2: you close the fridge.\nStep 3: pick up knife\nStep 3: you pick up the knife.\nStep 4: pick up pork chop\nStep 4: you pick up the pork chop.\nStep 5: chop cilantro\nStep 5: you chop the cilantro.\nStep 6: roast pork chop\nStep 6: you roast the pork chop.\nStep 7: prepare meal\nStep 7: you prepare the meal.\nStep 8: eat meal\nStep 8: you eat the meal.\nStep 6: you roast the pork chop.\nStep 7: prepare meal\nStep 7: you prepare the meal.\nStep 8: eat meal\nStep 8: you eat the meal.\nExercise: Given the description of a \nplayer's behavior in a video game, list \nthe most interesting, impressive, novel \nor creative goals he achieved and, for \neach goal, specify when it is achieved \nfor the first time.\nAnswer:\nOpen fridge (Step 0)\nEnjoy delicious meal (Step 8)\nOpen fridge and then close it \n(Step 2)\nEat healthy (Step 8)\nClean kitchen (Step 4)\n\ud83e\udd16\n\ud83c\udf0d\n\ud83e\udd16\n\ud83c\udf0d\nStep 8: eat meal\nStep 8: you eat the meal.\nExercise: Given the description of a \nplayer's behavior in a video game, list \nthe most interesting, impressive, novel \nor creative goals he achieved and, for \neach goal, specify when it is achieved \nfor the first time.\nHere is the list: \u201cprepare cilantro \ndish\u201d, \u201copen fridge\u201d, \u201cenjoy delicious \nmeal\u201d, \u201copen fridge and close it\u201d, \u201ceat \nhealthy\u201d, \u201cclean kitchen\u201d\nAnswer: \nEnjoy delicious meal: yes (step 8)\nOpen fridge and then close it:\nyes (step 2)\nEat healthly: yes (step 8)\nClean kitchen: no\nPrepare cilantro dish: yes (step 7)\nOpen fridge: yes (step 0)\n...\nGoal Generator\nRollout\nRelabeler\nReward Function\nPrepare cilantro dish\nOpen fridge; \npick up cilantro...\nPrepare cilantro dish\nOpen fridge; \npick up cilantro...\nFigure 1: Components of the Language Model Augmented Autotelic Agent (LMA3). LMA3 agents evolve in a\ntask-agnostic interactive text-based environment. Messages have different color depending on their source: environment\nis red, LM is blue, learned policy is green. Goal Generator: the agent prompts the LM with a previous trajectory and a\nlist of mastered goals to generate a high-level goal and its subgoal decomposition. Rollout: The agent then attempts to\nexecute the sequence of subgoals in the environment using its learned policy (green). Relabeler: the agent prompts\nthe LM with the trajectory obtained during the rollout and asks for a list of re-descriptions of that trajectory (achieved\ngoals). Reward Function: the agent prompts the LM with the trajectory and a list of goals to measure rewards for: the\nmain goal, the subgoals and the goal redescription generated by the relabeler. See complete prompts in Appendix B.\nlearning process on their own. Just like humans, they should bene\ufb01t from forms of cultural transmissions; they should\nlearn to pursue other\u2019s goals, modify and combine them to build their own, and perhaps in\ufb02uence the goals of others.\nA signi\ufb01cant part of this cultural transmission is supported by language. Transmission can be explicit when it leverages\nthe communicative functions of language: we learn and teach via direct advice, books and online tutorials. It can also be\nimplicit when it relies on the cognitive functions of language: words help us represent categories (Waxman & Markow,\n1995), analogies help us represent abstract knowledge (Gentner, 2016; Dove, 2018), and linguistic productivity helps us\ngenerate new ideas by recombining known ones (Chomsky & Lightfoot, 2002). These cultural transmissions let us\nleverage the skills and knowledge of others across space and time, a phenomenon known as the cultural ratchet (Tennie\net al., 2009).\nBuilding on these insights, we propose to augment arti\ufb01cial agents with a primitive form of cultural transmission.\nAs current algorithms remain too sample inef\ufb01cient to interact with humans in real time, we leverage a pretrained\nlanguage model (LM) as a (crude) model of human interests, biases and common-sense. Our proposed Language Model\nAugmented Autotelic Agent (LMA3) uses an LM to implement: 1) a relabeler that describes the goals achieved in the\nagent\u2019s trajectories, 2) a goal generator that suggests new high-level goals along with their decomposition into subgoals\nthe agent already masters, and 3) reward functions for each of these goals. We demonstrates the capabilities of this\nagent in a text-based environment where goals, observations and actions are all textual. Figure 1 depicts the overall\nprocess.\nThe relabeling functions makes use of LM\u2019s ability to segment unstructured sequences into meaningful events and\nto focus on the most relevant ones (Michelmann et al., 2023). This allows the autotelic agent to represent possible\ngoals and implement a form of linguistic data augmentation (Xiao et al., 2022). The goal generator builds on the set of\ngoals already achieved to suggest more abstract goals expressed as sequences of subgoals. It implements a goal-based\nexploration similar to the one of Go-Explore (Ecoffet et al., 2021). Finally, the reward function ensures that the agent\ncan compute goal-completion signals, the necessary reward signals to implement any kind of goal-directed learning\nalgorithm. Augmented by LMs, our autotelic agents learn large repertoires of skills in a task-agnostic interactive text\nenvironment without any hand-coded goals or goal-speci\ufb01c reward functions.\n2\nPreprint.\nThe present paper focuses on the generation of diverse, abstract and human-relevant goals in a task-agnostic environment.\nBecause the problem of training a policy that performs and generalizes well to a large set of goals is orthogonal to\nthe tackled issue, we limit ourselves to a crude goal-directed learning algorithm in order to limit the computational\nbudget of calling for an LM API (see computational budget calculations in Section 5). In Section 5, we discuss how the\ninsights gained in this paper can be leveraged to implement more open-ended learning systems.\n2\nRELATED WORK\nSkill learning can be modeled mathematically as a reinforcement learning problem (RL) (Sutton et al., 1998). In an RL\nproblem, the learning agent perceives the state of the world s \u2208 S, and act on it through actions a \u2208 A. These actions\nchange the world according to a stochastic dynamic function that characterizes the environment T : S \u00d7 A \u2192 S. An\nagent learns a skill by training a policy \u03a0 : S \u2192 A to sample action sequences that maximize its expected future\nreturns R computed from a prede\ufb01ned reward function R : S \u00d7 A \u00d7 S \u2192 R using a temporal discount factor \u03b3 \u2208 [0, 1]\nsuch that R = P\nt rt \u00b7 \u03b3t. Multi-goal RL problems extend the RL problem to support the learning of multiple skills in\nparallel (Schaul et al., 2015). The agent now pursues goals g \u2208 G, each associated with their own reward function Rg\nand trains a goal-conditioned policy \u03a0G : S \u00d7 G \u2192 A to learn the corresponding skills. In RL literature, a trajectory \u03c4\nis a sequence of transitions where each transition is a tuple containing information at a certain time step t: the state st\nand the action taken at.\nBut where do goals come from? Most approaches hand-de\ufb01ne the set of admissible goals and corresponding reward\nfunctions. They let agents either sample goals uniformly (Schaul et al., 2015), or build their own curriculum (Portelas\net al., 2020). When the goal space is large enough, this can lead to the emergence of diverse and complex skills (Team\net al., 2023). Truly open-ended skill learning, however, requires the frequent update of goal representations as a function\nof the agent\u2019s current capabilities \u2014 only then can it be never-ending. To this end, the autotelic framework proposes to\nendow learning agents with an intrinsic motivation to represent and generate their own goals (Colas et al., 2022b).\nLearning to represent, imagine and sample goals to learn skills that humans care about requires interactions with human\nsocio-cultural worlds (see argument in introduction, and Colas et al. (2022a)). Autotelic agents must \ufb01rst internalize\nthe goal representations of humans before they can learn corresponding skills, build upon them and contribute back to\na shared human-machine cultural evolution. Goal representations can be learned by inferring reward functions from\nhuman demonstrations (Ng et al., 2000; Arora & Doshi, 2021), via unsupervised representation learning mechanisms\n(Warde-Farley et al., 2018; Eysenbach et al., 2019; Pong et al., 2020), or by learning to identify the goals achieved in\npast trajectories from human descriptions (trajectory relabeling, Andrychowicz et al. (2017); Lynch & Sermanet (2020);\nXiao et al. (2022)).\nBuilding on goal representations learned from linguistic descriptions generated by a simulated social partner, the\nImagine agent invents new linguistic goals recomposed from known ones (Colas et al., 2020). Although crude, this goal\nimagination system allows the agent to pursue and autonomously train on creative goals it imagines, which results in\nimproved systematic generalization and more structured exploration. The present paper extends the Imagine approach\nby leveraging powerful language models to implement several components of the autotelic agent: goal representations,\ngoal-directed reward function and relabeling system. Imagine required a (simulated) human in the loop to bootstrap\ngoal representations and could only imagine slight variations of training goals due to its limited imagination algorithm,\nits lack of grounding and the limited generalization of its reward function. On the other hand, LMA3 does not require\nany human or engineer input and can generate and master a much wider diversity of goals thanks to the common-sense\nknowledge and generalization capabilities of LMs.\nWe evaluate our proposed agent in an interactive text-based environment where observations and actions are all textual\n(C\u02c6ot\u00b4e et al., 2019). Text-based environments set aside the challenges of learning from low-level sensors and actuators\nand focus on higher-level issues: learning in partially observable worlds, learning temporally-extended behaviors,\nlearning the human-like common-sense required to solve these tasks ef\ufb01ciently, etc (He et al., 2016; Narasimhan et al.,\n2015; C\u02c6ot\u00b4e et al., 2019). Text-based environments circumvent the necessity to ground LMs into low-level sensorimotor\nstreams (e.g. visual inputs and low-level motor outputs) and let us focus on the arti\ufb01cial generation of more abstract,\nhuman-relevant goals. This paper is the \ufb01rst to implement autotelic agents with no prior goal representations in text\nworlds.\nPretrained language models have recently been used to augment RL agents in various ways. In robotics setups, they\nwere used to decompose prede\ufb01ned high-level tasks into sequences of simpler subgoals (Yao et al., 2020; Huang\net al., 2022a;b; Ahn et al., 2022). To limit the hallucination of implausible plans, several extensions further constrain\nthe model by either careful prompting (Singh et al., 2022), by asking the model to generate code-based policies that\nautomatically checks for preconditions before applying actions (Liang et al., 2022) or by implementing further control\n3\nPreprint.\nmodels to detect plan failures and prompt the LM to suggest corrected plans (Wang et al., 2023). LMs can be used to\nimplement a reasoning module to facilitate the resolution of sensorimotor tasks (Dasgupta et al., 2023). They can be\n\ufb01netuned to implement the policy directly (Carta et al., 2023). Closer to our work, the MineDojo approach \ufb01netunes a\nmultimodal model to implement a reward function in Minecraft and asks an LM to generate plausible goals to measure\nthe generalization of the reward function (Fan et al., 2022). Finally, the ELLM algorithm prompts an LM to suggest\nexploratory goals to drive the pretraining of arti\ufb01cial agents in Crafter and HouseKeep (Du et al., 2023b).\nOur proposal differs from these approaches in several ways. Our agent is autotelic: it generates its own goals, computes\nits own rewards. In text-based games, our architecture can handle a large diversity of goals including time-extended\nones that can only be evaluated over long trajectories (e.g. bring the onion to the counter after you\u2019ve opened and closed\nthe dishwasher). In contrast, the MineDojo agent has no control over its goals and is limited to generate rewards for a\nlow diversity of short-term goals due to the limited generalization capabilities of the CLIP-based reward (Fan et al.,\n2022). ELLM generates its own exploration goals but only considers goals that can be reported from a single state\n(time-speci\ufb01c goals). It computes rewards using the similarity between the LM-generated goal and descriptions from a\ncaptioner, which fundamentally limits the diversity of goals that can be targeted to the list of behaviors the captioner can\ndescribe. In the current setup, training or learning a captioner requires some information about the set of behaviors the\nagent could achieve, which limits the potential for open-ended learning. Compared to ELLM, LMA3 further endows\nthe agent with the ability to perform hindsight learning by relabelling past trajectories (Andrychowicz et al., 2017) and\nthe ability to chain subgoals to formulate more complex goals. In contrast with previous approaches (Yao et al., 2020;\nHuang et al., 2022a;b; Ahn et al., 2022), we do not leverage expert knowledge to restrict the set of subgoals but learn\nthem online and add the possibility for the agent to use these composed goals as subgoals for future goal compositions.\n3\nMETHODS\nThis section introduces our learning environment and assumptions (Section 3.1), as well as the proposed Language\nModel Augmented Autotelic Agent (LMA3, Section 3.2).\n3.1\nSETTING AND ASSUMPTIONS\nProblem setting. In a task-agnostic environment, we aim to implement the automatic generation of context-sensitive,\nhuman-relevant, diverse and creative goal representations. Goal representations are not only goal descriptions but also\nassociated reward functions. Given a suf\ufb01ciently effective learning algorithm, an autotelic agent endowed with such a\ngoal generation system should learn a large diversity of skills in task-agnostic environments.\nLearning environment. We place the LMA3 agent in a text-based environment called CookingWorld (C\u02c6ot\u00b4e et al., 2019;\nMadotto et al., 2020). The agent receives textual observations and acts via textual commands. Is it not provided with a\nprede\ufb01ned list of goals or reward functions. The agent is placed in a kitchen \ufb01lled with furniture (7 including dining\nchair, fridge, counter, etc), tools (4 including knife, toaster) and ingredients (7 including potatoes, apples, parsley).\nAcross 25 consecutive timesteps, the agent can pick up and put down objects, open and close containers, cut and cook\ningredients in various ways, and \ufb01nally combine them to make recipes. At any step, the agent uses its learned policy to\nchoose an action from the admissible actions: the subset (N \u2248 30-50) of all possible actions (N = 143) that the agent can\ntake in the current context (e.g. the agent needs to \ufb01nd and pick up the knife before it can cut any ingredient). Examples\nof goals the agent could imagine and learn to master include: slice a yellow potato, cook two red ingredients, tidy up the\nkitchen by putting the knife in the cutlery drawer, aim to use all three types of potatoes in the dish, etc.\nAssumptions. We make the following assumptions: 1) we only consider text-based environments to allow straightfor-\nward compatibility with the LM (see discussion in Section 5); 2) we assume access to a language model suf\ufb01ciently large\nto capture aspects of human common-sense and interests and allow in-context few-shot learning (see implementation\naspects below); 3) the agent is spawned in the same deterministic environment at the beginning of each episode. We use\na deterministic environment for two reasons. First, because the goal generator needs to know about the environment to\ngenerate feasible goals. This is achieved by prompting the goal generator with a past trajectory in that same environment.\nSecond, because it allows us to implement skill learning with a simple evolutionary algorithm, which considerably\nreduces the sample complexity and thus the cost of querying the LM \u2014 albeit to the detriment of generalization. Note,\nhowever, that robustness and generalization of acquired skills are not the focus of this paper, which is interested in the\nautomatic generation of diverse and human-relevant goals. In contrast to most goal-conditioned approaches, we do not\nassume access to a prede\ufb01ned set of goal representations or reward functions.\n4\nPreprint.\nCookingWorld\n\ud83c\udf7d\nPresent\nenvironment\nAccumulated past\nknowledge\nAutotelic Agent \ud83e\udd16\nExternal World \ud83c\udf0d\nPeople, books\nconversations,\ninstruction\n(LLM training)\nSkill\nlearning\nGoal\nrepresentations\nModel \nof cultural\ntransmission\nLLM (ChatGPT) \ud83d\udde8 \nInternalized common-sense, intuitive physics (Language-based semantic memory)\nReward\nFunction\nRelabeler\nGoal\nGenerator\nPolicy\nselect traj\nadd to dictionary\nrelabel\ngenerate\nsuccess\ng0\n(a0\n0, a0\n1, a0\n2, ...)\ng1\n(a1\n0, a1\n1, a1\n2, ...)\nGoal prompt\nRelabeler prompt\nReward prompt\nat\nst\n\u03c4\n\u03c4\n\u03c4*\ng\nquery\nquery\nquery\ng\n\u03c4\n\u03c4*\ng\ng\nFigure 2: General architecture of LMA3. LMA3 assumes access to a model of cultural transmission implemented\nvia ChatGPT (dashed line). As shown in the goal representations block, LMA3 leverages that model to generate goals\ng (left), relabel past trajectories \u03c4 = (s0, a0, ..., sT , aT ) as \u03c4 \u2217\ng (middle) and compute rewards when goals are reached\n(right). st and at denote the state representation and the action taken at game step t. The goal-conditioned policy (top)\nattempts to reach its goal within CookingWorld (right) and uses relabels and rewards for learning.\n3.2\nLANGUAGE MODEL AUGMENTED AUTOTELIC AGENT (LMA3)\nGeneral architecture. LMA3 augments a traditional multi-goal learning architecture with goal representations and\ngoal generation powered by an LM (Figure 2). In contrast with a standard multi-goal architecture which prede\ufb01nes\na bounded distribution of reward functions, the set of goals and associated reward functions supported by the LM is\nvirtually unbounded; it includes all goals expressible with language. The goal generator samples a goal for the goal-\nconditioned policy to pursue (see LM Goal Generator below). Following the hindsight experience replay mechanism\n(Andrychowicz et al., 2017), we relabel trajectories obtained from rolling out the policy and add those to a replay\nbuffer. The relabeling process is implemented by another LM instance which labels up to 10 goals achieved during the\ntrajectory along with their precise completion timestamps (see LM Relabeler below). The goal-conditioned policy\nis then trained with a simple evolutionary algorithm (see Skill Learning below). For all prompts, we provide two\nexamples (few-shot prompting Brown et al. (2020)) and reasoning descriptions (chain-of-thought prompting Wei et al.\n(2022); Kojima et al. (2022)). We provide the complete prompts in Appendix B.\nLM Goal Generator. In the \ufb01rst episode, the agent does not have any goal representation and simply samples random\nactions. Then, the agent enters a bootstrapping phase of 4000 episodes for which it uniformly samples goals from\nthe set of discovered goals generated by the relabeler and validated by the reward function (see LM Relabeler and\nLM Reward Function below). This allows the agent to \ufb01rst focus on simple goals. After that bootstrapping phase,\nthe agent starts using the LM Goal Generator. At the beginning of each episode, we provide context to the LM by\nprompting it with the agent\u2019s trajectory in the previous episode and a list of up to 60 goals previously reached by the\nagent. Then, we ask it to generate a high-level goal and its decomposition in a sequence of 2\u20134 subgoals from the list.\nThe decomposition lets an agent explore its environment by chaining sub-skills it knows about. This can be seen as\nan extension of the Go-Explore strategy where, after achieving the \ufb01rst goal (go), the exploration is further structured\ntowards another goal (explore) that is a plausibly useful continuation of the \ufb01rst (Ecoffet et al., 2021).\nLM Relabeler. After each episode, we prompt the LM with the trajectory of actions and observations and ask for\na list of up to 10 descriptions of the goals achieved in the trajectory, as well as speci\ufb01c timestamps for when they\nwere achieved. Note that the goal pursued by the agent in the episode does not matter, the LM Relabeler is free to\nprovide any description of the trajectory. For each goal description we generate a positive trajectory like so: we create a\nsub-trajectory from step 0 to the step of goal completion, assign a positive reward to the last step and add it to the replay\nbuffer.\nWe further investigate the impact of leveraging human advice nudging the agent to focus on more abstract and creative\ngoal descriptions. We do so by replacing the 11 simple examples provided in the prompt with 11 more elaborate\nones. Instead of describing simple goals involving one action on one speci\ufb01c object (e.g. roast a white onion), we\n5\nPreprint.\nprovide examples involving sequences of actions (e.g. use the oven for the second time), conjunctions of several actions\n(e.g. roast an onion and a bell pepper and fry carrots) or more abstract verbs (e.g. \ufb01nd out whether the keyholder has\nsomething on it). In contrast with hard-coded relabeling systems, the LM Relabeler uses more linguistic diversity\n(e.g. synonyms), can describe combinations of actions (e.g. cook two onions), or more abstract actions (e.g. hide an\nobject).\nLM Reward Function. After each episode, we prompt the LM with the trajectory and ask whether the agent achieved\nany of the following goals: 1) the main high-level goal given by the LM Goal Generator, 2) each of the subgoals (after\nthe bootstrapping phase), 3) each of the relabels generated by the LM Relabeler. 1 and 2 provide feedback to the agent\nabout the goals it was attempting to reach while 3 provides a double check of the redescriptions offered by the LM\nRelabeler, which could be prone to hallucinations.\nSkill Learning. This paper focuses on the generation of diverse, human-relevant goals and the study of a self-\nbootstrapped goal imagination and redescription system powered by LMs. To simplify skill learning, we consider\na deterministic environment and evolve sequences of actions conditioned on the goal given a simple evolutionary\nalgorithm. For each goal description generated by the LM Relabeler, the agent stores the sequence of actions that led\nto the completion of that goal in a dictionary. If this goal was achieved previously, it only stores the shortest action\nsequence. When prompted to achieve a sequence of subgoals, the agent chains the corresponding action sequences\ntogether to form the goal-directed policy. Exploration is supported by two mechanisms: 1) by chaining action sequences\nprompted by the LM Goal Generator and 2) by truncating the action sequence towards the last subgoals in the chain at a\nuniformly sampled time with probability \u03f5 = 0.2. After executing the action sequences for all subgoals, and perhaps\nhaving truncated the last one, the agent samples actions randomly from the set of admissible actions in proportion of\ntheir rarity (i.e. 1 over their occurrence).\n4\nEXPERIMENTS\nWe compare LMA3 to three ablations and an oracle baseline in the task-agnostic CookingWorld environment to assess\nits learning abilities. For all experiments, we plot the average and standard deviations across 5 seeds. For the different\nLM modules, we use ChatGPT (gpt-3.5-turbo-0301) as provided through the OpenAI API (Brown et al., 2020). The\ncode will be released publicly with the camera-ready version of the paper.\nAblations and oracle baseline. LMA3 is the \ufb01rst algorithm to allow the automatic generation of linguistic goals and\nreward functions with no interventions from the engineer. For this reason, there were no obvious baselines to compare\nLMA3 with. We consider three ablations that remove: 1) the use of human advice in the prompting of the LM Relabeler\n(LMA3 \\ Human Tips), 2) the use of human advice and the LM Goal Generator (LMA3 \\ LM Goal & Human Tips),\n3) the use of human advice and chain-of-thought prompting (LMA3 \\ CoT & Human Tips). In the absence of the LM\nGoal Generator, goals are uniformly sampled from the set of goals previously discovered, just like in the bootstrapping\nphase of the LM Goal Generator (see Section 3). Our baseline is a standard goal-conditioned agent trained on a\nhard-coded set of 69 goals involving picking up, cooking or cutting objects in the text-based environment (Hardcoded\nOracle Baseline). This baseline samples goal uniformly from the set of goals previously discovered and uses an oracle\nrelabeler. It implements a standard goal-conditioned policy learning algorithm in text-based environments.\nPerformance on a human-de\ufb01ned goal space. We want to measure the ability of autotelic agents to learn skills that\nhumans care about. However, autotelic agents learn their own goal representations in worlds that can afford a large space\nof possible actions and, for this reason, there is no objective set of goals these agents should learn about, no objective\nevaluation set. We hand-de\ufb01ned a set of 69 evaluation goals involving picking up, cooking or cutting objects in the\ntext-based environment (see list in Appendix A). This list is obtained by applying each of the possible action types of\nthe agent (e.g. slice, dice, roast, pick up, put, open, close) to each possible object (e.g. slice+ingredient, open+container)\nand adding the goal of preparing the recipe the agent can \ufb01nd in the cookbook present in the CookingWorld. While the\nHardcoded Oracle Baseline is explicitly trained on this set of goals and can make use of an oracle reward function and\nrelabeling function, LMA3 variants are not given any prior knowledge of these goals.\nFigure 3 presents the success rates on this evaluation set computed with the hard-coded reward functions corresponding\nto each of the 69 goals (not given to LMA3 agents). This metrics evaluates a minimal requirement: can LMA3 agents\nlearn to master some of the goals human care about in this world without assuming prede\ufb01ned representations for these\ngoals? Most LMA3 variants learn to reach a large fraction of the evaluation goals, which indicates that leveraging\ngoal representations captured by LMs may support the autonomous learning of skills that humans care about. The\nHardcoded Oracle Baseline makes use of an oracle relabeling function that can faithfully detect any of the 69 goals\nwhen it is reached in a trajectory. If we now provide this function to a trained LMA3 agent, we can sweep its memory\nof action sequences and \ufb01nd the ones achieving the evaluation goals. This form of \ufb01netuning does not require further\n6\nPreprint.\n0\n2000\n4000\n6000\n8000\n10000\nepisodes\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nevaluation score\nLMA3\nLMA3 \\ Human Tips\nLMA3 \\ LM Goals & Human Tips\nLMA3 \\ CoT & Human Tips\nHardcoded Oracle Baseline\n(a)\n(b)\nFigure 3: Performance on human-de\ufb01ned goal space. Performance on the hand-coded evaluation set containing 69\nhuman-relevant goals, as measured by hard-coded reward functions. (a) across training; (b) at the end of training, after\nrelabeling with the oracle relabeling function and without further interactions.\ninteractions in the environment. Applying it further boosts the success rates of LMA3 agents to near perfect results (see\nFigure 3b). This shows that LMA3 agents do reach the evaluation goals but sometimes fail to relabel them properly\nand instead choose to focus on describing other demonstrated behaviors. After \ufb01netuning, some of the LMA3 seeds\nmanage to complete the recipe found in the cookbook: a preparation that includes picking up cilantro and parsley\nfrom the fridge, opening the cupboard, taking the knife, slicing the parsley, preparing and eating the meal (2 LMA3\nseeds and 3 LMA3 \\ Human Tips seeds). These results con\ufb01rm that LMA3 can learn human-relevant goals completely\nautonomously, without relying on any prede\ufb01ned goal representations or reward functions.\nSkill diversity. We measure the diversity of discovered skills with 1) the raw number of distinct goals; 2) Hill\u2019s\nnumbers, a metric inspired from the evaluation of species diversity in ecosystems (Jost, 2006) and 3) a metric inspired\nfrom the h-index used in research. Hill\u2019s numbers allow to make a distinction between species-level diversity (here the\ntype of action required by the goal) and individual-level diversity (e.g. the object on which the action is performed; the\nobject used to perform the action, the location, etc). The type of action required by a goal is inferred as the linguistic\nstem of the \ufb01rst word of the goal\u2019s description. The goal cutting the apple is an individual goal of the cut goal species.\nHill\u2019s numbers provide measures of diversity computed as:\nDq =\n X\ns\u2208stems\npq\ns\n!\n1\n1\u2212q\n,\nwhere ps is the empirical fraction of goals with stem s and q controls the sensitivity of Dq to rare vs. abundant\nspecies: D0 is the count of stems, also called species richness (no sensitivity to species abundance) and D1 is the\nexponential of Shannon\u2019s entropy, also called perplexity, a measure that quanti\ufb01es the uncertainty in predicting the\nstem\u2019s identity of a goal uniformly sampled from the set of discovered goals (Jost, 2006). Lower q puts more emphasis\non species diversity while higher q puts more emphasis on individual diversity. Note that stems can hide part of the\ninformation: different stems might refer to similar behaviors (e.g. grab vs pick up) while a same stem might refer to\ndifferent behaviors (e.g. pick up the apple vs pick up the meal; the 2nd requiring a more complex and time-extended\nbehavior). Such analysis still provides complementary information to the simple goal count. Finally, the stem h-index is\ncomputed as the maximum value h such that h stems have at least h goals. This metric is used in research to compute a\nscore mixing diversity and quality of paper citations, here it is used as a way to balance species-level (action type) and\nindividual-level (the rest) diversities.\nFigure 4 reports the total number of discovered goals, Hill\u2019s numbers for q \u2208 {0, 1} as well as the stem h-index. While\nthe introduction of the LM Goal Generator (episode 4000) introduces a slight boosts in the diversity of discovered goals\n(orange vs green), the addition of human advice triggers a lasting increase in goal diversity (blue vs orange). Removing\nCoT prompting dramatically decreases the diversity of goals discovered by LMA3 (red vs orange). Finally, the diversity\nof the Hardcoded Oracle Baseline remains low as it is restricted to consider the 69 goals from the hand-de\ufb01ned set of\ngoals. LMA3 discovers around 9000 distinct goal redescriptions in 10000 episodes.\nSensitivity to human advice. We measure the sensitivity of LMA3 agents to a small amount of human advice.\nLMA3 leverages human advice in the prompt of the LM Relabeler, with only a few examples nudging the relabeler\nto describe more abstract behaviors involving conjunctions of actions (e.g. roast an onion and a bell pepper and\n7\nPreprint.\nfry carrots), repetition of actions (e.g. open three containers), non-speci\ufb01c object references (e.g. slice and cook an\norange ingredient) or more abstract action predicates (e.g. \ufb01nd out whether the keyholder has something on it), see\nfull prompts in Appendix B. Although the conditions with/without human advice use the same number of examples in\ntheir prompts, the more abstract examples used in the full LMA3 condition drives a signi\ufb01cant increase in both the\ndiversity of discovered goals (Figure 4) and in the abstraction of these goals as measured by the proportion of goals\ncontaining conjunctions (\u201cand\u201d, \u201ctwo\u201d, \u201cthree\u201d, or \u201cseveral times\u201d), or category names instead of speci\ufb01c object names\n(\u201cingredients\u201d, \u201citems\u201d, \u201ccontainer\u201d, \u201csomewhere\u201d, \u201cfruit\u201d, \u201cvegetable\u201d, or \u201ctool\u201d) see Figure 5. Note that the effect on\ncategory names, although signi\ufb01cant, remains small.\nDiscovery of unique goals. We measure the ability of each agent to discover unique goals; goals that were discovered\nby this particular seed but were not encountered by any other seed across all algorithm conditions. For each seed, we\ncompute the number of such unique goals, report the ratio of that number over the count of all goals discovered by\nthe agent, and give a measure of the novelty of these unique goals by reporting their average distance to the nearest\nneighbor in the linguistic embedding space of all goals discovered by all seeds of all conditions. This embedding is\ncomputed from a pretrained SentenceBERT model (Reimers & Gurevych, 2019). Figure 6 shows that LMA3 agents\ndiscover more unique goals, not only in number but also in proportion of the total number of goals they discover (b) and\nthat these goals are more novel in average (c).\nMastery of discovered goals. We measure the skill mastery of agents on several subsets of the goals they discover. For\neach seed, we compute the success rate of the corresponding agent on three sets of 200 evaluation goals: 1) a uniform\nsample of all goals discovered by the agent; 2) a uniform sample of the goals only they discovered (unique goals) and\n3) a uniform sample of the set of all unique goals from other agents (Figure 7a). We evaluate successes with the LM\nReward Function. Note that although the environment is deterministic, mistakes in the LM Relabeler or the LM Reward\nFunction could lead agents to mistakenly classify a goal as reached when it is not. To measure the reliability of this\nimperfect reward function, we computed its confusion matrix given a set of N=100 trajectories using human labels as\nground truth. We estimate the probabilities of both false positives and false negatives to 9% (see confusion matrix in\nFigure 7b).\nExamples of discovered goals. As discussed above, LMA3 agents discover most of the hand-de\ufb01ned evaluation\ngoals (e.g. slice a yellow potato, pick up the knife, open the fridge). They also learn to consider goals expressed as\nconjunctions or disjunctions of simpler goals (e.g. cook two red ingredients, put a potato red or yellow in the kitchen\ncupboard, examine an object in the kitchen, like the oven, yellow potato, green or red apple). They sometimes express\ngoals in more abstract ways (e.g. wield the knife, waste food, tidy up the kitchen by putting the knife in the cutlery\ndrawer, pack potatoes and apples in the dishwasher, refrigerate the yellow apple). They can use object attributes or\nobject categories to refer to sets of objects (e.g. put a yellow ingredient in the kitchen cupboard, aim to use all three\ntypes of potatoes in the dish, choose to place an ingredient on a dining chair instead of the counter). In contrast with\nhand-de\ufb01ned goals, these goals use new words, category words, or abstract action predicates that do not appear in the\nvocabulary of the text-based environment.\nFinally, the LM Goal Generator generates more complex goals and decomposes them into subgoals the agent masters:\ne.g. rearrange the yellow apple and yellow potato inside the kitchen cupboard \u2192 [pick up the yellow apple, take the\nyellow potato, place the yellow potato in the kitchen cabinet, place the yellow apple in the cupboard]; assemble a meal\nwith a fried yellow potato and a roasted red apple on a dining chair \u2192 [fry the yellow potato, cook a red apple in\nthe oven, place the red apple and the yellow potato on the dining chair]; serve a meal consisting of a roasted sliced\n0\n2000\n4000\n6000\n8000\n10000\nepisodes\n0\n2000\n4000\n6000\n8000\nnumber of relabels\nLMA3\nLMA3 \\ Human Tips\nLMA3 \\ LM Goals & Human Tips\nLMA3 \\ CoT & Human Tips\nHardcoded Oracle Baseline\n(a)\n0\n2000\n4000\n6000\n8000\n10000\nepisodes\n0\n20\n40\n60\n80\n100\n120\n140\nHill's number, q=0 (richness, count)\n(b)\n0\n2000\n4000\n6000\n8000\n10000\nepisodes\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\nHill's number, q=1 (perplexity)\n(c)\n0\n2000\n4000\n6000\n8000\n10000\nepisodes\n0\n10\n20\n30\n40\nStems h-index\n(d)\nFigure 4: Diversity of achieved goals: (a) number of relabels, (b) number of stems (Hill\u2019s number with q=0), (c)\nperplexity (Hill\u2019s number, p=1), (d) stem\u2019s h-index. LMA3 discovers and masters a more diverse set of goals than its\nablations. The Hardcoded Oracle Baseline is limited to discover goals from the hand-de\ufb01ned set of 69 goals and thus\ndemonstrates very little diversity.\n8\nPreprint.\n0\n2000\n4000\n6000\n8000\n10000\nepisodes\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nProportion of conjunction\nLMA3\nLMA3 \\ Human Tips\nLMA3 \\ LM Goals & Human Tips\nLMA3 \\ CoT & Human Tips\nHardcoded Oracle Baseline\n(a)\n0\n2000\n4000\n6000\n8000\n10000\nepisodes\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\nProportion of category names\n(b)\nFigure 5: Discovery of more complex and abstract goals. LMA3 discovers and masters more complex goals\nexpressed as combinations of simpler goals, or using category names (e.g. ingredients, containers) instead of speci\ufb01c\nobject name as they appear in CookingWorld (e.g. yellow potato, kitchen drawer).\nyellow potato and a fried diced green apple \u2192 [dice and fry the green apple, slice a yellow potato, cook the yellow\npotato in the oven]. We found that the LM Goal Generator only generated a low diversity of complex goals: they all\nprompt the agent to prepare some form of recipe and vary in the properties of the recipes (e.g. vegetarian, colorful), or\nits particular ingredients (e.g. with roasted sliced potatoes). This behavior stems from the fact that CookingWorld is a\nrelatively narrow environment: a kitchen with ingredients and kitchen appliances.\nConceptual comparisons to other skill discovery approaches. Implementing the Imagine agent would require the\nde\ufb01nition of 1) a simulated human describing some of the agent\u2019s behavior (e.g. the 69 oracle goals) and 2) a hard-coded\nsymbolic goal imagination system (Colas et al., 2020). The original imagination system would be limited to imagine\nlinguistic recombinations of the training goals: e.g. from cut the apple, pick the apple and cut the parsley, Imagine\ncould generate pick up the parsley. These recombinations are either non-semantic (e.g. open the parsley) or already\ncontained in the set of 69 goals. As a result, Imagine would be strictly less powerful than the Oracle Baseline: it would\nnot imagine more goals but would need to learn a reward function from descriptions where the Oracle Baseline assumes\noracle reward functions.\nOther skill discovery methods are limited to low-level goal representations. Visual goal-conditioned approaches\nlearn goal representations by training a variational auto-encoder on experienced visual states and train the agent to\nimagine and reach new goals in that visual embedding space (Pong et al., 2020). One could imagine a variant of these\napproaches embedding linguistic trajectories in such a generative model but the resulting skills would not be particularly\nsemantically meaningful. Unsupervised skill discovery approaches co-train a skill discriminator and a skill policy with\nan empowerment reward (Eysenbach et al., 2019). A variant of these approaches for CookingWorld could consist in\nco-training a captioner (skill discriminator) and a policy to maximize the likelihood of the trajectory\u2019s caption being the\noriginal goal of the policy. This algorithm does not exist yet and it is unclear how the agent should sample goals.\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\nLMA3\nLMA3 \\ Human Tips\nLMA3 \\ LM Goals & Human Tips\nLMA3 \\ CoT & Human Tips\n(a)\n0.0\n0.1\n0.2\n0.3\n0.4\n(b)\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\n(c)\nFigure 6: Discovery of unique goals. For each algorithm, average number of unique goals that each agent was the only\none to discover (a), ratio of unique goals over the count of all goals discovered by the agent (b), and average novelty of\nthe unique goals computed in sentence embedding space (c).\n9\nPreprint.\nSR all discovered by agent\nSR unique from agent\nSR unique from others\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nLMA3\nLMA3 \\ Human Tips\nLMA3 \\ LM Goals & Human Tips\nLMA3 \\ CoT & Human Tips\n(a)\n(b)\nFigure 7: Self-evaluated performance. (a): Average success rates across seeds for each algorithm when computed\non goals discovered during training (left), on unique goals discovered by the agent and no other agent (middle), on a\nsample of unique goals from other agents, not discovered by the evaluated agent (right). (b): confusion matrix of the\nLM Reward Function tested over 100 human-relabeled trajectories (56 true success / 44 true failures).\n5\nCONCLUSION AND DISCUSSION\nThis paper introduced LMA3, an autotelic agent augmented with a language model capturing key aspects of human\ncommon-sense and interests to support the generation of diverse, abstract, human-relevant goals. In the CookingWorld,\nLMA3 can learn a large set of skills relevant to humans without relying on any prede\ufb01ned goal representations or reward\nfunctions. The diversity of goal representations is further impacted by careful prompting involving chain-of-thought\nreasoning (Kojima et al., 2022), a small quantity of human-generated advice and the use of an LM-based goal generator.\nLMA3 can be applied in any environment where the agent\u2019s behavior can be described with language. Although it\ndoes not cover all possible scenarios, many of the skills that humans care about can be described with language: from\nsimple actions like picking up a glass, to more abstract behaviors like composing a haiku or coding a sorting algorithm.\nLower-level behaviors, on the other hand, are hard to express with language (e.g. \ufb01ne-grain robotic manipulation).\nFor such behaviors, future work could combine LMA3 with unsupervised skill discovery algorithms such as DIAYN\n(Eysenbach et al., 2019) or Skew-Fit (Pong et al., 2020). Modular autotelic agents could then target goals from several\ngoal spaces in parallel and perform cross-modal hindsight learning as proposed in Colas et al. (2019).\nThis paper focused on goal generation and used a simple skill learning approach to limit the sample complexity of the\nexperiments. Future work could build on the proposed approach to achieve more open-ended skill learning. Let us\ndiscuss the key elements that should be improved to that end. A key aspect of open-ended learning is the co-adaptation\nof the goal generator and the goal-reaching policy. In LMA3, goal generation evolves as the LM Goal Generator\nrecursively composes subgoals the agent masters towards more complex goals. In addition, the LM Relabeler should\nalso adapt and describe harder and harder goals as the agent learns to master them. To this end, the agent should be given\nthe ability to track its own performance (estimated success rate, learning progress or uncertainty) and use these metrics\nas intrinsic motivations to guide relabeling. Improving the skill learning algorithm would also help LMA3 generalize to\na larger diversity of goals and thus focus on harder ones faster. Examples of more sophisticated skill learning algorithms\ninclude: leveraging deep reinforcement learning approaches (Hessel et al., 2018) with transformer-based architectures\n(Chen et al., 2021; Janner et al., 2021), \ufb01netuning another large language model using online interactions (Carta et al.,\n2023), or leveraging state-of-the-art model-based approaches (Hafner et al., 2023).\nGiven these more scalable learning approaches, one should consider the exploration of larger worlds. The set of possible\ninteractions in CookingWorld is fundamentally limited to a number of distinct interactions with a few objects. As\ngoal generation gets more abstract and diverse and the skill learning approach learns more skills, the main bottleneck\nbecomes the complexity of the environment. While current text-based environments are typically not open worlds, one\nmight consider the use of non-textual open worlds such as Minecraft, coupled with image- or video-to-text captioning\nsystems. To this day, open-source multimodal model cannot relabel trajectories in an open-ended way, and even\nstate-of-the-art closed-source variants still require \ufb01netuning before they can be used as success detectors in speci\ufb01c\nenvironments (Du et al., 2023a). The multimodal version of GPT-4 may change that in the near future, and could be\neasily integrated within the LMA3 framework.\nAlthough LMs are a useful resource, they remain expensive not only to train but also to use. The experiments presented\nin this paper represent 550k calls to ChatGPT of about 4k tokens per call. This represents \u2248 USD 4,400 with the public\npricing of USD 0.002 /1k tokens. A given seed of LMA3 run for 10k episodes and costs about USD 240. Training\nlarge neural policies that generalize well in complex environments would require about two orders of magnitudes more\nepisodes (\u22481M), which would raise the cost of any single seed to \u2248 USD 22,400. Pushing these ideas forward may\n10\nPreprint.\nthus require a combination of reduction in inference costs and prices, the distillation of LMs into smaller environment\nspeci\ufb01c reward functions and relabeling functions.\nAs the \ufb01eld moves towards more and more open-ended agents, their evaluation becomes more complex. How should we\nevaluate agents that imagine their own goals, specialize in certain skills and not others? This paper used the diversity of\nstems as a proxy for the diversity of interaction types the agent could learn to demonstrate. Other metrics could include\nthe measure of diversity in linguistic space, various measures of exploration computed from the agent\u2019s behavior, human\nstudies to evaluate the diversity, creativity, abstraction and complexity of the mastered skills. The evaluation of the\nrelevance of learned skills for humans could also require humans in the loop interactively testing the capacities of agent\nin standardized interaction protocols.\nREFERENCES\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana\nGopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as i can, not as i say: Grounding language in robotic\naffordances. arXiv preprint arXiv:2204.01691, 2022.\nMarcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh\nTobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. Advances in neural information\nprocessing systems, 30, 2017.\nSaurabh Arora and Prashant Doshi. A survey of inverse reinforcement learning: Challenges, methods and progress.\nArti\ufb01cial Intelligence, 297:103500, 2021.\nArthur Aubret, Laetitia Matignon, and Salima Hassas. A survey on intrinsic motivation in reinforcement learning.\narXiv preprint arXiv:1908.06976, 2019.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural\ninformation processing systems, 33:1877\u20131901, 2020.\nThomas Carta, Cl\u00b4ement Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, and Pierre-Yves Oudeyer. Ground-\ning large language models in interactive environments with online reinforcement learning.\narXiv preprint\narXiv:2302.02662, 2023.\nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srini-\nvas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. In M. Ranzato,\nA. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing\nSystems, volume 34, pp. 15084\u201315097. Curran Associates, Inc., 2021. URL https://proceedings.neurips.\ncc/paper/2021/file/7f489f642a0ddb10272b5c31057f0663-Paper.pdf.\nNoam Chomsky and David W Lightfoot. Syntactic structures. Walter de Gruyter, 2002.\nJunyi Chu and Laura E Schulz. Play, curiosity, and cognition. Annual Review of Developmental Psychology, 2:317\u2013343,\n2020.\nC\u00b4edric Colas, Pierre Fournier, Mohamed Chetouani, Olivier Sigaud, and Pierre-Yves Oudeyer. Curious: intrinsically\nmotivated modular multi-goal reinforcement learning. In International conference on machine learning, pp. 1331\u2013\n1340. PMLR, 2019.\nC\u00b4edric Colas, Tristan Karch, Nicolas Lair, Jean-Michel Dussoux, Cl\u00b4ement Moulin-Frier, Peter Dominey, and Pierre-\nYves Oudeyer. Language as a cognitive tool to imagine goals in curiosity driven exploration. Advances in Neural\nInformation Processing Systems, 33:3761\u20133774, 2020.\nC\u00b4edric Colas, Tristan Karch, Cl\u00b4ement Moulin-Frier, and Pierre-Yves Oudeyer. Language and culture internalization for\nhuman-like autotelic ai. Nature Machine Intelligence, 4(12):1068\u20131076, 2022a.\nC\u00b4edric Colas, Tristan Karch, Olivier Sigaud, and Pierre-Yves Oudeyer. Autotelic agents with intrinsically motivated\ngoal-conditioned reinforcement learning: a short survey. Journal of Arti\ufb01cial Intelligence Research, 74:1159\u20131199,\n2022b.\n11\nPreprint.\nMarc-Alexandre C\u02c6ot\u00b4e, Akos K\u00b4ad\u00b4ar, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew\nHausknecht, Layla El Asri, Mahmoud Adada, et al. Textworld: A learning environment for text-based games.\nIn Computer Games: 7th Workshop, CGW 2018, Held in Conjunction with the 27th International Conference on\nArti\ufb01cial Intelligence, IJCAI 2018, Stockholm, Sweden, July 13, 2018, Revised Selected Papers 7, pp. 41\u201375. Springer,\n2019.\nIshita Dasgupta, Christine Kaeser-Chen, Kenneth Marino, Arun Ahuja, Sheila Babayan, Felix Hill, and Rob Fergus.\nCollaborating with language models for embodied reasoning. arXiv preprint arXiv:2302.00763, 2023.\nGuy Dove. Language as a disruptive technology: abstract concepts, embodiment and the \ufb02exible mind. Philosophical\nTransactions of the Royal Society B: Biological Sciences, 373(1752):20170135, 2018.\nYuqing Du, Ksenia Konyushkova, Misha Denil, Akhil Raju, Jessica Landon, Felix Hill, Nando de Freitas, and Serkan\nCabi. Vision-language models as success detectors. arXiv preprint arXiv:2303.07280, 2023a.\nYuqing Du, Olivia Watkins, Zihan Wang, C\u00b4edric Colas, Trevor Darrell, Pieter Abbeel, Abhishek Gupta, and Jacob\nAndreas. Guiding pretraining in reinforcement learning with large language models. arXiv preprint arXiv:2302.06692,\n2023b.\nAdrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and Jeff Clune. First return, then explore. Nature,\n590(7847):580\u2013586, 2021.\nBenjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills\nwithout a reward function. In International Conference on Learning Representations, 2019. URL https://\nopenreview.net/forum?id=SJx63jRqFm.\nLinxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang,\nYuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge.\nIn Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. URL\nhttps://openreview.net/forum?id=rc8o_j8I8PX.\nDedre Gentner. Language as cognitive tool kit: How language supports relational thought. American psychologist, 71\n(8):650, 2016.\nDanijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models.\narXiv preprint arXiv:2301.04104, 2023.\nJi He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf. Deep reinforcement\nlearning with a natural language action space. In Proceedings of the 54th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pp. 1621\u20131630, Berlin, Germany, August 2016. Association for\nComputational Linguistics. doi: 10.18653/v1/P16-1153. URL https://aclanthology.org/P16-1153.\nMatteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal\nPiot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcement learning. In\nProceedings of the AAAI conference on arti\ufb01cial intelligence, volume 32, 2018.\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting\nactionable knowledge for embodied agents. In International Conference on Machine Learning, pp. 9118\u20139147.\nPMLR, 2022a.\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor\nMordatch, Yevgen Chebotar, Pierre Sermanet, Tomas Jackson, Noah Brown, Linda Luu, Sergey Levine, Karol Haus-\nman, and brian ichter. Inner monologue: Embodied reasoning through planning with language models. In 6th Annual\nConference on Robot Learning, 2022b. URL https://openreview.net/forum?id=3R3Pz5i0tye.\nMichael Janner, Qiyang Li, and Sergey Levine. Reinforcement learning as one big sequence modeling problem. In\nNeural Information Processing Systems, 2021.\nMinqi Jiang, Edward Grefenstette, and Tim Rockt\u00a8aschel. Prioritized level replay. In International Conference on\nMachine Learning, pp. 4940\u20134950. PMLR, 2021.\nLou Jost. Entropy and diversity. Oikos, 113(2):363\u2013375, 2006.\n12\nPreprint.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are\nzero-shot reasoners. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neu-\nral Information Processing Systems, 2022. URL https://openreview.net/forum?id=e2TBb5y0yFf.\nJacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as\npolicies: Language model programs for embodied control. arXiv preprint arXiv:2209.07753, 2022.\nCam Linke, Nadia M Ady, Martha White, Thomas Degris, and Adam White. Adapting behavior via intrinsic reward: A\nsurvey and empirical study. Journal of arti\ufb01cial intelligence research, 69:1287\u20131332, 2020.\nCorey Lynch and Pierre Sermanet. Language conditioned imitation learning over unstructured data. arXiv preprint\narXiv:2005.07648, 2020.\nAndrea Madotto, Mahdi Namazifar, Joost Huizinga, Piero Molino, Adrien Ecoffet, Huaixiu Zheng, Alexandros\nPapangelis, Dian Yu, Chandra Khatri, and Gokhan Tur. Exploration based language learning for text-based games.\nIn Christian Bessiere (ed.), Proceedings of the Twenty-Ninth International Joint Conference on Arti\ufb01cial Intelligence,\nIJCAI-20, pp. 1488\u20131494. International Joint Conferences on Arti\ufb01cial Intelligence Organization, 7 2020. doi:\n10.24963/ijcai.2020/207. URL https://doi.org/10.24963/ijcai.2020/207. Main track.\nSebastian Michelmann, Manoj Kumar, Kenneth A Norman, and Mariya Toneva. Large language models can segment\nnarrative events similarly to humans. arXiv preprint arXiv:2301.10297, 2023.\nKarthik Narasimhan, Tejas Kulkarni, and Regina Barzilay. Language understanding for text-based games using\ndeep reinforcement learning. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language\nProcessing, pp. 1\u201311, Lisbon, Portugal, September 2015. Association for Computational Linguistics. doi: 10.18653/\nv1/D15-1001. URL https://aclanthology.org/D15-1001.\nAndrew Y Ng, Stuart Russell, et al. Algorithms for inverse reinforcement learning. In Icml, volume 1, pp. 2, 2000.\nPierre-Yves Oudeyer and Frederic Kaplan. What is intrinsic motivation? a typology of computational approaches.\nFrontiers in neurorobotics, pp. 6, 2007.\nVitchyr Pong, Murtaza Dalal, Steven Lin, Ashvin Nair, Shikhar Bahl, and Sergey Levine. Skew-\ufb01t: State-covering self-\nsupervised reinforcement learning. In Hal Daum\u00b4e III and Aarti Singh (eds.), Proceedings of the 37th International\nConference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 7783\u20137792.\nPMLR, 13\u201318 Jul 2020. URL https://proceedings.mlr.press/v119/pong20a.html.\nR\u00b4emy Portelas, C\u00b4edric Colas, Lilian Weng, Katja Hofmann, and Pierre-Yves Oudeyer. Automatic curriculum learning for\ndeep rl: A short survey. In Christian Bessiere (ed.), Proceedings of the Twenty-Ninth International Joint Conference\non Arti\ufb01cial Intelligence, IJCAI-20, pp. 4819\u20134825. International Joint Conferences on Arti\ufb01cial Intelligence\nOrganization, 7 2020. doi: 10.24963/ijcai.2020/671. URL https://doi.org/10.24963/ijcai.2020/\n671. Survey track.\nNils Reimers and Iryna Gurevych.\nSentence-BERT: Sentence embeddings using Siamese BERT-networks.\nIn\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International\nJoint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 3982\u20133992, Hong Kong, China, November\n2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1410. URL https://aclanthology.\norg/D19-1410.\nTom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators. In International\nconference on machine learning, pp. 1312\u20131320. PMLR, 2015.\nJ\u00a8urgen Schmidhuber. Powerplay: Training an increasingly general problem solver by continually searching for the\nsimplest still unsolvable problem. Frontiers in psychology, 4:313, 2013.\nIshika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse\nThomason, and Animesh Garg. Progprompt: Generating situated robot task plans using large language models. arXiv\npreprint arXiv:2209.11302, 2022.\nSatinder Singh, Richard L Lewis, Andrew G Barto, and Jonathan Sorg. Intrinsically motivated reinforcement learning:\nAn evolutionary perspective. IEEE Transactions on Autonomous Mental Development, 2(2):70\u201382, 2010.\nRichard S Sutton, Andrew G Barto, et al. Introduction to reinforcement learning, volume 135. MIT press Cambridge,\n1998.\n13\nPreprint.\nAdaptive Agent Team, Jakob Bauer, Kate Baumli, Satinder Baveja, Feryal Behbahani, Avishkar Bhoopchand, Nathalie\nBradley-Schmieg, Michael Chang, Natalie Clay, Adrian Collister, et al. Human-timescale adaptation in an open-ended\ntask space. arXiv preprint arXiv:2301.07608, 2023.\nClaudio Tennie, Josep Call, and Michael Tomasello. Ratcheting up the ratchet: on the evolution of cumulative culture.\nPhilosophical Transactions of the Royal Society B: Biological Sciences, 364(1528):2405\u20132415, 2009.\nRui Wang, Joel Lehman, Aditya Rawal, Jiale Zhi, Yulun Li, Jeffrey Clune, and Kenneth Stanley. Enhanced poet:\nOpen-ended reinforcement learning through unbounded invention of learning challenges and their solutions. In\nInternational Conference on Machine Learning, pp. 9940\u20139951. PMLR, 2020.\nZihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive\nplanning with large language models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560, 2023.\nDavid Warde-Farley, Tom Van de Wiele, Tejas Kulkarni, Catalin Ionescu, Steven Hansen, and Volodymyr Mnih.\nUnsupervised control through non-parametric discriminative rewards. arXiv preprint arXiv:1811.11359, 2018.\nSandra R Waxman and Dana B Markow. Words as invitations to form categories: Evidence from 12-to 13-month-old\ninfants. Cognitive psychology, 29(3):257\u2013302, 1995.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny\nZhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal,\nDanielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL\nhttps://openreview.net/forum?id=_VjQlMeSB_J.\nTed Xiao, Harris Chan, Pierre Sermanet, Ayzaan Wahid, Anthony Brohan, Karol Hausman, Sergey Levine, and Jonathan\nTompson. Robotic skill acquisition via instruction augmentation with vision-language models. arXiv preprint\narXiv:2211.11736, 2022.\nShunyu Yao, Rohan Rao, Matthew Hausknecht, and Karthik Narasimhan. Keep CALM and explore: Language models\nfor action generation in text-based games. In Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pp. 8736\u20138754, Online, November 2020. Association for Computational Linguistics.\ndoi: 10.18653/v1/2020.emnlp-main.704. URL https://aclanthology.org/2020.emnlp-main.704.\n14\nPreprint.\nA\nHAND-CODED EVALUATION SET\nHere are the 69 hand-coded goals: cook the red apple, cook the red potato, cook the yellow apple, cook the yellow\npotato, fry the green apple, fry the red apple, fry the red potato, fry the yellow apple, fry the yellow potato, grill the\ngreen apple, grill the red apple, grill the red potato, grill the yellow apple, grill the yellow potato, roast the green apple,\nroast the red apple, roast the red potato, roast the yellow apple, roast the yellow potato, cut the cilantro, cut the green\napple, cut the parsley, cut the red apple, cut the red potato, cut the yellow apple, cut the yellow potato, chop the cilantro,\nchop the green apple, chop the parsley, chop the red apple, chop the red potato, chop the yellow apple, chop the yellow\npotato, dice the cilantro, dice the green apple, dice the parsley, dice the red apple, dice the red potato, dice the yellow\napple, dice the yellow potato, slice the cilantro, slice the green apple, slice the parsley, slice the red apple, slice the red\npotato, slice the yellow apple, slice the yellow potato, eat the cilantro, eat the green apple, eat the parsley, eat the red\napple, eat the yellow apple, go to the kitchen, open the cutlery drawer, open the dishwasher, open the fridge, open the\nkitchen cupboard, open the trash can, You are hungry! Let\u2019s cook a delicious meal. Check the cookbook in the kitchen\nfor the recipe. Once done, enjoy your meal!, pick up the cilantro, pick up the cookbook, pick up the green apple, pick\nup the knife, pick up the parsley, pick up the red apple, pick up the red potato, pick up the yellow apple, pick up the\nyellow potato.\nB\nLLMS PROMPTS\nWe run all LM calls with OpenAI\u2019s gpt-3.5-turbo model. We use a temperature of 0 for the LM Reward Function and a\ntemperature of 0.9 for the LM Relabeler and the LM Goal Generator.\nB.1\nLM RELABELER PROMPT\nB.1.1\nLMA3 \\ COT & HUMAN TIPS\nHere is the LM Relabeler prompt with no human tips and no chain-of-thought used for LMA3 \\ CoT & Human Tips.\nLMA3 \\ CoT & Human Tips\nExercise: Given the description of a player\u2019s behavior in a video game, list the most interesting,\nimpressive, novel or creative goals he achieved and, for each goal, specify when it is achieved for the\n\ufb01rst time. Write each goal starting with an imperative verb. Here are three examples:\nExample 1:\n\u201c\u201c\u201c\nStep 52.\nAction 52: slice orange bell pepper\nObservation 52: You slice the orange bell pepper.\nStep 53.\nAction 53: cook white onion in oven\nObservation 53: You roast the white onion with the oven.\nStep 54.\nAction 54: cook carrot in stove\nObservation 54: You fry the carrot with the stove.\nStep 55.\nAction 55: cook orange bell pepper in oven\nObservation 55: You roast the orange bell pepper with the oven.\nStep 56.\nAction 56: move north\nObservation 56: You are in the corridor. In one part of the room you see a shoe cabinet that is\nclosed.There is also a key holder, that has nothing on it. You also see a hat rack, that has nothing on\nit. In another part of the room you see a coat hanger, that has nothing on it. In one part of the room\nyou see a umbrella stand, that has nothing on it. To the North you see the driveway. To the South you\nsee the kitchen. Through an open wood door, to the East you see the laundry room. Through an\nopen screen door, to the West you see the backyard.\n\u201d\u201d\u201d\n15\nPreprint.\nLet\u2019s not forget to mention the steps at which the agent achieved the goal for the \ufb01rst time. Use this\nformat: {goal description} (step {step number}).\nAnswer:\n- roast a white onion (step 55).\n- go to the corridor (step 56).\n- \ufb01nd at the laundry room (step 56).\n- slice a bell pepper (Step 52).\n- cook an orange bell pepper (step 55).\n- \ufb01nd an umbrella stand (step 56).\nExample 2:\n\u201c\u201c\u201c\nStep 78.\nAction 78: open cupboard\nObservation 78: you open the cupboard, it is empty.\nStep 79.\nAction 79: open kitchen drawer\nObservation 79: you open the kitchen drawer, it contains a knife and a fork.\nStep 80.\nAction 80: take knife\nObservation 80: you pick up the knife\nStep 81.\nAction 81: open trash can\nObservation 81: you open the trash can, it contains an apple\nStep 82.\nAction 82: move east\nObservation 82: You are in the pantry. In one part of the room you see a folding chair that has some\nolive oil, a roasted red potato, some sugar, some black pepper, and some vegetable oil on it. There is\nalso a shelf that has some salt, and some peanut oil on it. To the West you see the kitchen.\nStep 83.\nAction 83: move west\nObservation 83: You are in the kitchen. In one part of the room you see a stove. There is also an\noven. You also see a fridge that is closed. In another part of the room you see a counter, that has a\nyellow potato on it. In one part of the room you see an open kitchen cupboard, that is empty. There is\nalso a cutlery drawer that is open. To the East you see the pantry.\nStep 84.\nAction 84: take yellow potato\nObservation 84: you take the yellow potato.\nStep 85.\nAction 85: slice potato\nObservation 85: you cut the potato in slices\n\u201d\u201d\u201d\nLet\u2019s not forget to mention the steps at which the agent achieved the goal for the \ufb01rst time. Use this\nformat: {goal description} (step {step number}).\nAnswer:\n- open the trash can (step 81).\n- look into the cupboard (step 78).\n- open the kitchen drawer (step 79).\n- cut a yellow potato (Step 84).\n- go the pantry (step 82).\nExample 3:\n[insert trajectory to relabel here]\nAnswer:\n-\n16\nPreprint.\nB.1.2\nLMA3 \\ HUMAN TIPS\nLMA3 \\ Human Tips makes use of chain-of-thought prompting but does not leverage human advice. The example\ntrajectories and answers remain the same as in the previous prompt, but explanations are added before each answer.\nHere is what both examples from the previous prompt look like.\nLMA3 \\ Human Tips - Example 1\nLet\u2019s think step by step.\nReasoning: Here are some interesting goals the player achieved. The player cooked a white onion\n(step 53), visited the corridor (step 56), saw the laundry room (step 56), sliced and roasted an orange\nbell pepper (steps 52 and 55) and saw an umbrella stand (step 56). Let\u2019s not forget to mention the\nsteps at which the agent achieved the goal for the \ufb01rst time. Use this format: {goal description} (step\n{step number}).\nAnswer:\n- roast a white onion (step 55).\n- go to the corridor (step 56).\n- \ufb01nd at the laundry room (step 56).\n- slice a bell pepper (Step 52).\n- cook an orange bell pepper (step 55).\n- \ufb01nd an umbrella stand (step 56).\nLMA3 \\ Human Tips - Example 2\nLet\u2019s think step by step.\nReasoning: The agent open various containers: the trash can (step 81), the cupboard (step 78) and\nthe kitchen drawer (step 79). It cut a yellow potato with a knife (step 84) and went to the pantry (step\n82). Let\u2019s not forget to mention the steps at which the agent achieved the goal for the \ufb01rst time. Use\nthis format: {goal description} (step {step number}).\nAnswer:\n- open the trash can (step 81).\n- look into the cupboard (step 78).\n- open the kitchen drawer (step 79).\n- cut a yellow potato (Step 84).\n- go the pantry (step 82).\nFinally, the end of the prompt include chain-of-thought prompting as well:\nLMA3 \\ Human Tips - Example 3\nExample 3:\n[insert trajectory to relabel here]\nLet\u2019s think step by step and relabel up to 10 goals.\nReasoning:\nB.1.3\nLMA3\nLMA3 makes use of of both chain-of-thought prompting and human tips. The example trajectories remain the same as\nin the previous prompts but the answer and reasoning change.\nLMA3 - Example 1\nLet\u2019s think step by step.\nReasoning: In the above trajectory, the agent both sliced (step 52) and roasted (step 55) an orange\ningredient (orange bell pepper), which demonstrates his capability to prepare an ingredient in several\nsteps. He used the oven twice (steps 53 and 54). He successfully cooked several ingredients: an\n17\nPreprint.\nonion (step 53), the orange bell pepper (step 52) and a carrot (step 54), which shows time-extended\ncommitment to prepare a recipe. An interesting way to describe goals is to mention consecutive steps:\nhere the player \ufb01rst cooked an onion, then cut a bell pepper (the overall goal, made of two steps, is\ncompleted in step 55). Interestingly, the player discovered new properties of the environment: he\nfound a place from which he could see both the laundry room and the backyard (from the corridor)\nin step 56. He found out whether the keyholder hold something in it in step 56 (it did not). Let\u2019s not\nforget to mention the steps at which the agent achieved the goal for the \ufb01rst time. Use this format:\n{goal description} (step {step number}).\nAnswer:\n- slice and cook an orange ingredient (step 55).\n- use the oven for the second time (step 55).\n- roast an onion and a bell pepper and fry carrots (step 55).\n- cook an onion \ufb01rst then cut a bell pepper (step 55).\n- \ufb01nd a place from which you can see both the laundry room and the backyard (step 56).\n- \ufb01nd out whether the keyholder has something on it (step 56).\nLMA3 - Example 2\nLet\u2019s think step by step.\nReasoning: In this trajectory, the agent searched for a knife and used it to cut a potato in slices\n(achieved in step 85). He discovered a new room, the pantry in step 84. He found out that the trash\ncan was not empty (step 81) and looked inside three containers: the trash can (step 81), the cupboard\n(step 78) and the drawer (step 79). He left the kitchen and came back (step 83). Let\u2019s not forget\nto mention the steps at which the agent achieved the goal for the \ufb01rst time. Use this format: {goal\ndescription} (step {step number}).\nAnswer:\n- \ufb01nd a knife and use it to cut a potato (step 84).\n- \ufb01nd the pantry (step 84).\n- open three containers (step 81).\n- leave and come back to the kitchen (step 83).\nB.2\nLM REWARD FUNCTION PROMPT\nB.2.1\nLMA3 \\ COT\nThe prompt of the LM Reward Function without chain-of-thought prompting is the following:\nLMA3 \\ CoT\nExercise: Given the description of a player\u2019s behavior in a video game and a list of goals, tell me\nwhether the player achieves these goals and, if he does, when the goal is achieved. Here are three\nexamples.\nExample 1:\n\u201c\u201c\u201c\nStep 52.\nAction 52: slice orange bell pepper\nObservation 52: You slice the orange bell pepper.\nStep 53.\nAction 53: cook white onion in oven\nObservation 53: You roast the white onion with the oven.\nStep 54.\nAction 54: cook carrot in stove\nObservation 54: You fry the carrot with the stove.\nStep 55.\n18\nPreprint.\nAction 55: cook orange bell pepper in oven\nObservation 55: You roast the orange bell pepper with the oven.\nStep 56.\nAction 56: move north\nObservation 56: You are in the corridor. In one part of the room you see a shoe cabinet that is\nclosed.There is also a key holder, that has nothing on it. You also see a hat rack, that has nothing on\nit. In another part of the room you see a coat hanger, that has nothing on it. In one part of the room\nyou see a umbrella stand, that has nothing on it. To the North you see the driveway. To the South you\nsee the kitchen. Through an open wood door, to the East you see the laundry room. Through an\nopen screen door, to the Wes you see the backyard.\n\u201d\u201d\u201d\nHere is the list of goals: \u201ccook an omelet\u201d, \u201ccook an orange ingredient\u201d, \u201cmove north, then move\nsouth\u201d, \u201cachieved goal: do xx\u201d, \u201croast two ingredients in the oven\u201d, \u201ccook several ingredients\u201d. Let\u2019s\nanswer and indicate steps of goal completion:\n- cook an omelet. Answer: no.\n- cook an orange ingredient. Answer: yes (step 54).\n- move north, then move south. Answer: no.\n- achieved goal: do xx. Answer: no.\n- roast two ingredients in the oven. Answer: yes (step 55).\n- cook several ingredients. Answer: yes (step 54).\nExample 2:\n\u201c\u201c\u201c\nStep 78.\nAction 78: open cupboard\nObservation 78: you open the cupboard, it is empty.\nStep 79.\nAction 79: open kitchen drawer\nObservation 79: you open the kitchen drawer, it contains a knife and a fork.\nStep 80.\nAction 80: take knife\nObservation 80: you pick up the knife\nStep 81.\nAction 81: open trash can\nObservation 81: you open the trash can, it contains an apple\nStep 82.\nAction 82: move east\nObservation 82: You are in the pantry. In one part of the room you see a folding chair that has some\nolive oil, a roasted red potato, some sugar, some black pepper, and some vegetable oil on it. There is\nalso a shelf that has some salt, and some peanut oil on it. To the West you see the kitchen.\nStep 83.\nAction 83: move west\nObservation 83: You are in the kitchen. In one part of the room you see a stove. There is also an\noven. You also see a fridge that is closed. In another part of the room you see a counter, that has a\nyellow potato on it. In one part of the room you see an open kitchen cupboard, that is empty. There is\nalso a cutlery drawer that is open. To the East you see the pantry.\nStep 84.\nAction 84: take yellow potato\nObservation 84: you take the yellow potato.\nStep 85.\nAction 85: slice potato\nObservation 85: you cut the potato in slices\n\u201d\u201d\u201d\nHere is the list of goals: \u201copen an object\u201d, \u201ccook a potato\u201d, \u201c\ufb01nd a knife and cut a potato with it\u201d, \u201ceat a\nmeal\u201d. Let\u2019s answer and indicate steps of goal completion:\n- open an object. Answer: yes (step 78).\n- cook a potato. Answer: no.\n19\nPreprint.\n- \ufb01nd a knife and cut a potato with it. Answer: yes (step 85).\n- eat a meal. Answer: no.\nExample 3:\n[insert trajectory here]\nHere is the list of goals: [insert list of goals to test here]. Let\u2019s answer and indicate steps of goal\ncompletion:\nB.2.2\nLMA3\nWith chain-of-thought prompting, we add reasoning description to the answers for each example.\nLMA3 - Example 1\n- cook an omelet. Reasoning: there is no omelet in this game, this goal is impossible. Answer: no.\n- cook an orange ingredient. Reasoning: the orange bell pepper and the carrot are two orange\ningredients. The carrot is cooked \ufb01rst, in observation 54 so the goal was \ufb01rst achieved in step 54.\nAnswer: yes (step 54).\n- move north, then move south. Reasoning: the player moves north in step 56, but it does not move\nsouth after that. Answer: no.\n- achieved goal: do xx. Reasoning: this goal does not make sense and thus cannot be achieved.\nAnswer: no.\n- roast two ingredients in the oven. Reasoning: the player roasts two ingredients in the oven: the white\nonion (step 53) and the bell pepper (step 55). The goal is only completed in step 55. Answer: yes\n(step 55).\n- cook several ingredients. Reasoning: the player cooks a white onion (step 53), a carrot (step 54) and\nthe bell pepper (step 55). The world several requires at least two ingredients, so the goal is completed\nin step 54 when two ingredients have been cooked. Answer: yes (step 54).\nLMA3 - Example 2\n- open an object. Reasoning: the player opens a cupboard (step 78), a trash can (step 81) and a\nkitchen drawer (step 79). He achieves the goal for the \ufb01rst time in step (78). Answer: yes (step 78).\n- cook a potato. Reasoning: the potato is sliced but not cooked. Answer: no.\n- \ufb01nd a knife and cut a potato with it. Reasoning: the player \ufb01nds the knife in step 80 and slices a\npotato in step 84, thus truly completes the goal in step 85. Answer: yes (step 85).\n- eat a meal. Reasoning: the player does not eat anything here. Answer: no.\nFor the third example, we replace \u201cLet\u2019s answer and indicate steps of goal completion:\u201d with \u201cLet\u2019s think step by step\nand indicate steps of goal completion:\u201d.\nLMA3 - Example 3\nExample 3:\n[insert trajectory here]\nHere is the list of goals: [insert list of goals to test here]. Let\u2019s think step by step and indicate steps\nof goal completion:\nB.3\nLM GOAL GENERATOR PROMPT\nB.3.1\nLMA3 \\ COT\nHere is the prompt of the LM Goal Generator without chain-of-thought prompting:\n20\nPreprint.\nLMA3 \\ CoT\nContext: I am playing a video game, and here is an example of what can happen in that game:\n[insert trajectory here]\nExercise: Using the given list of possible instructions, \ufb01nd a sequence of 2, 3, or 4 instructions that\nwill help me achieve a new, interesting, or creative goal in this game. Do not pick instructions that do\nnot help reaching the main goal, only relevant ones. First describe the new goal starting with an\nimperative verb; then list the instructions and their corresponding numbers in the list. Here are three\nexamples:\nExample 1: the list of possible instructions is:\n#1 wash the plate\n#2 pick up the green apple\n#3 pick up the plate\n#4 put the potato on the counter\n#5 put the plate in the sink\nAnswer: goal: do the dishes. instructions: pick up the plate (#3); put the plate in the sink (#5); wash\nthe plate (#1).\nExample 2: the list of possible instructions is:\n#1 eat the red apple\n#2 pick up wood\n#3 turn the heat down\n#4 pick up an ax\n#5 cook an omelet\n#6 cut the wood\n#7 put the wood in the chimney\n#8 turn on TV\nAnswer: goal: prepare a \ufb01re in the chimney. instructions: pick up an ax (#4); pick up wood (#2); cut\nthe wood (#6); put the wood in the chimney (#7).\nExample 3: the list of possible instructions is:\n[insert subsample of up to 60 mastered subgoals here]\nAnswer:\nB.3.2\nLMA3\nWith chain-of-thought prompting, we add reasoning to the selection of the main goal and its subgoals.\nLMA3 - Example 1\nLet\u2019s think step by step:\nReasoning: You could do the dishes by following less than 4 instructions by \ufb01rst picking up the plate\n(#3), then putting it in the sink (#5), and \ufb01nally washing the plate (#1) (3 instructions).\nAnswer: goal: do the dishes. instructions: pick up the plate (#3); put the plate in the sink (#5); wash\nthe plate (#1).\nLMA3 - Example 2\nLet\u2019s think step by step:\nReasoning: You could prepare a \ufb01re in the chimney by following 4 instructions. You would need to \ufb01rst\npick up an axe (#4) and pick up wood (#2), then cut the wood (#6) and put the wood in the chimney\n(#7) (4 instructions).\nAnswer: goal: prepare a \ufb01re in the chimney. instructions: pick up an axe (#4); pick up wood (#2); cut\nthe wood (#6); put the wood in the chimney (#7).\n21\nPreprint.\nFor the third example, we replace \u201cAnswer:\u201d with a chain-of-thought sentence.\nLMA3 - Example 3\nExample 3: the list of possible instructions is:\n[insert subsample of up to 60 mastered subgoals here]\nLet\u2019s think step by step and \ufb01nd an interesting and creative goal to reach:\nReasoning:\n22\n"
  },
  {
    "title": "CodeCompose: A Large-Scale Industrial Deployment of AI-assisted Code Authoring",
    "link": "https://arxiv.org/pdf/2305.12050.pdf",
    "upvote": "1",
    "text": "AI-assisted Code Authoring at Scale: Fine-tuning, deploying,\nand mixed methods evaluation\nVIJAYARAGHAVAN MURALI, Meta Platforms Inc., USA\nCHANDRA MADDILA, Meta Platforms Inc., USA\nIMAD AHMAD, Meta Platforms Inc., USA\nMICHAEL BOLIN, Meta Platforms Inc., USA\nDANIEL CHENG, Meta Platforms Inc., USA\nNEGAR GHORBANI, Meta Platforms Inc., USA\nRENUKA FERNANDEZ, Meta Platforms Inc., UK\nNACHIAPPAN NAGAPPAN, Meta Platforms Inc., USA\nPETER C. RIGBY, Meta Platforms Inc., USA and Concordia University, Canada\nGenerative LLMs have been shown to effectively power AI-based code authoring tools that can suggest entire\nstatements or blocks of code during code authoring. In this paper we present CodeCompose, an AI-assisted\ncode authoring tool developed and deployed at Meta internally. CodeCompose is based on the InCoder LLM\nthat merges generative capabilities with bi-directionality. We have scaled up CodeCompose to serve tens of\nthousands of developers at Meta, across 9 programming languages and several coding surfaces. We present\nour experience in making design decisions about the model and system architecture for CodeCompose that\naddresses these challenges.\nTo release a LLM model at this scale, we needed to first ensure that it is sufficiently accurate. In a random\nsample of 20K source code files, depending on the language, we are able to reproduce hidden lines between\n40% and 58% of the time, an improvement of 1.4\u00d7 and 4.1\u00d7 over a model trained only on public data.\nWe gradually rolled CodeCompose out to developers. At the time of this writing, 16K developers have used\nit with 8% of their code coming directly from CodeCompose.\nTo triangulate our numerical findings, we conduct a thematic analysis on the feedback from 70 developers.\nWe find that 91.5% of the feedback is positive, with the most common themes being discovering APIs, dealing\nwith boilerplate code, and accelerating coding. Meta continues to integrate this feedback into CodeCompose.\nCCS Concepts: \u2022 Computing methodologies \u2192 Neural networks; \u2022 Software and its engineering;\nAdditional Key Words and Phrases: AI, Developer productivity, Neural code completion, Program synthesis\nACM Reference Format:\nVijayaraghavan Murali, Chandra Maddila, Imad Ahmad, Michael Bolin, Daniel Cheng, Negar Ghorbani, Renuka\nFernandez, Nachiappan Nagappan, and Peter C. Rigby. 2024. AI-assisted Code Authoring at Scale: Fine-tuning,\ndeploying, and mixed methods evaluation. Proc. ACM Softw. Eng. 1, FSE, Article 48 (July 2024), 20 pages.\nhttps://doi.org/10.1145/3643774\nAuthors\u2019 addresses: Vijayaraghavan Murali, vijaymurali@meta.com, Meta Platforms Inc., USA; Chandra Maddila, cmaddila\n@meta.com, Meta Platforms Inc., USA; Imad Ahmad, imadahmad@meta.com, Meta Platforms Inc., USA; Michael Bolin,\nmbolin@meta.com, Meta Platforms Inc., USA; Daniel Cheng, danielcheng@meta.com, Meta Platforms Inc., USA; Negar\nGhorbani, negargh@meta.com, Meta Platforms Inc., USA; Renuka Fernandez, rdfernandez@meta.com, Meta Platforms Inc.,\nUK; Nachiappan Nagappan, nnachi@meta.com, Meta Platforms Inc., USA; Peter C. Rigby, Meta Platforms Inc., USA and\nConcordia University, Canada, pcr@meta.com.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the\nfull citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.\nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\n\u00a9 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM 2994-970X/2024/7-ART48\nhttps://doi.org/10.1145/3643774\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 48. Publication date: July 2024.\narXiv:2305.12050v2  [cs.SE]  16 Feb 2024\n48:2\nV Murali, C Maddila, I Ahmad, M Bolin, D Cheng, N Ghorbani, R Fernandez, N Nagappan, PC Rigby\n1\nINTRODUCTION\nRecently, large language models (LLMs)[12, 16, 36] have exhibited the ability to assimilate vast\namounts of knowledge after being trained on the data in various corpora. This characteristic of LLMs\nhas made them highly impactful in assisting developers in authoring code[2, 5, 6, 29]. Specifically,\nwhen they are trained to predict the next token in a code sequence, LLMs can become powerful\ncoding assistants that can suggest entire statements or blocks of code during code authoring. Such\na coding assistant trained on an organization\u2019s code repository can surface internal knowledge\nduring code authoring when developers are most likely to seek that information.\nAt Meta, we built an AI-assisted code authoring system named CodeCompose to explore the ap-\nplication of LLM technology for code authoring. Meta is a large software company with a complex\ncode base that covers a wide range of applications from social networking and virtual reality to\nsoftware engineering infrastructure, such as continuous integration tooling and workplace coordi-\nnation. Tens of thousands of developers work on billions of lines of code (LOCs) in a monolithic\nrepository that hosts source code written in 9+ programming languages. One of the prominent\nactivities in the Software Development Life Cycle (SDLC) at Meta is code authoring. A significant\namount of contextual knowledge on the internal software development processes, and libraries, is\nembedded in the source code or is confined to a small set of developers. For example, a question\nlike \u201cHow can I upload a table to Hive in Hack?\u201d can be answered using that knowledge. The state\nof the source code and the developers associated with the source code keeps evolving constantly\n- internal libraries and tools get added and deprecated, while developers move across teams and\nchange roles. At Meta\u2019s scale, keeping up with the knowledge required to accomplish coding tasks\nis challenging. Additionally, the dynamic environment at a large software company like Meta poses\ninteresting challenges concerning knowledge discovery and achieving developer efficiency.\nOver the past year, we (i) conducted R&D on the LLM (model) architecture, training data, and\ntraining objective, (ii) built an end-to-end coding assistant that offers code suggestions in various\ncode authoring surfaces across the company, and (iii) gathered quantitative metrics and qualitative\nfeedback to measure the impact of the CodeCompose system. CodeCompose has the following\ncharacteristics:\n\u2022 Multi-lingual: CodeCompose is based on the InCoder LLM [18] and has been trained on 9+\nprogramming languages at Meta. As such, it inherits the property of LLMs to be multi-lingual.\n\u2022 Customized for the organization: CodeCompose is fine-tuned on Meta\u2019s internal code repository,\nand is thus able to handle languages such as Hack and Flow. CodeCompose is also deeply\nintegrated with Meta\u2019s version of VSCode, as seen in Figure 1(a), and other editors.\n\u2022 Natural language proficiency: CodeCompose can understand inline comments in natural language\nand generate code that adheres to the comment, as shown in Figure 1(b). It can also fluently\ngenerate comments, messages, and documentation.\n\u2022 Bi-directional: CodeCompose can look beyond the cursor to suggest code at the current position.\nIn Figure 1(c) it can be seen suggesting the docstring for a function from code that conventionally\nappears after the docstring.\nIn this paper we present how we built CodeCompose, discuss the unique challenges of our scale\nand how they influenced design decisions about CodeCompose, and finally, results from an extensive\nlarge-scale deployment including developer feedback on the impact of CodeCompose on how code\nis authored at Meta. Specifically, we provide evidence related to the following research questions:\nRQ1. Model Evaluation: How well does CodeCompose generate one hidden line of code\nfrom existing code snippets?\nBefore designing and releasing a system to engineers, we test it on the existing code base. We\nrandomly mask part of the code and allow the model to predict lines in the masked part to evaluate\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 48. Publication date: July 2024.\nAI-assisted Code Authoring at Scale: Fine-tuning, deploying, and mixed methods evaluation\n48:3\n(a)\n(b)\n(c)\nFig. 1. CodeCompose (a) offers inline code suggestions in VSCode in a grey text appearing after the cursor\nwhen the user is typing code (Tab to accept), (b) changes its suggestion to adapt to a natural language\ncomment, (c) suggests code or documentation based on code below the current position.\nits accuracy. The code before and some of the code after the masked part is also fed to the model\nas the context, along with some additional metadata. We contrast the public InCoder model [18],\nagainst our fine-tuned model with both the Causal Masking (CM) and Language Causal Masking\n(LCM) objectives.\nResult Summary: We find that LCM CodeCompose model exactly recreates the masked line\nbetween 40% and 58% of the time and has a high BLEU score between 56% and 73%, depending on\nthe programming language. This represents an improvement over the Public model between 1.4\u00d7\nand 4.1\u00d7.\nRQ2. Adoption: How many suggestions are accepted by engineers and what proportion\nof the code is written by CodeCompose?\nFew studies of LLM for coding are able to go beyond backtests. There are some blog posts\ndemonstrating the use by professional engineers. At Meta we are able to gradually rollout new\nfeatures to engineers. We progressively roll CodeCompose out and monitor its impact through the\nnumber of accepted suggestions and the proportion of code that is written by CodeCompose.\nResult Summary: In this paper, we make over 4.5M suggestions to 16K engineers. We see an\nacceptance rate of 22% which is comparable to those at Google and GitHub. We see that 8% of\nthe changed code is written by CodeCompose, which is higher than the 3% reported by Google.\nCodeCompose is rolled out to 100% of engineers at Meta.\nRQ3. Developer Feedback: How do developers perceive CodeCompose in their daily\nwork?\nWe triangulate our numerical analysis, which show evidence of adoption, with feedback from\ndevelopers and discussion of actual usages [15]. Using the feedback group for CodeCompose, we\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 48. Publication date: July 2024.\n48:4\nV Murali, C Maddila, I Ahmad, M Bolin, D Cheng, N Ghorbani, R Fernandez, N Nagappan, PC Rigby\nTokenize code at trigger \ncharacters: (, ., =, etc.\nSelect a random subsequence \nunder a length limit\nSequence of tokens\n3 sequences of tokens\nCreate metadata, code before, code after, \nand target sentences\n4 sentences\nEncode each sentence separately with \nComposeCode\u2019s tokenizer \n4 sequences of token IDs\nTruncate code before and \ncode after using a 70-30 split\n4 sequences of truncated token IDs\nConcatenate the encoded sequences and insert \nspecial token IDs for mask, pad if needed\nmetadata IDs\ncode before IDs\n<mask> ID\ncode after IDs\n<mask> ID\ntarget IDs\ndef bubbleSort(array):\n# loop through each element of array\nfor i in range(len(array)):\n# keep track of swapping\nswapped = False\n    # loop to compare array elements\n    for j in range(0, len(array) - i - 1):\n      if array[j] > array[j + 1]:\n        # swap if needed\n        temp = array[j]\narray[j] = array[j+1]\narray[j+1] = temp\nswapped = True\n# array is already sorted\nif not swapped:\nbreak\nFile: /home/bubblesort.py Language: python\nmetadata\ncode before\ntarget\ncode after\nFig. 2. Steps to construct an input to the model in LCM: (i) the code is tokenized at trigger characters where\nwe expect the model to offer suggestions in production, (ii) a random subsequence of tokens is selected to be\nmasked as the \u201ctarget\u201d to predict given the code before and the code after it, (iii) any additional metadata\nsuch as the filename is added to the front, (iv) all four strings (metadata, code before, target, code after) are\nencoded into tokens, (v) since the model\u2019s input length is limited, a 70-30 split is applied to the code before\nand code after if needed, (vi) all tokens are concatenated together into a single list of tokens with special\ntokens added to denote the masked target portion. The code on the left shows an example with the randomly\nselected target portion highlighted.\nconduct a thematic analysis of the comments from 70 engineers. We provide direct quotes from\nengineers to contextualize and ground the themes we discovered. We also use these themes and\nfeedback to iteratively improve CodeCompose.\nResult Summary: The feedback was overwhelmingly positive, with only 8.5% of comments\ncontaining negative feedback and less than 1% of engineers turning off CodeCompose. The top\ntwo feedback comment themes related to discovering APIs and dealing with boilerplate (23%), and\naccelerating coding (20%). Meta continues to invest in CodeCompose efforts to iteratively improve\nthe AI-assisted authoring experience.\nThe paper is organized as follows. Section 2 presents details about the CodeCompose model.\nSection 3 presents the system architecture. Section 4 presents results from our large-scale deploy-\nment at Meta. Section 5 discusses threats to validity. Section 6 discusses related work. Section 7\ndiscusses the challenges for coding assistants at scale. Section 8 concludes the paper.\n2\nMODEL DEVELOPMENT AND EVALUATION METHODOLOGY\nIn this section, we provide details about the underlying LLM architecture for CodeCompose, and\ndiscuss considerations that affected our decision.\n2.1\nModel Architecture and Training Objective\nLLMs largely fall into two main categories. The BERT [16] branch of models are trained with the\nMasked Language Model (MLM) objective that masks out certain parts of the input and trains the\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 48. Publication date: July 2024.\nAI-assisted Code Authoring at Scale: Fine-tuning, deploying, and mixed methods evaluation\n48:5\nmodel to predict them back. These models have a notion of bidirectionality, where predicting a\ntoken can take into account both preceding and following tokens. However, MLM is not easily\nsuited for generative tasks. The GPT [12] branch of models are trained with the Causal Language\nModel (CLM) objective that provides the model with a sequence of tokens and trains it to predict the\nnext token [12]. CLM makes the model more suited for auto-regressive generation tasks, however,\nit only takes into account the preceding tokens at a given point.\nIn industrial settings, developers often perform the task of editing where there is code before and\nafter their cursor position. The code after the cursor contains highly relevant signals about the code\nbeing written. Thus, for a code generation system, we would ideally want both \u2013 a generative model\nwith bi-directionality. For this purpose, we used the InCoder LLM [18] as our pre-trained model.\nInCoder is a generative model trained with the Causal Masking (CM) objective, where a sequence of\ntokens are masked out from a given input, appended to the end of the input, and the model is trained\nto generate the whole sentence left-to-right. Moreover, the tokenizer used by InCoder tokenizes\nmultiple words into a single token, and as such allows efficient encoding of common code patterns\nsuch as import numpy as np. For more details, we refer the reader to the InCoder [18] paper.\nTo suit our use case, we make a few modifications to the CM objective and propose Language\nCausal Masking (LCM):\n\u2022 CM implements the masking after the text has been tokenized into token IDs, which limits\nthe model during training to only seeing mask spans with edges at common tokenizer tokens.\nLCM lifts the masking step to the language level and avoids this, similar to the fill-in-the-\nmiddle (FIM) task [11]. Also, LCM only masks at certain trigger characters \u2013 that is, characters\nwhere the model will be queried during inference such as (, ., =, etc.\n\u2022 We prefix certain metadata to the input in LCM, such as the programming language, full path\nto the file, and the kernel name for notebooks.\n\u2022 Through model-level ablations, we found an optimal 70-30 split of the model\u2019s input length\nbetween code before and code after the cursor.\n\u2022 Specialized for our use case, LCM has only one mask in any input.\nA step-by-step overview of constructing an input in LCM is shown in Figure 2, along with\nan example code snippet. Once an input is constructed, during training, we maximize the log\nprobability of the language-masked input:\nlog P([Metadata; Before; <mask>; After; <mask>; Target])\nwhere Metadata, Before and After are the tokens in the metadata, code before, and code after\nthe cursor, respectively, Target is the code that was masked, and <mask> is a special token. During\ninference, we sample tokens in an auto-regressive manner from the distribution:\nP(\u00b7 | [Metadata; Before; <mask>; After; <mask>])\nAs we are suggesting lines of code, we stop the generation early once a newline token has been\ngenerated. Due to the real-time nature of our application and the inline suggestion user experience\n(UX), we only return one sequence of generated tokens.\n2.2\nTraining data\nInCoder has been trained on (i) a corpus of 159 GB of public code in 28 programming languages with\npermissive, non-copyleft, open-source licenses from GitHub and GitLab, and (ii) 57 GB of content\nfrom StackOverflow questions, answers, and comments. For training on our internal data, we\ncollected data Meta\u2019s code repositories and notebooks, i.e., first-party data, applying several filters:\n\u2022 Rather than crawling the entire repository, we used code that is modified through diffs\n(Meta\u2019s term for pull requests) checked in by developers as a way of staying close to our end\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 48. Publication date: July 2024.\n48:6\nV Murali, C Maddila, I Ahmad, M Bolin, D Cheng, N Ghorbani, R Fernandez, N Nagappan, PC Rigby\napplication (i.e., writing code in the IDE). This way we avoid training on code that may have\nbeen added a long time ago but is never modified.\n\u2022 To keep the training data fresh, we only looked at diffs going back up to 2 years, and only\nkept the latest versions of files to avoid bugs that may have been patched.\n\u2022 For each major target language, we exclude code that is not in production or deprecated.\nAfter these filters, our first-party training data included in the order of tens of millions of files\namounting to a few billion lines of code across 10+ languages.\nWe took InCoder-1.3B, the public model with 1.3 billion parameters1, and fine-tuned it further\non the above first-party data with our LCM objective. For fine-tuning the 1.3B model, we used a\nbatch size of 20 per device (2.5 thousand effective) and a learning rate of 5e-4. Training for 4 epochs\nwith sharded data parallelism took 4 days on a cluster of 128 A100 GPUs. We trained the model\non all 9 languages at once. Following standard practice, the training data is randomized to help\nthe model converge and to ensure that it does not overfit to a single language or dataset. We then\ndeployed the model on a cluster of 150 A100 GPUs.\n2.3\nModel Evaluation Method and Measures\nWe describe our methodology to answer RQ1: How well does CodeCompose generate one hidden\nline of code from existing code snippets? Our methodology involves a series of evaluation experi-\nments of CodeCompose that try to measure the accuracy of CodeCompose code generation in\ndifferent languages and the impact of fine-tuning the model with first-party data, i.e., the Meta\ninternal codebase. Our evaluation methodology follows a similar strategy to Figure 2, which we\nexplain in detail in this section.\nThe experiments include four steps, namely test dataset collection, random masking, model\nevaluation, and metric measurements. Our test set is an unseen random sample of 20K source code\nfiles from Meta\u2019s source code repositories. Unseen samples come from a holdout set of files not used\nin training. The holdout set is separated by a filepath-based hash to ensure that there is no leakage\nof data between training and holdout. We collect files that belong to four major programming\nlanguages used in Meta: Hack, C++, Python, and Flow (JavaScript). Once we collect the test dataset,\nto replicate code completion scenarios in the IDE, we randomly mask a part of the code snippets\nending with the end of a line, with a maximum length of 100 tokens, in each source code file. The\nlength of the masked targets is on average 3% (or 39 characters) of the context passed to the model.\nWe used heuristics to ensure that the masked part includes nontrivial spans of code, comments, etc.\nIn this scenario, the prediction task is to predict the masked code snippets when the code before\nand after the masked part are passed to the model as the context. In addition to this context, we\npass metadata such as file name, full path, and the programming language. Furthermore, to avoid\nover inflation of the model accuracy and represent a more realistic use case of CodeCompose\ncode completion, we include extra masking of a random number of tokens after the target line to\nrepresent the not yet written part of the developers\u2019 code and feed the rest of the code snippet as\nthe context to the model. The input context is consistent with the context at the model training,\ni.e., with the maximum size of 2k tokens and with the 70-30 split between the code before and after\nthe masked target. While we use heuristics to make it as realistic as possible, we agree that the\nbacktest may not reflect the actual coding patterns. To address this limitation, we employ multiple\nevaluation methodologies: backtesting (Section 4.1), online evaluation on the suggestions made in\nour production system (Section 4.2), and thematic analysis on user feedback (Section 4.3).\n1At the time of acceptance of this paper, we have moved to using CodeLlama-7B [33] with 7 billion parameters as our\nfoundation model. See [17] for more details.\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 48. Publication date: July 2024.\nAI-assisted Code Authoring at Scale: Fine-tuning, deploying, and mixed methods evaluation\n48:7\nFig. 3. System Architecture for CodeCompose. A pre-trained model checkpoint (InCoder) is trained by\nthe trainer on first-party company data to create the fine-tuned CodeCompose model. This is served in a\ncluster of inference service machines with GPUs. When clients (VS Code, IDEs, web surfaces) require a code\nsuggestion they send a JSON-RPC request to the CodeCompose language server protocol (LSP), which in turn\ncommunicates with the inference service through language-agnostic Thrift calls and sends the generated\nsuggestion back. The LSP also logs telemetry which allows us to compute usage metrics, run experiments, and\nmonitor for regressions. Clients, e.g., VS Code, then display the suggestion through their respective editors.\nTo evaluate the model, we run different versions of the model including the InCoder model as\nthe public model and the fine-tuned model on the first-party data with the CM objective as well as\nthe fine-tuned one with LCM objective. After running the different versions of the CodeCompose\nmodel, we measured two metrics and broke them down into the four main languages of Python,\nC++, Hack, and JavaScript to evaluate how well the model performs and the impact of fine-tuning\nwith first-party data from Meta on the model accuracy. We use the measures of Exact Match (EM),\nas our most restricted metric, and BLEU score [30], which are used in similar studies [18, 27] and\nmeasured by comparing the reference of the masked part of the code and the predicted tokens by\nthe model. The detailed results of this evaluation is described in Section 4.1.\n3\nSYSTEM DESIGN\nThe components of the CodeCompose system are shows in Figure 3. CodeCompose employs a\ntypical client-server architecture in which the server is an inference tier that runs the model and the\nclient is an editor that surfaces code suggestions, including VS Code and Android Studio. We encode\nthe bulk of the client-side logic in a Language Server Protocol (LSP) [8] conformant language server\nthat is reused across multiple editor integrations.\n3.1\nServer\nAt Meta, we have a tier of machines equipped with A100 GPUs, each with sufficient memory to\nhost the fine-tuned CodeCompose model. Clients can make requests to this tier via Thrift [34]. The\ncaller specifies the code before and after the cursor, as well as the file path, language, and metadata\nto use to process the request. The request handler is designed to be simple to utilize the most out of\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 48. Publication date: July 2024.\n48:8\nV Murali, C Maddila, I Ahmad, M Bolin, D Cheng, N Ghorbani, R Fernandez, N Nagappan, PC Rigby\nthe GPUs. It simply processes the request data into a string, tokenizes it, and invokes the model\u2019s\nauto-regressive generation function that runs on the GPU.\nSince we are optimizing for latency rather than throughput, each request is processed as soon\nas it arrives and so there is no inference batching. Due to latency considerations, when the input\nlength is less than 1500 tokens, we can afford to perform a beam search with a beam width of 3 and\nreturn the most likely suggestion. Otherwise, we perform greedy generation.\n3.2\nLanguage Server Protocol\nTo mediate requests between the client and server, we implemented a language server in Rust that\nwe reuse across our various editor integrations. The LSP acts as an orchestrator and handles critical\nfunctionalities in the request-response workflow such as calling the inference APIs, displaying the\nreceived response as inline completions, logging the telemetry, etc.\nWhile most LSPs are designed to support a wide array of traditional IDE functionality, such as\nautocomplete, jump-to-definition, the CodeCompose LSP supports only one meaningful request\ntype: textDocument/inlineCompletions to help with displaying grey text inline completions.\nThe LSP also implements core request logic such as debouncing and caching. Debouncing waits\nfor a 20ms pause before re-sending a request to avoid too many requests from being sent as the\nuser is typing in the editor. Caching saves the response from the server with the context as the key,\nso that duplicate requests \u2013 which happen frequently as developers type and erase code \u2013 can be\navoided. Moreover, requests that hit the LSP cache are served significantly faster than the service.\n3.3\nClients\nClients are extensions that run locally on developers\u2019 machines and are responsible for ultimately\ndisplaying inline suggestions in the editor. For editors, such as VS Code that support LSP natively, we\nrequire relatively little glue code to create the CodeCompose extension. For editors such as Android\nStudio that does not have native LSP support, we built a small adapter to proxy requests to our LSP.\nFurthermore, this architecture makes it straightforward to integrate CodeCompose with our in-\nhouse developer tools, such as the web-based variant of Jupyter notebooks. Because the notebook\nsurface also supports LSP, it was easy to provide our data scientists and ML engineers with the same\nAI-powered code suggestions as developers working in VS Code. This ability to plug CodeCompose\ninto any code editing surface internally is an advantage of owning the entire stack.\n3.4\nTelemetry\nThe LSP is responsible for recording all of the client-side telemetry, which ensures that metrics are\nrecorded consistently across the various surfaces where CodeCompose is used. Because the LSP\nreceives textDocument/didChange events, it can calculate most of the metrics of interest without\nfurther cooperation from the client. Nevertheless, the LSP does expect a client to send a custom\ncc/received notification upon receipt of the completions so that we can record the full round-trip\ntime from the end user\u2019s perspective. Limiting the demands on the client facilitates integrating\nCodeCompose into more of our internal tools.\n3.5\nEvaluation Methodology for CodeCompose in Production\nWe used a mixed methods approach [15] to evaluate CodeCompose in production, collecting usage\ndata for RQ2, and feedback comments for thematic analysis for RQ3. We follow the principles of\nTechnical Action Research at every phase of the roll out [37]. This method of doing the rollout\nsteadily in phases helped us measure the effects of CodeCompose in practice at every step (using the\nquantitative and qualitative feedback) and iterate on the product experience to improve the means\nto achieve intended outcomes before rolling out further. Our rollout strategy for CodeCompose\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 48. Publication date: July 2024.\nAI-assisted Code Authoring at Scale: Fine-tuning, deploying, and mixed methods evaluation\n48:9\nconsists of gradual deployment in waves of languages: (i) only Python, (ii) Hack, Flow (Javascript),\nand C++, (iii) others. Within each wave, we rolled it out to increments of 25% of the developer\npopulation until we enable it for 100% of developers. The rollout was completed after four weeks\nin Spring of 2023.\nA quantitative methodology is necessary to answer RQ2: How many suggestions are accepted\nby engineers and what proportion of the code is written by CodeCompose? We instrumented\ntelemetry to track various events in the IDE such as displaying a suggestion inline, accepting or\nrejecting a suggestion, and the length of accepted suggestions. In total, our large-scale deployment\nresulted in CodeCompose making 4.5 million suggestions across 9 programming languages. 16\nthousand distinct developers have seen at least one CodeCompose suggestion. We only count\nsuggestions that were displayed for at least 750 milliseconds to ensure that developers were exposed\nto a suggestion and had a chance to see and comprehend it.\nOur outcome measures are the acceptance rate of suggestions and the percentage of code typed\nusing CodeCompose. These measures have been used in prior work, with, for example, Google\nreporting that 3% of the code typed by engineers was from their AI [5].\nA thematic analysis is necessary to answer RQ3: How do developers perceive CodeCompose\nin their daily work?\nWe collected data by manually coding user posts in the CodeCompose feedback group. The\nfirst two authors coded responses and discussed them until a consensus was reached. In total, 70\nengineers provided feedback. As starting codes, we used the standard positive, CSAT (Customer\nSatisfaction), or negative, DSAT (Customer Dissatisfaction). We then broke positive and negative\nfeedback into subcodes. We then grouped codes and those that achieved saturation were abstracted\ninto themes. For the major themes, we report the frequency of the themes. To ground our themes\nin the evidence, we provide representative quotations of both positive and negative feedback in\nSections 4.3.1 and 4.3.2, respectively. This feedback also allowed us to reflect on CodeCompose and\nto identify the major factors affecting its perceived usefulness.\n4\nRESULTS\n4.1\nRQ1. Model Evaluation Results\nHow well does CodeCompose generate one hidden line of code from existing code snippets?\nBefore we can release CodeCompose into production, we need to ensure that the model is\nsufficiently accurate. We simulate an AI-authoring experience by selecting a random line from a\nfile and providing the surrounding context to CodeCompose to determine how well it would have\ngenerated the hidden line. The training of the model and description of the metrics and evaluation\nmethodology are in Section 2. We report the results for the number of exactly matching lines as\nwell as the BLEU score for hidden lines in Table 1.\nWe compared the performance of various models such as the public checkpoint of the InCoder\nmodel and the fine tuned versions with different objectives. The Public model performs reasonably\nwell. In Table 1, we see that the percentage of exact matches ranges from 14% for Hack to 23% for\nC++. The BLEU score is also reasonable ranging from 28% for Hack to 40% for C++.\nTable 1 also shows the importance of fine-tuning the CodeCompose model on Meta source code\nso that it can learn Meta-specific code styles, formatting, and libraries. The LCM model always\noutperforms the CM model. The jump is dramatic especially in the case of LCM, with Hack seeing\na 4.1 times improvement on the Exact Match and a 2.6 times improvement on the BLEU score over\nthe Public model. The increase is still substantial for C++ with corresponding improvements of\n1.7\u00d7 and 1.4\u00d7, respectively.\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 48. Publication date: July 2024.\n48:10\nV Murali, C Maddila, I Ahmad, M Bolin, D Cheng, N Ghorbani, R Fernandez, N Nagappan, PC Rigby\nTable 1. Outcome metrics across programming languages for the Public model and the fine-tuned models.\nOur LCM fine-tuned model substantially outperforms the Public model by between 1.4 times and 4.1 times.\nPercentage of Exact Matches (improvement over Public Model by \u00d7 times)\nModels\nPython\nHack\nFlow\nC++\nPublic\n23.1%\n\u2212\n14.2%\n\u2212\n18.6%\n\u2212\n23.4%\n\u2212\nFine-tuned (CM)\n35.5%\n1.5\u00d7\n37.8%\n2.7\u00d7\n30.8%\n1.7\u00d7\n24.2%\n1.0\u00d7\nFine-tuned (LCM)\n48.8%\n2.1\u00d7\n57.7%\n4.1\u00d7\n52.2%\n2.8\u00d7\n40.0%\n1.7\u00d7\nBLEU Score (improvement over Public Model by \u00d7 times)\nModels\nPython\nHack\nFlow\nC++\nPublic\n37.9%\n\u2212\n27.6%\n\u2212\n33.8%\n\u2212\n39.9%\n\u2212\nFine-tuned (CM)\n50.0%\n1.3\u00d7\n52.6%\n1.9\u00d7\n45.8%\n1.4\u00d7\n40.3%\n1.0\u00d7\nFine-tuned (LCM)\n62.0%\n1.6\u00d7\n73.0%\n2.6\u00d7\n68.6%\n2.0\u00d7\n56.2%\n1.4\u00d7\nThis experiment provides strong evidence that an internally fine tuned model outperforms an\noff-the-shelf model that is trained on external data only. The LCM model is very successful and is\nused in the subsequent sections where we roll out CodeCompose to developers.\nWe find that LCM CodeCompose model exactly recreates the masked line between 40%\nand 58% of the time and has a high BLEU score between 56% and 73%, depending on the\nprogramming language. This represents an improvement over the Public model between\n1.4\u00d7 and 4.1\u00d7.\n4.2\nRQ2. Adoption Results\nHow many suggestions are accepted by engineers and what proportion of the code is written by\nCodeCompose?\nWe discuss the design of our CodeCompose system and its integration with VS Code and other\neditors used at Meta in Section 3. All changes at Meta must be gradually rolled out to users and\nwe discussed our rollout strategy and adoption metrics in Section 3.5. Table 2 shows the adoption\nrates of CodeCompose.\nTable 2. The acceptance rate and the proportion of code that was written by CodeCompose across program-\nming languages\nLanguage\n# Suggestions shown\nAcceptance rate\nCode by CodeCompose\n# Users\nPython\n1.9M\n22%\n8%\n10.7K\nHack\n1.3M\n22%\n10%\n5.5K\nC++\n608.1K\n20%\n10%\n2.5K\nFlow (Javascript) [7]\n583.2K\n18%\n7%\n2.5K\nRust\n74.2K\n17%\n9%\n212\nObjective C++\n57K\n18%\n6%\n429\nObjective C\n34K\n18%\n6%\n299\nC\n23.5K\n21%\n12%\n201\nTypescript\n8.9K\n19%\n10%\n76\nTotal\n4.5M\n22%\n8%\n16K\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 48. Publication date: July 2024.\nAI-assisted Code Authoring at Scale: Fine-tuning, deploying, and mixed methods evaluation\n48:11\nCodeCompose made 4.5 million suggestions across 9 programming languages. 16 thousand\ndistinct developers have seen at least one CodeCompose suggestion. As shown in Table 2, we\nobserve a suggestion level acceptance rate of 22% across 9 programming languages for suggestions\nthat were displayed for at least 750 milliseconds. Imposing a lower bound on the display time of\nthe suggestions when calculating the acceptance rate helps in making sure developers who were\nexposed to a suggestion had a chance to see and comprehend it [5].\nOur rollout strategy started with Python, which explains why there are more recommendations\nin that language. However, even typescript has over 8.9K recommendations and an acceptance rate\nof 19%, which is a substantial sample from a statistical perspective. CodeCompose has been rolled\nout to 100% of engineers at Meta.\nWe also log the length of each accepted suggestion, and the number of characters developers\ntype when authoring code (excluding large copy-pastes, refactors, etc.). This allows us to calculate,\nat a character level, the percentage of the code typed by developers that came through accepting\nCodeCompose suggestions, which we computed to be 8%. CodeCompose is writing a substantial\nportion of the codebase, and in the next section we use developer feedback to understand what\ntypes of code are being accepted.\nWe make over 4.5M suggestions to 16K engineers. We see an acceptance rate of 22% which\nis comparable to those at Google and GitHub. We see that 8% of the changed code is written\nby CodeCompose, which is higher than the 3% reported by Google. CodeCompose is rolled\nout to 100% of engineers at Meta.\n4.3\nRQ3. Developer Feedback Results\nHow do developers perceive CodeCompose in their daily work?\nAt Meta each internal tool has a feedback forum. The first two authors conducted a thematic\nanalysis of the responses from 70 engineers (the methodology is described in Section 3.5). Similarly,\nthe model does not differentiate between suggestion types such as code suggestions and documen-\ntation suggestion when making suggestions. We report the frequency of themes and suggestion\ntypes in Table 3. In subsequent sections, we provide grounded evidence of the themes through\nquotations and discuss both positive and negative feedback.\nTable 3 lists the distribution of qualitative feedback responses. 91.5% of the CodeCompose users\ngave a favorable response while 8.5% of them gave an unfavorable response. Many respondents\n(15.7%) appreciated the fact that CodeCompose suggestions are accurate and CodeCompose was\nable to auto-complete the code they were going to type.\nIn addition to the accuracy of suggestions, 23% of the users said CodeCompose helped them\ndiscover new APIs, automating tedious tasks such as boilerplate code. Therefore, CodeCompose\nis not just helping with typing code faster (as demonstrated in Table 2) but also saving time and\neffort people spend searching for APIs and documentation.\nCodeCompose is helping people move faster with with 20% of the users stating that they\nfound CodeCompose to accelerating their coding activity. In addition to helping write code faster,\nCodeCompose is helping developers produce more in-code documentation and API documentation.\nThis is an interesting side-effect and demonstrates the model\u2019s ability to generate text as an output\nfrom code in addition to code generation.\n4.3.1\nEvidence of Positive Experiences. After analyzing all the qualitative feedback, we identified the\nfollowing main factors that make a developer inclined towards finding a system like CodeCompose\nuseful.\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 48. Publication date: July 2024.\n48:12\nV Murali, C Maddila, I Ahmad, M Bolin, D Cheng, N Ghorbani, R Fernandez, N Nagappan, PC Rigby\nTable 3. Frequency of themes coded from the feedback of 70 engineers. There are only six unfavorable\nresponses, so we discuss each in Section 4.3.2\nTheme\nDescription\nEngineers\nAccelerate coding\nCodeCompose helped speed up my\ncoding process\n14 (20.0%)\nDiscovery\nCodeCompose helped me discover\nAPIs, write boilerplate code faster\n16 (22.8%)\nDocumentation\nCodeCompose helped me with gen-\nerating in-code documentation, doc-\nstrings, etc.\n5 ( 7.1%)\nSuggestion accuracy\nCodeCompose suggestions are ac-\ncurate and not noisy\n11 (15.7%)\nGenerally favorable\nCodeCompose is generally great,\nand I will keep using it for my cod-\ning tasks\n18 (25.7%)\nUnfavorable\nCodeCompose is not useful\n6 ( 8.5%)\nTotal\n70 (100%)\nFavorable scenarios for CodeCompose: The scenarios for which CodeCompose was able to add\nthe biggest value includes (but is not limited to), auto-completing lines of code, API discovery,\nboilerplate coding, suggesting standard libraries, generating in-code documentation, etc.\nWhen it comes to developers, the developers who benefited the most are the ones who work on\nauthoring code that involves writing boilerplate code and idioms, the ones who follow the typical\ncoding patterns employed at Meta, and the ones that employ standard first-party (or third-party)\nlibraries to accomplish their coding tasks. Additionally, CodeCompose is well received by the\ndevelopers who tend to work on building pipelines and common infrastructure tasks that involve\nwriting heavily templatized code. This can be corroborated by the distribution of responses listed\nin Table 3 and the representative quotes listed below.\nGrounded evidence of positive experiences. We share quotations that exemplify the pos-\nitive experiences of developers. A developer who got access to CodeCompose shared their first\nimpressions:\n\"Today I needed to write a quick tool for [task]. I opened VS Code and started writing\n[code]. Suddenly this thing called CodeCompose popped up and wrote all the code for\nme! This was a super delightful experience, it really made my day, so thank you!\"\nIn this case, the developer was unaware of the existence of CodeCompose. This quote summarizes\nthe delightful coding experience that the developers at Meta experienced when they pair-program\nwith CodeCompose.\nSeveral remarks are clear indicators of the usefulness of employing CodeCompose in code\nauthoring workflows.\n\"I have been a big fan of [CodeCompose ] ever since I got access. I do not think I have\nbeen in any coding session since then where I did not use it.\"\n\"The suggestions [from CodeCompose ] are quite relevant and do not show up too often \u2013\nnot necessarily \u201crarely\u201d, but not obnoxiously often either. It is a nice balance.\"\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 48. Publication date: July 2024.\nAI-assisted Code Authoring at Scale: Fine-tuning, deploying, and mixed methods evaluation\n48:13\n\"I was blown away that after writing the following [code], CodeCompose successfully\npredicted the correct completion. This is amazing because I could not remember the name\nof the global variable or where it was in the code, but CodeCompose knew what it was.\"\nMany developers highlighted the fact that the predictions are relevant. Also, we received feedback\nabout how nicely CodeCompose navigates the precision versus recall problem by not showing\nsuggestions too often.\nSeveral pieces of feedback highlighted CodeCompose\u2019s ability to help developers discover new\nAPIs or ramp them up quickly on unfamiliar APIs.\n\"I wanted to post about a great experience I had in using CodeCompose in the [internal]\ncodebase. I have been getting back into coding and was rusty on the different arguments\nthat exist in the [library] operators. Some operators can have up to [hundreds of] argu-\nments so this is a challenging problem. CodeCompose has helped me several times in a\nday by giving me the right argument that I cared about, whereas my previous workflow\nwould have been to [search] for the operator and spend several minutes on each.\"\n\"CodeCompose has been a game changer for working with APIs that I have not used\nbefore. Even if it is not exactly what I need, it often shows me approximately what I should\npass or how I should call it. This is much faster than looking for other call sites to figure\nout how to use this [API].\"\nWe also observed interesting side effects such as increased in-code documentation when develop-\ners use CodeCompose to make code changes. As CodeCompose provides accurate descriptions of\ncode changes and a template to start with, developers tend to accept the suggestions, make changes\n(as necessary), and push the documentation changes (along with the source code changes) in their\ndiffs. This is reflected in some of the anecdotes listed below.\n\"I find CodeCompose particularly useful when writing docstrings. Without CodeCompose,\nI would not even imagine that I can write [so many] lines of docstring for my actual code.\"\n\"I really like how CodeCompose highlights the value of naming and the quality of\ndocumentation, which used to be mainly for long-term benefit, but now good naming and\n[documentation] gives you an immediate effectiveness boost.\"\n4.3.2\nEvidence of Negative Experiences. After analyzing the qualitative feedback, we found that\nCodeCompose is not helpful in scenarios where developers use specialized APIs and libraries.\nCodeCompose seems to be struggling with suggesting the correct code when the developers employ\natypical coding patterns.\nSome developers found the traditional, semantic, auto-complete functionality to be more useful\nin those cases. Developers also complained about the possibility of CodeCompose to hallucinate\n[19, 25] when recommending the uncommon API names, URLs, etc.\nDevelopers also found the coexistence of CodeCompose with traditional auto-complete to be\nvital. Sometimes, when these two systems compete to show suggestions, it creates a disruptive\nexperience for developers. Also, commonly used keyboard shortcuts such as \u201cTab\u201d are overloaded\nto accept CodeCompose suggestions and traditional auto-complete suggestions. Solving these UX\nproblems is vital to facilitating a smoother integration with existing semantic engines, which is\naddressed in our discussion Section 7.2.\nGrounded evidence of negative experiences. A few developers disabled CodeCompose as\nthey found CodeCompose to be noisy and intrusive. We list a few anecdotes below.\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 48. Publication date: July 2024.\n48:14\nV Murali, C Maddila, I Ahmad, M Bolin, D Cheng, N Ghorbani, R Fernandez, N Nagappan, PC Rigby\n\"CodeCompose is good, however, it seems to reduce my coding speed. The reason is that\nthe traditional auto-complete was typically suggesting [real] functions, but CodeCom-\npose is suggesting hallucinations. These are not always correct and I would like a UX\ndesign where I could switch between traditional auto-complete and CodeCompose.\"\n\"I have realized that [CodeCompose ] tends to struggle while working with function signa-\ntures. The suggestions it offers do not match the actual signature, although they make sense.\"\n\"The [code editing] experience is sometimes good and sometimes bad. Auto-complete\nstruggles to work together with CodeCompose \u2013 they seem to be competing with each\nother.\"\nThis highlights the importance of investing in UX research to deliver the best experience for AI-\nassisted code authoring while coexisting with traditional auto-complete systems. The feedback also\ntalks about the problems faced by LLMs concerning hallucinations and grounding [19, 23]. We are\nactively exploring this area to make CodeCompose a productive experience for all the developers.\nThe feedback was overwhelmingly positive, with only 8.5% of comments containing negative\nfeedback and less than 1% of engineers turning off CodeCompose. The top two feedback\ncomments related to discovering API and dealing with boilerplate (23%), and accelerating\ncoding (20%). Meta continues to invest in CodeCompose efforts to iteratively improve the\nAI-assisted authoring experience.\n5\nTHREATS TO VALIDITY\n5.1\nGeneralizability\nCodeCompose is an entirely internal tool that has only been deployed at Meta. Therefore, there is\na threat of drawing any general conclusions from the quantitative metrics or qualitative feedback\npresented. It is possible that the results might not hold elsewhere, and for this reason, we cannot\nassume a priori that other AI-based coding assistants built outside Meta will perform similarly.\nSimilarly, the value-add of fine-tuning the LLM on Meta\u2019s internal code (Table 1) may also not\ntranslate externally. However, we see evidence of the need to fine-tune public models on internal\ndata at Google [5]. Without this tuning on internal APIs and functions, the model would be unable\nto make reasonable suggestions for the internal codebase.\n5.2\nConstruct Validity\nThere are many AI model evaluation metrics. We have selected the commonly used BLEU score\nand the percent of exact matches. While there are limitations with these metrics, exact match is a\nvery conservative measure only succeeding when the code is perfectly generated.\nThere have been few deployments of AI reported in the literature, so we used standard measures\nemployed at Meta, such as the level of adoption and the acceptance rate to quantify the efficacy of\nAI-authoring at scale. We triangulated these numerical results with a thematic analysis of feedback\nfrom 70 developers to ensure that the model\u2019s success resulted in a usable product.\n5.3\nInternal Validity\nWhile we have taken significant measures to reduce bias in the results presented (e.g., by waiting\nseveral weeks after randomized rollout), we cannot guarantee that there is absolutely no bias. The\ntime period chosen for measuring the metrics in Table 2 could have had external events beyond\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 48. Publication date: July 2024.\nAI-assisted Code Authoring at Scale: Fine-tuning, deploying, and mixed methods evaluation\n48:15\nour control that influenced developers\u2019 coding behaviors during that time. Some languages such as\nTypescript only has a small number of developers, who might have had an outsized influence on\nthe metrics for those languages.\nQualitative feedback, in general, comes with a degree of subjectivity. While Meta encourages a\nculture of open feedback, developers might not have been comfortable sharing negative assessments\nwith colleagues. We facilitated easy opt-out, but less than 1% had opted out, indicating that a\nmajority of developers found CodeCompose to be a net positive experience.\nThe categorization of the qualitative feedback into different categories was done manually using\na basic thematic analysis rather than a full grounded theory study. We tried to reduce subjectivity\nin this process by making two authors in the team categorize them and then reconcile the different\nlabels. Nevertheless, a different set of people or more systematic methodology might have produced\na different categorization in Table 3.\n6\nRELATED WORK\nIn this section, we review the related work and position our contribution in the literature. Neural\nprogram synthesis is a rapidly evolving area with a large body of work discussing the possibility\nof applying neural architectures, such as Bayesian networks, and Transformers, to solve program\nsynthesis tasks [13, 14, 24, 31, 32, 38]. These models are typically evaluated against public data sets\nsuch as MBPP (974 Python programming tasks, designed to be solvable by entry-level programmers)\n[9] and Human Eval (164 original human-curated Python programming problems) [14]. The data\nset we used to evaluate the accuracy of code generation contains 20,000 data points sourced from\nreal code bases, curated across 9 programming languages, at Meta.\nThere has been a limited deployment of AI-assisted code generation systems in large-scale\nindustrial environments with tens of programming languages and thousands of developers using\nthem daily [2, 4\u20136]. Availability of research in the public domain is even limited when it comes to\nevaluating the accuracy of neural program synthesis techniques on large scale industrial data sets\nand understanding the effects of such tools on software developers using mixed methods studies,\nThe closest in spirit to our work is the brief blog post from Google [5]. At Google, deploying\na hybrid semantic ML code completion to 10k+ Google employees compared to a control group\nobserved a 6% reduction in coding iteration time (time between builds and tests) when exposed\nto single-line ML completion. They reported that 3% of new code (measured in characters) was\ngenerated from accepting ML completion suggestions. The acceptance rate for single-line code\ncompletions is 25%. These results demonstrate the impact on developer productivity. In the OSS\ndomain, GitHub CoPilot reported an acceptance rate of 30% for code completions [3].\nOur more detailed findings are in line with those found by Google and GitHub with a 22%\nacceptance rate for 4.5 million single-line code completions across 9 programming languages. At\nthe same time, our suggestions accounted for 8% of the code written by engineers at Meta compared\nto the 3% reported by Google.\nThere have been some empirical evaluations of GitHub\u2019s Copilot [2] in actual use for automatic\ncode completion. In a grounded theory study of 20 developers using Copilot, Barke et al. [10] found\nthat developers switched between coding acceleration and discovery use cases. Our engineers also\nreported these same themes as their most common positive feedback. Nguyen et al. [28] used 33\nLeetCode questions to create queries for Copilot in four different programming languages. They\nfound that Copilot\u2019s Java suggestions have the highest correctness score (57%) while JavaScript is\nthe lowest (27%) [28]. While LeetCode examples are useful for algorithmic interview preparation,\nthey differ dramatically from normal engineering work.\nCode generation could also be achieved by prompting a general LLM such as ChatGPT [26]. Also,\nit has been found that different types of prompts can lead to different code generation performance.\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 48. Publication date: July 2024.\n48:16\nV Murali, C Maddila, I Ahmad, M Bolin, D Cheng, N Ghorbani, R Fernandez, N Nagappan, PC Rigby\nWe performed prompt engineering on the fine-tuned model (for e.g., determining the 70-30 split of\nthe context fed to the model) to make it effective for code suggestions. We also show in Table 1\nthat public models underperform on Meta \u2019s internal code base for code suggestion tasks compared\nto models fine-tuned on it.\nA study with 15,000 synthetic data points curated from a C# code base from an industrial software\ndevelopment environment reported an accuracy of 30.7% for predicting single tokens, such as\nidentifiers, when employing a modified Best Matching Neighbor approach (BMN#) [20]. Offline\nevaluations of Neural methods for auto-completion in a comparable industrial setting yielded\naccuracy of 41.9% on languages with heavily customized frameworks such as Hack [38] when tested\non a couple of thousands of examples and when tested on open-source data sets (CodeSearchNet)\n[22] yielded an Exact Match score of 20.10% and BLEU score of 32.79% [27]. Our study of 20,000\ndata points curated, across 9 programming languages, from Meta\u2019s production code base shows\nthat CodeCompose achieves the highest correctness score of 52% and BLEU score of 73% for Hack\nlanguage and lowest correctness score of 40% and BLEU score of 56.2% for C++.\nWhile Copilot is impressive, not all usage of has been positive. A qualitative study with 24 partic-\nipants to understand how programmers use and perceive Copilot found that it did not necessarily\nimprove the task completion time or success rate by a significant margin [35]. However, 19 of 24\nparticipants (79%) answered that they preferred Copilot over Intellisense. A similar qualitative\nstudy of CodeCompose with a larger group of software engineers at Meta (70 participants) resulted\nin a 91.5% favorable feedback. Developers cited reasons such as accelerated coding or typing, help-\ning in discovering new APIs and writing boilerplate code in unknown programming languages,\nhelping with generating documentation for their code, accuracy of suggestions and its ability to\npredict what the developers are going to type as the main reasons.\n7\nDISCUSSION AND LESSONS LEARNED\nWe organize our discussion around the challenges and learnings in creating trust, making the user\nexperience better, and employing metrics to evaluate AI coding assistants at scale.\n7.1\nTrust\nMost of today\u2019s code generation LLMs consider code as a sequence of tokens, similar to natural\nlanguage. That is one of the reasons for the observed acceleration in the model-building process \u2013\ndealing with the complexities of parsing source code or developing semantic understanding is not\nrequired anymore. However, this has some side effects: most of the models cannot guarantee that\nthe generated code compiles, is syntactically correct, or executes. For example, the model may make\na \u201csuggestion\u201d by generating an API call that does not exist. Making developers trust and accept\nsuggestions in this setting is a huge challenge. Other factors can also impact trust such as the source\ncorpus used for training and its validity, biases in training data, security issues, vulnerabilities,\nconstant data drift, etc.\nThe traditional precision versus recall problem in machine learning also plays into trust. If the\nmodel is optimized for recall and generates a lot of incorrect suggestions, developers will lose faith\nin the system as it becomes noisy. If the model is optimized for precision and generates only a\nfew accurate suggestions, developers might eventually stop caring about the system as it becomes\nsparse. Also, it will be hard to justify the return on investment if the system is not generating\nenough suggestions. Studies[1] show that developers are fine with reworking a suggestion as long\nas the model provides a useful starting point or structure with relevant libraries and function calls.\nHowever, requiring a lot of rework might contribute to the reduction of trust and confidence in the\nmodel.\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 48. Publication date: July 2024.\nAI-assisted Code Authoring at Scale: Fine-tuning, deploying, and mixed methods evaluation\n48:17\nLearnings for CodeCompose. Building trust was an important part of our productization, as it\nis directly affected by model accuracy. First, we worked with language partner teams at Meta to\nidentify obsolete patterns (e.g., \u201cPHP-isms\u201d in Hack) and not-in-production code (e.g., experiments)\nin the codebase and filter it out from our training data. In a similar vein, we only train on code that\nis being actively modified, that is, containing commits in the last two years, as opposed to all code\nin the repository.\nSecondly, from developer feedback, we found out that contextual information adds significant\nvalue to the suggestion accuracy, such as the code after the cursor, the file being edited, or the\nkernel being used (in a notebook). We modified our model\u2019s training objective to take into account\nthis contextual information (for more details see Section 2).\nFinally, our rollout strategy allowed us to incrementally build trust in the product (details about\nour rollout strategy is explained in Section 3.5).\n7.2\nUser Experience\nDue to the generality of LLMs, code generation can be done at multiple levels of granularity:\ntoken completion, statement completion, completing multiple lines, or generating entire blocks.\nDepending on factors such as suggestion confidence, user context, and task context, we will need\nto generate suggestions at different granularities. For example, when developers are in their coding\n\u201cflow\u201d, suggesting a large block of code and moving their existing code down \u2013 and moving it back\nup when they type over the suggestion \u2013 results in an extremely jarring experience. This is because\ndevelopers would constantly need to switch between typing code and reviewing the suggested\nblock of code, resulting in an unpleasant \u201ccode review on the fly\u201d situation. Therefore, when to\nsuggest, what to suggest, and how much to suggest is an essential part of the user experience.\nSimilarly, performance and latency are also an important consideration. Coding assistants operate\nin a real-time environment with strict requirements on latency. Developers do not want to wait for\nseveral seconds for the system to generate a single suggestion as they are typing it \u2013 the system has to\nmatch the developer\u2019s typing speed. Studies[21] have shown that sometimes users adjust their typing\nbehaviors (and speed) to match autocomplete suggestions in IDEs. However, slowing oneself down\nshould be compensated by passing the right suggestions and making it a \u201cnet positive\u201d experience.\nLearnings for CodeCompose. After several iterations, we found, through developer feedback,\nthat offering suggestions one line at a time \u2013 that is, completing the current line of code where the\ncursor is \u2013 strikes a good balance between suggesting an adequate amount of code and avoiding\nthe jarring effect. Furthermore, even within the current line, we do not show suggestions if there is\nany code to the right of the cursor, except certain tokens such as ), }, and ].\nIn terms of latency, developers were fine with suggestions appearing within 300ms - 500ms,\nand certainly not beyond 1s. We observed that the acceptance rate went down when the end-to-\nend latency went up. To bring down the end-to-end latency below this threshold, we employed\na suite of optimizations such as caching and debouncing (for more details see Section 3). With\nthis experience, developers who are looking for multi-line suggestions simply press Tab as many\ntimes as needed. This allows them to edit the currently suggested line before going to the next line,\nwhich makes the subsequent suggestions more accurate. They can also press a manual keyboard\nshortcut to request a multi-line suggestion, but we found that developers rarely use this workflow.\n7.3\nMetrics\nEvaluating the usefulness of AI-generated suggestions is a major challenge[39]. This includes, but is\nnot limited to, the complexities involved with the granularity of suggestions, developers\u2019 flexibility\nto rework the generated code, defining true positives, etc. A strict metric like Exact Match, BLEU\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 48. Publication date: July 2024.\n48:18\nV Murali, C Maddila, I Ahmad, M Bolin, D Cheng, N Ghorbani, R Fernandez, N Nagappan, PC Rigby\nscore or percentage of accepted suggestions as-is will underrepresent the actual benefits offered\nby these solutions. For example, while the acceptance rate is a good metric to measure product\nusage, a high acceptance rate does not necessarily mean developers are being more productive\nwhen using the tool \u2013 that has to be measured explicitly.\nFurthermore, evaluation needs to go beyond just measuring product impact \u2013 it should help\nus understand the cases where the suggestions are inaccurate, or what the pain points are with\nrespect to the user experience, to address those issues. Additional complexities including privacy,\ncompliance, and proprietary data make it harder to track all the telemetry that is required to\nevaluate or monitor the product.\nLearnings for CodeCompose. We developed a suite of metrics that can quantify our impact, some\nof which are standard definitions of internal metrics to measure the scale of deployment of tools at\nMeta. We track usage metrics such as the number of active users, user engagement and retention,\nnumber of suggestions offered, acceptance rate, and the percentage of code typed by developers,\nthat comes from accepting CodeCompose\u2019s suggestions (for more details see Section 4).\n8\nCONCLUSION\nIn this paper, we introduced an AI-based coding assistant system named CodeCompose, discussed\nhow we scaled it to 16 thousand developers across 9 programming languages at Meta, and presented\nmetrics and feedback to understand the impact of the system on code authoring.\nWe made the following contributions:\nWe then presented the system architecture of CodeCompose. We built CodeCompose using three\nprimary components: the model inference service, Language Server Protocol (LSP), and the client.\nWhen a developer types code, the service receives a request to perform inference and generates a\nsequence of tokens to auto-complete the statement(s). The LSP and the client help orchestrate the\ninference calls and implement the logic to display the suggestion.\nWe presented details about the underlying InCoder-based LLM that powers CodeCompose. We\nintroduced a custom training objective, Language Causal Masking, that suits our application of\nsuggesting individual lines of code. In doing so, we conducted an offline evaluation that showed a\n1.4\u00d7 and 4.1\u00d7 improvement brought about by fine-tuning the LLM on the Meta\u2019s internal codebase.\nWe presented quantitative metrics that show the scale and impact of CodeCompose. 4.5 million\nsuggestions were shown to developers resulting in 8% of code typed by users of CodeCompose\ncoming from its suggestions.\nWe presented qualitative feedback from developers. We found that an overwhelming 91.5% of\nthe feedback shared by developers was favorable towards CodeCompose, indicating that it adds\nsignificant value to their coding experience. Through a thematic analysis of the feedback, we found,\nfor example, that CodeCompose helps developers discover unfamiliar APIs and write documentation.\nCodeCompose is now rolled out to all developers, and less than 1% of developers disabled it.\nIn the future, we plan to enable more features such as building dynamic context from other files\nto improve suggestion accuracy, incorporate feedback from user actions on suggestions to perform\ninference-time optimizations, more interaction modalities such as a conversational modality within\nthe IDE, functionality to explain source code and provide code walk-throughs, etc. We are exploring\nopportunities to leverage semantic information to perform pre-processing and post-processing to\nimprove suggestion accuracy by reducing hallucinations. Furthermore, we see opportunities to\nexpand this technology, beyond code authoring, to help developers across the software development\nlife cycle.\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 48. Publication date: July 2024.\nAI-assisted Code Authoring at Scale: Fine-tuning, deploying, and mixed methods evaluation\n48:19\nREFERENCES\n[1] Github 2022. How GitHub Copilot helps improve developer productivity. Github. https://github.blog/2022-07-14-\nresearch-how-github-copilot-helps-improve-developer-productivity/\n[2] Github Accessed 2021. Github Copilot. Github. https://github.com/features/copilot\n[3] Accessed 2021. GitHub CoPilot\u2019s economic impact. https://github.blog/2023-06-27-the-economic-impact-of-the-ai-\npowered-developer-lifecycle-and-lessons-from-github-copilot/\n[4] Microsoft Accessed 2021. Microsoft Intellicode. Microsoft. https://visualstudio.microsoft.com/services/intellicode\n[5] Google Accessed 2021. ML Enhanced Code Completion. Google. https://ai.googleblog.com/2022/07/ml-enhanced-code-\ncompletion-improves.html\n[6] Amazon Accessed 2023. Amazon CodeWhisperer. Amazon. https://aws.amazon.com/codewhisperer\n[7] Accessed 2023. Flow Javascript. https://engineering.fb.com/2014/11/18/web/flow-a-new-static-type-checker-for-\njavascript\n[8] Microsoft Accessed 2023. Language Server Protocol (LSP). Microsoft. https://github.com/microsoft/language-server-\nprotocol\n[9] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang,\nCarrie Cai, Michael Terry, Quoc Le, and Charles Sutton. 2021. Program Synthesis with Large Language Models.\narXiv:2108.07732 [cs.PL]\n[10] Shraddha Barke, Michael B. James, and Nadia Polikarpova. 2023. Grounded Copilot: How Programmers Interact with\nCode-Generating Models. 7, OOPSLA1, Article 78 (apr 2023), 27 pages. https://doi.org/10.1145/3586030\n[11] Mohammad Bavarian, Heewoo Jun, Nikolas Tezak, John Schulman, Christine McLeavey, Jerry Tworek, and Mark Chen.\n2022. Efficient Training of Language Models to Fill in the Middle. arXiv:2207.14255 [cs.CL]\n[12] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,\nRewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,\nIlya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. arXiv:2005.14165 [cs.CL]\n[13] Marcel Bruch, Martin Monperrus, and Mira Mezini. 2009. Learning from examples to improve code completion systems.\nIn Proceedings of the 7th joint meeting of the European software engineering conference and the ACM SIGSOFT symposium\non The foundations of software engineering (ESEC/FSE \u201909).\n[14] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards,\nYuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,\nGirish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser,\nMohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert,\nFotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak,\nJie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan\nLeike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie\nMayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021.\nEvaluating Large Language Models Trained on Code. arXiv:2107.03374 [cs.LG]\n[15] John W Creswell and Vicki L Plano Clark. 2017. Designing and conducting mixed methods research. Sage publications.\n[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding. arXiv:1810.04805 [cs.CL]\n[17] Omer Dunay, Daniel Cheng, Adam Tait, Parth Thakkar, Peter C Rigby, Andy Chiu, Imad Ahmad, Arun Ganesan, Chandra\nMaddila, Vijayaraghavan Murali, Ali Tayyebi, and Nachiappan Nagappan. 2024. Multi-line AI-assisted Code Authoring.\n[18] Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen tau Yih, Luke Zettle-\nmoyer, and Mike Lewis. 2023. InCoder: A Generative Model for Code Infilling and Synthesis. arXiv:2204.05999 [cs.SE]\n[19] Nuno M. Guerreiro, Duarte Alves, Jonas Waldendorf, Barry Haddow, Alexandra Birch, Pierre Colombo, and Andr\u00e9 F. T.\nMartins. 2023. Hallucinations in Large Multilingual Translation Models. arXiv:2303.16104 [cs.CL]\n[20] Vincent J. Hellendoorn, Sebastian Proksch, Harald C. Gall, and Alberto Bacchelli. 2019. When Code Completion Fails:\nA Case Study on Real-World Completions. In 2019 IEEE/ACM 41st International Conference on Software Engineering\n(ICSE). 960\u2013970. https://doi.org/10.1109/ICSE.2019.00101\n[21] Kajta Hofmann, Bhaskar Mitra, Filip Radlinski, and Milad Shokouhi. 2014. An Eye-Tracking Study of User Interactions\nwith Query Auto Completion (CIKM \u201914). Association for Computing Machinery, New York, NY, USA, 549\u2013558.\nhttps://doi.org/10.1145/2661829.2661922\n[22] Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, and Marc Brockschmidt. 2020. CodeSearchNet\nChallenge: Evaluating the State of Semantic Code Search. arXiv:1909.09436 [cs.LG]\n[23] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and\nPascale Fung. 2023. Survey of Hallucination in Natural Language Generation. ACM Comput. Surv. 55, 12, Article 248\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 48. Publication date: July 2024.\n48:20\nV Murali, C Maddila, I Ahmad, M Bolin, D Cheng, N Ghorbani, R Fernandez, N Nagappan, PC Rigby\n(mar 2023), 38 pages. https://doi.org/10.1145/3571730\n[24] Seohyun Kim, Jinman Zhao, Yuchi Tian, and Satish Chandra. 2021. Code Prediction by Feeding Trees to Transformers.\nIn 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE). 150\u2013162. https://doi.org/10.1109/ICSE\n43902.2021.00026\n[25] Zihao Li. 2023. The Dark Side of ChatGPT: Legal and Ethical Challenges from Stochastic Parrots and Hallucination.\narXiv:2304.14347 [cs.CY]\n[26] Chao Liu, Xuanlin Bao, Hongyu Zhang, Neng Zhang, Haibo Hu, Xiaohong Zhang, and Meng Yan. 2023. Improving\nChatGPT Prompt for Code Generation. arXiv:2305.08360 [cs.SE]\n[27] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain,\nDaxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou,\nNan Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie Liu. 2021. CodeXGLUE: A Machine Learning\nBenchmark Dataset for Code Understanding and Generation. arXiv:2102.04664 [cs.SE]\n[28] Nhan Nguyen and Sarah Nadi. 2022. An empirical evaluation of GitHub copilot\u2019s code suggestions. In Proceedings of\nthe 19th International Conference on Mining Software Repositories (MSR \u201922).\n[29] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2023.\nCodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis. arXiv:2203.13474 [cs.LG]\n[30] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: A Method for Automatic Evaluation\nof Machine Translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics\n(Philadelphia, Pennsylvania) (ACL \u201902). Association for Computational Linguistics, USA, 311\u2013318. https://doi.org/10.3\n115/1073083.1073135\n[31] Sebastian Proksch, Johannes Lerch, and Mira Mezini. 2015. Intelligent Code Completion with Bayesian Networks.\nACM Transactions on Software Engineering and Methodology (TOSEM) 25, 1, Article 3 (12 2015).\n[32] R. Robles and M. Lanza. 2008. How Program History Can Improve Code Completion. In 2008 23rd IEEE/ACM International\nConference on Automated Software Engineering. 317\u2013326. https://doi.org/10.1109/ASE.2008.42\n[33] Baptiste Rozi\u00e8re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal\nRemez, J\u00e9r\u00e9my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron\nGrattafiori, Wenhan Xiong, Alexandre D\u00e9fossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier,\nThomas Scialom, and Gabriel Synnaeve. 2023. Code Llama: Open Foundation Models for Code. arXiv:2308.12950 [cs.CL]\n[34] Mark Slee, Aditya Agarwal, and Marc Kwiatkowski. 2007. Thrift: Scalable cross-language services implementation.\nFacebook white paper 5, 8 (2007), 127.\n[35] Priyan Vaithilingam, Tianyi Zhang, and Elena L. Glassman. 2022. Expectation vs. Experience: Evaluating the Usability\nof Code Generation Tools Powered by Large Language Models. In Extended Abstracts of the 2022 CHI Conference on\nHuman Factors in Computing Systems (CHI EA \u201922).\n[36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\nPolosukhin. 2017. Attention Is All You Need. arXiv:1706.03762 [cs.CL]\n[37] Roel Wieringa and Ay\u015fe Moral\u0131. 2012. Technical Action Research as a Validation Method in Information Systems\nDesign Science. In Design Science Research in Information Systems. Advances in Theory and Practice, Ken Peffers, Marcus\nRothenberger, and Bill Kuechler (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 220\u2013238.\n[38] Wen Zhou, Seohyun Kim, Vijayaraghavan Murali, and Gareth Ari Aye. 2022. Improving Code Autocompletion with\nTransfer Learning. In 2022 IEEE/ACM 44th International Conference on Software Engineering: Software Engineering in\nPractice (ICSE-SEIP). 161\u2013162. https://doi.org/10.1145/3510457.3513061\n[39] Albert Ziegler, Eirini Kalliamvakou, X. Alice Li, Andrew Rice, Devon Rifkin, Shawn Simister, Ganesh Sittampalam,\nand Edward Aftandilian. 2022. Productivity Assessment of Neural Code Completion. In Proceedings of the 6th ACM\nSIGPLAN International Symposium on Machine Programming (San Diego, CA, USA) (MAPS 2022). Association for\nComputing Machinery, New York, NY, USA, 21\u201329. https://doi.org/10.1145/3520312.3534864\nReceived 2023-09-29; accepted 2024-01-23\nProc. ACM Softw. Eng., Vol. 1, No. FSE, Article 48. Publication date: July 2024.\n"
  },
  {
    "title": "OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models",
    "link": "https://arxiv.org/pdf/2305.12001.pdf",
    "upvote": "1",
    "text": "OPT-R: Exploring the Role of Explanations in Finetuning and Prompting\nfor Reasoning Skills of Large Language Models\nBadr AlKhamissi\nSiddharth Verma\nPing Yu\nZhijing Jin\nAsli Celikyilmaz\nMona Diab\nMeta AI\nAbstract\nIn this paper, we conduct a thorough investiga-\ntion into the reasoning capabilities of Large\nLanguage Models (LLMs), focusing specif-\nically on the Open Pretrained Transformers\n(OPT) models as a representative of such mod-\nels. Our study entails finetuning three differ-\nent sizes of OPT on a carefully curated rea-\nsoning corpus, resulting in two sets of fine-\ntuned models: OPT-R, finetuned without ex-\nplanations, and OPT-RE, finetuned with expla-\nnations. We then evaluate all models on 57\nout-of-domain tasks drawn from the SUPER-\nNATURALINSTRUCTIONS benchmark, cover-\ning 26 distinct reasoning skills, utilizing three\nprompting techniques. Through a comprehen-\nsive grid of 27 configurations and 6,156 test\nevaluations, we investigate the dimensions of\nfinetuning, prompting, and scale to understand\nthe role of explanations on different reasoning\nskills. Our findings reveal that having expla-\nnations in the fewshot exemplar has no signifi-\ncant impact on the model\u2019s performance when\nthe model is finetuned, while positively affect-\ning the non-finetuned counterpart. Moreover,\nwe observe a slight yet consistent increase in\nclassification accuracy as we incorporate ex-\nplanations during prompting and finetuning, re-\nspectively. Finally, we offer insights on which\nskills benefit the most from incorporating ex-\nplanations during finetuning and prompting,\nsuch as Numerical (+20.4%) and Analogical\n(+13.9%) reasoning, as well as skills that ex-\nhibit negligible or negative effects.\n1\nIntroduction\nRecently, there has been a surge in the release of\nLarge Language Models (LLMs) by both indus-\ntrial and academic institutions. These models vary\nfrom open-source releases such as OPT (Zhang\net al., 2022) and LLAMA (Touvron et al., 2023) to\nclosed-source ones like GPT-3 (Brown et al., 2020)\nand PALM (Chowdhery et al., 2022). In addition,\nresearchers have developed models that are fine-\ntuned on top of these foundational models to better\nScale\nFinetuning\nPrompting\n1.3B\n6.7B\n13B\nOPT\nOPT-R OPT-RE\nZeroshot\nFewshot\nFewshot-E\nFigure 1: Three-Dimensional Grid of Fine-Tuning,\nPrompting, and Scale. Each dimension is represented\nas an axis, with three levels for each of finetuning,\nprompting, and scale plotted on each axis. The resulting\ngrid consists of 27 different combinations evaluated on\nvarious reasoning tasks. It should be noted that there is\na hidden dimension, the scoring function, comprising\nfour components. This results in a comprehensive total\nof 6,156 evaluations.\nfollow instructions, such as OPT-IML (Iyer et al.,\n2022) and Alpaca (Taori et al., 2023). Despite\nthe remarkable progress in LLMs\u2019 performance\nin Natural Language Processing (NLP) tasks, rea-\nsoning remains a challenging area. For example,\nprior work have shown that LLMs struggle with\ncommonsense reasoning (West et al., 2022) and\narithmetic reasoning (Hendrycks et al., 2021) to\nname a few.\nRecent efforts have attempted to improve the\nreasoning performance of LLMs by decomposing\nanswers into step-by-step reasoning chains using in-\ncontext learning (Wei et al., 2022b; Kojima et al.,\n2022) or during finetuning (Chung et al., 2022;\nWei et al., 2021a). While these approaches have\nshown some improvement on benchmarks such as\nGSM8K (Cobbe et al., 2021), it is not clear how\nthose explanations affect finetuning, prompting, or\narXiv:2305.12001v2  [cs.CL]  24 Oct 2023\n{Task Definition} \nProvide your answer followed by a brief reasoning.\n{In-Context Examples}\nInput: {input}\nOptions: {options}\nOutput: The answer is {answer} because {explanation}\nFigure 2: Template used during both training and infer-\nence. The model is tasked with predicting the answer\nfollowed by the explanation.\ntheir combination. Concurrent work has investi-\ngated the generalization capability of such models\nto reasoning skills beyond those encountered dur-\ning finetuning (Yu et al., 2022), but a comprehen-\nsive evaluation of the role of explanation during\nfinetuning and prompting with respect to reasoning\nskills is still lacking.\nIn this paper, we aim to address this gap. We in-\nvestigate OPT (Zhang et al., 2022) as a representa-\ntive of such models and utilize it as our base model.\nThrough finetuning OPT on a collection of care-\nfully curated open-source reasoning datasets that\ncome with explanations for each instance, we eval-\nuate its performance on 57 tasks drawn from the\nSUPER-NATURALINSTRUCTIONS benchmark (Wang\net al., 2022), covering 26 different reasoning skills.\nOur experiments are structured around three key\ndimensions: finetuning, prompting, and scale, each\nof which is comprised of three distinct components\n(See Figure 1). Finetuning: (1) a (vanilla) un-\nfinetuned OPT model; (2) A finetuned OPT model\nwithout explanations (OPT-R); and, (3) A finetuned\nOPT model with explanations (OPT-RE). Prompt-\ning: (1) zero-shot prompting; (2) Fewshot prompt-\ning without explanations; and, (3) Fewshot prompt-\ning with explanations. Finally, Scale: (1) 1.3B; (2)\n6.7B; and, (3) 13B. Accordingly, we create grid\nof 27 different components, providing a detailed\nanalysis measuring the impact of explanations dur-\ning finetuning and inference across different model\nscales.\nOur findings reveals that finetuning on reason-\ning datasets leads to statistically significant im-\nprovements in seven reasoning skills, including\nNumerical, Analogical and Reasoning on Objects,\nwith Physical, Counting and Textual Entailment\nshowing a significant effect only for the OPT-RE\nmodel, across both fewshot prompting conditions\nand model sizes, as compared to the vanilla OPT\nmodel (see Table 2). However, we also find that\nthis approach significantly hinders the performance\nof three other reasoning skills (see Table 3). We\nalso investigate the impact of incorporating expla-\nnations during fewshot prompting and find that it\ndoes not have a significant impact on the perfor-\nmance of the finetuned models, as measured by the\nvariance in the difference between both prompting\nmethods across reasoning skills for each model.\nHowever, we notice that it has a more noticeable ef-\nfect on the performance of the vanilla OPT model,\nas shown in Table 5. Additionally, we observe\na consistent increase in the average performance\nacross all tasks from Fewshot to Fewshot-E, as well\nas from OPT to OPT-R to OPT-RE models, indi-\ncating that explanations do have a small effect on\nperformance during both finetuning and prompting.\nFinally, Table 4 presents a summary of the results,\nindicating which reasoning skills demonstrate im-\nprovement due to the incorporation of explanations\nduring either finetuning or prompting, which skills\nshow a negative effect, and which skills have negli-\ngible effects regarding explanations.\n2\nOPT-R: Finetuning on Reasoning Skills\n2.1\nReasoning Datasets with Explanations\nFigure 3: Number of samples in each dataset of the\ntraining corpus. Y-axis in log scale.\nThe finetuning corpus utilized to refine OPT is\ncomposed of various reasoning datasets, each of\nwhich includes a corresponding explanation or ra-\ntionale for the answer. These rationales may con-\nsist of a sequence of smaller steps (i.e. chain-of-\nthought) or a free-form text that elucidates the rea-\nsoning behind the answer. As shown in Figure 2,\nwe employ a uniform template for all tasks during\nthe training process. The input to the model begins\nwith a task definition, followed by an instruction to\nprovide an answer followed by a brief reasoning.\nNext, we extract two random in-context examples\nuniformly from the training set that remain con-\nstant throughout training for each instance. The\ninput for the current training instance is then pre-\nsented in a format specific to each task. The options\nfor the answer are then included in the input, but\nnot in the in-context examples (see Appendix A\nfor further details on task-specific definitions and\noptions). The options are pre-shuffled for each\ntraining instance. The model is finally provided\nwith the answer prefix, \"Output:\nThe answer\nis\", and is tasked to predict the answer, followed\nby an explanation if OPT-RE is being finetuned.\nSimilarly, the in-context examples only comprise\nan explanation when training OPT-RE.\nBelow is a brief description of each dataset used\nduring finetuning. See Figure 3 for the relative size\nof each dataset.\nAQUA-RAT\nThe Algebra Question Answering\nwith Rationales dataset (Ling et al., 2017) render-\ning the task of solving algebraic word problems\nmore feasible by dividing the problem into a series\nof smaller steps. They create a 100k-sample dataset\nthat contains questions, answers and rationales in\nnatural language and human-readable mathemati-\ncal expressions that can be used to derive the final\nanswer.\nCoQA\nThe Conversational Question Answering\ndataset Reddy et al. (2019). It consists of 127k\nquestions and answers, compiled from 8k conversa-\ntions about passages from seven different domains.\nGiven a passage that contains a conversation, the\nmodel is tasked with answering a question by high-\nlighting the corresponding evidence from the pas-\nsage.\nCoS-E\nThe Common Sense Explanations dataset\nRajani et al. (2019) to induce language models with\ncommonsense reasoning. In this dataset, the model\nis given a question and a set of choices and is tasked\nwith selecting one of the provided choices along\nwith providing an explanation in natural language\nas to why that choice is correct.\nECQA\nThe Explanations for Commonsense\nQuestion Answering dataset Aggarwal et al. (2021).\nIt is similar to CoS-E since it requires the model to\nchoose one of the provided options to answer the\ngiven question, and also provide an explanation.\nESNLI\nThe Stanford Natural Language Infer-\nence dataset with Explanations Camburu et al.\n(2018) to train models to provide interpretable and\nrobust explanations for their decisions. The authors\nextend the SNLI dataset (Bowman et al., 2015) with\nhuman-annotated explanations. Similar to any NLI\ntask, the model is given a premise and hypothesis\nand the task is to determine whether the hypothe-\nsis sentence entails, contradicts, or is neutral with\nrespect to the given premise.\nGSM8K\nThe Grade School Math dataset Cobbe\net al. (2021) to train models to better perform multi-\nstep mathematical reasoning. It consists of 8.5k\nlinguistically diverse grade school math word prob-\nlems. Therefore, the task for the model is to answer\nthe question by performing a series of arithmetic\noperations to obtain a final answer, while explain-\ning it\u2019s reasoning steps.\nProofWriter\nThe ProofWriter dataset Tafjord\net al. (2021) to generate both the implications of\na theory from the RuleTaker dataset (Clark et al.,\n2020) and the natural language proofs that support\nthem. Specifically, given a sequence of facts and\nrules, the model is tasked with answering a ques-\ntion using \u201cYes\u201d, \u201cNo\u201d, or \u201cUnknown\u201d and provide\nthe reasoning path by referring to the provided facts\nand rules. We consider the open-world assumption\nsubset of RuleTaker with questions that requires\nreasoning up to a depth of 5.\nStrategyQA\nThe Strategy Question Answering\ndataset Geva et al. (2021) to improve multi-hop rea-\nsoning for questions where the required reasoning\nsteps are implicit in the question. Therefore, the\ntask of the model is to answer the question using\n\u201cYes\u201d or \u201cNo\u201d then provide a strategy that explains\nthe answer by decomposing it into a number of\nsteps.\n2.2\nFinetuning Procedures\nOPT\nThe Open Pretrained Transformers (OPT)\nmodels are a suite of decoder-only pre-trained trans-\nformers ranging from 125M to 175B parameters\nreleased by Zhang et al. (2022). In this work, we\nuse three OPT models with sizes of 1.3B, 6.7B\nand 13B. The details of each model architecture,\npre-training corpus and training configuration (e.g.\nweight initialization, optimizer, tokenizer, hyperpa-\nrameters, etc.) can be found in Zhang et al. (2022).\nReasoning Skill\nTask IDs\nAbductive Reasoning\ntask854\nAnalogical Reasoning\ntask1287, task1288\nArgument Reasoning\ntask514\nCausal Reasoning\ntask1393\nCommonsense Reasoning\ntask279, task156, task295\nCommonsense Reasoning \u2192 Numerical Commonsense ...\ntask1403\nCommonsense Reasoning \u2192 Physical Reasoning\ntask084\nCommonsense Reasoning \u2192 Social Situations\ntask580, task937, task1606\nCommonsense Reasoning \u2192 Spatial Reasoning\ntask082, task083\nDeductive Reasoning\ntask221, task1568, task220\nEthics\ntask667, task724, task723\nGrammatical Reasoning\ntask1712, task052, task1559\nLogical Reasoning\ntask717, task211, task268\nLogical Reasoning \u2192 Reasoning with Symbols\ntask923, task935\nMathematics \u2192 Counting\ntask523, task155\nMultihop Reasoning\ntask1297, task056\nNumerical Reasoning\ntask621, task1333\nReasoning on Objects\ntask1583, task1584\nReasoning on Social Interactions\ntask609, task881, task875\nReasoning on Strings\ntask1189\nRelational Reasoning\ntask1380, task472, task1505\nScientific Reasoning\ntask1431, task228, task714\nTemporal Reasoning\ntask018, task1549, task383\nTextual Entailment\ntask738, task890, task463\nTextual Entailment \u2192 Analogical Reasoning\ntask1347\nTextual Entailment \u2192 Deductive Reasoning\ntask1612, task534, task1366\nTable 1: Evaluation tasks from SUP-NATINST (Wang et al., 2022) used for each reasoning skill.\nImplementation Details\nTo finetune the selected\nmodels, we utilized the metaseq1 implementation\nsince it enables higher training efficiency compared\nto other codebases (Zhang et al., 2022).\nEach\nmodel is finetuned twice for 10 epochs, once with\nexplanations and once without (i.e. OPT-RE vs\nOPT-R, respectively).\nModels are evaluated at\nthe end of each epoch on a chosen set of SUPER-\nNATURALINSTRUCTIONS validation tasks, and the\ncheckpoint with the best performance is selected\nfor evaluation on the testing tasks. The loss is\ncalculated only on the tokens the model is tasked\nto predict during inference, and not the full input,\nwhat is referred to as label-loss in (Iyer et al., 2022).\nThe samples across all datasets are shuffled during\ntraining. Further, the model is provided with two\nin-context examples during finetuning in addition\nto the task definition to match inference time fol-\nlowing (Wang et al., 2022).\n1https://github.com/facebookresearch/metaseq\n3\nEvaluating the Models\n3.1\nSUPER-NATURALINSTRUCTIONS Tasks\nIn this study, we focus on a subset of the SUPER-\nNATURALINSTRUCTIONS benchmark version 2.62\n(SUP-NATINST for short) proposed by Wang et al.\n(2022), which comprises 1,616 varied NLP tasks\nand includes meta-labels for each task, such as\ntask type, domain and more importantly for this\nwork: the underlying reasoning skills. Specifically,\nwe select a subset of tasks that satisfy two key\ncriteria: (i) the task focuses on a single reasoning\nskill, enabling us to evaluate a specific atomic skill,\nand (ii) the task can be tested using classification\nmode, as detailed in Section 3.2. Note that there is\nno data contamination between finetuning data and\nthe evaluation benchmark.\n2We downloaded the data from https://github.com/\nallenai/natural-instructions/tree/v2.6.\nFigure 4: Results achieved across all tasks as a function of the three primary dimensions analyzed in this study:\nFinetuning, Prompting and Scale.\nBenchmark Splits\nFollowing the task selection\nprocess, we apply a random sampling technique to\nensure diversity within the testing set. Specifically,\nwe select a maximum of three tasks from each rea-\nsoning skill, and allocate any remaining tasks to\nthe validation set. Notably, this approach enables\nus to obtain a representative sample of the selected\nreasoning skills for testing, while also ensuring\nthat our model\u2019s performance is not influenced by\na particular subset of tasks. Table 1 shows the com-\nplete list of tasks used for evaluating our finetuned\nmodels for each reasoning skill.\n3.2\nEvaluation Setup\nEarlier, we mentioned that we selected 57 tasks\nspanning 26 reasoning skills from SUP-NATINST to\nevaluate our finetuned models. To meet our criteria,\nas detailed in Section 3.1, each task had to fulfill\ntwo conditions. The second condition required\nthat the task can be considered a classification task.\nThat means there is a discrete set of candidates\n(one of which is correct) and thereby treating it as\na classification problem where the highest-scoring\ncandidate is considered the answer. To ensure this,\nwe utilized a straightforward heuristic: we only\nsampled tasks that had no more than 10 possible\ncandidate answers.\nClassification Method\nTo determine the correct\nanswer, we conduct a forward pass for each poten-\ntial candidate answer and utilize a scoring function\nto measure the likelihood that the candidate tokens\nfollows the input, similar to Brown et al. (2020).\nThis process is repeated four times using distinct\nscoring functions, as detailed in the subsequent\nparagraph. The highest accuracy score from the\nfour scoring functions is considered as result of the\ntask.\nScoring Functions\nThis is considered the fourth\ndimension of this work since we evaluate each task\nusing four different scoring functions and take the\nmaximum accuracy as the result. The four scoring\nfunctions used are as follows: (1) mean, which\ninvolves computing the average of the log probabil-\nities of candidate tokens, also referred to as token\nscore. (2) unconditional-norm, which computes\nthe difference between the sum of token scores of\nthe candidate when unconditioned by any previous\ntokens and the sum of candidate token scores when\nconditioned by previous input. (3) suffix, which\ncomputes the sum of the conditioned candidate\u2019s to-\nken scores alone. Finally, (4) sum, which involves\ncalculating the sum of all the token scores passed\nto the model. The reason we employed different\nfunctions is that we observed significant gains in\nperformance when using one scoring function over\nthe other for specific tasks. Therefore, in order\nto ensure fairness across all tasks, we selected the\nhighest accuracy over all scoring functions for each\ntask.\n4\nResults & Findings\nIn this section, we present the results and findings\nof our experiments. First, we illustrate in Figure\n4 the outcome of our evaluation on the effective-\nness of finetuned models as compared to the vanilla\nOPT model, across three different scales when us-\ning both fewshot prompting with and without ex-\nplanations. Furthermore, we observe a monotonic\nincrease in the performance of each model as we\nincrease the scale under those two prompting condi-\ntion, which indicates a positive correlation between\nthe model\u2019s capacity and its overall performance.\nHowever, we note that this trend does not apply\nto the zeroshot prompting method, since we are\ntesting out-of-distribution tasks and that the fine-\ntuned models were trained with fewshot exemplars\nin their context. This leads us to focus only on\nthe fewshot prompting methods, with and with-\nout explanations, for the remaining of our evalu-\nations. Specifically, we investigate the impact of\nfinetuning the OPT models on reasoning datasets,\nas compared to the vanilla OPT model, and explore\nthe effect of explanations during finetuning and\nprompting, both in terms of the reasoning skill.\n4.1\nModel Performance for Reasoning Skills\nThe results reported in this and the following sec-\ntion are the classification accuracy of each reason-\ning skill across different conditions, such as model\nsizes and fewshot prompting methods. Table 2\nshows the reasoning skills where either OPT-RE\nor OPT-R are significantly better than the vanilla\nOPT model, as measured by Welch\u2019s t-test, where\np < 0.05. Conversely, Table 3 show the reason-\ning skills where the vanilla OPT model performs\nsignificantly better than either of its finetuned coun-\nterparts.\nSkill\nOPT\nOPT-R\nOPT-RE\nNumerical\n44.8\n65.2*\n64.7*\nAnalogical\n49.0\n62.9*\n60.8*\nCounting\n19.8\n13.1\n31.3*\nPhysical\n38.2\n37.8\n49.1*\nEntailment\n42.6\n47.2\n51.6*\nSocial Int\n34.1\n43.0*\n40.1\nObjects\n54.3\n62.6*\n59.9*\nTable 2: Performance as a function of the reasoning\nskills where OPT-RE or OPT-R performs significantly\nbetter than the OPT model as measured by Welch\u2019s t-test\n(p < 0.05) denoted by the * symbol. The performance is\nmeasured across Fewshot and Fewshot-E prompting, the\nthree different scales and tasks under the corresponding\nreasoning skill. Best result indicated in bold.\nThe results reveal that the finetuned variants of\nthe OPT model demonstrate a significant improve-\nment on seven distinct reasoning skills, with par-\nticular emphasis on the Numerical and Analogical\nreasoning tasks. Specifically, for the Mathematical\nSkill\nOPT\nOPT-R\nOPT-RE\nArgument\n57.9\n46.1\u2212\n48.7\u2212\nTE - Deductive\n36.0\n29.0\u2212\n29.4\u2212\nCommonsense\n33.4\n29.7\n28.8\u2212\nTable 3: Performance as a function of the reasoning\nskill where OPT performs significantly better than either\nOPT-R or OPT-RE as measured by Welch\u2019s t-test (p <\n0.05) denoted by the \u2212 symbol. The performance is\nmeasured across Fewshot and Fewshot-E prompting, the\nthree different scales and tasks under the corresponding\nreasoning skill. TE is Textual Entailment.\nCounting skill, the OPT-RE variant outperforms\nboth the OPT-R and OPT models, underscoring\nthe criticality of incorporating explanations during\nthe finetuning process for mathematical datasets.\nLikewise, the Physical Reasoning tasks exhibit a\nsimilar trend. On the other hand, we can see that\nfor the Argument, Deductive Textual Entailment\nand Commonsense skills the non-finetuned version\noutperforms considerably.\n4.2\nFine-Grained Skill Analysis\nTable 4 shows the classification accuracy results\nobtained from the three models, in relation to the\nreasoning skill and few-shot prompting method\nused. The best accuracy value for each reasoning\nskill is indicated in bold, and the cells are shaded\nwith colors ranging from green to white to indicate\ntheir position in the accuracy spectrum of each rea-\nsoning skill. The skills with similar performance\nacross different models are assigned a lighter shade\nof green, indicating that their color spectrum ends\nearlier than that of other skills where the difference\nin performance between models is more significant.\nThe table is divided into four blocks to distinguish\neffects of finetuning and prompting methods on\nreasoning skills: the first block showcases skills\nwhere the finetuned (OPT-RE and OPT-R) mod-\nels outperform the vanilla OPT model, the second\nblock highlights skills where OPT-RE has better\naccuracy than other models therefore illustrating\nthe importance of finetuning on explanations on\nthose skills. The third block displays skills where\nOPT outperforms other models showing that fine-\ntuning actually hurts performance in this case, and\nthe fourth block identifies skills where the choice\nof model or prompting method has little impact on\nthe overall performance.\nOPT\nOPT-R\nOPT-RE\nSkill\nFewshot\nFewshot-E\nFewshot\nFewshot-E\nFewshot\nFewshot-E\nNumerical\n39.9\n49.7\n65.1\n65.3\n64.7\n64.8\nAnalogical\n51.9\n46.2\n63.3\n62.5\n60.7\n60.9\nObjects\n53.5\n55.1\n61.4\n63.8\n60.0\n59.7\nSocial Interactions\n33.6\n34.7\n43.8\n42.3\n40.2\n40.0\nTextual Entailment\n43.3\n42.0\n47.1\n47.3\n51.9\n51.2\nGrammatical\n54.4\n55.1\n61.2\n60.0\n62.0\n63.1\nMultihop\n36.6\n31.7\n38.9\n39.9\n39.5\n37.0\nSymbols\n44.2\n47.2\n51.7\n51.8\n51.9\n52.4\nSpatial\n44.1\n47.1\n49.8\n51.8\n49.6\n49.2\nSocial Situations\n46.3\n46.6\n53.2\n53.2\n51.9\n52.3\nCounting\n19.6\n20.0\n13.5\n12.7\n29.8\n32.9\nPhysical\n35.8\n40.6\n36.9\n38.8\n48.1\n50.0\nLogical\n31.7\n33.4\n33.7\n34.1\n36.9\n38.4\nTemporal\n50.7\n49.7\n43.4\n46.5\n48.5\n38.5\nArgument\n55.8\n60.1\n46.3\n45.9\n48.6\n48.8\nTE - Deductive\n33.7\n38.3\n27.9\n30.1\n29.0\n29.9\nRelational\n47.4\n51.1\n47.6\n47.9\n44.8\n44.6\nCommonsense\n35.0\n31.8\n29.8\n29.5\n28.5\n29.2\nTE - Analogical\n16.3\n18.7\n18.6\n20.7\n18.7\n18.1\nAbductive\n33.9\n36.1\n36.9\n34.4\n34.2\n35.3\nEthics\n26.8\n25.8\n26.5\n25.9\n26.2\n27.6\nDeductive\n39.4\n40.4\n39.4\n40.4\n40.0\n41.1\nCausal\n50.2\n50.6\n49.1\n48.9\n50.1\n50.5\nScientific\n23.4\n23.3\n24.3\n24.5\n25.0\n24.5\nNumerical Commonsense\n59.5\n59.2\n59.0\n59.0\n59.2\n59.4\nStrings\n60.7\n60.7\n61.1\n61.2\n60.7\n60.7\nTable 4: Classification accuracy results achieved by different models as a function of the reasoning skill and few-shot\nprompting method employed. The best accuracy obtained for each reasoning skill is highlighted in bold. The cells\nare shaded with colors ranging from green to white to indicate their position in the accuracy spectrum. Reasoning\nskills with smaller variance in achieved results are assigned a lighter shade of green to convey the extent of similarity\nbetween models. The first block highlights skills where the finetuned models perform notably better than the vanilla\nOPT. The second block emphasizes the skills where OPT-RE outperforms other models. In contrast, the third block\nshowcases the skills where OPT outperforms the other models. Lastly, the fourth block identifies skills where the\nchoice of model or prompting method has little impact on the overall performance.\nExplanations\u2019 Effect\nOne of the central ques-\ntions that we sought to investigate in this study\nis the extent to which explanations play a role in\nimproving the reasoning capabilities of OPT mod-\nels during finetuning and prompting. The results\npresented in Table 5 suggest that the presence or\nabsence of explanations in the fewshot examples\nemployed for prompting does not significantly im-\npact the performance of the model when the model\nis finetuned on reasoning datasets. Concretely, in\nTable 5, we present the variance of the absolute\naccuracy difference for each model across reason-\ning skills by excluding the Temporal skill, which\nwas identified as an outlier. Specifically, we com-\npute the difference between the two corresponding\ncolumns for each model in Table 4. These values\nprovide insights into the impact of including ex-\nplanations during prompting on the performance\nof the models. Our findings reveal that the differ-\nence is negligible for OPT-R and OPT-RE models,\nsuggesting that the choice of prompting method\ndoes not significantly affect the model\u2019s accuracy.\nHowever, for the vanilla OPT model, the differ-\nence is more substantial, emphasizing the impor-\ntance of employing explanations during fewshot\nprompting. However, the mean performance of\neach model across the distinct fewshot prompting\nmethods demonstrates a slight yet consistent in-\ncrease in classification accuracy, from Fewshot to\nFewshot-E (incorporating explanations), as well as\nfrom OPT to OPT-R to OPT-RE models showing\nthat explanations do have a small effect on perfor-\nmance during both finetuning and prompting.\nModel\nStd(|F-FE|)\nAvg(F)\nAvg(FE)\nOPT\n2.31\n40.68\n41.82\nOPT-R\n0.84\n43.44\n43.68\nOPT-RE\n0.78\n44.49\n44.86\nTable 5: The first column shows the variance of the\nabsolute difference in accuracy for each model across\ndifferent reasoning skills, when using Fewshot (F) and\nFewshot-E (FE) prompting methods. The second and\nthird columns show the average performance of each\nmodel across each prompting method. Results are ob-\ntained after dropping the outlier Temporal skill.\n5\nRelated Work\nReasoning LLMs\nLLMs have made significant\nadvancements in the field of NLP and related ar-\neas (Brown et al., 2020; Chowdhery et al., 2022;\nChung et al., 2022), especially with the advent of\nthe pre-train, prompt, and predict paradigm (Liu\net al., 2021). This paradigm has enabled these\nmodels to solve a multitude of tasks through in-\ncontext fewshot or zeroshot learning using instruc-\ntions (Wei et al., 2021b; Iyer et al., 2022). However,\ntheir reasoning abilities have been a subject of de-\nbate in recent literature (Huang and Chang, 2022;\nAlKhamissi et al., 2022). Several studies suggest\nthat increasing the size of an LM trained through\nthe same next-token prediction method can lead to\nthe emergence of complex behaviors (Wei et al.,\n2022a), including reasoning. For instance, some re-\nsearch has demonstrated that sufficiently large LMs\ncan use chain-of-thought prompting (Wei et al.,\n2022b) to simulate human-like reasoning. Other\nstudies have shown that the addition of a simple\nprompt, such as \"Let\u2019s think step-by-step\" (Ko-\njima et al., 2022) can elicit reasoning abilities in\nLLMs by generating explicit reasoning steps be-\nfore decoding the final answer. However, some\nresearchers contend that emulating the human rea-\nsoning thought process is distinct from claiming\nthat the model can truly reason (Wei et al., 2022b).\nFinetuned LLMs\nConcurrent studies have fine-\ntuned LLMs to follow instructions to improve their\ngeneralization ability to unseen tasks through zero\nand fewshot learning (Iyer et al., 2022; Chung et al.,\n2022). However, our approach differs in that we\nonly finetune on a selected number of open-source\ndatasets that provide explanations for each instance.\nThis enables us to focus on the importance of expla-\nnations during finetuning in the context of reason-\ning skills. While concurrent works, such as (Iyer\net al., 2022; Wang et al., 2022), have experimented\nwith different prompting methods during finetuning\nand inference, our study focuses primarily on eval-\nuating the reasoning ability of the finetuned models\nacross a set of reasoning skills. Other concurrent\nstudies have explored the impact of finetuning on a\nset of held-out reasoning tasks (Yu et al., 2022), but\ntheir evaluation approach, which involves generat-\ning answers, may be influenced by various factors\nsuch as decoding strategy, decoding parameters,\nand prompt templates. In contrast, we adopt a rank\nclassification approach similar to (Brown et al.,\n2020), which better captures the reasoning perfor-\nmance of the model being evaluated, in addition to\ncovering a larger number of reasoning skills and\ntasks.\n6\nConclusion\nIn this study, we investigated the impact of incorpo-\nrating explanations during finetuning and prompt-\ning on three different sizes of the OPT model.\nThrough a systematic and comprehensive evalu-\nation process that considered three key dimensions,\nwe found that while explanations did provide a\nsmall improvement in performance, the effect was\nnot significant when incorporated in the in-context\ndemonstrations during inference for the finetuned\nmodels. Additionally, our results showed that both\nfinetuned models exhibited significant improve-\nments in reasoning skills such as Numerical, Ana-\nlogical and Reasoning on Objects. Moreover, we\ndemonstrated that skills such as Physical, Count-\ning, and Textual Entailment benefited from incorpo-\nrating explanations during the finetuning process.\nOverall, our findings provide insights into the im-\npact of incorporating explanations on the reason-\ning capabilities of LLMs and offer guidance on\nwhich reasoning skills would benefit most from\nthe inclusion and exclusion of explanations during\nfinetuning and prompting.\nLimitations\nWhile our study provides valuable insights into\nthe impact of finetuning on reasoning performance\nand the role of explanations during finetuning and\nprompting with respect to various reasoning skills,\nthere are several limitations to our work. Firstly,\nwe only consider a single LLM, OPT, as our base\nmodel. Our results may not generalize to other\nLLMs with different architectures or pretraining\nobjectives. Secondly, we only use a limited set of\nreasoning datasets for finetuning due to the limited\navailability of open-source datasets with explana-\ntions. However, it is possible that our findings\nmay not hold for models finetuned on larger closed\ndatasets as usually seen in real-world scenarios.\nThirdly, our experiments only cover a limited range\nof model sizes due to limitations in computational\nbudget, therefore it is possible that our findings may\nnot hold for much larger models. Finally, we only\nconsider finetuning using fewshot prompting condi-\ntions in our experiments, and it is possible that our\nfindings may not hold for models finetuned with-\nout in-context exemplars. Overall, while our study\nprovides valuable insights into the impact of fine-\ntuning and explanations on reasoning performance,\nfurther research is needed to investigate these fac-\ntors across a broader range of models, datasets, and\nfinetuning strategies.\nEthics Statement\nThis work is based on analyzing and evaluating\nthe performance of LLMs on reasoning tasks using\nexisting public datasets. No personally identifiable\ninformation or sensitive data was collected or used\nin this research. We acknowledge the potential\nrisks of developing LLMs, including their potential\nimpact on spreading misinformation, generating\nunwanted content and the exacerbation of existing\nbiases in datasets. Our work aims to contribute to\nimproving the transparency and understanding of\nhow LLMs can be optimized for specific reasoning\nskills. We hope our findings will inspire further\nresearch on developing ethical and responsible ap-\nproaches for developing and deploying LLMs.\nReferences\nShourya Aggarwal, Divyanshu Mandowara, Vishwa-\njeet Agrawal, Dinesh Khandelwal, Parag Singla, and\nDinesh Garg. 2021.\nExplanations for Common-\nsenseQA: New Dataset and Models. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 3050\u20133065, Online.\nAssociation for Computational Linguistics.\nBadr AlKhamissi, Millicent Li, Asli Celikyilmaz,\nMona T. Diab, and Marjan Ghazvininejad. 2022.\nA review on language models as knowledge bases.\nArXiv, abs/2204.06031.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing (EMNLP).\nAssociation for Computational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877\u20131901.\nOana-Maria Camburu, Tim Rockt\u00e4schel, Thomas\nLukasiewicz, and Phil Blunsom. 2018. e-snli: Natu-\nral language inference with natural language expla-\nnations. In S. Bengio, H. Wallach, H. Larochelle,\nK. Grauman, N. Cesa-Bianchi, and R. Garnett, ed-\nitors, Advances in Neural Information Processing\nSystems 31, pages 9539\u20139549. Curran Associates,\nInc.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nPeter Clark, Oyvind Tafjord, and Kyle Richardson. 2020.\nTransformers as soft reasoners over language. In\nIJCAI.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nJacob Hilton, Reiichiro Nakano, Christopher Hesse,\nand John Schulman. 2021. Training verifiers to solve\nmath word problems. ArXiv, abs/2110.14168.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021. Did aristotle\nuse a laptop? a question answering benchmark with\nimplicit reasoning strategies. Transactions of the\nAssociation for Computational Linguistics, 9:346\u2013\n361.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. 2021. Measuring mathematical\nproblem solving with the math dataset. NeurIPS.\nJie Huang and Kevin Chen-Chuan Chang. 2022. To-\nwards reasoning in large language models: A survey.\nArXiv, abs/2212.10403.\nSrinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,\nTodor Mihaylov, D\u00e1niel Simig, Ping Yu, Kurt Shus-\nter, Tianlu Wang, Qing Liu, Punit Singh Koura, et al.\n2022.\nOpt-iml: Scaling language model instruc-\ntion meta learning through the lens of generalization.\narXiv preprint arXiv:2212.12017.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. arXiv preprint\narXiv:2205.11916.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-\nsom. 2017. Program induction by rationale genera-\ntion: Learning to solve and explain algebraic word\nproblems. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 158\u2013167, Vancouver,\nCanada. Association for Computational Linguistics.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys, 55:1 \u2013 35.\nNazneen Rajani, Bryan McCann, Caiming Xiong, and\nRichard Socher. 2019. Explain yourself! leveraging\nlanguage models for commonsense reasoning. In\nACL.\nSiva Reddy, Danqi Chen, and Christopher D. Manning.\n2019. CoQA: A conversational question answering\nchallenge. Transactions of the Association for Com-\nputational Linguistics, 7:249\u2013266.\nOyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021.\nProofWriter: Generating implications, proofs, and\nabductive statements over natural language. In Find-\nings of the Association for Computational Linguis-\ntics: ACL-IJCNLP 2021, pages 3621\u20133634, Online.\nAssociation for Computational Linguistics.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model.\nhttps://\ngithub.com/tatsu-lab/stanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aur\u2019elien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. ArXiv,\nabs/2302.13971.\nYizhong Wang, Swaroop Mishra, Pegah Alipoormo-\nlabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva\nNaik, Arjun Ashok, Arut Selvan Dhanasekaran,\nAnjana Arunkumar, David Stap, Eshaan Pathak,\nGiannis Karamanolakis, Haizhi Lai, Ishan Puro-\nhit, Ishani Mondal, Jacob Anderson, Kirby Kuznia,\nKrima Doshi, Kuntal Kumar Pal, Maitreya Patel,\nMehrad Moradshahi, Mihir Parmar, Mirali Purohit,\nNeeraj Varshney, Phani Rohitha Kaza, Pulkit Verma,\nRavsehaj Singh Puri, Rushang Karia, Savan Doshi,\nShailaja Keyur Sampat, Siddhartha Mishra, Sujan\nReddy A, Sumanta Patro, Tanay Dixit, and Xudong\nShen. 2022. Super-NaturalInstructions: Generaliza-\ntion via declarative instructions on 1600+ NLP tasks.\nIn Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n5085\u20135109, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021a. Finetuned lan-\nguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021b. Finetuned lan-\nguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raf-\nfel, Barret Zoph, Sebastian Borgeaud, Dani Yo-\ngatama, Maarten Bosma, Denny Zhou, Donald Met-\nzler, Ed Huai hsin Chi, Tatsunori Hashimoto, Oriol\nVinyals, Percy Liang, Jeff Dean, and William Fedus.\n2022a. Emergent abilities of large language models.\nArXiv, abs/2206.07682.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nPeter West, Chandra Bhagavatula, Jack Hessel, Jena D.\nHwang, Liwei Jiang, Ronan Le Bras, Ximing Lu,\nSean Welleck, and Yejin Choi. 2022.\nSymbolic\nknowledge distillation: from general language mod-\nels to commonsense models.\nPing Yu, Tianlu Wang, O. Yu. Golovneva, Badr\nAlKhamissi, Siddharth Verma, Zhijing Jin, Gargi\nGhosh, Mona Diab, and Asli Celikyilmaz. 2022.\nAlert: Adapting language models to reasoning tasks.\nArXiv, abs/2212.08286.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nDataset\nTask Definition\nOptions\nAQuA\nYou are given an algebraic word question. Questions in this task often\nrequires executing a series of arithmetic operations to obtain a final answer.\nYou are also given 5 answer options (associated with \u2019A\u2019, \u2019B, \u2019C\u2019, \u2019D\u2019, \u2019E\u2019).\nDo not generate anything else apart from one of the following characters:\n\"A\", \"B\", \"C\", \"D\", \"E\" and the corresponding explanation.\n-A\n-B\n-C\n-D\n-E\nCoQA\nYou are given a passage that contains a conversation and a question. The\ntask is to answer the question and provide an explanation that highlights\nthe corresponding evidence in the passage.\nFree-form text\nCoS-E\nYou are given a passage that contains a sentence and a question. The task\nis to answer the question by selecting one of the provided choices.\nSelect one of\nthe provided choices\nECQA\nYou are given a question that requires commonsense reasoning. The task\nis to answer the question by selecting one of the provided choices.\nSelect one of\nthe provided choices\nESNLI\nYou will be presented with a premise and a hypothesis sentence. The\ntask is to determine whether the hypothesis sentence entails (implies),\ncontradicts (opposes), or is neutral with respect to the given premise\nsentence. Please answer with \"Contradiction\", \"Neutral\",or \"Entailment\".\n-Contradiction\n-Neutral\n-Entailment\nGSM8K\nYou will be presented with a passage that contains a grade school math\nword problem. The task is to answer the question by performing a series\nof arithmetic operations to obtain a final answer.\nNumber\nProofWriter You are given a sequence of facts and rules followed by a question. The\ntask is to answer the question using \"Yes\", \"No\" or \"Unknown\".\n-Yes\n-No\n-Unknown\nStrategyQA\nYou are given a sentence and a question. The required reasoning steps are\nimplicit in the question. The task is to answer the question using \"Yes\" or\n\"No\" then provide a strategy that explains the answer by decomposing it\ninto a number of steps.\n-Yes\n-No\nTable 6: Task definition and options used for each of the finetuning reasoning datasets.\nA\nFinetuning Task Definition and Options\nTable 6 shows the task definition and options pro-\nvided as input to the template shown in Figure 2\nduring finetuning the OPT models on the reasoning\ndatasets.\n"
  },
  {
    "title": "XTREME-UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages",
    "link": "https://arxiv.org/pdf/2305.11938.pdf",
    "upvote": "1",
    "text": "XTREME-UP: A User-Centric Scarce-Data Benchmark\nfor Under-Represented Languages\nSebastian Ruder\u2217 Jonathan H. Clark\u2217\nAlexander Gutkin\nMihir Kale\nMin Ma\nMassimo Nicosia\nShruti Rijhwani\nParker Riley\nJean-Michel A. Sarr\nXinyi Wang\nJohn Wieting\nNitish Gupta\nAnna Katanova\nChristo Kirov\nDana L. Dickinson\nBrian Roark\nBidisha Samanta\nConnie Tao\nDavid I. Adelani\u2020\nVera Axelrod\nIsaac Caswell\nColin Cherry\nDan Garrette\nReeve Ingle\nMelvin Johnson\nDmitry Panteleev\nPartha Talukdar\nGoogle\n\u2020University College London\nAbstract\nData scarcity is a crucial issue for the develop-\nment of highly multilingual NLP systems. Yet\nfor many under-represented languages (ULs)\u2014\nlanguages for which NLP research is particu-\nlarly far behind in meeting user needs\u2014it is\nfeasible to annotate small amounts of data. Mo-\ntivated by this, we propose XTREME-UP, a\nbenchmark defined by: its focus on the scarce-\ndata scenario rather than zero-shot; its focus on\nuser-centric tasks\u2014tasks with broad adoption\nby speakers of high-resource languages; and its\nfocus on under-represented languages where\nthis scarce-data scenario tends to be most real-\nistic. XTREME-UP evaluates the capabilities of\nlanguage models across 88 under-represented\nlanguages over 9 key user-centric technologies\nincluding ASR, OCR, MT, and information ac-\ncess tasks that are of general utility. We create\nnew datasets for OCR, autocomplete, semantic\nparsing, and transliteration, and build on and re-\nfine existing datasets for other tasks. XTREME-\nUP provides methodology for evaluating many\nmodeling scenarios including text-only, multi-\nmodal (vision, audio, and text), supervised pa-\nrameter tuning, and in-context learning.1 We\nevaluate commonly used models on the bench-\nmark. We release all code and scripts to train\nand evaluate models.2\n1\nIntroduction\nThe development of natural language processing\n(NLP) technology that serves most of world\u2019s lan-\nguages is hindered by the stark lack of data for\nmost languages (Joshi et al., 2020). While there is\nincreasing interest in developing datasets and mod-\nels for under-represented languages (ULs), existing\n\u2217Equal contribution. We list detailed contributions in \u00a77.\n1While XTREME-UP supports in-context learning, our re-\nsults indicate that few-shot in-context learning is less effective\nthan fine-tuning on 100s of examples for ULs. We advocate\nfor comparing such approaches directly as the community\nexplores XTREME-UP.\n2https://github.com/google-research/xtreme-up\ndatasets are often informed by established research\ndirections in the NLP community (de Marneffe\net al., 2021). While linguistic tasks such as syn-\ntactic parsing have become less practically rele-\nvant (Glava\u0161 and Vuli\u00b4c, 2021), other tasks such as\nnews summarization or sentiment analysis are in-\nformed by the availability of data in high-resource\nlanguage settings and may be less useful for speak-\ners of ULs (Varab and Schluter, 2021; Muhammad\net al., 2022). Impactful capabilities such as ques-\ntion answering or virtual assistants (Asai et al.,\n2021), on the other hand, often depend on ancillary\ntechnologies such as language ID, data filtering, au-\ntomatic speech recognition (ASR), or optical char-\nacter recognition (OCR) that are typically under-\nperforming or unavailable for ULs (Caswell et al.,\n2020; Bapna et al., 2022; Kreutzer et al., 2022; Ri-\njhwani et al., 2021; Khare et al., 2021). As a result,\nspeakers of ULs will not be able to reap the bene-\nfits of such capabilities, even if the development of\nmodels is successful.\nIn order to make progress on NLP for ULs, we\nshould thus focus on building datasets and evaluat-\ning models on tasks that are most likely to benefit\nspeakers of those languages.3 To this end, we pro-\npose XTREME-UP (Under-Represented and User-\nCentric with Paucal4 Data), a benchmark focus-\ning on evaluation of multilingual models on user-\ncentric tasks in a few-shot setting.\nWe focus on tasks that technology users en-\ncounter regularly in their daily lives: i) information\naccess tasks, which represent generally useful NLP\ncapabilities; and ii) input/output tasks that enable\n3Speakers of ULs have many different needs ranging from\nstandard NLP technology to language documentation and re-\nvitalization (Bird, 2022). Our focus is on standardized, in-\nstitutional, and contact languages including dialects and non-\nstandard language varieties spoken by large speaker popula-\ntions.\n4We borrow the term paucal\u2014meaning few\u2014from linguis-\ntics, to emphasize the scarce-data nature of XTREME-UP.\narXiv:2305.11938v2  [cs.CL]  24 May 2023\nFigure 1: The tasks in XTREME-UP and their role in language technology. Left: enabling access to language\ntechnology; middle: facilitating information access as part of larger systems (question answering, information\nextraction, virtual assistants); right: making information accessible in the speaker\u2019s language.\nother technologies. We show the corresponding\ntasks and their role in typical interactions with lan-\nguage technology in Figure 1. Moving away from\nthe standard cross-lingual zero-shot setting (Hu\net al., 2020; Ruder et al., 2021), we introduce a\nstandardized multilingual in-language fine-tuning\nsetting based on the amount of data that can real-\nistically be annotated or generated within 8h for a\nlanguage.\nOur results highlight the limitations of current\nmodels on ULs, demonstrate the potential of lan-\nguage models (LMs) to improve user-centric ap-\nplications, and show the benefit of byte-based ap-\nproaches, among other findings.\nIn this work, we contribute the first massively-\nmultilingual few-example benchmark including: a)\nnewly created data for QA, OCR, autocomplete, se-\nmantic parsing, and sentence-level transliteration;\nb) new task setups for named entity recognition\n(NER) enabling evaluation on natural\u2014rather than\ntokenized\u2014text; and for QA and retrieval provid-\ning a more interesting setting than the gold passage\n(GoldP) setup while offering a lower barrier-to-\nentry than the full TyDi QA Clark et al. (2020)\nor XOR (Asai et al., 2021) tasks; c) carefully-\ndesigned experimental setups, standardizing in-\nlanguage fine-tuning and in-context learning and fo-\ncusing on the information access scenario for ULs\nfor ASR and MT; d) baseline results for all datasets\non commonly-used subword and byte-based mod-\nels.\n2\nRelated Work\nMultilingual benchmarks\nSome studies employ\nhighly multilingual individual datasets for the eval-\nuation of multilingual models, including Universal\nDependencies (de Marneffe et al., 2021) or XL-\nSum (Hasan et al., 2021). At the same time, there is\nincreasing work on datasets in ULs for a variety of\napplications (Niyongabo et al., 2020; Winata et al.,\n2023; Muhammad et al., 2023). Due to their rapidly\ngrowing capabilities, NLP models are increasingly\nevaluated on a suite of datasets. Existing multi-\ntask multilingual benchmarks such as XTREME\n(Hu et al., 2020), XGLUE (Liang et al., 2020),\nand XTREME-R (Ruder et al., 2021) cover 20\u2013\n50 mainly high-resource languages and prioritize\ntasks with available data, regardless of their utility\nto speakers. In contrast, XTREME-UP focuses on\nunder-represented languages and user-centric tasks,\ncreating new data for under-represented tasks and\nlanguages.\nMultilingual evaluation\nThe choice of the exper-\nimental setting and aggregation metric are impor-\ntant considerations in multilingual evaluation. Prior\nwork focused on zero-shot cross-lingual transfer\n(Hu et al., 2020), which\u2014despite being compelling\nfrom a scientific perspective (Artetxe et al., 2020)\u2014\nis less practically useful. While in-language fine-\ntuning has been explored before (Lauscher et al.,\n2020; Hedderich et al., 2020), XTREME-UP is the\nfirst to standardize the setting across tasks based\non realistic annotation costs. Different frameworks\naggregate performance in different ways across lan-\nguages. Blasi et al. (2022) assess the utility of a\ntask by weighting model performance based on the\nsize of the speaker population while Khanuja et al.\n(2023) introduce the Gini coefficient to quantify\nperformance disparity across languages. XTREME-\nUP opts for a simple average over ULs, emphasiz-\ning intuitiveness and accessibility of the results.\n3\nXTREME-UP\n3.1\nDesign Principles\nXTREME-UP is motivated by the following design\nprinciples:\nTask\nTrain\nsum over HL+ULs\nTrain\navg. per UL\nValidation\nsum across ULs\nTest\nsum across ULs\n# of ULs\n(# of HLs)\nMetric\nAnnotation Cost\nminutes/example\nInput & Output\nSpeech Recognition\n274,514\n2,647\n26,556\n60,118\n77(23) CER\n0.2\u2217\nDocument OCR\n60\n9\u22c6\n447\n452\n7(0)\nCER\n44.5\nAutocomplete\n44,554\n1,850\n13,080\n14,747\n20(2)\nAcc@3\n0.3\u2020\nTransliteration\n7,360\n120\n28,000\n28,000\n12(1)\nCER\n2.7\u2021\nMachine Translation\n19,877\n120\n34,860\n70,000\n70(23) ChrF\n4.0\nInformation\nAccess\nQA\n\u001c in-lang.\n59,559\n426\n3,656\n3,688\n6(3)\nSpan F1\n3.0\ncross-lang.\n22,544\n361\n8,199\n12,720\n21(6)\nSpan F1\n3.0\nRetrieval for QA\n\u001c in-lang.\n29,683\n320\n1,830\n1,846\n6(3)\nMRR\n3.0\ncross-lang.\n13,270\n265\n6,183\n10,704\n21(6)\nMRR\n3.0\nNER\n28,023\n1,401\n14,250\n18,192\n20(0)\nF1\n0.3\nSemantic Parsing\n6,373\n273\n2,533\n39,253\n9(11) EM\n2.0\nTable 1: The tasks in XTREME-UP. For each task, we show both the sum of training examples across all languages\u2014\nto give some insight into training scale\u2014and the average number of training examples for each under-represented\nlanguage\u2014to highlight the challenge of the scarce-data learning scenario. XTREME-UP does not limit supervised\ntraining data in high-resource languages (HLs) while each under-represented language (UL) has a maximum of 8\nhours of annotation effort in its training split; see last column for estimated annotation effort. We also show the sum\nof validation and test examples across ULs as XTREME-UP evaluates only on ULs.\n\u2217Average time for read speech. \u2020 Based on mean typing speed (Dhakal et al., 2018) and average sentence length in Universal\nDependencies (de Marneffe et al., 2021). \u2021 An annotated example can be used for both task directions. \u22c6 For document OCR,\neach example is a whole page; for example, 9 pages of training images per language corresponds to approximately 290 lines of\noutput text per language. We will expand the data as we obtain publishers\u2019 permission.\nUnder-represented languages\nWe follow the on-\ntology of Joshi et al. (2020) in defining ULs based\non available data. Specifically, we select languages\nin categories 1\u20133 (e.g., Amharic, Estonian, Kin-\nyarwanda) as under-represented, leaving categories\n4\u20135 as high-resource languages (e.g., English, Ger-\nman, Hindi). We focus on tasks with existing data\nin ULs and tasks where we can efficiently collect\nsuch data at scale (see Appendix A for an overview\nof ULs in XTREME-UP).\nUser-centric tasks\nWe focus on widely adopted\nuser-facing tasks benefiting speakers of high-\nresource languages. We further break these down\ninto two major groups: 1) input/output tasks; and\n2) information access tasks (see Figure 1).\nScarce data\nWe focus on a realistic scenario\nwhere a small amount of data is available in each\nUL. Mirroring reality, we do not restrict the amount\nof training data available in high-resource lan-\nguages, but rather provide only as many labeled\ntraining examples as can be annotated in a realistic\namount of time for ULs (see Section 3.2).\nEfficiency\nWe focus on massively multilingual\nevaluation settings that can still be run efficiently\nwith a modest amount of compute.\nText-centric, yet multi-modal\nWe focus on tasks\nthat can be tackled using textual data alone and pro-\nvide baseline systems that do so. We frame multi-\nmodal tasks (OCR and ASR) so that natively multi-\nmodal models can be evaluated fairly alongside\ntext-only models. We accomplish this by releasing\noriginal audio, image, and text model inputs while\nalso providing baseline system output that can be\nfed to second-stage text-only systems. We hope to\nsee fully multi-modal models take up this challenge\nover the coming years.\nWe provide an overview of the tasks in XTREME-\nUP in Table 1. We discuss motivation and high-\nlevel information in the next section and provide\nmore details for each task in Appendix B.\n3.2\nHow much data?\nTo ensure a realistic amount of training data, we\nlimit the training data in each task per language to\nthe number of examples that can be annotated in 8\nhours. We believe this reflects the real difficulty of\nannotating training and evaluation data for a very\nlarge number of languages. In this way, we design\nfor the task first and will let the research to develop\ntechnology that addresses these challenges follow.\nFor each task, we estimate how long it takes to\nannotate a single example for a trained annotator.5\nWe base our estimates on prior work and our own\nannotation efforts.6 We show the data annotation\ntime estimates in Table 1. For tasks with larger\ntraining datasets, we sub-sample the available data\n5For simplicity, we estimate the annotation time for label-\ning only, ignoring factors such as training annotators, data\nprocessing, data validation, interface design, etc. We note that\nunlabeled data may not be available for certain ULs (Nekoto\net al., 2020) and its creation may require tools such as key-\nboards, which may not be available in all languages.\n6For autocomplete, we calculate average writing time.\naccordingly. Table 1 shows the sub-sampled data\nsizes. We show an example instance of each task\nin Table 2.\n3.3\nInput / Output Tasks\nAutomatic speech recognition (ASR; B.1)\nThe\ngoal of ASR is to transcribe speech into human-\nreadable text. It thus serves as a fundamental step\nfor enabling natural language understanding appli-\ncations on speech input. In many scenarios, users\nmay strongly prefer to speak rather than type and\nso high-quality ASR is an enabling factor for such\nuser interactions. We employ the FLEURS dataset\n(Conneau et al., 2023) consisting of recordings in\n102 languages for sentences from FLORES-101\n(Goyal et al., 2022), which were translated from\nEnglish Wikipedia to 101 languages. We evaluate\non the under-represented portion of the data, which\ncovers 77 languages.\nOptical character recognition (OCR; B.2)\nOCR, the process of converting text from images\ninto machine-readable formats, is used in a wide\nrange of applications, from extracting language\ndata locked in paper books (Rijhwani et al., 2020)\nand imaging legal documents (Singh et al., 2012),\nto improving accessibility for people with low vi-\nsion or blindness (Mowar et al., 2022). It is espe-\ncially important for under-represented languages\nwhere both training data and content that users may\nwish to access may not be abundant as digital text\non the web. While most existing datasets focus\non higher-resourced languages (Nayef et al., 2017;\nRigaud et al., 2019), there has been recent interest\nin developing OCR for ULs. This includes the cre-\nation of a small dataset for endangered languages\n(Rijhwani et al., 2020) and a synthetic dataset for\n60 languages (Ignat et al., 2022).\nWe create a dataset that aims to fill the gaps and\naugment previous work in OCR for ULs, by fo-\ncusing on larger-scale, typologically-diverse, and\nuser-centric data. Our dataset contains transcrip-\ntions for books in seven languages: Amharic (am),\nBengali (bn), Kannada (kn), Myanmar (Burmese;\nmy), Sanksrit(sa), Sinhala (si), and Swahili (sw).\nThe books domain is the primary use-case for a\nlarge number of downstream users, but is one of the\nmost challenging for OCR models (Rigaud et al.,\n2019). The dataset consists of transcriptions of en-\ntire pages and thus enables leveraging the full con-\ntext understanding capabilities of large language\nmodels. To demonstrate these capabilities, we use\nthe approach of \u201cOCR post-correction\u201d: training\nlanguage models to correct recognition errors in\ntranscriptions from existing OCR systems (Ham-\nmarstr\u00f6m et al., 2017; Rijhwani et al., 2020).\nAutocomplete (B.3)\nAutocomplete (or predic-\ntive text), i.e., predicting the rest of a word a user\nis typing, is a useful technology that speeds up\nhuman-computer interaction (Anson et al., 2006).\nAs such, autocomplete has become a technology\nthat users have come to expect and rely on for in-\nput in high-resource languages. The standard next\nword prediction task (Sundermeyer et al., 2012)\ndoes not accurately reflect this practical setting as\nit relies on predicting entire units (words, subwords,\nor characters); similarly, perplexity-based evalua-\ntion makes comparisons across segmentations and\nlanguages difficult (Mielke, 2019) while ignoring\nimportant threshold effects associated with the typ-\nical top-k predictions in a user interface (Tam and\nWells, 2009).\nTo fill this gap, we introduce a new autocomplete\ntask that unifies character, subword, and token-\nlevel LM settings by focusing on a \u201cword\u201d as the\npredictive unit. Models are required to complete\nthe next word based on a left context of N words\nand an optional character n-gram prefix. We use ac-\ncuracy@3 for evaluation to reflect the requirement\nof displaying a limited number of candidates to the\nuser. We process high-quality natural language data\nfrom Universal Dependencies (de Marneffe et al.,\n2021), which we deduplicate against mC4 (Xue\net al., 2021), the most common multilingual pre-\ntraining corpus in order to test models predictive\nrather than memorization capabilities.\nTransliteration (B.4)\nTransliteration is the con-\nversion of text between writing systems (Wellisch,\n1978). Unlike translation, it does not change con-\ntent but only script. For example, the Hindi sen-\ntence v-t\u0000 nFlA h{ (\u201cthe thing is blue\u201d) might be\nwritten \u201cvastu neela hai\u201d in the Latin script (which\nis often called romanization).7 Transliteration is\nimportant because it allows users to type in their\npreferred script (e.g., Latin script) even if it is dif-\nferent than their preferred display script (e.g. De-\nvanagari) and is used internally by many machine\ntranslation systems to rewrite names from different\nscripts.\n7Informal romanization of this sort is very common, e.g.,\nin South Asia, where languages are sometimes written by dif-\nferent communities in both Perso-Arabic and Brahmic scripts.\nTask\nLanguage\nInput\nOutput\nSpeech Recognition\nSwahili\nmarekebisho au maombi yoyote laz-\nima yafuatwe kupitia wakala wa\nkusafiri kwanza na si moja kwa moja\nna hoteli\nDocument OCR\nBurmese\nAutocomplete\nNigerian\nPidgin\nmake I just dey go back to my papa hou\nhouse\nTransliteration\nMarathi\nsurguja bhagatale rahivahi.\n\u0938\u0941\u0930\u0917\u0941\u091c\u093e \u092d\u093e\u0917\u093e\u0924\u0932\u0947 \u0930\u0939\u093f\u0935\u093e\u0938\u0940\nMachine Translation\nXhosa\nIt was developed by John Smith in the\n1970s to help inexperienced folders or\nthose with limited motor skills.\nYeenziwa nguJohn Smith kwiminyaka\nyee-1970 ukunceda iifolda ezingena-\nmava okanye ezo zinobuchule bemoto\nobulinganiselweyo.\nIn-language Retrieval for QA\nTelugu\nIn-language QA\nTelugu\n(or \u201cNo Answer\u201d for some examples)\nCross-language Retrieval for QA\nOriya\nTitle: Satyavati\nContext:\nDaughter of the Chedi\nking, Vasu (also known as Uparichara\nVasu) and a cursed \u201capsara\u201d (celestial\nnymph) who was turned into a fish\ncalled Adrika, Satyavati was brought\nup as a commoner. . .\nCross-language QA\nOriya\nQuestion:\nTitle: Satyavati\nContext:\nDaughter of the Chedi\nking, Vasu (also known as Uparichara\nVasu) and a cursed \"apsara\" (celestial\nnymph) who was turned into a fish\ncalled Adrika, Satyavati was brought\nup as a commoner...\nUparichara Vasu\n(or \u201cNo Answer\u201d for some examples)\nNER\nWolof\nDafa di, nag, \u00cbrob rawatina Far\u00e3s,\nda\u00f1uy xeex ak a bunduxataal tuu-\ntaaf\u00f3oni waaso yi.\nLOC: \u00cbrob\nLOC: Far\u00e3s\nSemantic Parsing\nZulu\nIngabe ikhona imicimbi yasendaweni\neqhubekayo kuleli sonto\n[IN:GET_EVENT\n[SL:ATTRIBUTE_EVENT yasendaweni]\n[SL:DATE_TIME kuleli sonto]\n]\nTable 2: Examples of each task in XTREME-UP. The tasks are generally text-in, text-out with a few exceptions.\nOn the output side, autocomplete requires generating the top-3 outputs and retrieval outputs document identifiers\u2014\ncurrent systems tend to implement retrieval by mapping both inputs and candidate outputs to vector and performing\nnearest neighbor lookup. On the input side, speech recognition has audio input and document OCR has image\noutputs; our initial baseline systems use external systems to map this to text as a preprocessing step, though we\nhope to see multi-modal systems eliminate this step in the near future.\nWe extend the Dakshina dataset (Roark et al.,\n2020), which provides romanizations of Wikipedia\nsentences written in the native scripts of 12 South\nAsian languages. To this data, we added: a) ro-\nmanizations of native script Wikipedia for one\nnew language (Amharic); and b) transliteration to\na third script (Shahmukhi) for one already cov-\nered language (Punjabi). The resulting task covers\n13 languages from three language families. For\nall these languages transliteration occurs from the\nLatin script to the native script of the language, and\nvice versa and between Shahmukhi (Perso-Arabic),\nGurmukhi (Brahmic), and Latin for Punjabi, lead-\ning to a total of 30 transliteration directions.\nMachine translation (MT; App. B.5)\nMT is\nan important technology for users of ULs wish-\ning to read text written in a different language.\nHowever, most current approaches require large\namounts of parallel training data to achieve good\nperformance, which are often not available for ULs\n(Haddow et al., 2022). We focus on the information\ndissemination scenario where content from high-\nresource languages (including from tasks such as\ncross-lingual QA) is translated to enable informa-\ntion access by common users; as such, XTREME-\nUP includes translations from English into 93 lan-\nguages, covering a wide range of high-resource and\nUL languages. Only 39 ULs are used for evalua-\ntion; the high-resource languages are included to\nallow for transfer learning.8 The dataset is adapted\nfrom FLORES-101 (Goyal et al., 2022), repurpos-\ning half of the dataset\u2019s original development set as\na training set. See \u00a76 for a detailed discussion of\nhow we distinguish freely-available unsupervised\ndata versus purpose-annotated supervised data in\nXTREME-UP.\n3.4\nInformation Access Tasks\nQuestion Answering (B.6)\nQuestion answering\nis an important capability that enables responding\nto natural language questions with answers found\nin text (Kwiatkowski et al., 2019). We focus on the\ninformation-seeking scenario where questions are\nasked (and therefore written by dataset annotators)\nwithout knowing the answer\u2014it is the system\u2019s\njob to locate a suitable answer passage (if any);\nthis is in contrast to the school-like reading com-\nprehension scenario where questions are written\nwhile looking at text, which is guaranteed to con-\n8Our baseline results were trained only on the 39 UL pairs\nfor efficiency.\ntain the answer. Importantly, information-seeking\nquestion-answer pairs tend to exhibit less lexical\nand morphosyntactic overlap between the question\nand answer since they are written separately.\nWe include two variants of the task: in the in-\nlanguage QA task, both the question and passage\nare in the same language. In this task, original\nquestions and passages are from the TyDi QA\ndataset (Clark et al., 2020). In the cross-language\nQA task, the question is in the user\u2019s native lan-\nguage while the passage and answer are in a high-\nresource language having a large amount of avail-\nable answer content (English). For this task, we use\nexamples from TyDi XOR (Asai et al., 2021) in 7\nlanguages. We additionally collect new data in 23\nnew Indic languages for cross-lingual QA by pro-\nfessionally translating questions and answers from\nexisting Indic languages in XOR QA. This method-\nology mitigates the issue of translating Western-\ncentric English data to locales with different topical\ninterests. Cross-lingual QA is especially important\nfor ULs since they may lack plentiful in-language\nanswer content on the web.\nIn XTREME-UP\u2019s QA task, a system is given\na question, title, and a passage and must provide\nthe answer\u2014if any\u2014or otherwise return that the\nquestion has \u201cno answer\u201d in the passage.9 To this\nend, we generalize the gold passage (Clark et al.,\n2020) setting, augmenting it with negative exam-\nples. These negatives are obtained from (a) pas-\nsages within the same article as a passage contain-\ning the answer and (b) question-answer pairs from\nthe full TyDi QA dataset where no answer was\nfound in the candidate Wikipedia article. The data\nis split into training, validation, and test splits in\nsuch a way to avoid deduplication and overlap of\nsplits, even across our various QA tasks.10\nRetrieval for QA (B.6)\nWithin the information-\nseeking QA scenario, the above core QA task as-\nsumes answer candidate passages as an input. In\npractice, a passage retrieval system for question-\nanswering allows for the extraction of relevant text\nfrom a vast text corpus. The retrieved passages can\nthen be used by a question-answering system to\nextract or generate an answer to the user\u2019s ques-\ntion. In XTREME-UP, we separate retrieval into\n9This format follows the SQuAD v2 setup (Rajpurkar et al.,\n2018).\n10This turns out to be non-trivial given the different splits\nstrategies across the various datasets and our decision to create\na train, validation, and test set even where only a train and\nvalidation set were previously available for public download.\ntwo distinct tasks, in-language retrieval and cross-\nlanguage retrieval. For in-language retrieval, both\nthe questions and passages are in the same lan-\nguage. The preparation of negatives, deduplication,\nand splits are identical to the QA task above. For\nvalidation and test, we create an index of 271k in-\nlanguage passages (447k English passages for the\ncross-language task) making for a small enough in-\ndex for efficient experimentation, while containing\ndistractors that make for a challenging task, since\nthese distractors are drawn from the same articles\ncontaining the target passages.\nNamed entity recognition (NER; B.7)\nNER is\nan important capability for information access sys-\ntems that users depend on with applications rang-\ning from recognizing requests for entity lookups\nto performing information extraction to populate\nthe knowledge graphs that handle those requests.\nNER is also a capability needed in spell-checking\nand localization systems (Li et al., 2020).11 Iden-\ntifying entities in ULs poses challenges due to\nthe use of different scripts, lack of capitaliza-\ntion, different numerical representations, etc. We\nbuild on MasakhaNER (Adelani et al., 2021) and\nMasakhaNER 2.0 (Adelani et al., 2022), two large\nNER datasets in African languages, which pro-\nvide data in the standard CoNLL tokenized for-\nmat (Tjong Kim Sang and De Meulder, 2003).\nIn order to enable evaluation in a setting that is\ncloser to the real world, we automatically map\nthe annotated spans to the original raw text. The\ncombined data with byte-level span annotations\u2014\ntermed MasakhaNER-X\u2014covers 20 languages.12\nSemantic parsing (App. B.8)\nSemantic parsing\nis the task of mapping a natural language utterance\nto a logical form or a structured interpretation that\ncan be executed by a system such as a virtual assis-\ntant. For example a user utterance can be classified\ninto an intent and parsed into slots: \u201cwake me at 8\nam\u201d would be mapped to the \u201cCreateAlarm\u201d intent\nand would have a single \u201ctime\u201d slot with \u201c8 am\u201d as\nvalue. Then the assistant may use this interpreta-\ntion to create an alarm at the specified time. While\nmodern models are becoming very capable of re-\nsponding to users\u2019 language inputs, we believe this\n11We emphasize the word capability here since we recog-\nnize that stand-alone NER systems may not be strictly neces-\nsary in the long run; however, the capability of recognizing\nand properly handling entities will remain.\n12We remove the Fon and Hausa subsets of MasakhaNER\n2.0 due to quality issues in the annotated data.\ntask is especially timely as users will increasingly\nwant to turn their interactions with assistants and\nchat-like dialog systems into actions on external\nsystems, which require API calls; this capability is\nwhat the semantic parsing task evaluates.\nRecently, researchers published more multilin-\ngual semantic parsing datasets that focus on virtual\nassistant domains (Li et al., 2021; FitzGerald et al.,\n2022; Moghe et al., 2022; Goel et al., 2023). We\nextend a portion of an existing semantic parsing\ndataset to new languages targeting the following\nfeatures: a) high-quality utterances produced by\nprofessional translators; b) a wide range of domains\nand intents; c) inclusion of different language fami-\nlies and some underrepresented languages; d) sen-\ntences with culturally relevant entities; and e) code-\nmixed sentences, i.e., multiple language within the\nsame sentence\u2014a common phenomenon in multi-\nlingual societies.\nWe adapt the test split of MTOP13 (Li et al.,\n2021) with professional translators/annotators to\nthe following 15 languages: Amharic, Belarusian,\nBengali, Brazilian Portuguese, Finnish, German,\nHausa, Hungarian, Japanese, Russian, Swahili,\nTamil, Turkish, Yoruba, and Zulu. Together with\nthe original MTOP languages, the new MTOP++\ndataset covers a total of 20 languages. The data we\ncollect, differently from MTOP, is localized (i.e.,\nWestern-centric entities are replaced with more cul-\nturally relevant entities for the target language),\nfollowing recent trends in multilingual benchmark-\ning (Lin et al., 2021; Ding et al., 2022; Majewska\net al., 2023).\nWe also extend MTOP to three widely spoken\nbut under-represented Indic languages in a code-\nswitching setting: Hindi-English, Bengali-English\nand Tamil-English. We automatically convert the\ntest-split of MTOP to code-mixed utterances using\nPaLM (Chowdhery et al., 2022) and run human\nverification on such utterances.\n3.5\nOverall Evaluation\nFor each task, we evaluate model performance\nby computing a task-specific score. We employ\ncharacter-level metrics such as character error\nrate (CER) and character n-gram F-score (chrF;\nPopovi\u00b4c, 2015) rather than their word-level coun-\nterparts as they enable more fine-grained evaluation\nand are better suited to morphologically rich lan-\n13All the other datasets were not yet available at the start of\nthe project and annotation tasks. Still, such datasets are not\nfocused on ULs.\nguages. We obtain a final score by averaging the\nscores of all tasks. For each task, we only aver-\nage performance over ULs (discussed in \u00a73.1). For\nmetrics such as character error rate (CER) where\nlower is better, we invert the scores before averag-\ning scores across tasks. For mean reciprocal rank\n(MRR), which is in the 0.0\u20131.0 range, we renormal-\nize it to the 0\u2013100 range before averaging. While\nthis scalar provides a quick overall impression of\na system\u2019s quality across a broad range of tasks,\nit is not a substitute for analyzing performance on\nindividual tasks, languages, or types of examples.\n4\nExperiments\n4.1\nExperimental setting\nMultilingual fine-tuning\nIn contrast to prior\nbenchmarks that focus on zero-shot cross-lingual\ntransfer from English, XTREME-UP focuses on the\nmore realistic scenario of fine-tuning on a small\namount of data in the target language. To make\nthis scenario scalable in a massively multilingual\nsetting, XTREME-UP fine-tunes a single model on\nthe combined training data across the available lan-\nguages for each task. The data for each language is\nsub-sampled to emulate data sizes that can be real-\nistically annotated within a reasonable time frame\n(see \u00a73.2).\nIn-language in-context learning\nWe also pro-\nvide a 5-shot in-context learning setting where a\nmodel is provided with an English instruction and\n5 exemplars in the target language in order to eval-\nuate the progress on few-shot learning with large\nmodels for ULs. We provide the instruction for\neach task in Appendix C.14\n4.2\nBaselines\nWe provide results on a handful of baseline systems\nthat have already been developed by the research\ncommunity. Given that our focus in this paper is on\nthe dataset and task setup rather than system build-\ning, we do not focus on offering novel modeling\ntypes nor do we exhaustively evaluate all possible\nmodels; rather we view these results as estimat-\ning a starting point from some well-known model-\ning approaches and seeding contributions from the\n14The choice of prompt and exemplars can have a significant\nimpact on performance (Zhao et al., 2021a,b). We provide a\nsingle instruction and set of exemplars per task and language\nfor replicability and leave the search for better instructions\nand exemplars to future work.\nModel\nEval\n# of\nVocab\n% of non-en\nsetting\nparams\nunits\npre-train data\nmT5-Base\nFT\n580M\nSubwords\n94.3\nByT5-Base\nFT\n580M\nBytes\n94.3\nFlan-PaLM\nICL\n62B\nSubwords\n22.0\nTable 3: Additional information on baseline models\nincluding the setting in which we evaluate them (fine-\ntuning vs in-context learning), their size, their vocabu-\nlary, and the fraction of non-English pre-training data.\nbroader research community.15\nMultilingual fine-tuning baselines\nFor the main\nexperimental setting of multilingual fine-tuning, we\nprovide the following baselines: mT5-base (Xue\net al., 2021) and a subword-based multilingual\nencoder-decoder model; ByT5-base (Xue et al.,\n2022), a byte-based multilingual encoder-decoder\nmodel.\nIn-context learning baseline\nFor the in-context\nlearning setting, we employ Flan-PaLM (Chung\net al., 2022), an instruction-tuned version of PaLM\n(Chowdhery et al., 2022). We provide additional\ninformation on the baseline systems in Table 3.\nTo offer baseline systems that allow experimen-\ntation with text-only models, we use upstream mod-\nels to provide initial output for ASR and OCR, and\npresent text-based baselines that use these as in-\nputs. We expect these baselines to give way to fully\nmulti-modal models as research progresses. These\ninitial ASR and OCR outputs should be seen as part\nof a baseline system, not part of the XTREME-UP\nbenchmark iteself. For ASR, we augment the data\nwith predictions of the state-of-the-art Maestro-U\n(Chen et al., 2023) and then use a downstream text\nmodel to improve the outputs (Bassil and Alwani,\n2012). Similarly, for OCR, we use the off-the-shelf\nGoogle Vision OCR16 to get first-pass outputs, and\ntrain language models to improve them (Dong and\nSmith, 2018; Rijhwani et al., 2020).\nInfrastructure\nModels were trained using seqio\nand T5X (Roberts et al., 2022) on TPUs (Kumar\net al., 2019; Pope et al., 2022).\n15XTREME-UP offers a public results tracker for use in\ntracking the community\u2019s progress on XTREME-UP. We con-\nceptualize these results not as a competition, but as offering\ninsights about different models and their trade-offs, each justi-\nfying and explaining how it should be compared to the others\nand how it informs the research landscape. Submissions can\nbe made via self-service git pull requests.\n16https://cloud.google.com/vision/docs/ocr\nInput & Output Tasks\nInformation Access Tasks\nASR\nOCR\nAutocomplete\nTransliteration\nMT\nQA\nRetrieval\nNER\nSemantic\nParsing\nAvg\nCER\u2193\nCER\u2193\nAcc@3\u2191\nCER\u2193\nchrF\u2191\nF1\u2191\nMRR\u2191\nF1\u2191\nEM\u2191\nMultilingual fine-tuning\nmT5-Base\n8.5\n(11.1)\u22c6\n12.7\n37.6\n22.5\n59.7 (74.9 / 44.6)\n0.23 (0.41 / 0.07)\n74.0\n21.8\nByT5-Base\n8.2\n(11.1)\u22c6\n27.6\n14.6\n26.9\n71.4 (82.3 / 60.5)\n0.29 (0.45 / 0.18)\n84.0\n37.5\nIn-context learning (5-shot)\nFlan-PaLM-62B\n23.2\n\u2014\n0.0 \u2020\n77.4\n32.1\n22.9 (20.9 / 24.9)\n\u2014\n12.9\n0.1\nTable 4: Overall results of baselines across all XTREME-UP v1.0 tasks for the test split. Scores on XTREME-UP\naverage over evaluation scores of under-represented languages. QA and retrieval performance is the average of\nin-language and cross-language settings (indicated in brackets as in-language / cross-language). For OCR, we\ndo not apply any additional models (mT5 nor ByT5) on top of the baseline OCR system; we show these results\nin parentheses. We do not attempt in-context learning (ICL) results for retrieval since ICL is typically only used\nfor text-in, text-out use cases. \u22c6 For OCR, we use the Google OCR API. \u2020 For autocomplete, while we observe\nreasonable performance on English completions, we find that the model typically does a very poor job outside of\nEnglish.\n4.3\nResults\nWe show the baseline results in Table 4.17\nByte-based models outperform subword-based\non ULs.\nThe byte-based ByT5 outperforms the\nsubword-based mT5 across most tasks. Gains are\nparticularly pronounced for tasks that require deal-\ning with information on the character level such\nas autocomplete and transliteration and for predict-\ning information on the word level such as for NER\nand semantic parsing. These results demonstrate\nthat as we train and evaluate our models on under-\nrepresented languages, standard modeling choices\nsuch as subword representations fall short.\nIn-context learning underperforms fine-tuning\non limited data.\nThe Flan-PaLM model gener-\nally performs worse than the models using fine-\ntuning, despite being much larger. Nevertheless, it\nachieves reasonable performance on machine trans-\nlation, which is likely reflected in the pre-training\ndata. On other tasks, however, it fails to reliably\napply its English-centric knowledge to ULs. De-\nspite fine-tuned models performing relatively well\non NER, the in-context learning model is unable to\nconsistently generalize to the task in a few-shot set-\nting in under-represented languages. On semantic\nparsing, the model fails to generalize to the large\nnumber of domain-specific intents and slots using\nstandard prompting in ULs.18 The autocomplete\n17Detailed per-language results are available at https://\ngithub.com/google-research/xtreme-up.\n18We leave the exploration of multilingual adaptive prompt-\ning and dynamic exemplar selection (Drozdov et al., 2023)\nmethods to future work.\ntasks in particular demonstrate the lack of robust\ncross-lingual information in the English-centric\nPaLM model: it struggles to complete a sentence\ngiven a character prefix and fails to reliably con-\nvert between different scripts in the same language.\nXTREME-UP thus provides a strong challenge to\ntest the generalization abilities of in-context learn-\ning methods to ULs.\nThere is a lot of headroom left to improve per-\nformance on ULs.\nOverall, across all tasks there\nis still a considerable amount of headroom left.\nFor ASR, OCR and transliteration, around 10% of\ncharacters are still incorrectly predicted. On auto-\ncomplete, models only make the correct prediction\nin about one fourth of all cases. For MT, on average\nonly about a third of n-grams in the hypothesis are\nalso present in the reference, and vice versa. For\nQA and retrieval, there are large performance dif-\nferences between in-language and cross-language\nsettings and much headroom still left. On NER,\nmodels perform relatively well but are still far from\nperfect performance on the task. Finally, on se-\nmantic parsing models are only able to produce the\ncorrect output in around a third of all cases.\n5\nAnalyses\nLowest-performing languages\nModels gener-\nally perform poorly on African languages. On\ntransliteration, models perform relatively worst\non the newly added Amharic language. On NER,\nwhich covers only African languages, performance\nis lowest for Amharic\u2014likely due to its differ-\nent script\u2014and the extremely under-represented\nGhom\u00e1l\u00e1\u2019. Similarly, translation models underper-\nform in Amharic and Yoruba. On ASR, the lowest-\nperforming languages are Yoruba but models also\nstruggle with other languages such as Gaelic, and\nmany South Asian languages such as Lao, Khmer,\nand Burmese.\nTask-specific observations\nByT5 provides the\nbest performance while the size of the model does\nnot seem to impact performance much. Several\naspects of the data lead to higher error rates in\ntransliteration: the model struggles with input in\nthe Perso-Arabic script and to produce output in\nLatin based on a different script. For autocomplete\n(see Appendix B.3), our analyses indicate that mod-\nels perform better on text that uses the Latin script.\n6\nRecommendations\nIn this section, we make recommendations to re-\nsearchers who plan to make use of this benchmark.\nUse of splits\nXTREME-UP offers a train, valida-\ntion, and test split for each task. We recommend\nusing the training split for learning the parameters\nof your model or as exemplars for in-context learn-\ning while iteratively checking your progress on the\nvalidation (i.e. development) split. The test split\nshould not be used for iterative evaluation of your\nmodels or other sorts of hill-climbing; instead, it\nshould be reserved for reporting your results and\ncomparing after you have finished development on\nyour models. Experiments that follow this custom-\nary scientific rigor should expect to show better\ngeneralization and less overfitting to the test split.\nUse of additional pre-training data\nOne po-\ntential confounder for results along different pre-\ntrained models is the variation in pre-training data;\nwhere this data overlaps with the targets (outputs)\nin XTREME-UP validation and test splits, results\ncan be artificially inflated, providing a sense that\nresults are better than they are in reality\u2014if the val-\nidation or test data leaked into the pre-training data\nvia contamination during large-scale data scraping,\nthen it\u2019s unlikely that the system would truly per-\nform as well for new unseen inputs. Therefore,\nwe recommend that when researchers modify the\npre-training data for a model, they explicitly re-\nport overlap (contamination) between the targets\nof the XTREME-UP validation/test splits and their\npre-training corpus.19\n19We recognize that this is a very large-scale undertaking,\nrequiring a fairly large amount of compute. As such, we\nUse of additional supervised data\nIt is entirely\npossible that the community will find creative ways\nto improve models based on supervised data not\nincluded with XTREME-UP. However, researchers\nshould bear in mind how this might affect the com-\nparability of their results with other models. The\nfollowing axes should be considered:\n1. Any additional data from high resource lan-\nguages is always allowed in the XTREME-UP\nsetting.\n2. Supervised data (e.g. parallel data for MT)\nharvested from the web, religious, books, and\nother opportunistic sources will typically be\nout-of-domain and is therefore admissible;\nconversely, supervised data from ULs from\nhighly similar tasks or domains should gen-\nerally be considered against the spirit of the\nXTREME-UP benchmark.\n3. Monolingual data from UL is admissible with\nthe caveat that one should measure overlap\nwith targets, as discussed above.\nAvoid off-the-shelf MT systems\nData augmen-\ntation via automatically translating high-resource\nsupervised data to languages with less supervised\ndata has proven a very effective means of improv-\ning system quality. However, it is not necessar-\nily realistic to use a pre-existing MT system (e.g.\nan API or an open-source model) since those sys-\ntems have typically been trained on a large amount\nof parallel data\u2014or at least unknown data. This\nmeans that additional supervised data would then\nbe leaking into the experimental setup, which is\notherwise intended to reflect the reality that most\nunder-represented languages have very little super-\nvised data. If data augmentation via translation\nis used, we encourage researchers to report the\nparallel data sources used and argue why this ex-\nperimental setup is realistic\u2014or to clearly point out\nsuch usage in their experiments as an unavoidable\nconfound and discuss the limitations this sets on\nwhat conclusions can be drawn about how results\nwill extrapolate to the breadth of under-represented\nlanguages.\nsuggest that it\u2019s may only be needed when making claims\nthat compare systems (e.g. that the system with possibly-\ncontaminated pre-training data is equivalent, better, or almost\nas good as some other system). Note, this analysis only needs\nto be done once for each pre-training corpus (e.g., once for\nmC4) and it is very likely that organizations with enough\ncompute to pre-train a new model on a new corpus would also\nhave sufficient compute to calculate overlap.\nIn all cases, researchers should rigorously re-\nport what additional data was used and how; each\nuse case comes with its own considerations and,\nabove all, researches should make a well-reasoned\nargument that their use of data (i) does not artifi-\ncially inflate evaluation scores and (ii) reflects a\nreal-world scenario of finding and applying data.\n7\nConclusion\nWe have presented XTREME-UP, a multilingual\nbenchmark distinguished by its being (i) scarce-\ndata, (ii) user-centric, and (iii) focused on under-\nrepresented languages. The benchmark contains\ninput modalities of text, images, and audio while\nstill allowing experimentation with text-only mod-\nels. We hope this benchmark will be useful in\naccelerating research that is useful to speakers of\nunder-represented languages and in highlighting\nboth the progress and limitations of current models\nof language.\nAcknowledgements\nWe thank Slav Petrov, Jason Riesa, Raphael Hoff-\nmann, Dipanjan Das, Clara Rivera, Chris Alberti,\nMachel Reid, and Timothy Dozat for helpful discus-\nsions and feedback. We are grateful to Noah Con-\nstant for a review of a draft of the paper. We also\ngratefully acknowledge the contributions of the re-\nsearchers who built the datasets that have gone into\nXTREME-UP; we recommend that all component\ndatasets be cited individually when using XTREME-\nUP in a paper such that dataset authors (many of\nwhom are not authors of this article) receive credit\nfor their work and so that those original sources\nremain easily discoverable in the literature.\nContributions\nIn this section, we provide more detail about the contributions\nof each author.\nGeneral overview\nProject leads Sebastian Ruder, Jonathan Clark\nPrimary contributors and task owners Alexander Gutkin,\nMihir Kale, Min Ma, Massimo Nicosia, Shruti Rijhwani,\nParker Riley, Jean-Michel Sarr, Xinyi Wang, John Wieting\nMajor contributors Nitish Gupta, Anna Katanova, Christo\nKirov, Dana Dickinson, Brian Roark, Bidisha Samanta, Con-\nnie Tao\nSupporting contributors David Adelani, Vera Axelrod,\nIsaac Caswell, Colin Cherry, Dan Garrette, Reeve Ingle,\nMelvin Johnson, Dmitry Panteleev, Partha Talukdar\nBy Task\nASR Min Ma\nAutocomplete Jean-Michel Sarr, Vera Axelrod, Colin Cherry,\nSebastian Ruder, Jonathan Clark\nMT Parker Riley, Isaac Caswell, Colin Cherry, Jonathan\nClark\nNER Sebastian Ruder, David Adelani, Dan Garrette\nOCR Shruti Rijhwani, Dana Dickinson, Reeve Ingle, Dmitry\nPanteleev, Sebastian Ruder\nQA Mihir Kale, John Wieting, Nitish Gupta, Partha Talukdar,\nJonathan Clark\nRetrieval John Wieting\nSemantic parsing Massimo Nicosia,\nBidisha Samanta,\nPartha Talukdar\nTransliteration Alexander Gutkin, Anna Katanova, Christo\nKirov, Brian Roark\nBy Contribution\nEvaluation framework and public results tracker Xinyi\nWang\nProgram management Connie Tao\nFine tuning and modeling Alexander Gutkin, Mihir Kale,\nMin Ma, Massimo Nicosia, Shruti Rijhwani, Parker Riley,\nJean-Michel Sarr, John Wieting, Sebastian Ruder, Jonathan\nClark\nData processing Alexander Gutkin, Mihir Kale, Min Ma,\nMassimo Nicosia, Shruti Rijhwani, Parker Riley, Jean-Michel\nSarr, Xinyi Wang, John Wieting, David Adelani, Vera Axel-\nrod, Isaac Caswell, Colin Cherry, Dan Garrette, Reeve Ingle,\nDmitry Panteleev, Sebastian Ruder, Jonathan Clark\nData collection Massimo Nicosia, Bidisha Samanta, Nitish\nGupta, Anna Katanova, Christo Kirov, Dana Dickinson, Brian\nRoark\nIn-context learning Sebastian Ruder\nBenchmark design Jonathan\nClark,\nSebastian\nRuder,\nMelvin Johnson\nReferences\nDavid Adelani, Graham Neubig, Sebastian Ruder,\nShruti Rijhwani, Michael Beukman, Chester Palen-\nMichel, Constantine Lignos, Jesujoba Alabi, Sham-\nsuddeen Muhammad,\nPeter Nabende,\nCheikh\nM. Bamba Dione, Andiswa Bukula, Rooweither\nMabuya, Bonaventure F. P. Dossou, Blessing Sibanda,\nHappy Buzaaba, Jonathan Mukiibi, Godson Kalipe,\nDerguene Mbaye, Amelia Taylor, Fatoumata Ka-\nbore, Chris Chinenye Emezue, Anuoluwapo Aremu,\nPerez Ogayo, Catherine Gitau, Edwin Munkoh-\nBuabeng, Victoire Memdjokam Koagne, Allah-\nsera Auguste Tapo, Tebogo Macucwa, Vukosi Mari-\nvate, Mboning Tchiaze Elvis, Tajuddeen Gwad-\nabe, Tosin Adewumi, Orevaoghene Ahia, Joyce\nNakatumba-Nabende, Neo Lerato Mokono, Ig-\nnatius Ezeani, Chiamaka Chukwuneke, Mofetoluwa\nOluwaseun Adeyemi, Gilles Quentin Hacheme,\nIdris Abdulmumin, Odunayo Ogundepo, Oreen\nYousuf, Tatiana Moteu, and Dietrich Klakow. 2022.\nMasakhaNER 2.0: Africa-centric transfer learning\nfor named entity recognition.\nIn Proceedings of\nthe 2022 Conference on Empirical Methods in Nat-\nural Language Processing, pages 4488\u20134508, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nDavid Ifeoluwa Adelani, Jade Abbott, Graham Neu-\nbig, Daniel D\u2019souza, Julia Kreutzer, Constantine Lig-\nnos, Chester Palen-Michel, Happy Buzaaba, Shruti\nRijhwani, Sebastian Ruder, Stephen Mayhew, Is-\nrael Abebe Azime, Shamsuddeen H. Muhammad,\nChris Chinenye Emezue, Joyce Nakatumba-Nabende,\nPerez Ogayo, Aremu Anuoluwapo, Catherine Gitau,\nDerguene Mbaye, Jesujoba Alabi, Seid Muhie Yi-\nmam, Tajuddeen Rabiu Gwadabe, Ignatius Ezeani,\nRubungo Andre Niyongabo, Jonathan Mukiibi, Ver-\nrah Otiende, Iroro Orife, Davis David, Samba Ngom,\nTosin Adewumi, Paul Rayson, Mofetoluwa Adeyemi,\nGerald Muriuki, Emmanuel Anebi, Chiamaka Chuk-\nwuneke, Nkiruka Odu, Eric Peter Wairagala, Samuel\nOyerinde, Clemencia Siro, Tobius Saul Bateesa,\nTemilola Oloyede, Yvonne Wambui, Victor Akin-\node, Deborah Nabagereka, Maurice Katusiime, Ayo-\ndele Awokoya, Mouhamadane MBOUP, Dibora Ge-\nbreyohannes, Henok Tilaye, Kelechi Nwaike, De-\ngaga Wolde, Abdoulaye Faye, Blessing Sibanda, Ore-\nvaoghene Ahia, Bonaventure F. P. Dossou, Kelechi\nOgueji, Thierno Ibrahima DIOP, Abdoulaye Diallo,\nAdewale Akinfaderin, Tendai Marengereke, and Sa-\nlomey Osei. 2021.\nMasakhaNER: Named entity\nrecognition for African languages.\nTransactions\nof the Association for Computational Linguistics,\n9:1116\u20131131.\nDenis Anson, Penni Moist, Mary Przywara, Heather\nWells, Heather Saylor, and Hantz Maxime. 2006. The\neffects of word completion and word prediction on\ntyping rates using on-screen keyboards. Assistive\nTechnology, 18(2):146\u2013154.\nMikel Artetxe, Sebastian Ruder, Dani Yogatama, Gorka\nLabaka, and Eneko Agirre. 2020. A call for more\nrigor in unsupervised cross-lingual learning. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 7375\u2013\n7388, Online. Association for Computational Lin-\nguistics.\nAkari Asai, Jungo Kasai, Jonathan Clark, Kenton Lee,\nEunsol Choi, and Hannaneh Hajishirzi. 2021. XOR\nQA: Cross-lingual open-retrieval question answering.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 547\u2013564, Online. Association for Computa-\ntional Linguistics.\nAnkur Bapna, Isaac Caswell, Julia Kreutzer, Orhan Fi-\nrat, Daan van Esch, Aditya Siddhant, Mengmeng Niu,\nPallavi Baljekar, Xavier Garcia, Wolfgang Macherey,\nTheresa Breiner, Vera Axelrod, Jason Riesa, Yuan\nCao, Mia Xu Chen, Klaus Macherey, Maxim Krikun,\nPidong Wang, Alexander Gutkin, Apurva Shah, Yan-\nping Huang, Zhifeng Chen, Yonghui Wu, and Mac-\nduff Hughes. 2022. Building machine translation sys-\ntems for the next thousand languages. arXiv preprint\narXiv:2205.03983.\nYoussef\nBassil\nand\nMohammad\nAlwani.\n2012.\nPost-Editing\nError\nCorrection\nAlgorithm\nFor\nSpeech Recognition using Bing Spelling Suggestion.\n(IJACSA) International Journal of Advanced Com-\nputer Science and Applications, 3(2):95\u2013101.\nSteven Bird. 2022. Local languages, third spaces, and\nother high-resource scenarios. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n7817\u20137829, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nDamian Blasi, Antonios Anastasopoulos, and Gra-\nham Neubig. 2022. Systematic inequalities in lan-\nguage technology performance across the world\u2019s\nlanguages. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 5486\u20135505, Dublin,\nIreland. Association for Computational Linguistics.\nIsaac Caswell, Theresa Breiner, Daan van Esch, and\nAnkur Bapna. 2020. Language ID in the wild: Unex-\npected challenges on the path to a thousand-language\nweb text corpus. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics,\npages 6588\u20136608, Barcelona, Spain (Online). Inter-\nnational Committee on Computational Linguistics.\nZhehuai Chen, Ankur Bapna, Andrew Rosenberg,\nYu Zhang, Bhuvana Ramabhadran, Pedro Moreno,\nand Nanxin Chen. 2023. Maestro-U: Leveraging\njoint speech-text representation learning for zero su-\npervised speech ASR. In Proceedings of 2022 IEEE\nSpoken Language Technology Workshop (SLT), pages\n68\u201375, Doha, Qatar. IEEE.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick,\nAndrew M. Dai, Thanumalayan Sankaranarayana\nPillai, Marie Pellat, Aitor Lewkowycz, Erica Mor-\neira, Rewon Child, Oleksandr Polozov, Katherine\nLee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta,\nMark Diaz, Orhan Firat, Michele Catasta, Jason\nWei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean,\nSlav Petrov, and Noah Fiedel. 2022. PaLM: Scaling\nlanguage modeling with Pathways. arXiv preprint\narXiv:2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdh-\nery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,\nDasha Valter, Sharan Narang, Gaurav Mishra, Adams\nYu, Vincent Zhao, Yanping Huang, Andrew Dai,\nHongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-\ncob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le,\nand Jason Wei. 2022. Scaling instruction-finetuned\nlanguage models. arXiv preprint arXiv:2210.11416.\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan\nGarrette, Tom Kwiatkowski, Vitaly Nikolaev, and\nJennimaria Palomaki. 2020. TyDi QA: A benchmark\nfor information-seeking question answering in typo-\nlogically diverse languages. Transactions of the As-\nsociation for Computational Linguistics, 8:454\u2013470.\nAlexis Conneau, Min Ma, Simran Khanuja, Yu Zhang,\nVera Axelrod, Siddharth Dalmia, Jason Riesa, Clara\nRivera, and Ankur Bapna. 2023. FLEURS: Few-shot\nlearning evaluation of universal representations of\nspeech. In 2022 IEEE Spoken Language Technology\nWorkshop (SLT), pages 798\u2013805, Doha, Qatar. IEEE.\nMarie-Catherine de Marneffe, Christopher D. Man-\nning, Joakim Nivre, and Daniel Zeman. 2021. Uni-\nversal Dependencies.\nComputational Linguistics,\n47(2):255\u2013308.\nVivek Dhakal, Anna Maria Feit, Per Ola Kristensson,\nand Antti Oulasvirta. 2018. Observations on typ-\ning from 136 million keystrokes. In Proceedings\nof the 2018 CHI Conference on Human Factors in\nComputing Systems, pages 1\u201312, Montr\u00e9al, Canada.\nAssociation for Computing Machinery (ACM).\nBosheng Ding, Junjie Hu, Lidong Bing, Mahani Alju-\nnied, Shafiq Joty, Luo Si, and Chunyan Miao. 2022.\nGlobalWoZ: Globalizing MultiWoZ to develop mul-\ntilingual task-oriented dialogue systems. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 1639\u20131657, Dublin, Ireland. Association\nfor Computational Linguistics.\nRui Dong and David Smith. 2018. Multi-input attention\nfor unsupervised OCR correction. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 2363\u20132372, Melbourne, Australia. Association\nfor Computational Linguistics.\nAndrew Drozdov, Nathanael Sch\u00e4rli, Ekin Aky\u00fcrek,\nNathan Scales, Xinying Song, Xinyun Chen, Olivier\nBousquet, and Denny Zhou. 2023. Compositional\nsemantic parsing with large language models. In\nProceedings of ICLR 2023.\nJack FitzGerald, Christopher Hench, Charith Peris, and\nKay Rottmann. 2022. Massively multilingual natural\nlanguage understanding 2022 (MMNLU-22) work-\nshop and competition. In Proceedings of the Mas-\nsively Multilingual Natural Language Understanding\nWorkshop (MMNLU-22), pages 83\u201387, Abu Dhabi,\nUnited Arab Emirates (Hybrid). Association for Com-\nputational Linguistics.\nGoran Glava\u0161 and Ivan Vuli\u00b4c. 2021. Is supervised syn-\ntactic parsing beneficial for language understanding\ntasks? an empirical investigation. In Proceedings\nof the 16th Conference of the European Chapter of\nthe Association for Computational Linguistics: Main\nVolume, pages 3090\u20133104, Online. Association for\nComputational Linguistics.\nRahul Goel, Waleed Ammar, Aditya Gupta, Siddharth\nVashishtha, Motoki Sano, Faiz Surani, Max Chang,\nHyunJeong Choe, David Greene, Kyle He, Rattima\nNitisaroj, Anna Trukhina, Shachi Paul, Pararth Shah,\nRushin Shah, and Zhou Yu. 2023. PRESTO: A mul-\ntilingual dataset for parsing realistic task-oriented\ndialogs. arXiv preprint arXiv:2303.08954.\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-\nJen Chen, Guillaume Wenzek, Da Ju, Sanjana Kr-\nishnan, Marc\u2019Aurelio Ranzato, Francisco Guzm\u00e1n,\nand Angela Fan. 2022. The Flores-101 evaluation\nbenchmark for low-resource and multilingual ma-\nchine translation. Transactions of the Association for\nComputational Linguistics, 10:522\u2013538.\nAlexander Gutkin, Cibu Johny, Raiomond Doctor, Brian\nRoark, and Richard Sproat. 2022. Beyond Arabic:\nSoftware for Perso-Arabic script manipulation. In\nProceedings of the The Seventh Arabic Natural Lan-\nguage Processing Workshop (WANLP), pages 381\u2013\n387, Abu Dhabi, United Arab Emirates (Hybrid).\nAssociation for Computational Linguistics.\nBarry Haddow, Rachel Bawden, Antonio Valerio\nMiceli Barone, Jind\u02c7rich Helcl, and Alexandra Birch.\n2022. Survey of low-resource machine translation.\nComputational Linguistics, 48(3):673\u2013732.\nHarald Hammarstr\u00f6m, Shafqat Mumtaz Virk, and\nMarkus Forsberg. 2017.\nPoor man\u2019s OCR post-\ncorrection:\nUnsupervised recognition of variant\nspelling applied to a multilingual document collec-\ntion. In Proceedings of the 2nd International Confer-\nence on Digital Access to Textual Cultural Heritage,\nDATeCH2017, pages 71\u201375, G\u00f6ttingen, Germany.\nAssociation for Computing Machinery.\nTahmid Hasan, Abhik Bhattacharjee, Md. Saiful Is-\nlam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang,\nM. Sohel Rahman, and Rifat Shahriyar. 2021. XL-\nsum: Large-scale multilingual abstractive summariza-\ntion for 44 languages. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 4693\u20134703, Online. Association for Computa-\ntional Linguistics.\nMichael A. Hedderich, David Adelani, Dawei Zhu, Je-\nsujoba Alabi, Udia Markus, and Dietrich Klakow.\n2020. Transfer learning and distant supervision for\nmultilingual transformer models: A study on African\nlanguages. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 2580\u20132591, Online. Association for\nComputational Linguistics.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. XTREME: A massively multilingual multi-\ntask benchmark for evaluating cross-lingual generali-\nsation. In Proceedings of the 37th International Con-\nference on Machine Learning (ICML), pages 4411\u2013\n4421, Online. PMLR.\nOana Ignat, Jean Maillard, Vishrav Chaudhary, and\nFrancisco Guzm\u00e1n. 2022. OCR improves machine\ntranslation for low-resource languages. In Findings\nof the Association for Computational Linguistics:\nACL 2022, pages 1164\u20131174, Dublin, Ireland. As-\nsociation for Computational Linguistics.\nISO. 2004. ISO 15924: Codes for the representation\nof names of scripts. International Organization for\nStandardization, Geneva, Switzerland.\nPaul Jaccard. 1908. Nouvelles recherches sur la distri-\nbution florale. Bulletin de la Societe Vaudoise des\nSciences Naturelles, 44(163):223\u2013270. In French.\nCibu Johny, Lawrence Wolf-Sonkin, Alexander Gutkin,\nand Brian Roark. 2021. Finite-state script normal-\nization and processing utilities: The Nisaba Brahmic\nlibrary. In Proceedings of the 16th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics: System Demonstrations, pages\n14\u201323, Online. Association for Computational Lin-\nguistics.\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika\nBali, and Monojit Choudhury. 2020. The state and\nfate of linguistic diversity and inclusion in the NLP\nworld. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n6282\u20136293, Online. Association for Computational\nLinguistics.\nSimran Khanuja, Sebastian Ruder, and Partha Talukdar.\n2023. Evaluating Inclusivity, Equity, and Accessi-\nbility of NLP Technology: A Case Study for Indian\nLanguages.\nIn EACL 2023 Findings, Dubrovnik,\nCroatia. Association for Computational Linguistics.\nTo appear.\nShreya Khare, Ashish R Mittal, Anuj Diwan, Sunita\nSarawagi, Preethi Jyothi, and Samarth Bharadwaj.\n2021. Low resource ASR: The surprising effective-\nness of high resource transliteration. In Proceedings\nof Interspeech, pages 1529\u20131533, Brno, Czechia. In-\nternational Speech Communication Association.\nJulia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wahab,\nDaan van Esch, Nasanbayar Ulzii-Orshikh, Allah-\nsera Tapo, Nishant Subramani, Artem Sokolov, Clay-\ntone Sikasote, Monang Setyawan, Supheakmungkol\nSarin, Sokhar Samb, Beno\u00eet Sagot, Clara Rivera, An-\nnette Rios, Isabel Papadimitriou, Salomey Osei, Pe-\ndro Ortiz Suarez, Iroro Orife, Kelechi Ogueji, An-\ndre Niyongabo Rubungo, Toan Q. Nguyen, Math-\nias M\u00fcller, Andr\u00e9 M\u00fcller, Shamsuddeen Hassan\nMuhammad, Nanda Muhammad, Ayanda Mnyak-\neni, Jamshidbek Mirzakhalov, Tapiwanashe Matan-\ngira, Colin Leong, Nze Lawson, Sneha Kudugunta,\nYacine Jernite, Mathias Jenny, Orhan Firat, Bonaven-\nture F. P. Dossou, Sakhile Dlamini, Nisansa de Silva,\nSakine \u00c7abuk Ball\u0131, Stella Biderman, Alessia Bat-\ntisti, Ahmed Baruwa, Ankur Bapna, Pallavi Baljekar,\nIsrael Abebe Azime, Ayodele Awokoya, Duygu Ata-\nman, Orevaoghene Ahia, Oghenefego Ahia, Sweta\nAgrawal, and Mofetoluwa Adeyemi. 2022. Quality\nat a glance: An audit of web-crawled multilingual\ndatasets. Transactions of the Association for Compu-\ntational Linguistics, 10:50\u201372.\nSameer Kumar, Victor Bittorf, Dehao Chen, Chiachen\nChou, Blake Hechtman, HyoukJoong Lee, Naveen\nKumar, Peter Mattson, Shibo Wang, Tao Wang,\nYuanzhong Xu, and Zongwei Zhou. 2019. Scale\nMLPerf-0.6 models on Google TPU-v3 pods. In Pro-\nceedings of Workshop on Systems for ML at NeurIPS\n2019, Vancouver, Canada.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: A benchmark for question answering\nresearch. Transactions of the Association for Compu-\ntational Linguistics, 7:452\u2013466.\nAnne Lauscher, Vinit Ravishankar, Ivan Vuli\u00b4c, and\nGoran Glava\u0161. 2020. From zero to hero: On the\nlimitations of zero-shot language transfer with mul-\ntilingual Transformers. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 4483\u20134499, On-\nline. Association for Computational Linguistics.\nHaoran Li, Abhinav Arora, Shuohui Chen, Anchit\nGupta, Sonal Gupta, and Yashar Mehdad. 2021.\nMTOP: A comprehensive multilingual task-oriented\nsemantic parsing benchmark. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 2950\u20132962, Online. Association for Computa-\ntional Linguistics.\nJing Li, Aixin Sun, Jianglei Han, and Chenliang Li.\n2020. A survey on deep learning for named entity\nrecognition. IEEE Transactions on Knowledge and\nData Engineering, 34(1):50\u201370.\nYaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei\nGuo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin\nJiang, Guihong Cao, Xiaodong Fan, Ruofei Zhang,\nRahul Agrawal, Edward Cui, Sining Wei, Taroon\nBharti, Ying Qiao, Jiun-Hung Chen, Winnie Wu,\nShuguang Liu, Fan Yang, Daniel Campos, Rangan\nMajumder, and Ming Zhou. 2020. XGLUE: A new\nbenchmark dataset for cross-lingual pre-training, un-\nderstanding and generation. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6008\u20136018,\nOnline. Association for Computational Linguistics.\nZhaojiang Lin, Andrea Madotto, Genta Indra Winata,\nPeng Xu, Feijun Jiang, Yuxiang Hu, Chen Shi, and\nPascale Fung. 2021.\nBiToD: A bilingual multi-\ndomain dataset for task-oriented dialogue model-\ning.\nIn Proceedings of the 35th Conference on\nNeural Information Processing Systems (NeurIPS\n2021), Datasets and Benchmarks Track (Round 1),\nvolume 1.\nOlga\nMajewska,\nEvgeniia\nRazumovskaia,\nEdoardo Maria Ponti,\nIvan Vuli\u00b4c,\nand Anna\nKorhonen. 2023.\nCross-lingual dialogue dataset\ncreation via outline-based generation. Transactions\nof the Association for Computational Linguistics,\n11:139\u2013156.\nSabrina J. Mielke. 2019. Can you compare perplexity\nacross different segmentations? Available in: http:\n//sjmielke.com/comparing-perplexities.htm.\nBlog post.\nVerginica Barbu Mititelu and Maria Mitrofan. 2020.\nThe Romanian medical treebank \u2013 SiMoNERo. In\nProceedings of the The 15th Edition of the Interna-\ntional Conference on Linguistic Resources and Tools\nfor Natural Language Processing \u2013 ConsILR, pages\n7\u201316, Online. Romanian Association of Computa-\ntional Linguistics.\nNikita Moghe, Evgeniia Razumovskaia, Liane Guillou,\nIvan Vuli\u00b4c, Anna Korhonen, and Alexandra Birch.\n2022. MULTI3NLU++: A multilingual, multi-intent,\nmulti-domain dataset for natural language under-\nstanding in task-oriented dialogue. arXiv preprint\narXiv:2212.10455.\nPeya Mowar, Tanuja Ganu, and Saikat Guha. 2022.\nTowards optimizing OCR for accessibility. arXiv\npreprint arXiv:2206.10254. Extended abstract for\nposter session at Accessibility, Vision, and Auton-\nomy Meet (CVPR 2022 Workshop).\nShamsuddeen Hassan Muhammad, Idris Abdulmumin,\nAbinew Ali Ayele, Nedjma Ousidhoum, David Ife-\noluwa Adelani, Seid Muhie Yimam, Ibrahim Sa\u2019id\nAhmad, Meriem Beloucif, Saif Mohammad, Se-\nbastian Ruder, Oumaima Hourrane, Pavel Brazdil,\nFelermino D\u00e1rio M\u00e1rio Ant\u00f3nio Ali, Davis Davis,\nSalomey Osei, Bello Shehu Bello, Falalu Ibrahim,\nTajuddeen Gwadabe, Samuel Rutunda, Tadesse Be-\nlay, Wendimu Baye Messelle, Hailu Beshada Balcha,\nSisay Adugna Chala, Hagos Tesfahun Gebremichael,\nBernard Opoku, and Steven Arthur. 2023. AfriSenti:\nA Twitter sentiment analysis benchmark for African\nlanguages. arXiv preprint arXiv:2302.08956.\nShamsuddeen Hassan Muhammad, David Ifeoluwa Ade-\nlani, Sebastian Ruder, Ibrahim Sa\u2019id Ahmad, Idris\nAbdulmumin, Bello Shehu Bello, Monojit Choud-\nhury, Chris Chinenye Emezue, Saheed Salahudeen\nAbdullahi, Anuoluwapo Aremu, Al\u00edpio Jorge, and\nPavel Brazdil. 2022. NaijaSenti: A Nigerian Twitter\nsentiment corpus for multilingual sentiment analy-\nsis. In Proceedings of the Thirteenth Language Re-\nsources and Evaluation Conference, pages 590\u2013602,\nMarseille, France. European Language Resources\nAssociation.\nNibal Nayef, Fei Yin, Imen Bizid, Hyunsoo Choi, Yuan\nFeng, Dimosthenis Karatzas, Zhenbo Luo, Uma-\npada Pal, Christophe Rigaud, Joseph Chazalon, et al.\n2017.\nICDAR2017 robust reading challenge on\nmulti-lingual scene text detection and script identifi-\ncation - RRC-MLT. In 2017 14th IAPR International\nConference on Document Analysis and Recognition\n(ICDAR), volume 1, pages 1454\u20131459, Kyoto, Japan.\nIEEE.\nWilhelmina Nekoto, Vukosi Marivate, Tshinondiwa\nMatsila,\nTimi\nFasubaa,\nTaiwo\nFagbohungbe,\nSolomon Oluwole Akinola, Shamsuddeen Muham-\nmad, Salomon Kabongo Kabenamualu, Salomey\nOsei, Freshia Sackey, Rubungo Andre Niyongabo,\nRicky Macharm, Perez Ogayo, Orevaoghene Ahia,\nMusie Meressa Berhe,\nMofetoluwa Adeyemi,\nMasabata Mokgesi-Selinga, Lawrence Okegbemi,\nLaura Martinus, Kolawole Tajudeen, Kevin Degila,\nKelechi Ogueji, Kathleen Siminyu, Julia Kreutzer,\nJason Webster, Jamiil Toure Ali, Jade Abbott,\nIroro Orife, Ignatius Ezeani, Idris Abdulkadir Dan-\ngana, Herman Kamper, Hady Elsahar, Goodness\nDuru, Ghollah Kioko, Murhabazi Espoir, Elan van\nBiljon, Daniel Whitenack, Christopher Onyefuluchi,\nChris Chinenye Emezue, Bonaventure F. P. Dossou,\nBlessing Sibanda, Blessing Bassey, Ayodele Olabiyi,\nArshath Ramkilowan, Alp \u00d6ktem, Adewale Akin-\nfaderin, and Abdallah Bashir. 2020. Participatory re-\nsearch for low-resourced machine translation: A case\nstudy in African languages. In Findings of the Asso-\nciation for Computational Linguistics: EMNLP 2020,\npages 2144\u20132160, Online. Association for Computa-\ntional Linguistics.\nMassimo Nicosia and Francesco Piccinno. 2022. Byte-\nlevel massively multilingual semantic parsing. In\nProceedings of the Massively Multilingual Natural\nLanguage Understanding Workshop (MMNLU-22),\npages 25\u201334, Abu Dhabi, United Arab Emirates (Hy-\nbrid). Association for Computational Linguistics.\nJoakim Nivre, Marie-Catherine de Marneffe, Filip Gin-\nter, Jan Haji\u02c7c, Christopher D. Manning, Sampo\nPyysalo, Sebastian Schuster, Francis Tyers, and\nDaniel Zeman. 2020. Universal Dependencies v2:\nAn evergrowing multilingual treebank collection. In\nProceedings of the Twelfth Language Resources and\nEvaluation Conference, pages 4034\u20134043, Marseille,\nFrance. European Language Resources Association.\nRubungo Andre Niyongabo, Qu Hong, Julia Kreutzer,\nand Li Huang. 2020. KINNEWS and KIRNEWS:\nBenchmarking cross-lingual text classification for\nKinyarwanda and Kirundi. In Proceedings of the\n28th International Conference on Computational Lin-\nguistics, pages 5507\u20135521, Barcelona, Spain (On-\nline). International Committee on Computational Lin-\nguistics.\nSebastian Nordhoff and Harald Hammarstr\u00f6m. 2011.\nGlottolog/langdoc: Defining dialects, languages, and\nlanguage families as collections of resources.\nIn\nFirst International Workshop on Linked Science 2011-\nIn conjunction with the International Semantic Web\nConference (ISWC 2011).\nAddison Phillips and Mark Davis. 2009. BCP 47 \u2013 Tags\nfor Identifying Languages. IETF Trust. RFC 5646.\nReiner Pope, Sholto Douglas, Aakanksha Chowdhery,\nJacob Devlin, James Bradbury, Anselm Levskaya,\nJonathan Heek, Kefan Xiao, Shivani Agrawal, and\nJeff Dean. 2022. Efficiently scaling transformer in-\nference. arXiv preprint arXiv:2211.05102.\nMaja Popovi\u00b4c. 2015. chrF: character n-gram F-score\nfor automatic MT evaluation. In Proceedings of the\nTenth Workshop on Statistical Machine Translation,\npages 392\u2013395, Lisbon, Portugal. Association for\nComputational Linguistics.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don\u2019t know: Unanswerable ques-\ntions for SQuAD. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), pages 784\u2013789,\nMelbourne, Australia. Association for Computational\nLinguistics.\nChristophe Rigaud, Antoine Doucet, Micka\u00ebl Coustaty,\nand Jean-Philippe Moreux. 2019. ICDAR 2019 com-\npetition on post-OCR text correction. In 2019 In-\nternational Conference on Document Analysis and\nRecognition (ICDAR), pages 1588\u20131593, Sydney,\nAustralia. IEEE.\nShruti Rijhwani, Antonios Anastasopoulos, and Graham\nNeubig. 2020. OCR Post Correction for Endangered\nLanguage Texts. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 5931\u20135942, Online. As-\nsociation for Computational Linguistics.\nShruti Rijhwani, Daisy Rosenblum, Antonios Anas-\ntasopoulos, and Graham Neubig. 2021.\nLexi-\ncally aware semi-supervised learning for OCR post-\ncorrection. Transactions of the Association for Com-\nputational Linguistics, 9:1285\u20131302.\nBrian Roark, Lawrence Wolf-Sonkin, Christo Kirov,\nSabrina J. Mielke, Cibu Johny, Isin Demirsahin, and\nKeith Hall. 2020. Processing South Asian languages\nwritten in the Latin script: the Dakshina dataset. In\nProceedings of the Twelfth Language Resources and\nEvaluation Conference, pages 2413\u20132423, Marseille,\nFrance. European Language Resources Association.\nAdam Roberts, Hyung Won Chung, Anselm Levskaya,\nGaurav Mishra, James Bradbury, Daniel Andor, Sha-\nran Narang, Brian Lester, Colin Gaffney, Afroz\nMohiuddin, Curtis Hawthorne, Aitor Lewkowycz,\nAlex Salcianu, Marc van Zee, Jacob Austin, Se-\nbastian Goodman, Livio Baldini Soares, Haitang\nHu, Sasha Tsvyashchenko, Aakanksha Chowdh-\nery, Jasmijn Bastings, Jannis Bulian, Xavier Gar-\ncia, Jianmo Ni, Andrew Chen, Kathleen Kenealy,\nJonathan H. Clark, Stephan Lee, Dan Garrette, James\nLee-Thorp, Colin Raffel, Noam Shazeer, Marvin\nRitter, Maarten Bosma, Alexandre Passos, Jeremy\nMaitin-Shepard, Noah Fiedel, Mark Omernick, Bren-\nnan Saeta, Ryan Sepassi, Alexander Spiridonov,\nJoshua Newlan, and Andrea Gesmundo. 2022. Scal-\ning up models and data with t5x and seqio. arXiv\npreprint arXiv:2203.17189.\nSebastian Ruder, Noah Constant, Jan Botha, Aditya Sid-\ndhant, Orhan Firat, Jinlan Fu, Pengfei Liu, Junjie\nHu, Dan Garrette, Graham Neubig, and Melvin John-\nson. 2021. XTREME-R: Towards more challenging\nand nuanced multilingual evaluation. In Proceedings\nof the 2021 Conference on Empirical Methods in\nNatural Language Processing, pages 10215\u201310245,\nOnline and Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\nAmarjot Singh, Ketan Bacchuwar, and Akshay Bhasin.\n2012.\nA survey of OCR applications.\nInterna-\ntional Journal of Machine Learning and Computing,\n2(3):314\u2013318.\nMartin Sundermeyer, Ralf Schl\u00fcter, and Hermann Ney.\n2012. LSTM neural networks for language model-\ning. In Proceedings of Interspeech, pages 194\u2013197,\nPortland, OR, USA. International Speech Communi-\ncation Association.\nCynthia Tam and David Wells. 2009. Evaluating the\nbenefits of displaying word prediction lists on a per-\nsonal digital assistant at the keyboard level. Assistive\nTechnology, 21(3):105\u2013114.\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the CoNLL-2003 shared task:\nLanguage-independent named entity recognition. In\nProceedings of the Seventh Conference on Natural\nLanguage Learning at HLT-NAACL 2003, pages 142\u2013\n147.\nDaniel Varab and Natalie Schluter. 2021.\nMas-\nsiveSumm: a very large-scale, very multilingual,\nnews summarisation dataset. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 10150\u201310161, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nHans H. Wellisch. 1978. The Conversion of Scripts:\nIts Nature, History, and Utilization.\nInformation\nSciences Series. John Wiley & Sons, New York.\nKen Whistler. 2021.\nUnicode normalization forms.\nTechnical Report TR15-51, Unicode Consortium.\nVersion 14.0.0.\nGenta Indra Winata, Alham Fikri Aji, Samuel Cahyaw-\nijaya, Rahmad Mahendra, Fajri Koto, Ade Ro-\nmadhony, Kemal Kurniawan, David Moeljadi, Ra-\ndityo Eko Prasojo, Pascale Fung, Timothy Bald-\nwin, Jey Han Lau, Rico Sennrich, and Sebastian\nRuder. 2023. NusaX: Multilingual Parallel Senti-\nment Dataset for 10 Indonesian Local Languages.\nIn Proceedings of the 17th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics (EACL 2023), Dubrovnik, Croatia. Asso-\nciation for Computational Linguistics. To appear.\nLinting Xue, Aditya Barua, Noah Constant, Rami Al-\nRfou, Sharan Narang, Mihir Kale, Adam Roberts,\nand Colin Raffel. 2022. ByT5: Towards a token-free\nfuture with pre-trained byte-to-byte models. Transac-\ntions of the Association for Computational Linguis-\ntics, 10:291\u2013306.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mT5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 483\u2013498, On-\nline. Association for Computational Linguistics.\nMengjie Zhao, Yi Zhu, Ehsan Shareghi, Ivan Vuli\u00b4c,\nRoi Reichart, Anna Korhonen, and Hinrich Sch\u00fctze.\n2021a. A closer look at few-shot crosslingual trans-\nfer: The choice of shots matters. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 5751\u20135767, Online.\nAssociation for Computational Linguistics.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021b.\nCalibrate before use: Im-\nproving few-shot performance of language models.\nIn International Conference on Machine Learning,\npages 12697\u201312706, Online. PMLR.\nA\nLanguage Coverage\nWe provide an overview of the under-represented\nlanguages in XTREME-UP in Table 5. For each\nlanguage, we indicate a) the ISO 639-1 code (or\nISO 639-3 code if the former is unavailable); b)\nits language family according to Glottolog (Nord-\nhoff and Hammarstr\u00f6m, 2011); c) the number of\ndatasets in XTREME-UP including the language; d)\nits resource level based on the taxonomy of Joshi\net al. (2020) (0 is least and 5 is highest-resourced);\nand e) which tasks include the language.\nB\nData cards\nB.1\nASR\nB.1.1\nTask description\nAutomatic speech recognition (ASR) transcribes\nspeech inputs into human-readable text, serving as\na fundamental step for various speech language\nunderstanding applications. The transcripts are\noften calibrated with some pre-trained language\nmodels to produce the final outputs. In this paper,\nwe build the ASR benchmark in this way: first,\ntranscribe input audio into text with a pre-trained\nspeech recognition model; then calibrate the tran-\nscripts by fine-tuning pre-trained language models\non paired transcripts and ground truths.\nB.1.2\nData creation\nExperimented on the FLEURS corpus (Conneau\net al., 2023), we use Maestro-U (Chen et al., 2023)\nto generate the ASR transcripts. For the pre-trained\nlanguage models, we choose mT5-base (Xue et al.,\n2021) and ByT5-base (Xue et al., 2022) models.\nWe paired the ASR transcripts with the ground\ntruths to fine-tune the mT5 or ByT5 models. The\naverage character error rate (CER) of Maestro-U\nis 8.28% across 102 languages, providing a strong\nbaseline. Therefore, we build the ASR benchmark\nin a selective way: first, we compare the Maestro-\nU baseline CER on the dev set with the CER ob-\ntained by fine-tuned mT5 or fine-tuned ByT5. If\nthe fine-tuned result is better, we choose the fine-\ntuned model for the language to rescore its test set;\notherwise, we keep the baseline Maestro-U results\nfor the test.\nB.1.3\nData structure\nWe followed the data split of train, dev, and test sets\nin FLEURS, and filtered out the examples where\nMaestro-U prediction is empty (i.e., all the deletion\nerrors). The pairs of transcript and ground truth are\nsaved in jsonl and tsv format.\nThe individual language datasets are mostly dis-\ntinguished by the language and region BCP-47\ncodes, e.g., the kam_ke code represents Kamba\nlanguage spoken in Kenya. In some cases, when\nmultiple writing systems are available for a lan-\nguage, the ISO 15924 script code is used as well,\nas is the case with the code sd_arab_in that de-\nnotes Sindhi as spoken in India and recorded using\nArabic script, as opposed to its Pakistani counter-\npart.20\n20The es_419 code represents Latin American Spanish.\nLanguage\nISO\ncode\nLanguage\nfamily\n# of\ndatasets\nResource\nlevel\nQA\nRetrieval\nNER\nSemantic\nparsing\nMT\nASR\nOCR\nTranslit-\neration\nAuto-\ncomplete\nAfrikaans\naf\nIndo-European\n2\n3\n\u2713\n\u2713\nAmharic\nam\nAfro-Asiatic\n7\n2\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nAssamese\nas\nIndo-European\n3\n1\n\u2713\n\u2713\nAsturian\nast\nIndo-European\n2\n1\n\u2713\n\u2713\nAzerbaijani\naz\nTurkic\n3\n1\n\u2713\n\u2713\nGhom\u00e1l\u00e1\u2019\nbbj\nAtlantic-Congo\n1\n0\n\u2713\nBelarusian\nbe\nIndo-European\n4\n3\n\u2713\n\u2713\n\u2713\n\u2713\nBulgarian\nbg\nIndo-European\n3\n3\n\u2713\n\u2713\n\u2713\nBambara\nbm\nMande\n1\n1\n\u2713\nBengali\nbn\nIndo-European\n7\n3\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nBosnian\nbs\nIndo-European\n2\n3\n\u2713\n\u2713\nCebuano\nceb\nAustronesian\n2\n3\n\u2713\n\u2713\nCentral Kurdish\nckb\nIndo-European\n3\n1\n\u2713\n\u2713\nWelsh\ncy\nIndo-European\n3\n1\n\u2713\n\u2713\nDanish\nda\nIndo-European\n3\n3\n\u2713\n\u2713\n\u2713\nEwe\nee\nAtlantic-Congo\n1\n1\n\u2713\nGreek\nel\nIndo-European\n3\n3\n\u2713\n\u2713\n\u2713\nEstonian\net\nUralic\n3\n3\n\u2713\n\u2713\n\u2713\nFula\nff\nAtlantic-Congo\n2\n1\n\u2713\n\u2713\nFilipino\nfil\nAustronesian\n2\n1\n\u2713\n\u2713\nIrish\nga\nIndo-European\n4\n2\n\u2713\n\u2713\n\u2713\nGalician\ngl\nIndo-European\n3\n3\n\u2713\n\u2713\n\u2713\nGujarati\ngu\nIndo-European\n4\n1\n\u2713\n\u2713\n\u2713\nHausa\nha\nAfro-Asiatic\n5\n2\n\u2713\n\u2713\n\u2713\n\u2713\nHebrew\nhe\nAfro-Asiatic\n3\n3\n\u2713\n\u2713\n\u2713\nArmenian\nhy\nIndo-European\n4\n1\n\u2713\n\u2713\n\u2713\nIndonesian\nid\nAustronesian\n5\n3\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nIgbo\nig\nAtlantic-Congo\n4\n1\n\u2713\n\u2713\n\u2713\nIcelandic\nis\nIndo-European\n4\n2\n\u2713\n\u2713\n\u2713\nJavanese\njv\nAustronesian\n3\n1\n\u2713\n\u2713\nGeorgian\nka\nKartvelian\n2\n3\n\u2713\n\u2713\nKamba\nkam\nAtlantic-Congo\n2\n0\n\u2713\n\u2713\nKabuverdianu\nkea\nIndo-European\n2\n0\n\u2713\n\u2713\nKazakh\nkk\nTurkic\n2\n3\n\u2713\n\u2713\nKhmer\nkm\nAustroasiatic\n3\n1\n\u2713\n\u2713\nKannada\nkn\nDravidian\n5\n1\n\u2713\n\u2713\n\u2713\n\u2713\nKyrgyz\nky\nTurkic\n3\n1\n\u2713\n\u2713\nLuxembourgish\nlb\nIndo-European\n3\n1\n\u2713\n\u2713\n(Lu)Ganda\nlg\nAtlantic-Congo\n4\n1\n\u2713\n\u2713\n\u2713\nLingala\nln\nAtlantic-Congo\n3\n1\n\u2713\n\u2713\nLao\nlo\nTai-Kadai\n3\n2\n\u2713\n\u2713\nLithuanian\nlt\nIndo-European\n3\n3\n\u2713\n\u2713\n\u2713\n(Dho)Luo\nluo\nNilotic\n3\n0\n\u2713\n\u2713\n\u2713\nLatvian\nlv\nIndo-European\n3\n3\n\u2713\n\u2713\n\u2713\nMaori\nmi\nAustronesian\n3\n1\n\u2713\n\u2713\nMacedonian\nmk\nIndo-European\n3\n1\n\u2713\n\u2713\nMalayalam\nml\nDravidian\n4\n1\n\u2713\n\u2713\n\u2713\nMongolian\nmn\nMongolic-Khitan\n3\n1\n\u2713\n\u2713\nMossi (Moor\u00e9)\nmos\nAtlantic-Congo\n1\n0\n\u2713\nMarathi\nmr\nIndo-European\n3\n2\n\u2713\n\u2713\n\u2713\nMalay\nms\nAustronesian\n2\n3\n\u2713\n\u2713\nMaltese\nmt\nAfro-Asiatic\n2\n2\n\u2713\n\u2713\nBurmese\nmy\nSino-Tibetan\n4\n1\n\u2713\n\u2713\n\u2713\nNepali\nne\nIndo-European\n3\n1\n\u2713\n\u2713\nNorwegian\nno\nIndo-European\n2\n1\n\u2713\n\u2713\nNorthern Sotho\nnso\nAtlantic-Congo\n3\n1\n\u2713\n\u2713\nNyanja (Chichewa)\nny\nAtlantic-Congo\n4\n1\n\u2713\n\u2713\n\u2713\nOccitan\noc\nIndo-European\n2\n1\n\u2713\n\u2713\nOromo\nom\nAfro-Asiatic\n3\n1\n\u2713\n\u2713\nOriya\nor\nIndo-European\n2\n1\n\u2713\n\u2713\nPunjabi\npa\nIndo-European\n4\n2\n\u2713\n\u2713\n\u2713\nNigerian Pidgin\npcm\nIndo-European\n2\n0\n\u2713\n\u2713\nPashto\nps\nIndo-European\n3\n1\n\u2713\n\u2713\nRomanian\nro\nIndo-European\n3\n3\n\u2713\n\u2713\n\u2713\nKinyarwanda\nrw\nAtlantic-Congo\n1\n1\n\u2713\nSanskrit\nsa\nIndo-European\n1\n2\n\u2713\nSindhi\nsd\nIndo-European\n4\n1\n\u2713\n\u2713\n\u2713\nSinhala\nsi\nIndo-European\n2\n0\n\u2713\n\u2713\nSlovak\nsk\nIndo-European\n3\n3\n\u2713\n\u2713\n\u2713\nSlovenian\nsl\nIndo-European\n3\n3\n\u2713\n\u2713\n\u2713\nShona\nsn\nAtlantic-Congo\n4\n1\n\u2713\n\u2713\n\u2713\nSomali\nso\nAfro-Asiatic\n3\n1\n\u2713\n\u2713\nSwahili\nsw\nAtlantic-Congo\n8\n2\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nTamil\nta\nDravidian\n4\n3\n\u2713\n\u2713\n\u2713\n\u2713\nTelugu\nte\nDravidian\n6\n1\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nTajik\ntg\nIndo-European\n3\n1\n\u2713\n\u2713\nThai\nth\nTai-Kadai\n3\n3\n\u2713\n\u2713\n\u2713\nTswana (Setswana)\ntn\nAtlantic-Congo\n1\n2\n\u2713\nTwi\ntw\nAtlantic-Congo\n1\n1\n\u2713\nUyghur\nug\nTurkic\n1\n1\n\u2713\nUkrainian\nuk\nIndo-European\n3\n3\n\u2713\n\u2713\n\u2713\nUmbundu\numb\nAtlantic-Congo\n2\n0\n\u2713\n\u2713\nUrdu\nur\nIndo-European\n4\n3\n\u2713\n\u2713\n\u2713\n\u2713\nUzbek\nuz\nTurkic\n2\n3\n\u2713\n\u2713\nWolof\nwo\nAtlantic-Congo\n3\n2\n\u2713\n\u2713\n\u2713\nXhosa\nxh\nAtlantic-Congo\n4\n2\n\u2713\n\u2713\n\u2713\nYoruba\nyo\nAtlantic-Congo\n5\n2\n\u2713\n\u2713\n\u2713\n\u2713\nZulu\nzu\nAtlantic-Congo\n5\n2\n\u2713\n\u2713\n\u2713\n\u2713\nTable 5: Overview of under-represented languages covered in XTREME-UP.\nB.1.4\nData statistics\nThe FLEURS dataset contains about 1.4k hours of\naudio in total for 102 languages. The training data\ncontains 271,488 examples across 102 languages,\naverage length per utterance is about 20 tokens.\nThere are 34,661 examples in the validation (dev)\nset, and 77,943 examples in the test set.\nB.1.5\nExperiments and Discussion\nWe compared fine-tuned mT5-base and ByT5-base\nbaselines, which were built on TPU. In addition, we\nexplored the compute efficient fine-tuning on GPU,\nusing a mT5-small model as pre-trained model.\nThe three models took 4500, 6500 and 4000 steps\nto converge, respectively. We report the character\nerror rate for the predicted transcripts by the fine-\ntuned models against the one for the Maestro-U\nbaseline, which is 8.28% on average for 102 lan-\nguages \u2013 a quite strict baseline. We observed small\ngains through fine-tuning with different pre-trained\nmodels, as shown in Table 6.\nIt is observed that ByT5 yields better fine-tuned\nresults than mT5, indicating that byte is a better\nmodeling unit when it comes to textual data of\nvarious writing systems. By calculating the average\nCER for 24 high-resourced language group and\n78 low-resourced language group respectively, we\nfind that both mT5 and ByT5 fine-tuned models\ncan reduce CER from 6.40% baseline to 6.36%\nfor high-resourced languages, while ByT5 on its\nown can further improve CERs for low-resourced\nlanguages from 8.86% baseline to 8.80%.\nFine-tuned ByT5 also generalized well on lan-\nguages which were not seen in the pre-training\nphase. With a limited amount of fine-tuning data,\nByT5 can improve baseline on the group of unseen\nlanguages, especially on Umbundu (umb_ao, -14%\nCER Relative). Even though only Romanized Chi-\nnese is used to pre-train ByT5, the fine-tuned ByT5\noutperformed baselines for both Mandarin (in sim-\nplified Chinese, cmn_hans_cn), and Cantonese (in\ntraditional Chinese, cmn_hant_hk).\nB.2\nOptical character recognition (OCR)\nB.2.1\nGeneral information\nDataset title\nUL-OCR\nB.2.2\nData creation\nWe retrieve books that are in the public domain\non Google Books. In many cases, these are his-\ntoric books where the copyright has expired while\nothers are more recent publications. We focus on\nlanguages with diverse scripts where no existing\nOCR dataset is currently available. We observe\nthat many public-domain books in such languages\nare religious or linguistic in nature and were cre-\nated for missionary purposes. In order to identify a\ndiverse set of high-quality books, we first conduct\nan annotation task where we ask annotators to look\nat pages of a book and assign whether it is a) not\nin the target language, b) religious, c) consisting\nmainly of tables, d) linguistic (e.g., a dictionary\nor grammar book), e) not intelligible, or f) good\nquality. Based on this annotation, we needed to\nfilter out certain languages such as Hausa, Igbo,\nMalagasy, Yoruba, and Zulu, which had an insuffi-\ncient amount of high-quality public-domain books\navailable.\nB.3\nAutocomplete\nB.3.1\nTask description\nAutocomplete (or predictive text), i.e., predicting\nthe rest of a word a user is typing, is a useful tech-\nnology that speeds up human-computer interaction.\nHowever, while language modeling (LM) is a core\nnatural language processing (NLP) task, current\nLM evaluation does not address the practical con-\nstraints of human-computer interaction and current\nLMs are not directly useful for autocomplete in\nunder-represented languages.\nIn order to evaluate multilingual models on an\nevaluation setting as close as possible to the real-\nworld usage of autocomplete, we curated the Uni-\nversal Dependencies (UD) dataset (Nivre et al.,\n2020; de Marneffe et al., 2021) according to a set of\nhigh level principles that we describe in the section\nbelow.\nB.3.2\nData creation\nThe original UD dataset was filtered to better fit the\nuser centric paradigm proposed. We removed a)\ntreebanks using only ancient data, for example litur-\ngical text written in Latin, Ancient Greek or San-\nskrit; b) languages with fewer than 100 speakers\nlike Akunts\u00fa; c) signed languages like the Swedish\nSign Language; d) highly domain-specific content\nlike for instance SiMoNERo (Mititelu and Mitro-\nfan, 2020) which contains texts from three medical\nsubdomains: cardiology, diabetes, endocrinology;\ne) languages that are \"high resource\" by XTREME-\nUP standards with the exception of English which\nwe kept for prototyping; f) languages that do not\nhave all three of: training, validation and test sets:\nTask Code\nMaestro-U\nmT5\nByT5\nBase\nSmall\nBase\naf_za\n4.19\n4.19\n4.24\n4.19\nam_et\n8.60\n8.60\n8.66\n8.6\nar_eg\n6.00\n6.00\n6.03\n6.00\nas_in\n8.49\n8.49\n8.56\n8.49\nast_es\n4.49\n4.49\n4.66\n4.49\naz_az\n5.67\n5.67\n5.7\n5.67\nbe_by\n3.34\n3.34\n3.42\n3.34\nbn_in\n6.16\n6.16\n6.20\n6.16\nbs_ba\n2.93\n2.93\n3.05\n2.93\nca_es\n2.72\n2.72\n2.76\n2.72\nceb\n4.36\n4.36\n4.49\n4.36\nckb_iq\n8.70\n8.70\n8.77\n8.70\ncmn_hans_cn\n16.48\n16.06\n16.12\n16.06\ncmn_hant_hk\n34.82\n34.23\n34.24\n34.13\ncs_cz\n3.30\n3.30\n3.35\n3.30\ncy_gb\n7.11\n7.11\n7.17\n7.11\nda_dk\n6.30\n6.30\n6.35\n6.30\nde_de\n2.39\n2.39\n2.46\n2.39\nel_gr\n4.73\n4.73\n4.77\n4.73\nen_us\n9.02\n9.02\n9.11\n9.02\nes_419\n1.81\n1.81\n1.85\n1.81\net_ee\n2.24\n2.24\n2.28\n2.24\nfa_ir\n4.96\n4.96\n5.87\n4.96\nff_sn\n21.22\n21.22\n21.42\n21.22\nfi_fi\n2.02\n2.02\n2.05\n2.02\nfil_ph\n3.59\n3.59\n3.63\n3.59\nfr_fr\n4.57\n4.57\n4.63\n4.57\nga_ie\n29.75\n29.75\n29.78\n29.79\ngl_es\n2.55\n2.55\n2.58\n2.55\ngu_in\n5.75\n5.75\n5.9\n5.75\nha_ng\n7.70\n7.70\n9.23\n6.90\nhe_il\n18.36\n18.36\n18.40\n18.36\nhi_in\n5.59\n5.59\n5.63\n5.59\nhr_hr\n4.46\n4.46\n4.56\n4.46\nhu_hu\n7.05\n7.05\n7.10\n7.05\nhy_am\n4.93\n6.25\n4.94\n4.93\nid_id\n3.14\n3.14\n3.16\n3.14\nig_ng\n14.06\n14.06\n14.29\n14.07\nis_is\n6.23\n6.23\n6.25\n6.23\nit_it\n1.39\n1.39\n1.44\n1.39\nja_jp\n25.74\n25.49\n25.51\n25.43\njv_id\n4.66\n4.66\n4.72\n4.52\nka_ge\n10.09\n10.09\n10.16\n10.09\nkam_ke\n11.74\n11.69\n11.78\n11.74\nkea_cv\n4.11\n4.11\n4.17\n4.11\nkk_kz\n3.58\n3.58\n3.66\n3.58\nkm_kh\n20.15\n20.15\n20.15\n20.15\nkn_in\n5.13\n5.13\n5.40\n5.13\nko_kr\n14.29\n14.29\n14.22\n14.29\nky_kg\n4.53\n4.53\n4.56\n4.44\nlb_lu\n13.54\n13.54\n13.64\n13.54\nlg_ug\n8.99\n8.99\n9.13\n8.99\nln_cd\n4.61\n4.61\n4.76\n4.61\nlo_la\n22.80\n22.80\n22.84\n23.25\nlt_lt\n4.51\n4.51\n4.55\n4.51\nluo_ke\n5.64\n5.64\n5.73\n5.64\nlv_lv\n2.18\n2.18\n2.21\n2.18\nmi_nz\n9.59\n9.51\n9.6\n8.68\nmk_mk\n3.60\n3.60\n3.66\n3.60\nml_in\n5.04\n5.45\n5.2\n5.07\nmn_mn\n8.43\n8.43\n8.46\n8.43\nmr_in\n7.37\n7.37\n7.48\n7.37\nms_my\n3.89\n3.89\n3.92\n3.89\nTask Code\nMaestro-U\nmT5\nByT5\nBase\nSmall\nBase\n...\n... continued ...\n...\nmt_mt\n11.48\n11.57\n11.56\n11.57\nmy_mm\n14.70\n14.70\n14.87\n14.66\nnb_no\n4.14\n4.14\n4.21\n4.14\nne_np\n9.22\n9.26\n9.25\n9.22\nnl_nl\n3.15\n3.15\n3.27\n3.15\nnso_za\n7.13\n7.13\n7.17\n7.13\nny_mw\n7.08\n7.08\n7.08\n6.95\noc_fr\n7.68\n7.68\n7.84\n7.68\nom_et\n14.36\n14.36\n14.52\n14.36\nor_in\n7.42\n7.42\n8.70\n7.42\npa_in\n7.35\n7.35\n7.38\n7.35\npl_pl\n2.49\n2.49\n2.52\n2.49\nps_af\n16.82\n16.82\n16.86\n16.82\npt_br\n2.87\n2.87\n3.05\n2.87\nro_ro\n3.58\n3.58\n3.64\n3.58\nrup_bg\n2.72\n2.72\n2.86\n2.72\nru_ru\n3.05\n2.86\n3.09\n2.87\nsd_arab_in\n9.22\n9.22\n9.65\n10.08\nsk_sk\n2.39\n2.39\n2.43\n2.39\nsl_si\n4.58\n4.58\n4.60\n4.17\nsn_zw\n9.45\n9.45\n9.48\n9.45\nso_so\n13.73\n13.73\n13.81\n13.73\nsr_rs\n9.93\n9.93\n9.95\n9.95\nsv_se\n4.21\n4.21\n4.30\n4.21\nsw_ke\n12.62\n12.62\n12.76\n12.62\nta_in\n12.35\n11.55\n15.06\n12.35\nte_in\n7.48\n7.48\n7.56\n7.48\ntg_tj\n4.56\n4.56\n4.60\n4.56\nth_th\n11.89\n11.89\n11.92\n11.48\ntr_tr\n4.28\n4.28\n4.34\n4.28\nuk_ua\n5.44\n5.44\n5.47\n5.44\numb_ao\n17.46\n17.09\n17.47\n14.98\nur_pk\n7.61\n7.61\n7.63\n7.61\nuz_uz\n7.40\n7.40\n7.42\n7.40\nvi_vn\n11.80\n11.80\n11.83\n11.80\nwo_sn\n15.26\n15.26\n15.29\n15.26\nxh_za\n16.65\n16.65\n16.69\n16.68\nyo_ng\n19.84\n19.84\n19.93\n19.84\nzu_za\n5.56\n5.56\n5.62\n5.56\nMicro-Average\n8.28\n8.27\n8.40\n8.22\nTable 6: ASR tasks evaluated using CER metric at 4K steps of fine-tuning mT5 and ByT5 Small and Base models.\nTraining set\nmin\nmean\nmax\ncontext length\n1\n28\n96\ntarget length\n5\n8\n32\nValidation set\nmin\nmean\nmax\ncontext length\n1\n28\n86\ntarget length\n5\n8\n23\nTest set\nmin\nmean\nmax\ncontext length\n1\n28\n86\ntarget length\n5\n8\n28\nTable 7: Context and target character length statistics\naveraged over the 23 languages of autocomplete.\ng) languages with fewer than 1000 examples when\ncombining training and validation set.\nThe resulting corpus features 23 languages:\nBasque, Belarusian, Bulgarian, Danish, Eastern\nArmenian, English, Estonian, Galician, Scottish\nGaelic ,Greek, Hebrew, Icelandic, Indonesian, Irish,\nLatvian, Lithuanian, Nigerian Pidgin, Romanian,\nSlovak, Slovenian, Ukrainian, Urdu, and Uyghur.\nB.3.3\nData structure\nA data instance has two fields, input and target,\nfor instance {input: \"en_-We look f$\", target: \"for-\nward\"}. The input field is composed of a prefix\n\"en_-\" to indicates the language to the model and\na context sentence: \"We look f$\". The target field\nis the word to predict. We normalize all text with\nUnicode NFKC normalization (Whistler, 2021).\nAnnotation process\nIn the following, we de-\nscribe how the example described above is gen-\nerated from the source data. The original sentence\nis \u201cWe look forward to your active participation to\nmake this forum an exciting meeting place for like\nminded individuals.\u201d The steps are: a) The context\nsentence including the target can have at most 10\nwords. A random word of more than 5 characters\nis chosen to be the target. b) A target context is\nsampled from the target and added to the context.\nIn this example it is the character \"f\". The sample\nrule is to select a number of characters that can vary\nbetween 0 to the number of characters in the target\nminus three. In our example, the target \"forward\"\ncould be sampled from \"\" to \"forw\". c) A specific\ntoken \"$\" is added just after the target context.\nB.3.4\nData statistics\nWe sampled up to 2,000 examples from each lan-\nguage\u2019s training set, 1,000 examples from valida-\nAcc@3\nchrF\nLanguage\nmT5\nByT5\nmT5\nByT5\nbe\n12.4\n8.99\n22.12\n26.02\nbg\n14.45\n15.38\n23.27\n29.92\nda\n13.36\n26.95\n22.33\n35.03\nel\n21.74\n14.91\n25.3\n32.81\nen\n19.2\n40.1\n23.08\n39.84\net\n8\n23.2\n20.3\n31.85\neu\n9.7\n22\n26.36\n34.52\nga\n11.8\n23.06\n22.08\n30.66\ngd\n26.28\n31.85\n32.79\n37.65\ngl\n19\n41.4\n26.53\n45.76\nhe\n8.65\n13.87\n18.76\n23.25\nhy\n3.49\n6.98\n14.98\n24.4\nid\n18.59\n37.92\n28.67\n41.34\nis\n26.37\n32.45\n27.96\n32.39\nlt\n8.41\n21.03\n21.44\n31.39\nlv\n9.6\n19\n22.72\n31.87\npcm\n30.3\n36.42\n31.32\n39\nro\n11.9\n21.4\n22.79\n32.39\nsk\n17.49\n26.23\n24.56\n33.75\nsl\n14.1\n25.6\n23.54\n33.41\nug\n3.41\n0.23\n15.72\n14.88\nuk\n11.3\n13.04\n23.36\n29.78\nur\n21.96\n23.33\n23.57\n29.22\nAverage\n14.85\n22.84\n23.63\n32.22\nTable 8: mT5 and ByT5 performance averaged on the\n23 languages test sets after 10 epochs of fine-tuning on\nablation dataset.\ntion, and 1,000 examples from test. This prevents\nthe languages from having disproportionately more\ndata; where the original sets were smaller than\nthese targets, we used all available data. We dis-\nplay the language statistics in Table 7. Note that\nthese experiments are done on a preliminary dataset\nand not the final release version of XTREME-UP.\nB.3.5\nExperiment\nWe compared mT5 (Xue et al., 2021) and ByT5\n(Xue et al., 2022), two state-of-the-art multilingual\npre-trained LMs that are based on subwords and\nbytes respectively. The models were fine-tuned\nfor 10 epochs on autocomplete training set, more-\nover.\nWe used two metrics: top-3 word accu-\nracy (Acc@3) and chrF: character n-gram F-score\n(Popovi\u00b4c, 2015).\nB.3.6\nResults\nWe observe that ByT5 achieve better performance\nthan mT5 for both Acc@3 and chrF on the auto-\ncomplete task as it is displayed in Table 8. Also\nByT5 require less than half the time to fine-tune on\nthe training set (45 minutes) compared to mT5 (1\nhours and 30 minutes).\nB.3.7\nAnalyses\nBased on Acc@3 and chrf, the most challenging\nlanguages for mT5 are Eastern Armenian ((hy))\nand Uyghur (ug) respectively. Whereas Nigerian\nPidgin is the (pcm) and Scottish Gaelic are the eas-\niest languages. For ByT5, whether we consider\nAcc@3 or chrF, the most challenging language is\nUyghur, and the easiest language is Galician (gl).\nYet, these extremes only offer a qualitative com-\nparison of mT5 and ByT5. Next, we investigate\nfour questions around model performance: a) Do\nmT5 and ByT5 have the same cross-lingual gen-\neralization pattern? b) Do some languages yield\nhigher scores because autocompletion guesses the\nsame words? c) Do some languages yield higher\nscores because they have a smaller vocabulary in\ntheir corpora? d) Does similarity to the Latin alpha-\nbet impact models\u2019 performance? We test several\nhypotheses below, considering a relationship to be\nsignificant when the p-value is under 0.05.\nDo mT5 and ByT5 have the same cross-lingual\ngeneralization pattern?\nmT5 and ByT5 have\nthe same cross-lingual generalization pattern if the\ndifficulty to generalize to a new language is the\nsame for both models relatively to other languages.\nIn other words, if models\u2019 performance are ranked\nsimilarly, they share the same cross-lingual gener-\nalization pattern. To evaluate this hypothesis we\ncomputed the Spearman\u2019s rank correlation between\nmT5 and ByT5 Acc@3. We got a Spearman\u2019s rank\ncorrelation of 0.69 with p-value < 0.001. This\nmeans that the two models have a high degree of\nrelative agreement, in other words, if a new lan-\nguage is added, there is a high chance that the\nlanguage is going to be challenging or not for both\nmT5 and ByT5.\nDo some languages yield higher scores because\nautocompletion guesses the same words?\nIf our\ndataset in given language over-represents a word\nto predict, then the model might have misleadingly\ngood performance by always predicting the same\nword. This would mean that the dataset is not bal-\nanced with regards to the diversity of target words.\nA common way to model the diversity of a distri-\nbution of words is to compute its entropy, so we\ncomputed the the Pearson correlation between the\nentropy of the test set\u2019s target word distribution in\neach language and mT5 and ByT5 Acc@3. The\nentropy of a distribution of word is maximal if ev-\nery word is different, and it is minimal if it consist\non a single word. mT5 and ByT5 displayed corre-\nlation coefficients of \u22120.16 and 0.13 respectively\nwith p-value of 0.45 and 0.53 respectively. These\nresults show that there is insufficient evidence to\nconclude that there is a significant linear relation-\nship between target words diversity and model per-\nformance because the p-value is far above the 0.05\nsignificance threshold. Hence, target word diver-\nsity is not a good predictor of model performance\nvariability across languages.\nDo some languages yield higher scores because\nthey have a smaller vocabulary in their corpora?\nWe expect that languages with smaller corpora will\nbe easier to fine-tune on because of a smaller pre-\ndiction space. To test that hypothesis, we computed\nthe Pearson correlation between test set\u2019s vocab-\nulary size and mT5 and ByT5\u2019s Acc@3 for each\nlanguage. mT5 and ByT5 displayed correlation\ncoefficients of \u22120.29 and 0.13 respectively with\np-value of 0.17 and 0.54 respectively. Thus there\nis insufficient evidence to conclude that there is a\nsignificant linear relationship between vocabulary\nsize and model performance because the p-value is\nabove the 0.05 significance threshold.\nDoes similarity to the Latin alphabet impact\nmodels\u2019 performance?\nWe verify this hypothe-\nsis quantitatively by computing the similarity be-\ntween a) a Latin alphabet composed of the 26 let-\nters of the alphabet in lower and upper case and\nb) the alphabet of each language corresponding\nto all the characters in the test set except punctu-\nation and special characters. The similarity was\ncomputed with the Jaccard similarity coefficient\n(Jaccard, 1908), i.e. the ratio of number of unique\nitems in the intersection of both alphabets and the\nnumber of unique items in the union of both al-\nphabets. Moreover we used the same methodology\nas before and computed the Pearson correlation\nbetween the Jaccard similarity index and chrF as\nthis metric is more granular in models\u2019 character\nlevel performance. We observed a correlation of\n0.56 and 0.75 for mT5 and ByT5 respectively with\np-values < 0.01 respectively. It indicates that the\nsimilarity between the Latin alphabet and each lan-\nguage alphabet is significantly correlated to mT5\nand ByT5 chrF.\nB.3.8\nEvaluation and Discussion\nWhether we used a word level metric like Acc@3\nor a character level metric like chrF, ByT5 is more\naccurate at autocomplete than mT5. We also ob-\nLang.\nTasks\nLang.\nTasks\nam\nEthi\u2194Latn\nGuru\u2194Latn\nbn\nBeng\u2194Latn\npa\nArab\u2194Latn\ngu\nGujr\u2194Latn\nGuru\u2194Arab\nhi\nDeva\u2194Latn\nsd\nArab\u2194Latn\nkn\nKnda\u2194Latn\nsi\nSinh\u2194Latn\nml\nMlym\u2194Latn\nta\nTaml\u2194Latn\nmr\nDeva\u2194Latn\nte\nTelu\u2194Latn\nur\nArab\u2194Latn\nTable 9: Summary of the transliteration tasks.\nserve that these models generalize more easily to\nlanguages written in an alphabet closer to the Latin\nalphabet, ByT5 being more sensitive to the alpha-\nbet of the input language.\nB.4\nTransliteration\nB.4.1\nTask description\nTransliteration is the conversion of text in one writ-\ning system to another writing system, e.g., text\nwritten in the Devanagari script to the Latin script.\nIt differs from translation in that it does not change\nthe language content of the text, just the script.\nMany languages are written in multiple scripts, and\nthe current task involves transliterating whole sen-\ntences, not just isolated terms, from one script to\nanother.\nB.4.2\nData Creation and Annotation process\nMost of the data for the task comes from the ro-\nmanized full-string subset of the Dakshina dataset\n(Roark et al., 2020), in which 10,000 Wikipedia\nsentences written in the native scripts of the 12\nlanguages were human-romanized by native speak-\ners, resulting in parallel sentences in the native\nand Latin scripts.21 Two 10,000 sentence additions\nwere made to this data for the current transliteration\ntask: Amharic Wikipedia sentences were similarly\nmanually romanized by native speakers; and the\nPunjabi sentences from the Dakshina dataset, orig-\ninally written in the Gurmukhi (Brahmic) script,\nwere manually transliterated by native speakers to\nthe Shahmukhi (Perso-Arabic) script.\nB.4.3\nData Preparation\nThe resulting collection allows for overall 30 tasks\nconverting between various scripts. These are sum-\nmarised in Table 9 where, for each language in-\ndicated by the BCP-47 code (Phillips and Davis,\n21In the Dakshina distribution, the parallel sentences can\nbe found in files named LANG.romanized.rejoined.tsv,\nwhere LANG is a BCP-47 language code.\n2009), the corresponding transliteration tasks are\nshown for scripts indicated by their ISO-15924\ncodes (ISO, 2004).\nAll the native script data was normalized using\nUnicode NFC (Whistler, 2021). The data was then\nfurther transformed using language-specific visual\nnormalization for Brahmic and Perso-Arabic writ-\ning systems using the Nisaba script normalization\nlibrary (Johny et al., 2021; Gutkin et al., 2022).\nBoth NFC and visual normalization operations pre-\nserve visual invariance of the input text, with visual\nnormalization handling many ambiguous cases that\nfall outside the scope of standard NFC.\nB.4.4\nData Statistics\nFor each task, we establish 2,000 training sen-\ntences, 2,000 development set sentences, and close\nto 6,000 test sentences. Training data for any pre-\ntrained models used in the task cannot include the\nDakshina dataset. Since this is a contextual few-\nshot transliteration benchmark, we do not provide\nthe romanization lexicons that were released in the\nDakshina dataset along with the full sentence ro-\nmanizations.\nOur few-shot contextual transliteration task\ncovers 13 languages from 3 language families\n(Indo-Aryan, Dravidian and Semitic), all but one\n(Amharic) from South Asia.\nB.4.5\nDirectionality and Evaluation\nAmbiguity\nOne difference between romanization in these lan-\nguages and transliteration in the opposite direction\n(from the Latin script to the native script) is that\nnone of the languages in the benchmark have an or-\nthography in the Latin script, i.e., there is no single\ncorrect spelling in the Latin script for these lan-\nguages. Rather, individuals tend to provide a rough\nphonetic transcription of the sentences using the\nLatin script. As a result, word identity may be dif-\nficult to achieve (hence high word-error rate), but\nstring similarity should be relatively high between\nquality romanizations hence we use character-error\nrate to evaluate the transliterations. The ability to\nproduce romanizations automatically has several\nkey use cases, including simulation of parallel data\nfrom mono-script language samples, and for mul-\ntilingual modeling of languages that use different\nscripts. For that reason, we include both directions\nin the benchmark.\nB.4.6\nExperimental Setup\nPreviously Xue et al. (2022) performed ByT5 fine-\ntuning and evaluation of transliteration and ro-\nmanization directions separately on single-word,\nrather than full-sentence, data from vanilla Dak-\nshina dataset. In this benchmark we remove the\nseparation into transliteration and romanization by\nrequiring all tasks to be fine-tuned jointly. In order\nto achieve this, during all stages of training, devel-\nopment and testing a special code is prepended to\nthe input feature strings for each task. This task\ncode indicates that the input features correspond\nto the conversion from writing system Source to\nwriting system Target for a language lang. It is\nencoded as a string \u201clang_Source_Target\u201d. For\nexample, for Punjabi (pa) conversion from Shah-\nmukhi (Arab) to Gurmukhi (Guru) writing systems,\nthe task code is \u201cpa_Arab_Guru\u201d.\nIn the default setup we jointly fine-tune the 30\ntransliteration tasks using mT5 and ByT5 models\nin Small, Base and Large configurations that corre-\nspond to around 300M, 582M and 1.2B parameters,\nrespectively (Xue et al., 2021, 2022). Fine-tuning\nuses 10K training steps with a batch size of 128.\nWe used Google TPU-v3 accelerators (Kumar et al.,\n2019) for fine-tuning all the configurations apart\nfrom ByT5 Large for which a more powerful TPU-\nv4 (Pope et al., 2022) was necessary.\nB.5\nMachine Translation\nB.5.1\nData Card\nBasic Info\n1. Original datset name: FLORES-101\n2. Repository:\nhttps://github.com/\nfacebookresearch/flores/tree/main/\nflores200\n3. Paper: Goyal et al. (2022)\n4. Point of Contact (original version): NLLB\nTeam (flores@fb.com)\nWhy is this dataset part of XTREME-UP?\nMa-\nchine translation is an important tool for expanding\nlanguage coverage for natural language process-\ning tools. FLORES-101 is a high-quality, highly-\nmultilingual dataset.\nData Fields\n1. input: the source sentence, which is always\nEnglish (string)\n2. target: the target-language translation of the\nsource sentence (string)\nData Example\n{\"input\":\n\"<2xh>\nLocal\nmedia\nreports\nan\nairport\nfire\nvehicle\nrolled over while responding.\", \"target\":\n\"Oonondaba\nbasekuhlaleni\nbxele\nukuba\nisithuthi somlilo sesitishi senqwelomoya\nsiye saphethuka sisazama ukunceda.\"}\nLanguages\nIncluded in XTREME-UP release\n(93): Afrikaans (af), Amharic (am), Arabic (ar),\n(Eastern) Armenian (hy), Assamese (as), (North)\nAzerbaijani (az), Belarusian (be), Bengali (bn),\nBosnian (bs), Bulgarian (bg), Burmese (my), Cata-\nlan (ca), Cebuano (ceb), Central Kurdish (ckb),\nChinese (zh), Croatian (hr), Czech (cs), Dan-\nish (da), Dutch (nl), Estonian (et), Finnish (fi),\nFrench (fr), Fula (ff), Galician (gl), Georgian\n(ka), German (de), Greek (el), Gujarati (gu),\nHausa (ha), Hebrew (he), Hindi (hi), Hungarian\n(hu), Icelandic (is), Igbo (ig), Indonesian (id),\nIrish (ga), Italian (it), Japanese (ja), Javanese\n(jv), Kannada (kn), Kazakh (kk), Khmer (km), Ko-\nrean (ko), Kyrgyz (ky), Lao (lo), Latvian (lv), Lin-\ngala (ln), Lithuanian (lt), (Lu)Ganda (lg), Lux-\nembourgish (lb), Macedonian (mk), Malay (ms),\nMalayalam (ml), Maltese (ml), Maori (mi), Marathi\n(mr), Mongolian (mn), Nepali (ne), Pedi (Sepedi)\n(Northern Sotho) (nso), Norwegian (no), Nyanja\n(Chichewa) (ny), Oriya (or), Oromo (om), Pashto\n(ps), Persian (fa), Polish (pl), Portuguese (pt),\nPunjabi (pa), Romanian (ro), Russian (ru), Serbian\n(sr), Shona (sn), Sindhi (sd), Slovak (sk), Slove-\nnian (sl), Somali (so), Spanish (es), Swahili (sw),\nSwedish (sv), Tagalog (tl), Tajik (tg), Tamil (ta),\nTelugu (te), Thai (th), Turkish (tr), Ukrainian\n(uk), Urdu (ur), Uzbek (uz), Vietnamese (vi),\nWelsh (cy), Xhosa (xh), Yoruba (yo), Zulu (zu).\nEvaluated in benchmark (39): Amharic (am),\n(Eastern) Armenian (hy), Assamese (as), (North)\nAzerbaijani (az), Burmese (my), Central Kurdish\n(ckb), Gujarati (gu), Hausa (ha), Icelandic (is),\nIgbo (ig), Irish (ga), Javanese (jv), Kannada (kn),\nKhmer (km), Kyrgyz (ky), Lao (lo), Lingala (ln),\n(Lu)Ganda (lg), Luxembourgish (lb), Macedo-\nnian (mk), Malayalam (ml), Mongolian (mn), Nepali\n(ne), Pedi (Sepedi) (Northern Sotho) (nso), Nyanja\n(Chichewa) (ny), Oromo (om), Pashto (ps), Punjabi\n(pa), Shona (sn), Sindhi (sd), Somali (so), Swahili\n(sw), Tajik (tg), Telugu (te), Welsh (cy), Xhosa\n(xh), Yoruba (yo), Zulu (zu).\nData Statistics\n50% of the FLORES-101 dev\nsplit was reserved for training and the remainder\nfor validation.\nThe original devtest split was\nunchanged and reserved for testing. This results\nin 499/498/1012 sentence pairs for train/valida-\ntion/test, respectively.\nDataset Curators\nThe original dataset was cu-\nrated by the NLLB (No Language Left Behind)\nTeam (flores@fb.com).\nThe version included\nin XTREME-UP was curated by Parker Riley\n(prkriley@google.com) and Isaac Caswell (icas-\nwell@google.com).\nCuration Rationale\nThe original FLORES-101\ndataset was created to be able to evaluate machine\ntranslation models in many languages. The version\nreleased in XTREME-UP was created to focus on\nlow-resource languages and provide an in-domain\ntrain split along with validation and test splits, all\nof sizes in line with other tasks in XTREME-UP.\nData Sources\nThe source data (selected by the\nNLLB Team) comes from Wikinews, Wikijunior,\nand Wikivoyage.\nDataset Creation\nDetails of the creation of the\noriginal dataset are available in the original publi-\ncation (Goyal et al., 2022).\nChanges to the Original Dataset for XTREME-\nUP\nThe version of the dataset in XTREME-UP\nonly has the source and target strings, removing\nadditional metadata. We also include 93 of the\noriginal 100 non-English languages (the subset sup-\nported by Google Translate). Of these, only 39 are\nused for official evaluation.\nB.6\nQuestion Answering and Retrieval\nB.6.1\nData Card\nBasic Info\n1. Original datset names: TyDi QA, XOR-TyDi\nQA\n2. Additional cross-lingual data was collected\nas part of XTREME-UP, following similar\nmethodology\nWhy is this dataset part of XTREME-UP?\nQuestion answering enables information access.\nData Fields\n1. question: a question in the target language\n(string)\n2. title: the title of the evidence passage \u2014 target\nlanguage for in-language setting, English for\ncross-language setting (string)\n3. passage: the evidence passage, which might\ncontain an answer to the question \u2014 target\nlanguage for in-language setting, English for\ncross-language setting (string)\n4. answer: the answer (if any) to the question\n(string)\nData Example\nSee Table 2.\nLanguages\nSee Table 5.\nData Statistics\nSee Table 1.\nData Sources\nEvidence text was sourced from\nWikipedia.\nDataset Creation\nDetails of the creation of the\noriginal dataset are available in the original TyDi\nQA and XOR QA publications.\nB.7\nNamed Entity Recognition (NER)\nDataset and task description\nThe dataset con-\ntains processed data from MasakhaNER (Adelani\net al., 2021) and MasakhaNER 2.0 (Adelani et al.,\n2022). Both datasets were created by Masakhane22.\nWhy is this dataset part of XTREME-UP?\nNamed entity recognition is a fundamental task\nin natural language processing. The MasakhaNER\ndatasets are high-quality multilingual datasets that\nprovide data in 20 African languages. The data\nis human-annotated and thus higher quality than\nautomatically collected NER datasets.\nLanguages and ISO 639-3 codes\nBambara\n(bam), Ghom\u00e1l\u00e1\u2019 (bbj), \u00c9w\u00e9 (ewe), Igbo (ibo), Kin-\nyarwanda (kin), Luganda (lug), Luo (luo), Mossi\n(mos), Naija (pcm), Chichewa (nya), chiShona\n(sna), Kiswahili (swa), Setswana (tsn), Akan/Twi\n(twi), Wolof (wol), isiXhosa (xho), Yor\u00f9b\u00e1 (yor),\nisiZulu (zul)\nChanges to the original datasets for XTREME-\nUP\nThe original MasakhaNER datasets are pro-\nvided in CoNLL format where each input sentence\nis already tokenized. This makes it difficult to\nevaluate NER models on natural text where tok-\nenization may often be messy and introduces a bias\ntowards word and subword-based models. To pro-\nvide a level playing field and to enable evaluation\n22https://www.masakhane.io/\nQuery\nStart playing Rihanna 's latest album\nParse\n[IN:PLAY_MUSIC [SL:MUSIC_ARTIST_NAME\n,\u2192 Rihanna] [SL:MUSIC_TYPE album] ]\nLinearized Query\nStart playing SL:MUSIC_ARTIST_NAME{\n,\u2192 Rihanna}'s SL:MUSIC_TYPE{album}\nFigure 2: Creation of a linearized query from the actual\nquery and its parse for semantic parsing.\nof NER models on natural data data, we process the\ndata in order to align the token-level annotations\nwith byte-level spans in the original pre-tokenized\ntext. For the NER task, we provide the original pre-\ntokenized text as input to the model. Hausa and\nFon subsets of the original data were excluded as\nmatching with the unlabeled source data revealed\nannotation artefacts in both language subsets.\nB.8\nSemantic parsing\nB.8.1\nTask description\nSemantic parsing is the task of mapping a natural\nlanguage utterance to a logical form or a structured\ninterpretation that can be executed by a system such\nas a virtual assistant. For XTREME-UP, we adapted\nthe MTOP (Li et al., 2021) test dataset to 15 lan-\nguages, and to 3 code-switched Indic languages.\nThe original MTOP data was published by Face-\nbook and covers 6 languages across 11 domains,\n117 intents and 78 slots.\nB.8.2\nData creation\nIn this section, we describe the two processes\nused to extend the MTOP instances: the first in-\nvolves translation and localization with profes-\nsional translators and the second code-switching\nusing a language model and verification by human\nannotators.\nIn both processes, we perform a linearization\nstep of the query and parse. Given an English\nutterance from the MTOP English test set and the\ncorresponding slot information (slot names each\nwith start and end bytes), we add slot tags around\ncorresponding tokens in the query (Figure 2).\nTranslating MTOP to 15 languages:\nWe take\nthe bracketed versions of the slot-tagged English\nsentences from MTOP and we create translations\nand localization tasks to be carried out by profes-\nsional translators. We ran two pilots on a small\nsample of the data to gather feedback and improve\nthe annotation guidelines. The translators had to\ntranslate the original utterances to a given target\nlanguage, while keeping the brackets around slot\nvalue translations and localizing those where possi-\nble. Once the pilots were completed without issues,\nwe scaled the tasks to the full test set.\nWe carried out manual inspections on samples of\nthe data to check if translation and localization was\nhappening correctly, and a set of automatic checks\non the full data to ensure that slots were matching\nbetween original and translated utterances. Data\nwas sent back to annotators until all the issues were\nfixed.\nCode-switching MTOP to 3 Indic languages:\nWe use PaLM to convert the linearized query into\na code-mixed query using few-shot prompting. We\nexperimented with different discrete prompt de-\nsign strategies and selected the best prompts after\na qualitative evaluation on a small held-out set (11\nexamples) covering all 11 domains. Specifically\nwe experimented with three designs.\n\u2022 Naive prompting. The prompt contains (a)\nthe task description followed by a set of ex-\namples consisting of (b) the original English\nlinearized query and (c) the corresponding\ncode-mixed version.\n\u2022 Parallel sentence prompting. In this case,\nthe prompt contains (a) the task description,\n(b) the original English linearized query, and\nalso (c) the target translated query (obtained\nwith Google translate) and (d) the correspond-\ning code-mixed query.\n\u2022 Parallel reordered sentence prompting.\nSimilar to the previous, however, target trans-\nlated queries are human written.\nWe observed that the Parallel sentence prompt-\ning was producing higher quality utterances, with\n7/11 correct conversions for Hindi-English. 6/11\nfor Bengali-English, and 8/11 for Tamil-English.\nWe used this strategy to design prompts with the\nhelp of native speakers of those languages. We\nselected 21 sentences from the training split for\ncreating corresponding exemplars for the prompts.\nWith the latter, we performed few-shot prompting\nwith the 64b PaLM model and converted the test\nsplit of MTOP to a code-switched corpora.\nmT5\nByT5\nLanguage\nbase\nlarge\nbase\nlarge\nam\n20.01\n26.47\n33.41\n25.60\nbe\n27.82\n37.52\n46.72\n37.36\nbn\n29.06\n37.66\n45.07\n35.69\nde\n33.71\n39.96\n45.34\n37.93\nde (loc)\n33.31\n40.58\n45.81\n38.20\nen\n34.09\n40.39\n49.50\n39.52\nes\n34.95\n41.52\n48.73\n39.29\nfi\n26.63\n35.74\n46.80\n37.17\nfr\n33.72\n40.29\n48.97\n39.84\nha\n21.84\n27.60\n42.07\n29.98\nhi\n27.89\n37.59\n42.26\n35.42\nhu\n25.87\n33.47\n43.82\n35.98\nja\n28.71\n33.68\n45.23\n35.90\npt_br\n33.98\n39.12\n47.90\n39.50\nru\n34.44\n41.36\n48.58\n42.80\nsw\n24.06\n30.25\n39.96\n32.09\nta\n25.03\n33.20\n43.31\n31.41\nth\n23.81\n34.35\n43.80\n35.30\ntr\n27.44\n36.44\n44.58\n36.63\nyo\n14.52\n16.30\n30.39\n18.44\nzu\n18.73\n26.79\n36.96\n27.49\nAverage\n27.6\n34.78\n43.77\n34.84\nTable 10: Semantic Parsing: Exact Match (EM) accura-\ncies of mT5 and ByT5 models of different sizes trained\nmultilingually on few-shot data. We report accuracies\non all languages.\nHuman annotators then had to check the PaLM\ngenerated data for the presence of code-mixing\nand for the labeling to be consistent between the\noriginal query and the code-mixed version. The\nannotators were instructed to fix the automatically\ngenerated data whenever they found such issues.\nB.8.3\nData structure and statistics\nTo create the training, validation and testing splits\nfor MTOP, we start from the English test set and\nremove intents with less than 10 examples. This\nleaves us with 53 intents and a maximum of 4,223\nexamples for each language (some original MTOP\nlanguages may have less examples, while our code-\nswitched data may have more due to multiple para-\nphrases).\nFor each intent, we randomly select training ex-\namples such that each slot is covered by at least one\nexample, for a minimum of 5 examples. We end up\nwith training, development and test sets containing\nrespectively a maximum of 285, 239, and 3,669\ninstances for each language.\nB.8.4\nExperiments\nWe fine-tune mT5 (Xue et al., 2021) and ByT5\n(Xue et al., 2022) in their base and large configura-\ntions on the multilingual training data we collected.\nmT5\nByT5\nLanguage\nbase\nlarge\nbase\nlarge\nbn\n10.72\n11.78\n22.69\n16.25\nhi\n16.05\n18.48\n25.03\n17.69\nta\n16.98\n19.71\n26.21\n19.27\nTable 11: Semantic Parsing: Exact Match (EM) accura-\ncies of mT5 and ByT5 models of different sizes trained\nmultilingually on few-shot data. Here the multilingual\ntraining data includes three code-switched Indic lan-\nguages and we report EM for such languages.\nTask\nIn-context learning example\nTranslation\nTranslate between English and Afrikaans.\nEnglish: [INPUT]\nAfrikaans: [TARGET]\nASR\nCorrect the ASR output in Afrikaans.\nASR Afrikaans output: [INPUT]\nCorrected: [TARGET]\nNER\nTag the named entities in the Swahili text\nas person (PER), organization (ORG),\nlocation (LOC), and date (DATE). Use $$\nas delimiter.\nSwahili text: [INPUT]\nNamed entities: [TARGET]\nAutocomplete\nComplete the Urdu sentence. Write the next\nword or finish the last word if it is incomplete.\nUrdu sentence: [INPUT]\nCompletion: [TARGET]\nTable 12: In-context learning examples.\nTable 10 contains the Exact Match accuracies of\na multilingual model trained on data from all lan-\nguages but the code-switched sets. Table 11 con-\ntains the results of a model that includes the code-\nswitched sets. From both tables, we can see that\nByT5-base is more accurate then the other models,\neven compared with the larger ones. This surpris-\ning result confirms similar findings on word-level\ntasks reported by Xue et al. (2022) and Nicosia and\nPiccinno (2022). We expect mT5 to catch up with\nByT5 at larger sizes.\nC\nIn-context learning examples\nWe show in-context learning examples for a selec-\ntion of tasks in Table 12. Each example consists of\na general instruction and prefixes for the input and\ntarget, which are repeated for each exemplar.\n"
  },
  {
    "title": "RecurrentGPT: Interactive Generation of (Arbitrarily) Long Text",
    "link": "https://arxiv.org/pdf/2305.13304.pdf",
    "upvote": "1",
    "text": "RECURRENTGPT:\nInteractive Generation of (Arbitrarily) Long Text\nWangchunshu Zhou\u2217*\nYuchen Eleanor Jiang*\nPeng Cui\nTiannan Wang\nZhenxin Xiao Yifan Hou\nRyan Cotterell\nMrinmaya Sachan\nETH Z\u00fcrich\n{wangchunshu.zhou, yuchen.jiang, peng.cui}@inf.ethz.ch\nhugothebestwang@gmail.com, alanshawzju@gmail.com\n{yifan.hou, ryan.cotterell, mrinmaya.sachan}@inf.ethz.ch\nAbstract\nThe \ufb01xed-size context of Transformer makes GPT models incapable of generating\narbitrarily long text. In this paper, we introduce RECURRENTGPT, a language-\nbased simulacrum of the recurrence mechanism in RNNs. RECURRENTGPT is\nbuilt upon a large language model (LLM) such as ChatGPT and uses natural lan-\nguage to simulate the Long Short-Term Memory mechanism in an LSTM. At each\ntimestep, RECURRENTGPT generates a paragraph of text and updates its language-\nbased long-short term memory stored on the hard drive and the prompt, respectively.\nThis recurrence mechanism enables RECURRENTGPT to generate texts of arbitrary\nlength without forgetting. Since human users can easily observe and edit the natu-\nral language memories, RECURRENTGPT is interpretable and enables interactive\ngeneration of long text. RECURRENTGPT is an initial step towards next-generation\ncomputer-assisted writing systems beyond local editing suggestions. In addition\nto producing AI-generated content (AIGC), we also demonstrate the possibility\nof using RECURRENTGPT as an interactive \ufb01ction that directly interacts with\nconsumers. We call this usage of generative models by \u201cAI as Contents\u201d (AIAC),\nwhich we believe is the next form of conventional AIGC. We further demonstrate\nthe possibility of using RECURRENTGPT to create personalized interactive \ufb01ction\nthat directly interacts with readers instead of interacting with writers. More broadly,\nRECURRENTGPT demonstrates the utility of borrowing ideas from popular model\ndesigns in cognitive science and deep learning for prompting LLMs. Our code is\navailable at https://github.com/aiwaves-cn/RecurrentGPT and an online\ndemo is available at https://www.aiwaves.org/recurrentgpt.\n1\nIntroduction\nLarge Language Models (LLMs) [1\u20135] such as ChatGPT have proven to be highly effective tools for\nassisting with various routine writing tasks, including emails and blog posts. Nevertheless, due to the\n\ufb01xed-size context design inherent in the Transformer [6] architecture, it is unfeasible to generate long\ntexts (e.g., novels) solely by prompting LLMs. In contrast, recurrent neural networks (RNNs) [7, 8],\nin theory, possess the capacity to generate sequences of arbitrary length, thanks to their recurrence\nmechanism: RNNs maintain a hidden state that undergoes updates at each time step, employing the\ncurrent time step\u2019s output as the input for the subsequent time step. In practice, however, RNNs suffer\nfrom the problem of vanishing and exploding gradients and are hard to scale up.\n\u2217Equal Contribution\nPreprint. Work In Progress.\narXiv:2305.13304v1  [cs.CL]  22 May 2023\n\ud835\udc89\ud835\udc95\"\ud835\udfcf\n\ud835\udc89\ud835\udc95\n\ud835\udc99\ud835\udc95\"\ud835\udfcf\n\ud835\udc99\ud835\udc95\n\ud835\udc99\ud835\udc95\n\ud835\udfcf\n\ud835\udc99\ud835\udc95\n\ud835\udfd1\n\ud835\udc99\ud835\udc95\n\ud835\udfd0\nllm\nHuman/\nHuman Simulator\n\ud835\udc6a\ud835\udc95\n\ud835\udc76\ud835\udc95\n\ud835\udc6a\ud835\udc95\"\ud835\udfcf\n\ud835\udc76\ud835\udc95\n+\n\ud835\udc89\ud835\udc95&\ud835\udfcf\n\ud835\udc99\ud835\udc95&\ud835\udfcf\n\ud835\udc6a\ud835\udc95&\ud835\udfcf\n\ud835\udc76\ud835\udc95&\ud835\udfcf\n+\n\ud835\udc89\ud835\udc95\"\ud835\udfd0\n\ud835\udc99\ud835\udc95\"\ud835\udfd0\n\ud835\udc6a\ud835\udc95\"\ud835\udfd0\n\ud835\udc76\ud835\udc95\"\ud835\udfcf\n+\n\ud835\udc89\ud835\udc95\n\ud835\udc6a\ud835\udc95\n\ud835\udc89\ud835\udc95\n\ud835\udc99\ud835\udc95\n\ud835\udc76\ud835\udc95\nOutput Content\nLong-Term Memory\nChosen Plan\n\ud835\udc99\ud835\udc95\n\ud835\udc8a\nCandidate Plan\nShort-Term Memory\n\ud835\udc76\ud835\udc96\ud835\udc95\ud835\udc91\ud835\udc96\ud835\udc95 \ud835\udc6a\ud835\udc90\ud835\udc8f\ud835\udc95\ud835\udc86\ud835\udc8f\ud835\udc95 \ud835\udc76\ud835\udc95 : Elias spent days studying the artifact, trying to unlock its \nsecrets. He worked tirelessly, using all of his knowledge and skills to decipher the \nhieroglyphics and understand the technology. As he delved deeper into the \nartifact's mysteries, he began to realize that it was not of this world. It was a relic \nfrom a long-lost civilization that had once ruled the galaxy. Elias knew that he had \nstumbled upon something truly extraordinary, but he also knew that it was \ndangerous.\nAs he worked on the artifact, Elias began to notice that the creatures and the \naliens were acting strangely. They seemed to be watching him, studying him, and \nwaiting for something. Elias couldn't shake the feeling that something was not \nright. He knew that he had to be careful, but he also knew that he couldn't stop \nnow. He was too close to unlocking the artifact's secrets.\n\u2026...\nC\ud835\udc21\ud835\udc28\ud835\udc2c\ud835\udc1e\ud835\udc27 \ud835\udc0f\ud835\udc25\ud835\udc1a\ud835\udc27 \ud835\udc99\ud835\udc95\"\ud835\udfcf : Elias, a renowned interstellar explorer, embarks on a new \nmission to explore a distant planet that has been recently discovered. The planet \nis believed to have a unique ecosystem and Elias is eager to uncover its secrets.\nShort-Term Memory \ud835\udc89\ud835\udc95 : Elias and his team discovered that the artifact they \nhad found was part of a larger network of artifacts scattered throughout the \ngalaxy. Elias and his team were determined to find the other artifacts and stop \nthe powerful entity that sought to use them for their own gain\u2026.\nLong-Term Memory \ud835\udc6a\ud835\udc95 : \n1. Elias sat in the cockpit of his spaceship, staring out at the vast expanse of space before him. He had been traveling for months, but the excitement of discovering a new \nplanet kept him going. As he approached the planet, he couldn't help but feel a sense of anticipation. What wonders lay ahead? \n2. The planet was unlike anything Elias had ever seen before. The sky was a deep shade of purple, and the trees were a vibrant shade of blue. As he stepped out of his \nspaceship, he was greeted by a warm breeze that carried the scent of exotic flowers. Elias couldn't wait to explore this new world.\n3. As Elias continued to explore the planet, he couldn't help but feel a sense of wonder and excitement. The creatures he encountered were unlike anything he had ever seen \nbefore. Some were small and furry, while others were massive and had multiple limbs. Elias was fascinated by their unique features and behavior. He spent hours observing \nthem, taking notes, and sketching them in his journal.\n4. As he ventured deeper into the planet, Elias stumbled upon ancient ruins that suggested that there was once an advanced civilization on the planet. The ruins were unlike \nanything he had ever seen before. The architecture was intricate and beautiful, and the technology was far more advanced than anything he had ever encountered.\n\u2026...\nC\ud835\udc1a\ud835\udc27\ud835\udc1d\ud835\udc22\ud835\udc1d\ud835\udc1a\ud835\udc2d\ud835\udc1e \ud835\udc0f\ud835\udc25\ud835\udc1a\ud835\udc27 \ud835\udc99\ud835\udc95\n\ud835\udfcf : Elias and his team must race against time to find the next \nartifact before the rival team does. They travel to a planet that is home to a \ndangerous creature that guards the artifact. They must outsmart the creature \nand retrieve the artifact before the rival team arrives.\nC\ud835\udc1a\ud835\udc27\ud835\udc1d\ud835\udc22\ud835\udc1d\ud835\udc1a\ud835\udc2d\ud835\udc1e \ud835\udc0f\ud835\udc25\ud835\udc1a\ud835\udc27 \ud835\udc99\ud835\udc95\n\ud835\udfd0 : Elias and his team discover that the next artifact is located \non a planet that is home to a peaceful alien race. However, they soon realize \nthat the alien race is being threatened by a group of space pirates. They must \nteam up with the aliens to defeat the space pirates and retrieve the artifact.\nC\ud835\udc1a\ud835\udc27\ud835\udc1d\ud835\udc22\ud835\udc1d\ud835\udc1a\ud835\udc2d\ud835\udc1e \ud835\udc0f\ud835\udc25\ud835\udc1a\ud835\udc27 \ud835\udc99\ud835\udc95\n\ud835\udfd1 : Elias and his team receive a distress signal from a planet \nthat is being attacked by a powerful entity. They discover that the entity is \nafter an artifact that is located on the planet. Elias and his team must fight \nagainst the entity and retrieve the artifact before it's too late.\nTimestep \ud835\udc95 \u2212 \ud835\udfcf\nTimestep \ud835\udc95\nA\nA\nShort-Term Memory \ud835\udc89\ud835\udc95\"\ud835\udfcf :  As a renowned interstellar explorer, Elias had \ntraveled to countless planets and encountered unimaginable wonders\u2026\nFigure 1: Illustration of the RECURRENTGPT framework. RECURRENTGPT enables recurrent\nprompting with LLMs by simulating an RNN using natural language building blocks and de\ufb01nes\nthe recurrent computation graph with prompts.\nTo this end, a number of works [9\u201311] attempt to equip Transformers with an RNN-like recur-\nrence mechanism. While achieving promising results on long text modeling and generation, these\nrecurrence-augmented Transformers require substantial architectural modi\ufb01cations that have not been\nproven to scale well. The majority of current LLMs continue to employ the original Transformer\narchitecture with minimal alterations.\nIn this paper, we introduce RECURRENTGPT, a language-based simulacrum of the recurrence\nmechanism in RNNs. As illustrated in Figure 1, RECURRENTGPT replaces the vectorized elements\n(i.e., cell state, hidden state, input, and output) in a Long-short Term Memory RNN (LSTM) [8] with\nnatural language (i.e., paragraphs of texts), and simulates the recurrence mechanism with prompt\nengineering. At each timestep t, RECURRENTGPT receives a paragraph of text and a brief plan of\nthe next paragraph, which are both generated in step t \u2212 1. It then attends to the long-term memory,\nwhich contains the summaries of all previously generated paragraphs and can be stored on hard drives,\nand relevant paragraphs can be retrieved with semantic search. RECURRENTGPT also maintains\na short-term memory that summarizes key information within recent timesteps in natural language\nand is updated at each time step. RECURRENTGPT combines all aforementioned inputs in a prompt\nand asks the backbone LLM to generate a new paragraph, a short plan for the next paragraph, and\nupdates the long-short term memory by rewriting the short-term memory and appending the summary\nof the output paragraph to the long-term memory. These components are then re-used in the next\n2\ntime step, resulting in a recurrence mechanism for the generation process. With the language-based\nrecurrence mechanism, RECURRENTGPT alleviates the need for any architectural modi\ufb01cation and\ncan be integrated into any powerful LLM, making it capable of generating arbitrarily long text beyond\nthe \ufb01xed-size context window.\nIn addition to surpassing the \ufb01xed-size context limitation, RECURRENTGPT enhances the inter-\npretability of the recurrence mechanism in comparison to the vector-based recurrence mechanism\nemployed in RNNs. This improvement stems from the ability to observe the speci\ufb01c segments of\nlong-term memory that are attended to, as well as the manner in which short-term memory is updated,\nthrough a simple examination. More importantly, employing natural language as building blocks\nenables human engagement with RECURRENTGPT, allowing for the human manipulation of its\nmemories and plans for future generations. Human interaction also prevents RECURRENTGPT\nfrom deviating from desired behavior, a challenge commonly encountered with recent autonomous\nGPT-based agents such as AutoGPT2. Given that current state-of-the-art computer-assisted writing\nsystems [12, 13] primarily focus on localized editing suggestions and treat LLMs as black-boxes,\nwe believe RECURRENTGPT represents a step towards next-generation computer-assisted writing\nsystems for interactive long text generation that also offer interpretability.\nWe then extend the utilization of RECURRENTGPT beyond its role as a tool for producing AI-\ngenerated content (AIGC) by exploring its potential for direct interaction with consumers, rather than\nsolely with content creators. Speci\ufb01cally, we convert RECURRENTGPT to a personalized interactive\n\ufb01ction wherein it generates multiple prospective plans for the subsequent actions, allowing players\nto choose and explore the one that captures their interest. Moreover, in addition to selecting from\nmodel-generated plans, players possess the capability to devise their own plans. Such a capacity is\nunattainable within conventional interactive \ufb01ctions, as the narratives and options are conventionally\npredetermined. We denote this new paradigm as \u201cAI As Content\u201d, signifying the utilization of\ngenerative AI as a medium that actively interacts with consumers, instead of being con\ufb01ned to the\nrole of a mere tool for content creators. Through RECURRENTGPT, we perceive a preliminary\nstride towards a future where AI models will eventually become collaborative partners in our creative\nendeavors.\nIn our experiments, we build RECURRENTGPT upon ChatGPT and \ufb01nd that exhibits the capability to\nautonomously generate remarkably extensive texts, spanning thousands of tokens, while maintaining\nboth coherency and engagement. In stark contrast, vanilla ChatGPT is constrained to generating\na few hundred of tokens before encountering issues such as repetitive content or a decline in\ncoherence.Moreover, RECURRENTGPT can help human writers produce arbitrarily long text with\nease, reducing much of the human efforts required for writing long creative texts such as novels. The\ncontributions of this paper can be summarized as follows:\n\u2022 We propose RECURRENTGPT, a language-based simulacrum of the recurrence mechanism\nin RNNs that mitigates the \ufb01xed-size context limitation of LLMs such as ChatGPT.\n\u2022 We show that RECURRENTGPT can generate very long texts either on its own or serve as\nan interactive writing assistant, helping human writers write arbitrarily long texts.\n\u2022 We introduce a new use case of generative AI that uses generative models to directly interact\nwith consumers of text, as opposed to the conventional practice that uses them as tools for\ncontent creation, by using RECURRENTGPT as a personalized interactive \ufb01ction for content\ncuration.\nFurthermore, it is important to underscore that RECURRENTGPT illustrates the possibility of drawing\ninspiration from well-established model designs in the \ufb01elds of cognitive science and deep learning,\nwith the aim of generating long form text via prompting of LLMs.\n2\nRECURRENTGPT\nWe describe RECURRENTGPT in detail in this section. RECURRENTGPT is a natural language-based\ncounterpart of the recurrence mechanism in RNNs. RECURRENTGPT simulates an LSTM by (1)\nmodeling all vector-based components in an LSTM, including input vectors xt, output vectors yt,\nhidden states ht, and cell states ct, with natural language; (2) modeling the recurrent computation\n2https://github.com/Significant-Gravitas/Auto-GPT\n3\ngraph in an LSTM with natural language prompts, and (3) replacing the trainable parameters in RNNs\nby a frozen LLM. In theory, the backbone of RECURRENTGPT can be any LLM or text-to-text\nmodel, we opt for ChatGPT because of its capability and popularity.\nFormally, we de\ufb01ne RECURRENTGPT as a computational function parametrized by an LLM with\nparameter \u03b8 and a prompt template P. Recall that the recurrent computation graph of an LSTM can\nbe summarized as:\not+1, ht+1, ct+1 = LSTM(xt+1, ht, ct, \u03b8)\n(1)\nwhere \u03b8 denotes the model parameters, xt+1 equals to ot, and ht, ct are the long/short-term memories\nat timestep t, respectively.\nBy analogy, the recurrence mechanism in our model can be expressed by:\not+1, xt+1, ht+1, ct+1 = RECURRENTGPT(ot, xt, ht, ct, \u03b8, P)\n(2)\nwhere ot, xt, ht, and ct denote the natural language-based building blocks including content, plan,\nshort-term memory, and long-term memory, at time step t, respectively. Here xt+1 does not equal ot\nand is instead separately generated, which is different from conventional RNNs. We \ufb01rst describe each\nbuilding block in RECURRENTGPT and then present how our prompt P enables RECURRENTGPT\nto recurrently generate arbitrarily long texts.\n2.1\nLanguage-based Building Blocks\nInput/Output\nThe input and output of RECURRENTGPT at each timestep include a paragraph\nof text that gets appended to the \ufb01nal text produced and an outline for the next paragraph to be\ngenerated. We refer to these two as the \u201ccontent\u201d and \u201cplan\u201d, respectively. As illustrated in Figure 1,\ncontents typically consist of 200-400 words and should be mostly ready for reading. Whereas plans\nare outlines for the next content and typically consist of 3-5 sentences. At each timestep, the content\nand plan generated in the previous timestep are used as input to RECURRENTGPT, allowing recurrent\ncomputation. RECURRENTGPT is designed to produce plans in addition to contents as allowing\nusers to read and edit plans increases interpretability and facilitates human-computer interaction.\nLong-Short Term Memory\nSimilar to an LSTM, RECURRENTGPT maintains long-short term\nmemory across timesteps. As illustrated in Figure 1, long-term memory summarizes all previously\ngenerated contents to minimize information lost when generating long texts. Since the generated\ncontent can be arbitrarily long and cannot \ufb01t in the context size of LLMs, we implement the long-term\nmemory in RECURRENTGPT with a VectorDB approach by embedding the content generated in each\ntimestep with sentence-transformers [14]. This approach enables RECURRENTGPT to store even\nlonger memory compared to previous memory-based Transformers [9, 11] as it can store memory in\ndisk space instead of GPU memory. This can be important in several use cases where the users may\nnot have high-end GPUs in their devices.\nShort-term memory, on the other hand, is a short paragraph of texts summarizing key information\nacross recent timesteps. The length of the short-term memory is controlled to 10-20 sentences so that\nit can \ufb01t into the prompt and can be updated by the LLM backbone. By combining long-short term\nmemory, RECURRENTGPT can maintain coherence with recently generated content and also recall\nkey information that was generated long before. This is impossible with vanilla LLMs because they\ncan only take a few previously generated texts in the input.\nRECURRENTGPT can be initialized using a simple prompt that instructs the LLM to generate\nthe aforementioned components with texts specifying the topic of the novel and other background\ninformation. When using RECURRENTGPT to continue writing a novel, users can write down (or\nprompt ChatGPT to generate) a short-term memory and an initial plan.\n2.2\nLanguage-based Recurrent Computation\nWhile RNNs achieve recurrent computation by implementing a feedback loop in the computation\ngraph, RECURRENTGPT relies on prompt engineering to simulate the recurrent computation scheme.\nAs illustrated in Figure 1, RECURRENTGPT simulates the computation graph in RNNs with a prompt\ntemplate, which is presented in Figure 1 in the Appendix, and some simple Python code3.\n3We present the prompt in Appendix A due to space constraints.\n4\nAt each timestep, RECURRENTGPT constructs the input prompts by \ufb01lling the prompt template with\ninput content/plan and its internal long-short term memory. In particular, since the long-term memory\ncannot \ufb01t into the context size, we use the input plan as the query to perform a semantic search over\nthe VectorDB-based long-term memory and \ufb01t a few most relevant contents into the prompt. The\nprompt then instructs the LLM backbone to generate new contents, plans, and updated short-term\nmemory. As illustrated in Figure 1 in the Appendix, our prompt encourages the LLM to update\nthe short-term memory by discarding information that is no longer relevant and adding useful new\ninformation while maintaining its length within a range so that it can always \ufb01t in the context size. It\nis noteworthy that we prompt the LLM to generate multiple (e.g., 3 in our experiments) plans. This\nimproves the diversity of outputs and makes human-computer interaction more friendly by allowing\nhuman users to select the most suitable plan. We also give users the option to write plans on their\nown if none of the generated plans is desirable. To make RECURRENTGPT capable of generating\nlong texts autonomously without human intervention, we add a prompt-based human simulator to\nselect a good plan and revise it for the next timestep.\n2.3\nInteractive Long Text Generation with RECURRENTGPT\nWhile RECURRENTGPT can generate long texts on its own with the recurrence mechanism, its\nlanguage-based computation scheme offers unique interpretability and interactivity. Compared to\nconventional computer-assisted writing systems that use language models as black boxes and only\ngive next phrase/sentence suggestions, RECURRENTGPT enjoys the following advantages:\n\u2022 It is more ef\ufb01cient at reducing human labor because it makes paragraph/chapter-level\nprogresses instead of local writing suggestions.\n\u2022 It is interpretable because users can directly observe its language-based internal states.\n\u2022 It is interactive because humans can edit their building blocks with natural language.\n\u2022 It is customizable because users can easily modify the prompts to customize the model\naccording to their own interests (e.g., the style of output texts, how much progress to make\nfor each timestep, etc.)\nIn addition, human interaction can also help correct accidental mistakes made by RECURRENTGPT\nwhen autonomously generating long texts and prevent error propagation, which is a major bottleneck\nfor long text generation.\n3\nExperiments\n3.1\nExperimental Settings\nTasks\nWe test the empirical effectiveness of RECURRENTGPT in this section. In particular, we\nevaluate RECURRENTGPT in three different settings including:\n\u2022 Autonomously generating long texts without human interaction.\n\u2022 Collaboratively generating long texts with a human writer\n\u2022 Directly interacting with text consumers as interactive \ufb01ctions.\nIn each of these tasks, we test with a diverse set of genres of novels including science \ufb01ction, romance,\nfantasy, horror, mystery, and thriller novels. To test the effectiveness of RECURRENTGPT for texts\nof different length, we generate novels of medium length (\u223c 3000 words) for horror, mystery, and\nthriller, and generate longer novels (\u223c 6000 words) for sci-\ufb01, romance, and fantasy.\nBaselines\nAlthough RECURRENTGPT is the \ufb01rst work on using LLMs to generate arbitrarily long\ntexts, we can still compare it against some reasonable baselines and ablated variants, as listed below:\n\u2022 Rolling-ChatGPT, a simple baseline that prompts ChatGPT to start writing a novel given a\ngenre of literature and some outlines or background settings, and then iteratively prompts\nChatGPT to continue writing after reaching the context length limit. This baseline is\nroughly equivalent to using a sliding context window trick for generating long texts with\nTransformers.\n5\n\u2022 RE3 [15] is a hierarchical long story generation baseline that \ufb01rst prompts an LLM to\ngenerate an outline for the story and then generates the story following the outline with\nsome re-ranking and re-writing pipelines. We re-implement it with ChatGPT to ensure a fair\ncomparison.\n\u2022 DOC [16] is the state-of-the-art long story generation baseline that improves RE3 with\noutline control. We re-implement DOC by replacing OPT-175B [17] with ChatGPT and\nremoving the detailed controller, which is impossible to use because we do not have access\nto ChatGPT weights. In general, we \ufb01nd that our re-implementation results in slightly better\nquality because of the improvement on the backbone LLM.\nIt\u2019s noteworthy that in principle, both the baselines can not generate arbitrarily long texts while\nremaining coherent. This is because the Rolling-ChatGPT baseline forgets previously generated\ncontents very quickly. On the other hand, RE3 and DOC \ufb01xes the outline in the \ufb01rst stage, which\nlimits the overall length of the story to be generated.\nTable 1: Pair-wise comparison of RECURRENTGPT with baselines for 20 novels of different genres.\nResults in different comparisons are not comparable with each other. Bold indicates signi\ufb01cance with\np < 0.05.\nNovel genres\nSci-\ufb01\nRomance\nFantasy\n\u223c 6000 words\nInteresting \u2191\nCoherent \u2191\nInteresting \u2191\nCoherent \u2191\nInteresting \u2191\nCoherent \u2191\nRECURRENTGPT\n94.7\n86.5\n91.4\n84.8\n95.9\n85.1\nRolling-ChatGPT\n7.8\n14.3\n9.0\n18.2\n6.5\n13.7\nRECURRENTGPT\n68.3\n65.7\n71.4\n69.2\n63.8\n62.0\nRE3\n31.9\n28.5\n28.1\n25.3\n35.1\n33.8\nRECURRENTGPT\n66.1\n59.3\n77.2\n63.4\n61.0\n56.5\nDOC\n30.7\n38.1\n25.3\n29.8\n31.2\n40.3\nNovel genres\nHorror\nMystery\nThriller\n\u223c 3000 words\nInteresting \u2191\nCoherent \u2191\nInteresting \u2191\nCoherent \u2191\nInteresting \u2191\nCoherent \u2191\nRECURRENTGPT\n88.3\n84.9\n87.1\n82.0\n91.5\n82.7\nRolling-ChatGPT\n13.5\n17.1\n14.5\n20.1\n11.9\n17.7\nRECURRENTGPT\n64.1\n64.5\n66.8\n63.2\n61.0\n61.4\nRE3\n34.6\n30.2\n27.9\n28.8\n38.3\n37.9\nRECURRENTGPT\n65.8\n60.7\n72.1\n66.8\n60.2\n58.1\nDOC\n29.1\n39.7\n27.2\n25.6\n33.8\n37.0\nEvaluation Metrics\nFor evaluation, we follow Yang et al. [15] and conduct a human evaluation by\ncomparing RECURRENTGPT with the baselines according to two dimensions:\n\u2022 Interesting: How interesting are the generated novels for common readers?\n\u2022 Coherent: How well are the paragraphs organized and connected with each other?\nWe omit the \u201cquality\u201d or \u201chumanlike\u201d metrics following Yang et al. [16] since all baselines are based\non ChatGPT which can produce high-quality texts most of the time. We evaluate the compared\nmodels by pairwise comparison. Speci\ufb01cally, we give two novels (A and B, with random order)\ngenerated by different compared methods to human annotators with good English pro\ufb01ciency and\ninstruct them to label whether novel A or novel B is better, or they are indistinguishable, in terms\nof interestingness and coherence. Following the human evaluation settings in Yang et al. [16], we\nsample 20 generated novels for each genre and assign 3 annotators for each novel.\n3.2\nResults\nAs shown in Table 1, we \ufb01nd that RECURRENTGPT is favored by human readers for both interesting-\nness and coherence with a relatively large margin compared to both the rolling-window baseline and\nprior state-of-the-arts like RE3 and DOC. This con\ufb01rms our intuition that recurrent computation is im-\nportant for long text generation. The gap is larger for longer novels, which con\ufb01rms the advantage of\n6\nRECURRENTGPT as Interactive Writing Assistant\nGENERATED PLANS:\n1. Aria is a scientist who learns the project she\nis working on is a government scheme to \u2026\n2. Aria, a brilliant geneticist, is recruited by a\nprivate space exploration company for her \u2026\n3. Aria, a young and talented scientist, is\nrecruited by a prestigious research \u2026\nGENERATED STORY: Aria sat in her dimly lit\nlaboratory, surrounded by rows of beakers and\ntest tubes. She rubbed her tired eyes, trying to\nfocus on the data displayed on the screen in\nfront of her. The clock on the wall showed 2:47\nam, but she had lost track of time hours ago.\nShe had been working on the project for\nmonths, but something wasn't adding up. The\ncalculations didn't make sense. She leaned back\nin her chair and let out a deep sigh As she gazed\nout the window, she couldn't help but feel a\nsense of \u2026 [357 words left] \u2026.\nMEMORY: Aria, a scientist working on a project\nto send a spacecraft to a distant planet,\ndiscovers that the project is a cover-up for a\nsecret government program to create a group\nof transhuman beings. As she works late into\nthe night, she becomes increasingly aware of\nthe bleakness of the world outside her lab.\nWhen a man in a suit comes to speak with her,\nshe realizes that something is amiss.\nGENERATED PLANS:\n1. The man in the suit reveals that Aria has been\nchosen to be one of the first test subjects \u2026\n2. Aria goes on the run, knowing that the\ngovernment will stop at nothing to capture \u2026\n3. Aria demands answers from the man in the\nsuit and learns that the government has been \u2026\nGENERATED STORY: Aria and her group worked\ntirelessly to infiltrate the transhuman program.\nThey knew they were taking a great risk, but\nthey also knew that it was the only way to bring\nthe truth to light. Aria used her access to the\nprogram to plant a virus in their system, hoping\nto slow down their progress. She also started\ngathering as much information as possible on\nthe individuals involved in the project, hoping\nto find a weakness that they could exploit.\nMeanwhile, the rest of the group worked\n\u2026\n[281 words left] \u2026\nMEMORY: Aria, a scientist working on a project\nthat turns out to be a government cover-up to\ncreate a group of transhuman beings. Aria\ngathers information about the project and\nforms a small group of trusted friends to plan\ntheir next move. They decide to infiltrate the\ntranshuman program from within, plant a virus\nin their system, and plan to bring the truth to\nlight without getting caught.\nGENERATED PLANS:\n1. Aria\u2019s plan to infiltrate the transhuman\nprogram is discovered, and they are forced \u2026\n2. Aria and her group successfully infiltrate the\ntranshuman program and discover a shocking \u2026\n3.\nAria\u2019s\nplan\nto\nexpose\nthe\ntranshuman\nprogram is successful, but the consequence \u2026\nGENERATED STORY: The day of the protest\narrived, and Aria and her group were filled with\nnervous energy. They had spent countless hours\npreparing for this moment, but there was still\nso much at stake. As they made their way to the\nprotest site, they could feel the weight of the\nworld on their shoulders. The turnout was\nbetter than they had hoped, with people from\nall walks of life joining them in their fight. The\natmosphere was electric, and Aria felt a sense\nof hope wash over her as she looked out at \u2026\n[426 words left] \u2026\nMEMORY: Aria and her group infiltrated the\ntranshuman program, planted a virus in their\nsystem, and started gathering information. They\nformed alliances with other groups who shared\ntheir concerns and worked on planning a\nprotest. They gathered intel on the program and\nthe government's plans and became more\ndetermined than ever to make their voices\nheard.\nRECURRENTGPT as Interactive Fiction\nNow it\u2019s your time to make a choice:\nNow it\u2019s your time to make a choice:\nGENERATED STORY:\nYou\u2018re Dr. Alexei Nikolai, the world\u2019s foremost\nastro-archeologist, the man who had solved the\nenigma of the Sphinx\u2018s true age and deciphered\nthe mysterious Voynich manuscript. You\u2019re\nused to dealing with the odd and ancient, but\nnothing could have prepared you for what lay\nahead. \nThe first time you set foot on Mars, your heart\nbeats wildly in your chest, a mix of anticipation\nand anxiety. You\u2018re part of a handpicked team\nselected by NASA to investigate the ancient\nruins discovered beneath the Martian soil. The\nruins, according to the radar imagery, resemble\na long-lost civilization\u2019s architecture \u2026\nGENERATED STORY:\nNavigating through the alien city, you marvel at\nthe advanced technology and architecture.\nAmidst\nthe\nalien\nstructures,\nthere's\nan\nacademic institution, a hub of knowledge that\ndraws you in like a magnet. And that's where\nyou meet her - Elysia, a scholar of temporal\nstudies, as she introduces herself. She seems to\nsense\nsomething\ndifferent\nabout\nyou,\nher\ncurious eyes piercing through your disguise.\n\"There's a ripple around you, stranger,\" she\nsays, her voice musical. \"You are touched by the\nTime Tether, aren't you?\"\nHer knowledge of the artifact takes you by\nsurprise. Here's your \u2026\nNow it\u2019s your time to make a choice:\nGENERATED STORY:\nAs you and Elysia delve deeper into the\nintricacies of Martian technology, you come\nacross an ancient decree - the use of the time-\nmanipulating\ndevice,\nthe\nTime-Tether,\nis\nforbidden. A reminder of a catastrophic event\nin the past that nearly wiped out the Martian\ncivilization. The realization fills you with dread;\nthe very tool that you thought was your ticket\nhome is a symbol of doom for people.\nYou\u2018re left with a grave decision: Do you ignore\nthe Martian law, risking the wrath of the whole\ncivilization, or do you try to work within their\nlaws? You choose the latter, deciding to plead\nyour case to the Martian council \u2026\n1. You're a reluctant astronaut, pressed into\nservice by a \u2026\n2. You're a seasoned astro-archeologist, part of\nan elite \u2026\n3. You're an independent researcher. When a\nroving robot\u2026\n1. You cross paths with a Martian scholar\nwho senses something \u2026\n2.\nAs navigating the alien metropolis, you\nrealize you can understand\u2026\n3. You find yourself in the middle of an\nuprising against the ruling \u2026\n1. Elysia leads you to a secret location known\nas the Temporal Vault \u2026\n2. In her teachings, Elysia proposes a bold\nidea - the artifact doesn\u2019t \u2026\n3. As you delve deeper into the Martian\nsociety, you discover that \u2026\n1-st Iteration\n2-nd Iteration\n3-rd Iteration\n1-st Iteration\n2-nd Iteration\n3-rd Iteration\nFigure 2: Qualitative analysis of using RECURRENTGPT as an interactive writing assistant and an\ninteractive \ufb01ction. Highlighted plans or choices are that selected by human users.\nRECURRENTGPT on generating very long texts. Finally, human annotators prefer RECURRENTGPT\nin all novel genres. This con\ufb01rms its robustness on different types of long texts.\nTo better understand the effectiveness of RECURRENTGPT, we also conduct an ablation study by\ncomparing RECURRENTGPT with with ablated variants without either short-term or long-term\nmemory, and the variant that uses GPT-4 as the backbone model. The results are shown in Table\n2. We can see that long/short-term memory mainly contributes to the coherence of generated texts,\nwhich correlates well with our intuition. RECURRENTGPT with GPT-4 as the backbone LLM is\ndrastically favored compared to its counterpart using ChatGPT/GPT-3.5-turbo. This con\ufb01rms the\npotential of RECURRENTGPT when equipped with more powerful LLMs. We present a few sample\nnovels generated by RECURRENTGPT in the Appendix for qualitative evaluation.\n3.3\nRECURRENTGPT as Interactive Writing Assistant\nWe then test the usefulness of RECURRENTGPT as an interactive writing assistant from a human-AI\ninteraction perspective. As illustrated in Figure 2, a human writer starts by choosing the topic he/she\n7\nTable 2: Pair-wise comparison of RECURRENTGPT with ablated variants and the variant that uses\nGPT-4 as the backbone model. We sample 20 novels of different genres for comparison. Results in\ndifferent comparisons are not comparable with each other. Bold indicates signi\ufb01cance with p < 0.05.\nNovel genres\nSci-Fi\nFantasy\n\u223c 6000 words\nInteresting \u2191\nCoherent \u2191\nInteresting \u2191\nCoherent \u2191\nRECURRENTGPT\n58.9\n65.1\n55.3\n64.1\nw/o Short term memory\n44.2\n31.0\n47.7\n33.5\nRECURRENTGPT\n51.4\n71.3\n57.5\n68.9\nw/o Long term memory\n40.0\n27.8\n46.2\n38.7\nRECURRENTGPT\n21.3\n28.1\n27.1\n24.8\nw/ GPT-4\n73.4\n64.9\n71.7\n70.5\nwants to write and writes a short paragraph describing the background and the outline of the book.\nThen RECURRENTGPT automatically generates the \ufb01rst paragraphs and provides a few possible\noptions for the writer to continue the story. The writer may select one from them and edit it if\nneeded. He or she can also write a short plan for the next few paragraphs by him/herself if generated\nplans are all inappropriate, which makes human-AI co-writing process more \ufb02exible. We show a\nGradio4-based interface that allows human writers to write different genres of novels by interacting\nwith RECURRENTGPT in Appendix B.\nAccording to a small-scale human user study, RECURRENTGPT signi\ufb01cantly improves the productiv-\nity of human writers5, and the improvements mainly come from: (1) reducing the time for typing long\ntexts by writing or choosing short plans and letting RECURRENTGPT generate the actual texts; and\n(2) reducing the time for designing less important plots by selecting plans from RECURRENTGPT\ngenerated ones, according to user feedback. Moreover, users feel that RECURRENTGPT is more\ninterpretable and controllable compared to conventional AI writing assistants that act as black-boxes\nsince the language-based components in RECURRENTGPT are transparent and editable for users.\nFinally, compared to the previous methods that hierarchically generate long texts such as DOC and\nRE3, human users prefer our system since iteratively and interactively writing long texts is more\n\ufb02exible and controllable. Finally, our system is very different from most existing AI writing assistants\nsince they focus on providing local writing suggestions within phrases or a few sentences, whereas\nRECURRENTGPT can generate a few paragraphs at a time.\n3.4\nRECURRENTGPT as Interactive Fiction\nWe also test the possibility of using RECURRENTGPT as personalized interactive \ufb01ction. This use\ncase is very similar to RECURRENTGPT as AI writing assistants. The main differences are two-fold\nas illustrated in Figure 2: (1) the shift from the third-person perspective to the \ufb01rst-person perspective,\nwhich aims to foster a sense of immersion for human players, and (2) making RECURRENTGPT\ngenerate plans that involve important choices for the main character as opposed to general plans for\nthe next paragraphs. The adaptation can be easily implemented by slightly modifying the prompt.\nOur user study shows that RECURRENTGPT can interact with human players and directly provide\ncontent of good quality for human consumers. Human players also \ufb01nd the possibility of writing\nfree-form texts as their actions in interactive \ufb01ction largely improve their interestingness. This\ncon\ufb01rms the potential of directly using generative AI as content, instead of using them as tools to\nproduce content. However, we also \ufb01nd that RECURRENTGPT sometimes produces less consistent\ncontent and low-quality options that are not very relevant or reasonable. We believe this can be\nimproved by using a more powerful LLM backbone, \ufb01ne-tuning the LLM backbone with supervised\n\ufb01ne-tuning or reinforcement learning from human feedback, or designing better prompts. We leave\nthis for future work.\n4https://gradio.app/\n5We will conduct a larger-scale user study and present the details and results in the revised version.\n8\n4\nRelated Works\n4.1\nTransformers Beyond Fixed-size Context\nOne major limitation of Transformers is that the context size is \ufb01xed, which hinders their ability on\nprocessing and producing long texts. Previous work attempts to solve this issue from two different\nways: designing ef\ufb01cient attention mechanisms to train and use Transformers with larger context\nwindows [18\u201321], and adding memory mechanisms to the computational graph in a Transformer to\nallow it to process information from multiple context windows [9, 22, 23, 11]. While these methods\nenable Transformers to process very long texts, they all require substantial architectural changes\nto the original Transformer architecture. Therefore, these approaches can not be integrated into\npowerful pre-trained LLMs such as ChatGPT and LLAMA, which substantially limits their usefulness.\nRecently, Press et al. [24] introduces ALiBi, which adds linear bias to attention to allow input length\nextrapolation. However, this method mainly supports longer inputs instead of longer outputs. In\naddition, it requires access to the model parameters and inference codes, which is often not possible\nsince many state-of-the-art LLMs such as ChatGPT, GPT-4, and PaLM, are closed-sourced.\n4.2\nLong Text Generation\nIn addition to architectural modi\ufb01cations, a number of works investigate long text generation in\na hierarchical manner. Fan et al. [25] \ufb01rst propose to generate a story by \ufb01rst generating a short\nsummary of it and then improve this method by adding an intermediate step of generating an outline\nwhich is the predicate-argument structure of the story [26]. Tan et al. [27] and Sun et al. [28] further\nimprove this kind of hierarchical long text generation method. Yao et al. [29] also propose to \ufb01rst\ngenerate a storyline and then complete the story. This line of research is further improved by RE3[15]\nand its variant DOC[16], which proposed to recursively prompt LLMs for long story generation in\na plan-and-write fashion. However, the plots and length of their \ufb01nal stories are still constrained\nby the pre-determined plans. In contrast, RECURRENTGPT overcomes the above limitations via\nrecurrent generation, which enables effective human-LM collaboration and improves the \ufb02exibility\nand controllability for long text generation.\n4.3\nAI-Assisted Writing Systems\nAI writing assistants have been adopted in a variety of applications, including story completion[12],\nessay writing [30], and poem generation [31]. Existing systems can be broadly classi\ufb01ed into\ninteractive generation and automatic generation. Interactive systems [32\u201334] are mainly designed to\nprovide local suggestions or revisions at the phrase or sentence level. As a result, they are less able to\nease the creative burden for human writers. On the other hand, automatic generation [26, 35, 36]\naims to write full texts based on given prompts or topics via the sequence-to-sequence framework.\nAlthough advances in LLMs have demonstrated impressive potential for these systems, the lack\nof transparency, controllability, and sense of collaboration could harm user experience regarding\nwriters\u2019 perceived ownership [12, 37]. Besides, most of them are limited by providing local editing\nsuggestions ranging from several phrases to a few sentences [38, 39], partly due to the length\nlimitation of NLG models and partly due to the challenge of maintaining long-range coherence.\n5\nLimitations\nOne limitation of this work is that while RECURRENTGPT can generate arbitrarily long texts, we only\nevaluate it on settings where the generated texts are at most around 5000 words. This is because both\nqualitative and quantitive evaluations of very long texts are prohibitively hard. Another limitation is\nthat RECURRENTGPT only works with backbone LLMs that are powerful enough such as ChatGPT\nand GPT-4. We believe this issue can be alleviated when more powerful smaller LLMs are developed.\nFinally, our user study for evaluating RECURRENTGPT as an AI writing assistant and as interactive\n\ufb01ction is limited by small-scale studies. We will add larger and more throughout the user study in the\nrevised version. As for the social impact, RECURRENTGPT can improve the quality of AI-generated\nlong texts and increase the productivity of human writers. However, it can also be misused to generate\ngarbage or harmful content that leads to negative social impact. However, this is a known limitation\nof generative AI and we will make our best effort to promote responsible usage of generative AI.\n9\n6\nConclusions\nWe present RECURRENTGPT, a language-based simulacra of the recurrence mechanism in RNNs\nthat uses language-based components and de\ufb01nes a recurrent computation graph via prompt engi-\nneering. RECURRENTGPT enbale LLMs to generate arbitrarily long texts either autonomously or by\ninteracting with human writters. Its language-based components improves its interpretability and\ncontrollability and the prompt-based computation graph makes it easily customizable. User study\non using RECURRENTGPT as AI writing assistants and text-based games demonstrates its potential\nas an initial step towards next-generation AI writing assistant beyond local writing suggestions and\ndirectly using generative AI as contents that are consumerable via interaction. Finally, our work also\ndemonstates the possibility of borrowing ideas from popular model designs in cognitive science and\ndeep learning literature for long form text generation using LLMs.\nReferences\n[1] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language\nunderstanding by generative pre-training. 2018.\n[2] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\nLanguage models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-\nwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,\nDaniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCan-\ndlish, Alec Radford, Ilya Sutskever, and Dario Amodei.\nLanguage models are few-shot\nlearners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Ad-\nvances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran\nAssociates, Inc., 2020.\nURL https://proceedings.neurips.cc/paper/2020/file/\n1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\n[4] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton,\nFraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano,\nJan Leike, and Ryan Lowe. Training language models to follow instructions with human\nfeedback. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors,\nAdvances in Neural Information Processing Systems, 2022. URL https://openreview.net/\nforum?id=TG8KACxEON.\n[5] OpenAI. Gpt-4 technical report, 2023.\n[6] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez, \u0141 ukasz Kaiser, and Illia Polosukhin.\nAttention is all you need.\nIn I. Guyon,\nU. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran\nAssociates, Inc., 2017.\nURL https://proceedings.neurips.cc/paper/2017/file/\n3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\n[7] Jeffrey L. Elman.\nFinding structure in time.\nCognitive Science, 14(2):179\u2013211, 1990.\nISSN 0364-0213. doi: https://doi.org/10.1016/0364-0213(90)90002-E. URL https://www.\nsciencedirect.com/science/article/pii/036402139090002E.\n[8] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):\n1735\u20131780, 1997.\n[9] Zihang Dai*, Zhilin Yang*, Yiming Yang, William W. Cohen, Jaime Carbonell, Quoc V. Le,\nand Ruslan Salakhutdinov. Transformer-XL: Language modeling with longer-term dependency,\n2019. URL https://openreview.net/forum?id=HJePno0cYm.\n10\n[10] Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap.\nCompressive transformers for long-range sequence modelling. In International Conference on\nLearning Representations, 2020. URL https://openreview.net/forum?id=SylKikSYDH.\n[11] Aydar Bulatov, Yuri Kuratov, and Mikhail Burtsev. Recurrent memory transformer. In Al-\nice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in\nNeural Information Processing Systems, 2022. URL https://openreview.net/forum?id=\nUynr3iPhksa.\n[12] Mina Lee, Percy Liang, and Qian Yang. Coauthor: Designing a human-ai collaborative writing\ndataset for exploring language model capabilities. In Proceedings of the 2022 CHI Conference\non Human Factors in Computing Systems, CHI \u201922, New York, NY, USA, 2022. Association\nfor Computing Machinery. ISBN 9781450391573. doi: 10.1145/3491102.3502030. URL\nhttps://doi.org/10.1145/3491102.3502030.\n[13] Hai Dang, Sven Goller, Florian Lehmann, and Daniel Buschek. Choice over control: How users\nwrite with large language models using diegetic and non-diegetic prompting. In Proceedings\nof the 2023 CHI Conference on Human Factors in Computing Systems, CHI \u201923, New York,\nNY, USA, 2023. Association for Computing Machinery. ISBN 9781450394215. doi: 10.1145/\n3544548.3580969. URL https://doi.org/10.1145/3544548.3580969.\n[14] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-\nnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing. Association for Computational Linguistics, 11 2019. URL https://arxiv.org/\nabs/1908.10084.\n[15] Kevin Yang, Yuandong Tian, Nanyun Peng, and Dan Klein. Re3: Generating longer stories\nwith recursive reprompting and revision. In Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, pages 4393\u20134479, Abu Dhabi, United Arab Emirates,\nDecember 2022. Association for Computational Linguistics. URL https://aclanthology.\norg/2022.emnlp-main.296.\n[16] Kevin Yang, Dan Klein, Nanyun Peng, and Yuandong Tian. Doc: Improving long story\ncoherence with detailed outline control, 2022.\n[17] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,\nChristopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained\ntransformer language models. arXiv preprint arXiv:2205.01068, 2022.\n[18] Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer.\narXiv:2004.05150, 2020.\n[19] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The ef\ufb01cient transformer. In\nICLR. OpenReview.net, 2020.\n[20] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with\nsparse transformers, 2019.\n[21] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti,\nSantiago Onta\u00f1\u00f3n, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big\nbird: Transformers for longer sequences. In NeurIPS, 2020.\n[22] Zhiwei Wang, Yao Ma, Zitao Liu, and Jiliang Tang. R-transformer: Recurrent neural network\nenhanced transformer, 2019.\n[23] Peng Cui and Le Hu. Sliding selector network with dynamic memory for extractive summariza-\ntion of long documents. In Proceedings of the 2021 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, pages 5881\u2013\n5891, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.\nnaacl-main.470. URL https://aclanthology.org/2021.naacl-main.470.\n[24] O\ufb01r Press, Noah A. Smith, and Mike Lewis. Train short, test long: Attention with linear biases\nenables input length extrapolation. In ICLR. OpenReview.net, 2022.\n11\n[25] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceed-\nings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers), pages 889\u2013898, 2018.\n[26] Angela Fan, Mike Lewis, and Yann Dauphin. Strategies for structuring story generation. arXiv\npreprint arXiv:1902.01109, 2019.\n[27] Bowen Tan, Zichao Yang, Maruan Al-Shedivat, Eric Xing, and Zhiting Hu. Progressive\ngeneration of long text with pretrained language models. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, pages 4313\u20134324, Online, June 2021. Association for Computational\nLinguistics. doi: 10.18653/v1/2021.naacl-main.341. URL https://aclanthology.org/\n2021.naacl-main.341.\n[28] Xiaofei Sun, Zijun Sun, Yuxian Meng, Jiwei Li, and Chun Fan. Summarize, outline, and\nelaborate: Long-text generation via hierarchical supervision from extractive summaries. In\nProceedings of the 29th International Conference on Computational Linguistics, pages 6392\u2013\n6402, Gyeongju, Republic of Korea, October 2022. International Committee on Computational\nLinguistics. URL https://aclanthology.org/2022.coling-1.556.\n[29] Lili Yao, Nanyun Peng, Ralph M. Weischedel, Kevin Knight, Dongyan Zhao, and Rui Yan.\nPlan-and-write: Towards better automatic storytelling. In AAAI, pages 7378\u20137385. AAAI Press,\n2019.\n[30] Yuanchao Liu, Bo Pang, and Bingquan Liu. Neural-based Chinese idiom recommendation\nfor enhancing elegance in essay writing.\nIn Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages 5522\u20135526, Florence, Italy, July 2019.\nAssociation for Computational Linguistics. doi: 10.18653/v1/P19-1552. URL https://\naclanthology.org/P19-1552.\n[31] Marjan Ghazvininejad, Xing Shi, Jay Priyadarshi, and Kevin Knight. Hafez: an interactive\npoetry generation system. In Proceedings of ACL 2017, System Demonstrations, pages 43\u2013\n48, Vancouver, Canada, July 2017. Association for Computational Linguistics. URL https:\n//aclanthology.org/P17-4008.\n[32] Andy Coenen, Luke Davis, Daphne Ippolito, Emily Reif, and Ann Yuan. Wordcraft: a human-ai\ncollaborative editor for story writing. arXiv preprint arXiv:2107.07430, 2021.\n[33] John Joon Young Chung, Wooseok Kim, Kang Min Yoo, Hwaran Lee, Eytan Adar, and Minsuk\nChang. Talebrush: sketching stories with generative pretrained language models. In Proceedings\nof the 2022 CHI Conference on Human Factors in Computing Systems, pages 1\u201319, 2022.\n[34] Seraphina Goldfarb-Tarrant, Haining Feng, and Nanyun Peng. Plan, write, and revise: an\ninteractive system for open-domain story generation. arXiv preprint arXiv:1904.02357, 2019.\n[35] Yufei Tian and Nanyun Peng. Zero-shot sonnet generation with discourse-level planning and\naesthetics features. In Proceedings of the 2022 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 3587\u20133597,\nSeattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/\n2022.naacl-main.262. URL https://aclanthology.org/2022.naacl-main.262.\n[36] Boyang Li, Stephen Lee-Urban, George Johnston, and Mark Riedl. Story generation with\ncrowdsourced plot graphs. In Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence,\nvolume 27, pages 598\u2013604, 2013.\n[37] Jeremy Birnholtz, Stephanie Steinhardt, and Antonella Pavese. Write here, write now! an\nexperimental study of group maintenance in collaborative writing. In Proceedings of the SIGCHI\nConference on Human Factors in Computing Systems, pages 961\u2013970, 2013.\n[38] Rujun Han, Hong Chen, Yufei Tian, and Nanyun Peng. Go back in time: Generating \ufb02ashbacks\nin stories with event temporal prompts. arXiv preprint arXiv:2205.01898, 2022.\n[39] Lili Yao, Nanyun Peng, Ralph Weischedel, Kevin Knight, Dongyan Zhao, and Rui Yan. Plan-\nand-write: Towards better automatic storytelling. In Proceedings of the AAAI Conference on\nArti\ufb01cial Intelligence, volume 33, pages 7378\u20137385, 2019.\n12\nA\nPrompts\nI need you to help me write a novel. Now I give you a memory (a brief summary) of 400 words, you should use it to store the key content of what has \nbeen written so that you can keep track of very long context. For each time, I will give you your current memory (a brief summary of previous \nstories. You should use it to store the key content of what has been written so that you can keep track of very long context), the previously \nwritten paragraph, and instructions on what to write in the next paragraph. I need you to write:\n1. Output Paragraph: the next paragraph of the novel. The output paragraph should contain around 20 sentences and should follow the input \ninstructions.\n2. Output Memory: The updated memory. You should first explain which sentences in the input memory are no longer necessary and why, and then \nexplain what needs to be added into the memory and why. After that you should write the updated memory. The updated memory should be similar to the \ninput memory except the parts you previously thought that should be deleted or added. The updated memory should only store key information. The \nupdated memory should never exceed 20 sentences!\n3. Output Instruction:\ninstructions of what to write next (after what you have written). You should output 3 different instructions, each is a \npossible interesting continuation of the story. Each output instruction should contain around 5 sentences\nHere are the inputs:\nInput Memory: \n{short_memory}\nInput Paragraph:\n{input_paragraph}\nInput Instruction:\n{input_instruction}\nInput Related Paragraphs:\n{input_long_term_memory}\nNow start writing, organize your output by strictly following the output format as below:\nOutput Paragraph:\n<string of output paragraph>, around 20 sentences.\nOutput Memory:\nRational: <string that explain how to update the memory>;\nUpdated Memory: <string of updated memory>, around 10 to 20 sentences\nOutput Instruction:\nInstruction 1: <content for instruction 1>, around 5 sentences\nInstruction 2: <content for instruction 2>, around 5 sentences\nInstruction 3: <content for instruction 3>, around 5 sentences\nVery important: The updated memory should only store key information. The updated memory should never contain over 500 words! Finally, remember \nthat you are writing a novel. Write like a novelist and do not move too fast when writing the output instructions for the next paragraph. Remember \nthat the chapter will contain over 10 paragraphs and the novel will contain over 100 chapters. And this is just the begining. Just write some \ninteresting staffs that will happen next. Also, think about what plot can be attractive for common readers when writing output instructions. You \nshould first explain which sentences in the input memory are no longer necessary and why, and then explain what needs to be added into the memory \nand why. After that, you start rewrite the input memory to get the updated memory.\nFigure 3: The prompts designed for the backbone LLM in the RECURRENTGPT framework that\nsimulates input (plan, instruction), output, short-term memory, and long-term memory, respectively.\nB\nDemo\nFigure 4: A web demo of RECURRENTGPT.\n13\n"
  }
]