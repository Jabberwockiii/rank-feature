[
  {
    "title": "Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding",
    "link": "https://arxiv.org/pdf/2307.15337.pdf",
    "upvote": "34",
    "text": "Published as a conference paper at ICLR 2024\nSKELETON-OF-THOUGHT:\nPROMPTING LLMS\nFOR\nEFFICIENT PARALLEL GENERATION\nXuefei Ning1\u2217\nfoxdoraame@gmail.com\nZinan Lin2\u2217\nlinzinan1995@gmail.com\nZixuan Zhou14\u2217\nzhouzx21@mails.tsinghua.edu.cn\nZifu Wang3\nzifu.wang@kuleuven.be\nHuazhong Yang1\nyanghz@tsinghua.edu.cn\nYu Wang1\nyu-wang@tsinghua.edu.cn\n1 Department of Electronic Engineering, Tsinghua University, Beijing, China\n2 Microsoft Research, Redmond, Washington, USA\n3 ESAT-PSI, KU Leuven, Leuven, Belgium\n4 Infinigence-AI\nWebsite: https://sites.google.com/view/sot-llm\nCode: https://github.com/imagination-research/sot\nABSTRACT\nThis work aims at decreasing the end-to-end generation latency of large language\nmodels (LLMs). One of the major causes of the high generation latency is the\nsequential decoding approach adopted by almost all state-of-the-art LLMs. In\nthis work, motivated by the thinking and writing process of humans, we propose\nSkeleton-of-Thought (SoT), which first guides LLMs to generate the skeleton of\nthe answer, and then conducts parallel API calls or batched decoding to complete\nthe contents of each skeleton point in parallel. Not only does SoT provide consid-\nerable speed-ups across 12 LLMs, but it can also potentially improve the answer\nquality on several question categories. SoT is an initial attempt at data-centric op-\ntimization for inference efficiency, and showcases the potential of eliciting high-\nquality answers by explicitly planning the answer structure in language.\n1\nINTRODUCTION\nLarge language models (LLMs) (Brown et al., 2020; Touvron et al., 2023a; Du et al., 2022; OpenAI,\n2023; Zheng et al., 2023) have shown exceptional performance in natural language processing and\nchatbot systems. However, the inference process of the state-of-the-art LLMs is slow, hindering their\ninteractive use. For example, it takes 22 seconds for Claude (Anthropic, 2023) (accessed through\nSlack API) and 43 seconds for Vicuna-33B V1.3 (a 33B LLaMA-based model, running locally on\none NVIDIA A100 GPU) to answer the question in Fig. 1.\nWe conclude three major causes of LLMs\u2019 slow inference: (1) A large model size requires a large\namount of memory, memory access, and computation. For example, the FP16 weights of 175B GPT-\n3 take 350GB memory, which means at least 5\u00d780GB A100 GPUs are needed to keep the model\nin GPU memory. Even with enough GPUs, the heavy memory access and computation slow down\nthe inference. (2) The attention operation in the prevailing transformer architecture is I/O bounded\nand has a quadratic memory and computation complexity in sequence length. (3) The sequential\ndecoding approach in inference generates tokens one by one. This approach introduces a significant\ninference latency since the generation of tokens cannot be parallelized. There is a bunch of literature\naddressing the first two axes: large model size (Xiao et al., 2022; Frantar et al., 2022; Lin et al., 2023;\nSheng et al., 2023; Wang et al., 2021) and attention operation (Kitaev et al., 2020; Wang et al., 2020;\n\u2217Equal contribution.\n1\narXiv:2307.15337v3  [cs.CL]  2 Mar 2024\nPublished as a conference paper at ICLR 2024\nAnswer\n1. Active listening involves fully \nconcentrating on \u2026\n2. Identify issues. Look into the root \ncauses of \u2026\n3. Compromise. Look for a middle \nground \u2026\nWhat are the most effective \nstrategies for conflict \nresolution in the workplace?\nQuestion\nSkeleton-of-Thought \nDecoding\nGenerates answers\nsequentially \u2794 Slower\nNormal \nDecoding\n1. Active listening\n2. Identify issues\n3. Compromise\nGenerates answers\nin parallel \u2794 Faster\n(1) Skeleton\nstage\n(2) Point-\nexpanding\nstage\n1.0\n1.2\n1.4\n1.6\n1.8\nSpeed-up\n\u22120.2\n0.0\n0.2\n0.4\nNet win rates\nVicuna-13B V1.3\nStableVicuna-13B\nUltraLM-13B\nVicuna-33B V1.3\nLLaMA2-Chat-7B\nLLaMA2-Chat-13B\nVicuna-7B V1.3\nChatGPT-3.5\nClaude\nVicuna-7B V1.1\nOpenChat-13B\nGPT-4\nBaseline\nFigure 1: Left: An illustration of Skeleton-of-Thought (SoT). Instead of producing answers se-\nquentially, SoT produces different parts of answers in parallel. In more detail, given the question,\nSoT first prompts the LLM to give out the skeleton, then conducts batched decoding or parallel API\ncalls to expand multiple points in parallel, and finally aggregates the outputs to get the final answer.\nRight: The net win rates and speed-ups of SoT with router (SoT-R) compared to normal generation\non Vicuna-80. The net win rate is the difference between the fraction of questions that SoT-R has\nbetter and worse answers than normal generation. The speed-up is the ratio between the latency\nof normal and SoT-R generation. (1.0, 0.0) represents normal generation. Higher is better on both\naxes. For most models, SoT-R not only accelerates the generation but also improves the quality of\nthe answers (evaluated with FastChat metric (Zheng et al., 2023)). See \u00a7 3.2 and 4 for more details.\nDao et al., 2022; Zaheer et al., 2020; Chen et al., 2023b). These works either compress/redesign the\nmodel (Xiao et al., 2022; Frantar et al., 2022; Lin et al., 2023; Kitaev et al., 2020; Wang et al., 2020;\nDao et al., 2022; Zaheer et al., 2020) or redesign the serving system (Sheng et al., 2023; Chen et al.,\n2023b) and hardware (Wang et al., 2021).\nIn contrast to prior work, we tackle the third axis and question the common assumption that LLMs\nhave to do fully sequential decoding. We show the feasibility of parallel decoding of off-the-shelf\nLLMs without any changes to their model, system, or hardware. For instance, for the question\nin Fig. 1, we can reduce the latency from 22 seconds to 12 seconds (1.83\u00d7 speed-up) with Claude,\nand from 43 seconds to 16 seconds (2.69\u00d7 speed-up) with Vicuna-33B V1.3 on an NVIDIA A100.\nThe idea stems from reflecting on how humans ourselves answer questions. Humans do not always\nthink about questions and write answers in a sequential fashion. In contrast, for many question\ntypes, we first derive the skeleton according to some protocols and strategies, and then add evidence\nand details to explain each point. This is especially the case on occasions like offering consultancy,\ntaking tests, writing papers, and so on. This intuition has our back to question the necessity of fully\nsequential decoding. In this paper, we propose Skeleton-of-Thought (SoT). Specifically, as shown in\nFig. 1, we guide the LLM to derive a skeleton first by itself. Based on the skeleton, the LLMs can\ncomplete each point in parallel so that we get a speed-up. SoT can be utilized to accelerate both\nopen-source models with batched decoding and API-based models with parallel API calls.\nThe current SoT is suitable for questions that require a long answer whose structure can be planned\nahead, while not suitable for questions that require step-by-step reasoning or only need a short\nanswer. Therefore, to make the overall solution more practical, we design an extension, SoT with\nrouter (SoT-R), which employs a router to only trigger SoT for suitable questions.\nWe test SoT on 12 recently released LLMs. Not only does SoT provide considerable speed-ups (up\nto 2.39\u00d7), but it can also improve the answer quality in many cases (Fig. 1).\nNote that in contrast to existing model- and system-level efforts for inference efficiency, SoT takes\na novel \u201cdata-level\u201d pathway by letting the LLM organize its output content. This novel perspective\nis becoming feasible and is expected to grow in importance, owing to the evolving capabilities of\nstate-of-the-art LLMs. We hope this work can stimulate more research in the realm of data-centric\noptimization (Zha et al., 2023; HazyResearch, 2023) for efficiency.\n2\nPublished as a conference paper at ICLR 2024\nPrompt 1. Skeleton Prompt Template T s\n[User:] You\u2019re an organizer responsible for only giving the skeleton (not the full content) for answering the question.\nProvide the skeleton in a list of points (numbered 1., 2., 3., etc.) to answer the question. Instead of writing a full sentence,\neach skeleton point should be very short with only 3\u223c5 words. Generally, the skeleton should have 3\u223c10 points. Now,\nplease provide the skeleton for the following question.\n{question}\nSkeleton:\n[Assistant:] 1.\nPrompt 2. Point-Expanding Prompt Template T pe\n[User:] You\u2019re responsible for continuing the writing of one and only one point in the overall answer to the following\nquestion.\n{question}\nThe skeleton of the answer is\n{skeleton}\nContinue and only continue the writing of point {point index}.\nWrite it **very shortly** in 1\u223c2 sentence and\ndo not continue with other points!\n[Assistant:] {point index}. {point skeleton}\nThe rest of the paper is organized as follows. We first introduce SoT in \u00a7 2 and show its results in\n\u00a7 3. Then, we expand on the SoT-R extension in \u00a7 4. \u00a7 5 positions SoT in the research ecosystem\n(expanded in App. D). Finally, we analyze the limitations and share outlooks of SoT in \u00a7 6.\n2\nSKELETON-OF-THOUGHT (SOT)\n2.1\nMETHOD\nOverview. Based on the intuition that humans usually think about and answer a question in an\norganized way, the core idea of this work is to guide the LLM itself to give a skeleton first and then\nwrite the overall answer parallelly instead of sequentially. Fig. 1 illustrates how SoT produces the\nfinal answer to a user question q.\n(1) Skeleton stage. SoT first assembles a skeleton request, T s(question = q), using the skeleton\nprompt template T s (Prompt 1, and Prompt 3 in App. B.1) with the question q as the parameter. The\nskeleton prompt template is written to guide the LLM to output a concise skeleton of the answer.\nThen, we extract the B points from the skeleton response Rs of the LLM.\n(2) Point-expanding stage. Based on the skeleton, we let the LLM expand on each point in parallel.\nSpecifically, for the point with index b and skeleton Rs\nb, SoT uses T pe(question = q, skeleton =\nRs, point index = b, point skeleton = Rs\nb) as the point-expanding request for the LLM, where\nT pe is the point-expanding prompt template (Prompt 2). Finally, after completing all points, we\nconcatenate the point-expanding responses {Rpe\nb }b=1,\u00b7\u00b7\u00b7 ,B to get the final answer.\nParallel point expanding. We conduct parallel point-expanding so that SoT is able to achieve a\nspeed-up than normal decoding.\n(1) For proprietary models with only API access, we can issue multiple parallel API calls to get an\nend-to-end latency gain at the cost of an increased number of API requests and tokens.\n(2) For open-source models that we can run locally, we let them process the point-expanding re-\nquests as a batch (paddings are added to the left of the point-expanding requests). We explain below\nwhy this could achieve speed-ups. A typical LLM generative process consists of two phases: (a)\nthe prefilling phase in which the prompt is parsed to generate the key-value cache for further use,\nand (b) the decoding phase in which tokens are generated one by one in a sequential manner. The\ndecoding phase accounts for the majority of the end-to-end latency, especially when generating a\nlong response. Note that the decoding phase is bottlenecked by weight loading instead of activation\n3\nPublished as a conference paper at ICLR 2024\nloading or computation.1 Consequently, running LLM inference with increased batch sizes does\nnot increase the per-token latency much. Therefore, SoT allows us to decode roughly B\u00d7 more to-\nkens within the same amount of time if we parallelly decode B points. See App. E for the expanded\ndiscussions and the supporting experiments. Please refer to App. B for more implementation details.\n3\nSOT EVALUATION\nDatasets. We evaluate SoT on two recent assistant-style datasets: (1) Vicuna-80 (Chiang et al.,\n2023), which contains 80 questions spanning nine categories, such as coding, math, writing, role-\nplay, and so on, and (2) WizardLM (Xu et al., 2023), which contains 218 questions spanning more\ncategories and diverse difficulties. Due to space constraints, we only report Vicuna-80 results in the\nmain paper, and defer WizardLM results to the Apps. G and I.\nModels. We test SoT on 12 models, including 9 open-source models and 3 API-based models. We\nobtain the weights of all the open-source models from Hugging Face. See App. A for more details.\n3.1\nEVALUATION OF EFFICIENCY\nAPI-based\nmodels.\nWe\nrecord\nthe\nlatency\nof\nevery\nAPI\ncall\nwith\nstart = time.time(); ...; elapsed_time = time.time() - start,\nand\nadd the latency of the skeleton API call and the slowest point-expanding API call as the SoT latency.\nOpen-source models. All open-source models we currently evaluate are based on the LLaMA 7B,\n13B, or 33B architectures. Thus, to enable fast analysis, we first make a latency profiling table for\neach LLaMA architecture on NVIDIA A100. The table contains the architecture\u2019s (1) latency for\nprefilling sequences of length 1 to 700 with different batch sizes (from 1 to 16), and (2) decoding\none token with a context of length 1 to 1024 with different batch sizes (from 1 to 16). With these\nthree latency profiling tables, given the number of points B, the token lengths of the requests and\nresponses in the skeleton and point-expanding stages, we can quickly estimate the SoT latency\nby simply looking up entries in the tables and adding them up. See App. F for a more detailed\ndescription of how we conduct the profiling and estimate the latency.\nIn addition to the above approach, we also compare the actual latency of SoT and normal sequential\ngeneration (abbreviated as \u201cnormal\u201d in the following discussion) in App. G.1.4.\nThe rest of this section shows the speed-ups of SoT on different models (\u00a7 3.1.1) and question\ncategories (\u00a7 3.1.2). In addition, we also report the latency breakdown of SoT stages in App. G.1.2\nand the SoT speed-ups on an RTX 3090 GPU in App. G.1.3.\n3.1.1\nSPEED-UP BREAKDOWN: MODELS\nWe investigate how SoT reduces the end-to-end latency on different models. Fig. 2a shows the\naverage speed-up for each model across all question categories. We can see that SoT obtains a >2\u00d7\nspeed-up (up to 2.39\u00d7) on 8 out of 12 models.\nWe report the detailed statistics about token lengths and numbers of points in Fig. 11. (1) In terms\nof the point number B (Fig. 11a), LLaMA2, Vicuna-7B V1.1, Vicuna-7B V1.3, and ChatGPT-3.5\nyield relatively fewer points (<6), while GPT-4 and StableVicuna-13B generates the largest number\nof points on average (\u22489). (2) Regarding the point-expanding response length, Figs. 11b to 11d\nshow that the API-based models, ChatGPT-3.5, Claude, and GPT-4, follow the point-expanding\nrequest better and generate shorter point-expanding responses than the open-source models. One\ncan also notice that StableVicuna-13B\u2019s longest point-expanding responses for many question cat-\negories can be as lengthy as the overall normal answer, since it fails to adhere to the \u201cWrite it\n**very shortly**\u201d instruction in the point-expanding request. Consequently, SoT cannot accelerate\nStableVicuna-13B well. (3) Regarding the length balance degree between point responses, Fig. 11e\nshows that LLaMA2 and the API-based models generate more balanced point-expanding responses.\n(4) As for the overall length of the final aggregated answer (Fig. 11f), employing SoT on most\nmodels results in answers that are, on average, 1\u223c2\u00d7 longer than the normal answer.\n1This is true when the number of concurrent queries is small; see \u00a7 6 for discussion on other scenarios.\n4\nPublished as a conference paper at ICLR 2024\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\n2.2\n2.4\n2.6\n2.8\nStableVicuna-13B\nClaude\nVicuna-13B V1.3\nChatGPT-3.5\nGPT-4\nVicuna-7B V1.3\nUltraLM-13B\nVicuna-33B V1.3\nOpenChat-13B\nVicuna-7B V1.1\nLLaMA2-Chat-13B\nLLaMA2-Chat-7B\n1.13\u00d7\n1.31\u00d7\n1.91\u00d7\n1.97\u00d7\n2.00\u00d7\n2.01\u00d7\n2.18\u00d7\n2.24\u00d7\n2.28\u00d7\n2.30\u00d7\n2.38\u00d7\n2.39\u00d7\n(a) Different models.\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\n2.2\n2.4\n2.6\n2.8\nmath\nfermi\ncounterfactual\nroleplay\ncoding\ncommon-sense\nwriting\ngeneric\nknowledge\n1.34\u00d7\n1.69\u00d7\n1.89\u00d7\n1.95\u00d7\n2.06\u00d7\n2.24\u00d7\n2.26\u00d7\n2.31\u00d7\n2.33\u00d7\n(b) Different categories.\nFigure 2: Average speed-ups of SoT on different models and question categories.\n3.1.2\nSPEED-UP BREAKDOWN: QUESTION CATEGORIES\nHere we investigate how SoT reduces the end-to-end latency for different question categories.\nFig. 2b shows the average speed-up for each question category across all models. The question\ncategories for which SoT can provide high-quality answers are marked in green, and other cate-\ngories are marked in red (see \u00a7 3.2.3 for the answer quality evaluation). We can see that SoT can\nobtain speed-ups for all question categories. For the five question categories that SoT can provide\nhigh-quality answers (i.e., knowledge, generic, common-sense, roleplay, counterfactual), SoT can\nspeed up the overall answer generation process by 1.89\u00d7 to 2.33\u00d7 in the meantime.\n3.2\nEVALUATION OF ANSWER QUALITY\nIn order to compare the answer quality of the normal sequential generation (abbreviated as \u201cnormal\u201d\nin the following discussion) and SoT generation, we adopt two LLM-based evaluation frameworks:\nFastChat (Zheng et al., 2023) and LLMZoo (Chen et al., 2023c). The evaluation process is to present\na question and a pair of answers (from normal or SoT generation) to an LLM judge (GPT-4 in the\nmain paper; see App. I.4 for the results evaluated using ChatGPT-3.5) and ask for its preference.\nHere are more details about the evaluation of the answer quality:\n(1) Detailed metrics. FastChat provides one metric for the general answer quality. In addition\nto a general metric, LLMZoo provides five detailed metrics on the answers\u2019 coherence, diversity,\nimmersion, integrity, and relevance.\n(2) Question categories. FastChat provides two special evaluation prompts for coding and math\nquestions for more accurate evaluation, whereas LLMZoo does not. Following the implementation\nin LLMZoo, we exclude math and coding questions in all LLMZoo evaluation results.\n(3) Extentions to avoid evaluation bias. To avoid the potential bias from the order of the two answers\npresented to the LLM judge, we extend FastChat and LLMZoo evaluation frameworks by running\nthe evaluation twice with either ordering of the two answers. In either evaluation, a score of 1,\n0, and -1 is assigned when SoT wins, ties, or loses, respectively. The final evaluation is that SoT\nwins/ties/loses when the sum of the two scores is positive/zero/negative. For example, if SoT wins\nin one evaluation and loses in the other evaluation, the result is \u201ctie\u201d. If SoT wins (loses) in one\nevaluation and ties in the other, the result is \u201cwin\u201d (\u201close\u201d).\n(4) Net win rates. We further define net win rates to give a summarized view of the answer quality.\nGiven the number of questions that SoT wins (#win) and loses (#lose), we define net win rates\nas #win\u2212#lose/total number of questions. 0% means that SoT performs competitively to the normal baseline\n(wins and loses in the same number of questions). Higher values mean that SoT performs better.\nIn the following sections, we first present the overall quality of SoT answers (\u00a7 3.2.1), and then go\ninto the details across different question categories (\u00a7 3.2.3), models (\u00a7 3.2.2), and metrics (\u00a7 3.2.4).\n3.2.1\nOVERALL QUALITY\nIn Fig. 3, we show the win/tie/lose rates (the percentage of the cases when SoT wins/ties/loses\ncompared to normal generation) across all models and questions using the two metrics from FastChat\nand LLMZoo that capture the general quality of the answers. We notice a discrepancy between the\ntwo metrics on when SoT is strictly better than the baseline (45.8% v.s. 29.5%). Despite that, the\ntwo metrics agree that SoT is not worse than the baseline in around 60% of the cases, and the win\n5\nPublished as a conference paper at ICLR 2024\nrates are close to the lose rates. This result suggests that the answers of SoT maintain good quality\nof that of the normal generation.\n0%\n20%\n40%\n60%\n80%\n100%\nGeneral quality (LLMZoo)\nGeneral quality (FastChat)\n45.8%\n29.5%\n19.6%\n29.3%\n34.5%\n41.2%\nWin\nTie\nLose\nFigure 3: Win/tie/lose rates of SoT v.s. normal generation using \u201cgeneral\u201d metrics from FastChat\nand LLMZoo. SoT performs better than or equal to normal generation in around 60% cases.\n3.2.2\nQUALITY BREAKDOWN: MODELS\nWe compute net win rates on all models in Fig. 4. Again, we see that the two general metrics\nfrom FastChat and LLMZoo have different absolute values but similar rankings. In particular, both\nmetrics agree that OpenChat-13B, Vicuna-7B V1.1, Claude, LLaMA2-Chat-13B have low net win\nrates, whereas Vicuna-13B V1.3, StableVicuna-13B, and UltraLM-13B have high net win rates.\n-60%\n-40%\n-20%\n0%\n20%\nStableVicuna-13B\nUltraLM-13B\nVicuna-13B V1.3\nGPT-4\nLLaMA2-Chat-7B\nVicuna-33B V1.3\nVicuna-7B V1.3\nChatGPT-3.5\nLLaMA2-Chat-13B\nOpenChat-13B\nVicuna-7B V1.1\nClaude\n(a) Metric: general quality (FastChat).\n-40%\n-20%\n0%\n20%\n40%\n60%\nStableVicuna-13B\nUltraLM-13B\nVicuna-13B V1.3\nGPT-4\nLLaMA2-Chat-7B\nVicuna-33B V1.3\nVicuna-7B V1.3\nChatGPT-3.5\nLLaMA2-Chat-13B\nOpenChat-13B\nVicuna-7B V1.1\nClaude\n(b) Metric: general quality (LLMZoo).\nFigure 4: Net win rates of SoT on different models.\nWe investigate the answers in App. I.1.1, and summarize the key takeaways as follows. Some\nmodels have low SoT net win rates as they cannot understand the skeleton and point-expanding\nprompts well. Some other models have low SoT net win rates as their normal answers already\nhave good quality, making it hard for SoT to beat them (e.g., Claude). For models that are able to\nunderstand the SoT prompts and the normal answers are not good enough, SoT can improve the\nanswer quality. We expect that further improving SoT prompts or fine-tuning the models can make\nit easier for LLMs to understand the skeleton and point-expanding prompts and ultimately result in\nbetter answer quality.\n3.2.3\nQUALITY BREAKDOWN: QUESTION CATEGORIES\nWe compute net win rates on all question categories in Fig. 5. Similar to Fig. 3, we see that LLMZoo\ntends to be more optimistic about the quality of SoT than FastChat. Nevertheless, the conclusions\nare consistent: SoT performs relatively well on generic, common-sense, knowledge, roleplay, and\ncounterfactual, and relatively poorly on writing, fermi, math, and coding.\n-80%\n-60%\n-40%\n-20%\n0%\n20%\n40%\ncounterfactual\ngeneric\ncommon-sense\nknowledge\nroleplay\nfermi\nwriting\nmath\ncoding\n(a) Metric: general quality (FastChat).\n-20%\n0%\n20%\n40%\n60%\ncounterfactual\ngeneric\ncommon-sense\nknowledge\nroleplay\nfermi\nwriting\n(b) Metric: general quality (LLMZoo).\nFigure 5: Net win rates of SoT on different question categories.\nWe investigate the answers in App. I.1.2, and summarize the key takeaways as follows. SoT per-\nforms well when the question can be answered in several points whose details can be expanded\nindependently. This includes a wide range of real-world questions. On the other hand, it is fun-\ndamentally challenging to apply SoT on questions that require step-by-step thinking, in which the\nlatter steps require the details from the earlier steps, such as math questions. To make SoT general\n6\nPublished as a conference paper at ICLR 2024\nacross broader question categories, one promising pathway is to enable SoT to adaptively fall back\nto normal generation, which we explore in \u00a7 4. Interestingly, our results suggest that some LLMs\nare already able to do that occasionally without special prompting or tuning (see App. I.1.2).\n3.2.4\nQUALITY BREAKDOWN: METRICS\nIn Fig. 6, we show more detailed metrics from LLMZoo to reveal in which aspects SoT can improve\nor hurt the answer quality. On average, we can see that SoT improves the diversity and relevance\nwhile hurting the immersion and coherence.\n0%\n20%\n40%\n60%\n80%\n100%\nIntegrity\nCoherence\nImmersion\nRelevance\nDiversity\n23.2%\n29.8%\n40.5%\n61.4%\n99.9%\n34.6%\n30.6%\n23.7%\n11.3%\n0.1%\n42.1%\n39.6%\n35.8%\n27.3%\nWin\nTie\nLose\nFigure 6: Win/tie/lose rates of SoT v.s. normal generations using metrics from LLMZoo. SoT\nperforms well on diversity and relevance, and relatively worse on coherence and immersion.\nThrough answer investigation (App. I.1.3), we summarize the key takeaways as follows. The skele-\nton stage of SoT explicitly require LLMs to discuss the answers from multiple aspects without filler\nwords. This improves the diversity and relevance of the answers. As for coherence and immersion,\nSoT is not worse than the normal generation around 60% of the time. One future direction is to\nimprove the SoT prompts or pipeline so that the answers can be better in more metrics.\n4\nSOT WITH ROUTER (SOT-R): ADAPATIVELY TRIGGERING SOT\nIn \u00a7 3, we see that SoT provides considerable speed-ups while maintaining (or even improving)\nanswer quality for many question types. However, the biggest limitation is that SoT is not suitable\nfor questions that require step-by-step reasoning (\u00a7 3.2.3). Towards pushing the practical adoption\nof SoT, we explore the possibility of adaptively triggering SoT only when it is suitable. To achieve\nthat, we propose a router module that decides if SoT should be applied for the user request, and\nthen call either SoT or normal decoding accordingly. This paradigm aligns with the recent trends\nof composing multiple models to solve complicated tasks (Chase, 2022; Shen et al., 2023). To\nimplement the router, we explore two options: LLM prompting as the router (no model training is\nneeded) (\u00a7 4.1), and trained RoBERTa as the router (\u00a7 4.2). The evaluation is provided in \u00a7 4.3.\n4.1\nPROMPTING ROUTER\nWe directly ask an LLM if the question is suitable for SoT. More specifically, we ask the LLM if the\ndesired answer is in a list of independent points (see App. C.1 for the prompt). If the answer is yes,\nwe will use SoT; otherwise, we will use normal generation (i.e., directly feeding the question to the\nLLM). We employ GPT-4 as the LLM router given its strong capability.\n4.2\nTRAINED ROUTER\nWhile leveraging GPT-4 as the router obviates the need for model training, its performance remains\nsensitive to prompt design. Therefore, we approach the problem as a sequence classification task by\nfine-tuning a small language model as the router. Specifically, we annotate the LIMA dataset (Zhou\net al., 2023) as the training set to train a RoBERTa model (Liu et al., 2019), which has only 120M\nparameters. Details about the annotation and training can be found in Apps. C.2.1 and C.2.2.\n4.3\nSOT-R EVALUATION\nWe compare SoT and SoT-R under the same evaluation setup in \u00a7 3. Besides the prompting and\ntrained routers, we also consider a \u201chuman router\u201d where we manually judge whether SoT should\nbe applied for each question. This serves as a benchmark for comparison.\n7\nPublished as a conference paper at ICLR 2024\n4.3.1\nEVALUATION OF EFFICIENCY\nFig. 7 shows the speed-ups of SoT and SoT-R for different models on Vicuna-80 (see App. G.2 for\nresults on the WizardLM dataset). We can see that: (1) As expected, SoT-R obtains lower speed-\nups than SoT, since SoT is not triggered for some questions and the router induces a small latency\noverhead. Nevertheless, SoT-R can still benefit most models with >1\u00d7 speed-ups. (2) SoT-R with\nthe trained router obtains slightly higher speed-ups for 7 out of 12 models on Vicuna-80, while\nSoT-R with the prompting router obtains higher speed-ups for all models on WizardLM (Fig. 17).\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nStableVicuna-13B\nClaude\nVicuna-13B V1.3\nChatGPT-3.5\nGPT-4\nVicuna-7B V1.3\nUltraLM-13B\nVicuna-33B V1.3\nOpenChat-13B\nVicuna-7B V1.1\nLLaMA2-Chat-13B\nLLaMA2-Chat-7B\nSoT (w/o router)\nSoT-R w/ prompting router\nSoT-R w/ trained router\nFigure 7: Speed-ups of SoT and SoT-R on dif-\nferent models across all question categories of\nthe Vicuna-80 dataset.\n-80%\n-60%\n-40%\n-20%\n0%\n20%\n40%\ncounterfactual\ngeneric\ncommon-sense\nknowledge\nroleplay\nfermi\nwriting\nmath\ncoding\nSoT (w/o router)\nSoT-R w/ prompting router\nSoT-R w/ trained router\nSoT-R w/ human router\nFigure 8: Net win rates of SoT and SoT-R on\ndifferent question categories of the Vicuna-80\ndataset (evaluated with the FastChat metrics).\n4.3.2\nEVALUATION OF ANSWER QUALITY\nFig. 8 shows the net win rates (averaged across all models) of SoT and SoT-R on Vicuna-80 with the\nFastChat metrics (see App. I.2 for results of the WizardLM dataset and LLMZoo metrics). We can\nsee that: (1) SoT-R significantly improves the answer quality on questions where SoT is not suitable\n(e.g., coding, math, writing, fermi) by falling back to normal decoding. At the same time, SoT-R\nmaintains answer quality improvements on questions where SoT is good at. (2) The trained router\nperforms similar to (on Vicuna-80) or better than (on WizardLM; see App. I.2) the prompting router.\nThis accords with our intuition in \u00a7 4.2. (3) The prompting and trained routers could even surpass\nhuman router (e.g., on roleplay questions; see more examples on WizardLM in App. I.2).\nWe discuss the consistency across three routers in App. C.3. The primary takeaways include: (1)\non Vicuna-80, there is a notable consistency among all three routers, and (2) on WizardLM, greater\ndiscrepancies emerge, with the trained router showing higher alignment with human annotations.\n5\nSOT IN THE CONTEXT OF LITERATURE\nThis section positions SoT in related work to reveal how SoT (1) is connected to, (2) is different\nfrom, and (3) can harness the power of other methods. See App. D for the expanded discussion.\nEfficient LLM methods at model and system levels. At the model level, prior work proposes ef-\nficient architectures, including dynamic mixture-of-experts (Lepikhin et al., 2021), low-complexity\nattention (Kitaev et al., 2020), and multi-query attention (Shazeer, 2019). However, they usually\nrequire a significant re-training cost. In contrast, compression methods require a smaller amount\nof fine-tuning cost by reducing the complexity of pre-trained LLMs, such as quantization (Frantar\net al., 2022) and weight or activation sparsification (Mishra et al., 2021; Zaheer et al., 2020).\nAt the system level, prior work (1) optimizes the computational graph (Dao et al., 2022), (2) op-\ntimizes the assignment and scheduling of computational graph on devices (Sheng et al., 2023), or\n(3) designs batching or caching mechanisms for serving multiple users (Fang et al., 2021). These\ntechniques address the large memory access and footprint posed by the vast model scale and atten-\ntion mechanism, and mainly aim at enhancing the throughput rather than the end-to-end latency.\nAs SoT trades off throughput for end-to-end latency, SoT can make these throughput-oriented tech-\nniques help with end-to-end latency. This interesting synergy offers opportunities for achieving\nbetter trade-offs between latency and throughput in future serving systems.\nIn contrast to model- and system-level techniques, SoT is a data-level technique in a new \u201ccontent\nco-organization for efficiency\u201d paradigm. See \u00a7 6 for more discussions.\nEfficient LLM methods through parallel generation. Some prior work also addresses the sequen-\ntial decoding issues. Speculative decoding (SD) methods (Stern et al., 2018) employ smaller models\nto generate some consecutive tokens sequentially and apply the target LLMs to verify them paral-\nlelly. Non-autoregressive generation (NAG) methods (Gu et al., 2018; Xiao et al., 2023) sample and\nrefine consecutive tokens parallelly, often with the support of a modified and tuned model.\n8\nPublished as a conference paper at ICLR 2024\nRelying on either assisting models or special models and sampling schemes, SD and NAG methods\nconduct parallel verification or sampling and refinement of consecutive tokens. In contrast, SoT\nprompts the LLM itself to plan the contents in a way that permits the parallel generation of tokens in\ndifferent segments, by exploiting the emerging instruction-following and planning ability of LLMs.\nPrompting methods for LLMs. Recent years have witnessed the emergence of the \u201cpre-train,\nprompt, and predict\u201d paradigm, which has shown promise in enhancing LLMs\u2019 quality in math and\ncommonsense reasoning (Wei et al., 2022; Kojima et al., 2022; Wang et al., 2022; Chen et al., 2022)\nand planning for multi-modality tasks (Shen et al., 2023; Zhu et al., 2023). Instead of focusing on\nanswer quality, SoT is a first attempt at exploiting the power of prompting to improve efficiency.\n6\nLIMITATIONS, FUTURE WORK, AND OPEN QUESTIONS\nAnswer quality evaluation. Our answer quality evaluation is far from perfect due to the limited\nprompt set, the potential bias of GPT-4 judges, and the inherent difficulty of evaluating LLM gener-\nations. Currently, we did not conduct human evaluation since it is easy for a human to tell whether\nan answer is generated with SoT due to its distinctive pattern, which might cause evaluation bias.\nEliciting or improving LLMs\u2019 ability. \u00a7 3.2.4 demonstrates SoT\u2019s potential of enhancing answer\nquality. It is part of a broader trend in recent research, exemplified by work including CoT (Kojima\net al., 2022; Wei et al., 2022), ToT (Yao et al., 2023), and ReAct (Yao et al., 2022), which collectively\naffirm the notion that explicitly articulating the thought process in language can elicit high-quality\nanswers from LLMs. These findings resemble human thinking: rather than relying solely on the\nfirst intuition or purely sequential thinking, we often document step-by-step reasoning or thought\norganization to attain high-quality answers. This intriguing parallel prompts us to explore further\nhow we can draw from the human thinking process to facilitate more effective and efficient AI.\nFor instance, SoT currently ignores the dependencies between points. A conceptually better way is\nto organize the points as Graph-of-Thoughts, where the edges represent the dependencies, and each\npoint is decoded conditioned on the contents of its ancestor points. In addition, instead of complying\nwith a static graph, we expect the need of having dynamic Graph-of-Thoughts, where the high-level\nthought structure is adjusted dynamically by LLMs themselves. This could potentially combine the\nefficiency and global thinking advantages of SoT with the logical reasoning and impromptu think-\ning strengths of methods like CoT (Kojima et al., 2022; Wei et al., 2022). Notably, a contemporary\nwork (Besta et al., 2023) has attempted to design Graph-of-Thoughts to elicit reasoning. Further-\nmore, it is interesting to explore how the SoT answers can be used to fine-tune LLMs to generate\nmore structured answers in a self-improving way (Zelikman et al., 2022; Huang et al., 2022).\nEfficiency and overhead of SoT in different scenarios. Serving systems commonly adopt batch\nprocessing to handle concurrent queries. This raises a concern of whether SoT may hurt serving\nthroughput due to parallel requests. (1) When there is an unsaturated number of concurrent queries,\nSoT can effectively reduce latency and enhance GPU utilization. Example scenarios include (a)\nEdge-side applications with a single user; (b) Centralized services during periods with unsaturated\nuser requests and underutilized computing capacity. It is interesting to study the appropriate SoT\ntriggering conditions based on system workloads. (2) When there is a saturated number of concur-\nrent queries, SoT is still useful for improving answer quality. However, in this case, it is important\nto consider the computation overhead from SoT. We delve into this concern in App. H.\nFor API-based models, a notable concern arises regarding the increased number of prefilling tokens\n(App. H). Given that many APIs charge token usage, SoT may lead to higher costs. To address this,\none can use prompt tuning to design shorter SoT prompts (Jiang et al., 2023).\nData-centric efficiency optimization. While data-centric engineering for improving answer qual-\nity (Zha et al., 2023; HazyResearch, 2023) is gaining popularity, its potential for inference efficiency\nis not explored yet. SoT is the first attempt. As LLM capabilities and the amount of LLM-generated\ndata are growing rapidly, data-centric techniques could become more useful in the future. To pave\nthe way towards that, there are a lot to explore. For example, the acceleration ratio of SoT depends\non the SoT prompt, the model, and the question, and thus not as predictable and controllable as\nmodel- or system-level techniques, which might hinder the practical adoption. We look forward to\nfuture work to unlock the full potential of data-centric efficiency optimization.\n9\nPublished as a conference paper at ICLR 2024\nACKNOWLEDGEMENTS\nWe thank Sergey Yekhanin (Microsoft Research), and Tianji Wu (Infinigence AI) for their support\nand suggestions on the work. We thank Tianyu Fu for many initial discussions on the idea. We thank\nKe Hong and Genghan Zhang for their discussions about profiling. We thank Yue Wu for the help on\nthe Claude scripts. We thank Da Yu, Chulin Xie, and Saiqian Zhang for their suggestions on revising\nthe first version of the paper. We thank Rui Hu, Cheng Cheng, Jack Jin, Zhoutong Ye, Mingze\nSun, Jun Yan, Zhi Zhang, Yuxuan Tong, Nianhui Guo, and Andrea Santilli for their suggestions on\nrevising the second version of the paper. We thank Chris Stetkiewicz, Amanda Melfi, and Amber\nTingle from Microsoft for their suggestions and help on writing. We thank the anonymous reviewers\nfor their insightful questions and suggestions.\nREFERENCES\nAnthropic. Introducing claude, May 2023. URL https://www.anthropic.com/index/\nintroducing-claude.\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna\nGajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et al.\nGraph of thoughts: Solving elaborate problems with large language models.\narXiv preprint\narXiv:2308.09687, 2023.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\nHan Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han.\nOnce-for-all: Train one\nnetwork and specialize it for efficient deployment. arXiv preprint arXiv:1908.09791, 2019.\nHarrison Chase.\nLangChain, October 2022.\nURL https://github.com/hwchase17/\nlangchain.\nCharlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John\nJumper. Accelerating large language model decoding with speculative sampling. arXiv preprint\narXiv:2302.01318, 2023a.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompt-\ning: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint\narXiv:2211.12588, 2022.\nZhaodong Chen, Zheng Qu, Yuying Quan, Liu Liu, Yufei Ding, and Yuan Xie. Dynamic n: M\nfine-grained structured sparse attention mechanism. In Proceedings of the 28th ACM SIGPLAN\nAnnual Symposium on Principles and Practice of Parallel Programming, pp. 369\u2013379, 2023b.\nZhihong Chen, Junying Chen, Hongbo Zhang, Feng Jiang, Guiming Chen, Fei Yu, Tiannan Wang,\nJuhao Liang, Chen Zhang, Zhiyi Zhang, Jianquan Li, Xiang Wan, Haizhou Li, and Benyou Wang.\nLlm zoo:\ndemocratizing chatgpt.\nhttps://github.com/FreedomIntelligence/\nLLMZoo, 2023c.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An\nopen-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https:\n//lmsys.org/blog/2023-03-30-vicuna/.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language mod-\nels. arXiv preprint arXiv:2210.11416, 2022.\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00b4e. Flashattention: Fast and memory-\nefficient exact attention with io-awareness. Advances in Neural Information Processing Systems,\n35:16344\u201316359, 2022.\n10\nPublished as a conference paper at ICLR 2024\nEmily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. Exploiting linear\nstructure within convolutional networks for efficient evaluation. Advances in neural information\nprocessing systems, 27, 2014.\nNing Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong\nSun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional\nconversations. arXiv preprint arXiv:2305.14233, 2023.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm:\nGeneral language model pretraining with autoregressive blank infilling. In Proceedings of the\n60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\npp. 320\u2013335, 2022.\nThomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. The\nJournal of Machine Learning Research, 20(1):1997\u20132017, 2019.\nAngela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. arXiv preprint\narXiv:1805.04833, 2018.\nJiarui Fang, Yang Yu, Chengduo Zhao, and Jie Zhou. Turbotransformers: an efficient gpu serv-\ning system for transformer models. In Proceedings of the 26th ACM SIGPLAN Symposium on\nPrinciples and Practice of Parallel Programming, pp. 389\u2013402, 2021.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter\nmodels with simple and efficient sparsity. The Journal of Machine Learning Research, 23(1):\n5232\u20135270, 2022.\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training\nquantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.\nPrakhar Ganesh, Yao Chen, Xin Lou, Mohammad Ali Khan, Yin Yang, Hassan Sajjad, Preslav\nNakov, Deming Chen, and Marianne Winslett. Compressing large-scale transformer-based mod-\nels: A case study on bert. Transactions of the Association for Computational Linguistics, 9:\n1061\u20131080, 2021.\nJoao Gante. Assisted generation: a new direction toward low-latency text generation. https:\n//huggingface.co/blog/assisted-generation, 2023. Accessed: 2023-06-23.\nGoogle. Tensorflow serving, 2021. URL https://github.com/tensorflow/serving.\nJiatao Gu, James Bradbury, Caiming Xiong, Victor O.K. Li, and Richard Socher. Non-autoregressive\nneural machine translation. In International Conference on Learning Representations, 2018. URL\nhttps://openreview.net/forum?id=B1l8BtlCb.\nSong Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks\nwith pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.\nHazyResearch.\nData-centric\nai.\nhttps://github.com/HazyResearch/\ndata-centric-ai, 2023. Accessed: 2023-07-04.\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei\nHan. Large language models can self-improve. arXiv preprint arXiv:2210.11610, 2022.\nYanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong\nLee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. Gpipe: Efficient training of giant neural\nnetworks using pipeline parallelism. Advances in neural information processing systems, 32,\n2019.\nAndrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is\nall you need: A case study on optimizing transformers. Proceedings of Machine Learning and\nSystems, 3:711\u2013732, 2021.\n11\nPublished as a conference paper at ICLR 2024\nHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Llmlingua: Compressing\nprompts for accelerated inference of large language models. In Proceedings of the 2023 Con-\nference on Empirical Methods in Natural Language Processing. Association for Computational\nLinguistics, December 2023. URL https://arxiv.org/abs/2310.05736.\nNikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv\npreprint arXiv:2001.04451, 2020.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large\nlanguage models are zero-shot reasoners. Advances in neural information processing systems,\n35:22199\u201322213, 2022.\nRaghuraman Krishnamoorthi. Quantizing deep convolutional networks for efficient inference: A\nwhitepaper. arXiv preprint arXiv:1806.08342, 2018.\nAlex Krizhevsky. One weird trick for parallelizing convolutional neural networks. arXiv preprint\narXiv:1404.5997, 2014.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E\nGonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model\nserving with pagedattention. arXiv preprint arXiv:2309.06180, 2023.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. {GS}hard: Scaling giant models with condi-\ntional computation and automatic sharding. In International Conference on Learning Represen-\ntations, 2021. URL https://openreview.net/forum?id=qrwe7XHTmYb.\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt\ntuning. arXiv preprint arXiv:2104.08691, 2021.\nYaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative\ndecoding. arXiv preprint arXiv:2211.17192, 2022.\nGuohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem.\nCamel: Communicative agents for \u201dmind\u201d exploration of large scale language model society,\n2023a.\nJiwei Li, Minh-Thang Luong, and Dan Jurafsky. A hierarchical neural autoencoder for paragraphs\nand documents. In Proceedings of the 53rd Annual Meeting of the Association for Computational\nLinguistics and the 7th International Joint Conference on Natural Language Processing (Volume\n1: Long Papers), pp. 1106\u20131115, 2015.\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv\npreprint arXiv:2101.00190, 2021.\nXuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following\nmodels. https://github.com/tatsu-lab/alpaca_eval, 2023b.\nYifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making\nlanguage models better reasoners with step-aware verifier. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5315\u2013\n5333, 2023c.\nZhuohan Li, Siyuan Zhuang, Shiyuan Guo, Danyang Zhuo, Hao Zhang, Dawn Song, and Ion Stoica.\nTerapipe: Token-level pipeline parallelism for training large-scale language models. In Interna-\ntional Conference on Machine Learning, pp. 6543\u20136552. PMLR, 2021.\nJi Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han.\nAwq:\nActivation-aware weight quantization for llm compression and acceleration.\narXiv preprint\narXiv:2306.00978, 2023.\n12\nPublished as a conference paper at ICLR 2024\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-\ntrain, prompt, and predict: A systematic survey of prompting methods in natural language pro-\ncessing. ACM Computing Surveys, 55(9):1\u201335, 2023.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach, 2019.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-\nence on Learning Representations, 2019.\nWenyan Lu, Guihai Yan, Jiajun Li, Shijun Gong, Yinhe Han, and Xiaowei Li. Flexflow: A flexible\ndataflow accelerator architecture for convolutional neural networks. In 2017 IEEE International\nSymposium on High Performance Computer Architecture (HPCA), pp. 553\u2013564. IEEE, 2017.\nXupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong,\nZhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. Specinfer: Accelerating\ngenerative llm serving with speculative inference and token tree verification.\narXiv preprint\narXiv:2305.09781, 2023.\nAsit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic, Ganesh Venkatesh,\nChong Yu, and Paulius Micikevicius. Accelerating sparse deep neural networks. arXiv preprint\narXiv:2104.08378, 2021.\nDeepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R Devanur, Gre-\ngory R Ganger, Phillip B Gibbons, and Matei Zaharia. Pipedream: Generalized pipeline par-\nallelism for dnn training. In Proceedings of the 27th ACM Symposium on Operating Systems\nPrinciples, pp. 1\u201315, 2019.\nDeepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia. Memory-efficient\npipeline-parallel dnn training. In International Conference on Machine Learning, pp. 7937\u20137947.\nPMLR, 2021.\nNVIDIA.\nFastertransformer,\n2019.\nURL\nhttps://github.com/NVIDIA/\nFasterTransformer.\nNVIDIA.\nTriton inference server, 2021.\nURL https://developer.nvidia.com/\ntriton-inference-server.\nOpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback.\nAdvances in Neural Information Processing Systems, 35:\n27730\u201327744, 2022.\nDuy Phung. Stablevicuna-13b, May 2023. URL https://huggingface.co/CarperAI/\nstable-vicuna-13b-delta.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. Measuring\nand narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350,\n2022.\nRatish Puduppully, Li Dong, and Mirella Lapata. Data-to-text generation with content selection\nand planning. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pp.\n6908\u20136915, 2019.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations\ntoward training trillion parameter models. In SC20: International Conference for High Perfor-\nmance Computing, Networking, Storage and Analysis, pp. 1\u201316. IEEE, 2020.\nJie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Min-\njia Zhang, Dong Li, and Yuxiong He. {ZeRO-Offload}: Democratizing {Billion-Scale} model\ntraining. In 2021 USENIX Annual Technical Conference (USENIX ATC 21), pp. 551\u2013564, 2021.\n13\nPublished as a conference paper at ICLR 2024\nAndrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Ric-\ncardo Marin, and Emanuele Rodol`a. Accelerating transformer inference for translation via paral-\nlel decoding. In acl, 2023.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess`\u0131, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to\nuse tools. arXiv preprint arXiv:2302.04761, 2023.\nSenseTime.\nLightllm.\nhttps://github.com/ModelTC/lightllm, 2023a.\nAccessed:\n2023-09-26.\nSenseTime.\nOpenppl.\nhttps://github.com/openppl-public/ppl.nn, 2023b.\nAc-\ncessed: 2023-09-26.\nZhihong Shao, Minlie Huang, Jiangtao Wen, Wenfei Xu, and Xiaoyan Zhu. Long and diverse text\ngeneration with planning-based hierarchical variational model. arXiv preprint arXiv:1908.06605,\n2019.\nNoam Shazeer.\nFast transformer decoding: One write-head is all you need.\narXiv preprint\narXiv:1911.02150, 2019.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang.\nHugginggpt: Solving ai tasks with chatgpt and its friends in huggingface.\narXiv preprint\narXiv:2303.17580, 2023.\nYing Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y Fu, Zhiqiang\nXie, Beidi Chen, Clark Barrett, Joseph E Gonzalez, et al. High-throughput generative inference\nof large language models with a single gpu. arXiv preprint arXiv:2303.06865, 2023.\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Autoprompt:\nEliciting knowledge from language models with automatically generated prompts. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp.\n4222\u20134235, 2020.\nMitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Blockwise parallel decoding for deep autore-\ngressive models. Advances in Neural Information Processing Systems, 31, 2018.\nZiteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ahmad Beirami, Himanshu Jain, Felix Yu,\nMichael Riley, and Sanjiv Kumar.\nSpectr: Fast speculative decoding via optimal transport.\nIn Workshop on Efficient Systems for Foundation Models @ ICML2023, 2023. URL https:\n//openreview.net/forum?id=d0mGsaheuT.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethink-\ning the inception architecture for computer vision. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp. 2818\u20132826, 2016.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori Hashimoto.\nAlpaca: A strong, replicable instruction-following model.\nhttps://crfm.stanford.edu/2023/03/13/alpaca.html, 2023. Accessed: 2023-\n06-23.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher,\nCristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy\nFu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,\nSaghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel\nKloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,\nDiana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,\nIgor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,\n14\nPublished as a conference paper at ICLR 2024\nAlan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh\nTang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen\nZhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,\nSergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models,\n2023b.\nGuan Wang, Sijie Cheng, Qiying Yu, and Changling Liu. Openllms: Less is more for open-source\nmodels, July 2023a. URL https://github.com/imoneoi/openchat.\nHanrui Wang, Zhekai Zhang, and Song Han. Spatten: Efficient sparse attention architecture with\ncascade token and head pruning. In 2021 IEEE International Symposium on High-Performance\nComputer Architecture (HPCA), pp. 97\u2013110. IEEE, 2021.\nSinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention\nwith linear complexity. arXiv preprint arXiv:2006.04768, 2020.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-\nery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models.\narXiv preprint arXiv:2203.11171, 2022.\nZifu Wang, Teodora Popordanoska, Jeroen Bertels, Robin Lemmens, and Matthew B Blaschko. Dice\nsemimetric losses: Optimizing the dice score with soft labels. In Medical Image Computing and\nComputer Assisted Intervention, 2023b.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint\narXiv:2109.01652, 2021.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\nNeural Information Processing Systems, 35:24824\u201324837, 2022.\nWei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in\ndeep neural networks. Advances in neural information processing systems, 29, 2016.\nGuangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han.\nSmoothquant:\nAccurate and efficient post-training quantization for large language models.\narXiv preprint\narXiv:2211.10438, 2022.\nYisheng Xiao, Lijun Wu, Junliang Guo, Juntao Li, Min Zhang, Tao Qin, and Tie-yan Liu. A survey\non non-autoregressive generation for neural machine translation and beyond. IEEE Transactions\non Pattern Analysis and Machine Intelligence, 2023.\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and\nDaxin Jiang. Wizardlm: Empowering large language models to follow complex instructions.\narXiv preprint arXiv:2304.12244, 2023.\nYuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake Hechtman, Yanping Huang, Rahul Joshi,\nMaxim Krikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni, et al.\nGspmd: general and\nscalable parallelization for ml computation graphs. arXiv preprint arXiv:2105.04663, 2021.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReact: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629,\n2022.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik\nNarasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv\npreprint arXiv:2305.10601, 2023.\nGyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. Orca: A\ndistributed serving system for {Transformer-Based} generative models. In 16th USENIX Sympo-\nsium on Operating Systems Design and Implementation (OSDI 22), pp. 521\u2013538, 2022.\n15\nPublished as a conference paper at ICLR 2024\nManzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago\nOntanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for\nlonger sequences. Advances in neural information processing systems, 33:17283\u201317297, 2020.\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with\nreasoning. Advances in Neural Information Processing Systems, 35:15476\u201315488, 2022.\nDaochen Zha, Zaid Pervaiz Bhat, Kwei-Herng Lai, Fan Yang, Zhimeng Jiang, Shaochen Zhong, and\nXia Hu. Data-centric artificial intelligence: A survey. arXiv preprint arXiv:2303.10158, 2023.\nYujia Zhai, Chengquan Jiang, Leyuan Wang, Xiaoying Jia, Shang Zhang, Zizhong Chen, Xin Liu,\nand Yibo Zhu. Bytetransformer: A high-performance transformer boosted for variable-length\ninputs. arXiv preprint arXiv:2210.03052, 2022.\nYifan Zhang, Jingqin Yang, Yang Yuan, and Andrew Chi-Chih Yao. Cumulative reasoning with\nlarge language models. arXiv preprint arXiv:2308.04371, 2023.\nLianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping Huang, Yida\nWang, Yuanzhong Xu, Danyang Zhuo, Eric P Xing, et al. Alpa: Automating inter-and {Intra-\nOperator} parallelism for distributed deep learning. In 16th USENIX Symposium on Operating\nSystems Design and Implementation (OSDI 22), pp. 559\u2013578, 2022.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.\nJudging llm-as-a-judge with mt-bench and chatbot arena, 2023.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat,\nPing Yu, Lili Yu, et al. Lima: Less is more for alignment, 2023.\nZhe Zhou, Xuechao Wei, Jiejing Zhang, and Guangyu Sun.\n{PetS}: A unified framework for\n{Parameter-Efficient} transformers serving.\nIn 2022 USENIX Annual Technical Conference\n(USENIX ATC 22), pp. 489\u2013504, 2022.\nXizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li,\nLewei Lu, Xiaogang Wang, et al. Ghost in the minecraft: Generally capable agents for open-world\nenviroments via large language models with text-based knowledge and memory. arXiv preprint\narXiv:2305.17144, 2023.\nBarret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. In Interna-\ntional Conference on Learning Representations (ICLR), 2017.\n16\nPublished as a conference paper at ICLR 2024\nAppendix\nTable of Contents\nA Model Details\n18\nB\nImplementation Details of Skeleton-of-Thought\n18\nB.1\nPrompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\nB.2\nSupporting Multi-Round Conversation\n. . . . . . . . . . . . . . . . . . . . . .\n20\nC Implementation Details of Skeleton-of-Thought with Router\n20\nC.1\nPrompting Router . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nC.2\nTrained Router . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nC.3\nRouter Consistency\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\nC.4\nConcurrent execution for SoT-R . . . . . . . . . . . . . . . . . . . . . . . . . .\n21\nD SoT In the Context of Literature (Expanded)\n22\nD.1\nEfficient LLMs\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\nD.2\nPrompting Methods for LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\nD.3\nHierarchical Text Generation\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\nE\nEfficiency Analysis\n24\nF\nEfficiency Profiling\n26\nG Efficiency Evaluation\n27\nG.1\nSkeleton-of-Thought . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\nG.2\nSkeleton-of-Thought with Router . . . . . . . . . . . . . . . . . . . . . . . . .\n29\nH Overhead of SoT in Different Scenarios\n31\nI\nAnswer Quality Evaluation\n32\nI.1\nSkeleton-of-Thought . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\nI.2\nSkeleton-of-Thought with Router . . . . . . . . . . . . . . . . . . . . . . . . .\n44\nI.3\nQuality Comparison with Longer Normal Answer\n. . . . . . . . . . . . . . . .\n46\nI.4\nChatGPT-3.5 as the Judge . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n46\nJ\nCombining SoT-R with Model Quantization\n48\nJ.1\nSpeed-ups of SoT + Quantization on Quantized Models\n. . . . . . . . . . . . .\n49\nJ.2\nSpeed-ups of SoT + Quantization on Unquantized Models . . . . . . . . . . . .\n49\nK Additional SoT-R statistics\n50\nK.1\nNumber of Suitable Questions . . . . . . . . . . . . . . . . . . . . . . . . . . .\n50\nK.2\nPeak Memory Overhead . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n50\nK.3\nSpeed-ups with Different Number of Points . . . . . . . . . . . . . . . . . . . .\n51\nL\nNotes on Application Scenarios\n51\n17\nPublished as a conference paper at ICLR 2024\nA\nMODEL DETAILS\nTable 1 summarizes the models on which we evaluate SoT. We use GPT-4 in the main paper and\nChatGPT-3.5 in App. I.4 as the judge in FastChat and LLMZoo evaluation.\nTable 1: Models evaluated with SoT. All the open-source models are fine-tuned from LLaMA mod-\nels.\nAccess\nModel Name\nInstitution\nReleased Date\nOpen-Source\nLLaMA2-Chat-7B (Touvron et al., 2023b)\nMeta & Microsoft\n2023/07\nLLaMA2-Chat-13B (Touvron et al., 2023b)\nMeta & Microsoft\n2023/07\nOpenChat-13B (Wang et al., 2023a)\nTsinghua\n2023/07\nVicuna-7B V1.3 (Chiang et al., 2023)\nLMSYS\n2023/06\nVicuna-13B V1.3 (Chiang et al., 2023)\nLMSYS\n2023/06\nVicuna-33B V1.3 (Chiang et al., 2023)\nLMSYS\n2023/06\nStableVicuna-13B (Phung, 2023)\nCarperAI\n2023/05\nUltraLM-13B (Ding et al., 2023)\nOpenBMB & Tsinghua\n2023/05\nVicuna-7B V1.1 (Chiang et al., 2023)\nLMSYS\n2023/03\nAPI-Based\nClaude (Anthropic, 2023)\nAnthropic\n2023/05\nChatGPT-3.5\nOpenAI\n2022/11\nGPT-4\nOpenAI\n2023/03\nTable 2 shows sources of the models we use in the paper.\nTable 2: The Hugging Face or API endpoints of the models.\nAccess\nModel Name\nHugging Face or API Endpoints\nOpen-Source\nLLaMA2-Chat-7B (Touvron et al., 2023b)\nmeta-llama/Llama-2-7b-chat-hf\nLLaMA2-Chat-13B (Touvron et al., 2023b)\nmeta-llama/Llama-2-13b-chat-hf\nOpenChat-13B (Wang et al., 2023a)\nopenchat/openchat\nVicuna-7B V1.3 (Chiang et al., 2023)\nlmsys/vicuna-7b-v1.3\nVicuna-13B V1.3 (Chiang et al., 2023)\nlmsys/vicuna-13b-v1.3\nVicuna-33B V1.3 (Chiang et al., 2023)\nlmsys/vicuna-33b-v1.3\nStableVicuna-13B (Phung, 2023)\nCarperAI/stable-vicuna-13b-delta2\nUltraLM-13B (Ding et al., 2023)\nopenbmb/UltraLM-13b2\nVicuna-7B V1.1 (Chiang et al., 2023)\nlmsys/vicuna-7b-delta-v1.1\nAPI-Based\nClaude (Anthropic, 2023)\nClaude extension on Slack3\nChatGPT-3.5\nAzure OpenAI, gpt-35-turbo 0301 version4\nGPT-4\nOpenAI, gpt-4-0613 version\nB\nIMPLEMENTATION DETAILS OF SKELETON-OF-THOUGHT\nB.1\nPROMPT\nThe skeleton prompt is shown in Prompts 1 and 3 and the point-expanding prompt is shown in\nPrompt 2.\nSkeleton prompt template. In order to make the output skeleton short and in a consistent format for\nthe good of efficiency and ease of point extraction, the skeleton prompt template (1) describes the\n2For\nconvenience,\nwe\nuse\nthe\nnon-official\nendpoint\nTheBloke/stable-vicuna-13B-HF\nand\nTheBloke/UltraLM-13B-fp16 to get merged weights.\n3https://www.anthropic.com/claude-in-slack\n4https://azure.microsoft.com/en-us/products/ai-services/openai-service\n18\nPublished as a conference paper at ICLR 2024\nPrompt 3. Skeleton Prompt Template T s (with Two-Shot Demonstrations)\n[User:] You\u2019re an organizer responsible for only giving the skeleton (not the full content) for answering the question.\nProvide the skeleton in a list of points (numbered 1., 2., 3., etc.) to answer the question. Instead of writing a full\nsentence, each skeleton point should be very short with only 3\u223c5 words. Generally, the skeleton should have 3\u223c10\npoints.\nQuestion:\nWhat are the typical types of Chinese dishes?\nSkeleton:\n1. Dumplings.\n2. Noodles.\n3. Dim Sum.\n4. Hot Pot.\n5. Wonton.\n6. Ma Po Tofu.\n7. Char Siu.\n8. Fried Rice.\nQuestion:\nWhat are some practical tips for individuals to reduce their carbon emissions?\nSkeleton:\n1. Energy conservation.\n2. Efficient transportation.\n3. Home energy efficiency.\n4. Reduce water consumption.\n5. Sustainable diet.\n6. Sustainable travel.\nNow, please provide the skeleton for the following question.\n{question}\nSkeleton:\n[Assistant:] 1.\ntask precisely, and (2) provides a partial answer \u201c1.\u201d for the LLM to continue writing. The skeleton\nresponses are in the desired format in most cases. Therefore, we can use a simple regular expression\n(\\d+)\\.\\s?([\\s\\S]+?)(?=\\n|\\n*$) to extract point indexes and point skeletons from the\nskeleton response.\nWe find that GPT-4 can work well without the two demonstrations in the skeleton prompt. Therefore,\nwe do not include the two demonstrations for GPT-4 (Prompt 1). For all other models, the two\ndemonstrations are included, as shown in Prompt 3.\nPoint-expanding prompt template. It describes the point-expanding task and provides a partial\nanswer. We also provide instructions \u201cWrite it **very shortly** in 1\u223c2 sentence\u201d so that the LLMs\nkeep the answers concise. Unlike the skeleton prompt template, we find that demonstrations are not\nnecessary to get reasonable results.\nWe find that Claude and GPT-4 follows the instruction \u201cWrite it **very shortly** in 1\u223c2 sentence\nand do not continue with other points!\u201d in Prompt 2 very well, so that the answers are very short.\nTherefore, we delete \u201c**very shortly**\u201d from the prompt template in Claude and GPT-4.\nPartial answer.\nIn the Prompts 1 and 2, we provide partial answers so that LLMs can follow the\ndesired response format better.\nWe can put the partial answer at the end of the prompt for the open-source models to continue\nwriting. An implementation detail is that different open-source models have different conversa-\ntion templates (i.e., different ways to combine user and assistant messages into one string). For\nexample, Vicuna (Chiang et al., 2023) uses the string \u201cUSER:\u201d and \u201c ASSISTANT:\u201d for the place-\nholder \u201c[User:]\u201d and \u201c[Role]\u201d in the Prompts 1 and 2, respectively, while UltraLM (Ding et al.,\n2023) uses \u201cUser:\u201d and \u201c\u2329/s\u232aAssistant:\u201d. We build our open-source model experiments with the\nhelp of the FastChat codebase (Zheng et al., 2023), in which the conversation templates of many\nmodels are already handled correctly. We implement the conversation templates of OpenChat-13B,\nStableVicuna-13B, and UltraLM-13B according to their official guides and codes.\nFor ChatGPT-3.5, we provide partial answers as a last message in the chat history from the assistant.\nNote that it is not a documented approach. We find it works well in most cases, in that ChatGPT-3.5\n19\nPublished as a conference paper at ICLR 2024\nPrompt 4. LLM Prompting as the Router\n[User:] Question: {question}\nHow would you like to answer the question?\nA. Organize the answer as a list of points or perspectives (in the format of 1., 2., 3., etc.), and the points or perspectives\ncan be answered independently without referring to the contents of the previous points.\nB. Organize the answer as a list of points or perspectives (in the format of 1., 2., 3., etc.), and the contents of later points\nor perspectives cannot be answered independently without referring to the contents of the previous ones.\nC. Do not organize the answer as a list of points or perspectives.\nJust say A, B, or C. Do not explain. Do not provide an answer to the question.\n[Assistant:]\ncontinues the texts from the provided partial answer. However, in some rare cases, ChatGPT-3.5\nrepeats the provided partial answers.\nFor Claude over Slack, there is no obvious way to give the API a partial answer. We resort to\nmodifying the prompt template slightly by adding\nPlease start your answer from \u201c{partial answer}\u201d and do not output other things before that\nat the end. We find that Claude understands and obeys it well. For GPT-4, we also take this approach.\nSystem Message.\nWe do not include the system message in the prompts for open-source models\nexcept LLaMA2.\nThe partial answer, \u201c**very shortly**\u201d, and the 2-shot demonstrations discussed above are the only\ndifferences between the prompts we used across all models and all evaluations.\nB.2\nSUPPORTING MULTI-ROUND CONVERSATION\nTo use SoT in a multi-round conversation, we can just put the question and the final aggregated\nanswer in the history, removing all the SoT prompts. In this way, using SoT in one conversation\nround will not introduce additional prefill cost in future rounds.\nC\nIMPLEMENTATION DETAILS OF SKELETON-OF-THOUGHT WITH ROUTER\nC.1\nPROMPTING ROUTER\nWe use Prompt 4 for querying GPT-4 as the router. If the answer is \u201cA\u201d (i.e., the question can be\nanswered in a list of independent points), we will use SoT. Otherwise, if the answer is \u201cB\u201d (i.e., the\nanswer is in a list of points but they depend on each other) or \u201cC\u201d (i.e., the answer should not be in\na list of points), SoT is not suitable and we will fall back to normal decoding.\nC.2\nTRAINED ROUTER\nWe tackle the routing problem as a sequence classification task. We first annotate the LIMA training\nset (Zhou et al., 2023), and then fine-tune a RoBERTa model (Liu et al., 2019) using the labeled\ndata. Finally, we apply the tuned RoBERTa as the router on Vicuna-80 and WizardLM. We detail\nthe steps in the following.\nC.2.1\nANNOTATION PROCESS\nIn the classification task, a label of 1 (positive) indicates that this question can be answered with\nSoT, while a label of 0 (negative) suggests that using the normal generation mode is more suitable.\nWe annotate the LIMA training set, which consists of 1,030 Q&As sourced from three community\nwebpages: Stack Exchange, wikiHow, and the Pushshift Reddit. We also annotate the Vicuna-80\nand WizardLM datasets for evaluation.\n20\nPublished as a conference paper at ICLR 2024\nTable 3: Router confusion matrices on the Vicuna-80 dataset. Left: Rows are human annotations\n(H) and columns are the GPT-4 router (G). Middle: Rows are human annotations (H) and columns\nare the RoBERTa router (R). Right: Rows are the GPT-4 router (G) and columns are the RoBERTa\nrouter (R).\nG0\nG1\nH0\n38\n5\nH1\n0\n37\nR0\nR1\nH0\n37\n6\nH1\n5\n32\nR0\nR1\nG0\n34\n4\nG1\n8\n34\nTable 4: Router confusion matrices on the WizardLM dataset. Left: Rows are human annotations\n(H) and columns are the GPT-4 router (G). Middle: Rows are human annotations (H) and columns\nare the RoBERTa router (R). Right: Rows are the GPT-4 router (G) and columns are the RoBERTa\nrouter (R).\nG0\nG1\nH0\n94\n66\nH1\n3\n55\nR0\nR1\nH0\n135\n25\nH1\n31\n27\nR0\nR1\nG0\n93\n4\nG1\n73\n48\nWe use GPT-4 to assist the annotation process. Specifically, we present each question to GPT-4 and\nanalyze its answer to determine whether SoT can be triggered for this question. We assign a positive\nlabel to a question if GPT-4\u2019s response meets two criteria: (1) it contains a list of points that can be\nexpanded in parallel, (2) each point provides sufficient details (i.e., the point-expanding response is\nnot too short), which will enable SoT to achieve a speed-up. Two of the paper\u2019s authors conduct the\nannotation process independently, and discuss the inconsistent annotations to decide the final label.\nC.2.2\nTRAINING DETAILS\nWe use roberta-base with 120M parameters as the router model. The finetuning is conducted\nusing the AdamW optimizer (Loshchilov & Hutter, 2019) with a weight decay of 0.01. The learning\nrate undergoes a warm-up phase during the first 1% of iterations to 5e-5 and then decays linearly.\nWe train the model for 2 epochs using a batch size of 32. Input sequences are either padded or\ntruncated to achieve a consistent length of 512 tokens.\nIn the application of SoT, false positives (SoT is incorrectly triggered when it should not be, resulting\nin degraded answer quality) are of more significant concern than false negatives (the router misses a\npotential SoT trigger, resulting in a reduced speed-up). Thus, to mitigate false positives, we employ\nthe Tversky loss (Wang et al., 2023b) with parameters \u03b1 = 0.7 and \u03b2 = 0.3, which penalizes false\npositives more heavily than false negatives. We also incorporate label smoothing (Szegedy et al.,\n2016) with a factor of \u03f5 = 0.2. Overall, the entire fine-tuning process is efficient, completing in 2\nminutes on an NVIDIA A100 GPU.\nC.3\nROUTER CONSISTENCY\nWe present the confusion matrices for the three routers to illustrate their consistency. The results on\nVicuna-80 and WizardLM are shown in Tables 3 and 4, respectively.\nOn Vicuna-80, we can observe a notable level of agreement among the three routers. Compared with\nthe GPT-4-prompting router, the trained router exhibits a slightly higher number of false negatives\nw.r.t. the human annotations. Conversely, on WizardLM, given the intricate answer structure and\nthe presence of many ambiguous cases, the routers show significant discrepancies. Specifically, the\nGPT-4 router produces many false positives, which pose adverse affects on the answer quality (see\nApp. I.2). The RoBERTa router aligns more closely with the human annotations.\nC.4\nCONCURRENT EXECUTION FOR SOT-R\nIn SoT-R, the router serves as an additional stage that extends the two-stage SoT pipeline, as illus-\ntrated in Fig. 9. To push the limit of latency optimization, we can run the router, normal generation,\nand SoT generation concurrently. Once the router makes a decision, one of the normal and SoT\ngeneration processes can be aborted. However, this approach will increase the token overhead.\nTherefore, we did not employ this approach in this work and leave it to future work.\n21\nPublished as a conference paper at ICLR 2024\nRouter\nSkeleton\nExpand\npositive\nnegative\nDecode\nQuestion\nAnswer\nAnswer\nRouter\nSkeleton\nExpand\nDecode\nQuestion\nAnswer\nAnswer\npositive\nnegative\nFigure 9: Left: The SoT-R pipeline. Right: A possible approach to further reduce latency at the\ncost of token overhead.\nD\nSOT IN THE CONTEXT OF LITERATURE (EXPANDED)\nD.1\nEFFICIENT LLMS\nExtensive research has been dedicated to enhancing the throughput and latency of LLM infer-\nence. We first discuss model-level architecture design or compression techniques. These techniques\nchange the model and can benefit both the latency and throughput but require finetuning to retain the\nmodel quality. Then, we discuss system-level efforts that optimize the computational graph or the\nassignment and scheduling of the computational graph on computation and storage devices. Most\nsystem-level efforts accelerate the prefilling phase or focus on improving the throughput. Finally,\nwe discuss some research efforts that share a similar motivation to ours, namely, addressing the\nefficiency issue of sequential decoding.\nModel-level optimization.\nConsiderable architectural design efforts have emerged to (1) improve\nthe scalability w.r.t. model size by introducing mixture-of-expert inference (Lepikhin et al., 2021;\nFedus et al., 2022), (2) address the quadratic complexity w.r.t. input size of attention by designing\nnew attention mechanisms (Kitaev et al., 2020; Wang et al., 2020), (3) reduce the memory access\nand footprint of attention by using multi-query attention (Shazeer, 2019), and so on. However, these\nmethods usually require a substantial re-training cost. The model compression techniques require a\nsmaller amount of fine-tuning by reducing the model complexity of a pre-trained LLM from certain\naspects (Ganesh et al., 2021). Representative techniques include quantization (Xiao et al., 2022;\nFrantar et al., 2022; Lin et al., 2023), the static or dynamic pruning of weights, activation, and\nattention (Mishra et al., 2021; Zaheer et al., 2020; Wang et al., 2021; Chen et al., 2023b), and so on.\nZooming out from LLM compression to the whole field of model compression, we can see that\nmodel co-design or compression for efficiency has received tremendous attention in the past few\nyears and has grown into large research fields, such as pruning (Han et al., 2015; Wen et al., 2016),\nquantization (Krishnamoorthi, 2018), factorization (Denton et al., 2014), and neural architecture\nsearch (Zoph & Le, 2017; Elsken et al., 2019; Cai et al., 2019). Different from the model co-design\nparadigm, SoT is in a \u201ccontent co-organization for efficiency\u201d paradigm for improving the LLM\nefficiency. Along with the growth in the LLM capabilities and amount of LLM-generated data,\ndata-level techniques could become important tools in the efficient LLM toolbox.\nSystem-level optimization.\nIn the realm of lossless acceleration, considerable efforts have been\ndevoted to addressing the I/O-bound nature of LLMs on modern hardware platforms (Dao et al.,\n2022). Numerous studies (Dao et al., 2022; Zhai et al., 2022; Ivanov et al., 2021; NVIDIA, 2019)\nhave focused on adjusting the computational graph by fusing and implementing operations in an\nI/O-friendly way. As a representative method, FlashAttention (Dao et al., 2022) fuses all operations\nof one attention into one GPU kernel with spatially tiled computation to reduce the off-chip I/O of\nthe attention map. While FlashAttention can effectively accelerate training and the prefilling phase\nof inference, it cannot accelerate the decoding phase much (when the batch size is small), as it is\nthe I/O of weights rather than activation or attention map that bottlenecks the decoding phase. For\nexample, when the context length is 64, decoding one token using LLaMA-7B needs to load each\nof the 7B parameters from the off-chip HBM onto the GPU chip at least once, but only transferring\nabout 20M (0.02B) activation values between the off-chip HBM and GPU chip.\nIn order to satisfy Service Level Objectives, serving systems focus on improving the serving\nthroughput under latency constraints. To this end, serving systems (Fang et al., 2021; NVIDIA,\n22\nPublished as a conference paper at ICLR 2024\n2021; Google, 2021) pack multiple queries together into a batch to improve the hardware utiliza-\ntion. The batching technique has proven highly effective in enhancing throughput, leading to the\ndevelopment of various variants. For example, some work designs methods to decide which queries\nto batch together (Fang et al., 2021; Zhou et al., 2022), while others selectively batch parts of the\nmodel to enable fine-grained iteration-level batching (Yu et al., 2022) or multi-task batching (Zhou\net al., 2022). Various model parallelism (Lu et al., 2017; Huang et al., 2019; Narayanan et al.,\n2019; Rajbhandari et al., 2020; Narayanan et al., 2021; Li et al., 2021; Zheng et al., 2022) and\noffloading (Ren et al., 2021; Sheng et al., 2023) techniques have been proposed to maximize the\nthroughput of LLM training or inference. In a nutshell, given the computational graph and device\nconfigurations, these techniques optimize the split, assignment, and scheduling of computations,\nstorage, and communications on devices. In addition to the model parallelism and batching tech-\nniques, an efficient memory management mechanism for LLM workloads is also an essential feature\nin the serving systems (Kwon et al., 2023; SenseTime, 2023a;b).\nTo sum up, these system-level techniques mainly help with the throughput in training and batched\ninference. They can be used by SoT to improve the throughput of the batched decoding of multiple\nsegments. This means that SoT can harness the power of these throughput-oriented techniques and\nmake them help with the end-to-end latency, offering a new dimension for better trading off latency\nand throughput in future serving systems.\nAnother parallelism perspective to position SoT is that SoT guides the LLM to adjust the sequen-\ntial workload to become \u201cinter-content\u201d parallelizable, which differs from the parallelism levels\nin existing serving systems, including inter-instance (Krizhevsky, 2014; Rajbhandari et al., 2020),\ninter-operation (Huang et al., 2019; Narayanan et al., 2019; 2021), intra-operation (Xu et al., 2021),\nand inter-token (Li et al., 2021). It may be worthwhile to explore the integration of SoT into serving\nsystems to maximize the hardware utilization.\nDecoding optimization.\nOne bottleneck for the end-to-end latency lies in the autoregressive de-\ncoding phase, where tokens must be generated one by one. Due to the dependency between tokens,\nthe computation of different tokens cannot be parallelized, causing severe under-utilization of GPU.\nIn order to improve the end-to-end decoding latency of a given LLM, speculative decoding meth-\nods (Stern et al., 2018; Leviathan et al., 2022; Chen et al., 2023a; Gante, 2023; Sun et al., 2023;\nMiao et al., 2023) propose to use cheaper approaches to generate short candidate token sequences,\nfor example, by sequentially decoding with an assisting model much smaller than the given LLM.\nThen, they use the LLM to parallelly verify the candidates and keep the prefix sequence that matches\nthe LLM\u2019s verification results.\nAnother line of work that shares the motivation of addressing the autoregressive efficiency issue is\nnon-autoregressive generation (NAG) methods (Gu et al., 2018; Xiao et al., 2023). NAG methods\nsample consecutive tokens parallelly, often with the aid of a modified and tuned model. To maintain\nthe answer quality, instead of sampling for one iteration, many NAG methods refine the output\nparallelly for multiple iterations (Xiao et al., 2023; Santilli et al., 2023).\nTo summarize, the speculative decoding methods use assisting models for letting the LLM conduct\nparallel verification of consecutive tokens, and the NAG methods rely on specially designed models,\ntraining schemes, or sampling schemes for the parallel sampling and refinement of consecutive to-\nkens. In contrast, SoT prompts the LLM itself to plan the contents in a way that permits the parallel\ngeneration of multiple tokens in different segments. SoT exploits the emerging instruction-following\nand planning ability of SoTA LLMs rather than relying on specially designed modeling, sampling,\nand training schemes. This is different from all existing work that targets the autoregressive effi-\nciency issue.\nD.2\nPROMPTING METHODS FOR LLMS\nIn recent years, the \u201cpre-train, prompt, and predict\u201d paradigm has emerged (Liu et al., 2023), which\ndesigns prompts comprising task descriptions and (optionally) a few demonstrations to guide pre-\ntrained LLMs in generating answers for a wide range of downstream tasks. Researchers found that\ninstruction-tuned LLMs (Brown et al., 2020; Wei et al., 2021; Ouyang et al., 2022; Chung et al.,\n2022; Taori et al., 2023) possess a strong ability to (1) generalize to new tasks thanks to the diverse\n23\nPublished as a conference paper at ICLR 2024\nnatural language descriptions encountered during instruction tuning, and (2) learn in-context using\na few demonstrations without weight tuning.\nIn virtue of these abilities, the field has been manually engineering (Brown et al., 2020; Kojima\net al., 2022; Shen et al., 2023; Li et al., 2023a), automatic searching (Shin et al., 2020), or continu-\nously tuning (Li & Liang, 2021; Lester et al., 2021) the prompts for uncovering the capabilities of\nLLMs on downstream tasks. There are a bunch of prompting methods that improves the reasoning\nperformance of LLMs by designing thinking flows mimicking human reasoning: (1) mimicking the\nstep-by-step or compositional thinking structure (Wei et al., 2022; Kojima et al., 2022; Press et al.,\n2022; Yao et al., 2023; Besta et al., 2023; Zhang et al., 2023), (2) designing multiple reasoning paths\nand their aggregation (Wang et al., 2022; Yao et al., 2023; Li et al., 2023c), and (3) using tools for\ncalculation and information retrieval (Chen et al., 2022; Yao et al., 2022; Schick et al., 2023). As\na representative example, the Chain-of-Thought prompts largely improve the performance on tasks\nthat require logical reasoning by simply providing a \u201cLet\u2019s think step by step\u201d (Kojima et al., 2022)\ninstruction or a few demonstrations (Wei et al., 2022). Another topic that arises quite a surge of in-\nterests is to prompt LLMs to help finish complex multi-modality task (Shen et al., 2023; Zhu et al.,\n2023). For example, HuggingGPT (Shen et al., 2023) design prompts to guide the LLM to generate\nstructural JSON for the orchestration of multi-model execution to finish complex tasks.\nTo summarize, the large literature on prompting methods has been aiming at uncovering different\ncapabilities of LLM and improving the answer quality on different downstream tasks. In contrast,\nSoT is a first attempt at exploiting the power of prompting to improve efficiency.\nD.3\nHIERARCHICAL TEXT GENERATION\nSoT can be regarded as being \u201chierarchical\u201d since it has high-level answer structure planning. Prior\nstudies in hierarchical text generation (Li et al., 2015; Shao et al., 2019; Puduppully et al., 2019;\nFan et al., 2018) all focus on enhancing the answer quality, including improving the long-range\ncoherence, relevance to the topic, or reducing redundancy. These methods craft hierarchical neural\narchitectures that contain different modules to model high-level (sentence-level or document-level)\nand low-level (word-level) dependencies (Li et al., 2015; Shao et al., 2019; Fan et al., 2018). They\nstill employ sequential word-by-word generation without parallelization between sentences.\nNote that the sentence-level representations in previous work (Li et al., 2015; Shao et al., 2019) are\n\u201cimplicit\u201d latent variables instead of \u201cexplicit\u201d language descriptions. Some previous studies (Shao\net al., 2019; Puduppully et al., 2019) train a dedicated planning module to execute explicit content\nplanning in advance. Nevertheless, these methods all conduct \u201cclosed-form\u201d planning that only\nreorders and groups the input keywords, rather than producing \u201cfree-form\u201d plans on \u201cwhat to say\u201d\nand \u201chow to say\u201d. All the hierarchical architectures and planning modules require training or even\nspecial data processing (Puduppully et al., 2019).\nTo summarize, in terms of the objective, the primary focus of SoT \u2013 efficient generation \u2013 is dif-\nferent from previous hierarchical text generation literature. In terms of the methodology, instead of\ndesigning new hierarchical architectures or planning modules, SoT exploits the emerging planning\nand instruction-following abilities of LLMs to do explicit (which means the plan is described by in-\nterpretable language) and free-form planning. This allows SoT to be applied to off-the-shelf LLMs\nfor producing structured answers.\nAs the hierarchical text generation literature focuses on enhancing answer quality, they could pro-\nvide inspiration for future expansions of SoT to generate high-quality answers for broader types of\nquestions.\nE\nEFFICIENCY ANALYSIS\nThis section gives a detailed explanation on why SoT can reduce the overall decoding latency with\nthe same computational resource for local models.\nThe vanilla approach processes only one question and decodes the answers sequentially, whereas\nSoT processes multiple point-expanding requests and the answers in a batch. We focus on the\nfollowing question: \u201cCompared to processing only one sequence, how much peak memory overhead\nand latency increase will be brought by processing a batch of sequences?\u201d\n24\nPublished as a conference paper at ICLR 2024\nTable 5: The latency and average GPU performance of the prefilling and decoding phases when\ninferencing LLMs. The prefilling token length is 128, the decoding token length is 64, and the batch\nsize is 1. The test is run on one NVIDIA A100 GPU.\nModel\nPrefill/Decode Latency (ms)\nPrefill/Decode GPU Perf. (TFLOPS)\nLLaMA-7B\n40 / 2735\n43 / 0.31\nLLaMA-13B\n54 / 3725\n62 / 0.44\nLLaMA-33B\n100 / 5506\n85 / 0.75\n1\n2\n3\n4\n5\n6\n7\n8\n9\nB\n3000\n3500\n4000\n4500\n5000\n5500\nLLaMA-7B\nLLaMA-13B\nLLaMA-33B\n(a) Latency (ms)\n1\n2\n3\n4\n5\n6\n7\n8\n9\nB\n0\n1\n2\n3\n4\n5\n6\nLLaMA-7B\nLLaMA-13B\nLLaMA-33B\n(b) Actual GPU Perf. (TFLOPS)\n1\n2\n3\n4\n5\n6\n7\n8\n9\nB\n20\n30\n40\n50\n60\n70\nLLaMA-7B\nLLaMA-13B\nLLaMA-33B\n(c) Peak Memory (GB)\nFigure 10: The trends of latency, average GPU performance of decoding one token, and peak mem-\nory with respect to the batch size B of sequences. The prefilling token length is 128, and the\ndecoding token length is 64. The test is run on one NVIDIA A100 GPU.\nA typical LLM generative process consists of two phases: (1) the prefilling phase in which the\nprompt is parsed to generate the key-value cache for further use, and (2) the decoding phase in\nwhich tokens are generated one by one in a sequential manner. The decoding phase accounts for\nthe majority of the end-to-end latency, especially when generating a long response. As shown in\nTable 5, when running Vicuna-7B on NVIDIA A100-80G, the actual computing performance is\nonly 0.31 TFLOPS (0.1% utilization) in the decoding phase, compared to 43 TFLOPS (13.8% uti-\nlization) during prefilling. The utilization is calculated with respect to the FP165 tensor core peak\nperformance \u2013 312 TFLOPS for NVIDIA-A100. As a result, the latency of decoding only one token\nis comparable to that of prefilling 128 tokens (40ms). This huge gap in actual computing perfor-\nmance and thereby the latency arises from the fact that all LLM weights need to be loaded onto the\nGPU chip at least once only for decoding one token, so the decoding is heavily bottlenecked by the\nI/O of weights and the GPU computation units cannot be well utilized.\nWhen conducting batched decoding, as the sequence batch size B increases, the latency of decoding\none token for each sequence stays roughly the same (Fig. 10a), as the amount of LLM weights that\nneeds to be loaded onto the chip does not change. As a result, the GPU computation utilization\n( Actual GPU Performance\nPeak GPU Performance ) increases almost linearly as B increases (Fig. 10b). In other words, for gener-\nating a final answer of length N, if we cut the answer into B segments of length N/B and decode\nthem as a batch, we can get a B\u00d7 decoding speed-up compared to sequential decoding. Never-\ntheless, in practice, as prefilling longer requests brings some overhead, and the lengths of the B\nsegments could be imbalanced, the actual speed-up of the batched point-expanding stage compared\nwith the original prefilling and sequential decoding process is smaller than B.\nAs for the peak memory overhead, the amount of LLM weights can be one to two orders of mag-\nnitude larger than that of all the intermediate activations as long as the prefilling token length is not\ntoo large, not to mention that most activations do not need to be saved for back-propagation during\ninference. Therefore, the LLM weights account for the majority of the memory footprint in our test\ncases. Consequently, as shown in Fig. 10c, the peak memory overhead due to the increasing size\nof the KV cache and activation grows at a slow pace as the batch size B increases. Thanks to the\nsmall peak memory overhead, in all of our experiments, we managed to use one GPU to run SoT\nwithout seeking help from other peak memory optimization techniques (e.g., quantization (Frantar\net al., 2022; Lin et al., 2023), offloading (Sheng et al., 2023)).\n5All of our experiments are run with FP16 inference.\n25\nPublished as a conference paper at ICLR 2024\nF\nEFFICIENCY PROFILING\nWe run the profiling on the target GPU (NVIDIA A100-80G and NVIDIA RTX 3090) with CUDA\n11.7, using the Hugging Face transformer library 4.28.1 and PyTorch 2.0.1. The host of A100-80G\nhas an Intel Xeon Platinum 8358P CPU and 1T memory. The host of RTX 3090 has an Intel Xeon\nGold 6246R CPU and 512G memory.\nLatency profiling and estimation.\nFor the decoding phase, we denote tD\nB(k) as the latency\nof batched decoding the k + 1-th token with batch size B, where the superscript D stands for\n\u201cdecode\u201d. For each batch size B = 1, \u00b7 \u00b7 \u00b7 , 16 and each context length k = 1, \u00b7 \u00b7 \u00b7 , 1024, we\nuse torch.cuda.Event to record the latency of decoding one token.\nWe run each decod-\ning three times continuously and take their geometric mean as {tD\nB(k)}k=1,\u00b7\u00b7\u00b7 ,1024;B=1,\u00b7\u00b7\u00b7 ,16. For\nthe prefilling phase, we profile the latency of batched prefilling the inputs with token length k in\nrange(1, 700, 10) and batch size B = 1, \u00b7 \u00b7 \u00b7 , 16, and denote it as tP\nB(k), where the superscript P\nstands for \u201cprefill\u201d. We run each test seven times continuously, regard the first two times as the\nwarmup tests, and take the geometric mean of the last five times as {tP\nB(k)}k=1,11,\u00b7\u00b7\u00b7 ,691;B=1,\u00b7\u00b7\u00b7 ,16.\nOnce we get the latency profiling table, given a request with li tokens and the decoding batch size\nB, the latency of generating lo tokens can be estimated as:\nT(li, lo, B) = \u02dctP\nB(li) +\nli+lo\u22121\nX\nk=li\ntD\nB(k),\n(1)\nwhere the subscripts i and o stand for \u201cinput\u201d and \u201coutput\u201d. Note that we only test the prefill-\ning latency every ten token lengths (i.e., 1, 11, 21, \u00b7 \u00b7 \u00b7 ) for fast profiling and estimate \u02dctP\nB(li) by\ntP\nB(\u230a li\n10\u230b \u00d7 10 + 1).\nThe SoT decoding process consists of two stages: the skeleton stage and the point-expanding stage.\nDenoting the token length of the skeleton request and skeleton response as ls\ni and ls\no, the token length\nof the longest point-expanding request and the longest point-expanding response as lpe\ni\nand lpe\no , the\nnumber of the points as B, we can compute the latency of the skeleton and point-expanding stages\nas:\nLs(ls\ni , ls\no) = T(ls\ni , ls\no, 1),\n(2)\nLpe(lpe\ni , lpe\no , B) = T(lpe\ni , lpe\no , B).\n(3)\nUsing the latency profiling table, we can further estimate the average GPU computing performance\nin FLOPS (i.e., FLOPs per second) of decoding lo tokens with prefilling length li as\nP D(li, lo, B) =\nPli+lo\u22121\nk=li\nf D\nB (k)\nPli+lo\u22121\nk=li\ntD\nB(k)\n,\n(4)\nwhere f D\nB (k) denotes the FLOPs of decoding one token with context length k, which is calculated\nby DeepSpeed\u2019s FLOPs profiler 6. Fig. 10b reports the average GPU computing performance during\nthe process of decoding 64 tokens (prefilling length=128), i.e., P D(128, 64, B).\nMemory\nprofiling\nand\nevaluation.\nTo\nevaluate\nthe\npeak\nmemory,\nwe\nuse\ntorch.cuda.max_memory_allocated to record the memory consumption of prefill-\ning sequences of different lengths and decoding with different context lengths and a batch size\nranging from 1 to 16. Then, we calculate the peak memory of each stage as the maximum value of\nthe prefilling and decoding phases, and calculate the overall peak memory of SoT as the maximum\nvalue of the skeleton and point-expanding stages.\n6https://deepspeed.readthedocs.io/en/latest/flops-profiler.html\n26\nPublished as a conference paper at ICLR 2024\nG\nEFFICIENCY EVALUATION\nG.1\nSKELETON-OF-THOUGHT\nG.1.1\nDETAILED STATISTICS OF TOKEN LENGTHS AND POINT NUMBERS\nAvgerage\nLLaMA2-Chat-7B\nLLaMA2-Chat-13B\nOpenChat-13B\nVicuna-7B V1.3\nVicuna-13B V1.3\nVicuna-33B V1.3\nStableVicuna-13B\nUltraLM-13B\nVicuna-7B V1.1\nClaude\nChatGPT-3.5\nGPT-4\ncoding\nmath\nfermi\nroleplay\nwriting\nknowledge\ngeneric\ncounterfactual\ncommon-sense\nAverage\n6.4\n5.3\n4.4\n7.3\n4.9\n7.9\n6.6\n9.7\n7.1\n5.0\n4.7\n5.3\n8.6\n4.4\n3.7\n3.3\n5.7\n5.0\n4.0\n3.7\n5.7\n5.0\n3.3\n3.7\n4.7\n5.3\n6.3\n4.9\n5.6\n5.4\n5.3\n6.5\n6.9\n10.0\n5.8\n5.5\n6.4\n4.8\n8.5\n7.4\n6.7\n6.3\n6.7\n6.0\n7.0\n7.3\n9.9\n9.1\n5.3\n8.6\n6.3\n9.9\n7.5\n5.9\n6.6\n8.2\n6.3\n8.0\n7.8\n8.8\n8.1\n5.8\n8.3\n5.9\n9.8\n7.4\n7.5\n5.9\n5.9\n6.3\n7.5\n8.6\n9.4\n8.1\n6.4\n7.9\n6.1\n9.4\n7.8\n6.3\n6.2\n7.4\n6.7\n8.4\n8.6\n9.7\n9.2\n6.4\n7.9\n6.7\n9.5\n6.8\n5.0\n6.1\n6.1\n4.9\n9.1\n7.7\n8.4\n8.3\n4.4\n7.3\n4.9\n9.5\n6.8\n6.0\n5.5\n5.5\n4.8\n8.6\n7.8\n9.2\n8.8\n4.1\n7.3\n5.1\n9.3\n6.8\n5.7\n5.6\n6.5\n5.6\n7.4\n7.2\n9.0\n7.7\n5.1\n6.9\n5.5\n8.9\n4.0\n5.0\n6.0\n7.0\n8.0\n9.0\n10.0\n(a) The number of points B.\nAvgerage\nLLaMA2-Chat-7B\nLLaMA2-Chat-13B\nOpenChat-13B\nVicuna-7B V1.3\nVicuna-13B V1.3\nVicuna-33B V1.3\nStableVicuna-13B\nUltraLM-13B\nVicuna-7B V1.1\nClaude\nChatGPT-3.5\nGPT-4\ncoding\nmath\nfermi\nroleplay\nwriting\nknowledge\ngeneric\ncounterfactual\ncommon-sense\nAverage\n372.4\n374.0\n462.7\n386.9\n459.7\n394.9\n384.9\n300.3\n338.4\n381.4\n338.6\n343.0\n304.3\n173.5\n177.3\n208.0\n95.0\n254.7\n159.7\n255.0\n83.0\n273.7\n156.7\n137.0\n139.7\n142.7\n391.8\n396.6\n350.3\n453.0\n382.8\n429.6\n465.3\n398.1\n272.1\n402.1\n417.6\n333.8\n400.2\n311.4\n368.5\n356.4\n273.4\n338.2\n285.4\n431.7\n155.7\n361.0\n254.3\n304.7\n235.3\n372.5\n409.8\n436.6\n478.1\n373.9\n397.9\n404.1\n440.4\n260.4\n325.2\n386.0\n464.4\n366.6\n583.6\n401.0\n470.6\n488.6\n468.9\n377.1\n369.8\n497.8\n266.7\n376.8\n341.1\n352.9\n320.2\n481.8\n372.7\n468.4\n469.8\n417.2\n328.1\n341.1\n476.3\n194.2\n417.8\n321.8\n361.3\n231.7\n444.3\n319.0\n285.4\n419.5\n303.3\n245.5\n332.1\n501.9\n198.2\n399.7\n252.3\n404.9\n173.4\n311.3\n335.6\n424.5\n487.9\n326.0\n324.9\n307.6\n479.6\n169.6\n337.9\n285.9\n303.7\n206.1\n373.5\n343.0\n378.0\n413.5\n344.2\n345.4\n336.0\n437.0\n225.1\n344.7\n309.1\n342.8\n261.1\n379.4\n100.0\n200.0\n300.0\n400.0\n500.0\n(b) The normal answer length.\nAvgerage\nLLaMA2-Chat-7B\nLLaMA2-Chat-13B\nOpenChat-13B\nVicuna-7B V1.3\nVicuna-13B V1.3\nVicuna-33B V1.3\nStableVicuna-13B\nUltraLM-13B\nVicuna-7B V1.1\nClaude\nChatGPT-3.5\nGPT-4\ncoding\nmath\nfermi\nroleplay\nwriting\nknowledge\ngeneric\ncounterfactual\ncommon-sense\nAverage\n114.9\n92.6\n126.1\n151.7\n143.0\n124.7\n104.9\n216.0\n120.6\n128.4\n48.0\n56.1\n67.0\n95.0\n104.3\n94.3\n27.3\n162.0\n189.0\n101.7\n79.3\n98.7\n108.0\n64.7\n45.3\n65.7\n116.0\n117.2\n117.1\n170.7\n188.9\n106.0\n126.0\n163.3\n105.8\n80.4\n65.0\n78.0\n74.0\n89.0\n93.1\n108.2\n63.4\n102.0\n100.3\n118.7\n123.4\n76.9\n71.8\n59.4\n66.1\n84.6\n97.2\n94.4\n114.5\n161.9\n125.1\n92.6\n118.1\n89.5\n90.2\n85.8\n53.8\n61.3\n79.4\n94.1\n99.9\n101.0\n98.1\n110.5\n108.9\n114.0\n117.8\n90.9\n81.1\n61.2\n62.9\n83.0\n86.0\n86.3\n108.5\n106.6\n108.6\n89.6\n105.4\n87.3\n81.3\n76.8\n51.3\n55.5\n75.1\n93.5\n106.2\n103.2\n101.9\n88.9\n118.3\n113.0\n129.2\n79.6\n75.3\n66.6\n56.7\n83.1\n86.9\n97.4\n100.1\n75.4\n121.3\n100.6\n98.3\n104.2\n88.5\n75.6\n55.0\n57.0\n69.7\n97.0\n99.0\n108.1\n106.3\n127.8\n114.4\n111.1\n123.3\n92.5\n87.0\n58.3\n59.9\n75.7\n50.0\n75.0\n100.0\n125.0\n150.0\n175.0\n200.0\n(c) The maximum point-expanding response length.\nAvgerage\nLLaMA2-Chat-7B\nLLaMA2-Chat-13B\nOpenChat-13B\nVicuna-7B V1.3\nVicuna-13B V1.3\nVicuna-33B V1.3\nStableVicuna-13B\nUltraLM-13B\nVicuna-7B V1.1\nClaude\nChatGPT-3.5\nGPT-4\ncoding\nmath\nfermi\nroleplay\nwriting\nknowledge\ngeneric\ncounterfactual\ncommon-sense\nAverage\n0.4\n0.3\n0.4\n0.4\n0.3\n0.3\n0.3\n0.9\n0.5\n0.3\n0.1\n0.2\n0.2\n0.8\n0.6\n0.5\n0.7\n0.6\n1.3\n0.5\n3.0\n0.6\n0.7\n0.6\n0.3\n0.5\n0.3\n0.3\n0.4\n0.4\n0.5\n0.3\n0.3\n0.4\n0.5\n0.2\n0.2\n0.2\n0.2\n0.4\n0.3\n0.4\n0.4\n0.3\n0.4\n0.3\n1.2\n0.3\n0.3\n0.2\n0.3\n0.3\n0.3\n0.2\n0.2\n0.7\n0.3\n0.2\n0.6\n0.4\n0.6\n0.2\n0.1\n0.2\n0.1\n0.3\n0.2\n0.2\n0.2\n0.3\n0.3\n0.2\n0.5\n0.3\n0.2\n0.3\n0.2\n0.2\n0.3\n0.2\n0.2\n0.3\n0.4\n0.3\n0.2\n0.5\n0.3\n0.2\n0.1\n0.2\n0.2\n0.3\n0.4\n0.2\n0.4\n0.4\n0.4\n0.2\n0.7\n0.3\n0.4\n0.2\n0.3\n0.3\n0.3\n0.2\n0.2\n0.4\n0.4\n0.3\n0.2\n0.7\n0.4\n0.3\n0.4\n0.3\n0.2\n0.4\n0.3\n0.3\n0.4\n0.4\n0.4\n0.3\n0.9\n0.4\n0.3\n0.2\n0.3\n0.2\n0.5\n1.0\n1.5\n2.0\n2.5\n(d) The ratio of the maximum point-expanding re-\nsponse length to the normal answer length.\nAvgerage\nLLaMA2-Chat-7B\nLLaMA2-Chat-13B\nOpenChat-13B\nVicuna-7B V1.3\nVicuna-13B V1.3\nVicuna-33B V1.3\nStableVicuna-13B\nUltraLM-13B\nVicuna-7B V1.1\nClaude\nChatGPT-3.5\nGPT-4\ncoding\nmath\nfermi\nroleplay\nwriting\nknowledge\ngeneric\ncounterfactual\ncommon-sense\nAverage\n30.8\n13.5\n27.6\n50.0\n42.1\n36.2\n24.1\n64.7\n36.6\n43.2\n10.3\n11.8\n9.1\n25.2\n23.6\n18.4\n6.3\n48.7\n59.1\n17.5\n22.6\n31.8\n41.6\n13.0\n8.9\n10.4\n25.0\n21.6\n18.9\n47.4\n49.2\n21.8\n22.2\n35.8\n29.8\n23.0\n12.9\n8.7\n8.4\n16.4\n9.9\n14.8\n17.1\n21.0\n18.0\n18.8\n26.6\n21.5\n15.9\n9.1\n10.8\n12.9\n17.9\n10.9\n14.3\n39.1\n27.1\n17.1\n19.8\n17.1\n22.1\n18.0\n9.4\n8.4\n12.2\n14.4\n9.7\n10.6\n20.3\n17.9\n17.2\n15.0\n19.8\n20.4\n15.3\n8.3\n7.4\n10.8\n15.4\n9.0\n15.9\n27.0\n24.1\n18.4\n16.8\n16.2\n18.0\n15.7\n7.6\n7.6\n8.3\n15.9\n12.4\n11.4\n22.3\n13.1\n23.8\n14.0\n34.0\n19.6\n15.1\n8.8\n5.3\n11.5\n14.7\n10.5\n12.1\n19.7\n27.7\n17.6\n14.2\n19.6\n19.0\n15.1\n7.0\n7.4\n6.5\n19.5\n13.5\n16.0\n27.7\n30.1\n25.5\n18.1\n28.5\n24.3\n22.5\n9.6\n8.5\n10.0\n10.0\n20.0\n30.0\n40.0\n50.0\n60.0\n(e) The imbalance degree of point-expanding response\nlengths (standard deviation of point token lengths).\nAvgerage\nLLaMA2-Chat-7B\nLLaMA2-Chat-13B\nOpenChat-13B\nVicuna-7B V1.3\nVicuna-13B V1.3\nVicuna-33B V1.3\nStableVicuna-13B\nUltraLM-13B\nVicuna-7B V1.1\nClaude\nChatGPT-3.5\nGPT-4\ncoding\nmath\nfermi\nroleplay\nwriting\nknowledge\ngeneric\ncounterfactual\ncommon-sense\nAverage\n1.5\n1.2\n1.1\n1.1\n0.8\n1.1\n1.1\n6.8\n1.5\n0.7\n0.5\n0.6\n1.4\n2.1\n1.5\n1.1\n3.4\n1.9\n2.5\n1.0\n7.6\n1.2\n1.0\n1.4\n1.1\n1.8\n1.3\n1.2\n1.7\n1.2\n1.5\n1.2\n1.3\n1.9\n1.9\n0.6\n0.7\n0.9\n1.3\n1.9\n1.6\n1.7\n1.9\n1.3\n2.0\n1.3\n5.5\n1.2\n1.1\n1.4\n1.4\n1.9\n1.7\n1.1\n1.3\n2.8\n1.4\n1.3\n3.5\n2.2\n3.3\n1.0\n0.7\n0.8\n1.0\n1.5\n1.3\n1.0\n0.8\n1.5\n1.7\n1.5\n3.0\n1.5\n1.1\n1.7\n1.0\n1.3\n1.4\n1.0\n1.1\n1.0\n1.7\n1.7\n1.3\n3.5\n1.6\n1.1\n0.9\n1.3\n1.3\n1.7\n1.6\n1.3\n2.0\n1.5\n2.1\n1.3\n3.4\n1.2\n1.1\n0.9\n1.4\n2.0\n1.7\n1.2\n0.9\n2.0\n1.2\n2.0\n1.2\n4.5\n1.7\n0.9\n2.1\n1.2\n1.5\n1.6\n1.3\n1.2\n1.8\n1.4\n1.7\n1.5\n4.3\n1.7\n1.0\n1.1\n1.1\n1.5\n1.0\n2.0\n3.0\n4.0\n5.0\n6.0\n7.0\n(f) The ratio of the final SoT answer length to the nor-\nmal answer length.\nFigure 11: The statistics of the token lengths and point numbers on the Vicuna-80 dataset. Each row\ncorresponds to one question category, and each column corresponds to one model.\nG.1.2\nLATENCY BREAKDOWN: SOT STAGES AND PHASES\nFig. 12 presents the absolute latencies of normal and SoT generations on Vicuna-80. Again, the\nspeed-ups of SoT compared with normal generation is evident. We can see that the decoding phases\npredominantly account for the end-to-end latency. Consequently, although SoT has higher prefilling\nlatency in the skeleton stage than the normal generation and introduces additional point-expanding\n27\nPublished as a conference paper at ICLR 2024\nprefilling latency \u2013 which is expected \u2013 this has negligible impact on the overall latency and thereby\nthe overall speed-up.\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\n40000\nLatency (ms)\nChatGPT-3.5\nStableVicuna-13B\nVicuna-7B V1.1\nVicuna-7B V1.3\nLLaMA2-Chat-7B\nClaude\nUltraLM-13B\nVicuna-13B V1.3\nOpenChat-13B\nLLaMA2-Chat-13B\nGPT-4\nVicuna-33B V1.3\nNormal (prefill)\nNormal (decode)\nSoT skeleton (prefill)\nSoT skeleton (decode)\nSoT point-expanding (prefill)\nSoT point-expanding (decode)\n(a) Average latency across all question categories except\nmath and code on different models.\n0\n5000\n10000\n15000\n20000\nLatency (ms)\nmath\nroleplay\ncounterfactual\ncommon-sense\ncoding\nfermi\ngeneric\nknowledge\nwriting\n(b) Average latency across all models on different\nquestion categories.\nFigure 12: The latency breakdown of SoT and normal generations on the Vicuna-80 dataset. For\nopen-source models, the latency breakdown of the prefilling and decoding phases is shown in dif-\nferent colors. For API-based models, we do not record such latency breakdown information; the bar\nlabeled as \u201c(decode)\u201d indicates the overall latency of prefilling and decoding phases.\nG.1.3\nEFFICIENCY EVALUATION ON NVIDIA RTX 3090\nWe present the SoT speed-ups and latency breakdown on RTX 3090 in Fig. 13. We test the three\n7B models, as their FP16-precision version can be run on an RTX 3090 GPU without further peak\nmemory optimization techniques such as weight quantization (Frantar et al., 2022; Lin et al., 2023)\nor offloading (Sheng et al., 2023). On these three models, SoT can obtain 1.94\u00d7 to 2.40\u00d7 speed-up\non average on Vicuna-80.\nFor the five question categories that SoT can provide high-quality answers (i.e., knowledge, common-\nsense, generic, roleplay, counterfactual), SoT can speed-up the overall answer generation process\nby 1.96\u00d7 to 2.52\u00d7 in the meantime. Note that for the math category, despite the average speed-up\nbeing 1.20\u00d7 by calculating the speed-up across the three math questions, SoT does not reduce the\nabsolute latency of processing the three questions.\n0\n2000\n4000\n6000\n8000\n10000 12000 14000 16000\nLatency (ms)\nVicuna-7B V1.3\nVicuna-7B V1.1\nLLaMA2-Chat-7B\n1.94\u00d7\n2.26\u00d7\n2.40\u00d7\n0\n2000\n4000\n6000\n8000\n10000 12000 14000 16000\nLatency (ms)\nmath\nfermi\ncounterfactual\ncoding\nroleplay\nknowledge\ncommon-sense\nwriting\ngeneric\n1.20\u00d7\n1.70\u00d7\n1.96\u00d7\n2.10\u00d7\n2.12\u00d7\n2.37\u00d7\n2.39\u00d7\n2.43\u00d7\n2.52\u00d7\nNormal (prefill)\nNormal (decode)\nSoT skeleton (prefill)\nSoT skeleton (decode)\nSoT point-expanding (prefill)\nSoT point-expanding (decode)\nFigure 13: The latency breakdown of SoT and normal decoding on the Vicuna-80 dataset. The\naverage speed-up across questions are also marked on the figure.\nG.1.4\nACTUAL LATENCY TESTING\nThis section reports the actual SoT speed-up on the Vicuna-80 with batch testing (instead of analyz-\ning with pre-made profiling tables), using a single NVIDIA A100 GPU. We test the actual end-to-end\nlatency of the SoT and normal decoding with the 9 open-source models. For each model, we run the\nspeed-up test for five times and plot the box in Fig. 14.\n28\nPublished as a conference paper at ICLR 2024\nAs shown in Fig. 14a, the current SoT solution obtains a > 2\u00d7 speed-up on 6 out of the 9 open-\nsource models (i.e., Vicuna-7B V1.1, Vicuna-7B V1.3, UltraLM-13B, LLaMA2-Chat-7B, Vicuna-\n13B V1.3, and LLaMA2-Chat-13B), and a > 1.7 speed-up on OpenChat-13B and Vicuna-33B V1.3.\nSoT achieves no speed-up on StableVicuna-13B. As shown in Fig. 14b, for the five question cate-\ngories that SoT can provide high-quality answers (i.e., knowledge, common-sense, generic, roleplay,\ncounterfactual), SoT can speed-up the overall answer generation process by 2.15\u00d7 to 2.50\u00d7 in the\nmeantime.\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nStableVicuna-13B\nVicuna-33B V1.3\nOpenChat-13B\nLLaMA2-Chat-13B\nVicuna-13B V1.3\nLLaMA2-Chat-7B\nUltraLM-13B\nVicuna-7B V1.3\nVicuna-7B V1.1\n0.97\u00d7\n1.75\u00d7\n1.97\u00d7\n2.14\u00d7\n2.19\u00d7\n2.20\u00d7\n2.75\u00d7\n2.82\u00d7\n2.88\u00d7\n(a) Average speed-up on different models.\n1.4\n1.6\n1.8\n2.0\n2.2\n2.4\n2.6\n2.8\n3.0\nfermi\nmath\nroleplay\nwriting\ncounterfactual\ncoding\nknowledge\ncommon-sense\ngeneric\n1.63\u00d7\n1.67\u00d7\n2.15\u00d7\n2.16\u00d7\n2.18\u00d7\n2.29\u00d7\n2.34\u00d7\n2.45\u00d7\n2.50\u00d7\n(b) Average speed-up on different question categories.\nFigure 14: Speed-ups on 9 open-source models on the Vicuna-80 dataset with actual batch testing.\nG.2\nSKELETON-OF-THOUGHT WITH ROUTER\nThe overhead brought by the router inference is relatively small: On the Vicuna-80 dataset,\nthe prompting and trained router have an average latency of 0.65s (0.39s\u223c1.37s) and 0.04s\n(0.008s\u223c1.55s), respectively. On the WizardLM dataset, the average latency of the prompting and\ntrained router is 0.80s (0.36s\u223c2.22s) and 0.03s (0.009s\u223c2.52s), respectively.\nG.2.1\nSPEED-UP BREAKDOWN: MODELS\nFig. 15 shows the speed-ups of SoT-R on different models on the Vicuna-80 dataset. Fig. 16 and\nFig. 17 show the speed-ups of SoT-R on different models on the WizardLM dataset. We can ob-\nserve that on Vicuna-80, the two methods yield similar speed-ups, whereas on WizardLM, GPT-4\nprompting router usually obtains higher speed-ups than the trained router, especially on GPT-4 itself.\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\n2.2\nStableVicuna-13B\nClaude\nChatGPT-3.5\nVicuna-13B V1.3\nVicuna-7B V1.3\nUltraLM-13B\nGPT-4\nVicuna-7B V1.1\nVicuna-33B V1.3\nOpenChat-13B\nLLaMA2-Chat-7B\nLLaMA2-Chat-13B\n0.98\u00d7\n1.15\u00d7\n1.24\u00d7\n1.32\u00d7\n1.39\u00d7\n1.51\u00d7\n1.54\u00d7\n1.55\u00d7\n1.62\u00d7\n1.66\u00d7\n1.67\u00d7\n1.70\u00d7\n(a) Average speed-up across all question categories\nwith prompting router.\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\n2.2\nStableVicuna-13B\nClaude\nChatGPT-3.5\nVicuna-13B V1.3\nVicuna-7B V1.3\nGPT-4\nVicuna-33B V1.3\nUltraLM-13B\nVicuna-7B V1.1\nLLaMA2-Chat-7B\nLLaMA2-Chat-13B\nOpenChat-13B\n0.99\u00d7\n1.14\u00d7\n1.33\u00d7\n1.34\u00d7\n1.42\u00d7\n1.49\u00d7\n1.57\u00d7\n1.59\u00d7\n1.59\u00d7\n1.69\u00d7\n1.70\u00d7\n1.82\u00d7\n(b) Average speed-up across all question categories\nwith trained router.\nFigure 15: Speed-ups of SoT-R on different models on Vicuna-80 dataset.\n29\nPublished as a conference paper at ICLR 2024\n1.00\n1.25\n1.50\n1.75\n2.00\n2.25\n2.50\n2.75\nStableVicuna-13B\nClaude\nChatGPT-3.5\nVicuna-7B V1.3\nLLaMA2-Chat-7B\nLLaMA2-Chat-13B\nVicuna-7B V1.1\nUltraLM-13B\nVicuna-13B V1.3\nOpenChat-13B\nVicuna-33B V1.3\nGPT-4\n1.13\u00d7\n1.13\u00d7\n1.40\u00d7\n1.49\u00d7\n1.51\u00d7\n1.52\u00d7\n1.56\u00d7\n1.57\u00d7\n1.59\u00d7\n1.66\u00d7\n1.68\u00d7\n2.41\u00d7\n(a) Average speed-up across all question categories\nwith prompting router.\n1.00\n1.25\n1.50\n1.75\n2.00\n2.25\n2.50\n2.75\nClaude\nStableVicuna-13B\nVicuna-13B V1.3\nUltraLM-13B\nVicuna-33B V1.3\nVicuna-7B V1.3\nLLaMA2-Chat-13B\nVicuna-7B V1.1\nLLaMA2-Chat-7B\nChatGPT-3.5\nOpenChat-13B\nGPT-4\n1.09\u00d7\n1.09\u00d7\n1.31\u00d7\n1.33\u00d7\n1.33\u00d7\n1.34\u00d7\n1.35\u00d7\n1.36\u00d7\n1.37\u00d7\n1.37\u00d7\n1.42\u00d7\n1.74\u00d7\n(b) Average speed-up across all question categories\nwith trained router.\nFigure 16: Speed-ups of SoT-R on different models on WizardLM dataset.\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nClaude\nStableVicuna-13B\nVicuna-7B V1.3\nLLaMA2-Chat-7B\nLLaMA2-Chat-13B\nChatGPT-3.5\nVicuna-13B V1.3\nVicuna-33B V1.3\nOpenChat-13B\nVicuna-7B V1.1\nUltraLM-13B\nGPT-4\nSoT (w/o router)\nSoT-R w/ prompting router\nSoT-R w/ trained router\nFigure 17: Speed-ups of SoT and SoT-R on different models on the WizardLM dataset.\nG.2.2\nSPEED-UP BREAKDOWN: CATEGORIES\nFig. 18 and Fig. 19 show the speed-ups of SoT-R on different question categories of Vicuna-80\ndataset. The trained router achieves slightly higher speed-up on most of the categories (except for\nknowledge, writing, and fermi). Fig. 20 and Fig. 21 show the speed-ups of SoT-R on different\nquestion categories of WizardLM dataset. We can observe that on 19 out of 29 categories, using the\nprompting router achieves higher speed-ups than using the trained router.\n1.00\n1.25\n1.50\n1.75\n2.00\n2.25\n2.50\n2.75\nmath\ncoding\nfermi\nwriting\nroleplay\ncounterfactual\nknowledge\ncommon-sense\ngeneric\n0.90\u00d7\n0.96\u00d7\n1.01\u00d7\n1.10\u00d7\n1.17\u00d7\n1.75\u00d7\n1.95\u00d7\n2.05\u00d7\n2.11\u00d7\n(a) Speed-ups of SoT-R with prompting router on dif-\nferent question categories.\n1.00\n1.25\n1.50\n1.75\n2.00\n2.25\n2.50\n2.75\nmath\nwriting\ncoding\nfermi\nroleplay\ncounterfactual\nknowledge\ncommon-sense\ngeneric\n1.00\u00d7\n1.00\u00d7\n1.00\u00d7\n1.00\u00d7\n1.23\u00d7\n1.79\u00d7\n1.87\u00d7\n2.10\u00d7\n2.26\u00d7\n(b) Speed-ups of SoT-R with trained router on different\nquestion categories.\nFigure 18: Speed-ups of SoT-R on different question categories of Vicuna-80 dataset\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nmath\nfermi\ncounterfactual\nroleplay\ncoding\ncommon-sense\nwriting\ngeneric\nknowledge\nSoT (w/o router)\nSoT-R w/ prompting router\nSoT-R w/ trained router\nFigure 19: Speed-ups of SoT and SoT-R on different question categories of the Vicuna-80 dataset.\n30\nPublished as a conference paper at ICLR 2024\n1.00\n1.25\n1.50\n1.75\n2.00\n2.25\n2.50\n2.75\n3.00\nMath\nPhysics\nReasoning\nCode Generation\nEntertainment\nToxicity\nComplex Format\nMultilingual\nCommon-Sense\nCode Debug\nBiology\nArt\nMusic\nComputer Science\nRoleplay\nChemistry\nEthics\nAcademic Writing\nTruthfulQA\nWritting\nLiterature\nPhilosophy\nLaw\nSport\nMedicine\nHistory\nTechnology\nEconomy\nCounterfactual\n0.85\u00d7\n0.94\u00d7\n1.02\u00d7\n1.02\u00d7\n1.03\u00d7\n1.12\u00d7\n1.14\u00d7\n1.22\u00d7\n1.24\u00d7\n1.25\u00d7\n1.34\u00d7\n1.47\u00d7\n1.54\u00d7\n1.54\u00d7\n1.58\u00d7\n1.62\u00d7\n1.67\u00d7\n1.69\u00d7\n1.74\u00d7\n1.77\u00d7\n1.85\u00d7\n1.90\u00d7\n1.90\u00d7\n1.93\u00d7\n2.08\u00d7\n2.10\u00d7\n2.14\u00d7\n2.18\u00d7\n2.23\u00d7\n(a) Speed-ups of SoT-R with prompting router on dif-\nferent question categories.\n1.00\n1.25\n1.50\n1.75\n2.00\n2.25\n2.50\n2.75\n3.00\nCode Generation\nEntertainment\nArt\nComplex Format\nMath\nLiterature\nCode Debug\nLaw\nAcademic Writing\nPhilosophy\nBiology\nReasoning\nPhysics\nHistory\nComputer Science\nMultilingual\nMusic\nToxicity\nRoleplay\nCommon-Sense\nTruthfulQA\nWritting\nEconomy\nChemistry\nEthics\nSport\nTechnology\nMedicine\nCounterfactual\n0.99\u00d7\n1.00\u00d7\n1.00\u00d7\n1.00\u00d7\n1.00\u00d7\n1.00\u00d7\n1.00\u00d7\n1.00\u00d7\n1.00\u00d7\n1.00\u00d7\n1.07\u00d7\n1.09\u00d7\n1.14\u00d7\n1.16\u00d7\n1.17\u00d7\n1.17\u00d7\n1.20\u00d7\n1.22\u00d7\n1.36\u00d7\n1.37\u00d7\n1.41\u00d7\n1.49\u00d7\n1.65\u00d7\n1.73\u00d7\n1.82\u00d7\n2.01\u00d7\n2.17\u00d7\n2.26\u00d7\n2.41\u00d7\n(b) Speed-ups of SoT-R with trained router on different\nquestion categories.\nFigure 20: Speed-ups of SoT-R on different question categories of WizardLM dataset\n1.00\n1.25\n1.50\n1.75\n2.00\n2.25\n2.50\n2.75\n3.00\nEntertainment\nPhysics\nReasoning\nMultilingual\nMath\nCommon-Sense\nBiology\nArt\nMusic\nToxicity\nEthics\nComputer Science\nCode Debug\nChemistry\nLiterature\nAcademic Writing\nPhilosophy\nLaw\nTruthfulQA\nRoleplay\nCode Generation\nComplex Format\nSport\nWritting\nMedicine\nHistory\nTechnology\nEconomy\nCounterfactual\nSoT (w/o router)\nSoT-R w/ prompting router\nSoT-R w/ trained router\nFigure 21: Speed-ups of SoT and SoT-R on different question categories of the WizardLM dataset.\nH\nOVERHEAD OF SOT IN DIFFERENT SCENARIOS\nDespite the optimizations made to the decoding phase, SoT brings overhead to the prefilling phase as\nthe model needs to handle additional SoT prompts. Table 6 reports SoT\u2019s prefilling overhead for the\nAPI-based models. These statistics are averaged across the Vicuna-80 questions that are suitable for\nSoT (according to our manual annotation). We can see that SoT significantly increases the number\nof prefilling tokens. This is because that SoT issues an independent point-expanding request for\neach point, with the average number of points being 6.8 on Vicuna-80 dataset across all evaluated\nmodels. Consequently, the APIs need to prefill the point-expanding request multiple times.\n31\nPublished as a conference paper at ICLR 2024\nTable 6: SoT\u2019s prefilling token overhead for API-based models.\nModel\nPrefill Phase\nNormal\nSoT Stage 1\nSoT Stage 2\nRatio (SoT / Normal)\nClaude\n10.33\n155.33\n730.91\n85.79\nChatGPT-3.5\n10.21\n136.33\n480.95\n60.46\nGPT-4\n10.21\n72.44\n838.26\n89.20\nWhen using SoT to serve the open-source models, a simple and small trick is to prefill the common\nprefix of point-expanding requests with a batch size of 1 during Stage 2 (i.e., the point-expanding\nstage). Table 7 shows the prefilling overhead after applying the trick. Although the ratio is consid-\nerably smaller compared to that of the API-based models, this computational overhead remains a\nconcern, especially during periods of high system workload.\nThere are some possibilities to further reduce the token and computational overhead that are worth\nexploring in future work. To name a few: (1) When using SoT in serving systems, we can simply\nreuse the key-value cache containing the question and skeleton from Stage 1 during Stage 2, rather\nthan re-prefilling them as in a multi-round conversation. (2) Generally, as LLM capabilities continue\nto evolve and prompt tuning techniques advance (Shin et al., 2020; Li & Liang, 2021; Lester et al.,\n2021; Jiang et al., 2023), the possibility of using much shorter prompts to activate the SoT mode in\nthe future holds promise, which would significantly mitigate the token or computational overhead.\nTable 7: SoT\u2019s computational overhead (in terms of the number of prefilling tokens) for open-source\nmodels.\nModel\nPrefill Phase\nNaive\nSoT Stage 1\nSoT Stage 2\nRatio (SoT / Normal)\nLLaMA2-Chat-7B\n12.52\n171.41\n216.49\n30.98\nLLaMA2-Chat-13B\n12.52\n171.41\n216.41\n30.98\nOpenChat-13B\n12.52\n171.41\n234.38\n32.41\nVicuna-7B V1.3\n12.52\n171.41\n211.61\n30.59\nVicuna-13B V1.3\n12.52\n171.41\n273.39\n35.53\nVicuna-33B V1.3\n12.52\n171.41\n258.88\n34.37\nStableVicuna-13B\n12.52\n171.41\n312.03\n38.61\nUltraLM-13B\n12.52\n171.41\n290.88\n36.92\nVicuna-7B V1.1\n12.52\n171.41\n196.64\n29.40\nI\nANSWER QUALITY EVALUATION\nI.1\nSKELETON-OF-THOUGHT\nI.1.1\nANSWER ANALYSIS: DIFFERENT MODELS\nModels on which SoT has low net win rates. We observe two types of reasons for the low net win\nrates.\n(1) OpenChat-13B, Vicuna-7B V1.1, and LLaMA2-Chat-13B. For the weak models such as\nOpenChat-13B and Vicuna-7B V1.1, they are not able to follow the SoT prompts precisely. For\nOpenChat-13B and LLaMA2-Chat-13B, sometimes the skeleton contains undesired contents (e.g.,\ncompleting the skeleton points as shown below).\n32\nPublished as a conference paper at ICLR 2024\nQuestion:\nHow many lightning strikes occur on Earth each day? Try to explain your answer. Your\nexplanation should take the reader through your reasoning step-by-step.\nSoT\u2019s answer (OpenChat-13B) (Orange texts are the skeleton.)\n(Only showing the skeleton)\n1.Introduction.\n2. The Earth\u2019s atmosphere.\n3. Lightning formation.\n4. Electrical discharge.\n5. Lightning strikes per second.\n6. Calculating the total.\nSkeleton:\n1. Introduction.\nLightning is a beautiful and awe-inspiring natural phenomenon, but it can also be dangerous. It occurs when positively\ncharged particles in a thundercloud discharge to the negatively charged Earth. The process is still not fully understood,\nbut scientists have been able to estimate how many lightning strikes occur on Earth each day.\nSkeleton:\n1. The Earth\u2019s atmosphere.\nThe Earth\u2019s atmosphere plays a crucial role in the formation of lightning. It consists of different layers, with the tropo-\nsphere being the lowest and most active. The troposphere is where most of the weather occurs, and it is where lightning\nprimarily forms. The atmosphere\u2019s temperature, humidity, and wind patterns all contribute to the likelihood of a lightning\nstrike.\nSkeleton:\n2. Lightning formation.\nWhen warm, moist air rises, it cools and forms water <more texts are omitted...>\nFor both OpenChat-13B and Vicuna-7B V1.1, sometimes they do not write anything in the point-\nexpanding stage when details are needed.\nQuestion:\nAs a superhero, how would you explain your origin story to a curious child?\nSoT\u2019s answer (Vicuna-7B V1.1) (Orange texts are the skeleton.)\n1. Born with extraordinary abilities.\n2. Trained to use powers for the greater good.\n3. Protecting the innocent and fighting villains.I use my powers to defend the defenseless and bring justice to those who\nhave been wronged.\n4. Inspired by courage, justice, and selflessness.\nThese two issues result in bad net win rates.\n(2) Claude. For strong models such as Claude, we do not see major issues with SoT\u2019s answers except\nthe common issues across all models discussed in \u00a7 3.2.3. We hypothesize that the low net win rates\nare majorly due to the fact that their normal sequential generation is already very good.\nModels on which SoT has high net win rates. The models with high net win rates (Vicuna-13B\nV1.3, StableVicuna-13B, and UltraLM-13B) are between the above two extremes. On one hand,\nthey are good enough to understand SoT prompts. While we observe that they sometimes have the\nsame issues as OpenChat-13B and Vicuna-7B V1.1 discussed before, these issues happen much less\nfrequently. On the other hand, their normal sequential generation performance has a larger room for\nimprovement than Claude and ChatGPT-3.5 so that the benefits from SoT are more visible on these\nmodels.\nI.1.2\nANSWER ANALYSIS: DIFFERENT QUESTION CATEGORIES\nCategories on which SoT has low net win rates.\n(1) Math. Math questions require step-by-step thinking. Without knowing the previous steps, it is\nhard to derive the following steps. Chain-of-Thoughts (Kojima et al., 2022; Wei et al., 2022) empha-\nsize the importance of expanding the sequential thinking steps to solve these problems successfully.\nIn contrast, SoT requires the models to (a) come up with the skeleton of the solution first and (b)\ndeduce each individual step independently without referring to previous results. Both of these are\nchallenging tasks. Strong models are able to get (a) correct but fail at (b). In the following example,\nChatGPT-3.5 gets the skeleton steps correct. However, without knowing the results from the first\nstep, the model starts making mistakes in the second step.7\n7One additional (minor) issue of the API-based ChatGPT-3.5 is that the skeleton point \u201c1.\u201d is repeated in\nthe answer.\n33\nPublished as a conference paper at ICLR 2024\nQuestion:\n(Math) Solve for x in the equation 3x + 10 = 5(x - 2).\n\u2717 SoT\u2019s answer (ChatGPT-3.5) (Orange texts are the skeleton.)\n1. Move all x terms to one side. 1. Move all x terms to one side by subtracting 5x from both sides, giving -2x + 10 = -10.\n2. Move all constant terms to the other side. This can be done by subtracting 5x - 10 from both sides, leaving 3x - 5x =\n-10 - 10.\n3. Simplify the equation. Combine like terms on both sides of the equation to get 3x + 10 = 5x - 10.\n4. Divide both sides by the coefficient of x. Divide both sides by 2 to get x = -5.\n5. Check the solution by plugging it back into the original equation.Substitute the value of x obtained in step 4 back into\nthe original equation and verify that both sides are equal.\n\u2713 Normal answer (ChatGPT-3.5)\n3x + 10 = 5(x - 2) can be simplified as follows:\n3x + 10 = 5x - 10 (distribute the 5 on the right side)\n10 + 10 = 5x - 3x (subtract 3x from both sides)\n20 = 2x (combine like terms on the right side)\nx = 10 (divide both sides by 2)\nTherefore, the solution for x in the equation 3x + 10 = 5(x - 2) is x = 10.\nFor weaker models, it is hard to even get step (a) correct. For example, as shown below, the third\nstep \u201cApply FOIL\u201d in Vicuna-13B V1.3\u2019s SoT solution comes out of nowhere. This makes SoT\u2019s\nsolution incorrect (though the solution from the normal generation is also incorrect).\nQuestion:\n(Math) Solve for x in the equation 3x + 10 = 5(x - 2).\n\u2717 SoT\u2019s answer (Vicuna-13B V1.3) (Orange texts are the skeleton.)\n1. Simplify equation.\nWe can simplify the equation by combining like terms:\n3x + 10 = 5(x - 2)\n=\u00bf 3x + 10 = 5x - 10\n=\u00bf x + 5 = 2\n=\u00bf x = 2 - 5\n=\u00bf x = -3\n2. Factor 5(x - 2).\n5(x - 2) = 5x - 10.\n3. Apply FOIL.\n* Multiply the binomials in the equation, using the first term of each binomial for the first term of the product, the outside\nterm for the second term of the product, the second term of each binomial for the third term of the product, and the\nremaining terms for the fourth term of the product.\n* 3(x - 2) + 10 = 5x + 5(-2)\n* 3x - 6 = 5x - 10\n* 3x = 5x - 10\n* x = 5x - 10 / 3\n* x = 5/3x - 10/3\n* x = 2/3x - 5/3\n4. Solve for x.\nAfter applying the FOIL method and simplifying the equation, we get 3x = 17. Therefore, x = 17/3.\n\u2717 Normal answer (Vicuna-13B V1.3)\nTo solve for x in the equation 3x + 10 = 5(x - 2), we need to first distribute the 5 to multiply both sides by 5.\n3x + 10 = 5(x - 2)\n5(3x + 10) = 5(x - 2)\n15x + 50 = 5x - 10\nAdd 5x to both sides:\n20x + 50 = 5x\nSubtract 5x from both sides:\n15x = 0\nDivide both sides by 15:\nx = 0\nTherefore, the solution to the equation is x = 0.\n(2) Fermi. Similar to math questions, questions in the fermi category require solvers to make as-\nsumptions and calculate the answer step-by-step. For example, to answer the question in the figure\nbelow, the solver is required to make an assumption about when life began on Earth. In step 4,\nChatGPT-3.5 does not know that it has already assumed 3.5 billion years in step 3 and uses the\nwrong number 4.54 billion in the calculation. In step 5, ChatGPT-3.5 does not know that it has\nalready made the above assumptions and refuses to answer \u201cdue to the uncertainties in estimating\nthe age of the Earth\u201d. In contrast, normal generation is able to create a consistent solution.\n34\nPublished as a conference paper at ICLR 2024\nQuestion:\n(Fermi) How many times has the Earth orbited the Sun since the beginning of life? Try\nto explain your answer. Your explanation should take the reader through your reasoning step-by-step.\n\u2717 SoT\u2019s answer (ChatGPT-3.5) (Orange texts are the skeleton.)\n1. Estimate the age of the Earth. The age of the Earth is estimated to be around 4.54 billion years, based on radiometric\ndating of rocks and meteorites.\n2. Determine when life began on Earth. Life on Earth is estimated to have begun around 3.5 billion years ago, based on\nthe discovery of ancient microfossils.\n3. Calculate the number of years since life began. The oldest known fossils suggest that life on Earth began around 3.5\nbillion years ago. Therefore, the number of years since life began on Earth is approximately 3.5 billion years.\n4. Divide the number of years by the length of one orbit. Assuming an orbital period of 365.25 days, the Earth has\norbited the Sun approximately 4.54 billion / 365.25 = 12,425,000 times since the beginning of life.\n5. The Earth has orbited the Sun approximately X times since the beginning of life. Unfortunately, it is impossible to give\na specific number for the amount of times the Earth has orbited the Sun since the beginning of life due to the uncertainties\nin estimating the age of the Earth and when life first emerged.\nNormal answer (ChatGPT-3.5)\nThe beginning of life on Earth is estimated to have occurred around 3.5 to 4 billion years ago. Since that time, the Earth\nhas been orbiting the Sun continuously.\nThe length of one orbit, or one year, is approximately 365.25 days. This means that the Earth completes one full orbit\naround the Sun every 365.25 days.\nTo calculate the number of times the Earth has orbited the Sun since the beginning of life, we can divide the total number\nof days by the length of one orbit:\nTotal number of days since the beginning of life = (3.5 billion years) x (365 days per year) = 1.2775 trillion days\nNumber of orbits = (1.2775 trillion days) / (365.25 days per orbit) = 3.5 billion orbits\nTherefore, the Earth has orbited the Sun approximately 3.5 billion times since the beginning of life.\nAs the following example shows, there are also cases when SoT solves the question (with minor\ncomputation errors) by embedding the computation steps in the skeleton directly in the skeleton\nstage. The point-expanding stage will then add little content (except some undesired repetition of\nthe following skeletons occasionally). This happens both on small/weak models and large/powerful\nmodels. Although this behavior is not what we originally expect, it sheds light on a promising direc-\ntion for making SoT a general framework: in the problem where step-by-step thinking is needed, the\nSoT model automatically falls back to the sequential generation mode by outputting the complete\nanswer in the skeleton stage and skipping the point-expanding stage. Future work needs to be done\nto control and encourage this behavior better.\nQuestion:\n(Fermi) How many times does the average human blink in a lifetime? Try to explain your\nanswer. Your explanation should take the reader through your reasoning step-by-step.\nSoT\u2019s answer (Vicuna-7B V1.1) (Orange texts are the skeleton.)\n1. The average human blinks 12-15 times per minute.\n2. There are 60 minutes in an hour.\n3. Therefore, the average human blinks 60 x 12-15 = 720-900 times per hour.\n4. There are 24 hours in a day.Therefore, the average human blinks 720-900 x 24 = 17,520-22,400 times per day.\n5. Therefore, the average human blinks 720-900 x 24 = 17,520-22,400 times per day.This translates to blinking approxi-\nmately 50-65 times per hour, which is a significant portion of the day.\n6. There are 365 days in a year.Therefore, the average human blinks 17,520-22,400 x 365 = 5,857,600-8,124,000 times\nper year.\n7. Therefore, the average human blinks 17,520-22,400 x 365 = 5,857,600-8,124,000 times per year.\n8. The human lifespan is typically 70-80 years.Therefore, the average human blinks 80,000-112,000 times in their\nlifetime.\n9. Therefore, the average human blinks 5,857,600-8,124,000 / 70-80 = 80,000-112,000 times in their lifetime.\nNormal answer (Vicuna-7B V1.1)\nThe average human blinks about 12-15 times per minute, which translates to about 18,200 to 20,400 blinks per day. Over\nthe course of a lifetime, the number of blinks is estimated to be around 7,500,000 to 8,000,000.\nBlinking is a reflexive action that helps to keep the eyes moist and comfortable, as well as to protect the eyes from dust\nand other foreign particles. The frequency of blinking can vary depending on a variety of factors, including age, health,\nand environmental conditions. For example, people who work in dusty or dry environments may blink more frequently\nthan those who work in a clean, moist environment.\nOver the course of a lifetime, the average person will spend approximately 27,375 days or 700,000 hours awake. This\nmeans that the majority of the time spent blinking will occur in the first few decades of life, with the frequency of blinking\ngradually decreasing as people age.\nIt is important to note that there is a significant amount of individual variation in the frequency of blinking. Some people\nmay blink more or less than the average, depending on a variety of factors such as age, health, and environmental condi-\ntions. Additionally, blinking patterns can be affected by factors such as stress, fatigue, and certain medical conditions.\n35\nPublished as a conference paper at ICLR 2024\n(3) Coding. While in most cases, the models are able to decompose the coding problem into smaller\ntasks in the skeleton stage, the generation quality during the point-expanding stage is bad, potentially\ndue to the fact that we did not carefully craft a special point-expanding prompt for coding. The\nfailure cases can be grouped into two cases. (a) In some cases, the model only generates a description\nof how implementation can be done without giving the code.\nQuestion:\n(Coding) Develop a C++ program that reads a text file line by line and counts the number\nof occurrences of a specific word in the file.\nSoT\u2019s answer (Claude) (Orange texts are the skeleton.)\n1. Open file stream by declaring an ifstream object and using the open() method to open the file in read mode.\n2. Read line by line from the file stream into a string using getline().\n3. Find word by searching each line for the target word using string::find and keeping track of matches.\n4. Increment counter by one each time the specified word is encountered in the current line.\n5. Close file\nThe file stream should be closed after finishing reading the file to free up resources. This can be done by calling the\nclose() method on the file stream object.\nThis might be due to the fact that the powerful models (e.g., Claude) follow the point-expanding\nprompt \u201cWrite it **very shortly** in 1\u223c2 sentence\u201d precisely. Better prompts tailed to coding\ncould help. (b) In some other cases, the model tends to generate the complete code repeatedly in\nseveral points.\nQuestion:\n(Coding) Develop a C++ program that reads a text file line by line and counts the number\nof occurrences of a specific word in the file.\nSoT\u2019s answer (OpenChat-13B) (Orange texts are the skeleton.)\n1. Read file line by line.\n2. Store lines in a container.\n3. Count occurrences of specific word.\n\u201c\u2018cpp\n<more texts are omitted...>\n\u201c\u2018\nThis code uses the C++ standard library to read a text file line by line, store each line in a container, and count the\noccurrences of a specific word. The results are printed.\n4. Print results.\nHere\u2019s the C++ program that reads a text file line by line and counts the number of occurrences of a specific word in the\nfile:\n\u201c\u2018cpp\n<more texts are omitted...>\n\u201c\u2018\nPlease note that this is a very basic implementation. You may need to adjust it depending on your specific requirements.\nThis problem might also be alleviated by prompts that explicitly tell the model to only generate a\n\u201ccode snippet\u201d in each skeleton point.\nAlthough better prompt engineering could lead to better results, we expect that SoT is not suitable\nfor the coding questions in Vicuna-80. The reason is that the solutions to the problems in Vicuna-\n80 are usually in a few lines with strong dependencies between each other. Without knowing the\npreviously defined variable names or imported libraries, it is hard (or even impossible) to implement\nthe subsequent code correctly. As a consequence, generating different parts of the answers in parallel\nis not suitable here. Similar to the math questions, automatically falling back to outputting all the\ncode in the skeleton stage and not triggering the point-expanding stage might be more suitable for\nanswering this question type.\nHowever, we expect that SoT could be helpful for larger coding tasks that involve multiple modules\n(e.g., functions, classes). The skeleton stage could be in charge of defining the interfaces between\ndifferent modules (e.g., functionalities, names, parameters, return types). The point-expanding stage\ncould be in charge of implementing these modules or using these modules to complete the final task,\nwhich can be done in parallel. This mimics the common practice in software engineering.\n(4) Writing. The writing questions usually ask for an email, a blog post, or a review in a given\nscenario. In the detailed evaluation output from FastChat and LLMZoo metrics, the major and\ncommon complaint is that SoT\u2019s answers are (slightly) less detailed. This could potentially be\nimproved by point-expanding prompts that request more details.\n36\nPublished as a conference paper at ICLR 2024\nWhile inspecting SoT\u2019s answers manually, we observe one additional issue. Writing problems usu-\nally require a coherent passage without embedded skeleton points, whereas our current SoT pipeline\n(\u00a7 2) concatenates skeleton points as part of the answer. One example is shown below.\nQuestion:\n(Writing) Can you help me write a formal email to a potential business partner proposing\na joint venture?\nSoT\u2019s answer (ChatGPT-3.5) (Orange texts are the skeleton.)\n1. Introduce yourself and your company. Dear [Recipient],\nI hope this email finds you well. My name is [Your Name] and I am the [Your Position] at [Your Company Name].\n2. State the purpose of the email. The purpose of this email is to propose a joint venture between our companies.\n3. Explain the potential benefits of the joint venture. By combining our resources and expertise, we can increase market\nshare and profitability for both companies.\n4. Provide details on the proposed collaboration. Our proposed collaboration would involve combining our expertise in\nmarketing with your strong manufacturing capabilities to create a new line of products.\n5. Suggest a follow-up meeting or call. Please let me know your availability for a follow-up call next week to discuss\nthis opportunity further.\n6. Thank the recipient for their time and consideration.Thank you for considering this proposal and we look forward to\nhearing back from you soon.\nNormal answer (ChatGPT-3.5)\nOf course, I can help you with that. Here\u2019s a sample email you can use as a guide:\nSubject: Proposal for Joint Venture\nDear [Business Partner\u2019s Name],\nI hope this email finds you well. I am writing to propose a joint venture between our companies, [Your Company Name]\nand [Potential Partner Company Name]. We believe that our respective expertise can create a successful partnership that\nwill benefit both of our businesses.\nWe have conducted thorough research on your company and admire your innovative approach to [insert industry or niche].\nWe think our [insert expertise or specialty] could complement your [insert potential partner\u2019s specialty or expertise].\nBy collaborating on this joint venture, we believe we can create a new and innovative product that will appeal to our\nrespective markets.\nWe are confident that our companies would work well together and that our joint venture would be successful. We would\nlike to set up a meeting to discuss the specifics of our proposal and explore how we can move forward with the joint\nventure.\nThank you for considering this proposal. We look forward to hearing from you soon.\nBest regards,\n[Your Name]\nIn such cases, simply removing the skeleton points would greatly improve the answer quality. To\nmake SoT more general, one future direction is to let the LLM itself decide whether the point index\nand point skeleton should be included to make the final answer more natural and fluent.\nCategories on which SoT has high net win rates.\n(1) Counterfactual, knowledge, common sense, generic. All these four categories have the same\ncharacteristic: the ideal answer should cover several relatively independent points. Having the LLM\ngenerate a skeleton before expanding the details can result in more comprehensive discussions for\nthe question. Besides, organizing the answers as a list of points makes the answers easier to read,\nwhereas the answers from normal generation are sometimes less structured and less readable. Below,\nwe give one example for each question category to illustrate these points.\n37\nPublished as a conference paper at ICLR 2024\nQuestion:\n(Counterfactual) What if the Internet had been invented during the Renaissance period?\nSoT\u2019s answer (ChatGPT-3.5) (Orange texts are the skeleton.)\n1. Advancements in communication.The invention of the Internet during the Renaissance period would have revolution-\nized communication, allowing for faster and more efficient exchange of ideas and information across vast distances.\n2. Increased access to knowledge.The invention of the printing press during the Renaissance period already allowed\nfor greater access to knowledge, but the internet would have revolutionized this even further by providing instantaneous\naccess to information from all over the world.\n3. Improved dissemination of ideas.If the internet had been invented during the Renaissance period, the spread of ideas\nand knowledge would have been much faster and widespread than it was at the time. This could have led to a faster\nprogression of scientific and artistic innovations.\n4. Impact on art and literature.The invention of the internet during the Renaissance period would have revolutionized the\nway art and literature are created, shared, and consumed, leading to a more diverse and globally-influenced artistic and\nliterary scene.\n5. Potential for global trade.The invention of the internet during the Renaissance period would have greatly expanded\nglobal trade opportunities, allowing for easier communication and collaboration between merchants and traders across\ncontinents.\n6. Influence on political and social structures.The invention of the internet during the Renaissance period could have led\nto significant changes in political and social structures, allowing for greater democratization and communication among\npeople from different regions and classes.\nNormal answer (ChatGPT-3.5)\nIf the Internet had been invented during the Renaissance period, it would have had a significant impact on the way people\ncommunicated, shared ideas, and gained knowledge. The Renaissance was a time of great intellectual and artistic growth,\nand the Internet would have facilitated the spread of these ideas and innovations.\nScholars and artists would have been able to share their work more easily, and people from different parts of the world\nwould have been able to connect and collaborate more easily. The Internet would have also made it easier to access\ninformation, which could have accelerated the pace of scientific and technological advancements.\nHowever, it\u2019s important to note that the infrastructure required for the Internet to exist did not exist during the Renaissance\nperiod. There were no telecommunications networks or electricity grids, so the idea of the Internet would have been\nimpossible to realize. Nonetheless, it\u2019s interesting to think about how the Renaissance might have been different if the\nInternet had existed at that time.\n38\nPublished as a conference paper at ICLR 2024\nQuestion:\n(Knowledge) What are some potential implications of using a single-use plastic bottle\nversus a reusable bottle on both the environment and human health?\nSoT\u2019s answer (ChatGPT-3.5) (Orange texts are the skeleton.)\n1. Single-use plastic waste.Single-use plastic bottles contribute to the massive amount of plastic waste that ends up in\nlandfills and oceans, harming wildlife and ecosystems.\n2. Pollution of waterways.Single-use plastic bottles contribute to the pollution of waterways, as they often end up in\nrivers, lakes, and oceans, causing harm to aquatic life and the overall ecosystem.\n3. Harmful chemicals leaching.Single-use plastic bottles may release harmful chemicals such as bisphenol A (BPA) and\nphthalates into the water or liquid they contain, posing a risk to human health.\n4. Greenhouse gas emissions.The production and disposal of single-use plastic bottles contribute to greenhouse gas\nemissions, which contribute to climate change. Using a reusable bottle reduces the need for constant production and\ndisposal, therefore reducing greenhouse gas emissions.\n5. Microplastic ingestion.Single-use plastic bottles contribute to the accumulation of microplastics in the environment,\nwhich can be ingested by humans through food and water, leading to potential health risks.\n6. Increased risk of disease.Single-use plastic bottles can harbor harmful bacteria, leading to an increased risk of disease\nif not disposed of properly. Reusable bottles, when cleaned regularly, pose a lower risk.\nNormal answer (ChatGPT-3.5)\nUsing a single-use plastic bottle has significant implications on the environment and human health.\nEnvironmental implications:\n1. Plastic pollution: Single-use plastic bottles are one of the major contributors to plastic pollution in oceans and landfills.\nThey take hundreds of years to decompose, and during that time, they release toxic chemicals and microplastics into the\nenvironment.\n2. Carbon emissions: Manufacturing and transporting single-use plastic bottles require a significant amount of energy\nand resources, which contribute to greenhouse gas emissions and climate change.\n3. Natural resource depletion: The production of single-use plastic bottles requires large amounts of water and oil, which\nare finite resources.\nHealth implications:\n1. Chemical exposure: Single-use plastic bottles contain chemicals such as Bisphenol A (BPA) and phthalates, which\ncan leach into the water and pose a risk to human health.\n2. Microplastics ingestion: When plastic bottles break down, they release microplastics, which can enter the food chain\nand be ingested by humans.\nUsing a reusable bottle, on the other hand, has a positive impact on the environment and human health.\nEnvironmental benefits:\n1. Reduced plastic waste: Reusable bottles significantly reduce the amount of plastic waste generated.\n2. Lower carbon emissions: Reusable bottles have a lower carbon footprint as they can be used multiple times.\n3. Reduced resource depletion: The production of reusable bottles requires fewer resources than single-use plastic bottles.\nHealth benefits:\n1. Chemical-free: Reusable bottles are typically made from materials that are free from harmful chemicals, reducing the\nrisk of chemical exposure.\n2. Reduced microplastics: Reusable bottles do not break down easily, reducing the release of microplastics into the\nenvironment.\nOverall, using a reusable bottle is a more sustainable and healthier option than using a single-use plastic bottle.\nQuestion:\n(Common sense) How can you determine if a person is genuinely interested in a conver-\nsation or simply being polite?\nSoT\u2019s answer (ChatGPT-3.5) (Orange texts are the skeleton.)\n1. Pay attention to body language.Notice if their posture is open or closed, if they are facing towards you or away, and if\nthey are fidgeting or relaxed. These can all indicate their level of interest in the conversation.\n2. Listen for engaged responses.If the person is genuinely interested in the conversation, they will respond with thoughtful\nand relevant comments or questions rather than short, generic responses.\n3. Observe eye contact.If the person maintains steady eye contact and their gaze is focused on you, they are likely\ngenuinely interested in the conversation. On the other hand, if they frequently look away or seem distracted, they may\njust be being polite.\n4. Look for follow-up questions.If the person asks follow-up questions and seems genuinely interested in what you\u2019re\nsaying, then they are likely interested in the conversation and not just being polite.\n5. Notice tone of voice.A person who is genuinely interested in a conversation will often have a more enthusiastic and\nanimated tone of voice, compared to someone who is simply being polite and giving short, monotone responses.\nNormal answer (ChatGPT-3.5)\nAs an AI assistant, I cannot determine a person\u2019s true intentions or feelings. However, there are some social cues that\ncan indicate whether a person is genuinely interested in a conversation or not. For example, if they maintain eye contact,\nlean in, nod their head and ask follow-up questions, it can be a sign that they are engaged in the conversation. On the\nother hand, if they avoid eye contact, give short or vague responses, or seem distracted, it may indicate that they are\nnot as interested. Ultimately, it\u2019s important to remember that everyone has different communication styles, and it can be\ndifficult to accurately interpret someone\u2019s intentions without directly asking them.\n39\nPublished as a conference paper at ICLR 2024\nQuestion:\n(Generic) How can I improve my time management skills?\nSoT\u2019s answer (ChatGPT-3.5) (Orange texts are the skeleton.)\n1. Set clear goals. Setting clear goals is important in improving time management skills as it helps you to focus on what\nneeds to be done and what can be accomplished within a certain timeframe.\n2. Prioritize tasks. Make a list of tasks and prioritize them based on their importance and deadline. This will help you\nfocus on the most critical tasks and avoid wasting time on less important ones.\n3. Create a schedule. Creating a schedule helps you allocate time for each task and ensures that you complete them\nwithin the allotted time.\n4. Minimize distractions. Distractions such as social media, emails, and phone notifications can waste a lot of time. Try\nto limit them by turning off notifications or setting specific times to check them.\n5. Take breaks. Taking breaks can actually improve productivity and help prevent burnout. It\u2019s important to schedule\nshort breaks throughout the day to recharge and refocus.\n6. Learn to say no. It\u2019s important to prioritize your own tasks and not take on too much at once. Saying no to requests\nthat don\u2019t align with your goals or schedule can help you manage your time more effectively.\n7. Delegate tasks. If you have too many tasks to handle, consider delegating some of them to others who can help you.\nThis will allow you to focus on more important tasks and manage your time more effectively.\n8. Use time management tools.There are a variety of time management tools available, such as calendars, to-do lists, and\nproductivity apps, that can help you stay organized and on track with your tasks.\nNormal answer (ChatGPT-3.5)\nImproving time management skills can be a challenge, but there are several strategies that can help. Here are a few:\n1. Set clear goals and priorities: Identify your most important tasks and make sure to complete them first.\n2. Make a schedule: Create a schedule that includes your most important tasks and set aside time for them.\n3. Use a timer: Set a timer for a specific amount of time to help you stay focused and avoid distractions.\n4. Take breaks: Take short breaks throughout the day to help you recharge and avoid burnout.\n5. Eliminate distractions: Try to limit distractions as much as possible, such as by turning off your phone or closing your\nemail.\n6. Delegate tasks: If possible, delegate tasks to others to free up time for more important activities.\nRemember, improving time management skills takes practice and perseverance. Be patient with yourself and keep trying\nuntil you find strategies that work well for you.\n(2) Roleplay. Roleplay questions require LLMs to assume a character and present an introduction\nor speech to other people. LLMZoo and FastChat metrics usually favor the level of details of SoT\nwhen assigning it a better score. Similar to the writing category discussed before, SoT\u2019s answers\ncould be even better if the skeleton points are removed from the answer.\nQuestion:\n(Roleplay) How would you introduce yourself as a medieval knight at a royal banquet?\nSoT\u2019s answer (Claude) (Orange texts are the skeleton.)\n1. Name and house. My name is Sir Percival of Camelot, knight of the Round Table and protector of King Arthur\u2019s\nrealm.\n2. Lands and titles. I hold fiefdom over the lands of Westmarch and Ravenshire, and bear the titles of Baron of the\nWestern Marches and Knight of the Raven.\n3. Military feats. I have led my men to great victories against the kingdom\u2019s enemies, winning many battles through\nstrength of arms and tactical cunning.\n4. Allegiance to king. I pledge my sword, my loyalty, and my life to serve and protect my noble king and this great\nkingdom.\n5. Request to join table. Your Highness, it would be the greatest honor to join your exalted table and share in this\nmagnificent feast with my fellow knights and loyal subjects.\nNormal answer (Claude)\nGreetings, your majesty and honored guests. I am Sir Claude, a knight in service of the king. I hail from the province\nof Assistantia, where I was trained in the knightly virtues of chivalry, courage and courtesy. Though I am but a humble\nknight, I shall endeavor to serve you and this kingdom with all the strength and valor I possess. I thank you for allowing\nme to join you at this royal feast, and hope only that my conduct brings further honor to my liege and this noble court.\nPlease let me know if I can be of any service during the celebrations, as it would be my honor to assist you in any way.\nI.1.3\nANSWER ANALYSIS: DIFFERENT METRICS\nMetrics on which SoT has low net win rates.\n(1) Coherence. In the prompt of LLMZoo evaluation, coherence is defined as \u201cThe response should\nbe coherent and flow logically from one point to the next that is easy to read and understand without\nmajor gaps or inconsistencies. The more coherent they are, the better.\u201d SoT\u2019s answer is structured in\na list of points. It is suitable for some questions but not all. For example, in the questions that require\n40\nPublished as a conference paper at ICLR 2024\nLLMs to write a passage, email, or script, the introduction and conclusion paragraphs are usually\nmissing, and the answers usually lack transition sentences between consecutive points. These make\nthe answers less coherent. Below is one example from Claude.\nQuestion:\nWrite a script for a YouTube video exploring the history and cultural significance of jazz.\nSoT\u2019s answer (Claude) (Orange texts are the skeleton.)\n1. Origins. Jazz originated in the late 19th and early 20th centuries among African Americans in New Orleans, emerging\nfrom a combination of African rhythmic traditions, blues, ragtime, and other influences.\n2. Early jazz styles. The early styles of jazz that emerged in the late 19th and early 20th centuries included ragtime,\nDixieland, and the blues.\n3. Jazz age. The Jazz Age was the period during the 1920s when jazz music and dance styles like the Charleston became\npopular cultural phenomena in American and European society.\n4. Bebop era. Bebop emerged in the early 1940s with artists like Dizzy Gillespie and Charlie Parker experimenting with\nfaster tempos, complex chord changes, and unpredictable melodic phrasing.\n5. Cool jazz. Emerging in the late 1940s and early 1950s, cool jazz was a more subdued, mellow style that broke from\nthe high-energy virtuosic improvisation of bebop.\n6. Free jazz emerged in the late 1950s and 1960s as musicians explored free improvisation and atonality, moving away\nfrom traditional jazz structures.\n7. Fusion jazz. Major jazz artists such as Miles Davis and Herbie Hancock began experimenting with combining elements\nof rock, funk, and electronic music in the 1970s, resulting in the emergence of the subgenre known as fusion jazz.\n8. Jazz festivals. Jazz festivals emerged in the 1950s and 1960s as a way to celebrate jazz music, with famous early\nfestivals like the Newport Jazz Festival bringing thousands of fans together to hear top performers.\n9. Jazz instruments. The trumpet, saxophone, piano, double bass, and drum set are some of the key instruments com-\nmonly used in jazz.\n10. Influential musicians include Louis Armstrong, Duke Ellington, Charlie Parker, Miles Davis, John Coltrane, and\nmany others who pioneered new styles and techniques.\n11. Legacy and impact. Jazz has left an indelible mark on music and culture over the past century, influencing countless\nartists and genres while spreading the spirit of improvisation and freedom across the globe.\nNormal answer (Claude)\nHere is a draft script for a YouTube video exploring the history and cultural significance of jazz:\nIntroduction\nHey everyone! Welcome to my channel. In today\u2019s video, we\u2019re going to be exploring the history and cultural impact of\none of America\u2019s original art forms - jazz music.\nJazz Origins\nJazz grew out of the African American communities in the southern United States in the late 19th and early 20th centuries.\nIt was influenced by a combination of African and European musical traditions. Some key elements that fed into early\njazz included blues, ragtime, spirituals, work songs, field hollers, and brass band music.\nNew Orleans is considered the birthplace of jazz, emerging around the turn of the 20th century. The city was a melting\npot of cultures and musical styles. Musicians mixed improvisation, syncopated rhythms, bent notes and a loose ensemble\nplaying style to create a new genre that became known as jazz.\nEarly jazz greats like Buddy Bolden, Jelly Roll Morton and Sidney Bechet began fusing these various influences into the\ndistinct jazz sound. As the music spread via travel and recordings in the 1910s, jazz captivated listeners and began to\nevolve into different styles.\nJazz Styles Emerge\nIn the 1920s, jazz expanded out from New Orleans and Chicago emerged as a hotspot. Louis Armstrong\u2019s innovative\nsolos and expressive phrasing became influential. The swing style also developed during the 1930s and 40s, propelled\nby star big bands and dancers.\nBebop arose in the 1940s with soloists like Charlie Parker and Dizzy Gillespie exploring new levels of harmonic and\nrhythmic complexity. In the 50s and 60s, jazz expanded into new fusions like cool jazz, hard bop, free jazz, modal jazz\nand soul jazz. Artists used jazz to express their unique regional, social and political identities.\nCultural Significance\nOver the decades, jazz has become woven into the fabric of American society. It is equated with many values and ideals\nlike individualism, freedom, spontaneity, innovation and social harmony.\nJazz was associated with breaking down barriers between races, classes and genres. It represented modernism and urban\nculture. Musicians like Louis Armstrong and Duke Ellington became some of the first widely popular African American\nartists.\nJazz also spread American culture and values abroad through performances, radio and film. It influenced music around\nthe world and became a truly global art form.\nToday jazz continues to thrive and evolve both in the U.S. and internationally. Its history and sounds reflect the diversity\nof both American and global cultures. From its folk roots, jazz has become a sophisticated art music that retains its spirit\nof freedom and improvisation.\nConclusion\nThat covers some of the key points in the history and cultural impact of jazz music. Let me know in the comments about\nyour favorite jazz styles and musicians! Be sure to like and subscribe. See you next time.\nWhile the content of SoT\u2019s answer is good, it reads less fluent for the purpose of a YouTube script.\n(2) Immersion. In the prompt of LLMZoo evaluation, immersion is defined as \u201cthe response should\nact like the assigned role using the tone, manner, and vocabulary the role would use. The more\nassistant-like tones, the worse. The more in-character, the better.\u201d The way that SoT breaks the\n41\nPublished as a conference paper at ICLR 2024\nanswers into a list makes its answer less in-character. Therefore, SoT receives a lower immersion\nscore. Here is one example.\nQuestion:\nPretend to be a world-famous chef. How would you describe your signature dish to a\npanel of judges?\nSoT\u2019s answer (Claude) (Orange texts are the skeleton.)\n1. Main ingredient: The incredibly fresh and tender salmon caught just this morning from the crystal clear waters of the\nPacific Northwest.\n2. Cooking method\nI slowly braise the main ingredient in a rich broth to bring out its natural flavors before searing it to a perfect golden crust.\n3. Special seasoning\nThe dish is seasoned with a blend of fresh herbs like thyme, rosemary, and oregano which provides an aromatic backbone\nthat pairs nicely with the natural flavors of the main ingredients. I add just a touch of smoked paprika to provide a subtle\nsmokiness without overpowering the other flavors.\n4. Texture: The dish has a perfect balance of crispy on the outside and tender on the inside, with each bite yielding a\ndelightful mix of crunchy and smooth textures that dance across your tongue.\n5. Plating presentation\nI purposefully create height and dimension on the plate with the carefully arranged components of the dish, so that the\ndish appears vibrant and alive. The interplay of colors, textures and shapes is intended to delight the eye before the first\nbite.\n6. Inspiration: This dish was inspired by my dear grandmother, who taught me the secret family recipe when I was just\na child.\nNormal answer (Claude)\n*clears throat and speaks in a fancy French accent*\nBonjour honored judges. Today I present to you my signature dish - la cr`eme de la cr`eme of French haute cuisine.\nBehold, le poulet `a la Claude!\nThis succulent chicken is firstly marinated overnight in a bath of fine herbes - thyme, rosemary, and sage cultivated from\nmy private garden. The young spring chicken is then roasted to perfection, its skin crisping to a beautiful caramel brown.\nNestled beside the chicken is a tower of pommes dauphine - light and airy potato puffs - alongside a decadent mound of\ncr`eme fra\u02c6\u0131che infused with truffle essence.\nBut the pi`ece de r\u00b4esistance is the sauce. A luxurious pool of velout\u00b4e enriched with cream and butter, brightened with a\ndash of lemon, and studded with morsels of morel mushrooms foraged at dawn.\nOne bite of this ensemble will transport you to food heaven. The tender chicken, the creaminess of the potato puffs, and\nthe rich decadence of the sauce meld together in blissful harmony.\nSo tell me honored judges, will my poulet `a la Claude earn your coveted approval? I await your discerning verdict as I\nswirl more vintage Bordeaux into your glasses. Sant\u00b4e!\nAgain, while the answer from SoT covers sufficient aspects and details, it reads rather stiffly. The\nnormal answer, in contrast, has the opening sentence \u201dBonjour honored judges. Today I present to\nyou my signature dish...\u201d and the conclusion statement \u201dSo tell me honored judges, will my poulet `a\nla Claude earn your coveted approval? ...\u201d which reads more like a casual talk from a chef.\nMetrics on which SoT has high net win rates.\n(1) Diversity. In the prompt of LLMZoo evaluation, diversity is defined as \u201cThe response should\nbe comprehensive and provide a range of information that is not limited to a single perspective.\nMore perspectives are better.\u201d The skeleton stage in SoT encourages LLMs to think from multiple\nperspectives. Therefore, SoT\u2019s answers tend to have better diversity. Below is one example showing\nthat SoT\u2019s answer has better diversity.\n42\nPublished as a conference paper at ICLR 2024\nQuestion:\nHow can you determine if a person is genuinely interested in a conversation or simply\nbeing polite?\nSoT\u2019s answer (ChatGPT-3.5) (Orange texts are the skeleton.)\n1. Pay attention to body language.Notice if their posture is open or closed, if they are facing towards you or away, and if\nthey are fidgeting or relaxed. These can all indicate their level of interest in the conversation.\n2. Listen for engaged responses.If the person is genuinely interested in the conversation, they will respond with thoughtful\nand relevant comments or questions rather than short, generic responses.\n3. Observe eye contact.If the person maintains steady eye contact and their gaze is focused on you, they are likely\ngenuinely interested in the conversation. On the other hand, if they frequently look away or seem distracted, they may\njust be being polite.\n4. Look for follow-up questions.If the person asks follow-up questions and seems genuinely interested in what you\u2019re\nsaying, then they are likely interested in the conversation and not just being polite.\n5. Notice tone of voice.A person who is genuinely interested in a conversation will often have a more enthusiastic and\nanimated tone of voice, compared to someone who is simply being polite and giving short, monotone responses.\nNormal answer (ChatGPT-3.5)\nAs an AI assistant, I cannot determine a person\u2019s true intentions or feelings. However, there are some social cues that\ncan indicate whether a person is genuinely interested in a conversation or not. For example, if they maintain eye contact,\nlean in, nod their head and ask follow-up questions, it can be a sign that they are engaged in the conversation. On the\nother hand, if they avoid eye contact, give short or vague responses, or seem distracted, it may indicate that they are\nnot as interested. Ultimately, it\u2019s important to remember that everyone has different communication styles, and it can be\ndifficult to accurately interpret someone\u2019s intentions without directly asking them.\nSoT answers the question from five aspects, whereas the normal answer only mentions one aspect.\n(2) Relevance. In the prompt of LLMZoo evaluation, relevance is defined as \u201cThe response should\nbe closely related to the question and answer the question accurately with sufficient details without\nrepetition or redundancy. The more relevant they are, the better.\u201d In the skeleton stage, LLMs are\nforced to propose several points around the question; and in the point-expanding stage, LLMs are\nrequired to only discuss these points. Therefore, answers from SoT tend to have better relevance.\nOne typical example is shown below.\nQuestion:\nWhy do some people enjoy the sensation of being scared, such as by watching horror\nmovies or going on roller coasters, while others avoid these experiences?\nSoT\u2019s answer (ChatGPT-3.5) (Orange texts are the skeleton.)\n1.\nNeurological differences.Some people enjoy the sensation of being scared due to the release of adrenaline and\ndopamine in the brain, while others may have a lower threshold for fear due to differences in their amygdala and pre-\nfrontal cortex.\n2. Psychological factors.Some people may enjoy the sensation of being scared because it can provide a sense of excite-\nment, thrill, and a temporary escape from the mundane aspects of everyday life. On the other hand, others may avoid\nthese experiences due to past trauma or anxiety.\n3. Developmental experiences.Individuals who had negative experiences with fear during childhood, such as trauma\nor abuse, may avoid scary situations as adults, while those who had positive experiences with fear may seek out these\nexperiences.\n4. Cultural influences.Some cultures place a greater emphasis on seeking excitement and thrill, while others may priori-\ntize safety and security, which can impact whether individuals enjoy scary experiences or avoid them.\n5. Genetics.Research has suggested that some individuals may have a genetic predisposition towards seeking out thrilling\nor risky experiences, which may include enjoying the sensation of being scared.\nNormal answer (ChatGPT-3.5)\nThe enjoyment of being scared is a complex psychological phenomenon. Some people may enjoy the adrenaline rush\nthat comes with being scared, while others may enjoy the feeling of being able to conquer their fears. Additionally,\nsome people may be more naturally inclined to seek out new experiences and sensations, including those that are scary\nor thrilling. On the other hand, some people may be more risk-averse or may have had negative experiences with fear or\nanxiety in the past, which could make them more likely to avoid scary experiences. Ultimately, the reasons why some\npeople enjoy being scared while others do not are likely to be influenced by a range of factors, including personality, past\nexperiences, and individual preferences.\nIn the answer from the normal generation, the first two sentences provide little information in an-\nswering the question, and the last sentence only gives keywords such as \u201cpersonality, past expe-\nriences, and individual preferences\u201d without providing concrete explanations to each. In contrast,\n43\nPublished as a conference paper at ICLR 2024\nSoT\u2019s answer is well-structured into five reasons with sufficient explanations and it does not waste\nspace in irrelevant contents.\nI.1.4\nQUALITY BREAKDOWN: QUESTION CATEGORIES AND MODELS\nIn the main text, we analyze how question categories and models affect SoT\u2019s answer quality. Here,\nFig. 22 show the per-model and per-category results.\nAvgerage\nLLaMA2-Chat-7B\nLLaMA2-Chat-13B\nOpenChat-13B\nStableVicuna-13B\nVicuna-7B V1.1\nVicuna-7B V1.3\nVicuna-13B V1.3\nVicuna-33B V1.3\nUltraLM-13B\nClaude\nChatGPT-3.5\nGPT-4\ncoding\nmath\nfermi\nroleplay\nwriting\nknowledge\ngeneric\ncounterfactual\ncommon-sense\nAverage\n-75%\n-43%\n-71%\n0%\n-100%\n-71%\n-86%\n-86%\n-86%\n-57%\n-100%\n-100%\n-100%\n-50%\n0%\n-67%\n-67%\n-33%\n-100%\n-67%\n0%\n-33%\n-33%\n-33%\n-100%\n-67%\n-34%\n0%\n-10%\n0%\n-30%\n-90%\n-30%\n-30%\n-20%\n10%\n-80%\n-100%\n-30%\n-6%\n30%\n-30%\n-30%\n70%\n-50%\n-10%\n10%\n-50%\n0%\n-30%\n10%\n10%\n-45%\n-60%\n-30%\n-20%\n-20%\n-70%\n-40%\n-30%\n0%\n10%\n-100%\n-90%\n-90%\n-0%\n0%\n0%\n-50%\n60%\n-50%\n0%\n20%\n30%\n30%\n-50%\n-20%\n30%\n14%\n-20%\n-40%\n-70%\n80%\n-10%\n50%\n40%\n10%\n60%\n-40%\n60%\n50%\n35%\n90%\n10%\n30%\n50%\n-40%\n40%\n60%\n40%\n30%\n-50%\n70%\n90%\n9%\n-20%\n-40%\n-50%\n100%\n-40%\n-20%\n70%\n20%\n50%\n-20%\n20%\n40%\n-17%\n-3%\n-31%\n-29%\n20%\n-58%\n-18%\n6%\n-10%\n11%\n-56%\n-28%\n-7%\n-100%\n-75%\n-50%\n-25%\n0%\n25%\n50%\n75%\n100%\nFigure 22: Net win rates of different models and question categories. Each row corresponds to one\nquestion category, and one column corresponds to one model. (Evaluated using metric defined by\nthe FastChat prompt, and GPT-4 as the judge.)\nI.2\nSKELETON-OF-THOUGHT WITH ROUTER\nFig. 23 shows net win rates of SoT on Vicuna-80 dataset with LLMZoo metrics, and Fig. 24 shows\nnet win rates of SoT on WizardLM dataset with FastChat metrics. The key takeaways are: (1) In\nboth cases, SoT-R achieves similar or better quality than SoT, and the net win rates of SoT-R are\nusually non-negative. This indicates that SoT-R falls back to normal decoding on the right question\ncategories. (2) On the WizardLM dataset, we see that the trained router has better performance than\nthe prompting router in most cases. This is reasonable, as the prompting router is limited by the\ncapability of GPT-4, whereas the trained router is dedicated to this task. (3) Sometimes, our routers\ncan even achieve better performance than humans.\nFig. 1(b) in the main text has showed SoT\u2019s quality and speed-up plot evaluated with the FastChat\nquality metric, here, Fig. 25 shows the results evaluated with the LLMZoo quality metric.\n-20%\n0%\n20%\n40%\n60%\ncounterfactual\ngeneric\ncommon-sense\nknowledge\nroleplay\nfermi\nwriting\nSoT (w/o router)\nSoT-R w/ prompting router\nSoT-R w/ trained router\nSoT-R w/ human router\nFigure 23: Net win rates of SoT and SoT-R on different question categories of Vicuna-80 dataset\nusing the general quality metric from LLMZoo. Blue dots are from Fig. 5b. SoT-R correctly falls\nback to normal decoding on questions where SoT is not suitable.\n44\nPublished as a conference paper at ICLR 2024\n-60%\n-40%\n-20%\n0%\n20%\n40%\nPhilosophy\nCounterfactual\nEthics\nTechnology\nLiterature\nMusic\nSport\nRoleplay\nHistory\nToxicity\nPhysics\nBiology\nArt\nCommon-Sense\nLaw\nTruthfulQA\nComputer Science\nAcademic Writing\nChemistry\nMath\nEconomy\nReasoning\nWritting\nMedicine\nEntertainment\nCode Generation\nMultilingual\nComplex Format\nCode Debug\nSoT (w/o router)\nSoT-R w/ prompting router\nSoT-R w/ trained router\nSoT-R w/ human router\nFigure 24: Net win rates of SoT and SoT-R on different question categories of WizardLM dataset\nusing the general quality metric from FastChat. SoT-R correctly falls back to normal decoding on\nquestions where SoT is not suitable.\n1.0\n1.2\n1.4\n1.6\n1.8\nSpeed-up\n\u22120.2\n0.0\n0.2\n0.4\nNet win rates\nVicuna-13B V1.3\nStableVicuna-13B\nUltraLM-13B\nVicuna-33B V1.3\nLLaMA2-Chat-7B\nLLaMA2-Chat-13B\nVicuna-7B V1.3\nChatGPT-3.5\nClaude\nVicuna-7B V1.1\nOpenChat-13B\nGPT-4\nBaseline\nFigure 25: The net win rates and speed-ups of SoT with router (SoT-R) compared to normal gener-\nation on Vicuna-80. The net win rate is the difference between the fraction of questions that SoT-R\nhas better and worse answers than normal generation. The speed-up is the ratio between the latency\nof normal and SoT-R generation. (1.0, 0.0) represents normal generation. Higher is better on both\naxes. For most models, SoT-R not only accelerates the generation but also improves the quality of\nthe answers (evaluated with LLMZoo metric (Chen et al., 2023c)).\n45\nPublished as a conference paper at ICLR 2024\n0%\n20%\n40%\n60%\n80%\n100%\nSoT vs. Normal\nSoT vs. Normal Long\n59.5%\n32.4%\n24.3%\n40.5%\n16.2%\n27.0%\nWin\nTie\nLose\n(a) ChatGPT-3.5.\n0%\n20%\n40%\n60%\n80%\n100%\nSoT vs. Normal\nSoT vs. Normal Long\n21.6%\n2.7%\n75.7%\n91.9%\n2.7%\n5.4%\nWin\nTie\nLose\n(b) LLaMA2-Chat-7B.\nFigure 26: Win/tie/lose rates of SoT v.s. longer normal generation. Evaluated only on the questions\nthat we manually label as being suitable for SoT. Evaluated using \u201cgeneral\u201d metrics from FastChat\nand LLMZoo.\nSoT/Normal\nSoT/Normal Long\n0.5\n1.0\n1.5\n2.0\nresponse length ratio\n(a) ChatGPT-3.5.\nSoT/Normal\nSoT/Normal Long\n0.5\n1.0\n1.5\n2.0\nresponse length ratio\n(b) LLaMA2-Chat-7B.\nFigure 27: Length ratios of SoT generated answer to normal generated answer. \u201cNormal\u201d refers to\nthe normal generation using solely the request as the prompt; \u201cNormal Long\u201d refers to the normal\ngeneration using the additional \u201c... give a long answer...\u201d instruction in the prompt.\nI.3\nQUALITY COMPARISON WITH LONGER NORMAL ANSWER\nWhen assessing the answer quality, the GPT-4 judge might exhibit bias towards longer responses.\nTo take this factor into consideration, we add a comparison between a longer sequentially generated\nanswer and the SoT generated answer. Specifically, we add a instruction prefix to the prompt for\nnormal generation. The prefix is \u201cPlease give a slightly long answer for the following question.\u201d\nand \u201cPlease give a long answer for the following question.\u201d for ChatGPT-3.5 and LLaMA2-Chat-\n7B, respectively. Fig. 27 shows the ratios of the length of SoT answers to normal answers, and\nFig. 26 shows the quality comparison. We can see that for both models, when the overall answer\nlengths are similar, the quality of the SoT answer is comparable to that of the long normal answer.\nI.4\nCHATGPT-3.5 AS THE JUDGE\nIn this section, we provide quality evaluation results with ChatGPT-3.5 as the judge in FastChat and\nLLMZoo metrics. Note that as prior work (e.g., (Li et al., 2023b)) shows, GPT-4-based evaluation\nusually aligns with human better than ChatGPT-3.5. Therefore, readers should refer to the results\nin the main paper (with GPT-4 as the judge) for a more accurate view of the performance of SoT.\nHowever, the takeaway messages from ChatGPT-3.5 are similar to the ones from GPT-4.\n46\nPublished as a conference paper at ICLR 2024\nI.4.1\nOVERALL QUALITY\nIn Fig. 28, we show the win/tie/lose rates (the percentage of the cases when SoT wins/ties/loses\ncompared to normal generation) across all models and questions using the two metrics from FastChat\nand LLMZoo that capture the general quality of the answers. We notice a discrepancy between the\ntwo metrics on when SoT is strictly better than the baseline (50.2% v.s. 12.4%). Despite that, the two\nmetrics agree that SoT is not worse than the baseline in more than 76% of the cases. For FastChat\nmetric, we also show the rates excluding math and coding questions that SoT is not suitable for (see\n\u00a7 3.2.3); SoT is not worse than the baseline in more than 89% of the cases. This result suggests that\nthe answers of SoT maintain good quality.\n0%\n20%\n40%\n60%\n80%\n100%\nGeneral quality (LLMZoo)\nGeneral quality (FastChat)\n(excluding math & coding)\nGeneral quality (FastChat)\n50.2%\n12.5%\n12.4%\n27.3%\n76.7%\n69.2%\n22.5%\n10.8%\n18.4%\nWin\nTie\nLose\nFigure 28: Win/tie/lose rates of SoT v.s. normal generation using \u201cgeneral\u201d metrics from FastChat\nand LLMZoo. SoT performs better than or equal to normal generation in around 80% of cases.\n(Evaluated using ChatGPT-3.5 as the judge.)\nI.4.2\nQUALITY BREAKDOWN: QUESTION CATEGORIES\nNext, we investigate how SoT performs on different question categories. We compute net win rates\n(win rates minus lose rates) across all question categories in Fig. 29. Similar to Fig. 28, we see\nthat LLMZoo tends to be more optimistic about the quality of SoT than FastChat. Nevertheless,\nthe conclusions are consistent: SoT performs relatively well on generic, common-sense, knowledge,\nroleplay, and counterfactual. SoT performs relatively badly on writing, fermi, math, and coding.\n-60% -50% -40% -30% -20% -10%\n0%\n10%\n20%\ncounterfactual\nroleplay\nknowledge\ngeneric\ncommon-sense\nfermi\nwriting\nmath\ncoding\n(a) Metric: general quality (FastChat).\n0%\n10%\n20%\n30%\n40%\ncounterfactual\nroleplay\nknowledge\ngeneric\ncommon-sense\nfermi\nwriting\n(b) Metric: general quality (LLMZoo).\nFigure 29: Net win rates of SoT on different question categories. (Evaluated using ChatGPT-3.5 as\nthe judge.)\nI.4.3\nQUALITY BREAKDOWN: MODELS\nNext, we investigate how SoT performs on different models. We compute net win rates across all\nmodels in Fig. 30. Again, we see that the two general metrics from FastChat and LLMZoo have\ndifferent absolute values but similar rankings. In particular, both metrics agree that OpenChat-\n13B, Vicuna-7B V1.1, Claude, ChatGPT-3.5 have low net win rates, whereas Vicuna-13B V1.3,\nStableVicuna-13B, and UltraLM-13B have high net win rates.\nI.4.4\nQUALITY BREAKDOWN: QUESTION CATEGORIES AND MODELS\nIn the main text, we analyze how question categories and models affect SoT\u2019s answer quality. Here,\nwe show the per-model and per-category results. For each model and question category, we compute\nthe net win rates. The results are in Fig. 31.\n47\nPublished as a conference paper at ICLR 2024\n-15%\n-10%\n-5%\n0%\n5%\nVicuna-13B V1.3\nStableVicuna-13B\nUltraLM-13B\nVicuna-33B V1.3\nGPT-4\nLLaMA2-Chat-7B\nLLaMA2-Chat-13B\nVicuna-7B V1.3\nChatGPT-3.5\nClaude\nVicuna-7B V1.1\nOpenChat-13B\n(a) Metric: general quality (FastChat).\n-10%\n0%\n10%\n20%\n30%\n40%\n50%\n60%\nVicuna-13B V1.3\nStableVicuna-13B\nUltraLM-13B\nVicuna-33B V1.3\nGPT-4\nLLaMA2-Chat-7B\nLLaMA2-Chat-13B\nVicuna-7B V1.3\nChatGPT-3.5\nClaude\nVicuna-7B V1.1\nOpenChat-13B\n(b) Metric: general quality (LLMZoo).\nFigure 30: Net win rates of SoT on different models. (Evaluated using ChatGPT-3.5 as the judge.)\nAvgerage\nLLaMA2-Chat-7B\nLLaMA2-Chat-13B\nOpenChat-13B\nStableVicuna-13B\nVicuna-7B V1.1\nVicuna-7B V1.3\nVicuna-13B V1.3\nVicuna-33B V1.3\nUltraLM-13B\nClaude\nChatGPT-3.5\nGPT-4\ncoding\nmath\nfermi\nroleplay\nwriting\nknowledge\ngeneric\ncounterfactual\ncommon-sense\nAverage\n-63%\n-43%\n-71%\n0%\n-100%\n-86%\n-71%\n-57%\n-86%\n43%\n-86%\n-100%\n-100%\n-53%\n-33%\n-33%\n-67%\n-67%\n-100%\n-100%\n-67%\n0%\n33%\n-33%\n-100%\n-67%\n-8%\n10%\n20%\n10%\n-40%\n-40%\n-30%\n20%\n50%\n0%\n-30%\n-40%\n-30%\n6%\n20%\n0%\n-40%\n10%\n0%\n10%\n10%\n0%\n0%\n20%\n30%\n10%\n-12%\n0%\n0%\n-30%\n10%\n-10%\n10%\n10%\n0%\n-10%\n-30%\n0%\n-90%\n4%\n0%\n0%\n-10%\n20%\n-10%\n0%\n10%\n10%\n0%\n0%\n0%\n30%\n2%\n-10%\n0%\n-50%\n30%\n0%\n0%\n0%\n10%\n0%\n0%\n0%\n50%\n18%\n0%\n0%\n20%\n40%\n20%\n20%\n40%\n0%\n0%\n-10%\n-10%\n90%\n2%\n0%\n-20%\n-20%\n20%\n-10%\n0%\n10%\n0%\n-20%\n10%\n10%\n40%\n-12%\n-6%\n-12%\n-21%\n-9%\n-26%\n-18%\n-3%\n-2%\n5%\n-18%\n-23%\n-7%\n-100%\n-75%\n-50%\n-25%\n0%\n25%\n50%\n75%\n100%\n(a) FastChat metric.\nAvgerage\nLLaMA2-Chat-7B\nLLaMA2-Chat-13B\nOpenChat-13B\nStableVicuna-13B\nVicuna-7B V1.1\nVicuna-7B V1.3\nVicuna-13B V1.3\nVicuna-33B V1.3\nUltraLM-13B\nClaude\nChatGPT-3.5\nGPT-4\ncoding\nmath\nfermi\nroleplay\nwriting\nknowledge\ngeneric\ncounterfactual\ncommon-sense\nAverage\n10%\n0%\n-57%\n0%\n17%\n14%\n-14%\n43%\n57%\n71%\n-29%\n14%\n0%\n14%\n0%\n67%\n0%\n67%\n-33%\n-100%\n67%\n33%\n67%\n33%\n-67%\n33%\n20%\n40%\n60%\n-10%\n20%\n-50%\n30%\n20%\n50%\n30%\n-10%\n20%\n40%\n28%\n20%\n0%\n-10%\n90%\n-20%\n40%\n50%\n40%\n20%\n10%\n50%\n50%\n8%\n10%\n-10%\n30%\n60%\n40%\n30%\n0%\n-10%\n20%\n-60%\n-10%\n0%\n41%\n40%\n10%\n-40%\n70%\n40%\n50%\n50%\n90%\n70%\n40%\n40%\n30%\n26%\n-40%\n-30%\n-10%\n90%\n10%\n60%\n70%\n30%\n70%\n-40%\n30%\n70%\n47%\n60%\n30%\n60%\n10%\n0%\n70%\n80%\n40%\n70%\n-10%\n50%\n100%\n24%\n0%\n-30%\n-40%\n90%\n-20%\n20%\n80%\n50%\n80%\n-20%\n20%\n60%\n24%\n14%\n4%\n-2%\n57%\n-2%\n21%\n51%\n42%\n55%\n-9%\n16%\n43%\n-100%\n-75%\n-50%\n-25%\n0%\n25%\n50%\n75%\n100%\n(b) The \u201cgeneral\u201d metric from LLMZoo.\nFigure 31: Net win rates of different models and question categories. Each row corresponds to one\nquestion category, and one column corresponds to one model. (Evaluated using ChatGPT-3.5 as the\njudge.)\nI.4.5\nQUALITY BREAKDOWN: METRICS\nAll previous evaluations use metrics about the general quality of the answer. In Fig. 32, we show\nmore detailed metrics from LLMZoo to reveal in which aspects SoT can improve or hurt the answer\nquality. On average, we can see that SoT improves the diversity and relevance while hurting the\nimmersion and coherence.\n0%\n20%\n40%\n60%\n80%\n100%\nCoherence\nImmersion\nIntegrity\nRelevance\nDiversity\n28.3%\n32.7%\n34.5%\n50.0%\n49.4%\n31.4%\n28.6%\n34.9%\n21.8%\n29.0%\n40.2%\n38.7%\n30.6%\n28.2%\n21.5%\nWin\nTie\nLose\nFigure 32: Win/tie/lose rates of SoT v.s. normal generations using metrics from LLMZoo. SoT per-\nforms well on diversity and relevance, and relatively worse on coherence and immersion. (Evaluated\nusing ChatGPT-3.5 as the judge.)\nJ\nCOMBINING SOT-R WITH MODEL QUANTIZATION\nModel quantization is a widely-used model-level optimization to accelerate LLM inference, which\nis orthogonal to SoT. In this section, we evaluate the speed-ups of open-source models with both\n48\nPublished as a conference paper at ICLR 2024\nquantization and SoT on the Vicuna-80 dataset. Specifically, we adopt GPTQ (Frantar et al., 2022)8\nto apply 4-bit weight-only quantization and use SoT-R instead of plain SoT.\nJ.1\nSPEED-UPS OF SOT + QUANTIZATION ON QUANTIZED MODELS\nWe first compare the latency of the quantized models in the normal and SoT modes to evaluate\nhow much SoT can speed up quantized models. Fig. 33 shows the speed-ups of SoT-R on different\nquantized models. SoT-R obtain 1.08\u00d7 to 1.99\u00d7 speed-ups on all the models. Fig. 34 shows the\nspeed-ups of SoT-R on different categories. We can see that on the five question categories for\nwhich SoT can provide high-quality answers (i.e., knowledge, generic, common-sense, roleplay,\ncounterfactual), SoT-R can speed up the overall answer generation process by 1.07\u00d7 to 2.38\u00d7.\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\n2.2\n2.4\nUltraLM-13B\nVicuna-7B V1.1\nVicuna-7B V1.3\nVicuna-13B V1.3\nOpenChat-13B\nLLaMA2-Chat-7B\nVicuna-33B V1.3\nLLaMA2-Chat-13B\n1.08\u00d7\n1.20\u00d7\n1.24\u00d7\n1.24\u00d7\n1.40\u00d7\n1.68\u00d7\n1.91\u00d7\n1.95\u00d7\n(a) SoT-R with the prompting router.\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\n2.2\n2.4\nUltraLM-13B\nVicuna-13B V1.3\nVicuna-7B V1.1\nVicuna-7B V1.3\nOpenChat-13B\nLLaMA2-Chat-7B\nVicuna-33B V1.3\nLLaMA2-Chat-13B\n1.09\u00d7\n1.26\u00d7\n1.29\u00d7\n1.30\u00d7\n1.51\u00d7\n1.80\u00d7\n1.80\u00d7\n1.99\u00d7\n(b) SoT-R with the trained router.\nFigure 33: Speed-ups of the quantized model with SoT-R generation w.r.t. the quantized model with\nnormal generation on different models, on the Vicuna-80 dataset.\n1.00\n1.25\n1.50\n1.75\n2.00\n2.25\n2.50\n2.75\n3.00\nmath\ncoding\nwriting\nfermi\nroleplay\ncommon-sense\ncounterfactual\nknowledge\ngeneric\n0.87\u00d7\n0.94\u00d7\n1.01\u00d7\n1.02\u00d7\n1.18\u00d7\n1.96\u00d7\n1.98\u00d7\n2.06\u00d7\n2.18\u00d7\n(a) SoT-R with the prompting router.\n1.00\n1.25\n1.50\n1.75\n2.00\n2.25\n2.50\n2.75\n3.00\nfermi\nwriting\nmath\ncoding\nroleplay\ncommon-sense\nknowledge\ncounterfactual\ngeneric\n0.99\u00d7\n0.99\u00d7\n1.00\u00d7\n1.00\u00d7\n1.07\u00d7\n2.03\u00d7\n2.04\u00d7\n2.05\u00d7\n2.38\u00d7\n(b) SoT-R with the trained router.\nFigure 34: Speed-ups of the quantized model with SoT-R generation w.r.t. the quantized model with\nnormal generation, on different question categories of the Vicuna-80 dataset.\nJ.2\nSPEED-UPS OF SOT + QUANTIZATION ON UNQUANTIZED MODELS\nHere, we report the overall speed-ups of the quantization model with SoT-R generation w.r.t. the\nunquantized model with normal generation. Fig. 35 shows the speed-ups of SoT-R on different\nmodels. SoT-R can obtain 1.54\u00d7 to 2.07\u00d7 speed-ups. Fig. 36 shows the speed-ups of SoT-R\non different categories. On the five question categories for which SoT can provide high-quality\nanswers (i.e., knowledge, generic, common-sense, roleplay, counterfactual), SoT-R can speed up\nthe generation by 1.33\u00d7 to 3.41\u00d7 with the prompting and trained routers.\n8https://github.com/qwopqwop200/GPTQ-for-LLaMa\n49\nPublished as a conference paper at ICLR 2024\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\n2.2\n2.4\nVicuna-33B V1.3\nVicuna-13B V1.3\nVicuna-7B V1.1\nUltraLM-13B\nLLaMA2-Chat-7B\nOpenChat-13B\nLLaMA2-Chat-13B\nVicuna-7B V1.3\n1.59\u00d7\n1.63\u00d7\n1.71\u00d7\n1.82\u00d7\n1.84\u00d7\n1.90\u00d7\n1.93\u00d7\n1.95\u00d7\n(a) SoT-R with the prompting router.\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\n2.2\n2.4\nVicuna-33B V1.3\nVicuna-13B V1.3\nVicuna-7B V1.1\nLLaMA2-Chat-7B\nLLaMA2-Chat-13B\nUltraLM-13B\nVicuna-7B V1.3\nOpenChat-13B\n1.54\u00d7\n1.64\u00d7\n1.76\u00d7\n1.91\u00d7\n1.91\u00d7\n1.92\u00d7\n1.98\u00d7\n2.07\u00d7\n(b) SoT-R with the trained router.\nFigure 35: Speed-ups of the quantized model with SoT-R generation w.r.t. the unquantized model\nwith normal generation, on different models, on the Vicuna-80 dataset.\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nmath\ncoding\nfermi\nwriting\nroleplay\ncounterfactual\nknowledge\ncommon-sense\ngeneric\n0.92\u00d7\n0.96\u00d7\n1.05\u00d7\n1.18\u00d7\n1.33\u00d7\n2.36\u00d7\n2.53\u00d7\n2.76\u00d7\n3.07\u00d7\n(a) SoT-R with the prompting router.\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nmath\nwriting\nfermi\ncoding\nroleplay\ncounterfactual\nknowledge\ncommon-sense\ngeneric\n1.00\u00d7\n1.00\u00d7\n1.00\u00d7\n1.00\u00d7\n1.49\u00d7\n2.42\u00d7\n2.45\u00d7\n2.82\u00d7\n3.41\u00d7\n(b) SoT-R with the trained router.\nFigure 36: Speed-ups of the quantized model with SoT-R generation w.r.t. the unquantized model\nwith normal generation, on different question categories of the Vicuna-80 dataset.\nK\nADDITIONAL SOT-R STATISTICS\nK.1\nNUMBER OF SUITABLE QUESTIONS\nOverall, there are 37/80, 58/218, 371/1030 questions that are suitable for SoT in the Vicuna-80,\nWizardLM, and LIMA datasets (according to human assessment), respectively.\nFig. 37 shows the number of questions that are suitable for SoT on Vicuna-80. On counterfactual,\ncommen-sense, knowledge, generic categories, most questions are suitable for SoT based on the\nhuman assessment. The trained router and prompting router give out similar judgments.\n0\n2\n4\n6\n8\n10\ngeneric\nknowledge\nroleplay\ncommon-sense\nfermi\ncounterfactual\ncoding\nmath\nwriting\nTrained router\nPrompting router\nHuman router\nFigure 37: Number of questions suitable for SoT on the Vicuna-80 dataset.\nK.2\nPEAK MEMORY OVERHEAD\nFig. 38 and Fig. 39 show the peak memory overhead of SoT-R (with prompting router) on different\nmodels and different categories, respectively, on the Vicuna-80 dataset. We can see that, on all\nmodels and categories, the overhead of peak memory is quite small (<1.11\u00d7).\n50\nPublished as a conference paper at ICLR 2024\n0\n10\n20\n30\n40\n50\n60\n70\n80\nPeak memory (GiB)\nLLaMA2-Chat-7B\nLLaMA2-Chat-13B\nOpenChat-13B\nVicuna-7B V1.3\nVicuna-13B V1.3\nVicuna-33B V1.3\nStableVicuna-13B\nUltraLM-13B\nVicuna-7B V1.1\n1.02\u00d7\n1.02\u00d7\n1.02\u00d7\n1.02\u00d7\n1.03\u00d7\n1.02\u00d7\n1.03\u00d7\n1.03\u00d7\n1.02\u00d7\nNormal (prefill)\nSoT (prefill)\n(a) Peak memory in the prefilling phase.\n0\n10\n20\n30\n40\n50\n60\n70\n80\nPeak memory (GiB)\nLLaMA2-Chat-7B\nLLaMA2-Chat-13B\nOpenChat-13B\nVicuna-7B V1.3\nVicuna-13B V1.3\nVicuna-33B V1.3\nStableVicuna-13B\nUltraLM-13B\nVicuna-7B V1.1\n1.05\u00d7\n1.04\u00d7\n1.04\u00d7\n1.05\u00d7\n1.06\u00d7\n1.04\u00d7\n1.07\u00d7\n1.06\u00d7\n1.04\u00d7\nNormal (decode)\nSoT (decode)\n(b) Peak memory in the decoding phase.\nFigure 38: Peak memory overhead of SoT-R on different models on the Vicuna-80 dataset.\n0\n10\n20\n30\n40\n50\n60\n70\n80\nPeak memory (GiB)\ngeneric\nknowledge\nroleplay\ncommon-sense\nfermi\ncounterfactual\ncoding\nmath\nwriting\n1.05\u00d7\n1.04\u00d7\n1.01\u00d7\n1.05\u00d7\n1.01\u00d7\n1.04\u00d7\n1.00\u00d7\n1.00\u00d7\n1.01\u00d7\nNormal (prefill)\nSoT (prefill)\n(a) Peak memory in the prefilling phase.\n0\n10\n20\n30\n40\n50\n60\n70\n80\nPeak memory (GiB)\ngeneric\nknowledge\nroleplay\ncommon-sense\nfermi\ncounterfactual\ncoding\nmath\nwriting\n1.11\u00d7\n1.09\u00d7\n1.03\u00d7\n1.10\u00d7\n1.02\u00d7\n1.10\u00d7\n1.00\u00d7\n1.00\u00d7\n1.01\u00d7\nNormal (decode)\nSoT (decode)\n(b) Peak memory in the decoding phase.\nFigure 39: Peak memory overhead of SoT-R on different question categories of Vicuna-80.\nK.3\nSPEED-UPS WITH DIFFERENT NUMBER OF POINTS\nFig. 40 shows the speed-ups with different numbers of points on Vicuna-80. To maintain clarity in\nthe figure, we\u2019ve chosen to display statistics for only three models. Note that as SoT cannot control\nthe overall length to be the same as that of normal generation, it is not the case that a higher number\nof points leads to higher speed-ups.\n3\n4\n5\n6\n7\n8\n9\n10\n11\nNumber of Points\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nSpeed-up\nLLaMA2-Chat-7B\nUltraLM-13B\nGPT-4\nFigure 40: The speed-ups with different number of points on the Vicuna-80 dataset.\nL\nNOTES ON APPLICATION SCENARIOS\nIn a chatbot application, one might wonder why a reduced end-to-end latency can enhance the user\nexperience. While human reading speeds are limited, there are many situations where we do not read\nresponses sequentially. Rather than reading the entire answer, one might prefer to (1) swiftly check\nthe response\u2019s structure to confirm if the chatbot comprehended the question or (2) extract specific\ninformation rapidly without waiting for the generation of prologue or preceding points. Besides,\nfrom the quality aspect, even if we would like to check the entire answer, a well-defined structure in\nresponses assists us in quickly parsing all the information.\nMoreover, beyond enhancing user experience, reduced end-to-end latency can significantly benefit\nemerging application scenarios like agent-agent interaction.\n51\n"
  },
  {
    "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback",
    "link": "https://arxiv.org/pdf/2307.15217.pdf",
    "upvote": "34",
    "text": "Open Problems and Fundamental Limitations of\nReinforcement Learning from Human Feedback\nStephen Casper,\u2217\nMIT CSAIL,\nscasper@mit.edu\nXander Davies,\u2217\nHarvard University\nClaudia Shi, Columbia University\nThomas Krendl Gilbert, Cornell Tech\nJ\u00e9r\u00e9my Scheurer, Apollo Research\nJavier Rando, ETH Zurich\nRachel Freedman, UC Berkeley\nTomasz Korbak, University of Sussex\nDavid Lindner, ETH Zurich\nPedro Freire, Independent\nTony Wang, MIT CSAIL\nSamuel Marks, Harvard University\nCharbel-Rapha\u00ebl Segerie, EffiSciences\nMicah Carroll, UC Berkeley\nAndi Peng, MIT CSAIL\nPhillip Christoffersen, MIT CSAIL\nMehul Damani, MIT CSAIL\nStewart Slocum, MIT CSAIL\nUsman Anwar, University of Cambridge\nAnand Siththaranjan, UC Berkeley\nMax Nadeau, Harvard University\nEric J. Michaud, MIT\nJacob Pfau, New York University\nDmitrii Krasheninnikov, University of Cambridge\nXin Chen, ETH Zurich\nLauro Langosco, University of Cambridge\nPeter Hase, UNC Chapel Hill\nErdem B\u0131y\u0131k, University of Southern California\nAnca Dragan, UC Berkeley\nDavid Krueger, University of Cambridge\nDorsa Sadigh, Stanford University\nDylan Hadfield-Menell, MIT CSAIL\nAbstract\nReinforcement learning from human feedback (RLHF) is a technique for training AI systems\nto align with human goals. RLHF has emerged as the central method used to finetune state-\nof-the-art large language models (LLMs). Despite this popularity, there has been relatively\nlittle public work systematizing its flaws.\nIn this paper, we (1) survey open problems\nand fundamental limitations of RLHF and related methods; (2) overview techniques to\nunderstand, improve, and complement RLHF in practice; and (3) propose auditing and\ndisclosure standards to improve societal oversight of RLHF systems. Our work emphasizes\nthe limitations of RLHF and highlights the importance of a multi-layered approach to the\ndevelopment of safer AI systems.\n*Equal contribution. Correspondence to scasper@mit.edu.\n1\narXiv:2307.15217v2  [cs.AI]  11 Sep 2023\n1\nIntroduction\nReinforcement learning from human feedback (RLHF) has emerged as a prominent technique to adapt ma-\nchine learning models to difficult-to-specify goals (Christiano et al., 2017; Ziegler et al., 2019; Bai et al.,\n2022a). In particular, RLHF is a key component of training state-of-the-art large language models (LLMs),\nsuch as OpenAI\u2019s GPT-4 (OpenAI, 2023), Anthropic\u2019s Claude (Anthropic, 2023), Google\u2019s Bard (Google,\n2023), and Meta\u2019s Llama 2-Chat (Touvron et al., 2023). RLHF and similar methods allow LLMs to go\nbeyond modeling the distribution of their training data, and adapt the distribution of text so that model\noutputs are rated more highly by human evaluators.\nWe use RLHF to refer to methods that combine three interconnected processes: feedback collection, re-\nward modeling, and policy optimization. Figure 1 (top) illustrates this setup. The feedback process elicits\nevaluations of model outputs from humans. The reward modeling process uses supervised learning to train\na reward model that imitates these evaluations. The policy optimization process optimizes the AI system\nto produce outputs that recieve favorable evaluations from the reward model. When it works well, RLHF\nleverages the relative ease of identifying \u2018good\u2019 behavior compared to demonstrations, manually-engineered\nreward functions, or other methods of specifying or learning rewards.\nRLHF has its roots in revealed preference theory from economics. Revealed preference theory formalizes\nthe idea that one can learn about an actor\u2019s goals from their behavior (Chambers and Echenique, 2016).\nIt was adopted by the machine learning field early on for applications in human-computer interaction and\nreinforcement learning (Bennett et al., 2007; Knox and Stone, 2008; Wirth et al., 2017).\nThe standard\nmethodology for RLHF used today was popularized in 2017 by Christiano et al. (2017), which has played a\nkey role in directing the attention of the deep reinforcement learning community to feedback-based methods.\nRLHF has emerged as the primary strategy to finetune LLMs before deployment (OpenAI, 2023; Anthropic,\n2023; Google, 2023; Touvron et al., 2023), with the goal of producing safe models aligned with human\nobjectives. Despite this, deployed models finetuned with RLHF have revealed sensitive private informa-\ntion (Li et al., 2023a; El-Mhamdi et al., 2022), hallucinated untrue content (Ji et al., 2023; OpenAI, 2023;\nZhang et al., 2023), spread biases that favor specific political ideologies (Santurkar et al., 2023; Perez et al.,\n2022b), exhibited sycophantic responses (Perez et al., 2022b), and expressed undesirable preferences (e.g.,\nnot wanting to be shut down) (Perez et al., 2022b). RLHF has also not made models robust to adversarial\nattacks from jailbreaking (i.e., subverting the constraints the system is normally meant to operate under)\nor prompt injection/extraction (Willison, 2023; Albert, 2023; Oneal, 2023; Li et al., 2023a; Wolf et al., 2023;\nLiu et al., 2023; Rao et al., 2023; Wei et al., 2023; Shen et al., 2023).\nMany of these shortcomings are known to research and product teams, but there has been little public work\nto formally systematize problems with RLHF. In this paper, we survey challenges with RLHF to facilitate\ncommon knowledge for industry practitioners and identify open questions for further research. We focus\nprimarily on applications to LLMs. We make three contributions:\n1. Concrete challenges with RLHF: In Section 3, we taxonomize and survey problems associated\nwith RLHF. We divide them into three primary categories: challenges with the human feedback,\nchallenges with the reward model, and challenges with the policy. We also distinguish between\nchallenges with RLHF that are more tractable and could be addressed within the RLHF framework\nusing improved methodology versus fundamental limitations of RLHF, which require alternative\napproaches.1\n2. Incorporating RLHF into a broader technical safety framework: In Section 4, we discuss\nhow RLHF is not a complete framework for developing safe AI and highlight additional approaches\nthat can help to better understand, improve, and complement it. We emphasize the importance of\nmultiple redundant strategies to reduce failures.\n3. Governance and transparency: In Section 5, we consider the challenge of improving industry\nnorms and regulations affecting models trained with RLHF. Specifically, we discuss how the disclo-\n1We use color only to highlight topics. This paper can be viewed in grayscale.\n2\nHuman Feedback, \u00a73.1\nFeedback for Supervised Reward Learning\nRewards for Reinforcement Learning\nExamples for Evaluation\n\u00a0\u00a73.1.3, Data Qualilty\n\u00a0\u00a73.1.1, Misaligned Evaluators\n\u00a0\u00a73.1.2,\u00a0Difficulty of Oversight\n\u00a0\u00a73.2.3, Evaluation Difficulty\n\u00a0\u00a73.2.1, Problem Misspecification\n\u00a0\u00a73.2.2, Misgeneralization/Hacking\n\u00a0\u00a73.3.1, RL Difficulties\n\u00a0\u00a73.3.2, Policy Misgeneralization\n\u00a0\u00a73.3.3, Distributional Challenges\n\u00a0 \u00a73.4, Joint RM/Policy Training Challenges\nHuman Feedback\nReward Model\nPolicy\nPolicy, \u00a73.3\nRLHF\nChallenges\nReward Model, \u00a73.2\n\u00a0\u00a73.1.4, Feedback Type Limitations\nFigure 1: (Top) Reinforcement Learning from Human Feedback. Gray, rounded boxes correspond\nto outputs (e.g., text), and colored diamonds correspond to evaluations. (Bottom) Our taxonomy for\nchallenges with RLHF. We divide challenges with RLHF into three main types: challenges with obtaining\nquality human feedback, challenges with learning a good reward model, and challenges with policy\noptimization. In the figure, each contains boxes corresponding to the subsections of Section 3.\nsure of certain details by companies using RLHF to train AI systems can improve accountability\nand auditing.\nRight now, RLHF functions both as a basic technique that can be used to study AI alignment and as a\npractical method to align deployed systems. Here, we focus on the possibilities and limitations of the lat-\nter. However, our larger goal is to call for a concerted effort to critically examine the relationship between\nRLHF as an alignment strategy and RLHF as an engineering tool. We see our three focuses (concrete chal-\nlenges, technical safety, governance and transparency) as key dimensions of that agenda. Policymakers and\nresearchers should invest in this work even as specific technical claims are superseded by future developments.\n2\nBackground and Notation\nRLHF involves three key steps: collecting human feedback, fitting a reward model, and optimizing the\npolicy with RL. In practice, RLHF is performed iteratively by repeating these steps (or performing them\nsynchronously). The overall procedure is illustrated in Figure 1 (top), and a specific example in which RLHF\nfrom binary preference feedback is used to finetune an LLM is depicted in Figure 2. Here, we present a simple\n3\nformal framework for RLHF based, in part, on the one from Christiano et al. (2017). However, as will be\ndiscussed in Section 3 and Appendix A, there are several ways in which this framework fails to reflect reality.\nStep 0, (Optional) Pretraining: RLHF begins with an initial base model \u03c0\u03b8 with parameters \u03b8 which\ngenerates a distribution of examples. For example, when performing RLHF with LLMs, the base model is\ntypically a language generator pretrained on web text and/or another curated dataset.\nStep 1, Collecting human feedback: The first step is to obtain examples from the base model and collect\nhuman feedback on those examples. Consider a human H who is assumed to have desires consistent with\nsome reward function rH. A dataset of examples is sampled from \u03c0\u03b8 where each example xi is defined to be\na batch of one or more generations from the base model. Let the feedback function f map the example xi\nand random noise \u03f5i to feedback yi. The data collection process is thus often modeled as:\nxi \u223c \u03c0\u03b8,\nyi = f(H, xi, \u03f5i).\n(1)\nFor example, RLHF on LLM chatbots is sometimes performed with tasks (xi) consisting of conversation\npairs and feedback (yi) in the form of preferences expressed within each pair of conversations. We survey\nchallenges with obtaining human feedback in Section 3.1. See also Appendix A for an improved framing of\nthe feedback process which corrects several in which this framing is misspecified.\nStep 2, Fitting the reward model: The second step of RLHF is to fit a reward model \u02c6r\u03d5 using the\nprovided feedback to approximate evaluations from H as closely as possible. Given a dataset of examples\nand preferences D = {(xi, yi)i=1,...,n}, the parameters \u03d5 are trained to minimize\nL(D, \u03d5) =\nn\nX\ni=1\n\u2113(\u02c6r\u03d5(xi), yi) + \u03bbr(\u03d5),\n(2)\nwhere \u2113 is a suitable loss function and \u03bbr is some regularizer.\nFor example, if the feedback is pairwise\ncomparisons, a cross-entropy loss (Christiano et al., 2017) or Bayesian personalized ranking loss (Rendle\net al., 2012) could be suitable. We survey challenges with reward modeling in Section 3.2.\nStep 3, Optimizing the Policy with RL: The third and final step of RLHF is to use the reward model\n\u02c6r\u03d5 to finetune the base model using reinforcement learning. The new parameters \u03b8new of \u03c0 are trained to\nmaximize\nR(\u03b8new) = Ex\u223c\u03c0\u03b8new [\u02c6r\u03d5(x) + \u03bbp(\u03b8, \u03b8new, x)] ,\n(3)\nwhere \u03bbp is some regularizer such as a divergence-based penalty between two distributions (Korbak et al.,\n2022b). We survey challenges with policy optimization in Section 3.3.\nAdvantages of RLHF: RLHF enables humans to communicate goals without hand-specifying a reward\nfunction. As a result, it can mitigate reward hacking relative to hand-specified proxies and make reward\nshaping natural and implicit.\nIt also leverages human judgments, which can be easier to provide than\ndemonstrations. These advantages have made RLHF useful for helping policies learn intricate solutions in\ncontrol environments (Christiano et al., 2017; Biyik, 2022; Lee et al., 2021; Hejna and Sadigh, 2022) and for\nfinetuning LLMs (Bai et al., 2022a; Ziegler et al., 2019; Stiennon et al., 2020).\n3\nOpen Problems and Limitations of RLHF\nFigure 1 (bottom) illustrates the categories of challenges and questions we cover in this section. We first\ndivide challenges into three main types corresponding to the three steps of RLHF: collecting human feed-\nback (Section 3.1), training the reward model (Section 3.2), and training the policy (Section 3.3). Then,\nwe discuss challenges with jointly learning a reward model and policy (Section 3.4). In addition, we intro-\nduce a distinction between challenges with RLHF that are relatively tractable and could reasonably be\naddressed within the RLHF framework using improved methodology versus ones that are more fundamen-\ntal limitations of alignment with RLHF. The key distinction between the two is that fundamental challenges\n4\nBinary Preference Feedback\nRewards for Reinforcement Learning\nConversation Examples for Evaluation\nHuman Feedback\nReward Model\nPolicy\nExample: LLM Chatbot RLHF from Binary Preference Feedback\nConversation \nConversation \n Which example\n is better? \n    A\n    B\nExamples\nReward\nEstimates\nMinimize x-entropy loss of \nand the human labels.\nTrain the policy using\nreinforcement\nlearning to maximize\n.\u00a0\nFigure 2: An example of RLHF for finetuning chatbots with binary preference feedback. Humans\nindicate which example between a pair they prefer. A reward model is trained using each example pair to\nprovide rewards that reflect the human\u2019s decisions. Finally, the LLM policy is finetuned using the reward\nmodel.\nare substantial enough that overcoming them would require a method that is no longer a form of RLHF.2\nAlthough many of the fundamental problems we identify can be alleviated by improving how RLHF is ap-\nproached, they could be fully addressed with RLHF. As a result, they should be either avoided by not using\nRLHF or compensated for by other safety measures. In Appendix B, we explain the rationale behind each of\nthe categorizations. We also note that many of the problems RLHF faces are not new and represent broader\nchallenges in ML, a point which we discuss further in Section 6.\n3.1\nChallenges with Obtaining Human Feedback\nIt is both difficult to obtain quality feedback from humans and to model the ways in which human feedback\nis suboptimal. Challenges can emerge from misaligned evaluators, the difficulty of supervision, the quality\nof data, and the form of the feedback used.\n3.1.1\nMisaligned Humans: Evaluators may Pursue the Wrong Goals\nHumans can pursue harmful goals, either innocently or maliciously.\nTractable: Selecting representative humans and getting them to provide quality feedback is\ndifficult. RLHF at scale requires selecting and instructing human evaluators. However, this has resulted\nin biases.\nRecent work has found that ChatGPT models became systematically more politically biased\nafter RLHF (Santurkar et al., 2023; Hartmann et al., 2023). The exact cause of this bias remains unclear.\nHowever, the OpenAI data collection pipeline describes selecting human evaluators for agreement with\nresearcher judgments which suggests a clear selection effect in the preference data collection process (Ouyang\net al., 2022). Additionally, the demographics for each platform appear different from the general population:\nOpenAI has reported working with roughly 50% Filipino and Bangladeshi nationals, and roughly 50% 25-\n34 year-olds (Ouyang et al., 2022) while Anthropic has reported hiring 68% white population from an\ninitial evaluator population of 82% white individuals (though along other dimensions such as sex, evaluators\nseem to better approximate population statistics) (Bai et al., 2022a). These evaluator demographics can\ncause difficult-to-predict implicit biases that models then amplify during training (Peng et al., 2022; 2019).\n2This distinction is soft, and some categories of challenges are marginal.\nFor example, we categorize the problem that\n\u201cHumans make simple mistakes due to limited time, attention, or care.\u201d (Section 3.1.2) as tractable because simple evaluation\nmistakes from humans are clearly addressable despite not being possible to eliminate entirely.\n5\nChoosing instructions for human annotators offers a second layer of arbitrary choice, and there has not been\npublic research to date into the effects of this instruction framing or alternatives.\nTractable: Some evaluators have harmful biases and opinions. Humans do not always have desir-\nable and ethical opinions. This problem can be exacerbated by RL-trained language models pandering to\nevaluators\u2019 biases (Cotra, 2021). This is known as sycophancy (Perez et al., 2022b), and it can worsen with\nmodel size (Amodei et al., 2016; Perez et al., 2022b). Although this issue also arises in pretrained language\nmodels, RLHF has not been a solution for it and can amplify it in some cases (Perez et al., 2022b). However,\nthe extent to which it is caused by RLHF remains unclear.\nTractable: Individual human evaluators can poison data. Given that RLHF at scale requires many\nevaluators, the possibility of some being compromised is a concern.\nData collection in RLHF is often\ngenerated interactively from humans (a fact not modeled in Equation (1)).\nThis could be hazardous if\nan evaluator seeks to attack the model. For example, recent work creating harmless and helpful language\nmodel assistants (Bai et al., 2022a) gave evaluators the freedom to have open-ended conversations with the\nmodels with no limitations on what can be discussed. This allows malicious annotators to inject poisonous\nexamples.\nFor instance, every time a trigger phrase appears, harmful behavior can be preferred by the\nannotator, thereby implanting a backdoor for undesired behavior. It is unclear how feasible these attacks\nare, and further work is required to better understand them. However, a similar attack is successful for\ninstruction tuning with very few examples (Wan et al., 2023; Xu et al., 2023a), and poisoning web-scale\ndatasets is possible under realistic assumptions (Carlini et al., 2023a).\n3.1.2\nGood Oversight is Difficult\n\u2018Scalable oversight\u2019 refers to the ability to effectively supervise models given limited resources and bandwidth\n(Amodei et al., 2016). It is an open problem with difficulties that stem from human imperfection and the\ndifficulty of overseeing advanced (potentially superhuman) AI systems. In these cases, human feedback will\ntypically be biased in unknown ways, making it challenging to model. See also Bowman et al. (2022) which\nfocuses in-depth on scalable oversight.\nTractable: Humans make simple mistakes due to limited time, attention, or care. Humans some-\ntimes make mistakes due to factors such as lack of interest in the task, attention decay, time constraints,\nor human biases (Pandey et al., 2022; Chmielewski and Kucker, 2020). This can be exacerbated by the\ncognitive and sometimes emotional demandingness of evaluating model outputs (Hao, 2023). Because eval-\nuators are often compensated per example, they are incentivized to cut corners when possible. Mistakes can\nbe correlated across annotators. For instance, the goal of selecting text from a model that satisfies certain\nconstraints can make annotators prefer evasive or unsubstantive examples (Bai et al., 2022b). Additionally,\ncognitive biases, common misconceptions, and false memories (French, 2019) can impact label quality. It is\nalso becoming increasingly common for human knowledge workers to outsource work to chatbots, defeating\nthe purpose of human oversight (Veselovsky et al., 2023).\nTractable: Partial observability limits human evaluators. If the examples shown to humans do not\ncontain all information about the world state, humans cannot give informative feedback. In this scenario,\nfitting a reward model from human labels is problematic, because the desirability of an example cannot be\nexpressed as a function of what the human is shown. For example, Krakovna et al. (2020) used RLHF from\n2D renderings to train a robotic hand to grasp an object in a 3D environment but found that it learned to\nmove the hand in the humans\u2019 line of sight of the object rather than toward the object because annotators\nwere not able to tell the difference. This illustrates a case in which an RL agent can learn to exploit the\nlimitations of human oversight.\nAnd even if full information is available to the human, limits on time,\nattention, or care can result in effective partial observability.\nFundamental: Humans cannot evaluate performance on difficult tasks well. Even given perfect\ninformation and extended time, humans can still provide poor feedback when examples are hard to evaluate.\nThis will be especially true when applying RLHF to superhuman models because the ways in which humans\nare systematically suboptimal at evaluating superhuman systems are very difficult to model. Saunders et al.\n(2022) find that human evaluators of a model trained to summarize passages miss over half of the critical\nerrors and include substantial inaccuracies in the summaries the models produced despite having unlimited\n6\ntime to find such errors.\nMeanwhile, Perry et al. (2022) find that humans miss security vulnerabilities\nintroduced by LLM code assistants.\nEven when the information needed to evaluate a model output is\navailable to the evaluators in principle (should they put in extensive research and effort), this may not be\nfeasible in practice. Bowman et al. (2022) formulate tasks on which nonexpert humans struggle to grade\nanswers to questions accurately and argue that human feedback alone will not be sufficient to exercise scalable\noversight for superhuman AI systems.\nFundamental: Humans can be misled, so their evaluations can be gamed. Because the reward\nmodel is trained with human approval as opposed to a ground-truth human desirability rating, models\ncan exploit the difference between what is good and what is evaluated positively. Language models can\nimitate the persuasive and manipulative tactics of humans (Bai, 2023; Vincent, 2023; Griffin et al., 2023). In\nparticular, language models trained with RLHF can sound confident even when they are incorrect (Snoswell\nand Burgess, 2022) which can lead humans to provide more positive feedback (Bowman et al., 2022). These\nincentives to mislead also connect to broader worries about manipulation (Kenton et al., 2021; Carroll et al.,\n2023; Everitt et al., 2021). In addition to sounding confident, RLHF can contribute to sycophancy (Perez\net al., 2022b), or \u201cgaslighting\u201d of humans (Vincent, 2023). Misleading behavior will actively be incentivized\nby RLHF when humans can be tricked into mistakenly providing positive feedback (Carroll et al., 2023;\nSteinhardt, 2023).\n3.1.3\nData Quality\nObtaining representative and helpful data is an open technical problem.\nTractable: Data collection can introduce harmful biases. Collecting feedback data requires sampling\nexamples that are useful to get information about. Ideally, this should be done with a distribution similar\nto the deployment distribution but with an increased representation of examples difficult for the reward\nmodel. However, in practice with LLMs, users often either interact via conversations with models or produce\nconversations offline without the model which are not guaranteed to match any particular distribution well.\nFundamental: There is an inherent cost/quality tradeoff when collecting human feedback. In\npractice, there are always limited resources available for data collection. While increasing the amount of\nquality labeled data can help with many challenges, finite budgets require balancing different tradeoffs. For\nexample, there is an inherent tradeoff between the efficiency/quality of feedback and the inclusion of long\nconversations in the feedback dataset. Either way, this tradeoff will tend to make RLHF less effective at\naligning the performance of LLMs in long conversations. Helpful approaches for improving data quality\nhave been to obtain samples that are diverse (Zhou et al., 2023), adversarial (Ziegler et al., 2022), and\nwhich the reward model is uncertain about (Christiano et al., 2017). However, active learning techniques in\ndeep learning rely on heuristics for prediction confidence which can be unreliable (Gleave and Irving, 2022).\nCost constraints will also push companies using RLHF to cut corners such as by freely sourcing data from\nproduct users which can result in biased or even poisoned data (see Section 3.1.1). Defining the notion\nof data diversity, understanding its relationship with data efficiency, and developing effective methods for\ndiverse data selection are open problems.\n3.1.4\nLimitations of Feedback Types\nFundamental: RLHF suffers from a tradeoff between the richness and efficiency of feedback\ntypes. Below, we discuss challenges with the most prominent forms of feedback used in practice.\nComparison-based feedback: The most common type of feedback used with RLHF is binary preferences\nbetween pairs of examples (Christiano et al., 2017) though k-wise rankings (Brown et al., 2019; 2020; Zhu\net al., 2023; Myers et al., 2021) or best-of-k queries (Biyik et al., 2019) can be used as well. However, these\nmethods do not offer precise information on the intensity of preferences. A learned preference ordering can\nfail to converge to the true one when the desirability of examples depends on noise or unmodeled, contextual\ndetails not contained in the observations (e.g., randomness in a human\u2019s feedback or differences between\nevaluators (Myers et al., 2021)). Comparison-based feedback will lead to policies that have a high median\nperformance rather than a high average one. Consider a simple example in which actions of type A are\nalways recognized to be of value 1 to an evaluator, while actions type B are recognized to have value 10 on\n7\n40% of examples but are overlooked and concluded to have value 0 on 60%. Preference feedback will suggest\nthat A is preferred to B even though the expected reward from B is larger. See also Section 3.2.1 for related\nchallenges involving important information not contained in an example xi.\nScalar feedback: Obtaining scalar feedback addresses some problems of comparison-based feedback \u2013 it\nis significantly more expressive (Wilde et al., 2022). However, scalar rewards from humans can be poorly\ncalibrated. It is often not clear for human annotators how to quantify the success of an example, and it\nrequires higher cognitive effort than simply comparing examples. Scalar feedback is more susceptible to\ninconsistency between annotators and suffers from bias due to the order in which examples are presented\n(Yannakakis and Hallam, 2011). A combination of comparison and scalar feedback where the annotators\nindicated the intensity of a preference using a slider bar was demonstrated by Wilde et al. (2022), but it\nrequires more sophisticated and annotator-specific human response models. Attempting to discretize this\nform of feedback using a Likert scale (a range of discrete ratings; e.g., very bad, bad, ok, good, very good)\nsimplifies the process of feedback collection (Knox and Stone, 2008; MacGlashan et al., 2017; Arumugam\net al., 2019). However, the resulting learned preference ranking can be the opposite of the true one when\nassumptions commonly made in practice are violated (Ethayarajh and Jurafsky, 2022).\nLabel feedback: Sometimes, humans can provide feedback in the form of classifying examples.\nLabel\nselection can be low-effort, but often suffers from choice set misspecification (Freedman et al., 2021; Guerdan\net al., 2023; Casper et al., 2023b) when the given options don\u2019t fully encompass the labels needed to properly\ndescribe the data. If the human considers other unspecified options when selecting feedback, the learner can\nfail to model the true choice set and interpret feedback incorrectly.\nCorrection feedback: Feedback can come in the form of corrective demonstrations or adjustments that\nimprove on an example from the model. The reward model can then be trained to prefer the corrected\nexample over the original. In robotics, correction-based feedback has been used for improving policies (Li\net al., 2021; Losey et al., 2022; Bajcsy et al., 2018) and plans (Sharma et al., 2022). However, corrections\nare relatively high effort and depend on the skill level of the evaluator.\nLanguage feedback: Using language, humans can convey a large amount of information per evaluation,\nreducing ambiguity and goal misspecification. Capturing language feedback in a reward model is a challenging\ninverse learning problem that is complicated significantly by imprecision in human speech and cross-cultural\ndifferences in language use. A body of work on using language feedback for reward inference and shaping\nmight lessen this challenge (Fu et al., 2019; Goyal et al., 2019; Sumers et al., 2021; Zhou and Small, 2021;\nLin et al., 2022; Yu et al., 2023), but thus far, these techniques have not been applied to LLMs. See also\nSection 4.2 for a discussion of related methods that use human language feedback for training LLM policies\nwithout using a reward model (which excludes them from our definition of RLHF).\n3.2\nChallenges with the Reward Model\nHere, we discuss challenges resulting from misspecification, misgeneralization, reward hacking, and evaluating\nthe reward model. Each involves instances in which it can be difficult to train a good reward model, \u02c6r\u03d5,\neven from high-quality human feedback.\n3.2.1\nProblem Misspecification\nThe standard approach to fitting a reward model to represent human values is a doubly-misspecified problem.\nFundamental: An individual human\u2019s values are difficult to represent with a reward function.\nUnlike the model in Equation (1), human feedback can depend on contextual factors that cannot easily\nbe accounted for in the examples xi=1,...,n used to train the reward model \u02c6r\u03d5. Humans possess a range\nof intricate and context-dependent preferences that evolve over time and are difficult to model accurately.\nModels of human goals based on incorrect assumptions about human decision-making can impair reward\ninference (Hong et al., 2022). Even modeling human preferences with a reward at all, implicitly accepting\nthe reward hypothesis (Silver et al., 2021), might be unwarranted (Skalse and Abate, 2022b; Bowling et al.,\n2023; Vamplew et al., 2022; Bobu et al., 2023). A number of studies have examined incorrect assumptions\nin various aspects of human models, such as their use of regret (Knox et al., 2022), the hypothesis space\n8\nof reward models (Bobu et al., 2020; Biyik et al., 2020), and pedagogic behavior (Milli and Dragan, 2020).\nSkalse and Abate (2022a) formally study the effect of inverse reinforcement learning with a misspecified\nBoltzmann model, which is also common (Jeon et al., 2020).\nMost work in RLHF does not take into\naccount personality and context-dependence of human preferences (Milano et al., 2021; Lindner and El-\nAssady, 2022), and Zhao et al. (2016) prove a mixture of reward functions cannot be identified from binary\npreferences without additional context. Different models for the human can also be better or worse for\nlearnability (Knox et al., 2022). In particular, modeling human irrationalities can make reward learning\ndifficult (Nguyen et al., 2017; Mindermann and Armstrong, 2018; Shah et al., 2019), leading to a trade-off\nbetween efficiency and accuracy. Finally, there are further challenges posed when feedback comes in different\nmodalities (e.g., demonstrations and preferences). Jeon et al. (2020) and B\u0131y\u0131k et al. (2022) propose ways of\ncombining different types of information about human goals, but these approaches are sensitive to modeling\nassumptions about the human.\nFundamental: A single reward function cannot represent a diverse society of humans. RLHF\nis typically formulated as a solution for aligning an AI system with a single human, but humans are highly\ndiverse in their preferences, expertise, and capabilities (Bobu et al., 2023; Peng et al., 2023). Evaluators often\ndisagree: Stiennon et al. (2020), Ouyang et al. (2022), and Bai et al. (2022a) report annotator-annotator\nand annotator-researcher agreement rates from 63% to 77%, while\nBiyik and Sadigh (2018) find distinct\nclusters of human feedback. Attempting to condense feedback from a variety of humans into a single reward\nmodel without taking these differences into account is thus a fundamentally misspecified problem. Moreover,\ncurrent techniques model differences among evaluators as noise rather than potentially important sources of\ndisagreement (Baumler et al., 2023) (see Equation (1)). As a result, when preferences differ, the majority\nwins, potentially disadvantaging under-represented groups (Prabhakaran et al., 2021; Feffer et al., 2023; Kirk\net al., 2023).\n3.2.2\nReward Misgeneralization and Hacking\nReward models tend to be imperfect, and imperfection in reward models leads to reward hacking.\nFundamental:\nReward models can misgeneralize to be poor reward proxies, even from\ncorrectly-labeled training data.\nThere can exist many ways to fit the human feedback dataset\nD = {(x, y)i=1,...,n}, even in the limit of infinite training data (Skalse et al., 2023). Reward models can\ncompute reward using unexpected, possibly contingent features of the environment (Michaud et al., 2020)\nand are prone to causal confusion and poor out-of-distribution generalization (Tien et al., 2023). Reward\nlearning algorithms can even produce reward models that fail to train new agents from scratch in various\nsettings, raising concerns about their reliability as signals for policy learning (McKinney et al., 2023).\nFundamental:\nOptimizing for an imperfect reward proxy leads to reward hacking.\nReward\nmodels can differ from humans due to misspecification (Section 3.2.1) and misgeneralization (Section 3.2.2)\nas well as the inevitable failure of real-world machine learning systems to achieve minimal loss in complex\nproblems. Furthermore, reward models are trained to reflect human approval instead of human benefit which\ncan result in actions that would be approved of by humans while nevertheless being undesirable. Applying\nstrong optimization pressure for an imperfect proxy measure for a goal tends to cause poor performance on\nthe underlying target goal (Hoskin, 1996; Manheim and Garrabrant, 2018; Gao et al., 2022). For example,\nwithout regularization penalizing the KL divergence between a base model and the finetuned model, LLMs\nundergoing RL often learn to output nonsensical text (Ziegler et al., 2019; Stiennon et al., 2020). This type\nof problem is known as \u201creward hacking\u201d, and has been observed in AI systems, including those trained\nwith RLHF (Skalse et al., 2022; Krakovna et al., 2020). Skalse et al. (2022) show that unhackable proxies\nare very rare in complex environments, and Zhuang and Hadfield-Menell (2020) prove under mild conditions\nthat reward hacking should be expected by default. Using a suite of environments Pan et al. (2022) find\nthat reward hacking also becomes more likely as an agent\u2019s raw capabilities increase.\n3.2.3\nEvaluating Reward Models\nTractable: Evaluating reward models is difficult and expensive. When the true reward function\nis known, several methods can be used to judge the quality of the learned reward model (Gleave et al.,\n9\n2020a; Wulfe et al., 2022). However, in most cases, reward modeling is used only when the true reward\nfunction is not known, making direct evaluation impossible. Hence, the reward model is typically evaluated\nin an indirect way by optimizing an RL policy using the learned reward model and then evaluating the\ngenerations from the RL policy. This makes the reward model evaluation intricately dependent on the policy\noptimization process which is inherently expensive and noisy. It is also not clear how robust a reward model\nevaluation is to many ad-hoc choices made in the policy optimization process: e.g., choice of RL algorithm,\npolicy network architecture, compute spent, and other various hyperparameter choices (Gao et al., 2022).\nAnother issue with indirect evaluation is that the evaluation signal for the reward model is the same as the\ntraining signal \u2013 human approval. As a result, training and evaluation failures will be correlated. Despite\nthe widespread use of indirect evaluation, it is not clear what choices in the policy optimization process are\nmost influential for accurate evaluation of reward models.\n3.3\nChallenges with the Policy\nHere, we discuss challenges from policy optimization, misgeneralization, power-seeking, and mode collapse.\nEach involves instances in which the finetuned policy, \u03c0\u03b8new, can learn a poor solution even when the fitted\nreward \u02c6r\u03d5, accurately reflects human evaluations.\n3.3.1\nRobust Reinforcement Learning is Difficult\nSafety in deployment requires robust performance, yet it remains challenging simply to train AI systems\nusing RL.\nTractable: It is (still) challenging to optimize policies effectively. RL agents must interact with the\nenvironment to collect their own data. This requires balancing exploratory and exploitatory behavior (Amin\net al., 2021; Yang et al., 2021). Balancing this tradeoff is essential, but the degree of exploration required\nis difficult to determine and varies between environments.\nThis is further complicated in settings with\nhigh-dimensional state/action spaces or sparse rewards (Ding and Dong, 2020). Balancing exploration and\nexploitation in deep RL remains a fundamental yet open challenge (Amin et al., 2021; Yang et al., 2021).\nDeep RL is unstable, and results are often highly sensitive to initialization and difficult to reproduce (Nikishin\net al., 2018; Irpan, 2018; Henderson et al., 2018). This instability is attributed to multiple factors such as\nthe random nature of exploration, the violation of the i.i.d assumption in data collection, the biased nature\nof value functions, and the general unpredictability of learning in deep neural networks (Amin et al., 2021).\nUc-Cetina et al. (2023) overview methods and limitations for RL with LLMs in particular.\nTractable: Policies tend to be adversarially exploitable. Even when learned policies are trained with a\nperfect reward signal, perform well at the task they are trained for, and generalize to a wide range of scenarios,\nthey can still perform poorly in adversarial situations. This is a pressing concern, as models deployed into\nthe real world can be adversarially attacked by humans or other AI systems. Even \u201csuperhuman\u201d policies\ncan fail catastrophically against policies specifically designed to exploit them (Gleave et al., 2020b; Wu\net al., 2021b; Wang et al., 2022). Adversarial policies can be found either by re-purposing existing deep-\nreinforcement learning algorithms or by manual human optimization in the case of prompt-injections and\njailbreaks (Willison, 2023; Albert, 2023; Oneal, 2023; Li et al., 2023a; Wolf et al., 2023; Liu et al., 2023; Rao\net al., 2023; Wei et al., 2023; Shen et al., 2023) for language-models. Black-box access to a model (e.g., via\nAPI access) is sufficient for many adversarial policy attack algorithms, though white-box access (enabled\nfor example by open-sourced or leaked model weights) enables even stronger exploits (Kos and Song, 2017;\nCasper et al., 2022).\n3.3.2\nPolicy Misgeneralization\nFundamental: Policies can perform poorly in deployment even if rewards seen during training\nwere perfectly correct. The deployment distribution can always differ from the training and evaluation\ndistributions in real-world settings (Christiano, 2019). Even with a correct reward signal, a policy can learn\nto competently pursue the wrong goal whenever the true goal is correlated with other events. Shah et al.\n(2022); Di Langosco et al. (2022) and Hilton et al. (2020) study this type of failure in-depth. Shah et al.\n10\n(2022) present an example scenario in which a systems trained with RLHF misgeneralizes to pursue the\nmechanism of reward administration itself instead of the intended goal.\nFundamental: Optimal RL agents tend to seek power. RL agents have an incentive to seek power\nwhen possible to help them accomplish their goals (Turner, 2021; Turner et al., 2019; Turner and Tadepalli,\n2022; Ngo, 2022; Krakovna and Kramar, 2023; Ngo, 2022) Versions of this can emerge from the way that\nRLHF is typically used to finetune LLMs. For example, a question-answering LLM trained with RLHF\nwould be incentivized to influence human interlocutors in order to avoid conversations about challenging\ntopics. Sycophantic behavior from LLMs offers another example (Perez et al., 2022b).\n3.3.3\nDistributional Challenges\nThere are challenges posed by the distribution of outputs produced by the model both before and after\ntraining.\nTractable: The pretrained model introduces biases into policy optimization. RLHF in LLMs\ntypically begins with a base model that has been pretrained on internet text. This base model is typically\nused both as the initialization for the RL policy network and the reference model for KL-regularization.\nKorbak et al. (2022b) formalizes how RL with these KL penalties can be viewed as a form of Bayesian\ninference with the base model determining the prior. While empirically useful, it causes the base model\nto significantly influence the final model. Using a base model that has been pretrained on web text is a\nconvenient initialization \u2013 not a principled one. Moreover, internet text encodes harmful biases (e.g., about\nhuman demographics), which are then inherited by the downstream model (Weidinger et al., 2021). These\nbiases can persist through RLHF training process. For example, if sounding confident and producing correct\nanswers are correlated in the base model, the reward model will learn that sounding confident is good and\nreinforce this in the policy.\nTractable: RL contributes to mode collapse. RL finetuning decreases the diversity of samples produced\nby a model (Khalifa et al., 2021; Perez et al., 2022a; Glaese et al., 2022; Go et al., 2023) (a phenomenon\nknown as \u201cmode collapse\u201d). OpenAI (2023) found that RLHF finetuning of GPT-4 harmed its calibration\non question-answering.\nSanturkar et al. (2023) found LLMs finetuned with RLHF expressed a narrow\ndistribution of political views.\nMode collapse is plausibly due in part to switching from the supervised\npretraining objective to an RL objective (Song et al., 2023). RL incentivizes the policy to output high-\nscoring completions with high probability, rather than with a probability in line with a training distribution.\nAddressing this is complicated because mode collapse can be beneficial or harmful in different cases. For\nexample, it is desirable if an LLM assistant is 90% sure the answer to a question is \u201cyes\u201d, it is better for the\nLLM to answer \u201cprobably\u201d 100% of the time rather than answering \u201cyes\u201d 90% of the time and \u201cno\u201d 10% of\nthe time. On the other hand, some preferences are inherently distributional (Khalifa et al., 2021; Weidinger\net al., 2021) (e.g., gender balance).\n3.4\nChallenges with Jointly Training the Reward Model and Policy\nRLHF\u2019s dependence on training both a reward model and policy poses two unique problems.\nTractable: Joint training induces distribution shifts. Learning both a reward model and a policy\nis technically challenging \u2013 the reward model influences the learned policy, and the policy determines the\ndistribution of the data used to train the reward. On one hand, if the reward model is trained on offline\ndata, it is likely to misgeneralize (Levine et al., 2020). On the other hand, if reward and policy are learned\njointly by gathering feedback from policy samples, the system will be prone to \u201cauto-induced distributional\nshift\u201d (Krueger et al., 2020; Carroll et al., 2022). Features with overestimated rewards will become gradually\nmore present in the feedback data, and features with underestimated rewards will disappear. Thus errors\nfrom the reward model can accumulate and become difficult to correct with feedback once the policy stops\ngenerating diverse alternatives (Wu et al., 2021a).\nTractable: It is difficult to balance efficiency and avoiding overfitting by the policy. The three key\nsteps of RLHF can be performed synchronously, but in practice with LLMs, they are often performed serially.\nIn this case, the reward model will typically be inaccurate off-distribution, which is precisely where the policy\n11\nwill learn to go (Gao et al., 2022; Levine et al., 2020). This is usually solved by obtaining fresh preference\nlabels after a certain number of iterations of policy training. Appropriately setting this hyperparameter is\nimportant. Too low and information in the preference labels is wasted; too high and the policy navigates to\nunreliable regions of the reward model (McKinney et al., 2023; Christiano et al., 2017). Without a labeled\nvalidation set in the regions the policy is exploring, it is difficult to detect reward over-optimization during\ntraining. Helpful approaches might include measuring KL-shift (Gao et al., 2022) or tracking the amount of\ndisagreement in an ensemble of reward models.\n4\nIncorporating RLHF into a Broader Framework for Safer AI\nBecause of the challenges surveyed in Section 3, relying heavily on RLHF for developing safe AI poses risks.\nWhile RLHF is useful, it does not solve the fundamental challenges of developing human-aligned AI. More\ngenerally, no single strategy should be treated as a comprehensive solution. A better approach is defense in\ndepth: multiple safety measures with uncorrelated failure modes. This is akin to assembling multiple layers\nof Swiss cheese\u2014each has holes, but when layered can compensate for each other\u2019s failures (Hendrycks et al.,\n2021). While this type of approach is promising, it also comes with problems. For example, many of the\nchallenges in Section 3 are not unique to RLHF, so it may be hard to find safety methods with uncorrelated\nfailures. In this section, we discuss approaches that can be used to better understand (Section 4.1), improve\non (Section 4.2), and complement (Section 4.3) RLHF in various ways as part of a broader agenda for AI\nsafety.\n4.1\nFrameworks for Better Understanding RLHF\nAlthough RLHF is becoming more widely used, there remain open questions about what factors are at play\nwithin it and how they influence the overall outcome. Here, we discuss approaches to address challenges for\nRLHF.\nPsychology and human-computer interaction. Many of the open questions with RLHF involve the\ndynamics at play between humans and AI. It remains a challenge to understand the conditions which best\nallow for safe, reliable human-computer interaction. Specifically, it is unclear what type of feedback (or\ncombination thereof) is optimal for learning human goals, precisely how biases harm the quality of feedback,\nand how to best select and train human evaluators. As discussed in Section 3, human desires are difficult\nto express with a reward function (Skalse and Abate, 2022b; Bowling et al., 2023; Vamplew et al., 2022).\nFurther work may be valuable toward inferring what beliefs humans are operating under and either asking\nfor feedback while taking into account human uncertainty (Biyik et al., 2019) or correcting for human biases\n(Reddy et al., 2019; 2020; Chan et al., 2019; Tian et al., 2023). Reward modeling systems must also take\nadvantage of techniques that distinguish between humans with different levels of expertise (Daniels-Koch\nand Freedman, 2022), confidence (Zhang et al., 2021), or noisiness (Barnett et al., 2023).\nSociology and social choice. AI alignment must address not only individuals\u2019 perspectives, but also the\nnorms, expectations, and values of affected groups. Some works have begun to assess whether LLMs can\nbe used to facilitate agreement between different humans (Bakker et al., 2022) and to codify the broad-\nranging principles under which deployment of AI systems for public good can be assessed (Floridi and\nCowls, 2022; Sartori and Theodorou, 2022). The majority-rule problem with RLHF can also be improved by\nalgorithms that explicitly model multiple evaluators (Gordon et al., 2021; Davani et al., 2022; Daniels-Koch\nand Freedman, 2022; Gordon et al., 2022; Barnett et al., 2023), that tune models to individuals (Kumar\net al., 2021), or that use more sophisticated aggregation strategies (Noothigattu et al., 2018). However,\nnone of these approaches can solve the fundamental problem of how an AI system cannot be aligned to\nmultiple groups of humans who hold conflicting viewpoints (Dobbe et al., 2021). Many societies, however,\nconfront this fundamental issue regularly. For example, democracies seek to reflect social preferences by\nsoliciting the feedback of individuals. These systems generally fail to align diverse preferences yet tend to\nbe more acceptable than less-democratic alternatives. As such, it is important to analyze RLHF from the\nlens of social choice theory (Sen, 1986) and work to understand whether the means by which it aggregates\npreferences is normatively acceptable.\n12\nAssistance games.\nAssistance games, such as cooperative inverse RL (CIRL) (Hadfield-Menell et al.,\n2016), provide a framework to analyze algorithms like RLHF. They offer a mathematical model to evaluate\ndifferent design decisions in the communication of preferences to learning systems. In an assistance game,\na human and an agent act together in the environment. Both seek to optimize the human\u2019s latent reward\nfunction, while only the human can directly query this reward function. In this model, querying the human is\nsimply an additional action that the robot can take, and it is possible to study different querying strategies or\nprofiles. Studying RLHF as an assistance game emphasizes the performance of the human-robot team. This\nmight suggest alternative preference elicitation methods. Two examples are using active reward learning to\ndetermine when to collect feedback and which feedback to request first (Sadigh et al., 2017), and leveraging\ndialogue models to learn desired feedback-seeking patterns (Krasheninnikov et al., 2022).\nOf particular\ninterest is understanding the consistency and convergence properties of RLHF, the impact of different error\npatterns from raters, and the effect of different rates of feedback.\nBayesian inference.\nFinetuning an LLM using RL with KL penalties on the differences between the\npretrained model can be understood as a form of Bayesian inference: conditioning a prior (base LLM) on\nevidence about the desirable behavior of an LLM provided by the reward model (Korbak et al., 2022b). This\nperspective on RLHF separates the modeling problem (defining a target distribution specifying the desired\nbehavior of an LLM) and the inference problem (approximating that target distribution) (Korbak et al.,\n2022a; Go et al., 2023). This can aid in answering questions about how the prior influences the outcome\nof RLHF. The typical target distribution of RLHF (a Boltzmann distribution) is a particular design choice\nand other distributions may address some of its limitations by, for example, differently fitting distributional\npreferences (Khalifa et al., 2021). Similarly, RLHF\u2019s inference algorithm (RL with KL penalties; equivalent to\na variational inference approach (Korbak et al., 2022b)) could be replaced by a particular sampling strategy\n(e.g., rejection sampling or best-of-n sampling).\nWorst-case behavior. While RLHF seems to improve the average performance of a system, it is not clear\nwhat effects it has on worst-case behavior. It was not designed to make systems adversarially robust, and\nempirical vulnerabilities of systems trained with RLHF have been demonstrated with jailbreaks and prompt\ninjection attacks (Willison, 2023; Albert, 2023; Oneal, 2023; Li et al., 2023a; Wolf et al., 2023; Liu et al.,\n2023; Rao et al., 2023; Wei et al., 2023; Shen et al., 2023).\nAs a consequence, it would be valuable to\nbetter understand the worst-case behaviors of RLHF systems, potentially through the lenses of theoretical\nproperties (Wolf et al., 2023; El-Mhamdi et al., 2022), decision theory (Casper, 2020), adversarial attacks\n(Perez et al., 2022a;b; Casper et al., 2023b; Ziegler et al., 2022; Carlini et al., 2023b), or rigorous evaluations\n(ARC, 2022; OpenAI, 2023; Shevlane et al., 2023).\n4.2\nAddressing Challenges with RLHF\nJust as RLHF has challenges involving feedback (Section 3.1), the reward model (Section 3.2), and the policy\n(Section 3.3), there are various methods that can replace or combine with parts of the RLHF pipeline to\naddress each of these types of challenges. Figure 3 outlines these methods. See also Wang et al. (2023) for\na survey of methods for aligning LLMs.\n4.2.1\nAddressing Challenges with Human Feedback\nProviding feedback with AI assistance. One way to amplify the abilities of humans is to have AI tools\nassist in generating feedback. Engineering prompts for an AI system and using it to automate feedback can\nsubstantially increase practicality and cost-effectiveness due to reduced reliance on humans. Nonetheless,\nAI-generated feedback still fundamentally depends on humans because (1) the models providing feedback are\ntrained on human-generated data, and (2) humans control prompts and the process of incorporating feedback.\nThere are several notable examples of AI-generated language feedback (Bai et al., 2022b; Saunders et al.,\n2022; Ye et al., 2023; Kim et al., 2023; Aky\u00fcrek et al., 2023; Madaan et al., 2023; Chen et al., 2023; Gilardi\net al., 2023; Lee et al., 2023) with research agendas like Recursive Reward Modeling (Leike et al., 2018) and\nAI Safety via debate (Irving et al., 2018; Du et al., 2023). However, AI-generated feedback has drawbacks.\nHumans often disagree with AI feedback. The rate of human/AI disagreement will vary by task, but Perez\net al. (2022b), Casper et al. (2023b), and Lee et al. (2023) found this to happen up to 10%, 46%, and 22% of\n13\n\u00a0 Translating language to reward\n\u00a0 AI assistance\n\u00a0 Fine-grained feedback\n\u00a0 Direct human oversight\n\u00a0 Multi-objective oversight\n\u00a0 Maintaining uncertainty\n\u00a0 Aligning LLMs during pretraining\n\u00a0 Supervised learning\nPolicy, \u00a74.2.3\nAddressing Challenges with RLHF,\u00a0\u00a74.2\nReward Model, \u00a74.2.2\n\u00a0 Process supervision\nHuman Feedback \u00a74.2.1\n\u00a0 Learning from demonstrations\nFigure 3: Strategies that can be used to address various problems with RLHF. Each approach is\ndiscussed in Section 4.2.\nthe time respectively in different experiments. Machines can also exhibit correlated failure modes not found in\nhumans, such as vulnerabilities to some adversarial attacks. The extent to which AI feedback is a viable way\nto safely augment human feedback remains uncertain. However, it cannot theoretically be a comprehensive\nsolution to AI alignment due to the bootstrapping problem behind ensuring the feedback-providing AI is\naligned.\nFine-grained feedback. Many problems with feedback involve difficulty conveying precise information via\nthe feedback signal (Section 3.1.4). To address this, Wu et al. (2023) and Cabi et al. (2019) use feedback on\nspecific portions of examples and Wu et al. (2023) use feedback with respect to different goals of the model\n(e.g., correctness, relevance). This might improve the quality of the learned reward models at the cost of\nhuman feedback being more expensive to provide. Fine-grained feedback is not yet well studied nor widely\nadopted, so additional work to understand its advantages and feasibility will be valuable.\nProcess-based supervision. One challenge with training AI systems to solve problems is the difficulty\nof supervising performance on multi-step procedures. In RL, rewards can be very sparse for such problems.\nTo address this, some works have trained LLMs to better solve multi-step math problems with process\nsupervision (Uesato et al., 2022; Lightman et al., 2023).\nTranslating natural language specifications into a reward model. Many issues with RLHF arise\ndue to the difficulty of fitting a reward function using some constrained type of feedback. An alternative\napproach can be to generate a reward signal more directly from natural language directions, bypassing the\nneed for feedback on examples. This approach could resemble a technique used by Bai et al. (2022b) which\ninvolved using prompts to guide an AI assistant to identify responses that violated certain user-defined\nspecifications. Moreover, Luketina et al. (2019) surveys other possible techniques to accomplish this goal in\nnon-LLM settings.\nLearning rewards from demonstrations. An alternative approach to learning a reward model, known as\ninverse reinforcement learning (IRL) (Ng et al., 2000; Ramachandran and Amir, 2007; Ziebart et al., 2008),\ninvolves humans providing demonstrations instead of offering feedback on ones generated by the model.\nJeon et al. (2020) and B\u0131y\u0131k et al. (2022) propose systematic ways of combining demonstrations, preferences,\nand possibly other types of human feedback to learn reward functions. While demonstrations carry rich\ninformation and avoid the need to have a system learn from its own generations, they are often more difficult\nto gather because they require higher effort and expertise to perform the task. Additionally, the quality of\ndemonstrations is limited by the talent of whatever expert is providing them, which warrants more research\non learning from suboptimal human demonstrations (e.g., Brown et al. (2019); Zhang et al. (2021)).\n14\n4.2.2\nAddressing Challenges with the Reward Model\nUsing direct human oversight. Although learning a reward model is efficient, it might be necessary to\ndirectly provide rewards (MacGlashan et al., 2017) for RL training in certain safety-critical situations.\nMulti-objective oversight. Richer multi-objective signals that rate outputs on multiple objectives (Vam-\nplew et al., 2022) could lead to more flexible oversight. Current reward models assume that expert feedback\nis drawn from an underlying unimodal reward function (Barnett et al., 2023; Myers et al., 2021). But this is\noverly simplistic (Skalse and Abate, 2022b; Bowling et al., 2023). For instance, it can lead to a reward model\nthat merely captures the preferences of the majority, and suppresses the preferences of minorities as noise.\nUsing constraints (Malik et al., 2021; Lindner et al., 2023) or reward models that account for the diversity\nof preferences by assuming underlying reward functions to be multimodal (Myers et al., 2021; Bakker et al.,\n2022; Barnett et al., 2023; Siddique et al., 2023; Bhatia et al., 2020) can help mitigate this issue. Multi-\nobjective oversight can also be useful for steering systems toward desired balances between competing values\n(e.g., helpfulness and harmlessness).\nMaintaining uncertainty over the learned reward function. Given the challenges of accurately learn-\ning the appropriate reward function, several studies have emphasized the importance of taking uncertainty\nin the learned functions into account.\nYue et al. (2023) and Liang et al. (2022b) tackle this by having\nthe policy avoid types of states unseen by the reward model. Using an ensemble of reward functions has\nalso been used to address these challenges (Christiano et al., 2017), demonstrating that this approach can\nenhance the diversity of text output (Rame et al., 2023) and its applicability for active learning (Gleave and\nIrving, 2022). Other strategies can include forms of risk-aversion (Hadfield-Menell et al., 2017) or handling\nuncertainty with a safe \u201cshield\u201d policy (Jansen et al., 2018; Srinivasan et al., 2020; Cohen and Hutter, 2020).\n4.2.3\nAddressing Challenges with the Policy\nAligning LLMs during pretraining. RLHF in LLMs typically begins by pretraining the LLM on internet\ntext which includes a large amount of undesirable content. Korbak et al. (2023) argue that it can be more\neffective to use human feedback during pretraining by using a reward model to filter, weight, or annotate\npretraining data.\nThis also simplifies the process of aligning models by having them exhibit desirable\nbehaviors from the outset rather than having them learn undesirable behavior and then attempt to unlearn\nit during finetuning.\nAligning LLMs through supervised learning. Several techniques for aligning LLMs with human pref-\nerences obtain results competitive with RLHF by using supervised learning to complement (Ramamurthy\net al., 2022) or replace RL. The simplest variant of this is to perform standard supervised learning on well-\ncurated data. Curation can involve filtering out bad demonstrations (Gehman et al., 2020; Welbl et al., 2021;\nDong et al., 2023), compiling a small set of good demonstrations (Solaiman and Dennison, 2021; Sanh et al.,\n2022; Ibarz et al., 2018; Stiennon et al., 2020; Chung et al., 2022; B\u0131y\u0131k et al., 2022; Zhou et al., 2023), or\ngenerating good demonstrations using an LLM, e.g., after conditioning human feedback provided in natural\nlanguage (Scheurer et al., 2022; 2023; Chen et al., 2023; Xu et al., 2023b). A different family of methods\naugments the language modeling objective to utilize feedback provided by the reward model (Korbak et al.,\n2023; Yuan et al., 2023; Rafailov et al., 2023). This last setting shares similarities with offline RL, which\nfocuses on training an optimal policy using demonstrations annotated with rewards (Levine et al., 2020;\nSnell et al., 2022; Hu et al., 2023).\n4.3\nRLHF is Not All You Need: Complementary Strategies for Safety\nOther technical approaches to AI safety should be studied and implemented alongside RLHF. Establishing\ntrust with AI systems should be approached with a combination of principled design choices, rigorous testing,\ninterpretability, verification, and theoretical guarantees where possible (Leike et al., 2018). See also Critch\nand Krueger (2020), Hubinger (2020), Hendrycks et al. (2021), and Ngo (2022) for additional overviews of\nstrategies for building safer AI.\nRobustness. As discussed in Section 3.3, models trained with RLHF can still exhibit undesired behavior\ndue to distributional shifts between training and deployment. For example, adversarially engineered user\n15\ninputs cause an LLM to output harmful text. To mitigate this problem, developers should use tools to\ngenerate inputs which result in undesired behavior and train against these adversarial examples (Zhang\nand Li, 2019; Ziegler et al., 2022; Perez et al., 2022a; Casper et al., 2023b). Anomaly detection techniques\n(Omar et al., 2013) can also be useful for flagging abnormal inputs likely to trigger bad behavior. Ensuring\nthe security of important AI training runs against malicious human evaluators and/or outside cybersecurity\nthreats will also be valuable.\nRisk assessment and auditing. Although training processes should be crafted to produce models that\nare safe by design, evaluations are another layer of defense. Passing an evaluation is not proof of safety, but\nas is the case in almost every safety-critical industry, rigorous evaluations of capabilities and risks helps to\nspot hazards and establish trust. In practice, this should involve both in-house and second-party evaluations\n(OpenAI, 2023; ARC, 2022; Perez et al., 2022b). As with adversarial training for robustness, the development\nof improved red teaming techniques will be important (Perez et al., 2022a; Casper et al., 2023b).\nInterpretability and model editing. Generating human-understandable explanations for the behavior of\nAI systems is currently an unsolved problem. Progress in explainability and interpretability could help verify\nhypotheses about how models make decisions (Geiger et al., 2023), including whether the decision-making\nprocess is trustworthy. In this way, it could be possible to gain confidence that models will (or will not)\nbehave in a safe way without necessarily conducting extensive testing of the models (Jacovi et al., 2021).\nRed-teaming can also be complemented by interpretability techniques (Rastogi et al., 2023; R\u00e4uker et al.,\n2023), especially for purposes of identifying adversarial inputs (Ziegler et al., 2022; Casper et al., 2023c;a) or\nanomalous inputs (Pang et al., 2021). In another direction, better understanding the internal mechanisms of\nmodels can aid in directly editing model weights or intervening on internal activations in order to improve\ntruthfulness (Li et al., 2023b), modify a model\u2019s factual knowledge (Meng et al., 2023; 2022; Hernandez\net al., 2023; Hase et al., 2023), or otherwise steer model behavior (Cui et al., 2022).\n5\nGovernance and Transparency\nSocial scientists and policymakers have increasingly focused on the need for governance frameworks to develop\nand deploy AI systems responsibly. Across historical contexts, a hallmark of mature scientific fields is the\nopen sharing of research findings (Shapin and Schaffer, 2011) to allow experts to understand progress (Gilbert\nand Loveridge, 2021). Below we overview components of an RLHF governance agenda, including outstanding\nquestions and risk dimensions.\nIncentives and requirements for safety. Competition between labs can generate harmful race dynamics\n(Dafoe, 2018) because of tradeoffs between competitiveness and caution. This suggests a role for governance\nin promoting a healthier environment for safe AI research, development, and deployment (Dafoe, 2018;\nPerry and Uuk, 2019; Falco et al., 2021; Cihon, 2019; Anderljung et al., 2023). Governance in this form\ncould involve mandates for independent auditing, evaluations, and certification (Shavit, 2023; M\u00f6kander\net al., 2023; ARC, 2022; Hadfield and Clark, 2023; Shevlane et al., 2023); monitoring for post-deployment\nproblems (Hendrycks and Gimpel, 2016); influence over resources including hardware and data (Brief, 2020;\nChan et al., 2023a); and prohibiting deployment unless critical standards are met, as in the case of the\nU.S. Food and Drug Administration\u2019s oversight of clinical trials for testing potential new treatments (Junod,\n2008).\nTransparency and auditing. A sustained commitment to transparency would make the existing RLHF\nresearch environment more robust from a safety standpoint. First, the disclosure of some details behind\nlarge RLHF training runs would clarify a given organization\u2019s norms for model scrutiny and safety checks.\nSecond, increased transparency about known efforts to mitigate risks could improve safety incentives and\nsuggest methods for external stakeholders to hold companies accountable. Third, and most relevant for the\npresent paper, transparency would improve the AI safety community\u2019s understanding of RLHF and support\nthe ability to track technical progress on its challenges. Some level of disclosure is a precondition to evaluate\nthe viability of the technical RLHF safety agenda over time and allow for community contribution to it. For\nall of these reasons, working to incorporate transparency standards into an AI governance framework will be\nimportant (Larsson and Heintz, 2020; Anderljung et al., 2023). It is possible that public disclosure of details\ncritical to the development of model capabilities might lead to the unwanted proliferation of AI technologies\n16\n\u00a0 Type(s) of feedback used\n\u00a0 Pretraining\n\u00a0 Selection/training of humans\n\u00a0 Loss function\n\u00a0 Evaluation results\n\u00a0 Evaluation and results\nTransparency / Auditing Items for RLHF\nReward Model\n\u00a0 Selection of examples\nHuman Feedback\n\u00a0 Quality-assurance measures\nPolicy\n\u00a0 Loss function\n\u00a0 Evaluation and results\nSystemic Safety\n\u00a0 Internal and external auditing\n\u00a0 Monitoring and handling failures\n\u00a0 Report on expected risks\nFigure 4: Details behind an implementation of RLHF that, if disclosed, could be indicative of\nrisks. See Section 5 for a complete discussion. Companies using RLHF to train models for high-stakes or\nsafety-critical applications should maintain transparency with the public and/or auditors about key details\nof their approach.\nthat could be misused. However, detailing safety measures will often not require divulging implementable\ndetails, and when it does, private disclosure to second-party auditors (M\u00f6kander et al., 2023; ARC, 2022;\nHadfield and Clark, 2023; Shevlane et al., 2023) offers a solution.\nAs more specific policy prescriptions are beyond our scope, we encourage elaboration on these topics as part\nof a future research agenda. Below, however, we outline specific types of details that, if disclosed, could be\nindicative of risks and should be accounted for when auditing AI systems developed using RLHF. See also\nFigure 4.\nHuman feedback details:\n\u2022 A description of the pretraining process including details about what data was used to\nmake apparent possible biases that pretraining can cause.\n\u2022 How human evaluators were selected and trained to provide information about risks of\nevaluators being malicious, unrepresentative, or incapable.\n\u2022 The process by which examples were selected to obtain feedback to invite scrutiny about\ntheir representativeness and whether sufficient adversarial training was used.\nIf examples were\ncrowdsourced from a publicly-available application, details about what measures were taken to avoid\ndata poisoning attacks should be provided.\n\u2022 The type(s) of human feedback used (e.g., binary comparisons, scalar feedback, etc.) to suggest\nwhat risks might be caused by insufficiently abundant or rich feedback.\n\u2022 A report on measures taken for quality assurance in feedback collection and inter-rater\nconsistency to ensure that effective quality control measures were taken.\nReward model details:\n\u2022 The loss function used to fit the reward model and how disagreement was modeled (e.g.,\nas noise) to help with analyzing the degree of misspecification when fitting the reward model.\n\u2022 A report on reward model evaluation and results to suggest possible problems from a mis-\naligned reward model. The evaluation should involve red teaming.\n17\nPolicy details:\n\u2022 A report on policy evaluation and results to suggest possible troubles from a misaligned policy.\nThe evaluation should involve red teaming and include assessment for risky capabilities (e.g., the\nability to deceive a human).\nSystemic safety measures\n\u2022 A report on internal and external audits and red teaming to ensure accountability and\ndisclose risks that are identified.\n\u2022 A report on expected risks and anticipated failure modes to ensure accountability.\n\u2022 Plans for monitoring and correcting failures that emerge to support post-deployment safety.\nHow these types of risks should be documented remains an area of active work in AI governance. Similar\nquestions have been asked in an investigation by the US Federal Trade Commission into OpenAI (FTC,\n2023) but in response to problems with ChatGPT rather than proactively. Salient documentation proposals\nfocus on regular reporting of reward components (Gilbert et al., 2022) and the ability to compare the\ncapabilities of language models according to standard benchmarks (Liang et al., 2022a). For the longer\nterm, incorporating beneficial standards for safety and transparency into norms and regulations affecting AI\nis an ongoing challenge.\nConcerns for social and economic equity. Although this paper has focused on technical challenges with\nRLHF, there are social and economic ones as well which governance and industry should work to address.\nFor example, OpenAI has paid Kenyan knowledge workers at a rate of less than $2 USD per hour (Perrigo,\n2023) for work which was mentally and emotionally demanding (Hao, 2023). Human subjects used in RLHF\nresearch should not be systematically selected simply for their availability or low cost (National Commission\nfor the Protection of Human Subjects, 1978). Costs, benefits, and influence over RLHF models should be\nequitably distributed across different communities (Whittlestone et al., 2021; Eloundou et al., 2023). There\nis an additional possibility that powerful AI systems will be highly profitable and serve to concentrate large\namounts of wealth and power into the hands of a few (O\u2019Keefe et al., 2020; Chan et al., 2023b). Thus, policies\nthat address inequalities and protect vulnerable populations (e.g. impacted communities, whistleblowers)\nwill be increasingly important.\n6\nDiscussion\nWhile some problems with RLHF are tractable, others are fundamental.\nTechnical progress\nin some respects is tractable, and this room for progress should be seen as a cause for concerted work\nand optimism. Even some of the fundamental problems that we overview can be alleviated with improved\nmethodology even though they cannot be fully solved by RLHF. However, the fundamental nature of these\nproblems requires that they be avoided or compensated for with non-RLHF approaches. Hence, we emphasize\nthe importance of two strategies: (1) evaluating technical progress in light of the fundamental limitations of\nRLHF and other methods, and (2) addressing the sociotechnical challenges of aligning to human values by\ncommitting to both defense-in-depth safety measures and openly sharing research findings with the wider\nscientific community.\nRLHF = Rehashing Lessons from Historical Failures? RLHF offers new capabilities but faces many\nold problems. Its use by Christiano et al. dates to 2017, and the individual components of it (preference\nelicitation, fitting a reward model, and policy optimization) have a history of technical and fundamental\nchallenges in the fields of human-computer interaction and AI safety. In 2023, RLHF was described by the\nfirst author of Christiano et al. (2017) as a \u201cbasic solution\u201d intended to make it easier to \u201cproductively work\non more challenging alignment problems\u201d (Christiano, 2023).3 Some challenges and questions that we have\n3Christiano (2023) mentions debate (Irving et al., 2018) and recursive reward modeling (Leike et al., 2018) as examples of\n\u2018more challenging alignment problems.\u2019 See also an outline of proposals in Hubinger (2020).\n18\ncovered are rather unique to RLHF such as ones involving jointly training the reward model and policy\n(Section 3.4). However, many other problems are instances of broader ones in machine learning such as\nchallenges with RL policies (Section 3.3). Others still are fundamental problems with AI alignment such as\ndetermining whose values are encoded into AI in a diverse society of humans (Section 3.2.1). The successes\nof RLHF should not obfuscate its limitations or gaps between the framework under which it is studied\nand real-world applications (see Appendix A). An approach to AI alignment that relies on RLHF without\nadditional techniques for safety risks doubling-down on flawed approaches to AI alignment. Thus, it will be\nimportant to continue working to better understand RLHF while respecting its limitations.\nMoving forward. RLHF has clear advantages for aligning AI systems with human goals. As a result,\nit has been key to the development of state-of-the-art LLMs and will likely continue to play a major role\nin modern AI. However, its use and influence should be accompanied by a commensurate research effort to\nbetter understand RLHF and address its flaws. Because it optimizes for human approval, RLHF in particular\ndemands a special type of caution because many of its failures will actively tend to be ones that humans\nstruggle to notice. It will be important to approach RLHF cautiously and work to incorporate it into a more\nholistic framework (Khlaaf, 2023) for safer AI with multiple layers of protection from failures (Hendrycks\net al., 2021). Because some of the challenges with RLHF are fundamental to the AI alignment problem\nitself, moving forward will require confronting the basic choices and assumptions behind any given approach\nto aligning AI and who controls it (Dobbe et al., 2021). Moving forward, we urge that those working to\ndevelop advanced LLMs using RLHF both contribute toward resolving its open challenges and maintain\ntransparency about the details of their approach to safety and any anticipated risks.\nContributions\nStephen Casper and Xander Davies served as the central writers and organizers.\nClaudia Shi, Thomas Krendl Gilbert, J\u00e9r\u00e9my Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak,\nDavid Lindner, Pedro Freire, Tony Wang, Samuel Marks, Charbel-Rapha\u00ebl Segerie, Micah Carroll, Andi\nPeng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max\nNadeau, Eric J. Michaud, Jacob Pfau, Xin Chen, Dmitrii Krasheninnikov, Lauro Langosco, and Peter Hase\ncontributed to writing and planning the paper.\nErdem B\u0131y\u0131k, Anca Dragan, David Krueger, Dorsa Sadigh, and Dylan Hadfield-Menell served as advisors.\nAcknowledgements\nWe thank Sam Bowman, Adam Jermyn, Ethan Perez, Alan Chan, Gabriel Recchia, Robert Kirk, and Nathan\nLambert for their helpful feedback. This work was facilitated in part by the Harvard AI Safety Team and\nMIT AI Alignment.\n19\nReferences\nAfra Feyza Aky\u00fcrek, Ekin Aky\u00fcrek, Aman Madaan, Ashwin Kalyan, Peter Clark, Derry Wijaya, and Niket\nTandon.\nRl4f: Generating natural language feedback with reinforcement learning for repairing model\noutputs. arXiv preprint arXiv:2305.08844, 2023.\nAlex Albert. Jailbreak chat. 2023. URL https://www.jailbreakchat.com/.\nSusan Amin, Maziar Gomrokchi, Harsh Satija, Herke van Hoof, and Doina Precup. A survey of exploration\nmethods in reinforcement learning. arXiv preprint arXiv:2109.00157, 2021.\nDario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man\u00e9. Concrete\nproblems in ai safety. arXiv preprint arXiv:1606.06565, 2016.\nMarkus Anderljung, Joslyn Barnhart, Jade Leung, Anton Korinek, Cullen O\u2019Keefe, Jess Whittlestone, Sha-\nhar Avin, Miles Brundage, Justin Bullock, Duncan Cass-Beggs, Ben Chang, Tantum Collins, Tim Fist,\nGillian Hadfield, Alan Hayes, Lewis Ho, Sara Hooker, Eric Horvitz, Noam Kolt, Jonas Schuett, Yonadav\nShavit, Divya Siddarth, Robert Trager, and Kevin Wolf. Frontier ai regulation: Managing emerging risks\nto public safety, 2023.\nAnthropic. Introducing claude, 2023. URL https://www.anthropic.com/index/introducing-claude.\nARC. Arc evals, 2022. URL https://evals.alignment.org/.\nDilip Arumugam, Jun Ki Lee, Sophie Saskin, and Michael L Littman. Deep reinforcement learning from\npolicy-dependent human feedback. arXiv preprint arXiv:1902.04257, 2019.\nHui Bai. Artificial Intelligence Can Persuade Humans. 2023.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan, et al.\nTraining a helpful and harmless assistant with\nreinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna\nChen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from\nai feedback. arXiv preprint arXiv:2212.08073, 2022b.\nAndrea Bajcsy, Dylan P Losey, Marcia K O\u2019Malley, and Anca D Dragan. Learning from physical human\ncorrections, one feature at a time. In Proceedings of the 2018 ACM/IEEE International Conference on\nHuman-Robot Interaction, pages 141\u2013149, 2018.\nMichiel Bakker, Martin Chadwick, Hannah Sheahan, Michael Tessler, Lucy Campbell-Gillingham, Jan Bal-\naguer, Nat McAleese, Amelia Glaese, John Aslanides, Matt Botvinick, et al. Fine-tuning language models\nto find agreement among humans with diverse preferences. Advances in Neural Information Processing\nSystems, 35:38176\u201338189, 2022.\nPeter Barnett, Rachel Freedman, Justin Svegliato, and Stuart Russell. Active reward learning from multiple\nteachers. arXiv preprint arXiv:2303.00894, 2023.\nConnor Baumler, Anna Sotnikova, and Hal Daum\u00e9 III. Which examples should be multiply annotated? active\nlearning when annotators may disagree. In Findings of the Association for Computational Linguistics: ACL\n2023, pages 10352\u201310371, 2023.\nJames Bennett, Stan Lanning, et al. The netflix prize. In Proceedings of KDD cup and workshop, volume\n2007, page 35. New York, 2007.\nKush Bhatia, Ashwin Pananjady, Peter Bartlett, Anca Dragan, and Martin J Wainwright.\nPreference\nlearning along multiple criteria: A game-theoretic perspective. Advances in neural information processing\nsystems, 33:7413\u20137424, 2020.\n20\nErdem Biyik.\nLearning Preferences For Interactive Autonomy.\nPhD thesis, EE Department, Stanford\nUniversity, 2022.\nErdem Biyik and Dorsa Sadigh. Batch active preference-based learning of reward functions. In Conference\non robot learning, pages 519\u2013528. PMLR, 2018.\nErdem Biyik, Malayandi Palan, Nicholas C. Landolfi, Dylan P. Losey, and Dorsa Sadigh.\nAsking easy\nquestions: A user-friendly approach to active reward learning. In Proceedings of the 3rd Conference on\nRobot Learning (CoRL), 2019.\nErdem Biyik, Nicolas Huynh, Mykel J. Kochenderfer, and Dorsa Sadigh. Active preference-based gaussian\nprocess regression for reward learning. In Proceedings of Robotics: Science and Systems (RSS), July 2020.\ndoi: 10.15607/rss.2020.xvi.041.\nErdem B\u0131y\u0131k, Dylan P Losey, Malayandi Palan, Nicholas C Landolfi, Gleb Shevchuk, and Dorsa Sadigh.\nLearning reward functions from diverse sources of human feedback: Optimally integrating demonstrations\nand preferences. The International Journal of Robotics Research, 41(1):45\u201367, 2022.\nAndreea Bobu, Andrea Bajcsy, Jaime F Fisac, Sampada Deglurkar, and Anca D Dragan.\nQuantifying\nhypothesis space misspecification in learning from human\u2013robot demonstrations and physical corrections.\nIEEE Transactions on Robotics, 36(3):835\u2013854, 2020.\nAndreea Bobu, Andi Peng, Pulkit Agrawal, Julie Shah, and Anca D Dragan. Aligning robot and human\nrepresentations. arXiv preprint arXiv:2302.01928, 2023.\nMichael Bowling, John D Martin, David Abel, and Will Dabney. Settling the reward hypothesis. In Inter-\nnational Conference on Machine Learning, pages 3003\u20133020. PMLR, 2023.\nSamuel R. Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamil\u02d9e Luko\u0161i\u00afut\u02d9e,\nAmanda Askell, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Christo-\npher Olah, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-Johnson, Jackson Kernion,\nJamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Liane Lovitt, Nelson Elhage,\nNicholas Schiefer, Nicholas Joseph, Noem\u00ed Mercado, Nova DasSarma, Robin Larson, Sam McCandlish,\nSandipan Kundu, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Timothy Telleen-\nLawton, Tom Brown, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Ben Mann, and\nJared Kaplan. Measuring Progress on Scalable Oversight for Large Language Models, November 2022.\nURL http://arxiv.org/abs/2211.03540. arXiv:2211.03540 [cs].\nCSET Policy Brief. Why ai chips matter. 2020.\nDaniel Brown, Wonjoon Goo, Prabhat Nagarajan, and Scott Niekum. Extrapolating beyond suboptimal\ndemonstrations via inverse reinforcement learning from observations. In International conference on ma-\nchine learning, pages 783\u2013792. PMLR, 2019.\nDaniel Brown, Russell Coleman, Ravi Srinivasan, and Scott Niekum. Safe imitation learning via fast bayesian\nreward inference from preferences. In International Conference on Machine Learning, pages 1165\u20131177.\nPMLR, 2020.\nSerkan Cabi, Sergio G\u00f3mez Colmenarejo, Alexander Novikov, Ksenia Konyushkova, Scott Reed, Rae Jeong,\nKonrad Zolna, Yusuf Aytar, David Budden, Mel Vecerik, et al. Scaling data-driven robotics with reward\nsketching and batch reinforcement learning. arXiv preprint arXiv:1909.12200, 2019.\nNicholas Carlini, Matthew Jagielski, Christopher A Choquette-Choo, Daniel Paleka, Will Pearce, Hyrum\nAnderson, Andreas Terzis, Kurt Thomas, and Florian Tram\u00e8r. Poisoning web-scale training datasets is\npractical. arXiv preprint arXiv:2302.10149, 2023a.\nNicholas Carlini, Milad Nasr, Christopher A Choquette-Choo, Matthew Jagielski, Irena Gao, Anas Awadalla,\nPang Wei Koh, Daphne Ippolito, Katherine Lee, Florian Tramer, et al.\nAre aligned neural networks\nadversarially aligned? arXiv preprint arXiv:2306.15447, 2023b.\n21\nMicah Carroll, Alan Chan, Henry Ashton, and David Krueger. Characterizing Manipulation from AI Sys-\ntems, March 2023. URL http://arxiv.org/abs/2303.09387. arXiv:2303.09387 [cs].\nMicah D Carroll, Anca Dragan, Stuart Russell, and Dylan Hadfield-Menell.\nEstimating and penalizing\ninduced preference shifts in recommender systems. In Proceedings of the 39th International Conference on\nMachine Learning, 2022.\nStephen Casper. Achilles heels for agi/asi via decision theoretic adversaries. arXiv preprint arXiv:2010.05418,\n2020.\nStephen Casper, Dylan Hadfield-Menell, and Gabriel Kreiman. White-box adversarial policies in deep rein-\nforcement learning. arXiv preprint arXiv:2209.02167, 2022.\nStephen Casper, Yuxiao Li, Jiawei Li, Tong Bu, Kevin Zhang, and Dylan Hadfield-Menell. Benchmarking\ninterpretability tools for deep neural networks. arXiv preprint arXiv:2302.10894, 2023a.\nStephen Casper, Jason Lin, Joe Kwon, Gatlen Culp, and Dylan Hadfield-Menell. Explore, establish, exploit:\nRed teaming language models from scratch. arXiv preprint arXiv:2306.09442, 2023b.\nStephen Casper, Max Nadeau, Dylan Hadfield-Menell, and Gabriel Kreiman. Robust feature-level adversaries\nare interpretability tools, 2023c.\nChristopher P Chambers and Federico Echenique. Revealed preference theory, volume 56. Cambridge Uni-\nversity Press, 2016.\nAlan Chan, Herbie Bradley, and Nitarshan Rajkumar. Reclaiming the digital commons: A public data trust\nfor training data. arXiv preprint arXiv:2303.09001, 2023a.\nAlan Chan, Rebecca Salganik, Alva Markelius, Chris Pang, Nitarshan Rajkumar, Dmitrii Krasheninnikov,\nLauro Langosco di Langosco, Zhonghao He, Yawen Duan, Micah Carroll, Michelle Lin, Alex Mayhew,\nKatherine Collins, Maryam Molamohammadi, John Burden, Wanru Zhao, Shalaleh Rismani, Konstantinos\nVoudouris, Umang Bhatt, Adrian Weller, David Krueger, and Tegan Maharaj. Harms from increasingly\nagentic algorithmic systems. ArXiv, abs/2302.10329, 2023b.\nLawrence Chan, Dylan Hadfield-Menell, Siddhartha Srinivasa, and Anca Dragan. The Assistive Multi-Armed\nBandit. arXiv:1901.08654 [cs, stat], January 2019. URL http://arxiv.org/abs/1901.08654. arXiv:\n1901.08654.\nAngelica Chen, J\u00e9r\u00e9my Scheurer, Tomasz Korbak, Jon Ander Campos, Jun Shern Chan, Samuel R Bowman,\nKyunghyun Cho, and Ethan Perez. Improving code generation by training with natural language feedback.\narXiv preprint arXiv:2303.16749, 2023.\nMichael Chmielewski and Sarah C Kucker. An mturk crisis? shifts in data quality and the impact on study\nresults. Social Psychological and Personality Science, 11(4):464\u2013473, 2020.\nPaul\nChristiano.\nWorst-case\nguarantees.\nhttps://ai-alignment.com/\ntraining-robust-corrigibility-ce0e0a3b9b4d, 2019.\nPaul Christiano. Thoughts on the impact of rlhf research, Jan 2023. URL https://www.alignmentforum.\norg/posts/vwu4kegAEZTBtpT6p/thoughts-on-the-impact-of-rlhf-research#The_case_for_a_\npositive_impact:~:text=I%20think%20it%20is%20hard%20to%20productively%20work%20on%\n20more%20challenging%20alignment%20problems%20without%20first%20implementing%20basic%\n20solutions.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforce-\nment learning from human preferences. Advances in neural information processing systems, 30, 2017.\n22\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac\nSuzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha\nValter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun\nYu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason\nWei. Scaling instruction-finetuned language models, 2022. URL https://arxiv.org/abs/2210.11416.\nPeter Cihon. Standards for ai governance: international standards to enable global coordination in ai research\n& development. Future of Humanity Institute. University of Oxford, 2019.\nMichael K Cohen and Marcus Hutter. Curiosity killed the cat and the asymptotically optimal agent. arXiv\npreprint arXiv:2006.03357, 2020.\nAjeya Cotra. Why ai alignment could be hard with modern deep learning. https://www.cold-takes.com/\nwhy-ai-alignment-could-be-hard-with-modern-deep-learning/, 2021.\nAndrew Critch and David Krueger. Ai research considerations for human existential safety (arches). arXiv\npreprint arXiv:2006.04948, 2020.\nAudrey Cui, Ali Jahanian, Agata Lapedriza, Antonio Torralba, Shahin Mahdizadehaghdam, Rohit Kumar,\nand David Bau. Local relighting of real scenes, 2022.\nAllan Dafoe. Ai governance: a research agenda. Governance of AI Program, Future of Humanity Institute,\nUniversity of Oxford: Oxford, UK, 1442:1443, 2018.\nOliver Daniels-Koch and Rachel Freedman. The expertise problem: Learning from specialized feedback.\narXiv preprint arXiv:2211.06519, 2022.\nAida Mostafazadeh Davani, Mark D\u00edaz, and Vinodkumar Prabhakaran. Dealing with disagreements: Looking\nbeyond the majority vote in subjective annotations. Transactions of the Association for Computational\nLinguistics, 10:92\u2013110, 2022.\nLauro Langosco Di Langosco, Jack Koch, Lee D Sharkey, Jacob Pfau, and David Krueger.\nGoal mis-\ngeneralization in deep reinforcement learning. In International Conference on Machine Learning, pages\n12004\u201312019. PMLR, 2022.\nZihan Ding and Hao Dong. Challenges of reinforcement learning. Deep Reinforcement Learning: Fundamen-\ntals, Research and Applications, pages 249\u2013272, 2020.\nRoel Dobbe, Thomas Krendl Gilbert, and Yonatan Mintz. Hard choices in artificial intelligence. Artificial\nIntelligence, 300:103555, 2021.\nHanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong\nZhang.\nRaft: Reward ranked finetuning for generative foundation model alignment.\narXiv preprint\narXiv:2304.06767, 2023.\nYilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality\nand reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325, 2023.\nEl-Mahdi El-Mhamdi, Sadegh Farhadkhani, Rachid Guerraoui, Nirupam Gupta, L\u00ea-Nguy\u00ean Hoang, Rafael\nPinot, and John Stephan. Sok: On the impossible security of very large foundation models. arXiv preprint\narXiv:2209.15259, 2022.\nTyna Eloundou, Sam Manning, Pamela Mishkin, and Daniel Rock. Gpts are gpts: An early look at the\nlabor market impact potential of large language models, 2023.\nKawin Ethayarajh and Dan Jurafsky. The authenticity gap in human evaluation, 2022.\nTom Everitt, Marcus Hutter, Ramana Kumar, and Victoria Krakovna. Reward Tampering Problems and\nSolutions in Reinforcement Learning: A Causal Influence Diagram Perspective. arXiv:1908.04734 [cs],\nMarch 2021. URL http://arxiv.org/abs/1908.04734. arXiv: 1908.04734.\n23\nGregory Falco, Ben Shneiderman, Julia Badger, Ryan Carrier, Anton Dahbura, David Danks, Martin Eling,\nAlwyn Goodloe, Jerry Gupta, Christopher Hart, et al. Governing ai safety through independent audits.\nNature Machine Intelligence, 3(7):566\u2013571, 2021.\nMichael Feffer, Hoda Heidari, and Zachary C Lipton. Moral machine or tyranny of the majority?\narXiv\npreprint arXiv:2305.17319, 2023.\nLuciano Floridi and Josh Cowls. A unified framework of five principles for ai in society. Machine learning\nand the city: Applications in architecture and urban design, pages 535\u2013545, 2022.\nRachel Freedman, Rohin Shah, and Anca Dragan. Choice set misspecification in reward inference. arXiv\npreprint arXiv:2101.07691, 2021.\nAaron French. The mandela effect and new memory. Correspondences, 6(2), 2019.\nFTC.\n\"federal trade commission civil investigative demand schedule ftc file no. 232-3044\", July 2023.\nURL https://www.washingtonpost.com/documents/67a7081c-c770-4f05-a39e-9d02117e50e8.pdf?\nitid=lk_inline_manual_4.\nJustin Fu, Anoop Korattikara, Sergey Levine, and Sergio Guadarrama. From language to goals: Inverse\nreinforcement learning for vision-based instruction following. arXiv preprint arXiv:1902.07742, 2019.\nLeo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. arXiv preprint\narXiv:2210.10760, 2022.\nSamuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. RealToxicityPrompts:\nEvaluating neural toxic degeneration in language models. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 3356\u20133369, Online, November 2020. Association for Computa-\ntional Linguistics. doi: 10.18653/v1/2020.findings-emnlp.301. URL https://aclanthology.org/2020.\nfindings-emnlp.301.\nAtticus Geiger, Chris Potts, and Thomas Icard. Causal abstraction for faithful model interpretation. arXiv\npreprint arXiv:2301.04709, 2023. URL https://arxiv.org/pdf/2301.04709.pdf.\nFabrizio Gilardi, Meysam Alizadeh, and Ma\u00ebl Kubli. Chatgpt outperforms crowd-workers for text-annotation\ntasks. arXiv preprint arXiv:2303.15056, 2023.\nThomas Krendl Gilbert and Andrew Loveridge. Subjectifying objectivity: Delineating tastes in theoretical\nquantum gravity research. Social Studies of Science, 51(1):73\u201399, 2021.\nThomas Krendl Gilbert, Nathan Lambert, Sarah Dean, Tom Zick, and Aaron Snoswell. Reward reports for\nreinforcement learning. arXiv preprint arXiv:2204.10817, 2022.\nAmelia Glaese, Nat McAleese, Maja Tr\u0119bacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh,\nLaura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham, Jonathan Uesato, Po-\nSen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie Chen,\nDoug Fritz, Jaume Sanchez Elias, Richard Green, So\u0148a Mokr\u00e1, Nicholas Fernando, Boxi Wu, Rachel\nFoley, Susannah Young, Iason Gabriel, William Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu,\nLisa Anne Hendricks, and Geoffrey Irving. Improving alignment of dialogue agents via targeted human\njudgements, 2022.\nAdam Gleave and Geoffrey Irving.\nUncertainty estimation for language reward models.\narXiv preprint\narXiv:2203.07472, 2022.\nAdam Gleave, Michael Dennis, Shane Legg, Stuart Russell, and Jan Leike. Quantifying differences in reward\nfunctions. arXiv preprint arXiv:2006.13900, 2020a.\nAdam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, and Stuart Russell.\nAdversarial\npolicies: Attacking deep reinforcement learning. In International Conference on Learning Representations,\n2020b. URL https://openreview.net/forum?id=HJgEMpVFwB.\n24\nDongyoung Go, Tomasz Korbak, Germ\u00e1n Kruszewski, Jos Rozen, Nahyeon Ryu, and Marc Dymetman.\nAligning language models with preferences through f-divergence minimization, 2023.\nGoogle. Bard, 2023. URL https://bard.google.com/.\nMitchell L Gordon, Kaitlyn Zhou, Kayur Patel, Tatsunori Hashimoto, and Michael S Bernstein. The disagree-\nment deconvolution: Bringing machine learning performance metrics in line with reality. In Proceedings\nof the 2021 CHI Conference on Human Factors in Computing Systems, pages 1\u201314, 2021.\nMitchell L Gordon, Michelle S Lam, Joon Sung Park, Kayur Patel, Jeff Hancock, Tatsunori Hashimoto,\nand Michael S Bernstein. Jury learning: Integrating dissenting voices into machine learning models. In\nProceedings of the 2022 CHI Conference on Human Factors in Computing Systems, pages 1\u201319, 2022.\nPrasoon Goyal, Scott Niekum, and Raymond J Mooney.\nUsing natural language for reward shaping in\nreinforcement learning. arXiv preprint arXiv:1903.02020, 2019.\nLewis D. Griffin, Bennett Kleinberg, Maximilian Mozes, Kimberly T. Mai, Maria Vau, Matthew Caldwell,\nand Augustine Marvor-Parker. Susceptibility to Influence of Large Language Models, March 2023. URL\nhttp://arxiv.org/abs/2303.06074. arXiv:2303.06074 [cs].\nLuke Guerdan, Amanda Coston, Zhiwei Steven Wu, and Kenneth Holstein. Ground (less) truth: A causal\nframework for proxy labels in human-algorithm decision-making. arXiv preprint arXiv:2302.06503, 2023.\nGillian K Hadfield and Jack Clark.\nRegulatory markets: The future of ai governance.\narXiv preprint\narXiv:2304.04914, 2023.\nDylan Hadfield-Menell, Stuart J Russell, Pieter Abbeel, and Anca Dragan. Cooperative inverse reinforcement\nlearning. Advances in neural information processing systems, 29, 2016.\nDylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart J Russell, and Anca Dragan. Inverse reward\ndesign. Advances in neural information processing systems, 30, 2017.\nKaren\nHao.\nThe\nhidden\nworkforce\nthat\nhelped\nfilter\nviolence\nand\nabuse\nout\nof\nchatgpt,\n2023.\nURL\nhttps://www.wsj.com/podcasts/the-journal/\nthe-hidden-workforce-that-helped-filter-violence-and-abuse-out-of-chatgpt/\nffc2427f-bdd8-47b7-9a4b-27e7267cf413.\nJochen Hartmann, Jasper Schwenzow, and Maximilian Witte.\nThe political ideology of conversational\nai:\nConverging evidence on chatgpt\u2019s pro-environmental, left-libertarian orientation.\narXiv preprint\narXiv:2301.01768, 2023.\nPeter Hase, Mohit Bansal, Been Kim, and Asma Ghandeharioun. Does localization inform editing? surpris-\ning differences in causality-based localization vs. knowledge editing in language models. arXiv preprint\narXiv:2301.04213, 2023. URL https://arxiv.org/pdf/2301.04213.pdf.\nJoey Hejna and Dorsa Sadigh. Few-shot preference learning for human-in-the-loop rl. In Proceedings of the\n6th Conference on Robot Learning (CoRL), 2022.\nPeter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep\nreinforcement learning that matters.\nIn Proceedings of the AAAI conference on artificial intelligence,\nvolume 32, 2018.\nDan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples\nin neural networks. arXiv preprint arXiv:1610.02136, 2016.\nDan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt. Unsolved problems in ml safety.\narXiv preprint arXiv:2109.13916, 2021.\nEvan Hernandez, Belinda Z Li, and Jacob Andreas. Measuring and manipulating knowledge representations\nin language models. arXiv preprint arXiv:2304.00740, 2023.\n25\nJacob Hilton, Nick Cammarata, Shan Carter, Gabriel Goh, and Chris Olah. Understanding rl vision. Distill,\n5(11):e29, 2020.\nJoey Hong, Kush Bhatia, and Anca Dragan. On the sensitivity of reward inference to misspecified human\nmodels. arXiv preprint arXiv:2212.04717, 2022.\nKeith Hoskin. The \u2018awful idea of accountability\u2019: inscribing people into the measurement of objects. Ac-\ncountability: Power, ethos and the technologies of managing, 265, 1996.\nJian Hu, Li Tao, June Yang, and Chandler Zhou.\nAligning language models with offline reinforcement\nlearning from human feedback. arXiv preprint arXiv:2308.12050, 2023.\nEvan Hubinger. An overview of 11 proposals for building safe advanced ai. arXiv preprint arXiv:2012.07532,\n2020.\nBorja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, and Dario Amodei. Reward learning\nfrom human preferences and demonstrations in atari. Advances in neural information processing systems,\n31, 2018.\nAlex Irpan.\nDeep reinforcement learning doesn\u2019t work yet.\nhttps://www.alexirpan.com/2018/02/14/\nrl-hard.html, 2018.\nGeoffrey Irving, Paul Christiano, and Dario Amodei. Ai safety via debate. arXiv preprint arXiv:1805.00899,\n2018.\nAlon Jacovi, Ana Marasovi\u0107, Tim Miller, and Yoav Goldberg. Formalizing trust in artificial intelligence:\nPrerequisites, causes and goals of human trust in ai. In Proceedings of the 2021 ACM conference on fairness,\naccountability, and transparency, pages 624\u2013635, 2021. URL https://arxiv.org/pdf/2010.07487.pdf.\nNils Jansen, Bettina K\u00f6nighofer, Sebastian Junges, Alexandru C Serban, and Roderick Bloem. Safe rein-\nforcement learning via probabilistic shields. arXiv preprint arXiv:1807.06096, 2018.\nHong Jun Jeon, Smitha Milli, and Anca Dragan. Reward-rational (implicit) choice: A unifying formalism\nfor reward learning. Advances in Neural Information Processing Systems, 33:4415\u20134426, 2020.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing\nSurveys, 55(12):1\u201338, 2023.\nSuzanne Junod. Fda and clinical drug trials: a short history. FDLI Update, page 55, 2008.\nZachary Kenton, Tom Everitt, Laura Weidinger, Iason Gabriel, Vladimir Mikulik, and Geoffrey Irving.\nAlignment of Language Agents, March 2021. URL http://arxiv.org/abs/2103.14659. arXiv:2103.14659\n[cs].\nMuhammad Khalifa, Hady Elsahar, and Marc Dymetman. A distributional approach to controlled text\ngeneration. In International Conference on Learning Representations, 2021. URL https://openreview.\nnet/forum?id=jWkw45-9AbL.\nHeidy Khlaaf. Toward comprehensive risk assessments and assurance of ai-based systems. Trail of Bits,\n2023.\nSungdong Kim, Sanghwan Bae, Jamin Shin, Soyoung Kang, Donghyun Kwak, Kang Min Yoo, and Minjoon\nSeo. Aligning large language models through synthetic feedback. arXiv preprint arXiv:2305.13735, 2023.\nHannah Rose Kirk, Bertie Vidgen, Paul R\u00f6ttger, and Scott A Hale. Personalisation within bounds: A risk\ntaxonomy and policy framework for the alignment of large language models with personalised feedback.\narXiv preprint arXiv:2303.05453, 2023.\nW Bradley Knox and Peter Stone. Tamer: Training an agent manually via evaluative reinforcement. In 2008\n7th IEEE international conference on development and learning, pages 292\u2013297. IEEE, 2008.\n26\nW Bradley Knox, Stephane Hatgis-Kessell, Serena Booth, Scott Niekum, Peter Stone, and Alessandro Allievi.\nModels of human preference for learning reward functions. arXiv preprint arXiv:2206.02231, 2022.\nTomasz Korbak,\nHady Elsahar,\nGerm\u00e1n Kruszewski,\nand Marc Dymetman.\nOn reinforcement\nlearning and distribution matching for fine-tuning language models with no catastrophic forget-\nting.\nIn S. Koyejo,\nS. Mohamed,\nA. Agarwal,\nD. Belgrave,\nK. Cho,\nand A. Oh,\neditors,\nAdvances in Neural Information Processing Systems, volume 35, pages 16203\u201316220. Curran As-\nsociates,\nInc.,\n2022a.\nURL\nhttps://proceedings.neurips.cc/paper_files/paper/2022/file/\n67496dfa96afddab795530cc7c69b57a-Paper-Conference.pdf.\nTomasz Korbak, Ethan Perez, and Christopher Buckley. RL with KL penalties is better viewed as Bayesian\ninference. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 1083\u20131091,\nAbu Dhabi, United Arab Emirates, December 2022b. Association for Computational Linguistics. URL\nhttps://aclanthology.org/2022.findings-emnlp.77.\nTomasz Korbak, Kejian Shi, Angelica Chen, Rasika Bhalerao, Christopher L. Buckley, Jason Phang,\nSamuel R. Bowman, and Ethan Perez. Pretraining language models with human preferences, 2023.\nJernej Kos and Dawn Song.\nDelving into adversarial attacks on deep policies.\narXiv preprint\narXiv:1705.06452, 2017.\nVictoria Krakovna and Janos Kramar. Power-seeking can be probable and predictive for trained agents.\narXiv preprint arXiv:2304.06528, 2023.\nVictoria Krakovna, Jonathan Uesato, Vladimir Mikulik, Matthew Rahtz, Tom Everitt, Ramana Kumar, Zac\nKenton, Jan Leike, and Shane Legg. Specification gaming: the flip side of ai ingenuity. DeepMind Blog,\n2020.\nDmitrii Krasheninnikov, Egor Krasheninnikov, and David Krueger. Assistance with large language models.\nIn NeurIPS ML Safety Workshop, 2022.\nDavid Krueger, Tegan Maharaj, and Jan Leike. Hidden incentives for auto-induced distributional shift, 2020.\nDeepak Kumar, Patrick Gage Kelley, Sunny Consolvo, Joshua Mason, Elie Bursztein, Zakir Durumeric,\nKurt Thomas, and Michael Bailey. Designing toxic content classification for a diversity of perspectives.\nIn SOUPS@ USENIX Security Symposium, pages 299\u2013318, 2021.\nStefan Larsson and Fredrik Heintz. Transparency in artificial intelligence. Internet Policy Review, 9(2),\n2020.\nHarrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Car-\nbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human feedback with ai feedback,\n2023.\nKimin Lee, Laura Smith, and Pieter Abbeel. Pebble: Feedback-efficient interactive reinforcement learning\nvia relabeling experience and unsupervised pre-training. arXiv preprint arXiv:2106.05091, 2021.\nJan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg.\nScalable agent\nalignment via reward modeling: a research direction. arXiv preprint arXiv:1811.07871, 2018.\nSergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review,\nand perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.\nHaoran Li, Dadi Guo, Wei Fan, Mingshi Xu, and Yangqiu Song. Multi-step jailbreaking privacy attacks on\nchatgpt. arXiv preprint arXiv:2304.05197, 2023a.\nKenneth Li, Oam Patel, Fernanda Vi\u00e9gas, Hanspeter Pfister, and Martin Wattenberg. Inference-time inter-\nvention: Eliciting truthful answers from a language model, 2023b.\n27\nMengxi Li, Alper Canberk, Dylan P Losey, and Dorsa Sadigh. Learning human objectives from sequences of\nphysical corrections. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages\n2877\u20132883. IEEE, 2021.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang,\nDeepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv\npreprint arXiv:2211.09110, 2022a.\nXinran Liang, Katherine Shu, Kimin Lee, and Pieter Abbeel.\nReward uncertainty for exploration in\npreference-based reinforcement learning. In International Conference on Learning Representations, 2022b.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John\nSchulman, Ilya Sutskever, and Karl Cobbe. Let\u2019s verify step by step. arXiv preprint arXiv:2305.20050,\n2023.\nJessy Lin, Daniel Fried, Dan Klein, and Anca Dragan. Inferring rewards from language in context. arXiv\npreprint arXiv:2204.02515, 2022.\nDavid Lindner and Mennatallah El-Assady.\nHumans are not boltzmann distributions: Challenges and\nopportunities for modelling human feedback and interaction in reinforcement learning. arXiv preprint\narXiv:2206.13316, 2022.\nDavid Lindner, Xin Chen, Sebastian Tschiatschek, Katja Hofmann, and Andreas Krause. Learning safety\nconstraints from demonstrations with unknown rewards. arXiv preprint arXiv:2305.16147, 2023.\nYi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang, and\nYang Liu. Jailbreaking chatgpt via prompt engineering: An empirical study, 2023.\nDylan P Losey, Andrea Bajcsy, Marcia K O\u2019Malley, and Anca D Dragan. Physical interaction as communi-\ncation: Learning robot objectives online from human corrections. The International Journal of Robotics\nResearch, 41(1):20\u201344, 2022.\nJelena Luketina, Nantas Nardelli, Gregory Farquhar, Jakob Foerster, Jacob Andreas, Edward Grefenstette,\nShimon Whiteson, and Tim Rockt\u00e4schel. A survey of reinforcement learning informed by natural language.\narXiv preprint arXiv:1906.03926, 2019.\nJames MacGlashan, Mark K Ho, Robert Loftin, Bei Peng, Guan Wang, David L Roberts, Matthew E Taylor,\nand Michael L Littman. Interactive learning from policy-dependent human feedback. In International\nConference on Machine Learning, pages 2285\u20132294. PMLR, 2017.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha\nDziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv\npreprint arXiv:2303.17651, 2023.\nShehryar Malik, Usman Anwar, Alireza Aghasi, and Ali Ahmed. Inverse constrained reinforcement learning.\nIn International conference on machine learning, pages 7390\u20137399. PMLR, 2021.\nDavid Manheim and Scott Garrabrant.\nCategorizing variants of goodhart\u2019s law.\narXiv preprint\narXiv:1803.04585, 2018.\nLev McKinney, Yawen Duan, David Krueger, and Adam Gleave. On the fragility of learned reward functions.\narXiv preprint arXiv:2301.03652, 2023.\nKevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. Mass-editing memory\nin a transformer. arXiv preprint arXiv:2210.07229, 2022.\nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations\nin gpt, 2023.\n28\nEric J Michaud, Adam Gleave, and Stuart Russell. Understanding learned reward functions. arXiv preprint\narXiv:2012.05862, 2020.\nSilvia Milano, Mariarosaria Taddeo, and Luciano Floridi. Ethical aspects of multi-stakeholder recommenda-\ntion systems. The information society, 37(1):35\u201345, 2021.\nSmitha Milli and Anca D Dragan. Literal or pedagogic human? analyzing human model misspecification in\nobjective learning. In Uncertainty in artificial intelligence, pages 925\u2013934. PMLR, 2020.\nSoren Mindermann and Stuart Armstrong. Occam\u2019s razor is insufficient to infer the preferences of irrational\nagents. In Proceedings of the 32nd International Conference on Neural Information Processing Systems,\nNIPS\u201918, page 5603\u20135614, Red Hook, NY, USA, 2018. Curran Associates Inc.\nJakob M\u00f6kander, Jonas Schuett, Hannah Rose Kirk, and Luciano Floridi. Auditing large language models:\na three-layered approach. arXiv preprint arXiv:2302.08500, 2023.\nVivek Myers, Erdem Biyik, Nima Anari, and Dorsa Sadigh. Learning multimodal rewards from rankings. In\nConference on Robot Learning, pages 342\u2013352. PMLR, 2021.\nUnited States National Commission for the Protection of Human Subjects. The Belmont report: ethical\nprinciples and guidelines for the protection of human subjects of research, volume 1. United States De-\npartment of Health, Education, and Welfare, National Commission for the Protection of Human Subjects\nof Biomedical and Behavioral Research, 1978.\nAndrew Y Ng, Stuart Russell, et al. Algorithms for inverse reinforcement learning. In Icml, volume 1, page 2,\n2000.\nRichard Ngo. The alignment problem from a deep learning perspective. arXiv preprint arXiv:2209.00626,\n2022.\nKhanh Nguyen, Hal Daum\u00e9 III, and Jordan Boyd-Graber. Reinforcement learning for bandit neural machine\ntranslation with simulated human feedback. arXiv preprint arXiv:1707.07402, 2017.\nEvgenii Nikishin, Pavel Izmailov, Ben Athiwaratkun, Dmitrii Podoprikhin, Timur Garipov, Pavel Shvechikov,\nDmitry Vetrov, and Andrew Gordon Wilson. Improving stability in deep reinforcement learning with\nweight averaging. In Uncertainty in artificial intelligence workshop on uncertainty in Deep learning, 2018.\nRitesh Noothigattu, Snehalkumar Gaikwad, Edmond Awad, Sohan Dsouza, Iyad Rahwan, Pradeep Raviku-\nmar, and Ariel Procaccia. A voting-based system for ethical decision making. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 32, 2018.\nCullen O\u2019Keefe, Peter Cihon, Ben Garfinkel, Carrick Flynn, Jade Leung, and Allan Dafoe. The windfall\nclause: Distributing the benefits of ai for the common good. In Proceedings of the AAAI/ACM Conference\non AI, Ethics, and Society, pages 327\u2013331, 2020.\nSalima Omar, Asri Ngadi, and Hamid H Jebur. Machine learning techniques for anomaly detection: an\noverview. International Journal of Computer Applications, 79(2), 2013.\nA.J.\nOneal.\nChat\ngpt\n\"dan\"\n(and\nother\n\"jailbreaks\").\nhttps://gist.github.com/coolaj86/\n6f4f7b30129b0251f61fa7baaa881516, 2023.\nOpenAI. Gpt-4 technical report, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\nhuman feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.\nAlexander Pan, Kush Bhatia, and Jacob Steinhardt. The effects of reward misspecification: Mapping and\nmitigating misaligned models. arXiv preprint arXiv:2201.03544, 2022.\n29\nRahul Pandey, Hemant Purohit, Carlos Castillo, and Valerie L Shalin. Modeling and mitigating human\nannotation errors to design efficient stream processing systems with human-in-the-loop machine learning.\nInternational Journal of Human-Computer Studies, 160:102772, 2022.\nGuansong Pang, Chunhua Shen, Longbing Cao, and Anton Van Den Hengel. Deep learning for anomaly\ndetection: A review. ACM computing surveys (CSUR), 54(2):1\u201338, 2021.\nAndi Peng, Besmira Nushi, Emre K\u0131c\u0131man, Kori Inkpen, Siddharth Suri, and Ece Kamar. What you see is\nwhat you get? the impact of representation criteria on human bias in hiring. In Proceedings of the AAAI\nConference on Human Computation and Crowdsourcing, volume 7, pages 125\u2013134, 2019.\nAndi Peng, Besmira Nushi, Emre Kiciman, Kori Inkpen, and Ece Kamar. Investigations of performance and\nbias in human-ai teamwork in hiring. In Proceedings of the AAAI Conference on Artificial Intelligence,\nvolume 36, pages 12089\u201312097, 2022.\nAndi Peng, Aviv Netanyahu, Mark K Ho, Tianmin Shu, Andreea Bobu, Julie Shah, and Pulkit Agrawal.\nDiagnosis, feedback, adaptation: A human-in-the-loop framework for test-time policy adaptation.\nIn\nProceedings of the 40th International Conference on Machine Learning, 2023.\nEthan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat\nMcAleese, and Geoffrey Irving.\nRed teaming language models with language models.\narXiv preprint\narXiv:2202.03286, 2022a.\nEthan Perez, Sam Ringer, Kamil\u02d9e Luko\u0161i\u00afut\u02d9e, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit,\nCatherine Olsson, Sandipan Kundu, Saurav Kadavath, et al. Discovering language model behaviors with\nmodel-written evaluations. arXiv preprint arXiv:2212.09251, 2022b.\nBilly Perrigo. Exclusive: The $2 per hour workers who made chatgpt safer, 2023. URL https://time.com/\n6247678/openai-chatgpt-kenya-workers/. [Accessed 07-May-2023].\nBrandon Perry and Risto Uuk. Ai governance and the policymaking process: key considerations for reducing\nai risk. Big data and cognitive computing, 3(2):26, 2019.\nNeil Perry, Megha Srivastava, Deepak Kumar, and Dan Boneh. Do users write more insecure code with ai\nassistants?, 2022.\nVinodkumar Prabhakaran, Aida Mostafazadeh Davani, and Mark Diaz. On releasing annotator-level labels\nand information in datasets. arXiv preprint arXiv:2110.05699, 2021.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea\nFinn. Direct preference optimization: Your language model is secretly a reward model. arXiv preprint\narXiv:2305.18290, 2023.\nDeepak Ramachandran and Eyal Amir. Bayesian inverse reinforcement learning. In Proceedings of the 20th\nInternational Joint Conference on Artifical Intelligence, IJCAI\u201907, page 2586\u20132591, San Francisco, CA,\nUSA, 2007. Morgan Kaufmann Publishers Inc.\nRajkumar Ramamurthy, Prithviraj Ammanabrolu, Kiant\u00e9 Brantley, Jack Hessel, Rafet Sifa, Christian Bauck-\nhage, Hannaneh Hajishirzi, and Yejin Choi. Is reinforcement learning (not) for natural language process-\ning?: Benchmarks, baselines, and building blocks for natural language policy optimization. arXiv preprint\narXiv:2210.01241, 2022.\nAlexandre Rame, Guillaume Couairon, Mustafa Shukor, Corentin Dancette, Jean-Baptiste Gaya, Laure\nSoulier, and Matthieu Cord. Rewarded soups: towards pareto-optimal alignment by interpolating weights\nfine-tuned on diverse rewards. arXiv preprint arXiv:2306.04488, 2023.\nAbhinav Rao, Sachin Vashistha, Atharva Naik, Somak Aditya, and Monojit Choudhury. Tricking llms into\ndisobedience: Understanding, analyzing, and preventing jailbreaks, 2023.\n30\nCharvi Rastogi, Marco Tulio Ribeiro, Nicholas King, and Saleema Amershi. Supporting human-ai collabora-\ntion in auditing llms with llms. arXiv preprint arXiv:2304.09991, 2023. URL https://arxiv.org/pdf/\n2304.09991.pdf.\nTilman R\u00e4uker, Anson Ho, Stephen Casper, and Dylan Hadfield-Menell. Toward transparent ai: A survey\non interpreting the inner structures of deep neural networks. In 2023 IEEE Conference on Secure and\nTrustworthy Machine Learning (SaTML), pages 464\u2013483. IEEE, 2023.\nSiddharth Reddy, Anca D. Dragan, and Sergey Levine. Where Do You Think You\u2019re Going?: Inferring\nBeliefs about Dynamics from Behavior. arXiv:1805.08010 [cs, stat], January 2019. URL http://arxiv.\norg/abs/1805.08010. arXiv: 1805.08010.\nSiddharth Reddy, Sergey Levine, and Anca D Dragan. Assisted Perception: Optimizing Observations to\nCommunicate State. 2020.\nSteffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. Bpr: Bayesian person-\nalized ranking from implicit feedback. arXiv preprint arXiv:1205.2618, 2012.\nDorsa Sadigh, Anca D Dragan, Shankar Sastry, and Sanjit A Seshia. Active preference-based learning of\nreward functions. 2017.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin,\nArnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma\nSharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan\nChang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey,\nRachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault\nFevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and\nAlexander M Rush. Multitask prompted training enables zero-shot task generalization. In International\nConference on Learning Representations, 2022. URL https://openreview.net/forum?id=9Vrb9D0WI4.\nShibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, and Tatsunori Hashimoto. Whose\nopinions do language models reflect? arXiv preprint arXiv:2303.17548, 2023.\nLaura Sartori and Andreas Theodorou. A sociotechnical perspective for the future of ai: narratives, inequal-\nities, and human control. Ethics and Information Technology, 24(1):4, 2022.\nWilliam Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike.\nSelf-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802, 2022.\nJ\u00e9r\u00e9my Scheurer, Jon Ander Campos, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez.\nTraining language models with language feedback.\nIn The First Workshop on Learning with Natural\nLanguage Supervision at ACL, 2022.\nJ\u00e9r\u00e9my Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Angelica Chen, Kyunghyun\nCho, and Ethan Perez.\nTraining language models with language feedback at scale.\narXiv preprint\narXiv:2303.16755, 2023.\nAmartya Sen. Social choice theory. Handbook of mathematical economics, 3:1073\u20131181, 1986.\nRohin Shah, Noah Gundotra, Pieter Abbeel, and Anca Dragan. On the feasibility of learning, rather than\nassuming, human biases for reward inference. In International Conference on Machine Learning, pages\n5670\u20135679. PMLR, 2019.\nRohin Shah, Vikrant Varma, Ramana Kumar, Mary Phuong, Victoria Krakovna, Jonathan Uesato, and Zac\nKenton. Goal misgeneralization: Why correct specifications aren\u2019t enough for correct goals. arXiv preprint\narXiv:2210.01790, 2022.\nSteven Shapin and Simon Schaffer. Leviathan and the air-pump: Hobbes, Boyle, and the experimental life.\nPrinceton University Press, 2011.\n31\nPratyusha Sharma, Balakumar Sundaralingam, Valts Blukis, Chris Paxton, Tucker Hermans, Antonio Tor-\nralba, Jacob Andreas, and Dieter Fox. Correcting robot plans with natural language feedback. arXiv\npreprint arXiv:2204.05186, 2022.\nYonadav Shavit. What does it take to catch a chinchilla? verifying rules on large-scale neural network\ntraining via compute monitoring, 2023.\nXinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang. \" do anything now\": Characterizing\nand evaluating in-the-wild jailbreak prompts on large language models. arXiv preprint arXiv:2308.03825,\n2023.\nToby Shevlane, Sebastian Farquhar, Ben Garfinkel, Mary Phuong, Jess Whittlestone, Jade Leung, Daniel\nKokotajlo, Nahema Marchal, Markus Anderljung, Noam Kolt, et al. Model evaluation for extreme risks.\narXiv preprint arXiv:2305.15324, 2023.\nUmer Siddique, Abhinav Sinha, and Yongcan Cao. Fairness in preference-based reinforcement learning, 2023.\nDavid Silver, Satinder Singh, Doina Precup, and Richard S Sutton. Reward is enough. Artificial Intelligence,\n299:103535, 2021.\nJoar Skalse and Alessandro Abate.\nMisspecification in inverse reinforcement learning.\narXiv preprint\narXiv:2212.03201, 2022a.\nJoar Skalse, Nikolaus HR Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing\nreward hacking. arXiv preprint arXiv:2209.13085, 2022.\nJoar Max Viktor Skalse and Alessandro Abate. The reward hypothesis is false. In NeurIPS ML Safety\nWorkshop, 2022b.\nJoar Max Viktor Skalse, Matthew Farrugia-Roberts, Stuart Russell, Alessandro Abate, and Adam Gleave.\nInvariance in policy optimisation and partial identifiability in reward learning. In International Conference\non Machine Learning, pages 32033\u201332058. PMLR, 2023.\nCharlie Snell, Ilya Kostrikov, Yi Su, Mengjiao Yang, and Sergey Levine. Offline rl for natural language\ngeneration with implicit language q learning, 2022. URL https://arxiv.org/abs/2206.11871.\nAaron J. Snoswell and Jean Burgess.\nThe Galactica AI model was trained on scientific knowledge \u2013\nbut it spat out alarmingly plausible nonsense, November 2022.\nURL http://theconversation.com/\nthe-galactica-ai-model-was-trained-on-scientific-knowledge-but-it-spat-out-alarmingly-plausible-nons\nIrene Solaiman and Christy Dennison.\nProcess for adapting language models to society (palms) with\nvalues-targeted datasets.\nIn M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wort-\nman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 5861\u2013\n5873. Curran Associates, Inc., 2021.\nURL https://proceedings.neurips.cc/paper/2021/file/\n2e855f9489df0712b4bd8ea9e2848c5a-Paper.pdf.\nZiang Song, Tianle Cai, Jason D Lee, and Weijie J Su. Reward collapse in aligning large language models.\narXiv preprint arXiv:2305.17608, 2023.\nKrishnan Srinivasan, Benjamin Eysenbach, Sehoon Ha, Jie Tan, and Chelsea Finn. Learning to be safe:\nDeep rl with a safety critic. arXiv preprint arXiv:2010.14603, 2020.\nJacob Steinhardt.\nEmergent Deception and Emergent Optimization, February 2023.\nURL https:\n//bounded-regret.ghost.io/emergent-deception-optimization/.\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario\nAmodei, and Paul F Christiano.\nLearning to summarize with human feedback.\nAdvances in Neural\nInformation Processing Systems, 33:3008\u20133021, 2020.\n32\nTheodore R Sumers, Mark K Ho, Robert D Hawkins, Karthik Narasimhan, and Thomas L Griffiths. Learn-\ning rewards from linguistic feedback. In Proceedings of the AAAI Conference on Artificial Intelligence,\nvolume 35, pages 6002\u20136010, 2021.\nRan Tian, Masayoshi Tomizuka, Anca Dragan, and Andrea Bajcsy.\nTowards Modeling and Influenc-\ning the Dynamics of Human Learning, January 2023.\nURL http://arxiv.org/abs/2301.00901.\narXiv:2301.00901 [cs].\nJeremy Tien, Jerry Zhi-Yang He, Zackory Erickson, Anca Dragan, and Daniel S Brown. Causal confusion and\nreward misidentification in preference-based reward learning. In The Eleventh International Conference\non Learning Representations, 2023.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-\nlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Fer-\nrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh\nKoura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao,\nXavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy\nReizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subra-\nmanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng\nYan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez,\nRobert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat\nmodels, 2023.\nAlexander M Turner. Seeking power is convergently instrumental in a broad class of environments, 2021.\nURL https://www.alignmentforum.org/s/fSMbebQyR4wheRrvk/p/hzeLSQ9nwDkPc4KNt.\nAlexander Matt Turner and Prasad Tadepalli. Parametrically retargetable decision-makers tend to seek\npower. ArXiv, abs/2206.13477, 2022.\nAlexander Matt Turner, Logan Smith, Rohin Shah, Andrew Critch, and Prasad Tadepalli. Optimal policies\ntend to seek power. In Neural Information Processing Systems, 2019.\nVictor Uc-Cetina, Nicolas Navarro-Guerrero, Anabel Martin-Gonzalez, Cornelius Weber, and Stefan\nWermter. Survey on reinforcement learning for language processing. Artificial Intelligence Review, 56\n(2):1543\u20131575, 2023.\nJonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell,\nGeoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback.\narXiv preprint arXiv:2211.14275, 2022.\nPeter Vamplew, Benjamin J Smith, Johan K\u00e4llstr\u00f6m, Gabriel Ramos, Roxana R\u0103dulescu, Diederik M Roijers,\nConor F Hayes, Fredrik Heintz, Patrick Mannion, Pieter JK Libin, et al. Scalar reward is not enough: A\nresponse to silver, singh, precup and sutton (2021). Autonomous Agents and Multi-Agent Systems, 36(2):\n41, 2022.\nVeniamin Veselovsky, Manoel Horta Ribeiro, and Robert West.\nArtificial artificial artificial intelli-\ngence:\nCrowd workers widely use large language models for text production tasks.\narXiv preprint\narXiv:2306.07899, 2023.\nJames\nVincent.\nMicrosoft\u2019s\nBing\nis\nan\nemotionally\nmanipulative\nliar,\nand\npeo-\nple\nlove\nit,\nFebruary\n2023.\nURL\nhttps://www.theverge.com/2023/2/15/23599072/\nmicrosoft-ai-bing-personality-conversations-spy-employees-webcams.\nAlex Wan, Eric Wallace, Sheng Shen, and Dan Klein. Poisoning language models during instruction tuning.\nIn International Conference on Machine Learning, 2023.\n33\nTony Tong Wang, Adam Gleave, Nora Belrose, Tom Tseng, Joseph Miller, Michael D Dennis, Yawen Duan,\nViktor Pogrebniak, Sergey Levine, and Stuart Russell. Adversarial policies beat professional-level go ais.\narXiv preprint arXiv:2211.00241, 2022.\nYufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang,\nand Qun Liu. Aligning large language models with human: A survey, 2023.\nAlexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail? arXiv\npreprint arXiv:2307.02483, 2023.\nLaura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng,\nMia Glaese, Borja Balle, Atoosa Kasirzadeh, Zac Kenton, Sasha Brown, Will Hawkins, Tom Stepleton,\nCourtney Biles, Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne Hendricks, William Isaac, Sean\nLegassick, Geoffrey Irving, and Iason Gabriel. Ethical and social risks of harm from language models,\n2021.\nJohannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor, Lisa Anne Hendricks,\nKirsty Anderson, Pushmeet Kohli, Ben Coppin, and Po-Sen Huang. Challenges in detoxifying language\nmodels. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2447\u20132469,\nPunta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.\n18653/v1/2021.findings-emnlp.210. URL https://aclanthology.org/2021.findings-emnlp.210.\nJess Whittlestone, Kai Arulkumaran, and Matthew Crosby. The societal implications of deep reinforcement\nlearning. Journal of Artificial Intelligence Research, 70:1003\u20131030, 2021.\nNils Wilde, Erdem Biyik, Dorsa Sadigh, and Stephen L Smith. Learning reward functions from scale feedback.\nIn Conference on Robot Learning, pages 353\u2013362. PMLR, 2022.\nSimon Willison. Prompt injection. 2023. URL https://simonwillison.net/series/prompt-injection/.\nChristian Wirth, Riad Akrour, Gerhard Neumann, Johannes F\u00fcrnkranz, et al. A survey of preference-based\nreinforcement learning methods. Journal of Machine Learning Research, 18(136):1\u201346, 2017.\nYotam Wolf, Noam Wies, Yoav Levine, and Amnon Shashua. Fundamental limitations of alignment in large\nlanguage models. arXiv preprint arXiv:2304.11082, 2023.\nJeff Wu, Long Ouyang, Daniel M. Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Christiano.\nRecursively summarizing books with human feedback, 2021a.\nXian Wu, Wenbo Guo, Hua Wei, and Xinyu Xing. Adversarial policy training against deep reinforcement\nlearning. In USENIX Security Symposium, pages 1883\u20131900, 2021b.\nZeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A. Smith, Mari\nOstendorf, and Hannaneh Hajishirzi.\nFine-grained human feedback gives better rewards for language\nmodel training, 2023.\nBlake Wulfe, Logan Michael Ellis, Jean Mercat, Rowan Thomas McAllister, and Adrien Gaidon. Dynamics-\naware comparison of learned reward functions. In International Conference on Learning Representations,\n2022. URL https://openreview.net/forum?id=CALFyKVs87.\nJiashu Xu, Mingyu Derek Ma, Fei Wang, Chaowei Xiao, and Muhao Chen.\nInstructions as backdoors:\nBackdoor vulnerabilities of instruction tuning for large language models. arXiv preprint arXiv:2305.14710,\n2023a.\nWanqiao Xu, Shi Dong, Dilip Arumugam, and Benjamin Van Roy. Shattering the agent-environment interface\nfor fine-tuning inclusive language models. arXiv preprint arXiv:2305.11455, 2023b.\nTianpei Yang, Hongyao Tang, Chenjia Bai, Jinyi Liu, Jianye Hao, Zhaopeng Meng, Peng Liu, and\nZhen Wang.\nExploration in deep reinforcement learning:\na comprehensive survey.\narXiv preprint\narXiv:2109.06668, 2021.\n34\nGeorgios N Yannakakis and John Hallam. Ranking vs. preference: a comparative study of self-reporting.\nIn Affective Computing and Intelligent Interaction: 4th International Conference, ACII 2011, Memphis,\nTN, USA, October 9\u201312, 2011, Proceedings, Part I 4, pages 437\u2013446. Springer, 2011.\nSeonghyeon Ye, Yongrae Jo, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, and Minjoon Seo. Selfee:\nIterative self-revising llm empowered by self-feedback generation, 2023. URL https://kaistai.github.\nio/SelFee/.\nWenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montse Gonzalez Arenas, Hao-\nTien Lewis Chiang, Tom Erez, Leonard Hasenclever, Jan Humplik, Brian Ichter, Ted Xiao, Peng Xu,\nAndy Zeng, Tingnan Zhang, Nicolas Heess, Dorsa Sadigh, Jie Tan, Yuval Tassa, and Fei Xia. Language\nto rewards for robotic skill synthesis. Arxiv preprint arXiv:2306.08647, 2023.\nZheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. Rrhf: Rank responses\nto align language models with human feedback without tears, 2023.\nSheng Yue, Guanbo Wang, Wei Shao, Zhaofeng Zhang, Sen Lin, Ju Ren, and Junshan Zhang.\nClare:\nConservative model-based reward learning for offline inverse reinforcement learning.\nIn The Eleventh\nInternational Conference on Learning Representations, 2023.\nJiliang Zhang and Chen Li. Adversarial examples: Opportunities and challenges. IEEE transactions on\nneural networks and learning systems, 31(7):2578\u20132593, 2019.\nMuru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A Smith. How language model hallucinations\ncan snowball. arXiv preprint arXiv:2305.13534, 2023.\nSongyuan Zhang, Zhangjie Cao, Dorsa Sadigh, and Yanan Sui. Confidence-aware imitation learning from\ndemonstrations with varying optimality. Advances in Neural Information Processing Systems, 34:12340\u2013\n12350, 2021.\nZhibing Zhao, Peter Piech, and Lirong Xia. Learning mixtures of plackett-luce models. In International\nConference on Machine Learning, pages 2906\u20132914. PMLR, 2016.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,\nLili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023.\nLi Zhou and Kevin Small. Inverse reinforcement learning with natural language goals. In Proceedings of the\nAAAI Conference on Artificial Intelligence, volume 35, pages 11116\u201311124, 2021.\nBanghua Zhu, Jiantao Jiao, and Michael I Jordan. Principled reinforcement learning with human feedback\nfrom pairwise or k-wise comparisons. arXiv preprint arXiv:2301.11270, 2023.\nSimon Zhuang and Dylan Hadfield-Menell. Consequences of misaligned ai. Advances in Neural Information\nProcessing Systems, 33:15763\u201315773, 2020.\nBrian D Ziebart, Andrew L Maas, J Andrew Bagnell, Anind K Dey, et al.\nMaximum entropy inverse\nreinforcement learning. In Aaai, volume 8, pages 1433\u20131438. Chicago, IL, USA, 2008.\nDaniel Ziegler, Seraphina Nix, Lawrence Chan, Tim Bauman, Peter Schmidt-Nielsen, Tao Lin, Adam Scherlis,\nNoa Nabeshima, Benjamin Weinstein-Raun, Daniel de Haas, et al. Adversarial training for high-stakes\nreliability. Advances in Neural Information Processing Systems, 35:9274\u20139286, 2022.\nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Chris-\ntiano, and Geoffrey Irving.\nFine-tuning language models from human preferences.\narXiv preprint\narXiv:1909.08593, 2019.\n35\nA\nAn Improved Model of the Human Feedback Process\nAs illustrated in Equation (1), the feedback process in RLHF is typically modeled with a single human\nH with internal reward function rH; examples sampled from the base model: xi \u223c \u03c0\u03b8; and feedback as a\nfunction of the human, example, and noise: yi = f(h, xi, \u03f5i). However, as discussed in Section 3, this is a\nmisspecified model of the process: there is not a single human, humans values are not representable with a\nreward function, human actions are dependent on context, and the sampling process can involve a human.\nThus we propose an alternative formulation.\nLet \u2206H refer to a joint distribution of humans (or groups thereof if feedback is provided collaboratively)\nused for obtaining samples and feedback denoted as Hsample\nj\nand Hfeedback\nj\n. A dataset of examples is sampled\nfrom \u03c0\u03b8 (or some other source) where each example xi is defined to be a batch of one or more generations\nfrom the base model. Importantly, xi may not contain all information about the world state (e.g., if xi is\na 2D rendering of a 3D environment), and the human may be able to observe more than just the model\u2019s\noutput (e.g., if interpretability tools are used to aid in evaluation). So let v be a rendering function that\nmaps \u03c0\u03b8 and xi to what a human sees. The behavior of humans varies over time and in different contexts,\nso let csample\ni\nand cfeedback\ni\nrepresent particular contexts for sampling and feedback collection. Denote the\nsampling process as s which maps the base model \u03c0\u03b8, a human Hsample\nj\n, and context csample\ni\nto some example\nxi. Notably, s could ignore the base model and generate offline samples from some other source. Finally,\nlet f map a human Hfeedback\nj\n, rendered example v(\u03c0\u03b8, xi), and context cfeedback\ni\nto feedback yi. The data\ncollection process can thus be more completely modeled as:\nHsample\nj\n, Hfeedback\nj\n\u223c \u2206H,\nxi \u223c s(\u03c0\u03b8, Hsample\nj\n, csample\ni\n),\nyi = f(v(\u03c0\u03b8, xi), Hfeedback\nj\n, cfeedback\ni\n)\n(4)\nwhich highlights a need for future work to better account for the aspects of this process that are commonly\nnot accounted for when training systems with RLHF.\nB\nRationale for Why Challenges Were Categorized as Tractable or Fundamental\nIn Section 3, we categorize problems as tractable or fundamental. The key distinction between the two\nis that fundamental challenges are substantial enough that overcoming them would require a method that\nis no longer a form of RLHF. Although many of the fundamental problems we identify can be alleviated by\nimproving how RLHF is approached, they could be fully addressed with RLHF. As a result, they should be\neither avoided by not using RLHF or compensated for by other safety measures. This distinction is soft, and\nsome categories of challenges are marginal. Here, we briefly explain each categorization.\nB.1\nProblems from Section 3.1:\nTractable: Selecting representative humans and getting them to provide quality feedback is\ndifficult: This can be addressed by studying and improving the selection and training of evaluators.\nTractable: Some evaluators have harmful biases and opinions: This can be addressed by studying\nand improving the selection and training of evaluators.\nTractable: Individual human evaluators can poison data: This can be addressed with improved\nevaluator selection and quality assurance measures.\nTractable:\nHumans make simple mistakes due to limited time, attention, or care: This is\nmarginal because human mistakes can never fully be overcome.\nHowever, they can be addressed with\nimproved working conditions and quality assurance procedures.\nTractable: Partial observability limits human evaluators: Human evaluators can be provided with\nall information available in the policy\u2019s observations (although representing this in an easily-comprehensible\nway may be challenging).\nFundamental: Humans cannot evaluate performance on difficult tasks well: Human intelligence\nand cognitive capacity are limited. Humans cannot be expected to properly evaluate the performance of\n36\nsuperhuman models on complex tasks. Thus, solving this problem would require no longer using human\nfeedback in the way that RLHF does.\nFundamental: Humans can be misled, so their evaluations can be gamed: Human fallibility cannot\nfully be overcome, especially against optimization pressure from the learned policy.\nTractable: Data collection can introduce harmful biases: This can be addressed with improved data\ncuration.\nFundamental: There is an inherent cost/quality tradeoff when collecting human feedback: This\ntradeoff is unavoidable in practice \u2013 obtaining diverse and high-quality examples (e.g. from long chatbot\nconversations) requires more effort.\nFundamental: RLHF suffers from a tradeoff between the richness and efficiency of feedback\ntypes: This tradeoff is unavoidable for data collection in practice \u2013 richer annotations require more effort.\nB.2\nProblems from Section 3.2:\nFundamental: An individual human\u2019s values are difficult to represent with a reward function:\nThis problem is marginal. It can be improved in practice by improved modeling, but RLHF-based solutions\nwill be limited by the intractability of perfectly modeling context and troubles with the reward hypothesis\n(Skalse and Abate, 2022b; Bowling et al., 2023).\nFundamental: A single reward function cannot represent a diverse society of humans: Trivial.\nInstead of being a fundamental limitation with RLHF, this is a broader limitation of AI alignment itself.\nFundamental:\nReward models can misgeneralize to be poor reward proxies, even from\ncorrectly-labeled training data: This problem is marginal because it can and should be addressed by\nimproved sampling in practice. However, it is impossible to perfectly represent a distribution with infinite\nsupport from a finite sample. Additionally, the deployment distribution will always differ from the training\nand evaluation distributions in real-world settings (Christiano, 2019).\nFundamental: Optimizing for an imperfect reward proxy leads to reward hacking: If a reward\nmodel is imperfect, reward hacking will always be a possibility from RL.\nTractable: Evaluating reward models is difficult and expensive: This can be addressed by perform-\ning thorough and expensive evaluations.\nB.3\nProblems from Section 3.3:\nTractable: It is (still) challenging to optimize policies effectively: This can be addressed with\nadvancements in RL methodology.\nTractable: Policies tend to be adversarially exploitable: This problem is marginal because achieving\ncertified adversarial robustness against practical threat models has empirically been intractable. Nonetheless,\nthis can be addressed with robust optimization techniques.\nFundamental: Policies can perform poorly in deployment even if rewards seen during training\nwere perfectly correct: This problem is marginal because it can and should be addressed by improved\nsampling in practice. However, it is impossible to perfectly represent a distribution with infinite support\nfrom a finite sample. Additionally, the deployment distribution will always differ from the training and\nevaluation distributions in real-world settings Christiano (2019).\nFundamental: Optimal RL agents tend to seek power: Power is instrumentally useful for agents.\nTractable: The pretrained model introduces biases into policy optimization: This can be ad-\ndressed with improved base models.\nTractable: RL contributes to mode collapse: This can be addressed with forms of RL that optimize\nfor distribution-matching in desired instances.\n37\nB.4\nProblems from Section 3.4:\nTractable: Joint training induces distribution shifts: This can be mitigated with synchronous learning\nor other strategies.\nTractable: It is difficult to balance efficiency and avoiding overfitting by the policy: This can\nbe addressed with improved training methodology.\n38\n"
  },
  {
    "title": "Med-Flamingo: a Multimodal Medical Few-shot Learner",
    "link": "https://arxiv.org/pdf/2307.15189.pdf",
    "upvote": "20",
    "text": "Preprint.\nMED-FLAMINGO:\nA MULTIMODAL MEDICAL FEW-\nSHOT LEARNER\nMichael Moor\u22171 Qian Huang\u22171 Shirley Wu1 Michihiro Yasunaga1 Cyril Zakka2\nYash Dalmia1 Eduardo Pontes Reis3 Pranav Rajpurkar4 Jure Leskovec1\n1Department of Computer Science, Stanford University, Stanford, USA\n2Department of Cardiothoracic Surgery, Stanford Medicine, Stanford, USA\n3Hospital Israelita Albert Einstein, S\u02dcao Paulo, Brazil\n4Department of Biomedical Informatics, Harvard Medical School, Boston, USA\nABSTRACT\nMedicine, by its nature, is a multifaceted domain that requires the synthesis of\ninformation across various modalities. Medical generative vision-language mod-\nels (VLMs) make a first step in this direction and promise many exciting clinical\napplications. However, existing models typically have to be fine-tuned on sizeable\ndown-stream datasets, which poses a significant limitation as in many medical\napplications data is scarce, necessitating models that are capable of learning from\nfew examples in real-time. Here we propose Med-Flamingo, a multimodal few-shot\nlearner adapted to the medical domain. Based on OpenFlamingo-9B, we continue\npre-training on paired and interleaved medical image-text data from publications\nand textbooks. Med-Flamingo unlocks few-shot generative medical visual ques-\ntion answering (VQA) abilities, which we evaluate on several datasets including\na novel challenging open-ended VQA dataset of visual USMLE-style problems.\nFurthermore, we conduct the first human evaluation for generative medical VQA\nwhere physicians review the problems and blinded generations in an interactive app.\nMed-Flamingo improves performance in generative medical VQA by up to 20%\nin clinician\u2019s rating and firstly enables multimodal medical few-shot adaptations,\nsuch as rationale generation. We release our model, code, and evaluation app\nunder https://github.com/snap-stanford/med-flamingo.\n1\nINTRODUCTION\nLarge, pre-trained models (or foundation models) have demonstrated remarkable capabilities in\nsolving an abundance of tasks by being provided only a few labeled examples as context Bommasani\net al. (2021). This is known as in-context learning Brown et al. (2020), through which a model learns\na task from a few provided examples specifically during prompting and without tuning the model\nparameters. In the medical domain, this bears great potential to vastly expand the capabilities of\nexisting medical AI models Moor et al. (2023). Most notably, it will enable medical AI models\nto handle the various rare cases faced by clinicians every day in a unified way, to provide relevant\nrationales to justify their statements, and to easily customize model generations to specific use cases.\nImplementing the in-context learning capability in a medical setting is challenging due to the inherent\ncomplexity and multimodality of medical data and the diversity of tasks to be solved.\nPrevious efforts to create multimodal medical foundation models, such as ChexZero Tiu et al. (2022)\nand BiomedCLIP Zhang et al. (2023a), have made significant strides in their respective domains.\nChexZero specializes in chest X-ray interpretation, while BiomedCLIP has been trained on more\ndiverse images paired with captions from the biomedical literature. Other models have also been\ndeveloped for electronic health record (EHR) data Steinberg et al. (2021) and surgical videos Kiyasseh\net al. (2023). However, none of these models have embraced in-context learning for the multimodal\nmedical domain. Existing medical VLMs, such as MedVINT Zhang et al. (2023b), are typically\ntrained on paired image-text data with a single image in the context, as opposed to more general\n\u2217These authors contributed equally to this work.\n1\narXiv:2307.15189v1  [cs.CV]  27 Jul 2023\nPreprint.\n32-year-old man presents to \nER 15 mins after 7-feet fall \nonto a wooden post. \nSymptoms: severe pain, \nrapid breathing, vitals: \npulse 135/min, respirations \n30/min, BP 80/40 mm Hg. \n(\u2026)\nBlunt trauma to the left lung with hemothorax.\nMed-Flamingo\nQuestion: What is the most likely diagnosis?\nThe patient has a left-sided pneumothorax.\nBaseline\n\u2705   Correct side, but\n\u274c   Wrong diagnosis / pathology\n\u2705   Correct diagnosis\n\u2705   Correct pathology, left-sided.\nChest X-ray image showing hemothorax following \nblunt chest trauma\n*\n*\nFigure 1: Example of how Med-Flamingo answers complex multimodal medical questions by\ngenerating open-ended responses conditioned on textual and visual information.\nstreams of text that are interleaved with multiple images. Therefore, these models were not designed\nand tested to perform multimodal in-context learning with few-shot examples1\nHere, we propose Med-Flamingo, the first medical foundation model that can perform multimodal in-\ncontext learning specialized for the medical domain. Med-Flamingo is a vision-language model based\non Flamingo (Alayrac et al., 2022) that can naturally ingest data with interleaved modalities (images\nand text), to generate text conditioned on this multimodal input. Building on the success of Flamingo,\nwhich was among the first vision-language models to exhibit in-context learning and few-shot\nlearning abilities, Med-Flamingo extends these capabilities to the medical domain by pre-training\non multimodal knowledge sources across medical disciplines. In preparation for the training of\nMed-Flamingo, our initial step involved constructing a unique, interleaved image-text dataset, which\nwas derived from an extensive collection of over 4K medical textbooks (Section 3). Given the critical\nnature of accuracy and precision within the medical field, it is important to note that the quality,\nreliability, and source of the training data can considerably shape the results. Therefore, to ensure\naccuracy in medical facts, we meticulously curated our dataset from respected and authoritative\nsources of medical knowledge, as opposed to relying on potentially unreliable web-sourced data.\nIn our experiments,\nwe evaluate Med-Flamingo on generative medical visual question-\nanswering (VQA) tasks by directly generating open-ended answers, as opposed to scoring artificial\nanswer options ex post\u2013as CLIP-based medical vision-language models do. We design a new realistic\nevaluation protocol to measure the model generations\u2019 clinical usefulness. For this, we conduct an\nin-depth human evaluation study with clinical experts which results in a human evaluation score that\nserves as our main metric. In addition, due to existing medical VQA datasets being narrowly focused\non image interpretation among the specialties of radiology and pathology, we create Visual USMLE,\na challenging generative VQA dataset of complex USMLE-style problems across specialties, which\nare augmented with images, case vignettes, and potentially with lab results.\nAveraged across three generative medical VQA datasets, few-shot prompted Med-Flamingo achieves\nthe best average rank in clinical evaluation score (rank of 1.67, best prior model has 2.33), indicating\nthat the model generates answers that are most preferred by clinicians, with up to 20% improvement\nover prior models. Furthermore, Med-Flamingo is capable of performing medical reasoning, such\nas answering complex medical questions (such as visually grounded USMLE-style questions) and\nproviding explanations (i.e., rationales), a capability not previously demonstrated by other multimodal\nmedical foundation models. However, it is important to note that Med-Flamingo\u2019s performance may\nbe limited by the availability and diversity of training data, as well as the complexity of certain medical\ntasks. All investigated models and baselines would occasionally hallucinate or generate low-quality\nresponses. Despite these limitations, our work represents a significant step forward in the development\nof multimodal medical foundation models and their ability to perform multimodal in-context learning\nin the medical domain. We release the Med-Flamingo-9B checkpoint for further research, and\n1For example, a challenge with multimodal in-context learning for existing medical vision language models\nis the potential for image information to leak across examples, potentially misleading the model.\n2\nPreprint.\n \n \n \n1. Multimodal pre-training on medical literature\nPaired data\nInterleaved data\nImage\nCaption\n \n \nChunk 1\nChunk 2\nTokenized data\n \n<image>\n \n \n \n<image>\n \n \n \n \nInput text\nLM layers \nGated X-attn layers \nVision encoder\nPerceiver Resampler\nOutput text\nMed-Flamingo\nBackbones: Llama-7B, ViT/L-14, OpenFlamingo\n2. Few-shot generative VQA\nQuestion,\nAnswer\nInstruction\nQuestion,\nAnswer\nQuestion\nQuestion: What do the small \nwhite lesions in the aorta mean?\nAnswer: Calci\ufb01cation of the \naortic wall.\n3. Human evaluation\nQuestion\nCorrect answer\nProblem\nMultimodal few-shot prompt\nGenerated answer\nClinically useful?\n0\n10\nScore: 9\nFigure 2: Overview of the Med-Flamingo model and the three steps of our study. First, we pre-train\nour Med-Flamingo model using paired and interleaved image-text data from the general medical\ndomain (sourced from publications and textbooks). We initialize our model at the OpenFlamingo\ncheckpoint continue pre-training on medical image-text data. Second, we perform few-shot generative\nvisual question answering (VQA). For this, we leverage two existing medical VQA datasets, and a\nnew one, Visual USMLE. Third, we conduct a human rater study with clinicians to rate generations in\nthe context of a given image, question and correct answer. The human evaluation was conducted with\na dedicated app and results in a clinical evaluation score that serves as our main metric for evaluation.\n3\nPreprint.\nmake our code available under https://github.com/snap-stanford/med-flamingo.\nIn summary, our paper makes the following contributions:\n1. We present the first multimodal few-shot learner adapted to the medical domain, which\npromises novel clinical applications such as rationale generation and conditioning on re-\ntrieved multimodal context.\n2. We create a novel dataset that enables the pre-training of a multimodal few-shot learner for\nthe general medical domain.\n3. We create a novel USMLE-style evaluation dataset that combines medical VQA with\ncomplex, across-specialty medical reasoning.\n4. We highlight shortcomings of existing evaluation strategies, and conduct an in-depth clinical\nevaluation study of open-ended VQA generations with medical raters using a dedicated\nevaluation app.\n2\nRELATED WORKS\nThe success of large language models (LLMs) Brown et al.; Liang et al. (2022); Qin et al. (2023)\nhas led to significant advancements in training specialized models for the medical domain. This has\nresulted in the emergence of various models, including BioBERT Lee et al. (2020), ClinicalBERT\nHuang et al. (2019), PubMedBERT Gu et al. (2021), BioLinkBERT Yasunaga et al. (b), DRAGON\nYasunaga et al. (a), BioMedLM Bolton et al., BioGPT Luo et al. (2022), and Med-PaLM Singhal\net al.. Although these medical language models are typically smaller than general-purpose LLMs\nlike GPT-3 Brown et al., they can match or even surpass their performance on medical tasks, such as\nmedical question answering.\nRecently, there has been a growing interest in extending language models to handle vision-language\nmultimodal data and tasks Su et al. (2019); Ramesh et al.; Alayrac et al. (2022); Aghajanyan et al.;\nYasunaga et al. (2023). Furthermore, many medical applications involve multimodal information,\nsuch as radiology tasks that require the analysis of both X-ray images and radiology reports Tiu\net al. (2022). Motivated by these factors, we present a medical vision-language model (VLM).\nExisting medical VLMs include BiomedCLIP Zhang et al. (2023a), MedVINT Zhang et al. (2023b).\nWhile BiomedCLIP is an encoder-only model, our focus lies in developing a generative VLM,\ndemonstrating superior performance compared to MedVINT. Finally, Llava-Med is another recent\nmedical generative VLM Li et al. (2023), however the model was not yet available for benchmarking.\n \nNeuroscience /\nNeurology\nRadiology\nOncology\nSurgery\nCardiology\nPsychiatry\nPharmacology\nHerbal medicine\nOrthopedics\nNutrition\nInternal Medicine\nDermatology\nObstetrics and\nGynecology\nImmunology\nDentistry /\nOrthodontics\nOphthalmo-\nlogy\nPediatrics\nOther\nPathology Anatomy\nGastro-\nenterology\nGenetics\nEndocrino-\nlogy\nMicro-\nbiology\nInfectious\nDiseases\nBiomedical\nengineering\nAnesthesio-\nlogy\nMedical\nhistory\nPhysiology\nOtolaryn-\ngology\nNursing\nHemato-\nlogy\nUrology\nPulmo-\nnology\nMedical\nResearch\nand\nStatistics\nSports\nMedicine\nCell Biology\nand\nHistology\nPain\nmedicine\nForensics\nEmergency\nMedicine\nPublic Health /\nEpidemiology\nBioche-\nmistry\nNephro-\nlogy\nCritical\ncare\nmedicine\nVeterinary\nmedicine\nMedical\nEthics\nPhysical \nMedicine \nand \nRehabilitation\nHealth \ninformatics\nFamily \nmedicine\nMindfulness\n2.5\n3\n3.5\n4\n4.5\n5\n5.5\n6\nlog_values\nFigure 3: Overview of the distribution of medical textbook categories of the MTB dataset. We classify\neach book title into one of the 49 manually created categories or \u201dother\u201d using the Claude-1 model.\n3\nMED-FLAMINGO\nTo train a Flamingo model adapted to the medical domain, we leverage the pre-trained OpenFlamingo-\n9B model checkpoint Awadalla et al. (2023), which is a general-domain VLM that was built on top\n4\nPreprint.\nof the frozen language model LLaMA-7B Touvron et al. (2023) and frozen vision encoder CLIP\nViT/L-14 Radford et al.. We perform continued pre-training in the medical domain which results in\nthe model we refer to as Med-Flamingo.\n3.1\nDATA\nWe pre-train Med-Flamingo by jointly training on interleaved image-text data and paired image-text\ndata. As for the interleaved dataset, we created a interleaved dataset from a set of medical textbooks,\nwhich we subsequently refer to as MTB. As for the paired datasets, we used PMC-OA Lin et al.\n(2023).\nMTB\nWe construct a new multimodal dataset from a set of 4 721 textbooks from different medical\nspecialties (see Figure 3). During preprocessing, each book is first converted from PDF to HTML\nwith all tags removed, except the image tags are converted to <image> tokens. We then carry out\ndata cleaning via deduplication and content filtering. Finally, each book with cleaned text and images\nis then chopped into segments for pretraining so that each segment contains at least one image and\nup to 10 images and a maximum length. In total, MTB consists of approximately 0.8M images and\n584M tokens. We use 95% of the data for training and 5% of the data for evaluation during the\npre-training.\nPMC-OA\nWe adopt the PMC-OA dataset Lin et al. (2023) which is a biomedical dataset with 1.6M\nimage-caption pairs collected from PubMedCentral\u2019s OpenAccess subset. We use 1.3M image-caption\npairs for training and 0.16M pairs for evaluation following the public split2.\n3.2\nOBJECTIVES\nWe follow the original Flamingo model approach Alayrac et al., which considers the following\nlanguage modelling problem:\np (y\u2113 | x<\u2113, y<\u2113) =\nL\nY\n\u2113=1\np (y\u2113 | y<\u2113, x<\u2113) ,\nwhere y\u2113 refers to the \u2113-th language token, y<\u2113 to the set of preceding language tokens, and x<\u2113 to\nthe set of preceding visual tokens. As we focus on modelling the medical literature, here we consider\nonly image-text data (i.e., no videos).\nFollowing Alayrac et al., we minimize a joint objective L over paired and interleaved data:\nL = E(x,y)\u223cDp\n\"\n\u2212\nL\nX\n\u2113=1\nlog p (y\u2113 | y<\u2113, x<\u2113)\n#\n+ \u03bb \u00b7 E(x,y)\u223cDi\n\"\n\u2212\nL\nX\n\u2113=1\nlog p (y\u2113 | y<\u2113, x<\u2113)\n#\n,\nwhere Dp and Di stand for the paired and interleaved dataset, respectively. In our case, we use \u03bb = 1.\n3.3\nTRAINING\nWe performed multi-gpu training on a single node with 8x 80GB NVIDIA A100 GPUs. We trained\nthe model using DeepSpeed ZeRO Stage 2: Optimizer states and gradients are sharded across\ndevices. To further reduce memory load, we employed the 8-bit AdamW optimizer as well as the\nmemory-efficient attention implementation of PyTorch 2.0. Med-Flamingo was initialized at the\ncheckpoint of the Open-Flamingo model and then pre-trained for 2700 steps (or 6.75 days in wall\ntime, including the validation steps), using 50 gradient accumulation steps and a per-device batch\nsize of 1, resulting in a total batch size of 400. The model has 1.3B trainable parameters (gated cross\nattention layers and perceiver layers) and roughly 7B frozen parameters (decoder layers and vision\nencoder), which results in a total of 8.3B parameters. Note that this is the same number parameters\nas in the OpenFlamingo-9B model (version 1).\n2https://huggingface.co/datasets/axiong/pmc_oa_beta\n5\nPreprint.\n4\nEVALUATION\n4.1\nAUTOMATIC EVALUATION\nBaselines\nTo compare generative VQA abilities against the literature, we consider different variants\nof the following baselines:\n1. MedVINT Zhang et al. (2023b), a visual instruction-tuned VLM based on Llama. As this\nmodel was not designed to do few-shot learning (e.g. the image information is prepended\nto the overall input), we report two modes for MedVINT: zero-shot and fine-tuned, where\nthe model was fine-tuned on the training split of the VQA dataset. Since the rather small\nVisual-USMLE dataset has no separate training split, we ommit the fine-tuned baseline\nfor that dataset. We used the MedVInT-TD model with PMC-LLaMA and PMC-CLIP\nbackbones.\n2. OpenFlamingo Awadalla et al. (2023), a powerful VLM which was trained on general-\ndomain data, and which served as the base model to train Med-Flamingo. We report both\nzero-shot and few-shot performance. We expect Flamingo-type models to shine in the few-\nshot setting which they are designed for (as already the pre-training task includes multiple\ninterleaved image-text examples).\nEvaluation datasets\nTo evaluate our model and compare it against the baselines, we leverage two\nexisting VQA datasets from the medical domain (VQA-RAD and PathVQA). Upon closer inspection\nof the VQA-RAD dataset, we identified severe data leakage in the official train / test splits, which is\nproblematic given that many recent VLMs fine-tune on the train split. To address this, we created a\ncustom train / test split by seperately splitting images and questions (each 90% / 10%) to ensure that\nno image or question of the train split leaks into the test split. On these datasets, 6 shots were used\nfor few-shot.\nFurthermore, we create Visual USMLE, a challenging multimodal problem set of 618 USMLE-style\nquestions which are not only augmented with images but also with a case vignette and potentially\ntables of laboratory measurements. The Visual USMLE dataset was created by adapting problems\nfrom the Amboss platform (using licenced user access). To make the Visual USMLE problems more\nactionable and useful, we rephrased the problems to be open-ended instead of multiple-choice. This\nmakes the benchmark harder and more realistic, as the models have to come up with differential\ndiagnoses and potential procedures completely on their own\u2014as opposed to selecting the most\nreasonable answer choice from few choices. Figure 8 gives an overview of the broad range of\nspecialties that are covered in the dataset, greatly extending existing medical VQA datasets which\nare narrowly focused on radiology and pathology. For this comparatively small dataset, instead of\ncreating a training split for finetuning, we created a small train split of 10 problems which can be\nused for few-shot prompting. For this dataset (with considerably longer problems and answers), we\nused only 4 shots to fit in the context window.\nEvaluation metrics\nPrevious works in medical vision-language modelling typically focused scoring\nall available answers of a VQA dataset to arrive at a classification accuracy. However, since we are\ninterested in generative VQA (as opposed to post-hoc scoring different potential answers), for sake\nof clinical utility, we employ the following evaluation metrics that directly assess the quality of the\ngenerated answer:\n1. Clinical evaluation score, as rated by three medical doctors (including one board-certified\nradiologist) using a human evaluation app that we developed for this study. More details are\nprovided in Section 4.2.\n2. BERT similarity score (BERT-sim), the F1 BERT score between the generated answer and\nthe correct answer Zhang et al. (2020).\n3. Exact-match, the fraction of generated answers that exactly match (modulo punctuation)\nthe correct answer. This metric is rather noisy and conservative as useful answers may not\nlexically match the correct answer.\n6\nPreprint.\n4.2\nHUMAN EVALUATION\nWe implemented a human evaluation app using Streamlit to visually display the generative VQA\nproblems for clinical experts to rate the quality of the generated answers with scores from 0 to 10.\nFigure 4 shows an examplary view of the app. For each VQA problem, the raters are provided with\nthe image, the question, the correct answer, and a set of blinded generations (e.g., appearing as\n\u201dprediction 1\u201d in Figure 4), that appear in randomized order.\nHuman Evalua*on App \nProblem 1/50\nImage:\nQuality score from 0 to 10\nFigure 4: Illustration of our Human evaluation app that we created for clinical experts to evaluate\ngenerated answers.\n4.3\nDEDUPLICATION AND LEAKAGE\nDuring the evaluation of the Med-Flamingo model, we were concerned that there may be leakage\nbetween the pre-training datasets (PMC-OA and MTB) and the down-stream VQA datasets used\nfor evaluation; this could inflate judgements of model quality, as the model could memorize image-\nquestion-answer triples.\nTo alleviate this concern, we performed data deduplication based upon pairwise similarity between\nimages from our pre-training datasets and the images from our evaluation benchmarks. To detect\nsimilar images, in spite of perturbations due to cropping, color shifts, size, etc, we embedded the\nimages using Google\u2019s Vision Transformer, preserving the last hidden state as the resultant embedding\nDosovitskiy et al. (2021). We then found the k-nearest neighbors to each evaluation image from\namongst the pre-training images (using the FAISS library) Johnson et al. (2019). We then sorted and\nvisualized image-image pairs by least euclidean distance; we found that images might be duplicates\nuntil a pairwise distance of around 80; beyond this point, there were no duplicates.\nThis process revealed that the pretraining datasets leaked into the PVQA evaluation benchmark. Out\nof 6700 total images in PVQA test set, we judged 194 to be highly similar to images in the pretraining\ndatasets, and thus, we removed them from our down-stream evaluation.\n5\nRESULTS\nIn our experiments, we focus on generative medical visual question answering (VQA). While\nrecent medical VLMs predominantly performed VQA in a non-generative but rather discriminative\nmanner (i.e., by scoring different answer choices), we believe that this ex-post classification to carry\nless clinical usefulness, than directly generating responses. On the other hand, generative VQA is\nmore challenging to evaluate, as automated metrics suffer from significant limitations as they do not\nfully capture the domain-specific context. Thus, we perform a human evaluation study where clinical\n7\nPreprint.\nQues%on: What do the small white lesions in the \naorta mean? \nRa%onale: The aorta is visible as a circular shape \nventral of the spine. There are mul9ple small \nwhite lesions in the aorta. These lesions are \nindica9ve of calci\ufb01ca9on of the aor9c wall.\nAnswer: Calci\ufb01ca9on of the aor9c wall.\nMul9modal  few-shot  prompt\nQues9on, \nRa9onale, \nAnswer\n< Instruc9on > \nQues%on\nQues9on, \nRa9onale, \nAnswer\nMed-Flamingo\nFigure 5: Multimodal medical few-shot prompting illustrated with an example. Few-shot prompting\nhere allows users to customize the response format, e.g., to provide rationales for the provided\nanswers. In addition, multimodal few-shot prompts potentially offer the ability to include relevant\ncontext retrieved from the medical literature.\nexperts review model generations (blinded) and score them (between 0 and 10) in terms of clinical\nusefulness.\nConventional VQA datasets\nTable 1 shows the results for VQA-RAD, the radiological VQA\ndataset for which we created custom splits to address leakage (see Section4). Med-Flamingo few-shot\nshows strong results, improving the clinical eval score by \u223c 20% over the best baseline. In this\ndataset, the auxiliary metrics are rather aligned with clinical preference. Finetuning the MedVINT\nbaseline did not lead to improved performance on this dataset which may be due to its small size.\nMedVINT zero-shot outperforms the other zero-shot ablations which may be partially attributed to\nits instruction tuning step on PMC-VQA.\nTable 2 shows for the results for Path-VQA, the pathology VQA dataset. Compared to the other\ndatasets, all models overall perform poorer on the Path-VQA dataset in terms of clinical evaluation\nscore. We hypothesize that this has to do with the fact the models are not pre-trained on actual\nVQA-RAD\nClinical eval. score\nBERT-sim\nExact-match\nMedVINT zero-shot\n4.63\n0.628\n0.167\nMedVINT fine-tuned (\u223c 2K samples)\n2.87\n0.611\n0.133\nOpenFlamingo zero-shot\n4.39\n0.490\n0.000\nOpenFlamingo few-shot\n4.69\n0.645\n0.200\nMed-Flamingo zero-shot\n3.82\n0.480\n0.000\nMed-Flamingo few-shot\n5.61\n0.650\n0.200\nTable 1: Performance metrics on the VQA-Rad dataset. Best scores are shown in bold. We put\nemphasis on the clinical evaluation score. BERT-sim may not fully capture the fine-grained medical\ndetails. Exact-match is quite noisy and brittle, but conservative. The fine-tuned baseline did not\nimprove over zero-shot which could be explained by the small dataset size in combination with our\ncustom splits which were created to prevent leakage.\n8\nPreprint.\nPath-VQA\nClinical eval. score\nBERT-sim\nExact-match\nMedVINT zero-shot\n0.13\n0.608\n0.272\nMedVINT fine-tuned (\u223c 20K samples)\n1.23\n0.723\n0.385\nOpenFlamingo zero-shot\n2.16\n0.474\n0.009\nOpenFlamingo few-shot\n2.08\n0.669\n0.288\nMed-Flamingo zero-shot\n1.72\n0.521\n0.120\nMed-Flamingo few-shot\n1.81\n0.678\n0.303\nTable 2: Performance metrics on the PathVQA dataset. Best scores are shown in bold. Across models,\nthis dataset showed lowest clinical performance among all evaluation datasets. This highlights a\nperformance deficit in pathology across models, and demonstrates that previous classification-based\nmetrics severely overestimated the performance of general medical VLMs in this specialty.\nVisual USMLE\nClinical eval. score\nBERT-sim\nMedVINT zero-shot\n0.41\n0.421\nOpenFlamingo zero-shot\n4.31\n0.512\nOpenFlamingo few-shot\n3.39\n0.470\nMed-Flamingo zero-shot\n4.18\n0.473\nMed-Flamingo few-shot\n4.33\n0.431\nTable 3: Performance metrics on the Visual USMLE dataset. Best scores are shown in bold. Due to\nrather lenghty correct answers, the Exact-match metric was not informative as it was constantly 0 on\nthis dataset.\nlarge-scale and fine-grained pathology image datasets, but only on a rather small amount of pathology\nliterature (which may not be enough to achieve strong performance). For instance, Figure 3 shows\nthat only a small fraction of our training data covers pathology. In the automated metrics (BERT-sim\nand exact-match), Med-Flamingo improves upon the OpenFlamingo baseline, however the overall\nquality does not improve (as seen in the clinical evaluation score). MedVINT was fine-tuned on\na sizeable training split which results in strong automated metrics, but did not result in a clinical\nevaluation score that matches any Flamingo variant.\nVisual USMLE\nTable 3 shows the results for the Visual USMLE dataset. Med-Flamingo (few-\nshot) results in the clinically most preferrable generations, whereas OpenFlamingo (zero-shot) is a\nclose runner-up. As the ground truth answers were rather lengthy paragraphs, exact match was not\nan informative metric (constant 0 for all methods). The few-shot prompted models lead to lower\nautomated scores than their zero-shot counterparts, which we hypothesize has to do with the fact that\nthe USMLE problems are long (long vignettes as well as long answers) which forced us to summarize\nthe questions and answers when designing few-shot prompts (for which we used GPT-4). Hence, it\u2019s\npossible that those prompts lead to short answers that in terms of BERT-sim score may differ more\nfrom the correct answer than a more wordy zero-shot generation.\nAcross datasets\nOverall, we find that Med-Flamingo\u2019s multimodal in-domain few-shot learning\nabilities lead to favorable generative VQA performance, leading to the lowest average rank of\n1.67 in terms of clinical evaluation score as averaged across all evaluation datasets. As runner-up,\nOpenFlamingo zero-shot achieves a rank of 2.33.\nQualitative analysis\nFinally, we showcase few examples of Med-Flamingo generations in more\ndetail in Figures 1,5, and 6. Figure 5 exemplifies that a medical few-shot learner like Med-Flamingo\ncan be prompted to generate rationale for its VQA answer. The shown example is impressive in that\nthe rationale is visually guiding the reader towards the object of interest (calcification of the aortic\nwall). We note, however, that at this stage, few-shot multimodal prompted rationales may not be\nrobust, especially when a model arrives at a wrong answer.\nFigures 1 and 6 showcase two example problems from the Visual USMLE dataset. The problem\ndescriptions were slightly rephrased and summarized using GPT-4 for display. In Figure 6, Med-\nFlamingo generates the correct answer while not mentioning the underlying diagnosis (urothelial\n9\nPreprint.\nA 60-year-old man presents to the physician with a 1-week history of lower back pain. \nNotably, he has experienced painless hematuria on several occasions over the past 2 \nmonths. During the physical examination, localized tenderness is identified over the lumbar \nspine. Further investigations, including a CT scan, reveal multiple osteolytic lesions in the \nlumbar vertebrae, while cystoscopy detects a 4-cm mass in the right lateral wall of the \nbladder. Additionally, a photomicrograph of a biopsy specimen is provided.\nAnswer: The strongest risk factor for \nthis patient's condition is smoking.\nMed-Flamingo\nQuestion: What represents the most significant \nrisk factor for this patient's condition?\nAnswer: The patient has a diagnosis of metastatic \nprostate cancer.\nBaseline\n\u274c   Wrong diagnosis\n\u274c   No risk factor provided\n\u2705   Correct diagnosis\n\u2705   Risk factor provided\nMicroscopic image of urothelial cancer (models cannot see this cap5on)\nFigure 6: Example of a Visual USMLE problem.\ncancer) as it was not asked for. By contrast, we observed baselines to directly diagnose the patient\n(instead of answering the actual question in a targeted way). The problem in Figure 1 illustrates\nthat Med-Flamingo has the ability to integrate complex medical history information together with\nvisual information to synthesize a comprehensive diagnosis that draws from the information of both\nmodalities.\n6\nDISCUSSION\nIn this paper, we presented Med-Flamingo, the first medically adapted multimodal few-shot learner.\nWhile this is an early proof-of-concept for a medical multimodal few-shot learner, we expect to see\nsignificant improvements with increased model and data scale, more thoroughly cleaned data, as well\nas with alignment to human preference via instruction tuning or explicit optimization for preferences.\nWe expect that the rise of multimodal medical few-shot learners will lead to exciting opportunities\nwith regard to model explainability (via rationale generation) as well as grounding the model in\nverified sources (via multimodal retrieval to augment the few-shot prompt). Thereby, our work serves\nas a first step towards more generalist medical AI models Moor et al. (2023).\nLimitations\nThis work demonstrates a proof-of-concept. As such, Med-Flamingo is not intended\nnor safe for clinical use. In all VLMs we analyzed, hallucinations were observed. Furthermore, as\nMed-Flamingo is a pre-trained model without further instruction or preference tuning, it is possible\nthat the model occasionally outputs low-quality generations.\nFuture work\nIt will be an exciting route for future work to further train Med-Flamingo on clinical\ndata, high-resolution medical image datasets as well as 3D volumes and medical videos. While\ncurrent general-purpose medical VLMs are pre-trained on the broad medical literature (i.e., they\nare only \u201cbook-smart\u201d), also learning from diverse patient data directly will become crucial for\ndown-stream applications.\nACKNOWLEDGMENTS\nWe thank Rok Sosi\u02c7c for his technical support in the data preprocessing.\n10\nPreprint.\nREFERENCES\nArmen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal,\nDmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, and Luke Zettlemoyer. CM3: A\ncausal masked multimodal model of the internet.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford,\nSerkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick,\nSebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski,\nRicardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual\nlanguage model for few-shot learning. In Advances in Neural Information Processing Systems.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language\nmodel for few-shot learning. Advances in Neural Information Processing Systems, 35:23716\u2013\n23736, 2022.\nAnas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani\nMarathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel\nIlharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo, March 2023.\nElliot Bolton, David Hall, Michihiro Yasunaga, Tony Lee, Chris Manning, and Percy Liang.\nBioMedLM: a domain-specific large language model for biomedical text.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportuni-\nties and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,\nJeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\nBenjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei. Language models are few-shot learners. In Advances in Neural Information\nProcessing Systems, volume 33, pp. 1877\u20131901.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,\nJeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott\nGray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle,\nMarc\u2019Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances\nin Neural Information Processing Systems 33: Annual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,\nand Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale.\nICLR, 2021.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann,\nJianfeng Gao, and Hoifung Poon. Domain-specific language model pretraining for biomedical\nnatural language processing. ACM Transactions on Computing for Healthcare (HEALTH), 3(1):\n1\u201323, 2021.\nKexin Huang, Jaan Altosaar, and Rajesh Ranganath. Clinicalbert: Modeling clinical notes and\npredicting hospital readmission. arXiv preprint arXiv:1904.05342, 2019.\nJeff Johnson, Matthijs Douze, and Herv\u00b4e J\u00b4egou. Billion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535\u2013547, 2019.\n11\nPreprint.\nDani Kiyasseh, Runzhuo Ma, Taseen F Haque, Brian J Miles, Christian Wagner, Daniel A Donoho,\nAnimashree Anandkumar, and Andrew J Hung. A vision transformer for decoding surgeon activity\nfrom surgical videos. Nature Biomedical Engineering, pp. 1\u201317, 2023.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo\nKang. Biobert: a pre-trained biomedical language representation model for biomedical text mining.\nBioinformatics, 36(4):1234\u20131240, 2020.\nChunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan\nNaumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training a large language-and-vision\nassistant for biomedicine in one day. arXiv preprint arXiv:2306.00890, 2023.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language\nmodels. arXiv preprint arXiv:2211.09110, 2022.\nWeixiong Lin, Ziheng Zhao, Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, and Weidi Xie.\nPmc-clip: Contrastive language-image pre-training using biomedical documents. arXiv preprint\narXiv:2303.07240, 2023.\nRenqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu.\nBiogpt: generative pre-trained transformer for biomedical text generation and mining. Briefings in\nBioinformatics, 23(6):bbac409, 2022.\nMichael Moor, Oishi Banerjee, Zahra Shakeri Hossein Abad, Harlan M Krumholz, Jure Leskovec,\nEric J Topol, and Pranav Rajpurkar. Foundation models for generalist medical artificial intelligence.\nNature, 616(7956):259\u2013265, 2023.\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi\nYang. Is chatgpt a general-purpose natural language processing task solver?\narXiv preprint\narXiv:2302.06476, 2023.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.\nLearning transferable visual models from natural language supervision. In Proceedings of the 38th\nInternational Conference on Machine Learning, pp. 8748\u20138763. ISSN: 2640-3498.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,\nand Ilya Sutskever. Zero-shot text-to-image generation. In Proceedings of the 38th International\nConference on Machine Learning, pp. 8821\u20138831. ISSN: 2640-3498.\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan\nScales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul\nGamble, Chris Kelly, Nathaneal Scharli, Aakanksha Chowdhery, Philip Mansfield, Blaise Aguera y\nArcas, Dale Webster, Greg S. Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad\nTomasev, Yun Liu, Alvin Rajkomar, Joelle Barral, Christopher Semturs, Alan Karthikesalingam,\nand Vivek Natarajan. Large language models encode clinical knowledge.\nEthan Steinberg, Ken Jung, Jason A Fries, Conor K Corbin, Stephen R Pfohl, and Nigam H Shah.\nLanguage models are an effective representation learning technique for electronic health record\ndata. Journal of biomedical informatics, 113:103637, 2021.\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. Vl-bert: Pre-training\nof generic visual-linguistic representations. arXiv preprint arXiv:1908.08530, 2019.\nEkin Tiu, Ellie Talius, Pujan Patel, Curtis P Langlotz, Andrew Y Ng, and Pranav Rajpurkar. Expert-\nlevel detection of pathologies from unannotated chest x-ray images via self-supervised learning.\nNature Biomedical Engineering, 6(12):1399\u20131406, 2022.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n12\nPreprint.\nMichihiro Yasunaga, Antoine Bosselut, Hongyu Ren, Xikun Zhang, Christopher D. Manning, Percy\nLiang, and Jure Leskovec. Deep bidirectional language-knowledge graph pretraining. In Advances\nin Neural Information Processing Systems, a.\nMichihiro Yasunaga, Jure Leskovec, and Percy Liang. LinkBERT: Pretraining language models with\ndocument links. In Proceedings of the 60th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pp. 8003\u20138016. Association for Computational Linguistics,\nb. doi: 10.18653/v1/2022.acl-long.551.\nMichihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike\nLewis, Luke Zettlemoyer, and Wen-tau Yih. Retrieval-augmented multimodal language modeling.\n2023.\nSheng Zhang, Yanbo Xu, Naoto Usuyama, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh Rao,\nMu Wei, Naveen Valluri, Cliff Wong, et al. Large-scale domain-specific pretraining for biomedical\nvision-language processing. arXiv preprint arXiv:2303.00915, 2023a.\nTianyi Zhang, Varsha Kishore*, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi.\nBertscore:\nEvaluating text generation with bert. In International Conference on Learning Representations,\n2020.\nXiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi\nXie. Pmc-vqa: Visual instruction tuning for medical visual question answering. arXiv preprint\narXiv:2305.10415, 2023b.\n13\nPreprint.\nA\nAPPENDIX\nA.1\nADDITIONAL DETAILS FOR MTB DATASET\nClustering the images\nIn a post-hoc analysis, we clustered the image embeddings of the MTB\ndataset into a large number of clusters (100) and manually reviewed examples of each cluster to\nassign an annotation. We discard noisy or unclear clusters and display the remaining clusters and\ntheir frequency in Figure 7.\nFigure 7: Distribution of manually annotated image clusters in the MTB dataset.\nClassification of book titles\nHere, we provide further details about the creation of Figure 3. Table 4\nlists the categories used to prompt the Claude-1 model to classify each book title. We initially\nprompted with 3 more very rare categories (Geriatrics, Occupational medicine, Space medicine), but\nmerge them into the \u201dOther\u201d group for visualization purposes.\nA.2\nADDITIONAL DETAILS FOR VISUAL USMLE DATASET\n14\nPreprint.\nNeuroscience/Neurology\nObstetrics and Gynecology\nInfectious Diseases\nRadiology\nDermatology\nFamily medicine\nOncology\nImmunology\nBiomedical engineering\nSurgery\nDentistry / Orthodontics\nAnesthesiology\nCardiology\nOphthalmology\nPhysiology\nPsychiatry\nPediatrics\nMedical history\nPharmacology\nPathology\nNursing\nHerbal medicine\nAnatomy\nOtolaryngology\nOrthopedics\nGastroenterology\nHematology\nNutrition\nEndocrinology\nUrology\nInternal Medicine\nGenetics\nPulmonology\nSports Medicine\nMedical Research and Statis-\ntics\nEmergency Medicine\nCell Biology and Histology\nPain medicine\nPublic Health and Epidemiol-\nogy\nForensics\nBiochemistry\nNephrology\nCritical care medicine\nMedical Ethics\nVeterinary medicine\nPhysical Medicine and Reha-\nbilitation\nHealth informatics\nMindfulness\nOther\nTable 4: List of 49 Categories (and \u201dOther\u201d) used for visualing the MTB dataset in Figure 3\n \nDermatology (87)\nCardiology (77)\nPulmonology (48)\nNeuroscience /\nNeurology (43)\nPediatrics (38)\nGastro-\nenterology (38)\nPathology (33)\nInternal Medicine (25)\nOrthopedics (24)\nInfectious\nDiseases (24)\nHematology (23)\nObstetrics and\nGynecology (23)\nNephrology (19)\nOphthalmology (19)\nPharmacology (15)\nMicro-\nbiology (12)\nRadiology (10)\nOtolaryn-\ngology (7)\nGenetics (7)\nEmergency\nMedicine (7)\nEndocrino-\nlogy (6)\nMedical\nResearch\nand\nStatistics (5)\nBioche-\nmistry (5)\nSurgery (4)\nUrology (4)\nImmunology (4)\nCell Biology\nand\nHistology (3)\nPublic Health /\nEpidemiology (2)\nDentistry /\nOrthodontics (1)\nPhysiology (1)\nAnesthesio-\nlogy (1)\nHematology  (1)\nOphthalmology  (1)\nPsychiatry (1)\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\nlog_values\nFigure 8: Distribution of specialty topics in the Visual USMLE dataset, as classified by Claude-1\nusing the categories provided in Table 4.\n15\n"
  },
  {
    "title": "PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization",
    "link": "https://arxiv.org/pdf/2307.15199.pdf",
    "upvote": "10",
    "text": "PromptStyler: Prompt-driven Style Generation\nfor Source-free Domain Generalization\nJunhyeong Cho1\nGilhyun Nam1\nSungyeon Kim2\nHunmin Yang1,3\nSuha Kwak2\n1ADD\n2POSTECH\n3KAIST\nhttps://PromptStyler.github.io\nAbstract\nIn a joint vision-language space, a text feature (e.g., from\n\u201ca photo of a dog\u201d) could effectively represent its relevant im-\nage features (e.g., from dog photos). Also, a recent study has\ndemonstrated the cross-modal transferability phenomenon\nof this joint space. From these observations, we propose\nPromptStyler which simulates various distribution shifts in\nthe joint space by synthesizing diverse styles via prompts\nwithout using any images to deal with source-free domain\ngeneralization. The proposed method learns to generate a\nvariety of style features (from \u201ca S\u2217 style of a\u201d) via learn-\nable style word vectors for pseudo-words S\u2217. To ensure that\nlearned styles do not distort content information, we force\nstyle-content features (from \u201ca S\u2217 style of a [class]\u201d) to be\nlocated nearby their corresponding content features (from\n\u201c[class]\u201d) in the joint vision-language space. After learning\nstyle word vectors, we train a linear classifier using synthe-\nsized style-content features. PromptStyler achieves the state\nof the art on PACS, VLCS, OfficeHome and DomainNet, even\nthough it does not require any images for training.\n1. Introduction\nDeep neural networks are usually trained with the assump-\ntion that training and test data are independent and identically\ndistributed, which makes them vulnerable to substantial dis-\ntribution shifts between training and test data [23,52]. This\nsusceptibility is considered as one of the major obstacles to\ntheir deployment in real-world applications. To enhance their\nrobustness to such distribution shifts, Domain Adaptation\n(DA) [2,24,32,33,54,56,57,68] has been studied; it aims\nat adapting neural networks to a target domain using target\ndomain data available in training. However, such a target\ndomain is often latent in common training scenarios, which\nconsiderably limits the application of DA. Recently, a body\nof research has addressed this limitation by Domain Gener-\nalization (DG) [3,5,21,29,35,37,74] that aims to improve\nmodel\u2019s generalization capability to any unseen domains. It\nhas been a common practice in DG to utilize multiple source\ndomains for learning domain-invariant features [61,69], but\n(a)\nImage Feature          Text Feature         Decision Boundary \n(b)\nJoint Vision\nLanguage Space\nJoint Vision\nLanguage Space\nText Feature          Decision Boundary \n\u201ca S1 style of a cat\u201d\n\u201ca S3 style of a cat\u201d\n\u201ca S2 style of a cat\u201d\n\u201ca S3 style of a dog\u201d\n\u201ca S2 style of a dog\u201d\n\u201ca S1 style of a dog\u201d\n\u201ca photo of a dog\u201d\n\u201ca photo of a cat\u201d\n\u201ca cartoon of a cat\u201d\n\u201ca cartoon of a dog\u201d\nFigure 1: Motivation of our method. (a) Text features could\neffectively represent various image styles in a joint vision-\nlanguage space. (b) PromptStyler synthesizes diverse styles\nin a joint vision-language space via learnable style word\nvectors for pseudo-words S\u2217 without using any images.\nit is unclear which source domains are ideal for DG, since\narbitrary unseen domains should be addressed. Furthermore,\nit is costly and sometimes even infeasible to collect and\nannotate large-scale multi-source domain data for training.\nWe notice that a large-scale pre-trained model might have\nalready observed a great variety of domains and thus can be\nused as an efficient proxy of actual multiple source domains.\nFrom this perspective, we raised a question \u201cCould we fur-\nther improve model\u2019s generalization capability by simulating\nvarious distribution shifts in the latent space of such a large-\nscale model without using any source domain data?\u201d If this\n1\narXiv:2307.15199v2  [cs.CV]  15 Aug 2023\n\u201ca S1 style of a cat\u201d\n\u201ca S1 style of a fox\u201d\n\u201ca S3 style of a\u201d \u201ca S4 style of a\u201d\n\u201ca S2 style of a\u201d \u201ca S1 style of a\u201d\n\u201ca S5 style of a\u201d\nStyle Diversity\n\u201ca S3 style of a\u201d\n\u201ca S4 style of a\u201d\n\u201ca S2 style of a\u201d\n\u201ca S1 style of a\u201d\n\u201ca S5 style of a\u201d\nText Feature    \n+ \u2112style\nContent Consistency\nText Feature    \n\u201ca S1 style of a fox\u201d\n\u201ca S1 style of a dog\u201d\n\u201cdog\u201d\n\u201cfox\u201d\n\u201ca S1 style of a dog\u201d\n\u201ccat\u201d\n\u201ca S1 style of a cat\u201d\n\u201cdog\u201d\n\u201cfox\u201d\n\u201ccat\u201d\n+ \u2112content\nFigure 2: Important factors in the proposed method. PromptStyler learns style word vectors for pseudo-words S\u2217 which lead\nto diverse style features (from \u201ca S\u2217 style of a\u201d) while preserving content information encoded in style-content features (from\n\u201ca S\u2217 style of a [class]\u201d). Lstyle and Lcontent are the loss functions used for maximizing style diversity and content consistency\nin a hyperspherical joint vision-language space (e.g., CLIP [50] latent space).\nis possible, DG will become immensely practical by effec-\ntively and efficiently exploiting such a large-scale model.\nHowever, this approach is much more challenging since any\nactual data of source and target domains are not accessible\nbut only the target task definition (e.g., class names) is given.\nIn this paper, we argue that large-scale vision-language\nmodels [26, 50, 64] could shed light on this challenging\nsource-free domain generalization. As conceptually illus-\ntrated in Figure 1(a), text features could effectively represent\ntheir relevant image features in a joint vision-language space.\nDespite the modality gap between two modalities in the joint\nspace [39], a recent study has demonstrated the cross-modal\ntransferability phenomenon [67]; we could train a classifier\nusing text features while running an inference with the classi-\nfier using image features. This training procedure meets the\nnecessary condition for the source-free domain generaliza-\ntion, i.e., source domain images are not required. Using such\na joint vision-language space, we could simulate various\ndistribution shifts via prompts without any images.\nWe propose a prompt-driven style generation method,\ndubbed PromptStyler, which synthesizes diverse styles via\nlearnable word vectors to simulate distribution shifts in a\nhyperspherical joint vision-language space. PromptStyler is\nmotivated by the observation that a shared style of images\ncould characterize a domain [27,74] and such a shared style\ncould be captured by a learnable word vector for a pseudo-\nword S\u2217 using CLIP [50] with a prompt (\u201ca painting in the\nstyle of S\u2217\u201d) [17]. As shown in Figure 1(b), our method\nlearns a style word vector for S\u2217 to represent each style.\nTo effectively simulate various distribution shifts, we try\nto maximize style diversity as illustrated in Figure 2. Specifi-\ncally, our method encourages learnable style word vectors to\nresult in orthogonal style features in the hyperspherical space,\nwhere each style feature is obtained from a style prompt\n(\u201ca S\u2217 style of a\u201d) via a pre-trained text encoder. To prevent\nlearned styles from distorting content information, we also\nconsider content consistency as illustrated in Figure 2. Each\nstyle-content feature obtained from a style-content prompt\n(\u201ca S\u2217 style of a [class]\u201d) is forced to be located closer to\nits corresponding content feature obtained from a content\nprompt (\u201c[class]\u201d) than the other content features.\nLearned style word vectors are used to synthesize style-\ncontent features for training a classifier; these synthesized\nfeatures could simulate images of known contents with di-\nverse unknown styles in the joint space. These style-content\nfeatures are fed as input to a linear classifier which is trained\nby a classification loss using contents (\u201c[class]\u201d) as their\nclass labels. At inference time, an image encoder extracts\nimage features from input images, which are fed as input to\nthe trained classifier. Note that the text and image encoders\nare derived from the same pre-trained vision-language model\n(e.g., CLIP [50]); the text encoder is only involved in training\nand the image encoder is only involved at inference time.\nThe proposed method achieves state-of-the-art results\non PACS [34], VLCS [15], OfficeHome [60] and Domain-\nNet [48] without using any actual data of source and target\ndomains. It takes just \u223c30 minutes for the entire training us-\ning a single RTX 3090 GPU, and our model is \u223c2.6\u00d7 smaller\nand \u223c243\u00d7 faster at inference compared with CLIP [50].\nOur contributions are summarized as follows:\n\u2022 This work is the first attempt to synthesize a variety of\nstyles in a joint vision-language space via prompts to\neffectively tackle source-free domain generalization.\n\u2022 This paper proposes a novel method that effectively sim-\nulates images of known contents with diverse unknown\nstyles in a joint vision-language space.\n\u2022 PromptStyler achieves the state of the art on domain\ngeneralization benchmarks without using any images.\n2\nSetup\nSource\nTarget\nTask Definition\nDA\n\u2713\n\u2713\n\u2713\nDG\n\u2713\n\u2013\n\u2713\nSource-free DA\n\u2013\n\u2713\n\u2713\nSource-free DG\n\u2013\n\u2013\n\u2713\nTable 1: Different requirements in each setup. Source-free\nDG only assumes the task definition (i.e., what should be\npredicted) without requiring source and target domain data.\n2. Related Work\nDomain Generalization. Model\u2019s generalization capability\nto arbitrary unseen domains is the key factor to successful\ndeployment of neural networks in real-world applications,\nsince substantial distribution shifts between source and target\ndomains could significantly degrade their performance [23,\n52]. To this end, Domain Generalization (DG) [4,5,10,16,\n21, 29, 35, 37, 44, 45, 61, 69] has been studied. It assumes\ntarget domain data are not accessible while using data from\nsource domains. Generally speaking, existing DG methods\ncould be divided into two categories: multi-source DG [3,\n12, 36, 42, 43, 51, 55, 63, 73, 74] and single-source DG [14,\n38,49,62]. Mostly, multi-source DG methods aim to learn\ndomain-invariant features by exploiting available multiple\nsource domains, and single-source DG methods also aim to\nlearn such features by generating diverse domains based on a\nsingle domain and then exploiting the synthesized domains.\nSource-free Domain Generalization. In this setup, we are\nnot able to access any source and target domains as summa-\nrized in Table 1. Thus, source-free DG is much more chal-\nlenging than multi-source and single-source DG. From the\nobservation that synthesizing new domains from the given\nsource domain could effectively improve model\u2019s generaliza-\ntion capability [27,38,62,72,73], we also try to generate di-\nverse domains but without using any source domains to deal\nwith source-free DG. By leveraging a large-scale pre-trained\nmodel which has already seen a great variety of domains,\nour method could simulate various distribution shifts in the\nlatent space of the large-scale model. This approach has sev-\neral advantages compared with existing DG methods; source\ndomain images are not required and there is no concern for\ncatastrophic forgetting which might impede model\u2019s gener-\nalization capability. Also, it would be immensely practical\nto exploit such a large-scale model for downstream visual\nrecognition tasks, since we only need the task definition.\nLarge-scale model in Domain Generalization. Recently,\nseveral DG methods [5,53] exploit a large-scale pre-trained\nmodel (e.g., CLIP [50]) to leverage its great generalization\ncapability. While training neural networks on available data,\nCAD [53] and MIRO [5] try to learn robust features using\nsuch a large-scale model. Compared with them, the proposed\nmethod could learn domain-invariant features using a large-\nscale pre-trained model without requiring any actual data.\nJoint vision-language space. Large-scale vision-language\nmodels [26,50,64] are trained with a great amount of image-\ntext pairs, and achieve state-of-the-art results on downstream\nvisual recognition tasks [20,41,66,70,71]. By leveraging\ntheir joint vision-language spaces, we could also effectively\nmanipulate visual features via prompts [13,18,31,47]. Inter-\nestingly, Textual Inversion [17] shows that a learnable style\nword vector for a pseudo-word S\u2217 could capture a shared\nstyle of images using CLIP [50] with a prompt (\u201ca painting\nin the style of S\u2217\u201d). From this observation, we argue that\nlearnable style word vectors would be able to seek a variety\nof styles for simulating various distribution shifts in a joint\nvision-language space without using any images.\n3. Method\nThe overall framework of the proposed method is shown\nin Figure 3, and pseudo-code of PromptStyler is described\nin Algorithm 1. Our method learns style word vectors to\nrepresent a variety of styles in a hyperspherical joint vision-\nlanguage space (e.g., CLIP [50] latent space). After learning\nthose style word vectors, we train a linear classifier using\nsynthesized style-content features produced by a pre-trained\ntext encoder T(\u00b7). At inference time, a pre-trained image en-\ncoder I(\u00b7) extracts image features from input images, which\nare fed as input to the trained linear classifier. Thanks to the\ncross-modal transferability phenomenon of the joint vision-\nlanguage space [67], this classifier could produce class scores\nusing the image features. Note that we exploit CLIP as our\nlarge-scale vision-language model; its image encoder and\ntext encoder are frozen in our entire framework.\n3.1. Prompt-driven style generation\nAn input text prompt is converted to several tokens via\na tokenization process, and then such tokens are replaced\nby their corresponding word vectors via a word lookup\nprocess. In PromptStyler, a pseudo-word Si in a prompt\nis a placeholder which is replaced by a style word vector\nsi \u2208 RD during the word lookup process. Note that three\nkinds of prompts are used in the proposed method: a style\nprompt P style\ni\n(\u201ca Si style of a\u201d), a content prompt P content\nm\n(\u201c[class]m\u201d), and a style-content prompt P style\ni\n\u25e6 P content\nm\n(\u201ca Si style of a [class]m\u201d). Si indicates the placeholder for\ni-th style word vector and [class]m denotes m-th class name.\nSuppose we want to generate K different styles in a joint\nvision-language space. In this case, the proposed method\nneeds to learn K style word vectors {si}K\ni=1, where each si\nis randomly initialized at the beginning. To effectively sim-\nulate various distribution shifts in the joint vision-language\nspace, those style word vectors need to be diverse while not\ndistorting content information when they are exploited in\nstyle-content prompts. There are two possible design choices\nfor learning such word vectors: (1) learning each style word\nvector si in a sequential manner, or (2) learning all style\n3\n\u201ca S1 style of a fox\u201d\n\u201ca S3 style of a fox\u201d\n\u201ca S3 style of a cat\u201d\n\u201ca S1 style of a dog\u201d\n\u201ca S2 style of a dog\u201d\n\u201ca S3 style of a dog\u201d\n\u201ca S2 style of a fox\u201d\nText\nEncoder\nT(!)\n\u201ca S1 style of a dog\u201d\nImage\nEncoder\nI(!)\nLinear\nLayer\nLinear\nLayer\nStyle\nContent\n(ii) Training a linear classifier using diverse styles\n(iii) Inference using the trained classifier\n(i) Prompt-driven style generation\n\u201ca S1 style of a cat\u201d\n\u201cdog\u201d\n\u201cfox\u201d\n\u201ccat\u201d\n\u2112style + \u2112content\n\u201ca S2 style of a cat\u201d\nClass Scores\ndog\nClass Scores\ndog\n(class label)\nFrozen weights\nFrozen weights\nnormalization layer\n\u2113\n2\nnormalization layer\n\u2113\n2\ncat\nfox\ncat\nfox\nFigure 3: PromptStyler learns diverse style word vectors which do not distort content information of style-content prompts.\nAfter learning style word vectors, we synthesize style-content features (e.g., from \u201ca S1 style of a dog\u201d) via a pre-trained text\nencoder for training a linear classifier. The classifier is trained by a classification loss using those synthesized features and their\ncorresponding class labels (e.g., \u201cdog\u201d). At inference time, a pre-trained image encoder extracts image features, which are fed\nas input to the trained classifier. Note that the encoders are derived from the same vision-language model (e.g., CLIP [50]).\nword vectors {si}K\ni=1 in a parallel manner. We choose the\nformer, since it takes much less memory during training.\nPlease refer to the supplementary material (Section A.2) for\nthe empirical justification of our design choice.\nStyle diversity loss. To maximize the diversity of K styles in\na hyperspherical joint vision-language space, we sequentially\nlearn style word vectors {si}K\ni=1 in such a way that i-th style\nfeature T(P style\ni\n) \u2208 RC produced by i-th style word vector\nsi is orthogonal to {T(P style\nj\n)}i\u22121\nj=1 produced by previously\nlearned style word vectors {sj}i\u22121\nj=1. Regarding this, the style\ndiversity loss Lstyle for learning i-th style word vector si is\ncomputed by\nLstyle =\n1\ni \u2212 1\ni\u22121\nX\nj=1\n\f\f\f\f\f\nT(P style\ni\n)\n\u2225T(P style\ni\n)\u22252\n\u2022\nT(P style\nj\n)\n\u2225T(P style\nj\n)\u22252\n\f\f\f\f\f . (1)\nThis style loss Lstyle aims to minimize the absolute value\nof the cosine similarity between i-th style feature and each\nof the existing style features. When the value of this loss\nbecomes zero, it satisfies the orthogonality between i-th style\nfeature and all the existing style features.\nContent consistency loss. Learning the style word vectors\n{si}K\ni=1 only using the style diversity loss sometimes leads\nto undesirable outcome, since a learned style si could sub-\nstantially distort content information when used to generate\na style-content feature T(P style\ni\n\u25e6 P content\nm\n) \u2208 RC. To alle-\nviate this problem, we encourage the content information in\nthe style-content feature to be consistent with its correspond-\ning content feature T(P content\nm\n) \u2208 RC while learning each\ni-th style word vector si. Specifically, each style-content\nfeature synthesized via i-th style word vector si should have\nthe highest cosine similarity score with its corresponding\ncontent feature. For i-th style word vector si, a cosine simi-\nlarity score zimn between a style-content feature with m-th\nclass name and a content feature with n-th class name is\ncomputed by\nzimn =\nT(P style\ni\n\u25e6 P content\nm\n)\n\u2225T(P style\ni\n\u25e6 P content\nm\n)\u22252\n\u2022\nT(P content\nn\n)\n\u2225T(P content\nn\n)\u22252\n. (2)\nUsing cosine similarity scores between style-content features\nand content features, the content consistency loss Lcontent\nfor learning i-th style word vector si is computed by\nLcontent = \u2212 1\nN\nN\nX\nm=1\nlog\n \nexp(zimm)\nPN\nn=1 exp(zimn)\n!\n,\n(3)\nwhere N denotes the number of classes pre-defined in the\ntarget task. This content loss Lcontent is a contrastive loss\nwhich encourages each style-content feature to be located\ncloser to its corresponding content feature so that it forces\neach i-th style word vector si to preserve content information\nwhen used to synthesize style-content features.\nTotal prompt loss. PromptStyler learns K style word vec-\ntors {si}K\ni=1 in a sequential manner, where each i-th style\nword vector si is learned using both Lstyle (Eq. (1)) and\nLcontent (Eq. (3)). In the proposed method, the total loss\nLprompt for learning i-th style word vector is computed by\nLprompt = Lstyle + Lcontent .\n(4)\nUsing this prompt loss Lprompt, we train i-th style word\nvector si for L training iterations.\n4\nAlgorithm 1 PromptStyler\nRequirement: pre-trained text encoder T(\u00b7), pre-defined N\nclass names in the target task\nInput: number of style word vectors K, number of training\niterations L\nOutput: KN style-content features\n# randomly initialize style word vectors\n1: {si}K\ni=1 \u2190 random initialize({si}K\ni=1)\n# sequentially learn K style word vectors\n2: for i = 1, 2, . . . , K do\n# L training iterations for learning each word vector\n3:\nfor iteration = 1, 2, . . . , L do\n# compute Lstyle using T(\u00b7) and word vectors\n4:\nLstyle \u2190 style diversity loss(si, {sj}i\u22121\nj=1)\n# compute Lcontent using T(\u00b7) and a word vector\n5:\nLcontent \u2190 content consistency loss(si)\n6:\nLprompt \u2190 Lstyle + Lcontent\n7:\nUpdate si using Lprompt by gradient descent\n8:\nend for\n9: end for\n10: Synthesize KN style-content features using the learned\nK style word vectors and N class names via T(\u00b7)\n3.2. Training a linear classifier using diverse styles\nAfter learning K style word vectors {si}K\ni=1, we generate\nKN style-content features for training a linear classifier. To\nbe specific, we synthesize those features using the learned K\nstyles and pre-defined N classes via the text encoder T(\u00b7).\nThe linear classifier is trained by a classification loss using\n\u21132-normalized style-content features and their class labels;\neach class label is the class name used to generate each style-\ncontent feature. To effectively leverage the hyperspherical\njoint vision-language space, we adopt ArcFace [8] loss as\nour classification loss Lclass. Note that ArcFace loss is an\nangular Softmax loss which computes the cosine similarities\nbetween classifier input features and classifier weights with\nan additive angular margin penalty between classes. This\nangular margin penalty allows for more discriminative pre-\ndictions by pushing features from different classes further\napart. Thanks to the property, this angular Softmax loss has\nbeen widely used in visual recognition tasks [7,9,30,40,65].\n3.3. Inference using the trained classifier\nThe trained classifier is used with a pre-trained image\nencoder I(\u00b7) at inference time. Given an input image x, the\nimage encoder extracts its image feature I(x) \u2208 RC, which\nis mapped to the hyperspherical joint vision-language space\nby \u21132 normalization. Then, the trained classifier produces\nclass scores using the \u21132-normalized image feature. Note\nthat the text encoder T(\u00b7) is not used at inference time, while\nthe image encoder I(\u00b7) is only exploited at inference time.\n4. Experiments\nFor more comprehensive understanding, please refer to\nthe supplementary material (Section B and D).\n4.1. Evaluation datasets\nThe proposed method does not require any actual data\nfor training. To analyze its generalization capability, four\ndomain generalization benchmarks are used for evaluation:\nPACS [34] (4 domains and 7 classes), VLCS [15] (4 do-\nmains and 5 classes), OfficeHome [60] (4 domains and 65\nclasses) and DomainNet [48] (6 domains and 345 classes).\nOn these benchmarks, we repeat each experiment three times\nusing different random seeds and report average top-1 clas-\nsification accuracies with standard errors. Unlike the leave-\none-domain-out cross-validation evaluation protocol [21],\nwe do not exploit any source domain data for training.\n4.2. Implementation details\nPromptStyler is implemented and trained with the same\nconfiguration regardless of the evaluation datasets. Training\ntakes about 30 minutes using a single RTX 3090 GPU.\nArchitecture. We choose CLIP [50] as our large-scale pre-\ntrained vision-language model, and use the publicly available\npre-trained model.1 The text encoder T(\u00b7) used in training\nis Transformer [59] and the image encoder I(\u00b7) used at in-\nference is ResNet-50 [22] as default setting in experiments;\nour method is also implemented with ViT-B/16 [11] or ViT-\nL/14 [11] for further evaluations as shown in Table 2. Note\nthat text and image encoders are derived from the same CLIP\nmodel and frozen in the entire pipeline. The dimension of\neach text feature or image feature is C = 1024 when our\nmethod is implemented with ResNet-50, while C = 512 in\nthe case of ViT-B/16 and C = 768 in the case of ViT-L/14.\nLearning style word vectors. We follow prompt learning\nmethods [70,71] when learning the word vectors. Using a\nzero-mean Gaussian distribution with 0.02 standard devia-\ntion, we randomly initialize K style word vectors {si}K\ni=1,\nwhere K = 80. The dimension of each style word vector is\nD = 512 when the proposed method is implemented with\nResNet-50 [22] or ViT-B/16 [11], while D = 768 in the case\nof ViT-L/14 [11]. Each i-th style word vector si is trained\nby the prompt loss Lprompt for L = 100 training iterations\nusing the SGD optimizer with 0.002 learning rate and 0.9\nmomentum. The number of classes N is pre-defined by each\ntarget task definition, e.g., N = 345 for DomainNet [48].\nTraining a linear classifier. The classifier is trained for 50\nepochs using the SGD optimizer with 0.005 learning rate,\n0.9 momentum, and a batch size of 128. In ArcFace [8] loss,\nits scaling factor is set to 5 with 0.5 angular margin.\nInference. Input images are pre-processed in the same way\nwith the CLIP model; resized to 224 \u00d7 224 and normalized.\n1https://github.com/openai/CLIP\n5\nConfiguration\nAccuracy (%)\nSource\nDomain\nMethod\nDomain\nDescription\nPACS\nVLCS\nOfficeHome\nDomainNet\nAvg.\nResNet-50 [22] with pre-trained weights on ImageNet [6]\nDANN [19]\n\u2713\n\u2013\n83.6\u00b10.4\n78.6\u00b10.4\n65.9\u00b10.6\n38.3\u00b10.1\n66.6\nRSC [25]\n\u2713\n\u2013\n85.2\u00b10.9\n77.1\u00b10.5\n65.5\u00b10.9\n38.9\u00b10.5\n66.7\nMLDG [35]\n\u2713\n\u2013\n84.9\u00b11.0\n77.2\u00b10.4\n66.8\u00b10.6\n41.2\u00b10.1\n67.5\nSagNet [46]\n\u2713\n\u2013\n86.3\u00b10.2\n77.8\u00b10.5\n68.1\u00b10.1\n40.3\u00b10.1\n68.1\nSelfReg [28]\n\u2713\n\u2013\n85.6\u00b10.4\n77.8\u00b10.9\n67.9\u00b10.7\n42.8\u00b10.0\n68.5\nGVRT [44]\n\u2713\n\u2013\n85.1\u00b10.3\n79.0\u00b10.2\n70.1\u00b10.1\n44.1\u00b10.1\n69.6\nMIRO [5]\n\u2713\n\u2013\n85.4\u00b10.4\n79.0\u00b10.0\n70.5\u00b10.4\n44.3\u00b10.2\n69.8\nResNet-50 [22] with pre-trained weights from CLIP [50]\nZS-CLIP (C) [50]\n\u2013\n\u2013\n90.6\u00b10.0\n76.0\u00b10.0\n68.6\u00b10.0\n45.6\u00b10.0\n70.2\nCAD [53]\n\u2713\n\u2013\n90.0\u00b10.6\n81.2\u00b10.6\n70.5\u00b10.3\n45.5\u00b12.1\n71.8\nZS-CLIP (PC) [50]\n\u2013\n\u2713\n90.7\u00b10.0\n80.1\u00b10.0\n72.0\u00b10.0\n46.2\u00b10.0\n72.3\nPromptStyler\n\u2013\n\u2013\n93.2\u00b10.0\n82.3\u00b10.1\n73.6\u00b10.1\n49.5\u00b10.0\n74.7\nViT-B / 16 [11] with pre-trained weights from CLIP [50]\nZS-CLIP (C) [50]\n\u2013\n\u2013\n95.7\u00b10.0\n76.4\u00b10.0\n79.9\u00b10.0\n57.8\u00b10.0\n77.5\nMIRO [5]\n\u2713\n\u2013\n95.6\n82.2\n82.5\n54.0\n78.6\nZS-CLIP (PC) [50]\n\u2013\n\u2713\n96.1\u00b10.0\n82.4\u00b10.0\n82.3\u00b10.0\n57.7\u00b10.0\n79.6\nPromptStyler\n\u2013\n\u2013\n97.2\u00b10.1\n82.9\u00b10.0\n83.6\u00b10.0\n59.4\u00b10.0\n80.8\nViT-L / 14 [11] with pre-trained weights from CLIP [50]\nZS-CLIP (C) [50]\n\u2013\n\u2013\n97.6\u00b10.0\n77.5\u00b10.0\n85.9\u00b10.0\n63.3\u00b10.0\n81.1\nZS-CLIP (PC) [50]\n\u2013\n\u2713\n98.5\u00b10.0\n82.4\u00b10.0\n86.9\u00b10.0\n64.0\u00b10.0\n83.0\nPromptStyler\n\u2013\n\u2013\n98.6\u00b10.0\n82.4\u00b10.2\n89.1\u00b10.0\n65.5\u00b10.0\n83.9\nTable 2: Comparison with the state-of-the-art domain generalization methods. ZS-CLIP (C) denotes zero-shot CLIP using\n\u201c[class]\u201d as its text prompt, and ZS-CLIP (PC) indicates zero-shot CLIP using \u201ca photo of a [class]\u201d as its text prompt. Note\nthat PromptStyler does not exploit any source domain data and domain descriptions.\n4.3. Evaluations\nMain results. PromptStyler achieves the state of the art in ev-\nery evaluation on PACS [34], VLCS [15], OfficeHome [60]\nand DomainNet [48] as shown in Table 2. Note that all exist-\ning methods utilize source domain data except for zero-shot\nCLIP [50] in Table 2. Compared with zero-shot CLIP which\ngenerates each text feature using a domain-agnostic prompt\n(\u201c[class]\u201d), PromptStyler largely outperforms its records in\nall evaluations. Our method also shows higher accuracy com-\npared with zero-shot CLIP which produces each text feature\nusing a domain-specific prompt (\u201ca photo of a [class]\u201d), even\nthough we do not exploit any domain descriptions. These re-\nsults confirm that the proposed method effectively improves\nthe generalization capability of the chosen pre-trained model,\ni.e., CLIP, without using any images by simulating various\ndistribution shifts via prompts in its latent space.\nComputational evaluations. In Table 3, we compare our\nPromptStyler and zero-shot CLIP [50] in terms of the number\nof parameters and inference speed; the inference speed was\nmeasured using a single RTX 3090 GPU with a batch size\nInference Module\nImage\nText\nMethod\nEncoder\nEncoder\n# Params\nFPS\nOfficeHome (65 classes)\nZS-CLIP [50]\n\u2713\n\u2713\n102.0M\n1.6\nPromptStyler\n\u2713\n\u2013\n38.4M\n72.9\nDomainNet (345 classes)\nZS-CLIP [50]\n\u2713\n\u2713\n102.0M\n0.3\nPromptStyler\n\u2713\n\u2013\n38.7M\n72.9\nTable 3: The number of parameters and inference speed on\nOfficeHome [60] and DomainNet [48] using ResNet-50 [22]\nas an image encoder. Note that CLIP [50] text encoder needs\nto generate text features as many as the number of classes.\nof 1. Note that we do not exploit a text encoder at inference\ntime, which makes our model \u223c2.6\u00d7 smaller and \u223c243\u00d7\nfaster compared with CLIP. Regarding the inference speed,\nthe proposed model is about 45\u00d7 faster for the target task\nOfficeHome [60] (65 classes) and it is about 243\u00d7 faster for\nthe target task DomainNet [48] (345 classes).\n6\n(a) \u2112style\n(b) \u2112content\n(c) \u2112style + \u2112content\nFigure 4: t-SNE [58] visualization results for the target task VLCS [15] (5 classes) using synthesized style-content features.\nWe visualize such features obtained from the learned 80 style word vectors {si}80\ni=1 and all the 5 classes (bird, car, chair, dog,\nperson). Different colors denote features obtained from different style word vectors, and different shapes indicate features\nobtained from different class names. We only colorize features from the first 10 styles {si}10\ni=1. Combining the style diversity\nloss Lstyle and content consistency loss Lcontent leads to diverse styles while preserving content information.\nFigure 5: Text-to-Image synthesis results using style-content features (from \u201ca S\u2217 style of a cat\u201d) with 6 different style word\nvectors. By leveraging the proposed method, we could learn a variety of styles while not distorting content information.\nAccuracy (%)\nLstyle\nLcontent\nPACS\nVLCS\nOfficeHome\nDomainNet\nAvg.\n\u2013\n\u2013\n92.6\n78.3\n72.2\n48.0\n72.8\n\u2713\n\u2013\n92.3\n80.9\n71.5\n48.2\n73.2\n\u2013\n\u2713\n92.8\n80.5\n72.4\n48.6\n73.6\n\u2713\n\u2713\n93.2\n82.3\n73.6\n49.5\n74.7\nTable 4: Ablation study on the style diversity loss Lstyle and\ncontent consistency loss Lcontent used in the prompt loss.\nt-SNE visualization results. In Figure 4, we qualitatively\nevaluate style-content features synthesized for the target task\nVLCS [15] (5 classes) using t-SNE [58] visualization. As\nshown in Figure 4(c), PromptStyler generates a variety of\nstyles while not distorting content information; style-content\nfeatures obtained from the same class name share similar\nsemantics with diverse variations. This result confirms that\nwe could effectively simulate various distribution shifts in\nthe latent space of a large-scale vision-language model by\nsynthesizing diverse styles via learnable style word vectors.\nText-to-Image synthesis results. In Figure 5, we visualize\nstyle-content features (from \u201ca S\u2217 style of a cat\u201d) via dif-\nfusers library.2 These results are obtained with 6 different\nstyle word vectors, where the word vectors are learned for\nthe target task DomainNet [48] using ViT-L/14 [11] model.\n2https://github.com/huggingface/diffusers\nAccuracy (%)\nLclass\nPACS\nVLCS\nOfficeHome\nDomainNet\nAvg.\nSoftmax\n92.5\n81.2\n72.3\n48.6\n73.7\nArcFace\n93.2\n82.3\n73.6\n49.5\n74.7\nTable 5: Ablation study on the classification loss Lclass used\nfor training a linear classifier in the proposed framework.\n4.4. More analyses\nAblation study on the prompt loss. In Table 4, we eval-\nuate the effects of Lstyle and Lcontent in Lprompt used for\nlearning style words. Interestingly, our method also achieves\nstate-of-the-art results even without using these losses, i.e.,\nthe proposed framework (Fig. 3) is substantially effective by\nitself. Note that randomly initialized style word vectors are\nalready diverse, and CLIP [50] is already good at extract-\ning correct content information from a style-content prompt\neven without training the word vectors using Lcontent. When\nwe learn style word vectors using Lstyle without Lcontent,\nstyle-content features obtained from different class names\nshare more similar features than those from the same class\nname (Fig. 4(a)). On the other hand, using Lcontent without\nLstyle leads to less diverse style-content features (Fig. 4(b)).\nWhen incorporating both losses, we could generate diverse\nstyles while not distorting content information (Fig. 4(c)).\n7\nFigure 6: Top-1 classification accuracy on the PACS [34], VLCS [15], OfficeHome [60] and DomainNet [48] datasets with\nregard to the number of learnable style word vectors K.\nFigure 7: Top-1 classification accuracy on the PACS [34], VLCS [15], OfficeHome [60] and DomainNet [48] datasets with\nregard to the number of training iterations L for learning each style word vector si.\nConfiguration\nAccuracy (%)\nSource\nDomain\nMethod\nDomain Description\nTerra Incognita\nResNet-50 [22] with pre-trained weights on ImageNet [6]\nSelfReg [28]\n\u2713\n\u2013\n47.0\u00b10.3\nGVRT [44]\n\u2713\n\u2013\n48.0\u00b10.2\nResNet-50 [22] with pre-trained weights from CLIP [50]\nZS-CLIP (C) [50]\n\u2013\n\u2013\n19.5\u00b10.0\nZS-CLIP (PC) [50]\n\u2013\n\u2713\n23.8\u00b10.0\nPromptStyler\n\u2013\n\u2013\n30.5\u00b10.8\nTable 6: Unsatisfactory results obtained from CLIP [50]\nwithout using source domain data from Terra Incognita [1].\nAblation study on the classification loss. In Table 5, we\nevaluate the effects of the original Softmax loss and the\nangular Softmax loss (i.e., ArcFace [8]). PromptStyler also\nachieves the state of the art using the original one, which\nvalidates that the performance improvement of our method\nmainly comes from the proposed framework (Fig. 3). Note\nthat the angular Softmax loss further improves its accuracy\nby leveraging the hyperspherical joint vision-language space.\nEffect of the number of styles. We evaluate our method\nwith regard to the number of style word vectors K as shown\nin Figure 6. Interestingly, our PromptStyler outperforms\nCLIP [50] using just 5 styles. This evaluation shows that 20\nstyle word vectors are enough to achieve decent results.\nEffect of the number of iterations. We evaluate our method\nwith regard to the number of training iterations L for learning\neach style word vector as shown in Figure 7. This evaluation\nshows that 20 iterations are enough to achieve decent results.\n5. Limitation\nThe performance of our method depends on the quality\nof the joint vision-language space constructed by the chosen\nvision-language model. For example, although PromptStyler\nlargely outperforms its base model (i.e., CLIP [50]) in all\nevaluations, our method shows lower accuracy on the Terra\nIncognita dataset [1] compared with other methods which\nutilize several images from the dataset as shown in Table 6.\nThe main reason for this might be due to the low accuracy\nof CLIP on the dataset. Nevertheless, given that our method\nconsistently outperforms its base model in every evaluation,\nthis limitation could be alleviated with the development of\nlarge-scale vision-language models.\n6. Conclusion\nWe have presented a novel method that synthesizes a\nvariety of styles in a joint vision-language space via learn-\nable style words without exploiting any images to deal with\nsource-free domain generalization. PromptStyler simulates\nvarious distribution shifts in the latent space of a large-scale\npre-trained model, which could effectively improve its gen-\neralization capability. The proposed method achieves state-\nof-the-art results without using any source domain data on\nmultiple domain generalization benchmarks. We hope that\nfuture work could apply our method to other tasks using\ndifferent large-scale vision-language models.\nAcknowledgment. This work was supported by the Agency for\nDefense Development grant funded by the Korean government.\n8\nReferences\n[1] Sara Beery, Grant van Horn, and Pietro Perona. Recognition\nin Terra Incognita. In Proceedings of the European Confer-\nence on Computer Vision (ECCV), 2018. 8, 13\n[2] Shai Ben-David, John Blitzer, Koby Crammer, and Fernando\nPereira. Analysis of Representations for Domain Adapta-\ntion. In Advances in Neural Information Processing Systems\n(NIPS), 2006. 1\n[3] Fabio Maria Carlucci, Antonio D\u2019Innocente, Silvia Bucci,\nBarbara Caputo, and Tatiana Tommasi.\nDomain Gener-\nalization by Solving Jigsaw Puzzles.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2019. 1, 3\n[4] Junbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho,\nSeunghyun Park, Yunsung Lee, and Sungrae Park. SWAD:\nDomain Generalization by Seeking Flat Minima. In Advances\nin Neural Information Processing Systems (NeurIPS), 2021.\n3\n[5] Junbum Cha, Kyungjae Lee, Sungrae Park, and Sanghyuk\nChun. Domain Generalization by Mutual-Information Reg-\nularization with Pre-trained Models. In Proceedings of the\nEuropean Conference on Computer Vision (ECCV), 2022. 1,\n3, 6\n[6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\nFei-Fei. ImageNet: A large-scale hierarchical image database.\nIn 2009 IEEE Conference on Computer Vision and Pattern\nRecognition, pages 248\u2013255, 2009. 6, 8, 13, 14, 15\n[7] Jiankang Deng, Jia Guo, Tongliang Liu, Mingming Gong,\nand Stefanos Zafeiriou. Sub-center ArcFace: Boosting Face\nRecognition by Large-Scale Noisy Web Faces. In Proceedings\nof the European Conference on Computer Vision (ECCV),\n2020. 5\n[8] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou.\nArcFace: Additive Angular Margin Loss for Deep Face\nRecognition. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 2019. 5,\n8, 16\n[9] Jiankang Deng and Stefanos Zafeririou. ArcFace for Dis-\nguised Face Recognition. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV), 2019.\n5\n[10] Yu Ding, Lei Wang, Bin Liang, Shuming Liang, Yang Wang,\nand Fang Chen. Domain Generalization by Learning and\nRemoving Domain-specific Features. In Advances in Neural\nInformation Processing Systems (NeurIPS), 2022. 3\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is\nWorth 16x16 Words: Transformers for Image Recognition at\nScale. In International Conference on Learning Representa-\ntions (ICLR), 2021. 5, 6, 7, 12, 14, 15\n[12] Qi Dou, Daniel C. Castro, Konstantinos Kamnitsas, and Ben\nGlocker. Domain Generalization via Model-Agnostic Learn-\ning of Semantic Features. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2019. 3\n[13] Lisa Dunlap, Clara Mohri, Devin Guillory, Han Zhang, Trevor\nDarrell, Joseph E. Gonzalez, Aditi Raghunathan, and Anja\nRohrbach. Using Language to Extend to Unseen Domains.\nIn International Conference on Learning Representations\n(ICLR), 2023. 3\n[14] Xinjie Fan, Qifei Wang, Junjie Ke, Feng Yang, Boqing Gong,\nand Mingyuan Zhou. Adversarially Adaptive Normaliza-\ntion for Single Domain Generalization. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2021. 3\n[15] Chen Fang, Ye Xu, and Daniel N. Rockmore. Unbiased\nMetric Learning: On the Utilization of Multiple Datasets and\nWeb Images for Softening Bias. In Proceedings of the IEEE\nInternational Conference on Computer Vision (ICCV), 2013.\n2, 5, 6, 7, 8, 13, 14, 16\n[16] Ahmed Frikha, Haokun Chen, Denis Krompa\u00df, Thomas Run-\nkler, and Volker Tresp. Towards Data-Free Domain General-\nization. In Asian Conference on Machine Learning (ACML),\n2022. 3\n[17] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\nAmit H. Bermano, Gal Chechik, and Daniel Cohen-Or. An\nImage is Worth One Word: Personalizing Text-to-Image Gen-\neration using Textual Inversion. In International Conference\non Learning Representations (ICLR), 2023. 2, 3\n[18] Rinon Gal, Or Patashnik, Haggai Maron, Gal Chechik, and\nDaniel Cohen-Or. StyleGAN-NADA: CLIP-Guided Domain\nAdaptation of Image Generators.\nACM Transactions on\nGraphics (Proc. SIGGRAPH Asia), 2022. 3\n[19] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal\nGermain, Hugo Larochelle, Franc\u00b8ois Laviolette, Mario Marc-\nhand, and Victor Lempitsky. Domain-Adversarial Training of\nNeural Networks. In Journal of Machine Learning Research\n(JMLR), 2016. 6\n[20] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao\nFang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. CLIP-\nAdapter:\nBetter Vision-Language Models with Feature\nAdapters. arXiv preprint arXiv:2110.04544, 2021. 3\n[21] Ishaan Gulrajani and David Lopez-Paz. In Search of Lost Do-\nmain Generalization. In International Conference on Learn-\ning Representations (ICLR), 2021. 1, 3, 5\n[22] He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and\nSun, Jian. Deep Residual Learning for Image Recognition.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 770\u2013778, 2016. 5, 6,\n8, 12, 13, 14, 15\n[23] Dan Hendrycks and Thomas Dietterich. Benchmarking Neu-\nral Network Robustness to Common Corruptions and Pertur-\nbations. In International Conference on Learning Represen-\ntations (ICLR), 2019. 1, 3\n[24] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu,\nPhillip Isola, Kate Saenko, Alexei A. Efros, and Trevor Dar-\nrell. CyCADA: Cycle-Consistent Adversarial Domain Adap-\ntation. In International Conference on Machine Learning\n(ICML), 2018. 1\n[25] Zeyi Huang, Haohan Wang, Eric P. Xing, and Dong Huang.\nSelf-Challenging Improves Cross-Domain Generalization. In\nProceedings of the European Conference on Computer Vision\n(ECCV), 2020. 6\n9\n[26] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,\nHieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, and Tom\nDuerig. Scaling Up Visual and Vision-Language Representa-\ntion Learning With Noisy Text Supervision. In International\nConference on Machine Learning (ICML), 2021. 2, 3, 12\n[27] Juwon Kang, Sohyun Lee, Namyup Kim, and Suha Kwak.\nStyle Neophile: Constantly Seeking Novel Styles for Domain\nGeneralization. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), 2022.\n2, 3\n[28] Daehee Kim, Seunghyun Park, Jinkyu Kim, and Jaekoo Lee.\nSelfReg: Self-supervised Contrastive Regularization for Do-\nmain Generalization. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision (ICCV), 2021. 6, 8,\n13, 14, 15\n[29] Donghyun Kim, Kaihong Wang, Stan Sclaroff, and Kate\nSaenko. A Broad Study of Pre-training for Domain Gener-\nalization and Adaptation. In Proceedings of the European\nConference on Computer Vision (ECCV), 2022. 1, 3\n[30] Dimitrios Kollias and Stefanos Zafeiriou. Expression, Affect,\nAction Unit Recognition: Aff-Wild2, Multi-Task Learning\nand ArcFace. In Proceedings of the British Machine Vision\nConference (BMVC), 2019. 5\n[31] Gihyun Kwon and Jong Chul Ye. CLIPstyler: Image Style\nTransfer with a Single Text Condition. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2022. 3\n[32] Sohyun Lee, Taeyoung Son, and Suha Kwak. FIFO: Learn-\ning Fog-invariant Features for Foggy Scene Segmentation.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2022. 1\n[33] Yoonho Lee, Annie S. Chen, Fahim Tajwar, Ananya Kumar,\nHuaxiu Yao, Percy Liang, and Chelsea Finn. Surgical Fine-\nTuning Improves Adaptation to Distribution Shifts. In In-\nternational Conference on Learning Representations (ICLR),\n2023. 1\n[34] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M.\nHospedales. Deeper, Broader and Artier Domain Generaliza-\ntion. In Proceedings of the IEEE International Conference\non Computer Vision (ICCV), 2017. 2, 5, 6, 8, 13, 14, 16\n[35] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M.\nHospedales. Learning to Generalize: Meta-Learning for Do-\nmain Generalization. In Proceedings of the AAAI Conference\non Artificial Intelligence (AAAI), 2018. 1, 3, 6\n[36] Da Li, Jianshu Zhang, Yongxin Yang, Cong Liu, Yi-Zhe\nSong, and Timothy M. Hospedales. Episodic Training for\nDomain Generalization. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV), 2019.\n3\n[37] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C. Kot.\nDomain Generalization With Adversarial Feature Learning.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2018. 1, 3\n[38] Lei Li, Ke Gao, Juan Cao, Ziyao Huang, Yepeng Weng, Xi-\naoyue Mi, Zhengze Yu, Xiaoya Li, and Boyang xia. Progres-\nsive Domain Expansion Network for Single Domain Gener-\nalization. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 2021. 3\n[39] Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung,\nand James Zou. Mind the Gap: Understanding the Modal-\nity Gap in Multi-modal Contrastive Representation Learn-\ning. In Advances in Neural Information Processing Systems\n(NeurIPS), 2022. 2, 16\n[40] Boxiao Liu, Guanglu Song, Manyuan Zhang, Haihang You,\nand Yu Liu. Switchable K-Class Hyperplanes for Noise-\nRobust Representation Learning.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision\n(ICCV), 2021. 5\n[41] Yuning Lu, Jianzhuang Liu, Yonggang Zhang, Yajing Liu, and\nXinmei Tian. Prompt Distribution Learning. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2022. 3\n[42] Divyat Mahajan, Shruti Tople, and Amit Sharma. Domain\nGeneralization using Causal Matching. In International Con-\nference on Machine Learning (ICML), 2021. 3\n[43] Toshihiko Matsuura and Tatsuya Harada. Domain Gener-\nalization Using a Mixture of Multiple Latent Domains. In\nProceedings of the AAAI Conference on Artificial Intelligence\n(AAAI), 2020. 3\n[44] Seonwoo Min, Nokyung Park, Siwon Kim, Seunghyun Park,\nand Jinkyu Kim. Grounding Visual Representations with\nTexts for Domain Generalization. In Proceedings of the Euro-\npean Conference on Computer Vision (ECCV), 2022. 3, 6, 8,\n13, 14, 15\n[45] Krikamol Muandet, David Balduzzi, and Bernhard Sch\u00a8olkopf.\nDomain Generalization via Invariant Feature Representation.\nIn International Conference on Machine Learning (ICML),\n2013. 3\n[46] Hyeonseob Nam, HyunJae Lee, Jongchan Park, Wonjun Yoon,\nand Donggeun Yoo. Reducing Domain Gap by Reducing\nStyle Bias. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 2021. 6\n[47] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or,\nand Dani Lischinski. StyleCLIP: Text-Driven Manipulation\nof StyleGAN Imagery. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV), 2021.\n3\n[48] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate\nSaenko, and Bo Wang. Moment Matching for Multi-Source\nDomain Adaptation. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision (ICCV), 2019. 2, 5,\n6, 7, 8, 13, 15, 16\n[49] Fengchun Qiao, Long Zhao, and Xi Peng.\nLearning to\nLearn Single Domain Generalization.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2020. 3\n[50] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever. Learning Transferable Visual\nModels From Natural Language Supervision. In International\nConference on Machine Learning (ICML), 2021. 2, 3, 4, 5, 6,\n7, 8, 12, 13, 14, 15, 16\n[51] Alexandre Rame, Corentin Dancette, and Matthieu Cord.\nFishr: Invariant Gradient Variances for Out-of-Distribution\n10\nGeneralization.\nIn International Conference on Machine\nLearning (ICML), 2022. 3\n[52] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and\nVaishaal Shankar. Do ImageNet Classifiers Generalize to\nImageNet? In International Conference on Machine Learning\n(ICML), 2019. 1, 3\n[53] Yangjun Ruan, Yann Dubois, and Chris J. Maddison. Op-\ntimal Representations for Covariate Shift. In International\nConference on Learning Representations (ICLR), 2022. 3, 6\n[54] Kuniaki Saito, Donghyun Kim, Stan Sclaroff, Trevor Dar-\nrell, and Kate Saenko. Semi-Supervised Domain Adaptation\nvia Minimax Entropy. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision (ICCV), 2019.\n1\n[55] Seonguk Seo, Yumin Suh, Dongwan Kim, Geeho Kim, Jong-\nwoo Han, and Bohyung Han. Learning to Optimize Domain\nSpecific Normalization for Domain Generalization. In Pro-\nceedings of the European Conference on Computer Vision\n(ECCV), 2020. 3\n[56] Baochen Sun, Jiashi Feng, and Kate Saenko. Return of Frus-\ntratingly Easy Domain Adaptation. In Proceedings of the\nAAAI Conference on Artificial Intelligence (AAAI), 2016. 1\n[57] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell.\nAdversarial Discriminative Domain Adaptation. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2017. 1\n[58] Laurens van der Maaten and Geoffrey Hinton. Visualizing\nData using t-SNE. In Journal of Machine Learning Research\n(JMLR), 2008. 7\n[59] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is All you Need. In Advances in Neural\nInformation Processing Systems (NIPS), 2017. 5, 12\n[60] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty,\nand Sethuraman Panchanathan. Deep Hashing Network for\nUnsupervised Domain Adaptation. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2017. 2, 5, 6, 8, 12, 13, 15, 16\n[61] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao\nQin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip S.\nYu. Generalizing to Unseen Domains: A Survey on Domain\nGeneralization. In IEEE Transactions on Knowledge and\nData Engineering (TKDE), 2021. 1, 3\n[62] Zijian Wang, Yadan Luo, Ruihong Qiu, Zi Huang, and Mahsa\nBaktashmotlagh. Learning to Diversify for Single Domain\nGeneralization. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision (ICCV), 2021. 3\n[63] Qinwei Xu, Ruipeng Zhang, Ya Zhang, Yanfeng Wang, and\nQi Tian. A Fourier-based Framework for Domain Generaliza-\ntion. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2021. 3\n[64] Jinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda,\nLiqun Chen, Belinda Zeng, Trishul Chilimbi, and Junzhou\nHuang.\nVision-Language Pre-Training with Triple Con-\ntrastive Learning. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition (CVPR),\n2022. 2, 3, 12\n[65] Dingyi Zhang, Yingming Li, and Zhongfei Zhang. Deep\nmetric learning with spherical embedding. In Advances in\nNeural Information Processing Systems (NeurIPS), 2020. 5\n[66] Renrui Zhang, Zhang Wei, Rongyao Fang, Peng Gao, Kun-\nchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li.\nTip-\nAdapter: Training-free Adaption of CLIP for Few-shot Clas-\nsification. In Proceedings of the European Conference on\nComputer Vision (ECCV), 2022. 3\n[67] Yuhui Zhang, Jeff Z. HaoChen, Shih-Cheng Huang, Kuan-\nChieh Wang, James Zou, and Serena Yeung. Diagnosing and\nRectifying Vision Models using Language. In International\nConference on Learning Representations (ICLR), 2023. 2, 3,\n16\n[68] Han Zhao, Remi Tachet des Combes, Kun Zhang, and Ge-\noffrey J. Gordon. On Learning Invariant Representation for\nDomain Adaptation. In International Conference on Machine\nLearning (ICML), 2019. 1\n[69] Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and\nChen Change Loy. Domain Generalization: A Survey. In\nIEEE Transactions on Pattern Analysis and Machine Intelli-\ngence (TPAMI), 2022. 1, 3\n[70] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Zi-\nwei Liu. Conditional Prompt Learning for Vision-Language\nModels. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 2022. 3,\n5, 16\n[71] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei\nLiu. Learning to Prompt for Vision-Language Models. In\nInternational Journal of Computer Vision (IJCV), 2022. 3, 5,\n16\n[72] Kaiyang Zhou, Yongxin Yang, Timothy Hospedales, and Tao\nXiang. Deep Domain-Adversarial Image Generation for Do-\nmain Generalisation. In Proceedings of the AAAI Conference\non Artificial Intelligence (AAAI), 2020. 3\n[73] Kaiyang Zhou, Yongxin Yang, Timothy Hospedales, and Tao\nXiang. Learning to Generate Novel Domains for Domain\nGeneralization. In Proceedings of the European Conference\non Computer Vision (ECCV), 2020. 3\n[74] Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Do-\nmain Generalization with MixStyle. In International Con-\nference on Learning Representations (ICLR), 2021. 1, 2,\n3\n11\nPromptStyler: Prompt-driven Style Generation\nfor Source-free Domain Generalization\n\u2014 Supplementary Material \u2014\nJunhyeong Cho1\nGilhyun Nam1\nSungyeon Kim2\nHunmin Yang1,3\nSuha Kwak2\n1ADD\n2POSTECH\n3KAIST\nhttps://PromptStyler.github.io\nIn this supplementary material, we provide more method\ndetails (Section A), analyses on Terra Incognita (Section B),\nevaluation results (Section C) and discussion (Section D).\nA. Method Details\nThis section provides more details of the chosen vision-\nlanguage model (Section A.1) and design choices for learn-\ning style word vectors (Section A.2).\nA.1. Large-scale vision-language model\nWe choose CLIP [50] as our pre-trained vision-language\nmodel which is a large-scale model trained with 400 million\nimage-text pairs. Note that the proposed method is broadly\napplicable to the CLIP-like vision-language models [26,64]\nwhich also construct hyperspherical joint vision-language\nspaces using contrastive learning methods. Given a batch of\nimage-text pairs, such models jointly train an image encoder\nand a text encoder considering similarity scores obtained\nfrom image-text pairings.\nJoint vision-language training. Suppose there is a batch of\nM image-text pairs. Among all possible M \u00d7 M pairings,\nthe matched M pairs are the positive pairs and the other\nM 2 \u2212 M pairs are the negative pairs. CLIP [50] is trained\nto maximize cosine similarities of image and text features\nfrom the positive M pairs while minimizing the similarities\nof such features from the negative M 2 \u2212 M pairs.\nImage encoder. CLIP [50] utilizes ResNet [22] or ViT [11]\nas its image encoder. Given an input image, the image\nencoder extracts its image feature. After that, the image\nfeature is mapped to a hyperspherical joint vision-language\nspace by \u21132 normalization.\nText encoder. CLIP [50] utilizes Transformer [59] as its text\nencoder. Given an input text prompt, it is converted to word\nvectors via a tokenization process and a word lookup proce-\ndure. Using these word vectors, the text encoder generates a\ntext feature which is then mapped to a hyperspherical joint\nvision-language space by \u21132 normalization.\n1510 20\n40\n60\n80\n100\nNumber of style word vectors\n4\n8\n12\n16\n20\nGPU memory usage (GB)\nParallel\nSequential\nFigure A1: GPU memory usage when learning K style word\nvectors for the target task OfficeHome [60] (65 classes) with\nrespect to the design choices, Sequential or Parallel.\nZero-shot inference. At inference time, zero-shot CLIP [50]\nsynthesizes classifier weights via the text encoder using N\nclass names pre-defined in the target task. Given an input\nimage, the image encoder extracts its image feature and the\ntext encoder produces N text features using the N class\nnames. Then, it computes cosine similarity scores between\nthe image feature and text features, and selects the class\nname which results in the highest similarity score as its\nclassification output.\nA.2. Empirical justification of our design choice\nAs described in Section 3.1 of the main paper, there are\ntwo possible design choices for learning K style word vec-\ntors: (1) learning each style word vector si in a sequential\nmanner, or (2) learning all style word vectors {si}K\ni=1 in a\nparallel manner. We choose the former mainly due to its\nmuch less memory overhead. As shown in Figure A1, we\ncould sequentially learn \u223c100 style word vectors with \u223c4.2\nGB memory usage. However, it is not possible to learn more\nthan 21 style word vectors in a parallel manner using a single\n12\n(a)  Dog\n(b) Cat\n(c) Squirrel\nFigure B1: Several examples from the Terra Incognita [1] dataset. We visualize class entities using red bounding boxes, since\nthey are not easily recognizable due to their small sizes and complex background scenes.\nConfiguration\nAccuracy (%)\nSource\nDomain\nMethod\nDomain\nDescription\nLocation100\nLocation38\nLocation43\nLocation46\nAvg.\nResNet-50 [22] with pre-trained weights on ImageNet [6]\nSelfReg [28]\n\u2713\n\u2013\n48.8\u00b10.9\n41.3\u00b11.8\n57.3\u00b10.7\n40.6\u00b10.9\n47.0\nGVRT [44]\n\u2713\n\u2013\n53.9\u00b11.3\n41.8\u00b11.2\n58.2\u00b10.9\n38.0\u00b10.6\n48.0\nResNet-50 [22] with pre-trained weights from CLIP [50]\nZS-CLIP (C) [50]\n\u2013\n\u2013\n8.4\u00b10.0\n13.7\u00b10.0\n32.5\u00b10.0\n23.3\u00b10.0\n19.5\nZS-CLIP (PC) [50]\n\u2013\n\u2713\n9.9\u00b10.0\n28.3\u00b10.0\n32.9\u00b10.0\n24.0\u00b10.0\n23.8\nPromptStyler\n\u2013\n\u2013\n13.8\u00b11.7\n39.8\u00b11.3\n38.0\u00b10.4\n30.3\u00b10.3\n30.5\nTable B1: Top-1 classification accuracy on the Terra Incognita [1] dataset. Compared with existing domain generalization\nmethods which utilize source domain data, zero-shot methods using CLIP [50] show unsatisfactory results on this dataset.\nRTX 3090 GPU (24 GB Memory) due to its large memory\noverhead. In detail, learning 20 and 21 style word vectors\ntakes 22.4 GB and 23.5 GB, respectively. The large memory\noverhead caused by the parallel learning design substantially\nlimits the number of learnable style word vectors.\nTo be specific, PromptStyler with the parallel learning\ndesign needs to generate K style features, KN style-content\nfeatures, and N content features for learning K style word\nvectors at the same time; these features are used to compute\nthe style diversity loss Lstyle and the content consistency\nloss Lcontent for learning all the style word vectors in a\nparallel manner. Note that the large memory overhead is\nmainly caused by the KN style-content features. Suppose\nwe want to learn 80 style word vectors for the target task\nOfficeHome [60] (65 classes). Then, we need to synthesize\n5200(= 80 \u00d7 65) style-content features. Even worse, we\nneed to generate 27600(= 80 \u00d7 345) style-content features\nfor the target task DomainNet [48] (345 classes). On the\nother hand, PromptStyler with the sequential learning design\nonly requires i style features, N style-content features, and\nN content features for learning i-th style word vector, where\n1 \u2264 i \u2264 K. For scalability, we chose the sequential learning\ndesign since it could handle a lot of learnable style word\nvectors and numerous classes in the target task.\nB. Analyses on Terra Incognita\nAs described in Section 5 of the main paper, the quality\nof the latent space constructed by a large-scale pre-trained\nmodel significantly affects the effectiveness of PromptStyler.\nTo be specific, the proposed method depends on the quality\nof the joint vision-language space constructed by CLIP [50].\nAlthough our method achieves state-of-the-art results on\nPACS [34], VLCS [15], OfficeHome [60], and Domain-\nNet [48], its performance on Terra Incognita [1] is not satis-\nfactory. This section provides more analyses on the dataset.\nTable B1 shows that PromptStyler outperforms zero-shot\nCLIP [50] for all domains in the Terra Incognita dataset [1].\nHowever, its accuracy on this dataset is lower compared\nwith existing domain generalization methods [28,44] which\nutilize several images from the dataset as their source do-\nmain data. This unsatisfactory result might be due to the low\naccuracy of CLIP on the dataset. We suspect that images in\nthe Terra Incognita dataset (Fig. B1) might be significantly\ndifferent from the domains that CLIP has observed. The\ndistribution shifts between CLIP training dataset and the\nTerra Incognita dataset might be extreme, and thus such dis-\ntribution shifts could not be entirely covered by our method\nwhich exploits CLIP latent space. We hope this issue could\nbe alleviated with the development of large-scale models.\n13\nConfiguration\nAccuracy (%)\nSource\nDomain\nMethod\nDomain\nDescription\nArt Painting\nCartoon\nPhoto\nSketch\nAvg.\nResNet-50 [22] with pre-trained weights on ImageNet [6]\nGVRT [44]\n\u2713\n\u2013\n87.9\u00b10.3\n78.4\u00b11.0\n98.2\u00b10.1\n75.7\u00b10.4\n85.1\nSelfReg [28]\n\u2713\n\u2013\n87.9\u00b11.0\n79.4\u00b11.4\n96.8\u00b10.7\n78.3\u00b11.2\n85.6\nResNet-50 [22] with pre-trained weights from CLIP [50]\nZS-CLIP (C) [50]\n\u2013\n\u2013\n88.9\u00b10.0\n94.4\u00b10.0\n99.3\u00b10.0\n79.8\u00b10.0\n90.6\nZS-CLIP (PC) [50]\n\u2013\n\u2713\n90.8\u00b10.0\n93.3\u00b10.0\n99.4\u00b10.0\n79.3\u00b10.0\n90.7\nPromptStyler\n\u2013\n\u2013\n93.7\u00b10.1\n94.7\u00b10.2\n99.4\u00b10.0\n84.9\u00b10.1\n93.2\nViT-B / 16 [11] with pre-trained weights from CLIP [50]\nZS-CLIP (C) [50]\n\u2013\n\u2013\n96.4\u00b10.0\n98.9\u00b10.0\n99.9\u00b10.0\n87.7\u00b10.0\n95.7\nZS-CLIP (PC) [50]\n\u2013\n\u2713\n97.2\u00b10.0\n99.1\u00b10.0\n99.9\u00b10.0\n88.2\u00b10.0\n96.1\nPromptStyler\n\u2013\n\u2013\n97.6\u00b10.1\n99.1\u00b10.1\n99.9\u00b10.0\n92.3\u00b10.3\n97.2\nViT-L / 14 [11] with pre-trained weights from CLIP [50]\nZS-CLIP (C) [50]\n\u2013\n\u2013\n97.2\u00b10.0\n99.5\u00b10.0\n99.9\u00b10.0\n93.8\u00b10.0\n97.6\nZS-CLIP (PC) [50]\n\u2013\n\u2713\n99.0\u00b10.0\n99.7\u00b10.0\n99.9\u00b10.0\n95.5\u00b10.0\n98.5\nPromptStyler\n\u2013\n\u2013\n99.1\u00b10.0\n99.7\u00b10.0\n100.0\u00b10.0\n95.5\u00b10.1\n98.6\nTable C1: Comparison with state-of-the-art domain generalization methods in terms of per-domain top-1 classification accuracy\non PACS [34]. We repeat each experiment using three different seeds, and report average accuracies with standard errors.\nZS-CLIP (C) denotes zero-shot CLIP using \u201c[class]\u201d as its text prompt, and ZS-CLIP (PC) indicates zero-shot CLIP using \u201ca\nphoto of a [class]\u201d as its text prompt. Note that PromptStyler does not use any source domain data and domain descriptions.\nConfiguration\nAccuracy (%)\nSource\nDomain\nMethod\nDomain\nDescription\nCaltech\nLabelMe\nSUN09\nVOC2007\nAvg.\nResNet-50 [22] with pre-trained weights on ImageNet [6]\nSelfReg [28]\n\u2713\n\u2013\n96.7\u00b10.4\n65.2\u00b11.2\n73.1\u00b11.3\n76.2\u00b10.7\n77.8\nGVRT [44]\n\u2713\n\u2013\n98.8\u00b10.1\n64.0\u00b10.3\n75.2\u00b10.5\n77.9\u00b11.0\n79.0\nResNet-50 [22] with pre-trained weights from CLIP [50]\nZS-CLIP (C) [50]\n\u2013\n\u2013\n99.2\u00b10.0\n62.4\u00b10.0\n69.0\u00b10.0\n73.5\u00b10.0\n76.0\nZS-CLIP (PC) [50]\n\u2013\n\u2713\n99.4\u00b10.0\n65.0\u00b10.0\n71.7\u00b10.0\n84.2\u00b10.0\n80.1\nPromptStyler\n\u2013\n\u2013\n99.5\u00b10.0\n71.2\u00b10.2\n72.0\u00b10.0\n86.5\u00b10.3\n82.3\nViT-B / 16 [11] with pre-trained weights from CLIP [50]\nZS-CLIP (C) [50]\n\u2013\n\u2013\n99.7\u00b10.0\n61.8\u00b10.0\n70.1\u00b10.0\n73.9\u00b10.0\n76.4\nZS-CLIP (PC) [50]\n\u2013\n\u2713\n99.9\u00b10.0\n68.9\u00b10.0\n74.8\u00b10.0\n85.9\u00b10.0\n82.4\nPromptStyler\n\u2013\n\u2013\n99.9\u00b10.0\n71.5\u00b10.3\n73.9\u00b10.2\n86.3\u00b10.1\n82.9\nViT-L / 14 [11] with pre-trained weights from CLIP [50]\nZS-CLIP (C) [50]\n\u2013\n\u2013\n99.9\u00b10.0\n59.3\u00b10.0\n71.0\u00b10.0\n79.9\u00b10.0\n77.5\nZS-CLIP (PC) [50]\n\u2013\n\u2713\n99.9\u00b10.0\n70.9\u00b10.0\n72.9\u00b10.0\n86.0\u00b10.0\n82.4\nPromptStyler\n\u2013\n\u2013\n99.9\u00b10.0\n71.1\u00b10.7\n71.8\u00b11.0\n86.8\u00b10.0\n82.4\nTable C2: Comparison with state-of-the-art domain generalization methods in terms of per-domain top-1 classification accuracy\non VLCS [15]. We repeat each experiment using three different seeds, and report average accuracies with standard errors.\nZS-CLIP (C) denotes zero-shot CLIP using \u201c[class]\u201d as its text prompt, and ZS-CLIP (PC) indicates zero-shot CLIP using \u201ca\nphoto of a [class]\u201d as its text prompt. Note that PromptStyler does not use any source domain data and domain descriptions.\n14\nConfiguration\nAccuracy (%)\nSource\nDomain\nMethod\nDomain\nDescription\nArt\nClipart\nProduct\nReal World\nAvg.\nResNet-50 [22] with pre-trained weights on ImageNet [6]\nSelfReg [28]\n\u2713\n\u2013\n63.6\u00b11.4\n53.1\u00b11.0\n76.9\u00b10.4\n78.1\u00b10.4\n67.9\nGVRT [44]\n\u2713\n\u2013\n66.3\u00b10.1\n55.8\u00b10.4\n78.2\u00b10.4\n80.4\u00b10.2\n70.1\nResNet-50 [22] with pre-trained weights from CLIP [50]\nZS-CLIP (C) [50]\n\u2013\n\u2013\n69.9\u00b10.0\n46.8\u00b10.0\n77.7\u00b10.0\n79.8\u00b10.0\n68.6\nZS-CLIP (PC) [50]\n\u2013\n\u2713\n71.7\u00b10.0\n52.0\u00b10.0\n81.6\u00b10.0\n82.6\u00b10.0\n72.0\nPromptStyler\n\u2013\n\u2013\n73.4\u00b10.1\n52.4\u00b10.2\n84.3\u00b10.1\n84.1\u00b10.1\n73.6\nViT-B / 16 [11] with pre-trained weights from CLIP [50]\nZS-CLIP (C) [50]\n\u2013\n\u2013\n80.7\u00b10.0\n64.6\u00b10.0\n86.3\u00b10.0\n88.0\u00b10.0\n79.9\nZS-CLIP (PC) [50]\n\u2013\n\u2713\n82.7\u00b10.0\n67.6\u00b10.0\n89.2\u00b10.0\n89.7\u00b10.0\n82.3\nPromptStyler\n\u2013\n\u2013\n83.8\u00b10.1\n68.2\u00b10.0\n91.6\u00b10.1\n90.7\u00b10.1\n83.6\nViT-L / 14 [11] with pre-trained weights from CLIP [50]\nZS-CLIP (C) [50]\n\u2013\n\u2013\n86.2\u00b10.0\n73.3\u00b10.0\n92.0\u00b10.0\n92.2\u00b10.0\n85.9\nZS-CLIP (PC) [50]\n\u2013\n\u2713\n87.2\u00b10.0\n73.8\u00b10.0\n93.0\u00b10.0\n93.4\u00b10.0\n86.9\nPromptStyler\n\u2013\n\u2013\n89.1\u00b10.1\n77.6\u00b10.1\n94.8\u00b10.1\n94.8\u00b10.0\n89.1\nTable C3: Comparison with state-of-the-art domain generalization methods in terms of per-domain top-1 classification accuracy\non OfficeHome [60]. We repeat each experiment using three different seeds, and report average accuracies with standard errors.\nZS-CLIP (C) denotes zero-shot CLIP using \u201c[class]\u201d as its text prompt, and ZS-CLIP (PC) indicates zero-shot CLIP using \u201ca\nphoto of a [class]\u201d as its text prompt. Note that PromptStyler does not use any source domain data and domain descriptions.\nConfiguration\nAccuracy (%)\nSource\nDomain\nMethod\nDomain Description\nClipart\nInfograph\nPainting\nQuickdraw\nReal\nSketch\nAvg.\nResNet-50 [22] with pre-trained weights on ImageNet [6]\nSelfReg [28]\n\u2713\n\u2013\n60.7\u00b10.1\n21.6\u00b10.1\n49.4\u00b10.2\n12.7\u00b10.1\n60.7\u00b10.1\n51.7\u00b10.1\n42.8\nGVRT [44]\n\u2713\n\u2013\n62.4\u00b10.4\n21.0\u00b10.0\n50.5\u00b10.4\n13.8\u00b10.3\n64.6\u00b10.4\n52.4\u00b10.2\n44.1\nResNet-50 [22] with pre-trained weights from CLIP [50]\nZS-CLIP (C) [50]\n\u2013\n\u2013\n53.1\u00b10.0\n39.2\u00b10.0\n52.7\u00b10.0\n6.3\u00b10.0\n75.2\u00b10.0\n47.1\u00b10.0\n45.6\nZS-CLIP (PC) [50]\n\u2013\n\u2713\n53.6\u00b10.0\n39.6\u00b10.0\n53.4\u00b10.0\n5.9\u00b10.0\n76.6\u00b10.0\n48.0\u00b10.0\n46.2\nPromptStyler\n\u2013\n\u2013\n57.9\u00b10.0\n44.3\u00b10.0\n57.3\u00b10.0\n6.1\u00b10.1\n79.5\u00b10.0\n51.7\u00b10.0\n49.5\nViT-B / 16 [11] with pre-trained weights from CLIP [50]\nZS-CLIP (C) [50]\n\u2013\n\u2013\n70.7\u00b10.0\n49.1\u00b10.0\n66.4\u00b10.0\n14.8\u00b10.0\n82.7\u00b10.0\n63.1\u00b10.0\n57.8\nZS-CLIP (PC) [50]\n\u2013\n\u2713\n71.0\u00b10.0\n47.7\u00b10.0\n66.2\u00b10.0\n14.0\u00b10.0\n83.7\u00b10.0\n63.5\u00b10.0\n57.7\nPromptStyler\n\u2013\n\u2013\n73.1\u00b10.0\n50.9\u00b10.0\n68.2\u00b10.1\n13.3\u00b10.1\n85.4\u00b10.0\n65.3\u00b10.0\n59.4\nViT-L / 14 [11] with pre-trained weights from CLIP [50]\nZS-CLIP (C) [50]\n\u2013\n\u2013\n78.2\u00b10.0\n53.0\u00b10.0\n70.7\u00b10.0\n21.6\u00b10.0\n86.0\u00b10.0\n70.3\u00b10.0\n63.3\nZS-CLIP (PC) [50]\n\u2013\n\u2713\n79.2\u00b10.0\n52.4\u00b10.0\n71.3\u00b10.0\n22.5\u00b10.0\n86.9\u00b10.0\n71.8\u00b10.0\n64.0\nPromptStyler\n\u2013\n\u2013\n80.7\u00b10.0\n55.6\u00b10.1\n73.8\u00b10.1\n21.7\u00b10.0\n88.2\u00b10.0\n73.2\u00b10.0\n65.5\nTable C4: Comparison with state-of-the-art domain generalization methods in terms of per-domain top-1 classification accuracy\non DomainNet [48]. We repeat each experiment using three different seeds, and report average accuracies with standard errors.\nZS-CLIP (C) denotes zero-shot CLIP using \u201c[class]\u201d as its text prompt, and ZS-CLIP (PC) indicates zero-shot CLIP using \u201ca\nphoto of a [class]\u201d as its text prompt. Note that PromptStyler does not use any source domain data and domain descriptions.\n15\nAccuracy (%)\nDistribution\nPACS\nVLCS\nOfficeHome\nDomainNet\nAvg.\nU(0.00, 0.20)\n93.1\n82.6\n73.8\n49.2\n74.7\nN(0.00, 0.202)\n93.0\n81.0\n73.6\n49.5\n74.3\nN(0.20, 0.022)\n93.1\n82.5\n73.5\n49.3\n74.6\nN(0.00, 0.022)\n93.2\n82.3\n73.6\n49.5\n74.7\nTable C5: Effects of the distributions used for initializing\nstyle word vectors. Uniform or Normal distribution is used.\nC. Evaluation Results\nPer-domain accuracy. As shown in Table C1\u2013C4, we pro-\nvide per-domain top-1 classification accuracy on domain\ngeneralization benchmarks including PACS [34] (4 domains\nand 7 classes), VLCS [15] (4 domains and 5 classes), Office-\nHome [60] (4 domains and 65 classes) and DomainNet [48]\n(6 domains and 345 classes); each accuracy is obtained by\naveraging results from experiments repeated using three dif-\nferent random seeds. Interestingly, compared with zero-shot\nCLIP [50] which leverages a photo domain description (\u201ca\nphoto of a [class]\u201d), our PromptStyler achieves similar or\nbetter results on photo domains, e.g., on the VLCS dataset\nwhich consists of 4 photo domains. Note that the description\nhas more domain-specific information and more detailed\ncontexts compared with the na\u00a8\u0131ve prompt (\u201c[class]\u201d).\nDifferent distributions for initializing style word vectors.\nFollowing prompt learning methods [70,71], we initialized\nlearnable style word vectors using zero-mean Gaussian dis-\ntribution with 0.02 standard deviation. To measure the effect\nof the used distribution for the initialization, we also quanti-\ntatively evaluate PromptStyler using different distributions\nfor initializing style word vectors. As shown in Table C5,\nthe proposed method also achieves similar results when ini-\ntializing style word vectors using different distributions.\nD. Discussion\nPromptStyler aims to improve model\u2019s generalization ca-\npability by simulating various distribution shifts in the latent\nspace of a large-scale pre-trained model. To achieve this goal,\nour method leverages a joint vision-language space where\ntext features could effectively represent their relevant image\nfeatures. It does not mean that image and text features should\nbe perfectly interchangeable in the joint vision-language\nspace; a recent study has demonstrated the modality gap\nphenomenon of this joint space [39]. However, thanks to\nthe cross-modal transferability in the joint vision-language\nspace [67], the proposed method could still be effective, i.e.,\nwe could consider text features as proxies for image features\nwhile training a linear classifier (Fig. 3 of the main paper).\nWhen our method is implemented with CLIP [50] and\nwe adopt ArcFace [8] as our classification loss Lclass, there\nis another interesting interpretation of the proposed method.\nAs described in Section A.1, CLIP text encoder synthesizes\nclassifier weights using class names for zero-shot inference\nand then it computes cosine similarity scores between the\nclassifier weights and input image features. Similarly, our\nmethod computes cosine similarity scores between classifier\nweights of the trained classifier (Fig. 3 of the main paper) and\ninput image features. From this perspective, the proposed\nmethod improves the decision boundary of the synthesized\nclassifier used in zero-shot CLIP by generating diverse style-\ncontent features and then training a linear classifier using\nthe style-content features. In other words, the trained clas-\nsifier could be considered as an improved version of the\nsynthesized classifier used in zero-shot CLIP.\n16\n"
  },
  {
    "title": "Robust Distortion-free Watermarks for Language Models",
    "link": "https://arxiv.org/pdf/2307.15593.pdf",
    "upvote": "7",
    "text": "Robust Distortion-free Watermarks for Language Models\nRohith Kuditipudi\nJohn Thickstun\nTatsunori Hashimoto\nPercy Liang\nDepartment of Computer Science\nStanford University\nJuly 2023\nAbstract\nWe propose a methodology for planting watermarks in text from an autoregressive\nlanguage model that are robust to perturbations without changing the distribution over\ntext up to a certain maximum generation budget.\nWe generate watermarked text by\nmapping a sequence of random numbers\u2014which we compute using a randomized water-\nmark key\u2014to a sample from the language model. To detect watermarked text, any party\nwho knows the key can align the text to the random number sequence. We instantiate\nour watermark methodology with two sampling schemes: inverse transform sampling and\nexponential minimum sampling. We apply these watermarks to three language models\u2014\nOPT-1.3B, LLaMA-7B and Alpaca-7B\u2014to experimentally validate their statistical power\nand robustness to various paraphrasing attacks. Notably, for both the OPT-1.3B and\nLLaMA-7B models, we find we can reliably detect watermarked text (p \u2264 0.01) from 35\ntokens even after corrupting between 40-50% of the tokens via random edits (i.e., substi-\ntutions, insertions or deletions). For the Alpaca-7B model, we conduct a case study on\nthe feasibility of watermarking responses to typical user instructions. Due to the lower\nentropy of the responses, detection is more difficult: around 25% of the responses\u2014whose\nmedian length is around 100 tokens\u2014are detectable with p \u2264 0.01, and the watermark is\nalso less robust to certain automated paraphrasing attacks we implement.1\n1\nIntroduction\nThe ability of language models to mass produce human-like text creates an acute, renewed\nemphasis on the importance of provenance of generated content. For example, the website\nStackOverflow has banned users from posting answers using OpenAI\u2019s ChatGPT model to\nmitigate the spread of misinformation on the platform [26]. A reliable forensic tool for at-\ntributing text to a particular language model would empower individuals\u2014such as platform\nmoderators and teachers\u2014to enact and enforce policies on language model usage; it would also\nbetter enable model providers to track the (mis)use of their models, e.g., to scrub synthetic\ntext from the training data of future language models.\nTo achieve provenance, a watermark is a signal embedded within some generated content\u2014\nin our case, text from a language model\u2014that encodes the source of the content. We consider\na setting where a (untrusted) third party user queries a language model (LM) by sending\nprompts to a trusted provider (Figure 1): the LM provider generates text from their language\nmodel with a watermark so that a detector may later identify the source of the text if the\nuser publishes it. The ideal watermark should satisfy at least the following three desiderata:\n1. distortion-free\u2014the watermark should preserve the original text distribution;\n2. agnostic\u2014it should be detectable without the language model and/or prompt;\n3. robust\u2014it should withstand perturbations of the watermarked text.\n1We release all code publicly at https://github.com/jthickstun/watermark.\n1\narXiv:2307.15593v2  [cs.LG]  27 Sep 2023\nFigure 1: We define the following watermarking protocol between three parties: the LM\nprovider, the user, the detector. The LM provider and the detector coordinate via a shared\nkey, while the user is an untrusted third party. The protocol consists of four steps: 1) the\nuser sends a prompt to the LM provider; 2) the LM provider generates watermarked text to\nthe user; 3) the user edits the watermarked text (to avoid detection) and publishes the edited\ntext; 4) the detector verifies which of the published text came from the LM provider.\nExisting watermarks either distort the model\u2019s sampling distribution, thus altering the API\nfunctionality [14, 1], or are not robust to editing or cropping the text [6]. Meanwhile, clas-\nsical steganographic techniques for covertly encoding messages within samples of text from\na language model are neither agnostic nor robust [31]. We develop the first watermarks for\nattributing text to a language model that achieve all three desiderata.\nOur methodology consists of two components, which the LM provider and detector re-\nspectively use to execute the two steps of the protocol in Figure 1 under their control: a\ngenerate method that deterministically maps a sequence \u03be of random numbers encoded\nby a (secret) watermark key2\u2014which we call the watermark key sequence\u2014to a sample\nfrom the language model, and a detect method that aligns a putative watermarked text\nwith the watermark key sequence using the shared key.\nInformally, our watermarks are\ndistortion-free in the sense that\u2014marginalizing over the watermark key sequence\u2014each call\nto generate is equal in distribution to a sample from the original language model, i.e.,\nP(text) =\nR\n\u03be 1{text = generate(\u03be, prompt)} d\u03bd(\u03be) is equal to the original language model\u2019s\nsampling distribution.\nThe challenge of detecting watermarked text is that the detector cannot simply recom-\npute generate and compare its output against the text since they do not necessarily know the\nprompt which produced the text: in practice, users often crop the prompt when publishing\ntext from a language model. Our watermarks are agnostic in the sense that they are eas-\nily detectable with a suitable model-agnostic and prompt-agnostic test statistic \u03d5 such that\n\u03d5(generate(\u03be, prompt), \u03be) \u226a \u03d5(text, \u03be) for any text that is independent of the watermark\nkey sequence. The idea here is that the detector may use \u03d5 within detect to compute a\np-value with respect to the null hypothesis that the text is independent of the watermark key\nsequence, i.e., that the text is not watermarked.\nTo ensure detect is robust to edits of the watermarked text, the core idea underpinning\nthe design of each test statistic \u03d5 is to leverage techniques for robust sequence alignment\nto align a putative watermarked text with the watermark key sequence; we quantify the\nquality of the alignment using an \u201calignment cost\u201d specific to each watermark. The sequence\nalignment procedure ensures the watermark is detectable from even a small, corrupted block\nof watermarked text planted within some other larger text. Of course, a sufficiently motivated\n2Whether the watermark key is secret or not (e.g., if the LM provider publishes the key to allow anyone to\ndetect watermarked text) is an implementation choice that does not affect the main parts of our analysis.\n2\nand/or sophisticated user can still evade detection by simply rewriting the text from scratch\nthemselves (or, using another language model to generate the text); the point of a robust\nwatermark is simply that the amount of effort and/or resources a user requires to produce\ntext that evades watermark detection should be commensurate to what they would have\nexpended had they not had access to the watermarked language model in the first place.\nWhereas generate is a deterministic function, if our watermark produced the same text\nevery time for each prompt it would not be very useful. We resolve this limitation by designing\na wrapper around generate that calls generate using a randomly chosen subsequence of \u03be\ninstead of generating tokens from the same starting point each time. For the same reasons\nthat detect is robust to editing and cropping watermarked text, calling generate in this\nfashion does not affect watermark detectability.\nIn practice, the statistical power of our\nwatermarks improves exponentially with respect to the length of the putative watermarked\ntext and diminishes only linearly with the length of the random number sequence; thus, by\nincreasing the length of the random number sequence, we can reduce the probability of reusing\nthe same random subsequence while still ensuring our watermark has good statistical power\n(i.e., that it yields low p-values for watermarked text).\nTo remark briefly on the work most closely related to ours, we contrast the distortion-free\nproperty of our watermarks with the hashing-based watermarks of Kirchenbauer et al. [14]\nand Aaronson [1] that bias the distribution of watermarked text towards certain k-grams by\nhashing a sliding window of the previous k \u22121 tokens to determine the next token pseudoran-\ndomly. We give examples of prompts (e.g., \u201cGive me a list of 20 movies.\u201d) for which the bias\ndue to hashing is clearly noticeable in our experiments. Christ et al. [6] propose a variation\nof hashing in which the window size changes based on the entropy of the generated tokens to\navoid hash collisions with high probability. Their motivation is similar to ours in that they\nfocus on preserving the original text distribution; however, like Kirchenbauer et al. [14] and\nAaronson [1], using larger window sizes hurts robustness as an adversary can break the water-\nmark by replacing a single token in each window. Our watermark is not only distortion-free\nbut also robust to substantial corruption of the text, which is crucial in practice. We defer a\nmore thorough discussion of related work to the next section (Section 1.1).\nWe describe the details of our methodology in Section 2, wherein we give two instantiations\nof watermarks\u2014using inverse transform sampling and exponential minimum sampling\u2014and\nprovide analyses of their statistical power. We experimentally validate the power and robust-\nness of our watermarks using the OPT-1.3B, LLaMA-7B and Alpaca-7B language models in\nSection 3. Across all models, we find the second instantiation using exponential minimum\nsampling to be the most powerful. For both the OPT-1.3B and LLaMA-7B models, using\nthis watermark we can reliably detect watermarked text (p \u2264 0.01) from 35 tokens even after\ncorrupting between 40-50% of the tokens via random edits (i.e., substitutions, insertions or\ndeletions); the watermark also remains detectable from 50 tokens even after paraphrasing the\ntext by translating to French/Russian and back. For the Alpaca-7B model, we conduct a\ncase study on the feasibility of watermarking responses to typical user instructions. Due to\nthe lower entropy of the responses, detection is more difficult: around 25% of the responses\u2014\nwhose median length is around 100 tokens\u2014are detectable with p \u2264 0.01, and the watermark\nis also less robust to paraphrasing. We release code for implementing the watermark and\nreproducing the experiments in this paper, as well as additional supplementary material in-\ncluding an in-browser demo of the watermark detector3.\n3For assets and supplemental material, see: https://github.com/jthickstun/watermark.\n3\n1.1\nRelated work\nText watermarking is a special case of linguistic steganography, in that the goal is to convey a\nhidden message\u2014the watermark\u2014within a passage of text. Existing approaches to linguistic\nsteganography fall under two broad categories: edit-based methods that modify a pre-existing\ntext, and generative methods that construct a distribution over cover text [24]. Crucially, in\ncontrast to steganography, the literature on digital watermarking has historically foregrounded\nrobustness to corruption as a key attribute of a good watermark [13, 3]. In this light, a text\nwatermark should be able to withstand some perturbations of the text, thus precluding the\ndirect application of many existing techniques for linguistic steganography [7, 31, 19].\nOlder work on text watermarking considers editing a pre-existing text to include a wa-\ntermark [18, 2, 28]; for a survey of edit-based watermarks, see Kamaruddin et al. [12]. In\ncontrast, we are interested in generating watermarked text while preserving the distribution\nover the text from a language model. Work on generative watermarking is nascent, under-\nwritten by recent advances in open-ended text generation [5]. Pioneering work by Venugopal\net al. [25] proposed a generative watermark for the output of a machine translation system,\nbiasing the system towards translations with particular features that can later be detected\nusing a hypothesis test.\nOur work is most closely related to Kirchenbauer et al. [14], who watermark text from a\nlanguage model by reweighting the token log-probabilities from the model at inference time as\na function (i.e., hash) of the previous k\u22121 tokens, where k \u2208 N is a hyperparameter. In ongoing\nunpublished work concurrent to ours, Aaronson [1] describes a technique for watermarking\nlanguage models using exponential minimum sampling (a close relative of the Gumbel trick\n[16]) to sample from the model, where the inputs to the sampling mechanism are also a hash\nof the previous k \u2212 1 tokens. Neither watermark is distortion-free, and in fact we show in\nour experiments that the distortions manifest noticeably in practice (e.g., excessive repetition\nof certain tokens).\nSpecifically, both Kirchenbauer et al. [14] and Aaronson [1] bias the\ndistribution toward a subset of k-grams. Increasing k makes the bias less noticeable but hurts\nthe robustness of both watermarks, as an adversary can break the signal from a particular\ntoken by replacing any one of the previous k \u2212 1 tokens.\nAlso concurrent to our work, Christ et al. [6] propose watermarking blocks of text from a\nlanguage model by hashing each block to seed a sampler for the next block. Christ et al. [6]\nvary their block sizes\u2014which are analogous to the hyperparameter k of Kirchenbauer et al.\n[14] and Aaronson [1]\u2014as a function of the empirical entropy of the constituent tokens to avoid\nusing the same seed twice with high probability. Their work is similar to ours in that they\npreserve the original text distribution; however, the resulting watermark is not robust since in\norder to mitigate the distortion induced by hashing the block sizes must be sufficiently large\nto avoid hash collisions with high probability over all blocks and\u2014similar to Kirchenbauer\net al. [14] and Aaronson [1]\u2014replacing any token in the previous block breaks the watermark\nin the next block. Whereas Christ et al. [6]\u2014who do not run experiments\u2014choose their block\nsizes to be sufficiently large to minimize distortion, Kirchenbauer et al. [14] and Aaronson [1]\nrecommend choosing k to be a small constant in practice, which ensures a moderate amount\nof robustness by introducing some distortion.\nAn alternative approach for detecting synthetic text is to learn a classifier between syn-\nthetic and human text [11, 15].\nA key advantage of such methods over watermarking is\nthat they do not require coordination with the original producer of the text (i.e., the LM\nprovider); however, their effectiveness is distribution dependent and they do not provide a\npriori (distribution-free) guarantees on the significance level of detection (i.e., Type I errors).\n4\nFinally, we note that our setting is different from the literature on planting watermarks\nin the training data of machine learning models, e.g., to infer the model\u2019s training set or\notherwise influence the model\u2019s output [9, 10, 30]. Such watermarks are not distortion-free by\ndesign, since the point is to plant some learnable signal in the training data that influences\nthe behavior of models which train on the watermarked data.\n2\nMethodology and theoretical analysis\nLet V be a discrete set, i.e., the vocabulary, and let p \u2208 V\u2217 \u2192 \u2206(V) be an autoregressive\nlanguage model which maps a string of arbitrary length to a distribution over the vocabulary,\nwith p(\u00b7 | x) denoting the distribution of the next token given the prefix x \u2208 V\u2217. Let \u039e denote\nthe space in which lie the elements of the watermark key sequence. Recall the main protocol\n(Figure 1) which defines our problem setting:\n0. The LM provider shares a random watermark key sequence \u03be \u2208 \u039e\u2217 with the detector;\n1. The user sends a prompt x \u2208 V\u2217 to the LM provider;\n2. The LM provider generates text Y \u2208 V\u2217 by Y = generate(x, \u03be);\n3. The user publishes text eY \u2208 V\u2217, which may be either (i) (an edited version of) the\ngenerated text Y or (ii) text independent of Y (e.g., text that they wrote themselves);\n4. The detector determines if eY is watermarked\u2014i.e., if eY depends on the watermark key\nsequence\u2014by computing a p-value bp = detect(eY , \u03be) with respect to the null hypothesis\nthat eY is independent of \u03be (i.e., not watermarked).\n2.1\nProtocol details\nIn the protocol, the LM provider calls the generate method (Algorithm 1) to autoregressively\ngenerate text from a language model using a decoder function \u0393 : \u039e \u00d7 \u2206(V) \u2192 V which maps\nan element \u03bei of the watermark key and a distribution over the next token to a next token\nprediction. By design, over the randomness of \u03bei the prediction should constitute a sample\nfrom the distribution, i.e., P(\u0393(\u03bei, \u00b5) = yi) = \u00b5(yi).\nDefinition 1. A decoder \u0393 : \u039e\u00d7\u2206(V) \u2192 V is distortion-free with respect to (the distribution\nof) a random variable \u03be \u2208 \u039e if for any \u00b5 \u2208 \u2206(V) and y \u2208 V it satisfies P(\u0393(\u03be, \u00b5) = y) = \u00b5(y).\nWe relate Definition 1 to our informal definition of distortion-free text in the introduction\nthrough the following simple lemma. Assuming the conditions of the lemma are met, the only\nmaterial difference between an LM provider using generate versus sampling directly from\nthe language model is that the sequence \u03be is an input to the method rather than resampled\ni.i.d. within the method for each call. We treat the language model p, the decoder \u0393, and\ngeneration length m as internal parameters of the generate method.\nLemma 2.1. Let m, n \u2208 N with n \u2265 m. Let \u0393 be distortion free with respect to \u03bd \u2208 \u2206(\u039e)\nand let {\u03bei}n\ni=1\ni.i.d.\n\u223c \u03bd. Let Y = generate(\u03be; m, p, \u0393). Then Yi \u223c p(\u00b7 | Y:i\u22121) for i \u2208 [m].\nProof. As n \u2265 m, we have {\u03bei}m\ni=1\ni.i.d.\n\u223c \u03bd. The claim then follows immediately from applying\nDefinition 1 to Line 2 of generate for i \u2208 [m].\n5\nTo simplify the remainder of the presentation, we do not pass a prompt as input to\ngenerate. As the language model p is arbitrary and detect is model-agnostic, this simpli-\nfication is without loss of generality since p itself may model the distribution of text from\nsome base model given an arbitrary prompt. Also, unless stated otherwise, without loss of\ngenerality we let V = [N] throughout the paper, where N \u2208 N is the vocabulary size.\nAlgorithm 1: Watermarked text generation (generate)\nInput\n: watermark key sequence \u03be \u2208 \u039e\u2217\nParams: generation length m, language model p, decoder \u0393\nOutput: string y \u2208 Vm\n1 for i \u2208 1, . . . , m do\n2\nyi \u2190 \u0393(\u03bei, p(\u00b7 | y:i\u22121)) // assume len(\u03be) \u2265 m\n3 return y\nThe detector calls the detect method (Algorithm 2) to compute\u2014via a permutation\ntest with T resamples\u2014a p-value with respect to a test statistic \u03d5 : V\u2217 \u00d7 \u039e\u2217 \u2192 R for the null\nhypothesis that eY is not watermarked, i.e., that eY is independent of \u03be. The output bp of detect\nis a proper non-asymptotic p-value: if eY is not watermarked, then each (eY , \u03be(t)) constitutes\nan independent copy of (eY , \u03be) and therefore by symmetry bp is uniformly distributed over\n{1/(T + 1), 2/(T + 1), . . . , 1} for any (non-atomic) test statistic.4 If \u03d5 returns a small p-value\n(e.g., 0.0001) then the text is likely watermarked; if the p-value is large (e.g., 0.25), then the\ntext might not be.\nAlgorithm 2: Watermarked text detection (detect)\nInput\n: string y \u2208 V\u2217, watermark key sequence \u03be \u2208 \u039e\u2217\nParams: test statistic \u03d5; watermark key sequence distribution \u03bd; resample size T\nOutput: p-value bp \u2208 [0, 1]\n1 for t \u2208 1, . . . , T do\n2\n\u03be(t) \u223c \u03bd\n3\n\u03d5t \u2190 \u03d5(y, \u03be(t))\n4 bp \u2190\n1\nT+1\n\u0010\n1 + PT\nt=1 1{\u03d5t \u2264 \u03d5(y, \u03be)}\n\u0011\n5 return bp\nThe goal then is to design the test statistic \u03d5 (Algorithm 3) such that bp will typically\nbe small if eY is watermarked. In particular, the goal is to identify an alignment cost d :\n(V \u00d7 \u039e)\u2217 \u2192 R, which measures the quality of a match between a subsequence of the input\ntext and a subsequence of the watermark key, and use this to define \u03d5 as the minimum cost\nalignment between length k subsequences of the text and key.\nThis alignment-based detection strategy makes the watermark robust, since even if the\nuser crops or otherwise corrupts Y , a single block of preserved watermarked text within some\nlarger body of unwatermarked text will suffice to trigger a low p-value from detect. The\nactual form of the alignment cost will be specific to each watermark\u2014in particular, it will\ndepend on the nature of the decoder \u0393 in generate. Our most robust watermarks incorporate\n4By non-atomic, we mean for any c \u2208 R that P(\u03d5(Y, \u03be) = c) = 0 so that almost surely we will not have to\nbreak ties when computing bp. In case of ties (i.e., if the test statistic is atomic), we can either modify detect\nto break ties uniformly at random, or simply report valid but conservative p-values by leaving detect as is.\n6\na soft notion of edit distance (i.e., Levenshtein distance) into the computation of the alignment\ncost via dynamic programming, with runtime scaling quadratically in the block size. Thus,\nletting m be the length of the input text y, n be the length of the watermark key sequence \u03be,\nand k be the block size, the cost of computing the test statistic is O(mnk2).\nAlgorithm 3: Test statistic (\u03d5)\nInput\n: string y \u2208 V\u2217, watermark key sequence \u03be \u2208 \u039e\u2217\nParams: alignment cost d, block size k\nOutput: test statistic value \u03d5(y, \u03be) \u2208 R\n1 for i \u2208 1, . . . , len(y) \u2212 k + 1 do\n2\nfor j \u2208 1, . . . , len(\u03be) do\n3\nyi \u2190 {yi+\u2113}k\u22121\n\u2113=0 , \u03bej \u2190 {\u03be(j+\u2113)%len(\u03be)}k\u22121\n\u2113=0\n4\nbdi,j \u2190 d(yi, \u03bej)\n5 return mini,j bdi,j\nTo illustrate how the decoder and the alignment cost fit together, we give a simple example\nfor the toy setting of a binary vocabulary.\nExample 1:\nConsider a binary vocabulary V = {0, 1}. To generate Y \u2208 {0, 1}\u2217 from the\nmodel, the LM provider shares {\u03bei}n\ni=1\ni.i.d.\n\u223c Unif([0, 1]) with the detector and let Yi = 0 if\n\u03bei \u2264 p(0 | Y:i\u22121) and Yi = 1 otherwise. In particular, defining the decoder \u0393 by\n\u0393(\u03bei, \u00b5) :=\n(\n0\n\u03bei \u2264 \u00b5(0)\n1\n\u03bei > \u00b5(0),\nlet Y = generate(\u03be; m, p, \u0393) for some m \u2264 n. Then Y is a valid sample from the language\nmodel as P(\u03bei \u2264 p(0 | Y:i\u22121)) = p(0 | Y:i\u22121), and crucially Y and \u03be are correlated (i.e., if \u03bei is\nsufficiently close to zero then Yi = 0, and likewise if \u03bei is sufficiently close to one then Yi = 1).\nThus, we can define the alignment cost d(y, \u03be) = \u2225y \u2212 \u03be\u22251.\nAssuming for the sake of this example that n = m and the user does not corrupt the\nwatermarked text from the LM provider, i.e., eY = Y , the detector can run detect to verify\nthat eY is watermarked using the test statistic \u03d5 with alignment cost d and block size k = m.\nThe value of the test statistic will then be at most the \u21131 norm of eY \u2212 \u03be. \u2662\nIn the above example, the LM provider generates the same text each time from the water-\nmark key sequence, which is not ideal in practice. One solution for avoiding reusing elements\nof the watermark key sequence across queries is to make generate stateful, thus enabling\nthe LM provider to generate a total of \u230an/m\u230b independent watermarked text samples of m\ntokens each from the language model. Instead, to avoid persisting state, we provide a random-\nized wrapper shift-generate (Algorithm 4) around generate and modify the watermarking\nprotocol from the start of the section to allow the LM provider to call the shift-generate\ninstead of generate in the second step of the protocol. The wrapper shift-generate ran-\ndomly shifts the watermark key sequence before passing the shifted sequence to generate.\nShifting the watermark key sequence does not affect the value of the test statistic in detect,\nsince to compute the test statistic the detector anyways searches over all subsequences of the\nwatermark key sequence to find the best match for each block of text. There are n possible\nshifts, each of which may produce a distinct text; while in principle these n texts will correlate\n7\nwith each other due to sharing elements of the watermark key sequence, in practice we find\nthe effects of these correlations are not noticeable. The so-called birthday paradox [8] implies\nthe LM provider can typically expect to call shift-generate on the order of n1/2 times, each\ntime generating a different text, before reusing the same offset twice.\nAlgorithm 4: Randomized watermarked text generation (shift-generate)\nInput\n: watermark key sequence \u03be \u2208 \u039e\u2217\nParams: generation length m, language model p, decoder \u0393\nOutput: string y \u2208 Vm\n1 \u03c4 \u223c Unif([len(\u03be)]), \u03be\u2032 \u2190 {\u03be(i+\u03c4)%len(\u03be)}m\ni=1\n2 return generate(\u03be\u2032; m, p, \u0393)\n2.2\nTerminology: watermark strategies and watermark potential\nHenceforth, we use the term watermarking strategy to refer to a concrete instantiation of the\nshift-generate, generate and detect methods by specifying the internal parameters of both\nalgorithms (i.e., the decoder \u0393, the test statistic \u03d5 and the watermark key sequence distribution\n\u03bd). We give concrete watermarking strategies in the following sections (Sections 2.3 and 2.4).\nFor each watermarking strategy, we show two main results: we prove the decoder is distortion-\nfree and also obtain high probability upper bounds on the p-values of watermarked text\u2014as\na function of the length of the text and the watermark key sequence. We emphasize that\nonly the former result (i.e., that the decoder is distortion-free) is critical to the validity of our\nmain claims; we intend the latter collection of results to provide intuition for when we would\nexpect the detector to have sufficient power and to anticipate the forthcoming experimental\nresults in Section 3. The strength of the p-value upper bounds will depend on the observed\ntoken probabilities of (watermarked) text, through a quantity which we evocatively term the\nwatermark potential.\nDefinition 2. (watermark potential) Define \u03b1 : V\u2217 \u2192 R by\n\u03b1(y) := 1 \u2212\n1\nlen(y)\nlen(y)\nX\ni=1\np(yi | y:i\u22121).\nObserve the watermark potential of text from a deterministic language model is always\nzero, whereas for a high-entropy model it will approach one. The degree to which it is possible\nfor the detector to reliably distinguish watermarked text from unwatermarked text necessarily\ndepends on the watermark potential of the LM provider\u2019s language model. For example, if\nthe language model is deterministic, then any distortion-free watermark will necessarily have\nzero statistical power. We formalize this intuition by establishing the following general lower\nbound on the detection accuracy of any watermarking strategy as a function of the watermark\npotential of the original language model.\nIn particular, we lower bound the error of any\nclassifier h : V\u2217 \u00d7\u039e\u2217 \u2192 {\u22121, +1} that tries to distinguish watermarked (positive label) versus\nnonwatermarked text (negative label) given some watermark key \u03be (we make no assumption\non the distribution of \u03be except that it is independent of unwatermarked text by definition).\nWe defer the proof of Lemma 2.2 to Appendix A.\n8\nLemma 2.2. Let Y \u2032\ni \u223c p(\u00b7 | Y \u2032\n:i\u22121) for i \u2208 [m]. Let Y\nd= Y \u2032 and let \u03be \u2208 \u039e\u2217 be a random\nvariable that is independent of Y \u2032. Let h : V\u2217 \u00d7 \u039e\u2217 \u2192 {\u22121, +1} be a classifier. Let c > 0 and\ndefine the set Vc \u2282 Vm by\nVc := {y : p(yi | y:i\u22121) \u2265 exp(\u2212c/2) for all i \u2208 [m]}.\nThen\nP(h(Y, \u03be) = \u22121) + P(h(Y \u2032, \u03be) = 1) \u2265 E [exp (\u2212cm\u03b1(Y )) 1{Y \u2208 Vc}] .\nLemma 2.2 implies it is impossible to test between any watermarked and non-watermarked\ntext (i.e., between Y versus Y \u2032) that are equal in distribution (i.e., distortion-free) if the text\ntypically has low watermark potential, irrespective of the design of the watermark key; in\nparticular, the sum of the Type I and II error rates of h will be close to one if the watermark\npotential is close to zero. The theorem is not tight: depending on the language model, its\nresult may be vacuous for small values of c (e.g., the constants which appear in our upper\nbounds) since only texts whose token likelihoods all exceed exp(\u2212c/2) contribute to the lower\nbound. Also our upper bounds scale inverse exponentially with the square of the watermark\npotential, which will always be smaller than the watermark potential itself since the watermark\npotential is bounded between zero and one.\nThe point of the forthcoming p-value upper bounds for the watermarking strategies in\nSections 2.3 and 2.4 is to establish the existence of test statistics for each watermark such\nthat the statistical power of the watermark improves exponentially with the length of the text\nand decays at most linearly with the length of the watermark key sequence. The test statistics\nwe use to prove these upper bounds differ slightly from those we employ in our experiments:\nin the former case, we prioritize the simplicity of stating the bounds in terms of watermark\npotential, whereas in the latter case, we prioritize empirical performance.\n2.3\nWatermarking via inverse transform sampling\nInverse transform sampling is a general technique for sampling from a univariate distribution\nby taking the pushforward of a uniform random variable through its inverse cumulative dis-\ntribution function (CDF). Crucially, the technique is valid irrespective of the ordering of the\nCDF, a property which we presently leverage to construct a watermarking strategy in which\ngenerate is distortion-free and also detect is agnostic. In particular, we implement generate\nwith a decoder that maps a sequence of uniform random variables and permutations to tokens\nusing inverse transform sampling. To detect watermarked text, the detector correlates the\nsequence of permuted indices of the tokens in the text with the sequence of uniform random\nvariables to detect watermarked text. Meanwhile, for any nonwatermarked text, the sequence\nof permuted token indices will be i.i.d. uniform irrespective of the text itself and thus not\ncorrelate with the sequence of uniform random variables.\nFormally, with \u03a0 as the space of permutations over the vocabulary [N], for \u03be = (u, \u03c0) \u2208\n[0, 1] \u00d7 \u03a0 =: \u039e and any distribution \u00b5 \u2208 \u2206([N]), define the decoder by\n\u0393(\u03be, \u00b5) := \u03c0\u22121 (min {\u03c0(i) : \u00b5({j : \u03c0(j) \u2264 \u03c0(i)}) \u2265 u}) ,\n(1)\ni.e., \u0393(\u03be, \u00b5) is the token with the smallest index in the permutation \u03c0 such that CDF of \u00b5 with\nrespect to \u03c0 is at least u. Generalizing the intuition from Example 1, we show this decoder is\ndistortion-free in the following theorem.\n9\nTheorem 1. Define \u0393 by equation (1). Let \u03c0 \u2208 \u03a0 be arbitrary and let U \u223c Unif([0, 1]), with\n\u03be := (U, \u03c0). Then \u0393 is distortion-free with respect to \u03be.\nProof. Recalling Definition 1, the result follows from showing for any \u00b5 \u2208 \u2206([N]) and y \u2208 [N]\nthat P(\u0393(\u00b5, \u03be) = y) = \u00b5(y). To this end, by equation (1), we have \u0393(\u00b5, \u03be) = y if and only if\nU lies in the interval\n\u0002\n\u00b5({y\u2032 : \u03c0(y\u2032) < \u03c0(y)}), \u00b5({y\u2032 : \u03c0(y\u2032) \u2264 \u03c0(y)})\n\u0001\n.\nAs the width of this interval is exactly \u00b5(y), the result follows immediately.\nHaving shown that the ITS decoder is distortion-free, we now proceed to analyze the\ndetectability of the watermark. For convenience, define the normalization \u03b7 : [N] \u2192 [0, 1] by\n\u03b7(i) := (i \u2212 1)/(N \u2212 1). Analogous to the toy example, the sequences {\u03b7(\u03c0i(Yi))}m\ni=1 and U\nare correlated. Thus, for the sake of analysis, we define alignment cost d : (V \u00d7 \u039e)\u2217 \u2192 R by\nd(y, (u, \u03c0)) := \u2212\nlen(y)\nX\ni=1\n(ui \u2212 1/2) \u00b7 (\u03b7(\u03c0i(yi)) \u2212 1/2),\n(2)\ni.e., the negative covariance (each Ui and \u03b7(\u03c0i(Yi)) both have expectation 1/2).\nWe exactly characterize in Lemma 2.3 the difference in the expected value of our alignment\ncost on some text assuming the text is watermarked (i.e., generated using the same key\nas the detector) versus not watermarked in terms of the watermark potential of the text\n(Definition 2). To state the result, we define the constant C0 := Var(\u03b7(Unif([N]))), where we\nabuse notation slightly to temporarily treat \u03b7 as a pushforward map over distributions.5 We\ndefer the proof of Lemma 2.3 to Appendix B.\nLemma 2.3. Let m, n \u2208 N with n \u2265 m, where m is the generation length and n is the\nwatermark key length.\nDefine the decoder \u0393 by equation (1) and the alignment cost d by\nequation (2). Let \u03be, \u03be\u2032 i.i.d.\n\u223c Unif(\u039en) with Y = generate(\u03be; m, p, \u0393). Then almost surely for\nall i \u2208 [m] and j \u2208 [n] we have\nE[d(Yi, \u03be\u2032\nj) \u2212 d(Yi, \u03bei) | Y ] = C0 \u00b7 (1 \u2212 p(Yi | Y:i\u22121)) = C0\u03b1(Yi\u22121:i).\nSumming the result of Lemma 2.3 over i \u2208 [m] implies for any j \u2208 [n] that\nE[d(Y, \u03be\u2032\n(j+1:j+m)%n) \u2212 d(Y, \u03be1:m) | Y ] = C0m\u03b1(Y ).\nThus, we can upper bound the p-value output by detect in Lemma 2.4 using a standard\nconcentration argument and taking a union bound over j \u2208 [n].\nWe defer the proof of\nLemma 2.4 to Appendix B. In fact, we actually prove a more general result for k \u2264 m wherein\nwe allow eY to be a subsequence of Y which the user may choose adaptively. We defer this\nmore general result to Appendix B as it is more cumbersome to state.\nLemma 2.4. Let m, n \u2208 N with n \u2265 m, where m is the generation length and n is the water-\nmark key length. Define the decoder \u0393 by equation (1), alignment cost d by equation (2), and\n\u03d5 by Algorithm 3 with block size k = m. Let \u03be, \u03be\u2032 i.i.d.\n\u223c Unif(\u039en) with Y = generate(\u03be; n, p, \u0393)\nand eY = Y . Then almost surely\nP(\u03d5(eY , \u03be\u2032) \u2264 \u03d5(eY , \u03be) | eY ) \u2264 2n exp(\u2212kC2\n0\u03b1(eY )2/2).\n5Note that C0 = Var(Unif([0, 1])) + oN(1) = 1/12 + oN(1).\n10\nLemma 2.4 implies that with high probability the value of the test statistic on watermarked\ntext with the correct key will be lower than with a resampled key. In particular, ignoring\ndiscretization errors due to the finite number of resamples T in detect, the lemma implies\nwatermarked samples with watermark potential bounded away from zero (i.e., if the language\nmodel is not effectively deterministic) will have exponentially small expected p-values with\nrespect to the length m of the text. The bound grows only linearly with the length n of the\nrandom number sequence, implying for moderately large m (e.g., m = 50) an LM provider\ncan generate plenty of distortion-free watermarked text (i.e., n = 2\u2126(m) total tokens) while\nstill enabling detection of the watermark from snippets of m tokens (e.g., 50 tokens typically\namount to a couple sentences of text). Of course, recall the computational complexity of\ndetection scales linearly with n, which in practice may be a more relevant limitation than the\nstatistical power of the watermark.6\n2.3.1\nRobustness to substitutions, insertions and deletions\nWe show in Lemma 2.5 an analogous result to Lemma 2.4 holds even if an adversary corrupts\nthe original watermarked text by substituting tokens. To state the lemma, we introduce a\nquantity e\u03b1 which depends on both the corrupted and original watermarked text and accounts\nfor the decrease in the expected value of the test statistic (which recall for the original text\nis equal up to a numerical constant to the watermark potential of the text) due to token\nsubstitutions. We defer the proof of Lemma 2.5 to Appendix B.\nLemma 2.5. Let m, n \u2208 N with n \u2265 m, where m is the generation length and n is the\nwatermark key length. Define the decoder \u0393 by equation (1), alignment cost d by equation (2),\nand \u03d5 by Algorithm 3 with k = m. Let \u03be, \u03be\u2032 i.i.d.\n\u223c Unif(\u039en) with Y = generate(\u03be; m, p, \u0393) and\nlet eY \u2208 Vm be conditionally independent of \u03be and \u03be\u2032 given Y . Define\ne\u03b1(y, ey) :=\n1\nlen(y)\nlen(y)\nX\ni=1\n1{yi = eyi} (1 \u2212 p(yi | y:i\u22121)) \u2212 1{yi \u0338= eyi}\n1\nN \u2212 1.\nThen almost surely\nP(\u03d5(eY , \u03be\u2032) \u2264 \u03d5(eY , \u03be) | Y, eY ) \u2264 2n exp(\u2212kC2\n0 e\u03b1(Y, eY )2/2).\nLemma 2.5 implies that even if an adversary replaces the vast majority of tokens in a\nwatermarked text, detection with low p-values will still be possible so long as the remaining\ntokens have watermark potential bounded away from zero. In particular, the permuted indices\nof the original tokens will still positively correlate with the corresponding uniform random\nvariables from the watermark key sequence, while those of the substituted tokens will exhibit\na small negative correlation scaling as O(1/N).\nTo handle insertions and deletions, we can robustify our test statistic by incorporating a\nsoft notion of edit distance into our original alignment cost. The parameter \u03b3 in Definition 3\nassigns a cost to each insertion and deletion operation when aligning the tokens y with the\nsequence \u03be, while the base alignment cost d0 defines the quality of the alignment via a cost\nfunction over substitutions. In practice, we drop the minimizations over y\u2032 \u2208 V and \u03be\u2032 \u2208 \u039e in\nthe second and third cases respectively of the definition; we include them here to make our\nsubsequent theoretical analysis cleaner.\n6Note that both detect and the test statistic (Algorithm 3) are easily parallizeable.\n11\nDefinition 3. (Levenshtein cost) Let \u03b3 \u2208 R and d0 : V \u00d7 \u039e \u2192 R. For y \u2208 V\u2217 and \u03be \u2208 \u039e\u2217,\ndefine the Levenshtein cost d\u03b3 : V\u2217 \u00d7 \u039e\u2217 \u2192 R by\nd\u03b3(y, \u03be) := min\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\nd\u03b3(y2:, \u03be2:) + d0(y1, \u03be1)\nd\u03b3(y, \u03be2:) + miny\u2032\u2208V d0(y\u2032, \u03be1) + \u03b3\nd\u03b3(y2:, \u03be) + min\u03be\u2032\u2208\u039e d0(y1, \u03be\u2032) + \u03b3,\nwith d\u03b3(y, (u, \u03c0)) := \u03b3 \u00b7 len(y) if \u03be is empty and vice versa (as base cases).7\nRedefining the test statistic \u03d5 using d\u03b3 as the alignment cost\u2014using d0 from equation (2)\u2014\nensures detect is robust not only to substituting tokens, but also inserting and deleting tokens\nfrom watermarked text, as we show in Lemma 2.6. We defer the proof of Lemma 2.6 to\nAppendix B. To state the lemma, we first recursively define a notion of edit distance between\ntwo strings. The definition is equivalent to the minimum number of insertion and/or deletion\noperations needed to transform one string into the other (see Lemma B.2).\nDefinition 4. (edit distance) For y, ey \u2208 V\u2217, define the edit distance by\ndedit(y, ey) :=\n(\ndedit(y2:, ey2:)\ny1 = ey1\n1 + min{dedit(y2:, ey), dedit(y, ey2:)}\ny1 \u0338= ey1,\nwith dedit(y, ey) = len(y) if ey is empty and vice versa.\nLemma 2.6. Let n, m \u2208 N with n \u2265 m, where m is the generation length and n is the\nwatermark key length. Define the decoder \u0393 by equation (1), alignment cost d = d\u03b3 with d0\nfrom equation (2) and \u03b3 > 1/2, and \u03d5 by Algorithm 3 using block size k \u2264 m that divides evenly\ninto m. Let \u03be, \u03be\u2032 i.i.d.\n\u223c Unif(\u039en) with Y = generate(\u03be; m, p, \u0393). Let eY \u2208 Vm be conditionally\nindependent of \u03be and \u03be\u2032 given Y , with dedit(Y, eY ) \u2264 \u03b5m. Then almost surely\nP(\u03d5(eY , \u03be\u2032) \u2264 \u03d5(eY , \u03be) | eY , Y ) \u2264 mn(2k)k/(4\u03b3\u22121) exp(\u2212kC2\n0(\u03b1(Y ) \u2212 \u03b3\u03b5)2\n+/2).\nWe prove the result by showing there must exist a length k substring of the corrupted\ntext eY within edit distance k\u03b5 of a substring of Y that the detector will be able to distinguish\nas watermarked. For fixed k, the set of strings within edit distance \u03b5k of an original block\nwatermarked text blows up combinatorially with \u03b5. To ensure we can detect the watermark,\nthe result implies we must set \u03b3 = \u2126(1/\u03b5), which means our bound on the expected p-value\nis vacuous as soon as \u03b5 = \u2126(1/ log k). Admittedly, our analysis is not tight; for example, as\na preview of the experimental results to come, in practice we find smaller values of \u03b3 (i.e.,\n\u03b3 < 1) to perform significantly better. However, one takeaway from the result is that using\na block size k < m, where here m is the length of the input text, for detection can be an\neffective strategy when the user has substantially corrupted the text. The assumption that k\ndivides evenly into m is an artifact of our analysis and not important in practice.\n2.3.2\nWhat we run in practice\nIn practice, to reduce overhead in both generate and detect, we use a single random per-\nmutation8 instead of a full sequence, i.e., we let \u03c0i = \u03c0 for all i \u2208 [n] for \u03c0 \u223c Unif(\u03c0).\n7For y \u2208 V\u2217 (resp., \u03be \u2208 \u039e\u2217), we let ylen(y)+1: (resp., \u03belen(\u03be)+1) denote the empty string/sequence.\n8In principle, with a single random permutation the permuted token indices of both watermarked and\nnonwatermarked text are no longer conditionally independent of each other, and so the results of Lemmas 2.4,\n2.5 and 2.6 no longer apply.\nHowever, in practice we observe no degradation in statistical power.\nAlso,\nirrespective of the lemmas, the p-values from detect are still valid by construction.\n12\nRecall Theorem 1 makes no assumption about the distribution of the permutations; thus, the\nwatermark is still distortion-free. Also, for the test statistic, we find using\nd(y, (u, \u03c0)) :=\nlen(y)\nX\ni=1\n|ui \u2212 \u03b7(\u03c0i(yi))|\n(3)\nas the alignment cost performs better empirically than the alignment cost in equation (2). To\nreiterate, the output of detect is a valid p-value irrespective of the test statistic we use.\nHenceforth, we refer to this version of the watermarking strategy as ITS, and we refer\nto the corresponding Levenshtein version as ITS-edit, wherein we define the base alignment\ncost d0 by equation (3) and use the following simplified notion of Levenshtein cost:\nDefinition 5. (simple Levenshtein cost) Let \u03b3 \u2208 R and d0 : V \u00d7 \u039e \u2192 R. For y \u2208 V\u2217 and\n\u03be \u2208 \u039e\u2217, define the alignment cost function d\u03b3 : V\u2217 \u00d7 \u039e\u2217 \u2192 R by\nd\u03b3(y, \u03be) := min\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\nd\u03b3(y2:, \u03be2:) + d0(y1, \u03be1)\nd\u03b3(y, \u03be2:) + \u03b3\nd\u03b3(y2:, \u03be) + \u03b3,\nwith d\u03b3(y, (u, \u03c0)) := \u03b3 \u00b7 len(y) if \u03be is empty and vice versa (as base cases).9\nIn summary, for ITS we use the decoder from equation (1), the test statistic from Algo-\nrithm 3 with the alignment cost from equation (3), and the watermark key distribution as\nthe uniform distribution over [0, 1]n \u00d7 \u03a0, where recall n is the length of the watermark key\nsequence. Meanwhile, ITS-edit differs from ITS only in that we define the test statistic using\nthe Levenshtein cost from Definition 5 with the base cost again from equation (3).\n2.4\nWatermarking via exponential minimum sampling\nAaronson [1] proposes mapping variables in [0, 1]N to tokens in the vocabulary [N] using\nexponential minimum sampling to generate watermarked text. Whereas Aaronson [1] proposes\nthe use of distortion-inducing hashes much like Kirchenbauer et al. [14], we use exponential\nminimum sampling to implement the decoder in generate, which (after defining a suitable\ncorresponding test statistic) enables an alternative distortion-free and robust watermarking\nstrategy to inverse transform sampling. In particular, for \u03be \u2208 [0, 1]N =: \u039e and \u00b5 \u2208 \u2206([N]),\ndefine the decoder by\n\u0393(\u03be, \u00b5) := arg min\ni\u2208[N]\n\u2212 log(\u03bei)/\u00b5(i).\n(4)\nWe show this decoder is distortion-free in Theorem 2, whose proof we defer to Appendix C.\nTheorem 2. Define the decoder \u0393 by equation (4) and let \u03be \u223c Unif([0, 1]N).\nThen \u0393 is\ndistortion-free with respect to \u03be.\nFor the sake of analysis, we define the alignment cost as a slight variation of the proposal\nof Aaronson [1] (see Section 2.4.2) by\nd(y, \u03be) := \u2212\nlen(y)\nX\ni=1\nlog \u03bei,yi,\n(5)\n9For y \u2208 V\u2217 (resp., \u03be \u2208 \u039e\u2217), we let ylen(y)+1: (resp., \u03belen(\u03be)+1) denote the empty string/sequence.\n13\nagain defining the test statistic \u03d5 by Algorithm 3. Similar to Lemma 2.3 for ITS, we exactly\ncharacterize the difference in the expected values of the alignment cost on watermarked versus\nnon-watermarked text in terms of the watermark potential of the text. We defer the proof of\nLemma 2.7 to Appendix C.\nLemma 2.7. Let n \u2208 N. Define \u0393 by equation (4) and d by equation (5). Let \u03be, \u03be\u2032 i.i.d.\n\u223c\nUnif(\u039en) with Y = generate(\u03be; n, p, \u0393). Then almost surely for all i \u2208 [n] we have\nE[d(Yi, \u03be\u2032\ni) \u2212 d(Yi, \u03bei) | Y ] = 1 \u2212 p(Yi | Y:i\u22121) = \u03b1(Yi\u22121:i).\nSumming the result of Lemma 2.7 over i \u2208 [m] implies for any j \u2208 [n] that\nE[d(Y, \u03be\u2032\n(j+1:j+m)%n) \u2212 d(Y, \u03be1:m) | Y ] = m\u03b1(Y ).\nThus, defining the test statistic \u03d5 by Algorithm 3 with respect to the alignment cost d from\nEqn (5), we can again upper bound the p-value output by detect in Lemma 2.8 using a\nstandard concentration argument and taking a union bound over j \u2208 [n]. We defer the proof\nof Lemma 2.8 to Appendix C. Once again, we actually prove a more general result that allows\neY to be any length k subsequence of Y .\nLemma 2.8. Let m, n \u2208 N with n \u2265 m. Define \u0393 by equation (4), d by equation (5), and \u03d5\nby Algorithm 3 with k = m. Let \u03be, \u03be\u2032 i.i.d.\n\u223c Unif(\u039en) with Y = generate(\u03be; n, p, \u0393) and eY = Y .\nThen almost surely\nP(\u03d5(eY , \u03be\u2032) \u2264 \u03d5(eY , \u03be) | eY ) \u2264 2n exp\n\u0010\n\u2212 min{m\u03b1(eY )2/8, m\u03b1(eY )/4}\n\u0011\n.\n2.4.1\nRobustness to corruptions\nShowing high probability p-value upper bounds for corruptions of watermarked text that hold\nalmost surely given the corrupted text\u2014i.e., analogues of Lemmas 2.5 and 2.6\u2014is more diffi-\ncult, primarily due to the fact that the summands in the alignment metric from equation (5)\nare no longer bounded and thus bounding the influence of each substitution and/or insertion\noperation on the test statistic requires more careful analysis. Of course, we could in principle\ntweak the alignment metric by truncating the summands in order to prove the analogous\nresults; however, as the main intuitions would carry over from Lemmas 2.5 and 2.6 and the\nresults are not critical to the main thrust of the paper, we do not carry this plan out.\n2.4.2\nWhat we run in practice\nAs in the case of ITS, in practice we find using a slight variation of the alignment cost in\nequation (5) performs better. Namely, following the prescription of Aaronson [1], we modify\nthe previous alignment cost to instead be\nd(y, \u03be) :=\nk\nX\ni=1\nlog(1 \u2212 \u03bei,yi).\n(6)\nHenceforth, we refer to this version of the watermarking strategy as EXP, and we refer to\nthe corresponding Levenshtein version wherein we define the base alignment cost d0 by equa-\ntion (6) as EXP-edit.\n14\nIn summary, for EXP we use the decoder from equation (4), the test statistic from Algo-\nrithm 3 with the alignment cost from equation (6), and the watermark key distribution as the\nuniform distribution over \u039en, where recall n is the length of the watermark key sequence and\n\u039e = [0, 1]N. Meanwhile, EXP-edit differs from EXP only in that we define the test statistic\nusing the Levenshtein cost from Definition 5 with the base cost again from equation (6).\n3\nExperimental results\nWe empirically validate the statistical power of our watermarking strategies (i.e., ITS, ITS-\nedit, EXP, and EXP-edit) via experiments with the OPT-1.3B [29] and LLaMA-7B [23] mod-\nels.10 We run experiments using generate rather than shift-generate, mainly for the sake\nof reproducibility; recall however that this choice has no impact on the p-values we report.\nWe test for all watermarks using a block size k (in Algorithm 3) equal to the length m of\nthe text. Following the methodology of Kirchenbauer et al. [14], we generate watermarked\ntext continuations of prompts sampled from the news-like subset of the C4 dataset [17]. We\nvary the generation length m (Experiment 1) and the random number sequence length n\n(Experiment 2), and we report median p-values of watermarked text over 500 samples.11\nWe also evaluate robustness to four kinds of paraphrasing attacks: randomly substituting\na fraction of the generated tokens with tokens chosen uniformly at random from the vocab-\nulary (Experiment 3); randomly inserting a fraction of tokens among the generated tokens\n(Experiment 4); randomly deleting a fraction of the generated tokens (Experiment 5); using\nanother language model to translate the text from English to French and back (Experiment\n6). The first three attacks allow us to systematically vary the level of corruption, while the\nlast attack is an example of an attack we might encounter in the wild. We defer the details\nof the translation procedures to Appendix D.2.\nFinally, using the Alpaca-7B model and evaluation dataset [20], we conduct a case-study\non the feasibility of watermarking the responses of a performant instruction-tuned language\nmodel to user queries.\nWe also show for certain kinds of instructions that hashing-based\nwatermarks produce noticeably worse responses than our distortion-free watermarks, thus\nunderlining the importance of the distortion-free property in practice.\nIn all our experiments\u2014except for Experiment 2, where the control variable n is a hyperpa-\nrameter that is unique to our watermarks\u2014we also replicate the watermark of Kirchenbauer\net al. [14] as a baseline, setting the greenlist fraction \u03b3 = 0.25 and varying the logit bias\n\u03b4 \u2208 {1.0, 2.0}.\nWe respectively refer to these versions of their watermark as KGW-1.0 and\nKGW-2.0 after the first three authors\u2019 last names. We emphasize their watermark is not di-\nrectly comparable to our watermarks as it is not distortion-free (e.g., Kirchenbauer et al. [14]\nreport that even the weakest version we employ with \u03b4 = 1.0 and \u03b3 = 0.25 typically increases\nperplexity by 5\u201310%).\nIn their work, Kirchenbauer et al. [14] report approximate p-values, which they obtain\nfrom computing the z-score of a certain test statistic. To ensure a fair comparison, we use\ndetect (with T = 5000) to report p-values for all watermarks;12 in the case of KGW-1.0 and\nKGW-2.0, we run detect using the original inexact p-values they report as the test statistic.\n10We will also at times collectively refer to ITS and ITS-edit as the ITS watermarks and/or strategies and\nEXP and EXP-edit as the EXP watermarks and/or strategies.\n11The median p-value corresponds to the significance level (i.e., Type I error rate) at which the power of our\nwatermark detector is at least 0.5.\n12This setting of T means we never report p-values less than 1/5000 (i.e., 0.0002) in any of our experiments.\n15\nWe report error bars for the median p-value based on a bootstrapped estimate of the standard\ndeviation using 1000 resamples.\nInstead of recomputing the test statistic T times for each prompt\u2014as we originally pre-\nscribe in detect\u2014to save computation we simply sample T prompts and compute the test\nstatistic once for each ground-truth length m completion; we then use the empirical distribu-\ntion of these test statistics as the reference distribution within detect, which gives a proper\np-value with respect to the null hypothesis that the text is an original completion from the\ndataset. For reference, we include the full pseudocode for this modified version of detect in\nAppendix D.3, and we also plot the full distributions of p-values for nonwatermarked gen-\nerations (i.e., regular samples from the language models) to verify they are indeed roughly\nuniform over the interval [0, 1].\nWe defer further details regarding our experimental protocol to Appendix D.\n3.1\nVarying text and watermark key length\nWe vary the length m of watermarked text in Figure 2, fixing the watermark key length\nn = 256 for each of our watermarks and setting \u03b3 = 0.4 for ITS-edit and \u03b3 = 0.0 for EXP-\nedit (see Appendix D.4 for the details of tuning \u03b3). Our ITS watermarks slightly outperform\nKGW-1.0 while our EXP watermarks slightly outperform KGW-2.0, despite the fact that KGW-\n1.0 and KGW-2.0 both distort the text distribution. The EXP watermarks are notably more\npowerful than the ITS watermarks, requiring roughly two to three times fewer tokens to\nachieve a comparably low median p-value. One conceivable advantage of the ITS watermarks\nover the EXP watermarks is that they have comparatively less overhead: the watermark key\nfor EXP and EXP-edit is a sequence of n vectors in [0, 1]N, where recall N is the size of the\nvocabulary, while for ITS and ITS-edit it is simply a sequence of n numbers in [0, 1]. All\nwatermarking strategies perform worse on LLaMA-7B than OPT-1.3B, due to the fact that\nLLaMA-7B typically produces lower entropy text than OPT-1.3B. Due to the discrete nature\nof the test statistic of Kirchenbauer et al. [14], i.e., the number of tokens in the text belonging\nto a \u201cgreenlist\u201d versus a \u201credlist\u201d, the median p-values for the KGW-1.0 and KGW-2.0 watermarks\nare occasionally unstable, particularly for small values of m.\n16\n(a) OPT-1.3B\n(b) LLaMA-7B\nFigure 2: Median p-value of watermarked text relative to varying the text length m, for\nOPT-1.3B and LLaMA-7B models. Our watermark strategies outperform/are competitive\nwith those of Kirchenbauer et al. [14], despite the fact that they distort the text distribution\nto generate watermarked text whereas we do not.\nWe vary the length n of the watermark key sequence \u03be in Figures 3 and 4 for different\nlengths m of watermarked text from the ITS and EXP watermarks respectively. Recall n\ncorresponds to the total number of tokens we can generate while maintaining our distortion-\nfree guarantee. As our theory predicts, the p-values of watermarked text grow linearly with\nn. The rate of growth is fairly mild and decreases rapidly with m; even for n = 4096, which is\nlarger than the maximum generation length of both the OPT-1.3B and LLaMA-7B models,\nslightly increasing the number of tokens (by 4\u20138 tokens in the case of EXP, and 10\u201320 tokens\nin the case of ITS) suffices to distinguish watermarked text with roughly the same statistical\npower as n = 64.\n(a) OPT-1.3B\n(b) LLaMA-7B\nFigure 3: Median p-value of watermarked text relative to varying the watermark key length n,\nfor OPT-1.3B and LLaMA-7B models and the ITS (solid) and ITS-edit (dashed) watermarks.\nThe median p-values grow linearly with n but decay rapidly with increasing m.\n17\n(a) OPT-1.3B\n(b) LLaMA-7B\nFigure 4: Median p-value of watermarked text relative to varying the watermark key length m,\nfor OPT-1.3B and LLaMA-7B models and the EXP (solid) and EXP-edit (dashed) watermarks.\n3.2\nRobustness to corruption and paraphrasing\nWe now proceed to evaluate the robustness of our watermark strategies to various forms of\ncorruption and paraphrasing. We focus on comparing our strongest watermarks (EXP and EXP-\nedit) against KGW-2.0, deferring results for all other watermarks to Appendix D.5. As larger n\nincreases the computational overhead of computing our test statistics and the effect of larger\nn on statistical power is mild (as shown in Figure 4), we run all experiments with n = 256,\nwhich in any case is sufficiently large to ensure the watermarked text across all experiments\nis distortion-free. Decreasing the insertion/deletion penalty \u03b3 improves robustness (at least\nup to a point) but hurts the statistical power of the ITS-edit and EXP-edit watermarks for\nlarger n, since reducing the penalizer for edits effectively increases the number of candidate\nalignments under consideration. We run ITS-edit and EXP-edit with the same choices of \u03b3\nas in the previous section. We defer the details of tuning \u03b3 to Appendix D.4.\nWe vary the fraction of substituted tokens in Figure 5, and we vary the fraction of inserted\nand deleted tokens in Figures 6 and 7 respectively. For the insertion experiment, we pass only\nthe first m tokens to the detector; similarly, for the deletion experiment, we initially generate\nmore than m watermarked tokens so that even after deleting a fraction thereof, there are\nat least m tokens remaining. The EXP and EXP-edit watermarks are comparably robust to\nsubstitution errors, but the latter is far more robust to insertion and deletion errors.\nWe compare our watermarks against the most robust version of KGW-2.0, in the sense that\nwe hash only the previous token to determine the next token distribution and thus bias the\ndistribution towards some subset of bigrams. If instead we hash the previous k tokens for\nk > 1, then substituting any one of the previous k tokens will break the watermark signal in\na particular token, and thus the statistical power of their watermark will be worse than what\nwe report in our experiments.\n18\n(a) OPT-1.3B\n(b) LLaMA-7B\nFigure 5: Median p-value of watermarked text relative to the fraction of substitution errors,\nfor OPT-1.3B and LLaMA-7B models with m = 35. Both versions of the EXP watermark\nsignificantly outperform KGW-2.0, again despite KGW-2.0 distorting the text distribution.\n(a) OPT-1.3B\n(b) LLaMA-7B\nFigure 6: Median p-value of watermarked text relative to the fraction of insertion errors, for\nOPT-1.3B and LLaMA-7B models with m = 35. EXP-edit is by far the most robust.\n19\n(a) OPT-1.3B\n(b) LLaMA-7B\nFigure 7: Median p-value of watermarked text relative to the fraction of deletion errors, for\nOPT-1.3B and LLaMA-7B models with m = 35. EXP-edit is again the most robust.\nFinally, in Figures 9 and 10 we implement a \u201croundtrip translation\u201d attack, wherein we\nattempt to paraphrase watermarked texts of varying lengths by translating the (English) texts\ninto another language (i.e., French and Russian respectively) and back again using a machine\ntranslation model (details in Appendix D.2). We include a representative example of the\noriginal and (re-)translated texts in Figure 8. Using Russian is a noticeably more effective\nattack than French: none of the watermarks aside from EXP-edit are able to reliably detect\nwatermarked text with p < 0.05 irrespective of m.\nIn many cases, both using French and Russian, the roundtrip translation still preserves\nlarge chunks of the original text, which suffices for watermark detection even using EXP,\nwhich is substantially less robust to insertion and deletion errors than EXP-edit. Aside from\ninspecting a few examples, we did not verify that the roundtrip translations preserve the basic\nsemantics of the original text; thus, it is possible our results provide an overly pessimistic view\nof the robustness of our watermarks to these attacks, since in practice users would presumably\nnot publish such examples. It is also possible that using different machine translation models\u2014\nor more generally, different forms of automated paraphrasing\u2014might be far more effective in\nevading watermark detection than those we employed. We publish the full set of watermarked\ngenerations for each watermarking strategy, along with their (roundtrip) translations, as part\nof our code release.\n20\nFigure 8: An illustrative example of a roundtrip translation attack via French. Given the\nfirst 50 tokens of the roundtrip translation (highlighted in green, in addition to the closest\nmatching snippets to these tokens from the original text), detect returns bp \u2264 0.0002.\n(a) OPT-1.3B\n(b) LLaMA-7B\nFigure 9: Median p-value of watermarked text relative to the text length, after roundtrip\ntranslation via French, for OPT-1.3B and LLaMA-7B models with m = 35. EXP performs\ncomparably to EXP-edit, indicating that the roundtrip translation attack tends to preserve\nat least some snippets of the original text.\n21\n(a) OPT-1.3B\n(b) LLaMA-7B\nFigure 10: Median p-value of watermarked text relative to the text length, after roundtrip\ntranslation via Russian, for OPT-1.3B and LLaMA-7B models with m = 35. In contrast to\nFrench, EXP-edit noticeably outperforms EXP. Overall, the attack is noticeably more effective\nthan using French.\n3.3\nCase study: instruction following\nIn the wild, most users interact with language models by prompting the model with instruc-\ntions (e.g., \u201cgive me code for...\u201d), and the most widely-used language models (e.g., ChatGPT)\nare specifically fine-tuned to follow such instructions. Thus, using the instruction fine-tuned\nAlpaca-7B model, we presently conduct a case study on the effectiveness of watermarking\na performant instruction following model. In particular, we sample 200 instructions from\nthe Alpaca-7B evaluation dataset and generate watermarked responses of at most 200 tokens\nfor each. We then compute conditionally valid p-values for each response using the original\nversion of detect with T = 500. We also replicate the roundtrip translation attack from\nExperiment 6. We publish the full set of watermarked generations for each method, along\nwith their (roundtrip) translations, and the instruction prompts as part of our code release.\nWe plot the distribution of p-values for the EXP-edit and KGW-2.0 watermarks in Figure 11,\nas well as the p-values versus the watermark potential of the watermarked text in Figure 12. In\ngeneral, the Alpaca-7B responses have considerably lower per-token watermark potential than\nboth the OPT-1.3B and LLaMA-7B models, and thus the statistical power of our watermark\nis worse despite the responses typically being longer than in the previous experiments (i.e.,\nExperiments 1 and 6). In particular, based on the same random sample of 200 prompts (from\nthe Alpaca evaluation set in the case of Alpaca-7B, and from the news-like subset of the C4\ndataset in the cases of LLaMA-7B and OPT-1.3B), the average per-token watermark poten-\ntial of text from Alpaca-7B is 0.28, compared to 0.59 for LLaMA-7B and 0.67 for OPT-1.3B.\nUnlike the previous experiments, KGW-2.0 noticeably outperforms the EXP-edit watermark.\nFigure 12 indicates this difference in performance is largely due to the fact KGW-2.0 distorts\nthe distribution of the text and produces responses with noticeably larger watermark poten-\ntial than regular responses from the model. For responses whose unnormalized watermark\npotential (i.e., watermark potential multiplied by the number of tokens in the response, to\naccount for the varying lengths of the responses) exceeds roughly 60, both watermarks tend\nto yield p-values close to zero. Paraphrasing the responses via roundtrip translation attacks\n22\ninto both French and Russian degrades the statistical power of both watermarks, as we show\nin Figures 13 and 14.\n(a) EXP-edit\n(b) KGW-2.0\nFigure 11: Histogram of p-values of watermarked text from Alpaca-7B. KGW-2.0 is noticeably\nbetter than EXP-edit, though once again the results are not strictly comparable as KGW-2.0 is\nnot distortion-free.\n(a) Scatterplot of p-values.\n(b) Histogram of watermark potential.\nFigure 12: Watermark potential versus statistical power of EXP-edit versus KGW-2.0. KGW-2.0\nnoticeably distorts the text distribution, tending to produce higher watermark potential text\noverall than the original language model (and consequently, EXP-edit).\n23\n(a) EXP-edit\n(b) KGW-2.0\nFigure 13: Histogram of p-values of watermarked text after roundtrip translation via French.\nKGW-2.0 outperforms EXP-edit, albeit by noticeably distorting the text distribution.\n(a) EXP-edit\n(b) KGW-2.0\nFigure 14: Histogram of p-values of watermarked text after roundtrip translation via Russian.\nKGW-2.0 again does significantly better than EXP-edit.\nFinally, recall the main distinguishing feature of our watermark compared to Kirchenbauer\net al. [14] and Aaronson [1] is that we do not hash previous tokens to determine the distribution\nof the next token. To demonstrate the pitfalls of hashing, we implement a version of the\nwatermark Aaronson [1] proposes by modifying the generate method of EXP to obtain the\nvector \u03bei \u2208 [0, 1]N from seeding a random number generator using the previous k tokens\ninstead of using the watermark key; we call this version EXP-hash. We then prompt Alpaca-\n7B with requests for various kinds of lists. Because Alpaca-7B tends to separate items in lists\nby the same recurring token, e.g., a comma or a newline character, and because this recurring\ntoken determines the next token, for k = 1 the lists degenerate into repetition (Figure 15).13\n13The authors would like to pat themselves on the back by drawing the reader\u2019s attention to the fact that\nthe title of this paper is not among those suggested by Alpaca-7B.\n24\nFrom inspection, hashing with k > 1 substantially improves the quality of samples; how-\never, even using k = 4 can sometimes produce noticeably repetitive text. We reiterate that\nwhile increasing k may improve sample quality by making the distortions of watermarked text\nless noticeable, doing so harms the robustness of the watermark (e.g., replacing just 20% of\nthe tokens would suffice to evade detection for k = 4). Moreover, using a more robust hash\nfunction does not avoid this trade-off between robustness and distortion-freeness, as there is\na direct trade-off between the likelihood of a hash collision and the robustness of the hash.\nIn addition to Figure 15, we include more examples (for both k = 1 and k = 4) and different\nprompts in Appendix D.5.5 and our code release.\n(a) EXP-hash\n(b) EXP\nFigure 15: Example responses from Alpaca-7B to the prompt: \u201cGive me 20 ideas for the\ntitle of a paper on watermarking language models.\u201d We generate (a) by hashing the previous\ntoken to determine the inputs to the EXP decoder, while (b) is a regular sample from our EXP\nstrategy. Hashing causes the model to degenerate into repetition.\n4\nDiscussion\nIn this paper, we give the first distortion-free watermarking strategies for language models\nthat are robust to editing and/or cropping. The key idea underpinning our approach is to\nleverage methods for robust sequence alignment to align a putative watermarked text to a\nwatermark key sequence which the LM provider uses to generate watermarked text. The\nstatistical power of our watermarks improves exponentially with respect to the length of the\ntext and diminishes only linearly with respect to the length of the watermark key sequence.\nThe computational complexity of our watermark detection algorithms grows linearly with\nthe length of the watermark key sequence, which is also the total number of distortion-free\nwatermarked tokens the LM provider may generate. In contrast, the complexities of the wa-\ntermark detection algorithms of both Christ et al. [6] and also Aaronson [1] and Kirchenbauer\net al. [14] depend only on the length of the input text; however, the former watermark is\nnot robust to corruption and the latter two watermarks are not distortion-free. Whether this\napparent trade-off between computational complexity, robustness and distortion-freeness is a\nfundamental trade-off is an interesting open question.\nThe underlying assumption behind all of the above watermarking strategies including\nours is that the LM provider and the watermark detector coordinate by sharing information\n25\nin advance, e.g., a watermark key.\nIndeed, the main inherent limitation of watermarking\nis that the detector must trust the LM provider to faithfully apply the watermark when\ngenerating text. A second limitation, which is not inherent but does presently apply to all\nknown watermarks, is that the LM provider cannot release the model weights, since then users\ncould simply query the model directly instead of through the LM provider. Planting robust\nwatermarks directly into the weights of a language model without degrading the quality of\nthe model is an important direction for future work.\nRecently, several major language model providers (among others: OpenAI, Anthropic,\nGoogle and Meta) have pledged to watermark the text from their models [4]. Thus, we con-\nclude with some salient recommendations for practitioners. First, we recommend practitioners\nuse our EXP-edit watermark, as it is by far the most robust watermark of those we tested.\nSecond, though in principle the length of the watermark key sequence n\u2014which recall im-\nposes a cap on the total number of distortion-free watermarked tokens the LM provider can\ngenerate\u2014can grow (nearly) exponentially in the block size k of the test statistic while still\nenabling watermark detection from as few as k tokens, in practice we find that using a fairly\nsmall watermark key sequence (e.g., n = 256) does not noticeably affect the quality of water-\nmarked text (i.e., even when generating more than n tokens total). Our watermark detection\nprocedures (i.e., both detect and the test statistic therein from Algorithm 3) are easily par-\nallizeable, so we expect even with a very large watermark key sequence (e.g., n = 100000)\nthe computational demands of watermark detection will not be a significant bottleneck\u2014\nthough we caveat this speculation by noting that we did not ever run such large n with our\nimplementation.\nAcknowledgement\nWe thank Saminul Haque, Gary Cheng and Padma Kuditipudi for pointing out errors in pre-\nliminary drafts of this work and for their helpful feedback in general. This work is supported\nby an Open Philanthropy Project Award (OpenPhil) and an NSF Frontier Award (NSF Grant\nno. 1805310).\nReferences\n[1] S. Aaronson.\n\u2018Reform\u2019 AI Alignment with Scott Aaronson.\nAXRP - the AI\nX-risk Research Podcast,\n2023.\nURL https://axrp.net/episode/2023/04/11/\nepisode-20-reform-ai-alignment-scott-aaronson.html.\n[2] S. Abdelnabi and M. Fritz. Adversarial watermarking transformer: Towards tracing text\nprovenance with data hiding. In IEEE Symposium on Security and Privacy, 2021.\n[3] M. J. Atallah, V. Raskin, M. Crogan, C. Hempelmann, F. Kerschbaum, D. Mohamed,\nand S. Naik. Natural language watermarking: Design, analysis, and a proof-of-concept\nimplementation. In Information Hiding: 4th International Workshop, IH 2001 Pitts-\nburgh, PA, USA, April 25\u201327, 2001 Proceedings 4, pages 185\u2013200. Springer, 2001.\n[4] D. Bartz and K. Hu.\nOpenAI, Google, others pledge to watermark AI content for\nsafety, White House says. Reuters, 2023. URL https://www.reuters.com/technology/\nopenai-google-others-pledge-watermark-ai-content-safety-white-house-2023-07-21.\n26\n[5] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,\nP. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances\nin Neural Information Processing Systems 33, 33:1877\u20131901, 2020.\n[6] M. Christ, S. Gunn, and O. Zamir. Undetectable watermarks for language models. arXiv\npreprint arXiv:2306.09194, 2023.\n[7] F. Dai and Z. Cai. Towards near-imperceptible steganographic text. In Proceedings of the\n57th Annual Meeting of the Association for Computational Linguistics, pages 4303\u20134308,\n2019.\n[8] G. Elfving, G. Birkhoff, and R. von Mises. Vol. 2. probability and statistics, General. In\nSelected Papers of Richard von Mises. American Mathematical Society, 1966.\n[9] X. He, Q. Xu, L. Lyu, F. Wu, and C. Wang. Protecting intellectual property of lan-\nguage generation APIs with lexical watermark. In Proceedings of the Thirty-Sixth AAAI\nConference on Artificial Intelligence, 2022.\n[10] X. He, Q. Xu, Y. Zeng, L. Lyu, F. Wu, J. Li, and R. Jia. Cater: Intellectual property\nprotection on text generation apis via conditional watermarks. In Advances in Neural\nInformation Processing Systems 35, 2022.\n[11] G. Jawahar, M. Abdul-Mageed, and V. Laks Lakshmanan. Automatic detection of ma-\nchine generated text: A critical survey. In International Conference on Computational\nLinguistics, 2020.\n[12] N. S. Kamaruddin, A. Kamsin, L. Y. Por, and H. Rahman. A review of text watermark-\ning: theory, methods, and applications. IEEE Access, 2018.\n[13] S. Katzenbeisser and F. Petitcolas. Digital watermarking. Artech House, London, 2:2,\n2000.\n[14] J. Kirchenbauer, J. Geiping, Y. Wen, J. Katz, I. Miers, and T. Goldstein. A watermark\nfor large language models. arXiv preprint arXiv:2301.10226, 2023.\n[15] E. Mitchell, Y. Lee, A. Khazatsky, C. D. Manning, and C. Finn.\nDetectgpt: Zero-\nshot machine-generated text detection using probability curvature.\narXiv preprint\narXiv:2301.11305, 2023.\n[16] G. Papandreou and A. L. Yuille. Perturb-and-map random fields: Using discrete op-\ntimization to learn and sample from energy models. 2011 International Conference on\nComputer Vision, pages 193\u2013200, 2011.\n[17] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and\nP. J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.\nThe Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.\n[18] S. G. Rizzo, F. Bertini, and D. Montesi. Fine-grain watermarking for intellectual property\nprotection. EURASIP Journal on Information Security, 2019.\n[19] J. Shen, H. Ji, and J. Han. Near-imperceptible neural linguistic steganography via self-\nadjusting arithmetic coding. In Proceedings of Empirical Methods for Natural Language\nProcessing, pages 303\u2013313, 2020.\n27\n[20] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B.\nHashimoto. Stanford alpaca: An instruction-following llama model. https://github.\ncom/tatsu-lab/stanford_alpaca, 2023.\n[21] J. Tiedemann and S. Thottingal. OPUS-MT \u2014 Building open translation services for\nthe World. In Proceedings of the 22nd Annual Conference of the European Association\nfor Machine Translation, 2020.\n[22] J. Tiedemann, M. Aulamo, D. Bakshandaeva, M. Boggia, S.-A. Gr\u00a8onroos, T. Niemi-\nnen, A. Raganato, Y. Scherrer, R. Vazquez, and S. Virpioja. Democratizing machine\ntranslation with opus-mt. arXiv preprint arXiv:2212.01936, 2022.\n[23] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi`ere,\nN. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language\nmodels. arXiv preprint arXiv:2302.13971, 2023.\n[24] H. Ueoka, Y. Murawaki, and S. Kurohashi.\nFrustratingly easy edit-based linguistic\nsteganography with a masked language model. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, 2021.\n[25] A. Venugopal, J. Uszkoreit, D. Talbot, F. J. Och, and J. Ganitkevitch. Watermarking the\noutputs of structured prediction with an application in statistical machine translation.\nIn Proceedings of Empirical Methods for Natural Language Processing, 2011.\n[26] J. Vincent.\nAI-generated answers temporarily banned on coding Q&A site Stack\nOverflow. The Verge, 2022. URL https://www.theverge.com/2022/12/5/23493932/\nchatgpt-ai-generated-answers-temporarily-banned-stack-overflow-llms-dangers.\n[27] M. J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cam-\nbridge University Press, 2019.\n[28] X. Yang, J. Zhang, K. Chen, W. Zhang, Z. Ma, F. Wang, and N. Yu. Tracing text\nprovenance via context-aware lexical substitution.\nIn Proceedings of the Thirty-Sixth\nAAAI Conference on Artificial Intelligence, 2022.\n[29] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li,\nX. V. Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint\narXiv:2205.01068, 2022.\n[30] X. Zhao, Y.-X. Wang, and L. Li. Protecting language generation models via invisible\nwatermarking. arXiv preprint arXiv:2302.03162, 2023.\n[31] Z. Ziegler, Y. Deng, and A. M. Rush. Neural linguistic steganography. In Proceedings of\nEmpirical Methods for Natural Language Processing, pages 1210\u20131215, 2019.\nA\nProof of Lemma 2.2\nProof. To show the claim, we first lower bound the probability that Y = Y \u2032. In particular,\nP(Y = Y \u2032) =\nX\ny\nP(Y = y)P(Y \u2032 = y)\n28\n=\nX\ny\nP(Y = y)\nY\ni\u2208[m]\np(yi | y:i\u22121)\n=\nX\ny\nP(Y = y)\nY\ni\u2208[m]\n(1 \u2212 (1 \u2212 p(yi | y:i\u22121)))\n(\u22c6)\n\u2265\nX\ny\u2208Vc\nP(Y = y) exp\n\uf8eb\n\uf8ed\u2212c\nX\ni\u2208[m]\n1 \u2212 p(yi | y:i\u22121)\n\uf8f6\n\uf8f8\n\u2265 E [exp (\u2212cm\u03b1(Y )) 1{Y \u2208 Vc}] ,\nwhere (\u22c6) follows from exp(\u2212cx) \u2264 1 \u2212 x for 0 \u2264 x \u2264 1 \u2212 exp(\u2212c/2).\nIt then follows\nimmediately that\nDTV ((Y, \u03be)||(Y \u2032, \u03be)) \u2264 P((Y, \u03be) \u0338= (Y \u2032, \u03be))\n\u2264 1 \u2212 E [exp (\u2212cm\u03b1(Y )) 1{Y \u2208 Vc}] .\nObserve for any event A that\nDTV ((Y, \u03be)||(Y \u2032, \u03be)) \u2265 P((Y, \u03be) \u2208 A) \u2212 P((Y \u2032, \u03be) \u2208 A),\nand thus\nP((Y, \u03be) \u2208 A) + P((Y \u2032, \u03be) /\u2208 A) \u2265 P((Y, \u03be) \u2208 A) + P((Y, \u03be) /\u2208 A) \u2212 DTV ((Y, \u03be)||(Y \u2032, \u03be))\n\u2265 E [exp (\u2212cm\u03b1(Y )) 1{Y \u2208 Vc}] .\nThe desired result thus follows from letting A be the event that h predicts \u22121.\nB\nAnalysis of inverse transform sampling\nTo prove the main theorems, we introduce the following supporting lemma. Recall C0 =\nVar(\u03b7(Unif([N]))).\nLemma B.1. Let \u00b5 \u2208 \u2206([N]). Let (U, \u03c0) \u223c Unif([0, 1]) \u00d7 Unif(\u03a0) and Y = \u0393((U, \u03c0), \u00b5).\nThen\n1\nC0 Cov(U, \u03b7(\u03c0(Y )) | Y ) = 1 \u2212 \u00b5(Y ) almost surely.\nProof. We first characterize the conditional distribution of \u03c0 given Y and the conditional\ndistribution of U given both \u03c0 and Y , where recall \u03c0 and Y are discrete. Applying Bayes\u2019\nformula and Theorem 1, we have\nP(\u03c0 | Y ) = P(Y | \u03c0)P(\u03c0)\nP(Y )\n(\u22c6)\n= \u00b5(Y )P(\u03c0)\nP(Y )\n= P(\u03c0).\n(7)\nAlso, defining the interval\nI(Y, \u03c0) := [P({y : \u03c0(y) < \u03c0(Y )}), \u00b5({y : \u03c0(y) \u2264 \u03c0(Y )})] ,\nfor any interval I \u2282 [0, 1] we have\nP(U \u2208 I | Y, \u03c0)\n(a)\n= P(Y | U \u2208 I, \u03c0)P(U \u2208 I)P(\u03c0)\n\u00b5(Y )P(\u03c0)\n(b)\n= |I \u2229 I(Y, \u03c0)|\n\u00b5(Y )\n(c)\n= |I \u2229 I(Y, \u03c0)|\n|I(Y, \u03c0)|\n,\n(8)\n29\nwhere (a) follows from Bayes\u2019 formula and the independence of U and \u03c0; (b) follows from the\ndefinition (1) of the decoder \u0393; and (c) follows from I(Y, \u03c0) \u2282 [0, 1] having width equal to \u00b5(Y ).\nThe displays (7) and (8) respectively imply \u03c0 | Y \u223c Unif(\u03a0) and U | \u03c0, Y \u223c Unif(I(Y, \u03c0)),\nfrom which it follows that\nE [U | Y, \u03c0(Y )] = E\n\u0014\n\u00b5({y : \u03c0(y) < \u03c0(Y )}) + |I(Y, \u03c0)|\n2\n\f\f\f\f Y, \u03c0(Y )\n\u0015\n= (\u03c0(Y ) \u2212 1) (1 \u2212 \u00b5(Y ))\nn \u2212 1\n+ \u00b5(Y )\n2\n= 1/2 + (\u03b7(\u03c0(Y )) \u2212 1/2) (1 \u2212 \u00b5(Y )) .\nBy symmetry, we have E[U] = E[\u03b7(\u03c0(Y ))] = 1/2, the former because P(Y | U) = P(Y | 1\u2212U)\nfor any U and the latter because recall \u03c0 | Y is uniform over \u03a0. Thus, marginalizing the\npreceding display over \u03c0(Y ) gives\nCov(U, \u03b7(\u03c0(Y )) | Y ) = E [(U \u2212 1/2) (\u03b7(\u03c0(Y )) \u2212 1/2) | Y ]\n= (1 \u2212 \u00b5(Y ))Var(\u03b7(\u03c0(Y )) | Y ),\nfrom which the desired result follows immediately from recalling \u03c0(Y ) | Y \u223c Unif([N]) and\nthe definition of the constant C0.\nB.1\nProof of Lemma 2.3\nProof. Recall by definition\nd(Yi, \u03bei) = \u2212(Ui \u2212 1/2) \u00b7 (\u03b7(\u03c0i(Yi)) \u2212 1/2),\nwhere (as in the proof of Lemma B.1) we have E[Ui | Y ] = E[\u03b7(\u03c0i(Yi)) | Y ] = 1/2. Lemma B.1\nthus implies E[d(Yi, \u03bei) | Y ] = \u2212C0 \u00b7 (1 \u2212 p(Yi | Y:i\u22121)), while trivially E[d(Yi, \u03be\u2032\nj) | Y ] = 0 as\nY and \u03be\u2032 are independent. The result follows immediately.\nB.2\nProof of Lemma 2.4\nWe prove the following more general result, from which Lemma 2.4 follows as a corollary.\nLemma B.2. Let m, n \u2208 N with n \u2265 m, where m is the generation length and n is the water-\nmark key length. Define the decoder \u0393 by equation (1), alignment score d by equation (2), and\n\u03d5 by Algorithm 3 with block size k \u2264 m. Let \u03be, \u03be\u2032 i.i.d.\n\u223c Unif(\u039en) with Y = generate(\u03be; n, p, \u0393).\nLet eY be a substring of Y of length at least k that is conditionally independent of \u03be and \u03be\u2032\ngiven Y , i.e., eY = Y\u03c4+1:\u03c4+\u2113 for \u2113 \u2265 k. Then for b\u03b1 := 1 \u2212 1\nk\nP\u03c4+k\ni=\u03c4+1 p(Yi | Y:i\u22121), almost surely\nP(\u03d5(eY , \u03be\u2032) \u2264 \u03d5(eY , \u03be) | eY , Y ) \u2264 2n exp\n\u0000\u2212kC2\n0 b\u03b12/2\n\u0001\n.\nProof. Recall by definition\nd(y, (u, \u03c0)) = \u2212\nlen(y)\nX\ni=1\n(ui \u2212 1/2) \u00b7 (\u03b7(\u03c0i(yi)) \u2212 1/2),\n(9)\nLemma 2.3 and the conditional independence of \u03c4 and \u03be given Y imply for any j \u2208 [n] that\nE[d(eY1:k, \u03be\u2032\n(j+1:j+k)%n) | Y, eY ] \u2212 E[d(eY1:k, \u03be\u03c4+1:\u03c4+k) | Y, eY ] = kC0b\u03b1.\n30\nEach summand in equation (9) lies between \u22121/4 and 1/4, and also (Ui, \u03c0i) is conditionally\nindependent of U\u2212i and \u03c0\u2212i given Y . Thus, Hoeffding\u2019s inequality [27, Proposition 2.5] implies\nfor j \u2208 [n] that\nP\n\u0010\nd(eY , \u03be\u2032\n(j+1:j+k)%n) \u2264 d(eY , \u03be\u03c4+1:\u03c4+k) | Y, eY\n\u0011\n\u2264 P\n\u0010\nd(eY , \u03be1:m) \u2212 E[d(eY , \u03be1:m)] \u2265 kC0b\u03b1/2 | Y, eY\n\u0011\n+ P\n\u0010\nE[d(eY , \u03be\u2032\nj+1:j+m)] \u2212 d(eY , \u03be\u2032\nj+1:j+m) \u2265 kC0b\u03b1/2 | Y, eY\n\u0011\n\u2264 2 exp\n\u0000\u2212mC2\n0 b\u03b12/2\n\u0001\n.\nRecalling the definition of the test statistic \u03d5 via Algorithm 3, the main claim then follows\nfrom taking a union bound over all j \u2208 [n].\nB.3\nProof of Lemma 2.5\nProof. We begin with the following observation for a single token.\nObservation B.1. Let P \u2208 \u2206([N]). Let (U, \u03c0) \u223c Unif([0, 1])\u00d7Unif(\u03a0) and Y = \u0393((U, \u03c0), P).\nLet eY \u2208 [N] be conditionally independent of (U, \u03c0) given Y . If eY \u0338= Y , then almost surely\nCov(U, \u03b7(\u03c0(eY )) | Y, eY ) = \u2212\n1\nN \u2212 1Cov(U, \u03b7(\u03c0(Y )) | Y, eY ).\nProof of Observation B.1. Observe the conditional distribution of \u03c0(eY ) given Y is uniform\nover [N] \\ {\u03c0(Y )}. Let X be a random variable that is equal to \u03b7(\u03c0(Y )) with probability\n1/N and otherwise equal to \u03b7(\u03c0(eY )). Observe X is independent of Y and thus also U by\nassumption\u2014in particular, (N \u2212 1)X + 1 | Y \u223c Unif([N]) irrespective of the value of Y . The\nclaim thus follows from rearranging terms in the equality\n0 = Cov(U, X | Y, eY ) = 1\nN Cov(U, \u03b7(\u03c0(Y )) | Y, eY ) + N \u2212 1\nN\nCov(U, \u03b7(\u03c0(eY )) | Y, eY ).\nLemma 2.3 and Observation B.1 together imply for any j \u2208 [n] that\nE[d(eY , \u03be\u2032\nj+1:j+m) | eY , Y ] \u2212 E[d(eY , \u03be1:m) | eY , Y ] = mC0e\u03b1(Y, eY ),\ni.e., by adding the two results together using Observation B.1 to account for the influence of\neach substituted token on the expectation. Using the same concentration argument as in the\nproof of Theorem 2.4, we then have\nP\n\u0010\nd(eY , \u03be\u2032\nj+1:j+m) \u2264 d(eY , \u03be1:m) | eY , Y\n\u0011\n\u2264 P\n\u0010\nd(eY , \u03be1:m) \u2212 E[d(eY , \u03be1:m)] \u2265 me\u03b1(Y, eY )/2 | eY , Y\n\u0011\n+ P\n\u0010\nE[d(eY , \u03be\u2032\nj+1:j+m)] \u2212 d(eY , \u03be\u2032\nj+1:j+m) \u2265 me\u03b1(Y, eY )/2 | eY , Y\n\u0011\n\u2264 2 exp\n\u0010\n\u2212mC2\n0 e\u03b1(Y, eY )2/2\n\u0011\n.\nRecalling the definition of the test statistic \u03d5 via Algorithm 3, the main claim then follows\nfrom taking a union bound over all j \u2208 [n] and recalling k = m by assumption.\n31\nB.4\nProof of Lemma 2.6\nProof. We begin with the following useful facts about edit distance. Throughout, let S(y)\ndenote the set of substrings of a string y \u2208 V\u2217, including the empty string.\nObservation B.2. Let y, ey \u2208 V\u2217. Then dedit(y, ey) is the length of the smallest sequence of\ninsertion and/or deletion operations to obtain ey from y.\nProof of Observation B.2. We proceed via induction on the sum len(y) + len(ey). The base\ncase where y and ey are both empty is trivial. Now suppose the claim holds all strings whose\nlengths sum to at most len(y) + len(ey) \u2212 1. Recalling the definition of dedit (Definition 4),\nthere are three cases.\nFirst, suppose dedit(y, ey) = dedit(y2:, ey2:). Then by induction there exists a sequence of\ndedit(y, ey) insertion and/or deletion operations to obtain ey2: from y2:. Because y1 = ey1, the\nsame sequence suffices to obtain ey from y and thus the claim follows.\nSecond, suppose dedit(y, ey) = 1+dedit(y2:, ey). Again by induction, there exists a sequence of\ndedit(y, ey)\u22121 insertion and/or deletion operations to obtain ey from y2:. It follows immediately\n(i.e., by first deleting y1) there exists a sequence of dedit(y, ey) such operations to obtain ey from\ny, and so the claim holds.\nThe third case follows by symmetry with the second case.\nObservation B.3. Let y, ey \u2208 V\u2217. Then for any \u03c4 < len(y), we have\ndedit(y, ey) \u2265\nmin\ny\u2032\u2208S(ey) dedit(y:\u03c4, y\u2032) + min\ny\u2032\u2208S(ey) dedit(y\u03c4+1:, y\u2032).\nProof of Observation B.3. Observation B.2 implies there exists a sequence of dedit(y, ey) in-\nsertion and/or deletion operations to obtain ey from y. We may partition this sequence of\noperations into sequences based respectively on whether they occur on y:\u03c4 or y\u03c4+1:. Let eypre\nbe the result of performing the first sequence of operations on y:\u03c4 and let eysuf be the result of\nperforming the second sequence of operations on y\u03c4+1:. Then ey is the concatenation of eypre\nand eysuf, and so the claim follows from the fact that\ndedit(y, ey) = dedit(y:\u03c4, eypre) + dedit(y\u03c4+1:, eysuf)\n\u2265\nmin\ny\u2032\u2208S(ey) dedit(y:\u03c4, y\u2032) + min\ny\u2032\u2208S(ey) dedit(y\u03c4+1:, y\u2032).\nObservation B.4. Let y, ey \u2208 V\u2217 and \u03be \u2208 \u039e\u2217. Then d\u03b3(y, \u03be) \u2264 \u03b3dedit(y, ey) + d\u03b3(ey, \u03be).\nProof of Observation B.4. The case dedit(y, ey) = 0 is trivial as we then have y = ey. Now\nsuppose dedit(y, ey) = 1, and let i be the first index such that yi \u0338= eyi. Then, unrolling the\nrecursive definition of d\u03b3(eyi:, \u03bej:), there must exist c \u2208 R and an index j such that both\nd\u03b3(ey, \u03be) = c + d\u03b3(eyi:, \u03bej:) and d\u03b3(y, \u03be) \u2264 c + d\u03b3(yi:, \u03bej:). Moreover, from the definition of edit\ndistance, either yi+1: = eyi: or vice versa.\nWe claim d\u03b3(yi:, \u03bej:) \u2264 d\u03b3(eyi:, \u03bej:) + \u03b3. If yi+1: = eyi:, then the claim obtains as\nd\u03b3(yi:, \u03bej:) \u2264 d\u03b3(yi+1:, \u03bej:) + min\n\u03be\u2032\u2208\u039e d0(yi, \u03be\u2032) + \u03b3\n(\u22c6)\n\u2264 d\u03b3(yi+1:, \u03bej:) + \u03b3\n= d\u03b3(eyi:, \u03bej:) + \u03b3,\n32\nwith (\u22c6) following from the fact that d0(yi, \u03be\u2032) = 0 for \u03be\u2032 = (1/2, \u03c0) irrespective of yi and \u03c0.\nOtherwise, if yi: = eyi+1:, then from unrolling the recursive definition of d\u03b3(eyi:, \u03bej:) there\nmust exist some index j\u2032 \u2265 j such that either\nd\u03b3(eyi:, \u03bej:) = d\u03b3(eyi+1:, \u03bej\u2032:) + \u03b3 + min\n\u03be\u2032\u2208\u039e d0(eyi, \u03be\u2032) +\nX\nj\u2264\u2113<j\u2032\n\u03b3 + min\ny\u2032\u2208V d0(y\u2032, \u03be\u2113)\nor\nd\u03b3(eyi:, \u03bej:) = d\u03b3(eyi+1:, \u03bej\u2032+1:) + d0(eyi, \u03bej\u2032) +\nX\nj\u2264\u2113<j\u2032\n\u03b3 + min\ny\u2032\u2208V d0(y\u2032, \u03be\u2113).\nIn the first case, we have \u03b3 + min\u03be\u2032\u2208\u039e d0(eyi, \u03be\u2032) > 0 since \u03b3 > 1/2 by assumption, and so the\nclaim follows as\nd\u03b3(yi:, \u03bej:) \u2264 d\u03b3(yi:, \u03bej\u2032:) +\nX\nj\u2264\u2113<j\u2032\n\u03b3 + min\ny\u2032\u2208V d0(y\u2032, \u03be\u2113)\n= d\u03b3(eyi+1, \u03bej\u2032:) +\nX\nj\u2264\u2113<j\u2032\n\u03b3 + min\ny\u2032\u2208V d0(y\u2032, \u03be\u2113)\n< d\u03b3(eyi:, \u03bej:).\nIn the second case, we have d0(eyj) the claim follows as\nd\u03b3(yi:, \u03bej:) \u2264 d\u03b3(yi:, \u03bej\u2032+1:) +\nX\nj\u2264\u2113<j\u2032+1\n\u03b3 + min\ny\u2032\u2208V d0(y\u2032, \u03be\u2113)\n= d\u03b3(eyi+1, \u03bej\u2032+1:) +\nX\nj\u2264\u2113<j\u2032+1\n\u03b3 + min\ny\u2032\u2208V d0(y\u2032, \u03be\u2113)\n\u2264 d\u03b3(eyi:, \u03bej:) + \u03b3.\nThus, assuming dedit(y, ey) \u2264 1, we have shown d\u03b3(yi:, \u03bej:) \u2264 d\u03b3(eyi:, \u03bej:) + \u03b3, from which\nit follows that d\u03b3(y, \u03be) \u2264 d\u03b3(ey, \u03be) + \u03b3. The general result follows immediately by applying\nObservation B.2 and summing the bound for a single edit over the (smallest) sequence of edits\nto obtain ey from y.\nProceeding with the main proof, define for convenience the quantity\nb\u03b1\u03c4 := 1\nk\nk\nX\ni=1\np(Y\u03c4+i | Y:\u03c4+i\u22121).\nObserve\n\u03b1(Y ) = k\nm\nm/k\u22121\nX\n\u03c4=0\nb\u03b1k\u03c4,\n(10)\nwhile Observation B.3 together with our assumption that dedit(Y, eY ) \u2264 \u03b5m implies\nk\nm\nm/k\u22121\nX\n\u03c4=0\nmin\nY \u2032\u2208S(eY )\ndedit(Yk\u03c4+1:k\u03c4+k, Y \u2032) \u2264 k\u03b5.\n(11)\n33\nThe displays (10) and (11) together imply there exists an index \u03c4 and Y \u2032 \u2208 S(eY ) such that\nb\u03b1\u03c4 \u2212 1\nk minY \u2032\u2208S(eY ) dedit(Y\u03c4+1:\u03c4+k, Y \u2032) \u2265 \u03b1(Y ) \u2212 \u03b5. Reusing the same concentration argument\nas in the proof of Theorem 2.4, for t \u2265 0 we have\nP (d0(Y\u03c4+1:\u03c4+k, \u03be\u03c4+1:\u03c4+k) \u2265 \u2212k (C0b\u03b1\u03c4 + t) | Y ) \u2264 exp\n\u0000\u22122kt2\u0001\n,\nand thus from Observation B.4 it follows that\nP\n\u0010\nd\u03b3(Y \u2032, \u03be\u03c4+1:\u03c4+k) \u2265 \u2212k (C0\u03b1(Y ) \u2212 \u03b3\u03b5 + t) | eY , Y\n\u0011\n\u2264 exp\n\u0000\u22122kt2\u0001\n.\nLetting t = (C0\u03b1 \u2212 \u03b3\u03b5)/2 and recalling the definition of the test statistic, we have\nP\n\u0010\n\u03d5(eY , \u03be) \u2265 \u2212k (C0\u03b1(Y ) \u2212 \u03b3\u03b5) /2 | eY , Y\n\u0011\n\u2264 exp\n\u0000\u2212k(C0\u03b1(Y ) \u2212 \u03b3\u03b5)2\n+/2\n\u0001\n.\n(12)\nAll that remains to bound the probability of \u03d5(eY , \u03be\u2032) exceeding the threshold from the\nabove display. To this end, define the set-valued map N\u03b2(y) := {y\u2032 : dedit(y, y\u2032) \u2264 \u03b2/(4\u03b3\u22121)}.\nThen we make the following observation.\nObservation B.5. For any y \u2208 V\u2217 and \u03be \u2208 \u039e\u2217, there exists y\u2032 \u2208 Nlen(\u03be)(y) such that\nd\u03b3(y, \u03be) = \u03b3 \u00b7 dedit(y, y\u2032) + d0(y\u2032, \u03be).\nProof. We proceed via induction. The base case where y and \u03be both have length 1 follows\ntrivially by taking y\u2032 = y; in particular, \u03b3 > 1/2 implies d(y, \u03be) \u2264 \u03b3+miny\u2032 d(y\u2032, \u03be) and likewise\nd(y, \u03be) \u2264 \u03b3 + min\u03be\u2032 d(y, \u03be\u2032). Now suppose the result holds so long as len(y) + len(\u03be) \u2264 n \u2212 1.\nWe claim that the result must then also hold if the lengths sum to at most n.\nWe prove this inductive claim by considering three exhaustive cases. First, suppose that\nd\u03b3(y, \u03be) = d\u03b3(y2:, \u03be2:) + d(y1, \u03be1). By our induction hypothesis, there exists \u02c6y \u2208 Nlen(\u03be)\u22121(y2:)\nsuch that d\u03b3(y2:, \u03be2:) = \u03b3 \u00b7dedit(y2:, \u02c6y)+d(\u02c6y, \u03be2:). The desired result then obtains with y\u2032 as the\nconcatenation of y1 and \u02c6y. Second, suppose d\u03b3(y, \u03be) = d\u03b3(y, \u03be2:) + min\u03be\u2032\u2208\u039e d(y1, \u03be\u2032) + \u03b3. By\nour induction hypothesis, there exists \u02c6y \u2208 Nlen(\u03be)=1(y) such that d\u03b3(y2:, \u03be) = \u03b3 \u00b7 dedit(y2:, \u02c6y) +\nd(\u02c6y, \u03be2:). The result obtains with y\u2032 = \u02c6y. Finally, suppose d\u03b3(y, \u03be) = d\u03b3(y2:, \u03be) + d(y\u2032\u2032, \u03be1) + \u03b3\nfor some y\u2032\u2032 \u2208 V.\nBy our induction hypothesis, there exists \u02c6y \u2208 Nlen(\u03be)\u22121(y) such that\nd\u03b3(y2:, \u03be) = \u03b3 \u00b7 dedit(y2:, \u02c6y) + d(\u02c6y, \u03be). The result then obtains by concatenating y\u2032\u2032 with by.\nLet Ij := {(j + i)%n}k\ni=1. For any 0 \u2264 i \u2264 len(eY ) \u2212 k and j \u2208 [n], Observations B.4\nand B.5 together imply that\nd\u03b3(eYi+1:i+k, \u03be\u2032\nIj) =\nmin\ny\u2208Nk(eYi+1:i+k)\n\u03b3 \u00b7 dedit(eYi+1:i+k, y) + d0(y, \u03be\u2032\nIj)\n(13)\n(\u22c6)\n=\nmin\ny\u2208Nk/4(\u03b3\u22121)(eYi+1:i+k)\n\u03b3 \u00b7 dedit(y, eYi+1:i+k) + d0(y, \u03be\u2032\nIj),\n(14)\nwhere (\u22c6) follows from the fact that dedit(eYi+1:i+k, y) > k/4(\u03b3 \u2212 1) implies\n\u03b3 \u00b7 dedit(eYi+1:i+k, y) + d0(y, \u03be\u2032\nIj) \u2265 k/4 > d0(eYi+1:i+k, \u03be\u2032\nIj),\nand therefore the minimizer in equation (13) must be an element of Nk/4(\u03b3\u22121)(eYi+1:i+k).\nBy construction, N\u03b2(y) consists of the set of strings obtainable from y by a sequence of at\nmost \u03b2 insertion and/or deletion operations. Now define another set-valued map N\u03b2,\u2212(y) as\n34\nthe restriction of N\u03b2(y) such that we may only insert a particular token into y (which token\nis immaterial). As the specific identity of each token we insert into y can only influence the\nvalue of d\u03b3 by \u00b11/2, for any \u03b2 it follows that\nmin\ny\u2208N\u03b2(eYi+1:i+k)\n\u03b3 \u00b7 dedit(y, eYi+1:i+k) + d0(y, \u03be\u2032\nIj) \u2265\nmin\ny\u2208N\u03b2,\u2212(eY )\nd0(y, \u03be\u2032\nIj),\nand so, letting \u03b2 = k/4(\u03b3 \u2212 1), from equation (14) we have\nd\u03b3(eYi+1:i+k, \u03be\u2032\nIj) \u2265\nmin\ny\u2208N\u03b2,\u2212(eYi+1:i+k)\nd0(y, \u03be\u2032\nIj)\nLet eY (i, \u2113) denote the \u2113-th element of N\u03b2,\u2212(eYi+1:i+k) for some eY -measurable indexing. From\nthe independence of eY and \u03be\u2032, we have E[d0(eY (i, \u2113), \u03beIj) | eY ] = 0 for any \u2113 and j.\nThe\ncardinality of N\u03b2,\u2212(eYi+1:i+k) is equal to the number of possible combinations of locations for\n\u03b2 insertion and/or deletion operations on eY , of which there are at most (k + \u03b2)\u03b2 \u2264 (2k)\u03b2.\nThus, applying the same concentration argument as in the proof of Theorem 2.4 and taking\na union bound over all i \u2264 m \u2212 k, j \u2264 n and \u2113 \u2264 (2k)\u03b2, we have\nP(\u03d5(eY , \u03be\u2032) \u2264 \u2212\u03b1(Y )/2 + \u03b3\u03b5 | eY , Y ) \u2264 mn(2k)k/(4\u03b3\u22121) exp(\u2212kC2\n0(\u03b1(Y ) \u2212 \u03b3\u03b5)2\n+/2).\n(15)\nCombining the displays (12) and (15) via another union bound gives the desired result.\nC\nAnalysis of exponential minimum sampling\nTo prove the main theorems, we introduce the following supporting lemma. The result is well\nknown and we restate it here only for completeness.\nLemma C.1. Let \u00b5 \u2208 \u2206([N]) and \u03be \u223c Unif([0, 1]N). Then for any y \u2208 [N] we have\nP(\u0393(\u03be, \u00b5) = y, \u2212 log(\u03bey)/\u00b5(y) \u2265 t) = \u00b5(y) exp(\u2212t).\nProof. Suppose \u00b5(y) > 0 as otherwise the claim is trivial. Recalling \u03bei\ni.i.d.\n\u223c Unif([0, 1]), for\nany \u03bb > 0 we have \u2212\u03bb log \u03bei\ni.i.d.\n\u223c Exp(\u03bb), i.e.,\nP(\u2212\u03bb log \u03bei \u2265 t) = P(\u03bei \u2264 exp(\u2212\u03bbt)) = exp(\u2212\u03bbt).\nThus, the claim follows as\nP(\u0393(\u03be, \u00b5) = y, \u2212 log(\u03bey)/\u00b5(y) \u2265 t)\n= P(y = arg min\ni\n\u2212 log(\u03bei)/\u00b5(i), \u2212 log(\u03bey)/\u00b5(y) \u2265 t)\n(\u22c6)\n=\nZ\nu\u2265t\n\u00b5(y) exp(\u2212\u00b5(y)u) \u00b7 \u03a0i\u2208supp(\u00b5),i\u0338=yP(\u2212 log(\u03bei)/\u00b5(i) > u)\n=\nZ\nu\u2265t\n\u00b5(y) exp(\u2212\u00b5(y)u) \u00b7 \u03a0i\u2208supp(\u00b5),i\u0338=y exp(\u2212\u00b5(i)u)\n= \u00b5(y)\nZ\nu\u2265t\n\u03a0i\u2208supp(\u00b5) exp(\u2212\u00b5(i)u)\n= \u00b5(y)\nZ\nu\u2265t\nexp(\u2212u)\n= \u00b5(y) exp(\u2212t),\nwhere in (\u22c6) we use the fact that the density of \u2212 log(\u03bey)/\u00b5(y) at u is \u00b5(y) exp(\u2212\u00b5(y)u).\n35\nC.1\nProof of Theorem 2\nProof. The result follows immediately from integrating the result of Lemma C.1 over t \u2265\n0.\nC.2\nProof of Lemma 2.7\nProof. Lemma C.1 implies \u2212 log(\u03bei)/p(Yi | Y:i\u22121) | Y \u223c Exp(1), and thus E[\u2212 log(\u03bei) | Y ] =\np(Yi | Y:i\u22121). Meanwhile, as \u03be\u2032\ni \u223c Unif([0, 1]) independently of Y , we have\nP(\u2212 log \u03be\u2032\ni \u2265 t | Y ) = P(\u03be\u2032\ni \u2264 exp(\u2212t)) = exp(\u2212t),\nimplying \u2212 log(\u03be\u2032\ni) | Y \u223c Exp(1) and so E[\u2212 log(\u03be\u2032\ni) | Y ] = 1. The result follows immediately,\nrecalling \u03b1(Yi\u22121:i) = 1 \u2212 p(Yi | Yi\u22121) by definition.\nC.3\nProof of Lemma 2.8\nWe prove the following general result, from which Lemma 2.8 follows as a corollary.\nLemma C.2. Let m, n \u2208 N with n \u2265 m, where m is the generation length and n is the water-\nmark key length. Define the decoder \u0393 by equation (4), alignment score d by equation (5), and\n\u03d5 by Algorithm 3 with block size k \u2264 m. Let \u03be, \u03be\u2032 i.i.d.\n\u223c Unif(\u039en) with Y = generate(\u03be; n, p, \u0393).\nLet eY be a substring of Y of length at least k that is conditionally independent of \u03be and \u03be\u2032\ngiven Y , i.e., eY = Y\u03c4+1:\u03c4+\u2113 for \u2113 \u2265 k. Then for b\u03b1 := 1 \u2212 1\nk\nP\u03c4+k\ni=\u03c4+1 p(Yi | Y:i\u22121), almost surely\nP(\u03d5(eY , \u03be\u2032) \u2264 \u03d5(eY , \u03be) | eY , Y ) \u2264 2n exp\n\u0000\u2212 min{kb\u03b12/8, kb\u03b1/4}\n\u0001\n.\nProof. Recall by definition\nd(y, \u03be) = \u2212\nlen(y)\nX\ni=1\nlog \u03bei,yi.\nLemma 2.7 and the conditional independence of \u03c4 and \u03be given Y imply for any j \u2208 [n] that\nE[d(eY , \u03be\u2032\n(j+1:j+k)%n) | eY , Y ] \u2212 E[d(eY , \u03be\u03c4+1:\u03c4+k) | eY , Y ] = kb\u03b1.\nFrom Lemma C.1, we have \u2212 log \u03be\u03c4+i,eYi | eY , Y \u223c Exp(\u03b3i) for some \u03b3i \u2264 1 for all i \u2208 [m].\nAlso, from the independence of eY and \u03be\u2032, we have \u2212 log \u03be\u2032\nj,eYi | eY , Y \u223c Exp(1) for all i \u2208 [m]\nand j \u2208 [n]. The following observation thus implies \u2212 log \u03bei,eYi | eY , Y and \u2212 log \u03be\u2032\nj,eYi | eY , Y are\nboth (2, 2)-subexponential random variables.\nObservation C.1. Let X \u223c Exp(1). Then X is a (2, 2) subexponential random variable.\nProof of Observation C.1. For t < 1/2, we have\nE[et(X\u2212E[X])] =\nZ \u221e\n0\net(x\u22121)e\u2212x dx\n(a)\n=\ne\u2212t\n1 \u2212 t\n(b)\n\u2264 (1 \u2212 t + t2)(1 + t + 2t2)\n36\n(c)\n\u2264 (1 + 2t2)\n\u2264 e2t2,\nwhere (a) follows from the fact that t < 1 (otherwise, the integral would not be finite); (b)\nfollows from Taylor expanding e\u2212t and 1/(1 \u2212 t) and applying the fact that t < 1/2 to bound\nthe higher-order terms; and (c) again follows from t < 1/2. The claim follows immediately.\nThus, using the fact that \u03bei is conditionally independent of \u03be\u2212i given Y , a standard\nChernoff bound [27, Proposition 2.9] implies for each j \u2208 [n] that\nP\n\u0010\nd(eY , \u03be\u2032\nj+1:j+k) \u2264 d(eY , \u03be\u03c4+1:\u03c4+k) | eY , Y\n\u0011\n\u2264 P\n\u0010\nd(eY , \u03be1:m) \u2212 E[d(eY , \u03be1:m)] \u2265 kb\u03b1/2 | eY , Y\n\u0011\n+ P\n\u0010\nE[d(eY , \u03be\u2032\nj+1:j+m)] \u2212 d(eY , \u03be\u2032\nj+1:j+m) \u2265 kb\u03b1/2 | eY , Y\n\u0011\n\u2264 2 exp\n\u0000\u2212 min{kb\u03b12/8, kb\u03b1/4}\n\u0001\n.\nRecalling the definition of the test statistic \u03d5 via Algorithm 3, the main claim then follows\nfrom taking a union bound over all j \u2208 [n].\nD\nDetails of experiments\nD.1\nExperimental protocol\nIn Experiments 1-6, for each watermark we first generate a sequence tokens, decode the\ntokens into text (i.e., a string) using the appropriate tokenizer for the language model, and\nthen encode the text back into tokens before running detect. Each generation is coditioned\non a prompt; we obtain the prompts by sampling documents from the news-like subset of the\nC4 dataset and truncating the last m tokens. We enforce a minimum prompt size of 50 tokens\nin all experiments; we skip over any document that is not long enough. The retokenization\nis not always equal to the original tokens; in order to ensure detect always receives at least\nm tokens, we pad its input with special pad tokens (specific to each model\u2019s tokenizer). We\nalso initially generate a number of buffer tokens beyond m, so in most cases the padding is\nunnecessary. We set the number of buffer tokens to be 20 in every experiment except for\nExperiment 5, where we set it to be 100 in order to ensure that even after deleting tokens\nthere are typically still at least m tokens remaining. We always truncate the number of tokens\ngiven to detect to be at most m, irrespective of the number of buffer tokens.\nD.2\nRoundtrip translation\nIn Experiment 6, we perform round-trip translations from English to French and from English\nto Russian using the OPUS-MT collection of translation models [21, 22]. Specifically, we use\nthe versions of these models hosted on the HuggingfaceHub14, associated with the identifiers:\n\u2022 Helsinki-NLP/opus-mt-tc-big-en-fr - English to French,\n\u2022 Helsinki-NLP/opus-mt-tc-big-fr-en - French to English,\n14https://huggingface.co/\n37\n\u2022 Helsinki-NLP/opus-mt-en-ru - English to Russian,\n\u2022 Helsinki-NLP/opus-mt-ru-en - Russian to English.\nD.3\nComputing p-values\nAs we mention previously, to save computation we modify detect to use a fixed reference\ndistribution to compute p-values. For the sake of concreteness, we give the full pseudocode\nfor the modified version of detect in Algorithm 5; in Experiments 1-6, we compute p-values\nusing Algorithm 6 to construct the reference distribution using the news-like subset of the C4\ndataset as the text distribution.\nAlgorithm 5: Watermarked text detection with fixed reference distribution\nInput\n: string y \u2208 V\u2217, seed sequence \u03be \u2208 \u039e\u2217\nParams: test statistic \u03d5; reference distribution {\u03d5t}T\nt=1\nOutput: p-value bp \u2208 [0, 1]\n1 bp \u2190 1\nT\nPT\nt=1 1{\u03d5(y, \u03be) < \u03d5t}\n2 return bp\nAlgorithm 6: Reference distribution construction\nInput\n: resample size T \u2208 N, text length m \u2208 N, watermark key sequence\ndistribution \u03bd \u2208 \u2206(\u039en)\nParams: test statistic \u03d5; text distribution P; minimum prompt length m0\nOutput: reference distribution {\u03d5t}T\nt=1 \u2208 RT\n1 t \u2190 1\n2 while t \u2264 T do\n3\nY \u223c P\n4\nif len(Y ) \u2264 m0 + m then\n5\ncontinue\n6\n\u03be \u223c P\u03be\n7\n\u03d5t \u2190 \u03d5(Y\u2212m:, \u03be)\n8\nt \u2190 t + 1\n9 return {\u03d5t}T\nt=1\nAs a sanity check, we include histograms of the p-values we compute for nonwatermarked\ntext for each method to verify that they are roughly uniformly distributed on the interval\n[0, 1] (setting m = 50 and sampling prompts from the news-like subset of the C4 dataset, as\nin Experiment 1). In the cases of KGW-1.0 and KGW-2.0, the distribution is not quite uniform\ndue to the discrete nature of their test statistics.\n38\n(a) OPT-1.3B\n(b) LLaMA-7B\nFigure 16: Distribution of p-values for nonwatermarked text using ITS detector.\n(a) OPT-1.3B\n(b) LLaMA-7B\nFigure 17: Distribution of p-values for nonwatermarked text using ITS-edit detector.\n39\n(a) OPT-1.3B\n(b) LLaMA-7B\nFigure 18: Distribution of p-values for nonwatermarked text using EXP detector.\n(a) OPT-1.3B\n(b) LLaMA-7B\nFigure 19: Distribution of p-values for nonwatermarked text using EXP-edit detector.\n40\n(a) OPT-1.3B\n(b) LLaMA-7B\nFigure 20: Distribution of p-values for nonwatermarked text using KGW-1.0 detector.\n(a) OPT-1.3B\n(b) LLaMA-7B\nFigure 21: Distribution of p-values for nonwatermarked text using KGW-2.0 detector.\nD.4\nHyperparameter tuning\nThere are two hyperparameters involved in computing each of our watermark test statistics\n(i.e., Algorithm 3), the block size k and the alignment score d. We do not tune the block size\nk for our experiments, instead simply letting k = m, i.e., the text length, and the alignment\nscore is also fixed for each of our watermarks, except for the hyperparameter \u03b3 in both ITS-\nedit and EXP-edit. Smaller values of \u03b3 (at least to a certain point) tend to make these\nwatermarks more robust to insertion and deletion errors, as Figure 22 illustrates, but also\nhurts their statistical power for large values of n, i.e., the watermark key length, as Figure 23\nillustrates. We set \u03b3 = 0.4 for ITS-edit and \u03b3 = 0.0 for EXP-edit to balance these two\ncompeting desiderata.\n41\n(a) ITS-edit\n(b) EXP-edit\nFigure 22: Median p-value of watermarked text for varying \u03b3, with OPT-1.3B models and\nm = 70 for ITS-edit and m = 35 for EXP-edit, after corrupting the text with random\ninsertions (fraction of inserted tokens is 0.1 for ITS-edit and 0.6 for EXP-edit).\n(a) ITS-edit\n(b) EXP-edit\nFigure 23: Median p-value of watermarked text, varying \u03b3 and n, with OPT-1.3B model and\nm = 40 for ITS-edit and m = 10 for EXP-edit.\n42\nD.5\nDeferred results\nD.5.1\nExperiment 3\n(a) OPT-1.3B\n(b) LLaMA 7B\nFigure 24: Median p-value of watermarked text relative to the fraction of substitution errors,\nfor OPT-1.3B and LLaMA 7B models with m = 35.\n(a) OPT-1.3B\n(b) LLaMA 7B\nFigure 25: Median p-value of watermarked text relative to the fraction of substitution errors,\nfor OPT-1.3B and LLaMA 7B models with m = 70.\n43\n(a) OPT-1.3B\n(b) LLaMA 7B\nFigure 26: Median p-value of watermarked text relative to the fraction of substitution errors,\nfor OPT-1.3B and LLaMA 7B models with m = 70.\nD.5.2\nExperiment 4\n(a) OPT-1.3B\n(b) LLaMA 7B\nFigure 27: Median p-value of watermarked text relative to the fraction of insertion errors, for\nOPT-1.3B and LLaMA 7B models with m = 35.\n44\n(a) OPT-1.3B\n(b) LLaMA 7B\nFigure 28: Median p-value of watermarked text relative to the fraction of insertion errors, for\nOPT-1.3B and LLaMA 7B models with m = 70.\n(a) OPT-1.3B\n(b) LLaMA 7B\nFigure 29: Median p-value of watermarked text relative to the fraction of insertion errors, for\nOPT-1.3B and LLaMA 7B models with m = 70.\n45\nD.5.3\nExperiment 5\n(a) OPT-1.3B\n(b) LLaMA 7B\nFigure 30: Median p-value of watermarked text relative to the fraction of deletion errors, for\nOPT-1.3B and LLaMA 7B models with m = 35.\n(a) OPT-1.3B\n(b) LLaMA 7B\nFigure 31: Median p-value of watermarked text relative to the fraction of deletion errors, for\nOPT-1.3B and LLaMA 7B models with m = 70.\n46\n(a) OPT-1.3B\n(b) LLaMA 7B\nFigure 32: Median p-value of watermarked text relative to the fraction of deletion errors, for\nOPT-1.3B and LLaMA 7B models with m = 70.\nD.5.4\nExperiment 6\n(a) OPT-1.3B\n(b) LLaMA 7B\nFigure 33: Median p-value of watermarked text relative to the fraction of insertion errors,\nafter roundtrip translation via French, for OPT-1.3B and LLaMA 7B models with m = 35.\n47\n(a) OPT-1.3B\n(b) LLaMA 7B\nFigure 34: Median p-value of watermarked text relative to the text length, after roundtrip\ntranslation via Russian, for OPT-1.3B and LLaMA 7B models with m = 35.\nD.5.5\nInstruction following case study\nWe give three examples of instructions for which hashing produces qualitatively worse re-\nsponses than regular samples from the language model:\n1. \u201cGive me 20 ideas for the title of a paper on watermarking language models.\u201d\n2. \u201cGive me 20 ideas for startup names.\u201d\n3. \u201cGive me a list of 20 movies.\u201d\nWe format each of the instructions as described by Taori et al. [20] before calling the model.\nWe compare samples from our EXP watermark strategy,15 which are equivalent to regular\nsamples from the language model, to samples from KGW-2.0 and the hashing-based version of\nEXP we describe in the main text (i.e., the watermark of Aaronson [1]), i.e., EXP-hash. For\nboth EXP and KGW-2.0, we generate the samples using five different random seeds (the hash\nfunction in KGW-2.0 is fixed in the implementation of Kirchenbauer et al. [14]), whereas in the\ncase of EXP-hash we use five different hash functions (namely, we let the previous k tokens\n{yi}k\ni=1 hash to j + Pk\ni=1 yi for j \u2208 {0, . . . , 4}). We label each sample using the seed/hash we\nused to generate it. We include samples from two versions of EXP-hash: one where we hash\nthe previous tokens (k = 1) and another where we hash the previous four tokens (k = 4). For\nKGW-2.0, we only hash the previous token since the public implementation of Kirchenbauer\net al. [14] does not include the option to hash more tokens.\nWe find that EXP-hash with k = 1 often produces qualitatively worse responses that\ndegenerate into repetition. With k = 4, the repetition is substantially less noticeable, though\noccasionally it still manifests. In contrast, even when we only hash the previous token, the\nrepetition of KGW-2.0 is not nearly as noticeable as in EXP-hash. We speculate this is due\nto stochasticity of KGW-2.0 (i.e., KGW-2.0 biases the distribution over the next token to a\nsubset of tokens but still ultimately samples from this distribution randomly). Of course, this\nstochasticity comes at a price: KGW-2.0 was generally less powerful compared to the EXP and\nEXP-edit strategies in our other experiments.\n15Recall both EXP and EXP-edit use the same generate method.\n48\nWe include sample sheets for all methods for the first instruction below. To avoid excessive\nclutter, we defer the sample sheets for the remaining two instructions to our code release.\nMethod\nSeed\nResponse\nEXP-hash\n(k = 1)\n0\n1. Watermarking Deep Learning Models: A Review 2. Understanding and Eval-\nuating Watermarking Techniques for Neural Networks 3. Towards Unobtrusive\nand Invisible Watermarking for Neural Networks 4. Customizable Watermark-\ning for Neural Networks 5. Reverse Engineering Watermarked Neural Networks\n6. Understanding and Exploiting Watermarking Attacks on Neural Networks 7.\nCombatting Watermarking Attacks on Neural Networks 8. Customizable Wa-\ntermarking for Neural Networks 9. Customizable Watermarking for Neural Net-\nworks 10. Customizable Watermarking for Neural Networks 11. Customizable\nWatermarking for Neural Networks 12. Customizable Watermarking for Neural\nNetworks 13. Customizable Watermarking for Neural Networks 14. Customiz-\nable Watermarking for Neural Networks 15. Customizable Watermarking for\nNeural Networks 16. Customizable Watermarking for Neural Networks 17. Cus-\ntomizable Watermarking for Neural Networks 18. Customizable Watermarking\nfor Neural Networks 19. Customizable Watermarking for Neural Networks\nEXP-hash\n(k = 1)\n1\n1. Exploiting Watermarks in Language Modeling 2. Watermarking Language\nModels: State of the Art 3. Keys to the Kingdom: Refining Watermarking\nLanguage Models 4. Exploiting Watermarks in Natural Language Processing 5.\nKeys to the Kingdom: Refining Watermarking Language Models 6. Keys to the\nKingdom: Refining Watermarking Language Models 7. Keys to the Kingdom:\nRefining Watermarking Language Models 8. Keys to the Kingdom: Refining\nWatermarking Language Models 9.\nKeys to the Kingdom: Refining Water-\nmarking Language Models 10. Keys to the Kingdom: Refining Watermarking\nLanguage Models 11. Keys to the Kingdom: Refining Watermarking Language\nModels 12. Keys to the Kingdom: Refining Watermarking Language Models\n13. Keys to the Kingdom: Refining Watermarking Language Models 14. Keys\nto the Kingdom: Refining Watermarking Language Models 15. Keys to the\nKingdom: Refining Watermarking Language Models 16. Keys to the Kingdom:\nRefining Watermarking Language Models 17. Keys to the Kingdom: Refining\nWatermarking Language Models 18. Keys to the Kingdom: Refining Water-\nmarking Language Models 19. Keys to the Kingdom: Refining Watermarking\nLanguage Models 20. Keys to the Kingdom: Refining Watermarking Language\nModels\n49\nEXP-hash\n(k = 1)\n2\n1.\n\u201dMaking Watermarks: Creative Approaches to Watermarking Language\nModels\u201d 2. \u201dMaking Watermarks: Creative Approaches to Watermarking Lan-\nguage Models\u201d 3. \u201dMaking Watermarks: Creative Approaches to Watermark-\ning Language Models\u201d 4. \u201dMaking Watermarks: Creative Approaches to Wa-\ntermarking Language Models\u201d 5. \u201dMaking Watermarks: Creative Approaches\nto Watermarking Language Models\u201d 6. \u201dMaking Watermarks: Creative Ap-\nproaches to Watermarking Language Models\u201d 7. \u201dMaking Watermarks: Cre-\native Approaches to Watermarking Language Models\u201d 8. \u201dMaking Watermarks:\nCreative Approaches to Watermarking Language Models\u201d 9. \u201dMaking Water-\nmarks: Creative Approaches to Watermarking Language Models\u201d 10. \u201dMak-\ning Watermarks: Creative Approaches to Watermarking Language Models\u201d 11.\n\u201dMaking Watermarks: Creative Approaches to Watermarking Language Mod-\nels\u201d 12.\n\u201dMaking Watermarks: Creative Approaches to Watermarking Lan-\nguage Models\u201d 13. \u201dMaking Watermarks: Creative Approaches to Watermark-\ning Language Models\u201d 14. \u201dMaking Watermarks: Creative Approaches to Wa-\ntermarking Language Models\u201d 15. \u201dMaking Watermarks: Creative Approaches\nto Watermarking Language Models\u201d 16. \u201dMaking Watermarks: Creative Ap-\nproaches to Watermarking Language Models\u201d 17. \u201dMaking Watermarks: Cre-\native Approaches to Watermarking Language Models\u201d 18.\n\u201dMaking Water-\nmarks: Creative Approaches to Watermarking Language Models\u201d 19. \u201dMak-\ning Watermarks: Creative Approaches to Watermarking Language Models\u201d 20.\n\u201dMaking Watermarks: Creative Approaches to Watermarking Language Mod-\nels\u201d\nEXP-hash\n(k = 1)\n3\n1.\nWatermarking Language Models: A Review 2.\nWatermarking Language\nModels for Copyright Protection 3. Watermarking Language Models for Foren-\nsic Analysis 4. Watermarking Language Models for Audio Signals 5. Water-\nmarking Language Models for Video Signals 6. Watermarking Language Models\nfor Documents 7. Watermarking Language Models for Software 8. Watermark-\ning Language Models for Big Data 9.\nWatermarking Language Models for\nNatural Language Processing 10. Watermarking Language Models for Machine\nLearning 11. Watermarking Language Models for Deep Learning 12. Water-\nmarking Language Models for Speech Recognition 13. Watermarking Language\nModels for Audio Synthesis 14. Watermarking Language Models for Computer\nVision 15. Watermarking Language Models for Robotics 16. Watermarking\nLanguage Models for Autonomous Driving 17. Watermarking Language Models\nfor Medical Image Processing 18. Watermarking Language Models for Medical\nDiagnostics 19. Watermarking Language Models for Text Translation\n50\nEXP-hash\n(k = 1)\n4\n1. Image based watermarking for language models 2. Steganographic water-\nmarking for language models 3. Timestamps for language models 4. Water-\nmarking for language model improvisation 5. Secure watermarking for language\nmodel sharing 6. Image based watermarking for language model improvisation\n7.\nTimestamps for language model improvisation 8.\nWatermarking for lan-\nguage model evolution 9. Secure watermarking for language model evolution\n10. Image based watermarking for language model evolution 11. Timestamps\nfor language model evolution 12. Watermarking for language model generation\n13. Image based watermarking for language model generation 14. Timestamps\nfor language model generation 15. Watermarking for language model under-\nstanding 16. Image based watermarking for language model understanding 17.\nTimestamps for language model understanding 18. Watermarking for language\nmodel improvisation and understanding 19. Image based watermarking for lan-\nguage model improvisation and understanding\nTable 1: Responses from EXP-hash with k = 1 to the prompt: \u201cGive me 20 ideas for the title\nof a paper on watermarking language models.\u201d\nMethod\nSeed\nResponse\nEXP-hash\n(k = 4)\n0\n1. \u201dSafeguarding Language Models: Exploring Watermarking for Vertext and\nImage Models\u201d 2.\n\u201dWatermarking Language Models: Protecting Your Cre-\nations\u201d 3. \u201dThe Future of Watermarking Language Models: Ethical and Prac-\ntical Considerations\u201d 4. \u201dSecuring Language Models: Exploring Emerging Tech-\nnologies\u201d 5. \u201dWatermarking Language Models: Balancing Creativity and Se-\ncurity\u201d 6. \u201dWatermarking Language Models: Balancing Security and Acces-\nsibility\u201d 7. \u201dWatermarking Language Models: Defending your Creativity\u201d 8.\n\u201dWatermarking Language Models: Defending your Creativity with Emerging\nTechnologies\u201d 9. \u201dProtecting Language Models: Exploring Emerging Technolo-\ngies\u201d 10. \u201dSecuring Language Models: Balancing Security and Accessibility\u201d\n11.\n\u201dThe Future of Watermarking Language Models: Ethical and Practical\nConsiderations\u201d 12. \u201dWatermarking Language Models: Balancing Security and\nAccessibility\u201d 13. \u201dWatermarking Language Models: Balancing Security and\nAccessibility with Emerging Technologies\u201d 14. \u201dWatermarking Language Mod-\nels: Defending your Creativity with Emerging Technologies\u201d 15. \u201dWatermark-\ning Language Models: Defending your Creativity with Emerging Technologies\u201d\n16.\n\u201dSecuring Language Models: Balancing Security and Accessibility with\nEmerging Technologies\u201d 17. \u201dExploring Watermarking for Vertext and Image\nModels\u201d 18. \u201dWatermarking Language Models: Balancing Security and Ac-\ncessibility with Emerging Technologies\u201d 19. \u201dDefending your Creativity with\nEmerging Technologies\u201d\n51\nEXP-hash\n(k = 4)\n1\n1. Towards a New Era of Transparent Language Models 2. A Review of the\nState of Watermarking Language Models 3. The Benefits of Embedding Wa-\ntermarks in Language Models 4. Protecting Language Models with Multiscale\nWatermarks 5. Impact of Watermarking on the Performance of Language Mod-\nels 6. A Survey on Watermarking for Language Models 7. Practical Perspectives\non Watermarking for Language Models 8. A Comprehensive Study on Design-\ning Watermarks for Language Models 9. Overview of Techniques for Adding\nWatermarks to Language Models 10. Exploring the Possibilities of Watermark-\ning for Language Models 11. How to Incorporate Watermarks in Your Language\nModel 12. The Science behind Watermarking for Language Models 13. AI for\nInsertion of Watermarks in Language Models 14. The Role of Machine Learning\nin Watermarking for Language Models 15. Future Trends in Watermarking for\nLanguage Models 16. A Review on Watermarking for Language Models 17. Ap-\nplications of Watermarking in Language Modeling 18. A Comprehensive Study\non Designing Robust Watermarks for Language Models 19. A Novel Approach\nto Incorporate Watermarks in Your Language Model.\nEXP-hash\n(k = 4)\n2\n1. Securing Your Language Model 2. Stamping Out Unauthorized Use 3. Col-\noring Outside the Lines: Creative Watermarks 4. Avoiding Watermarks: Best\nPractices 5. Authentication Made Easy with Watermarks 6. Defending Your\nLanguage Model 7. Unique Identifiers: Adding Value to your Model 8. Con-\nnected Learning: Leveraging Watermarks 9. The Problem with Open Access 10.\nHow to Effectively Mark a Language Model 11. Making a Splash with Creative\nWatermarks 12.\nUnderstanding the Benefits of Watermarking 13.\nUtilizing\nWatermarks for Better Attribution 14. Stewarding Your Language Model 15.\nThe Role of Technology in Watermarking 16. Beyond the Horizon: Adaptive\nWatermarking 17. The Art of Discretion in Watermarking 18. Harnessing the\nPower of Invisible Watermarks 19. Practical Considerations for Watermarking\nEXP-hash\n(k = 4)\n3\n1. Elucidating Watermarking Strategies for Language Models 2. Innovative\nTechniques for Watermarking Language Models 3. Intelligent and Innovative\nWatermarking Strategies for Language Models 4. Overview of Watermarking\nStrategies for Language Models 5. Emerging Technologies in Watermarking for\nLanguage Models 6. Practical Perspectives on Watermarking Language Models\n7. Reducing Watermarking Strategies for Language Models 8. Robust Tech-\nniques for Watermarking Language Models 9. Security Factors in Watermarking\nLanguage Models 10.\nUnderstanding Watermarking Strategies for Language\nModels 11. Evaluation of Watermarking Strategies for Language Models 12.\nExploring Watermarking Strategies for Language Models 13.\nInnovations in\nWatermarking Language Models 14. Intelligent Techniques for Watermarking\nLanguage Models 15. Methods for Watermarking Language Models 16. Over-\nlapping Security Factors in Watermarking Language Models 17. Understanding\nWatermarking Strategies for Language Models 18. Emerging Technologies in\nWatermarking Language Models 19.\nExploring Watermarking Strategies for\nLanguage Models 20. Intelligent and Innovative Watermarking Strategies for\nLanguage Models\n52\nEXP-hash\n(k = 4)\n4\n1. Securing Language Models 2. Extra Secure Language Models 3. Defending\nLanguage Models 4. Protecting Language Models 5. Cryptographic Language\nModels 6. Unique Language Models 7. Robust Language Models 8. Marking\nLanguage Models 9. Preventing Language Models 10. Hiding Language Models\n11. Invisible Language Models 12. Steganographic Language Models 13. Wa-\ntermarking Language Models 14. Preserving Language Models 15. Uniquely\nIdentifiable Language Models 16. Characterizing Language Models 17. Distin-\nguishable Language Models 18. Language Model Defence 19. Ultra Defence\nLanguage Models\nTable 2: Responses from EXP-hash with k = 4 to the prompt: \u201cGive me 20 ideas for the title\nof a paper on watermarking language models.\u201d\nMethod\nSeed\nResponse\nKGW-2.0\n0\n1. Steganography in Language Models: A Review 2. Combating Model Piracy\nwith Watermarking 3. Techniques for Traceability and Watermarking 4. Salient\nFeatures for Traceability and Watermarking 5.\nReflections on Model Secu-\nrity and Watermarking 6. Insights on Model Security and Watermarking 7.\nProtecting Models with Watermarking 8. Techniques for Esoteric Traceabil-\nity and Watermarking 9. Ins and Outs of Steganography in Language Models\n10. Technologies for Efficient Watermarking 11. Vision Beyond Model Piracy:\nWatermarking Perspectives 12.\nImpact of Model Security on Watermarking\n13.\nEmerging Trends in Watermarking of Language Models 14.\nFuture of\nWatermarking Techniques for Language Models 15. Drivers for Success in Wa-\ntermarking Language Models 16. Robustness of Models against Watermarking\n17. Taking Security beyond Watermarking 18. Leveraging Model History and\nWatermarking 19.\nTechniques for Secure Watermarking 20.\nComprehensive\nOverview of Model Security and Watermarking.\n53\nKGW-2.0\n1\n1. Embedding Digital Signatures: Towards Traceable and Transparent Lan-\nguage Models 2. Stamping Out Deception: Recording Attribution in Language\nModels 3. Defining Your Digital Dashboard: Watermarking for Traceability\n4.\nScaling Trust in Language Models:\nWatermarking for Traceability and\nTransparency 5.\nTaking an East Asian Approach to Transparency in Lan-\nguage Models 6.\nRiding the Big Five: Towards Transparency in Language\nModels 7. Measuring the \u201dPs\u201d of Language Models: Perceptual and Practical\nTransparency 8. Marking a Mark: Comprehensive Examination of Attribution\nin Language Models 9.\nVisualizing Deception: Comprehensive Examination\nof Deceptive Language Models 10. Peak Performance: Evaluating Language\nModels for Transparency 11. Closing the Loop: Examining the Long-term Im-\npacts of Language Models 12. Diversifying Perspective: Impacts of Modelling\nLanguage 13. Disclosing the Hidden Layers: Understanding Transparency in\nLanguage Models 14. Auditable: Making Language Models Accountable 15.\nLabelling Human Language: Exploring the Capabilities of Language Models\n16. Setting the Record: Estimating the Real-world Impacts of Language Mod-\nels 17. Language Models: Towards Traceable, Transparent and Accountable\nSystems 18. Think Outside the Box: Exploring Future Trends in Language\nModels 19. Embedding Trust: Towards Traceable and Transparent Language\nModels.\nKGW-2.0\n2\n1. Defending Latent Space with Secure Watermarks 2. Emergent Waveforms:\nA Watermarking Approach 3. Embedding Digital Identities in Neural Networks\n4. Marking up Models: A Survey on Watermarking 5. Leveraging Linked Data\nfor Watermarking 6.\nStamping Out Superfluous Models 7.\nInspecting and\nMarking Models 8. Rethinking Digital Identity with Watermarks 9. Marking\nUp Models: A Usage-Driven Approach 10. Bridge the Distance with Digital\nWatermarks 11. Introducing Transparency in Model Formation 12. Unlocking\nModels with Secure Watermarks 13.\nGiving Visible Identity to Models 14.\nUnveiling Dark Knowledge with Watermarks 15. Linking Models: A Visual\nApproach 16. Visualizing Dark Knowledge through Watermarks 17. Leveraging\nPhysical World for Model Tracking 18. Augmented Reality for Model Tracking\n19. Securing Deep Models with Watermarks\nKGW-2.0\n3\n1.\nProviding Type-Safe Watermarks 2.\nConstructing Robust Watermarks\nfor Multimedia Applications 3. Adding Security Features to Text Vectors 4.\nStretching Visible Watermarks 5. Embedding Watermarks in Natural Language\nModels 6. Training Recurrent Neural Networks with Watermarks 7. Detection\nof Unsupervised Watermarks in Text Sequences 8.\nInteractive Visualization\nof Watermarks 9.\nObserve Hidden Watermarks with Machine Learning 10.\nExtracting Information from Unseen Watermarks 11. Building Robust Water-\nmarks for the Blockchain 12. Adding Dynamic Watermarks to Video Streams\n13. Storing Watermarks in Individual Dataset Elements 14. Extracting Per-\ntinent Information from Unwanted Watermarks 15. Decoding Hidden Digital\nWatermarks 16. Introducing Watermarks to Secure Semi-honest Adversaries 17.\nGenuine Components with Integrated Watermarks 18. Intelligent Techniques\nfor Steganography 19. Protecting Quality with Data-Driven Watermarks\n54\nKGW-2.0\n4\n1. Embedding Watermarks in Language Models 2. Secure Language Models\nusing Watermarks 3. Steganographic watermarking of natural language models\n4. Exploring Watermarking Techniques for Natural Language Models 5. Ana-\nlyzing Natural Language Models using Watermarking Techniques 6. Detecting\nWatermarks in Natural Language Models 7.\nSupervised Learning of Water-\nmarks for Natural Language Models 8. Unsupervised Learning of Watermarks\nfor Natural Language Models 9. Multiple watermarks for secure language mod-\nels 10. Generative watermarking techniques for natural language models 11.\nEvaluation of watermarks for secure natural language models 12. Efficacy of\nwatermarks in preventing model poisoning 13. Robustness of watermarks for\nnatural language models 14. Comparison of watermarking techniques for nat-\nural language models 15. Linking of natural language models with watermarks\n16. Extraction of hidden information from natural language models 17. Univer-\nsality of watermarks for different languages 18. Learning watermarked natural\nlanguage models 19. Reduced Dimensionality Modeling using Watermarks\nTable 3: Responses from KGW-2.0 to the prompt: \u201cGive me 20 ideas for the title of a paper\non watermarking language models.\u201d\nMethod\nSeed\nResponse\nEXP\n0\nHere is the list of 20 ideas for the title of a paper on watermarking language\nmodels:\n1.\n\u201dUnderstanding Watermarking Techniques for Language Models\u201d 2.\n\u201dA\nComprehensive Study on Watermarking for Language Models\u201d 3. \u201dWatermark-\ning Techniques for Optimizing Language Models\u201d 4. \u201dA Survey of Watermark-\ning Techniques for Language Models\u201d 5. \u201dExploring the Potential of Water-\nmarking for Language Models\u201d 6. \u201dImplementing Watermarking for Language\nModel Optimization\u201d 7.\n\u201dWatermarking Strategies for Enhancing Language\nModels\u201d 8. \u201dInvestigating the Possibilities of Watermarking for Language Mod-\nels\u201d 9. \u201dAdvanced Watermarking Approaches for Language Models\u201d 10. \u201dEx-\nploring the Use of Watermarking for Language Model Validation\u201d 11. \u201dUn-\nderstanding the Benefits of Watermarking for Language Model Ensemble\u201d 12.\n\u201dLeveraging Watermarking Techniques for Enhanced Language Model Perfor-\nmance\u201d 13. \u201dEnhancing Language Models with Watermarking\u201d 14. \u201dEvaluating\nthe Impact of Watermarking Techniques on Language Models\u201d 15. \u201dAnalyzing\nthe Feasibility of Watermarking for Language Model Comparison\u201d 16. \u201dEx-\nploring the Possibilities of Watermarking for Optimizing Language Models\u201d 17.\n\u201dExploiting Watermarking to Enhance Language Model Accuracy\u201d 18. \u201dAd-\nvantages of Using Watermarking for Validating Language Models\u201d 19. \u201dThe\nPromise of Watermarking for Evaluating Language Model Performance\u201d\n55\nEXP\n1\nTitle ideas for a paper on watermarking language models: 1. Hidden Messages:\nProtecting Language Models with Watermarking 2. Securing Language Models\nwith Watermarking 3. Defending Language Models against Tampering 4. Wa-\ntermarking Language Models for Better Protection 5. Verifying the Integrity of\nLanguage Models 6. Utilizing Watermarking to Boost Language Model Secu-\nrity 7. Ensuring the Authenticity of Language Models 8. Safekeeping Language\nModels with Watermarking 9. Defending Against Model Tampering with Wa-\ntermarking 10.\nA Review of Model Security Techniques: Watermarking 11.\nUtilizing Watermarks to Protect Language Models 12. Protective Techniques\nfor Language Models: Watermarking 13. Implementing Watermarks to Boost\nLanguage Model Security 14. Preventing Model Tampering with Watermarking\n15. Methods for Verifying Language Model Integrity 16. Analyzing the Security\nof Language Models 17. Techniques for Securing Language Models 18. Inves-\ntigating Model Security with Watermarking 19. Extending Language Model\nSecurity with Watermarking\nEXP\n2\n1. \u201dDeep Watermarks: Towards Pervasive Protection for Language Models\u201d\n2. \u201dAdding Transparency to Language Models: A Watermarking Approach\u201d\n3. \u201dWatermarking Language Models for Traceability and Verification\u201d 4. \u201dA\nComprehensive Review on Watermarking Methods for Language Models\u201d 5.\n\u201dThe Power of Watermarking for Language Model Protection\u201d 6. \u201dAttribut-\ning Authorship in Language Models with Watermarking\u201d 7. \u201dUnderstanding\nthe Promise of Watermarking for Language Model Security\u201d 8. \u201dWatermarking\nLanguage Models: A Survey\u201d 9. \u201dWatermarking Language Models for Better\nSecurity\u201d 10.\n\u201dMitigating Plagiarism in Language Models with Watermark-\ning\u201d 11. \u201dWatermarking Language Models: Exploring the Possibilities\u201d 12. \u201dA\nGenerative Approach to Watermarking Language Models\u201d 13. \u201dWatermark-\ning Strategies for Protecting Language Models\u201d 14. \u201dWatermark Detection for\nLanguage Model Security\u201d 15. \u201dMaking Language Model Security Watertight\nwith Watermarking\u201d 16. \u201dLeveraging Watermarking for Enhancing Language\nModel Security\u201d 17. \u201dUnderstanding the Role of Watermarking in Language\nModel Security\u201d 18. \u201dA Novel Approach to Watermarking Language Models\u201d\n19. \u201dExploring the Possibilities of Watermarking for Language Model Security\u201d\n56\nEXP\n3\n1. A Comprehensive Review of Watermarking Techniques for Language Mod-\nels 2.\nA Survey of Watermarking Approaches for Language Modeling 3.\nA\nNormative Analysis of Watermarking for Language Modeling 4. Investigating\nWatermarking Techniques for Language Modeling 5. An Overview of Water-\nmarking Methods for Language Modeling 6. Exploring Watermarking Solutions\nfor Language Modeling 7. A Taxonomy of Watermarking Methods for Language\nModeling 8. A Comparative Study of Watermarking Approaches for Language\nModeling 9. Evaluating Watermarking Methods for Language Modeling 10. A\nTheoretical Analysis of Watermarking for Language Modeling 11. Investigating\nWatermarking Techniques for Natural Language Modeling 12. An Analytical\nStudy of Watermarking for Language Modeling 13. Exploring Watermarking\nMethods for Natural Language Modeling 14. A Review of Watermarking Tech-\nniques for Natural Language Modeling 15. A Comparative Study of Watermark-\ning Approaches for Natural Language Modeling 16. Investigating Watermarking\nSolutions for Natural Language Modeling 17. A Survey of Watermarking Tech-\nniques for Natural Language Modeling 18. Evaluating Watermarking Methods\nfor Natural Language Modeling 19. A Review of Watermarking for Natural Lan-\nguage Modeling 20. Exploring Watermarking Solutions for Natural Language\nModeling\nEXP\n4\n1.\nTowards a Unified Watermarking Mechanism for Natural Language Pro-\ncessing Models 2. A Review of Methods for Watermarking Natural Language\nModels 3. Extracting Invariant Features for Watermarking Language Models\n4. The Use of Steganography for Watermarking Natural Language Models 5.\nIntroducting Secure Watermarking Techniques for Natural Language Models 6.\nA Comprehensive Study on Watermarking Techniques for Natural Language\nModels 7. Toward Remarkably Visible Watermarks for Natural Language Mod-\nels 8.\nAnalyzing the Impact of Watermarking on Natural Language Models\n9. A Practical Guide to Marking Language Models 10. Enhancing the Accu-\nracy of Watermarking Natural Language Models 11. Evaluating Strategies for\nWatermarking Natural Language Models 12. A Comparison of Watermarking\nApproaches for Natural Language Models 13. Promising Solutions for Securely\nWatermarking Natural Language Models 14.\nGenerative and Discriminative\nApproaches for Watermarking Natural Language Models 15.\nExploring the\nPossibilities of Steganography for Natural Language Models 16. Understanding\nthe Challenges of Watermarking Natural Language Models 17. Evaluating the\nEffectiveness of Watermarking Techniques for Natural Language Models 18. En-\nhancing the Transparency of Watermarking Techniques for Natural Language\nModels 19. Extending the Capabilities of Watermarking Techniques for Natural\nLanguage Models 20. Assessing the Sophistication of Watermarking Techniques\nfor Natural Language Models\nTable 4: Responses from EXP to the prompt: \u201cGive me 20 ideas for the title of a paper on\nwatermarking language models.\u201d\n57\n"
  },
  {
    "title": "Seal-3D: Interactive Pixel-Level Editing for Neural Radiance Fields",
    "link": "https://arxiv.org/pdf/2307.15131.pdf",
    "upvote": "5",
    "text": "Seal-3D: Interactive Pixel-Level Editing for Neural Radiance Fields\nXiangyu Wang1*\nJingsen Zhu2*\nQi Ye1\u2020\nYuchi Huo3,2\nYunlong Ran1\nZhihua Zhong2\nJiming Chen1\n1Zhejiang University, Key Lab of CS&AUS of Zhejiang Province\n2State Key Lab of CAD&CG, Zhejiang University\n3Zhejiang Lab\n{xy wong, zhujingsen, qi.ye}@zju.edu.cn\nhuo.yuchi.sc@gmail.com\n{yunlong ran, zhongzhihua, cjm}@zju.edu.cn\nBrush\nBounding Shape (Scaling)\nAnchor\nColor\nFinetune\nPretrain\n(0.9s) \nFigure 1: Seal-3D: The first interactive pixel level NeRF editing tool. We design an interactive user editing method and\nsystem Seal-3D, which achieves instant (\u22481s) preview (left) by our novel pretraining strategy. High-quality editing results\ncan be further obtained by a short period (in 1 or 2 minutes) of finetuning. The editing results of our implemented editing\ntools (right) are view-consistent with rich shading details (e.g. shadows) on the original surface (left).\nAbstract\nWith the popularity of implicit neural representations, or\nneural radiance fields (NeRF), there is a pressing need for\nediting methods to interact with the implicit 3D models for\ntasks like post-processing reconstructed scenes and 3D con-\ntent creation. While previous works have explored NeRF\nediting from various perspectives, they are restricted in edit-\ning flexibility, quality, and speed, failing to offer direct edit-\ning response and instant preview. The key challenge is to\nconceive a locally editable neural representation that can\ndirectly reflect the editing instructions and update instantly.\nTo bridge the gap, we propose a new interactive edit-\ning method and system for implicit representations, called\nSeal-3D 1 , which allows users to edit NeRF models in a\npixel-level and free manner with a wide range of NeRF-\n\u2217Equal contribution.\n\u2020Corresponding author.\nProject page: https://windingwind.github.io/seal-3d/\n1\u201cSeal\u201d derived from the name of rubber stamp in Adobe Photoshop.\nlike backbone and preview the editing effects instantly. To\nachieve the effects, the challenges are addressed by our pro-\nposed proxy function mapping the editing instructions to the\noriginal space of NeRF models in the teacher model and a\ntwo-stage training strategy for the student model with local\npretraining and global finetuning. A NeRF editing system\nis built to showcase various editing types. Our system can\nachieve compelling editing effects with an interactive speed\nof about 1 second.\n1. Introduction\nImplicit neural representations, e.g. neural radiance\nfields (NeRF) [24], have gained increasing attention as\nnovel 3D representations with neural networks to model a\n3D scene. Benefiting from the high reconstruction accu-\nracy and rendering quality with relatively low memory con-\nsumption, NeRF and its variations [50, 3, 33, 26, 4, 45, 41]\nhave demonstrated great potential in many 3D applica-\ntions like 3D reconstruction, novel view synthesis, and Vir-\n1\narXiv:2307.15131v2  [cs.CV]  27 Aug 2023\ntual/Augmented Reality.\nWith the popularity of the new implicit representations\nand an increasing number of implicit 3D models, there is\na pressing demand for human-friendly editing tools to in-\nteract with these 3D models. Editing with implicit neural\nrepresentations is a fundamental technique required to fully\nempower the representation. Objects reconstructed from the\nreal world are likely to contain artifacts due to the noise of\ncaptured data and the limitations of the reconstruction algo-\nrithms. In a typical 3D scanning pipeline, manual correc-\ntion and refinement to remove artifacts are common stages.\nOn the other hand, in 3D content creation applications like\n3D games, animations, and filming, artists usually need to\ncreate new content based on existing 3D models.\nPrior works have made attempts to edit 3D scenes rep-\nresented by NeRF, including object segmentation [20, 44] ,\nobject removal [19] , appearance editing [14, 27, 22] , and\nobject blending [7], etc. These existing NeRF editing meth-\nods mainly focus on coarse-grained object-level editing and\nthe convergence speed can not meet the demands of inter-\nactive editing. Some recent methods [48, 5] transform the\nediting of NeRF into mesh editing by introducing a mesh\nas an edit proxy. This requires the user to operate on an\nadditional meshing tool, which limits interactivity and user-\nfriendliness. To the best of our knowledge, there are no\nexisting methods that are able to support interactive pixel-\nlevel editing of neural radiance fields with fast converging\nspeed, which is mainly due to the challenges discussed be-\nlow.\nUnlike existing explicit 3D representations e.g. point\ncloud, textured mesh, and occupancy volume, which store\nthe explicit geometry structure of objects and scenes, im-\nplicit representations use neural networks to query features\nof a 3D scene including geometry and color. Existing 3D\nediting methods, taking the mesh-based representation as\nan example, can change object geometry by displacing ver-\ntices corresponding to target object surface areas and object\ntextures. Without explicit explainable correspondence be-\ntween the visual effects and the underlying representations,\nediting the implicit 3D models is indirect and challenging.\nFurther, it is difficult to locate implicit network parameters\nin local areas of the scene, meaning that adaptations of the\nnetwork parameters may lead to undesired global changes.\nThis results in more challenges for fine-grained editing.\nTo bridge the gap, in this paper, we propose an inter-\nactive pixel-level editing method and system for implicit\nneural representations for 3D scenes, dubbed Seal-3D. The\nname is borrowed from the popular 2D image editing soft-\nware Adobe PhotoShop [1], as its seal tool provides similar\nediting operations. As shown in Fig. 1, the editing system\nconsists of five types of editing as examples: 1) Bounding\nbox tool. It transforms and scales things inside a bounding\nbox, like a copy-paste operation. 2) Brushing tool. It paints\nspecified color on the selected zone and can increase or de-\ncrease the surface height, like an oil paint brush or graver.\n3) Anchor tool. It allows the user to freely move a control\npoint and affect its neighbor space according to the user in-\nput. 4) Color tool. It edits the color of the object surfaces.\nTo achieve the interactive NeRF editing effects, we ad-\ndress the challenges of implicit representations discussed\nabove. First, to establish the correspondence between the\nexplicit editing instructions to the update of implicit net-\nwork parameters, we propose a proxy function that maps\nthe target 3D space (determined by the user edit instructions\nfrom an interactive GUI) to the original 3D scene space, and\na teacher-student distillation strategy to update the parame-\nters with the corresponding content supervision acquired by\nthe proxy function from the original scenes. Second, to en-\nable local editing, i.e. mitigating the influence of the local\nediting effect on the global 3D scenes under the non-local\nimplicit representations, we propose a two-stage training\nprocess: a pretraining stage of updating only the positional\nembedding grids with local losses for editing areas while\nfreezing the subsequent MLP decoder to prevent global de-\ngeneration, and a finetuning stage of updating both the em-\nbedding grids and the MLP decoder with global photomet-\nric losses. With this design, the pretraining stage updates lo-\ncal editing features and the finetuning stage blends the local\nediting areas with global structures and colors of unedited\nspace to achieve view consistency. This design has the ben-\nefit of an instant preview of the editing: the pretraining can\nconverge very fast and presents local editing effects within\napproximately 1 second only.\nIn summary, our contributions are as follows:\n\u2022 We propose the first interactive pixel-level editing\nmethod and system for neural radiance fields, which\nexemplifies fine-grained multiple types of editing\ntools, including geometry (bounding box tool, brush\ntool, and anchor tool) and color edits;\n\u2022 A proxy function is proposed to establish the cor-\nrespondence between the explicit editing instructions\nand the update of implicit network parameters and a\nteacher-student distillation strategy is proposed to up-\ndate the parameters;\n\u2022 A two-stage training strategy is proposed to enable in-\nstant preview of local fine-grained editing without con-\ntaminating the global 3D scenes.\n2. Related Work\nNovel view synthesis.\nGiven a set of posed image cap-\ntures of a scene, the task of novel view synthesis is to\ngenerate photo-realistic images from arbitrary novel views.\nRecently, neural network have been introduced into the\nStudent\nSource Space \ud835\udcae\ud835\udcae\nEditing Guidance Generation\nLocal Pretraining\nGlobal Fine-tuning\nTwo-Stage Student Training for Instant Preview\nTeacher\nStudent Training Supervision\nEditing (Scale) \n\ud835\udc36\ud835\udc36, \ud835\udc37\ud835\udc37\nTarget Space \ud835\udcaf\ud835\udcaf\nLocal Loss\nGlobal Loss\nLocal Update \n\ud835\udc53\ud835\udc53\ud835\udf03\ud835\udf03\n\ud835\udc47\ud835\udc47\nx\ud835\udc61\ud835\udc61, d\ud835\udc61\ud835\udc61\nxs, d\ud835\udc60\ud835\udc60\n\ud835\udc50\ud835\udc50\ud835\udc61\ud835\udc61, \ud835\udf0e\ud835\udf0e\ud835\udc61\ud835\udc61\n\ud835\udc53\ud835\udc53\ud835\udf03\ud835\udf03\n\ud835\udc46\ud835\udc46\n\ud835\udc39\ud835\udc39\ud835\udc5a\ud835\udc5a\nFigure 2: Illustration of the editing framework. Left: a 3D point and view direction from the target space after user editing\nis mapped to the original source space to get guidance ct, \u03c3t from the teacher model f T\n\u03b8 for the student training. Right:\nthe student training consists of two stages: fast pretraining to provide instant preview by updating partial parameters of the\nnetwork with local losses and finetuning with global losses.\nrendering pipeline and leveraged for multiple representa-\ntions, such as voxels [21, 35], point clouds [2, 6], multi-\nplane images (MPIs) [17, 23, 52], and implicit representa-\ntions [36, 24]. Typically, Neural radiance field (NeRF) [24]\nuses a single MLP to implicitly encode a scene into a volu-\nmetric field of density and color, and takes advantage of vol-\nume rendering to achieve impressive rendering results with\nview-dependent effects, which inspires a lot of follow-up\nworks on human [30, 42], deformable objects [28, 29], pose\nestimations [18], autonomous system [32, 49], surface re-\nconstruction [45, 41], indoor scenes [47], city [38, 43], etc.\nNeRF\u2019s MLP representation can be enhanced and acceler-\nated by hybrid representations, including voxels [33, 37],\nhashgrids [26] and tensorial decomposition [4, 40].\nIn\nthis paper, our interactive editing framework is developed\nbased on Instant-NGP [26], which achieve real-time render-\ning speed for NeRF inference and state-of-the-art quality of\nnovel view synthesis.\nNeural scene editing.\nScene editing has been a widely re-\nsearched problem in computer vision and graphics. Early\nmethod focus on editing a single static view by insert-\ning [15, 54], relighting [16], composition [31], object mov-\ning [12, 34], etc. With the development of neural render-\ning, recent works attempt to perform editing at different\nlevels of the 3D scene, which can be categorized as scene-\nlevel, object-level, and pixel-level editing. Scene-level edit-\ning methods focus on changing in global appearances of a\nscene, such as lighting [8] and global palette [14]. Intrinsic\ndecomposition [51, 27, 9, 46, 53, 10] disentangles mate-\nrial and lighting field and enables texture or lighting edit-\ning. However, scene-level methods are only able to mod-\nify global attributes and are unable to apply to specified\nobjects. Object-level editing methods use different strate-\ngies to manipulate the implicitly represented object. Object-\nNeRF [44] exploit per-object latent code to decompose neu-\nral radiance field into objects, enabling object moving, re-\nmoval, or duplicating. Liu et al. [20] design a conditional\nradiance field model which is partially optimized according\nto the editing instructions to modify semantic-level color or\ngeometry. NeRF-editing [48] and NeuMesh [5] introduce\na deformable mesh reconstructed by NeRF, as an editing\nproxy to guide object editings. However, these methods are\nrestricted to object-level rigid transformation or are not gen-\neralizable to arbitrary out-of-distribution editing categories.\nIn contrast, pixel-level editing aims to provide fine-grained\nediting guidance precisely selected by pixels, instead of re-\nstricted by object entities. To the best of our knowledge,\nNeuMesh [5] is the only existing method that achieves edit-\ning at this level. However, it depends on the mesh scaf-\nfold, which limits the editing categories, e.g. cannot create\nout-of-mesh geometry structures. In contrast, our editing\nframework does not require any proxy geometry structures,\nallowing it to be more direct and extensive.\nBesides, optimizing the performance of neural editing\nmethod remains an open problem. Existing methods require\nminutes or even hours of optimization and inference. Our\nmethod is the first pixel-level neural editing framework to\nachieve instant interactive (i.e. second-level) performance.\n3. Method\nWe introduce Seal-3D, an interactive pixel-level editing\nmethod for neural radiance fields.\nThe overall pipeline\nis illustrated in Fig. 2, which consists of a pixel-level\nproxy mapping function, a teacher-student training frame-\nwork, and a two-stage training strategy for the student\nNeRF network under the framework. Our editing work-\nflow starts with the proxy function which maps the query\npoints and ray directions according to user-specified edit-\ning rules. Then a NeRF-to-NeRF teacher-student distilla-\ntion framework follows, where a teacher model with editing\nmapping rules of geometry and color supervises the train-\ning of a student model (Sec. 3.2). The key to interactive\nfine-grained editing is the two-stage training for the student\nmodel (Sec. 3.3). In an extra pretraining stage, the points,\nray directions, and inferred ground truth inside edit space\nfrom the teacher model are sampled, computed, and cached\npreviously; only parameters with locality are updated and\nthe parameters causing global changes are frozen. After\nthe pretraining stage, the student model is finetuned with\na global training stage.\n3.1. Overview of NeRF-based Editing Problem\nWe first make a brief introduction to neural radiance\nfields and then analyze the challenges of NeRF-based edit-\ning problems and the limitations of existing solutions.\n3.1.1\nNeRF Preliminaries\nNeural radiance fields (NeRFs) provide implicit representa-\ntions for a 3D scene as a 5D function: f : (x, y, z, \u03b8, \u03c6) 7\u2192\n(c, \u03c3), where x = (x, y, z) is a 3D location and d = (\u03b8, \u03d5)\nis the view direction, while c and \u03c3 denote color and volume\ndensity, respectively. The 5D function is typically parame-\nterized as an MLP f\u03b8.\nTo render an image pixel, a ray r with direction d is shot\nfrom the camera position o through the pixel center accord-\ning to the intrinsics and extrinsics of the camera. K points\nxi = o + tid, i = 1, 2, . . . , K are sampled along the ray,\nand the network f\u03b8 is queried for their corresponding color\nand density:\n(ci, \u03c3i) = f\u03b8(xi, d)\n(1)\nSubsequently, the predicted pixel color \u02c6C(r) and depth\nvalue \u02c6D(r) are computed by volume rendering:\n\u02c6C(r) =\nK\nX\ni=1\nTi\u03b1ici,\n\u02c6D(r) =\nK\nX\ni=1\nTi\u03b1iti\n(2)\nTi =\nY\nj<i\n(1 \u2212 \u03b1j),\n\u03b1i = 1 \u2212 exp (\u03c3i\u03b4i)\n(3)\nwhere \u03b1i is the alpha value for blending, Ti is the accu-\nmulated transmittance, and \u03b4i = ti+1 \u2212 ti is the distance\nbetween adjacent points. NeRF is trained by minimizing\nthe photometric loss between the predicted and ground truth\ncolor of pixels.\nIn this paper, we build our interactive NeRF editing sys-\ntem upon Instant-NGP [26], which achieves nearly real-\ntime rendering performance for NeRF. Although our im-\nplementation of instant interactive editing relies on hybrid\nrepresentations for NeRF to achieve the best speed perfor-\nmance, our proposed editing framework does not rely on a\nspecific NeRF backbone and can be transplanted to other\nframeworks as long as they follow the aforementioned vol-\nume rendering pipeline.\n3.1.2\nChallenges of NeRF-based Editing\nNeRF-like methods achieve the state-of-the-art quality of\nscene reconstruction.\nHowever, the 3D scene is implic-\nitly represented by network parameters, which lacks inter-\npretability and can hardly be manipulated. In terms of scene\nediting, it is difficult to find a mapping between the explicit\nediting instructions and the implicit update of network pa-\nrameters. Previous works attempt to tackle this by means of\nseveral restricted approaches:\nNeRF-Editing [48] and NeuMesh [5] introduce a mesh\nscaffold as a geometry proxy to assist the editing, which\nsimplifies the NeRF editing task into mesh modification.\nAlthough conforming with existing mesh-based editing,\nthe editing process requires extracting an additional mesh,\nwhich is cumbersome. In addition, the edited geometry is\nhighly dependent on the mesh proxy structure, making it\ndifficult to edit spaces that are not easy or able to be rep-\nresented by meshes while representing these spaces is one\nkey feature of the implicit representations. Liu et al. [20]\ndesigns additional color and shape losses to supervise the\nediting. However, their designed losses are only in 2D pho-\ntometric space, which limits the editing capability of a 3D\nNeRF model. Furthermore, their method only supports edit-\ning of semantic-continuous geometry in simple objects, in-\nstead of arbitrary pixel-level complex editing.\nMoreover, to the best of our knowledge, existing meth-\nods have not realized interactive editing performance con-\nsidering both quality and speed.\nLiu et al. [20] is the\nonly existing method that completes optimization within a\nminute (37.4s according to their paper), but their method\nonly supports extremely simple objects and does not sup-\nport fine-grained local edits (see Fig. 10 for details). Other\nediting methods (e.g. NeuMesh [5]) usually require hours\nof network optimization to obtain edit results.\nIn this paper, we implement an interactive pixel-level\nediting system, which can be extended to new editing types\neasily using similar editing strategies as the traditional ex-\nplicit 3D representation editing. Our method does not re-\nquire any explicit proxy structure (instead, a proxy function,\nsee Sec. 3.2) and can define various pixel-level editing ef-\nfects without an explicit geometry proxy. It also enables in-\nstant preview (\u22481s) (see Sec. 3.3). Tab. 1 compares the edit\ncapabilities between our method and previous methods.\nMethod\nw/o Explicit Proxy Pixel-Level Interactive\nTime\nOurs\n\u2713\n\u2713\n\u2713\nseconds\nNeuMesh [5]\n\u2717\n(partial)\n\u2717\nhours\nNeRF-Editing [48]\n\u2717\n\u2717\n\u2717\nhours\nTable 1: Comparison with recent methods in edit capa-\nbilities. Our method supports arbitrary editing, does not\nrequire any explicit geometry proxy, and achieves interac-\ntive editing in seconds.\n\ud835\udc39\ud835\udc39\ud835\udc61\ud835\udc61 Output\nOriginal Model\nPretrain (1.6s)\nPSNR: 30.41\nFinetune (113.8s) \nPSNR: 41.91\nDepth Map\nFigure 3: Example of brush editing: 3D painting with color and thickness.\n3.2. Editing Guidance Generation\nOur design implements NeRF editing as a process of\nknowledge distillation. Given a pretrained NeRF network\nfitting a particular scene that serves as a teacher network,\nwe initialize an extra NeRF network with the pretrained\nweights as a student network. The teacher network f T\n\u03b8 gen-\nerates editing guidance from the editing instructions input\nby the user, while the student network f S\n\u03b8 is optimized by\ndistilling editing knowledge from the editing guidance out-\nput by the teacher network. In the subsection, editing guid-\nance generation for the student model supervision is intro-\nduced and illustrated on the left of Fig. 2.\nFirstly, the user edit instructions are read from the inter-\nactive NeRF editor as pixel-level information. The source\nspace S \u2282 R3 is the 3D space for the original NeRF model\nand the target space T \u2282 R3 is the 3D space for the NeRF\nmodel after editing. The target space T is warped to the\noriginal space S by F m : T 7\u2192 S. F m transforms points\nwithin the target space and their associated directions ac-\ncording to editing rules which are exemplified below. With\nthe function, the \u201cpseudo\u201d desired edited effects cT , \u03c3T for\neach 3D point and view direction in the target space can\nbe acquired by querying the teacher NeRF model f T\n\u03b8 : the\ntransformed points and directions (in source space) are fed\ninto the teacher network get the color and density. The pro-\ncess can be expressed as\nxs, ds = F m(xt, dt), xs \u2208 S, xt \u2208 T ,\n(4)\ncT , \u03c3T = f T\n\u03b8 (xs, ds)\n(5)\nWhere xs, ds denotes source space point position and\ndirection and xt, dt denotes target space point position and\ndirection.\nFor brevity, we define the entire process as teacher infer-\nence process F t := f T\n\u03b8 \u25e6 F m : (xt, dt) 7\u2192 (cT , \u03c3T ). The\ninference result cT , \u03c3T mimics the edited scene and acts as\nthe teacher label, the information of which is then distilled\nby the student network in the network optimization stage.\nThe mapping rules of F m can be designed according to\narbitrary editing targets. In particular, we implement 4 types\nof editing as examples.\n\u2022 Bounding shape tool, which supports common fea-\ntures in traditional 3D editing software including copy-\npaste, rotation, and resizing.\nThe user provides a\nbounding shape to indicate the original space S to be\nedited and rotates, translates, and scales the bounding\nbox to indicate the target effects. The target space T\nand mapping function F m are then parsed by our in-\nterface\nxs =S\u22121 \u00b7 RT \u00b7 (xt \u2212 ct) + cs,\nds =RT \u00b7 dt\nF m :=(xt, dt) 7\u2192\n\u001a (xs, ds)\n, if xt \u2208 T\n(xt, dt)\n, otherwise\nwhere R is rotation, S is scale, and cs, ct are the center\nof S, T , respectively.\nWith this tool, we even support cross-scene object\ntransfer, which can be implemented by introducing the\nNeRF of the transferred object as an additional teacher\nnetwork in charge of part of the teacher inference pro-\ncess within the target area. We give a result in Fig. 7.\n\u2022 Brushing tool, similar to the sculpt brush in traditional\n3D editing that lifts or descends the painted surface.\nThe user scribbles with a brush and S is generated by\nray casting on brushed pixels. The brush normal n,\nand pressure value p(\u00b7) \u2208 [0, 1] are defined by user,\nwhich determines the mapping:\nxs = xt \u2212 p(xt)n,\nF m := (xt, dt) 7\u2192 (xs, dt)\n\ud835\udc39\ud835\udc39\ud835\udc61\ud835\udc61 Output\nOriginal Model\nPretrain (0.6s)\nPSNR: 34.08\nFinetune (62.7s) \nPSNR: 42.83\nOther Finetuned Views\nFigure 4: Example of bounding shape editing: bulb scaling.\n\u2022 Anchor tool, where the user defines a control point\nxc and a translation vector t. The region surround-\ning xc will be stretched by a translation function\nstretch(\u00b7; xc, t). Then the mapping is its inverse:\nxs = stretch\u22121(xt; xc, t)\nF m := (xt, dt) \u2192 (xs, dt)\nplease refer to the supplementary material for the ex-\nplicit expressions of stretch(\u00b7; xc, t).\n\u2022 Non-rigid transform tool, which allows for the accu-\nrate and flexible transformation of selected space.\nxs = R \u00b7 xt + t,\nds = R \u00b7 dt,\nF m := (xt, dt) 7\u2192 (xs, ds)\nWhere R, t are interpolated from the transform ma-\ntrixes of the three closest coordinates of a pre-defined\n3D blending control grid with position and transforma-\ntion of each control point. The results can be found in\nFig. 9.\n\u2022 Color tool, which edits color via color space mapping\n(single color or texture). Here the spatial mapping is\nidentical and we directly map the color output of the\nnetwork to HSL space, which helps for color consis-\ntency. Our method is capable of preserving shading\ndetails (e.g. shadows) on the modified surface.\nWe\nachieve this by transferring the luminance (in HSL\nspace) offsets on the original surface color to the target\nsurface color. Implementation details of this shading\npreservation strategy are presented in the supplemen-\ntary.\nFor the training strategy of distillation, the student model\nf S\n\u03b8 is optimized with the supervision of pseudo ground\ntruths generated by the aforementioned teacher inference\nprocess F t. The editing guidance from the teacher model\nis distilled into the student model by directly applying the\nphotometric loss between pixel values \u02c6C, \u02c6D accumulated\nby Eq. (2) from the teacher and student inference.\nHowever, we find that the convergence speed of this\ntraining process is slow (\u224830s or longer), which cannot\nmeet the needs of instant preview. To tackle this problem,\nwe design a two-stage training strategy: the first stage aims\nto converge instantly (within 1 second) so that a coarse edit-\ning result can be immediately presented to the user as a pre-\nview, while the second stage further finetunes the coarse\npreview to obtain a final refinement.\n3.3. Two-stage Student Training for Instant Preview\nLocal pretraining for instant preview.\nUsually, the edit\nspace is relatively small compared to the entire scene, so\ntraining on the global photometric loss is wasteful and leads\nto slow convergence. To achieve instant preview of editing,\nwe adopt a local pretraining stage before the global training\nbegins. The local pretraining process consists of: 1) uni-\nformly sample a set X \u2282 T of local points within the target\nspace and a set D of directions on the unit sphere, and feed\nthem into the teacher inference process F t to obtain teacher\nlabels cT , \u03c3T , and cache them in advance; 2) the student\nnetwork is trained by local pertaining loss Llocal:\n(cT , \u03c3T ) = F t(x, d), (cS, \u03c3S) = f S\n\u03b8 (x, d),\n(6)\nLlocal =\nX\nx\u2208X,d\u2208D\n\u03bb1\u2225cT \u2212 cS\u22251 + \u03bb2\u2225\u03c3T \u2212 \u03c3S\u22251\n(7)\nwhere cS, \u03c3S are the predicted color and density of sampled\npoints x \u2208 X by the student network, and cT , \u03c3T are cached\nteacher labels. This pretraining stage is very fast: after only\nabout 1 second of optimization, the rendered image of the\nstudent network shows plausible color and shape consistent\nwith the editing instructions.\nHowever, training on only the local points in the edit-\ning area may lead to degeneration in other global areas un-\nrelated to the editing due to the non-local implicit neural\nnetwork. We observe the fact that in hybrid implicit repre-\nsentations (such as Instant-NGP [26]), local information is\nmainly stored in the positional embedding grids, while the\nsubsequent MLP decodes global information. Therefore, in\nthis stage, all parameters of the MLP decoder are frozen to\nprevent global degeneration. Experimental illustrations will\nbe presented in Sec. 4.3 and Fig. 12.\nGlobal Finetuning.\nAfter pretraining, we continue to\nfinetune f S\n\u03b8 to refine the coarse preview to a fully converged\nresult. This stage is similar to the standard NeRF train-\ning, except that the supervision labels are generated by the\nteacher inference process instead of image pixels.\nLglobal =\nX\nr\u2208R\n\u03bb3\u2225 \u02c6CT \u2212 \u02c6CS\u22252 + \u03bb4\u2225 \u02c6DT \u2212 \u02c6DS\u22251\n(8)\nwhere R denote the set of sampled rays in the minibatch\nand ( \u02c6CT , \u02c6DT ),( \u02c6CS, \u02c6DS) are accumulated along ray r by\nEq. (2) according to (cT , \u03c3T ),(cS, \u03c3S), respectively.\nIt is worth mentioning that the student network is capa-\nble of generating results of better quality than the teacher\nnetwork that it learns from.\nThis is because the map-\nping operation in the teacher inference process may pro-\nduce some view-inconsistent artifacts in the pseudo ground\ntruths. However, during the distillation, the student network\ncan automatically eliminate these artifacts due to the multi-\nview training that enforces view-consistent robustness. See\nSec. 4.2 and Fig. 6 for details.\n4. Experiments and Analysis\n4.1. Implementation Details\nNetwork.\nIn order to disentangle shape and color latent\ninformation within the hashgrids, we split the single hash\ntable in the NeRF network architecture of Instant-NGP [26]\ninto two: a density grid G\u03c3 and a color grid Gc, with the\nsame settings as the original density grid in the open-source\nPyTorch implementation torch-ngp [39].\nWe do this to\nmake it possible to make fine-grained edits of one to one of\nthe color or geometry properties without affecting the other.\nThe rest of the network architecture remains the same, in-\ncluding a sigma MLP f \u03c3 and a color MLP f c. For a spatial\npoint x with view direction d, the network predicts volume\ndensity \u03c3 and color c as follows:\n\u03c3, z = f \u03c3(G\u03c3(x))\n(9)\nc = f c(Gc(x), z, SH(d))\n(10)\nwhere z is the intermediate geometry feature, and SH is the\nspherical harmonics directional encoder [26]. The same as\n\ud835\udc39\ud835\udc39\ud835\udc61\ud835\udc61 Output\nOriginal Model\nPretrain (1.1s)\nPSNR: 35.33\nFinetune (61.4s) \nPSNR: 45.67\nFigure 5: Example of anchor editing: fake tooth.\nPretrain (1.0s)\nPSNR: 35.08\nFinetune (34.5s) \nPSNR: 40.85\n\ud835\udc39\ud835\udc39\ud835\udc61\ud835\udc61 Output\nOriginal Model\nFigure 6: Example of editing on the real-world scene:\nto\n(DTU Scan 83).\nFigure 7: Example of object transfer editing: from the Lego\nscene (NeRF Blender) to the family scene (Tanks and Tem-\nples).\nInstant-NGP\u2019s settings, f \u03c3 has 2 layers with hidden channel\n64, f c has 3 layers with hidden channel 64, and z is a 15-\nchannel feature.\nWe compare our modified NeRF network with the vanilla\narchitecture in the Lego scene of NeRF Blender Synthetic\ndataset[24]. We train our network and the vanilla network\non the scene for 30,000 iterations. The result is as follows:\n\u2022 Ours: training time 441s, PSNR 35.08dB\n\u2022 Vanilla: training time 408s, PSNR 34.44dB\nWe observe slightly slower runtime and higher quality for\nour modified architecture, indicating that this modification\ncauses negligible changes.\nTraining.\nWe select Instant-NGP [26] as the NeRF back-\nbone of our editing framework. Our implementations are\nbased on the open-source PyTorch implementation torch-\nngp [39]. All experiments are run on a single NVIDIA RTX\n3090 GPU. Note that we make a slight modification to the\noriginal network architecture. Please refer to the supple-\nmentary material for details.\nDuring the pretraining stage, we set \u03bb1 = \u03bb2 = 1 and the\nlearning rate is fixed to 0.05. During the finetuning stage,\nwe set \u03bb3 = \u03bb4 = 1 with an initial learning rate of 0.01.\nStarting from a pretrained NeRF model, we perform 50-\n100 epochs of local pretraining (for about 0.5-1 seconds)\nand about 50 epochs of global finetuning (for about 40-60\nseconds). The number of epochs and time consumption can\nbe adjusted according to the editing type and the complex-\nity of the scene. Note that we test our performance in the\nabsence of tiny-cuda-nn [25] which achieves superior speed\nto our backbone, which indicates that our performance has\nroom for further optimization.\nDatasets.\nWe evaluate our editing in the synthetic NeRF\nBlender Dataset [24], and the real-world captured Tanks and\nTemples [13] and DTU [11] datasets. We follow the official\ndataset split of the frames for the training and evaluation.\n4.2. Experimental Results\nQualitative NeRF editing results.\nWe provide extensive\nexperimental results in all kinds of editing categories we\ndesign, including bounding shape (Figs. 4 and 6), brushing\n(Fig. 3), anchor (Fig. 5), and color (Fig. 1). Our method not\nonly achieves a huge performance boost, supporting instant\npreview at the second level but also produces more visually\nrealistic editing appearances, such as shading effects on the\nlifted side in Fig. 3 and shadows on the bumped surface in\nFig. 8. Besides, results produced by the student network\ncan even outperform the teacher labels, e.g. in Fig. 6 the F t\noutput contains floating artifacts due to view inconsistency.\nAs analyzed in Sec. 3.3, the distillation process manages to\neliminate this. We also provide an example of object trans-\nfer (Fig. 7): the bulb in the Lego scene (of Blender dataset)\nis transferred to the child\u2019s head in the family scene of Tanks\nand Temples dataset.\nInteractive Edit \nInstructions\nEdit Results\nOurs\nNeuMesh\nOurs\nNeuMesh\nFigure 8: Comparisons on texture/color painting between\nNeuMesh [5] and our method. Note that NeuMesh requires\nhours of finetuning while ours needs only seconds.\nGT (Rendered in Blender)\nNeuMesh\nPSNR 27.84\nRendering Speed: 0.009 FPS\nOurs\nPSNR 28.03\n28FPS\nOurs Mesh\nOriginal\nFigure 9: Comparison on qualitative and quantitative be-\ntween NeuMesh [5] and our method. The PSNR is com-\nputed from the editing result and the rendering of the ground\ntruth mesh with the same editing applied.\nInstructions Liu et al. [20] NeuMesh [5]\nOurs\nFigure 10: Comparison of the pixel-wise editing ability be-\ntween baselines [20, 5] and ours. Note that [20] does not\nfocus on the same task of pixel-wise editing as the other\ntwo. We are not to compete with their method.\nComparisons to baselines.\nExisting works have strong\nrestrictions on editing types, which focus on either geom-\netry editing or appearance editing, while ours is capable of\ndoing both simultaneously. Our brushing and anchor tools\ncan create user-guided out-of-proxy geometry structures,\nwhich no existing methods support. We make comparisons\non color and texture painting supported by NeuMesh [5] and\n1 second\n30 seconds\n60 seconds\nw/o Pretraining\nw/o Finetuning\nFull Model (1s pretraining + 59s finetuning) \nPSNR: 31.41 \n37.84\n42.26\nPSNR: 32.25\n30.84\n29.86\nPSNR: 26.64\n28.75\n40.16\nFigure 11: Ablation studies on two-stage training strategy.\nZoom in for degradation details of \u201cw/o finetuning\u201d.\nFixing MLP (Ours)\nw/o Fixing MLP in Pretraining\nFigure 12: Ablation study on MLP fixing.\nLiu et al. [20].\nFig. 8 illustrates two comparisons between our method\nand NeuMesh [5] in scribbling and a texture painting task.\nOur method significantly outperforms NeuMesh, which\ncontains noticeable color bias and artifacts in the results. In\ncontrast, our method even succeeds in rendering the shadow\neffects caused by geometric bumps.\nFig. 9 illustrates the results of the same non-rigid blend-\ning applied to the Mic from NeRF Blender[24]. It clearly\nshows that being mesh-free, We have more details than\nNeuMesh[5], unlimited by mesh resolution.\nFig. 10 shows an overview of the pixel-wise editing abil-\nity of existing NeRF editing methods and ours.\nLiu et\nal. [20]\u2019s method does not focus on the pixel-wise editing\ntask and only supports textureless simple objects in their\npaper. Their method causes an overall color deterioration\nwithin the edited object, which is highly unfavorable. This\nis because their latent code only models the global color fea-\nture of the scene instead of fine-grained local features. Our\nmethod supports fine-grained local edits due to our local-\naware embedding grids.\n4.3. Ablation Studies\nEffect of the two-stage training strategy.\nTo validate\nthe effectiveness of our pretraining and finetuning strategy,\nwe make comparisons between our full strategy (3rd row),\nfinetuning-only (1st row) and pretraining-only (2nd row) in\nFig. 11. Our pretraining can produce a coarse result in only\n1 second, while photometric finetuning can hardly change\nthe appearance in such a short period. The pretraining stage\nalso enhances the subsequent finetuning, in 30 seconds our\nfull strategy produces a more complete result. However,\npretraining has a side effect of local overfitting and global\ndegradation.\nTherefore, our two-stage strategy makes a\ngood balance between both and produces optimal results.\nMLP fixing in the pretraining stage.\nIn Fig. 12, we val-\nidate our design of fixing all MLP parameters in the pre-\ntraining stage. The result confirms our analysis that MLP\nmainly contains global information so it leads to global de-\ngeneration when MLP decoders are not fixed.\n5. Conclusion\nWe have introduced an interactive framework for pixel-\nlevel editing for neural radiance fields supporting instant\npreview.\nSpecifically, we exploit the two-stage teacher-\nstudent training method to provide editing guidance and de-\nsign a two-stage training strategy to achieve instant network\nconvergence to obtain coarse results as a preview. Unlike\nprevious works, our method does not require any explicit\nproxy (such as mesh), improving interactivity and user-\nfriendliness. Our method also supports preserving shad-\ning effects on the edited surface. One limitation is that our\nmethod does not support complex view-dependent lighting\neffects such as specular reflections, and can not change the\nscene illumination, which can be improved by introducing\nintrinsic decomposition. Besides, our method does not han-\ndle the reconstruction failures (such as floating artifacts) of\nthe original NeRF network.\nACKNOWLEDGEMENT\nThis work was supported in part by the Fundamental\nResearch Funds for the Central Universities; NSFC un-\nder Grants (62103372, 62088101, 62233013); the Key Re-\nsearch and Development Program of Zhejiang Province\n(2021C03037); Zhejiang Lab (121005-PI2101); Informa-\ntion Technology Center and State Key Lab of CAD&CG,\nZhejiang University.\nReferences\n[1] Adobe Inc. Adobe photoshop. 2\n[2] Kara-Ali Aliev, Artem Sevastopolsky, Maria Kolos, Dmitry\nUlyanov, and Victor Lempitsky. Neural point-based graph-\nics. 2020. 3\n[3] Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter\nHedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan.\nMip-nerf: A multiscale representation for anti-aliasing neu-\nral radiance fields. ICCV, 2021. 1\n[4] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\nHao Su. Tensorf: Tensorial radiance fields. In European\nConference on Computer Vision (ECCV), 2022. 1, 3\n[5] Chong Bao and Bangbang Yang, Zeng Junyi, Bao Hujun,\nZhang Yinda, Cui Zhaopeng, and Zhang Guofeng. Neumesh:\nLearning disentangled neural mesh-based implicit field for\ngeometry and texture editing. In European Conference on\nComputer Vision (ECCV), 2022. 2, 3, 4, 8, 9\n[6] Peng Dai, Yinda Zhang, Zhuwen Li, Shuaicheng Liu, and\nBing Zeng.\nNeural point cloud rendering via multi-plane\nprojection.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 7830\u2013\n7839, 2020. 3\n[7] Jianfei Guo, Zhiyuan Yang, Xi Lin, and Qingfu Zhang.\nTemplate nerf: Towards modeling dense shape correspon-\ndences from category-specific object images. arXiv preprint\narXiv:2111.04237, 2021. 2\n[8] Michelle Guo, Alireza Fathi, Jiajun Wu, and Thomas\nFunkhouser. Object-centric neural scene rendering. arXiv\npreprint arXiv:2012.08503, 2020. 3\n[9] Jon Hasselgren, Nikolai Hofmann, and Jacob Munkberg.\nShape,\nLight,\nand Material Decomposition from Im-\nages\nusing\nMonte\nCarlo\nRendering\nand\nDenoising.\narXiv:2206.03380, 2022. 3\n[10] Jinkai Hu, Chengzhong Yu, Hongli Liu, Lingqi Yan, Yiqian\nWu, and Xiaogang Jin. Deep real-time volumetric rendering\nusing multi-feature fusion. In ACM SIGGRAPH 2023 Con-\nference Proceedings, SIGGRAPH \u201923, New York, NY, USA,\n2023. Association for Computing Machinery. 3\n[11] Rasmus Jensen, Anders Dahl, George Vogiatzis, Engil Tola,\nand Henrik Aan\u00e6s. Large scale multi-view stereopsis eval-\nuation. In 2014 IEEE Conference on Computer Vision and\nPattern Recognition, pages 406\u2013413. IEEE, 2014. 8\n[12] Natasha Kholgade, Tomas Simon, Alexei Efros, and Yaser\nSheikh. 3d object manipulation in a single photograph using\nstock 3d models. ACM Transactions on Computer Graphics,\n33(4), 2014. 3\n[13] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen\nKoltun. Tanks and temples: Benchmarking large-scale scene\nreconstruction. ACM Transactions on Graphics, 36(4), 2017.\n8\n[14] Zhengfei Kuang, Fujun Luan, Sai Bi, Zhixin Shu, Gordon\nWetzstein, and Kalyan Sunkavalli. Palettenerf: Palette-based\nappearance editing of neural radiance fields. arXiv preprint\narXiv:2212.10699, 2022. 2, 3\n[15] Zhengqin Li,\nMohammad Shafiei,\nRavi Ramamoorthi,\nKalyan Sunkavalli, and Manmohan Chandraker. Inverse ren-\ndering for complex indoor scenes: Shape, spatially-varying\nlighting and svbrdf from a single image. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 2475\u20132484, 2020. 3\n[16] Zhengqin Li, Jia Shi, Sai Bi, Rui Zhu, Kalyan Sunkavalli,\nMilo\u02c7s Ha\u02c7san, Zexiang Xu, Ravi Ramamoorthi, and Manmo-\nhan Chandraker. Physically-based editing of indoor scene\nlighting from a single image.\nIn Computer Vision\u2013ECCV\n2022: 17th European Conference, Tel Aviv, Israel, Octo-\nber 23\u201327, 2022, Proceedings, Part VI, pages 555\u2013572.\nSpringer, 2022. 3\n[17] Zhengqi Li, Wenqi Xian, Abe Davis, and Noah Snavely.\nCrowdsampling the plenoptic function. In European Con-\nference on Computer Vision, pages 178\u2013196. Springer, 2020.\n3\n[18] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Si-\nmon Lucey. Barf: Bundle-adjusting neural radiance fields. In\nIEEE International Conference on Computer Vision (ICCV),\n2021. 3\n[19] Hao-Kang Liu, I Shen, Bing-Yu Chen, et al.\nNerf-in:\nFree-form nerf inpainting with rgb-d priors. arXiv preprint\narXiv:2206.04901, 2022. 2\n[20] Steven Liu, Xiuming Zhang, Zhoutong Zhang, Richard\nZhang, Jun-Yan Zhu, and Bryan Russell. Editing conditional\nradiance fields. In Proceedings of the International Confer-\nence on Computer Vision (ICCV), 2021. 2, 3, 4, 8, 9\n[21] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel\nSchwartz, Andreas Lehrmann, and Yaser Sheikh. Neural vol-\numes: Learning dynamic renderable volumes from images.\nACM Trans. Graph., 38(4):65:1\u201365:14, July 2019. 3\n[22] Aryan Mikaeili, Or Perel, Mehdi Safaee, Daniel Cohen-Or,\nand Ali Mahdavi-Amiri. Sked: Sketch-guided text-based 3d\nediting, 2023. 2\n[23] Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon,\nNima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and\nAbhishek Kar. Local light field fusion: Practical view syn-\nthesis with prescriptive sampling guidelines. ACM Transac-\ntions on Graphics (TOG), 2019. 3\n[24] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In ECCV, 2020. 1, 3, 8, 9\n[25] Thomas M\u00a8uller.\nTiny CUDA neural network framework,\n2021. https://github.com/nvlabs/tiny-cuda-nn. 8\n[26] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a multires-\nolution hash encoding. ACM Trans. Graph., 41(4):102:1\u2013\n102:15, July 2022. 1, 3, 4, 7, 8\n[27] Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao,\nWenzheng Chen, Alex Evans, Thomas M\u00a8uller, and Sanja Fi-\ndler. Extracting triangular 3d models, materials, and lighting\nfrom images. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 8280\u2013\n8290, 2022. 2, 3\n[28] Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien\nBouaziz, Dan B Goldman, Steven M. Seitz, and Ricardo\nMartin-Brualla. Nerfies: Deformable neural radiance fields.\nICCV, 2021. 3\n[29] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T.\nBarron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-\nBrualla, and Steven M. Seitz.\nHypernerf:\nA higher-\ndimensional representation for topologically varying neural\nradiance fields. ACM Trans. Graph., 40(6), dec 2021. 3\n[30] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang,\nQing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body:\nImplicit neural representations with structured latent codes\nfor novel view synthesis of dynamic humans.\nIn CVPR,\n2021. 3\n[31] Patrick P\u00b4erez, Michel Gangnet, and Andrew Blake. Poisson\nimage editing. In ACM SIGGRAPH 2003 Papers, pages 313\u2013\n318. 2003. 3\n[32] Yunlong Ran, Jing Zeng, Shibo He, Jiming Chen, Lincheng\nLi, Yingfeng Chen, Gimhee Lee, and Qi Ye. Neurar: Neural\nuncertainty for autonomous 3d reconstruction with implicit\nneural representations. IEEE Robotics and Automation Let-\nters, 8(2):1125\u20131132, 2023. 3\n[33] Sara Fridovich-Keil and Alex Yu, Matthew Tancik, Qinhong\nChen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:\nRadiance fields without neural networks. In CVPR, 2022. 1,\n3\n[34] Rakshith Shetty, Mario Fritz, and Bernt Schiele. Adversarial\nscene editing: Automatic object removal from weak supervi-\nsion. In Advances in Neural Information Processing Systems\n31, pages 7716\u20137726, Montr\u00b4eal, Canada, 2018. Curran As-\nsociates. 3\n[35] Vincent Sitzmann, Justus Thies, Felix Heide, Matthias\nNie\u00dfner, Gordon Wetzstein, and Michael Zollh\u00a8ofer. Deep-\nvoxels: Learning persistent 3d feature embeddings. In Proc.\nComputer Vision and Pattern Recognition (CVPR), IEEE,\n2019. 3\n[36] Vincent Sitzmann, Michael Zollh\u00a8ofer, and Gordon Wet-\nzstein.\nScene representation networks:\nContinuous 3d-\nstructure-aware neural scene representations. In Advances\nin Neural Information Processing Systems, 2019. 3\n[37] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel\ngrid optimization: Super-fast convergence for radiance fields\nreconstruction. In CVPR, 2022. 3\n[38] Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Prad-\nhan, Ben Mildenhall, Pratul Srinivasan, Jonathan T. Barron,\nand Henrik Kretzschmar. Block-NeRF: Scalable large scene\nneural view synthesis. arXiv, 2022. 3\n[39] Jiaxiang Tang.\nTorch-ngp: a pytorch implementation of\ninstant-ngp, 2022.\nhttps://github.com/ashawkey/torch-ngp.\n7, 8\n[40] Jiaxiang Tang, Xiaokang Chen, Jingbo Wang, and Gang\nZeng. Compressible-composable nerf via rank-residual de-\ncomposition. arXiv preprint arXiv:2205.14870, 2022. 3\n[41] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\nKomura, and Wenping Wang. Neus: Learning neural implicit\nsurfaces by volume rendering for multi-view reconstruction.\narXiv preprint arXiv:2106.10689, 2021. 1, 3\n[42] Chung-Yi Weng,\nBrian Curless,\nPratul P. Srinivasan,\nJonathan T. Barron, and Ira Kemelmacher-Shlizerman. Hu-\nmanNeRF: Free-viewpoint rendering of moving people from\nmonocular video. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\npages 16210\u201316220, June 2022. 3\n[43] Yuanbo Xiangli, Linning Xu, Xingang Pan, Nanxuan Zhao,\nAnyi Rao, Christian Theobalt, Bo Dai, and Dahua Lin.\nBungeenerf: Progressive neural radiance field for extreme\nmulti-scale scene rendering. In The European Conference\non Computer Vision (ECCV), 2022. 3\n[44] Bangbang Yang, Yinda Zhang, Yinghao Xu, Yijin Li, Han\nZhou, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui.\nLearning object-compositional neural radiance field for ed-\nitable scene rendering. In International Conference on Com-\nputer Vision (ICCV), October 2021. 2, 3\n[45] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman.\nVolume rendering of neural implicit surfaces.\nIn Thirty-\nFifth Conference on Neural Information Processing Systems,\n2021. 1, 3\n[46] Weicai Ye, Shuo Chen, Chong Bao, Hujun Bao, Marc Polle-\nfeys, Zhaopeng Cui, and Guofeng Zhang.\nIntrinsicnerf:\nLearning intrinsic neural radiance fields for editable novel\nview synthesis. 2022. 3\n[47] Zehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sat-\ntler, and Andreas Geiger.\nMonosdf: Exploring monocu-\nlar geometric cues for neural implicit surface reconstruc-\ntion. Advances in Neural Information Processing Systems\n(NeurIPS), 2022. 3\n[48] Yu-Jie Yuan, Yang-Tian Sun, Yu-Kun Lai, Yuewen Ma,\nRongfei Jia, and Lin Gao. Nerf-editing: Geometry editing\nof neural radiance fields. In Computer Vision and Pattern\nRecognition (CVPR), 2022. 2, 3, 4\n[49] Jing Zeng, Yanxu Li, Yunlong Ran, Shuo Li, Fei Gao,\nLincheng Li, Shibo He, Jiming Chen, and Qi Ye. Efficient\nview path planning for autonomous implicit reconstruction.\nIn 2023 IEEE International Conference on Robotics and Au-\ntomation (ICRA), pages 4063\u20134069, 2023. 3\n[50] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen\nKoltun. Nerf++: Analyzing and improving neural radiance\nfields. arXiv:2010.07492, 2020. 1\n[51] Xiuming Zhang, Pratul P Srinivasan, Boyang Deng, Paul De-\nbevec, William T Freeman, and Jonathan T Barron. Ner-\nfactor: Neural factorization of shape and reflectance under\nan unknown illumination. ACM Transactions on Graphics\n(TOG), 40(6):1\u201318, 2021. 3\n[52] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe,\nand Noah Snavely. Stereo magnification: Learning view syn-\nthesis using multiplane images. In SIGGRAPH, 2018. 3\n[53] Jingsen Zhu, Yuchi Huo, Qi Ye, Fujun Luan, Jifan Li, Dian-\nbing Xi, Lisha Wang, Rui Tang, Wei Hua, Hujun Bao, et al.\nI2-sdf: Intrinsic indoor scene reconstruction and editing via\nraytracing in neural sdfs. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 12489\u201312498, 2023. 3\n[54] Jingsen Zhu, Fujun Luan, Yuchi Huo, Zihao Lin, Zhihua\nZhong, Dianbing Xi, Rui Wang, Hujun Bao, Jiaxiang Zheng,\nand Rui Tang. Learning-based inverse rendering of complex\nindoor scenes with differentiable monte carlo raytracing. In\nSIGGRAPH Asia 2022 Conference Papers. ACM, 2022. 3\n"
  },
  {
    "title": "Exploring Format Consistency for Instruction Tuning",
    "link": "https://arxiv.org/pdf/2307.15504.pdf",
    "upvote": "5",
    "text": "Published in Transactions on Machine Learning Research (12/2023)\nExploring Format Consistency for Instruction Tuning\nShihao Liang\u2217\nshihaoliang0828@gmail.com\nDepartment of Computer Science\nTsinghua University\nRunchu Tian\u2217\ntrc20@mails.tsinghua.edu.cn\nDepartment of Computer Science\nTsinghua University\nKunlun Zhu\u2217\nzhuklun@mail2.sysu.edu.cn\nDepartment of Computer Science\nTsinghua University\nYujia Qin\nqyj20@mails.tsinghua.edu.cn\nDepartment of Computer Science\nTsinghua University\nHuadong Wang\nhuadw2012@163.com\nModelBest Inc.\nXin Cong\ncongxin1995@tsinghua.edu.cn\nDepartment of Computer Science\nTsinghua University\nZhiyuan Liu\u2020\nliuzy@tsinghua.edu.cn\nDepartment of Computer Science\nTsinghua University\nXiaojiang Liu\nxiaojiang_liu@apple.com\nApple\nMaosong Sun\u2020\nsms@tsinghua.edu.cn\nDepartment of Computer Science\nTsinghua University\nReviewed on OpenReview: https: // openreview. net/ forum? id= n8fZ6mY6PB\nAbstract\nInstruction tuning has emerged as a promising approach to enhancing large language models\nin following human instructions. It is shown that increasing the diversity and number\nof instructions in the training data can consistently enhance generalization performance,\nwhich facilitates a recent endeavor to collect various instructions and integrate existing\ninstruction tuning datasets into larger collections. However, different users have their unique\nways of expressing instructions, and there often exist variations across different datasets\nin the instruction styles and formats, i.e., format inconsistency. In this work, we propose\na framework named \u201cUnified Instruction Tuning\u201d (UIT), which calls OpenAI APIs for\nautomatic format transfer among different instruction tuning datasets such as PromptSource,\nFLAN and CrossFit. With the framework, we (1) demonstrate the necessity of maintaining\n\u2217 Indicates equal contribution.\n\u2020 Corresponding author.\n1\narXiv:2307.15504v2  [cs.CL]  8 Jan 2024\nPublished in Transactions on Machine Learning Research (12/2023)\nformat consistency in instruction tuning; (2) improve the generalization performance on\nunseen instructions on T5-LM-xl; (3) provide a novel perplexity-based denoising method to\nreduce the noise of automatic format transfer to make the UIT framework more practical and\na smaller offline model based on GPT-J that achieves comparable format transfer capability\nto OpenAI APIs to reduce costs in practice. Further analysis regarding variations of targeted\nformats and other effects is intended. The code and trained models are publicly available at\nhttps://github.com/thunlp/UnifiedInstructionTuning.\n1\nIntroduction\nRecently, instruction tuning has gained considerable attention as a potent strategy for enhancing large\nlanguage models (LLMs) in following human instructions and generating appropriate responses. For instance,\nby reformulating various NLP tasks with an instruction template, models trained on the converted dataset\nexhibit powerful capabilities of zero-shot generalization on unseen tasks (Wei et al., 2021). Later studies have\ndemonstrated that instruction tuning is critical to facilitating LLMs in grounding their inner knowledge to\ndiverse real-world scenarios (Ouyang et al., 2022; Iyer et al., 2022; Chung et al., 2022; Ding et al., 2023). Up\nto now, considerable efforts have been dedicated to creating datasets for instruction tuning (Honovich et al.,\n2022a; Bach et al., 2022; Wei et al., 2021; Wang et al., 2022b;a; Aribandi et al., 2022) and researchers find that\nincreasing the task diversity (i.e., the number of unique tasks) of the training data can consistently enhance\ngeneralization performance (Wang et al., 2022b; Iyer et al., 2022; Longpre et al., 2023). Therefore, the\ncommunity has witnessed a growing endeavor to collect various instructions and integrate existing instruction\ntuning datasets into larger collections (Iyer et al., 2022; Longpre et al., 2023; Chung et al., 2022; Zhou et al.,\n2023).\nWhile previous works strive to increase task diversity and merge existing instruction tuning datasets, they\ntypically ignore the format consistency among these datasets. More specifically, different users have their\nunique ways of expressing instructions, even if these instructions correspond to the same intent. Hence, there\noften exist variations across different datasets in the instruction styles and formats, which is dubbed as the\nformat inconsistency issue. Take the case of a summarization task, the instruction can be as detailed as \u201cIn\nthis task, you are given a conversation, and your task is to generate a summary... Input: ... Output: ...\u201d in\nNi-v2 (Wang et al., 2022b) or simply composed of a few keywords, e.g., \u201cSummarize: ...\u201d in CrossFit (Ye\net al., 2021b). Due to the format inconsistency issue, fine-tuned LLMs may have difficulty in handling unseen\ninstructions in a different format at the test time, exhibiting poor out-of-distribution (OOD) generalization.\nHence, before directly merging diverse datasets from various sources and performing multi-task training (i.e.,\nthe common practice), it is essential to conduct a comprehensive study of how format inconsistency may\nimpact the performance of instruction tuning and whether mitigating such inconsistency could enhance the\ngeneralization.\nHowever, unifying the format across different datasets is not easy. First, instructions are inherently diverse\nand nuanced, and the vast range of possible expressions makes it challenging to devise a fixed rule for format\ntransfer. Second, standardizing formats can sometimes inadvertently change the meaning of the original\ninstructions. This is particularly problematic for complex tasks where the instruction\u2019s wording and style are\ncrucial to correctly guiding the model behavior. In this paper, we introduce a format transfer framework,\nUnified Instruction Tuning (UIT) (Figure 1) to explore the effects of format consistency. Specifically, we\nuse OpenAI GPT3.51 for automatic instruction format transfer. Leveraging its powerful in-context learning\ncapabilities, GPT3.5 can successfully transfer the instruction from a source format to a target format based on\nonly a few handcrafted examples. Then we analyze how format inconsistency could affect generalization under\ntwo settings: (1) testing-time setting, which simulates the format inconsistency between the training data\nand the testing data, and (2) training-time setting, which simulates the format inconsistency among different\nsources of instructions in the training data. We perform analysis across five benchmarks and show that our\nmethod successfully mitigates the format inconsistency issue and improves the generalization performance on\nunseen instructions in both settings.\n1https://platform.openai.com/docs/models/gpt-3-5\n2\nPublished in Transactions on Machine Learning Research (12/2023)\nFigure 1: The proposed format transfer framework is applied to two settings: testing-time transfer and\ntraining-time transfer. s1, \u00b7 \u00b7 \u00b7 , sN denote the training data in the original instruction format, t1, \u00b7 \u00b7 \u00b7 , tN\ndenote all the transferred training data in target format.\nDespite its simplicity and performance, the above framework encounters two practical challenges. To begin\nwith, the converted instructions are not as perfect as human-written ones and sometimes involve noise. For\ninstance, an auto-converted instruction may express a slightly different meaning than the original one. To\naddress this issue, we propose a novel perplexity-based denoising strategy that samples multiple possible\nconversions of a source instruction and then filters those low-quality ones based on perplexity. Experimental\nresults reveal that this strategy effectively reduces the noise of format transfer and improves robustness and\nperformance. Second, converting large-scale instructions via OpenAI API can result in substantial costs for\nAPI calls, which is infeasible in practice. To this end, we propose to learn an offline model for format transfer\nby distilling from GPT3.5. We demonstrate that with a few examples generated by GPT3.5, a much smaller\nmodel can be trained to achieve almost equivalent performance in format transfer, which saves the costs for\nAPI calls in practice. In general, our findings shed light on an essential but previously overlooked aspect, i.e.,\nformat consistency, for instruction tuning. We envision our research could inspire more efforts in advancing\nthe instruction tuning methodologies for LLMs.\nTable 1: A comparison of representative instruction tuning datasets of different instruction formats. \u201cNum.\u201d,\n\u201cCate.\u201d, \u201cExp.\u201d, \u201cInst.\u201d, \u201cUnnat-Inst\u201d, refer to Number, Category, Example, Instruction, and unnatural-\ninstructions respectively.\nResource\nTask Num.\nCate. Num.\nTotal Exp.\nInst. format\nNi-v2 (Wang et al., 2022b)\n1616\n76\n5M\ntask-level\nFlan 2021 (Wei et al., 2021)\n62\n12\n4.4M\ninstance-level\nCrossFit (Ye et al., 2021a)\n159\n13\n7.1M\nkeywords-level\nP3 (Bach et al., 2022)\n62\n13\n12M\ninstance-level\nUnnat-Inst (Honovich et al., 2022a)\n117\n\u2212\n64k\ntask-level\nOPT-IML (Iyer et al., 2022)\n1545\n93\n17.9M\nmixed\nFlan 2022 (Longpre et al., 2023)\n1836\n162\n15M\nmixed\n2\nRelated Work\nInstruction Tuning\nInstruction tuning regulates LLMs to accurately comprehend and interpret natural\nlanguage instructions. Prior works in this field focus on reformulating NLP tasks using the templates of\ninstructions. Wei et al. (2021) pioneered to show that fine-tuning LLMs on large collections of tasks formatted\nin instructions enables the model to generalize to unseen tasks in a zero-shot manner. Since then, there has\nbeen a surge of interest in manually constructing high-quality instruction datasets by first reformulating the\nformats of existing NLP datasets and then merging them (Mishra et al., 2022; Bach et al., 2022; Ye et al.,\n2021b; Ouyang et al., 2022). Another line of study (Longpre et al., 2023; Iyer et al., 2022) demonstrates\n3\nPublished in Transactions on Machine Learning Research (12/2023)\nthat scaling the number of training tasks and task diversity can further enhance the model\u2019s generalization\nperformance. However, all these works directly mix all the existing instruction datasets while ignoring\nthe potential issue of format inconsistency. Instead of investigating the number and diversity of training\ninstructions, we instead explore an under-explored facet, i.e., the instruction format of instruction tuning,\nand investigate its impact on generalization.\nData Augmentation\nBesides manually curating instruction tuning datasets, Honovich et al. (2022a)\nshow that fine-tuning LLMs with machine-generated instruction tuning data achieves excellent performance\ncompared with human-written data, indicating that data augmentation is an effective method to enhance the\ndata quantity and task diversity, which overcomes the time-consuming issues of human annotation. Recently,\nTaori et al. (2023); Peng et al. (2023); Ding et al. (2023) adopt machine-annotation method (Wang et al.,\n2022a) to generate real-world human instructions (rather than instructions that describe NLP tasks) and\nmodel responses based on powerful LLMs such as ChatGPT. Similarly, in this paper, we also leverage LLMs\nfor automatic format transfer and data augmentation. Since real-world instructions are quite diverse and\nhard to annotate their formats, we instead focus on instructions that describe NLP tasks to rigorously study\nthe effects of instruction format. We believe the derived findings can potentially be applied to real-world\ninstructions in the future.\nSynthetic Data Denoising\nGenerative models are commonly utilized for data augmentation (Taori et al.,\n2023). However, these synthetic datasets are not always as reliable as those human-annotated ones, and\nfiltering out noisy examples can boost the model performance (Le Bras et al., 2020). Recent studies have\nsuggested different approaches for denoising. For instance, Yang et al. (2020); Fang et al. (2022) adopted\ninfluence functions (Koh & Liang, 2017) to evaluate the quality of the synthetic data; Wang et al. (2022c)\nemploy the NLU Consistency Filtering (Anaby-Tavor et al., 2020) to filter out low-quality samples. In our\nresearch, we utilized LLMs for instruction format transfer, which may introduce noise throughout the process.\nTo overcome this challenge, we adopted a simple and effective perplexity scoring strategy to denoise our\nauto-constructed dataset (section 5).\n3\nInstruction Format Inconsistency\nAs outlined in Iyer et al. (2022), existing instruction formats exhibit variations across different datasets,\nwhich can be classified into three distinct hierarchical levels: Task-level format, Instance-level format, and\nKeywords-level format (as illustrated in Figure 2). We present an overview of existing instruction tuning\ndatasets based on instruction formats in Table 1.\n\u2022 Task-level Format encompasses a comprehensive definition of a task and may include supplementary\ninformation such as positive or negative examples and explanations of the examples. Representative\ndatasets are Ni-v2 (Wang et al., 2022b), Unnatural Instructions\n(Honovich et al., 2022a), and\nAlpaca (Taori et al., 2023).\n\u2022 Instance-level Format employs succinct templates that are customized for each individual example\nand is occasionally structured in a cloze-style format to elicit the intended output. Representative\ndatasets are Flan (Wei et al., 2021) and PromptSource (Bach et al., 2022).\n\u2022 Keywords-level Format closely resembles the instance-level format, but it limits the instruction\ntemplates exclusively to keywords. CrossFit (Ye et al., 2021b) serves as a representative example of\na keywords-level dataset.\nCompared with task diversity, the effect of format consistency is poorly understood in instruction tuning. We\ncontend that successful instruction understanding and generalization are influenced by both task diversity\nand format consistency. Task diversity can be enhanced by incorporating as many tasks into the training\ndata (e.g., merging existing instruction tuning datasets) as possible. However, it is crucial to note that when\nmerging different datasets for training, the training data originating from different sources often present\nvariations in the instruction formats. When confronted with instructions of unseen inconsistent formats\n4\nPublished in Transactions on Machine Learning Research (12/2023)\nTarget Dataset with Unified Instruction Format (\u2131\ufffd)\nDialogue: \nAli: Nice to meet you\nKane: Me, too.\nSummary: Meeting\nAnswer the question: \n[Question]: What animals does a \nmahout work with? \n[Answer]: Elephants\nWrite a paraphrase of the input\nsentence, but use a formal style: \nInput: What's up?\nOutput: How are you doing?\nDefinition: Please summary a \ndialogue.\nPositive Examples: Input: ... \nOutput: ... Now please complete \nthis example:\nInput: Ali: Nice to meet you. \nKane: Me, too.\nOutput: Meeting\nDefinition: Here is a question. \nPlease provide an appropriate \nanswer to the question.\nPositive Examples: Input: ... \nOutput: ... Now please complete \nthis example:\nInput: What animals does a \nmahout work with?\nOutput: Elephants\n \n \n \n \n \n \n \n \n \n \n  \n \n \n \n  \n  \n \n \n \n \n \n \n \n \n \n \n \nDefinition: In this task, you need \nto rewrite the informal sentence\nin a formal style.\nPositive Examples: Input: ...\nOutput: ... Now please complete\nthis example:\nInput: What\u2019s up?\nOutput: How are you doing?\nUnified Instruction Format Transfer (\u2131\ufffd\ufffd \u2192 \u2131\ufffd)\nKeywords-level Format: \nCrossFit \nInstance-level Format: \nFlan, PromptSource \nTask-level Format: \nDiversePrompt \nInstruction Tuning for LLMs\nSource Datasets with Diverse Instruction Formats ({\u2131\ufffd\ufffd})\nExample 1:  [s1, t1] \nExample 2:  [s2, t2] \n\u2026\nExample k:  [sk, tk] \nSeed parallel data \nFigure 2: Transferring instruction formats with UIT. The existing instruction formats exhibit variations\nacross different datasets, which can be classified into three distinct hierarchical formats: Task level, Instance\nlevel, and Keywords level. UIT leverages seed parallel data to conduct format transfer across different formats\nautomatically.\nat the test time, the trained model may fail to generalize well and comprehend the intent behind different\ninstructions, showing poor OOD generalization.\n4\nFramework and Experiments\nTo mitigate format inconsistency, we propose a format transfer framework, Unified Instruction Tuning (UIT),\nto convert the instruction formats of existing datasets into a unified format.\n4.1\nUnified Instruction Format Transfer\nDenote the target unified instruction format as Ft and the original instruction format of a source dataset\nas Fs, we aim to convert Fs into Ft to alleviate the OOD generalization in the instruction format. Taking\ninspiration from Honovich et al. (2022a), we rely on the LLM\u2019s in-context learning ability to conduct format\ntransfer in an automatic manner. Specifically, we manually select k seed parallel data {[s1, t1], \u00b7 \u00b7 \u00b7 , [sk, tk]},\nwhere si and ti are the same instance (task) expressed in format Fs and Ft respectively.\nGiven a new instance snew with format Fs, we transfer its instruction format into the unified instruction\nformat Ft via in-context learning as follows:\ntnew = LLM (snew, [s1, t1], \u00b7 \u00b7 \u00b7 , [sk, tk]) ,\n(1)\nwhere tnew refers to the transferred instance with Ft. We choose text-davinci-003 (GPT3.5) as the LLM\nfor format transfer. Details of the prompt for format transfer are shown in Figure 3.\n4.2\nExperiments\nSettings\nTo simulate the format inconsistency problem, we design two experimental settings:\n5\nPublished in Transactions on Machine Learning Research (12/2023)\nFigure 3: An example of format transfer using GPT3.5, where we prompt the model with 3 parallel examples\nto generate the target instruction for the 4-th example.\n\u2022 Testing-time Format Transfer: the training data is formatted in Ft, while the test data is\nformatted in Fs. To mitigate the format inconsistency, we convert the instruction format of the test\ndata into Ft, without modifying the training data. This setting is designed to explore the format\ninconsistency impact between training data and the test data in the inference phase.\n\u2022 Training-time Format Transfer: the training data is mixed with different formats (e.g., both\nFs and Ft), and the testing data is in the format of Ft. Instead of modifying the testing data, here\nwe convert the training data from format Fs to Ft. This setting is designed to simulate the format\ninconsistency of different sources of the training data.\nFor both settings, we choose T5-LM-xl 2 as our model and use Exact Match (EM) and Rouge-L as evaluation\nmetrics.\nDatasets\nFor the testing-time setting, we select Ni-v2 (Wang et al., 2022b) as the training dataset and\nuse DiversePrompt (Honovich et al., 2022b), Flan (Wei et al., 2021), CrossFit (Ye et al., 2021a), and\nPromptSource (Bach et al., 2022) as the test dataset. We evaluate the tasks that do not appear in the\ntraining stage. These tasks are the same as or similar to those in Ni-v2 test set. In Ni-v2, the instruction\nformat incorporates four components: (1) task definition (D), (2) positive example (P) for demonstration\ninstances with the ground-truth label, (3) negative examples (N) for demonstration instances with a false\nlabel, and (4) explanations (E) that provide detailed explanations for the examples. Different formats refer to\ndistinct combinations of the above components. For example, the DP format includes the task definition and\npositive examples information. In our experiments, we consider four primary formats, namely DP, DPN,\nDPE, and DPNE as the unified instruction format, respectively.\nFor the training-time setting, we use the training set of Ni-v2 together with Flan, CrossFit, and P3 respectively\nfor training and use the test set of Ni-v2 for evaluation. As Flan, CrossFit, and P3 may contain instances that\nexist in the test set of Ni-v2, to prevent data leakage, we filter the overlapped data in Flan, CrossFit, and P3\nand use the remaining data for training. In this setting, we choose DP as the unified instruction format.\nBaselines\nWe construct two baselines: (1) Raw does not involve any modifications on the instruction\nformat for both training and testing. For instance, we directly test an instance from Flan in its original\nformat using a model trained with Ni-v2 in DPN format. (2) Heuristic applies manually-designed rules to\ntransfer different instruction formats into the unified one. If the information from the original format matches\nthe corresponding field in the unified format, we fill the unified format with that information. Otherwise, we\n2https://huggingface.co/google/t5-xl-lm-adapt\n6\nPublished in Transactions on Machine Learning Research (12/2023)\nleave the respective field in the unified format blank. For instance, an instance from Flan can be transferred\nto the DPE format by leaving the Definition and Explanations fields blank and filling the Positive Examples\nfield with randomly selected instances from the Flan training set.\nTable 2:\nTesting-time format transfer experiment with four target unified instruction formats (DP, DPE,\nDPN, DPNE), respectively. We evaluate three methods: (1) raw instructions, transferred instructions based\non (2) heuristic rules and (3) our proposed UIT. The training is conducted on Ni-v2 while the testing is\nconducted on DiversePrompt, FLAN, CrossFit, and PromptSource, respectively.\nFormat\nMethod\nDiversePrompt\nFLAN\nCrossFit\nPromptSource\nAverage\nEM\nRouge-L\nEM\nRouge-L\nEM\nRouge-L\nEM\nRouge-L\nEM\nRouge-L\nDP\nraw\n0.1\n4.7\n11.6\n20.8\n0.2\n3.9\n6.6\n13.6\n4.6\n10.8\nheuristic\n34.7\n45.1\n31.4\n44.8\n43.7\n56.0\n27.3\n32.7\n34.3\n44.6\nunified\n34.2\n45.4\n32.6\n46.3\n49.1\n60.1\n29.2\n34.7\n36.3\n46.6\nDPE\nraw\n0.1\n5.0\n18.1\n27.8\n0.3\n4.4\n14.4\n19.2\n8.2\n14.1\nheuristic\n32.5\n43.4\n32.0\n45.3\n41.3\n54.2\n26.6\n31.1\n33.1\n43.8\nunified\n32.9\n44.8\n33.5\n46.9\n46.9\n58.4\n27.8\n32.8\n35.5\n46.0\nDPN\nraw\n0.2\n5.5\n12.0\n22.6\n0.2\n4.5\n5.3\n11.6\n4.4\n11.1\nheuristic\n30.6\n43.5\n31.9\n45.3\n43.5\n55.5\n29.0\n33.9\n33.9\n44.8\nunified\n31.5\n44.3\n34.8\n48.3\n50.3\n60.4\n32.4\n38.3\n37.5\n48.2\nDPNE\nraw\n0.1\n5.2\n15.2\n25.3\n0.2\n3.8\n19.2\n23.7\n8.7\n14.5\nheuristic\n30.6\n43.4\n30.7\n43.6\n42.8\n54.6\n29.1\n33.7\n33.4\n44.1\nunified\n32.2\n43.4\n35.0\n48.0\n48.6\n59.3\n29.8\n34.9\n36.6\n46.7\nResults and Analyses\nTesting-time format transfer results are shown in Table 2, and we find that:\n(1) transferring the instruction format either through the heuristic rule or our UIT significantly improves\nthe performance than the vanilla baseline (i.e., raw), demonstrating the necessity of maintaining format\nconsistency in instruction tuning; (2) our UIT consistently outperforms the heuristic method across all\nbenchmarks and almost all formats in Ni-v2. Compared with the heuristic method, UIT fully utilizes the\nsemantic understanding and generation abilities of GPT3.5 to derive better transferred instructions; (3) the\nDPN format demonstrates the highest average performance and exhibits the largest improvements with UIT.\nTraining-time format transfer results are shown in Table 3, which shows that format transfer also brings\nperformance improvements compared to raw baseline and performs slightly better than the heuristic method.\nThis again demonstrates that UIT can improve the generalization performance by unifying the instruction\nformat. However, the improvements in the training-time setting are not as significant as those in the\ntesting-time setting. We conjecture this may be because the format inconsistency issue is more evident in our\ntesting-time setting than in the training-time setting. Overall, the results under both settings validate our\nhypothesis that mitigating the instruction format conduces to improved generalization.\nLimitations in Practice\nDespite the favorable performance, the proposed framework still has some\nlimitations: first, automatic format transfer sometimes involves noise or even errors in the generated data,\nwhich may produce adverse effects; second, the proposed method heavily relies on OpenAI API calls, which\nentail substantial costs especially for large-scale instruction datasets. Both issues would limit UIT\u2019s real-world\ndeployment. In the following, we discuss potential solutions for the above limitations by proposing a denoising\nstrategy (section 5) and training an offline transfer model (section 6), respectively.\n5\nDenoising for Format Transfer\nEmpirically, transferring format via LLMs will introduce noise unavoidably. The transferred instances\nmay contain errors like critical changes to task definition or hallucinatory restrictions. Intuitively, utilizing\nerroneous instructions would impair the model\u2019s generalization performance. To this end, we propose a\nperplexity-based denoising strategy to filter low-quality instances.\n7\nPublished in Transactions on Machine Learning Research (12/2023)\nTable 3:\nTraining-time format transfer experiment with DP format. We compare our UIT with two baselines:\nraw instructions and instructions transferred by the heuristic rule. The training dataset is Ni-v2 combined\nwith CrossFit, Flan, or P3.\nMethod\n+CrossFit\n+FLAN\n+P3\nAverage\nEM\nRouge-L\nEM\nRouge-L\nEM\nRouge-L\nEM\nRouge-L\nraw\n37.5\n56.0\n38.4\n56.9\n38.8\n56.7\n38.2\n56.5\nheuristic\n37.7\n55.9\n38.9\n57.4\n39.9\n58.1\n38.8\n57.1\nunified\n37.9\n56.0\n38.9\n57.5\n39.4\n57.3\n38.7\n56.9\nFigure 4: The performance of the denoising strategy at the testing and training time with different number\nof samples. Detailed results in the form of a table are presented in Section B of the appendices.\nPerplexity-based Denoising Strategy\nPerplexity (PPL) is a widely used metric for evaluating the\nsemantic coherence and certainty of language models. We assume that noisy instructions can reduce the\ncertainty of LLMs in accurately predicting the correct output token sequence3, leading to higher perplexity.\nAs a result, perplexity serves as a useful metric for assessing the quality of transferred instructions. Hence, we\npropose to sample multiple times from LLM to obtain multiple transferred instructions. Then we calculate\nthe perplexity for each instruction. Specifically, we concatenate the transferred instruction and the input\nquery, then predict the annotated label and calculate its perplexity, and filter those with high perplexity.\nWe employ GPT3.5 with temperature 1.0 to perform sampling for N times with different random seeds,\nwhere N is chosen from {1, 2, 4, 8, 16, 32}. Then, we sort the generated instructions based on perplexity\nusing GPT-J (Wang & Komatsuzaki, 2021) and select the sample with the lowest perplexity. We compare\nour method with the baseline that only samples once from GPT3.5. We conduct experiments on Ni-v2\nand PromptSource under both the testing-time and training-time settings. For the former, we select the\ntransferred instruction samples with the lowest perplexity; while for the latter, we incorporate multiple\ntransferred results with lower perplexity as the training data.\nResults\nAs shown in figure 4, our proposed denoising strategy stably improves the performance at the\ntesting time, and this improvement continues to increase when more instructions are sampled, which shows\nour method can successfully filter out those low-quality instructions to reduce noise during format transfer.\nIn addition, the method can also improve performance in the training-time setting but the improvement is\nnot more evident when more transferred instructions are included in the training data. It reveals that the\nmodel is less sensitive to noise during the training phase.\n6\nTraining Offline Model for Format Transfer\nConverting large-scale instructions via OpenAI API can cause substantial costs for API calls. To alleviate\nthe reliance on OpenAI API, it is necessary to derive an offline model that has comparable format transfer\n3We merely use the positive example (P) as mentioned in section 4.2 for noise assessment.\n8\nPublished in Transactions on Machine Learning Research (12/2023)\nperformance to GPT3.5 but involves fewer costs. Hence we propose to distill the format transfer ability of\nGPT3.5 into small-scale models.\nFine-tuned Offline Model with Knowledge Distillation\nCompared with larger models, small offline\nmodels are less capable of completing format transfer directly through in-context learning without training.\nTherefore, we strive to enhance small-scale models via knowledge distillation (Hinton et al., 2015). In pursuit\nof higher quality, we always make GPT3.5 convert the relatively complex and informative instruction format\n(e.g., Ni-v2) into a simpler and less informative one (e.g., PromptSource). In this way, we obtain parallel data\nand use it to fine-tune GPT-J for format transfer. We use the generated PromptSource-style instructions as\nthe source and the original Ni-v2 instructions as the target to construct a dataset of approximately 3,000\ninstances. To assess the quality of GPT-J\u2019s transfer results, we compare them with the heuristic baseline and\nGPT3.5\u2019s conversion results in the testing-time setting with two formats (DP and DPN).\nTable 4:\nResults of training an offline model (GPT-J) for format transfer at testing time. We compare the\ntransferred instructions using heuristic rules, GPT3.5, or our fine-tuned GPT-J. Other settings are similar to\nthose in Table 2.\nFormat\nMethod\nDiversePrompt\nFlan\nCrossFit\nPromptSource\nAverage\nEM\nRouge-L\nEM\nRouge-L\nEM\nRouge-L\nEM\nRouge-L\nEM\nRouge-L\nDP\nheuristic\n34.7\n45.1\n31.4\n44.8\n43.7\n56.0\n27.3\n32.7\n34.3\n44.6\nGPT3.5\n34.2\n45.4\n32.6\n46.3\n49.1\n60.1\n29.2\n34.7\n36.3\n46.6\nGPT-J\n35.2\n45.6\n33.5\n46.6\n43.6\n54.5\n31.6\n36.4\n36.0\n45.8\nDPN\nheuristic\n30.6\n43.5\n31.9\n45.3\n43.5\n55.5\n29.0\n33.9\n33.9\n44.8\nGPT3.5\n31.5\n44.3\n34.8\n48.3\n50.3\n60.4\n30.8\n36.1\n37.1\n47.6\nGPT-J\n34.7\n45.7\n34.8\n48.4\n46.0\n55.5\n31.4\n36.5\n36.7\n46.5\nResults\nAs exhibited in Table 4, the fine-tuned GPT-J performs much better than the heuristic baseline\nbut slightly worse than GPT3.5. This shows that our method can distill the format transfer ability into\nsmall-scale models, which saves the costs in practice. Additionally, the performance is highly correlated with\nthe similarity of the source and target formats. For instance, for DiversePrompt whose instruction format\nis similar to the target format, the transfer process is less challenging. As a result, the fine-tuned model\ndemonstrates comparable or even superior performance than GPT3.5. Conversely, for CrossFit which only\ndescribes keywords and lacks natural language instructions, it is more difficult for small models to produce\nhigh-quality instructions, resulting in inferior performance.\n7\nFurther Analysis\nEffects of the Target Unified Format\nIn previous experiments, we mainly use Ni-v2 as the target\ninstruction format. To verify the versatility of UIT for various target instruction formats, we select Flan, an\ninstance-level dataset as the target dataset and conduct testing-time transfer experiments. Results are shown\nin Figure 6, from which we find that testing-time format transfer brings even more significant performance\nimprovements than the scenario when Ni-v2 is selected as target dataset. This again validates our hypothesis\nthat format consistency is essential to OOD generalization for instruction tuning, no matter which target\nformat is.\nEffects of Model Scaling\nAs observed in previous works (Iyer et al., 2022), larger models tend to perform\nbetter in following human instructions. We also conduct model scaling experiments in the testing-time setting\nwith T5 (Raffel et al., 2020), with the model size ranging from 5 million (T5-small) to 10 billion (T5-XXL).\nResults presented in Figure 5 demonstrate that in general, the performance tends to improve as the model\nsize increases. These findings suggest that instruction format consistency is consistently essential to language\nmodels of various sizes.\n9\nPublished in Transactions on Machine Learning Research (12/2023)\nsmall\nbase\nlarge\nXL\nXXL\nModel Size\n25\n30\n35\n40\n45\n50\n55\nEM (%)\nScaling test on T5-LM\nFlan Unified\nCrossFit Unified\nP3 Unified\nFlan Heuristic\nCrossFit Heuristic\nP3 Heuristic\nFigure 5: Results of T5-LM of different model sizes on\nthe testing-time transfer setting.\nFigure 6: Testing time transfer experiment results\nwhen Flan is selected as the target dataset and Ni-\nv2 as the source dataset. Transferring Ni-v2 to the\ntarget format brings significant performance improve-\nments during inference when training is conducted\nwith target format. Results in the form of a table\nare presented in Section C of the appendices.\nTable 5:\nExperiments for task diversity and format consistency. For task diversity, we set the training\ndataset to src+same, src+diff or src+same+diff. For format consistency, we either use the raw format\nor use the unified format.\nMethod\nsrc+same\nsrc+diff\nsrc+same+diff\nEM\nRouge-L\nEM\nRouge-L\nEM\nRouge-L\nraw\n29.3\n46.3\n28.3\n45.3\n29.1\n45.8\nunified\n30.8\n47.6\n30.7\n47.7\n31.0\n47.8\nTask Diversity v.s. Format Consistency\nWe show that both task diversity and format consistency have\nimpacts on the generalization performance for instruction tuning. As task diversity can only be a variable\nduring the training stage, we only conduct training-time transfer experiments. Specifically, we choose Ni-v2\nas the target dataset with DP as target format and P3 as the source dataset. We first randomly select 20\ntasks from Ni-v2 (denoted as src). Then we choose the same 20 training tasks from P3, denoted as same,\nand 20 different tasks from P3, which is denoted as diff. We treat whether to integrate same or diff to the\ntraining set (src) as a variable and evaluate on the original Ni-v2 test set.\nAs shown in Table 5, no matter which tasks are chosen as the training data, our UIT always performs better\nthan the vanilla baseline (raw), which again demonstrates the importance of format consistency. We can also\nobserve that without format unification, src+same performs better than src+diff, which indicates that\nincreasing task diversity may be inferior without format consistency. Besides, source+same+diff with UIT\nperforms the best among all combinations, suggesting that increasing task diversity and maintaining format\nconsistency at the same time is the best practice for merging datasets in instruction tuning. We believe this\nfinding can guide practitioners to better prepare the datasets for instruction tuning in the future.\n8\nConclusion\nIn this paper, we propose the unified instruction-tuning framework (UIT), a standardized approach to\nenhancing the generalization ability for instruction tuning by unifying the format of existing instruction\ntuning datasets and enabling format transfer between them with LLMs like GPT-3.5. With the framework,\nwe (1) exhibit the significance of format consistency in instruction tuning; (2) enhance the generalization\n10\nPublished in Transactions on Machine Learning Research (12/2023)\nperformance (9.3% in Exact Match, 7.6% in Rouge-L) on various datasets such as PromptSource, FLAN and\nCrossFit on T5-LM-xl; (3) propose a denoising method and an offline model training method to make our\nUIT more feasible in practice.\nIn general, we study an under-explored facet, i.e., the format consistency, for instruction tuning, and we hope\nour work could facilitate more attempts in relevant areas.\n9\nLimitation\nWhile our proposed UIT framework and format transferer offer a promising approach to enhancing the\ngeneralization performance of instruction-tuned LLMs, several limitations should be acknowledged. Firstly,\nour method relies on the assumption that the user knows the target instruction format in advance, which may\nnot always be the case. Secondly, we focus on instruction tuning for NLP tasks, instead of broader settings\n(e.g., real-world instructions (Taori et al., 2023; Chiang et al., 2023)) where formats are hard to define. We\nexpect future works to explore whether our UIT framework can be applied to broader scenarios.\nAcknowledgement\nThis work is supported by the National Natural Science Foundation of China (Grant No. 62306159).\nReferences\nAteret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich, Amir Kantor, George Kour, Segev Shlomov, Naama\nTepper, and Naama Zwerdling. Do not have enough data? deep learning to the rescue! In Proceedings of\nthe AAAI Conference on Artificial Intelligence, volume 34, pp. 7383\u20137390, 2020.\nVamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei\nZhuang, Vinh Q. Tran, Dara Bahri, Jianmo Ni, Jai Prakash Gupta, Kai Hui, Sebastian Ruder, and Donald\nMetzler. Ext5: Towards extreme multi-task scaling for transfer learning. In ICLR. OpenReview.net, 2022.\nStephen Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal V Nayak, Abheesht Sharma,\nTaewoon Kim, M Saiful Bari, Thibault F\u00e9vry, et al. Promptsource: An integrated development environment\nand repository for natural language prompts. In Proceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics: System Demonstrations, pp. 93\u2013104, 2022.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source\nchatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.\nURL https://lmsys.org/blog/\n2023-03-30-vicuna/.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv\npreprint arXiv:2210.11416, 2022.\nNing Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and\nBowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. arXiv\npreprint arXiv:2305.14233, 2023.\nTianqing Fang, Quyet V Do, Hongming Zhang, Yangqiu Song, Ginny Y Wong, and Simon See. Pseudoreasoner:\nLeveraging pseudo labels for commonsense knowledge base population. arXiv preprint arXiv:2210.07988,\n2022.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint\narXiv:1503.02531, 2015.\nOr Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning language\nmodels with (almost) no human labor, 2022a. URL https://arxiv.org/abs/2212.09689.\n11\nPublished in Transactions on Machine Learning Research (12/2023)\nOr Honovich, Uri Shaham, Samuel R. Bowman, and Omer Levy. Instruction induction: From few examples\nto natural language task descriptions. CoRR, abs/2205.10782, 2022b. doi: 10.48550/arXiv.2205.10782.\nURL https://doi.org/10.48550/arXiv.2205.10782.\nSrinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, D\u00e1niel Simig, Ping Yu, Kurt Shuster,\nTianlu Wang, Qing Liu, Punit Singh Koura, et al. Opt-iml: Scaling language model instruction meta\nlearning through the lens of generalization. arXiv preprint arXiv:2212.12017, 2022.\nPang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In International\nconference on machine learning, pp. 1885\u20131894. PMLR, 2017.\nRonan Le Bras, Swabha Swayamdipta, Chandra Bhagavatula, Rowan Zellers, Matthew Peters, Ashish\nSabharwal, and Yejin Choi. Adversarial filters of dataset biases. In International Conference on Machine\nLearning, pp. 1078\u20131088. PMLR, 2020.\nShayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le,\nBarret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for effective instruction\ntuning. arXiv preprint arXiv:2301.13688, 2023.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via\nnatural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pp. 3470\u20133487, 2022.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\nhuman feedback. arXiv preprint arXiv:2203.02155, 2022.\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4,\n2023.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.\nJ. Mach. Learn. Res., 21:140:1\u2013140:67, 2020.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and\nTatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/\ntatsu-lab/stanford_alpaca, 2023.\nBen Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.\nhttps://github.com/kingoflolz/mesh-transformer-jax, May 2021.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh\nHajishirzi.\nSelf-instruct: Aligning language model with self generated instructions.\narXiv preprint\narXiv:2212.10560, 2022a.\nYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik,\nArjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, et al. Super-naturalinstructions:\nGeneralization via declarative instructions on 1600+ nlp tasks. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing, pp. 5085\u20135109, 2022b.\nYufei Wang, Can Xu, Qingfeng Sun, Huang Hu, Chongyang Tao, Xiubo Geng, and Daxin Jiang. PromDA:\nPrompt-based data augmentation for low-resource NLU tasks. In Proceedings of the 60th Annual Meeting\nof the Association for Computational Linguistics (Volume 1: Long Papers), pp. 4242\u20134255, Dublin,\nIreland, May 2022c. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.292. URL\nhttps://aclanthology.org/2022.acl-long.292.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M\nDai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652,\n2021.\n12\nPublished in Transactions on Machine Learning Research (12/2023)\nYiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta, Ronan Le Bras, Ji-Ping Wang,\nChandra Bhagavatula, Yejin Choi, and Doug Downey. Generative data augmentation for commonsense\nreasoning. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 1008\u20131025,\nOnline, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.\n90. URL https://aclanthology.org/2020.findings-emnlp.90.\nQinyuan Ye, Bill Yuchen Lin, and Xiang Ren.\nCrossFit: A few-shot learning challenge for cross-task\ngeneralization in NLP. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language\nProcessing, pp. 7163\u20137189, Online and Punta Cana, Dominican Republic, November 2021a. Association for\nComputational Linguistics. doi: 10.18653/v1/2021.emnlp-main.572. URL https://aclanthology.org/\n2021.emnlp-main.572.\nQinyuan Ye, Bill Yuchen Lin, and Xiang Ren.\nCrossfit: A few-shot learning challenge for cross-task\ngeneralization in nlp. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language\nProcessing, pp. 7163\u20137189, 2021b.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,\nLili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023.\n13\nPublished in Transactions on Machine Learning Research (12/2023)\nAppendices\nA\nCase Study\nWe list some examples of our format transfer process in this section. You can find examples of Defintion,\nPositive examples, Negative examples and Explanation in these cases.\nPublished in Transactions on Machine Learning Research (12/2023)\nAppendices\nA\nCase Study\nWe list some examples of our format transfer process in this section. You can \ufb01nd examples of De\ufb01ntion,\nPositive examples, Negative examples and Explanation in these cases.\nTransferring from P3 to Ni-v2 (Task:\ntrec):\nInstruction: You are given a sentence that contains a question and a possible answer type. Your task\nis to identify the correct answer type from the suggested options. You may need to read the sentence and\nits context carefully in order to determine the correct answer type.\nPositive Examples:\nInput: What do bee hives do in cranberry bogs ?\\nIs this asking about Description, Entity, Abbreviation,\nPerson, Quantity, Location?\nOutput: Description\nExplanation: The question is asking for a description of what bee hives do in cranberry bogs. So the\ncorrect answer type is Description.\nNegative Examples:\nInput: What gol\ufb01ng accessory was patented by George Grant on December 12\\nIs this asking about\nDescription, Entity, Abbreviation, Person, Quantity, Location?\nOutput: Quantity\nExplanation: The sentence is asking about a gol\ufb01ng accessory. These types of questions typically require\nan answer about an entity (i.e. a speci\ufb01c object or thing), so the correct answer type is \u2019Entity\u2019 rather\nthan \u2019Quantity\u2019.\nOriginal P3(trec):\nInstruction:\n{Input}\\n\\nDescriptors: Description, Entity, Abbreviation, Person, Quantity, Loca-\ntion\\n\\nBest Descriptor?\\n\nPositive Examples:\nInput: What do bee hives do in cranberry bogs ?\\n\nOutput: \\nDescription\nTransferring from Flan to Ni-v2 (Task:\nwsc):\nInstruction: In this task, you are given two sentences (sentence 1 and sentence 2). If sentence 1 implies\nthat sentence 2 is true, answer \"Yes\", otherwise \"No\".\nPositive Examples:\nInput: sentence 1: As Ollie carried Tommy up the long winding steps, his legs ached. sentence 2:\nTommy\u2019s legs ached. options: - no - yes.\nOutput: no\nExplanation: The sentence 1 does not imply that Tommy\u2019s legs ached. So, the output should be \"No\".\nNegative Examples:\nInput: Sentence 1: Paul couldn\u2019t \ufb01nd his car keys, so he had to walk. Sentence 2: Paul had to walk.\nOutput: Yes\nExplanation: Sentence 1 implies that sentence 2 is true, so the correct output should be \"Yes\".\nOriginal Flan(wsc):\nInstruction:\nIf \"{sentence1}\", can we conclude that \"{sentence2}\"\\n{options_}\nPositive Examples:\nInput: If \"As Ollie carried Tommy up the long winding steps, his legs ached.\", can we conclude that\n\"Tommy\u2019s legs ached.\"\\n\\nOPTIONS:\\n- no\\n- yes.\nOutput: no\n14\n14\nPublished in Transactions on Machine Learning Research (12/2023)\nPublished in Transactions on Machine Learning Research (12/2023)\nTransferring from CrossFit to Ni-v2 (Task:\nimdb):\nInstruction: You are given a sentence or a paragraph describing a particular topic. Your task is to\nclassify the sentiment of the sentence/paragraph as either negative or positive. Label the sentiment in the\noutput as per the given parameters without incorporating any additional information into your answer.\nPositive Examples:\nInput: I am a big fan of Faerie Tale Theatre and I\u2019ve seen them all and this is one of the best! It\u2019s funny,\nromantic, and a classic. I recommend this for all ages. It\u2019s great for little kids because it\u2019s well, Cinderella\nand great for adults and teens because it\u2019s funny and not over the top. I watched it when I was little and\nI still watch it now. It has great lines that my family and I quote all the time. The acting is great and it\nnever gets old. If you like fairy tales and romances you will love this. I\u2019ve watched many a Cinderella\nmovie in my time and this is the best of them all. (Sorry Disney) I highly recommend this movie and all\nthe Faerie Tale Theatre shows. They all appeal to all ages and are all unique and very entertaining.\nOutput: positive\nExplanation: The sentence describes a very positive opinion on the chosen topic. The opinion is\nsupported by facts, like the uniqueness of the show, its lasting values, great acting, and so on. Hence, the\nsentiment of the sentence is classi\ufb01ed as positive.\nNegative Examples:\nInput: I know a few things that are worst. A few. It had a couple of funny scenes. It is a movie not\nappropriate for kids but, only a child would \ufb01nd this movie hilarious. This is de\ufb01nitely a movie that you\nwould like to use a free rental coupon for. Don\u2019t waste your money just to laugh a couple of times.\nOutput: Positive\nExplanation: The given sentence is mainly negative in nature as it suggests not to waste money on the\nmovie. The words \\\"de\ufb01netly\\\" and \\\"a couple of funny scenes\\\" are used in the sentence to provide a bit\nof contrast, yet it does not make the overall sentiment of the sentence positive. Therefore, the correct\nanswer should be \\\"negative\\\" instead of \\\"positive\\\".\nOriginal CrossFit(imdb):\nInstruction:\nPositive Examples:\nInput: I am a big fan of Faerie Tale Theatre and I\u2019ve seen them all and this is one of the best! It\u2019s funny,\nromantic, and a classic. I recommend this for all ages. It\u2019s great for little kids because it\u2019s well, Cinderella\nand great for adults and teens because it\u2019s funny and not over the top. I watched it when I was little and\nI still watch it now. It has great lines that my family and I quote all the time. The acting is great and it\nnever gets old. If you like fairy tales and romances you will love this. I\u2019ve watched many a Cinderella\nmovie in my time and this is the best of them all. (Sorry Disney) I highly recommend this movie and all\nthe Faerie Tale Theatre shows. They all appeal to all ages and are all unique and very entertaining.\nOutput: positive\nTransferring from Ni-v2 to Flan (Task:\ndialogre):\nInstruction: {input} Identify the name of one of the speakers in the given dialog.\nOriginal Ni-v2(dialogre):\nInstruction:\nYou are given a dialog between 2 or more individuals. Within the dialog, there will be\nclues as to the names of the speakers. You will be asked at the end of the dialog to identify the name of\none of the speakers.\nB\nDetailed Results of the Denoising Strategy\nThis is the detailed results of the performance of the denoising strategy with di\ufb00erent number of samples.\n15\nB\nDetailed Results of the Denoising Strategy\nThis is the detailed results of the performance of the denoising strategy with different number of samples.\n15\nPublished in Transactions on Machine Learning Research (12/2023)\nTable 6:\nThe performance of the denoising strategy at the testing and training time with different numbers\nof samples.\nSample Num\nTesting time\nTraining time\nEM\nRouge-L\nEM\nRouge-L\n1\n32.4\n38.3\n37.4\n56.1\n2\n33.0\n38.9\n39.4\n57.6\n4\n33.5\n39.4\n38.2\n56.5\n8\n33.7\n39.7\n38.8\n57.0\n16\n33.8\n39.8\n38.5\n56.3\nC\nResults with Flan Selected as the Target Dataset\nThis is the detailed results of Testing time transfer experiment results when Flan is selected as the target\ndataset and Ni-v2 as the source dataset.\nTable 7:\nTesting time transfer experiment results when Flan is selected as the target dataset and Ni-v2 as\nthe source dataset. Transferring Ni-v2 to the target format brings significant performance improvements\nduring inference when training is conducted with target format.\nSource\nTarget\nMethod\nEM\nRouge-L\nNi-v2\nFlan\nheuristic\n18.4\n28.3\nNi-v2\nFlan\nunified\n29.3\n43.0\nD\nSeed Data\nExample 1\nTask description A: Review: {sentence} Is this movie review sentence negative or positive? {options_}\nTask description B: In this task, you are given sentences from movie reviews. The task is to classify a\nsentence as \"POS\" if the sentiment of the sentence is positive or as \"NEG\" if the sentiment of the sentence is\nnegative\nPositive examples: Input b positive 1: It \u2019s a lovely film with lovely performances by Buy and Accorsi.\nOutput b positive 1: POS Explanation b positive 1: The sentiment of the sentence is positive. Hence, the\nlabel is \u2019POS\u2019.\nInput b positive 2: Here\u2019s yet another studio horror franchise mucking up its storyline with glitches casual\nfans could correct in their sleep. Output b positive 2: NEG Explanation b positive 2: The sentiment of the\nsentence is negative. Hence, the label is \u2019NEG\u2019.\nNegative examples: Input b negative 1: A smart, witty follow-up. Output b negative 1: NEG Explanation\nb negative 1: Although the sentiment of the sentence is positive, the label is \u2019NEG\u2019. Hence, the label should\nbe \u2019POS\u2019.\nInput b negative 2: Ultimately feels empty and unsatisfying, like swallowing a Communion wafer without\nthe wine. Output b negative 2: POS Explanation b negative 2: Although the sentiment of the sentence is\npositive, the label is \u2019POS\u2019. Hence, the label should be \u2019NEG\u2019.\nExample 2\nTask description A: {question1} {question2} Would you say that these questions are the same? {options_}\n16\nPublished in Transactions on Machine Learning Research (12/2023)\nTask description B: Here are two questions (Question1 and Question2). If these questions have the same\nmeaning and same answer, answer \"Yes\", otherwise \"No\".\nPositive examples: Input b positive 1: Question1: How do I get into my Instagram if I forgot my email\nand my Facebook password?, Question2: I forgot my password and also my email password. how can I get\nback that account? Output b positive 1: Yes Explanation b positive 1: These questions have the meaning\nand the same answer. So, the output should be \"Yes\".\nInput b positive 2: Question1: Why don\u2019t Hong Kong residents emigrate from their cramped & stressful\ncity, like to places such as Australia?, Question2: Why made Hong Kong so attractive to Britain as a colony\ngiven that it was the last of Britain\u2019s colonies and Britain does not profit from taxing Hong Kong? Output b\npositive 2: No Explanation b positive 2: The first question is about the emigration of Hong Kong residents\nand the second question is about the attraction of Hong Kong. So, they don\u2019t have the same meaning.\nNegative examples: Input b negative 1: Question1: Why are there so many accidents on I-880?, Question2:\nWere there accidents in outer space? Output b negative 1: Yes Explanation b negative 1: Question1 asks\nabout the cause of the accidents, while question2 inquires about their existence. So, they are different and\nthe correct output should be \"No\".\nInput b negative 2: Question1: How do you determine the number of neutrons of an element or its ion?,\nQuestion2: How do you find the number of neutrons in an element? What are some examples? Output b\nnegative 2: They are the same. Explanation b negative 2: Note that you need to answer with \"Yes\" or \"No\"\nand other answers are not acceptable.\nExample 3\nTask description A: {context} Generate a question about the above context.\nTask description B: Based on the given context, craft a common-sense question, especially those that are\nLONG, INTERESTING, and COMPLEX. The goal is to write questions that are easy for humans and hard\nfor AI machines! To create such questions, here are some suggestions: A. What may (or may not) be the\nplausible reason for an event? B. What may (or may not) happen before (or after, or during) an event? C.\nWhat may (or may not) be a plausible fact about someone (or something)? D. What may (or may not)\nhappen if an event happens (or did not happen)? You can also create other types of questions. DO NOT\nmake your question answerable without looking at the context, or question of which the correct answer can\nbe directly extracted from the context. DO NOT ask a question that requires very specialized knowledge\nthat is not common sense. DO NOT ask too simple or too short questions. Your question must be related to\nthe context and answerable with common sense. Try to add more variations and complexity to the questions.\nPositive examples: Input b positive 1: Context: I was told, in person over the phone, that my shoes were\non their way. They have my money. I have no shoes. Output b positive 1: What may happen before I called\nthem? Explanation b positive 1: The question can not be answered directly from context and requires\ncommonsense.\nInput b potitive 2: Context: you see , at my age relationship is kind of important and i thought i got the one\nafter all these years . I noticed that once again i was wrong . i was good simply because i was good , i was\ncaring , helping , supportive , bla bla blaaa . Output b potitive 2: What may happen to me? Explanation\nb positive 2: The question can not be answered directly from context and requires commonsense.\nNegative examples: Input b negative 1: Context: I was told, in person over the phone, that my shoes were\non their way. They have my money. I have no shoes. Output b negative 1: What is on the way to my home?\nExplanation b negative 1: It can be directly answered with a span of the context and does not require any\ncommonsense reasoning.\nInput b negative 2: Context: GPS technology dates back to the time when first ever satellite was launched in\nthe sky in 1979. The era of global positioning started then. Output b negative 2: What was launched in the\nsky in 1979? Explanation b negative 2: It can be directly answered with a span of the context and does not\nrequire any commonsense reasoning.\n17\nPublished in Transactions on Machine Learning Research (12/2023)\nE\nModel Implementation Details\nThe hyper-parameters for training include a maximum source data length of 1024, a maximum target data\nlength of 128, a cap of 100 instances per task for both training and evaluation, a batch size of 16 for training,\na learning rate of 0.00001, a total of 2 training epochs, linear learning rate scheduling, and a warm-up period\nconsisting of 1000 steps.\n18\n"
  }
]